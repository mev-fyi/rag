00:00:00.280 - 00:01:02.646, Speaker A: I think that the way that I would put it is the integrated approach. I think that people are really wrong in the Ethereum community to dismiss that, because it's far more likely that in the end state DeFi primitives will be run on something that looks way more like Solana than Ethereum. I think the SVM is just a better choice for a lot of applications that demand high throughput and compostability. But I think that those chains will be run in an aggregated environment where the applications that don't require such tight composability with those DeFi primitives that are running in high throughput environments will be run on different chains. And so I think that these two efforts are really complimentary, where you have vertical scaling that's occurring on integrated chains like Solana and Aptos and sweat. And those are really, really important for the future of crypto. But I think that aggregation is what allows us to offer developers more choice.
00:01:02.646 - 00:01:16.050, Speaker A: And so they can use heterogeneous environments, they can access different trade offs for applications that might benefit from those different trade offs. And so that's kind of the way that I see the future developing.
00:01:19.230 - 00:02:12.396, Speaker B: Brendan, thank you so much for coming on the podcast. Really looking forward to this. Thank you again. I just, we're kind of chit chatting a little bit before the podcast and excited to nerd out on all the technical details that you have been working on as one of the co founders of Polygon, really diving into the aggregation layer that you guys have recently announced and all the unique intricacies that come with that from the engineering design, from the product point of view, and ultimately how that is reflected in the user experience. Because at the end of the day, I think what I would love to really push forward from the crypto industry is getting towards products. And I think all this infrastructure is really that stepping stone that will allow us to get to that point, hopefully sooner than later. So thank you again for coming on and diving into the details with me.
00:02:12.588 - 00:02:15.120, Speaker A: Yeah, thanks, Logan. Really appreciate you having me.
00:02:15.500 - 00:02:45.450, Speaker B: Amazing. So if we can maybe start high level with the polygon aggregation, or maybe even higher level than that, where do you see the future state or the end state blockchain architectures ultimately looking like? Because I know there's many different computing ecosystems, all for developer attention from the user experience, as kind of mentioned. I'm curious what the polygon point of view is on the end state of this architecture design.
00:02:45.950 - 00:03:54.760, Speaker A: Yeah, so maybe I'll start super, super high level and you can prop me to get to the point if I don't get there, but I think the way to look at it is the scaling debate is sort of caught between two sides. So on the one hand you have the monolithic side, where proponents argue that sort of the best way to scale is with super high capacity l one s like Solana or SWe or Aptos. And on the other hand, you have the modular side, which says that there are efficiency benefits to be gained from splitting up different components of blockchains. But moreover, the way that you want to scale is with many, many chains functioning as the execution layer for crypto. I think that both sides get something right. I think on the monolithic side, there's a really, really good point that's made, which is that when we talk about scaling in crypto, we're really talking about scaling access to liquidity into shared state. And it doesn't really matter if we have a ton of block space that we're adding, if it means fragmenting liquidity in state.
00:03:54.760 - 00:05:01.240, Speaker A: And then on the modular side, I think that there's a good point, which is that basically there are two arguments. The first is the argument that a single chain, if we're being very, very optimistic, a single chain is probably not going to accommodate all of the capacity of crypto or web3. And on the other hand, different applications have heterogeneous requirements for security and latency and user experience, and they might want different execution environments that are finely tuned to their application. And we should be able to permit this. When we talk about building an Internet scale crypto ecosystem, the polygon approach is basically to say, look like the monolithic side is right. We need to be able to scale access to liquidity and to shared state. And so let's see if we can basically take the best of both worlds and like develop a multi chain modular ecosystem where settlement happens on Ethereum and we have this universe of many chains.
00:05:01.240 - 00:05:36.630, Speaker A: But fundamentally, we don't sacrifice unification of liquidity. Like we have a unified state, unified liquidity. And the experience to users feels like using a single chain, even as they sort of go between, like many chains, many different execution environments. And so I think there's a lot that's required to kind of make that happen. But in like at a very high level, that's the dream, is like a modular ecosystem, or I think a multi chain ecosystem is a better way to put it, with unified liquidity instead.
00:05:37.370 - 00:06:49.438, Speaker B: Amazing. Definitely appreciate all the additional context. Maybe if we could dive into, since you mentioned the modular or integrated approach versus the multi chain or modular thesis, in my mind, you can really scale both, whether it's integrated or modular. To me, the big things that you need to scale is like the data availability layer, obviously having high throughput, doing some form of parallel execution, whether that's integrated or doing parallel execution across multiple L2s. And then consensus can be, as you scale number of nodes, it can be a bit of a challenge. And then also the read and write access to state. How, though, when you kind of view this from an engineering point of view, what bottlenecks do you ultimately see that kind of the modular, or the integrated chains or monolithic chains will bought up to that, say a modular chain would have a little bit more ease in addressing.
00:06:49.614 - 00:07:56.814, Speaker A: Yeah, so I think it's two things. So I think like, obviously Solana has done amazing work in implementing things like parallel execution and local fee markets. And I think that goes a long way to isolating hotspots where popular applications are driving a ton of congestion and making sure that that doesn't degrade the experience for other users. But the problem is that those hotspots are still, all applications on Solana are still contending for certain shared resources. So that could be network bandwidth, it could be like RPC demand. And so I think that when you look at it, the nice property of modular systems is that we can have completely separate execution environments that are running on entirely separate chains, and there's no contention for any shared resources in that respect. I think the second point is that different applications, you can imagine, would require different things.
00:07:56.814 - 00:09:13.784, Speaker A: So you could have a game where the economic value of that game is very, very low, and sort of like the security risk for someone rugging users or stealing user assets is pretty low. The requirement for censorship resistance is maybe not something that users really care about. You can get a lot of performance and better user experience for that game if you're running in a different setting where you have maybe a single sequencer. This isn't something that conforms to the platonic ideals of what blockchain should be about, but it gives a better user experience for specific cases where users might care about other things more than censorship, resistance, and decentralization. Likewise, you can imagine, even for applications we have now, like clouds or dexs, you can imagine an execution environment that is specifically tuned for that application where you can get higher efficiency, you can exploit parallelism in a way that you can't on existing vms. And so I think those two things is being able to add capacity elastically in the form of new block space that's disconnected from existing chains. I think is a nice feature.
00:09:13.784 - 00:09:49.978, Speaker A: And the second is being able to accommodate heterogeneity in terms of latency and security and censorship, resistance and execution environments. I think those are advantages of the modular approach. But obviously, I think for a lot of users, Solana offers the best experience because it's doing the thing that we care most about, which is scaling access to composable shared state and liquidity and a single user base. I think that's how I would frame that debate.
00:09:50.154 - 00:11:10.110, Speaker B: Yeah, I think it is interesting. I personally have gone fairly far down the rabbit hole just because I think, again, what I get excited about is the user applications from all this, because I think I fell in love with Ethereum in 2017, but just grew increasingly frustrated as it strayed from the original vision of banking the unbanked. And to me, that promise of really scaling Ethereum was going a little bit to the wayside, and you're getting farther away from like, actual applications. And learning, like, the technical nuance has really helped me come to understand of like, all right, how can we actually scale the different blockchain architectures and what was the true limitations of each? And I think it's just important why I really started the podcast was just to get smart people like yourself to come on and explain the engineering differences between the different chains. And not that there's a right way or a wrong way, so to speak. I think it's really just putting all the facts out on the table and saying this is the trade offs that we're making, because I think each chain makes their own trade offs, as you kind of highlighted. And this is what we're optimizing for.
00:11:10.110 - 00:12:15.230, Speaker B: And so excited to continue to nail down into the specific things that polygon is optimizing for. I think the two things that you mentioned there kind of on the integrated side that would be more comparative to what polychain and the aggregation layer are unlocking is the shared resources that a higher integrated chain ultimately will have, and then the customizations that an l two allows you to do. And so I'm curious, if we were to nail into the shared resources, is that, when you mentioned that, is that for like the polygon or the aggregation layer, is that being able to scale up kind of additional nodes in the l two or increase block times? How do you kind of envision that unfolding compared to, say, a high throughput integrated blockchain that may just do like, intra validator sharding and add more compute capacity as the network gets saturated or more bandwidth?
00:12:15.620 - 00:12:27.396, Speaker A: Yeah, sure. So I think that's like a really good way to put it. The way that I would describe it is in the integrated side. And sorry if like monolithic is maybe.
00:12:27.428 - 00:12:31.044, Speaker B: Like, no, no, either or. I go back and forth on the.
00:12:31.052 - 00:13:35.518, Speaker A: Words, yeah, I'll say integrated from here on out. But on the integrated side, there's this really interesting phenomenon where you have like really high value applications that really need censorship, resistance and decentralization. And so these could be like clubs or loan primitives or everything DeFi related, and you really care about censorship, resistance and being run in a decentralized setting. And then you also have games. And so I would argue that games actually don't care that much about being run in that setting. And on an integrated chain, those games and those really valuable DeFi primitives are contending for the same disk resources, memory bandwidth. And so it doesn't actually make sense for the game, which is a low economic value application, to be contending for those resources against super high value deFi applications.
00:13:35.518 - 00:14:41.470, Speaker A: And so the idea from the polygon perspective is we can add capacity in the form of new chains that are entirely logically and architecturally separate from the chains that actually run our DeFi applications. I think that's a really nice property of the aggregated approach, because those chains can still share liquidity. You could take a the NFT that you get from your game and you could access liquidity and marketplaces on other chains. But the nice thing is that those applications don't have to contend at all times for shared resources. And so I think that the way that I would put it is the integrated approach. I think that people are really wrong in the Ethereum community to dismiss that, because it's far more likely that in the end state DeFi primitives will be run on something that looks way more like Solana than Ethereum. I think the SVM is just a better choice for a lot of applications that demand high throughput and composability.
00:14:41.470 - 00:15:31.660, Speaker A: But I think that those chains will be run in an aggregated environment where the applications that don't require such tight composability with those DeFi primitives that are running in high throughput environments will be run on different chains. I think that these two efforts are really complimentary, where you have vertical scaling that's occurring on integrated chains like Solana and Aptos and SwE. And those are really, really important for the future of crypto. But I think that aggregation is what allows us to offer developers more choice. They can use heterogeneous environments, access like different trade offs for applications that might benefit from those different trade offs. And so that's kind of the way that I see the future developing, I think.
00:15:31.740 - 00:17:02.596, Speaker B: Yeah, it makes a lot of sense. And I almost kind of envision a world where these architectures kind of converge, so to speak. Just because theoretically, if you wanted to run like a L2, or one of these asynchronous execution environments and settle to either a polygon or a solana or a sue, I think that is almost a similar type architecture where that aggregation layer is the high throughput data availability layer that's aggregating everything that can almost be settled on. I think, to your point, I envision a world, and I'm curious to see, hear your thoughts on this, as well as where kind of the high throughput integrated chains are really focused on, I would say all to all propagation, where all the nodes in the network are generally receiving the information in a very low latency. It is a lot of bandwidth to propagate that information to all nodes, but all those nodes are really synchronized pretty much in real time. And then on the other side of this kind of spectrum, where I see the endgame is kind of what Vitalik's endgame post was, where it's like the ZK proof on top of the ZK proof that's on top of the ZK proof that will settle to an aggregation layer. And in this world, the asynchronous composer, asynchronous execution environments, latency really doesn't matter as much.
00:17:02.596 - 00:17:50.300, Speaker B: You and I, Brendan, can run a transaction, we can create a proof that happened, settle that to another L2, or layer three, and then settle that ultimately to polygon aggregation layer, where then that's where the liquidity is aggregated and shared amongst all other transaction, and that's my mental model, one is making sure state is synchronized as fast as possible, and then the other one is asynchronous execution environments that have the proof aggregation that can potentially, potentially settle to something like polygon aggregation layer. Is that like, do you think that's directionally correct, or do you feel like there's other worldviews that could possibly exist in that?
00:17:50.840 - 00:18:42.520, Speaker A: Yeah, so I definitely think it's directionally correct. One nitpick is that chains would still be settling to ethereum in the end. The AG layer is basically what allows there to be safety for cross chain interaction that occurs at lower than ethereum latency speed. But I think you're completely right. If we were running two chains and we wanted to be more tightly coupled, or to do asynchronous interaction or even synchronous interaction with atomic bundles. We could do that, and we could do it in a way that's cryptographically safe, because the AG layer would enforce that. I couldn't equivocate and rug your chain, or I couldn't produce an invalid block that would steal funds from your chain.
00:18:42.520 - 00:19:20.202, Speaker A: We can get into what exactly the AG layer is, but I think that's the right way to look at it. Instead of having all to all constantly synchronized state, we have chains that are maybe connecting pairwise and can have tight composability or asynchronous interaction between small subsets of chains. But there's no longer a requirement that some game that's running often in some other corner of the polygon ecosystem or the AG layer ecosystem would need to know what's happening on your chain if.
00:19:20.226 - 00:19:30.316, Speaker B: It'S logically separated, until it's settled to that aggregation layer, where everything would be then shared amongst everybody, that's settled there.
00:19:30.508 - 00:20:11.384, Speaker A: Yeah. So the AG layer doesn't provide DA like chains would navigate their own DA. So it could be like you use Ethereum for DA or Celestia, or you just use a committee. But the AG layer is basically what ensures consistency across the entire ecosystem so that you know, that no chain can equivocate or can build on inconsistent state, that all bundles have to be correctly executed. You can't have a chain that gets unbundled and only includes a part of an atomic bundle. And so the AG layer basically cryptographically enforces that and provides safety before that final proof is verified by Ethereum.
00:20:11.552 - 00:20:50.066, Speaker B: Gotcha. Okay, perfect. Well, maybe this is a good time to pivot into the ag layer. I think we've prefaced this conversation very nicely with kind of starting with the future or end state of blockchains, kind of the different kind of two forks in the road, one with like synchronized state and the integrated blockchain, and the other one kind of more this asynchronous worldview and that kind of polygon ag layer is kind of pursuing and, yeah, happy to kick it off and maybe go into a little bit of the technical details and how this aggregation ultimately works.
00:20:50.248 - 00:21:34.160, Speaker A: Yeah, sure. So the way that I would think about the iglare is it basically does two things. So it allows chains to coordinate or to operate asynchronously. And we call it synchronously with atomic bundles at lower than ethereum latency. And the second thing that it allows is like, it provides safety for chains that use a shared bridge. So all of the l one assets are locked in this single contract, and the l two native assets are locked in a shared bridge and are shared across chains. And so we can kind of get into what that means exactly.
00:21:34.160 - 00:22:31.470, Speaker A: Like, if you think about how Ethereum works and how fragmented the l two ecosystem is right now, like, if I have ethnic on polygon ZKVM and I want to move it to Zksync, the only way that I can do that trustlessly is through Ethereum. I would have to initiate a transaction on ZKVM. I would have to wait for a proof to be generated, which might take five minutes. I need to wait for that proof to be verified on Ethereum, which takes twelve to 19 minutes. For that block to be finalized on Ethereum. I need to deposit my ETH to Zksync, and I need to wait for my deposit transaction to be finalized on Ethereum, which takes another twelve to 19 minutes. And so we're looking at maybe like a 30 to 45 minutes to an hour transaction, which is not, I think, a great UX or like a great model for Internet scale crypto, I think you might agree.
00:22:31.510 - 00:23:02.364, Speaker B: But I agree with that, especially prior to my time at Tesla was at Expedia Group. And we would measure like bounce rates just on how long it would take a page to load. Yeah, if a page loaded in like slower than a second, kind of just making up the number, but he would like increase bounce rate by like 20%. And so 40 minutes or 30 minutes, I think, is a little ungodly for what people are really accustomed to in today's modern age.
00:23:02.542 - 00:23:56.570, Speaker A: Yeah, I think that's exactly right. And so the fundamental concept behind the ag layer is like, let's do two things. So let's enable unified liquidity and asset fungibility across all chains in the polygon ecosystem. Like, I should be able to have ETH on polygon ZKVM, send it over to polygon Zkpos and get Eth. I shouldn't get like a wrapped synthetic version of a token that I then have to swap into the native version. And the second thing, I think the most important thing is that we should be able to have super, super low latency cross chain transactions, both asynchronous and synchronous in a way that's safe. Like, the danger is if you're running a chain and I'm running a chain and you're accepting a cross chain message from me, then I can send you across chain message, but then I could equivocate and I could set, like, create a different state of the chain and settle that on Ethereum.
00:23:56.570 - 00:24:30.914, Speaker A: And your chain would be rugged. Like, the bridge would be under collateralized, or there would be some issue. Similarly, I could create an invalid state for my chain. And if you want to accept messages at lower latency than it takes for me to generate a proof, you could be rugged. I might mint a billion eth and send you some amount of money. And your bridge would be, would be under collateralized. And so the idea behind the ag layer is really simple.
00:24:30.914 - 00:25:19.032, Speaker A: It's just, we allow people to conditionally accept, we allow chains to conditionally accept messages or conditionally accept bundles from other chains, and we ensure, like, cryptographically that their chain cannot be settled to ethereum if it depends on an inconsistent or an invalid state from another chain. There are basically two components that enable this. The first is the cryptographic component that we built at Polygon, and the second is the coordination layer. So we call this emergent coordination infrastructure that allows this ecosystem to work. And this could be things like relays. It could be things like shared sequencers or builders that are actually building cross chain bundles that affect many chains. That's how we see the future.
00:25:19.032 - 00:25:30.900, Speaker A: Developing is like providing this guarantee of cryptographic safety and then allowing other projects to build shared sequencers on top of that foundation.
00:25:32.320 - 00:26:00.194, Speaker B: Definitely appreciate all the product point of view and the end goal from the technical perspective, maybe diving into that a little deeper. You mentioned shared sequencers, but it's also not a data availability layer. Can you dive into, I mean, is it kind of one aggregation node that's everybody sending it to? Is there like consensus because you have multiple nodes, all the nitty gritty and the technical side.
00:26:00.362 - 00:26:53.746, Speaker A: Yeah. So the ag layer, like, from a technical perspective, is a decentralized service that's run by validating nodes, that takes in proofs from all these chains and verifies that those proofs are basically consistent with one another, aggregates those proofs, and then settles that to Ethereum. It's not a DA layer. Chains can pick whatever da mechanism they'd like, whether that's Ethereum or an external dA service or a committee. But that's basically how you should think about that. The cryptographic component of the AG layer is as a decentralized service that's, uh, taking in and aggregating proofs. But fundamentally, like, proof aggregation doesn't satisfy the design considerations that we have set out alone.
00:26:53.746 - 00:27:47.798, Speaker A: Like, we also need those proofs to check that, like, chain states are consistent with one another. That bundles are in fact executed across all chains that are included in that bundle. But that's the way that I would think about it, is like a shared sequencer actually enables coordination between chains, but in order to do that trustlessly, a shared sequencer could behave maliciously and could include one part of a bundle on another chain, but could ensure that the transaction in that bundle fails to execute another chain. And so in order to not have to trust a shared sequencer, you need something like the AG layer to ensure that the chain states are consistent and that bundles are executed across the.
00:27:47.814 - 00:28:15.748, Speaker B: So maybe let's dive into that a little bit and parse apart the technical nuances or differences between, like, a shared sequencer and the ag layer. You mentioned the ag layer is decentralized. Are there any numbers today of, like, how many nodes are ultimately running the ag layer, or will run the ag layer, and how they're, I'm assuming there's some type of consensus mechanism for those nodes to talk to each other in between them.
00:28:15.924 - 00:29:06.640, Speaker A: Yeah, so I should clarify that that component of the ag layer is not live yet. So there's your nodes that are currently running the ag layer. But, yeah, so the interesting thing is that because the only sort of job that those nodes need to do is in its proof aggregation. And when you have proof aggregation, it's actually not necessary to replicate proofs to every node in the network. So you could have some proofs that are replicated to some subset of nodes. Those proofs aggregate, or those nodes aggregate those proofs and then come up with, like, present an aggregated proof to the rest of the network. And so it's this nice property where it's sort of has this trustless, parallelized design, because it's the same.
00:29:06.640 - 00:29:30.580, Speaker A: Checking some subset of proofs is identical to checking the aggregated proof that verifies that subset. And so the thing that nodes need to come to consensus on is the final proof that gets posted to Ethereum that basically reflects the pre conformation of the ecosystem before it's verified by theory.
00:29:31.040 - 00:30:25.910, Speaker B: Okay, that does make sense to me, but I want to make sure I can kind of explain it a little bit in layman speaks. And please correct me if I'm wrong. So, for example, if you had like 100 l three s, all these have kind of like siloed individual execution environments. They can be tailored however you want. Those hundred l three s create proofs to a L2, which can kind of, for simplicity sakes, be aggregated into like 50 proofs, and then those can all be submitted to the ag layer, they don't have to be necessarily verified by this decentralized aggregation layer because they're just validity proofs that can say this has happened and you can combine those ultimately into one proof and that one proof ultimately is submitted to Ethereum.
00:30:26.570 - 00:31:24.914, Speaker A: Yeah, so we sort of hate the term l three s because in this model, maybe a spicy take, l three s don't make any sense. And this isn't like, I'm not being critical of you. I think that the way that you're describing it is a perfectly reasonable way to put it. But if you think about it, what l three implies is that chains are settling to an l two. So to get to ethereum, they basically have to go through an l two. This makes sense if you don't have something like the aggregation layer because it basically guarantees that all the proofs for your l three s, instead of having to be verified on ethereum individually, which is really expensive, they'll be aggregated effectively on nl two, and then that l two proof, exactly like you said, will be posted to ethereum. But with the ag layer, we can make everything an l two.
00:31:24.914 - 00:32:36.560, Speaker A: So all of these l three s can settle directly to ethereum. And this changes the topology of the ecosystem because you can think of l three s only being connected if they share an l two. But now we can connect every chain, in effect, because every chain has like a guarantee that it will only be settled to ethereum if and only if it relies on valid states from other chains. And so what we do is instead of relying on l two s to do proof aggregation, we just do proof aggregation separately, sort of like out of band on the ag layer. And then all of those l two s that maybe used to be l three s pay like a very, very small cost for ethereum verification, because that ethereum verification cost is amortized across like a huge number of chains. And so I think this is really nice because we don't have in this model, why would you be an l three when you can be an l two? You can always settle directly to ethereum. You can trade liquidity with ethereum, you can trade liquidity with every other chain in the polygon ecosystem.
00:32:36.560 - 00:33:36.960, Speaker A: And so I think it's a much better model. And I think it's like this is maybe an aside, but I think it's strange to think about basically replicating ethereum, where you're building this platform as an l two, where a bunch of l three s are settling to your platform. And it makes sense, I think in a short term value capture, value accrual perspective. But that's not what we're trying to build at Polygon. We're trying to build a fully neutral and shared ecosystem where everyone is on the same footing. And we don't have this topology where you need to be an l three on polygon, or polygon needs to be the intermediary between your chain and ethereum. It's just join this ecosystem, you're on a completely equal footing with every other chain in that ecosystem, and you still get all the benefits of not having to pay Ethereum verification cost.
00:33:37.580 - 00:34:56.490, Speaker B: Yeah, I think it's beautiful explanation, and I think to me it again highlights the differences from the technical point of view of the integrated chain thesis, which is propagating state as quickly as possible. All nodes receiving the same information essentially in real time, really focused on state synchronization, and then the aggregation or the Zk ification of the world where you are having polygon aggregation layer, where there are these separate execution environments, or each l two, that then creates the proof, the aggregation layer aggregates those proofs and then settles to Ethereum or whatever other DA layer that you went. And I think those nuances are just important to clarify for the builders and the users. And in this polygon ag layer worldview. And correct me if I'm wrong, is the liquidity only shared once that proof has been created? Or where does that shared liquidity less fragmentation ultimately come in if each of these l two s can ultimately serve as their own execution environment?
00:34:56.870 - 00:35:30.950, Speaker A: Yeah, yeah. So I think that's a very, very good question. So when we say shared liquidity, we effectively mean two things. So we mean like asset fungibility. So basically, like I was saying earlier, you should never have to use a wrapped synthetic version of a token. Like you should be able to just take that native token, move it through the bridge, the shared bridge, and get that native token on any other chain. Then the second thing is the low latency interaction that's powered by the ag layer.
00:35:30.950 - 00:36:51.978, Speaker A: This is actually a really interesting topic, because there are some subtle things that you really, really need to be careful of in this ecosystem. The first thing is that the way that we enable asset fungibility is by taking all the l one and l two native assets and depositing them in this bridge. And the sort of downside or the drawback, if you take a naive approach to handling this, is assuming that you support heterogeneous execution environments. That means that you need to allow anyone to add a new vm and a new ZK prover that proves the validity of this vm. And for every marginal prover that you admit into this ecosystem, the probability that there's a soundness bug somewhere in the ecosystem goes up. And if there's a soundness bug, that's actually catastrophic for this ecosystem, like for the naive version of the Pygon ecosystem, because it means that any unsound prover can drain the entire bridge. So you could have some shit chain number 1000 came up with a ZK prover that's unsound.
00:36:51.978 - 00:38:43.440, Speaker A: It turns out that an attacker or the operator of that chain can mint like a billion ETH and can withdraw that ETH to Ethereum and drain the entire bridge. A really, really important property that the AG layer provides is that it checks that each chain. It basically tracks a total running balance of tokens for each chain, and it ensures that a chain cannot withdraw more tokens than are deposited in that chain at any point in time. From the AG layer perspective, you actually don't need to trust that the prover for a given chain is sound. If you're a user on that chain, you actually need to be very careful that your funds are deposited on a chain with a sound prover, because otherwise your funds could be, could be rugged by an attacker. But from the AG layer perspective, we're able to have these pools of shared tokens and shared liquidity in the bridge without the risk that an unsound prover in some distant corner of the ecosystem can rug or can steal funds from other chains. That's a really, really important point, I think, wrapping around to actually answer your question, what gives us shared liquidity is this ability to have funds that are stored in a shared bridge, and we don't need to wait for the proof to be verified or generated, or by the AG layer, or verified on ethereum, because we can operate within the ag layer at lower latency than ethereum or proof generation speed.
00:38:43.440 - 00:39:10.860, Speaker A: But we know that if there is something that goes wrong with another chain that we're depending on, that our chain will be safe and will be isolated from soundness issues, or from a chain equivocating, or a chain producing an invalid block. That's what allows us to have shared liquidity that operates at lower than proof generation latency, if that makes sense.
00:39:11.400 - 00:39:51.240, Speaker B: It does. Maybe if we could attack it from another angle, from the user experience or product side. Is that from, say, optimism settles to ag layer and then you want to use a polygon? L two does the optimism? L two create a proof that submits it to the aggregation layer. Once that aggregate, once it is accepted or validated in the aggregation layer, then you have access to that liquidity on this polygon chain too.
00:39:51.780 - 00:40:04.920, Speaker A: Yeah. So for the optimism case, the optimistic chain would need to convert to a ZK roll up, or at least provide a ZK proof of consensus for its latest.
00:40:05.330 - 00:40:08.510, Speaker B: Maybe that was in a no. No.
00:40:09.970 - 00:41:09.690, Speaker A: I think it will be. But the crucial thing is the optimistic chain would need to deposit its l one native assets into a shared bridge. And that basically guarantees that the way that most third party bridges work now is funds are escrowed in some bridge contract, and then a wrap synthetic version of that asset is minted on the destination chain. And that doesn't happen in the polygon ecosystem, because all funds are escrowed in the chain that they were minted on. So l one native assets are all escrowed in the bridge contract on Ethereum. And that's what allows us to basically guarantee that as we move funds between chains in this ecosystem, they're always collateralized by an l one native version of the SS on Ethereum. And so those are basically the two requirements is like, there has to be some way to verify the latest state of a chain.
00:41:09.690 - 00:41:58.830, Speaker A: And so the best way for this to happen is for an optimistic roll up to upgrade to a Zk rollup. And second, the funds that are backing the l one native assets on that roll up must be deposited into a shared bridge. The reason I think that this is a really interesting example is if you think about it from the perspective of an optimistic roll up, they only gain by doing this. Unlike other l two ecosystems, Polygon doesn't place any restrictions on the token that they use for staking or for gas. There's no requirement to do a revenue share or some profit sharing agreement between the chain and the DAO. You can use whatever execution environment you want. You don't have to be bound by Polygon governance.
00:41:58.830 - 00:42:39.930, Speaker A: Basically, a chain is giving up nothing, and they're now able to access shared liquidity and all the users that exist across all polygon chains, all the shared state that exists in the polygon ecosystem, the combined aggregate liquidity of all chains in the ecosystem. I think it's a really interesting trade for other l two s and even l one s to join this ecosystem and be able to sort of operate within the ecosystem instead of within a silo or a walled garden.
00:42:40.550 - 00:43:08.010, Speaker B: So when you're moving the assets from, I guess like the layer one into this bridge contract or the ag layer, is this kind of like de facto the new layer one my terminology may be like incorrect, but if you're moving all the assets to here, is that like kind of the canonical version is that's where, like, they all actually living.
00:43:08.310 - 00:43:39.940, Speaker A: Well, so the assets stay on l one, right? They're just deposited in a different contract on l one. The way that I would describe it is every chain in the polygon ecosystem settles to l one. The ultimate decider of finality is still the ethereum contract verifying the aggregated proof for all chains. The AG layer just allows us to have much lower latency in cross chain interaction than ethereum finality.
00:43:40.450 - 00:44:02.590, Speaker B: And you're able to have lower latency because you're aggregating those proofs. And then able is there, like, I guess if you're not doing consensus, is there, you wouldn't need for block times, would you? Or do you still have to communicate in some form or fashion to the other decentralized nodes within the ag layer?
00:44:03.850 - 00:45:25.440, Speaker A: The way that I would put it is that interaction happens between chains. So if I'm running chain a and you're running chain beat, and let's say that we have super, super low block times, we have either through a decentralized sequencer or a centralized sequencer, you can accept messages from me as soon as they are finalized by my consensus mechanism. But fundamentally, when you submit a proof to the AG layer, you include this commitment that basically says, I depended on this state from chain a, my block is not valid unless that state, like the same state from chain a, is also submitted to the Ag layer. And so when the Ag layer aggregates proofs, it doesn't just aggregate proofs of state validity. It also checks that all of these cross chain messages were built and received by consistent states. And all the if we're talking about synchronous composability, the atomic bundles that were executed across chains were all successfully executed. And then it checks that even if my prover is unsound and created an invalid block and generated a proof for that invalid block, that I'm not able to withdraw more than the tokens that were deposited in my chain.
00:45:25.440 - 00:45:42.430, Speaker A: It provides safety for these two things, cross chain interaction at lower than ethereum latency. And it provides safety for an ecosystem where we have a shared bridge that functions across heterogeneous execution environments.
00:45:43.410 - 00:46:10.190, Speaker B: That makes sense. I guess the only piece that I'm missing here is with the atomic bundles or just making sure chain a and chain b are valid state transitions and this atomic bundle can happen because each are valid. If there's not block time or consensus. How often or how frequent are the Ag layer able to perform these functions?
00:46:11.050 - 00:47:24.650, Speaker A: So the Ag layer is not actually the component of the Ag layer that polygon has built, is not responsible for coordinating between chains. What actually does that coordination would be a shared sequencer and specifically basically a really advanced, highly centralized block builder that would be running a bunch of full nodes for chains and executing transactions across those chains to basically come up with a bundle and then a proof that every transaction in that bundle was executed successfully across all chains. The ag layer, I think, is a dumb decentralized service where all it's doing is proof aggregation and checking that the bundles received are consistent across every chain. It doesn't actually execute those bundles. It just checks like, okay, chain B committed to the inclusion of this bundle. Let's make sure that chain C and chain D and chain e also committed to that bundle. And if we know that they committed to that bundle, then the proof guarantees that that bundle was executed successfully across all those chains.
00:47:24.990 - 00:47:59.720, Speaker B: Okay, so maybe just a quick like mental monitor check you have again, you're going back to 100 L2s. These are each individual single sequencers. But to make that decentralized, you use the shared sequencer network across those hundred L2s to have some atomic composability within those. That shared sequencer would ultimately publish the state transitions to the ag layer. The AG layer could aggregate those and then post them to ethereum. The l one.
00:48:00.420 - 00:49:04.608, Speaker A: Yeah. So the way that I would look at shared sequencers, and I had this misconception for the longest time, and I thought shared sequencing was incredibly stupid because I didn't understand how it was meant to work. But I think that the l two s, they could be a single sequencer or they could have decentralized sequencing. But what shared sequencing really means is it's a chain that says, okay, let's take this slot and let's auction off the proposal rights to this slot for to like a community of builders that are like, you know, centralized. They're running in data centers, they're hyper professional. Those builders are going to look at all of the slots and all the transactions that they have received and all the slots that are available across a bunch of different chains, and they're going to build blocks that basically maximize profit for both the sequencer of a chain or the operator of a chain and for themselves in the form of Mevv. That's the way that I would look at it.
00:49:04.608 - 00:49:37.588, Speaker A: The shared sequencing mechanism is a marketplace that enables coordination across chains, but the shared sequencer itself is not like taking. I think the naive way to, and the way that I thought shared sequencers worked was they took in like a ton of different transactions. They ordered all, one of the validators within the shared sequencer at every slot would order all of those transactions. I think the better way to look at it is like shared sequencing is just a marketplace that enables coordination between builders and between operators of chainst.
00:49:37.674 - 00:50:21.790, Speaker B: Yep, totally aligned there, I think. Yeah, in crypto, we like to complicate things, and wording can ultimately get confusing. But yes, I appreciate the clarification. And so in that like synchronization or the MEV, what was the exact words that you use making sure that MEV is kind of maximized across these chains, that proof or result could be settled to the aggregation layer. And then if there's other l, two s or other shared sequencers, they could also post to the aggregation layer. And those kind of combined proofs can then be subtle to Ethereum.
00:50:22.250 - 00:50:23.554, Speaker A: Yes, exactly.
00:50:23.722 - 00:50:25.470, Speaker B: Okay, interesting.
00:50:27.010 - 00:51:16.600, Speaker A: Oh, sorry. But the nice thing is that there doesn't have to be coordination. You could have subsets of chains that don't need to compose or to interoperate at all with the chains run by a particular shared sequencer. And so these can be sort of on entirely different timelines. There's no cost required to synchronize between those chains. And I think that this is interesting because I think that there will be a cost to synchronous composability. Part of the design principle around the Ag layer is let's let chains decide when to pay that cost for synchrony, because most chains probably won't actually need to ever interoperate in a synchronous way.
00:51:16.600 - 00:52:08.486, Speaker A: I think this connects to what you were saying earlier about the design approach between the integrated and the aggregated models, where the integrated chains are constantly paying that cost of synchrony. And I think that this makes sense for a lot of those applications for DeFi or for an order book or something. Of course you're going to always pay that cost to synchronize. But I think that I wonder if that will lead to a world in which individual integrated chains focus on a particular vertical or a particular use case and then have this way of interacting asynchronously through something like the Ag layer with chains that they don't need to pay the cost to synchronize with.
00:52:08.678 - 00:52:21.650, Speaker B: Yeah, I mean, theoretically you could create like a Solana suite, aptos Monad type architecture within a L2 that has all to all communication that could settle to the aggregation layer that could connect with other things.
00:52:23.590 - 00:53:02.470, Speaker A: Exactly. And this is why I think it's so short sighted for people in the ethereum community to criticize Solana or integrated chains. Because ultimately, if you think about what's required in the endgame to run a highly performant Dex or some really, really performance critical piece of defi infrastructure, it's not going to be the EVM. I'm sorry to all the Ethereum maxis, but it's just not going to be. And so I think it's really, really important to work on, like we're working on horizontal scaling. I think it's also really important to work on vertical scaling.
00:53:02.890 - 00:53:32.180, Speaker B: Yeah, agreed. I mean, one of my favorite sayings from Tesla was like, you can't fight physics. You can kind of move some of these pieces around. If you don't want to do high throughput on the data or on the layer one, like Ethereum, you can move that to, say, an eigen DA or a Celestia, but that high throughput still has to happen somewhere in the stack. And if you don't want to do high compute at the base layer, you can push that to a L2. But it still has to happen at somewhere in the stack. And it's not like ZK magic eliminates it.
00:53:32.180 - 00:53:40.392, Speaker B: It allows you to just run it on one node and verify that. But that is still one beefy machine running all those computations.
00:53:40.576 - 00:53:41.780, Speaker A: Yeah, for sure.
00:53:43.400 - 00:55:00.240, Speaker B: I think it's fascinating to me. I just really love diving into the different architecture designs and having these types of conversations, because I think it's very important not only for the engineers, but kind of the builders are not only for the users, but also the builders as well, trying to figure out what ecosystem works best for their type of application. Do they want like kind of this all to all synchronized state? Do they want to build that as a L2 and build out like the decentralized validator set, or do they want to build in an integrated chain? Do you want this kind of composability with the AG layer? There's a lot of different trade offs, but I think the thing that really excites me now is that there are these different opportunities builders can choose. And at the end of the day, I think we're now just getting to the point where we're moving one layer up the stack and can actually focus more on applications and application engineers can focus on that and not focus on infrastructure as much, which to me is the only way that we can really progress as an industry to get from out of the infrastructure kind of infrastructure games, and move up the stack to build usable products.
00:55:00.740 - 00:55:58.250, Speaker A: Yeah, I mean, I think that's been like a really, really crucial insight for us, because I think it's like the status quo for l two s is actually really bad, because if you think about what's required now for an L two, you could be a builder that's building a really cool NfT collection or a game, or some really, really innovative defi primitive. And so you need to succeed at doing that, but you also need to succeed at a lot of things that maybe you aren't good at. So you need to launch an l two, you need to bootstrap liquidity on your l two. You need to do airdrops and liquidity mining incentives. And this is repeated across every single l two. And so no longer are you focused on, okay, I need to build the best NFT collection or the best defi primitive. It's also in order for my chain to succeed, I need to bootstrap liquidity and users and integrations.
00:55:58.250 - 00:56:38.198, Speaker A: I think this is a really compelling, from the l two perspective, a really compelling vision for Polygon, because the liquidity is already there, the users are already there. You can focus on your area of comparative advantage, where if you build the best defi primitive, like, you can instantly tap into a pool of liquidity that's already there. You don't have to bootstrap it or build it from scratch. I think this will just lead to a much more efficient experience for developers, and hopefully one that can become more product focused instead of more crypto liquidity mining game focused.
00:56:38.374 - 00:56:42.086, Speaker B: It's a fun game, but at the end of the day, we do have to build products.
00:56:42.278 - 00:56:44.930, Speaker A: Yeah, awesome.
00:56:46.270 - 00:57:27.790, Speaker B: I feel like we've kind of touched on a lot, starting from the overall vision, kind of comparing and contrasting, kind of the more integrated versus modular approach, getting into the technical details, and pretty good depth on the ag layer, how that ultimately comes about. Talking about differences between shared sequencing and ag layer versus the proof aggregation, the product perspective as well. Is there anything in particular that you feel like we haven't touched upon or missed that you feel is important for either the engineer or the crypto community more broadly to understand about AG layer or polygon at a higher level?
00:57:29.130 - 00:58:38.112, Speaker A: No, I think that's pretty comprehensive. I think in the past we've done a really poor job of focusing and actually letting people know what's important and what's the architecture of the polygon ecosystem. And now it's basically just the AG layer, and we have polygon CDK, which allows people to build chains that connect to the AG layer. But I think it's going to be for us. We're really excited to see further l one s and l two s joining the ag layer and participating in this. This community that we believe is fundamentally positive sum and a win win decision for chains. Because from the perspective of the polygon ecosystem, if another chain, whether it's an l one or l two, has economic value, there's something on that chain that provides economic value to users, then it's going to be purely positive sum for the polygon ecosystem to be able to access that economic value on that chain, and vice versa.
00:58:38.112 - 00:59:11.090, Speaker A: If you're building something that's innovative or a more capital efficient defi primitive, you get access to all the liquidity and all the users that are in the polygon ecosystem. I think it's a really exciting vision for how we solve l two fragmentation. In a way that's nothing value extractive. Like there's no. You don't have to pay a tax to like the polygon dao in order to use the ag layer. Like there's. There's nothing that's sort of like restrictive or limiting for you as a builder.
00:59:11.090 - 00:59:14.890, Speaker A: I think it's just purely positive sum and growing the pie.
00:59:15.470 - 01:00:16.874, Speaker B: Yeah, trying to. I think the biggest thing has been as we, the modular world continues to kind of grow, making sure that to me, the magic of the blockchains, which was the shared state, unified liquidity, continues to. We continue to uphold that, because that is where a lot of the interesting stuff can happen. I wouldn't even say just atomic composability, but just developers being able to access and touch different parts of state, being able to do some, even if it's not atomic composability, but compose through different blocks and kind of add on unique features. To me, that's kind of the magic sauce that makes blockchains magic. And so appreciate that polygon ag layer and yourself, Brendan, are pushing that forward for the industry. Because I think this has been a real wake up call.
01:00:16.874 - 01:00:32.080, Speaker B: I think to the l two community of. It's a little bit more cumbersome than we originally thought, the original vision. And we need to really unify that as much as possible to make the UX as simplistic as possible.
01:00:32.860 - 01:00:35.960, Speaker A: Completely agree. I think that's a really good way to put it.
01:00:36.300 - 01:01:06.700, Speaker B: Awesome. Well, I really appreciate you coming on, Brendan, thank you so much for nerding out with me for an hour, diving into the differences of all the tech stack. But more importantly, thank you for helping push the industry forward, because I think truly, blockchain is very cool technology. I think, honestly, some of the coolest technology in the entire world right now. Anybody can build on it, have access to it. You don't need 10,000 gpu's to get started. You just need a computer and an Internet connection.
01:01:06.700 - 01:01:13.800, Speaker B: And I appreciate what you and the Polygon team are trying to do to advance that vision of bringing this technology to more people.
01:01:14.260 - 01:01:16.820, Speaker A: Yeah, thanks for having me, Logan. Really appreciate being here.
