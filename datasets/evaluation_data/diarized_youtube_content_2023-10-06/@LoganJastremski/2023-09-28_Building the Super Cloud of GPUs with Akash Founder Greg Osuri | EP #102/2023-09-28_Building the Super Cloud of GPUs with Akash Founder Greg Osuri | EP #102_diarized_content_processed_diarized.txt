00:00:00.560 - 00:00:40.270, Speaker A: With the people cloud as a foundation, we can build incredible products. So it's very important to understand that a user uses a product because they see value in it. And then over the last three years, we did about six updates, five updates really after the first one. And with each update, we added new features, be it discoverable to your store and whatnot. But the last feature we added is the most important feature so far, which is GPU's. I don't know if you saw the news, but turns out it is very hard to get GPU's right now through the cloud, even through your traditional hardware channels. Right? It's impossible.
00:00:40.270 - 00:00:55.530, Speaker A: And why? Because two main reasons. There is an increased demand for GPU's for machine learning applications or AI, and there is a huge supply crunch for making these chips.
00:00:58.280 - 00:01:22.700, Speaker B: Greg, thank you so much for coming on the podcast. Really looking forward to this one and really looking forward to talking about what you're building at Akash. I think in general, the world is just now starting to appreciate kind of breaking out of these monopolies, being able to access decentralized compute. And you guys are really at the forefront of this new wave and you've been working on it for some time.
00:01:23.500 - 00:01:46.636, Speaker A: Yes. Thank you so much for having me, Logan. I know we've been trying to get this under the books for a while, but super excited to be here and super excited to be building Akash. It's been been at it for a little over eight years, and seeing the network grow gradually and breaking new barriers and breaking new grounds is exciting.
00:01:46.828 - 00:02:13.740, Speaker B: Yeah, it's fantastic. And maybe I would say the people in the decentralized physical infrastructure space are more familiar with who you are now. But for those that are not as familiar with deepen or Akash, could you perhaps just do a quick intro on yourself? How ultimately you came about the idea of building the cash network and really, what got you excited about starting the problem in 2015?
00:02:15.610 - 00:03:03.350, Speaker A: Well, the work began a lot before 2015, really. I think my interaction with the cloud, so cloud is this invisible layer that's so integral to our society that we use it pretty much. I would say, at least I use it 24 hours. Everything I interact with interacts with the cloud in some form or the other, including my bed, that talks to the cloud. There is no escaping the cloud. So we use a cloud every day without realizing that we use the cloud. There is this invisible layer that was gaining importance, really.
00:03:03.350 - 00:04:38.892, Speaker A: I think in 2000, 822, seven timeframe, I discovered this. There's this amazing way you could, you know, when you're application developer. When you're developing applications, Internet based applications, you're required to have something called a server. That means a large machine that sits on another machine called the data center, right? So it's just another computer hosting a web based software. So I, you know, spent quite a lot of time in working for enterprise companies doing quite a lot of, you know, large scale deployments. And one of the companies I was working at was Kaiser permanente in California where I helped design the, now they call the internal cloud, where I helped design a lot of their internal patient monitoring systems, right? So a normal use of normal assumption is that you need to have a server and then you need to buy the server and have the server sitting in a data center, right? But I discovered this new service where you can just rent out space instead of buying. So it's like it was so important because it's the paradigm shift of learning that, hey, you can actually lease an apartment versus buying an apartment, right? I mean, it's obvious now, but you can see the excitement that paradigm shift that, hey, you can actually lease someone else's computer versus your computer to host your Internet software.
00:04:38.892 - 00:04:59.702, Speaker A: So that was an incredible realization for me. And I was a cloud evangelist. In fact, I was super pro cloud. I was building on something that was very early. Amazon was one of the companies that I really fell in love with. And then quickly we realized this cloud. Then I started moving to startup companies in San Francisco.
00:04:59.702 - 00:05:55.440, Speaker A: I was working with several companies in Mission district in SF in 2000, 920, ten timeframe. And SF is the pinnacle of technology. And we realize anything that's new, at least it was, right? I guess it still is. But anything new and great really starts off with SF, right? So we started noticing a trend that all new companies, new companies back then were GitHub. GitHub was founded in 2006, I think 2007, Timeframe, GitHub, we look at Basecamp, we look at Twitter, all these new, Netflix included, all these new companies were starting off on cloud. So it's no longer just, hey, this new service you can rent, it's actually becomes your first sort of like choice for shipping new software.
00:05:56.300 - 00:06:08.348, Speaker B: Historically, like, if you didn't start on cloud, like everybody was trying to migrate to cloud, you were really at a large disadvantage from like auto scaling and everything if you were not kind of on cloud from the get go.
00:06:08.524 - 00:06:56.980, Speaker A: Oh, this is way before auto scaling even was reality to scale on demand. Auto scaling came in 2014, right? All that bells and whistles came way later. But this is the ability, the fundamental ability to rent someone's space versus having your own servers was a big paradigm shift. So it became very evident that cloud is going to gain importance in our lives to a point that it was taking over. So people were getting rid of the data centers to go out to the cloud. And that presents another problem, heavy centralization. So a lot of us in the industry and in the academia recognize that cloud, if left alone the way it is, it is going to go to a size that is detrimental to our society.
00:06:56.980 - 00:07:33.946, Speaker A: Why? Because when you have something so big and so concentrated, you end up with inefficiencies. A large inefficiency would be price resource distribution like we're seeing right now with GPU's. It's impossible to get GPU's because cloud has been the traditional way to get compute right. And you have stifling innovation. A lot of the times you see cloud actually crushing open source companies. We saw lawsuits, we saw us government investigations on Amazon. A whole lot of impact, this heavy centralization bar.
00:07:33.946 - 00:08:34.924, Speaker A: So a lot of us, it was a very common expectation that we have to do something. So that's when the real idea of a supercloud was birthing. I think the roots go back to 2012 2013 timeframe as they're working on this notion of like, hey, can we reduce a cloud to a, to a layer that's just contributing resources instead of cloud as a first point of getting access to resources. So what that means is, can we create a super cloud that resides on all clouds and essentially takes the control away, moves the control plane to a higher level wherein the user is in control versus a cloud provider, dictating who should get what resources. So id of a supercloud was formed 2015, I think Cornell wrote a paper. I started contributing quite a lot on this commodization of cloud platform called Kubernetes. I was early contributor to this ecosystem.
00:08:34.924 - 00:09:10.536, Speaker A: I've written a few libraries that are still very actively used by this gigantic new platform called Kubernetes. This is 2014, 2015, nobody heard of Kubernetes created overclock Labs to take kubernetes to market. If you think about, there's a tech crunch article somewhere that talks about overclock labs bets on kubernetes. Now it's very obvious that Kubernetes is used by 80% of the cloud. We immediately saw the potential. I put together how a multi cloud Kubernetes could look. There's a demo somewhere out there in 2015 and that became the basis for the supercloud essentially.
00:09:10.536 - 00:09:52.688, Speaker A: And Supercloud was formulated by Cornell 2015. And then we started the company in 2015 around the timeframe. And it took us about a year and a half of experimentation, research to really write what the paper on supercloud, the Akash network, the decentralized marketplace, we published in 2017. And you can see the foundations and the motivation section as we read. We wrote that a supercloud greatly beneficial machine learning. And this is in 2018, right? Look at the white paper, motivation section and 2023. Now you see machine learning as AI and all that 100% shift to AI now.
00:09:52.744 - 00:11:05.040, Speaker B: Because it's crazy how whether it kind of be AI or crypto or even decentralized computing, it seems like the hot topic can be moved around very quickly. And today, especially after chat, GPT-3 and four, it's become fairly obvious that, I mean, you have more amount of data, you throw kind of more compute at it, you get more interesting results. And I think to that point, and you have been like cracked in identifying a lot of these things super early, is that a lot of the cloud providers now kind of are monopolies in some sense, that they hold a lot of power. And kind of the idea to come about to kind of flip that power model really put the power back into the people. But not only putting the power back into the people, I think over the long term, actually creating a better product. And so, yeah, I would love to just dive more into Akash and kind of the super cloud and what you guys have been trying to build since putting out that paper.
00:11:06.020 - 00:11:47.186, Speaker A: Great. Yes. I mean, power back to the people is what we, what we want, right? I'm like, come and take it with a GPU here. But with the people cloud as a foundation, we can build incredible products. So it's very important to understand that a user uses a product because they see value in it. They may not connect to the values that is aligned to them, but the perceived value of a cloud, we started tackling what's the most valuable feature that makes Akash stand out. So the most obvious one is decentralization.
00:11:47.186 - 00:12:28.798, Speaker A: So we began two years after publishing the paper, we released the Testnet and eventually the Mainnet in 2020. And the platform launch was ephemeral platform that's non custodial in nature, first of its kind. So, so far, the way you would use a cloud was you had to go to Amazon and Google and Microsoft and log in, give you a credit card. But here for the first time, you do not need to give a credit card or have an email password. You can actually deploy something in a non custodial manner. And that's a game changer. Right? So again, we were very early in this ecosystem called Cosmos.
00:12:28.798 - 00:13:22.270, Speaker A: We are the first Cosmos chain, have a track record of identifying good tech. And we selected Cosmos because of its layer one capabilities, the only SDK to essentially deploy a layer one chain. And that also means it's limiting in terms of the ecosystem it brings with. So it's only limited by the wallet the ecosystem provides, which is Kepler. And we were, you know, I think for the most part, safe to say about 40% of Cosmos ecosystem deploys on Akash of some form or either non custodial websites. Great for daos on custodial web services, call it data services for Daos a whole lot. And we also saw some interesting stories like where we, there was somebody from, I think this is a kid in Africa who never used the cloud before.
00:13:22.270 - 00:13:52.396, Speaker A: And I, someone sent him like, AKT tokens on the Internet. I think it was a tip bot that sent him like some AKT tokens. And he was able to deploy for the first time ever without a credit card. He wouldn't, he couldn't use a cloud because there was no credit card. And that sort of like, tells you quite a lot. Like, hey, Akash can go places where your traditional software just cannot. Because the assumption that you need a credit card, the assumption that you need these payment rails that the west are so used to is so different.
00:13:52.396 - 00:14:25.978, Speaker A: Every country now has their own payment trails. India has UPI system, which is like, completely indian, Chinese have their own system. So is there a global standard on how do you use a cloud? Akash is answering that question. Right. So we saw some of these interesting usage patterns, and then over the last three years, we did about six updates, five updates, really after the first one. And the, with each update, we added new features, be it discoverability with store and whatnot. But the last feature we added is the most important feature so far, which is GPU's.
00:14:25.978 - 00:14:26.874, Speaker A: Right.
00:14:27.042 - 00:14:32.110, Speaker B: And historically, up until that point, was it strictly cpu's or file storage?
00:14:32.730 - 00:14:59.670, Speaker A: So we need a foundation for GPU based applications to function, which is container runtime, ephemeral container runtime. That means no storage. Then we added storage. That means the ability to attache network storage and boundary storage. You can request how much average storage you want. And third is discoverability using IP addresses. That way you can have an IP address with a port of your choice, and you can route and you can discover other services.
00:14:59.670 - 00:15:51.120, Speaker A: So you needed this foundation to have compute intensive or data intensive applications, really data intensive application with machine learning and whatnot. So GPU's were a new feature that did not exist before. Why GPU's are important is, I don't know if you saw the news, but turns out it is very hard to get GPU's right now through the cloud, even through your traditional hardware channels. It's impossible. Why? Because two main reasons. There is an increased demand for GPU's for machine learning applications or AI. And there is a huge supply crunch for making these chips.
00:15:51.120 - 00:16:32.190, Speaker A: Machine learning or AI, uses GPU's because GPU's are incredible. GPU's for those that don't know what they are. They were traditionally they stand for graphical processing units. They're what we call accelerators. That means they accelerate a certain function or certain type of mathematical operations way better than a CPU can. Sometimes in the range of thousand times better. And turns out these acceleration features are incredible for machine learning, especially to do matrix multiplications.
00:16:32.190 - 00:17:20.400, Speaker A: GPU's are proven to be amazing for these applications. And there are three companies that make them, really Nvidia, AMD and Intel, in which Nvidia has 80% market share. Why? Because they make the best GPU's for machine learning to drill down. They make the best software and hardware combination. So Nvidia has a software suite called CuDA. That means if you write your application for CUDA, you can run the application on pretty much any Nvidia that supports the CUDA, the version that you use. And CuDA by far is the most mature, most widely accepted standard for developing machine learning or data intensive applications.
00:17:20.400 - 00:17:52.060, Speaker A: And AMD is not even close. It's really bad. And we look at really Nvidia as like Apple of GPU's, they have the best sort of device. When you're building something innovative, you always build for the best device, not necessarily the most widely used device in this case, it's also most widely used device, happened to be. But that presents another problem, right, this is supply chain. Can Nvidia produce chips as fast as the. The demand is growing? Turns out they cannot.
00:17:52.060 - 00:18:30.098, Speaker A: So today it takes about the Nvidia's best and most advanced chip is called H 100. Today it takes about two years to get HGX clustered directly from Nvidia. You try to go to the cloud to get the chips. Because remember, cloud is this new paradigm of renting compute. Cloud is the first sort of choice for any company to go get compute because, well, it's easy to get from the cloud. And two, cloud is known for its abundance, right? Scale when you want it. So now, if you want your chips, you go to the cloud.
00:18:30.098 - 00:18:34.630, Speaker A: Guess what? There are no chips. Why? Because cloud is out.
00:18:36.850 - 00:19:19.232, Speaker B: It is pretty amazing just how much the demand for GPU's has risen. I mean, I just remember even building, I built a custom PC for a gaming computer. And to get a 3080 was extremely hard. And then every time the 4080 or the new series every year, I mean, they come out. And these are just on the consumer side of GPU's not even like the more high powered, kind of like h 100s, as you were mentioning. They're extremely hard to get. And I think before it was kind of used on kind of whether that was like Ethereum mining or some type of mining for cryptocurrencies.
00:19:19.232 - 00:19:46.550, Speaker B: And now it's kind of like the digital gold for AI. And that demand capacity, especially on the AI side, is, I don't think, slowing down. I think truly what we have found just in these neural network models is the more data that you have and the larger compute that you can do, you just get more interesting results. It is remarkable. And the fact that Nvidia also has majority of market share, I don't know if that's good or bad.
00:19:49.090 - 00:20:12.180, Speaker A: Bad in the short term, good in the long term. I mean, the market share is coming because there is demand for AI. So that's good. Depending on what lens you look at it, we can go down the rabbit hole. But I saw it, interesting stats somewhere that said 1% of global gdp will be used for machine learning very soon.
00:20:12.220 - 00:20:13.120, Speaker B: I believe it.
00:20:13.540 - 00:20:47.616, Speaker A: And that's. Yeah, I believe it as well, because it's not hard to see how machine learning can enrich people's lives. We haven't touched the, you know, we're so early, we haven't even touched what machine learning can do at the current level, basic level. Like, one of my favorite use cases would be medical diagnostics. Like, imagine it's known. It's proven that LLMs are actually, LLMs means large language models, which is a type of machine or type of AI are incredible. Which chat.
00:20:47.616 - 00:21:45.250, Speaker A: GPD is an LLM. Right. GPT four, in terms of GPT four is very accurate for medical diagnostics. Why? Because, you know, diagnosing something is essential recollection, right? It's limited by the human capability to recollect. So when your doctors are looking at your lab results, they're trying to, you know, pattern match and trying to remember, recall from the memory from the, from the med school and figure out what's wrong with you, right? So it turns out machines are way better at it. So what that means is it's not going to replace doctors, but it's going to make them a lot more productive. Now, it doesn't take a long time for a doctor to look at all your results, but rather just put through the lab results through an LLM and figure out get early diagnostics and have a redundancy across multiple LLMs to get the accuracy of the diagnostics.
00:21:45.250 - 00:22:29.106, Speaker A: And some cases, even I saw Chai GPD doing way better than some of the doctors. People would put their, you know, lab results and chat gpu would tell them like, look, this is something that your doctor probably hadn't looked at. So, yeah, so medical diagnosis is just one of those areas that is so needed. Right? Imagine now, in places where it's hard to get healthcare, a single doctor can see a lot more patients than what they're able to. So imagine the kind of competition power we need for that. Imagine kind of computational power that we're looking for. Creative arts right now, right? So there are just basic use cases like that right now, LLMs can do, haven't even penetrated to any degree of satisfaction.
00:22:29.106 - 00:22:55.230, Speaker A: Right. So I think the demand is going to just, you know, not going to stop anytime soon. The only question is, how do we offset the supply? One of the ways is to make supply more accessible. That's what Akash is doing. Right. And there is supply. There is right now supply locked away in large companies that is unable to see the market because there is no way for the supply to come out.
00:22:57.090 - 00:23:40.680, Speaker B: Yeah, I think it makes a lot of sense. I mean, when something, the demand capacity of something literally 100 xs within a short time frame, all the existing supply out there is kind of figured out how to go towards really facilitating that demand. And so I think the access to a lot of the GPU's that historically have been underutilized or just even offline now have a great incentive to really be brought online because they can make money for what was previously an idle resource.
00:23:41.510 - 00:24:21.208, Speaker A: I agree. There's so much latent GPU capacity. Your gaming consoles at your homes have GPU capacity that is barely used. I have two gaming consoles that I probably use once a year during Christmas. That's it. Most of the time, it's just sitting there and all the way to say, your phones and your device at home, there are a lot more devices than humans on the planet. So what happens to those devices that are not being used right now? So there's enormous supply, and the demand is not going to slow down.
00:24:21.208 - 00:25:12.018, Speaker A: Now, the question is, how do we connect this demand to the supply? Is it going to be a heavily controlled system, or is it going to be a decentralized system that works like magic? And why that is important is decentralized and open systems can go far, whereas centralized and closed systems can go fast. So a lot of times when you look at cloud and look at a lot of the systems, we take the centralized route, because the cost of coordination is lower for centralized systems. So we reduce the cost of coordination. It becomes faster. But if you ease the mode of coordination, you can go further. That's why I believe open and decentralized systems will reach places where centralized systems cannot.
00:25:12.074 - 00:26:01.820, Speaker B: Just, yeah, let's dive into it ultimately started Akash in 2015. Slowly realized that you needed to build the supercloud. Really focusing on the GPU side. You've recently made a advancements to add GPU's. Can you share a little bit more about kind of the current network metrics? I know you've been putting out some on Twitter about how the network's doing. I think I've seen a couple of spikes recently in supply capacity. But can you talk about one, the incentive on the cost side to kind of add a GPU to the network that could potentially be idle to be more fully utilized, how you're going to bootstrap the compute side? And then again, like, how will you try to facilitate the demand side as well?
00:26:02.120 - 00:26:23.158, Speaker A: Sure. So cash lot launched about four weeks ago, and during which we saw a very interesting patterns with demand supply. Right. We saw a few companies that were former mining companies, I believe. Foundry. Not former, actually. They do mining as well.
00:26:23.158 - 00:26:48.310, Speaker A: Foundry staking or mining. It's a DCG company, listed about 50 some odd GPU's on Akash. On day one, we had no idea they were about to. And that's the beauty of permissionless systems. There's no sign up, it's just there you launch and you see there are GPU's. And that was pleasantly surprising. And they bought in, like advanced GPU's a right.
00:26:48.310 - 00:27:48.860, Speaker A: I mean, they bought in a 100 SDE that just brewed out a thesis that, hey, there is mining software, mining hardware. That's not cloud, that's just sitting there with a bunch of GPU's. And we started seeing similar type of companies all over pop up supplying GPU's. Obviously, if you are a professional operation, you're going to justify your ROI first before you can make investments. So we started seeing slowly the capacity ramp up, and along with that, surprisingly, demand also ramped up now we have believe around 60% utilization of the network out of which the high end GPU's, which are most high demand, are 100% utilized. That proves our thesis that there is demand. Although we know this, we talk to people, it's actually good to see the actual thesis play out with quantitative confidence.
00:27:48.860 - 00:28:30.560, Speaker A: That's what we're able to prove. And that confidence is gaining as we get adoption. Of course, Akash is relatively unknown. I mean, it's known in the crypto world, but the users of Akash happen to be machine learning and very few people know about Akash and several reasons. We just were not that sort of like aggressive in spreading the world before product, because a lot of products promise GPU's, but GPU's are these hot new things on the market. And we all know what products are promised GPU's but don't actually deliver GPU's look like. So we wanted to avoid that because reputation is a big, big deal.
00:28:30.560 - 00:29:12.456, Speaker A: So now we are stepping up our efforts in reaching out to the broader audience and we expect volatility in demand supply for the next three months. And that's expected for a new product. But right now it's more step function, right? So demand comes, supply come. So supply comes, demand comes, and the frequency is wider and that frequency is narrowing as we get adoption plans for adoption. Big barrier right now for adoption is crypto. As much as we love crypto, crypto is great for supply, but not so good for demand. So the two aspects of this two sided marketplace, demand and supply for supply.
00:29:12.456 - 00:29:35.890, Speaker A: We know how to scale supply. We've seen over and over again with networks like helium, Filecoin. Crypto is incredible at attracting supply. Why? Because we have incentives. So we can design incentives to incentivize a certain type of behavior in this supplying the right of compute. Mark my keyword, the right type of compute. And not just any compute.
00:29:35.890 - 00:30:09.534, Speaker A: That means we need a degree of homogeneity in the compute, where Akash is very heterogeneous in supporting the compute model. How do we identify the homogeneity? We identify the demand side, right? So we can incentivize, say we see demand quite a lot for a 100s, more than h 100s. Even the h 100s are better. But turns out devs are actually building for a 100s because it's more accessible. So there are these dynamics of supply demand that we need to understand. And that's why we did not do incentives at launch. And we wanted to incentives post launch at growth phase.
00:30:09.534 - 00:31:26.190, Speaker A: And now next big thing is to turn on incentives for providers while we do so. Enable frictionless deployment on a cache means removing barriers for the limit of folks to use a cache network, especially for machine learning devs. Right now, they happen to be crypto. If you're in crypto, if you're a crypto dev, if you're one of the 17,000 active developers in crypto, crypto is great because once you are in the crypto land, it's just magic, right? So you can swap tokens, you can do all kinds of fun things, but if you're not in crypto land, it's. It is scary to use crypto because the cognitive load is so high that you need to incur that you may not get to value in the right amount of time for you to actually convert to be a user. So we need to reduce a cognitive load by removing barriers, which is giving an option for using a cache without a wallet. Go even further, actually building a product that machine learning devs like to use in their workflows, regardless of crypto or not.
00:31:26.310 - 00:32:22.986, Speaker B: Yeah, yeah. Can you talk about, I mean, just briefly kind of. You said initially when the network start, you didn't really have the incentives. We've seen a lot of other kind of earlier generation of the decentralized physical infrastructure. Networks, as you mentioned, have the capability to attract a lot of supply to the network because of those incentives. Can you talk a little bit more? Just about once those incentives are turned on for a cosh kind of what they're going to look like? Oh, yeah. Will anybody be able to add their home gpu? Like, I have a 3080, can I add it? Maybe it's not the right type of compute, but if I had a 4080, how much, I guess, are you looking just for like, those higher end servers on the GPU side versus like, the everyday person that may have a PS five or something that they could potentially add as well.
00:32:23.178 - 00:32:38.596, Speaker A: Right. So, gosh, when I say incentives, you can still earn money if you have a 3090 4090, as long as there's demand for it. So if you have a 3090 listing sitting somewhere, just go put it on a cache versus what, we're not using it.
00:32:38.708 - 00:32:39.284, Speaker B: Yep.
00:32:39.412 - 00:33:04.122, Speaker A: Right. So there is an inherent incentive for you to list the incentives I'm talking about for unused computing. What that means is crypto has been traditionally good at attracting supply first without demand. Right. That means these suppliers need to have some form of income to justify their. Their supply on the network. Gollum was able to track quite a lot of supply.
00:33:04.122 - 00:33:51.750, Speaker A: Helium had about 300,000 nodes with very little demand. Filecoin has, I guess 1% utilization. Really? So 99% unused disks, right? People will do crazy things if there's a good incentive. I mean, the kind of filecoin providers are nuts, right? Like they're. Anyway, so crypto over and over and again proves that if there is and incentives, people will supply. That's proven. For Akash, the current linear modeling we did can support up to an excess of 1000 or 1500 a 100s that's unused at any given time.
00:33:51.750 - 00:34:14.574, Speaker A: So what that translates to is a believe are about 30 to 40 nineties. Don't quote me on this, but roughly around that. I mean, I might have messed with the conversion in terms of floating point 32 flops.
00:34:14.702 - 00:34:15.490, Speaker B: Okay.
00:34:17.230 - 00:35:37.810, Speaker A: So that's fairly large, this fairly 32,490 equivalent of excess supply at any given time. So you want the excess supply because that's attractive for two things, for devs to come and look at the network and actually be convinced that there are chips they can leverage. And two, it reduces the price because excess supply means even though we are artificially sort of like incentivizing the extra supply, the price still will drop. So it's very attractive for a buyer. And we are incentivizing excess supply, not current supply, because if there's demand, you don't need incentivize. So that's how we're looking at incentive models, where traditionally incentive models did not take into account the supply or, sorry, the demand. Because my thesis is so far, crypto, deep in networks either offered a commodity like storage, that's excess in nature, or a brand new product like helium that doesn't have a market.
00:35:37.810 - 00:36:18.318, Speaker A: So far, crypto has not been able to provide a resource that's high in demand. So we had to think about incentives in a little different way. Like what happens when you have something like 100% utilization for a 100s right now? Like how does incentives look like? Do we need to still give out free a 100s? No, we don't. That would be stupid, right? Then you just. So how do we design an incentive model based on the current demand to ensure there's always supply on a cash network? So that's how we think about incentives. Obviously, if you want to join this discussion, Akash is fully open source. Every aspect of Akash is open source.
00:36:18.318 - 00:36:56.152, Speaker A: And Akash operates by open groups called special interest groups. And there are special interest groups for every aspect of the network. And there's one for economics. So I highly encourage you to go to Akash GitHub, the Cathig economics, and you'll get all the design, sort of like discussions around listening center models, all the trade offs that people are talking about, different phases of testing out incentive models. Very fascinating to get involved. Your economist, or if you have interest in seeing how this new paradigm of high demand resource is designed, is a new design space for deepen.
00:36:56.336 - 00:37:55.050, Speaker B: Yeah, I do fully agree. It's an interesting rabbit hole. And you could go down pretty far in terms of just actually operating the network. One is creating both the demand and supply side. But once that network is operational, and especially on the lower compute side, having say, 4080 or consumer GPU's, what in terms of the raw resources are needed to string those together and create this supercomputer? Are individual or professionals really incentivize to have high bandwidth connections? Do you need low bandwidth or high bandwidth? Can you talk a little bit about utilizing multiple gpu's instead of just one single gpu to make this large cluster?
00:37:55.510 - 00:38:56.946, Speaker A: So the way it works, the way you think about feasibility, is to take two variables into consideration. First is the cost of compute and cost of communication. Your total cost is these both together, if the total cost is actually lower than if you're evaluating two sources for training your model. Machine learning model is essentially a neural network. There are neurons fired from each nets, and now the way it works is in node a and node b. They both talk to each other, and the cost is essentially the cost to run node a and node b, and the cost to communicate between these two. And there is cost involves both time as well, because the longer it takes, the longer you need nodes running, the shorter it takes, shorter you need this nodes running.
00:38:56.946 - 00:39:50.930, Speaker A: That's why the bandwidth is extremely critical for cost of machine learning. So that's why you have heavily distributed clusters that are globally distributed, may not be as efficient as local clusters if the cost is the same. But if you reduce the cost of this geographically distributed clusters to a point where it's significantly cheaper than the counterpart, it is optimal. So the question 4090s are great. They're great chips. Three thousand ninety s, forty nineties, they're not as fast as or powerful as h, but they are good chips. But if you give me a million 4090s clustered in a single location, that's great.
00:39:50.930 - 00:41:51.280, Speaker A: But the question is, can you get a million 49 t's in a single location? Or if you give me a million 49 ts in a single city, may not be in a single data center. I guess I can get by with ten millisecond latency between clusters, right? But if you give me a million GPU's for all across the world, or even continental us course to coast, about 300 milliseconds, right? Latency that may not be 300 milliseconds is a lot for each packet, that may not be great, but if the cost of compute is marginal, even zero, or close to zero maybe, or if we employ techniques that can reduce the cost of communication, can we have like a topology where you have large local clusters that have the gradients? So there are techniques that are being developed now so that can leverage a large distributed cluster. Why it's not very obvious now, because so far the research has been uni focused on local clusters and we have nothing put enough work into optimizing for large clusters, because most work is right now on in AI is really to optimize for better and higher performance models, not cheaper cost of operations. Because optimizing cost of operation comes post production, not pre production. So before you productionize something or productize something, you want to move as fast as possible, you want to iterate as fast as possible. So we're still in a very iterative phase of AI. I think that's going to be the case until we get AGI, because race to AGI is a lot more important than saving money, because AI has not penetrated the globe yet.
00:41:51.280 - 00:42:41.470, Speaker A: But what we notice is as models get mature, like open source models like Lama Lama two, that is very powerful, not as powerful as GPT four, but it's getting there very close. As models mature over time, we start seeing a lot of optimizations that can leverage distributed clusters. We see Lama alpaca run really well on 3090, for example, inference. We are seeing some of the bottom up research now that can leverage large clusters that are globally replicated. So, to answer your question, right now it's not the most optimal setup to leverage a globally distributed clusters of 4090s. But in two years, I bet you is going to be the case. That's really where we're heading.
00:42:42.370 - 00:43:23.200, Speaker B: As you mentioned, the two primary costs are just the raw hardware costs itself of the GPU's, and then the bandwidth cost between them to communicate. In that world. Where do you see the end state of Akash long term? Do you see these local clusters forming up of large GPU's in certain regions, and people able to tap into them? They have lower milliseconds to communicate because they're geographically co located? Or do you think it will just be geographically dispersed GPU's around the world that anybody can tap into and try to create local clusters.
00:43:24.300 - 00:43:27.880, Speaker A: I envision a world where every home will have a supercomputer.
00:43:30.300 - 00:43:31.920, Speaker B: There's no doubt about it.
00:43:34.540 - 00:43:53.906, Speaker A: As we, as AI gets more penetration, we're going to realize there are privacy implications for AIH. Peter Thiel was famous to say this in 2018. He said, AI is communists and blockchains are capitalists.
00:43:53.938 - 00:43:54.750, Speaker B: I love that.
00:43:56.210 - 00:44:57.450, Speaker A: And there's a lot of truth to it because AI empowers a tyrant, because you have single point of control and blockchains are for the lovers of liberty. Right. Going to take from Eric Wu for his. But really, I think sovereign AI is going to be key part of AI evolution. We haven't gotten there yet because slowly starting to realize, like, hey, chat, GPT knows a lot about you and you're giving it all your data without even realizing just how stupid and dumb we were about sharing our location every time. And in 2008 and we started seeing break ins and whatnot, right? So we're starting to see quite a lot of, like, I wouldn't say data abuse, but definitely like, you know, surveillance capitalism in full force right now. Right? And sovereign AI is possible considering you have sovereign compute.
00:44:57.450 - 00:45:45.040, Speaker A: Guess what? A lot of us actually have sovereign compute in our houses. There's so much GPU power in our homes. Gaming consoles, your computers, M two Mac has incredible GPU. You have your phones that are not used at times in night, especially in charge. There's enormous amount of unused compute in your home. And if 1% of the global GDP is spent on machine learning, why not that? GPU's that compute comes from your home. Just like today, you're able to produce energy from your renewables at your home and contribute back to the grid.
00:45:45.040 - 00:46:54.870, Speaker A: Why don't we have a similar mode of, you know, compute generation or compute contribution from your home to the, to the compute grid? That is going to be a reality. And I have no doubt in my mind if, if we bet that AI is going to take over our lives. Akashi is an early form of the technology. In fact, Akash is really good right now to deploy on a gaming server, not yet on your PlayStation five, but is it possible? Absolutely. So I see a future where all of us that use AI will have sovereignty, or I do not want to live in that world like that. And we are working very hard, not in a decentralized machine learning ecosystem, DML, to ensure that we have sovereignty over our data, our lives. It's important we do that or else we end up in a feudalistic society where you have the lords that have control over all aspect of our lives.
00:46:54.870 - 00:47:14.906, Speaker A: So I see a world, and as a optimist, I want to believe in a world that we are going to have sovereignty on the AI's. The only way to get sovereignty is control the chip. The only way to control the chip is to have it in your house. Right? Like, that's why I wear the shirt. Come and take it, because, you know.
00:47:14.938 - 00:47:15.682, Speaker B: It'S a great shirt.
00:47:15.746 - 00:47:23.390, Speaker A: You have your shirt, if you. Sorry, if you have your chips, you're protected by the loss of your land.
00:47:24.010 - 00:47:55.020, Speaker B: So in that world, I mean, particularly in the kind of GPU computing side, the two dominant forms of utilization today are that training aspect and then the inference as well. Do you see that? Majority of training is still kind of done outside of the house, but then the inference calls just live kind of on local compute as the model is already built, and just kind of pinging the model for requests.
00:47:56.720 - 00:48:46.342, Speaker A: I think both can be done in the house without leaving the nature. So there's foundational model that is trained on all the things the globe can. Right? And now we have something called fine tuning, which is a lighter weight training, or making the computer, making the model work for you, customizing the model to work for you. And there are several techniques in fine tuning itself that take less computational resource. A famous one is called Lora. Lora is a new technique where you can take a foundational model and you can train it and define classes that are not part of the foundational model. And so you can say man means greg, woman means my wife.
00:48:46.342 - 00:49:28.212, Speaker A: You can say dog means dog. You can actually train it with relatively less resources fairly quickly. Actually, you can do that right now on Akash. It's incredible. And have complete control over that aspect of the data and get the weights, essentially, and do not share the weights with anyone else, or only share the weights with your family or whoever you choose. So I think that's where the data marketplaces get very interesting. And so it is possible to have sovereign AI, and we are seeing the roots right now form little, not that easy to use.
00:49:28.212 - 00:49:53.932, Speaker A: But our goal is to make sure that the interaction gets simpler and simpler. And, like, honestly, like, the tech right now is. Sirius is so dumb. Right? Like, it's really dumb. I want to replace my Siri with essentially an AI that's sitting in my house and my house only. I do not want to connect. I do not have Alexa.
00:49:53.932 - 00:50:22.510, Speaker A: I do not have listening devices in the house only. Well, I do have some cameras, which is scary because that's connected to the cloud. Right? So I want to live in a world where I want guarantees that anything I say and I do lives in my house. It's going to get a lot more obvious with AI getting very, very powerful and enabling tyrants.
00:50:23.780 - 00:51:05.404, Speaker B: Yeah, I definitely appreciate that vision. I share a lot of the sentiments of not wanting your private data on someone else's server, especially as it gets more robust and even more intimate. The private conversations and private data should definitely live within the local compute instead of global compute. But to specifically on the cache network side and network utilization over the long term, do you think the network will be utilized more on training or will inference over time become a majority of the compute power using for the network?
00:51:05.572 - 00:51:29.650, Speaker A: I mean, you're always going to infer more than you train, just the nature of it. Just like they're always going to be more reads than writes for a database, right? So think of AI as a database. Training is writing, inference is reading. So Akash is going to. Right now, Akash is excellent for fine tuned models. That's what people right now are using. Akash for.
00:51:29.650 - 00:52:23.166, Speaker A: A big complaint we hear is using some of the cloud models, you don't have the customizability that you get with owning your own model. So people want to fine tune and people want to control the cost for that. Right now, if you're building a product, sure you can go start off on the cloud, which is quicker, but when you're actually getting users in scale, you want to run your own server because it gets exponentially very expensive. Like Netflix pays half the fees you pay to Netflix goes to AWS pretty well. People don't like that. I mean, this generation of companies are very, very aware of the runaway costs, uncontrolled runaway costs that the previous generation of companies had incurred. And we had to learn about all of them in the s one filings.
00:52:23.166 - 00:53:09.800, Speaker A: And so now people are trying to optimize those costs and getting early open source or getting early solventry. And our job, my job is to go make sure that you as a builder are not incurring heavy cognitive costs that will take away resources from building a product to actually running your AI. So we are making it extremely simple. Akash ML, for example, the goal is to get you to production in 30 seconds. Literally, you should come to the website, copy paste a bunch of code and fine tune your model and deploy a model in a matter of seconds. So that's what we are going for, right? Under 30 seconds is my mark. Like, I'm going to see people do this and I'm going to clock their time.
00:53:09.800 - 00:54:20.646, Speaker A: So. And even things like, I was doing my taxes yesterday, right, and had to go hunt down all the medical bills, like, because supposedly you get some tax break. Like, where do I go and type in, like, show me all my medical expenses or in 2022, you know, even basic functionality that's possible with AI today. And we haven't even gotten to, like, personal finance. Like, can I optimize? Like, it turns out I spent a lot of money in medical, and the question is, like, why don't I have an HSA? And like, I don't know, like, I think, like, I want to live in a future where my data, all my private data, gets fine tuned or ingested by an AI that I know is not going to expose or share my data with anyone that I don't want, especially financial health data. And I want it to recommend things as to how big should my FSA contribution be, considering my health costs. You know, last year were to a point that I should probably use FSA that can give me a tax benefit.
00:54:20.646 - 00:54:59.458, Speaker A: But do you know how much your FSA contribution should be? You don't like basic things like that, I think. And you realize how much money you're spending on Uber Eats versus groceries. What does that look like? Considering health benefits, there could be some correlation of, hey, uber eat consumption to your average walking heart rate. What does the correlation look like? Okay, more home cooked meals, better heart rate, higher. Where is that data that's being collected from my apple health? Why is just sitting in my phone? Why is it not going to AI that's getting me better insights?
00:54:59.634 - 00:55:00.434, Speaker B: 100%.
00:55:00.522 - 00:55:29.540, Speaker A: So much opportunity right now. Seriously, there's so much sort of like, I can throw like a thousand ideas right now that you can go build a company on buy union sovereignty. And biggest fear for me when I connect my data is to ensure that this data is not leaving my realm of control. And that's really, I think, is a key area of opportunity for sovereign AI builders.
00:55:30.000 - 00:56:05.860, Speaker B: Yeah, totally agree. In terms of looking ahead, say like three to five years, what do you feel like are going to be some of the largest challenges for continuing to bootstrap the network? Is it either going to be, do you see bigger challenges on getting GPU's to the network on the supply side, or do you think it's making the product, in a sense, simple enough that you abstract some of the crypto complexities away and get outside non crypto natives to start using the network for decentralized inference or training?
00:56:09.520 - 00:57:22.544, Speaker A: My ultimate vision is end to end custody of all aspects of the stack to the user. What that means is the user should have custody over the compute that they run on and our agency on the compute they should run on. That agency comes in custody and that means that we need a chain of trust that goes all the way to the user's key. Right now that seems very challenging, I'll be fully honest with you. Why? Because Akash is decentralized, but Akash ML is not? We can't. Do you optimize for user experience for a developer or do you optimize for sovereignty for the user? But without optimizing for developer experience, that sovereignty cannot be given to the user. So how do we really create truly decentralized systems where the end user has complete sovereignty of all aspects of the stack? That will be the biggest challenge for a cosh network to achieve.
00:57:22.544 - 00:58:04.098, Speaker A: And how we get there really depends on every step, how we take. It's like climbing a mountain, right? You know, you need to get to the top of mountain and you see a certain pathway, but that path could change as you climb. You gotta catch a different rock depending on the strength of the rock, right? So it is mountain climbing. We can see the top of the mountain, but we don't know what rock we need to climb the mountain with. And we're going to leverage every rock possible to get to the mountain so we can come down easier or go back easier next time. Right. So we're, that's going to be a challenge.
00:58:04.098 - 00:58:09.260, Speaker A: A truly decentralized end to end self custody system.
00:58:11.320 - 00:58:53.380, Speaker B: I guess over long term. One thing that ive been thinking about as well is do the next marginal user outside of the current crypto ecosystem value decentralization as much as the current participants. Im not sure we totally have concrete answers yet, but I think one of it is, to your point, how much do you balance the decentralization versus the more product experience? Do you feel like you have to compromise on one to achieve the other or over time both will sufficiently scale? Well be able to figure out some of these problems and just make it a more seamless experience while still having both of those properties.
00:58:54.640 - 00:59:35.980, Speaker A: I like to believe future where you have seamless experience while having sovereignty. That's a reality. What does it take? Well, it comes with the device that you're using. Like if Apple had key support in the devices, you know, across all devices we don't. If Apple removes a barrier of wallet install and wallet backup, right. If every Apple computer comes with a private key that you can customize with the Apple experience. So yeah, I believe in a world that the seamless crypto experience is a reality.
00:59:35.980 - 01:00:27.930, Speaker A: And I'm not talking in just optimistic sense, but I'm actually talking based on historic evidence. We look at TCB IP and we look at TCP IP was not the first decentralized protocol. We had so many protocols before Nolan network, I worked on Nolan network. Before TCP IP, I worked on so many different protocols that I've used. Ultimately, TCVIP became the most popular, widely used networking protocol because as decentralized as it is, the client experience got better and better over time. So a client experience will get better as long as there's value and it's not going to be overnight. And we may need to choose the right battles to fight to win the war.
01:00:27.930 - 01:01:27.400, Speaker A: What that means is how do we get more people to use decentralized network? Well, in my case I would say Akashmo. For example, Akash has eight different ways to use right now, non custodially. So their web clients, their desktop clients, Clis, there's all kinds of things, APIs, more time, but there's not a single good fully hosted experience. Why? Because clock is decentralized and it really takes that non custodial ethos to hard. Unlike several other protocols that started off with hosted option v, two k, non custodial option first. Now we are fixing that with Akash ML. And the way we're doing it is by not building what we think the user wants, but actually having the users use a cache network directly and understand the barriers of entry and actually solving those barriers.
01:01:27.400 - 01:02:27.742, Speaker A: So that means Akash ML will do the custody for you in a manner that you don't even know you're using. Custodies a cache ML will abstract all the complexities to use a cache. Is that a battle worth fighting? Yes, without which you cannot. User will not use Akash network and will not know the value of it. While Akash ML is actually a lot more expensive than using Akash network directly because there is a cost to what do you call user experience, but the premium for simplicity, there's a premium for someone holding your hand and you pay the premium. But once you realize that, hey, you can actually get cheaper, compute, sometimes even two to three times cheaper than using a Kashml on a kash network, than using a kashmo, you're going to go build it yourself. And guess what? No one can stop you.
01:02:27.742 - 01:03:07.104, Speaker A: I can't stop you. As a creator of the protocols, Akash is permissionless. And that's the beauty of Akash. We are seeing actually interestingly, I've talked to a few people like, well, Akash is general purpose computing. What that means is it can do a lot of things really well, but you cannot do a single thing very well in the sense like if you want today to build a conscious 60 odd providers, it's a pretty good distribution for a CDN. For example, a CDN is called delivery network. Wherever the content lives at the edge of the network and it's easy to access, it's faster to access by the clients.
01:03:07.104 - 01:03:34.842, Speaker A: Like I said, us has 300 milliseconds. So you have a server in New York, user is in California. It takes a long time. So you want the server to be in California or you can't in a server in California. Can you do that in Akash right now with 60 regions? Yes. But what kind of effort does it take for you to do it? A lot, because you got to write software to distribute the content, refresh the content, whatnot. And there's somebody actually building a CDN on top of Akash right now leveraging this.
01:03:34.842 - 01:04:12.762, Speaker A: And because CDNs are very popular and high in demand and that is doing non custodial. So the clients are getting better and better and better because of Akash is permissionless. This person is building on Akash doesn't need to advertise Akash. They're advertising their product to their users and by doing so, getting more users to cache. And there are no terms of service limiting people on how to use a cache network. There's nobody watching in proper use of API and proper use of branding to use Akash network. They're just building that like the way they want to and the way they deem fit for their users to use their product.
01:04:12.762 - 01:04:46.050, Speaker A: Akash gives them that leverage, that power for them to go ship a product without having to fear that the product access will be taken away tomorrow. So I think that is a very powerful concept for a developer. That sense of agency that hasn't existed in web too. I mean, I've built stuff on Twitter, I built stuff, GitHub, something or the other that is misaligned with the API granter that'll piss them off and they take away my access several times. And you don't have that with Akash. I think it's incredible.
01:04:48.280 - 01:05:04.340, Speaker B: Just kind of wrapping up the podcast. I'd be remiss if I didn't ask about kind of Cosmos and the Cosmos ecosystem being one of the first people to deploy there. Can you talk just about some of the learning lessons of kind of building on Cosmos and being there over time.
01:05:07.920 - 01:05:41.596, Speaker A: Yeah. So our history of the cosmos began in 2018. Background. We wrote the paper in 2017, but we were also experimenting quite a lot before we wrote the paper, as you would, because you want to make sure what you're writing is not just complete horseshit. So we deployed our first model on Ethereum in 2016. 2017 timeframe, and it failed miserably. I mean, I think cryptocurrencies launched in 2018.
01:05:41.596 - 01:06:17.270, Speaker A: Was it a. Yeah, 2018. We were running a prototype, and that got us like a downtime with 40,000 users. I think cryptokitties launched with, if I remember correctly. So it became very evident that building on Ethereum or any shared state chain is going to not going to work out for something like Akash, which is infrastructure. And the expectation of infrastructure protocols or networks is 100% live in this right, or uptime. But we wanted to start off on a shared state because it's easier to get started.
01:06:17.270 - 01:06:56.052, Speaker A: So when you're starting a project, you always want to optimize for the least amount of time to get to early validation with the least amount of resources. So you have a thesis that you want to put it out there and you want to test it very quickly. So in that sort of like, we chose Ethereum because Ethereum is really quick to get started, but not so great to scale. We didn't think we would hit scaling bottlenecks that early because that's the current state of crypto. And the other choice was to go build a layer one. Well, that's not easy. You need a team of researchers.
01:06:56.052 - 01:07:41.000, Speaker A: This is before proof of stake was even a thing, we found a library called tendermint, which proposed a BFT byzantine fault tolerant consensus mechanism called tendermint consensus, which is a delegated proof of stake mechanism. Fascinating. Proof of stake is a lot better in terms of resource consumption for securing the chain. For something like Akash, it's perfect. There was no idea of an app chain, but this was the first app chain, essentially 2019. We loved it. The way tendermint works is by having this foundational consensus model and writing modules on top of it, accounts, taking whatnot, banking or whatnot.
01:07:41.000 - 01:08:28.268, Speaker A: And we ended up writing a lot of modules for tendermint. And while we're doing this, there was a parallel effort happening at Interchain foundation developing another set of modules called Cosmos SDK. Remember, it was not Cosmos before. And 2019, of course, our modules were written by our team, and their modules are written by a bigger team and better open source. Our modules didn't. We had a lot of parallel effort. So we emerged that effort in 2019 and we adopted Cosmos SDK, the first chain, I think, to really adopt Cosmos SDK, because IBC, or inter blockchain communication, was a no brainer.
01:08:28.268 - 01:09:08.969, Speaker A: What that means is cosmos being layer one, Akash being layer one. It is an independent island disconnected from the world. It's great if you're just doing compute, because our users don't really care about what chain runs on. So it was good for us. But the bad aspects were lack of liquidity, lack of decentralized exchange support, lack of ecosystem tooling. Like, for example, you wanted a wallet, a explorer, you had to go build all that yourself as a layer one. So we wanted to avoid that and get that connectivity with the rest of the islands in this small nation.
01:09:08.969 - 01:09:56.910, Speaker A: So adopting IBC was obvious for us. And as we adopted IBC, a lot of other change. We're the first chain to connect between IBC from Cosmos Hub and Akash, a lot of other chains started adopting IBC. Osmosis was the second chain that launched pool number three and pool number four are Akash pools. So Akash was the first and only, I guess, the only change besides Cosmos hub that you could actually go and talk to. So, we have a long history in Cosmos, and now where you see the ecosystem is what I love about cosmos is incredible organic growth. There is no strongman like we get in Ethereum or Solana.
01:09:56.910 - 01:10:59.128, Speaker A: There is no single voice, but it's a collective voice, and it's an incredible aspect. It's very organic, but it's also detrimental because there are too many voices now and there is a cost of coordination. When you have a decentralized system, too many voices, whose voice do you listen to? So that ends up chaotic at times when it comes to consensus. And then you have factions that develop from the chaos. We have one faction that's going to prophesies on one way of doing shared security, another faction going to do other forms of shared security, which is very good because you have the sovereignty, but in a way, bad because there are too many choices, and choices saturated here. So cosmos is going through that phase of growing up just like the Internet did. Well, growing up, you had multiple standards for networking protocols.
01:10:59.128 - 01:11:26.420, Speaker A: You had multiple standards. Ultimately, the market will decide, and the market is driven by the developers that built, at least in the early days. So. And that's the beauty of an open system, and that's beauty of a system like cosmos versus a system. Not to talk bad about Polkadot, but yes, a system like Polkadot, where you have, you know, Kevin Wood as a primary arbiter of technological decisions. Right. Or so.
01:11:26.420 - 01:12:28.612, Speaker A: So I think, like, what's great is cosmos chains are testing right performance. We saw with terra Luna when the terra is a cosmos chain. Those of you that don't know, when terra collapsed, there was a record amount of transactions because people are trying to get the money out or some kind of pump money, I suppose, but the chain never went down. And it also stress tested governance because I was very active in saving our bags and I was able to make proposals in terra ecosystem and actually have those proposals pass during adverse conditions where I have no friends, really, entire ecosystem. So that really tells you that, hey, an outsider can make a proposal and the proposal makes sense by the community at large. It will pass. So we saw a lot of tests of real battle test battle tests for cosmos ecosystem during terra collapse.
01:12:28.612 - 01:13:12.212, Speaker A: And now we are seeing the experience, right? So, yes, it's not, it doesn't have all the. It is not as rapid in terms of innovating as Ethereum and other shade state ecosystems. Because, remember, I wouldn't say close, but shared states is kind of, you know, for lack of a better word, more close than something like cosmos, where the sovereignty is higher, they go fast, but cosmos goes far. Right. So apply the same analogy here. I think it's very evident that Cosmos is going to come out winner. And I tend to have a history of picking the right tech, or at least the tech that that succeeds eventually Kubernetes, Docker and whatnot.
01:13:12.212 - 01:14:03.998, Speaker A: And I see parallels of the parallels between Cosmos SDK development and kubernetes quite a lot. And we saw kubernetes go through this phase and eventually became an unstoppable ecosystem to a point where you have a lot of bureaucracy now. But I guess that's a different problem to solve. But in the early days, when you have such decentralized coordination and such difference of opinion, the difference of opinion comes because people care. And that's a function. It's a property, it's a feature, not a bug. So I have very high hopes on Cosmos, and all of us are even the biggest Cosmos conference is hosted by a community member cosmoverse, which I'm going to in Turkey next very soon.
01:14:03.998 - 01:14:48.340, Speaker A: So, like, I want that to continue, right? I want the community driven, consensus, community driven aspect of cosmos to continue because I know the only thing that's going to get us to eventually win. So I hold no maximalism, but this is just my thoughts. Would I consider using something like a shade system? I thought about this quite a lot. And for background, I helped angel hack a company founded before Akash. We helped launch several products, and one of one of the biggest launches there was was Firebase, which eventually got acquired by Google. Firebase, we know we love. Everybody loves Firebase, everybody uses them.
01:14:48.340 - 01:15:18.630, Speaker A: And I was, you know, this is me at Firebase. They did a demo before launch. And this is me, a senior engineer in San Francisco that will never pay for a database, ever. That's a no no. I mean, this is a time when you have SQL and postgres and MySql and MongodB, even we just don't. Databases are supposed to be free and open source. Firebase came with this fully closed source, fully hosted.
01:15:18.630 - 01:16:00.430, Speaker A: And I was able to build an app on Firebase in 20 minutes in an Uber, like in 2012, I believe. And it was very obvious the value it provided was speed at which I can bootstrap an app, but not necessarily scale. It didn't even have authentication at that. It was just a public database. It was like a blockchain. And I never wanted to build on Firebase for the longest time, even though I was friends with them and whatnot. And it took about six to seven years after Firebase launched for me to get that level of confidence.
01:16:00.430 - 01:16:38.266, Speaker A: I think you can scale to a certain degree. And then now you look at Firebase, very scalable. So do shared state systems come to a level of scalability that's good enough? May not be as good as a fully controlled database or fully sovereign database, but good enough? I think they can. Will Akash consider something like a shared system? Hard to say, but we are considering shared security with the most secure chain, which is bitcoin. Right.
01:16:38.298 - 01:16:39.950, Speaker B: So with Babylon.
01:16:40.450 - 01:16:58.970, Speaker A: Babylon is a good, incredible, you know, Cosmos chain. And it's a utility chain, which is very fascinating. It's a new category of like, applications now coming up in cosmos. Right. You have app chains and then you have utility chains. Noble. No, Babylon and all that.
01:16:58.970 - 01:17:20.920, Speaker A: So yes, we do want to use shared state. Bitcoin undoubtedly is the most secure chain in the world. And so. Yeah. But at the same time retain sovereignty. Right. So I think that's the design space that's very exciting to explore right now for us.
01:17:21.340 - 01:18:06.648, Speaker B: Okay, amazing. And I appreciate all the large color and kind of filling in all the gaps, but no, truly excited for what you're building with Akash. Greg, excited that the team has been thinking about this for so long. Obviously, you've been in as evident, being involved in cosmos since the beginning, since the inception. You do have a good track record of picking these things. I'm very excited to see the network of Akash grow, the amount of GPU compute grow, and also just the demand side. I think you and I very much agree that this is really just the beginning for where AI and amount of compute needed is going to take us.
01:18:06.648 - 01:18:18.620, Speaker B: It's going to scale for a very long time and we're going to need to tap into those idle resources. So really, again, thank you so much for coming on the podcast and appreciate you sharing all the color about what you're building.
01:18:19.280 - 01:18:21.000, Speaker A: Thank you so much, Logan. This was super fun.
