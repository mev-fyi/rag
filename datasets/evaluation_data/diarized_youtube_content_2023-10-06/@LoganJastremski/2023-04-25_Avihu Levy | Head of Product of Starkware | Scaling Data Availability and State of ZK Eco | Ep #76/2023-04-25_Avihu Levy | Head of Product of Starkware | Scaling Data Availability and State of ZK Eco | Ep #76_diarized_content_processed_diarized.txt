00:00:00.160 - 00:00:05.566, Speaker A: And the other thing that we really got into which I really enjoyed was the data availability problem.
00:00:05.638 - 00:00:26.014, Speaker B: Anyway, other way that you turn it around is not the proof size or the proof verification cost anymore. Those are negligible compared to the on chain cost from data, from transactions. And you see it on Stacknet, you see it in the same way on all other ZK evms, and you see it on even an optimistic roll ups in some sense, like this is the major cost right now.
00:00:26.142 - 00:00:32.600, Speaker C: I know you're looking forward to this one. Omar. Big fan of Zero knowledge, all things Ek.
00:00:33.260 - 00:01:52.760, Speaker A: I don't have to edit it, you have to edit welcome everyone to another episode of the Logan Jiramisi podcast. It's my distinction to be here again to ask a few questions that mainly arise of sort of my interests and my background, and Logan sort of gives me the opportunity to do these things sometimes. But I think it's going to be a lot of fun. Today we have the great distinction of hosting Avi Hu Levi from Starkware, the company that brought you Starkx, Starknet and a lot of other wonderful things that we'll talk about in this podcast. And per usual, we have the host of the podcast, Logan Jestremsky, a dear friend and a deep thinker when it comes to holdings, blockchain architecture and a self proclaimed performance maxi. I am also a self proclaimed performance maxi. Ultimately, what we'll be trying to get to the bottom of today is the architecture that enables starkware to provide a lot of highly performant solutions and hopefully we touch upon the path dependence of this architecture and how we got here and what is coming down the line.
00:01:52.760 - 00:02:12.620, Speaker A: Per usual, we will discuss all things ZK at a high level to begin with, and then at some point we will start getting into the nitty gritty implementation details to the extent that it makes sense and hopefully we have a lot of fun. Back to you, Logan.
00:02:12.780 - 00:02:32.852, Speaker C: Perfect. No, I think we're going to have to start rolling with Omar doing all the intros. Very great intro. Perfect. No, definitely appreciate it. Really excited for this one. I mean, obviously we have some industry experts here and so as Martha said, I think this is going to be a lot of fun.
00:02:32.852 - 00:02:44.640, Speaker C: Maybe before we kind of deep dive the tech, I'd love for you to give a quick background just on yourself and how you ultimately got involved on the starkware side.
00:02:46.550 - 00:04:09.084, Speaker B: Okay, so first, hi everyone. I'll describe, I'll go as early as possible. So I started with bitcoin. I got into bitcoin in March 2011, so quite some time ago. And at the beginning, it was mainly some technical and economical interests. So by the time I entered more and more into this rabbit hole in this community, and I also was participating very early in Ethereum, and I've been following the space very closely, and I did few small projects in the space, but really, my main profession and background was more in the area of cybersecurity and vulnerability research and blockchain. During all this time was a fun hobby, and I could find myself spending many, many hours outside of my work time doing side projects there, but never as my main profession.
00:04:09.084 - 00:04:49.612, Speaker B: And at some point, it was like August or September 2017, I got a friend who was saying something like, hey, look, there is this talk by this professor, Eli Benson, and he was talking in technique on summer school about these things called starks, and maybe it will be interesting for you. So I just messaged Ellie over LinkedIn, and luckily, Eli is one of those few people who, when you message them on LinkedIn, they actually answer. So he did. And the rest. The rest, I guess, is history.
00:04:49.796 - 00:04:58.520, Speaker C: That is rare. LinkedIn. I feel like people send lots of messages and ultimately no response. I think Twitter is much more fun from that point of view.
00:04:59.380 - 00:05:19.940, Speaker B: Yeah, yeah. So. And back then, there was no starcore yet, but the idea was already there. The founders were there. It was quite interesting because we. When we started, there was. The technology was definitely in place, and we spent a lot of time developing starks.
00:05:19.940 - 00:06:20.800, Speaker B: And so in some way, we were already pioneers. But there was another aspect to which I feel we were first in, and this was talking about Starks and ZK technology in general for scaling and not for privacy. So back then, Zcash was already in place, and there were some other. Something between experiments and other chains running for privacy, mainly payments privacy. And it was the easiest route to take would be to say, let's go and do another privacy chain. But we realized pretty early on that our technology excels in something that is more important, and that's the route we took. And I took the position of running the product side as Tagore since almost the beginning.
00:06:21.700 - 00:06:39.840, Speaker A: So maybe. Maybe it's a good time to actually start off with the historical context. What was the state of the art at the time, especially when it came to snarks that yourself and Ellie and the rest of the team saw as a fundamental limitation and decided we need to look elsewhere?
00:06:41.860 - 00:07:46.660, Speaker B: Yeah, I don't know if the state of the art, but the snarks that were being used there back then had several drawbacks. I'll separate them. There were a few more that I think that are more negligible, but the main two or three drawbacks. So one was the trusted setup, but when I said the trusted setup, it wasn't just in the context of you need to generate a ceremony and trust the fact that there is at least one honest entity in the ceremony for this whole structure to be secure. It was all also the fact that back then what was in use was gross. 16 and that means particular different new setup for every type of circuit that you want to prove. So if you change something in the structure of your proof, in what you are proving, you need to redo the entire ceremony, which makes it reasonable and even good if you want to use some privacy circuit that you rarely use, but very limiting factor.
00:07:46.660 - 00:08:34.090, Speaker B: If you want to develop a complex system that then you want to prove and change over time, which when we look at today world of, you know, Starknet and all the other systems, it's pretty clear that there is going to be an involvement and you can't leave with redoing a ceremony every time you need to upgrade your system. It just doesn't work very well. And the other point was really about how fast and efficient your prover system can run on what size of computation. And Elie, I remember even back then strongly believed that the stark technology just have some inherent advantage that can be scaled farther high than what snarks could do that days.
00:08:34.990 - 00:08:47.149, Speaker A: So when we talk about scaling in that historical context, were we talking about complexity for generating the proofs? Were we talking about the size of the proofs? Were we talking about verification burden?
00:08:48.729 - 00:09:43.804, Speaker B: Good question. So there was no stark verifier when we started on Ethereum. Ethereum definitely did support snark verification through a precompile. And in snark there is this illusion that everything is much simpler to verify because you just take pairing as a black box and then the complexity becomes simpler. While in stark we didn't have this luxury of people relating to some primitives like fries black boxes and having a pre compile for them so we could execute things easier. So we have to like pave our way there in the ice some way. It wasn't that easy, but we understood from very early on, and I think it's true up till today, that when you talk about scaling, the limiting factor is not really the size of the proof or even the the cost of the verification.
00:09:43.804 - 00:10:29.512, Speaker B: As long as your verification scale logarithmically the constant is not negligible, but is much less important. And when we see today, even with just a single digit number of transactions per second in starknet, the cost factor in any other way that you turn it around is not the proof size or the proof verification cost anymore. Those are negligible compared to the on chain cost from data, from transactions. And you see it on stagnet, and you see it in the same way on all other zkvms. And you see it on even an optimistic roll ups in some sense. This is the major cost right now on that front.
00:10:29.576 - 00:11:02.120, Speaker C: I mean, maybe can you talk about backing up a little bit on just like the progression of Ethereum, like how they're planning to scale some of that? Because I do find, as you said, it took me personally a little bit of time to understand, uh, why this is kind of the main cost. Could you explain that to like the everyday person, uh, and then kind of how ethereum is progressing on that front. And then we can jump back into some of the more nuances on the zero knowledge side.
00:11:02.910 - 00:12:36.160, Speaker B: Sure. So the most important part for rollups is the fact that they can rely on layer one security. And relying on layer one security in particular means that you use layer one to ensure that first of all, there is at all point the right state on chain. If I convert for seconds directly to Zika rollops, then this is done by verification of proofs that basically showed the chain that the state that they got is a valid state transition. And another important part is to make sure that everyone can retrieve the state of your chain by looking at layer one directly. And the way it is achieved today is by publishing sometimes transaction data and sometimes just transaction or sorry state differences that caused by transaction directly on their one. And so for example, if Omar transacted yo Logan $5 on Starknet, then his balance decreases by $5 and your balance increases by dollar five.
00:12:36.160 - 00:13:17.300, Speaker B: And what Starknet would do at this point when the proof will hit the chain and there will be a new state. Starknet would also present data that says Omar's balance changed by that amount and your balance changed bye by the same amount. And everybody watching layer one would be able to derive the new state of the chain by looking at this state. Some chains are doing even more by publishing the entire transaction data on chain. In the case of optimistic roll up, this is done because they need a way for the external observer to actually see that everything is valid. They don't have the proofs to do that for them. And for other chains.
00:13:17.300 - 00:14:21.940, Speaker B: For example, I think currently Polygon ZKVM, they're publishing all transactions data because they stay, they think that this data should be also available on railroad. But the extra data that is not relevant for reconstructing the state, just maybe for reconstructing transaction history what Ethereum is going and sorry and I didn't say and this since in the worst case this cost occurs with every new transaction and it's non negligible cost because it derives from layer one cost. And in particular our layer one is Ethereum. And Ethereum is, as you know, congested most of the time, then you pay congestion cost on some L2 s. So even if your L2 is not congested at this point, since Ethereum is, you pay some part of your transaction as Ethereum cost. So it makes the most of the largest part of the cost in every transaction, in every CK roll up or optimistic roll up is just the on chain data.
00:14:22.240 - 00:14:31.576, Speaker A: But because these transactions are batched on the L2, you get some cost savings. You sort of amortize that, right?
00:14:31.608 - 00:15:34.140, Speaker B: So in those, exactly, that's correct. So in those roll ups that that batch state updates, and I think Starknet and also Zksync era are doing that you get some saving, which in some use case can be even significant. I'll give an example. If you have a DeFi application and it gets an oracle update once per minute, and you have whatever 100 oracle updates in a block, then it might be the case. It might be the case that when you publish transaction data directly, you will publish this Oracle price 100 times. But when you publish state difference, you maybe just want to update the first and last changes in the oracle price through their block. Because for the external observer they don't necessarily need to know what was the state ifs inside a block, just between blocks.
00:15:34.140 - 00:15:51.020, Speaker B: So that's an example where the saving will be high. But from empirical use we see that this is still the major cost. Even if we do all this compression of state, this is still the major cost. Sorry, go ahead.
00:15:52.560 - 00:16:39.730, Speaker C: Just trying to wrap it back to the user perspective in blockchains, because there is only a certain amount of block space you're posting, whether these state diffs or all transactions, whether it's a zero knowledge proof or a optimistic roll up that's posted back down to the layer one. Today Ethereum has a certain amount of block space, as you mentioned. Sometimes it can be rather full, but with Ethereum introducing like 4844 and then ultimately adding kind of the more final roadmap end state with dank sharding, that will be more data available for roll ups to post these commitments. And ultimately that is the highest cost on chain, correct?
00:16:40.470 - 00:17:17.666, Speaker B: Correct. So I'll say just two more words on 4844, because it is important to understand right now, when you publish data on Ethereum, you publish it in a way that the network can distinguish if you want to publish the data, because there is a smart contract on the other side of the transaction that needs to consume this data and execute on it, or you publish this data just for visibility for everyone else. So because the network doesn't have this separation, the network can't handle those two resources separately. So there is some price.
00:17:17.818 - 00:17:24.762, Speaker A: It prices them sort of the same. All data that's posted has the same price, which we know isn't the case. Which we know, right.
00:17:24.786 - 00:18:33.234, Speaker B: And if you want, so basically 4844 in a way, introduce this new resource that says here you can publish data. And there are different, the mechanism itself work a bit differently, but it basically tells you use the new resources for data that you just want to publish, but contracts cannot access and they don't expect to access and you won't need for the execution. So from a full node perspective now, and from everyone else in the network, the idea is that now you have this resource, you know that you never need it for execution, you only need it for data availability purposes, and so you can price it differently. And there, and the hope is that there will be a different market for this resource and hence the cost for the resource will be significantly lower than the current data availability cost on here. I didn't hear anyone committing on how much significantly lower it's going to be, but numbers throwing around, I saw something like ten x or maybe 20 x lower, which is significant cost reduction.
00:18:33.282 - 00:18:56.560, Speaker A: Still, how about the spectrum in between? So data that is published that occupies valuable memory because it has to be accessed by smart contracts until a certain expiry date, after which that data is deprecated, or it's maybe moved to the just for visibility category, or maybe it's completely deprecated.
00:18:57.220 - 00:20:18.280, Speaker B: So there has been some discussions, I'll just take one step back. I think four years ago we proposed EIP 2028 and the idea there was to say let's just take the on chain data cost and reduce it from 64 gas per byte to 16. And we did a bunch of experiments to show that the network does not get affected too strong from this change and this change passed and this is what you pay today. And an alternative to 4844 would be to say let's do the same forex cost reduction from 16 to four and just reduce. And instead of adding another resources, just let's reduce the cost of data some more. And the pushback on that, one of the pushbacks was to say, look, eventually nodes store this data forever, so it caused them to use more disk space in the worst case scenario. And one idea was exactly what Omar said, which would be to prune to ESC nodes for, after some time, to prune some of this data because they use it once for execution.
00:20:18.280 - 00:20:49.520, Speaker B: Enough time has passed and they now don't need it anymore. And everybody else who are seeking from them, they sink from a later point in time. So maybe they can discard and prune this data. But I don't think that, I'm not up to date with the latest decisions on the ECM core front, but I don't think this progressed to a point where people actually took it and took this direction.
00:20:49.940 - 00:21:38.396, Speaker C: Maybe. While we're on the topic of just data availability and where that data ultimately lives, Omar and I kind of love to talk about volitions and lydiums. And ultimately I think the industry is starting to explore or has explored some of these, but in my point of view, they're not widely adopted as of yet. Can you one speak to each of those kind of how you view them from a security point of view with the introductions of proofs and how those ultimately can kind of manifest or help the industry scale a bit more and.
00:21:38.428 - 00:22:11.120, Speaker A: Also maybe comment on one thing with. Regarding to volition in particular is it basically touches on the idea of pruning, because there is a finite non zero probability that some of that data becomes inaccessible anyway after some time. There's an economic cost to that, certainly, and there's a convenience cost to that, but it is a possibility. Now you basically, it's not no longer on the l one, but there is a possibility that some of that valuable data no longer exists after block n plus one or something.
00:22:12.780 - 00:22:24.420, Speaker B: Yeah, okay, great question. I think it can very easily take the 30 minutes or even an hour just to talk about this one particular topic.
00:22:24.540 - 00:22:25.440, Speaker A: We have time.
00:22:25.900 - 00:23:29.164, Speaker B: Okay, so we'll start. We'll start by. Maybe we'll start by saying what validium is. So actually the name came after like some short twitter thread I had with Vitalik, but Ellie came with that name. We felt there was like this, if you look on what was around like three years ago or two and a half years ago, then we had roll up which said, okay, there is proofs and there is a data availability on chain. And there was the question of do you use proofs or you don't use proof and do you use on chain data or you use off chain data. And we felt that the category of using proofs to validate the new state, but publishing the data elsewhere for the state, not only theorem is an interesting category to explore.
00:23:29.164 - 00:24:54.114, Speaker B: And we basically were looking for the name, and the name Elie came with was validium for one, for a single system. And I think that in Latin that translates to Validia for many systems. And Stark X systems back at the time and up until today, some of them operate in a full roll up mode where they publish all data on chain, but some work in a validium mode where they publish data in a less secure way, in off chain, in Starkx's data availability committee. So a list of entities that sign the availability of the data. And the reason it's an interesting construction is because unlike in all other previous construction, would be optimistic roll up, or if you remember plasma, and if not, never mind, then you didn't have the right, the guarantee that a new state is valid if you haven't seen the data, or other players haven't seen the data. And this is the first time that we could say, okay, even if you don't see the data, the state update will always be valid. So it's still limiting you in some other aspects, and it does reduce security in some other aspects.
00:24:54.114 - 00:26:02.180, Speaker B: But you know that every time the system progresses, it progresses to a valid state, which was a very important point to understand, because when you don't publish the data, only one, you can get scale to a much higher level. And then we played with all kind of alternative designs, and we started with validium, where all the data sits on this data availability committee. So you could choose, you can either go full roll up or you can go validium. And the second option that we came with was to do something hybrid, which is what volition is. And in volition, the idea is that you can have separate types of storage. Some of them, the type that is on chain is more secure and published every time you change the storage cell. The new storage cell status is being updated on chain as data.
00:26:02.180 - 00:26:47.542, Speaker B: And the other type of storage where the updates are being published on a different layer of data availability, that is not necessarily serum. So we started with doing it on StarcX, but really the point where it becomes where it will be even more interesting is when volition get to Starknet. I want to stop here and let you ask questions, but then I also want to talk a bit more about the security of this and how this would work from a developer perspective and users perspective, because I think those are like the interesting questions there, well, that's.
00:26:47.566 - 00:27:14.060, Speaker A: Exactly where I was going to stop you and ask which aspect of security is undermined? And actually, maybe it's a good time to technically define security and the spectrum that results from that technical definition. And maybe even so, there's cryptographic security, right? People talk about bit security, but then there's also other types of security for the network itself, regardless of the actual cryptography. So maybe we get into that, right?
00:27:14.400 - 00:28:10.710, Speaker B: So let's go like with an extreme example. Let's say that you store some, like you said, Omar, that you store some data that is important to you somewhere on the chain, and maybe you even change it, and for some reason nobody stored a new state of this data. So from this moment on, if you don't know the data and nobody else knows the data, then unless they can guess it, there is no way in the world they could prove things on it, because they don't know what is the right statement that relates to this data to begin with. So the risk here is not that there will be a wrong state transition, but that some parts that relates to this data will not be able to continue and be live in the liveness definition of the network. Nobody will be able to access them or change them or take them out of the system.
00:28:10.870 - 00:28:19.220, Speaker A: And what happens if enough nodes don't have access to the data, and how does that affect liveness?
00:28:19.920 - 00:29:17.762, Speaker B: So it really depends on what is the structure and what kind of data that is. But at the very least, when you send a transaction, there should be a way for whomever, including this transaction, whomever generating the proofs based on the block that include this transaction, to access this data in order for them to be able to execute it, in order for them to be able to prove statements on this data. So if you get to a point where most nodes don't hold this data, you might get to a point where you can't execute for a long time your transaction. If there are enough nodes in the world so that at some point this data will be available for them, then potentially you will still be able. This is only pathetical discussion because we didn't define the right structure there and.
00:29:17.786 - 00:29:21.710, Speaker A: So on, but, or the consensus mechanism, obviously right.
00:29:22.490 - 00:29:59.850, Speaker B: The more dangerous situation is not if some nodes forget the data, but if for some reason the network has progressed to a point where the data has been changed, but no node was informed for this change. So everybody can see the state update, know that it's valid, but nobody in the world can access this data and tell everyone, hey, this data is the data for this new state, in which case you won't be able to execute any more transaction on this new state. This is the more risky situation. I think the challenge will be like.
00:30:00.470 - 00:30:12.720, Speaker A: What do we do then? Do we just stop and say we're going to ignore everything past this block, and then we're going to start where there is consensus and the data is available? You just basically roll it back?
00:30:13.700 - 00:31:29.428, Speaker B: I just want to, like, before I answer this question, I want to say something that is important. Like, I do believe that sooner rather than later, Starknet, but also other systems, like basically many of the ZK roll ups, they're going to get to a position where the data availability are not necessarily going to be just a single source of data availability. Maybe they use data availability on e zero, maybe they also use 4844, but maybe they also use data availability on other chains. And in ZCO, ROPs is definitely a possibility, and starting it will explore this possibility rather sooner rather than later. Startnet will have volition. That means that when developers write contracts and when users interact with the contract, they might interact with contracts with on chain data, and they might interact with contracts that publish data elsewhere, not on Ethereum, and most likely it will still have very good security. But users and applications that want to be more cautious about the security, they should probably use the layer one on chain data.
00:31:29.428 - 00:32:17.220, Speaker B: Still, while many, many other applications that are potentially less sensitive to this part, they can utilize, at least partially the off chain data and gain significant cost and throughput benefits in the long term. If you lose one type of data and still obtain the other, the other parts that are not touching the mist data, they can still go and execute without a problem. In theory, you know, there might be cases where the data that will be lost will be so precious that you will have issue to move forward. I don't want to commit.
00:32:17.640 - 00:32:55.410, Speaker A: The reason I mention is because some data models allow you to specify the dependency, so it tells you exactly which contracts, which wallets, which parts of the data are interacting with each other. Other data models, the Ethereum one included, doesn't sort of do that. So there's no guarantee if I don't actually have access to that data. Or at least it's not trivial to say I know exactly which parts of the network doesn't use that data for something like Solana or something like Monad that we spoke to before. They have a sort of different data model and it makes it sort of. Go ahead.
00:32:55.950 - 00:34:13.158, Speaker B: Yeah, I want to say that if the question is, would in the model that we have in mind, would it be possible for the network to continue and operate? And the answer is definitely yes. But I'm just saying that if you lost some parts that are very important for you, then maybe significant. And even if you can continue and operate, you will consider other options. I want to say one more thing, sorry if I'm going too deep on that topic, but that the design space for data availability is very big and we looked at the past in some other models where you can have even more than two types of data. And the idea there, back at the time we call it power users, is that you have data availability providers and they take responsibility on specific part of data for specific user or specific applications. And the idea is that as long as they are alive, they can just approve the network that they have the data. And if they go missing, then you can just extract their data once to layer one and prove it to the network and move on.
00:34:13.158 - 00:34:32.878, Speaker B: So I'm just thinking that the design space for data availability, which is a very important problem, we haven't as a, as a community or as like as an ecosystem we haven't explored in full yet. But we are definitely going to start using volition in stagnet sooner than that.
00:34:32.894 - 00:35:32.920, Speaker C: Gets me super excited. I think if anything now I try to just be like a high throughput maxi. Just because I use the analogy of blockchains being very similar to the early days of the Internet with 56k modems and broadband and then fiber optics, I feel like we're seeing that same path playing out with block space and the throughput of networks definitely gets me excited that the team is exploring different options, having that data live elsewhere, ultimately just making transactions cheaper, being able to do the large amounts of compute that you're doing or proving very interesting. I'm definitely excited for that world. I want more users at the end of the day and I truly believe the blockchain industry is some of the coolest tech going on. And the technology that you and the Starkware team are working on I think pushes that forward even further.
00:35:34.060 - 00:36:20.498, Speaker B: Logan, can I mention one more thing? That is touching what you just said. So about volition, it's very important to understand that L2s as performant as they would be. They start with a certain cost per transaction. If you go to l two fees, considers cost is running anywhere between five x and ten x cheaper than ethereum just on on chain data costs. So something like $30 to $0.60 on swap transactions. And it's an interesting situation because you start like when you, when you spin a new layer one, you don't have a lot of security, but you also start with transaction costs that are ultimately zero.
00:36:20.498 - 00:38:07.380, Speaker B: And when you spin l two s, you get, you start with some security that is based on your level one security, but you pay the cost of the expenses per transaction that are non negligible. Now there are two ways that this cost becomes less significant and we practically want to utilize both, right? One is that you have so much demand for so many transactions with so much high value that anyway, your network is flooded by transaction and the congestion even arrives to your layer too. And you can facilitate hundreds and thousands of transactions per second, and they all live very well with value that is higher than this cost. But in this case you have the problem of throughput because you are limited eventually by layer one. And the other route that you can take is that you reduce this cost, this inherent layer one cost, by introducing alternative data availability directions that for some transaction will just allow for the lower expenses per transaction and hence for the lower value per transaction. Hence for some new use cases that you don't necessarily explore at the beginning. So this is an important point to understand about the economics of l two s today.
00:38:09.060 - 00:38:57.120, Speaker C: Fully agree. And maybe from the product perspective or we're seeing, I mean, now there's kind of a variety of new layer ones, some of them focusing more on high throughput. How, I guess, do you see this transition ultimately playing out? I mean, as you said, the team in starkware is going to explore these different ecosystems is the end goal, I would say, to have the starkware technology on as many blockchains as possible. Or is it strictly to continue to advance Ethereum and that broader ecosystem?
00:38:57.820 - 00:40:04.530, Speaker B: Yeah, when I say we go to explore other data availability, I mean just in the context of the data availability, I don't mean in the context of publishing the data or having stack network on different chains. We are 100% focused right now on working just on Ethereum, and this is where Starknet is. I do want to say one comment, that Cairo is a language and technology, and to some extent starting, but mainly Cairo, we already see it being used elsewhere. Not only and to be fair, it's not even so much with our stackware involvement. We developed the language, but people just see it as a useful, efficient language to generate proofs and so they use it elsewhere. And we've seen, I think the most interesting use case right now outside of Startnet and Ethereum is basically, there is some attempts. One notable one is zero sync to prove the bitcoin blockchain.
00:40:04.530 - 00:40:38.700, Speaker B: So basically we write Cairo programs that represent bitcoin scripts and have the entire bitcoin blocks proven in Cairo, which gives value to the bitcoin chain, even if you can't operate roll ups on bitcoin. So it is something that is happening. And I think that strong proven technology like the one that we are building is definitely going to be used elsewhere, whether we want it or not.
00:40:39.360 - 00:41:23.588, Speaker A: Beautiful. It's a good segue to actually talk about Cairo. Let's maybe take it a step back all the way to. I think the lecture was in 2013 when the idea of like Tinyram and the architecture for a sort of ZK native virtual machine, it sprouted around that time. And there are other dsls or domain civic languages that people currently continue to use to develop relatively flexible schemes. Circum is popular, noir on Aztec, zinc on Zksync. So there was a lot of innovation on that side, and there's a lot of back and forth between.
00:41:23.588 - 00:42:25.994, Speaker A: Should I be doing most of the, most of the abstraction at the program level? Should I be doing it at the virtual machine level? Should I have an interpreter? Should I have a sort of middle layer? How do I worry about arithmetization? Is this something the programmer should be aware of? These sort of things? And all of these questions inspired Cairo in its current form. And we've seen Cairo evolve a lot from the first instances down to, or up to rather Cairo 1.0. And then we have this advent of Sierra, which talks to the sort of middle layer that I just mentioned. So maybe walk us through that entire sort of progression history and which parts were the inflection points where you decided, I need to build a new architecture. I need to build basically a new type of computing machine. It has a name, sort of von Neumann architecture. How did that all come to place? I knew I threw a lot out there, but this is the only way to get you to talk about them is to throw around the keywords and then maybe you tell me what they mean.
00:42:26.082 - 00:43:39.866, Speaker B: Okay, I'll try to, to touch mainly a few turn points that were important for us as a company and for stocknet. So when we started, I don't know if it's even possible to understand it today, but when we started, we did a lot of the work of describing complex logic to be proven practically by hand or with some tools that we created for it. But it was completely, it got to the point where unless you are special, like special expert in this field, there is no way that we could enter that you could enter and add logic simply to Starkick. So if you wanted to express transfer or a trade, that would be maybe something you could fight with. But if you wanted to express some very complex logic on your Merkle tree design, then we would basically lost it. Wouldn't be possible. And this is what pushed us as a company rather early to develop Cairo.
00:43:39.866 - 00:44:38.590, Speaker B: Even before, before we had any idea to push it externally, outside to the world, we just use it internally as our own tool. And Stark X one was built without Cairo. Starx two, sorry, was already built with Cairo. And the difference between the velocity of features and the speed of where which we could take out versions, it wasn't something that you could compare and imagine that back then we were a small company. If you go and want to scale your development effort from five people to 50 to 500, then you definitely need the most convenient language you can write with from one end, and to make sure that it's efficient enough for complex logic from another end. And this is exactly where Cairo took its place. I think there were two more turning points.
00:44:38.590 - 00:46:09.430, Speaker B: So we continue to develop Cairo, what we today call Cairo Zero. But it remained fairly low level language. And at some point there was the question of should we go and take the EVM route and maintain Cairo just as an intermediate language for our provisions. But everything else in the network will be written on top of that. And there are at least two main models that are being in the work today that we also considered but one main turnpoint. There was to say, we understand that at the end of the day, when the known optimizations for provers are all out there and everybody have their provers open source and available, and they understand, and we all understand how to work with the best primitives and so on, there is still some significant gain from using Cairo as the language over everything else when it comes to proving efficiency. And proving efficiency is not just cost at the end of the day, when you run in the very highest through TPS, it can also in some ways affect other things like latency and potentially also throughput.
00:46:09.430 - 00:46:40.564, Speaker B: You want this advantage. And if layer tools and ZK rollups were meant to give scalability, you want to make sure that you give scalability and you maintain your network to be as efficient as possible. So this is one decision, and the other important decision or turning point was around Cairo, one that is now coming out. I want to touch, I don't know how much time do we have, but I want to touch Cairo one, do.
00:46:40.572 - 00:46:42.840, Speaker A: We have, we have plenty of time for it.
00:46:43.500 - 00:47:31.990, Speaker B: Okay, so there are two. It's funny because the two aspects of Cairo, one, they're so very different, but I think they both affect the network in a significant way. So one thing is that caravan is just a higher level. It's just a language that is much more convenient and fun to write with. And this turns out to be a non negligible issue or thing at all. I'm not a rust developer myself, so I can't compare, but many people, when they do compare and they see similarities, I'm happy to hear that the feedback right now is very, very positive. I think it's very important that you maintain not just efficiency, but also a convenient environment for your developers.
00:47:31.990 - 00:47:57.490, Speaker B: And that means all kind of things, but in particular a language that they like. By the way, I'm not sure that solidity is at a level of a language that developers like, maybe is in the level of a language that is very well mature, much more than Cairo at this point. But I'm not sure that developers can say, I love writing in solidity. I don't think that's the current.
00:47:58.150 - 00:48:26.180, Speaker A: But don't you think that, don't you think. I think. Don't you think that it's not really at the programming level per se at the language level, but it's really at the programming paradigm how you have to think about structures and how you have to think about memory and how you have to think about access. It's less to do. I used to always say the same thing about solidity until you realize that it's really not, it's not syntax that people are struggling with. Right.
00:48:26.600 - 00:49:08.710, Speaker B: Yeah, I agree with you. It's not, I can't. I don't blame solidity. It brought a new paradigm of like, how do you think about what you write? I don't know where and what was. I'm not definitely not the expert on programming language to say that, but just the sentiment I'm getting. I think that in other aspects and in different programming languages, including some other blockchains that took this route, maybe you can get to better results in how much your developers feel comfortable with the language they're writing in the. But anyway, when I compare chiral one to chiral Zero, it's definitely.
00:49:08.710 - 00:50:29.406, Speaker B: There is like huge leap forward. And the other thing that this actually pushed us to Cairo one, not less than the first part that I was making is that Cairo Zero was made as a language that we used at the beginning, but it's nothing. It wasn't made in a way that when you write starknet contract, you can prove each and every transaction in each and every contract. And so in the current starknet version, you theoretically can write contracts that when transaction will call those contracts that for example, will fail. But you won't be able to prove the failure of this transaction. And in a situation where you have a decentralized blockchain, you can either overcome this with some economical models, or you can create a way where the only code that will compile eventually to Cairo Zero will be code that is always provable, and Cairo one and Sierra achieve exactly that. So when the network is fully transited will be transit in full to Cairo one.
00:50:29.406 - 00:51:11.632, Speaker B: You will basically have a guarantee that all your contracts are provable. And this is significant for in a decentralized war this is significant when you want to prevent denial of service on sequencer. This is significant if at some point you want to enable layer one access for transactions for any transaction in the world and any contract in the world. So there is some significance that Caravan brings in terms of security and better networks. So it's not just programming language. There is also the aspect of provability that comes with Cairo one and Sierra.
00:51:11.816 - 00:51:44.792, Speaker A: Well, let's dig into that a little bit. Sierra being the crown jewel from a very high level like mathematical standpoint. Can you walk me through, if I take Cairo the first, so Cairo zero, if I take that and I impose a set of constraints, or I put it through some sort of mapping, we'll call that mapping Sierra. How does it output something that is always guaranteed to produce a probable program? From a logical perspective?
00:51:44.976 - 00:52:51.880, Speaker B: Yeah, just from the logical perspective, because I'm really not the expert to jump into the actual implementation details. The idea is that whatever path in the code your transaction is going to take, this path is going to end in something that the prover can relate to or the that trace can mark as an output. And it's not something that just ends without any result that the prover can relate. So when you go to EVM, you already have that because you prove a set of rules that always end with some result. But in Cairo, in Cairo zero, it's not necessarily the case. So you can, if you access something that is not the right value or doesn't exist, maybe you end up in some point that this is just invalid, but you have no way that this will translate to something that is provable. So that's how I think about it in the, in my mental model.
00:52:52.620 - 00:54:02.126, Speaker C: Yeah, there are lots of different like, nuances. It does get fairly interesting. I always try to wrap it back to like the product perspective and I guess how it affects the end users, I guess. I mean all this being said, I think blockchains are the most interesting things today. Zero knowledge, kind of creating the invention of Ethereum smart contracts, creating high throughput smart contracts, blockchains. I guess like today, a lot of the different debates ultimately come down to how does this either manifest from a developer experience on building applications in any of these ecosystems, or from the end user perspective, whether it's a zero knowledge roll up, l two or l three, how do you ingest would say starkware kind of think about the product and developer side of things, of building in these different ecosystems, and there's ultimately the.
00:54:02.158 - 00:54:09.010, Speaker A: User or the developer. Do they need to know what a felt is? Because for the life of me I still can't figure out what a felt is.
00:54:10.710 - 00:55:24.510, Speaker B: Obviously users, unless they really want to, they don't need to know anything about it. Yeah, I think there is a big challenge here, but there is also an opportunity. The challenge is that of course we need to work out our dev tools and developer environment from scratch. And that's obviously a huge benefit that EVM and Solidi has today. The opportunity is that because the infrastructure in parts is missing then, and we Stackware are not going to develop all of it or even half of it. It gives the opportunity to developers to take part very early on in the very core infrastructure of the ecosystem. So sometimes it's really core infrastructure, sometimes it's really core developer tools and potentially also build things that are more well fitted for this new world of high scale and l two s that was not there before.
00:55:24.510 - 00:56:12.260, Speaker B: So it's hard for me to project what will be the change when it comes to devtools. But when it comes to infrastructure, I can definitely point out that on many other aspects, starting from full nodes and up all the way to indexers and block explorers, there is a different challenge when you need to handle many transactions per second, for example, just not something that you get with the language, even if you have the same language. But your tools should target completely different pace of like a speed of transaction, then you will anyway need to rebuild them. And there is some advantage in the fact that but we are doing it from scratch anyway. So thats some points that we can gain there.
00:56:13.040 - 00:56:56.220, Speaker C: And I guess that was primarily on the developer side of things. I see in some comical sense theres a lot of debating between Ethereum point of view and some of these newer ecosystems coming online. A lot of it again goes back to the user experience of say bridging or going from an l two to an l two or going from an l two to an l three or an l four. How do you ultimately think that user experience will manifest itself? Or again, the user ultimately, can that all be abstracted as much as possible and the user just not care about these things?
00:56:57.970 - 00:58:13.266, Speaker B: Yeah, it is a friction, but I used to think it's a much larger friction than what I think about it today because we do have the experience from Stark X. And I'll just give like a random example, like the feeling that you got from operating on Dy DX, which is an application that runs on top of Stark X. It didn't like. Sure, you have to do one transaction on layer one, lock the funds and then do your operation and if you want, and at the end of the day do another transaction on layer one. But if you anyway interact with applications directly on Ethereum, then very often you have this same experience, or even worse experience because you do all your transactions on layer one. So when you interact with, even with Uniswap, which is a simple application, but definitely when you interact with more complex ones and you want to do several operations, maybe in some reasonable sequence that takes also some time, then you have anyway not so good user experience. So as long as we maintain better user experience on the L2, then actually I don't think this point of friction is that big.
00:58:13.266 - 00:58:28.920, Speaker B: I mean, users will have to do layer one transaction, that's true. But the layer one users today, they already used to it. For the other users that don't exist today on layer one, it's not even clear to me that they are going to start from layer one anyway. So this point of friction does not exist.
00:58:31.580 - 00:59:30.908, Speaker C: Interesting. Now I'm super fascinated by all these different design choices and how they'll ultimately play out, I guess from like the l two to l three or even. I know Starkware was kind of pretty famous and kind of pioneering l three s. Omar and I love to talk about these quite a bit as well. How, I guess, does Starkware view these different layers? And I guess at the end of the day, how many layers do you need? I think that's like another kind of like funny question that the industry asks and I yeah, how many, I guess, do you think we'll need at the end of the day to ultimately get the level of scale or build the products that actually have many users and create like useful things that people want to do on chain and maybe even.
00:59:30.964 - 01:00:03.180, Speaker A: Just touch on one point, because I do see some confusion in the wild about what exactly the layer three is. So maybe talk about specialized sequencers, specialized provers, shared provers. When we talk about a different instance of the virtual machine, is it identical or does it allow for some modification? Where does the verifier lie? All of these details. So people talk about l two s and l three s, like the same way they talk about sports, right? I mean, basketball and boxing, they both qualify as sports, but they're sort of, they're mechanically very different things.
01:00:04.080 - 01:01:36.326, Speaker B: Yeah, I was actually going to touch exactly what Omar was talking about, because I think that sometimes there is like maybe mis presentation, I don't know, maybe it's from other l one s that says basically, oh, you need l three and then you need l four s. And they're all just there just for the sake of scalability. And while it has some value to scale as well, this is nothing. The only, or even sometimes the main reason why you want them, right? Like at the end of the day, what drive layer threes, or I would call it up chains on top of existing layers is the fact that there is some specific provider project company that wants some customization to its platform. And it's not extremely important for them to have their state in interaction with the entire world of other applications on whatever they choose to deploy, whether it be layer one or L2 or any other layer for that matter of fact. So it is important for them to have some communication with this layer, maybe some communication in the level with other options on the same layer, or with some application that exists on the top layer. But the reasons they will choose to go with an option is because they want to control all kind of things.
01:01:36.326 - 01:02:29.196, Speaker B: So in particular, they want, for example, to gain benefit from the fact that their state does not need the interaction. So they want their state separately, and they want to pay for it because it's separated, they want to pay for it less. And maybe they want to have their own execution environment where you know that you always touch just their state and maybe just with a specific set of transaction that can speed up their execution, right. Because it's, it's more focused just on their state and it's more focused, it's just their type of transaction. So you can build sequencers that are faster and chains that are cheaper for this particular application, and there can be a bunch of other reasons. They maybe want to operate on top of a layer that uses different language. So maybe you can think of an app chain that is an EVM chain on top of Starknet.
01:02:29.196 - 01:03:22.470, Speaker B: And it is important for them to have their chain interact with Starknet, but at the same time they want to use EVM environment or they want to use one of five different other environment. You can think of anything crazy from RISC zero to X 86 environments. I really don't know what will evolve, but nothing like all those things are on the table, right? And maybe they even want to control the way they do upgrades, all those things that not necessarily affect them just from a scalability point of view. They can be five other reasons why they would want to run their own chain. So I think, yeah, I think it's definitely an interesting pestle to investigate and we will investigate it, and I'm sure that many others I wouldn't just fall into like, okay, we will have gazillion layers necessarily five years from now because it's european scale. This is just not the right take.
01:03:23.970 - 01:03:53.158, Speaker A: And what about the composability and non shared states? So the second we start talking about isolated states, it becomes a little bit tricky to talk about. Okay, how does everybody else know about this state, first of all? And then are there certain checkpoints where they can interact? Because presumably if I bridge onto one of these app chains, at some point I'm going to want to leave, right? Or maybe at some point I'm going to want to interact intermediately with another app chain, right?
01:03:53.254 - 01:04:43.786, Speaker B: So we did a lot of this design space is actually quite similar to Stark, where we did ask ourselves all of those questions. And for example, one mechanism that we applied in Stark X and that we can also apply here, even if the execution environment is not stark anymore, but Starknet or anything else, is that as a user you can request to exit those up chains. And if you're not being served after some time, you can do it directly from the, from the upper layer. We called it an escape edge. And this is something that, especially when the up chain logic is more well defined, it becomes easier to implement. So if you have some predetermined logic, then it's the easiest. But even if you can, the operator can maintain some basic set of rules to become easier.
01:04:43.786 - 01:05:33.932, Speaker B: And even if not, it's possible. I wanted to say one thing about composability. Like part of the idea of why you want to have upchain is because you are telling yourself, I want my state isolated, I want to do different things and I give up composability. But there are ways to create some composability between different app chains, communicating on the same layer or even a different layer. I think there are a bunch of projects that are trying to do just that. But I will mention that, sure. When you operate on a third, on a layer three, on top of Styrocnet, for example, then you gain the benefit of your communication between one up chain that is alert three to another is that you move, you can move messages or funds or anything else really directly on Stocknet and you don't need to go to layer one.
01:05:33.932 - 01:05:37.000, Speaker B: This is an advantage that is non negligible.
01:05:38.460 - 01:05:43.836, Speaker A: So the L2 in some sense serves as the bridge or quasi bridge.
01:05:44.028 - 01:05:56.190, Speaker B: Yeah. And the advantage is that cheaper, potentially it can be also like slightly faster to finalize than I wanted and certainly cheaper.
01:05:56.810 - 01:05:58.066, Speaker A: Yeah, cheaper for sure.
01:05:58.178 - 01:05:58.778, Speaker B: Yeah.
01:05:58.914 - 01:06:09.470, Speaker A: Now what's about the remaining aspect of security here? So for now I'm using L2 as a quasi bridge. What is my weakest link?
01:06:10.290 - 01:07:06.130, Speaker B: So it really depends on what the application choose to do. I kind of get a feeling that in some applications, like for example games, really what important for them is to maintain the security between the L2 and themselves. And they really never expect assets to go to layer one or to maintain the security there. So maybe they will not bother to for example publish data on layer one directly, but will relay just on L2 security for that. They still gain significant security comparing to like just doing it on a separate, their own chain with like very low value of whatever token they have to protect them. So there is significant security gain even just by doing that. Yeah.
01:07:08.070 - 01:07:57.150, Speaker C: Interesting, interesting, super fascinating conversation. As I said, I'm really looking forward to how all these things manifest themselves and the user experience that they all create and extremely excited for, I guess what's to come, maybe as we wrap up the podcast, are there specific things that either the Starkware team or even more broadly, the zero knowledge space has made progress in as recently, or things that you're the, have not quite been cracked as of yet, but you feel like are going to make significant progress in 2023 that the industry or podcast listeners should be aware of in the coming months, coming years.
01:07:58.770 - 01:09:11.080, Speaker B: Yeah, I'll mention two things that actually three things. One we didn't get a chance to talk about, but particularly excited about. So one thing that we did touch is volition. I expect Stagnet to tackle this rather sooner, and I'm very like it will be a first and I'm curious to see what kind of applications and new use cases it will bring to start. Another one is maybe some exploration on the economics in stagnet in particular, and even more to be more specific when it comes to what is the right economy for encouraging developers to work on network, which is something that I personally am very excited about. I think if I have to choose two things, those would be the two. There are topics that are discussed more broadly, but we already made some progress there, which is the account abstraction that I think startnet pioneered.
01:09:11.080 - 01:09:36.737, Speaker B: And this is basically the default there. And I think this is a huge advantage, that it is the default just because it causes more innovation in this particular direction and more adoption for wallets that operate with that. But those are two topics that we haven't touched. Maybe on the next opportunity next time, for sure.
01:09:36.833 - 01:09:43.989, Speaker C: Omar, any closing thoughts? I know this is your child and the area that you love the most.
01:09:46.609 - 01:11:04.752, Speaker A: I think what we discussed earlier, it really causes one to think we've yet to scratch the surface on a lot of the design space in certain primitives that are only enabled once you start thinking about L2s and once you start thinking about scaling in general. The one that we talked about the most, and the one that I'm certainly most interested in, is the data availability problem. And I do think a lot of the guesswork that people are doing now to determine whether it's sort of viable, I think it doesn't give l two s a fair shake, only because there has been so little thinking and so little work today to get us to this point. You didn't have to worry too much about data availability so far. Now that you've broken or you've crossed the chasm of this idea that you can now securely transact off chain, and you have to build this entire prover architecture, and you have to worry about Snarex versus Starix versus all these different prover systems, all these things. These were all enormous mountains that you had to climb before you started to worry about. Okay, now what do I do with data availability? Right? I think a lot of people, Starcourt included, don't get enough credit for basically climbing that mountain.
01:11:04.752 - 01:11:22.540, Speaker A: Now that we're at the peak of that mountain, we realize, oh, once you're at the peak, you realize there are many other mountains that need to be scaled. And so that's something that I always like to remind people of, that we're only worrying about data availability problems now because we've come so far, not because we haven't.
01:11:27.040 - 01:12:00.820, Speaker C: Perfect. Well, I guess we can end it there. But truly, thank you both for coming on the podcast, amazing conversation, and really look forward to what this industry is going to bring forth in terms of users, in terms of products. I think at the end of the day, we're all working towards trying to build things people actually want to use on chains. And then I think that's a future that is coming rather sooner than later. So again, thank you so much for coming on really fun conversation.
