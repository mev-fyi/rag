00:00:00.330 - 00:00:21.470, Speaker A: Next up, a panel discussion on decentralized proving infrastructure and incentives. Please welcome Cooper Koontz of Aztec Labs, Nazar Khan from Terra Wolf, Ben live sheets from matter Labs, and Michael Gow of fabric cryptography. Moderating this panel is Robert Koshig of one Kx.
00:00:22.290 - 00:01:04.034, Speaker B: All right, hello, everybody. Good afternoon. Better say, yeah. Welcome to the panel of decentralized proving. Yeah, I'm Robert from one Kx, and I have four wonderful panelists here to dig deeper into the topic and discuss decentralization of proving with the incentive design lens to it. And yeah, to start, maybe will we make around, can you give a brief introduction to you what you're doing or better, like your protocol or your company is doing? And yeah, how does it relate to the decentralization of proving so the audience gets a context to it and yeah, maybe we start with you.
00:01:04.152 - 00:01:42.270, Speaker C: Yeah, hi, my name is Cooper. I'm the head of decentralization at Aztec Labs. Fancy name for product manager on our sequencing and proving infrastructure and governance. Aztec is a privacy first zero knowledge rollup. We would call it a zero knowledge, zero knowledge rollup. So users are generating zero knowledge proofs client side, sending them to the network to be aggregated and verified later on Ethereum and through that client side proving, that's how you get the privacy properties of the aztec network. And so when we talk about decentralized proving, all of our proofs are generated client side, and we're talking about decentralizing the aggregation to submit one final proof to the Ethereum network.
00:01:42.270 - 00:01:46.480, Speaker C: Yeah, that's enough about Aztec, probably.
00:01:52.710 - 00:02:22.970, Speaker D: Hello? Oh, much better. My name is Ben Lichitz. I'm the vp of research at Zksync Metal Labs. Yeah, so I think there's obviously been quite a lot of excitement about decentralized sequencing. And of course, as we look into the future, I think opening up the prover pool is something that we're starting to look at. And I think this is going to be interesting from just about every angle. Software, hardware, software, hardware, code design.
00:02:22.970 - 00:02:56.760, Speaker D: We've already seen a bunch of really interesting talks in the preceding 2 hours about many of these topics. So yeah, happy to dig in to this. There is quite a lot of depth in this topic, and I think it's really fun to think about sort of steady state outcomes. I think the economics of this, how to make it so that the benefits of successful software, hardware code design are things that the end user takes advantage of, which lead to things like fee reduction. So quite a lot to talk about. Thanks.
00:02:59.630 - 00:03:52.330, Speaker A: I'm the CEO of fabric cryptography. We're a company building general purpose cryptography hardware. Yeah, so basically we're building general purpose cryptography hardware. For those of you who are less familiar with us, it's basically like a GPU built from the ground up for all of cryptography. So whether it's a ZK snark or a ZK stark or any of the future proving schemes, it all is supported by our custom silicon. So I'm very excited to be here and to talk about how do we actually get this out there into the hands of users, and how do we scale up decentralized proving and really foster a sea change and how big statements can actually be proved in real time.
00:03:53.820 - 00:04:39.370, Speaker E: Good afternoon. Nazra Khan here. I'm the co founder and CEO of Terrawolf. We're a publicly listed bitcoin miner, but our background is really in energy infrastructure. And so our interest is in really, as we look out three, four, five years from now, if we're anywhere close to right in the ubiquitousness of how this is going to be used, it's going to be a tremendous amount of power that's needed to kind of support this. And so whether that's in an aggregate at one location or kind of dispersed throughout individuals, understanding kind of how the power supports that is an important part that we see. And so we've been very involved in understanding and helping kind of shape that narrative in terms of how the decentralization kind of scale up as quickly as possible.
00:04:40.940 - 00:05:02.640, Speaker B: All right, thanks a lot though. Then let's dig a little bit deeper and, yeah, let's maybe start with the network side here. And, yeah, maybe with you, Ben, because you already touched upon a couple of points, because my question would be, what are you focused on the most when you think about designing your network? When it comes to decentralization of the provers?
00:05:04.660 - 00:06:25.096, Speaker D: I guess you start with the easy questions, right? Look, yeah, let's see if this works. There is a lot of focus in the near term on just continuously delivering improvements to the proof system that we currently are running. And I think chances are not just us, but other zero knowledge roll ups will be in a situation where for the near to medium term, we'll be delivering sort of improvements to the proof systems that we have. And I think that's going to be the case for at least some time. And I think perhaps over time, we'll find ourselves in a situation where sort of other parties will appear. So, firstly, I think we'll be in a situation where hardware based improvements is something that we'll want to avail ourselves of, and we will want to deliver those kinds of things to the end user. We'll want to, as I said before, open ourselves up for other people doing proofs, and by doing so, hopefully reducing the cost of proof production, proof generation, and over time, transferring these kinds of savings to the end user as well, and by doing so, ultimately reducing the fees.
00:06:25.096 - 00:06:35.500, Speaker D: But, yeah, I think this will take a little bit of time. So at this point, things are very much in an experimental stage. Pause.
00:06:37.460 - 00:07:18.236, Speaker C: Yeah. And then from Aztec's perspective, we're a privacy first roll up. We believe that privacy protocols have to be credibly neutral. We think that we will start from day one with a fully permissionless sequencing and proving designs. Most of the prioritization for this is generally censorship resistance. Having fully permissionless proving marketplaces where anyone can add compute to them is a much harder to attack network architecture than enshrining a specific prover or prover who might not agree with users in a particular jurisdiction or trying to do a specific thing on chain. So across the board, aztec is trying to be as credibly neutral as possible.
00:07:18.236 - 00:07:38.500, Speaker C: And you do that by allowing anyone to run your hardware. And so that's why we're super excited about letting anyone add and participate in auctions or other mechanisms to prove different parts of aztec blocks. And we do see this as a fundamental day one requirement for the aztec network, given our posture on privacy.
00:07:39.400 - 00:08:10.076, Speaker B: Okay, well, then let's maybe involve the supply side, because I would obviously be curious to which extent, for example, you, michael, can help with these things because when you talk about eventually also going the hardware route or allowing anybody to run a machine, basically run approver. Yeah, maybe, michael, you can touch upon what hardware side can support that, what.
00:08:10.098 - 00:08:13.484, Speaker A: The hardware side is supporting, or how.
00:08:13.522 - 00:08:15.176, Speaker B: The hardware side can support that.
00:08:15.218 - 00:09:09.404, Speaker A: Oh, yeah. So we're pretty agnostic to the format that roll ups are taking because really every layer two or every chain is taking a different approach to accelerating and also incentivizing and also decentralizing ZK proof production. And what we really want is to benefit the entire ecosystem. So whether you're taking a really decentralized approach and you're doing it from day one, or whether you currently have centralized proving and you want to move towards the decentralized proving kind of method, we have several offerings in our roadmap for all of these use cases. So we have a service platform that's coming later this year. We have boxes shipping in volume later this year. Both of those are going to help people who just want more compute and they want it in a data center format.
00:09:09.404 - 00:09:39.000, Speaker A: And then we're also talking with a few protocols about a consumer grade kind of PCIe card that can go in, let's say a home computer, and that'll be the equivalent of orders of magnitude above 128 core xeon processor. So we're super excited that that kind of compute power can be in everyday people's hands. And so whatever the protocol design, we're willing to roll up our sleeves and work with the protocol to achieve the goals that they set out to achieve.
00:09:39.900 - 00:10:10.800, Speaker B: Okay, I mean, that sounds really good, but I heard a lot of times compute power, and that might be the time to involve nest in the conversation, because maybe you can share in the introduction. You started pointing towards the direction, okay, now things look good, but a couple of years from now, there might be problems, and maybe you can elaborate on that, because that might help also us to understand how we should already now think about the way we should architect the networks.
00:10:11.220 - 00:11:15.592, Speaker E: Absolutely. And I think a number of the discussions today and on the panel before cost, I mean, ultimately, cost is going to be an important metric in the ability for these ZK proofs to really scale up and spread. And so that's where we really come in, ultimately, understanding how to run the infrastructure to support it, whether it's a fabric, VPU, or any other kind of infrastructure that's needed, how to place it, and how to kind of support and run it, is ultimately where we come in. For 20 plus years, we were in the infrastructure space. And so our job was to basically deliver power at the cheapest cost. And so what we've effectively done is now we're kind of taking power, putting that into some sort of compute, and kind of delivering that compute at the lowest possible cost. And so as we look out over three, four, five years, ultimately, without having a very clear and concise understanding of how to be able to drive down the cost of power, which is not just kind of the layer two s that are running, not just the hardware, but also where that hardware sits and how that's integrated back into the overall electric grid is important.
00:11:15.592 - 00:12:15.100, Speaker E: And that's really where we focus our time in understanding how we can kind of provide that. And again, we're agnostic to how that plays out, because ultimately, whatever it is, is going to need some power associated with it. And as we look out, when we look at kind of the data field more broadly, not just ZK proofs, but all forms of data compute, there's a fairly significant growth that's projected over the next five years, people are talking about the size of that industry going three, four, five times. And so the ability to place that kind of a load into the grid requires, again, a very nuanced understanding. And so we spend a lot of time early on in thinking about these protocols, these hardware applications, how they fit in and providing our input into how over the long run, especially at scale, how they'll fit into the system. And so that's where, again, we always kind of come back to ultimate cost in the long run, because again, that lowest cost is what's going to drive the greatest kind of scale.
00:12:17.750 - 00:13:03.902, Speaker D: Sure. Thanks. Let's imagine we have a couple of orders of magnitude more transactions being settled on oral ops compared to mainnet, right? So let's imagine a future like that, which is probably not so far away. So what does the shape of this prover industry look like? And my sense is that there's a bunch of open questions. Like, for example, does solo proving, right, you have a card that goes into your pc. Does it make financial sense? And can we really foster that in order to grow the level of decentralization, sort of similar to what ethereum is trying to do with solid staking or whatnot? Right? I mean, it's an interesting question, right. Here's another one, which is what is the sort of correlation and relationship between hardware manufacturers, hardware makers and hardware operators? Right.
00:13:03.902 - 00:14:16.440, Speaker D: And to what extent are these open designs that are subject to scrutiny, other standards around these things? Or do people basically make the hardware and they keep it in their vault and they never open it up, and they just use it to basically maintain their competitive advantage when it comes to proving, if it's an auction model, allows them to win just about every auction. Of course, you can imagine a situation in which things get consolidated, which at least superficially is similar to what you would see in case of proof of work mining pools. Right. And I think at this point, given that it's quite early, I think we still have a chance to sort of reflect on some of the challenges around POW. And these are challenges that people like to complain about, such as power efficiency or inefficiency, but also like new challenges that you just don't really see in the bitcoin mining space. And kind of try to see if we can combine the features of cryptographic systems that we build with the opportunities that come in terms of mechanism design to potentially create a slightly different landscape for this prover industry, so to speak.
00:14:17.770 - 00:14:50.766, Speaker B: All right, yeah, very interesting. But let's maybe talk a little bit about the different approaches to it. And you, Cooper, for example, in your forum, there's quite a lively discussion on the different approaches. So maybe, I'm sure not everybody in the audience went through all the forum posts. Yeah. Maybe give brief recap of the approaches that you considered and maybe also share why you are advocating for your sidecar approach when it comes to decentralization.
00:14:50.958 - 00:15:31.754, Speaker C: Yeah. So when I was tasked at Aztec to try to figure out how we're going to decentralize our roll up, we really took building in public very seriously. If you go to forum aztec network, there's probably eight to ten different sequencer selection protocols that are all fully decentralized there. There's probably, I don't know, five to ten proving marketplace protocols as well that were articulated either by employees of ours, employees at the Ethereum foundation, employees at Espresso Systems. There's a lot of really nice designs, honestly. So if you're looking for how roll ups are thinking about decentralizing their infrastructure, I would say this is probably, as far as I'm aware, the biggest body of work on the subject. In our case, we have kind of just a random leader election like you have in Ethereum.
00:15:31.754 - 00:16:08.566, Speaker C: You get a randomly elected proposer. Anyone can participate in this. We support out of protocol pbs like you see in the Ethereum landscape right now. And then today we're here to talk about proving. So through that research, we kind of found two different categories of proving marketplace designs. You can have a cooperative design which would be kind of similar with consensus networks or byzantine fault tolerant consensus protocols, where you have a group of machines who are collectively working together to produce proofs. And so you could have traditional byzantine assumptions over failure modes, redundancy, types of things that you would expect to see in byzantine consensus protocols applied to actual proving redundancy.
00:16:08.566 - 00:16:53.574, Speaker C: So you have one category of cooperative, and on the other side you have competitive marketplaces. And within competitive marketplaces, you can have proof races, right? First person to produce proof wins. You can have economic marketplaces and competitions where people are actually proposing auctions and trying to participate in an economic competition for their rights to prove aztec blocks. All decentralized proving market designs will fall into one of these categories of competitive or cooperative. And then kind of depending on what you're optimizing for or what properties you want out of your system, you're going to know different steps along this decision tree. Aztec, actually, after deciding and kind of mapping out this landscape, decided not to enshrine a particular method for proving an aztec block. And so we are currently looking at a protocol articulated on our forum called Sidecar.
00:16:53.574 - 00:17:28.562, Speaker C: It's a way where a sequencer can out of protocol, dedicate or delegate proving rights to any subcontractor that they want through an open source, permissionless auction. We call it proverboost. It's kind of an analog to me. V boost nil, Gevulot, succinct. Any third party proving marketplace, any person who has enough compute can directly participate in an auction to prove Aztec's block. You put up an economic commitment, you say, hey, for $10,000 or whatever, I'm going to prove this block. And if they don't submit the proofs within a given amount of time, we're going to slash their stake and treat it as a missed slot.
00:17:28.562 - 00:18:10.610, Speaker C: And so the current aztec designs are kind of unappinionated and support both cooperative and competitive proven marketplaces entirely outside of the scope of our protocol. If any of these models eventually end up being incredibly successful, incredibly centralizing or otherwise aztec's governance or would potentially consider releasing a future version of the protocol that enshrines the best model of sorts. But given the relative immaturity of the industry at the moment, we don't think any of these models is well understood enough to enshrine it in particular yet. So hopefully that gives you a sense of the landscape and kind of where we're thinking. Highly recommend going to the aztec forum. We just did a request for comments on the end to end of aztec's block production.
00:18:11.270 - 00:18:17.694, Speaker B: All right, then maybe possible back to Ben, because did you already comment on the aztec forum?
00:18:17.822 - 00:19:06.642, Speaker D: I guess not, but we're pretty well aware of the work that's going on. We like it a lot, actually. And I think we also admire the process that's been run by Adstec at least twice with sequences selection as well as proof of selection activities that are ongoing. I guess just to add a little bit to what was said, how should I put it? I think we're trying to figure out a way in which we can experiment with these different mechanisms to see what the steady state outcome might be. Right. And trying to figure out a forum or an approach that we can take to basically do these kinds of experiments. We haven't quite decided what it should be.
00:19:06.642 - 00:19:17.160, Speaker D: I think that's sort of one of the things that perhaps is stopping us or slowing us down at the moment, but, yeah. So over time, I think we'll hopefully figure this out. As a community.
00:19:19.370 - 00:19:25.100, Speaker C: You can wait a few months and see what aztec does wrong, and then you can hopefully do a better job than us.
00:19:27.800 - 00:19:30.390, Speaker D: It's like the opposite of front running. Right.
00:19:32.780 - 00:20:09.904, Speaker B: Okay, nice discussion here. Yeah. Maybe also to ask Michael here, there's also some discussion. This is very much about the general different proving approaches and where to locate. There's also a lot of discussions around, they highlight very much the benefit of distributed proving and kind of like benefiting from these sharing economies putting in the hand of everybody. And this way, no need for kind of like centralized large operators, no need for specialized hardware. Yeah.
00:20:09.904 - 00:20:15.972, Speaker B: Curious how you would view these arguments, Michael, and what you would respond to that for sure.
00:20:16.026 - 00:21:22.760, Speaker A: I mean, there are a lot of people who talk about the centralizing effects of specialized hardware. And as someone who has seen the semiconductor industry play out over the years, the semiconductor industry is generally a friend of mass adoption. So, for example, when cpus first came out in the 1980s and really started proliferating, that led to the rise of the personal computer. And eventually, when process node shrinks got so good and so big, and then gpus became co integrated with cpus on mobile processors, we saw the rise of the mobile phone, and now everyone has a supercomputer in their pocket. And this is exactly the future that we want for not just the VPU, but all categories of ZK acceleration. We want everyone to have access to this powerful cryptography, and that's the future that we ultimately want to create. So that's where our consumer kind of PCIe card is going as an initiative, and it's where our company wants to go as a vision.
00:21:24.220 - 00:21:34.664, Speaker B: Okay, yeah. Maybe also, Nas, to which extent do you think decentralization, or what Michael plans to enable, can help the problems you raised earlier?
00:21:34.792 - 00:22:32.060, Speaker E: Yeah, it's interesting. Our background is really on the physical infrastructure side, right? I mean, again, we built power plants and operated power plants for 20 years. And so when we talk about decentralization within these layers, the idea that maybe one place is running all of these nodes and the compute power sits there, but it's controlled or operated by many different people. Both of them could be true, right? The compute could be decentralized and how it's being used, but the compute could also reside at one place. And so that's kind of a fundamental issue that at least I've been kind of thinking through, is that because ultimately, again, if I always kind of come back to the cost, right, I mean, ultimately the cost has to be low enough for it to be pervasive, and that's what's going to make it most pervasive, is a really low cost. And so if it's going to be used quite a bit, has to have a low cost. And so when you start to break down these things and say, hey, where can we operate this proving at the lowest possible cost? It's likely going to be at something that's more than just someone's individual personal computer.
00:22:32.060 - 00:22:59.560, Speaker E: And so that's, I think, kind of a discussion that needs to be had is when we talk about decentralization, what layer is it occurring at and how do we ensure that if there is a significant amount of compute at one place, that ultimately the underlying compute operates in decentralized way? And so that's where, again, we have our own perspective on how that's being drawn up. But I think as a broader community, that's something we should all be engaging in and thinking through. How do we, I think, kind of talk about that level of decentralization.
00:23:00.060 - 00:23:55.284, Speaker A: Yeah. And there's something that I'd like to add to this, which is I've seen an amazing economy of scale play out in the bitcoin mining world. I've been in bitcoin since 2011. I've seen how the proof of work incentives have led to a cost per terrahash and joules per terrahash, that frankly, if you asked a standard ASIC engineer and they did some simulations, they would think it's unthinkable that we have the level of efficiency that we have, the level of cost that we do now. It's a little bit wasteful, but in the context of the ZK industry, if we're doing this incredibly useful thing with positive externalities for all of society, that's something that we want to drive economies of scale on. Right? So I don't think it's kind of an either or. I don't think it's like necessarily decentralized versus centralized.
00:23:55.284 - 00:24:03.230, Speaker A: I think that you can have economies of scale and simultaneously you can enable a lot of people to prove at home. It doesn't have to be either or.
00:24:04.880 - 00:24:49.500, Speaker D: I was going to say, yeah, proving at home, stay at home, all of these things resonate a lot. And so let's not forget that the monolithic roll up case is not the only case, right? Because there is the privacy friendly case of the user doing their own proving right in the comfort of their own home. And as time goes by, chances are we'll want to prove more complex statements beyond what we can do now on a small machine or phone, which has to do, let's say, with credentials, things like that. That's very nice, but I think over time we'd like to do a bit more. So that's one thing. The other thing is that there is also the case for variable pricing. That is to say, you pay more, your proof will happen faster, you pay less.
00:24:49.500 - 00:25:20.340, Speaker D: Well, maybe not quite as fast, but maybe it will go to a bunch of people who are doing this at home, solo proving style. Right? And everybody is happy because these solo provers get some input to work on, and the user is not paying so much. And maybe this is the right sort of equilibrium to achieve there. Right? Again, what I'm saying is that there is more diversity of these use cases. If you think about this end to end. Some of these things come from privacy. Others come from sort of different economic parameters.
00:25:22.360 - 00:25:48.892, Speaker C: Yeah. On that note, I would say Aztec is trying to be one of the first fully programmable private smart contract platforms in the world. You can deploy contracts where the actual bytecode is encrypted and other things. And the first version of it is very novel. It will be somewhat limited in the expressivity that you have within that smart contract. Right. There is only so much we can do to keep block times, to keep proof times, to keep execution times sufficiently low that you can actually use our system.
00:25:48.892 - 00:26:25.080, Speaker C: ZK is something that will fundamentally consume all of the compute, all of the spare compute that will be available in the world. As much expressivity as our proving systems will allow. Aztec will try to use as much hardware as available and as much data availability as our da layers, et cetera, et cetera. And so we're really at, like, day one of the amount of expressivity that you're currently getting in zero knowledge proving systems. And I would say if you're looking at aztec smart contracts as a kind of metric, we're probably at, what, the eight bit resolution equivalent of expressivity within our smart contracts. And so it's going to be a very long game that's going to use.
00:26:25.150 - 00:27:10.712, Speaker E: A lot of hardware and to that point, and that's where the scale rises exponentially. Right. It's not a linear progression. And so as that expressivity kind of increases, it's going to be kind of a nonlinear growth. And that's where, again, understanding how these loads are going to be integrated back to the physical grid is, we think, in a terrible, we think it's kind of an underappreciated point. And so we're solving this big problem of how do we get the system to work. But if we're so fortunate to figure that out, which, given my perspective, I think will happen, then we'd have this other challenge of saying, now, okay, now, how do these loads kind of integrate back to the grid? And again, that's where someone, from a physical energy infrastructure perspective, I think, has and can provide input into that process.
00:27:10.712 - 00:27:51.540, Speaker E: And so that's, again, where Terrell is really thinking about not just kind of what's happening in the immediate, but more, hey, as these kind of things catch on and that demand from it kind of grows exponentially, how do you make sure that these loads can kind of give you integrated back? And that's where, again, I always come back to this just kind of fundamental thing of kind of cost and availability. So long as the cost and availability continue to support that growth, it's going to keep growing. And to the extent that there's bottlenecks in it, that's where you're going to see a big problem. So that's where, again, I think as we look at it, there's the development side of it, which is obviously critical and which is the base layer. But beyond that, for it to really catch on, there has to be this thought around, how do these loads kind of fit into the grid?
00:27:51.880 - 00:28:44.448, Speaker A: And I'm incredibly excited to see protocols talking about latency and throughput incentives, because I've seen how far, just exponentially speaking, this kind of incentive structure can play out in terms of how big of a technological lift, right? So the bitcoin network started out as maybe like terrahashes per second of compute in 2011 when I first saw it. And today just one ant miner is terrahashes. We have almost a zeta hash per second. It would be the first zeta scale computer on earth. And that's pretty cool. And that same kind of like billion x increase in compute has happened in the field of traditional cpus. And that is why we have Chat GPT, and that is why we have 3d gaming, and that's why we have ray tracing.
00:28:44.448 - 00:29:29.220, Speaker A: And it's also why you see all of the devices in this room be able to operate the way they do. And so what I'm incredibly excited about is you're going to see transactions that go way beyond what we think of as transactions today, right? We think of like sending one eth from one person to another. We think of things like uniswap. Okay, imagine dexes that reach the same TPS as binance, right? That's a really revolutionary thing to finally reach parity, at least in functionality, with centralized systems. And that's something that I'm incredibly excited to see because I can see it happening as a domino effect from what we're talking about on stage today. And so thank you to Cooper for starting that conversation.
00:29:31.080 - 00:30:14.004, Speaker B: Yeah, I want to come back to a point you guys mentioned earlier, right, that proof of work centralization effect, but then also economies of scale, which is also centralization effect. The marketplace approach actually also typically favors the bigger players on the longer term. And that's maybe for you and Ben. A question. Are you a little bit worried about, for example, verticalization, which we, for example, also see on ethereum, kind of like, even to a point that might be threatened, the original idea of your protocol? So, like, the workings, or is it something where it's like, yeah, I mean, we're so early. Let's not worry about it.
00:30:14.122 - 00:31:19.048, Speaker C: Yeah, it's a really good question. And so Aztec started our design, decentralizing our sequencers, which is our analog of proposers, who actually determines what gets included in the history of the chain. We think that's where you should start if you're worried about decentralization through the lens of censorship, resistance. And so we have a very decentralized sequencing set. Anyone can stake some tokens on l one run, something that looks like an ETH validator, hardware, you'll get to win a random leader election and propose a block. And so, starting from there, we actually see a little bit of centralization within the proving marketplaces to be okay as long as that there's an ability for, let's say, a zero day or a black day event or like a black swan event, where all of your provers to randomly drop off the face of the earth one day. How long does it take for your network to actually produce the next block and come back to life? And so we looked through the lens of, like, it's okay if the proving market actually centralizes on a few actors or a few people vertically integrating, as long as if those people stop doing this service, someone else can compete immediately in the next block and be sufficiently incentivized to provide that service.
00:31:19.048 - 00:32:15.064, Speaker C: And so, in the current designs of what we call Frenet, of our sequencing selection protocol and sidecar is our proving protocol, we do allow a sequencer to prove and stake to their own address to vertically integrate as a sequencer, the protocol really doesn't care about that. They'll have the same slashing conditions as if it was a fully decentralized proving marketplace. If that does end up resulting in, let's say, censorship in some way, shape or form, or you see a Lido like event coming into the network where they're picking themselves to vertically integrate every block, I would say that's not very aztec aligned of you, and please don't do that. And we'll deal with those things as they come. In general, no one's really seen a fully decentralized roll up, let alone a zero knowledge one, let alone the one that has privacy. And so we're pretty happy with the level of censorship guarantees the current designs provide. But that's a long way of saying yes.
00:32:15.064 - 00:32:17.210, Speaker C: It's something that we're constantly thinking about.
00:32:20.430 - 00:32:55.734, Speaker D: Well, I think we roughly understand what the good outcome is and what the bad outcome is as well. So the bad outcome is probably easy to describe, which is, well, we basically have one party just sort of ending its innovation, so to speak. Right. We're just like deciding, well, this is the final proof system, and this is the final hardware setup. Some mix of cpus and gpus and maybe fpgas. And it's like, this is it, right? This is all you get. I think a more exciting, successful outcome is when you have multiple parties.
00:32:55.734 - 00:33:29.234, Speaker D: And perhaps these are relative large players. Maybe there's not hundreds of these players, but maybe a handful competing on this software code design sort of axis, so to speak. Right. So where things are reasonably open on both fronts, let's say. And we're currently seeing that across roll ups, of course. Right, where there's competition within the space of proof systems. But I think some of this can happen within a single roll up as well.
00:33:29.234 - 00:33:41.480, Speaker D: Right. And as long as that competition continues for the foreseeable future, there are benefits to the end user. And I think that's sort of the right focal point for us as well.
00:33:42.810 - 00:34:23.810, Speaker C: Yeah, I was just going to say one more thing is that we do see, similar to the terra wolf folks, we see the cost of proving eventually, hopefully trending as close to zero, plus electricity and the cost of running these things, et cetera, et cetera. And so that is an argument in the favor of vertical integration. The cheapest way that you will prove a block is by having enough hardware and doing it yourself rather than outsourcing it. And so we really treat decentralizing, our proving as kind of a subcontracting labor problem. If you're a sequencer and you need to get a job done, you can choose to do that job yourself, or you can choose to subcontract it to a separate labor market. And so it has very nice economic analogs to just any subcontracting based labor market. And the cost of that should trend as close to the market can tolerate.
00:34:27.690 - 00:34:50.698, Speaker B: Unfortunately, we're running a little bit out of time here. I think we could go on and on, but, yeah, maybe for Michael, the last question here. Can you maybe give an outlook of what we can expect with respect to scale and cost improvements, what the hardware side will deliver? What can we be excited about over the next year? No pressure.
00:34:50.874 - 00:35:48.830, Speaker A: Yeah, I think the cost of proving over the next kind of five to ten years, I anticipate it going down by multiple orders of magnitude. This is across infrastructure and the infrastructure overhead and things like the cost of power, the cost of the rack setup and stuff like that. It's across companies like us and what we're doing with custom silicon, we can deliver orders of magnitude by ourselves. And it's also companies that are developing new proof systems that are even more efficient and that have lower overhead with respect to plain text computation. But I think the combination of all of these, you see an AI, it got 44 times more efficient in something like a six year period from a software perspective. It got a similar amount in hardware efficiency. It went from the terraflop to the petaflop level, on the chip level, and all of that at the same time.
00:35:48.830 - 00:36:12.950, Speaker A: And there were better economies of scale on the AWS and GCP fronts as well. And we see that same thing playing out, especially because of the incentive structures that we're talking about. And so what I'm really excited about is that I think the overall market is going to grow even as the proof cost shrinks, because people are going to be proving exponentially larger statements exponentially more frequently once the cost just dwindles.
00:36:14.250 - 00:36:24.660, Speaker B: All right, well, then, thanks again, everybody, for this very interesting discussion. And, yeah, we'll keep building the right incentives, I guess. Thanks a lot.
