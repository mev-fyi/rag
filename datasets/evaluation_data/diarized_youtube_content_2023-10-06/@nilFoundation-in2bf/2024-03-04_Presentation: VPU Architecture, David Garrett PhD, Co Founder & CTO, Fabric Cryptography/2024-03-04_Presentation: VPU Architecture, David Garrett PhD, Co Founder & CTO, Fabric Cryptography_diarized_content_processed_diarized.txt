00:00:01.290 - 00:00:33.636, Speaker A: Now let's turn our attention to VPU architecture. Please welcome David Garrett, PhD, co founder and CTO of Fabric Cryptography. Thank you so much. I don't know if I recognize that guy. Is that actually right? We'll get these slides set up here. Well, excited to be here today. We're going to give you a vision of where compute is going with cryptography.
00:00:33.636 - 00:01:39.270, Speaker A: We're going to talk about the VPU. It's a general purpose cryptography compute paradigm. And you're going to see some lessons we've seen from the company fabric, how we've learned through the AI world, through the semiconductor world and cryptography world, how to bring these all together and produce something that we think is going to be a game changer for the world of cryptography. What's interesting, the company's been around over a year now and really striving to rethink cryptography, how we can deploy a canvas for all the algorithms and all the things that cryptographers want to do. And, you know, conferences like this, you see how every single day there's a new proposal, there's all kinds of changes in cryptography and protocols, and we want to make sure that we have the hardware that's ready for that. Just 1 second. Waiting for the slides here.
00:01:39.270 - 00:02:25.938, Speaker A: There we go. Well, again, we're going to talk about the VPU, the verifiable processing unit. You've seen cpus, you've seen the fpgas, gpus. We're actually looking at a paradigm that we're going to create something really specifically for cryptography, yet it's going to be general purpose. And so the idea behind this fabric is the company we're working to scale zero knowledge proofs and we'll deep dive into this GPU architecture. So we'll do next slide. Okay, thank you so much.
00:02:25.938 - 00:03:15.634, Speaker A: Again, what we're doing with the VPU, it's custom silicon. So we're going to build a device ultrafast end to improving encryption. The idea behind this is we've built a crypto native instruction set. The idea is we're looking at the computing primitives for cryptography and not just building a hardened accelerator for any one particular protocol. The things we've learned through the AI world and about hardware and software is one of the most important things to deploying the best hardware is actually to have the best software story. So at the same time that we're developing the VPU, we've got the compiler and the SDK, we've got the tooling that's going to make it easy for anybody to deploy and use and actually explore the space of cryptography. And then as the company, we're building the compute cards and the hardware that's going to scale this up.
00:03:15.634 - 00:03:41.576, Speaker A: So we have the custom silicon, we've rethought cryptography, and we're going to bring the boxes and compute cards to actually make this real. There we go. So let's take a look at the compute paradigms you've seen. The CPU the last 50 years has been revolutionized by processors. General purpose. It's easy to go do kinds of software that you want to do, but it's really hard to scale that up. For cryptography.
00:03:41.576 - 00:04:18.728, Speaker A: It's really not meant for the types of workloads that we see. More recently, another paradigm, the FPGA, the field programmable gate array is not really a CPU. It's something that allows you to build circuits. It's really cheap to get going, but what happens is it runs out of gas. In the economics of scaling, you can build something quickly, but then if you want to scale that up, it's an enormous cost. The GPU is an interesting use case in 20 years ago, the graphics processor. It's really part of doing the heavy lifting that a CPU couldn't do by building a huge array of parallel data paths.
00:04:18.728 - 00:05:05.780, Speaker A: And so you see the GPU a lot mentioned, but it's really not optimized for cryptography. It has a lot of things that are shifting towards AI. You're building floating point math, and it's missing out the key things that we know for cryptography. And so that's when we talk about this new domain, the VPU. General purpose cryptography, optimized for prime field math. The goal of our design is to do this field math at the best rate possible and provide all the primitives that you're going to need, we'll do next. So why build this? So the idea is today, zero knowledge proofs are this amazing revolution in how we're going to take back our privacy and security of all of our information.
00:05:05.780 - 00:05:41.200, Speaker A: But it's running on cpus and it's just not fast enough. So the idea, these apps, the use cases, are limited by the speed of what's available. The idea behind the VPU is I want you to envision what you could do with that if you had 1000 x to compute, give you a million times the resources out there to actually help you do that. And really we want to focus on building more than what you can see today. Think of how it could go in the future. And revolutionize security and privacy. Really, the goal is for us to build a performant, flexible and scalable general purpose cryptography compute engine.
00:05:41.200 - 00:06:36.990, Speaker A: And what we talk about is this is this blank canvas for you guys to do what you want with it. Next. So the idea, why do we talk about general purpose and scalability? The idea behind the VPU is it takes a lot of effort to build a silicon device. But once you've done that, the economies of scale in this fabless semiconductor world will allow us to roll out a device that can cover just about any protocol. We can look at over things as algorithms change. When you compare that to where you would go with fixed function asics, you would pick a protocol, you'd fix that in you build an accelerator, you pay that cost, and then you've got a limited lifetime, it's going to run out of room, and all of a sudden you have to throw that away and start the whole process over again. So the idea behind this general purpose is the way that the GPU really revolutionized AI.
00:06:36.990 - 00:07:24.580, Speaker A: We want the VPU to do the same for cryptography. Next slide. So, our VPU architecture, the idea behind this is we want to support all of the types of things in cryptography, ZK starks, ZK snarks, msms, ntts, fries hashing, all of these things are you're familiar with. We want to be good at all of those. Even the constructs of fully homomorphic encryption are really going to change the composition of the data center. What we do with the VPU architecture, we have four main components, a big array of our compute tiles that allow you to do this prime field math efficiently, and the movement of data. We have a huge number of external memory interfaces that allow us to move large gigabyte tables around.
00:07:24.580 - 00:07:59.950, Speaker A: We have an eight core risk five processor that allows us to be the brains of the system and do all kinds of neat things, like the workloads that don't fit on the tile. Witness generation would be a good example, and then PCIe interfaces that can move things through. What we look at is the VPU is just a chip, but what we do that is we deploy that on a compute card. And so you can see the VPU 860. It's a GPU like form factor PCIe card. We actually can deploy three of these devices on one of these compute cards and scale up from there. We'll do next slide.
00:07:59.950 - 00:08:24.710, Speaker A: So we looked at three lessons. I want to give you three lessons. Today we talk about AI and what we've seen in hardware and compute and really three things stand out. We'll dive into those. It's not all just about the compute. So if I focus on prime field math only, what I'm missing is there's four other legs that actually compose the full cryptography workload and problem. So you can't just focus on compute.
00:08:24.710 - 00:09:11.590, Speaker A: You got to minimize hardwiring. And the idea there's a concept of dark silicon is anytime I harden something on a device, and then that's not part of the proof that I'm working on. It's actually dark silicon and its inefficiency. So we look at how to minimize hardwired features on the device and make sure it's small, modular instructions that give us the flexibility to eke as much efficiency out of our entire device as possible. And then lastly, what's important for the best hardware, you actually have to have the best software teams, and you've got to make it programmable and usable to enter in. If we have the most exotic chip and no one can program it, it goes nowhere. And so, actually, most of our company, after deploying silicon, is all about making it programmable.
00:09:11.590 - 00:09:45.946, Speaker A: Let's go. Next slide. So, lesson one, we said it's not just the compute. The idea is inside this FC 1000 chip. I mean, we've built in all five legs of this pentagram, the compute 40 VPU tiles, parallel processing engines that can go run around and do prime field math. We've built in the memory, tons of memory on chip, tons of bandwidth off chip to be able to retrieve and move things around. We've got a high bandwidth network on chip, and that allows us to exchange information from all our compute engines.
00:09:45.946 - 00:10:18.374, Speaker A: When you talk about, say, large ntts two to the 26, all of our tiles can participate in the problem and share that compute, and we can actually move data around with extreme high bandwidth. We've got our cpu eight cycle or eight core risk five, that's able to help us in other parts of the workload. And of course, we have I o. We're able to move large amounts of data in and off our device. We'll go next slide. So here's an example. We'll talk about why compute is not just the compute.
00:10:18.374 - 00:11:06.442, Speaker A: Local witness generation is actually one of these critical things that come in zero knowledge proofs. And you can see on the right hand side, it's a small footprint for a ZKP. The inputs and the function are actually quite compact. But what you go through is a witness generation step, which generates huge amounts of data, which then ripples into the proof and the compute, so small input blown up into a large piece of data set with a large amount of compute, and then you release a proof at the end. So the idea with the architecture for the VPU is we can do all of the witness generation on device. The artifact is we are not limited by our PCIe bandwidth. We're not a huge bandwidth choke point if you have a computer doing a server doing witness generation and a proof engine somewhere else.
00:11:06.442 - 00:11:23.002, Speaker A: And so again, this is one of these key things of hardware, software design. You can't just look at the compute problem, you've got to look at the full problem. Let's skip two slides. There you go. Lesson number two. Let's minimize hardwiring. And this is critical.
00:11:23.002 - 00:12:17.682, Speaker A: If you look at the top, the completely fixed function ASIC, you can compose MSM and NTT engines and hashes. But if the mix, if the change in algorithms doesn't follow that ratio, you're in big trouble because your protocol has changed and you have pieces of your silicon that are not usable. You may even have fixed elliptic curves, you may have other things that it doesn't even work for, the changes you want in your prime field. So the idea behind that is really we work on more programmability, small modular instructions instead of these complex big accelerators. And it avoids that concept of dark silicon. How do I make sure that my device is fully participating in all of the workload, regardless of how I change my protocols? We'll do next. So the VPU instruction set, the idea, we try and make it easy.
00:12:17.682 - 00:13:01.360, Speaker A: We don't break these things up into very small 32 bit integers. We're moving around large instructions in the instruction set, like modulo vector 384 bit. We can do elliptic curve math very easily. We can still switch the same exact engine and work on the Poseidon inner loops, the hashing, the starks of the world, Modulo vector, Goldilocks primes. We have all the adds and subtracts in the vector space. And then finally, the NTT butterflies are just as easy for us because we can do the add, subtract butterflies and the rotations. And so you can see breaking it down into what cryptographers want, not building all of the other things that we don't need.
00:13:01.360 - 00:13:28.854, Speaker A: Let's go. Next slide. Okay, the lesson three, let's make it programmable. And it's an well understated problem that in the AI space, many of the early companies built great accelerators that had no ability for any customer to use it. We actually spend a lot of time making it programmable. And that's the lesson of general purpose cryptography, is this is one of the critical pieces. So if you look at the tool chain, we have a number of things that we do.
00:13:28.854 - 00:14:08.286, Speaker A: So we have our kernel development kit, and that's a repo that you can actually take advantage and write kernels targeted for the VPU instructions. You can look at how you move data and how you actually run compute on all these 40 different tiles. And so that lets us take a lot of the blocks in crypto primitive protocols and build optimized versions of those kernels. We do a lot of that work ourself to make sure our architecture is in great shape. Our hardware team, software and cryptography are all working together. We're simulating, we're building these protocols, and we're co designing the hardest parts of cryptography to make sure we have the right balance. But it's not just writing kernels.
00:14:08.286 - 00:14:51.860, Speaker A: What we end up doing is assembling a whole graph development kit. And the idea behind if you go look at plunky two or plunky three, it really is assembling a lot of different things, and the graph is the best representation for that. So just like in the AI space, Tensorflow has made it easy for just about anyone to develop neural networks. The idea behind the graph development kit is we can build large workloads, recompose those workloads, and build it in an easy way that uses kernels as big atomic units of compute, and dump that into our graph compiler, and we end up with workloads. And so there's a rust C plus plus python front end. It makes it easy to develop graphs for cryptography. So do next.
00:14:51.860 - 00:15:35.082, Speaker A: So finally, we'll enter in the workload. Now, how do we think about this at scale? Well, there's two different ways you can go. On the right hand side, we can take our compiled graph and it runs on our compute card. So this is this three VPU, GPU compute card. We have the fabric runtime, interprets those graphs and can get those workloads done. So just about anybody, and a decentralized way, in a developer way, can actually load these kernels, run the graphs, and execute that on their hardware using our compute card. But what really gets exciting at scale is the exact same graph we end up in our data center, the ZX 100 chassis, and there's a cluster orchestrator.
00:15:35.082 - 00:16:03.190, Speaker A: The same runtime is able to scale up those graphs to enormous workloads. And so, prover as a market, you can build data centers around this that are executing those graphs that just about anyone could develop. And so we're super excited to see this unified software infrastructure that makes this easy to use. We'll go next slide. So again, I want to wrap that up. I mean, really, what's at stake. Fabric has been building these devices.
00:16:03.190 - 00:16:54.938, Speaker A: We're ramping up both the hardware teams, the software teams, the cryptography teams, and the operation teams to cover all of the things we need to do to release this in the field. So in 2024, what you're actually going to see from us look for fabrics, announcements, basically the equivalent of a billion dollars of GPU compute is going to be deployed with our VPU servers. And so when we look at those advantages of how much better we're going to be at cryptography, it's really just, it's night and day how much your capital dollars for scaling are going to go. We're going to have the SDK releases. We've got the compiler, the SDK, and these kernel development modules that will make sure that this is available for many people to use and program. And then finally the fabric VPU cloud. I mean, we're going to have resources so that you can actually deploy proof as a market.
00:16:54.938 - 00:17:55.270, Speaker A: So building these data centers, building the orchestrators that these graph workloads can be run, is super important to the odyssey of our company. Obviously, what is interesting from the engineering point of view, running all these teams, it's a long term game for us. And so what is so fascinating about this market and how we see this growing is learning, is one of the key things that we need to do. So deployment feedback from our sdks people finding different ways to optimize kernels and using our hardware is really interesting to us. We're going to roll that into our second generation, and we're going to continue to improve the workloads so we can take advantage of technology, we can take advantage of better architecture. And as we grow the company, do the first gen deployments and get better and better at this. And the idea now we really want the hardware, software, crypto codesign is one of the big reasons we're here, reaching out to see how you guys will use these devices.
00:17:55.270 - 00:18:18.160, Speaker A: And I really want you to think of this blank canvas. If I had this compute, and I'm removing the compute restrictions that you have today, what kind of applications, what kind of massive workloads are you going to be able to build and envision? And that's what we really want to bring to you with the VPU. So with that, I really want to thank you for your time, and we'll move on from there. Thank you so much.
