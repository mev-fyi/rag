00:00:00.330 - 00:00:44.860, Speaker A: Next up, please welcome Brian Retford, CEO of Risk Zero, for his presentation 100 x scaling victories, challenges, and opportunities in general. Purpose ZK. All right, hello, everyone. Today I'm going to be talking about, I'm calling it ZK 100 X. But it's really just kind of a reflection on the progress that zero knowledge has made over the course of the past year, two years. And I think some opportunities that are available in the future to ZK as it scales up. And I think we've seen some of the other speakers talk to that as well.
00:00:44.860 - 00:01:46.130, Speaker A: Okay, so really quickly, how many people here feel like they understand zero knowledge cryptography? Raise your hand a little bit. Okay. I'm trying to decide at what point I stop explaining ZK to people in talks, but it's kind of important. So, modern ZK systems have these kind of four properties, at least as I define them. They're trustlessly verifiable, they're information limiting, they're irreversibly concise, and they're composable. So when we say that ZK is trustlessly verifiable, what we really mean is that there is this program that's actually a circuit. It looks more like, structurally, it looks more like an actual bit of silicon than a program that you might think of in python, but it's encoded using these mathematical ZK proving schemes.
00:01:46.130 - 00:03:02.160, Speaker A: But the key, I would say one of the most useful properties of ZK, especially in the blockchain context, is that this circuit win run may have some inputs, it may have an output. Anyone can verify that that circuit was run correctly and only rely on the cryptography. You don't need to rely on anyone else to tell you that this was done correctly. So the zero knowledge part of the zero knowledge name comes from this property that when you're talking about this circuit, it has inputs, it has outputs, and if it's a vm, it has a program. And you, as the prover, can choose to reveal none of the inputs or the program or the outputs, some of them, part of them or all of them. And people are able to verify that you still ran the program correctly without revealing any of the information that the prover did not want to reveal. So Zk is irreversibly reduced.
00:03:02.160 - 00:03:49.970, Speaker A: Some people call this succinct or concise. And generally what this means is it might take, maybe you compute PI to 7 billion digits, and it takes two years. The amount of compute traces that would go on would fill terabytes of data. But the size of the proof is still either constant or some exponential logarithmic reduction in the amount of the size, basically, and complexity required to verify these things. So modern ZK systems also are recursive and composable. And people use different words for all of these. They mean slightly different things to us.
00:03:49.970 - 00:05:00.804, Speaker A: The most interesting, and this is a bit hard to wrap your head around property about this, but it's very relevant for scaling, is that you can have a circuit that proves, say, one ethereum block, and you could prove three different ethereum blocks, and then you could pass all of those proofs to a different circuit that verifies those proofs. And in so verifying those proofs, it proves that all of those proofs were valid and produces a new, smaller proof. So this property allows you to take an arbitrary number of these already succinct proofs and keep making them no bigger over time. So you can imagine from a blockchain perspective, especially, how useful these primitives are for scaling sharing of what actually happened. So I'm the CEO at RISC zero. We're one of the pioneers in general purpose ZK computing. So when we think of ZK, we don't usually think of writing circuits, which is more of like an EE skill than a CS skill.
00:05:00.804 - 00:05:50.440, Speaker A: Instead, we have written a circuit that is a computer, so one of its inputs is just normal computer code. So in this example, on the side, you can see we take rust, turn it into RISC five, and then this ZkVM executes the RISC five and produces a proof that it was executed correctly. And again, because it's ZK, the prover can choose to reveal any of the inputs. It doesn't even need to reveal what the program is. It just attach and you can still get a proof. And there are all kinds of applications for things like that. So one thing about ZK is that it's very slow.
00:05:50.440 - 00:07:20.116, Speaker A: I think the PCP algorithm and some of the first algorithms were actually, as far as anyone could tell, impossible to actually create a peruf because the amount of time involved was more than the number of atoms in the universe. So we've come a long way. I think Wei Dai was talking about this notion of kappa, which is just sort of the cost per proof, per power, something, I'm not sure exactly how he is defining it, but if you look at the progression over the past, I don't know, seven years, and especially over the past three years, we see ourselves going from impossible to six orders of magnitude, five orders of magnitude more overhead to do the same computation in a zero knowledge context. And we are starting to see some systems for specialized problem domains get closer to 100 x slower or more overhead. And then with the advent of technology potentially like fabrics, you might see the cost overhead actually drive down closer towards ten or even one. It will always be slower, fundamentally, or very likely. Okay, so as pioneers in the general purpose ZKVM space, let's see.
00:07:20.116 - 00:07:52.832, Speaker A: Can I go back? Oh, I can. Okay. So one of the advantages of doing this, and I think for a long time, people didn't really believe this was a sound way to do ZK, because there is some overhead which can be addressed. But the most important thing about this is that you don't have to write all the code yourself. You can use other people's code. So as part of some work with op, I'll talk about in a second. We built the first.
00:07:52.832 - 00:08:45.596, Speaker A: I call it type zero ZKVM. It's actually just a type one ZKE EVM. But as far as we're aware, this was the first ZK proof of an actual ethereum block that was ever created. And we created it using it. Sorry, we created it using this general purpose ZK paradigm, because it allows us to just reuse about 95% of the rest in our UVM ecosystem without needing to reproduce it ourselves. So the sort of capital outlay and time to market goes dramatically down at an increased cost. This is not competitive with Zksync or polygon or anything like that yet, but you're going to start to see more movement in that direction.
00:08:45.596 - 00:09:43.776, Speaker A: And so I think this is a good example of proof systems getting to this sort of five orders of magnitude more effective. And some of these other scaling techniques we'll talk about that actually enables a lot more experimentation and freedom in the Ethereum ecosystem. So we actually did Zeth well, because I said we would do it two years ago in Paris, but also as part of this optimism mission that we've been on. And if anybody's familiar with the op stack, it's a lot more complex than straight up Ethereum. And they've been working on actually releasing their fraud proof mechanism. So they had a grant to figure out how to apply ZK to op, which we applied for. And we've actually pretty much finished this mission about six months early.
00:09:43.776 - 00:10:43.216, Speaker A: And we've actually proven, okay, I'll say an entire optimism epoch. Technically, it's only a 10th of an epoch, but what that looks like is over 180 op blocks proven together, rolled up into a single proof. And each of those blocks actually have to examine any number of other Ethereum blocks. So you're really talking about one computation proving this epoch that spans 180, that spawns 180 new computations in turn, which each spawn maybe even 50 other ZK computations. So when we talk about horizontal scaling versus vertical scaling, this is a way to sort of horizontally scale out the ability to solve a very complex ZK problem. And this, I'm very proud of this. I will say that we were not the first people to do this.
00:10:43.216 - 00:11:51.360, Speaker A: I don't think we might be the first people to do this in a ZKVM. I believe that somebody at nil may have gotten doom working in ZK. But yeah, we just released this. You can check out the source code and it actually renders all of doom in ZK. And what this lets you do is you can basically run a game, prove that all the rules were followed, reveal nothing about the intermediate gameplay, and simply show the score at the end, and prove that you were able to get that score without violating the logic inside the game. Now, is there any direct applications of this other than just speed running? Maybe not that plain, but I think these kinds of dynamics can be used in a lot of games, and probably DFI as well. This is some really cool research, and I think this is another area where when we're thinking about 100 x and what 100 xing the capabilities of a ZK system makes possible.
00:11:51.360 - 00:12:54.020, Speaker A: It's this idea of ZK fully homomorphic encryption. So fhe is this sort of holy grail of encryption, because parties can compute on the data without being able to read it. So especially when you're thinking of building unstoppable, decentralized network of computers. Boy, it would be really nice if you could have private data, actually have that private data be used when you want it to be used, and still not necessarily reveal anything about it. So by combining ZK and fhe, there's actually an opportunity to reveal attenuated information and have it stored privately in blockchains. So we did some research with some venture partners and the people at Zama, their name should be on this slide, but it's not. They are pioneers in, I think, the most promising fhe regime right now.
00:12:54.020 - 00:13:52.440, Speaker A: And what we were able to prove is like a key component of the TFHE algorithm called bootstrapping. And it's the thing that you need to prove was done correctly in order to ensure that the information doesn't get leaked out as you continue to compute on it. I think this took a billion cycles, which is like 30 minutes or something, to just do this one bootstrapping so I think it's about 100 times too slow than it needs to be to be kind of basically useful. But even once it's kind of basically useful, I think there's a lot that it has to offer to the blockchain system. So there's a lot that can be done to get there. Programming custom accelerators, I mean accelerators in the circuit sense like Validia is doing, and our next version will enable. But there's also great opportunities for programmable hardware as well.
00:13:52.440 - 00:14:56.468, Speaker A: I really think that Zkfhe is going to be a huge part of how everybody's personal data is stored in the 510 year time frame. So yeah, one little pitch for our ZkVM. If you are interested in figuring out what it's possible to do with ZK, in addition to these sort of big examples that we have, we have a lot of examples just in our source code repository. You can check them out and yeah, hopefully get inspired about things you might do with CK. Excuse me, I want to talk a little bit. We talked about recursion and composition earlier. So this sort of zeth program that I think nobody thought was realistic until we did it.
00:14:56.468 - 00:16:04.250, Speaker A: The way we figured out how to do this is by taking one ZK proof and then doing some kind of memory magic to split it up into thousands of proofs and then simply use recursion to combine them into one proof. So this has basically let us scale like a single node performance is about 100 khz, and right now we're at about 5 parallel cluster. But that's because there's some bottlenecks that we'll remove. Fundamentally, if you can solve some of the bottlenecks in these proving systems, there's no reason you couldn't use parallel proving to get basically full rate ZK computing. It might be expensive, but it should be possible. So the kind of subtle difference between recursion and composition in the recursive context, like our system, magically figures all of this out for you. You don't have to worry about how many segments to split your program into.
00:16:04.250 - 00:16:49.450, Speaker A: Risero will just figure this out and it will execute this one program. However, for cases like optimism or many other examples, you often have many work streams. You could even think ZK Mapreduce effectively a bunch of independent work streams. And by fanning things out to as many jobs as possible, effectively multithreading your ZK systems. This allows you to remove some of the serial bottlenecks that are unavoidable at the start, such as sort of witness generation is parallelizable. But simulation is not. And I think the fabrip people were also getting at the utility of that.
00:16:49.450 - 00:17:45.064, Speaker A: So by combining these things you're able to build these incredibly complex ZK systems in. Yeah, I think the op ref work took us two people for about three months full time to really do something that I don't think anybody thought was possible. And then again we iterate that again to make these epochs don't have a ton of time and these kind of trade offs and limitations are pretty wonky. If you're really into proof systems, I'm happy to talk about them. But yeah, these are areas where proof systems getting better at these things are going to increase the set of everything we can do faster. Yeah, even more details here. There's a ton of cool research coming out.
00:17:45.064 - 00:18:23.168, Speaker A: In terms of zkvms, we're certainly not the only one. There's lots of copies popping up and there's also lots of really amazing novel research like the stuff mine's doing and a 16 z is doing. So yeah, that's pretty much all I have. Any questions? Oh, I guess call to action. We are going to be launching like right now. You can use reservoir to generate very small gross 16 proofs. You can post them to testnet, but we will be launching to mainnet Q two, hopefully sooner.
00:18:23.168 - 00:19:39.014, Speaker A: But this will actually let you build real programs, lock real TvL up in your risero systems. Yeah. Is there any mic runner or just yell? I can repeat the question too avi. The question is what is the open research problem that keeps me up at? I honestly, there's so much work to do right now that I'm not even sure that there is one. I feel like the path forward is obvious in so many respects for us. I think I'm personally curious about the information theoretical lower bounds on overhead for a ZK system. Like how much extra work do you need to do to actually maintain those four properties? And I guess the other interesting research problem that I haven't seen a lot of traction on is the ability to try to massively speed up proofs at the cost of security.
00:19:39.014 - 00:20:27.490, Speaker A: Those things are probably related, but I think there are use cases for low, not very secure proofs that are only valid for 5 seconds. Yeah, just curious on your view on the next 100 x for ZKP, is it going to be hardware research or things like risk zero as continuations that allow parallelization? Yeah, I think it's going to be all four things. It's going to be new research methods, new fields. You see binius, you see circle Starks, see M 31. It's going to be just raw engineering. I think we probably have ten x of just. We're in a hurry.
00:20:27.490 - 00:21:11.276, Speaker A: And. Yeah, hardware, I hope, plays an increasing role, but I think there's 100 x easy to go without needing asics. But then once you get that and an ASIC, we'll really be going. The question is, do I think proof standardization will ever occur? I hope so. I think there's directions sort of towards that right now. Some proof systems are more flexible than others. Like plonky.
00:21:11.276 - 00:21:28.060, Speaker A: Three is really configurable, so I do hope we'll get there, but it's very early. And then once we have a proof standard, there will be three or four. All right. I think out of time. All.
