00:00:00.570 - 00:00:22.590, Speaker A: Next up, we delve into the economics of zero knowledge proofs. For this panel, please welcome back to the stage Avi Zurlo from Nil Foundation, Umaroy from Succinct Labs, RJ Catalan from yet another company, Nick Matthew. Moderating this discussion is Ray Xiao, senior director at IOSG Ventures. Please welcome them to the stage.
00:00:26.810 - 00:00:45.170, Speaker B: Hello. Yeah, it's so nice that we can have discussion about economics of zkps. I'm ray from iOS revenues. We are crypto VC. First of all, before the discussion today, we'd love to have a brief intro about yourself and what your focus recently.
00:00:47.430 - 00:01:10.970, Speaker C: I'm Uma. I'm one of the co founders of Succinct. We're broadly focused on making tooling to make it easy for any developer to build with zero knowledge proofs. In particular, recently we announced our plans for a decentralized prover network and also launched an open source ZkVM called SP one that has state of the art performance and is really useful for a variety of use cases.
00:01:12.270 - 00:01:47.800, Speaker A: Hey everyone, I'm Avi. I'm chief product officer at Nil foundation. Yeah, Nil foundation is a team of primarily engineers and researchers who's been building since 2018. We have a crypto three library, which is c plus plus cryptography library. We have a proof market for sourcing proofs. We have a ZKlvM, which is a circuit compiler. And then our focus for this year is an L two, which we've coined Nil and yeah.
00:01:50.090 - 00:02:04.650, Speaker D: Hi everyone, my name is RJ. I'm the founder of eternal company. We are building a product called Line Layer. It's a decentralized network verifiers and we're using eigen layer to make verification cheap and also more flexible.
00:02:05.630 - 00:02:18.980, Speaker E: Hey everybody. Hey, I'm Nick. I'm an investor at standard Crypto. We're a crypto native venture fund that's really interested in ZK.
00:02:20.440 - 00:03:05.780, Speaker B: Okay, thank you guys. I prepared some questions, but I'm not sure I can cover often. But I'll try to make it the first question to try to give that audience to have a more recap. Could you give us a little bit more background compare with two, three years gold? Do you have any onchain or change in terms of the visions? We know that a lot of progression in both academic side and product side compared with two, three years ago. I think you also have a lot of change in different focuses. You have new products and could you give more introductions about this site, both like product size and the research size in different focuses?
00:03:08.500 - 00:04:05.072, Speaker C: Yeah, our most recent product that we launched was this ZkVM SP one. The reason we built it is I think we had been building at our company a bunch of ZK applications for a long time, including ZK bridging, ZK Oracles and things like that. And I think as ZK developers ourselves, we kind of realized that the ZK development experience today is really horrible. You have to write all these custom circuits and all these custom frameworks and it requires a lot of cryptographic expertise and is just really arduous. A ZKVM lets people basically write normal programming languages like Rust and then get a proof. And I think that is really the future of ZK and the thing that will let it go super mainstream, because anyone now can use zero knowledge proofs in their application, whether it be a bridge or roll up or things like that. And I think that is one of the biggest challenges of the space, is the developer accessibility and ease of use.
00:04:05.072 - 00:04:09.990, Speaker C: And so that's why we decided to focus on that more recently, because I think it's one of the most important problems.
00:04:11.800 - 00:05:25.230, Speaker A: Yeah, I think similar to succinct, we've spent a lot of time building a variety of applications and very low level tooling like a cryptography suite, to more abstracted, easier to use tooling like the ZKLVM. I think the insight over the last few years is that research and engineering around zero knowledge proofs has made leaps and bounds to the point where at Nil foundation we noticed that maybe we could apply sort of the state of the art to solve old problems. And this is where sort of ZK sharding came to be. Where sharding has had a long standing problem of data validity. But with these state of the art proof systems that are really pushing the extremes of what you can do in theory, that will then be implemented in production ready systems over the next few years. It's a great time to be revisiting these old problems that we didn't have the tools to actually solve for at nil. That's been a big change for us.
00:05:27.280 - 00:06:23.820, Speaker D: Yeah, we share more or less the same vision. One of our goals is to make CK more accessible. And we know already that a lot of people are working on the proverb side and working on new ckbms, and at some point developer UX and accessibility will be solved probably by many teams. And what we figure out is, okay, if we know CK is moving way faster than we expected, and things are advancing and actually getting way better, what is going to be the next bottleneck for CK in order to be accessible, in particular to Ethereum, because we believe people want to build an Ethereum. So the question we ask ourselves is, okay, what kind of infrastructure can we actually build in Ethereum that will help people use more CK? Because we already know that CK is already here, it's quite cheap and it's getting way better. So in our opinion, the main problem is the verification. And the verifier in itself is Ethereum.
00:06:23.820 - 00:07:07.100, Speaker D: So what we're doing is removing verification off chain, but we're trying to use crypto economic guarantees to make that possible. And yeah, we have the same thesis around zero knowledge and we believe this is the last mile in order to make ck way more accessible because we will lower costs and by lowering costs as well, we'll allow people to use the latest cK tech made from new foundation or Saxene labs. We're actually including the Saxene Labs VM in our mvp near soon, and we just want more people to use the latest. We don't want Ethereum to be the constraint. So taking into account all this innovation that has been happening, we are adding this missing piece of the puzzle in order to make people use CK and Ethereum.
00:07:08.240 - 00:07:27.990, Speaker E: And we're not building anything in ZK ourselves, but we talk to a lot of the teams that do. And one of the things that's been most exciting for us is as the ZK cryptography on the academic side has gotten better and better. As everyone has been saying here, there's been more and more exciting things to build in Zk. And so we're excited to see that continue.
00:07:29.000 - 00:08:18.260, Speaker B: Yeah. So back to our topics today, the economics of zkps. I think Nick also published an interesting article about the whole cycle of the zkps and succeed labs also building zkvms and near foundations, like doing some zkevms and align layer, doing some verifications layer. So we have different process like from proof generation, proof marketplace and verification, this kind of stuff. And which part you think for your current focus, which part you think could be accrued most of the value when related to Ethereum based layer in terms of applications or sequencers on and other things you think capture the most of the value in the whole lifecycle?
00:08:20.200 - 00:09:08.944, Speaker C: Maybe this isn't a part that will necessarily capture the most value, although I do think it'll create a lot of value and so hopefully it can capture some of it. But I think aggregation is really exciting because as I think aligned layer is working on today. If you want to verify a ZK proof on Ethereum it's between 200 to 300k gas. One really magical thing about ZK is that you can recursively verify a bunch of proofs in another proof. This is called proof aggregation, and then you can take that aggregated proof and verify that on chain. So basically for 200 or 300k gas you can verify arbitrarily many proofs and arbitrarily large amounts of compute. So that is like a clear benefit to the end ZK developer where they can just directly reduce their on chain costs.
00:09:08.944 - 00:09:42.690, Speaker C: And I think aggregation is like a really exciting area where it can make a lot more ZK applications feasible. So today, for example, your proof needs to generate ten dollars to twenty dollars or whatever the corresponding dollar amount of 200k gas is on Ethereum to make it worth it. But in the future, hopefully in the limit with aggregation, the cost of verifying a proof on chain will basically go to zero once you amortize it across all the aggregated proofs. And so I think the aggregator here is providing a lot of value and so obviously I think they can hopefully charge for some of that value as well.
00:09:45.140 - 00:10:57.400, Speaker A: Yeah, I definitely agree with Uma said here, and I think it's difficult to say where value lies in the stack. And ultimately it's up to ZK application developers to decide on what kind of trade offs they want to make. As Muma had described, these aggregational layers have this really magical property that allows you to combine many proofs into one and effectively amorturize your verification costs on chain. But the trade off is latency, right? And so for ZK application developers there's this balance that you have to strike, which is what is your latency for hard finale on Ethereum, where verification is very expensive? And what is your price sensitivity for that verification? I don't think we've developed the ZK application space enough because I think we're all sitting on this panel and we're all building infrastructure, right? And so over the coming years I think we'll see some of these things start to solidify and application developers express their opinions.
00:10:59.660 - 00:11:42.272, Speaker D: Yeah, I tend to agree. I think it's too early to think about exactly where the value capture is going to be. We have been thinking about this lately. We believe maybe something like a line layer has a lot of volume, can have MEB because we decide the order of how we post the results of certain proofs on chain. And that could have a relationship with the finality Sabi has described. But we think aggregation is really interesting and useful, but we believe it's not going to solve most of the problems that we have today. So aggregation is really interesting when you want to aggregate the same type of proofs.
00:11:42.272 - 00:13:01.950, Speaker D: The main problem is we don't know exactly which proving system people are going to use in the next five years. So we believe instead of using aggregation, by using kind of this optimistic verification oracle that we're creating, you gain in the short term like way more benefits by lowering the cost of verification and also allowing proving systems that are new, for example, new ckbms, to actually go into production even if they don't have the full infrastructure to have aggregation because of volume. Because if you want to aggregate, as Abby described, you have latency issues, and their latency issues are correlated with amount of throughput you have. Right? So with something as our infrastructure, people can aggregate, but it's not our decision, it will be the developer's decision, and people can choose the trade off between latency and cost by their own. So we believe aggregation is something useful, but it's not going to solve 80% of the problem, because if you want to move into a world with many different CK tech stacks, it will be a really hard problem to solve. That said, we want to aggregate the same type of proof long term, but we believe we are not going to do that in the next couple of months.
00:13:04.000 - 00:14:02.544, Speaker E: Yeah. In terms of market structure, the proof supply chain has a very similar structure to the transaction supply chain. And in the transaction supply chain, the sort of value capture there is mev. And so I think who sort of captures value in the proof supply chain is going to be quite analogous to who captures value in the transaction supply chain, which once we've built out all the necessary infrastructure, is for the most part probably going to be the front ends that sort of control the order flow or get the order flow directly from the user, because they're the ones who have the most sort of inherent leverage in the system. And so similarly, I think whatever entity in the proof supply chain has the direct relationship with the application or user that's submitting the request to generate the proof, I think they have the most leverage. And so once all the infrastructure is built out, there's probably the most value capture there. That being said, there's a lot of sort of path dependency in terms of who actually ends up getting that proof flow and how different entities can monetize from that.
00:14:02.544 - 00:14:07.380, Speaker E: So it's still very much TBD, but that would be sort of like, my guess, in equilibrium.
00:14:09.080 - 00:14:34.750, Speaker B: Yeah. Regarding mv values captured, how do you think of addressing Jace, the proposal of base row ops. I know you guys, some of you guys right now you are doing roll ups. I'm not sure. I think it's pretty new for you guys. We love to hear. What's your thoughts? Could you share us a little bit ideas? Would you consider it or not?
00:14:36.800 - 00:15:37.090, Speaker C: Yeah, I think for base roll ups, the value capture layer for the roll up right now at least is like sequencer fees and then also Meb for the sequencing of the transactions. And I think a lot of roll ups anticipate that being the biggest source of revenue in the future, especially as the margin on the sequencer fees perhaps goes down a lot. For base roll ups, you're basically delegating sequencing to the l one block proposer. And so I think the reason Justin and other people who are very ethereum aligned are a fan of this is because it takes the roll up sequencing, mev value capture, and directly gives it back to l one proposers, which is good for ETh and Ethereum. I'm not sure whether the roll up teams would like this because maybe they want to keep some of the value capture for themselves. But yeah, that's how I think about kind of base roll ups and the incentives behind.
00:15:39.940 - 00:17:00.360, Speaker A: And I agree with UMA to elaborate a bit more on the economic relationship between Ethereum and zero knowledge proof infrastructure, I share a similar vision with many others of proof singularity check. Okay. And in that we is it cutting out or okay, test. We're back. Yeah, so we have this universe of proofs, machines that are generating proofs, and at the base layer we have the arbiter of truth, who is Ethereum. And you want a very censorship resistant, operationally resilient, decentralized base layer to sort of be the canonical truth of verification. And in between this constellation of machines that are generating proofs, and Ethereum, you have these really interesting layers that RJ and folks are building to actually aggregate them.
00:17:00.360 - 00:17:26.880, Speaker A: And again, I come back to my original comment where it's not quite clear how value flows through that system and where it lands. But one thing is absolutely certain, that Ethereum plays a very critical role in that future, and it's going to be up to application developers to decide on how that all shapes out.
00:17:35.480 - 00:18:41.416, Speaker D: I tend to agree. I think we'll see. The thing I don't agree is trying to find one type of architecture for everything, because I don't think the tech is too ready. And also we haven't figured out savvy described when exactly do we actually capture the value? And many developers will choose different type of systems and where they want to actually do Meb if they want or if they don't. There are some great teams working in using CK for example, for me for roll ups like radius. And in general, what we do believe is that we need more kind of this openness of deciding okay, it will be up to the applications even more than the developers or the business logic of designing which is the best architecture for the rollup or for the users as well. And we're excited to see more experimentations, not only merry with one single architecture to rule them all in roll ups, or even in CK in our opinion as well.
00:18:41.416 - 00:18:52.860, Speaker D: But yeah, we're more pragmatic in the sense that out of people we experiment. We have to build infrastructure that can help. People are experimenting and can be long term thinkers.
00:18:53.760 - 00:19:19.670, Speaker E: Yeah, I don't have much to add on booster rolls beyond what everybody else said, but I think it's good to see more experimentation with different architectures for roll ups. And just given that they're in such an early stage of research, I don't know if we have a queer sense of what their sort of terminal economics are going to look like, but it's certainly an experiment worth running and we'll see how infrastructure teams and their application customers end up viewing it.
00:19:20.920 - 00:20:07.590, Speaker B: Thank you regarding with proof aggregations, I know Lambda published some research and Anir also linked some research about aggregate some firmware a and firmware b together try to compress the ZK and RJ have different solutions inherent economic security from Ethereum and lets the verification happen off chain. And we'd love to note your ideas. The proof aggregations when it become more and more mature, do you think you would make fundamental change to your technical architecture? Do you need to upgrade a lot based on this?
00:20:08.920 - 00:21:23.230, Speaker C: Yeah, I think the end game of proof aggregation just really depends on how proof systems play out in the next few years. I think right now a lot of different teams use a lot of different proof systems, but maybe that won't be so true in a few years. Maybe there will be some settling towards a few canonical proof systems. Just from a security perspective, I think it's a lot easier to deal with that audit surface, and especially if zkvms take off and everyone's using zkvms, maybe it ends up being one or a few zkvms that everyone uses that are fully open source and kind of like a Linux community project in that world, if the proof systems that are being used are pretty homogeneous, I think aggregation looks very different than if there's many, many different kinds of proof systems. You could imagine, for example, that the final aggregation layer looks something like all these different proof systems get aggregated into one vm because you write your verifier and rust and then that gets put on chain. But you could imagine another world in which everyone does their own wrapping to something like route 16 and then all those proofs get aggregated. I think it's pretty early to tell how it'll play out.
00:21:23.230 - 00:21:26.510, Speaker C: So yeah, I think we'll have to see.
00:21:28.580 - 00:22:30.240, Speaker A: Yeah, I'm going to give this mic a go again. I think proof aggregation, it already plays like a really important role in, I think, pretty much every large scale ZK application today. Right. Roll ups already do sort of an internalized vertical proof aggregation in batching many proofs together in order to amortize costs across transaction batches. I'm quite sure that worldcoin is also doing something similar, so that's going to persist. Right. I see no reason why we would go backwards where I think things get really interesting and at nil, where we're certainly excited to contribute towards is sort of potentially heterogeneous aggregation across different proof systems.
00:22:30.240 - 00:22:54.810, Speaker A: And here there's incredible economies of scale to take a proof of one proof system from one ZK application and combine it with another. And yeah, I think frankly whoever builds that is probably going to make quite a bit of money.
00:22:59.490 - 00:24:01.718, Speaker D: Yeah, I tend to agree, but at the same point our thesis is that we don't exactly know what will happen. So I don't know if ckbms. I think ckbms are awesome and we should use them more. But I'm not exactly sure if they will have only one of them or we'll have many of them. And because the future is uncertain, our philosophy is okay, if we don't know exactly how CK will look like in the next five years, how can we build something that can help accelerate CK being used today? And also we are talking about economics, right? So incentives are important. And I think incentives in order to keep building your own proving system for some systems is going to be really high. Like why a CK rollup will abandon completely their proving system if there's a new CKBM? I think there are many teams trying to build great tech and the incentives to be the winner will always exist.
00:24:01.718 - 00:24:56.930, Speaker D: So I don't see that disappearing the next five years. So taking that into account, I think we will still have many proof systems, therefore we will not be able to aggregate them altogether and it will be up to the user, I think, how they want to use aggregation or not, because as Abby described very well before, latency is something applications have to take care of. If you're a bridge, maybe you want to have low latency, or if you're a roll up maybe say okay, I want to have less cost so I create more. Latency is not a big deal. So I think that decision should be up to the application. So aggregation makes a lot of constraints to the developers. That's the way we see it and that's why we're building something that is not married with only one proving system or one idea or framework for the developer.
00:24:58.550 - 00:25:49.986, Speaker E: I don't have a strong view as to what proving scheme or schemes are going to be involved with proof aggregators, but one interesting question I think to consider is I think vital gave a talk at ECC last year where he painted this vision of the proof singularity where an ethereum block would just be a bunch of proofs that sort of got recursively aggregated, or maybe just a bunch of proofs included in a block. And now people are starting to build these sort of like out of protocol proof aggregators that then just submit the one proof on chain. And so I think it'll be interesting to consider whether or if at some point proof aggregators or a proof aggregator gets enshrined into the protocol at some point. Yeah, so that'll be something interesting to explore in the future. I had another point that I forgot, yeah.
00:25:50.088 - 00:26:43.880, Speaker B: About this topic in future. Definitely. We have so many proven system proven schemes and like I just said, Vitalik also shares some views that we may have risk that if we rely on certain cryptographic system. And do you think in future for the target audience, for the users, do they have different requirements when they have so many program system to choose and do we need to do some customizations plan for the users? And about the security side, do you think is it easy to auditing those codes? Is it easy to be more automated than auditing a smart contract like running by solid code?
00:26:45.450 - 00:27:45.538, Speaker C: Yeah, I think, I actually believe that there won't be that many different proof systems in the future. Hopefully there will be like one or a few for zkbms, maybe a few for machine learning or really specialized use cases that get a lot of benefits from having customization. But I think otherwise the auditing and security story gets very tricky even at succinct. We kind of encountered this where we've built and developed, I think at this point with like four or five different proof systems and it feels a little unsustainable because basically for every new proof system you have to audit the whole stack. Like you have to audit even the foundational cryptographic primitives that are being used all the way up to how you express circuits. And I think that just becomes very difficult. So I am definitely biased, but I think there's kind of a future where everyone writes their application in something like a normal programming language, and there the audit surface is very minimal.
00:27:45.538 - 00:28:38.490, Speaker C: You use a normal compiler that everyone already uses today that has been well audited, battle tested. And then there becomes this kind of like hopefully super well audited canonical proof system that can generate proofs of bytecode that is pretty standardized. I don't think there'll be just one, maybe there will be a couple, but I don't think it looks like in the limit having hundreds and hundreds of proof systems because I don't think it's very secure, I don't think it's maintainable, it's not auditable. And there's like a lot of real trade offs that come with that. So if it's possible to have maybe a couple proof systems that have the same performance as the more customized ones, which I think especially with lookup centric architectures do become really possible, that is like a much more ideal version of the future for kind of everyone involved in the supply chain.
00:28:40.430 - 00:29:30.140, Speaker A: Yeah, I agree. I think we end up consolidating to a few proof systems, namely because it's just totally unsustainable to keep up with security practices for hundreds of proof systems. I also think that developer tooling is progressing really fast in such a way that it's incredibly easy to build a ZK application without ever having to touch the back end cryptography. Right. And I see no reason why these tools get any worse. I think they're going to get much better. And that actually sort of puts the onus on the infrastructure team, the developer team who's building out those developer tools to make decisions about what type of proof system they're using about security.
00:29:30.140 - 00:29:57.540, Speaker A: And coming back to UmA's point here, I think the only practical sort of end state is that we agree that there are a few proof systems that work well for certain types of computations, and these developer tooling infrastructure teams kind of join efforts to ensuring we have incredibly high security standards for those.
00:30:00.380 - 00:30:51.496, Speaker D: Yeah, I 100% agree. I think we will see, it will be a small subset, but we will have, I don't know, maybe 1015 kind of proven systems like long term. In the meantime we will have many. And yeah, I hope to see more people using ckvms instead of writing their own circuits. And something that we didn't touch, that I think is important because I agree 100% with everything I said, is that this will lower the cost for building in CK, not only for actually operating it, but actually going into production. So you will be able to go much faster into production, and it will be easier to build a team because today is quite expensive. And that's also a constraint that is not a technical constraint, it's an organizational constraint that is not allowing us to use more CK, because a lot of great teams, they don't use CK because they are afraid of using it.
00:30:51.496 - 00:31:19.650, Speaker D: And they also don't know that CK is already useful. So we are really excited to lower the barrier of entry, not only by lowering the verification cost, but allowing people to use more ckvms or systems that they currently don't use because of the trade off they have to make in order to work with Ethereum. So we're really excited to see way more SIG applications, and we think that the tooling will mean it's already getting way better.
00:31:20.980 - 00:32:13.684, Speaker E: I'm going to say the same thing as UmA and AVi, just in a slightly different way. You can think of all these proving schemes as sitting within some trade off space, and some of those proving schemes are in very different spots in that trade off space. So starks and Grot 16. And so maybe there's viable reasons to use one instead of the other in some context, because they're just so different. And then there are others that are marginally different on some axes, but aren't dramatically better than some other for some given context. And so you'll probably see shelling points around three or four. And then even if there's some other proving scheme that's slightly better for some narrow use case, people would just gravitate towards wherever the shelling point is, because that has the better infrastructure and tooling and security and so on.
00:32:13.684 - 00:32:16.310, Speaker E: So yeah, that's how I think it's going to play out.
00:32:17.640 - 00:32:58.240, Speaker B: I think we have already talked about economic incentive like MEV events, but we didn't discuss much about the cost. I assume that many of the audience also have the similar question marks as I have, is that where are we at when we consider about cost of the estimated cost of the proof generation, proof verifications, and where we are now, and which of these are the main buttonnecks for us and what we can expect in the next, let's say two, three years to improve?
00:32:59.780 - 00:33:40.468, Speaker C: I think already today the costs of ZK are actually cheap enough to be very practical and viable in a variety of contexts. As like a really concrete example, recently the Polygon team released their type one ZKE EVM and the cost to generate a proof per transaction was maybe around 0.2 to 0.3 cents per transaction already. Roll ups pay at least ten, sometimes twenty cents to ethereum for data availability. So fractions of a penny is really completely trivial compared to that. And so I think already ZK, even for a very complicated use case like ZKE EVM, is extremely cost effective.
00:33:40.468 - 00:34:24.460, Speaker C: And I think this is kind of like a myth that ZK is too expensive. I think that's actually not very true. If you look at some of the more recent stuff that's coming out, even with our VM SP one, we implemented SP one ref which implemented a ZKE EVm in like 1000 lines of rust and it's very reusable. It uses ref alloy revim, all these primitives, so it's super auditable. And the transaction costs for that are around one cent a transaction and also rapidly decreasing. So even for example, something that's general purpose and not handwritten like the polygon Zkevm, is still quite cost competitive, especially when you put it in the context of what we're already paying Ethereum for. Da.
00:34:24.460 - 00:35:00.052, Speaker C: So I think the zke adoption problem is a lot around education that actually, hey look, the costs aren't that bad. You can actually plug this in today. Also making the developer experience really seamless. So not making it so people have to run their own proving infrastructure. And I think that's where things like proverb marketplaces are really useful, similar to what us and nil are building. But yeah, in terms of cost, I actually think it's kind of a myth and it's kind of all of our jobs to make it clear to people that hey, every roll up should be a ZK roll up, every bridge should be a ZK bridge. Every oracle should be a ZK oracle.
00:35:00.052 - 00:35:12.910, Speaker C: There's no excuse to not have the best tech. Why not use the best tech? Why not have the gold standard? Instead of all this committee based, optimistic, whatever stuff, let's just use the best tac because it's already cheap enough now.
00:35:14.800 - 00:35:53.720, Speaker A: Yeah, I agree. Again, we're going to have to find some things to disagree on. I think to give you kind of an example or an insight into where we're at in the market, at least for proof generation costs. The demand for proof generation is pretty inelastic. And again it comes back to what are the ZK applications that are consuming proof generation today. And you pretty much have ZK roll ups and then you have attestation protocols. There's a long tail, but these are sort of the primary drivers of proof generation demand.
00:35:53.720 - 00:36:51.740, Speaker A: And if you look at attestations, attestations are pretty cheap to generate proofs for, right. They're very small computations. So as you improve proof generation costs, you actually don't see the demand curve reacting in any meaningful way. Where you do see sort of demand supply dynamics is with proof verification for attestations, right. And that we can aggregate many of these together and amortize our cost. And then if you look at ZK roll ups, these are, as to Uma's point, predominantly their cost structure is comprised of DA and proof generation is a fraction if that. So again, proof generation cost reductions don't actually play much of a role in increasing demand.
00:36:51.740 - 00:37:10.560, Speaker A: What proof generation cost reductions do play a role in is sort of the design space for ZK applications, right. And making existing systems more trustless. And I think this is where we ultimately see proof generation cost reductions playing an impact.
00:37:13.800 - 00:37:49.008, Speaker D: Yeah, I think in this panel we will agree more or less with the same things constantly. I'm on the same page. The only thing that I will add is that it's true, it's already really cheap to generate proofs and it's going to get cheaper. Like it's a matter of time. So taking that into account, the next thing we should ask ourselves is, okay, what exactly is not going to get cheaper? And it's going to get probably more expensive. And that's the verifier that in most of these trustless applications is ethereum. So that's why our focus is there.
00:37:49.008 - 00:38:51.780, Speaker D: And yeah, I think now that with more education and also something that was the bottleneck for many of these CK applications was the tooling that's getting way better. I still don't understand to Zuma's point, like why we're still using a lot of this optimistic infrastructure. I think it's also kind of an economic incentive. We are using old technology because we need to justify maybe high valuations, I don't know. And yeah, I'm excited to see many teams working and building more stuff and especially because the proof generation costs are going lower and I think the verifier, ethereum will be able to make it also cheaper. I want to see more applications that are not related to blockchain and we couldn't use them to start using CK and be trustless. New trustless applications like CK, peer to peer, for example, these type of systems that it will be easier to build because we have better tools and better vms.
00:38:51.780 - 00:39:00.310, Speaker D: So yeah, I think it's a myth that is quite expensive, and in the next one or two years it will be extremely obvious.
00:39:01.800 - 00:39:51.060, Speaker E: One thing that often confuses people is that when somebody says that ZK is expensive, they're often not speaking economically, they're speaking temporarily of saying, oh, this proof takes a while to generate, but obviously that's like an absolute statement as to something that's relative, right? Some computations are very fast and some computations take a long time, right? So naturally the same thing is going to be true for ZK. Some proofs are going to be quick to generate and some are going to take a longer period of time. But over time, as all these systems get built out and as sort of ZK cryptography improves, both the cost of proof generation is going to go down and the sort of temporal cost of generating the proof is going to go down. And so I think a lot of these issues that people raise are actually not going to be problems in the long term.
00:39:51.880 - 00:40:34.260, Speaker B: Okay, maybe my final questions before I give the Q a time for the audience, my final question is more about using scenario. A lot of narrative happens, coprocessors. And when we look at the ZK space, the use case is mainly about scalability, like privacy, like ZK, Oracle, like lite clients bridge this kind of stuff. And in the next three to five years, I assume that you also have a lot of discussion from the potential clients or customers which kind of use case you think have more potential upside to get mass adoption.
00:40:35.720 - 00:41:36.744, Speaker C: Yeah, I think all the things you named, roll ups, bridges, coprocessors, oracles, attestation services are very exciting. I guess to me personally, I think we've kind of done a bunch of different applications, including like bridging coprocessors, oracles, and now especially with SP on rath rollups, I think one thing that's really exciting is with a general purpose CKVM, where anyone can write rust and get their zero knowledge proof, you can just enable all of these applications and more. And the bottleneck is no longer like having an advanced team with cryptographic expertise having to handwrite all these circuits for all these different use cases. I think finally, with the ZkVM, that's kind of when all of these applications can go mainstream. And then also people can experiment with even more applications beyond just the ones you highlighted. So for me, that's really exciting. It finally feels like the infrastructure for ZK, especially with zkvms, is like here.
00:41:36.744 - 00:42:06.530, Speaker C: They're easy to use, people can experiment a lot more and it's not bottlenecked by our expertise on our team or expertise on, for example, Neil's team. That's very specialized. It's like truly any developer can apply their creativity and have ZK in their app. And so I'm excited for all of the above use cases. I think roll ups are a very obvious one. Coprops are obviously really cool, but now it's just going to hopefully thousand x the number of developers that can experiment with stuff.
00:42:08.100 - 00:43:15.992, Speaker A: Yeah, I think the two largest ZK applications today, again in terms of proof generation, are roll ups and attestations. I think because both of those have found some product market fit on a relative basis, those will likely persist and continue to grow. ZK roll ups obviously sort of the end game state for scaling and attestations, in particular, private attestations for private personal information I think make a lot of natural sense. There's a lot of synergy there with the use of zero knowledge proofs and privacy around identity. I'm also really excited about what RJ had mentioned in his last comment about ZKP to P, right, in gluing sort of existing infrastructure together using zero knowledge proofs to attest to the correctness of the computation of these systems. And this goes far beyond just blockchains. Right.
00:43:15.992 - 00:43:37.900, Speaker A: And I think ZkPDP actually started as like a Venmo or it was a venmo on and off ramp. But you can extend this to venmo, to swift or to zelle. Right. And it doesn't have to be necessarily on chain. So I'm excited for these three categories.
00:43:39.860 - 00:44:13.640, Speaker D: Yeah, we'll say the same. Roll ups and bridges are the clear. They are going to be the first users that are going to use more ck. And the other one is these CK trustless applications that we haven't seen and we are going to see more. I want to see more experimentation. And also it would probably be easier to write a CK application because you just use rust than actually writing a smart contract in solidity. That's something that I like personally.
00:44:13.640 - 00:45:00.248, Speaker D: I think that will be better and this will mean that we will have even for example, games. We are experimenting a lot where we want to include, for example for a line, the probable doom that Rick zero did. And we want to see people use CK everywhere. And I think that will happen. And yeah, the main thing is not the prover. We believe that in order for that to happen, you will need a trustless layer, and for that you need cheap verification. But yeah, I think it's quite exciting what's going to happen the next couple of years, especially considering that we live in a world that is very complicated and probably multipolar, and trustlessness is going to be extremely important for any type of system, not only money.
00:45:00.248 - 00:45:03.610, Speaker D: So we want to see more things build there.
00:45:04.140 - 00:46:00.990, Speaker E: Yeah, those are all good examples. Just to add to that, maybe I'll answer a bit in the abstract. You can think of a computation as taking some time x to run, and then to do that computation in Zk takes some like f of x, right? Because there's some overhead over the baseline computation. And as Zk gets better and better, sort of f of x is sort of less and less of an overhead relative to x. And so I think the exciting question for people in Zk to ask is how do people start using Zk as f of X approaches X? Because you can imagine that in a world where f of X equals x, people would use Zk for pretty much every computation, because it's just like a free benefit, right? You can just prove that the computation was done correctly. And so these are all examples of things as f of X approaches X. But as it continues to move along that trend, what additional things will we see? I think is the exciting thing for us to discover in the years ahead.
00:46:02.080 - 00:46:13.384, Speaker B: Okay, thanks for insight. We still have around five minutes for Q-A-I mean, two questions. Hello.
00:46:13.522 - 00:46:20.460, Speaker F: Thanks a lot for really inspiring talks. We at bonus technologies are actually building ZK friendly execution environments.
00:46:20.540 - 00:46:21.872, Speaker E: We could call it like this.
00:46:22.006 - 00:47:11.168, Speaker F: And what we discovered is that actually there is no end of the game when it comes to cost of ZK proof execution. Of course, if we speak about about few cents, it looks like negligible. But let's say in the course of this year, Ethereum DA, thanks to proto dank sharding, and sharding itself will be decreased for order of magnitude at least, which will put DA cost. And by the way, there are many other projects like Celestia availing so on. And so it's expected that DA cost will rapidly drop like for few orders of magnitude, which will actually be almost at similar or the same as the cost of ZK proof execution. Of course, it depends what kind of ZK protocol and so on and so on. My question would be related on that.
00:47:11.168 - 00:48:36.270, Speaker F: What would happen if, for instance, for a real production use case, let's say gaming or whatever, you have millions of required ZK proofs per day. In that case, ZK proving cost becomes not negligible, but basically the cheaper. I mean, there is no cheap as can be, the cheaper the better. So that's one of things that I think in future we will anyway face. And I would like to add one more point on this, that I think that interoperability between zks and convergence to two or three or ten ZK protocols will be at the level mostly of using, finally using standardized hash functions, elliptic curves or arithmetic fields. I think that this is something around what is going to be centered both ZK protocols on your side and ZK computation infrastructures that we are building on our side. Just my comment that I think we should all work together as there is no silver bullet and there is no cheap enough ZK computation might be for this use case of today, but let's say in a year or ten years from now, adoption would require really some kind of coupling between ZK protocols and infrastructure, as for instance happens with IA today.
00:48:36.270 - 00:49:28.850, Speaker F: So artificial intelligence has a number of infrastructure solutions that are supporting this computation, and there is always and always new and new and new solution on both sides. So I think that the only way is to work, to couple those efforts and work together and might be interesting, for instance, to know what kind of ZK protocols some of you is using and what is exactly cost now at some kind of, I don't know, whatever solution, probably some mainstream cloud you're using for that. Because what we heard is a kind of theoretical calculation for very unrealistic case on spot market when it comes to calculation of specific ZK protocol. So some real example and real cost would be really interesting to hear because we have our own calculations that are a little bit different.
00:49:35.040 - 00:51:04.890, Speaker A: Well, we should have invited you up on stage. I think you make a really valid comment here about what are the correct me if I'm wrong, but what are the practical costs for? Frankly, it's kind of an impossible question to answer without parameterizing what kind of computations you're looking to prove. I think there are always going to be certain computations that just do not fit well with ZK. Right? You see that for example, most ZKE evms, they opt out ketchak hash functions for Poseidon. Right? And I think there'll probably always be these sorts of trade offs that as an application developer, you'll need to be aware to a certain degree of what those trade offs are and implement them as you're designing your application. I think it's on us and developer tooling and infrastructure teams to make that really easy, right, to alarm you whether or not you're using a computation that's going to blow up your proof generation costs. But I think to defend kind of like the state of ZK if you design your application to be friendly with these existing proof systems, which I'm empathetic, is oftentimes difficult.
00:51:04.890 - 00:51:13.660, Speaker A: It is very efficient. It is there. So, yeah, at some point we'd love to know more about your struggles.
00:51:16.500 - 00:51:18.000, Speaker B: Any more comments?
00:51:19.140 - 00:52:14.450, Speaker D: No. The last thing I would add is I believe at some point some of these things that you will be able to prove any type of computation. In my opinion, it's just a matter of we will see many provers for different type of computations, or if we will see one, to rule them all. We have seen, for example, with the hash functions, the team of Ulbetana, they released a proving system called Venus and allows you to use normal hash functions that are not CK friendly. And I think at some point we will see more standardized or normal computation being done with CK without having to decide about these trade offs. But maybe the trade offs are not around the computation that you do, but basically ethereum, for example, if you want to use proven system in particular, the proofs are huge. So I think that's where the future.
00:52:14.450 - 00:52:18.130, Speaker D: Yeah. That's all.
00:52:18.980 - 00:53:02.370, Speaker C: Yeah. I think another thing I want to point out is, for example, I used to do a lot of machine learning, and even four years ago, it was totally unthinkable that you would have a model as big as GPT four. We were doing models that were like, in the billions of parameters, and GPT four has maybe hundreds of billions of not more parameters. And I think I view ZK in a very similar way with the trajectory of where it's going and how fast costs are reducing. And so I remain very optimistic that especially with things like hardware, especially with things like ZK, like, as there becomes more demand, there will become more specialized hardware to make the supply side go down and just make crazier and crazier computations in ZK possible.
00:53:05.780 - 00:53:10.768, Speaker B: Yeah. If. No more comments, that's the end for this panel. Thank you, guys.
00:53:10.934 - 00:53:11.310, Speaker C: Thank you.
