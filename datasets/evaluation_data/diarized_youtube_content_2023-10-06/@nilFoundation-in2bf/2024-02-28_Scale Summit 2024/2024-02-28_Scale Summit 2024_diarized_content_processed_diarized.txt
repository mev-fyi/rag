00:02:21.160 - 00:03:15.132, Speaker A: When a monopoly can naturally occur and control everything, and if they don't like you, they can exclude you, or they can change subtle rules of the game to favor slightly certain players and create better positions for them. We will end up reinventing the same web, two story, we will just recreate the same structures and the same problems. Who do you see as the monopolist in the proposals? Proposers. Because you will eventually end up with like one or two proposals which we already see on if I mean rocket pool, ido, sorry, builders. I always mix them up. I think you mean builders. Yeah, but fundamentally it's a permissionless marketplace, and if you're able to create blocks, not because you sell like, it's not really the proposer will accept the block that gives them the most economic value.
00:03:15.132 - 00:04:12.690, Speaker A: Look, the problem with this approach is you force everyone in the same solution, like you say. Sure, you don't need to use Google or Gmail for email, you can go with any other provider, but then all the websites only have login with Google button, right? So if you don't have a Google account, you effectively, or Apple account, you effectively exclude it from a convenience of single sign on. Conversely. But conversely, if you don't participate, like if users are getting a better deal from shared sequencing, if they're getting better ux, if they're able to access more economic value because liquidity is shared, then by not opting in, you are imposing a cost on your users. They should not be. We can design systems that are equally usable, just without shared sequencing using the EZK proof system. I have actually an extra point I want to make about this.
00:04:12.690 - 00:04:49.470, Speaker A: We're ganging up. So disclaimer this is Justin Drake's proposal. And so if you argue against it, you're not aligned with. Right, I know, but there is another thing that upsets me with the shared sequencing environment, which is it sort of completely gave up on solving MeV to whatever. By Mev, I mean like all the front running type of meV. And that upsets me very much. The fact that we normalize the idea that transactions should be visible ahead of time is highly problematic to me.
00:04:49.470 - 00:05:37.880, Speaker A: Are you saying that with threshold encryption you don't get the scale benefits of. I mean, the thing is, for people we don't know, that is my perspective. The only true way to solve mev, at least to some capacity, is ordering without execution. So for those who don't understand just very clearly, blockchain are just consensus is just about which order transaction happened, and not so much about actually what happened. In them. And I won't get into the detail here, but if you could encrypt a transaction in a way that only unlock itself at finality, which. What would be possible in the context of single, short finality, meaning the second I get included, the transaction itself.
00:05:37.880 - 00:06:01.776, Speaker A: Transaction reveal itself after finalization, then there is no mev in the send that we hear about today. Liquidation will still happen, and that would still disturb mev. But you still don't have mev of the sandwiching attack and all those sort of things that nefarious the system. I mean, come on. It reduces the mev. It doesn't eliminate that. You can't eliminate, like, okay, you eliminate sandwiching attacks, and that's it.
00:06:01.776 - 00:06:10.004, Speaker A: But like everything else, like front running. No, of course not. Front running is. Yes, front running. You can solve it. It solves. Yes.
00:06:10.004 - 00:06:23.124, Speaker A: Because you cannot see what you're. You don't know who you do. You front run. Yeah, but you don't eliminate all kinds of mvp. That's my point. Right. You eliminate the harmful.
00:06:23.124 - 00:06:43.580, Speaker A: The most harmful type of. Yeah. With all the love for flashbuds, they did this marjorie's thing of mixing normal mev, which is like liquidation, with nefarious mev, and claim that it's all the same thing. They cannot be sold at the same time. And that's very annoying. There are such a thing as front running. It's forbidden international market for plenty of reasons.
00:06:43.580 - 00:07:34.080, Speaker A: It should be also prevented from technical perspective. In blockchain, we just gave up on that so far, unfortunately, in my opinion. Yeah, I would say that to your first point. So your first point was that lack of fairness and then whether adamicity is actually useful to users. And I think that's just something that the market will determine. And if, in fact, atomicity is good, and if it does present, like, a meaningful benefit to users, then I think that there will be, like, a market pressure to use shared sequencing. And I don't think that we can run away from that because we're already seeing momentum to support shared sequencing on ethereum.
00:07:34.080 - 00:08:10.590, Speaker A: I think in some sense, if it is, in fact, better for users, then it's inevitable. Here goes. Not so hot take on shared sequencers. Right. Shared sequencers, being not deeply embedded into the protocol, are just another l one, like, for real. And using shared sequencing without having it designed, being deeply embedded into the protocol, like, from the first principle, just from the very beginning, you effectively reduce the security of the whole system to just this one more l one security. So it's like anyone wants that.
00:08:10.590 - 00:09:23.570, Speaker A: So I think this has been a very interesting discussion on your views on round shared sequencing and what the right approaches are to maybe take here. I think at the end of the day developers are going to choose what makes most sense for their application. I think one thing that I've been thinking a lot about is also, okay, you have shared sequencing. That seems like the option today that can get you something close to atomicity, right? But I think long term, and you all alluded to this earlier too, as the length of time it takes to generate a ZKP keeps coming down and the cost keeps coming down. If we can get to the level where you can generate proofs within like a second, maybe even like milliseconds, then I think for all intents and purposes, for a lot of developers it doesn't actually matter or not whether it's truly atomic. Right? So I think on that note, something I wanted to get all your takes on because I think the fact of the matter is most developers today who are building a roll up, they are choosing to go with optimistic roll up stacks, I suspect. And I'm open to hearing your views on this too.
00:09:23.570 - 00:10:19.296, Speaker A: One of the reasons for that is just ZK is still a bit expensive today, right? And I think on this point about getting the cost and the latencies down, that's going to take some time. So I'm curious if you all have thoughts on how much left do we have? How far do we have to keep going? When will ZK truly get to the point where it's economically like a no brainer for developers? Right? So if you open growdepy.com, I think, and look at transaction costs, ZK sync is by far like 50% cheaper than all the optimistic roll ups already today for average transactions. So this happened since two months. I think the reason why ZK has not yet taken off is because there was a period of warming up when optimistic rollups launched. Initially they had a lot of developer fiction. Things worked just slightly differently.
00:10:19.296 - 00:10:40.200, Speaker A: There were some inconveniences, some tooling did not pass. It took a while. It took approximately a year for arbitrum to polish the experience and make it really mature. It took a bit longer for optimism. We experienced the same thing. We did not have some basic tooling and some basic things available on ZK sync. It's not like EVM coolant roll up yet you need to recompile.
00:10:40.200 - 00:11:03.492, Speaker A: But things work now and we're, I think like 95% there and we're serving the last miles. But you're right, costs were a major factor, they were more expensive than optimistic roll ups. That's not the case anymore. At least one ZK roll up is cheaper than optimistic roll ups. You can check it for yourself. I will agree with Alex. And right now he's right.
00:11:03.492 - 00:12:05.492, Speaker A: They are cheaper than every other LTU, including Starknet because they launched their very good upgrade boardroom with compression, which I congrats you for. It was a very impressive work. But more, I mean, to go back to your point about why people still do stay, tend to favor optimistic roll up. My perspective, there is two answers to this. The first one is we are still talking at this stage with the same builder that we have lifecycle I haven't seen yet truly new things, truly new application that has been coming up in the last two years and regardless of tech stack and I'm not going to participate. Starknet and Cairo and all that. Just when you look at what's going on in the market, we see a lot of new project coming up with new incentive mechanism, new ways to do things.
00:12:05.492 - 00:13:08.440, Speaker A: But the core of the product hasn't truly changed. And so I'm expecting that in this new bull we're going to see new things. Things like on chain gaming or things like advanced cryptography for Oracle, things like erodotus Axiom destroyed proof, all the zkecoprocessors, all of that is getting to market and the implication of their outcome will finally arrive, will arrive soon and will be as a native base home in the ZK roll up word. Because if you want to verify proof, I can tell you that ZK rollup are the right place if you want. ZK Rob are the chain for proofs, especially ZK rollups with state divs because you don't have to publish the proofs. But I want to say that we actually see really interesting use cases being developed, at least on Zksync in the ZK rollup world. We have, for example, Quark ID, the project we're doing with the government of Buenos Aires where this Friday they open source the platform.
00:13:08.440 - 00:13:40.710, Speaker A: It's a platform for self sovereign id where citizens can control their know, they don't delegate it to governments. The government actually gave it to them. And you can use your identity on Ziki sync with Quark ID to access goods and services and governmental things. And they're now expanding to Mexico and Colombia. We're seeing things like tokenization of real world assets. We're working with traditional institutions. It's taking them longer time to warm up.
00:13:40.710 - 00:14:18.992, Speaker A: They take years longer than startups. And obviously than crypto native folks. But they are coming there. We're in conversations with banks, with central banks, even they are coming there. They are building things that are coming to, they're going to on chain the traditional finance and they will only use EQ roll ups. They are looking beyond marketing hype. They are looking into the essence, what's the most secure technology available to us? What is the technology that gives us privacy and yet interoperability with other systems? Like my bank interacts with other banks, but I don't want to disclose the data of my customers.
00:14:18.992 - 00:14:44.788, Speaker A: That's only possible ZK rollups. I think the ZK revolution will actually unlock the full potential of Ethereum. And this is already happening. We can totally see mean. I was more referring to optimistic rollup based project I've seen without dissing anyone, just to be clear. And to go alongside on the Starknet front, a new application that has been very exciting. There are multiple dimensions, people using the language.
00:14:44.788 - 00:15:54.852, Speaker A: So Starknet has this Carol language it's optimized for proving, but also now also looking towards privacy. And so you have a team right now called Giza, which is presenting a Giza workshop around here where you can see how they're using this new language to make VKML provable inference of machine learning model and on chain gaming. Once again, what I'm trying to say is the stage where we see projects taking advantage of what ZK allows is rising and not fully there yet. When it's going to come out, you're going to be surprised of the new application coming in. You kind of touched all the points, I would say, except the points for which all of those roll ups were building. Are there high performance stuff? It's like managing the block building, for example, or transactions operations over high performance roll ups. Or it's like kind of suave style, right? Just not an l one.
00:15:54.852 - 00:16:39.840, Speaker A: That's the thing. Or it's like perfect changes, the most usual thing. It didn't get the question quite. You gave those examples of apps. It's like, which kind of are kind of still possible on ETH, just a little bit more expensive, but still possible in there. Well, I think it's prohibitively expensive. On layer one, the costs are a factor.
00:16:39.840 - 00:17:25.330, Speaker A: If a transaction fee costs like $10, you cannot buy a coffee with it. Right. Architecturally, I can give you a very good example. Star queer has been having Starkx, which has been by far, in terms of volume, volume of trade happening on the system, the most successful ziki, and even per environment in crypto and with Dydx we did a trillion dollars of volume with apex, with brine, with all the new, the other one. Now the vulnerable flying. And those application could literally not have been built on any environment that is non decay based. And the very reason is, the reason is actually quite simple.
00:17:25.330 - 00:17:59.080, Speaker A: The reason is those application cannot leave in fee based environment. In environment where they have to compete for block space. And so this is the bullcase or those Zk servers, those centralized app chain where I can define my own fee mechanism. Or maybe I just don't take any fees because I have other DDoS protection. And I know that I'm going to make money in average of all the thing happening in my system. No, I'm talking no, it's an l three. So paradex for instance right now is running a stock in the app chain.
00:17:59.080 - 00:18:44.920, Speaker A: They did 200 million volume daily I think lately. And why they use an app chain here is or a private app chain is because they are protecting themselves against dose through regular web two solution meaning ip filtering, rate limiting and so on. And they can allow their user to trade, to accept limit orders, to cancel limit orders without fees. Because that's the biggest hurdle for a centralized order book on the blockchain. And they know that in average all the settlement that's going to happen, they're going to make money. And so they don't have to price actual transaction fee at every transaction. But the environment itself is the same environment that if you were running on a public network.
00:18:44.920 - 00:19:28.840, Speaker A: For me that's actually what the end game of those l three looks like. Centralized noncustodial environment that understand what the smart contract is and can run without running fees for your specific application. And just I would even argue to make the bullcase for l three is actually this one, which is cheap, is never cheap enough for every application. If you have application that requires to be cheap, there is always something will price you out at some point if the block space is limited and you need an environment where you can control what's going on. So here's a slightly spicy rejoinder to that. They don't need to be l three s, right? You could have l two s. Where we do proof aggregation.
00:19:28.840 - 00:20:05.520, Speaker A: There's no dependence that they settle on an l two. It comes down to semantic. Basically what you're saying is ag layer versus settling proof on an l two itself. In my opinion it's actually better to do proof submission on a public network because it's asynchronous and I don't need to co organize on which order I need to send my proof. And if I trust the l two itself, I just can trust what's going on. The settlement itself, it's now based on trust assumption of the consensus mechanism of the l two versus the l one. And at this point it's kind of semantic and like what level atomicity we're talking about.
00:20:05.520 - 00:21:03.360, Speaker A: Well, but the problem is you can't have, it's much more difficult to have cross l three transactions for l three s that are settling on different l two s. That's true, but I think it's literally, honestly, you can probably draw exact the same sort of operation between settling your proof and directly executing a transaction to the other environment versus coordinating everything in a single proof. So when I'm saying like, for instance, let me give you an example, paradex. Let's say you have an EB three that settled on starknet. Okay? They send the proof as part of the state of date of the execution of that transaction. A transaction is automatically emitted on behalf of the user, and this transaction directly deposit to the l three. That's equivalent to aggregation within a single l two within the same environment.
00:21:03.360 - 00:21:55.140, Speaker A: Yeah, but I do think that we're sort of like, the nice thing in my opinion about the ag layer is that there's not a contention for block space and for resources that you run into on an l two, because on an l two you're not just aggregating proofs or sort of being this settlement layer for l three s. You also have transactions and smart contracts that are deployed directly to the l two. And so I'm not sure we're going to resolve this. No. So you can argue in this case, and this is a Solana bull case, which is paralyzation. Paralyzation at transaction level, when you, I'm not saying that food access is the way they do it, but at some level of paralyzation gives you solve a block space issue while becoming. You can build the transaction dependency graph and those settlements will all be isolated from the rest.
00:21:55.140 - 00:22:32.272, Speaker A: Yeah, but you're still contending for network resources with the Aglair. Not every validator on the Aglair even has to download or receive every proof. This is sort of the nice thing about proof aggregation, but there is now a coordination requirement between the two l three s to know that the message was sent from one to the other. Yeah, of course. But you're going to have that. You're going to have some coordination. Yeah, but in this case, the other l three doesn't actually just have to look on the l two as opposed to actually talking to the other, the one that's sending the message itself.
00:22:32.272 - 00:23:32.472, Speaker A: But I think we're digressing. I think it gets to semantic at some point. Guys, this has been an interesting discussion, but I think we also want to give the audience some time to ask some questions. Sure. Does anyone have any questions they want to ask? So actually, I want to revisit question, because his last question is about the cost of ZK being high. Do you think that with things like technology advancement or like hardware acceleration, this can be practical so that people will be thinking about ZK instead of optimistic? What do you guys think? All of you guys invest into ZK hardware like companies? Yeah. So it's interesting because I think everyone on this stage has been working on ZK for a long time.
00:23:32.472 - 00:25:01.072, Speaker A: But my personal experience was starting a ZK project in 2019 and being extremely nervous that we were raising money and the technology was not ready for what we wanted to build. And I think that everyone on this stage has gotten very lucky that we entered the technology at a time when it was increasing in speed and performance and efficiency exponentially. And so I think it's fundamentally just a mistake to ever bet against technology. But I would just echo Alex and Louie when if you look at transaction fees currently on l two s, they're extremely competitive with optimistic roll ups. There is a question about whether proving costs are being included in these fees, but I think what this argument misses is that optimistic roll up users, they aren't just paying transaction fees, right? Because the seven day withdrawal delay imposes capital inefficiency on the users of optimistic roll ups. And so if you try to quantify this capital inefficiency, and you look at the aggregate fees that users have paid to third party bridges, it's like tens of millions of dollars to avoid the seven day withdrawal delay. And so if you compare that with the cost that it would have taken to prove every single transaction on arbitrum or optimism, we just released a type one prover.
00:25:01.072 - 00:25:50.404, Speaker A: We could generate proofs for optimism and arbitrum tomorrow. It's like orders of magnitude less. And so optimistic roll ups are already extremely cost prohibitive, and we haven't even started talking about their limitations in cross chain and composable settings. To me, the debate is no longer really ongoing. Just to add to that, I would say regarding the proving cost, it's like eventually it is going to be priced in into transactions cost on ZTL two s. Like for sure the best way maybe to reduce that is just marketplace like market dynamics, something like this, just apply that to proof generation, you're good. If the hardware comes up, show if it doesn't comes up.
00:25:50.404 - 00:27:16.842, Speaker A: And there's a lot of software implementations, a lot of software optimizations, show, I mean, just apply market dynamics, the market will solve it all. You don't believe it? Yeah, and the proof being expensive is a meme for all capacity and for a very long time. And I think all of us can just apt this type for it. It's been a meme for like two years at this point, and a meme that's been pushed by vcs to justify funding other companies. With all the respect I have for other companies, which I appreciate, we just released a new paper called Circus Stark literally last week, and Bronky three and a bunch of other announcements coming up just say that the improvement at the research in the software level already, I mean, there is so much to improve just right now that this claim of it's going to stay expanded forever is just like mid curve completely. I think we might have time for one or two more questions. Hey guys, that was a great conversation about shared sequencing.
00:27:16.842 - 00:28:12.930, Speaker A: I tend to agree with Misha that shared sequencing is basically creating a new l one because you're recreating consensus. Another thing that you could do is that just use a faster l one. And I'm specifically talking about Monad, which is going to be copy paste with Ethereum. So how do you guys view Monad? Do you think that you could see yourselves having products moving to Monad as roll ups? So Monad is doing optimistic parallelization of transactions, and this way they can process more. They claim they will see how it plays out. They are not live yet, but you can totally do the same thing on any other l two, right? Like there are way more high load system engineers in the world than there are ZK researchers. So that's a lesser bottleneck in my opinion.
00:28:12.930 - 00:29:02.750, Speaker A: We will all add that eventually, some sooner, some later, it makes sense, right? And then we can prove all of the transactions in parallel, in batches, with your knowledge proofs. They are basically infinitely paralyzable. So I think Monet will compete with Ethereum for the mine share and for liquidity and for decentralization. So yeah, it's like in layer one category, whereas ltos will be able to offer the same thing. I would even say that the question is, will we settle? First of all, starknet specifically. For instance, we are working right now on Block STM, which was invented by Aptos. Aptos, right, block STM.
00:29:02.750 - 00:29:53.794, Speaker A: Whatever Monad is doing all the l two s will do it at some point, because there is no reason why not. As now settling on Monad versus Ethereum. Never say never. But at the end of the day, it's a question of crypto is not just AWS versus GCP. Otherwise we'll be settling on EOS or Solana or whatever chain that might be relevant. There is a notion of who you cut it to and which currency you cut it to. And like Mona has a very happy battle to prove its credible naturality and attract the user base before, I think, any of the established shell two, we think before moving there, but never say ever, someone may just take the stack and deploy there, and good for them.
00:29:53.794 - 00:30:24.740, Speaker A: It's a permission at the end of the day, I mean, it's like mentioning Monad. You're effectively trying to just write piggyback on the parallelized EVM meme, which is of course like parallelization on a VM level. Sure. I mean, just sit and do it. It's not a big deal. It's like at Ziki Shorty and things we've done much more than just EVM parallelization. It's like data parallelization.
00:30:24.740 - 00:31:10.088, Speaker A: Not just computation parallelization, but also data parallelization, effectively, because just to avoid locks, I mean, you kind of asked some time ago, by the way, about what's the point of doing a damn sharding? Just settle on whatever, right? So it's like, my point is, data locks, state locks, it's like data exclusivity access. You effectively get a lock to particular index when several users access to that. So to prevent the locking of that, just putting the whole thing on pause, you got to shard that thing, just split the data. And that's what makes it different from salon and things. That's what it makes sense, and it keeps it decentralized. So why to not to use a better l one? Better l one still has. First of all, it's like, what's the better l one? All right, it's just different design.
00:31:10.088 - 00:31:36.720, Speaker A: Second of all, it doesn't mean that there are no such problems, which eth kind of encountered already. And the third thing, protocol design. Sure, it's like decentralized volops will all have their own kind of protocol. We have it from day one. Guys will achieve it. All of them will get their protocol up and running, of course, as well. So it's much easier.
00:31:36.720 - 00:32:19.194, Speaker A: It's not that of a big deal. And one last thing about the Monad versus EVM or Ethereum paralyzation. The current problem of scale on Ethereum is not execution, it's state. So the only thing Monad will achieve if applied on the Ethereum state will make proof, I mean computation cheaper. The state will still be super expensive because it's growing. It's a single thread at this point. There is no excess list or other mechanism that is going there.
00:32:19.194 - 00:33:03.350, Speaker A: It's still going to grow. And I mean, I would love Ethereum to be paralyzable. You make stark proof much cheaper, so please do. But other than that, I don't really see the advantage of the narrative behind it, like outside of being a cool feature on top of the network. I think we're at time guys. Thank you for all joining this panel and thank you for your thoughtful questions. Guys, just one last thing, guys, I got a bunch of really interesting ZK quest things, so you can find me after the panel and you can win all up to several hundred usd just by passing the test the quest.
00:33:03.350 - 00:33:59.050, Speaker A: And it's pretty fun and a lot of cool swag that you can get at our bullseye if Denver we've got a hands on workshop happening at 230 by Maro Toscano Applied Cryptography league Lambda class. If you're interested, feel free to make your way to the coworking lounge to join Maro. After that, please join us back here for the first presentation of the day on PI Squared. Proof of proof for universal verifiable computing by Gregory Rosu, CEO, PI squared next up, we delve into the economics of zero knowledge proofs. For this panel, please welcome back to the stage Avi Zurlo from Nil Foundation, Umaroy from succinct Labs, RJ Catalan from yet another company, Nick Matthew. Moderating this discussion is Ray Xiao, senior director at IOSG Ventures. Please welcome them to the stage.
00:33:59.050 - 00:35:45.130, Speaker A: Yeah, they'll be out, I think. If you guys need me, text me. Sorry guys, this is text me. And then the other one, gentleman, the next hello. Yeah, it's so nice that we can have discussion about economics of zkps. I'm ray from iOS adventures. We are crypto VC.
00:35:45.130 - 00:36:26.170, Speaker A: First of all, before the discussion today, we'd love to have a brief intro about yourself and what your focus recently. I'm Uma. I'm one of the co founders of Succinct. We're broadly focused on making tooling to make it easy for any developer to build with zero knowledge proofs in particular. Recently we announced our plans for a decentralized prover network and also launched an open source ZkvM called SP one that has state of the art performance and is really useful for a variety of use cases. Hey everyone, I'm Avi. I'm chief product officer at Nil foundation.
00:36:26.170 - 00:37:03.206, Speaker A: Yeah, Nil foundation is a team of primarily engineers and researchers who's been building since 2018. We have a crypto three library, which is c plus plus cryptography library. We have a proof market for sourcing proofs. We have a ZKlvm, which is a circuit compiler. And then our focus for this year is an l two, which we've coined Nil and yeah. Hi everyone, my name is RJ. I'm the founder of uterine company.
00:37:03.206 - 00:37:31.328, Speaker A: We are building a product called Line Layer. It's a decentralized network verifiers, and we're using layer to make verification cheap and also more flexible. Hey everybody. Hey, I'm Nick. I'm an investor at standard Crypto. We're a crypto native venture fund that's really interested in ZK. Okay, thank you guys.
00:37:31.328 - 00:38:34.260, Speaker A: I prepared some questions, but I'm not sure I can ask covered often, but I'll try to make it the first question to try to let audience to have a more recap. Could you give us a little bit more background compared with two, three years gold, do you have any onchain or change in terms of the visions? We know that a lot of progression in both academic side and product side compared with two, three years ago. I assume that. I think you also have a lot of change in different focuses. You have new products and could you give more introductions about this site, both like product size and the research side in different focuses? Yeah. Our most recent product that we launched was this ZkVM SP one. The reason we built it is I think we had been building at our company a bunch of ZK applications for a long time, including ZK bridging, ZK Oracles and things like that.
00:38:34.260 - 00:39:40.028, Speaker A: And I think as ZK developers ourselves, we kind of realize that the ZK development experience today is really horrible. You have to write all these custom circuits and all these custom frameworks and it requires a lot of cryptographic expertise and is just really arduous. A ZKVM lets people basically write normal programming languages like Rust and then get a proof. And I think that is really the future of ZK and the thing that will let it go super mainstream, because anyone now can use zero knowledge proofs in their application, whether it be a bridge or roll up or things like that. And I think that is one of the biggest challenges of the space is the developer accessibility and ease of use. And so that's why we decided to focus on that more recently, because I think it's one of the most important problems. Yeah, I think similar to succinct, we spent a lot of time building a variety of applications and very low level tooling, like a cryptography suite, to more abstracted, easier to use tooling like the ZKlVM.
00:39:40.028 - 00:40:43.048, Speaker A: And I think the insight over the last few years is that research and engineering around zero knowledge proofs has made leaps and bounds to the point where at Nil foundation we noticed that maybe we could apply sort of the state of the art to solve old problems. And this is where sort of ZK sharding came to be. Where sharding has had a long standing problem of data validity. But with these state of the art proof systems that are really pushing the extremes of what you can do in theory that will then be implemented in production ready systems over the next few years, it's a great time to be revisiting these old problems that we didn't have the tools to actually solve for at nil. That's been a big change for us. Yeah, we share more or less the same vision. One of our goals is to make CK more accessible.
00:40:43.048 - 00:41:40.924, Speaker A: And we know already that a lot of people are working on the proverb side and working on new ckbms. And at some point developer UX and accessibility will be solved, probably by many teams. And what we figure out is, okay, if we know CK is moving way faster than we expected, and things are advancing and actually getting way better, what is going to be the next bottleneck for CK in order to be accessible, in particular to Ethereum, because we believe people want to build an ethereum. So the question we ask ourselves is, okay, what kind of infrastructure can we actually build in Ethereum that will help people use more CK? Because we already know that CK is already here, it's quite cheap and it's getting way better. So in our opinion, the main problem is the verification. And the verifier in itself is ethereum. So what we're doing is removing verification off chain, but we're trying to use crypto economic guarantees to make that possible.
00:41:40.924 - 00:42:22.764, Speaker A: And yeah, we have the same thesis around zero knowledge. And we believe this is the last mile in order to make CK way more accessible because we will lower costs. And by lowering costs as well, we'll allow people to use the latest CK tech made from new foundation or Saxene Labs. We're actually including the Saxene Labs VM in our MVP near soon and we just want more people to use the latest. We don't want Ethereum to be the constraint. So taking into account all this innovation that has been happening, we are adding this missing piece of the puzzle in order to make people use CK and Ethereum. And we're not building anything in ZK ourselves, but we talk to a lot of the teams that do.
00:42:22.764 - 00:43:08.796, Speaker A: And one of the things that's been most exciting for us is as the ZK cryptography on the academic side has gotten better and better. As everyone has been saying here, there's been more and more exciting things to build in ZK. And so we're excited to see that continue. Yeah. So back to our topics today, the economics of zkps. I think Nick also published an interesting article about the whole cycle of the zkps and succeed labs also building like zkvms and near foundations, like doing some zkevms and align layer, doing some verifications layer. So we have different process, like from proof generation, proof marketplace and verification, this kind of stuff.
00:43:08.796 - 00:44:10.780, Speaker A: And which part you think for your current focus, which part you think could be accrued most of the value when related to Ethereum based layer in terms of applications or sequencers only, and other things you think capture the most of the value in the host lifecycle, maybe this isn't a part that will necessarily capture the most value, although I do think it'll create a lot of value. And so hopefully it can capture some of it. But I think aggregation is really exciting because as I think aligned layer is working on today. If you want to verify a ZK proof on Ethereum, it's between 200 to 300k gas. One really magical thing about ZK is that you can recursively verify a bunch of proofs in another proof. This is called proof aggregation, and then you can take that aggregated proof and verify that on chain. So basically for 200 or 300k gas, you can verify arbitrarily many proofs and arbitrarily large amounts of compute.
00:44:10.780 - 00:45:03.932, Speaker A: So that is like a clear benefit to the end ZK developer where they can just directly reduce their on chain costs. And I think aggregation is like a really exciting area where it can make a lot more ZK applications feasible. So today, for example, your proof needs to generate ten dollars to twenty dollars or whatever the corresponding dollar amount of 200k gas is on Ethereum to make it worth it. But in the future, hopefully in the limit with aggregation, the cost of verifying a proof on chain will basically go to zero. Once you amortize it across all the aggregated proofs. And so I think the aggregator here is providing a lot of value and so obviously I think they can hopefully charge for some of that value as well. Yeah, I definitely agree with Uma said here, and I think it's difficult to say where value lies in the stack.
00:45:03.932 - 00:46:38.792, Speaker A: And ultimately it's up to ZK application developers to decide on what kind of trade offs they want to make. As Muma had described, these aggregational layers have this really magical property that allows you to combine many proofs into one and effectively amorturize your verification costs on chain. But the trade off is latency, right? And so for ZK application developers there's this balance that you have to strike, which is what is your latency for hard finale on Ethereum where verification is very expensive and what is your price sensitivity for that verification? I don't think we've developed the ZK application space enough because I think we're all sitting on this panel and we're all building infrastructure, right? And so over the coming years I think we'll see some of these things start to solidify and application developers express their opinions. Yeah, I tend to agree. I think it's too early to think about exactly where the value capture is going to be. We have been thinking about this lately. We believe maybe something like a line layer has a lot of volume, can have MEb because we decide the order of how we post the results of certain proofs on chain and that could have a relationship with the finality Sabi has described.
00:46:38.792 - 00:47:41.984, Speaker A: But we think aggregation is really interesting and useful. But we believe it's not going to solve most of the problems that we have today. So aggregation is really interesting when you want to aggregate the same type of proofs. The main problem is we don't know exactly which proving system people are going to use in the next five years. So we believe instead of using aggregation by using kind of this optimistic verification oracle that we're creating, you gain in the short term like way more benefits by lowering the cost of verification and also allowing proving systems that are new, for example new ckbms, to actually go into production even if they don't have the full infrastructure to have aggregation because of volume. Because if you want to aggregate, as Abby described, we have latency issues and their latency issues are correlated with the amount of throughput you have. Right? So with something as our infrastructure, people can aggregate, but it's not our decision, it will be the developer's decision.
00:47:41.984 - 00:48:46.950, Speaker A: And people can choose the trade off between latency and cost by their own. So we believe aggregation is something useful, but it's not going to solve 80% of the problem, because if you want to move into a world with many different ck tech stacks, it will be a really hard problem to solve. That said, we want to aggregate the same type of proof long term, but we believe we are not going to do that in the next couple of months. Yeah. In terms of market structure, the proof supply chain has a very similar structure to the transaction supply chain. And in the transaction supply chain, the sort of value capture there is mev. And so I think who sort of captures value in the proof supply chain is going to be quite analogous to who captures value in the transaction supply chain, which once we've built out all the necessary infrastructure, is for the most part probably going to be the front ends that sort of control the order flow or get the order flow directly from the user, because they're the ones who have the most sort of inherent leverage in the system.
00:48:46.950 - 00:49:32.212, Speaker A: And so similarly, I think whatever entity in the proof supply chain has the direct relationship with the application or user that's submitting the request to generate the proof, I think they have the most leverage. And so once all the infrastructure is built out, there's probably the most value capture there. That being said, there's a lot of sort of path dependency in terms of who actually ends up getting that proof flow and how different entities can monetize from that. So it's still very much TBD, but that would be sort of like my guess, in equilibrium. Yeah. Regarding mv values captured, how do you think of adjusting Jake's proposal? Like base row ups? I know you guys, some of you guys right now, you are doing rops. I'm not sure.
00:49:32.212 - 00:50:40.190, Speaker A: I think it's pretty new for you guys. We'd love to hear. What's your thoughts? Could you share us a little bit? Like ideas? Would you consider it or not? Yeah, I think for base roll ups, the value capture layer for the roll up right now at least is like sequencer fees and then also mev for the sequencing of the transactions. And I think a lot of roll ups anticipate that being the biggest source of revenue in the future, especially as the margin on the sequencer fees perhaps goes down a lot. For base roll ups, you're basically delegating sequencing to the l one block proposer. And so I think the reason Justin and other people who are very Ethereum aligned are a fan of this is because it takes the roll up sequencing, mev value capture and directly gives it back to l one proposers, which is good for ETH and Ethereum. I'm not sure whether the roll up teams would like this because maybe they want to keep some of the value capture for themselves.
00:50:40.190 - 00:51:40.434, Speaker A: But yeah, that's how I think about kind of base roll ups and the incentives behind. And I agree with Uma. To elaborate a bit more on the economic relationship between Ethereum and zero knowledge proof infrastructure, I share a similar vision with many others of proof singularity. Check. Okay, and in that we is it cutting out? Okay, test. We're back. Yeah, so we have this universe of proofs, machines that are generating proofs, and at the base layer we have the arbiter of truth, who is Ethereum.
00:51:40.434 - 00:53:14.444, Speaker A: And you want a very censorship resistant, operationally resilient, decentralized base layer to sort of be the canonical truth of verification. And in between this constellation of machines that are generating proofs and Ethereum, you have these really interesting layers that RJ and folks are building to actually aggregate them. And again, I come back to my original comment where it's not quite clear how value flows through that system and where it lands, but one thing is absolutely certain, that Ethereum plays a very critical role in that future, and it's going to be up to application developers to decide on how that all shapes out testing. Yeah, I tend to agree. I think we'll see. The thing I don't agree is trying to find one type of architecture for everything, because I don't think the tech is too ready. And also we haven't figured out savvy described when exactly do we actually capture the value? And many developers will choose different type of systems and where they want to actually do Meb if they want or if they don't.
00:53:14.444 - 00:54:12.316, Speaker A: There are some great teams working in using CK, for example, for me for rollups like radius. And in general, what we do believe is that we need more kind of this openness of deciding, okay, it will be up to the applications even more than the developers or the business logic of deciding which is the best architecture for the rollup or for the users as well. And we're excited to see more experimentations, not only merry with one single architecture to rule them all in roll ups, or even in CK in our opinion as well. But yeah, we're more pragmatic in the sense that out of people we experiment. We have to build infrastructure that can help. People are experimenting and can be long term thinkers. Yeah, I don't have much to add on booster roll ups beyond what everybody else said, but I think it's good to see more experimentation with different architectures for roll ups.
00:54:12.316 - 00:55:40.824, Speaker A: And just given that they're in such an early stage of research, I don't know if we have like a queer sense of what their sort of terminal economics are going to look like, but it's certainly an experiment worth running and we'll see how infrastructure teams and their application customers end up viewing it. Thank you. Regarding with proof aggregations, I know Lambda published some research and then near also linked some research about aggregate some firmware a and firmware b together. Try to compress the ZK and RJ have different solutions like inherent economic security from Ethereum and let the verification happen off chain. And we love to note your ideas the proof aggregations when it become more and more mature, do you think you would make fundamental change to your technical architecture? Do you need to upgrade a lot based on this? Yeah, I think the end game of proof aggregation just really depends on how proof systems play out in the next few years. I think right now a lot of different teams use a lot of different proof systems, but maybe that won't be so true in a few years. Maybe there will be some settling towards a few canonical proof systems.
00:55:40.824 - 00:56:36.050, Speaker A: Just from a security perspective, I think it's a lot easier to deal with that audit surface. And especially if zkvms take off and everyone's using zkvms, maybe it ends up being like one or a few zkvms that everyone uses that are fully open source and kind of like a Linux community project. In that world, if the proof systems that are being used are pretty homogeneous, I think aggregation looks very different than if there's many, many different kinds of proof systems. You could imagine, for example, that the final aggregation layer looks something like all these different proof systems get aggregated into one vm because you write your verifier and rust and then that gets put on chain. But you could imagine another world in which everyone does their own wrapping to something like route 16 and then all those proofs get aggregated. I think it's pretty early to tell how it'll play out. So yeah, I think we'll have to see.
00:56:36.050 - 00:57:53.050, Speaker A: Yeah, I'm going to give this mic a go again. I think proof aggregation, it already plays like a really important role in, I think, pretty much every large scale ZK application today, right? Roll ups already do sort of an internalized vertical proof aggregation in batching many proofs together in order to amorturize costs across transaction batches. I'm quite sure that worldcoin is also doing something similar. So that's going to persist. Right? I see no reason why we would go backwards where I think things get really interesting. And at nil where we're certainly excited to contribute towards, is sort of potentially heterogeneous aggregation across different proof systems. And here there's incredible economies of scale to take a proof of one proof system from one ZK application and combine it with another.
00:57:53.050 - 00:59:03.470, Speaker A: And yeah, I think frankly whoever builds that is probably going to make quite a bit of money. Yeah, I tend to agree. But at the same point our thesis is that we don't exactly know what will happen. So I don't know if ckbms, I think ckbms are awesome and we should use them more, but I'm not exactly sure if they will have only one of them or we'll have many of them. And because the future is uncertain, our philosophy is okay. If we don't know exactly how CK will look like in the next five years, how can we build something that can help accelerate CK being used today? And also we are talking about economics, right? So incentives are important and I think incentives in order to keep building your own proving system for some systems is going to be really high. Like why a CK rollup will abandon completely their proving system if there's a new CKBM.
00:59:03.470 - 00:59:54.590, Speaker A: I think there are many teams trying to build great tech and the incentives to be the winner will always exist. So I don't see that disappearing the next five years. So taking that into account, I think we will still have many proof systems, therefore we will not be able to aggregate them altogether and it will be up to the user, I think how they want to use aggregation or not, because as Abby described very well before, latency is something applications have to take care of. If you're a bridge, maybe you want to have low latency, or if you're a roll up, maybe you say, okay, I want to have less cost, so I create more. Latency is not a big deal. So I think that decision should be up to the application. So aggregation makes a lot of constraints to the developers.
00:59:54.590 - 01:00:58.050, Speaker A: That's the way we see it and that's why we're building something that is not married with only one proving system or one idea or framework for the developer. I don't have a strong view as to what proving scheme or schemes are going to be involved with proof aggregators, but one interesting question I think to consider is I think vital gave a talk at ECC last year where he painted this vision of the proof singularity, where an Ethereum block would just be a bunch of proofs that sort of got recursively aggregated or maybe just like a bunch of proofs included in a block. And now people are starting to build these sort of like out of protocol proof aggregators that then just submit the one proof on chain. And so I think it'll be interesting to consider whether or if at some point proof aggregators or a proof aggregator gets enshrined into the protocol at some point. Yeah. So that'll be something interesting to explore in the future. Yeah, I had another point that I forgot.
01:00:58.050 - 01:02:18.590, Speaker A: Yeah. About this topic in future. Definitely. We have so many improving system proving schemes and like you said, Vitalik also shares some views that we may have risk that if we rely on certain cryptographic system. And do you think in future for the target audience, for the users, do they have different requirements when they have so many proven system to choose and do we need to do some customizations plan for the users? And about the security side, do you think, is it easy to audit those codes? Is it easy to be more automated than auditing a smart contract like running by solid code? Yeah, I think, I actually believe that there won't be that many different proof systems in the future. Hopefully there will be like one or a few for zkbms, maybe a few for machine learning or really specialized use cases that get a lot of benefits from having customization. But I think otherwise the auditing and security story gets very tricky.
01:02:18.590 - 01:03:12.634, Speaker A: Even at succinct. We kind of encountered this where we've built and developed, I think at this point with like four or five different proof systems. And it feels a little unsustainable because basically for every new proof system you have to audit the whole stack like you have to audit even the foundational cryptographic primitives that are being used all the way up to how you express circuits. And I think that just becomes very difficult. So I am definitely biased, but I think there's kind of a future where everyone writes their application in something like a normal programming language and there the audit surface is very minimal. You use a normal compiler that everyone already uses today that has been well audited, battle tested. And then there becomes this kind of like hopefully super well audited canonical proof system that can generate proofs of bytecode that is pretty standardized.
01:03:12.634 - 01:04:19.458, Speaker A: And maybe there will be like, I don't think there'll be just one, maybe there will be a couple, but I don't think it looks like in the limit having hundreds and hundreds of proof systems because I don't think it's very secure. I don't think it's maintainable, it's not auditable, and there's like a lot of real trade offs that come with that. So if it's possible to have maybe a couple proof systems that have the same performance as the more customized ones, which I think, especially with lookup centric architectures do become really possible, that is like a much more ideal version of the future for kind of everyone involved in the supply chain. Yeah, I agree. I think we end up consolidating to a few proof systems, namely because it's just totally unsustainable to keep up with security practices for hundreds of proof systems. I also think that developer tooling is progressing really fast in such a way that it's incredibly easy to build a ZK application without ever having to touch the back end cryptography. Right.
01:04:19.458 - 01:05:23.468, Speaker A: And I see no reason why these tools get any worse. I think they're going to get much better. And that actually sort of puts the onus on the infrastructure team, the developer team who's building out those developer tools to make decisions about what type of proof system they're using about security. And coming back to Uma's point here, I think the only practical sort of end state is that we agree that there are a few proof systems that work well for certain types of computations, and these developer tooling infrastructure teams kind of join efforts to ensuring we have incredibly high security standards for those. Yeah, I 100% agree. I think we will see, it will be a small subset, but we will have, I don't know, maybe 1015 kind of proven systems like long term. In the meantime, we will have many.
01:05:23.468 - 01:06:27.284, Speaker A: And yeah, I hope to see more people using ckvms instead of writing their own circuits. And something that we didn't touch that I think is important because I agree 100% with everything I said, is that this will lower the cost for building in CK, not only for actually operating it, but actually going into production. So you will be able to go much faster into production and it will be easier to build a team because today is quite expensive. And that's also a constraint that is not a technical constraint, it's an organizational constraint that is not allowing us to use more CK because a lot of great teams, they don't use CK because they are afraid of using it. And they also don't know that CK is already useful. So we are really excited to lower the barrier of entry, not only by lowering the verification cost, but allowing people to use more ckvms or systems that they currently don't use because of the trade off they have to make in order to work with Ethereum. So we're really excited to see way more sikh applications and we think that the tooling will get.
01:06:27.284 - 01:07:25.830, Speaker A: It's already getting way better. I'm going to say the same thing as Uma and Avi, just in a slightly different way. You can think of all these proving schemes as sitting within some trade off space, and some of those proving schemes are in very different spots in that trade off space. Starks and Grot 16 and so maybe there's viable reasons to use one instead of the other in some context, because they're just so different. And then there are others that are marginally different on some axes, but aren't dramatically better than some other for some given context. And so you'll probably see shelling points around three or four, and then even if there's some other poofing scheme that's slightly better for some narrow use case, people would just gravitate towards wherever the shelling point is, because that has the better infrastructure and tooling and security and so on. So yeah, that's how I think it's going to play out.
01:07:25.830 - 01:08:37.688, Speaker A: I think we already talked about economic incentive like MeV, but we didn't discuss much about the course. I assume that many of the audience also have the similar question marks as I have, is that where are we at when we consider about cost of the estimated cost of the proof generation, proof verification, and where we are now, and which of these are the main buttonnecks for us and what we can expect in the next, let's say two, three years to improve. I think already today the costs of ZK are actually cheap enough to be very practical and viable in a variety of contexts. As like a really concrete example, recently the Polygon team released their type one ZKE EVM and the cost to generate a proof per transaction was maybe around 0.2 to 0.3 cents per transaction already. Roll ups pay at least ten, sometimes twenty cents to Ethereum for data availability.
01:08:37.688 - 01:09:21.464, Speaker A: So fractions of a penny is really completely trivial compared to that. And so I think already ZK, even for a very complicated use case like ZkevM, is extremely cost effective. And I think this is kind of like a myth that ZK is too expensive. I think that's actually not very true. If you look at some of the more recent stuff that's coming out, even with our VM SP one, we implemented SP one ref which implemented a ZkeVm in like 1000 lines of rust and it's very reusable. It uses ref alloy revim, all these primitives, so it's super auditable. And the transaction costs for that are around one cent a transaction and also rapidly decreasing.
01:09:21.464 - 01:09:56.310, Speaker A: So even for example, something that's general purpose and not handwritten like the polygon ZKE EVM is still quite cost competitive, especially when you put it in the context of what we're already paying ethereum for DA. So I think the zke adoption problem is a lot around education that actually, hey look, the costs aren't that bad. You can actually plug this in today. Also making the developer experience really seamless. So not making it. So people have to run their own proving infrastructure. And I think that's where things like proverb marketplaces are really useful, similar to what us and Niller are building.
01:09:56.310 - 01:10:43.700, Speaker A: But yeah, in terms of cost, I actually think it's kind of a myth and it's kind of all of our jobs to make it clear to people that hey, every roll up should be a ZK roll up, every bridge should be a ZK bridge, every oracle should be a ZK oracle. There's no excuse to not have the best tech. Why not use the best tech? Why not have the gold standard instead of all this committee based, optimistic, whatever stuff, let's just use the best tech because it's already cheap enough now. Yeah, I agree. Again, we're going to have to find some things to disagree on. I think to give you kind of an example or an insight into where we're at in the market, at least for proof generation costs. The demand for proof generation is pretty inelastic.
01:10:43.700 - 01:11:37.588, Speaker A: And again this comes back to what are the ZK applications that are consuming proof generation today? And you pretty much have ZK roll ups and then you have attestation protocols. There's a long tail, but these are sort of the primary drivers of proof generation demand. And if you look at attestations, attestations are pretty cheap to generate proofs for, right. They're very small computations. So as you improve proof generation costs, you actually don't see the demand curve reacting in any meaningful way. Where you do see sort of demand supply dynamics is with proof verification for attestations, right. And that we can aggregate many of these together and amortize our cost.
01:11:37.588 - 01:12:40.680, Speaker A: And then if you look at ZK roll ups, these are, as to Uma's point, predominantly their cost structure is comprised of DA and proof generation is a fraction, if that. So again, proof generation cost reductions don't actually play much of a role in increasing demand what proof generation cost reductions do play a role in is sort of the design space for ZK applications, right. And making existing systems more trustless. And I think this is where we ultimately see proof generation cost reductions playing an impact. Yeah, I think in this panel we will agree more or less with the same things constantly I'm on the same page. The only thing that I will add is that it's true, it's already really cheap to generate proofs and it's going to get cheaper. Like it's a matter of time.
01:12:40.680 - 01:13:28.250, Speaker A: So taking that into account, then the next thing we should ask ourselves is, okay, what exactly is not going to get cheaper? And it's going to get probably more expensive, and that's a verifier that in most of these trustless applications is ethereum. So that's why our focus is there. And yeah, I think now that with more education and also something that was the bottleneck for many of these CK applications was the tooling that's getting way better. I still don't understand to Zuma's point, like why we're still using a lot of this optimistic infrastructure. I think it's also kind of an economic incentive. We are using old technology because we need to justify maybe high valuations. I don't know.
01:13:28.250 - 01:14:54.356, Speaker A: And yeah, I'm excited to see many teams working and building more stuff, especially because the proof generation costs are going lower and I think the verifier Ethereum will be able to make it also cheaper. I want to see more applications that are not related to blockchain and we couldn't use them to start using CK and be trustless. New trustless applications like CK, peer to peer, for example, these type of systems that it will be easier to build because we have better tools and better vms. So yeah, I think it's a myth that is quite expensive, and in the next one or two years it will be extremely obvious. One thing that often confuses people is that when somebody says that ZK is expensive, they're often not speaking economically, they're speaking temporarily of saying, oh, this proof takes a while to generate, but obviously that's like an absolute statement as to something that's relative, right? Some computations are very fast and some computations take a long time, right? So naturally the same thing is going to be true for ZK. Some proofs are going to be quick to generate and some are going to take a longer period of time. But over time, as all these systems get built out and as sort of ZK cryptography improves both the cost of proof generation is going to go down and the sort of temporal cost of generating the proof is going to go down.
01:14:54.356 - 01:15:53.348, Speaker A: And so I think a lot of these issues that people raise are actually not going to be problems in the long term. Okay, maybe my final questions before I give the Q a time for the audience. My final question is more about using scenario. A lot of narrative happens, coprocessors. And when we look at the ZK space, the use case is mainly about scalability, like privacy, like ZK, Oracle, like light clients breach this kind of stuff. And in the next three to five years, I assume that you also have a lot of discussion from the potential clients or customers which kind of use case you think have more potential upside to get mass adoption. Yeah, I think all the things you named roll ups, bridges, coprocessors, oracles, attestation services are very exciting.
01:15:53.348 - 01:16:46.264, Speaker A: I guess to me personally, I think we've kind of done a bunch of different applications, including bridging, coprocessors, oracles, and now especially with SB on rath roll ups. I think one thing that's really exciting is with a general purpose CKVM, where anyone can write rust and get their zero knowledge proof, you can just enable all of these applications and more. And the bottleneck is no longer like having an advanced team with cryptographic expertise having to handwrite all these circuits for all these different use cases. I think finally with the ZKVM, that's kind of when all of these applications can go mainstream. And then also people can experiment with even more applications beyond just the ones you highlighted. So for me that's really exciting. It finally feels like the infrastructure for ZK, especially with zkvms, is like here.
01:16:46.264 - 01:17:27.696, Speaker A: They're easy to like. People can experiment a lot more and it's not bottlenecked by our expertise on our team or expertise on, for example, Neil's team. That's very specialized. It's like truly any developer can apply their creativity and have ZK in their app. And so I'm excited for all of the above use cases, I think roll ups are a very obvious one. Coprocessor, obviously really cool, but now it's just going to hopefully 1000 x the number of developers that can experiment with stuff. Yeah, I think the two largest ZK applications today, again in terms of proof generation, are roll ups and attestations.
01:17:27.696 - 01:18:25.592, Speaker A: I think because both of those have found some product market fit on a relative basis, those will likely persist and continue to grow. ZK roll ups obviously sort of the end game state for scaling and attestations, in particular, private attestations for private personal information I think make a lot of natural sense. There's a lot of synergy there with the use of zero knowledge proofs and privacy around identity. I'm also really excited about what RJ had mentioned in his last comment about ZKP to p, right. In gluing sort of existing infrastructure together using zero knowledge proofs to attach to the correctness of the computation of these systems. And this goes far beyond just blockchains. Right.
01:18:25.592 - 01:18:59.860, Speaker A: And I think ZKPDP actually started as like a venmo, or it was a venmo on and off ramp, but you can extend this to Venmo, to swift or to zelle. Right. And it doesn't have to be necessarily on chain. So I'm excited for these three categories. Yeah, we'll say the same. Roll ups and bridges are the clear. They are going to be the first users, they're going to use more CK.
01:18:59.860 - 01:19:29.420, Speaker A: And the other one is these CK trustless applications that we haven't seen. And we are going to see more. I want to see more experimentation. And also it would probably be easier to write a CK application because you just use rust than actually writing a smart contract in solidity. That's something that I like personally. I think that will be better. And this will mean that we will have even for example, games.
01:19:29.420 - 01:20:13.150, Speaker A: We are experimenting a lot where we want to include, for example for a line, the probable doom that Rick zero did. And we want to see people use CK everywhere. And I think that will happen. And yeah, the main thing is not the proverb. We believe that in order for that to happen, you will need a trustless layer and for that you need cheap verification. But yeah, I think it's quite exciting what's going to happen the next couple of years, especially considering that we live in a world that is very complicated and probably multipolar, and trustlessness is going to be extremely important for any type of system, not only money. So yeah, we want to see more things build there.
01:20:13.150 - 01:21:02.536, Speaker A: Yeah, those are all good examples. Just to add to that, maybe I'll answer a bit in the abstract. You can think of a computation as taking some time x to run, and then to do that computation in Zk takes some like f of x, right. Because there's some overhead over the baseline computation. And as ZK gets better and better, sort of f of x is sort of less and less of an overhead relative to x. And so I think the exciting question for people in Zk to ask is how do people start using Zk as f of x approaches x? Because you can imagine that in a world where f of x equals x, people would use ZK for pretty much every computation, because it's just like a free benefit, right? You can just prove that the computation was done correctly. And so these are all examples of things as f of x approaches x.
01:21:02.536 - 01:21:39.604, Speaker A: But as it continues to move along that trend, what additional things will we see? I think is the exciting thing for us to discover in the years ahead. Okay, thanks for insights. We still have around five minutes for Q-A-I mean, two questions. Hello, thanks a lot for really inspiring talks. We at bonus technologies are actually building ZK friendly execution environments. We could call it like this. And what we discovered is that actually there is no end of the game when it comes to cost of ZK proof execution.
01:21:39.604 - 01:22:28.968, Speaker A: Of course, if we speak about about few cents, it looks like negligible. But let's say in the course of this year, Ethereum DA thanks to protodank sharding, and sharding itself will be decreased for order of magnitude at least, which will put the a cost. And by the way, there are many other projects like Celestia availance. It's expected that VA cost will rapidly drop like for few orders of magnitude, which will actually be almost similar or the same as the cost of ZK proof execution. Of course, it depends what kind of ZK protocol and so on and so on. My question would be related on that. What would happen if for instance, for a real production use case, let's say gaming or whatever, you have millions of required ZK proofs per day.
01:22:28.968 - 01:23:55.948, Speaker A: In that case, ZK proving cost becomes not negligible, but basically the cheaper. I mean, there is no cheap as can be, the cheaper the better. So that's one of things that I think in future we will anyway face. And I would like to add one more point on this, that I think that interoperability between zks and convergence to two or three or ten ZK protocols will be at the level mostly of using, finally using standardized hash functions, elliptic curves or arithmetic fields. I think that this is something around what is going to be centered both ZK protocols on your side and ZK computation infrastructures that we are building on our side. Just my comment that I think we should all work together as there is no silver bullet and there is no cheap enough ZK computation might be for this use case of today, but let's say in a year or ten years from now, adoption would require really some kind of coupling between ZK protocols and infrastructure, as for instance happens with IA today. So artificial intelligence has a number of infrastructure solutions that are supporting this computation, and there is always and always new and new and new solution on both sides.
01:23:55.948 - 01:25:19.796, Speaker A: So I think that the only way is to work, to couple those efforts and work together and might be interesting, for instance, to know what kind of ZK protocols some of you is using and what is exactly cost now at some kind of, I don't know, whatever solution, probably some mainstream cloud you're using for that. Because what we heard is a kind of theoretical calculation for very unrealistic case on spot market when it comes to calculation of specific ZK protocol. So some real example and real cost would be really interesting to hear because we have our own calculations that are a little bit different. Well, we should have invited you up on stage. I think you make a really valid comment here about what are the correct me if I'm wrong, but what are the practical costs for. Frankly, it's kind of an impossible question to answer without parameterizing what kind of computations you're looking to prove. I think there are always going to be certain computations that just do not fit well with ZK.
01:25:19.796 - 01:26:14.410, Speaker A: Right. You see that for example, most ZKE evms, they opt out ketchak hash functions for Poseidon. Right? And I think there'll probably always be these sorts of trade offs that as an application developer, you'll need to be aware to a certain degree of what those trade offs are and implement them as you're designing your application. I think it's on us and developer tooling and infrastructure teams to make that really easy. Right. To alarm you whether or not you're using a computation that's going to blow up your proof generation costs. But I think to defend kind of like the state of ZK, if you design your application to be friendly with these existing proof systems, which I'm empathetic, is oftentimes difficult.
01:26:14.410 - 01:27:09.208, Speaker A: It is very efficient. It is there. So yeah, at some point we'd love to know more about your struggles. Any more comments? No. The last thing I would add is I believe at some point some of these things that you will be able to prove any type of computation. In my opinion, it's just a matter of we will see many provers for different type of computations or if we will see one, to rule them all. We have seen, for example, with the hash functions, the team of Ulbetana, they released a proven system called Venus and allows you to use normal hash functions that are not ck friendly and I think at some point, we will see more standardized or normal computation being done with CK without having to decide about these trade offs.
01:27:09.208 - 01:27:56.300, Speaker A: But maybe the trade offs are not around the computation that you do, but basically Ethereum, for example, if you want to use proven system in particular, the proofs are huge. So I think that's where the future. Yeah, that's all. Yeah. I think another thing I want to point out is, for example, I used to do a lot of machine learning, and even four years ago, it was totally unthinkable that you would have a model as big as GPT four. We were doing models that were, like, in the billions of parameters, and GPT four has maybe hundreds of billions, if not more parameters. And I think I view ZK in a very similar way with the trajectory of where it's going and how fast costs are reducing.
01:27:56.300 - 01:28:41.720, Speaker A: And so I remain very optimistic that especially with things like hardware, especially with things like zkvms, as there becomes more demand, there will become more specialized hardware to make the supply side go down and just make crazier and crazier computations in ZK possible. Yeah. If no more comments, that's the end for this panel. Thank you, guys. Thank you. Our first presentation here on the main stage showcases PI Squared's innovative approach to universal verifiable computing. Please welcome Gregory Rasho, CEO of PI Squared, as he shares insights into the proof of proof technology and its potential impact on the blockchain landscape.
01:28:41.720 - 01:29:42.550, Speaker A: Please welcome Gregory. All right, thank you. I am Gregory. I am a professor of computer science at UIUC, also the founder and CEO of ranchive verification and PI squared. And today I'll talk about PI squared, which comes from proof of proof ZK, proof of mathematical proof, which we believe will serve as a foundation for what we call universal verifiable computing. Before I dive into details, I'd like to tell you the main theme of my presentation today, which is that I believe that we are close to having a paradigm shift in verifiable computing, where correctness is paramount and also universality. We call it verifiable computing.
01:29:42.550 - 01:30:17.842, Speaker A: 20. Let me first tell you a few words about verifiable computing 1.0, the current state of the art in verifiable computing and why we believe we need to do something about it. So, first of all, it is a very fragmented space that lacks interoperability. We have lots of different programming languages, lots of different virtual machines, and VM specific ZK circuits. And all this leads to a huge trust base. There is literally millions of lines of code that we have to blindly trust in order to claim correctness of computation.
01:30:17.842 - 01:30:57.470, Speaker A: So to claim correctness, we have to trust millions of lines of code in compilers, in vms, in CK circuits. Maintenance is very hard because all these vms and languages evolve. Any VM language or programming language worth its salt evolves every few months. So we spend a lot of time to implement a ZK VM for a particular vm, and then we got two or three new versions in the meanwhile. So if we wonder about correctness, then that's a lost cause. We should forget correctness. Nobody can formally verify millions of lines of code, period.
01:30:57.470 - 01:32:02.550, Speaker A: That's well known. It's very hard to verify such large systems. What we propose in verifiable computing 20 is to actually take the several decades of work in formal semantics, which unfortunately is mostly ignored by verifiable computing 1.0. So I want to take all this and make it a central piece of verifiable computing 2.0, the core of it in a Nutshell. Why do we want to have formal semantics? What do formal semantics do for us? Well, simplistically speaking, they allow us to regard computations as mathematical proofs, rigorous mathematical proofs. And now we know how to deal with mathematical proofs the same way TCP IP as a data integrity protocol is at the core of the Internet and enabled innovation above and below because it assures the data integrity of the Internet.
01:32:02.550 - 01:32:42.786, Speaker A: The same way we believe that integrity of mathematical proofs should be the centerpiece of verifiable computing 20, which will enable innovation above and below. Above. Programming languages, virtual machines. We can invent them as many as we need, even domain specific, application specific languages. Once we have a formal semantics for them, we have a mathematical definition for the programming language, then that produces a mathematical proof whenever we execute programs. And now we only need to worry about mathematical proofs. We have to check the integrity of the mathematical proofs.
01:32:42.786 - 01:33:22.414, Speaker A: And now we check the integrity of the computation without having to worry at all about implementing a Zk circuit or any complicated artifact for that particular programming language or VM. And at the same time, underneath the proof checking integrity, we can have all the universe of ZK circuits and ZK vms around. You don't have to implement a Zk circuit specific to a particular VM or a particular programming language. We implement it for mathematics. So that's the vision. And we believe that we enable verifiable computing 2.0. This is not only a dream.
01:33:22.414 - 01:34:15.150, Speaker A: We started working on this about three years ago well, been working for more than 20 years on programming language semantics, but the connection with ZK is relatively new, only two, three years. So let's look at this picture. Look at the top where we have the crossed arrows. ZK proof of math proof, right? So imagine a vertical bar to the middle. And now look at the left, where we have programming language semantic, the programming language semantics world, and to the right, where we have the Zk circuitry world. To the left, we use programming language framework, like the K framework, which takes a programming language as input and gives you tools for that programming language, in particular, interpreters for that programming language that produce mathematical proofs in addition to the results. So they are like any other interpreter.
01:34:15.150 - 01:34:56.734, Speaker A: You can execute programs with a semantics driven, semantics generated interpreter, works exactly like any interpreter. It's even faster than many handwritten interpreters, but not only gives you a result, but also it gives you a mathematical proof why that result is correct. And when I say mathematical proof, I mean traditional mathematical proof, step by step, like we learned in school. Not a cryptography proof, a mathematical proof. Why that result is correct according to the mathematical definition of the programming language. You see what I'm getting to? This is what we mean by correctness, correct by construction. There is nothing to formally verify here.
01:34:56.734 - 01:35:46.750, Speaker A: I just have to verify the mathematical proof, which is done now in the second stage with Zk circuits. So before we even go into ZK, notice that there are many implementations of proof checkers. Mathematical proof checkers. And the beauty of detailed mathematical proofs is that they are very easy to verify with very simple mathematical proofs. Mathematical proof checkers. Just to give you an idea, the smallest proof checker that we have for these mathematical proofs produced by the K framework, in this case, the smallest of them has only 200 lines of code. Okay, so 200 lines of code, which, of course, public code that you can use to verify any mathematical proof of any program in any programming language.
01:35:46.750 - 01:36:18.060, Speaker A: There's a trust base from millions of lines of code to 200 lines of code. Okay? Now the next step is to implement these 200 lines of code, this proof checker, using Zk technology. All right? And if we do that, then what we get is the same result in the verifiable Computing 1.0, namely you. Oops. What? So you haven't seen the slides for all this time. All right, we need some help here.
01:36:18.060 - 01:36:50.210, Speaker A: Can somebody connect the slides? They're on the outside screens. Oh, I see. Not on this one. Okay, that works for me better, but this doesn't work. All right, good. So what I wanted to say is that what we get in the end? Now I get it why nobody looked at me and you guys looked on the site. So what we get is the same final result as invarifiable computing 1.0.
01:36:50.210 - 01:37:37.150, Speaker A: You start with a program, sorry, you start with a program in some programming language, you execute it and you get a ZK proof in the end. But that ZK proof now is a proof of a mathematical proof of the claim that has been made. So it serves the same purpose. It still justifies the correctness, the test for the correctness of the claim, but using the same pipeline for all programming languages. You see there is nothing specific to EVM or RISC five or WASM or any, or Python or Javascript or any language here. Right? So that is the big picture and our current implementation of it. And now let's go into the implementation.
01:37:37.150 - 01:38:14.666, Speaker A: But first let me say a few words about the baseline. So what we've done was to start with some programs in two domains where we believe PI squared will have an impact in the future. Blockchain and AIML. So we took two programs of each, an ERC 20 transfer program and a batch transfer where we use the same program. We run it 5000 times to do 5000 transactions. And from AIML we took two very simple models, the perceptron and SVM model. And we implemented all this in the existing, the major existing zkvms.
01:38:14.666 - 01:38:41.734, Speaker A: And I would like to express right here, right now, my gratitude to the teams of RISC, zero, ZkllvM, Lurk and Cairo. They were extremely helpful once we explained them what we want to do. They were very enthusiastic and they said let's do it. Let's implement the best possible proof checker using our ZKVM. So these are the numbers that we got. These are one month old. I'm sure now they are better and I'm sure if they implemented this for us they will be even better.
01:38:41.734 - 01:39:15.762, Speaker A: But just to get a baseline, they are currently between, as you can see, between 0.6 seconds and two, 3 seconds. The best transfer is a bit heavier, but overall we are in the seconds range. Again, what happens here? We took these programs written in a high level language, python or rust. Then we compiled into the zkvms and then we ran them with the zkvms that got the ZK pros. So that's how we got these numbers. And again, it's just a table with numbers.
01:39:15.762 - 01:40:05.054, Speaker A: But this took months of work, lots of people to make them work and get the best possible results. All right, so nothing specific to PI squared yet. That's just the baseline, what you expect in the best possible scenario, if you implement the proof checker using the existing zkvms. Right? So the next step is to implement the right side of this picture, the 200 lines of code proof checker in the existing zkvms, to see what we get. Right. So now we can use zkvms, existing zkvms, to verify computation in any programming language without a compiler in between, necessarily. Okay, so this is what we've got initially, we started about six months ago.
01:40:05.054 - 01:40:34.390, Speaker A: Initially, complete disappointment. We couldn't terminate any proof because our mathematical proofs were horrible, huge, unstructured. We just got something, and then we expected magic from the zkvms. So then we sat down and said, okay, it's time to rethink proof engineering. We have to generate better mathematical proofs. And the beauty of mathematics is that you can always engineer mathematical proofs, make them smaller, more structured. And we did all that algorithmically, not by hand.
01:40:34.390 - 01:41:13.406, Speaker A: So by the end of. In a couple of weeks, we managed to finally get some numbers, like half an hour. It was like a big success. Then one month later, after a bit more proof engineering, mathematical proof engineering, we got to the order of we made it twice faster, but still far right from the existing approaches. And that's not unexpected, because now you have two levels of indirection, right? You have an interpreter of an interpreter in some sense, going on. So then we decided to change everything to start optimizing a little bit, not too much. And we got to much better numbers, almost an order of magnitude faster.
01:41:13.406 - 01:42:08.990, Speaker A: And now with the GPU, it gets pretty reasonable, right? Within less than a minute. And again, this is just a few months of work using the existing ZK technology to implement the proof checker. And again, I'd like to express my kudos to all the ZKVM teams who are very helpful in this process. All right, no interesting optimizations or improvements so far, just a plain mathematical proof verified with a plain ZKVM. What else can we do to make it faster? Right? Because speed is the problem, the big elephant in the room, performance. How can we make it faster? So here are three optimizations that we are currently working on. The first one, and the most obvious one, is let's implement a direct circuit for the proof checker.
01:42:08.990 - 01:43:06.054, Speaker A: Because the zkvms pay a price. They are general, they work with all possible programs, but we need only one program here, the proof checker. Right? And we have some experiments, and we get an order of magnitude performance only there, then mathematical proofs have an interesting property different from programs, which is that a mathematical proof is correct if and only if every single step in the mathematical proof is correct. Obviously. But what is nice about is that you can verify every single step completely independently and in parallel with all the other steps, and we are not exploiting that thing at all currently. So the next step is to make it even more parallel. And finally, there are conventional proof search techniques where we can do sybolic execution of basic blocks and compress the semantics of large portions of the program into just one step.
01:43:06.054 - 01:43:55.670, Speaker A: It's like having a big mathematical proof and compressing it into lemmas and using lemmas to do proofs. And then we get much smaller proofs actually, and we did experiments that will bring another one or two orders of magnitude in performance. So we expect at least three orders of magnitude increasing performance over the next few months, hopefully. And finally, and most importantly, I think for verifiable computing 2.0 is that formal verification, which we are very scared of. But formal verification will be the ultimate optimization that will make things faster, a lot faster. Okay, why? Because when you formally verify a program, you eliminate the necessity for iterations for loops.
01:43:55.670 - 01:44:31.922, Speaker A: So now you get mathematical proof to childlinear in the size of the program, as opposed to linear in the size of the execution of the program. And this basically makes know sky is the limit. Now you can make it really fast if you formally verify your program. So developers will be incentivized to formally verify their program. This function in Uniswap, we formally verified Uniswap, actually the runtime verification. But once you formally verify this, you can use that to make Zk proving even faster. And with that, let me dream a bit.
01:44:31.922 - 01:45:05.486, Speaker A: So everything I showed so far is concrete. Let me dream a bit how I believe, or we believe that blockchains of the future could be if we power them using PI squared. Programming languages should not be hardwired or predetermined in the programming language. In the blockchain. You should not have a blockchain on EVM blockchain, on a Wasm blockchain, or a python blockchain, or a rust blockchain. Programming languages can be data like anything else on the chain. Programming languages are formal semantics, and the formal semantics are publicly available.
01:45:05.486 - 01:45:46.214, Speaker A: They will be on chain publicly available now. Transactions will have to refer to a particular programming language, actually even version of a particular programming language, because the same program can give you different results depending on different versions of the language. Okay. And now these programs will be executed once and for all. A proof of proof will be generated, a ZK proof, ultimately that attests for the correctness of the transaction. And now the transaction can be settled universally on the blockchain without ever executing the program more than once, and correct by construction. You see, you don't have to trust compilers, you don't have to trust interpreters.
01:45:46.214 - 01:46:45.090, Speaker A: You simply verify the mathematical proof. That's it. And with this I'd like to conclude, and maybe take a question or two, we believe that verifiable computing is the future, and we have a unique opportunity to do it right at this point. And by right we mean to be universal and correct by construction. All right, any question? Three more minutes. Yes, with a mic or yell what? Go back more. Can you come closer? Come closer.
01:46:45.090 - 01:47:14.590, Speaker A: The support vector machine. It's a very small machine learning model, nothing deep. It's just a textbook example. Yes, but that was not our goal. Yes, we can use it for fine tuning, but that was not our goal. Our goal was simply to generate a ZK proof for an ML influencer. That was it.
01:47:14.590 - 01:48:24.648, Speaker A: How much cost, in the sense of cost of ZK proof generation? Well, as you can see, 28 seconds in the latest version, but horribly, initially. All right, 1 minute and a half left. Yes, please. Thank you. Just really quickly, this is verifiable compute without the possibility of privacy, or can we have privacy as well? Sorry, can you speak closer? Does this allow for privacy or is it just straight up verifiable compute? So we believe privacy can be done at the level of mathematical proofs. It depends on how you generate the mathematical proof. For example, if you don't want to reveal your age, but only that it's larger than you can prove, the mathematical property that exists, an age such that this happens, and you get that mathematical proof from the execution of the program.
01:48:24.648 - 01:49:07.880, Speaker A: But now the way you encode it in the formula and you store it on chain, you see? So here you'll have to, I cannot point. But that blue box is a mathematical claim that everybody can see. There's a claim that there exists an age larger than 21, such debt, and you get approved for that, and now that is decayed and you don't see the actual mathematical proof, so you cannot extract the information. That's a very good question. So we believe that's how it should be done. We believe that separation of concerns is golden. So we should separate the concerns of mathematical reasoning as we know it from the ZK reasoning and use the ZK to implement verifiers for mathematics.
01:49:07.880 - 01:49:40.970, Speaker A: All right, zero blinking. So I'm done. Thank you. Returning to the stage to discuss horizontal scaling, please welcome Avi Zerlo, chief product officer at Nil foundation. Guess who's back. Back again. I promised this is the last time I'll be on stage today.
01:49:40.970 - 01:51:05.360, Speaker A: Today we're going to talk about horizontal scaling primarily. Bit of storytelling and some background, the motivation behind what we're doing at nil. I'm just going to wait until they get my notes up here on the screen. Those are the wrong notes. Okay, we're good. Welcome everyone. Thanks again.
01:51:05.360 - 01:51:37.334, Speaker A: You probably know me by now. This is my third time on stage and thankfully my last. I'm Avi. I'm the chief product officer at Nil foundation. Today I'm going to be talking about horizontal scaling for Ethereum and pretty much laying the foundation for the motivation of what we're doing at nil this year. And by the end of this presentation, hopefully you get an understanding of why we're doing it. So we're talking about scaling for Ethereum.
01:51:37.334 - 01:52:28.966, Speaker A: So the natural starting point here is Ethereum's rollup centric roadmap, right? And after spending a lot of resources researching execution sharding, Ethereum opted for a rollup centric roadmap. And this has served the needs for users and developers quite well. Roll ups have done well. Transaction costs are about an order of magnitude cheaper on layer two s today than they are on l one. Faster. They're faster transaction times in a practical setting, thanks to the soft finality, guarantees that a roll up can offer end users. I know this says okay job, but they've actually done a very good job at rolling out pretty much state of the art cryptography and proof mechanisms to maintain security of these systems.
01:52:28.966 - 01:53:46.130, Speaker A: And by the grace of l two beat and together, the Ethereum l two ecosystem has become home for application design innovation. But l two s are not necessarily perfect. They have problems of their own. First, transaction fees are still too high, and there's going to be quite a few memes, so bear with me. High load and price sensitive applications are not viable on general purpose roll ups today. And while people often think about lower transaction fees as sort of a terminal value for end users, which is absolutely true, they're also incredibly important for moving more computation and data processing onto trustless systems, and these ultimately lead to more robust systems. Second, we have a bit of a paradox in that Ethereum, a maximally decentralized, censorship resistant, permissionless blockchain which has spearheaded those values across crypto for years has effectively opted for roll up architectures that run under single operator models.
01:53:46.130 - 01:55:10.400, Speaker A: And while many roll ups are working towards promoting these values, these core values, from a community perspective, frankly fewer are actually substantiating that with engineering and design decisions. And without the engineering to substantiate it, l two s potentially pose a risk to compromising these values for Ethereum at large. Third, we live in a world of overabundance, which is ironic because I'm up here about to tell you about another layer two, but there is so many choices for an application developer to make before simply deploying their application. And this in of it itself has become a problem. And in modularity in spirit was supposed to simplify the developer experience, and instead we've overcomplicated it with a number of decisions that a developer needs to make before even getting to their application. And finally, and most importantly, and this is a problem many of you have probably heard already, but l two s, metaphorically speaking, are bleeding out over this state fragmentation problem. And this is where I'm going to focus the talk today.
01:55:10.400 - 01:56:30.966, Speaker A: So if we look closely at the consequences of state fragmentation, which again may be redundant for some of you, but it compromises the network effects of global state. And this is in terms of liquidity, user distribution and mindshare, who's focused on building on a global notion of state. We have compounding complexity of interoperability standards, and this is for heterogeneous interoperability and homogeneous in terms of execution environments. And we have this huge global coordination problem that we actually have to come to an agreement on what are the standards if we want interoperability between these roll ups across thousands of stakeholders. And at the end of the day, we objectively have a worse developer and user experience, and we're talking about Ethereum scaling here. But I think there's a lot to be learned from this sort of monolithic integrated l ones in delivering a really seamless, pleasant, easy to use experience for developers who are just looking to deploy an application on trustless infrastructure. And unfortunately, these problems are getting worse by the day.
01:56:30.966 - 01:58:00.740, Speaker A: So, limited by existing infrastructure, price sensitive applications, typically high load applications, are forced onto app specific infrastructure. And as we head into this next cycle, which is hopefully here to stay, congestion fees are only going to go up, which is only going to force more applications onto their own app specific infrastructure, which is only going to make this state fragmentation problem more pervasive. And so all things considered here, I think we really need to ask ourselves what is going on with state fragmentation how have we ended up here? And to revisit our Monty Python meme from just a few moments ago, state fragmentation has fundamentally been miscategorized as an interoperability problem. It's not an interoperability problem, it is a scaling problem. The more scalable a layer two can be, the less state fragmentation you impose on the ecosystem. And this is sort of our starting insight at nil. So what do we do? Well, we scale, and there are two primary frameworks that you can adopt to scale a blockchain or any database, or any data intensive application for that matter.
01:58:00.740 - 01:59:25.946, Speaker A: One is to vertically scale, and this is effectively boosting the hardware utilization or the hardware requirements that a given node actually runs on. And we can think of paralyzed virtual machines or introducing sort of more powerful hardware to that node under this vertical scaling framework. The second option, which I think everybody who was here earlier today for the future of rollups panel heard people talk about, is horizontal scaling. And horizontal scaling gives this really nice property of being able to simply add nodes, add a new machines to the system, distribute the load over those nodes, and effectively increase the scale of the system of interest. And this is most commonly done in traditional systems with sharding, and there's actually a few examples of it in blockchains as well. To dig a little bit deeper into what are some of the trade offs of either approach. Vertical scaling is a pretty easy fix in that you can very quickly upgrade and boost the hardware of a given system by simply purchasing a new piece of hardware.
01:59:25.946 - 02:00:23.966, Speaker A: It scales with the hardware, which has followed Moore's law for a number of years. And it's relatively easy to upgrade in a permission setting. The trade off is that you're actually limited by hardware, or rather you're limited by the capacity of a single piece of hardware. And in sort of distributed systems, this typically trends towards centralization, right, because you have higher hardware requirements. Alternatively, you can scale with horizontal scaling, and this has a great benefit of keeping hardware requirements relatively low, and thus you're able to promote decentralization in these distributed systems. You have actually higher theoretical scaling limits. And just intuitively, you can think of two of the world's most powerful computers if they're not even operating at full capacity.
02:00:23.966 - 02:01:47.340, Speaker A: Those two working together are more powerful than a single one, and it actually still allows for vertical optimizations to take place, right, for paralyzed virtual machines, or increasing the hardware requirements where necessary. And the big trade off is complexity. These horizontally scalable solutions are quite complex. There's networking overheads you have to solve for data validity which is a big open problem for blockchains, and you have to bootstrap a network of nodes, which is typically more expensive. And so when we come back to this roll up centric sort of view with these scaling frameworks in mind, and I recognize this is maybe a little controversial, but roll ups haven't really scaled in that they haven't taken advantage of vertical scaling techniques yet. And there are a few teams that I think I'll mention later on in a few slides that are actually implementing things like paralyzed virtual machines and taking advantage of more powerful hardware. And when we consider that all blockchains are sort of defined by the ledger that they maintain, and that today a smart contract deployed on a roll up can arbitrarily call another smart contract on a different roll up.
02:01:47.340 - 02:02:35.830, Speaker A: Unequivocally, roll ups do not horizontally scale. And this is sort of a current view of the landscape, but there are obviously solutions. Sorry. So there's two starting points. We can take existing roll ups and merge them into a single system. And there's a lot of work being done towards these efforts. And I think this slide is probably, there's too small, you may not be able to read, but.
02:02:35.830 - 02:03:39.430, Speaker A: Okay, I'm losing my notes. But in essence, these solutions introduce a middle layer to what are existing roll up execution environments. And this middle layer offers some sort of shared consensus guarantee that effectively allows you to facilitate cross roll up communication. And broadly speaking, if we can pull this off and we can identify interoperability standards to actually allow certain message passing through these roll up nodes, then we effectively have horizontal scalability. Right. And again, I'm drastically oversimplifying how complicated these solutions are. I definitely don't want to take away of the work that's being put into them, but they do generally follow this architecture and they don't come without trade offs.
02:03:39.430 - 02:04:44.380, Speaker A: So first, all of these systems, because they're essentially relying on this middle layer for some sort of consensus guarantee, which is effectively soft finality, you have this weakest chain vulnerability in that these middle layers aren't actually checking the data execution or the validity of data execution. And this is primarily today a function of we don't have real time zero knowledge proofs. In a world where we have real time zero knowledge proofs, we can generate those proofs in parallel along the side of the computation, and then very quickly send it to whatever this middle layer is. And this middle layer can check whether or not a roll up node was executed, their transactions properly, or sequences their transactions properly, or they're not withholding data, et cetera. But today we do not have this. Right. So what ends up happening is you have these permissioned sets, you have to permission these sets, these middle layers to roll up nodes that you can trust.
02:04:44.380 - 02:06:31.286, Speaker A: Second, we still haven't fundamentally solved anything at the developer interface level, right? These middle layers are essentially glue that sit behind these developer environments, these execution environments. They sit behind where liquidity lives. They sit behind where user addresses live. They sit behind where actually developers deploy their smart contracts. And so we still have this sort of very fragmented developer interface. And then third, and I mentioned this a little bit earlier, but this is truly, maybe it's not necessarily an engineering challenge as much as it is a social challenge, in that we have thousands of stakeholders who have their own set of incentives who are trying to decide on what these interoperability standards will be. And this starts at the very low level of what does the actual transaction formatting need to be in order to enable this cross chain messaging to how do we streamline roll up node upgrades such that whenever a roll up upgrades their node, that everybody else within this system is aware of the upgrade and the implications that it has? And so all that being said, what might it look like if we actually started fresh? What if we started from sort of very first principles approached of how do we enable more scale for layer twos? And we can revisit our friendly little diagram here where there's two frameworks to do this, vertical or horizontal scaling.
02:06:31.286 - 02:07:35.586, Speaker A: And I think there's a shout out here to eclipse and movement labs who are introducing paralyzed virtual machines that allow for vertical scaling to take place at the l two. And they're likely to hold very significant performance improvements for l two s that implement these virtual machines. But as we sort of mentioned before about sort of the benefits of horizontal versus vertical scaling, there's actually some really interesting properties that we would want at the l two that horizontal scaling gives us. And this is essentially why, coming to what we're doing at nil, this is how we came to ZK sharding, right? And Zk sharding is a provable sharding architecture. It brings the power of thousands of rollups to a single unified layer two and developer environment. And this helps us promote decentralization. It is a network, it's not a single operator l two.
02:07:35.586 - 02:08:10.170, Speaker A: We have much higher scaling limits than any single operator roll up model. And it still allows us to explore things like parallelized virtual machines. Right. And to push vertical optimizations where necessary. And cool thing we got going on is essentially a mullet right. We have this integrated front end where for a developer, you simply deploy a smart contract to nil. You don't have to think about internalizing any sort of application specific infrastructure.
02:08:10.170 - 02:09:31.640, Speaker A: You don't have to worry about congestion fees, because we handle all of the complexity of horizontal scalability in protocol. And on the back end, we utilize Ethereum for verifying correct state transitions at the local and global level, and we use it for data availability of our consensus shard. We also have sort of a fancy sharding algorithm, dynamic sharding. And this allows us to allocate resources, network resources, to in demand pieces of state, right, and essentially splitting and merging shards based on the load that they're receiving for any particular piece of state. And no shade at local fee markets, because those are great. Finally, and maybe most importantly, nil structurally allows for us to substantiate sort of our community values of decentralization, censorship, resistance, permissionlessness with engineering and design. And ultimately we believe that these values are imperative for any l two that's going to last for decades to come.
02:09:31.640 - 02:10:22.198, Speaker A: We're also very committed to the EVM, and I know there's been a lot of talk as of late about alternative virtual machines. And as I mentioned, there's some really great teams that are working on that. But the EVM works just fine. It is a fully turing complete instruction set. It does everything you would want it to do. It's just been moving quite slow, and we're very committed, I think, along with many other l two s, to improving the EVM, right, enabling parallelized virtual machines, adding opcodes and support for additional curves, or allowing for, in our case, async communication across two different EVM state machines. And with that, I've got 20 seconds left.
02:10:22.198 - 02:11:01.986, Speaker A: I'm sure you have many questions, or maybe you don't, but either way, go check out what misha, our CEO, and Elia is going to share in our workshop sort of co working space in the back. We're going to be diving a little bit deeper into sort of the architecture of ZK sharding. We also have a new spec, which we released our initial spec in November. We have a new one coming out in the coming days. And that is all. Thank you. We have a hands on workshop happening at 04:00 p.m.
02:11:01.986 - 02:11:33.150, Speaker A: By Misha Kamarov, CEO and co founder of Nil foundation, and Ilya Morazu, senior decentralized systems engineer at Nil foundation. If you're interested, feel free to make your way to the coworking lounge to join Misha and Ilya after that. Please join us back here for the last panel of the day. Zero day why ZK security is important. At 05:10 p.m. Now let's turn our attention to VPU architecture. Please welcome David Garrett, PhD, co founder and CTO of Fabric Cryptography.
02:11:33.150 - 02:12:00.896, Speaker A: Thank you so much. I don't know if I recognize that guy. Is that actually me? We'll get these slides set up here. Well, excited to be here today. We're going to give you a vision of where compute is going with cryptography. We're going to talk about the VPU. It's a general purpose cryptography compute paradigm.
02:12:00.896 - 02:13:17.198, Speaker A: And you're going to see some lessons we've seen from the company fabric, how we've learned through the AI world, through the semiconductor world and cryptography world, how to bring these all together and produce something that we think is going to be a game changer for the world of cryptography. What's interesting, the company's been around over a year now and really striving to rethink cryptography, how we can deploy a canvas for all the algorithms and all the things that cryptographers want to do. And, you know, conferences like this, you see how every single day there's a new proposal, there's all kinds of changes in cryptography and protocols, and we want to make sure that we have the hardware that's ready for that. Just one. Still waiting for the slides here. There we go. Well, again, we're going to talk about the VPU, the verifiable processing unit.
02:13:17.198 - 02:13:55.792, Speaker A: You've seen cpus, you've seen the fpgas, gpus. We're actually looking at a paradigm that we're going to create something really specifically for cryptography, yet it's going to be general purpose. And so the idea behind this fabric is the company we're working to scale zero knowledge proofs, and we'll deep dive into this GPU architecture. So we'll do next slide. Okay, thank you so much again. What we're doing with the VPU, it's custom silicon. So we're going to build a device ultrafast into improving encryption.
02:13:55.792 - 02:14:51.098, Speaker A: The idea behind this is we've built a crypto native instruction set. The idea is we're looking at the computing primitives for cryptography and not just building a hardened accelerator for any one particular protocol. The things we've learned through the AI world and about hardware and software is one of the most important things to deploying the best hardware is actually to have the best software story. So at the same time that we're developing the VPU, we've got the compiler and the SDK, we've got the tooling that's going to make it easy for anybody to deploy and use and actually explore the space of cryptography. And then as the company, we're building the compute cards and the hardware that's going to scale this up. So we have the custom silicon, we've rethought cryptography, and we're going to bring the boxes and compute cards to actually make this real. There we go.
02:14:51.098 - 02:15:15.990, Speaker A: So let's take a look at the compute paradigms you've seen. The CPU the last 50 years has been revolutionized by processors. General purpose. It's easy to go do kinds of software that you want to do, but it's really hard to scale that up. For cryptography, it's really not meant for the types of workloads that we see. More recently, another paradigm, the FPGA, the field programmable gate array is not really a cpu. It's something that allows you to build circuits.
02:15:15.990 - 02:15:55.294, Speaker A: It's really cheap to get going, but what happens is it runs out of gas. In the economics of scaling, you can build something quickly, but then if you want to scale that up, it's an enormous cost. The GPU is an interesting use case in 20 years ago, the graphics processor. It's really part of doing the heavy lifting that a CPU couldn't do by building a huge array of parallel data paths. And so you see the GPU a lot mentioned, but it's really not optimized for cryptography. It has a lot of things that are shifting towards AI. You're building floating point math and it's missing out the key things that we know for cryptography.
02:15:55.294 - 02:16:46.568, Speaker A: And so that's when we talk about this new domain, the VPU general purpose cryptography, optimized for prime field math. The goal of our design is to do this field math at the best rate possible and provide all the primitives that you're going to need. We'll do next. So why build this? So the idea is today, zero knowledge proofs are this amazing revolution in how we're going to take back our privacy and security of all of our information. But it's running on cpus and it's just not fast enough. So the idea, these apps, the use cases, are limited by the speed of what's available. The idea behind the VPU is I want you to envision what you could do with that if you had 1000 x to compute, give you a million times the resources out there to actually help you do that.
02:16:46.568 - 02:17:37.034, Speaker A: And really, we want to focus on building more than what you can see today. Think of how it could go in the future and revolutionize security and privacy. Really, the goal is for us to build a performant, flexible and scalable general purpose cryptography compute engine. And what we talk about is this is this blank canvas for you guys to do what you want with it next. So the idea, why do we talk about general purpose and scalability? The idea behind the VPU is it takes a lot of effort to build a silicon device. But once you've done that, the economies of scale in this fabless semiconductor world will allow us to roll out a device that can cover just about any protocol. We can look at over things as algorithms change.
02:17:37.034 - 02:18:21.900, Speaker A: When you compare that to where you would go with fixed function asics, you would pick a protocol, you'd fix that in. You build an accelerator, you pay that cost, and then you've got a limited lifetime, it's going to run out of room, and all of a sudden you have to throw that away and start the whole process over again. So the idea behind this general purpose is the way that the GPU really revolutionized AI. We want the VPU to do the same for cryptography. Next slide. So, our VPU architecture, the idea behind this is we want to support all of the types of things in cryptography, ZK starks, ZK snarks, msms, ntts fries hashing, all of these things are you're familiar with. We want to be good at all of those.
02:18:21.900 - 02:19:08.858, Speaker A: Even the constructs of fully homomorphic encryption are really going to change the composition of the data center. What we do with the VPU architecture, we have four main components. A big array of our compute tiles that allow you to do this prime field math efficiently, and the movement of data. We have a huge number of external memory interfaces that allow us to move large gigabyte tables around. We have an eight core risk five processor that allows us to be the brains of the system and do all kinds of neat things, like the workloads that don't fit on the tile. Witness generation would be a good example, and then PCIe interfaces that can move things through. What we look at is the VPU is just a chip, but what we do that is we deploy that on a compute card.
02:19:08.858 - 02:19:26.494, Speaker A: And so you can see the VPU 860. It's a gpu like form factor PCIe card. We actually can deploy three of these devices on one of these compute cards and scale up from there. We'll do next slide. So we looked at three lessons. I want to give you three lessons. Today.
02:19:26.494 - 02:19:57.722, Speaker A: We talk about AI and what we've seen in hardware and compute, and really three things stand out. We'll dive into those. It's not all just about the compute. So if I focus on prime field math only, what I'm missing is there's four other legs that actually compose the full cryptography workload and problem. So you can't just focus on compute. You got to minimize hardwiring. And the idea there's a concept of dark silicon is anytime I harden something on a device, and then that's not part of the proof that I'm working on.
02:19:57.722 - 02:20:36.310, Speaker A: It's actually dark silicon and its inefficiency. So we look at how to minimize hardwired features on the device and make sure it's small, modular instructions that give us the flexibility to eke as much efficiency out of our entire device as possible. And then lastly, what's important for the best hardware, you actually have to have the best software teams, and you've got to make it programmable and usable to enter in. If we have the most exotic chip and no one can program it, it goes nowhere. And so, actually, most of our company, after deploying silicon, is all about making it programmable. Let's go. Next slide.
02:20:36.310 - 02:21:27.634, Speaker A: So, lesson one, we said it's not just the compute. The idea is, inside this FC 1000 chip. I mean, we've built in all five legs of this pentagram, the compute 40 VPU tiles, parallel processing engines that can go run around and do prime field math. We've built in the memory, tons of memory on chip, tons of bandwidth off chip to be able to retrieve and move things around. We've got a high bandwidth network on chip, and that allows us to exchange information from all our compute engines. When you talk about, say, large ntts two to the 26, all of our tiles can participate in the problem and share that compute, and we can actually move data around with extreme high bandwidth. We've got our cpu eight cycle or eight core risk, five that's able to help us in other parts of the workload.
02:21:27.634 - 02:21:53.338, Speaker A: And of course, we have I o. We're able to move large amounts of data in and off our device. We'll go next slide. So here's an example. We'll talk about why compute is not just the compute. Local witness generation is actually one of these critical things that come in zero knowledge proofs. And you can see on the right hand side, it's a small footprint for a ZKP, the inputs and the function are actually quite compact.
02:21:53.338 - 02:22:37.200, Speaker A: But what you go through is a witness generation step, which generates huge amounts of data, which then ripples into the proof and the compute. So small input blown up into a large piece of data set with a large amount of compute, and then you release a proof at the end. So the idea with the architecture for the VPU is we can do all of the witness generation on device. The artifact is we are not limited by our PCIe bandwidth. We're not a huge bandwidth choke point if you have a computer doing a server doing witness generation and a proof engine somewhere else. And so again, this is one of these key things of hardware, software design. You can't just look at the compute problem, you've got to look at the full problem.
02:22:37.200 - 02:23:10.280, Speaker A: Let's skip two slides. There you go. Lesson number two, let's minimize hardwiring. And this is critical. If you look at the top, the completely fixed function aSIC, you can compose MSM and NTT engines and hashes. But if the mix, if the change in algorithms doesn't follow that ratio, you're in big trouble because your protocols change and you have pieces of your silicon that are not usable. You may even have fixed elliptic curves, you may have other things that it doesn't even work for, the changes you want in your prime field.
02:23:10.280 - 02:23:55.346, Speaker A: So the idea behind that is really we work on more programmability, small modular instructions instead of these complex big accelerators. And it avoids that concept of dark silicon. How do I make sure that my device is fully participating in all of the workload, regardless of how I change my protocols? We'll do next. So the VPU instruction set, the idea, we try and make it easy. We don't break these things up into very small 32 bit integers. We're moving around large instructions in the instruction set, like modulo vector 384 bit. We can do elliptic curve math very easily.
02:23:55.346 - 02:24:29.582, Speaker A: We can still switch the same exact engine and work on the Poseidon inner loops, the hashing, the starks of the world, modulo vector, Goldilocks primes. We have all the adds and subtracts in the vector space. And then finally, the NTT butterflies are just as easy for us because we can do the add, subtract butterflies and the rotations. And so you can see breaking it down into what cryptographers want, not building all of the other things that we don't need. Let's go. Next slide. Okay, the lesson three, let's make it programmable.
02:24:29.582 - 02:25:12.490, Speaker A: And it's an well understated problem that in the AI space, many of the early companies built great accelerators that had no ability for any customer to use it. We actually spend a lot of time making it programmable. And that's the lesson of general purpose cryptography, is this is one of the critical pieces. So if you look at the tool chain, we have a number of things that we do. So we have our kernel development kit, and that's a repo that you can actually take advantage and write kernels targeted for the VPU instructions. You can look at how you move data and how you actually run compute on all these 40 different tiles. And so that lets us take a lot of the blocks in crypto primitive protocols and build optimized versions of those kernels.
02:25:12.490 - 02:25:50.438, Speaker A: We do a lot of that work ourselves to make sure our architecture is in great shape. Our hardware team, software and cryptography are all working together. We're simulating, we're building these protocols, and we're co designing the hardest parts of cryptography to make sure we have the right balance. But it's not just writing kernels. What we end up doing is assembling a whole graph development kit. And the idea behind, if you go look at plunky two or plunky three, it really is assembling a lot of different things, and the graph is the best representation for that. So just like in the AI space, Tensorflow has made it easy for just about anyone to develop neural networks.
02:25:50.438 - 02:26:27.430, Speaker A: The idea behind the graph development kit is we can build large workloads, recompose those workloads, and build it in an easy way that uses kernels as big atomic units of compute, and dump that into our graph compiler, and we end up with workloads. And so there's a rust C Python front end. It makes it easy to develop graphs for cryptography. So do next. So finally, we'll enter in the workload. Now, how do we think about this at scale? Well, there's two different ways you can go. On the right hand side, we can take our compiled graph and it runs on our compute card.
02:26:27.430 - 02:27:17.000, Speaker A: So this is this three VPU, GPU, compute card. We have the fabric runtime, interprets those graphs and can get those workloads done. So just about anybody and a decentralized way, in a developer way, can actually load these kernels, run the graphs, and execute that on their hardware using our compute card. But what really gets exciting at scale is the exact same graph we end up in our data center, the ZX 100 chassis. And there's a cluster orchestrator, the same runtime is able to scale up those graphs to enormous workloads. And so, prover as a market, you can build data centers around this that are executing those graphs that just about anyone could develop. And so we're super excited to see this unified software infrastructure that makes this easy to use.
02:27:17.000 - 02:27:59.380, Speaker A: We'll go next slide. So again, I want to wrap that up. I mean, really, what's at stake. Fabric has been building these devices. We're ramping up both the hardware teams, the software teams, the cryptography teams, and the operation teams to cover all of the things we need to do to release this in the field. So in 2024, what you're actually going to see from us look for fabrics, announcements, basically the equivalent of a billion dollars of gpu compute is going to be deployed with our VPU servers. And so when we look at those advantages of how much better we're going to be at cryptography, it's really just night and day how much your capital dollars for scaling are going to go.
02:27:59.380 - 02:28:42.402, Speaker A: We're going to have the SDK releases. We've got the compiler, the SDK, and these kernel development modules that will make sure that this is available for many people to use and program. And then finally the fabric VPU cloud. I mean, we're going to have resources so that you can actually deploy proof as a market. So building these data centers, building the orchestrators that these graph workloads can be run, is super important to the odyssey of our company. Obviously, what is interesting from the engineering point of view, running all these teams, it's a long term game for us. And so what is so fascinating about this market and how we see this growing is learning, is one of the key things that we need to do.
02:28:42.402 - 02:29:40.840, Speaker A: So deployment feedback from our sdks people finding different ways to optimize kernels and using our hardware is really interesting to us. We're going to roll that into our second generation, and we're going to continue to improve the workloads so we can take advantage of technology, we can take advantage of better architecture, and as we grow the company, do the first gen deployments and get better and better at this. And the idea now we really want the hardware, software, crypto codesign is one of the big reasons we're here, reaching out to see how you guys will use these devices. And I really want you to think of this blank canvas. If I had this compute, and I'm removing the compute restrictions that you have today, what kind of applications, what kind of massive workloads are you going to be able to build and envision, and that's what we really want to bring to you with the VPU. So with that, I really want to thank you for your time, and we'll move on from there. Thank you so much.
02:29:40.840 - 02:31:20.002, Speaker A: You. Next up, please welcome Brian Retford, CEO of Risk Zero, for his presentation, 100 x scaling victories, challenges, and opportunities in general. Purpose ZK. I just want to check. All right, hello, everyone. Today I'm going to be talking about, I'm calling it ZK 100 x, but it's really just kind of a reflection on the progress that zero knowledge has made over the course of the past year, two years. And I think some opportunities that are available in the future to ZK as it scales up.
02:31:20.002 - 02:32:10.944, Speaker A: And I think we've seen some of the other speakers talk to that as well. Okay, so really quickly, how many people here feel like they understand zero knowledge cryptography? Raise your hand a little bit. Okay. I'm trying to decide at what point I stop explaining ZK to people in talks, but it's kind of important. So, modern ZK systems have these kind of four properties, at least as I define them. They're trustlessly verifiable, they're information limiting, they're irreversibly concise, and they're composable. So when we say that ZK is trustlessly verifiable, what we really mean is that there is this program that's actually a circuit.
02:32:10.944 - 02:33:41.718, Speaker A: It looks more like, structurally, it looks more like an actual bit of silicon than a program that you might think of in python, but it's encoded using these mathematical ZK proving schemes. But the key, I would say one of the most useful properties of ZK, especially in the blockchain context, is that this circuit win run may have some inputs, it may have an output. Anyone can verify that that circuit was run correctly and only rely on the cryptography. You don't need to rely on anyone else to tell you that this was done correctly. So the zero knowledge part of the zero knowledge name comes from this property that when you're talking about this circuit, it has inputs, it has outputs, and if it's a vm, it has a program. And you, as the prover, can choose to reveal none of the inputs or the program or the outputs, some of them part of them or all of them, and people are able to verify that you still ran the program correctly without revealing any of the information that the prover did not want to reveal. So ZK is irreversibly reduced.
02:33:41.718 - 02:34:52.778, Speaker A: Some people call this succinct or concise and generally what this means is it might take, maybe you compute PI to 7 billion digits and it takes two years. The amount of compute traces that would go on would fill terabytes of data. But the size of the proof is still either constant or some exponential logarithmic reduction in the amount of the size, basically, and complexity required to verify these things. So, modern ZK systems also are recursive and composable. And people use different words for all of these. They mean slightly different things to us. The most interesting, and this is a bit hard to wrap your head around property about this, but it's very relevant for scaling, is that you can have a circuit that proves, say, one Ethereum block, and you could prove three different Ethereum blocks, and then you could pass all of those proofs to a different circuit that verifies those proofs.
02:34:52.778 - 02:35:49.234, Speaker A: And in so verifying those proofs, it proves that all of those proofs were valid and produces a new, smaller proof. So this property allows you to take an arbitrary number of these already succinct proofs and keep making them no bigger over time. So you can imagine from blockchain perspective, especially, how useful these primitives are for scaling, sharing of what actually happened. So, I'm the CEO at RiSC Zero. We're one of the pioneers in general purpose ZK computing. So when we think of ZK, we don't usually think of writing circuits, which is more of like an EE skill than a Cs skill. Instead, we have written a circuit that is a computer, so one of its inputs is just normal computer code.
02:35:49.234 - 02:36:45.580, Speaker A: So in this example, on the side, you can see we take rust, turn it into RISC five, and then this ZKVM executes the RISC five and produces a proof that it was executed correctly. And again, because it's ZK, the prover can choose to reveal any of the inputs. It doesn't even need to reveal what the program is. It just attach, and you can still get a proof. And there are all kinds of applications for things like that. So one thing about ZK is that it's very slow. I think the PCP algorithm and some of the first algorithms were actually, as far as anyone could tell, impossible to actually create a proof, because the amount of time involved was more than the number of atoms in the universe.
02:36:45.580 - 02:37:59.716, Speaker A: So we've come a long way. I think Wei Dai was talking about this notion of kappa, which is just sort of the cost per proof per power, something, I'm not sure exactly how he's defining it, but if you look at the progression over the past, I don't know, seven years. And especially over the past three years, we see ourselves going from impossible to six orders of magnitude, five orders of magnitude more overhead to do the same computation in a zero knowledge context. And we are starting to see some systems for specialized problem domains get closer to 100 x slower or more overhead. And then with the advent of technology potentially like fabrics, you might see the cost overhead actually drive down closer towards ten or even one. It will always be slower, fundamentally, or very likely. Okay, so as pioneers in the general purpose ZkVM space, let's see.
02:37:59.716 - 02:38:35.628, Speaker A: Can I go back? Oh, I can. Okay, so one of the advantages of doing this, and I think for a long time people didn't really believe this was a sound way to do ZK, because there is some overhead which can be addressed. But the most important thing about this is that you don't have to write all the code yourself. You can use other people's code. So as part of some work with Op, I'll talk about in a second. We built the first. I call it type zero ZkVM.
02:38:35.628 - 02:39:44.100, Speaker A: It's actually just a type one ZKE EVM. But as far as we're aware, this was the first ZK proof of an actual Ethereum block that was ever created. And we created it using it. Sorry, we created it using this general purpose ZK paradigm, because it allows us to just reuse about 95% of the rest in our UVM ecosystem without needing to reproduce it ourselves. So the sort of capital outlay and time to market goes dramatically down at an increased cost. This is not competitive with ZK sync or polygon or anything like that yet, but you're going to start to see more movement in that direction. And so I think this is a good example of proof systems getting to this sort of five orders of magnitude more effective, and some of these other scaling techniques we'll talk about that, actually enables a lot more experimentation and freedom in the Ethereum ecosystem.
02:39:44.100 - 02:40:46.772, Speaker A: So we actually did Zeth well, because I said we would do it two years ago in Paris, but also as part of this optimism mission that we've been on. And if anybody's familiar with the op stack, it's a lot more complex than straight up Ethereum. And they've been working on actually releasing their fraud proof mechanism. So they had a grant to figure out how to apply ZK to op, which we applied for. And we've actually pretty much finished this mission about six months early. And we've actually proven, okay, I'll say an entire optimism epoch. Technically it's only a 10th of an epoch, but what that looks like is over 180 op blocks proven together, rolled up into a single proof, and each of those blocks actually have to examine any number of other Ethereum blocks.
02:40:46.772 - 02:41:33.844, Speaker A: So you're really talking about one computation proving this epoch that spans 180, that spawns 180 new computations in turn, which each spawn maybe even 50 other ZK computations. So when we talk about horizontal scaling versus vertical scaling, this is a way to sort of horizontally scale out the ability to solve a very complex ZK problem. And this, I'm very proud of this. I will say that we were not the first people to do this. I don't think we might be the first people to do this in a ZKVM. I believe that somebody at nil may have gotten doom working in ZK. But yeah, we just released this.
02:41:33.844 - 02:43:06.794, Speaker A: You can check out the source code, and it actually renders all of doom in ZK. And what this lets you do is you can basically run a game, prove that all the rules were followed, reveal nothing about the intermediate gameplay, and simply show the score at the end, and prove that you were able to get that score without violating the logic inside the game. Now, is there any direct applications of this other than just speed running? Maybe not that plain, but I think these kinds of dynamics can be used in a lot of games, and probably DFI as well. This is some really cool research, and I think this is another area where when we're thinking about 100 x and what 100 xing the capabilities of a ZK system makes possible. It's this idea of ZK fully homomorphic encryption. So fhe is this sort of holy grail of encryption, because parties can compute on the data without being able to read it. So especially when you're thinking of building unstoppable, decentralized network of computers, boy, it would be really nice if you could have private data, actually have that private data be used when you want it to be used, and still not necessarily reveal anything about it.
02:43:06.794 - 02:44:07.322, Speaker A: So by combining ZK and fhe, there's actually an opportunity to reveal attenuated information and have it stored privately in blockchains. So we did some research with some venture partners, and the people at Zama, their name should be on this slide, but it's not. They are pioneers in, I think, the most promising fhe regime right now. And what we were able to prove is like a key component of the TFHe algorithm called bootstrapping. And it's the thing that you need to prove was done correctly in order to ensure that the information doesn't get leaked out as you continue to compute on it. I think this took a billion cycles, which is like 30 minutes or something, to just do this one bootstrapping. So I think it's about 100 times too slow than it needs to be to be kind of basically useful.
02:44:07.322 - 02:45:04.094, Speaker A: But even once it's kind of basically useful, I think there's a lot that it has to offer to the blockchain system. So there's a lot that can be done to get there. Programming custom accelerators, I mean accelerators in the circuit sense like validia is doing, and our next version will enable. But there's also great opportunities for programmable hardware as well. I really think that ZkFhe is going to be a huge part of how everybody's personal data is stored in the 510 year time frame. So yeah, one little pitch for our ZKVM. If you are interested in figuring out what it's possible to do with ZK, in addition to these sort of big examples that we have, we have a lot of examples just in our source code repository.
02:45:04.094 - 02:46:03.816, Speaker A: You can check them out and yeah, hopefully get inspired about things you might do with CK. Excuse me, I want to talk a little bit. We talked about recursion and composition earlier. So this sort of zeth program that I think nobody thought was realistic until we did it. The way we figured out how to do this is by taking one ZK proof and then doing some kind of memory magic to split it up into thousands of proofs and then simply use recursion to combine them into one proof. So this has basically let us scale like a single node. Performance is about 100 khz, and right now we're at about 5 mhz in this parallel cluster.
02:46:03.816 - 02:46:56.344, Speaker A: But that's because there's some bottlenecks that we'll remove. Fundamentally, if you can solve some of the bottlenecks in these proving systems, there's no reason you couldn't use parallel proving to get basically full rate ZK computing. It might be expensive, but it should be possible. So the kind of subtle difference between recursion and composition in the recursive context, our system magically figures all this out for you. You don't have to worry about how many segments to split your program into. Risero will just figure this out and it will execute this one program. However, for cases like optimism or many other examples, you often have many work streams.
02:46:56.344 - 02:48:02.064, Speaker A: You can even think ZK Mapreduce effectively a bunch of independent work streams. And by fanning things out to as many jobs as possible, effectively multithreading your ZK systems. This allows you to remove some of the serial bottlenecks that are unavoidable at the start, such as sort of witness generation is parallelizable, but simulation is not. And I think the fabrip people were also getting at the utility of that. So by combining these things, you're able to build these incredibly complex ZK systems in. Yeah, I think the op ref work took us two people for about three months full time to really do something that I don't think anybody thought was possible. And then again we iterate that again to make these epochs don't have a ton of time and these kind of trade offs and limitations are pretty wonky.
02:48:02.064 - 02:48:45.488, Speaker A: If you're really into proof systems, I'm happy to talk about them. But yeah, these are areas where proof systems getting better at these things are going to increase the set of everything we can do faster. Yeah, even more details here. There's a ton of cool research coming out in terms of zkvms, we're certainly not the only one. There's lots of copies popping up and there's also lots of really amazing novel research like the stuff mine's doing and a 16 z is doing. So yeah, that's pretty much all I have. Any questions? Oh, I guess.
02:48:45.488 - 02:49:35.030, Speaker A: Call to action. We are going to be launching like right now. You can use resero to generate very small gross 16 proofs. You can post them to testnet, but we will be launching to mainnet Q two, hopefully sooner. But this will actually let you build real programs, lock real TvL up in your risero systems. Yeah, is there any mic runner or just yell? I can repeat the question too Avi. The question is what is the open research problem that keeps me up at.
02:49:35.030 - 02:50:51.916, Speaker A: I honestly, there's so much work to do right now that I'm not even sure that there is one. I feel like the path forward is obvious in so many respects for us. I think I'm personally curious about the information theoretical lower bounds on overhead for a ZK system. Like how much extra work do you need to do to actually maintain those four properties? And I guess the other interesting research problem that I haven't seen a lot of traction on is the ability to try to massively speed up proofs at the cost of security. Those things are probably related, but I think there are use cases for low, not very secure proofs that are only valid for 5 seconds. Yeah, just curious on your view on the next 100 x for ZKP. Is it going to be hardware research or things that risk zero continuations that allow parallelization? Yeah, I think it's going to be all four things.
02:50:51.916 - 02:51:39.340, Speaker A: It's going to be new research methods, new fields. You see binius, you see circle Starks, see M 31. It's going to be just raw engineering. I think we probably have ten x of just, we're in a hurry. And yeah, hardware, I hope, plays an increasing role, but I think there's 100 x easy to go without needing asics. But then once you get that and an ASIC, we'll be going. The question is, do I think proof standardization will ever occur? I hope so.
02:51:39.340 - 02:52:33.836, Speaker A: I think there's directions sort of towards that right now. Some proof systems are more flexible than others, like plonky three is really configurable. So I do hope we'll get there, but it's very early. And then once we have a proof standard, there will be three or four. All right, I think out of time. So next up, a panel discussion on decentralized proving infrastructure and incentives. Please welcome Cooper Koontz of Aztec Labs, Nazar Khan from Terra Wolf, Ben live sheets from Matter Labs, and Michael Gow of fabric cryptography.
02:52:33.836 - 02:53:13.540, Speaker A: Moderating this panel is Robert Koshig of one Kx. All right, hello, everybody. Good afternoon. Better say, yeah. Welcome to the panel of decentralized proving. Yeah, I'm Robert from one Kx, and I have four wonderful panelists here to dig deeper into the topic and discuss decentralization of proving with the incentive design lens to it. And yeah, to start, maybe will we make around.
02:53:13.540 - 02:54:08.980, Speaker A: Can you give a brief introduction to you what you're doing or better, like your protocol or your company is doing? And yeah, how does it relate to the decentralization of proving? So the audience gets a context to it and yeah, maybe we start with you. Yeah, hi, my name is Cooper, I'm the head of decentralization at Aztec Labs. Fancy name for product manager on our sequencing and proving infrastructure and governance. Aztec is a privacy first zero knowledge rollup. We would call it a zero knowledge, zero knowledge rollup. So users are generating zero knowledge proofs client side, sending them to the network to be aggregated and verified later on Ethereum and through that client side proving, that's how you get the privacy properties of the aztec network. And so when we talk about decentralized proving, all of our proofs are generated client side and we're talking about decentralizing the aggregation to submit one final proof to the Ethereum network.
02:54:08.980 - 02:54:35.450, Speaker A: Yeah, that's enough about that. Hello. Hello. Oh, much better. My name is Ben Lichitz. I'm the vp of research at Zksync Metal Labs. Yeah, so I think there's obviously been quite a lot of excitement about decentralized sequencing.
02:54:35.450 - 02:55:06.406, Speaker A: And of course, as we look into the future, I think opening up the prover pool is something that we're starting to look at. And I think this is going to be interesting from just about every angle. Software, hardware. Software, hardware, code design. We've already seen a bunch of really interesting talks in the preceding 2 hours about many of these topics. So, yeah, happy to dig in to this. There is quite a lot of depth in this topic, and I think it's really fun to think about sort of steady state outcomes.
02:55:06.406 - 02:55:50.546, Speaker A: I think the economics of this, how to make it so that the benefits of successful software, hardware, code design are things that the end user takes advantage of, which lead to things like fee reduction. So quite a lot to talk about. Thanks, Michael. I'm the CEO of fabric cryptography. We're a company building general purpose cryptography hardware. Yeah, so basically we're building general purpose cryptography hardware. For those of you who are less familiar with us, it's basically like a GPU built from the ground up for all of cryptography.
02:55:50.546 - 02:56:56.266, Speaker A: So whether it's a ZK snark or a ZK stark or any of the future proving schemes, it all is supported by our custom silicon. So I'm very excited to be here and to talk about how do we actually get this out there into the hands of users, and how do we scale up decentralized proving and really foster a sea change, and how big statements can actually be proved in real time. Good afternoon. Nazar Khan here. I'm the co founder and CEO of Terrawolf. We're a publicly listed bitcoin miner, but our background is really in energy infrastructure, and so our interest is in really, as we look out three, four, five years from now, if we're anywhere close to right in the ubiquitousness of how this is going to be used, it's going to be a tremendous amount of power that's needed to kind of support this. And so whether that's in an aggregate at one location or kind of dispersed throughout individuals, understanding kind of how the power supports that is an important part that we see.
02:56:56.266 - 02:57:50.250, Speaker A: And so we've been very involved in understanding and helping kind of shape that narrative in terms of how the decentralization kind of scale up as quickly as possible. All right, thanks a lot though. Then let's dig a little bit deeper. And, yeah, let's maybe start with the network side here. And, yeah, maybe with you, Ben, because you already touched upon a couple of points. Because my question would be, what are you focused on the most when you think about designing your network when it comes to decentralization of the provers? I guess you start with easy questions, right? Look, yeah, let's see if this works. There is a lot of focus in the near term on just continuously delivering improvements to the proof system that we currently are running.
02:57:50.250 - 02:59:02.100, Speaker A: And I think chances are not just us, but other zero knowledge roll ups will be in a situation where for the near to medium term, we'll be delivering sort of improvements to the proof systems that we have. And I think that's going to be the case for at least some time. And I think perhaps over time, we'll find ourselves in a situation where sort of other parties will appear. So firstly, I think we'll be in a situation where hardware based improvements is something that we'll want to avail ourselves of and we will want to deliver those kinds of things to the end user. We'll want to, as I said before, open ourselves up for other people doing proofs, and by doing so, hopefully reducing the cost of proof production, proof generation, and over time, transferring these kinds of savings to the end user as well, and by doing so, ultimately reducing the fees. But, yeah, I think this will take a little bit of time. So at this point, things are very much in an experimental stage for us.
02:59:02.100 - 02:59:44.926, Speaker A: Yeah. And then from Aztec's perspective, we're a privacy first roll up. We believe that privacy protocols have to be credibly neutral. We think that we will start from day one with a fully permissionless sequencing and proving designs. Most of the prioritization for this is generally censorship resistance. Having fully permissionless proving marketplaces where anyone can add, compute to them is a much harder to attack network architecture than enshrining a specific prover or prover who might not agree with users in a particular jurisdiction or trying to do a specific thing on chain. So across the board, Aztec is trying to be as credibly neutral as possible.
02:59:44.926 - 03:00:42.706, Speaker A: And you do that by allowing anyone to run your hardware. And so that's why we're super excited about letting anyone add and participate in auctions or other mechanisms to prove different parts of aztec blocks. And we do see this as a fundamental day one requirement for the aztec network given our posture on privacy. Okay, well, then let's maybe involve the supply side, because I would obviously be curious to which extent. For example, you, michael can help with these things, because when you talk about eventually also going the hardware route or allowing anybody to run a machine, basically run approver. Yeah, maybe, michael, you can touch upon what hardware side can support that, what the hardware side is supporting, or how the hardware side can support that. Oh yeah.
03:00:42.706 - 03:01:51.574, Speaker A: So we're pretty agnostic to the format that roll ups are taking, because really every layer two or every chain is taking a different approach to accelerating and also incentivizing and also decentralizing ZK proof production. And what we really want is to benefit the entire ecosystem. So whether you're taking a really decentralized approach and you're doing it from day one, or whether you currently have centralized proving and you want to move towards the decentralized proving kind of method, we have several offerings in our roadmap for all of these use cases. So we have a service platform that's coming later this year. We have boxes shipping in volume later this year. Both of those are going to help people who just want more compute and they want it in a data center format. And then we're also talking with a few protocols about a consumer grade kind of PCIe card that can go in, let's say a home computer, and that'll be the equivalent of orders of magnitude above 128 core xeon processor.
03:01:51.574 - 03:03:10.050, Speaker A: So we're super excited that that kind of compute power can be in everyday people's hands. And so whatever the protocol design, we're willing to roll up our sleeves and work with the protocol to achieve the goals that they set out to achieve. Okay, I mean, that sounds really good, but I heard a lot of times compute power, and that might be the time to involve nest in the conversation, because maybe you can share in the introduction. You started pointing towards the direction, okay, now things look good, but a couple of years from now, there might be problems, and maybe you can elaborate on that, because that might help also us to understand how we should already now think about the way we should architect the networks. Absolutely. And I think in a number of the discussions today and on the panel before cost, ultimately cost is going to be an important metric in the ability for these ZK proofs to really scale up and spread. And so that's where we really come in, ultimately, understanding how to run the infrastructure to support it, whether it's a fabric VPU, or any other kind of infrastructure that's needed, how to place it, and how to kind of support and run it, is ultimately where we come in.
03:03:10.050 - 03:04:14.470, Speaker A: For 20 plus years, we were in the infrastructure space, and so our job was to basically deliver power at the cheapest cost. And so what we've effectively done is now we're kind of taking power and putting that into some sort of compute and kind of delivering that compute at the lowest possible cost. And so as we look out over three, four, five years, ultimately, without having a very clear and concise understanding of how to be able to drive down the cost of power, which is not just kind of the layer two s that are running, not just the hardware, but also where that hardware sits and how that's integrated back into the overall electric grid is important. And that's really where we focus our time in understanding how we can kind of provide that. And again, we're agnostic to how that plays out, because ultimately, whatever it is, is going to need some power associated with it. And as we look out, when we look at kind of the data field more broadly, not just ZK proofs, but all forms of data compute, there's a fairly significant growth that's projected over the next five years. People are talking about the size of that industry going three, four, five times.
03:04:14.470 - 03:05:05.180, Speaker A: And so the ability to place that kind of a load into the grid requires, again, a very nuanced understanding. And so we spend a lot of time early on in thinking about these protocols, these hardware applications, how they fit in and providing our input into how over the long run, especially at scale, how they'll fit into the system. And so that's where, again, we always kind of come back to ultimate cost in the long run, because again, that lowest cost is what's going to drive the greatest kind of scale. Sure. Thanks. Let's imagine we have a couple of orders of magnitude more transactions being settled on oral ops compared to main net, right? So let's imagine a future like that, which is probably not so far away. So what does the shape of this prover industry look like? And my sense is that there's a bunch of open questions.
03:05:05.180 - 03:06:43.150, Speaker A: Like, for example, does solar proving, right, you have a card that goes into pc. Does it make financial sense? And can we really foster that in order to grow the level of decentralization? Sort of similar to what Ethereum is trying to do with solid staking or whatnot, right? I mean, it's an interesting question, right? Here's another one, which is what is the sort of correlation and relationship between hardware manufacturers, hardware makers and hardware operators, right? And to what extent are these open designs that are subject to scrutiny, other standards around these things? Or do people basically make the hardware and they keep it in their vault and they never open it up, and they just use it to basically maintain their competitive advantage when it comes to proving, if it's an auction model, allows them to win just about every auction. Of course, you can imagine a situation in which things get consolidated, which at least superficially is similar to what you would see in case of proof of work mining pools. Right. And I think at this point, given that it's quite early, I think we still have a chance to sort of reflect on some of the challenges around POW. And these are challenges that people like to complain about, such as power, efficiency or inefficiency, but also like new challenges that you just don't really see in the bitcoin mining space. And kind of try to see if we can combine the features of cryptographic systems that we build with the opportunities that come in terms of mechanism design to potentially create a slightly different landscape for this prover industry, so to speak.
03:06:43.150 - 03:07:17.456, Speaker A: All right. Yeah, very interesting. But let's maybe talk a little bit about the different approaches to it. And you, Cooper, for example, in your forum, there's quite a lively discussion on the different approaches. So maybe, I'm sure not everybody in the audience went through all the forum posts. Yeah. Maybe give brief recap of the approaches that you considered and maybe also share why you are advocating for your sidecar approach when it comes to decentralization.
03:07:17.456 - 03:07:55.504, Speaker A: Yeah. So when I was tasked at Aztec to try to figure out how we're going to decentralize our roll up, we really took building in public very seriously. If you go to forum aztec network, there's probably eight to ten different sequencer selection protocols that are all fully decentralized there. There's probably, I don't know, five to ten proving marketplace protocols as well that were articulated either by employees of ours, employees at the Ethereum foundation, employees at Espresso Systems. There's a lot of really nice designs, honestly. So if you're looking for how roll ups are thinking about decentralizing their infrastructure, I would say this is probably, as far as I'm aware, the biggest body of work on the subject. In our case.
03:07:55.504 - 03:08:24.140, Speaker A: We know kind of just a random leader election like you have in Ethereum. You get a randomly elected proposer. Anyone can participate in this. We support out of protocol pbs like you see in the Ethereum landscape right now. And then today we're here to talk about proving. So through that research, we kind of found two different categories of proving marketplace designs. You can have a cooperative design which would be kind of similar with consensus networks or byzantine fault tolerant consensus protocols where you have a group of machines who are collectively working together to produce proofs.
03:08:24.140 - 03:09:15.608, Speaker A: And so you could have traditional byzantine assumptions over failure modes, redundancy, types of things that you would expect to see in byzantine consensus protocols applied to actual proving redundancy. So you have one category of cooperative, and on the other side, you have competitive marketplaces. And within competitive marketplaces, you can have proof races, right? First person to produce proof wins. You can have economic marketplaces and competitions where people are actually proposing auctions and trying to participate in an economic competition for their rights to prove Aztecs blocks. All decentralized proving market designs will fall into one of these categories of competitive or cooperative. And then kind of depending on what you're optimizing for or what properties you want out of your system, you're going to know different steps along this decision tree. Aztec, actually, after deciding and kind of mapping out this landscape, decided not to enshrine a particular method for proving an aztec block.
03:09:15.608 - 03:09:49.056, Speaker A: And so we are currently looking at a protocol articulated on our forum called Sidecar. It's a way where a sequencer can, out of protocol, dedicate or delegate proving rights to any subcontractor that they want through an open source permissionless auction. We call it Proverboost. It's kind of an analog to Mevboost. Nil Gevulot, succinct. Any third party proving marketplace, any person who has enough compute can directly participate in an auction to prove Aztec's block. You put up an economic commitment, you say, hey, for $10,000 or whatever, I'm going to prove this block.
03:09:49.056 - 03:10:37.300, Speaker A: And if they don't submit the proofs within a given amount of time, we're going to slash their stake and treat it as a missed slot. And so the current aztec designs are kind of unapinionated and support both cooperative and competitive, proven marketplaces entirely outside of the scope of our protocol. If any of these models eventually end up being incredibly successful, incredibly centralizing or otherwise Aztec's governance, or would potentially consider releasing a future version of the protocol that enshrines the best model of sorts. But given the relative immaturity of the industry at the moment, we don't think any of these models is well understood enough to enshrine it in particular yet. So hopefully that gives you a sense of the landscape and kind of where we're thinking. Highly recommend going to the aztec forum. We just did a request for comments on the end to end of Aztec's block production.
03:10:37.300 - 03:11:30.916, Speaker A: All right, then I may be pass the ball back to Ben because did you already comment on the aztec forum? I guess not, but we're pretty well aware of the work that's going on. We like it a lot actually. And I think we also admire the process that's been run by Adstec at least twice with sequences selection as well as proof of selection activities that are ongoing. I guess just to add a little bit to what was said, I would like to. How should I put it. I think we're trying to figure out a way in which we can experiment with these different mechanisms to see what the steady state outcome might be right. And trying to figure out a forum or an approach that we can take to basically do these kinds of experiments.
03:11:30.916 - 03:12:04.766, Speaker A: We haven't quite decided what it should be. I think that's sort of one of the things that perhaps is stopping us or slowing us down at the moment. But yeah. So over time I think we'll hopefully figure this out as a community, you can wait a few months and see what Aztec does wrong and then you can hopefully do a better job than us. It's like the opposite of front running. Right? Okay, nice discussion here. Yeah.
03:12:04.766 - 03:12:42.650, Speaker A: Maybe also to ask Michael here, there's also some discussion. This is very much about the general different proving approaches and where to locate. There's also a lot of discussions around. They highlight very much the benefit of distributed proving and kind of like benefiting from these sharing economies putting in the hand of everybody. And this way no need for kind of like centralized large operators, no need for specialized hardware. Yeah. Curious how you would view these arguments, Michael, and what you would respond to that for sure.
03:12:42.650 - 03:13:49.390, Speaker A: I mean, there are a lot of people who talk about the centralizing effects of specialized hardware. And as someone who has seen the semiconductor industry play out over the years, the semiconductor industry is generally a friend of mass adoption. So, for example, when cpus first came out in the 1980s and really started proliferating, that led to the rise of the personal computer. And eventually when process node shrinks got so good and so big and then gpus became co integrated with cpus on mobile processors, we saw the rise of the mobile phone and now everyone has a supercomputer in their pocket. And this is exactly the future that we want for not just the VPU, but all categories of ZK acceleration. We want everyone to have access to this powerful cryptography and that's the future that we ultimately want to create. So that's where our consumer kind of PCIe card is going as an initiative and it's where our company wants to go as a vision.
03:13:49.390 - 03:14:43.514, Speaker A: Okay, yeah, maybe also, Nas. To which extent do you think decriminalization or what Michael plans to enable can help the problems you raised earlier? Yeah, it's interesting. Our background is really on the physical infrastructure side. Right? Again, we built power plants and operated power plants for 20 years. And so when we talk about decentralization within these layers, the idea that maybe one place is running all of these nodes and the compute power sits there, but it's controlled or operated by many different people, you could have both of them could be true, right? The compute could be decentralized and how it's being used, but the compute could also reside at one place. And so that's kind of a fundamental issue that at least I've been kind of thinking through is that because ultimately, again, if I always kind of come back to the cost, right. I mean, ultimately the cost has to be low enough for it to be pervasive.
03:14:43.514 - 03:15:27.114, Speaker A: And that's what's going to make it most pervasive, is a really low cost. And so if it's going to be used quite a bit, has to have a low cost. And so when you start to break down these things and say, hey, where can we operate this proving at the lowest possible cost? It's likely going to be at something that's more than just someone's individual personal computer. And so that's, I think, kind of a discussion that needs to be had is when we talk about decentralization, what layer is it occurring at and how do we ensure that if there is a significant amount of compute at one place, that ultimately the underlying compute operates in a decentralized way? And so that's where, again, we have our own perspective on how that's being drawn up. But I think as a broader community, that's something we should all be engaging in and thinking through. How do we, I think, kind of talk about that level of decentralization. Yeah.
03:15:27.114 - 03:16:29.860, Speaker A: And there is something that I'd like to add to this, which is I've seen an amazing economy of scale play out in the bitcoin mining world. I've been in bitcoin since 2011. I've seen how the proof of work incentives have led to a cost per terrahash and joules per terrahash, that frankly, if you asked a standard ASIC engineer and they did some simulations, they would think it's unthinkable that we have the level of efficiency that we have, the level of cost that we do. Now, it's a little bit wasteful, but in the context of the ZK industry, if we're doing this incredibly useful thing with positive externalities for all of society, that's something that we want to drive economies of scale on, right? So I don't think it's kind of an either or. I don't think it's like necessarily decentralized versus centralized. I think that you can have economies of scale and simultaneously you can enable a lot of people to prove at home. It doesn't have to be either or.
03:16:29.860 - 03:17:16.190, Speaker A: I was going to say, yeah, proving at home, safety at home, all of these things resonate a lot. And so let's not forget that the monolithic roll up case is not the only case, right? Because there is the privacy friendly case of the user doing their own proving right in the comfort of their own home. And as time goes by, chances are we'll want to prove more complex statements beyond what we can do now on a small machine or phone, which has to do, let's say, with credentials, things like that. That's very nice, but I think over time we'd like to do a bit more. So that's one thing. The other thing is that there is also the case for variable pricing. That is to say, you pay more, your proof will happen faster, you pay less.
03:17:16.190 - 03:17:58.622, Speaker A: Well, maybe not quite as fast, but maybe it will go to a bunch of people who are doing this at home, solar proving style, right? And everybody is happy because these solar provers get some input to work on and the user is not paying so much. And maybe this is the right sort of equilibrium to achieve there. Right? Again, what I'm saying is that there is more diversity of these use cases. If you think about this end to end, some of these things come from privacy, others come from sort of different economic parameters. Yeah. On that note, I would say Aztec is trying to be one of the first fully programmable private smart contract platforms in the world. You can deploy contracts where the actual bytecode is encrypted and other things.
03:17:58.622 - 03:18:32.354, Speaker A: And the first version of it is very novel. It will be somewhat limited in the expressivity that you have within that smart contract. Right. There is only so much we can do to keep block times, to keep proof times, to keep execution times sufficiently low that you can actually use our system. ZK is something that will fundamentally consume all of the compute, all of the spare compute that will be available in the world. As much expressivity as our proving systems will allow. Aztec will try to use as much hardware as available and as much data availability as our da layers know, et cetera and et cetera.
03:18:32.354 - 03:19:13.542, Speaker A: And so we're really at day one of the amount of expressivity that you're currently getting in zero knowledge proving systems. And I would say if you're looking at aztec smart contracts as a kind of metric, we're probably at what, the eight bit resolution equivalent of expressivity within our smart contracts. And so it's going to be a very long game that's going to use a lot of hard work and to that point. And that's where the scale rises exponentially, right? It's not a linear progression. And so as that expressivity kind of increases, it's going to be kind of a nonlinear growth. And that's where, again, understanding how these loads are going to be integrated back to the physical grid is we think in a terrible. We think it's kind of an underappreciated point.
03:19:13.542 - 03:20:44.578, Speaker A: And so we're solving this big problem of how do we get the system to work. But if we're so fortunate to figure that out, which given my perspective, I think will happen, then we'd have this other challenge of saying, now, okay, now, how do these loads kind of integrate back to the grid? And again, that's where someone from a physical energy infrastructure perspective, I think, can provide input into that process. And so that's, again where Terrell is really thinking about not just kind of what's happening in the immediate, but more, hey, as these kind of things catch on and that demand from it kind of grows exponentially, how do you make sure that these loads can kind of give you integrated back? And that's where, again, I always come back to this just kind of fundamental thing of kind of cost and availability. So long as the cost and availability continue to support that growth, it's going to keep growing. And to the extent that there's bottlenecks in it, that's where you're going to see a big problem. So that's where, again, I think as we look at it, there's the development side of it, which is obviously critical and which is the base layer. But beyond that, for it to really catch on, there has to be this thought around, how do these loads kind of fit into the grid? And I'm incredibly excited to see protocols talking about latency and throughput incentives because I've seen how far, just exponentially speaking, this kind of incentive structure can play out in terms of how big of a technological lift, right? So the bitcoin network started out as maybe like terrahashes per second of compute in 2011 when I first saw it.
03:20:44.578 - 03:21:24.960, Speaker A: And today just one amp miner is terrahashes we have almost a zeta hash per second. It would be the first zeta scale computer on earth. And that's pretty cool. And that same kind of like billion x increase in compute has happened in the field of traditional cpus. And that is why we have Chat GPT, and that is why we have 3d gaming, and that's why we have ray tracing. And it's also why you see all of the devices in this room be able to operate the way they do. And so what I'm incredibly excited about is you're going to see transactions that go way beyond what we think of as transactions today.
03:21:24.960 - 03:22:16.090, Speaker A: We think of like sending one eth from one person to another. We think of things like uniswap. Okay, imagine dexes that reach the same TPS as binance, right? That's a really revolutionary thing to finally reach parity, at least in functionality, with centralized systems. And that's something that I'm incredibly excited to see, because I can see it happening as a domino effect from what we're talking about on stage today. And so, thank you to Cooper for starting that conversation. Yeah, I want to come back to a point you guys mentioned earlier, right, that proof of work centralization effects, but then also economies of scale, which is also centralization effect. The marketplace approach actually also typically favors the bigger players on the longer term.
03:22:16.090 - 03:22:59.774, Speaker A: And that's maybe for you and Ben. A question. Are you a little bit worried about, for know, verticalization, which we, for example, also see on Ethereum, kind of like even to a point that might be threatened, the original idea of your protocol? So, like the workings, or is it something where it's like, yeah, I mean, we're so early, let's not worry about it. Yeah, it's a really good question. And so Aztec started our design, decentralizing our sequencers, which is our analog of proposers, who actually determines what gets included in the history of the chain. We think that that's where you should start if you're worried about decentralization through the lens of censorship, resistance. And so we have a very decentralized sequencing set.
03:22:59.774 - 03:24:34.742, Speaker A: Anyone can stake some tokens on l one run, something that looks like an ETH validator, hardware, you'll get to win a random leader election and propose a block. And so starting from there, we actually see a little bit of centralization within the proving marketplaces to be okay, as long as that there's an ability for, let's say a zero day or a black day event or like a black swan event, where all of your provers to randomly drop off the face of the earth one day, how long does it take for your network to actually produce the next block and come back to life? And so we looked through the lens of, like, it's okay if the proving market actually centralizes on a few actors or a few people vertically integrating, as long as if those people stop doing this service, someone else can compete immediately in the next block and be sufficiently incentivized to provide that service. And so in the current designs of what we call frenet, of our sequencing selection protocol and sidecar is our proving protocol, we do allow a sequencer to prove and stake to their own address to vertically integrate as a sequencer. The protocol really doesn't care about that. They'll have the same slashing conditions as if it was a fully decentralized proving marketplace. If that does end up resulting in, let's say, censorship in some way, shape or form, or you see a lido like event coming into the network where they're picking themselves to vertically integrate every block, I would say that's not very aztec aligned of you, and please don't do that, and we'll deal with those things as they come. In general, no one's really seen a fully decentralized roll up, let alone a zero knowledge one, let alone the one that has privacy.
03:24:34.742 - 03:25:12.756, Speaker A: And so we're pretty happy with the level of censorship guarantees the current designs provide. But that's a long way of saying yes. It's something that we're constantly thinking about. Well, I think we roughly understand what the good outcome is and what the bad outcome is as well. So the bad outcome is probably easy to describe, which is, well, we basically have one party just sort of ending its innovation, so to speak. Right. We're just like deciding, well, this is the final proof system, and this is the final hardware setup, some mix of cpus and gpus and maybe fpgas.
03:25:12.756 - 03:25:44.884, Speaker A: And it's like, this is it, right? This is all you get. I think a more exciting, successful outcome is when you have multiple parties. And perhaps these are relative large players. Maybe there's not hundreds of these players, but maybe a handful competing on this software code design sort of axis, so to speak. Right. So where things are reasonably open on both fronts, let's say. And we're currently seeing that across roll ups, of course.
03:25:44.884 - 03:26:24.284, Speaker A: Right, where there's competition within the space of proof systems. But I think some of this can happen within a single roll up as well. Right. And as long as that competition continues for the foreseeable future, there are benefits to the end user. And I think that's sort of the right focal point for us as well. Yeah, I was just going to say one more thing is that we do see, similar to the terra wolf folks, we see the cost of proving eventually, hopefully trending as close to zero, plus electricity and the cost of running these things, et cetera, et cetera. And so that is an argument in the favor of vertical integration.
03:26:24.284 - 03:27:03.852, Speaker A: The cheapest way that you will prove a block is by having enough hardware and doing it yourself rather than outsourcing it. And so we really treat decentralizing our proving as kind of a subcontracting labor problem. If you're a sequencer and you need to get a job done, you can choose to do that job yourself, or you can choose to subcontract it to a separate labor market. And so it has very nice economic analogs to just any subcontracting based labor market. And the cost of that should trend as close to the market can. Yeah, unfortunately, we're running a little bit out of time here. I think we could go on and on, but, yeah, maybe for Michael, the last question here.
03:27:03.852 - 03:28:09.948, Speaker A: Can you maybe give an outlook of what we can expect with respect to scale and cost improvements, what the hardware side will deliver? What can we be excited about over the next year? No pressure. Yeah, I think the cost of proving over the next kind of five to ten years, I anticipate it going down by multiple orders of magnitude. This is across infrastructure, and the infrastructure overhead and things like the cost of power, the cost of the rack setup and stuff like that. It's across companies like us and what we're doing with custom silicon, we can deliver orders of magnitude by ourselves. And it's also companies that are developing new proof systems that are even more efficient and that have lower overhead with respect to plain text computation. But I think the combination of all of these, you see an AI, it got 44 times more efficient in something like a six year period from a software perspective. It got a similar amount in hardware efficiency.
03:28:09.948 - 03:28:51.810, Speaker A: It went from the terraflop to the petaflop level, on the chip level, and all of that at the same time. And there were better economies of scale on the AWS and GCP fronts as well. And we see that same thing playing out, especially because of the incentive structures that we're talking about. And so what I'm really excited about is that I think the overall market is going to grow even as the proof cost shrinks, because people are going to be proving exponentially larger statements exponentially more frequently once the cost just dwindles. All right, well, then, thanks again, everybody, for this very interesting discussion. And, yeah, we'll keep building the right incentives, I guess. Thanks a lot.
03:28:51.810 - 03:30:36.694, Speaker A: Our final panel today delves into the critical topic of ZK security. Please welcome Gregory Rosu of PI Squared, Maro Toscano from Lambda class, and Costas Ferles of veradice. Moderating this discussion is Ibrahim Yusafali of polychain. Please welcome them to the. Hello. So mine is working. Yeah.
03:30:36.694 - 03:30:56.830, Speaker A: Hello, guys. Yeah, I guess we can get started with this panel. It's going to be about zero day. Why? ZK security is important. Yeah, my name is Ibrahim Yusuf Ali. I'm a researcher at Polychain Capital. And, yeah, I just wanted to let you guys go on with intros, so, yes, is it working? Yes.
03:30:56.830 - 03:31:17.374, Speaker A: Okay. All right, I will just use this one. I'm Marutoscano from Lambda class. Cryptography, engineering, yes, mostly working with cryptography on the applied side. Okay. So, hi, I'm Cosas Farles. I'm the chief research officer at Veridice.
03:31:17.374 - 03:31:56.656, Speaker A: We're a security firm and we're kind of like, specializing in ZK. And we also develop a lot of automated tooling. So my job is kind of know see the needs of our auditors and kind of steer our tool development towards kind of more useful automated tools. Hi, I am Gregorio Rochu. I am a professor of computer science, also the founder and CEO of runtime verification and of PI Squared. And I'm interested in correctness in general of systems. All right, guys, thank you for those intros.
03:31:56.656 - 03:32:53.590, Speaker A: Yeah, I guess we can get started with the panel for the first question. This is tomorrow. Can you walk us through how a circuit bug could result to the loss of funds for ZK roll up? So, yes, ZK rollups use, of course, zero knowledge proofs to prove that they have done a correct computation. Usually this is done by the company running the roblab, if decentralized or something. So that's a way to say, okay, we are being honest, but if someone can manage to send a proof that maybe he did a transaction that they weren't allowed to, or they were managing funds that they were not theirs, they can, of course, try to steal the money from the roll up or from anybody, or do any other malicious activity. So the prover is like the heart of it. If you manage to crack the verifier, you can do whatever you want.
03:32:53.590 - 03:33:40.530, Speaker A: I don't know if you want to add something more. So, yeah, I guess I think it's more or less the same as any ZKD app, right. If your verifier doesn't capture the semantics of your intended behavior. Precisely, then that opens the door to an attack. So if you have a trace that transfer funds and then one of the opcodes is not modeled properly, then I can say that instead of transferring x, I transfer ten x and then exploit the roll up. Now I have nothing else to add. Let's move on to the next.
03:33:40.530 - 03:34:30.252, Speaker A: Yeah, can you speak closer to the mic? I can barely hear. Yeah, I guess we can move on. So the ZK ecosystem is pretty nascent, but I'm wondering if you guys have any specific exploits that you wanted to talk about in the past. Yeah, I'll let you take it off from here if you want to detail any of the reasons why exploits happen and the projects involved. Yeah, I can start with that. So I guess one interesting one was like the tornado cast people that hacked themselves. So there was like an issue in one of the hash functions in circumlib, there was like a missing constraint, and that opened the gate to kind of drain all the funds.
03:34:30.252 - 03:35:21.180, Speaker A: Then they kind of moved them to a new contract and conducted everybody to mitigate things. So I guess missing constraints is kind of more common thing that you'll see. So yeah, I guess that was kind of interesting, because the protocol itself called it and mitigate everything. Yeah, I could add some other ones. For example, exploit on the constraint level, so probably some unconstrained value or something. There also have been exploits like on the layer below, for example, Ariel phones, some exploits from ccash that allow you to print money. So that was really bad.
03:35:21.180 - 03:36:04.570, Speaker A: The good thing is usually people who know this, they are kind of good actors and they report it. And nowadays it has actually happened for a while. I think maybe you have some contract examples, but the most critical ones I know they were solved with anybody losing boons. And yes, then protocols that are widely implemented, for example, frozen hard exploit, allow you to exploit the value implementation of Pierre Shamir. That's also on the low level. And then you have exploits on the high growth level, maybe on the protocol, how you reach consensus, how the nodes operate. So yeah, for me those are like the three points you can actually exploit, and you will see things done.
03:36:04.570 - 03:36:54.260, Speaker A: Yeah, so I have not run specifically in any ZK errors directly. Compilers have errors, lots of errors. Right. And we are taking a big risk already with compilers. And then additionally we have all the problems with the ZK circuits for the vms. It's a huge trust base that we build everything up on. So we simply have to trust millions of lines of code in order to verify correctness.
03:36:54.260 - 03:37:53.580, Speaker A: So it's a serious challenge, no? Yeah, thank you guys for that. So this is a good primer on why ZK security is important. And so we've dived into a couple of examples of ZK security exploits. But before this, I guess it's important to dive into advice for current ZK projects about how they can prevent exploits. And I guess a good question to get us started on that is how do we know if circuits improving systems are secure? I'm sure you guys'experience within that field. What advice would you give to starting CK companies? Okay, I can start. The short answer is, of course, if you have been audited and well tested by someone credible.
03:37:53.580 - 03:38:40.264, Speaker A: The best case scenario is if you have formally verified something. So the longer answer involves several security checks. You have to make sure that you have avoided overflows in fields, that you have instantiated cryptographic primitives correctly. That mostly applies for the proving side of things like the back ends of ZK systems. And another thing that I personally think is important is if the projects and the circuits have been open source. So the more pair of eyes on the code, the better. I totally agree with the open source part.
03:38:40.264 - 03:39:21.032, Speaker A: It's really important for people to be able to understand the code base and see if they. The more eyes you have on the code, the more chances you have to find a bug. So that's really useful. I like the definition that gave some other cryptographer, I don't remember the name right now, but probably your system is secure if enough people try to break it during a long time. But that's not really satisfying because it's like, okay, I can't do anything. All these kind of things like having really clear code, open source, that's probably really important. Yeah.
03:39:21.032 - 03:40:40.384, Speaker A: So there are many facets to security starting with basic correctness. Many security issues are because of lack of correctness, and you don't really know unless you formally verify it. And even then you make lots of assumptions about the environment, about the formal verification tools, about the language, about the specification. The deeper you get into this, the more you realize how fragile everything is, to a point where it's almost scary, actually how fragile everything is. So I think we may actually need to rethink the whole approach. That's why I firmly believe in verifiable computing in general, because mixing the complexity of the programming language or computing environment with the complexity of the zero noise circuits and with lack of specifications and with programmers who do not have a background in formal verification or analysis even, and with auditors who are in high demand, but not enough. Right.
03:40:40.384 - 03:41:26.944, Speaker A: So I think there are lots of problems that we have to worry about, and I don't have a solution. I'm just noticing that this is a very complex field that accumulates all the problems we had already in the programming language and software engineering field. And now, on top of it, we also put all the ZKe complexity. Yeah. And I guess Zke also is kind of like a new paradigm, right, where people have to think as two different entities. Right. You have to think from the side of the prover and then the side of the verifier, and you have to define kind of like sufficient and necessary conditions that kind of guarantee the correctness of your system.
03:41:26.944 - 03:42:22.380, Speaker A: And I think that's kind of like one of the most common things we've seen in our audits. And another thing, maybe moving to the advice part that you asked, what advice you would give to new ZK developers. I would say target correctness first and then efficiency, because we have seen a lot of cases where kind of like, premature optimizations kind of lead to very messy situations. And then if your code has been kind of focused on optimization first, it's hard to kind of refactor it to make it correct. Right. Because you are already into the weeds. An alternative that might be interesting is like, to simply have more proverbs and verifier deployed.
03:42:22.380 - 03:43:27.664, Speaker A: So the probability of finding the same exploit in many proven systems, in many circuits is lower. So that might add some kind of extra resilience. That might be. Yeah, it's something downstream of this is what would you have seen as the most common know, I guess cost us with your experience as an auditor. Gregor, as somebody who's seen a lot of audits before, and Mara, as somebody who's written a lot of circuits, what would you identify as the most common mistakes that you've seen cryptographers make that they can avoid? Number one should be things around the field, arithmetic, because now you have languages where the overflow is not like c, where it's undefined behavior, like the overflow. Now you know exactly where the wraparound is going to end up. Right.
03:43:27.664 - 03:44:14.542, Speaker A: So attackers can leverage that pretty often. Can you hear me? Yeah. Another class is kind of like missing constraints. We have seen a lot of systems that are essentially are nondeterministic. For example, you can prove that you have two balances, and then if that's the case, people will always choose the. And. Yeah, I guess that's kind of like the big cases that are kind of like ZK, then the other kind of big class of errors that we've seen is kind of like the ones that in the intersection of the ZK and DeFi component.
03:44:14.542 - 03:45:22.116, Speaker A: Right. Because they do usually have to interact. And we have seen several cases where the ZK developer thinks that the tech will be performed on the defi side, and then the defi side believes that the tech will be done on the ZK side, and nobody does the tech. Yeah, I think also we touched on the fragility of the ZK ecosystem, and something that's been talked about recently is the composability within the ZK space that's proliferated within the traditional programming languages of leveraging off of audited modular code. But, yeah. Curious if you guys have any specific takes on that. Ideally, we should not write ZK circuits, we should not craft them, we should not custom made them for applications.
03:45:22.116 - 03:45:50.220, Speaker A: We should have tools that generate them for us. Right. Directly. Correct. By construction, like the ZkllvM approach is a great approach in that direction. So once we know what we want the circuit for, let's have machines generate the circuit for us. Because if we can generate code, that's common knowledge in program verification, that the best way to, or the alternative to formal verification is program synthesis.
03:45:50.220 - 03:47:08.070, Speaker A: Generate the programs directly from their specifications, and then you don't have to verify them. They are correct by construction. To be nice to have something similar to ZK circuits, because sometimes even the business logic of the contract is complex enough that we mess it up, right? So if on top of that, we combine it with designing a circuit, with how many polynomials, how many parameters, there are lots of ad hoc choices made that can be in conflict with the business logic decisions that you make. So I just think the ideal, if we are to dream about an ideal world, it would be nice to generate all these circuits automatically from the security properties, maybe even, or correctness properties. Is that too much to ask? I don't know. No, I'll check on that. And I think right now, most of the languages are kind of like writing assembly in the traditional kind of programming style.
03:47:08.070 - 03:48:58.744, Speaker A: And that also not only complicates the life of developers, it also complicates the life of auditors, because you need auditors that kind of are familiar with several languages, or they have to learn a specific DSL, or they have to learn an API of ZK framework that uses rust, for example, really well. And even in that case, as an auditor, you have to essentially run a compiler in your head, right, to kind of see what are the generated constraints and how the ZK system behaves. So yeah, even if we had this pipeline that kind of spits out something from a higher level, it will also make the life of security analysts easier. Plus talking from Verida's perspective will also make the life of creating automated tooling for security. Because right now we kind of have to try to support several different frameworks and ecosystems, each of which kind of makes their own design decision. And it's kind of like tough honing in to what you mentioned, Costas, about languages and dsls within the Zk ecosystem. Can the language or framework used to write a Zk system affect the type of bugs that might be introduced? So definitely.
03:48:58.744 - 03:50:34.864, Speaker A: So I think there's a split between dsls that are kind of used only to run to write ZK components or languages are Zk centric. And then you have frameworks that are written in a general purpose language like rust, and then developers have to use an API to generate circuits, and then these circuits participate in the whole protocol. So in the first case of the ZK specific frameworks, there we see more kind of like Zk type of bugs, like overflows in the fields, like unconstrained signals and so on and so forth. So for the latter case we kind of see those as well. But then you also see more traditional bugs, right? That then if their values kind of flow into the ZK API will also affect the generated circuits. So another difference we can make is between universal circuits and non universal circuits. So let's say you are writing, trying to prove a vm, like it could be maiden, Kaido or CKE VM, or that kind of vms, then your circuit is going to be unique no matter the program.
03:50:34.864 - 03:51:09.672, Speaker A: That's what we call a universal circuit. So in that case, the probability of programmer that's trying to write its own program of adding an unconstrained value, it doesn't exist because the programmer is not actually allowed to change the constraints of the systems. So maybe that's another difference you can make. In universal circuits, the user can of course add security backs because it's their program. They might do something wrong. They might use non deterministic memory. That means they might hint something that might be wrong.
03:51:09.672 - 03:52:40.922, Speaker A: But usually there won't be any mistakes on the constraints. And of course the other examples that were given before, when you are trying to, if you are the one writing manually, constraints the probability of making a mistake of having an under constraint value is really high, and then you have intermediate cases, maybe things that compile into circuits so they are more higher level, but not that higher level as to have a universal constraint. Yeah, Gregor, if you want to jump in here, but, yeah, happy to move into the next question. Yeah, I guess talking more about the future now, about the future of ZK security, what do you see as the biggest challenges that you see in the ZK security space in the near future? I think we've talked about a couple of them, fragility dsls. But alternatively, what would you interpret as the thing that you're most excited about in the coming year as innovations within the ZK security space? Happy for you guys to take the question wherever you guys want to take it. Well, I can start. I can tell you what I'm really excited about, and I think that is the marriage of two important domains that are both concerned with correctness, but they lived separate lives until now.
03:52:40.922 - 03:53:45.314, Speaker A: One is the formal verification community and formal semantics community, and the other one is the verifiable computing community, mathematicians and logicians versus cryptographers. And they don't talk to each other enough. They don't talk to each other almost at all, probably. Veridice is one of the few companies that will try to use formal verification to analyze cryptography, but I believe that we can do more than that, more than just using formal verification to analyze cryptography, but to put them together in a way that they are more than the sum of the parts. I know I'm unfolding my own agenda now, but I truly believe that we should go through mathematical proofs. So computation is mathematical proof, so why mix it with ZK? So let's leave the computation in its world, produce mathematical proofs, and then let's use cryptography to verify mathematical proofs. Because if we do that, then we get end to end verifiable computing.
03:53:45.314 - 03:54:46.600, Speaker A: But going through the rigor of mathematics by construction forces us to do it right. It doesn't mean that we will not have security leaks or that we'll not have other problems, because the program itself may be buggy, but at least we don't take the additional risk that we mix the complexity of cryptography with the complexity of the programming language implementation. That's what I believe could help us innovate, actually even more, both on the ZK front and the programming language front. Imagine that you want to invent a DSL for a particular kind of chain or upchain. You have a perfect DSL for your application, and then you'd like to have ZK, that's a nightmare. Now you have to hire a team of cryptographers and a team of programming language people and come up with a circuit for the DSL, right. That's a huge effort which is not reusable by others.
03:54:46.600 - 03:56:23.046, Speaker A: I think to me that would be really how I see the future. That what would make the future exciting, I think where we put together two important domains that lots of years of research has been done in in a way that is more than the sum of the parts. So yeah, I kind of agree. And then going back to what we're saying earlier, right, higher level abstractions where you kind of specify what you want to kind of achieve with your privacy preserving application and then you have a layer that automatically translates to something that you know will run correctly. That's kind of like an exciting kind of future to me. But to come back to the silence part of the question then the challenge is there is kind of like, okay, now your trust base becomes like this compiler or synthesizer or call it whatever you want. So that will be the next big challenge if we go down that road and then touching upon kind of another challenge that I think it's kind of like a challenge even now navigating the alternatives like one has when picking with ZK framework or language to pick is going to become harder and harder.
03:56:23.046 - 03:57:55.660, Speaker A: And I think byproduct here is that we will need more kind of education for developers to make sure that they pick the framework that fits their needs and they just don't pick something just because it's currently kind of like the hot thing to use. So there are things I'm waiting for Cam, that I expect we can do in the future. I think moving to some higher level language, maybe starting to code Justin Ras or whatever language people know, make the proofs that will probably make system safer. If we get the proving systems to go fast enough, maybe we can dream of not even using one proverb, then go just one time, call one time in Russ or whatever, use many different provers, have many verifiers and that's going to be much, much more secure. The other thing I'm quite excited about is that more people are showing in the space every day we have more and more people. So at the beginning, I don't know, maybe you have like 1050 people that could really say if there was a problem. Now we have a lot more of eyes on these things and combining this with open source projects will make that in the long term our projects are going to be much more secure, much so I'm quite optimistic on all of this.
03:57:55.660 - 03:58:25.486, Speaker A: Yeah. Related to the future of ZK and the things that we're excited about. Although there is a lot of things to build towards, it is also our responsibility to build towards this future responsibly. Right. And balancing the trade off between ZK security and the pace of innovation. Just to set the scene here also. Yeah, ZK rollups started almost four years ago, or the ZK space kind of even at large.
03:58:25.486 - 03:59:03.804, Speaker A: And a lot of our code is finally going to mainnet after years of development inside house. And so, yeah, I'm curious about you guys'perspective on balancing this tension. I think this tension is not something special in CK. Happened a lot in web two and traditional computing. There's always maybe a tension of people wanting new stuff and people wanting things to be secure. And if you want things really fast, maybe you want to, hey, let's push these changes. I want this new feature.
03:59:03.804 - 03:59:46.770, Speaker A: I want this to go really fast. I need this that generates attention between change and moving forward and having something stable and secure. I don't have the best answer on how to handle that, but think it's a challenge we'll have to deal with. Yeah, I guess I agree that this is not kind of like a ZK specific problem. It's kind of like you always have this mentality of move fast and break things type of thing. I think what's a bit scarier about the case, we have to be a bit more careful because especially for privacy preserving applications. Right.
03:59:46.770 - 04:00:44.720, Speaker A: There might be like an exploit out there that we don't know about it yet. Right. Because things are encrypted most of the time. So I guess maybe to balance things. I would say if we kind of ask as community, kind of decide on a set of primitives that we are interested in kind of using to build things moving forward, and have these kind of primitives being well tested and well audited or security analyzed and being open source, I think that kind of will open the door to kind of building more secure things into the future a bit faster. So I think, for example, polygon started some sort of initiative to that front with plonky three. Sorry.
04:00:44.720 - 04:01:53.576, Speaker A: Where they have gathered a set of primities for implementing polynomial iops more efficiently. So I think things like that will kind of help us kind of balance the tension between the two. Yeah. Related to that about how do we build sustainably and move quicker into the future. What about security checks for ZK circuits and automating that I know Costas, you might have some specific thoughts on mean like, you know, similar to any other kind of know. At veridice, we kind of develop tools to help us during our audits. So for ZK specifically, we do have static analyzers that will help us kind of cut the low hanging fruits on a system that we are about to addict.
04:01:53.576 - 04:03:05.704, Speaker A: And there we kind of look for specific patterns. Now, these patterns are a bit different. We kind of adapted and kind of developed new, new kind of detectors for ZK specifically. And then you move to kind of more exotic stuff where you formally verify properties for your circuits and then do have a tool that kind of proves that your circuit is at least deterministic, right? Whatever it calculates is a function, it's not a relation. You cannot have case where you have, for example. So that's kind of like one step forward to kind of more sophisticated. So yeah, the next step after that will be to kind of verify arbitrary properties, which I think people do work on it.
04:03:05.704 - 04:04:02.172, Speaker A: We have done some work in very nice as well. So yeah, I think that opens to ZK has a lot of opportunities for automating security. So Costas answers the question so thoroughly that there is nothing left for me to add. I just want to emphasize that formal verification is the only way to assure correctness. No matter how much analysis, static analysis you do, or template or template detection, error programming patterns, you'll never be sure unless you formally verify. And even then, even when you formally verify, then you are still not sure. I know this is not a positive answer, it's just that it's very hard.
04:04:02.172 - 04:04:54.378, Speaker A: I personally believe that we need to rethink the whole problem going through rigor, through mathematical proofs. People have done that already in the programming language community. Initially, programming languages were just compilers, interpreters, and that's it. What is c? Well, whatever GCC implements, that's the definition of C. But then things evolved and we have formal semantics of languages, and those define what the language does. So similarly, I think we should take the problem at least equally seriously with ZK. Yes, ZK is mathematics polynomials, yes, but it's a different kind of mathematics, right? Yeah.
04:04:54.378 - 04:06:00.470, Speaker A: I don't have a clear solution, more like a belief that we need to go through mathematical rigor between computation and ZK and not mix them. Yeah. How do you think moving forward, we should reframe that problem that you mentioned for the new age? Is there some kind of shift in thinking that needs to happen? What are your thoughts on the role of formal semantics in CK security moving forward? I guess even overall, the role of formal verification. So formal semantics and formal verification are different beasts. So formal semantics is the enabler for formal verification. We cannot do formal verification if you don't have a formal semantics, because you cannot define the problem of correctness unless you have a formal semantics of the programming language. It's not clear how you can define that this program is correct, or that it does anything rigorously unless you have a formal semantics.
04:06:00.470 - 04:07:00.930, Speaker A: But the same formal semantics can be used to verify or to enable verifiable computing, where use the formal semantics to generate mathematical proofs of execution and then verify those with ZK circuits. And this way you don't have to worry about the implementation of the programming language itself, whether you capture the right semantics. And that at least eliminates some headaches. At least now you have to worry about the program only. The program may have security problems, but at least I eliminate the programming language and the compiler from the picture. Yeah, in order to do that, we need a formal semantics. Right, and many languages don't have formal semantics, unfortunately, but many also have formal semantics, including the popular languages.
04:07:00.930 - 04:07:28.930, Speaker A: But again, without formal semantics, we cannot claim anything about correctness. It's just the best effort. Great. Yeah. I'll also leave it to the panelists. If there are any certain questions that you want to push back with each other, or you want to talk about anything in specific in particular, feel free to also the audience if they have questions. Oh yeah, let's open up to the audience.
04:07:28.930 - 04:08:12.246, Speaker A: Yeah, happy to feel. Oh, we have one over there. Sorry. Hi. What's the security vulnerability for ZK that keeps you up at night? You're most scared of the ones we don't know about. If we know about them, we can probably fix them before. You usually don't have any clue of what can come.
04:08:12.246 - 04:09:15.848, Speaker A: We have the areas that maybe they will be attacked, but you never know. No, you read my mind. That's what I was going to say as well. Yeah, well, in the world of formal verification, there was a case where the same question was asked about interactive theory improvers like Cock lean and so on, and the main fear was that maybe I can prove false with an empty theory. So if you can prove false, starting with an empty theory, it means that you can prove everything, right? So the whole thing doesn't make any sense whatsoever. And people prove false using coke the first time, and then they fix the problem, and then somebody proved again false. So that's the nightmare of formal verification.
04:09:15.848 - 04:09:38.160, Speaker A: And if you do what I suggested to go through mathematical proofs, if you have a framework where you can prove false in an empty theory, a logical foundation. You can prove false from an empty theory. Then the whole thing collapses. You can prove that factorial of three equals one. You can prove that sum of numbers up to ten is minus three. You can prove everything. And that's scary.
04:09:38.160 - 04:10:33.506, Speaker A: That's the ultimate worry that I have, that I have a proof system, logical proof system that can prove false. That's inconsistent and that's not obvious. It's very hard to prove things mathematically consistent. I just had one follow up question based on your answer. If you sort of were to observe the boundary of known and unknown, what boundary is most blurry, so to speak, that maybe keeps you up at night or gives you some fear? So not sure if I completely understand the question, but I would answer to do tell me if this doesn't answer. I agree with what has been said. Probably you can qualify the possible exploits in different things.
04:10:33.506 - 04:11:09.134, Speaker A: I think, for example, the lowest kind of exploit usually is denial of service. That is not related to constraints and probably doesn't worry us that much. The worst thing is breaking the whole proving system. And as they said before, proving whatever you want, no matter the statement. And in the middle maybe just breaking one individual circuit. When you say, okay, this application is broken, but the rest is good. So maybe at least the damage is more localized, not affecting the whole system.
04:11:09.134 - 04:11:42.694, Speaker A: So maybe that's the categorization of the unknowns. We probably fear more the unknown of something that breaks everything. Yeah, just to maybe rephrase my question. I guess what I'm getting at is I want to get an understanding of your intuition for where security vulnerabilities may lie. Even though you don't have empirical evidence to prove that they're there. Okay. We have seen vulnerabilities in all the layers.
04:11:42.694 - 04:12:28.578, Speaker A: To be honest. We have seen papers that claim a proof and the proof was wrong and they have been hacked. I think that's probably not the most common things. Then there are a lot of implementation mistakes when people think they are implementing a protocol, but they make decision change or the code is not doing what they think they do. So that's probably more related to format verification. So maybe that's a way to solve those kind of implementation issues. Then maybe as we keep adding things and adding more and more and more things and building things on top of the other things, picking all the teams are picking something that someone else made and tweaking it a bit and changing and adding some changes.
04:12:28.578 - 04:13:09.990, Speaker A: Those changes can also lead to mistakes because maybe the original author had a really good idea and the next one comes and just copy them and doesn't understand something and it gets changed and it gets exploit. Maybe that's the area. I'm not sure if that answers a bit the question, but what keeps you at night and how can we help you? I don't know, the stuff that you guys are doing. We try to find solutions. Right, right. But you're finding solutions at the edges, so to speak. So it's really what you guys discover.
04:13:09.990 - 04:14:17.180, Speaker A: I mean, it's truly scary. If you can prove everything, right, then it means that you cannot trust the proofs. Why do we have them? Right? To answer again, like your previous question with the more specificity that you wanted from the second one, I guess, to me are kind of like very deep, logical bugs that typically as either developers or security analysts or whatever title you want to put on someone, we have kind of like a very limited buffer on how far ahead we can think in terms of how an attack might play out. Right. So at the end of the day, the most critical bugs are like logical bugs. Right. So if there is a bug that requires, I don't know, like ten steps to be exploited, and then that will drain the whole protocol, that will keep me up at night for sure.
04:14:17.180 - 04:15:05.990, Speaker A: Yeah, I guess that's the zero days that we're talking about. So, yeah, thank you so much for elaborating on that. If there's any more questions, happy to field them. But, yeah, feel free to come up to us and ask us any questions after the panel. But, yeah, thank you guys so much for listening in and, yeah, hope you guys enjoy the rest of the event. All right, thanks for everyone who's left. Thanks for making it to the bitter end.
04:15:05.990 - 04:15:20.240, Speaker A: Feel free to follow us if you want to see the live stream recording event follow ups, what have you. Also, we're going to be sending out a survey after this, so please take a couple of minutes to let us know how we did. And thank you so much. Enjoy your time in Denver.
