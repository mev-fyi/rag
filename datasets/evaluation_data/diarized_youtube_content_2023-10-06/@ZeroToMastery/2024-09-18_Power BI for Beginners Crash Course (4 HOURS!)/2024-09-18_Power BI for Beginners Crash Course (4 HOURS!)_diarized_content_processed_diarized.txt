00:00:04.520 - 00:00:33.999, Speaker A: Ready to take your data analysis skills to the next level and build interactive real world dashboards with Power bi. Well, you're in the right place. In this video, Travis Kuzik, who has spent over a decade crafting data solutions for Fortune 500 companies, will guide you through the basics of Power BI while you build a really cool project. Along the way, you will be building a UFO Sightings dashboard using real world data. Yes, you heard that right. We're going to dive into the world of UFOs while mastering Power BI.
00:00:34.127 - 00:00:34.983, Speaker B: You're welcome.
00:00:35.119 - 00:01:34.321, Speaker A: As you can see, this crash course is about four hours long and it'll take you from a complete newbie to someone with a solid foundation in Power bi. But by the end you're going to be craving a whole lot more. So we strongly recommend you check out Travis's complete Power BI Bootcamp course which has over 16 hours of additional in depth lectures. You'll learn everything from importing and transforming data to building visuals and KPIs that'll make your work shine and you'll dive deeper into topics like DAX programming, advanced Power query and data modeling. Plus you'll get to work on some advanced projects that'll help you stand out as a business intelligence analyst, including a very cool Bigfoot sightings project, if I say so myself. So if you like the sound of that, make sure to click the link in the top right hand corner or check out the description down below for the full course. Lastly, if you enjoyed this crash course, please help me show Travis some love for providing this amazing content to us for free by dropping this video a like and leaving a comment with your thoughts, questions or feedback down below.
00:01:34.321 - 00:01:42.565, Speaker A: He'll be reading every single one. Alright, that's enough chit chat for me. Let me hand it over to Travis so you can dive into the world of Power bi. Enjoy.
00:02:01.895 - 00:03:03.945, Speaker B: All right, so now that you've got an idea of what this course is all about, I imagine you're ready to dive in and start building cool stuff like right now. And don't worry, we will very soon. But first let's zip through a brief thousand foot overview of what Power BI is, what it does and where it fits into the modern business intelligence landscape. In as good a place to start as any is a simple definition. What is Power bi? Well, per Wikipedia, Power BI is an interactive data visualization software product developed by Microsoft with a primary focus on business intelligence. And of course that begs the question, what is Business intelligence? So this is my definition. Business Intelligence is the art and science of transforming data into actionable insights that help organizations make decisions.
00:03:03.945 - 00:04:41.927, Speaker B: So where's the Business Intelligence, or BI for short, in Power bi? Well, Power BI is a complete BI solution in a box, so to speak, that combines capabilities for fetching data from a variety of sources, modeling that data by forming relationships between different data sources, analyzing that data, then translating the analysis into a wide range of visualizations. Power BI's components and their functionality by design align with a typical business intelligence workflow. Let's explore those components a little bit more deeply. So Power BI has a built in ETL tool that's extract, transform and load called Power Query that enables you to fetch data sets from a variety of sources, text files, spreadsheets, databases, even the web and more, and then clean and transform that data to prepare it for analysis and visualization. Now, while Power Query is easy to use, since most operations can be performed with a few mouse clicks and no code, it's also industrial strength, capable of working with large datasets consisting of millions of rows, depending on your machine. Of course, Power BI also has strong capabilities in data modeling. After pulling in distinct datasets with Power Query, often from different sources into one Power BI file, the next challenge is to somehow analyze all of this data.
00:04:41.927 - 00:05:25.855, Speaker B: A whole Power BI lets us form a data model in which we can link those separate data sets together so they can be analyzed holistically. And perhaps what Power BI does best of all is visualize all of that data. Power BI has a wide range of visually impressive charts that can tell a story about all that data you fetched, transformed and modeled. This is typically the end product of the BI workflow. Executives usually don't want to see raw data, but rather a nice summarization. The more visually pleasing and easily understandable, the better. That leads them to insights and helps them make quick decisions.
00:05:25.855 - 00:07:21.245, Speaker B: Now, supporting those visualizations is your analysis of that data. And Power BI has a built in formula language called dax, which we'll be exploring in much greater depth later. That allows you to create incredibly powerful and sophisticated calculations which can then be visualized in one of Power BI's many charts. So where does Power BI typically fit into the business intelligence ecosystem in the corporate world? Well, if you're lucky enough to have a data engineering team that handles wrangling, transforming and modeling data for you, data which typically ends up being stored in some kind of enterprise database, you'll likely use Power BI mostly as a visualization tool to translate all of that nice data into compelling visuals and useful insights. At the opposite extreme, Power BI can be and often is used as an end to end business intelligence solution with a robust ETL tool for fetching and transforming data from a variety of sources, powerful data modeling capabilities, a vast array of slick visualizations, and a supercharged formula language to fill those visualizations with metrics and calculations of most any complexity. Power BI can handle a lot more than you'd think a desktop application ever could, but the most common scenario in my experience is somewhere in between one in which you do have access to an enterprise data store that has some of what you need some of the time, but not all of what you need all of the time. In those cases, you'll use Power BI mostly as a visualization tool when the datastore has what you're looking for, and as a do it yourself business intelligence in a box solution when it doesn't.
00:07:21.245 - 00:08:21.305, Speaker B: So it turns out that there are actually a couple of different variations of Power bi. Power BI Desktop, which you'll be installing in the next video, is, as the name implies, an application you install on your local machine. It's free. Yay for that and has everything you need to build incredibly sophisticated visualizations, reports and end to end BI solutions. This is the tool we'll be covering in this course. However, there is another flavor of Power BI Pro, which is a cloud based subscription service offered by Microsoft that provides sophisticated options for sharing reports, collaborating with other report developers, and automating workflows. If your company uses Power BI widely, there's a very good chance they already have a subscription to Power BI Pro, and this is the platform on which you'll ultimately be sharing your reports.
00:08:21.305 - 00:09:14.545, Speaker B: Now, Power BI Pro is a paid subscription service and actually requires a business email to subscribe. Something like Gmail or Hotmail won't work, for example, so we won't be covering it in this course. However, the good news is, regardless of whether or not your organization has Power BI Pro, the vast majority of the work still takes place in Power BI Desktop. This is where you transform and model data, analyze it, and build visualizations. Power BI Pro is more of an administrative tool that governs how reports built in Power BI Desktop are organized, shared, published, etc. So the bottom line is you can learn everything you need to build amazing BI solutions right in Power BI Desktop, and we'll get started with that in the next video. I'll see you then.
00:09:14.545 - 00:10:33.745, Speaker B: Welcome back. So now that you have a conceptual understanding of Power bi, let's take a quick tour of the actual Power BI interface where we'll be spending most of our time for the remainder of the course. So we're starting off here in the Power Query Editor. Now, Power query is power BI's built in ETL tool that we can use for fetching data from a variety of external sources and then transforming it so that it's more usable for analysis. Now, I don't expect you to understand what most of these buttons and tabs do, but I do want you to take note of how the functionality here at the top of the window is organized. Note that we have a Home tab here that has all these different buttons for doing all sorts of different things. But then if I hit this Transform tab, we see a whole different array of functions and commands and so on and so forth.
00:10:33.745 - 00:11:44.935, Speaker B: Basically, this series of tabs is power BI's way of organizing a large amount of functionality in a way that's still manageable and relatively straightforward to navigate. This structure in which related functionality is organized under different tabs, generally referred to as the Ribbon. And if you've used other Microsoft applications like Microsoft Excel, for example, it may be familiar to you already. Now closing out of Power Query, that takes us into the actual Power BI interface. And the first view I want to explore here is actually not this very splashy looking report, but instead the Data view, which we can access by clicking this little grid icon over here on the left. Now, the Data view is where we can preview the raw data in any of the data sets we've imported into our file. And as you can see, all I have to do is just select a data set over here on the right and we get a preview of the data that's inside that data set.
00:11:44.935 - 00:12:59.975, Speaker B: Also, if I click this little icon to the left of a data set, we get a list of the fields that data set contains and additional information about those fields denoted by these little icons you see next to each field. Now, note that just like when we were in Power Query, the functionality in this view is still organized in different tabs. So if I hit the Home tab, I see a whole different set of widgets and functions versus what I see if I hit the Help tab versus what I see if I hit the Table Tools tab. Next up I'll switch over to Model View, also known as Diagram View. This is where we can form links or relationships between the different data sets we've imported so we can analyze them holistically. If you've ever worked with relational databases, this schematic here probably reminds you a lot of an entity relationship diagram or erd. But if not, the intuitive graphical nature of this view is actually a terrific introduction to some of the most important concepts in regard to relational databases.
00:12:59.975 - 00:14:18.325, Speaker B: Now, finally, I'll switch over to what is inarguably the most interesting view in a Power BI file, which is Report View. This is where we build the end product of our BI workflow visualizations. These visualizations are dropped onto the Report canvas, where they can be moved around, resized, and combined with other visualizations as we see here. Moreover, Report View can further be broken down into pages, essentially whole different canvases where we can organize different sets of visualizations. If you've worked with Excel or some other spreadsheet software, this concept of pages or tabs should be pretty familiar. And once again, even in this completely different view, the functionality we see at the top of the window is still organized into tabs, each of which contains a different but related set of Power BI features. Okay, so now that you understand the lay of the land, so to speak, it's time for you to actually install Power BI on your computer so you can get your hands dirty building out all the projects we'll be tackling throughout the course.
00:14:18.325 - 00:15:14.595, Speaker B: We'll cover that surprisingly painless process in the next video. See you then. Hey, welcome back. So before we can use Power BI to become wizards in the way of data, we first need to actually install it on our computer. But fortunately, this process is super easy. Now, it is worth noting that Power BI requires a Windows machine, so if you don't have one, you can try installing a virtual machine on your computer. But that is definitely going to be a much greater technical challenge than simply installing Power BI on a regular Windows machine.
00:15:14.595 - 00:16:29.429, Speaker B: So because the Power BI landscape is constantly shifting, and the exact sequence of links you need to follow to install Power Bi might very well change. Ten minutes after I record this, I want to follow the most repeatable path, which is to simply Google Power bi. Now doing that, the very first link listed takes me right to Microsoft's Power BI website, and here I'll want to click this little caret next to the Products heading. And among these products listed, I'll click Power BI Desktop, since that's the application we'll be working with in the course. And that takes me to another page where I'm able to click a link to download Power BI for free. Now note that when you click this, you'll be taken to the Microsoft App Store, where you'll follow some very straightforward prompts to complete the installation. So once you complete the installation process and open Power Bi for the first time, you're most likely going to see a popup window that looks something like this, with a prompt suggesting that you should sign in to publish reports, access certified data sets, etc.
00:16:29.429 - 00:17:25.375, Speaker B: Etc. And as tempting as this get started button is here, this process of signing in is actually for Power BI Pro, which as I mentioned in the last video, is a paid service that requires a business email account. So we'll actually just close out of this pop up window. And now in this new Power BI file, I want to configure a specific option within the file to ensure that we don't encounter any issues with the data we'll be working with. Specifically, this option pertains to how different locales across the world have different ways to display data. In particular, date and currency values tend to be formatted in a wide variety of ways depending on where you live. In any Power BI file that you create, Power BI will be using your operating system's locale as the default locale.
00:17:25.375 - 00:18:50.161, Speaker B: This locale setting is very important since it determines how Power BI interprets text, numeric and date time values in the data we import. So several of the CSV files we'll be working with throughout the course contain date and currency fields that follow United States specific formatting. So if you're located outside of the United States, your system's locale settings may keep these fields from being recognized as date or currency values. This can result in errors, while also preventing us from using Some of power BI's built in tools for working with those specific kinds of data to ensure consistent results with Power BI when working with the data in this course. Regardless of your current operating system's locale settings, you can change Power BI's locale setting to English United States on a File by File basis. So to do that, we'll click the File tab up here in the Power BI ribbon, and then Options and Settings and then Options and then under Current file I'll select Regional Settings and just confirm that the locale for import is English United States. That was the default for my installation of Power bi, but it might not be for yours.
00:18:50.161 - 00:19:45.745, Speaker B: So here is where you'll want to make sure that locale for import is set to English United States. And again, just to reiterate, you'll want to do this on a file by file basis. So any new Power BI file you open, you'll want to follow the same sequence of steps to make sure your locale for import is English United States. All right, so with Power BI installed and configured, it's time for the fun stuff actually getting your hands on real Data in Power bi. We'll kick that off in the next video. I'll see you then. Hey, welcome back.
00:19:45.745 - 00:20:55.635, Speaker B: So now that we understand the general landscape of Power BI, let's jump right into Power BI's ETL tools, which are the foundation of the overall Power BI workflow. But before we get ahead of ourselves, what the heck does ETL even mean, especially in the context of Power bi? So ETL is a super common acronym in the business intelligence world, which stands for extract, transform and load. And this is exactly what we can do with a variety of external data sources in Power bi. First up, the extract phase of the ETL process entails Power BI extracting data from a wide range of sources. Text files, comma separated value files, databases of many kinds, spreadsheets and so on, just to name a few. But extracting external data is just the beginning. Next, we can use power BI's built in query editor, also known as Power Query, to apply all sorts of transformations to that data.
00:20:55.635 - 00:22:04.945, Speaker B: The point here is to take the kind of messy raw data that commonly exists in the real world and transform it, prettify it, so to speak, so that it meets our particular needs. Then finally, we can load that transformed data from Power Query into Power bi. It's not hard to see why these steps are super important in the grand scheme of things. While analyzing and visualizing data may be our end goal, and for many folks the fun part of being a data analyst, the reality is that you first have to actually get a hold of that data and very often transform it so it can be used for all those sophisticated charts and analytics you're going to impress your boss with. So now that we have the thousand foot view of the ETL process in Power bi, let's put acronyms and definitions and other boring stuff behind us for now and dive into the nuts and bolts of how this all actually works on a live data set. So, as we now know, extracting data is the first step in the ETL process. So that's what we'll tackle first.
00:22:04.945 - 00:23:06.545, Speaker B: Now, as our example, in this video and the first part of the course, we'll use a CSV or comma separated value file that contains data on Bigfoot sightings in the United States. And yes, by the way, this is a real data set, whether or not you believe you know Bigfoot is real. Now, if you're unfamiliar with the comma separated values format, it's a very, very common format in which raw data is stored in the real world. So to extract this Bigfoot data into Power bi. We'll first click this Get Data button on the Home tab of the Ribbon, which will display a list of the various sources we can fetch data from. And as you can see, it's a pretty long list featuring Excel spreadsheets, text and CSV files, SQL Server databases, and more. And if we click the more option at the bottom of the dialog box, we see an even longer list of sources.
00:23:06.545 - 00:24:05.845, Speaker B: So long, in fact, that there's even a Google like search field that lets us enter a search term to narrow the list down a bit. So if we were looking for databases, for example, I could just type database here. And now we see just a list of databases that Power BI can fetch data from. But again, for our first example, we're going to keep things straightforward and just fetch our data from a CSV file. So let's get rid of our search term and then select the text CSV option from the list and hit Connect at the bottom of the dialog box. Now, Power BI brings up a File Explorer dialog box, and all I have to do at this point is just navigate through my file system until I find the file that I'm trying to import. Now, luckily enough for us, we're already in the right folder, so I'll just click that file to select it.
00:24:05.845 - 00:24:58.125, Speaker B: But before I click open, I first want to call out here that certain operations in Power bi, especially those that involve fetching or loading data sets like this one, can take a while, depending on your system. Maybe five seconds, maybe ten or even more. We're going to encounter several such scenarios throughout the course. And to spare us both from the awkwardness of staring at a static screen for that length of time, I'm generally going to fast forward through them. So don't be thrown off when things occasionally take longer on your computer than what you see on my screen. So with that little disclaimer out of the way, I'll go ahead and click Open. Now, after waiting a bit for the data to load, we see that Power bi gives us a preview of the data that we'll be fetching from that CSV file.
00:24:58.125 - 00:26:04.435, Speaker B: Now, it's not showing us the entire file or all the data. It's just going to show us all the columns and then a sample of the first few rows of the data. And as you can probably glean just from this little preview, our Bigfoot data set basically consists of one row of data per Bigfoot sighting, with each column containing a different class of details about that sighting. From this first column, which has freeform notes about each sighting to other columns with harder data points like the date, location, and weather conditions as of when the sightings were reported. That may seem like a lot of data, but finding Bigfoot is serious business after all. But before I get pulled down a rabbit hole here, or a Bigfoot hole, I guess, let's get back to the task at hand, which is actually fetching this data into power bi. Now, of particular importance when you're importing a text file is your choice of the delimiter, which is how the columns of data in text files are separated from one another.
00:26:04.435 - 00:26:54.205, Speaker B: So since we're dealing with a CSV or comma separated values file, Power bi correctly guesses that we want to use a comma as our delimiter. But of course we have the option to choose a variety of other delimiters as well, which we can see if we just click this little dropdown here. But again, since we're dealing with a CSV file, we're going to stick with comma. Now, overall, our data looks good, pretty much like I would expect, to the extent that I have expectations for a data set about Bigfoot sightings. So now to load this data set into power bi, all we need to do, unsurprisingly, is click this load button at the bottom of our dialog box here. But hold up. Before we do that, let's briefly take note of the Transform data button right next to it.
00:26:54.205 - 00:28:06.955, Speaker B: This is the button we click if we're not satisfied with every single aspect of this data as is and want to apply some transformations to it before we pull it into power bi. We'll get into that much more just a little bit later, but for now I'll go ahead and click Load. Then, after waiting for the data to load, or more likely, a handful of moments, the dialog box disappears and power bi returns us to the report view. But it's not exactly obvious where our data went, but if we look over to the right under fields, it looks like we found our data set. Or rather, I should say we found our query, which is how we conventionally refer to external data sets that we fetch into power bi. Note that by default, power bi based the name of this query on the name of the file we imported, although this is certainly something we can customize and change, as we'll see down the road. And if we click this little arrow just to the left of the name of the query, we see a list of all the fields from the CSV file.
00:28:06.955 - 00:29:40.931, Speaker B: And to get a much more comprehensive preview, we can switch from the Report view in our Power BI file to the Data view, and in this view we can explore the contents of the file in much the same way as you could in a spreadsheet like Microsoft Excel or Google Sheets. Now, at this point we with our dataset successfully loaded, I'll go ahead and save our file by hitting File and Save as, and then I'll name our file Bigfoot Sightings, and finally click Save and it looks like it saved successfully. Now, this file is going to be the basis for our exploration of the Bigfoot Sightings dataset throughout this section of the course. So if you're following along, I also recommend that you save your changes after each video. All right, up next, we're going to see how to fetch data into Power BI from a slightly more exotic source than plain old text files, that is A relational database. I'll see you then. Welcome back.
00:29:40.931 - 00:30:54.175, Speaker B: So now that we've imported a simple text file into Power bi, I also want to demonstrate how to import a type of data that, if you work in the world of business intelligence, you just can't avoid, and that's data from a database. Databases, especially relational databases like for example, Microsoft SQL Server, are absolutely ubiquitous in the world of business intelligence. Chances are, if you're working in a business intelligence analytics or data science type role, you're going to encounter data in a relational database at some point and have to work with that data. So next I want to show you how you can pull data directly from a SQL Server table right into Power bi. Now, just as a disclaimer, I don't expect you to follow along here, since you'd need SQL Server and an example database installed on your machine to do so. But the good news is you should be able to apply these concepts on the job whenever you need to fetch data from a SQL Server database, or another database for that matter, into Power bi. I'll start just like before by going to the Home tab on the ribbon and then hitting Get Data.
00:30:54.175 - 00:32:04.895, Speaker B: But now instead of selecting a text or CSV file as our data source, I'll choose SQL Server. Now here I'm prompted to enter the name of our database server. This is something you would have to know in advance in order to connect to your particular database. I've got mine copied to the clipboard, so I'll just go ahead and paste that in. And then optionally I can also specify the name of the database I'm interested in. And I happen to know that that's AdventureWorks 2019, so I'll type that in as well, and then I'll just click ok, and right away I get a preview of all the objects in that database that are available for us to fetch into Power bi, most of which are either tables containing data or views another database object similar to a table. So, scrolling down through the list here, let's say just for example, I want to pull in this person person table.
00:32:04.895 - 00:33:13.975, Speaker B: All I have to do is click it to select it, and then just take a quick preview of the data over here on the right to see if it looks like what I expect and it appears to. So to load this data into Power bi, I'll just click this checkbox over to the left of the name of the table here and then hit load exactly like we did before. So now that our data has finished loading, back in the data view of Power BI here, I can select this new query to explore it by clicking its name over in the fields pane on the right, directly underneath our Bigfoot query. Incidentally, you can see that Power BI once again guessed what the name of our query should be, this time based on the name of the table we imported from SQL Server. So now we can once again scroll around here through our data to check out the different columns of data we pulled in. And it looks like everything was in fact imported successfully. And the great thing about these external data connections that we're establishing here is that they're refreshable.
00:33:13.975 - 00:34:39.315, Speaker B: So if the actual source table in the database changes periodically, we can always fetch the latest and greatest data just by clicking the refresh button up here on the Home tab. Now, before we go, I want to take this concept of fetching data from a relational database with Power BI to the next level by not just pulling in an entire table, but actually issuing a SQL statement directly to the database. Now, don't worry if you're not familiar with SQL or not terribly comfortable with SQL, you won't need to understand the syntax you're about to see to appreciate how the process works in Power bi. The bottom line is that SQL is basically a language we use to interact with databases and fetch subsets of data from those databases according to criteria that we specify. So, as our example, let's say we don't want to pull in all the people from the person person table, but instead only want to pull in people with the initials tlc. That is to say, the first letter of their first name is T, the first letter of their middle name is L, and the first letter of their last name is C. To do all that, let's first delete our current query against this table by clicking these three dots to the right of the query name and then choosing Delete from Model.
00:34:39.315 - 00:35:25.215, Speaker B: Now, don't worry about what Model means in this context for now. We'll cover that in much greater depth later. And I'll just click yes here to confirm that I do in fact want to delete this query. And now that it's been deleted, I'll hit our Get Data button once more and then again choose SQL Server and specify the same server and the same database. But now instead of clicking ok, I'll click this Advanced options toggle here at the bottom of the dialog box. And sure enough, these advanced options include issuing a custom SQL statement to the database. And that's exactly what we're going to do.
00:35:25.215 - 00:36:06.825, Speaker B: So I've already written a SQL statement that pulls in people with those TLC initials in advance. So I'll switch over to SQL Server and copy that and then back to Power bi. I'll paste that in here, and now I can click ok. And once again, Power Query is showing me a preview of the data that will be returned. But instead of the 19,000 plus rows that were in the unfiltered person table, we only see five rows. And if we look at the first, middle and last names of the people in these rows, it looks like their initials are all tlc. So I think our query worked.
00:36:06.825 - 00:37:11.535, Speaker B: So now I'll click Load and then if I select our query, which Power BI imaginatively named Query 1 by default over in the fields pane, there we have it. A custom view of data from our database fetched right into Power BI with SQL. Now, as we'll see in the coming videos, Power BI has a ton of functionality for transforming the data we're pulling from external sources. But when it comes to transforming data from relational databases, SQL is still the undisputed king. So if you know SQL and your data is from an external database, consider this option to embed SQL queries directly into your Power BI external data connection. Ok, so speaking of transforming data, up next we'll introduce the Power query Editor, Power BI's built in tool for applying useful transformations to data from any source you can access with Power BI text files, databases, and of course, many more. I'll see you then.
00:37:11.535 - 00:38:10.235, Speaker B: Welcome back. So now that we know how to get external data into Power BI, let's begin our exploration of what really makes Power BI's ETL capabilities so special. The ability to transform that data in a seemingly infinite number of ways to suit your analytics and business intelligence. Needs. Now, just to introduce you to the standard Power BI workflow, I'll start out by deleting our existing queries over here. So again, I'll just hover over the right until I see those three dots, click those, and choose Delete from model. Confirm that.
00:38:10.235 - 00:38:47.915, Speaker B: Yes, indeed, I want to. And then I'll rinse and repeat that process for our second query. And now we're back to square one. So now I can re import our Bigfoot data set, which I'll do by hitting our Get Data button on the Home tab of the Ribbon and then choosing Text CSV. Then I'll select our source file here and click Open. And now we see the same preview of the data that we saw previously. But here's where we're going to change things up a little bit.
00:38:47.915 - 00:39:50.695, Speaker B: Instead of just hitting this load button, we're going to click the Transform Data button right next to it. So hitting Transform data loaded an entirely new interface, the Power BI Query Editor, more commonly known as Power Query. If you've ever used Power Query in Microsoft Excel, this screen probably looks familiar to you, and it should. The underlying technology is actually the same for Power Query in both applications. Now, these first couple of fields here, which contain free form descriptions of people's Bigfoot sightings and the locations where the sightings occurred, are taking up a ton of room. So I'm just going to scroll over to the right so we get a more representative view of of all the fields in our data. So you might have noticed that this preview of our data is just a little bit more elaborate than what we saw earlier when we were just importing the data and loading it directly to Power bi.
00:39:50.695 - 00:40:42.413, Speaker B: It's not just columns of data being displayed. Next to each column header, we see a little icon or symbol. That symbol to the left of each column header indicates the data type that Power BI automatically assigned to that column based on the data it detected in the column. Essentially, this is Power BI's educated guess as to what type of data is in each column. The 1, 2, 3 symbol next to the UV index column stands for numeric data. And then if we scroll slightly to the left, the ABC symbol next to the summary column stands for text data, and so on. Now, all the way over on the right, to the right of our data preview, I want to call out this query settings pane just to reiterate.
00:40:42.413 - 00:41:34.655, Speaker B: Each connection we establish to an external data source in Power BI is considered a query. And each of these queries will have properties, one of which is a name. Power BI assigned the name BFRO reports to our query by default because that was the name of the file we imported. But if we wanted to change the name of the query to make it a little easier to reference later, we can by simply typing in a new name here at the top of the query settings pane. So I think I'll rename our query to the slightly more intuitive Bigfoot Sightings. Now, even more important is this list of applied steps that we see right below the name of our query. This is where we're going to see all the steps we take to apply different transformations to our data.
00:41:34.655 - 00:42:19.661, Speaker B: Now, as you can see, Power BI has already applied a couple of steps on its own without us having to tell it what to do. The first of these was to promote headers. Now, what that means is that Power BI correctly guessed that the first row of our data contained column headers. Now, Power BI insists that the data we pull into Power Query be structured as what's called tabular data. This means data that is broken out into columns with descriptive column headers and consistent data types in each column. So if the first row of our data isn't column names, Power Query will supply automatically generated column names of its own. But most of the time, our data will have column headers.
00:42:19.661 - 00:43:03.005, Speaker B: And in this case, Power Query correctly guessed that it did, and consequently promoted that first row of our data to be column headers. Now, the next applied step was to specify data types for each of the columns in our data. And again, Power Query does this based on an educated guess informed by the data it sees in each column. It correctly guessed that our summary column, for example, contains text data. And it also correctly guessed that our visibility column over here, slightly to the right, contains decimal data, as indicated by the little 1.2 symbol to the left of the column name. So Power BI has given us a little bit of help already.
00:43:03.005 - 00:43:50.355, Speaker B: But that's just the beginning of the transformations that we're ultimately going to want to apply to this data. The first step that we'll take is to make our column headers more human, readable by placing spaces in between the words where the column names consist of more than one word. So, just as an example, our UV index column header is technically all one word, uv, then an underscore, then index. Now, it's very common for data from databases to come out in this format, where column headers don't have spaces between the words. But often our end users are more comfortable with the more common pattern of words being separated by spaces. So if I want to Change that column header. There's a couple approaches I can take.
00:43:50.355 - 00:44:51.245, Speaker B: One is to first select the entire column by actually clicking the column header, and then I can right click and then in the menu that pops up, choose Rename. I'll point out here that in Power Bi there are usually multiple ways to access the same feature, and the ability to rename columns is no different. But in the Power Query Editor, right, clicking a column after you've selected it is one of the most common and straightforward ways to access a whole host of functionality. As you can see, there's a pretty extensive list of actions you can take after you right click this column, but for now we'll just choose Rename. And now I'll just put a space in between the words of the column and click away. And now our column has been renamed. Now, another more direct approach we can take is to simply double click directly into the cell that has the column header.
00:44:51.245 - 00:46:08.595, Speaker B: So for say, our wind bearing column here, if I just double click that column header, I can edit it directly. And honestly, that's probably the easier way. So I'll just delete that underscore, replace it with a space, hit Enter, and just like that, the column has been renamed. So with those techniques at my disposal, I'll go into fast forward mode here and quickly go ahead and add spaces between the rest of our column headers that consist of multiple words. And now that we've done that, if you look over at our list of applied steps, you'll see a new step has been added called Renamed Columns. It turns out that Power Query is keeping track of the changes that we're making as we make them, so it can play those changes back in the future if we want to apply them to a refreshed or updated version of the source file. Power Query accomplishes this by translating our changes into a programming language built right into Power Query, called M.
00:46:08.595 - 00:47:06.785, Speaker B: If you look up at the bar directly above our data preview, which looks very much like the formula bar in a spreadsheet program such as Microsoft Excel, you'll see some very strange looking code that corresponds with the transformation step we just applied. That's M code, and Power Query writes it for us every time we apply a data transformation. Now, learning M is its own somewhat complicated can of worms, and one that we'll open briefly later in the course. But the good news is 99% of what you'd ever need to do in Power Query you can accomplish with graphical tools like the ones we've seen so far. So for now, we'll Stick to the easy way. Okay, so now that we've covered the basics of how transformations work, it's time for us to dig into how to perform some of the most important and foundational data transformations you can do in Power Query. See you in the next one.
00:47:06.785 - 00:48:25.505, Speaker B: Hey, welcome back. Now, as I pointed out previously, Power Query works with structured data, the kind that comes from databases, which is also known as tabular data. To reiterate, tabular data is organized into columns with descriptive column headers and typically also means that the data in each column has a consistent data type, whether that be text, numbers, dates, etc. As I briefly alluded to, Power Query has already taken its best shot at guessing what the data types of our different columns should be. And since Power Query is pretty smart, its guesses make sense for the most part. But sometimes we may have a more specific data type in mind. For example, our Humidity and Precipitation Probability columns are formatted as standard decimals, when in reality they could be more precisely described as percentages.
00:48:25.505 - 00:49:22.135, Speaker B: Therefore, our next step will be to change the data type of these columns to be percentage. And of course, as per usual, there are multiple ways to do this. The first is just to click the column header to select the entire column, then right click and hover over Change type, and then simply change that type to Percentage. And now you can see that the values in our Humidity column are in fact formatted as percentages, and the little icon to the left of the column header has changed from a decimal number to a percentage sign, indicating that we've successfully changed the data type. Next up, I'll do the same thing for the Precipitation Probability column. I'll again select the entire column by clicking the column header. But instead of right clicking, I'm going to click this Transform tab on the ribbon of the Power Query editor.
00:49:22.135 - 00:50:23.285, Speaker B: So the Transform tab contains commands that allow us to do just that to transform the values in one of our columns of data in place. You'll also note that there is an Add Column tab on the ribbon right next to the Transform tab, which actually contains a lot of the same functionality. The only difference being that the commands on the Add Column tab don't change the data in place, but instead create a brand new column with the transform data. But what we want in this situation is to transform the data in place. So we'll go back to our Transform tab, and then here we'll find where the data type is listed as decimal. So we can click that and simply choose Percentage. And with that, using very different methods to do the same thing, we've converted two of our columns to different data types now pivoting to a different kind of transformation.
00:50:23.285 - 00:51:17.845, Speaker B: It's often the case that our data has columns that we just don't need, and fortunately Power Query makes it really easy to remove those. Just as an example, if we scroll to the left here to take a look at our Title field, the values in this field serve more as labels for the records in our data set rather than actual data that will be useful to our analysis. So let's go ahead and remove this column with Power Query. So I've already clicked the column header to select the entire column, so now I can just right click and then choose Remove. And it's gone. Now for another example, scrolling all the way over to the left. If you recall that very first column in our data set with freeform descriptions of Bigfoot sightings that was taking up so much room in our preview.
00:51:17.845 - 00:52:19.667, Speaker B: While these anecdotal accounts do make for very interesting reading, trust me, I've read a few of them. This sort of messy text data isn't the most friendly for building analytical reports and dashboards. So I'll click this column header to select the column, then go to the Home tab and click the Remove Columns button and then choose Remove Columns. And poof, the column has disappeared. Now, while I'm at it, since our Location Details column here is similarly a big jumble of freeform text, I think I'll go ahead and remove that one too. Now, if you check our list of applied steps, you'll see all the transformations we've applied listed out, renaming columns, changing columns, data types, and finally our most recent transformation, where we removed these three columns. Now, getting rid of unneeded columns isn't the only way to trim the fat from your data sets.
00:52:19.667 - 00:53:05.227, Speaker B: In Power Query, you can also remove unneeded rows. This can be done through a process called filtering, which if you're familiar with spreadsheets like Microsoft Excel et al. You've probably already done quite a bit of and in Power Query it works just the same. So let's say, for example, we're only interested in sightings where the date of the sighting was documented. In other words, we need to filter out any records where there's no date. Now, just scrolling through the values of this date column, you'll see the word null repeated quite a lot. So what does null mean exactly? Well, if you're familiar with databases, you know that the answer is a little more complicated than it seems.
00:53:05.227 - 00:54:04.005, Speaker B: It technically means unknown value, which strictly speaking isn't Quite the same as a value we do know that looks blank, like a text string of zero length, for example. But for most purposes in Power bi we can safely interpret null as empty or blank. So let's go ahead and filter those nulls out of our date field. To do that, I'll just click this little filter button to the right of the Date column header. And now I see a list of distinct values in the column, the first of which, fortuitously enough, is null. So if I uncheck that, click ok and then scroll down through our data, we see that all the remaining rows of data now appear to have a corresponding date, just like we wanted. And we also see our latest transformation added to our list of applied steps over in the Applied Steps pane.
00:54:04.005 - 00:54:56.665, Speaker B: Now, at this point we've done all the transformations that I intend to tackle for this video, so I'm ready to load that transform data into Power bi. To do that from the Power Query Editor, all we need to do is hit this Close and apply button on the left side of the ribbon. So I'll click that. And now back in the data view of Power bi, I'll select our Bigfoot Sightings data set over in the Fields pane. And our data appears to be displaying normally, but let's just take a quick peek to verify that our transformations are working. So, looking at the column headers that consist of two words, it looks like they are separated by spaces instead of underscores. I no longer see those big freeform text columns at the beginning of our data set.
00:54:56.665 - 00:55:53.275, Speaker B: And if we look at the dates column, it looks like all the rows of data actually have values in that column. So I think I'm satisfied that the transformations we applied are reflected in this data. All right, I think that was enough fun for one video. Up next, we'll take our data transformation skills to the next level by learning to edit the transformations we've already applied. I'll see you then. Welcome back. So, just the transformations we've learned so far give us a lot of power to transform raw data in useful ways.
00:55:53.275 - 00:56:58.315, Speaker B: But processes like this usually aren't a one time thing. Instead, you'll often find yourself needing to apply the same series of steps to new versions of your data over and over again. So does this mean we have to rebuild our Power Query process every single time? Absolutely not. It turns out that the applied steps listed in the Power Query Editor record or transcribe the different transformations you've defined so that they can be reapplied or played back, so to speak. Against refreshed or updated data sets. If you've ever used the Macro Recorder in Microsoft Excel, the general concept is very much the same here. So as an example, I've pulled up our raw source data file, our CSV file that we've been importing and transforming via Power Query, and now I'm going to delete a record from this file just to see if that change is ultimately reflected in our imported data.
00:56:58.315 - 00:58:19.775, Speaker B: So I think I'll just delete this first row here and then save the file. And now back in Power bi, I'll close out of Power Query and then here in the data view. The first thing I want to do is take note of the number of records in our current Transformed data set, 4036. Note that this number accounts for the records we have filtered out for having no date values, and is significantly lower than the record count of our raw CSV file. So now to fetch and transform the latest version of the source data file, all I have to do is hit this refresh button on the Home tab of the Ribbon. And now if you look down to the bottom of the window again, you'll see that our query now returns 4,035 records, one fewer than before, but still far fewer than our source file, meaning our data filter was applied to the refreshed data. So basically, Power BI not only imported the updated data from our CSV file, it also replayed all the transformation steps that we had applied previously all over again.
00:58:19.775 - 00:59:29.165, Speaker B: So the bottom line is, once you've built out your import process, you can reuse it over and over again, as long as it still transforms the source data how you need it to. But that does raise the question, what if our data or what we need to do with our data changes and we need to change one or more of our transformation steps? Or even worse, what if we simply made a mistake when we created the process in the first place? Now, fortunately, Power Query makes it super easy to change our applied steps. Let's take a look at a few examples of this in action. Now, first, we can go back into our Power Query editor anytime by hitting this Transform data button up here on the Home tab of the Ribbon. And now we're right back in the Power Query editor. And Power BI even remembers the connection we set up to the external file previously and all the transformation steps we applied to it, as you can see over in our Query settings pane on the right. So let's say, for example, that we feel like we made a mistake by deleting any columns from Our source data.
00:59:29.165 - 01:00:18.275, Speaker B: Maybe we want to keep everything just in case. The same philosophy. Which explains why there's no room in my garage anymore. So, to remove that applied step, I'll first select it in the Applied Steps list. And one thing I want you to note before we actually remove this step is that if I scroll over to the right until we get to our date column and then scroll down a bit, it looks like we're now seeing records without dates back in our data. But our next step in the Applied Steps pane filtered rows was supposed to take care of that. So what's going on here? Well, basically, by selecting any of the steps over here in the list of applied steps, you're effectively time traveling to that particular step in the process.
01:00:18.275 - 01:01:21.295, Speaker B: So any changes made by subsequent steps are not reflected in the data you see when you select a given step. So if I go all the way back to the third step, which is where Power bi automatically applied data types to our columns. And then if we scroll over to the right just a bit to see some of our columns whose names consist of multiple words, you'll see that those column headers no longer have spaces in between them. And none of the other changes that we applied have been made here either. So you can always preview what your data looked like at any point in the sequence of transformations you're applying just by selecting a particular step. Now, in our case, we want to remove the step where we're removing columns, so we'll select that again, and then to remove the step, we'll just click this X to the left. So here Power Query is very prudently cautioning us that removing a step that has other steps after it could cause our overall process to break.
01:01:21.295 - 01:02:03.205, Speaker B: That's because a later step may reference something that is the output of a previous step. So then if you remove that previous step, the later step could break. But in our case, we know our last step to filter rows doesn't apply to any of the columns we're removing. So we should be good to go. So I'll go ahead and click Delete. And then if we scroll over to the left, we'll see that our observed and Location Details columns are back. Now, if I change my mind and I want to remove these columns once again, all I have to do is select a column and I'll just start with this observed column right here and then choose Remove Columns.
01:02:03.205 - 01:03:12.395, Speaker B: And then I'll also remove the Location Details column once again, and finally the title column. Now, with these columns removed, I can even move this new removed column step back to its original position in the sequence of steps. I just need to select the step, then right click it and choose Move before and Now. Our sequence of steps is exactly what it was previously. Now note that if you wanted to move this step even further back in the sequence of applied steps, you could continue right clicking and choosing Move before as many times as needed. Now, in addition to adding and removing steps, we can also edit them in place. For example, let's say we want to change our filter that removes rows with no date values to be a bit more surgical by removing any dates prior to 1980, just to keep our analysis more timely and relevant.
01:03:12.395 - 01:04:39.043, Speaker B: Now we could delete our current step and then re add a modified step, but all that work is not necessary. Instead, we can edit the step in place by selecting it first and then clicking this little gear icon over to the right. And that brings up a dialog box specific to filtering values. Currently this filter is set to keep rows where date does not equal null, but we can easily change that to be keep rows where date is after or equal to and then I could use this handy date picker widget Power Query was clever enough to give us since the column we're filtering has a date data type and use it to select 1-1-1980. But because that would involve a lot of clicking to get that far in the past, I'll just type in the date here in a common format that Power Query will recognize. So I'll type in 111980 and finally click OK and now scrolling back over to our date column. If we sort our dates in ascending order by clicking the little button here to the right of the column header and choosing Sort Ascending, this counts as another transformation.
01:04:39.043 - 01:05:47.575, Speaker B: By the way, it looks like our edited date filter transformation is working since the very first date listed is January 1, 1980, and then the dates gradually increase in chronological order from then on. Now, in addition to editing the actual functionality of the steps, we can also edit the names of these steps to make them easier to interpret and easier to distinguish from one another. So let's say, for example, we wanted to make the name of our filtered rows step more descriptive. First I would just select that step over here in the Applied steps pane and then right click and choose Rename. And then I'll just change the name to filtered dates and hit enter. And now our step has been renamed to something that's more meaningful and gives us more insight into what it actually does. And we could just as easily go through all these other steps and Rename them in ways that make their purpose, their intention, more clear.
01:05:47.575 - 01:06:34.665, Speaker B: In general, that is a best practice, especially as the number of steps in your data transformation process grows. You can imagine if you remove multiple columns or change the data types of multiple columns, suddenly all the versions of change type 1 vs. ChangeType 2 would lose their meaning, and it would be hard to figure out which step is doing what. So you could modify them if you needed to. Ok, at this point, we've pretty much covered the very basics of transforming data with Power Query. Pretty crazy that you can accomplish so much with just a few mouse clicks, but there's still so much more that you can do. Up next, we'll take a deep dive into a set of Power Query transformations that we can apply to numeric data.
01:06:34.665 - 01:07:34.775, Speaker B: Things like temperatures and wind speeds. I'll see you then. Welcome back. So, now that we've learned enough about Power Query to do some useful things with data, it's time for you to start working through a project with a little less guidance that I've provided so far. Over the next two sections of the course, you'll be building out the UFO Sightings dashboard you see here. In parallel with the project, we're tackling in the course videos. Now, instead of walking you through the exact steps to build this, I'll provide you with requirements for different features to be added and then give you the opportunity to build them on your own.
01:07:34.775 - 01:08:52.255, Speaker B: However, I will be demonstrating my solutions with quick walkthrough videos at the end of every phase of the project. Now, regarding the project itself, you'll be analyzing a real data set containing information about UFO sightings, things like the location of the sighting, the sightings, duration, the shape of the craft observed, and so on. And as you learn more about Power BI working with the Bigfoot sightings data, you'll be in a position to apply that knowledge here in your own independent project. So with that, let's get on with phase one of the project. Have fun. Okay, solution time. So, first up, we're going to hit this Get Data button so we can fetch our UFO Sightings CSV file into Power bi.
01:08:52.255 - 01:09:53.087, Speaker B: So I'll hit that and then choose Text CSV and then I'll just navigate to where that file is stored on my computer, select it and hit open. And now that that's loaded, I'll hit this Transform Data button to take us into the Power Query editor. And then looking over at the applied steps pane and the preview of the data itself, I see that headers were promoted. So next I'm going to actually delete this automatically Applied changed type step. And then I'm going to delete several columns from this data set, that being duration in hours and minutes, comments, date posted, and latitude and longitude. Now, I could delete all of these individually, but I'm actually going to use a little trick here to save some time. I'm going to hold down the control key on my keyboard and then select all of the columns at once.
01:09:53.087 - 01:10:43.723, Speaker B: Then simply hit delete once, as opposed to having to delete each column individually. So, scrolling over, I'll find my duration, hours and minutes column and click that to select it. And then holding down the control key, I'll also select Comments and then select Date posted, latitude and longitude. And with all of those selected, I can simply hit my Remove columns button here. And just like that, they're gone. Now, up next, I'm going to look through these columns and change the data types of the columns that are left as appropriate or necessary. So just scanning this, I see that our date time column is formatted as text, and that's obviously not appropriate for date time.
01:10:43.723 - 01:11:52.925, Speaker B: So I'll select that column and then go up to my Transform tab on the ribbon and then change that data type to date time. And then scrolling over, we see our Duration in seconds column is also formatted as text, and that seems like it should be a whole number. So I'll select that column and then change that data type to whole number. Next up, I'm going to change the name of that Duration in Seconds column to just say Seconds observed with an underscore. So I'll double click the column header and then just type seconds observed. Now, as our next step, I'll apply a filter to remove any sightings where the value in this country column here is blank. So to do that, I'll click this little filter button to the right of the country column header and then simply uncheck the blank value in the list that appears and then click ok.
01:11:52.925 - 01:13:06.851, Speaker B: And now those blanks are gone. Next up, I'll apply a filter to remove any sightings where the value in our Seconds observed column is less than five seconds, which is another way of saying greater than or equal to 5. So let's hit that filter button here and then hover over number filters and then choose greater than or equal to. And here I'll just say greater than or equal to five and then hit ok. And then just scrolling down through the list of seconds, they all appear to be greater than or equal to five. So it looks like that filtering step worked but now our next step is to edit the step we just created such that any sightings with a Duration less than 10 seconds are filtered out. So to do that, I'll click this little gear icon to the right of our filtered row step, and that takes us into the same dialog box where we specified the filter to begin with.
01:13:06.851 - 01:14:00.915, Speaker B: So here I'll just change that 5 to a 10 and then click OK. And once again, it looks like our filter worked. So now for our next step, I'm going to rename these two steps we just created these two filtering steps so that their names are a little bit more descriptive. So for our first one, I think I'll go with filtered blank country names. So I'll just right click this step in the Applied steps pane and then choose rename. And again I'll rename that filtered blank country names. And then for our step that filtered out sightings less than or equal to 10 seconds, I'll naturally rename that to Filtered sightings less than or equal to 10 seconds.
01:14:00.915 - 01:15:13.295, Speaker B: Now, finally, I'll head up to our home tab and then hit Close and Apply to lock in our changes. And looking over at the fields pane, we can see that our UFO Sightings data set has been successfully loaded. So now it's time to simply go to File and save as. And then save our power bi file as UFO sightings. And then I'll hit save and that's that for that. Hey, welcome back. So, deleting columns, filtering rows, and changing data types can be really useful when tidying up raw data into something manageable and meaningful.
01:15:13.295 - 01:16:14.195, Speaker B: But that's often not enough. Quite commonly, we also need to apply transformations to the actual data in those columns in ways beyond something as simple as just changing a data type. One example of this is numeric transformations like addition or multiplication. Just for example, in addition to changing the values of a column in place, these kinds of transformations can also add useful new columns to our data set. Say, for example, that we'd like to add a new column to our data displaying the difference between the high and low temperatures on the date of a given sighting, also known as the diurnal range for you weather nerds out there. Basically, we need to subtract the values in our temperature low column from the values in our temperature high column. Now, fortunately, Power Query makes it incredibly easy to do something like this.
01:16:14.195 - 01:17:05.775, Speaker B: Your first step, as usual, is to select the column of interest. So we'll select the temperature high column by clicking its column header. We need to select the column because the Values in this column are going to be the basis, at least partially, of our new derived column. Now, looking up at our ribbon, we see that we have our transform tab and our Add column tab. And if you recall, I said the main difference between the commands on these two tabs is that the commands on the transform tab change data in place, whereas the commands on the Add column tab drop that transformed data into a brand new column. So you've probably guessed at this point we're going to go with our Add column tab. And here we see that our commands are grouped into a few categories.
01:17:05.775 - 01:17:34.425, Speaker B: We have general from text, from number and from date and time. Well, we're going to be deriving a column from a numeric value, a temperature. So let's look in the from number section. Now what we're doing doesn't involve trigonometry or scientific calculations or anything like that. It's just plain old subtraction. Seems pretty standard. So let's go ahead and hit this standard button.
01:17:34.425 - 01:18:11.965, Speaker B: And here we see all the standard arithmetic operations. Add, multiply, subtract, divide, etc. Now in our case we're subtracting temperature low from temperature high. So we'll choose subtract. And now we're prompted to enter a number that we want to subtract from each value in the temperature high column. The problem is we're not subtracting a set value. We actually want to subtract the corresponding values on a row by row basis in the temperature low column.
01:18:11.965 - 01:19:04.955, Speaker B: But that turns out to not be a problem at all if I click this little dropdown box under value. Now we see the option to use the values in a column, so we'll go with that. And then of course select our temperature low column from this drop down list and finally just hit ok. And now a new column has been inserted into our dataset all the way over on the right side called subtraction. Now this is a case where Power Query's naming skills aren't exactly the best, but in its defense, it can't read our minds. And know what we might want to call this column? So it just names it after the arithmetic operation that we applied. Now obviously we don't want to stick with such a generic name, so we'll rename the column diurnalrange.
01:19:04.955 - 01:20:08.105, Speaker B: So I'll just double click the column header and type that in for the column name. And it would also be more logical if this column was placed directly to the right of the temperature columns it's based on. So I'm actually going to click this column header and then hold down my mouse button so I can drag it over to the right of the temperature low column. And there we are. A brand new derived column that we renamed and positioned logically within our data. And if we look over at our list of applied steps, we see that we have a new step for adding our new column, a new step for renaming the column, and a new step for moving that column. And while I won't waste your time by making you watch me rename all these steps in the real world, the best practice would be to name all of the steps so that they're easy to distinguish from one another in case we want to edit them later.
01:20:08.105 - 01:21:15.645, Speaker B: So now that we've performed a numeric calculation to add a new derived column to our data, let's try another one to transform a column of our data in place. So, scrolling over to the right, if you look at the values in our visibility column, you can see that they range from near zero all the way up to a maximum of 10, which clearly means 100% visibility. But it would arguably be more intuitive if these values were just expressed as a percentage to begin with. So what we would like to do is divide the numbers in this column by 10 to convert them to percentages. The result of this calculation would be that a value of 9 would be converted to 0.9 or 90%, a value of 10 would be converted to 1 or 100%, and so on. Now, we don't need to create a brand new column to do this, so instead of going to the Add column tab this time we'll opt for our Transform tab.
01:21:15.645 - 01:21:55.295, Speaker B: So I'll cancel out of this and then hit our Transform tab. And again, we're applying a numeric transformation here and a straightforward one division at that. So with our visibility column selected, I'll click standard once again and then divide. So we know that we're dividing by 10. So I'll just enter 10 here and then click OK. And there we are. Our visibility values have been converted to percentages, but Power Query is still treating them as standard decimal values.
01:21:55.295 - 01:23:08.435, Speaker B: So let's also change the data type of the column to percentage. And at this point, it looks like we're good to go. But before we go, I want to look at adding a new column to our data that isn't based on any columns in our existing data and isn't based on a calculation. So you might be asking, how is this new column numeric in nature at all? Well, what I want to add is what's known as an index column. So what the heck is that? Well, an index column has a number that uniquely identifies each row in our data set. Now, depending on your background with databases or structured data, that may or may not make a lot of sense to you right now, but take my word for it that when it comes to the kind of data you'll deal with in the realm of analytics and especially business intelligence, having a field with values that uniquely identify every row in a data set is crucial. And the typical convention for these types of columns is that their values are a sequence of numbers from 1 to the number of rows in the data set.
01:23:08.435 - 01:24:13.583, Speaker B: And Power query actually makes it super easy to add a column like this to our data. But before I get ahead of myself, I want to make sure that our data is ordered or sorted in such a way that will give the values in our index column, remember, ranging from one to the number of records in the data set, some actual meaning. Specifically, I'd like the very first sighting, chronologically speaking, to have index one, the second sighting to have index two, and so forth. Now, we already have a column in our data containing the dates on which sightings occurred. So if that column was sorted in ascending order, that's oldest to newest in date terms, then our data set would be ordered in such a way that our index column would make intuitive sense. But if we look at our list of applied steps over here, we see one for sorted rows. And then if we go over to our date column, we can see that the sorting we need has already been applied.
01:24:13.583 - 01:25:08.585, Speaker B: We've already taken care of this before, so our data set is primed and ready for our index column. So to add that, since we're adding a column, we'll naturally click our Add column tab. And right away, almost directly underneath the Add column tab, I see a button for index column. So I'll click that. And by default, Power query generated a sequence of numbers starting from zero all the way to one less than the number of rows in our data set, which is almost what we wanted, but not quite. We actually want our sequence to begin with the number one, so we could delete this column and add a new index starting with the number one. But if you watched the last video, you probably know at this point there's a better way, and that's to edit the step in place.
01:25:08.585 - 01:25:53.255, Speaker B: So with our added index step selected in our list of applied steps, we'll click the little gear icon. And now we're presented with a dialog box that lets us specify the starting index power query chose 0 for us by default, but we'll just change that to one. And then the default increment, which is the number by which each value in the index column increases from row to row, is one as well, which makes sense 99.9% of the time for index columns. So we'll leave that as is. So now I'll click ok. And here, with just a few clicks, we've generated a sequential index column for our data set.
01:25:53.255 - 01:26:46.085, Speaker B: So now the last step will be to just move this column in front of all of our other columns, which is pretty conventional when it comes to index type columns. Now, to do this, we could just click and drag the column to reposition it, much like we did with our diurnal range column earlier. But moving it all the way to the front of the data set would take a surprisingly and annoyingly long period of time. A much better method when you want to move a column to the beginning or end of the data set, is to first make sure it's selected and then click the Transform tab on the ribbon and then hit the arrow next to this move button. And here we see an option to move the column to the beginning of the data set in one fell swoop. So I'll click that. Success.
01:26:46.085 - 01:27:49.985, Speaker B: It looks like our new index column is right where we wanted it. And we have a corresponding new step added to the list of applied steps over on the right. So now, just to lock in all these changes to our data without closing out of Power Query, I'll go to the Home tab of the ribbon and then hit Apply as opposed to close and apply. And at this point, with everything saved, we're ready to do even more with our Bigfoot Sightings dataset. So now that we've seen how to apply transformations to numeric data, up next we'll explore Power Query's features for working with date and time data. I'll see you then. Welcome back.
01:27:49.985 - 01:28:50.155, Speaker B: Now that we've explored Power Query's data transformations for numeric data, let's continue our coverage of the big three data types, numeric date, time and text, by diving into how we can use Power Query to transform date and time data. So date and time transformations in Power Query are most commonly used to extract components from a date for example, the day of the week, the month, the year, etc. Pretty simple stuff, really. For example, we can quickly add a new column to our data set that lists the name of the day of the week corresponding to the date of each Bigfoot sighting. We've already got our date column Selected. And we know we want to add a new column, so we'll select our Add column tab on the ribbon. And now we'll look in the from date and time section of that tab and then click the date button.
01:28:50.155 - 01:29:34.907, Speaker B: And here we can see a lot of options for how we can parse individual components out from a date. The particular component we have in mind is the name of the day of the week. So we'll roll down to day and hover over that. And we see that this last option is name of day. So we'll click that. And then if we scroll over to the right, we see that we now have the name of the day of the week on which each sighting occurred in a new day name column. Now, this is the kind of transformation that were you to try to accomplish it in a spreadsheet or database, would require some kind of formula or function.
01:29:34.907 - 01:30:24.275, Speaker B: Whereas with Power Query you're able to accomplish it with just a few mouse clicks. It's so easy. In fact, let's go ahead and parse a few more components of our Bigfoot siting dates out into their own columns. So let's scroll back to our date column, and first we'll add a month column to our data set. So again, I'll click the arrow under our date button here and then hover over month, and then I'll just choose month. And there we see a new column with the number of the month corresponding with each siting. So then going back to our date column, I'll select it again and then add quarter in the same fashion.
01:30:24.275 - 01:31:16.857, Speaker B: And finally, let's go ahead and add year. So I'll select our date column again, click the arrow under date, hover over year, and then select that. And as easy as it was to add these columns, they can still add a ton of value to any analysis or visualization we may want to perform on our data set down the road. It's not hard to imagine charts displaying trends in sightings over the years, or identifying months or days of the week in which sightings are particularly prevalent. All right, so now that we've seen how to apply transformations to numbers and dates, let's turn our attention to perhaps the most common variety of data. Text data, like the names of people or things. That's up next.
01:31:16.857 - 01:32:17.865, Speaker B: I'll see you then. Welcome back. So the next type of data transformation we'll tackle is text transformations, which apply to non numeric and non date values, like the names of people, places and things. Let's dive right in. So one very common kind of Text based transformation is splitting data from a single column into separate columns. Such columns often contain two pieces of information, for example, a city and a state in each value. Now, this may seem innocent enough, but it's actually kind of a no, no for tabular data.
01:32:17.865 - 01:33:22.045, Speaker B: Tabular data sets are supposed to be organized into columns which each contain a single type of information about the overall subject of the table. So in that scenario, you would have a column that contains just cities and a column that contains just states. Now, while there isn't a great example of that particular issue in our data set, if we look at our classification column right here, which classifies our Bigfoot sightings into different buckets, you'll see another reason we might want to split values out into two separate columns. Note that in this column the word class is repeated before every single class value. But we already know that the values in this column are in fact classes. As a result, the inclusion of the word class is redundant and therefore unnecessary. So what I'd like to do is split this classification column into two columns, one with just the class value and one with the word class.
01:33:22.045 - 01:34:20.947, Speaker B: The most common way to perform a transformation like this is to identify a delimiter, which is to say a particular text character or combination of characters that separates the values that we want to drop in one column from the values we want to drop in another column. Now, looking at our values here, we see that there's a space in between the word class and the actual class values that we can use as that delimiter. So now that we know basically what we're trying to accomplish, let's go ahead and see how we can split these values on that space. So first I'll make sure the classification column is selected. And now up in the transform tab of the ribbon, I'll look over in the text column section. And here I'll click this split column button, because that's exactly what we're trying to do. Now, we've already identified that we have a delimiter that we can split this column on.
01:34:20.947 - 01:35:06.505, Speaker B: So we'll choose the by delimiter option. And here Power Query was smart enough to guess that we needed a space as our delimiter and selected that by default. But if we click this drop down list here, we see a list of other comma, delimiters, colons, commas, which in my experience are probably the most frequently used delimiter in the real world, tabs and more. Or you can even specify your own custom delimiter. If none of the options listed are what you need. But again, space is the delimiter we need for our current predicament, so I'll select that again. Now, the other defaults we see here should work fine.
01:35:06.505 - 01:36:04.065, Speaker B: For example, it's fine to split at each occurrence of the delimiter, because we only expect to have one space per value. Now, if there were the possibility of having multiple spaces in our individual text values, we might need to be more strategic in terms of whether we split at the leftmost, rightmost, or each occurrence of the delimiter. But for our purposes here, each occurrence will work fine. So we'll go ahead and click ok. And with that, we've successfully split these values into two columns, one with the actual classes, and one that simply has the word class repeated over and over. Now, don't worry, we'll be getting rid of that one soon enough. Now, another, somewhat less common, but still occasionally useful text transformation is the exact inverse of splitting text merging columns together.
01:36:04.065 - 01:36:49.755, Speaker B: One example of this is when our data has people's names that are split up into separate columns, with first and last name and sometimes even middle name that we later want to combine back into a single value that has the person's first and last name just separated by a space. Now, our data set doesn't have any people's names. Bigfoot, after all, only has one name. But to demonstrate how this works, I'll merge the two columns we just split apart back together. Now, just like with splitting columns, Power Query has a feature built right in that can execute a merge operation in a couple of mouse clicks. So first we need to select the two columns we want to merge. And keep in mind, it could actually be more than two.
01:36:49.755 - 01:37:43.625, Speaker B: So our first column is already selected here. So then to select the second column, we can either hold down our shift key or our control key, and then just click that second column header. And now, with both columns selected and still on the Transform tab of the ribbon, we'll click this merge columns button in the text column section. And like so many other dialog boxes in Power Query, it's really very straightforward. First, it asks us to provide a separator, which is just the text that will be positioned between our two values. Now, in our case, we want the word class, then a space, then the actual class value. So our separator should be a space, basically the same character we ID'd as the delimiter when we split the column in the first place.
01:37:43.625 - 01:38:55.735, Speaker B: So, looking down this list of separators, I'll choose space. Next, we're given the option to provide a name for the new column and I'll just call it what it was called originally, classification. And finally click OK and as you can see, we're back where we started. The word class and the actual class values have now been combined into single values just separated by a space. But since this transformation just basically undid our split transformation, I'm going to undo it, which is as easy as clicking the X next to that step over in the applied steps pane and poof. It's like it never happened. So now I can remove this column, which just consists of the word class over and over again, and then rename our other column back to classification, and it looks like we're good to go.
01:38:55.735 - 01:40:43.655, Speaker B: Now moving on from splitting and merging, Another very common scenario when working with real world text data is finding and replacing certain bits of text in that data at a large scale. Just as one example, misspelled words such as names, labels and categories are quite common in data sets you'll find in the wild. But if you find that certain words in your data are being systemically misspelled, it's not exactly efficient to try to correct them one by one. Since these kinds of issues are so common, Power Query has a built in find and replace feature similar to ones you might have encountered in spreadsheets or word processing software that makes identifying and replacing specified snippets of text across the values of an entire column a breeze. Regarding our dataset specifically, I haven't identified any obvious examples of systemic misspellings, but scrolling over to look at our county column here, you'll see that similar to the classification column, the word county is repeated after every county name. And while we could theoretically tackle this redundancy like we did before by splitting the actual values off into their own column, this time it would be a bit more complicated, since some county names might themselves consist of more than one word, meaning there could be multiple spaces in any given value. Besides, if you think about it, this is fundamentally more of a find and replace type problem anyway, since all we really want to do here is replace all those instances of county with, well, nothing, there's no need for another column with the same value in every row.
01:40:43.655 - 01:41:41.295, Speaker B: We just want the word county to disappear from all the values in the column. So with that in mind, let's use Power Query's find and replace feature to get this done. So first we'll select the column as usual, and then, since we're not creating a new column but simply transforming the values of our selected column in place, we'll stay on the Transform tab. And then if we look at the Any column section of the ribbon, we'll see that there is a Replace Values button. So I'll go ahead and click that. And now in this Find and Replace dialog box that appears, we basically just need to type the value we want to replace, which is the word county, and then the value we want to replace that with, which is basically nothing, an empty string. I really just don't need to type anything here in this second text box at all.
01:41:41.295 - 01:42:31.265, Speaker B: But before I click the proverbial big red button here, or I guess it's more of a big yellow button, let's tap the breaks for just a moment. I want to get rid of all these instances of county. But looking at the data, that word is always preceded by a space. So if I just replace the word, we'll still have all these orphaned spaces after our county names. So what we really want is to find and replace any text strings consisting of a space followed by the word county. So I'll go ahead and add a space here before the word county, and then click ok. And just like that, all those pesky instances of county are gone, and our data looks much cleaner.
01:42:31.265 - 01:43:44.779, Speaker B: Now, the last category of transformations I want to introduce you to pertains more to formatting or visual enhancement. So, looking once again at our county names and the state names alongside them, we see that the first letter of each word is capitalized, which is conventional for proper names like these. But for the sake of example, suppose that they were not capitalized properly and instead were, say, all lowercase. How could we transform the values back to what's known as proper case? Well, to find out, we'll first need to transform them to lowercase, I guess. So, with our county column already selected, I'll hold down my shift or control key and then click the state column so both columns are selected. Then, with both of our target columns selected, let's take a look in the text column section of the Transform tab on the ribbon, and then click the Format button. And here we see a number of options for applying visual type transformations to text data, among them to lowercase every letter of our names.
01:43:44.779 - 01:44:34.105, Speaker B: So we'll go ahead and do that. And now all those capital letters in our county and state names have been converted to lowercase, but what we're really trying to do is get back to the proper case these names were originally in. So with county and state still selected, I'll click that format button again. And since our objective here is basically making the first letter of each state or county name uppercase. I'll go with this capitalize each word option. And now our county and state names are proper case once again. Now, of course, we just applied two unnecessary transformations here for the sake of example, so I'll just go ahead and remove those from our list of applied steps, and we're right back where we started.
01:44:34.105 - 01:45:21.525, Speaker B: Now, there's one more option. Under this format menu I'd like to apply. Before we go rolling over to the quarter column that we created in the last video, we see that the value representing each quarter is simply a number. And while this is technically accurate, it's also going to make this field less useful as a means of categorizing our data. Once we start building visualizations, the numbers 1, 2, 3, and 4 are going to make for pretty cryptic labels in any chart we apply them to. Fortunately, with the click of a button, we can add a prefix to these values to make what they represent much clearer. This will be a matter of simply appending the letter Q to each value in the column.
01:45:21.525 - 01:45:58.205, Speaker B: So to do that, let's select this quarter column here and then back in the Transform tab on the ribbon. Since we're trying to transform a column in place, rather than add a new one, we'll once again hit the Format button. And here we'll select Add Prefix. Now, at this point, we simply have to specify what we want to be glued to the beginning of each of the values in the column. And as I mentioned, that will be the letter Q. So let's enter that and hit ok. And there we go.
01:45:58.205 - 01:46:51.185, Speaker B: Now, instead of just numbers, our quarter column contains values that make clear what they represent and will be suitable for labels in reports and visualizations. Again, all with a couple of clicks and keystrokes. Now, I do want to emphasize that there are many more text transformation methods than I've shown you here. My goal is not to demonstrate every single transformation method that Power Query makes available to us. Instead, my goal is to show you the ones that you'll use most of the time and get you familiar enough with the structure and layout of the menus so that you can find the ones that you need that maybe you don't already have experience with. All right, up next, we're going to switch gears a little bit and take a look at how you can apply conditional if then style logic to your data transformations. I'll see you then.
01:46:51.185 - 01:48:13.305, Speaker B: Hey, welcome back. So, an extremely common need when working with data is to apply conditional logic to that data. If you've worked with if statements in Excel or case Statements in SQL. This concept may be very familiar to you. If not, the idea is basically to apply if then type logic to generate new data based on the existing data in our data set. One common scenario is to create a flag, so to speak, in our data, that is to say, a derived column that holds a one for records, where the value in some other column meets some specified criteria, and a 0 otherwise. The 10 composition of such a column allows it to be easily used in downstream reporting and analytics, either by summing the ones or averaging the ones and zeros to get a percentage.
01:48:13.305 - 01:49:25.095, Speaker B: Let's say, for example, we want to flag any sightings in our data set where there was a full moon at the time of the sighting. Now, I'm not trying to conflate Bigfoot with werewolves here, but we all know that strange things tend to happen during the full moon, right? So our logical condition in this case will need to be based on the Moon phase column of our data, which gives us a percentage representing how full the Moon was as of a given sighting. Now, since full moons aren't usually 100% full barring a lunar eclipse, as a proxy, we'll call the Moon full if it's at least 95% full and not full. Otherwise or in terms of our data for our derived field, we'll return a 1 if the value in the Moon phase column is greater than or equal to 0.95 and a zero otherwise, pretty straightforward, right? And Power Query makes it equally straightforward to implement. So let's dive right into how we would build this conditional column with Power Query. Now, first, because we're going to add a new column, I'll click my Add column tab.
01:49:25.095 - 01:50:16.385, Speaker B: And now right under the name of that tab, we see a button for Conditional column that seems like what we want, so I'll go ahead and click it. And just like all the other Power Query dialog boxes we've seen, this is super intuitive. It kind of walks us through the entire process step by step. First it asks us to enter a name for our new column, and I'll just call it Full Moon. Now, we have a template for building out our conditional expression, and it's really just a matter of filling in the blanks from left to right. So we'll say if, and then our column name here will just be Moon Phase. Now, for our logical operator here, we don't want equals.
01:50:16.385 - 01:51:00.285, Speaker B: We need the operator to be greater than or equal to, because we want to test each of the values in our Moon phase column to see if it's greater than or equal to 0.95. So I'll click this dropdown and sure enough, one of the options is greater than or equal to. And then my value is just 0.95, which I can type in. Then finally, I just need to fill in the value that I want to return in my derived column if my condition is met, which of course is one. Now, down here at the bottom, we have what's called the else clause. You can think of this as kind of the catch all if our logical condition wasn't met.
01:51:00.285 - 01:51:45.385, Speaker B: Here's where we specify the alternative value that should populate in our new column in that case. So that basically means if the moon is less than 95% full, what do we want to return? And in our case that's just zero. So it looks like we filled out everything we need to. So I'll go ahead and click ok. And now we have a new column called Full Moon. And while we do see plenty of values here, zeros so far, we also see lots of errors, which is to say the word error. So what's going on here? Well, let's try clicking in one of the error cells to see if we can find out.
01:51:45.385 - 01:52:58.465, Speaker B: And down below the data preview power query tells us it cannot convert the value null to type logical. Hmm, sounds like there must have been some null values in that Moon phase column that are causing the problem. So if we click the row header of this row to select the entire row and then scroll over to the left to the Moon phase column, we see that, sure enough, there's a null value in that row and for several other rows in the column as well. These nulls are a problem for our conditional logic, because since a null value is technically an unknown value, you can't say for sure whether it is, for example, less than or equal to 0.95. So what to do? Well, the logical thing would be to replace the errors with nulls, since we don't really know whether to flag the affected records as Full Moon sightings or not. And lucky for us, we learned how to do find and replace operations in just the last video. So let's scroll back over to our new derived column so we can select it.
01:52:58.465 - 01:54:00.365, Speaker B: Then under the transform tab on the ribbon, I'll hit the arrow under the Replace values button. But here, instead of opting for replace values, I'll choose Replace errors. Then I'll just type in null as the value I want to replace those errors and click ok. And just like that, all those errors have been replaced by null values. Now, scrolling down through the column, along with the zeros and null values that we see for most of the rows, we also see a few ones. And if we select one of those records with a 1 and then scroll back over to our moon phase column, sure enough, the moon was at least 95% full for that record. So it looks like our conditional logic is working just fine.
01:54:00.365 - 01:55:01.015, Speaker B: Now, another common use case for conditional logic in Power Query is to create columns that help us segment our data into different categories, or buckets, so to speak. As an example, what if we wanted to be able to classify sightings based on their latitude, that is to say, how far north or south they are? Now, we do have a latitude column that gives us a precise numeric representation of this, but that won't be very useful to us later on as we try to break down and analyze sightings by different categories and classifications that make actual sense to human beings. You see that column? Our latitude column here has approximately 11 billion unique values. So there wouldn't be many insights to be gained from saying that exactly one siting occurred at 47.71782 degrees and another siting happened at exactly 37.303 degrees. And so on.
01:55:01.015 - 01:56:19.575, Speaker B: To make the most of this latitude data in our analysis, we need to break it down into discrete categories, hence the term categorical data, something you'll be hearing more about throughout the course. Now, to do that, we'll need to apply some conditional logic to our latitude column, logic with a little bit more nuance than our last example. Now, our Bigfoot data set only has sightings from the United States, and the vast majority of the U.S. excluding Hawaii and Alaska, is between 20 and 50 degrees of latitude. So we'll categorize the latitudes of our sightings as greater than 50 degrees, greater than 40 degrees, but less than or equal to 50, greater than 30, but less than or equal to 40, greater than 20, but less than or equal to 30, and finally, less than or equal to 20. So now that we have the contours of our logic ironed out, we'll hit the conditional column button once again here under the add column tab of the ribbon, and we'll call our new column LatitudeCategory. Now, our first if condition will test to see if our latitude column is greater than 50 degrees.
01:56:19.575 - 01:57:23.485, Speaker B: If so, we want to return the text far north. So I'll say latitude and then greater than for our operator, 50 for our value, and then the text far north. Now, here's where things get a little bit more complicated than our last Example, we can't just return a single other value for all the rows that didn't meet our first condition because we actually have four more buckets that we potentially want to place those rows into. Basically, we need to be able to apply more logical tests before we get to that else condition. And that's exactly what this add clause button is for. So if we click this now, we have the opportunity to define another logical test that will be applied if the logical condition in the first test wasn't met. So once again, we'll say latitude for our column and now greater than 40 degrees.
01:57:23.485 - 01:58:28.427, Speaker B: And in this case we'll categorize the latitudes as north. Now, note that the way this if then logic works is that the very first logical criteria that's met effectively ends the process. So a siding with latitude 60 degrees obviously meets both of our first two criteria, but we'll only classify it as far north, since the criteria associated with that category came first. And we can continue to take advantage of this principle as we build out the rest our criteria. So we'll add a third category for latitudes greater than 30, but of course implicitly also less than or equal to 40, since anything greater than 40 would have been handled in the first two layers of logic. So again, I'll say latitude greater than 30 and we'll call this one central. And then another layer for latitudes greater than 20 and less than or equal to 30.
01:58:28.427 - 01:59:16.155, Speaker B: So again, latitude is greater than 20 and we'll call this one South. And finally, our catchall else clause will account for any sightings that didn't fall into any of the above categories. That's basically anything with a latitude less than or equal to 20 degrees. So we can call this last category far south. And now I can finally click ok. And now looking at the new column in our data, we see a nice mixture of these categories that line up with our definitions for them based on the associated latitudes. So with just a few clicks, we were able to apply some fairly sophisticated conditional logic.
01:59:16.155 - 02:00:13.445, Speaker B: Pretty cool, I'd say. So at this point, we've created quite a few new interesting columns in our data. But we haven't yet seen how those columns, along with the original unaltered columns from the data set, of course, can be used in actual analysis. Getting insights from our data that we can explain to another human being, otherwise known as telling a story, is, after all, the real point of adding all these columns. In fact, the practice of adding new columns or features to your data to make it suitable for analysis and visualization is so common, it has a feature engineering. This is a phrase you'll hear a lot in the realm of machine learning, but it absolutely has its place in the world of business, intelligence and analytics as well. So, armed with all these new columns, I mean, features, we're ready to start really analyzing our data.
02:00:13.445 - 02:01:38.921, Speaker B: And for human beings, visual creatures that we are, arguably the best way to do that is visualizations that summarize the thousands or millions of rows in our data sets in an intuitive way. Fortunately, visualizing data might be Power BI's single most powerful feature, and we're going to jump right into building our first visualization in the next video. I'll see you then. Okay, time for a solution walkthrough to part two of the UFO Sightings project. So I'll start things off by getting us back into Power Query by hitting the transform data button here on the ribbon. And now back inside of Power Query, I'm going to add a new calculated column called Minutes observed that translates the values here in our seconds observed column into minutes. And that should be a pretty straightforward operation.
02:01:38.921 - 02:02:28.567, Speaker B: First, we'll select that Seconds observed column and then go to the Add column tab on our ribbon. Since we're adding a new column, and in this case we're just going to be applying some standard arithmetic. So I'll hit this arrow under the standard button, and basically the mathematical operation here is just to divide all of these numbers by 60. So I'll hit divide and then just punch in 60 as my value and hit OK. And looks like that worked. So we'll go ahead and rename this column to Minutes observed. And now for the next step in the exercise, I'm going to round the values in this new column to one digit after the decimal.
02:02:28.567 - 02:03:14.121, Speaker B: So with the column selected, I'll hit the transform tab on the ribbon, since we're just going to change this column in place. So over here in the number column section, I'll click this down arrow next to rounding and then choose round. And then for decimal places, I'll just select one and then hit ok. And then looking at our values, it looks like that worked exactly as designed. So for our next step, we're going to add an index column. So since we're adding a column, you know that we're going to hit the add column tab on the ribbon, and then here's that index column button. But we don't want to go with the default where the index starts at zero.
02:03:14.121 - 02:04:13.195, Speaker B: We want it to start counting up by one. So we'll hit the down arrow Next to index column and then specify from one and there's our new column. But now we want to move that column to the beginning of the data set, which I technically could do by dragging it all the way over, but that would be kind of a labor intensive and irritating operation. So instead I'm going to right click that column and then go down to move and then choose to beginning and there it is at the very beginning of our data set. So next we're going to add a series of date based columns to our data set. Year, month, name, day, name, and then just the date as opposed to the date with a time component like we have here in our date time column. So to do that, I'll of course select our date time column, which is going to be the basis of all those columns.
02:04:13.195 - 02:05:36.215, Speaker B: And then here on the Add column tab, I'll click the arrow under date and then first for year I'll hover over year and then select year and then scroll back over to my date time column to select it. And then click that arrow under date again, hover over month and then choose name of month and then once again down arrow and hover over day and then choose name of day. And then again following the same steps, I'll click that arrow under date and this time I'll just choose date only to get that date component without the time component. And then looking over here to our right, we see that sure enough, we have the year, month, name, day name and just the date only associated with each of our values in this date time column. Next up, transitioning to more text based transformations, we want to replace any blanks in our state column here with the text na. So I'll of course select that column and then transform, since we're changing the column in place. And then I'll hit this Replace values button.
02:05:36.215 - 02:06:17.825, Speaker B: And then for our value to find, we're really just looking for nothing. We're looking for an empty string, so we can leave this empty. And then of course we're going to replace that with N A. Then I'll click ok. And as you can see, all of those blanks have now been replaced with that text string na. So now our next step with that state column still selected is to uppercase all the values in that column. So again in the same section of the transform tab, I'll click the down arrow under this format button and then simply choose uppercase.
02:06:17.825 - 02:07:21.759, Speaker B: And just like that, all the state abbreviations are transformed to uppercase. Next up, I want to convert the values in the city column to Proper case, which basically means the first letter of each word is capitalized. So I'll select that column and then still under this transform tab, once again, I'll click the down arrow under format and then just choose capitalize each word. And that worked as well. Now, turning our attention to conditional logic, I'll use conditional logic to create a 10 flag, basically a column where one means our logical condition was met and zero means it wasn't called hour plus duration. Now, this column should return a one if our siting was an hour or more and a zero otherwise. And we can key off of the values in this minutes observed column and just test whether they're greater than or equal to 60.
02:07:21.759 - 02:08:08.605, Speaker B: Basically 60 minutes equaling an hour. So to do that, since we're adding a new column, I'll hit this Add column tab and then the conditional column button on that tab. And here for my column name, that will obviously be our minutes observed column. And for our operator, I'll go with greater than or equal to. And then my value of course will be 60, and my output if that logical condition was met, will be 1 and then 0 otherwise. And again we're going to call this column our plus duration. And I think this all looks good, so I'll click ok.
02:08:08.605 - 02:09:08.165, Speaker B: And now just to test this out, let's find a minutes observed value that's greater than 60, and there's one right here on row 44. And if we scroll across, sure enough we have a one in our hour plus duration column. So we know that's working. Now, the next step will be just to change the data type of this column to whole number instead of text. So I'll go to the transform tab and change that data type. And now we're again going to use conditional logic to create a new column, but this time we're going to return text instead of just a one or a zero. So this column is going to be called siting duration category and it should return the text string greater than or equal to 10 minutes if the value in our minutes observed column is greater than or equal to 10 and the string less than 10 minutes otherwise.
02:09:08.165 - 02:09:25.205, Speaker B: So it's going to work a lot like the column we just set up, except we're returning text strings instead of ones and zeros. So again, I'll hit my add column tab and then conditional column. And then our column name again is going to be siting duration category.
02:09:28.065 - 02:09:28.375, Speaker A: And.
02:09:28.405 - 02:10:13.305, Speaker B: And then our column name that's going to be the basis of this is again minutes observed. And I'll say if the value in that column is greater than or equal to 10 minutes, then our output is going to be the text string greater than or equal to 10 minutes. Otherwise the output is going to be the text string less than 10 minutes. And I think this all looks good, so I'll click ok. And now again, just eyeballing a couple values from our minutes observed column, we see the one in the first row is 45 minutes and the one in the second row is 0.3 minutes. So that should give us an idea of whether our column is working.
02:10:13.305 - 02:11:04.235, Speaker B: And sure enough, our first value is greater than 10. Our second value is less than 10, so it looks like that's working as designed. So now for our last exercise, we're going to create one more column with conditional logic called country name. And what this is going to do is use these country abbreviations in our country column as the basis for a new column that translates these abbreviations into the actual names of the countries. So to do this, we'll again hit our Conditional Column button on the Add column tab and we'll call this one country name. And then for my column name, of course that's going to be Country. Then for the logical operator, I'll obviously go with equals.
02:11:04.235 - 02:11:41.725, Speaker B: And then for our first country abbreviation, I'll use au, and that will be for Australia. And now because our solution is going to entail multiple levels of conditional logic, I'll hit this Add clause button here. And now we can add a whole new layer to our conditional column. So again I'll say Country equals. And then this time our abbreviation will be ca, and that's for Canada. And then I'll hit the Add Clause button again. Once again choose Country.
02:11:41.725 - 02:12:43.333, Speaker B: And this time our Abbreviation will be U.S. for United States. And then add clause again. And this time our country abbreviation will be DE, and that's for Germany. And then add clause one more time and this time our country will be gb and that's for Great Britain. And then finally I'll just add an else clause of other in case there are any values that we didn't account for in our mapping here, and this all looks good at this point, so I think I'll go ahead and click ok. So looking at the first two values in this column, United States and Great Britain, and then scrolling over to our country column, the corresponding abbreviations are US and gb.
02:12:43.333 - 02:14:09.485, Speaker B: So it looks like everything is working ok. Welcome back. So far we've spent a lot of time using Power Query to transform data in various ways. But in the end, the real point of all that work is to ultimately be able to visualize our data in such a way that we or others can glean meaningful insights from it. So since we've got a pretty good handle on our raw data at this point, let's shift our focus to building visualizations to translate our raw Bigfoot data into real insights. Insights like, where do we go to find Bigfoot? Or maybe even better, where do we go to avoid Bigfoot? So, if you've used Microsoft Excel much, some aspects of Power BI may be fairly familiar to you. But while many of Power BI's features overlap with Excel, the flexibility and power of its visualizations really set it apart from Excel or most any other visualization tool.
02:14:09.485 - 02:15:29.945, Speaker B: So one of the most straightforward but still powerful visualizations in Power BI is something called a matrix, which if you've ever used Excel, you you'll find to be very similar to a pivot table. So while a matrix doesn't summarize our data with pretty pictures like we might expect from something classified as a visualization, it is nonetheless a very powerful way to get rapid, intuitive insights into even large data sets. So I'm starting things off here in the report view of Power bi, which we've seen from time to time but haven't really explored. So this blank central area here is called the report canvas, which you can think of as a blank slate where we can build our visualizations. Now, over to the right, under the heading visualizations, is what's called the visualizations pane. This is where we can select From Among Power BI's vast library of visualizations to find one suitable for telling the story we're trying to tell with our data. So again, for now, I'm going to focus on matrix visualizations, and I can actually hover over these little icons here, and the tooltips will tell me what charts the icons represent.
02:15:29.945 - 02:16:26.205, Speaker B: And if I hover over this little guy right here, it says matrix. So I can just click that, and now a blank matrix visual appears on the canvas. So our next step is to actually add data to this visualization, which we can do by selecting fields from our Bigfoot sightings data set over here in the fields pane. So I'll start by finding the state field and then just clicking the checkbox to the left of it. Now, if you look below the list of visualization options on the visualizations pane, you'll see a heading called rows, and it looks like our state field has been added to that automatically. And then if we look over at our matrix visual on the Report canvas. We see a list of all of our states.
02:16:26.205 - 02:17:53.654, Speaker B: Now, the data and matrix visualizations can be a little bit hard to read by default, but fortunately we can zoom in on visualizations in power BI to make them much clearer by going into focus mode, which we can do by clicking on this little button right here. And now we have a much clearer view of what's going on with our matrix visual so far. So, generally speaking, matrices, I think that's the plural of matrix and power bi. Visualizations in general work by breaking down numeric fields in our data, the kind of fields you might want to apply calculations to, like counts, sums and averages by categorical fields which divide the calculations against those numeric fields into groups. These categorical fields are often referred to as dimensions, especially in the business intelligence space. So with matrix visualizations, categorical fields typically either go into the rows area, which power BI selected for us by default when we opted to add the state field to the matrix, or the columns area right beneath it. In either case, the unique values in that column will either be listed out vertically as rows, as in the current case, or horizontally as column headers.
02:17:53.654 - 02:18:33.655, Speaker B: Then any numbers we want to summarize in the matrix will be broken down by those unique values. This uniqueness is important not to overlook. It doesn't matter whether there are five or five thousand sightings in our data. Where the state is Washington, for example, that category will only get exactly one row in our matrix. The proportionality comes later when we start breaking out numbers by those categories. Speaking of which, let's start that now. So our goal here will be to break out our count of Bigfoot sightings by these various states.
02:18:33.655 - 02:19:45.685, Speaker B: So first we need to pick a field whose individual rows can be counted, so the count of those sightings can be broken out by the list of states we see here. Now, many fields will actually do for this purpose since you can count numeric and non numeric data alike. But we want to make sure that it's a field with no null values, since nulls won't be counted, given that the best candidate is probably our index field, which is guaranteed to have a value for every row. Now that field will need to go into the values area on our visualizations pane. Fields we're going to use as the basis of summary calculations like counts and sums and so on will go into this values area of the matrix. So let's find this index field over here in the fields list and then instead of clicking the checkmark next to it, I'm going to click the Field and just drag it over here into the values area. And while this seems to have worked to an extent, these numbers look way bigger than they should.
02:19:45.685 - 02:20:35.175, Speaker B: After all, our entire data set only has around 5,000 records. And this is telling us that just the state of Arkansas had over 117,000 sightings. So why is this happening? Well, it's because power bi basically outsmarted itself. It saw a numeric field with integers, that being our index field, and assumed that we would want to sum those numbers, but we actually want to count them. So how do we change that guess that power bi defaulted to. Well, let's start by clicking this little down arrow on the index field in the values area to see what options power bi gives us to customize it. And right away we see a check mark next to sum here.
02:20:35.175 - 02:21:23.105, Speaker B: So that's telling us that power bi is currently summarizing that field with a sum calculation. So all we need to do is change that summary calculation to count. That's better. Now our visualization is producing much more reasonable looking numbers. I can buy, for example, that Arkansas had 78 Bigfoot sightings over the period of time that the data was gathered. Now, one little cosmetic enhancement I'll make to our visualization before we move on is to change this ugly column header here that just says count of index. I'd like to change that to something more meaningful that communicates the intention of our visualization, like number of Bigfoot sightings.
02:21:23.105 - 02:22:13.075, Speaker B: So I can do that by once again clicking this down arrow next to our field in the values area and then choosing rename for this visual. And I'll just change that name to be number of sightings and hit enter. And I think that's a pretty big improvement over what we had before. So right away, our little starter visual has provided us with some real insights. We can now see the number of sightings by state, but we're probably most interested in which states had the most sightings. We can't see that easily here. However, we can sort by the number of sightings instead by just clicking the column header of our number of sightings column here.
02:22:13.075 - 02:22:46.395, Speaker B: And now the rows of our matrix visual are sorted in descending order by number of sightings. And you can tell they're sorted in descending order because this arrow on the number of sightings column header is pointing down. And if we click that column header again, the direction of the arrow flips. And now we're sorting in ascending order with the smallest number of sightings on top. But of course, what we're going for is to see the states with the most sightings. So I'll click the header one more time. And there's our answer.
02:22:46.395 - 02:23:19.565, Speaker B: Washington is clearly the state with the most sightings, and it's not particularly close. Makes you wonder what's going on up there. Maybe Bigfoot likes Starbucks. Okay, so next, again, taking advantage of one of the fields we created. See how important feature engineering is. Let's see how many sightings happened during a full moon. So let's go over here to our fields list and drag the full moon field into our values area alongside number of sightings.
02:23:19.565 - 02:23:55.123, Speaker B: Now that's odd. Power bi is not trying to summarize this field by a sum or account. Instead it says just first full moon. So let's click the down arrow next to that field in the values area to see what's going on. And the list of summarizations we see for this field are very different than the ones we saw for our index field. We just see first, last, count, distinct, and count. Now, if you think about it, there are no summarizations here that would apply only to numeric data.
02:23:55.123 - 02:24:43.407, Speaker B: You can average numeric data, but you can't average text data. And we don't have an average option here. So it's almost like power bi thinks our full moon field is a text field. Now that's odd, because as we know, since we created the field, it just consists of ones, zeros, and nulls. But now that I think of it, I don't recall explicitly telling power query what data type that field was. So let's actually go into power query here by hitting our transform data button, and then I'll scroll over to find our Full moon field. And if we look at the little symbol to the left of the field name, it says ABC123.
02:24:43.407 - 02:25:22.695, Speaker B: Which tells me Power BI thinks it's just a generic data type. It's not explicitly defined as a numeric data type. So let's select that field, go up to our transform tab, and we see that it is classified with a data type of any. So let's change that to whole number. And then in our home tab, hit close and apply. And just like that, power bi has changed the default calculation. It's applying to this field from first to min.
02:25:22.695 - 02:25:54.083, Speaker B: And that should give you a hint that those numeric calculations are now available to us. Because min is something you probably wouldn't apply to text data, but only to numeric data. So let's again click that down arrow next to our field in the values area. And now we see all of our usual suspects for performing aggregate calculations against numeric data. And I'll choose sum here. Since our goal is to get a sum of all the full moon sightings for each state. And there we are.
02:25:54.083 - 02:27:06.135, Speaker B: These numbers make a lot more sense. Our visualization is telling us, for example, that Florida had 250 sightings total, 12 of which occurred during a full moon, or, per the logic of our full moon field, when the moon was at least 95% full. Now, finally, as a cool trick, at least I think it is, let's change the calculation on our full moon field to show the percentage of sightings during a full moon Instead of the number of sightings. But how would we go about doing this? Well, one of the advantages of a 10 field like our full moon field Is that if you take the average of the field, you'll end up with the percentage of values that were one, that is to say, the percentage of the individual sightings where the moon was full. Let's say you have five, four zeros and one one. If you add them all up and divide by five, which gives you the average, you get one divided by five, which is 20%. So we can leverage this to display the percentage of sightings During a full moon Right in our matrix.
02:27:06.135 - 02:28:19.085, Speaker B: So all we have to do here is again click that down arrow next to our field in the values area, and then summarize this field by average instead of sum. And this did work technically, but these numbers are definitely in need of some formatting magic. Right now, we're just seeing raw decimal numbers when what we really need to see are percentages. So to format this field just for the purposes of this visualization, we can select the full moon field over in our fields pane, and then up in the ribbon in this contextual column tools tab that popped up automatically, we'll change the format from whole number to percentage, and that looks much better. Now, it's important to call out here that this does not change our underlying data in any way. It's just changing how that field is formatted here in our report view. Now, finally, once again, as a bit of aesthetic housekeeping, I'll rename our new field here to percentage full moon sightings.
02:28:19.085 - 02:29:15.589, Speaker B: Nice. I think our visualization is off to a strong start, but there's still so much more we can do. Up next, we're going to look into some ways to add more nuance and complexity to our matrix visualizations. I'll see you then. Hey, welcome back. So, I would argue that our little breakout of bigfoot sightings by state here is pretty interesting as is. I mean, now I know to be on high alert for Bigfoot next time I'm in Florida.
02:29:15.589 - 02:30:29.335, Speaker B: Florida or Washington state for example. But we often want to break down the numbers in our visuals by more than one categorical field or dimension. Right now we have two calculations. Count of sightings and the percentage of those sightings that occurred under a full moon, broken out simply by the state the sightings occurred in. But what if we wanted to make our analysis more granular? What if, for example, we wanted to see these numbers broken out by county as well, not just by state? Well, fortunately, Power BI's visuals make this kind of multilayered analysis intuitive and fast. Although we shouldn't get carried away and make our visuals too complicated, since this will make our reports less usable and comprehensible for the non technical people we're likely to be presenting to. So in the spirit of showing more than telling, which is generally conducive to the learning process when explaining visualizations, I'm just going to drag the county field from over here in our fields list directly underneath the state field in the rows area of our visualizations pane and see what happens.
02:30:29.335 - 02:31:41.745, Speaker B: And as far as we can tell right now, nothing happened, except that all our states have little plus signs next to them. But if we click one of those plus signs now we see a list of counties under the state, all counties which are contained within that state. And then looking over to our two metrics, we see that those summary statistics are broken out now by state and county. So dragging an additional field into the rows area created hierarchy in which each state which you can think of as a parent entity in the hierarchy, contains at least one, but typically more than one child entity, in this case counties. Now keep in mind here that while Power BI will technically allow you to stack fields in the rows area, or the columns area for that matter. We'll get to that in a moment. However you like, fields with fewer distinct values, like for example state should go higher in the list of fields, and fields with more distinct values like county should go lowest.
02:31:41.745 - 02:32:43.095, Speaker B: To get an idea why, let's just drag county above state here in the rows area. So while the breakout that results from that still produces accurate numbers, they are much less meaningful than before. And that's because the hierarchies are no longer meaningful. So instead of a large state level number being broken out by smaller subtotals, those being for counties, we start with the smaller subtotal. So let's drill into Pierce County Here, and under our total for Pierce county, we get the subtotal for the associated state, but just for that county, which of course is the same number. Again, you want to think of this as a hierarchy where the field with the fewest distinct values is the parent of the field with more distinct values. And this principle will hold true as we explore other kinds of visualizations down the road.
02:32:43.095 - 02:33:48.481, Speaker B: So let's go ahead and clean this up and move county back under state in the rows area. Now, to take this concept of slicing and dicing our aggregate calculations by multiple categorical fields even further, let's add yet another layer of grouping, this time to the columns area. So what I'll do is take our season field from the fields list over here and just drag it into the columns area for our visual. And this results in the columns for each of our two calculations, count of sightings and percentage of full moon sightings being repeated for each unique value in our seasons column. So we see that we have a top level heading for fall here, under which we have a breakout for both of our metrics, number of sightings and percentage of full moon sightings. And then that pattern repeats for our other seasons. Spring, summer, unknown, by the way, that's not a real season.
02:33:48.481 - 02:35:00.082, Speaker B: And winter. Now, while this is perfectly valid and does pack a lot of information into a relatively small space, it also, at least to my eyes, makes it somewhat difficult to figure out what's going on. So to keep our focus on the effect of breaking our data out by multiple dimensions or categorical fields, go ahead and remove our percentage of full moon sightings metric for now. So I'll just go down to the values area over on our visualizations pane and remove that. So what I want to focus on now is how numbers at different levels within the matrix are calculated. You can think of each number here inside of the matrix as a kind of intersection or address of any number of filters that are being applied in the rows area or the columns area of the matrix. If we zoom in on a particular number here in our matrix, and I'll choose this number 19 here for Pierce County, Washington in the fall.
02:35:00.082 - 02:36:39.255, Speaker B: This number 19 represents our calculation that being the number or count of Bigfoot sightings being applied to only those rows of data in the source data set corresponding to this particular address in our matrix visual, we can think of this address as the intersection of all row and column headers. And as we'll see later on, filters applicable to this particular data point. So here that is season equals fall state equals Washington and County equals Pierce. And we can even confirm this by taking note of our number, which is 19, and then switching to the data view in our power bi file and applying the same corresponding filters in the data view. So we'll filter for state equals Washington, and then we'll filter for county equals Pierce. And then finally, we'll filter for season equals Fall. And looking down at the bottom left, we see that our table consists of 3,480 rows, but only 19 filtered rows, the same rows that were reflected in that number 19 in our matrix visual.
02:36:39.255 - 02:37:50.285, Speaker B: This is of course, because our metric number of Bigfoot sightings was a simple count, and the number of rows we see here is the count of rows where season equals fall, state equals Washington, and county equals Pierce. But we could apply a similar validation to another type of calculation in our matrix. Let's say an average of our 1, 0 full moon flag in our data, we would apply the filters just the same, but then take the average of the values in that full moon column just for those filtered rows. And that's pretty much what our matrix visual is doing for each data point we see behind the scenes. This concept of a data point's address relative to the row and column headers in a report determining how that data point is calculated is called filter context. And it's a very important concept to keep in your back pocket. Now, the visualization techniques we've played with so far have been pretty simple and intuitive, and it may not be absolutely necessary to think so precisely about how numbers in a visual are calculated just yet.
02:37:50.285 - 02:39:00.185, Speaker B: But once we get into DAX programming, you'll want the whole filter context thing to be second nature. So why not get into the habit now? Right now, before we move on, let's swap out the row labels in our matrix with something perhaps a little more interesting. Date field. So first I'll remove state and county, and then going over to our field list, I'll grab date and drag that into the rows area. Now, note that even though our date field simply had date values, Power bi has automatically grown grouped the values in our visualization here by year. And if we click on this now familiar plus sign next to a year, let's just say 2004. We see that our 2004 sightings have been broken out further by quarter, and we can keep going in this fashion, drilling all the way down to month, and then within a month even to the actual day of the month.
02:39:00.185 - 02:39:52.035, Speaker B: So Power BI clearly has some built in intelligence around how to handle date and time values in a matrix visual. It just assumed that I wanted these additional fields included as part of a date hierarchy. But we don't have to depend on Power BI's artificial intelligence to decide how to group date values. Let's say, for example, I didn't want quarter to be included in my matrix. Fortunately, I can remove any of these automatically added pseudo fields I want just as easily as any other. So looking back over to the visualizations pane in the rows area, we see under date that these four additional fields that we didn't explicitly add have been included. But if I click this X next to quarter, it disappears from our visualization.
02:39:52.035 - 02:40:57.785, Speaker B: Alright, now that we've learned to construct a complex visualization with multiple layers, in the next video, we'll see how we can navigate through those layers efficiently and effectively. I'll see you then. Hey, welcome back. So, with our count of Bigfoot sightings now broken out by season, year, month, and day. Whew, that's a lot. I'll clean up our visual here by collapsing these groupings that have expanded for day of month, and month. So to do that, I can right click a level in the visualization and then hover over, collapse, and then just choose entire level.
02:40:57.785 - 02:42:07.331, Speaker B: And that will make sure that our visualization is grouped at that level for every unique value in that field, in this case being month. And then I can do the same thing to roll our visualization back up to the level of year. I'll just right click any of our year values over here in our row headers and again hover over, collapse and then choose entire level. And now we're right back where we started, but without that quarter field in our groupings. So in the same way that you can collapse these levels or layers of grouping in our matrix visualizations, you can also expand them. So if I right click one of our years over here in our matrix visual, and then again hover over, expand, and then choose selection that expands just the child elements for that particular item among our row headers. And by the way, the effect of that is the exact same as clicking a plus sign next to any one of these items.
02:42:07.331 - 02:43:11.433, Speaker B: As you can see if I just click the plus sign next to 2004, but we can also expand all of the items at this level to see their child elements. So to see how that works, let me go ahead and collapse both of these individual years, and then I can right click any of these years again, hover over, expand, and then choose entire level. And now we see that all of our years have been expanded, and then we can Keep going in this fashion by right clicking one of our months, hovering over expand, and again choosing entire level. And now all of our months have been expanded so that we see the days of the month within them. Now, keep in mind that the numbers you see here are all specific to the level of hierarchy immediately to their left. So in this first row, the numbers we all see are for 2004 as a whole. And then in the second row, the numbers we see are specific to October in 2004.
02:43:11.433 - 02:44:01.625, Speaker B: And then the numbers we see in the third row here are specific to the 15th day of October just in 2004. Now, this may seem obvious, but we'll soon see another method for navigating the layers of the hierarchy that does things just a little differently. But before we get to that, let's go ahead and collapse these levels of our visualization for now. So again, I'll right click month, hover over collapse, and choose entire level. And then I'll do the same thing for year. Now, that other approach I mentioned a moment ago involves removing the current top level of the hierarchy altogether. Currently, that's year.
02:44:01.625 - 02:45:24.475, Speaker B: So if I right click one of our years here and then choose show next level. Now, year has been removed from our visualization entirely so that each category in the subordinate group month represents the grand total for that category. So instead of a row for October here giving us the numbers for October in a particular year, these numbers represent the numbers for October over all the years in our data set. So which of these approaches should you take? It really just depends on what your analysis requires. Do you need to see grand totals for individual categories like month or day, not split out by any parent groups? Or do you only need to see the numbers broken out by the hierarchy within the data, in our case, year, month, and day, as is usually the case when it comes to analytics, you should let your customers needs and the reality of your data drive your decisions instead of having an approach in mind from the beginning. Now, for now, I think our report here makes more sense broken out in that more hierarchical way by year, month, and day. So to get back to that, I'll just right click one of our months and then choose Drill up.
02:45:24.475 - 02:46:35.565, Speaker B: And now that restores our old hierarchy of year, month, and day. Okay, now aside from general structure and layout, we haven't been paying much attention to the aesthetic appeal of our matrix visualizations. Up to this point, we focused strictly on number crunching. But up next, we'll explore some of the ways that you can customize the look and feel of your Matrix visuals. I'll see you then. Welcome back. So, before we dive into all the splashy, visually impressive charts that Power BI has in its arsenal, let's explore a few methods for adding visual pizzazz, not to mention some additional insights right to a plain old matrix visual.
02:46:35.565 - 02:48:00.015, Speaker B: And as a bonus, the menus we'll navigate to find these features will be the same ones we'll use to customize those other fancier visualizations later on. Now, to give ourselves a somewhat less busy visualization to work with, I mean, our current matrix here has quite a bit going on. I'll add a simplified version of this breakout to the canvas. Now, first we need to get back to the canvas by leaving the focus mode for this visual, which I can do by hitting this Back to Report button here. Now, back on the Report canvas, we could create a new matrix from scratch, but it's often simpler, and it certainly is in this case to just copy an existing visual and then customize the copy as needed. So to copy the existing matrix without confusing it by selecting any of the numbers in it as we try to click inside of it, I'll actually widen the background area here by hovering over the bottom right corner of the matrix's boundary with the canvas and then dragging down and to the right. So this exposes some white space inside the boundary of the matrix, which we can now right click and then hover over copy, and then choose Copy Visual.
02:48:00.015 - 02:49:15.255, Speaker B: And now click anywhere in the canvas and hit Control V to paste. Now, at first it looks like we didn't do anything, but that's because the copy was dropped directly over the original version. So to move the copy, I'll click anywhere on this white space inside the boundary of the matrix, hold down the mouse button, and then drag to wherever I'd like the copy to be positioned. And when I let go, there we are, a brand new copy of our existing matrix. So now I'll just remove the season field from the columns area on the copy of our matrix visual and then replace this date hierarchy with the state field from our data. And then I'll also add county as a sub level grouping in our rows area. So this breakout should allow us to zero in on the effects of the formatting we're going to apply a bit more effectively.
02:49:15.255 - 02:50:04.427, Speaker B: And speaking of that formatting, we'll start with perhaps the easiest way to change not one, but several formatting options for a visualization at the same time. So easy, in fact, it only takes a single mouse click. This feature is called Style Presets. Now, these presets are found where lots of other formatting related customizations are found. The Format tab of the Visualizations pane Now keep in mind, if you're dealing with multiple visualizations like we are here, you'll want to make sure that the one you're actually trying to format is selected before you go. Customizing Options over here on this Format Visual tab. And boy are there lots of options here.
02:50:04.427 - 02:51:19.825, Speaker B: You'll find many, but crucially not all of the different ways you can customize the look and feel of a matrix or any other Power BI visualization. Now, I won't be going through all of these since the process of making these various formatting customizations is quite similar from one option to the next, but I will touch on a couple of my favorites, which brings me right back to style presets. So first, to better see these style presets in action, let's take our new Visual Here into Focus mode. And now if you click this Style presets section over in the Format Visual tab and then choose an item from this drop down list, I think I'll choose Bold Header Flashy Rows. You'll see that not one aspect of our matrix's appearance has changed, but several. Exactly what changes and how varies from preset to preset, but things like fonts, background colors, and more can all change. And all these tweaks are designed to work together by people with a much better eye for design than I at least could ever hope to have.
02:51:19.825 - 02:52:31.455, Speaker B: These presets are so easy to change. In fact, I recommend just clicking around the options in this drop down list to find one that works for you and or your audience. Now, as for me, I'm just going to stick with the default style preset since I trust Microsoft's default selection a lot more than my own design skills or even my ability to choose among a list of style presets. Now, of course, if you need to get more granular with your customizations, you can always set style properties such as font size or background color for individual components of your visual to your particular liking. For example, even with a style preset applied, we can still target, say the font size of our row and column headers. So to do that I can just roll down to first the Column headers section here on the Format Visual tab, and then we see a number of options for customizing the font, the text color, the background color, and so on. But all I'll do is just tick this font size up a bit and we can immediately see that change reflected over in our Visual.
02:52:31.455 - 02:53:30.765, Speaker B: And then I can do the very same thing for our row headers. And once again, we immediately see the effect of our change in our matrix visual. Now, as just one other of gazillions of possible examples, we can change the background color here in the values area where the actual data in our visualizations live. More specifically, I want to change the background of the alternate rows in our visual, since the default style has alternating rows with different background colors. To do that, I'll collapse this row header section over in the Format Visual tab and then roll up to the values section and expand that. And then scrolling down through the options, we see alternate background color. So we'll click this down arrow next to that.
02:53:30.765 - 02:54:16.895, Speaker B: And now we see a whole color palette of colors that we have to choose from. So I think I'll go with this kind of light orangish color here. And now those alternating rows in our visual are filled with that light shade of orange. Now, this change may or may not have actually made things better. Again, I am not known for my eye for design, but that's why I'm so grateful for Microsoft's presets, which makes so many of these little design decisions for me. But if your design skills are better than mine, and they probably are, have at it and customize away. As you can see from all the options over here on the Format Visual tab, there is no shortage of ways to do so.
02:54:16.895 - 02:55:14.415, Speaker B: We've honestly barely scratched the surface. But the good news is, fine tuning other aspects of a visual's appearance works pretty much the same way as with the options we've already worked with. Okay, so now that we've got a handle on the basics of formatting visualizations, let's look at a more advanced approach that makes our formatting respond dynamically to the data in those visualizations. That's up next. I'll see you then. Hey, welcome back. So we now know how to apply specific formatting options to various components of our matrix visuals.
02:55:14.415 - 02:56:09.175, Speaker B: But what if we want that formatting to be dynamic changing based on the actual data in those visualizations? This may sound daunting, but with a Power BI feature called conditional Formatting, it's surprisingly straightforward. Conditional formatting, as the name implies, allows us to apply different formatting to different values in our visuals, depending on what those values are. One conditional formatting option, my personal favorite, which will actually give us our first taste of charts, is data bars. So data bars are terrific for giving scale to a column of numbers in the values area of a matrix. The largest number in the Column gets the biggest bar, the smallest number gets the smallest bar, etc. You get the idea. So let's see what this looks like in action.
02:56:09.175 - 02:57:42.861, Speaker B: Let's say we want to add some sense of scale and proportionality to our Bigfoot sightings numbers so we can easily identify the states that had the most sightings and get an intuitive sense of how the different states stack up against each other. Now, data bars, along with the other conditional formatting options in Power bi aren't in the Format Visual tab of the Visualizations pane like you might expect. Rather, you'll need to navigate back to the other tab of the Visualizations pane and then select the individual field in the values area here that you want to apply conditional formatting to. So to apply conditional formatting to our number of sightings metric, we'll click this down arrow next to it in the Values area of the Visualizations pane and then hover over conditional formatting. Note that there are actually a few different options here, but for now we'll select Data bars, and that brings up a dialog box with a bunch of options for configuring our data bars. So the default minimum and maximum values here, which say lowest value and highest value respectively, basically mean that the shortest bar is associated with the smallest value and the longest bar is associated with the largest value, which is almost always what we want. There is also the option to set different colors for a positive bar versus a negative bar, since it's possible to have negative numbers in our values area.
02:57:42.861 - 02:58:35.175, Speaker B: In those cases, the negative bars go from right to left by default, while positive bars go from left to right. But it's much more commonly the case and pretty much always the case when dealing with counts like we are here that all your numbers are positive, so we just need to worry about the color of the positive bar for now, which I'll leave as this default blue. With that said, as with almost everything in Power bi, you can certainly customize this color if you'd like. In fact, there's a whole palette of colors to choose from. Now, looking over here under bar direction, the default is left to right. Now that's conventional too, so we'll stick with that. Now that was admittedly a lot of discussion of these various options to ultimately say, welp, let's just stick with the defaults.
02:58:35.175 - 02:59:37.625, Speaker B: But this is often the case since the defaults are chosen for a reason and make sense for most common use cases. So with that said, I'll click ok. And sure enough, now we get little bars superimposed over the values in our matrix. Note that since we have our data sorted from the most sightings to the least, naturally the largest bar ends up on top, but the bars still give a nice sense of proportionality to our data. Just from eyeballing this, we can see that Washington State had roughly twice as many sightings as the next three states. And if we drill down a level, say into Washington, we'll see that the data bars at the lower level of the hierarchy reflect where each individual data point fits in relative to its peers. So many of the bars under Washington nearly span the width of the column, since several of Washington State's counties have a high number of Bigfoot sightings compared to other counties in the US As a whole.
02:59:37.625 - 03:00:59.515, Speaker B: But if we then expand the row heading for Kentucky, we see that the bars for the individual counties are pretty short, and that's because none of Kentucky's counties stack up particularly well against all the other counties in the data. Now, sometimes you may not even care about the actual numbers at all, but only care about how they compare relative to each other. In this case, you can even opt to only show the data bars, which we can do easily by going back into our Conditional formatting menu by clicking on our Number of Sightings metric over in the Format Visualizations pane and then hovering over Conditional formatting again and then data bars and now back on this menu. I'll just check here next to show bar only and then click ok. And now we only have bars in that values area, but no numbers. But personally, I generally prefer to see the numbers and the bars together, so I'll go ahead and hit the undo button at the top of our window to bring those numbers back. So data bars are cool and all, but they're not the only game in town.
03:00:59.515 - 03:02:06.745, Speaker B: Up next, we'll explore another conditional formatting technique that allows us to target specific values in our data dynamically. I'll see you then. Welcome back. So, now that we've gotten our feet wet with conditional formatting, I'd like to cover another somewhat more surgical approach. The data bars we've worked with up to this point apply to some extent to every cell in our targeted column. Every value gets a data bar of some length, but sometimes we'd rather not call attention to every single value in some way. Rather, we might only be interested in those values that meet a certain specific requirement or set of requirements.
03:02:06.745 - 03:03:10.685, Speaker B: For example, let's say we'd like to call out any states where the percentage of sightings that occurred during a full moon what's greater than 10%, but apply no additional special formatting to any of the other values. Data bars won't work in this scenario, but we can make use of another kind of conditional formatting called a rule. So a rule is basically one or more logical conditions that determine whether or not any special or additional formatting will be applied to a value. To see this kind of conditional formatting rule in action, we'll first need to add that Percentage of Full Moon Sightings column we created a couple videos ago back into this matrix visual. Now, unfortunately, that means going through the associated steps, customizing the aggregate calculation, and then renaming it, for example. Once again, if this seems inefficient to you, you're 100% correct. But don't worry, we'll learn how to solve for that very soon.
03:03:10.685 - 03:04:13.295, Speaker B: For now, though, let's quickly run through all those steps to get that field back into our visualization. So first I'll expand the fields under our Bigfoot Sightings data set over in the Fields list, and then I'll drag the Full Moon field here in the Values area of our visualization. Note that since we already told Power bi that we want the Full Moon field formatted as a percentage when we added it to another visualization earlier on, we actually don't have to specify the formatting again. But think about it. It would be pretty inconvenient if we wanted to do a simple sum of the same field, our Full Moon field in another visual not formatted as a percent. But I digress back to the task at hand, which is getting this Full Moon percentage field back in our visual the same way we had it earlier. I'll go into that field in our visualizations pane and make sure it's summarized as an average, not as a sum.
03:04:13.295 - 03:05:43.585, Speaker B: And then I'll also rename the field to percent Full Moon Sightings and hit enter. And now we've got our old familiar Percentage of Full Moon Sightings field back in our visual, which means that we're finally ready to add that conditional formatting rule to this field. So let's say that to call out the values in our Percentage of Full Moon Sightings column that are greater than 10%, we'd like to format them with a font color that kind of pops and makes them stand out. To do this, let's click the arrow next to our percentage Full Moon Sightings field over in the visualizations pane and then hover over Conditional formatting. And because we want to conditionally alter the font formatting in this case, we'll hit font color and now we're taken to a dialog box that will allow us to define the logic and visual effect of our conditional formatting rule. As our first step here, we'll change the value in this format style dropdown from gradient to rules. Next, we need to make sure that the conditional formatting will be based on the same summarization or calculation of our target field as we currently have in our matrix.
03:05:43.585 - 03:06:40.775, Speaker B: This will ensure that the conditional formatting is consistent with the values we actually see. So right now, sum is selected, but we're actually averaging the full moon field in our visual, so we'll change the selection here to reflect that. So now comes the interesting part, the actual rules. Here we specify the logical condition or conditions that determine whether a particular data point will receive special formatting. If you recall the video on using conditional logic to create derived columns in Power Query, this interface is quite similar. It has the outline of a logical if then type statement built out for us, and then it's up to us to fill in the blanks. More specifically, it's asking us to specify a range of values that is to say, greater than one threshold less than another.
03:06:40.775 - 03:07:22.197, Speaker B: So for the first blank in the template, we'll want to make sure that we're selecting the appropriate comparison greater than, greater than, or equal to or just plain old equal to. Note that if we select just plain old equal to, the second half of the if template disappears. That's because there's no need to define a range in this case, since we're only checking for equality to a single value. But that's not what we want. So we'll hit this dropdown again and select greater than. Now we need to specify the lower end of our range. In our case, that's pretty clear.
03:07:22.197 - 03:07:55.175, Speaker B: We want to highlight values greater than 10%. So I can just type in 10 here. Now, next, we're asked to make sure that we're specifying the correct type of numerical data. Number or percent? Well, the answer is kind of both. Percentages are numbers just expressed in a different way. When you do calculations with percentages, they're really just treated as decimal numbers. For example, 90% is just 0.9.
03:07:55.175 - 03:08:26.805, Speaker B: Therefore, to simplify things, I pretty much always stick with number here. But that means we need to change our 10 to 0.1, which is the number equivalent of 10%. So I'll do that. Now, the top end of the range is a little less obvious. We could enter less than 1 since it doesn't make much sense to have a percentage value higher than 100%. But that logic won't work in every case.
03:08:26.805 - 03:09:20.765, Speaker B: A better approach if our range is open ended, as it is in our case, since we really only care about where the bottom end of the range begins, that being 0.1, is to enter the keyword max here. This basically says that the maximum value of all the values in the field is the top end of the range, which ends up being just another somewhat more awkward way of generally Expressing Greater than 0.1. So I'll go ahead and type max here. Now, keep in mind, you could flip this logic if you wanted to highlight all values below a certain threshold, say anything less than 0.1. In this case, 0.1 would be your maximum value and you would just type the keyword min for the minimum value.
03:09:20.765 - 03:10:22.255, Speaker B: So with our logic defined, the last step is simply to choose a font color for the values we're trying to highlight. Now, given my profound lack of intuition for visual design, I'll just sort of randomly go with this reddish color here and at this point we should be good to go. But before we get the satisfaction of hitting ok, I do want to point out one thing, and that is that we can actually add more rules. This new rule button here lets us add as many new rules as we want. Now, if you're just trying to highlight outliers that meet specific criteria, as is often the case, additional rules typically aren't necessary. But they can be useful if you want to apply different formats contingent on different logical conditions. The main thing to watch out for here is to make sure your logical conditions don't overlap, or else you may not get the conditional formatting you expect.
03:10:22.255 - 03:11:21.535, Speaker B: Now, the other thing I want to call out is that you're not limited to applying these formatting rules just to font color. You can actually apply them in pretty much the same fashion as we saw here, to background colors as well. So with all that said, let's finally hit ok. And then scrolling through our matrix, we see that only the small handful of data points where more than 10% of sightings occurred during a full moon have been clearly highlighted with red font. Pretty cool. Ok, so remember how annoyingly inefficient it was to recreate that percentage full moon sightings field after having followed all the same steps to add it to another visual previously? Well, up next we're going to look into a technique for making our metrics more portable and more powerful using something called measures. I'll see you then.
03:11:21.535 - 03:12:49.295, Speaker B: Okay, time for another solution Walkthrough. So my first step Here back in the report view of our power bi file is to rename the one page in Report view from the default page 1 to Matrix. So I'll go ahead and do that. Next, I'm going to add a matrix visual to the canvas. So over here in the visualizations pane, I'll click the icon for matrix visual and then I'll expand that just a bit. And I'll also minimize our filters pane here just so we can see things a bit more clearly. And now my goal for this matrix visual will be to summarize the count of UFO sightings by the shape of the sightings on rows and that sighting duration category field we created in the last solution walkthrough.
03:12:49.295 - 03:13:45.011, Speaker B: So to do that, I'll expand our UFO sightings data set over here in the fields pane and I'll first drag in shape on rows and then I'll grab our sighting duration category column and add that to the columns area. And now to get to the count of sightings, I just need to pick a field that I know will have a value for every row, because that's how count works. Count is just going to count every non null value, non blank value in a column. And one column we know will have a value for every row is that index column we added. So I'll grab that and then drag it into the values area. And as you can see by default, power bi is summing that field as a sum because it has numeric values. But what we want is a count.
03:13:45.011 - 03:14:25.705, Speaker B: So I'll click this little down arrow next to the field in the visualizations pane and then choose count as the summarization method. And that looks a lot more sensible. So now to see our work a little bit more clearly, I'll take this matrix visual into focus mode. And now in focus mode, I'll make a few more modifications. First up, I'll sort the matrix visual by total number of sightings in descending order. So that means I'll want to click this total column here. And now, as we can see, the sightings are sorted in descending order by total number of sightings.
03:14:25.705 - 03:15:14.345, Speaker B: Now I'll also add data bars to give a sense of scale to the numbers we see here in the visual. So I'll need to go to the count of index in our visualizations pane, click that and then hover over conditional formatting and then data bars. And then I'll just stick with the defaults here. And there we go. Now I also want to bold the column and row headers and Also increase the font size of the column headers to 12. So to do that, I'll go over to this format visual tab on the visualizations pane, and then I'll go to column headers first and bold those. And also tick this font size up to 12.
03:15:14.345 - 03:16:05.515, Speaker B: And then I'll go to row headers and just bold those. Nice. So at this point, I think we're about done with this visual, so I'll go back to the main report, and now I'll add a new matrix visual to the report. Now, my goal for this matrix will be to break out average siting duration in minutes by country name and city. Now, country name and city should be layered hierarchically so that if you click a country name, you'll see the cities in that country along with their associated average sighting durations. So let's get to that. So first up, I'll drag in country name into the rows area, and then I'll add city directly beneath that.
03:16:05.515 - 03:17:22.345, Speaker B: And then to get to our siting duration in minutes, I'll drag our minutes observed field into the values area and then make sure it's summarized as an average. And now taking our new matrix into focus mode, I'll also rename the column of our matrix that lists the average siting duration to average sighting duration minutes. So to do that, I'll click the actual field in the values area of the visualizations pane and then choose rename for this visual. And again, I'll just call this average sighting duration minutes. And then I'll add a few visual flourishes as well. For one thing, I'll customize the alternate background color that you see here in these two rows shaded gray to be some shade of blue. So to do that, I'll again go back to the format visual tab of the visualizations pane, and then I'll expand the values section and scroll down to alternate background color.
03:17:22.345 - 03:18:12.913, Speaker B: And here I'll just choose a light shade of blue. And we see that that appears to have worked. And if we even drill into a particular country to see the cities within that country, sure enough, the alternating rows in that subgroup are light blue as well. Now, another step I'll take is to bold the row and column headers and also increase the font size of the column headers to 12, just like we did in the last example. So I'll collapse this section we've been working with and then go to column headers and I'll bold those columns and tick the font size up to 12. And then I'll also bold the Row headers. And with that, I think this is looking pretty good.
03:18:12.913 - 03:18:58.215, Speaker B: So I'll go ahead and go back to report. And now I'll add a third visualization to the canvas, another matrix. And for this one I want to break out the percentage of sightings that were an hour or more in duration by year, quarter and month, but not day. Now we shouldn't have to add all those date related fields one at a time. We should get them all for free by just dragging in a single date field to our matrix. In fact, it'll actually be our job to remove one of those fields. So to see what I'm talking about, let me drag in this date field to the rows area of this matrix.
03:18:58.215 - 03:19:27.565, Speaker B: And as you can see, we were given this entire date hierarchy for free. We have year, quarter, month and day. Now I want those first three, but I don't want day. So I'll click this X here to remove day. And now if we click next to a single year to expand it, we see we have quarters. And then within quarters we have months, but no individual days. Just what we wanted.
03:19:27.565 - 03:20:31.505, Speaker B: So now to get to the percentage of sightings that were an hour plus in duration, we'll use that hour plus duration flag that we set up in the last solution walkthrough. But instead of summarizing this as a sum to get to a percentage, we're going to summarize it as an average. So let's click this down arrow and then summarize by average. But because we're looking for a percentage and not a decimal, we'll also want to format this field as a percentage. So to do that, I'll want to select that field over here in the fields pane. So I'll click that to select it. And then up here in the ribbon in this contextual tab that appears because we selected a field in the fields pane, I'll change the format to percentage and I'll also specify one digit after the decimal.
03:20:31.505 - 03:22:04.855, Speaker B: And now if we take this matrix into focus mode to see it a bit more clearly, we can see that the formatting did in fact work. And in years where we did have sightings that were over an hour, the percentage of all sightings that were over an hour is reflected here in our matrix. So just remember, when you want to format whatever field you're using as the basis of your calculation in a visual, at least until we get into measures a little bit later on, you have to apply that formatting to the field it where it actually lives under the table, it belongs to over here in the fields pane, as opposed to the visualizations pane, which is specific to whatever visual you're building. So now that we have the basic structure of our matrix visual down, I want to add a little bit of conditional formatting here, such that any of these percent values that are greater than or equal to 10% will be formatted with a red font. So to do that again, I'll click this field here in the values area of the visualizations pane, the field that we're using as the basis for that percentage. And then I'll hover over conditional formatting and then font color. And here I'll switch the format style from gradient to rules.
03:22:04.855 - 03:22:52.221, Speaker B: And I want to base this on not the sum of our plus duration, but the average. So I'll change my summarization here from sum to average. And now for my logical condition that the rule will be based on. I want to say if my value is greater than or equal to not 10%, but 0.1 as a number. Again, as I mentioned in an earlier video, I generally find applying conditional formatting based on decimal numbers to be more reliable and less buggy than applying the same conditional formatting based on percentages. So instead of going by greater than or equal to 10%, we'll go by greater than or equal to 0.1,
03:22:52.221 - 03:23:20.791, Speaker B: which is mathematically equivalent. Now for the less than piece, instead of zero, we'll just go with max. And then for our actual color, we'll just choose a shade of red. And I think that will work. So now let's click ok. And sure enough, the two numbers we can see here that are greater than or equal to 10% are formatted with a red font. So now just a couple more housekeeping items.
03:23:20.791 - 03:24:34.925, Speaker B: I think I'll actually rename our field here that has those numeric values to percent, our plus sightings. And finally, I'll sort our matrix by year in descending order so that the most recent years show up on top. And now scrolling down to the values the very first year where at least 10% of the sightings were an hour or more. 1991 is formatted in red font, and all of our years do appear to be sorted from most recent year to oldest. Finally, just like before, I'll bold the row and column headers and increase the size of the column headers to 12. So on the format visual tab I'll go under column headers, bold those, increase the font size to 12, and then I'll do the same for the row headers, at least in terms of bolding, and I think we're looking good at this point. So that's a wrap.
03:24:34.925 - 03:25:28.725, Speaker B: Welcome back. So, in this one, we're going to switch our focus away from building and formatting matrix visualizations and towards the efficiency of our workflow as we build those visualizations out. Specifically, one inefficiency in particular has revealed itself as we've progressed to constructing multiple visualizations from our Bigfoot dataset. Can you guess what it is? I'll give you five seconds. Five. Four. Too late, I lied.
03:25:28.725 - 03:26:10.635, Speaker B: Sadly, I am not known for my patience. I can't even wait through my own countdown. So the issue is this. Remember how in the last video we had to recreate our percentage of full moon sightings field? Now, it would be one thing if this was just a simple matter of dragging the field into our matrix visual, but we also had to define what the calculation would be, an average, then rename the field. Even worse, as we build even more visualizations. And we will be, I can assure you of that. We'll have to follow the exact same steps for the exact same calculation over and over again anytime we want to add percentage of full moon sightings to a new visual.
03:26:10.635 - 03:27:03.685, Speaker B: And the same is true for any of the other aggregate calculations in our visuals, like number of sightings. That's a big problem because we often want to see the same calculations or metrics broken out and visualized in many different ways. So what we need, ideally, is a kind of portable calculation that we can define once and then add to as many visualizations as we like. Well, such a thing actually does exist in power. Bi hey, sometimes dreams do come true. It's called a measure. So measures allow us to define a calculation, name it, and set its formatting a single time, and then reuse that measure as many times and in as many visualizations as we want.
03:27:03.685 - 03:28:15.505, Speaker B: This not only makes us more efficient while saving us from annoying and repetitive work, it also makes our work less error prone. Trust me, whenever you try to define something the exact same way in a bunch of different places, inconsistencies will inevitably start to creep in. And this can cause major problems with sensitive data where accuracy and consistency is a must, like financial records or, you know, Bigfoot sightings. So I'd be remiss not to mention that another huge advantage to measures versus the standard canned summarizations we've been using so far is that they are incredibly flexible and powerful, allowing us to incorporate custom calculations in our visualizations that make the mirror counts and averages we've been working with so far look like child's play. If you've ever worked with calculated fields and Excel pivot tables, it's kind of the same idea, but way, way more powerful and portable to boot. That is, unless you're taking advantage of the Power BI like tools built right into modern versions of Excel. But that's a story for a different course.
03:28:15.505 - 03:29:11.145, Speaker B: So anyway, what makes measures so darn powerful? Well, they're actually based on a supercharged formula language called dax, specifically designed by Microsoft for for analyzing data. Now, if the word language, as in programming language, scares you, don't let it. If you've ever used formulas and functions in a spreadsheet program like Excel or even Google Sheets before, you'll probably find DAX very similar. At least at first. I say at first because DAX is one of those technologies that can be as straightforward or complex as you make it. You can use it to calculate a simple count or average, which, as we'll see in a moment, merely requires about the simplest formula imaginable. Or at the other end of the spectrum, you can use it to perform some seriously gnarly detailed data analysis.
03:29:11.145 - 03:30:08.485, Speaker B: We're actually going to dive pretty deep into the gnarlier parts of DAX later in the course. It's actually my personal favorite aspect of Power bi. But for now, we're going to focus on the portability of measures, rather than all the crazy calculations we can do with them. So to that, coming up in the next video, we'll create our very first measure to make our analysis of Bigfoot sightings more efficient. See you then. Welcome back. So, now that we know what measures are, it's time to get our hands dirty and start putting them to work in our report.
03:30:08.485 - 03:31:14.685, Speaker B: And while again, there are lots of cool things we can do with measures, for now, we're going to stick with using them as a portable version of the built in summarizations that Power BI gives us here in the user interface. For instance, sum, count, average, etc. So to create our first measure, we'll first hit the modeling tab of the ribbon in the Power BI user interface and then hit this new measure button. Now, right below the ribbon, we see a new widget that looks like a really wide text box. That's the formula bar, which is where we will enter the formula that will define our measure. Now, as I mentioned, measures are written in the DAX language and languages have a syntax, so we'll need to get to know the very basics of DAX syntax to get our measures working. The first piece is the measure name, which is immediately followed by an equal sign.
03:31:14.685 - 03:32:11.435, Speaker B: You can see that power bi has given us the word measure here as a placeholder, but we'll go ahead and replace that with number of sightings. So I'll go ahead and delete measure and just type in number of sightings. Now, on the right side of the equals sign, we will have our actual formula. This is where we can use one of power BI's built in functions to perform the calculation we want. Now, if you've done any programming or even used formulas and functions in a spreadsheet program like Microsoft Excel, the concept of functions is probably pretty familiar to you. But just in case, here's a definition straight from Microsoft. So per Microsoft, functions are predefined formulas that perform calculations by using specific values, called arguments in a particular order or structure.
03:32:11.435 - 03:33:10.815, Speaker B: Ok, so let's unpack this a little first. By formula we basically mean a calculation. For example, 2:2 would be considered a formula. So what a function does is take one or more of the arguments we provided, use them as inputs to a predefined formula or calculation, and ultimately return the result of that calculation. You can think of a function as a kind of black box which you pass data into and then receive an output from without necessarily needing to understand the gory details of the calculation that produced that result. As one of the simplest examples of this, the built in sum function simply adds up any numbers we provide it. Now, since in power bi, the data we work with is stored in columns, DAX functions like sum typically take entire columns as their arguments rather than individual values.
03:33:10.815 - 03:33:56.425, Speaker B: Then the predefined calculation or formula that the function represents is executed on the values from the column. For the sum function, that of course simply means adding the values from the column and returning the result. For the count function, it would mean counting the values in a column, and so on. You get the idea. So for our first measure here in our Power BI report, we want to count Bigfoot sightings. So here in the formula bar, after the equal sign, we'll start typing out the word count. And you can see, even as we begin typing out count, Power BI starts showing us a list of its built in functions that begin with what we've already typed.
03:33:56.425 - 03:34:54.355, Speaker B: We can use our arrow keys to navigate up and down this list. And then once we get to the function we want, which is actually the first one listed, we can hit the tab key to lock it in. Alternatively, you could have just selected the count function with your mouse as well. So this handy feature that recommended a whole list of functions that began with what we typed, and let us select one of those functions without typing out. Its full name is called IntelliSense, and it can not only save you a ton of typing as you saw, it can even help you find functions if you don't know their exact name. Now, as someone who detests typing any more than I absolutely, positively have to, and is very bad at it, I find intellisense invaluable. Now, you'll notice here in the formula bar that after selecting our count function, an open parenthesis was added after the function by default.
03:34:54.355 - 03:35:40.585, Speaker B: That's because all DAX functions, and if you've used functions in a spreadsheet like Excel or Google Sheets, it's pretty much the same thing. There require a pair of parentheses after the function name. Then we enter any arguments the function requires between those parentheses. So, Speaking of arguments, IntelliSense is already back at work here, supplying us with a list of the possible arguments we can provide to the count function. Remember how I said since Power BI data is stored in columns, these arguments are often, but not always columns from our data. Well, that's exactly what we see in this list. Columns from our Bigfoot sightings table preceded by the name of the table itself.
03:35:40.585 - 03:36:50.675, Speaker B: The table name is included because it's possible, and in fact very common, to have multiple tables in a power bi file. So specifying the table prior to the field lets us select the right field from the right table. So, considering that our goal is to reproduce the number of sightings calculation we've been using so far, we'll feed the count function the same field from our data that we've been using in our current calculation, the index field we created using Power Query. So let's arrow down until we find that, and then hit tab to lock it in, and then finally add our closing parenthesis and hit enter to complete the process. And after waiting a few seconds, well, it seems like nothing particularly interesting has happened. But if we look over to the top of our field list, we see that our number of sightings measure has apparently been added to our table. Now, that may seem a little odd, since a portable formula like our measure is just a categorically different thing than the columns of data it's lumped in next to.
03:36:50.675 - 03:37:47.175, Speaker B: But it turns out that power bi requires that measures be assigned to a table, and since we only have one table in our power bi file currently, that kind of narrowed down the options. Now you can distinguish measures from conventional fields in our fields list, since the measure name is preceded by this little calculator icon you see Here. And it's important to be able to identify these measures in the field list once we've created them, because we often want to modify or perhaps further refine aspects of measures after we create them. More on that in a bit, but for now, let's prove to ourselves that this actually works. So I'll take our second matrix visual here into focus mode so we can see what's going on a bit more clearly. Now, it turns out that adding a measure to a visualization works just like adding any other field. We can simply drag it in from the field list.
03:37:47.175 - 03:39:18.445, Speaker B: So, without further ado, I'll drag in our number of sightings measure just below its current equivalent in the matrix. And as you can see, the numbers line up exactly at every level of the matrix. Even if we drill down, everything still works. That's because the rules of filter context that we discussed earlier apply to measures. So at every address in our matrix, every intersection of every layer of row headers plus any column headers we may have added or filters we may have applied, defines the portion of our data over which our measure calculation is applied. So, at the risk of being repetitive, if we target a single number in our measure column, say this 31 for King County, Washington, and then go to our data view and apply the equivalent filters, which would be state equals Washington and then county equals King, you'll see at the bottom left that 31 filtered rows were returned. In other words, 31 is the count of the filtered rows.
03:39:18.445 - 03:40:48.905, Speaker B: So, behind the scenes, our measure is first letting the matrix visual whittle down the scope of our data to a certain set of rows for any given cell within that matrix, and only then applying its calculation just to those rows. So for this number 31 in our matrix, that's at the intersection of Washington State and King county, our measure calculation is being applied over just the 31 rows in our raw data for Washington State and King County. Now, thinking about this process in such a precise way may not seem terribly important at the moment, but it will need to become second nature once we start working with more advanced DAX functionality later on. So now is a good time to start developing the deeper understanding of how the calculations that drive our visuals actually work. All right, so having created our very first measure, in the next video, we'll explore some of the finer points of adding measures to your power BI visuals and customizing them to your needs. I'll see you then. Hey, welcome back.
03:40:48.905 - 03:41:51.835, Speaker B: So, moving right along with our example from the last video, one convenient feature of measures is that we can Define their formatting, that is how the values they produce will be displayed. If the measure calculation pertains to dollar amounts, we could specify a currency format, whereas for an average of numbers we might want a decimal format. You get the idea. So to specify the format of our measure and fine tune it in any other way we might want, let's first click it over in the field list to make sure it's selected. And now if you look up at the ribbon, you'll see that a new contextual tab has appeared called Measure Tools. This is where we can specify a format for our measure along with a number of other attributes. So here in the drop down list next to the format label, we see that power bi went with the default of whole number, which stands to reason since our measure in this case is a count of something and shouldn't need decimal level precision.
03:41:51.835 - 03:43:03.247, Speaker B: However, below that there is an option we should probably customize. If we hover over this little comma icon, it says display the values in this column with commas as a thousand separator. This formatting makes sense for our measure, considering it consists of counts that could possibly exceed 1,000. So I'll go ahead and click this comma and while the effect of our change isn't immediately obvious, since none of our individual state level siting totals are 1,000 or more, if we look down to the totals row, sure enough, that 3480 is formatted with a comma just like we specified. Now, before we move on to creating any other measures, I want to call out some of the other ways you can customize measures. On this Measure Tools tab, for example, you can change and customize the name of the measure. And if your file happens to have more than one table, you can also change the table that the measure is assigned to by selecting the table here in this drop down list.
03:43:03.247 - 03:43:36.165, Speaker B: Now, you're not seeing any options here because we again only have one table in our current report. You can also specify the data category of your measure, but that's something we'll get into a little bit later. For now though, our measure looks good. So while we're at it just for good measure. Hey, I thought it was funny. Let's whip up another measure real quick. This one mirroring the other calculation in our visual here, the percentage of Bigfoot sightings that occurred during a full moon.
03:43:36.165 - 03:44:57.053, Speaker B: If you recall, for this calculation, we averaged the 10 field we created that flagged sightings as having occurred during a full moon or not, and then formatted it as a percentage when we used it in our visualizations. Let's replicate all that work here so we never have to do it again when we want to use this calculation. So once again I'll create a new measure by clicking this new measure button on the ribbon. And then for the name of our measure, I'll just type percent Full Moon sightings. Then for our actual formula, I'll take the average of our Bigfoot sightings full Moon field and then hit enter to lock it in. And now that we see that our new percent Full Moon sightings measure has been added right underneath our existing number of sightings measure over in the field list, back up in the Measure Tools tab, I'll specify a format for this measure of percentage. And with that we now have two measures we can drag and drop into any of our visualizations.
03:44:57.053 - 03:45:38.405, Speaker B: No additional configuration required. So let's get to doing just that. So switching focus back to our matrix visual, now that we've clearly got our number of sightings measure working, I can go ahead and remove the original non measure version of that calculation. So I'll go ahead and just remove that. Then just to keep things looking nice, I'll add those data bars back in. So I'll click on the number of sightings measure in the visualizations pane, hover over conditional formatting, then data bars. And just like before when we added data bars, all the defaults here should be fine.
03:45:38.405 - 03:46:36.385, Speaker B: So I'll just click ok. So with that measure squared away, I'll repeat the process for our percentage full Moon sightings measure. First, I'll drag the measure into the values area of the visualizations pane right underneath our existing percent Full Moon sightings calculation. This will allow us to verify that the measure is doing the same thing as our existing calculation, which if we just scroll through our values here it looks like it is. Which means we should be good to remove the original calculation. Now note here that because we defined this percent Full Moon sightings measure with a format of percentage, that format carries through to the visual with no additional steps required. So with this visual taken care of, we can quickly demonstrate how convenient the portability of measures is by swapping one of our new measures into our other matrix visual.
03:46:36.385 - 03:47:43.125, Speaker B: We'll first hit the back to report button to go back to the main report view. And now back in this view, just to create a little more room on the canvas for us to actually see our visualizations, I'll click this X next to the formula bar to remove that from view. Now clicking our other matrix visual to select it and then taking it into focus mode. To see things a bit more clearly, I'll remove the existing calculation from the values area and then swap in our number of sightings measure. And there we have it. In just a few clicks we were able to add the same calculation with the same name and formatting to a second visual. And maybe the best part of this is if we decide at some point that we want to change some aspect of our measure, we only need to update it once and whatever change we made will cascade to all the visualizations we've used the measure in.
03:47:43.125 - 03:49:04.551, Speaker B: Say for example, that we want to change the name of our number of sightings measure to Total Sightings. To do that, I can just select the measure in the field list and then up in here in this text box I'll just type in Total Sightings and then hit Enter. And while the updated name of our measure isn't reflected in this matrix, because this matrix is grouped both by row headers and by column headers, if we go back to the report canvas and then again close this formula bar so we can see what we're doing and then look at our other matrix, sure enough, in this first column the heading is Total Sightings instead of number of sightings. So the name changed in this visual. Without us needing to do anything specific to this visual, we simply change the definition of the measure and then any visuals that used that measure were updated accordingly. Now, given how awesome measures clearly are, you might be wondering when would you ever opt for the built in summarizations we've been using up to this point over measures? And the answer is basically never. Confession.
03:49:04.551 - 03:49:55.135, Speaker B: I use those built in summarizations to ramp you up to a certain comfort level with using calculations in your visuals. But with even a basic understanding of measures, something you now have, there's no good reason not to use them unless you're just slapping together some really quick and dirty ad hoc analysis that you're confident you won't need to revisit in the future. So in summation, it's all measures all the time from here on out. And we'll have plenty of opportunities to practice our measuring skills very soon. Because a visualization is only as meaningful as the calculation behind it. And in the next section, we're going to be building lots of visualizations. Specifically, we'll be expanding our horizons beyond simple matrix visuals and into the wild and wonderful world of charts.
03:49:55.135 - 03:51:26.403, Speaker B: Those splashy eye catching visualizations that you see in executive boardroom presentations, science symposiums and and of course, you know, Bigfoot tracker command centers Speaking of which, a Bigfoot sightings dashboard sounds like a pretty great goal to work towards, so we'll take our first step in that direction coming up next. I'll see you then. Okay, so it's exercise solution time once again. And for the first exercise we want to create a measure that returns the number of sightings in our UFO sightings data set. So to do that here on the home tab of the ribbon, I'm going to hit the new measure button. And now here in our formula bar, I'll first specify the name of this measure, which will just be number of sightings. And then our actual formula is going to be super simple.
03:51:26.403 - 03:52:35.773, Speaker B: I'll use the count formula to simply count the index field in our UFO sightings data set, which I'll arrow down to and then hit tab to select. And then I'll close our parenthesis and hit enter. And now that that's been created, I'll make a slight formatting tweak by adding a comma here as a thousand separator. And now that we've created the measure and formatted it, I'll close our formula bar here to make a little bit more room to see our visuals. And now to do something useful with our new measure, I'll actually use it to replace the existing calculation in our very first matrix visual where we're breaking out count of UFO sightings shape and sighting duration category. So I'll click that matrix to make sure it's selected and then I'll go ahead and delete the field we currently have in the values area. And now looking over to the fields list, we see one of our fields here has a calculator icon in front of it.
03:52:35.773 - 03:53:23.615, Speaker B: And sure enough, that's our number of sightings measure. So I'll grab that and drag it into the values area. And it looks like that mostly worked, except our sorting and our data bars are gone, which is because both of those things were based on the field that we just deleted from our matrix visual. So first I'll reapply that sorting to the total column. And then to reapply data bars, I'll click our number of sightings measure in the visualizations pane, not the fields pane. Hover over conditional formatting, then data bars and then just stick with the defaults just like we did when we applied data bars to a built in calculation a few videos ago. And there they are.
03:53:23.615 - 03:54:35.455, Speaker B: Now up next, I want to create a second measure called average siting duration minutes, which as you might have guessed, will simply be an average of our minutes observed field. So again I'll hit the new measure button and then in the formula bar I'll name this measure average siting duration minutes. And then for the formula I'll use a simple average. And then for the single field I'll feed the average function as its argument. I'll of course go with minutes observed. So I'll arrow down to select that and then tab to lock it in, close my parenthesis and hit enter. And now I'll format this as a decimal number with one digit after the decimal and then close our formula bar so we can see all of our report canvas.
03:54:35.455 - 03:55:46.425, Speaker B: And just like before, what's the point of creating a measure unless you're using it in one of your visuals? So in our bottom left visual here, that's breaking out average sighting duration by country and city. I'll select that and then remove the summarization we have there currently and replace it with our new measure. And it looks like it's working exactly like before. Now. Finally, I want to create a third measure called percent hour plus sightings that will return simply the percentage of sightings that were an hour or more in duration. So again I'll hit the new measure button and name this 1% hour plus sightings. And then for our formula recall that you can get to a percentage by taking an average of a 10 field, much like the 10 hour plus duration flag we created a few videos back.
03:55:46.425 - 03:57:01.029, Speaker B: So first I'll use that average function and then I'll feed that our hour plus duration flag and then hit enter and then format this as a percent with one digit after the decimal. Then I'll close out our formula bar. And to put our measure to good use, I'll use it, as you might have guessed at this point, to replace the built in summarization we added to our third matrix up here at the top right of the canvas. So I'll first get rid of that and then drag in our new percent hour plus sightings measure. And this appears to be working, except that as we scroll down, it looks like our conditional formatting that conditionally formats any values greater than or equal to 10% is no longer there. And again, this is for the same reason that the data bars disappeared from our first matrix visual. That's that the field we were using as the basis for this conditional formatting is no longer even the matrix.
03:57:01.029 - 03:57:38.865, Speaker B: We replaced it with something brand new, which is the measure. So we'll need to reapply that conditional formatting, which of course is pretty straightforward. So I'll Click our measure in the visualizations pane and then hover over conditional formatting and then font color. And then for our format style, I'll go with rules. And then I'll say if value is greater than or equal to 0.1 as a number and and less than max. Then we'll go with a red font.
03:57:38.865 - 03:58:20.835, Speaker B: And now clicking OK and then scrolling down through our visual. Sure enough, 11% is formatted in red, as is 11.5, 11.1, and so on and so forth. Looking good. So onto the next one. Hey, welcome back.
03:58:20.835 - 03:59:17.275, Speaker B: So just by adding a little conditional formatting to one of our matrix visualizations, we saw how incorporating a few visual effects can help provide insight into our data by drawing attention to noteworthy values in that data. But sometimes individual values, even if they're spruced up with some conditional formatting, aren't the best mechanism for conveying insights or understanding. Computers are great at processing large tables of numbers quickly, but human beings aren't computers. And sometimes the meaning of our data gets lost amid a forest of individual data points. You know the saying a picture is worth a thousand words? Well, sometimes a picture can also be worth a thousand data points. And that's where charts come in. Charts are basically visual representations of data that are separate from the data itself.
03:59:17.275 - 04:00:16.905, Speaker B: A good chart should tell a story about the data it's based on that provides summary and insight that a human being can easily grasp and use to make decisions. And Power BI happens to have extremely powerful charting capabilities, with a wide variety of charts that we can use to visually describe our data. One of the most straightforward yet useful of these, and one you've almost certainly seen many times before, is a column chart. So a column chart is useful for breaking down values or calculations in our data by different categories, allowing us to easily compare those categories based on the relative size of vertical bars, AKA columns. But when it comes to charts, showing always beats telling. So let's dive right into creating our first column chart. So to give ourselves plenty of room to explore, we'll create a brand new page for our column chart to live on.
04:00:16.905 - 04:01:26.285, Speaker B: Now, if you've used a spreadsheet program like Microsoft Excel or Google Sheets, you can think of a page in Power BI as pretty much the same thing as a tab. It's basically its own self contained little universe, a blank slate where we can create a whole new visualization or group of visualizations from the data queries available to us in our Power BI file. Right now we just have one page, which is fine because we've only Been playing around with a couple of visualizations, but if we want to create a new one, the process is super easy. We just click this plus sign down here and poof, we have a new blank page waiting for us to add cool new visualizations to it. Now, before we move on, we've simply got to do something about the names of our two pages here. So let's rename our first page to Matrix Visuals by just double clicking the page name here and then typing Matrix Visuals. And then I'll do the same for our second page, which I'll call column charts.
04:01:26.285 - 04:02:32.065, Speaker B: Now, over in the visualizations pane, I'll hover over the various icons until I see a tooltip for stacked column chart. So I'll go ahead and click that. And now we see that Power bi has given us an empty sort of ghost chart with the vertical bars letting us know that this is, in fact, a column chart. Now, to get started fleshing this out, our first step should be to figure out what calculation exactly we ultimately want to break out by different categories. The obvious choice is total sightings, which has kind of been our go to so far. But where exactly should this go down? Here on the visualizations pane, These options we see for dragging and dropping our data, X axis, Y axis legend, and so on are quite a bit different from the ones we had for matrix visuals. Well, first we need to understand the difference between the X and Y axes.
04:02:32.065 - 04:03:19.265, Speaker B: Basically, the X axis is a horizontal axis that runs along the bottom of the chart, while the Y axis is a vertical axis. Now, we've established that column charts use vertical bars to compare numbers across categories. So it makes sense that our value, the number of Bigfoot sightings, would be placed in the Y axis section. So let's go ahead and drag our total sightings measure into that Y axis area. And then we'll take our visualization into focus mode so I don't have to break out my X tree strength glasses. So, now, looking at our chart, we see a single vertical bar. Not terribly compelling or useful.
04:03:19.265 - 04:04:38.435, Speaker B: But if we hover over the bar, we see that it represents the count of all the Bigfoot sightings in the data set. But this is only because we haven't added any categorical fields to our chart to break this number out by different, well, categories. So let's drag in a categorical field, say the latitude category field we created in Power Query earlier on, into the X axis area. Finally, a chart that actually looks like a chart. So by plotting a categorical field on the X axis, each of the distinct values or categories in that field get their own vertical bar, with the height of each bar being proportionate to that respective category share of the value we're plotting on the y axis. Basically, that single bar that encompassed all of our sightings has been broken up into four smaller bars, each of which represent the number of sightings In a single latitude range. Which makes it super easy to compare the number of bigfoot sightings in different latitude ranges at a glance and also spot any obvious outliers or unusual, unexpected patterns.
04:04:38.435 - 04:05:28.025, Speaker B: For example, this breakout makes it very clear that the vast majority of our bigfoot sightings Occur in the central and north latitude categories, with very few in the south and almost none in the far north. So this is a big step up from a single bar. But what if we want to add Even more layers to our analysis? Well, that's what the legend is for. And no, I'm not talking about a legend like the legend of zelda. I mean, a chart legend. So what the heck is that? Basically, it's a visual reference that helps users identify what the various elements on the chart represent. We haven't needed one on our chart thus far because we're only breaking out our total sightings measure by a single category.
04:05:28.025 - 04:06:32.355, Speaker B: But if we want to break down bigfoot sightings by another category, let's say the season of the year, you know, spring, winter, fall, summer, we'll need to bring a legend into play. So let's go ahead and drag our season field, if I can find it over here in the field list, into the legend area on the visualizations pane. Whoa. So there is suddenly a lot more going on in our chart. What's happening here is each of our bars, again, Representing the number of bigfoot sightings In a particular latitude range, Is itself being broken out into little color coded chunks, each of which represents the proportion of those sightings that occurred in a particular season. And looking up to the top of the chart, we'll see our legend, which provides us with a reference that we can use to interpret what those color coded chunks actually mean. For example, we see from the legend that orange represents summer.
04:06:32.355 - 04:07:22.759, Speaker B: And with that in mind, we can quickly scan our chart and get an idea of what proportion of sightings occurred in the summertime per latitude range. Now, as you might imagine, you'll want to make sure that the fields you drag into the legend area don't have too many unique values, not because of any technical limitations, but rather a human being's ability, or lack thereof, to make sense of a color Coded reference consisting of more than a small handful of items. So if we try to drop a categorical field with a lot of unique values, like, say, state, into our legendary instead of season. So I'll go ahead and remove season here and. And swap in state. While everything still technically works, this chart is now well nigh unreadable. So let's go ahead and reverse that.
04:07:22.759 - 04:07:52.675, Speaker B: So I'll remove state and replace it with season. And now we're back to our nice little breakout. All right, so now that we've created one kind of chart, spoiler alert. There are a lot of different charts to choose from in power bi. Up next, we'll explore an alternative to stacked column charts and how. Not to mention why we might choose one type versus the other. I'll see you then.
04:07:52.675 - 04:09:12.135, Speaker B: Welcome back. So, before we move on to exploring entirely different categories of charts, I want to show you another spin on column charts. So, right now, we're using a stacked column chart, so named because any field we drag into the legend area will be broken out into these little color coded bars that appear to be stacked on top of one another. But it turns out there's another flavor of column chart that can work just as well for breaking down a calculation by multiple categories in a single chart. And that's the clustered column chart. To see what one of these looks like, let's convert our stacked column chart here to a clustered column chart, which we can do with a single click of a button. So let's hover over our chart icons over here in the visualizations pane until we see a tooltip that says clustered column chart, and then just click that.
04:09:12.135 - 04:10:35.885, Speaker B: And just like that, our chart has been transformed. As you can see, Power bi makes it super easy to toggle among different types of visualizations, especially when those visualizations have a similar structure and similar components as the different varieties of column charts obviously do. So changing the chart type was easy, but what really changed? Well, instead of the seasons being broken out within the individual bars, each season now gets its own bar. But these bars representing individual seasons are clustered together, hence the name of the chart based on their association with a particular latitude category, our x axis field. So in this formulation, each unique item from that x axis field gets its own cluster, and the bars within that cluster visualize the distribution of Bigfoot sightings among the different seasons for a given latitude category. Right away, we get a sense of scale and proportionality, not only among the different latitude categories, but also among the seasons within each latitude category. Now, technically, this view isn't showing us anything the stacked column chart didn't.
04:10:35.885 - 04:11:51.595, Speaker B: So why would we bother with exploring both variations? Well, this is a question you'll often find yourself asking when trying to decide which among Power BI's many charts you should use to visualize your data. Quite often, there are multiple chart types that can adequately convey a similar story about your data. Choosing the right chart, then, is more art than science. But with that said, I'll try to provide you with some guidelines for which charts work best. For what purposes as we continue exploring Power BI's vast arsenal of charts. So as to the use cases for the different types of column charts, you'll generally want to go with a stacked column chart when your focus is on visualizing the individual items within your legend field as parts of a whole, and also when it's important to have a clear comparison and scale between the categories in your main X axis field. So, switching back to that stacked column chart, you see that the color bands in each bar all clearly add up to a whole, giving you an intuitive sense of how our latitude category field is broken out by season.
04:11:51.595 - 04:12:58.365, Speaker B: Also, it's easy to compare the sightings in one latitude category to another, since each latitude category gets its own bar. Now contrast that with the clustered column chart, where instead of a single bar for each latitude category, we get a cluster of smaller bars. This makes it a little harder to compare the number of sightings between different latitude categories the field in our X axis, but also makes it easier to compare the sightings by season our legend field within each cluster. Since the bars for each season are starting from the same baseline, the bottom of the chart, as opposed to being stacked on top of each other as we'd see in a stacked column chart. It's pretty straightforward to make educated comparisons among sightings in different seasons with this layout. Now, these are good guidelines to keep in mind, but remember that they are just that, guidelines. In my experience, there is no substitute for simply sampling a number of different charts to tell the story you're trying to convey.
04:12:58.365 - 04:13:15.525, Speaker B: Often it's a case of just knowing what works when you see it. Okay, in the next video, we're finally going to move past column charts and learn another way to visualize categorical data Bar charts. See you then.
04:13:34.265 - 04:14:18.135, Speaker A: Congrats, you made it through the Power BI crash course. But before you get too comfortable, remember that there's still over 16 hours of content waiting for you in Travis complete Power BI Boot Camp course. Not only will you get to finish your UFO Sightings dashboard, but you'll also get to work on more advanced projects and even master skills like DAX programming, power query and data modeling. If you're serious about leveling up your skills and becoming a top tier business Intelligence analyst, then don't stop here. Click the link in the description or on your screen to continue learning with Travis in Zero to Mastery. Oh, and if you enjoyed this crash course, please drop it a like and consider subscribing for more content just like this. That's it for me today and I look forward to seeing you inside the Zero to Mastery Academy.
