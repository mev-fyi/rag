00:00:00.200 - 00:00:55.460, Speaker A: All right, welcome, everyone, to the validator community call. Hello, hello. February 2, 2023. And our agenda here. So, first of all, we're going to talk a little bit about the identity transition demo that pumpkin pool put together. I'm going to intentionally talk about the validator panic after that because we should have some engineers from labs joining just a little bit after the start of this call so they can give a little bit more technical detail to what actually happened with that panic, kind of why it happened in the fix that was proposed. And then we'll talk a little bit about mitigations around things that hopefully labs and foundation could do better to avoid or hopefully catch more issues like this, and then general validator updates and Q and a at the end of the day.
00:00:55.460 - 00:01:32.254, Speaker A: All right, so let's get started. Yeah, first, I wanted to highlight this. So Michael Vines wrote a GitHub, gist or whatever for this identity transition, and I think it's been sort of polished, and there's a video added, and there's a little bit more detail that maybe makes a little easier for operators to work with. Um, so, yeah, I wanted to kick it off to. I don't actually know your name, but pumpkin pool to talk about it.
00:01:32.714 - 00:01:34.330, Speaker B: Yeah, it's. It's Blake.
00:01:34.442 - 00:01:35.386, Speaker A: Blake, nice to meet you.
00:01:35.410 - 00:01:36.778, Speaker B: Sorry, no.
00:01:36.826 - 00:01:39.854, Speaker A: Recognize your. Your handle? Uh, yeah, go ahead.
00:01:41.434 - 00:01:44.654, Speaker B: Yeah. So, uh, let me open this side, actually.
00:01:47.254 - 00:01:47.590, Speaker A: Yeah.
00:01:47.622 - 00:02:52.216, Speaker B: So, like you said, michael Vines had put together a. It was like a GitHub repo that basically showed how to do this, only it was a little abstract in the way that it was all on one machine, and it was kind of a not production scenario. So it was a little hard to follow, in my opinion. But it was obviously really good because it allowed us to understand how we could transition between validators. But basically, I just distilled that into a much simpler format with two shell scripts, dumbed it down, and there's, I'm sure, an infinite amount of ways you can do this with ansible, and you could probably just make it one shell script, but this is just the way that I happen to do it. And, um, people are, you know, certainly free to modify that and do whatever they, they please, but hopefully it gives. A lot of people were asking kind of how it's done.
00:02:52.216 - 00:03:08.060, Speaker B: Is it reliable? Um, that kind of thing. So I just wanted to share how I've been doing it. Um, put the cat on screen. Where's that cat? That cat is here somewhere. I think she's probably sleeping, but, uh, she's definitely here.
00:03:08.132 - 00:03:08.380, Speaker A: Yeah.
00:03:08.412 - 00:03:38.094, Speaker B: Anyway, I think the video. I added the video because I wanted people to see just how, how quick and easy it really is. Because I know when you read the, especially the Mvines GitHub page, there's a lot there. It seems confusing. I just wanted to distill that down to what really is going on, which is just a few commands. And you can see in the video it takes like 5 seconds to transition. So it's not complicated at all.
00:03:38.094 - 00:04:04.024, Speaker B: You just need obviously two validator nodes that aren't delinquent running. You need identities on both that you can swap to that are not staked, that basically placeholder identities for them. They're not actively serving as your active validator. And then you just need to update the scripts. So there's really not too much to it.
00:04:04.524 - 00:04:32.844, Speaker A: Yeah. For anybody who hasn't really looked through this yet, it's a couple paragraphs of text here. It's not really too much to read through. And the gist of it, like he's saying, is basically a symbolic link in an SCP and a couple commands after that. So really easy to play around with and test out maybe on Testnet if you're not sure about doing it on mainnet yet. Yeah. Any questions? Yeah, Zen Tetsu, go ahead.
00:04:35.024 - 00:05:05.544, Speaker C: Hi. So I know that, is this the same technique that Michael Hubbard was using like a year ago to do validator swaps? Hot swaps, because I know that at that time there was some suggestion that it might have caused some problems. And if that was true, then is the same technique being used now and has that problem gone away or was that never really a problem, just confusion or what?
00:05:06.724 - 00:05:47.176, Speaker A: Yeah. So I don't know if the problem he's having was the same. He said it is the same method in the chat there. There was never any proof that it caused issues other than the correlation of loss of vote credits with the use of the script. Yeah. So for those who don't know, Michael had an issue about six months ago during a transition from one identity to another, and it wasn't really clear whether or not it was the transition or something else, but there was lack of votes landing. So, yeah, I don't know.
00:05:47.176 - 00:06:02.728, Speaker A: I think it's something you experiment with and try on your own. But I will add that, like, I don't think there was an issue ever found. Maybe there was something completely unrelated that caused the issue. Worth experimenting, I think.
00:06:02.856 - 00:06:32.216, Speaker C: Yeah, there was also overclock. I think they also mentioned having some weird issues, but they were at the same time moving to Japan and moving to vegito based code base. So I don't know if there's a lot of sort of, like, uncertainty around this technique and whether or not it can cause problems. However, if, you know, Blake is saying that he's used this repeatedly and it isn't causing problems, and if Michael Hubbard is using it, it's not causing problems, then maybe that was all just coincidence or something.
00:06:32.400 - 00:06:49.212, Speaker A: Yeah. Another thing to be certain of is your clock on both machines. You want to make sure it's NTP is set up correctly. I had that issue during the testnet restart. Right. The root cause essentially was that my clock was a little behind. It was causing all sorts of issue with gossip.
00:06:49.212 - 00:06:55.260, Speaker A: So maybe something that Michael ran into, maybe not, I don't know, but something to check.
00:06:55.372 - 00:07:24.870, Speaker C: And does your validator, I know that when we, like, you see those logging messages that go through the logs that show the big table of all the validators and what IP addresses are. When you do this, does that switch? Does it, like, fairly quickly switch, does the table, is that what's happening? Is that your validator is now known by a different IP address in some sense? And if that's true, is there ever any confusion about where the packets go or where they come from or, you know, I don't know. I just, I'm wondering about that facet of it.
00:07:25.022 - 00:07:39.362, Speaker A: Yeah. I believe it should be identified differently on gossip when you switch. So if that's not happening, I think that would be the issue. But again, I'm not positive what the root cause was. Yeah, there was a. Oh, sorry, go ahead. Well, I was just gonna say, I think.
00:07:39.458 - 00:08:03.534, Speaker B: Cause Michael Vines mentioned something about that, because I was concerned about how you were hitting the duplicate instance check. Cause I've actually triggered that a couple times. The only times I've had issues, though, is when the validator already had a problem. Like, if. If you get paged in the middle of the night and the validator is like, 500, 600 slots behind, and then I try to transition.
00:08:03.974 - 00:08:04.814, Speaker A: It's.
00:08:04.894 - 00:08:10.514, Speaker B: I always run into issues, like. But they're, when they're both running perfectly, I've never had an issue transitioning.
00:08:12.614 - 00:08:13.038, Speaker A: Hmm.
00:08:13.086 - 00:08:17.502, Speaker C: That's troubling, because I think one of the reasons that I'd want to transition is if a validator was having a problem.
00:08:17.598 - 00:08:47.164, Speaker B: I know. Yeah, exactly. Like, so what Michael has in his, Michael Vynes has in his sash script is like a, he has, like, aff flip flag, which actually kills the validator process completely. So I think that's what I was running into. I think if the validator is already having an issue. I wouldn't even try to use this method. I'd probably just kill it and then just go to the other hot spare and just set identity.
00:08:47.164 - 00:09:02.224, Speaker B: I think it's probably the method because if you try and transition off an already messed up validator, it's probably just going to clash. In my experience. I wouldn't try it. I've tried it many times since, it's never gone well.
00:09:04.444 - 00:09:08.708, Speaker A: There was a hand raise there by Falco. Also, did you have a question?
00:09:08.836 - 00:09:20.504, Speaker C: Yeah, another quick question. What's the main difference between this setup and the payload setup? Is it purely the door file or is there anything else.
00:09:24.154 - 00:09:26.734, Speaker B: Are you saying between this and the ETCD?
00:09:27.634 - 00:09:28.774, Speaker C: Yeah, exactly.
00:09:29.314 - 00:09:50.674, Speaker B: Yeah, there's definitely a huge difference. Like the ETCD has a ton of overhead. You need a lot more servers. This is like the cheapest, easiest way that you can possibly have a failover. You're literally just setting identity. There's no extra hardware, there's no extra software involved, nothing.
00:09:52.134 - 00:09:52.518, Speaker A: Right.
00:09:52.566 - 00:09:56.074, Speaker C: And which one is to recommend it option, if there's any?
00:09:59.894 - 00:10:01.474, Speaker B: I can't answer that question.
00:10:02.174 - 00:10:04.422, Speaker A: I didn't quite catch that. Which one is.
00:10:04.478 - 00:10:07.994, Speaker B: Which one is recommended? I don't know that either of them are recommended.
00:10:08.574 - 00:10:14.714, Speaker A: Yeah, I wouldn't say there's a best practice for either. I think it's more of a, like, explore on your own what works best for you.
00:10:17.224 - 00:10:59.380, Speaker C: Yeah, this is a good topic. I actually spent hours today trying to design out if there's any way to have a separate process run on a validator node that reads the tower file, keeps track of what votes the validator believes it's casting, watches. RPC sees what votes are actually landing and makes a local decision about whether the validator is actually functioning properly and then shuts it down if it's not, and sets identity to, you know, a hot backup, sort of. So this is the exactly part of the technique I would try to use. But it's a tough problem because there are many edge cases where you don't know, you don't want both to try to run at the same time. Uh, so if both are independently trying to make decisions about when they should go, it. It gets really hairy.
00:10:59.380 - 00:11:22.944, Speaker C: Um, I'm going to keep working on it though, because I think it'd be really cool to have. I'd like to be able to know that if I was somehow incapacitated and something happened, it could sort of hot failover. So that's the holy grail, being manual. Also, manual hot fails are really or switchovers are really valuable too, because during software upgrades you don't have to lose vote credits because you can quickly transition from one to the other. And that's valuable also.
00:11:24.324 - 00:11:25.544, Speaker A: Anish question.
00:11:30.804 - 00:12:09.336, Speaker D: Sorry, I was speaking. So we did the failover using this step, and the problem is like, our failover validator is not going to vote. Okay. So on the primary side, we are going to update that step to a latest version. And again we have to move back, like back and forth of tower configuration changes and all. So in this demo, I can see that there is only one side of transition is happening from primary means, like primary become secondary and secondary is becoming primary. Okay.
00:12:09.336 - 00:12:25.564, Speaker D: But again we have to do a reverse clock also, okay. To again started the start, to start the voting setup. Okay, so, and means like in this process, have you ever encountered like your tower file got corrupted, something like that?
00:12:28.304 - 00:12:48.234, Speaker B: I have never encountered that now. Yeah, you're right. It's only one side. And usually I'll transition over, update everything, maybe reboot, you know, give the validator a kernel update or whatever, and then. Yeah, transition back. Yeah, I've never, I've never encountered like a corrupted tower file or anything like that.
00:12:50.774 - 00:13:17.874, Speaker D: So what will happen? Like if, let's say we switch the identity to prime. Like I'm coming from the ETCD scenario, okay, because we tested eTCD. So in ETCD, when secondary become primary, okay, and when primary restart, it reclaim the identity, okay, it is going to be same, like when, when we upgrade the primary, it is going to reclaim its identity.
00:13:22.314 - 00:13:24.534, Speaker B: I'm sorry, what was the, what was the question there?
00:13:25.754 - 00:14:04.714, Speaker D: So in scenario, what is happening now is like when we upgrade primary, okay, and we already did a failover, okay? And during the restart, when HCD found out, okay, the primary is again active, okay, it just do a switch by its own, okay. So in this scenario, if we do a switch, okay, let's say we move to our file from one to, from a to b, okay? And then when we restart a, okay, whether same thing will happen or like identity will be reclaimed by a or.
00:14:05.374 - 00:14:40.864, Speaker B: No, no, it won't be because of that. Symlink, when you, when you swap here, you're setting the identity to a junk identity, basically. I mean, it's not really junk, it's just a non staked identity that has no soul in the account. And then you're setting the symlink in the script to reference that unstaked identity so you could restart it 800 times. It would never claim your main identity unless you manually set that symlink back to the staked identity and then did the set identity command.
00:14:41.744 - 00:14:42.964, Speaker D: Okay. Okay.
00:14:44.784 - 00:14:45.644, Speaker A: Okay.
00:14:46.144 - 00:14:46.520, Speaker C: Yeah.
00:14:46.552 - 00:15:03.884, Speaker D: Thanks. Thanks. I think we have to test this stuff because we have to move multiple part here from a to b, from b to a, from a to b, and b to a. That's the worry part. And that's where ATCD is doing that for us by default.
00:15:05.024 - 00:15:32.024, Speaker B: Yeah, eTCD is kind of nice like that. This is definitely a dumber method. I mean, it's very manual, but it just depends. I mean, if you're like many of the older validators who've been around for a while that they don't even have hot spares at all, they just, when they upgrade, they restart and they just accept like, you know, 30 minutes of downtime and that's not even really an issue for them. So it's crazy how things have developed in that respect.
00:15:33.684 - 00:15:34.372, Speaker D: Yeah, thanks.
00:15:34.428 - 00:15:37.776, Speaker A: Thanks. Thanks.
00:15:37.920 - 00:16:23.316, Speaker C: This is definitely an area where we need more effort put into making. I mean, this is a great step. Having this kind of documentation is a great step, but all the things we can do to have automated ways for this to happen and really clearly well understood documentation about exactly what goes on with the scenes, what are the dangers and whether or not it really does fail. If the validator is unhealthy and what that means, this is all stuff that we can continue to improve and should because it's definitely like the leading edge of what validators really want to be doing. And it's one place that we have kind of a glaring hole. There's no really, you know, comprehensive, there's manual ways to do this, but there's no automated way to failover using this. You'd have to wrap it.
00:16:23.380 - 00:16:23.628, Speaker E: Right?
00:16:23.676 - 00:16:26.624, Speaker C: Am I right, Blake? This doesn't happen automatically. You have to do this manually.
00:16:28.004 - 00:16:30.784, Speaker B: Yeah, it's a completely manual process.
00:16:33.624 - 00:17:24.670, Speaker C: So the blocking improvement was in some ways more automated, which I think is what Anish was sort of talking about. Some aspects to it are more automated. So. Yeah, and I've talked to a friend about this. He's kind of familiar with how ETCD works, kind of works in that space in the software world. He said that the technique that's being used with the way ETCD was being used for Solana, where it's gating every single vote, he said maybe that does lead to latencies that I think people found unacceptable, that maybe just having, you know, there's other ways to do it where like once every five minutes you have like a checkpoint or something like that. And instead of doing it on every vote or something, and it then becomes the case that if there's a problem, you may, you know, have the failed validator not really be voting for, you know, a minute or two until the next checkpoint and the switch over, but at least it doesn't gate you on every single vote.
00:17:24.670 - 00:17:38.164, Speaker C: So there may be ways to improve the ETCD process to make it more performant. Also, I guess, is what I'm getting at at the cost of not having it be absolutely perfect in that it never misses votes. Like maybe it sometimes would have a little bit of downtime while switching over or something.
00:17:38.584 - 00:17:39.764, Speaker D: Got it. Got it.
00:17:43.224 - 00:18:20.834, Speaker A: All right, great discussion, and thank you, Blake, for talking about this and putting together the docs. I want to transition now, unless there's. Yeah, it seems like there's some good chat about what to do in the case of a validator falling behind your transitioning your validator to the spare. But yeah, I want to transition and talk about the panic on Mainnet. So, as most of you know, I'm sure on February 4 or, sorry, February 1, there was a panic for all 1.14 validators on Mainnet. So about 10% of the stake at the time.
00:18:20.834 - 00:18:55.790, Speaker A: And labs announced to revert back to 113.6. The details about this, I think Steve is on the call now. Maybe be better to talk about and answer questions. Steve, are you there? Yep, he's there. Maybe not. Well, okay. So I know.
00:18:55.902 - 00:18:56.878, Speaker E: Hey, can you hear me now?
00:18:56.966 - 00:18:57.742, Speaker A: Yeah, I can hear you now.
00:18:57.798 - 00:19:02.430, Speaker E: Okay, I'm on my phone. I had to leave the meeting to enable permissions for Mike. Sorry.
00:19:02.582 - 00:19:03.374, Speaker A: Yeah, no problem.
00:19:03.494 - 00:19:34.724, Speaker E: Yeah, so, yeah, I missed a couple seconds there, but any case. Yeah, so there's a couple items at play. So the panic that was initially reported, I think it was lane, if I'm not mistaken. So that was kind of a symptom of another issue. So the panic was something like the progress map, expecting a slot to be there. That the reason that happened is because we purged a slot that we had already frozen. So we replayed a bank.
00:19:34.724 - 00:20:15.524, Speaker E: We saw that the 1.14 knows we're not agreeing with the cluster, so they said we need to purge those slots. So in purging those, then something there was transient state, and that had a panic. But the actual issue is the fact that we diverged initially. So there's kind of two issues. I think we're maybe looking to tighten up that panic and do either better error reporting or better fault tolerance. But the panic or the divergence, rather, that one's called out in GitHub, issue 3060, there's actually a pretty good write up.
00:20:15.524 - 00:20:39.586, Speaker E: Pretty easy to follow along, but more or less. There's a feature in 1.13 that's just hard coded to be false, like not enabled. And it was. We could actually. It was like a compute budget instruction and that was allowable in 1.14. So we eventually somebody submitted a transaction.
00:20:39.586 - 00:21:05.222, Speaker E: Santetsu is actually the leader, funny enough, and that caused the divergent behavior and saw those nodes forked off. So, yeah, I think that's probably all really have to say. Like I said, the write up is pretty good. There's lots of details. Tao is probably the most knowledgeable person on this, so there's. Yeah, plenty of details there, but. Yeah, sucks.
00:21:05.222 - 00:21:23.030, Speaker E: So sorry to everybody that upgraded and then down, had to downgrade, but we are trying to minimize those. I see some hands raised can, but I'm on my mobile. Can somebody else read the question out loud? Sorry.
00:21:23.062 - 00:21:24.234, Speaker A: Zen tetsu, go ahead.
00:21:25.014 - 00:22:07.136, Speaker C: Yeah, I mean, bugs happen. So in as much as it sucks that we had to go back and forth a little bit on those releases, it's so much better that it got caught before it caused the cluster crash. So I'll take this kind of problem over a cluster crash any day and twice on Sundays. However, I am wondering if. Let's just say that the code in the progress map or whatever didn't cause a panic, and instead the purging of the slot because there was a discrepancy between 114 and 113 happened in. It didn't cause a panic. What would have happened? Is it that 114 nodes never would have been able to vote again because they wouldn't have had a certain bank that they could never get or something because only 113 nodes could produce.
00:22:07.136 - 00:22:08.976, Speaker C: I don't. Can you explain what would have happened?
00:22:09.080 - 00:22:56.264, Speaker E: Yeah, so this actually. So we have some recent, in my mind, their improvements, but like a couple versions ago, what would happen is your node would enter a loop where it would, you know, it have calculate the bank hash, it'd see that it differed from the cluster, it would purge the slot, repair the slot, replay, and then it get the same answer again, you know, the same wrong hash. And so get into an infinite loop of doing this. And it actually used to out of memory, but we've made the stack better, so it doesn't out of memory in the situation anymore. But, yeah, you're essentially in an unrecoverable state there. You know, you're never going to get the. You know, there's determinism there that you're, you know, you're computing the wrong, the wrong hash.
00:22:56.264 - 00:23:17.324, Speaker E: So we actually have in 1.15 we detect this, and after, like, ten iterations of doing this, we explicitly panic and say, like, hey, we just computed the wrong hash ten times. Something's wrong. We're, you know, we're bailing out. The idea there is to fail faster and try to raise a panic up quicker.
00:23:18.064 - 00:23:32.744, Speaker C: So could it wait until some slot after that slot is rooted and then just assume, okay, the cluster routed this slot. After that, I can resume from there. Like, I can just assume that the state's good if I don't have to get a good hash for something prior to that.
00:23:32.824 - 00:23:33.804, Speaker D: Does that make sense?
00:23:34.424 - 00:24:27.914, Speaker E: Kind of. But, like, you, like, in order, like, you're to, like, you wouldn't have the correct state on disk. So let's say if it's slot x that you diverged, you can't necessarily skip past two slots, x plus five, because you need the changes that happen in slot x for your accounts db state to be consistent with the rest of the cluster. So, I mean, really, the way past this would be like, if you, like, if you couldn't downgrade is like you need a snapshot from somebody else past the divergent slot where, you know, you essentially, you know, because at that point you're deserializing somebody else's bank and starting from there. But it's like a pretty hard failure. There's like, not, you know, it's your node's computing the wrong hash and it's going to keep computing the wrong hash. So you're kind of stuck.
00:24:29.534 - 00:24:29.950, Speaker A: Okay.
00:24:29.982 - 00:25:08.504, Speaker C: And I don't want to monopolize this, but one more quick question. I think this is the second panic that was caused by divergence in transaction handling between 113 and 114. Is there any focus around trying to identify any other places this could have happened so that we can proactively? Because I know that there's probably a lot of changes between 113 and point 14. It may be difficult to say what the entire sum total of all possible instances of this could be. But is there any proactive work to, to identify if any of the other changes, to compute any of this stuff could happen again?
00:25:09.084 - 00:25:41.436, Speaker E: Yeah, it's a good question. So, I mean, in general, the audits attempt to catch these. So, like, I don't know if you follow along, I think it's probably in the release engineering rolling or DevOps, at least one of those two. You'll see people like Michael Vines or others probably will talking about audits. So if you go back to, like early January, there's chatter like, you know, we're waiting for the audits. They should come back in whatever. How many weeks? So we use a couple auditors there.
00:25:41.436 - 00:26:13.434, Speaker E: It's like, you know, the hope there like that, that's one step. You know, the hope is that it's a, it's a third party, you know, and they're reviewing these things and catching potential issues in terms of other things we're doing. Like, we run nodes on mainnet that are like versions ahead. So like, we've had 1.14 nodes running against Mainnet for a while. We have master nodes which would technically be like, what, it's now 115. So we are always running some nodes ahead.
00:26:13.434 - 00:26:45.300, Speaker E: I think one area that we're deficient right now is stake node. There's a different code path for being a validator versus being a leader. We are looking to get around that. There's some legal trouble there with running a steak node, but trying to work through it. But yeah, I mean, we've talked about some stuff, but like, I mean, I think running, running nodes, the Canaries would hopefully catch this. But yeah, I mean, it's definitely an area that we're. I mean, yeah, clearly these things have made it to Mainnet.
00:26:45.300 - 00:27:06.874, Speaker E: So, you know, we still have work to do. I mean, another complaint is that testnet's not really a realistic load. We're not getting all the variety of transactions on Testnet that we are with Mainnet. So these little corner cases pop up that only pop up on mainnet will bite us. So, I mean, that's another area for potential exploration from our team.
00:27:12.594 - 00:28:14.016, Speaker A: So I wanted to jump in here and add this slide. I'm kind of compiling a list of things that I've seen talked about as, you know, potential solutions to a problem like this, or maybe not solutions, but more test coverage so that we have both a better release process and a better testing process. So one thing we talked about a little bit in the last call was some simulated market making in testnet to try to bring some more mainnet like activity onto testnet. So that's running now, and I think the plan is to ramp that up a little bit more. There's a channel created called project Mainnet load emulation that is hopefully going to talk about other ideas for that. Also what Steve mentioned. So will Hickey is in charge of most release engineering tasks, and he pointed us to this, this doc in the repo that talks about the Canaries and what they do.
00:28:14.016 - 00:28:47.384, Speaker A: So for the curious, if you want to read up more about how the Canaries are working currently. This should be a pretty good summary. And the plan is to maybe tweak this process a little bit to try to stagger releases so that some are on the old version and some are on the new version so that we can catch mismatches between 1.14 and 1.13 or, you know, other minor version mismatch. Excuse me, minor version mismatches going forward. And then.
00:28:47.384 - 00:29:22.884, Speaker A: Yeah, the last one I'll mention is this Discord channel project branching and release revamp. Mouthful is another place we're talking about release processes and how to sort of tighten those up. So anybody who wants to follow along and kind of learn more about what's going on, these would be places to check out. Any questions, thoughts around that? Any more questions for Steve?
00:29:25.824 - 00:29:56.986, Speaker C: I have one more. A while ago I made a thing that I tried to simulate transactions on Testnet. It was called the hammer. It was a fairly simple idea. You just not knowing what shape of transaction could cause, what problem. It just composes randomized transactions selecting from a variety of different types of actions that a transaction could take. I didn't include in that things like setting compute units or requesting compute units because honestly at that time I wasn't even aware of that functionality of transactions.
00:29:56.986 - 00:30:05.754, Speaker C: Do you think that that kind of thing has any value at all? Because I could revisit it if it seems like it could, but I'm not going to it if it doesn't seem like it's something that could be helpful.
00:30:08.374 - 00:30:08.686, Speaker A: Yeah.
00:30:08.710 - 00:30:52.974, Speaker E: I mean, out of gay. I mean, yeah, I'd be. I'd definitely be interested to take a look. I think we have. I'm probably not the best person to speak on this, but I think specifically, like Kirill, maybe somebody else have been like that's thing Tim was talking about with mango. I think like Kirill's been working with them, so they might, maybe they would be better people to at least take a look. But yeah, I mean, I think in general, just exercising more instructions, that seems good and maybe even the realm of fuzzing of maybe not necessarily putting together coherent things, coherent transactions that you'd expect that could turn up random issue, you know, corner cases as well.
00:30:52.974 - 00:31:00.434, Speaker E: So yeah, I think there could be some merit to it and it's definitely worth at least a look from or, you know, our discussion.
00:31:06.014 - 00:31:22.094, Speaker A: So I'm going to copy his handle here and put it in the chat. He would be the one to talk to on testnet load. And again, this channel would be a good one to follow.
00:31:22.914 - 00:31:35.402, Speaker E: Main net load emulation Tim, maybe send Carol a message to just give him some context there. Sure. Yeah, he's actually probably up right now. I think it's like 09:00 a.m. For him.
00:31:35.538 - 00:31:43.614, Speaker A: Yeah. Okay. One other thing I'd like to point out.
00:31:44.184 - 00:31:45.216, Speaker E: So there was a little bit of.
00:31:45.240 - 00:32:35.040, Speaker A: Chatter internally and I guess slightly on MB validators as well. Someone brought up in the delegation program that they're a little bit discouraged from upgrading to a new minor release when, you know, there could be consequences for that for them. Right. They could lose their entire delegation because of a bug that they didn't, they don't really have a lot of control over. So one idea that we're going to implement is to give people sort of a break when they upgrade to new minor versions. So if you're upgrading to 1.14 when we call for 10% of the stake and you're in the delegation program, we'll waive performance requirements while you're upgrading, or not while you're upgrading, but during that window where people are testing out 1.14
00:32:35.040 - 00:33:20.898, Speaker A: before we call for the whole cluster to upgrade. So that's another thing that I think will help out and sort of spread the burden a little bit for these new miner releases so that it's not, you know, it seems like historically it's been higher stakes validators that have taken this leap and then also have been sort of the ones who have to deal with crashes as well in the case of 1.14 a couple of times. So hopefully that helps out and helps incentivize people to upgrade earlier. Yep. Cool. I think that's all I've got in terms of content normal updates here.
00:33:20.898 - 00:33:47.164, Speaker A: 113.6 is recommended for Mainnet and with this, let's see here. With this pull request, it should be in probably within the next day or two. There's going to be another 1.14 release for testnet pretty soon. So if you are testnet validator, look out for that. Hopefully getting that out and test it on testnet relatively shortly.
00:33:47.164 - 00:33:49.528, Speaker A: Yeah, Zintetsu, go ahead.
00:33:49.616 - 00:34:36.284, Speaker C: Sorry. Just with regards to the main net suspending the performance criteria, maybe it would be a good idea to have like a separate category that, you know, is populated by those validators that are running the latest version only. Because if you eliminate it completely, then that means that they kind of have, someone could abuse that as a free pass to basically like always be on the most updated version yet always have bad performance. And if it's completely eliminated as a criteria, then they could do that. But if instead it's all validators that are on the newest release have a reduced but still existing performance criteria. And maybe it could be something like relative to other validators on that version. So if like all of them crash at the same time, then none of them get penalized.
00:34:36.284 - 00:34:44.180, Speaker C: But if one crashes, then maybe, or whatever has downtime, then maybe it gets penalized just as a concept, instead of eliminating completely. Maybe just a subcategory.
00:34:44.372 - 00:35:29.464, Speaker A: Yeah, yeah. All things we're thinking about. I think for the initial version, we'll probably do whatever's the simplest for, specifically for the 1.14 release. But I think, you know, as we improve it, it'll probably be something a lot more like what you're describing, right, where it's reduced requirements or maybe some sort of credit system where they, by upgrading, they have credits for later when they don't do as well. Something along those lines that maybe is a little trickier to implement, but also less prone to cheating. That's all I got in terms of slides, any other questions or topics people want to bring up?
00:35:37.164 - 00:36:24.794, Speaker C: From what I read about this new issue and the way it's being handled, by making 114 have the same behavior as 113, even though 113 was kind of incorrect behavior. I think around the way this compute unit and priority fees are handled, I thought what I read was that if you issue a certain instruction, then the priority fee no longer even applies, is essentially what's going to happen. So you can declare priority fee, and then if you issue, I forget what it was like, I don't know what, some kind of alloc instruction or something like that, then your priority fee gets sort of like zeroed and you don't even pay it. Is that true? And is that like a potential sort of like exploit path where people can just as long as they include this extra, one extra instruction in their transaction, they can sort of like declare priority fee, but not actually pay it.
00:36:27.094 - 00:37:09.654, Speaker E: I'd probably have to defer out to Tao on that one. I'm not the most familiar with that part of the code. But at least your point being like we, you're a bit about like 113 having incorrect behavior and 1.14 having correct behavior in the sense of like what we want to do moving forward, that's accurate. So we're kind of like disabling a chunk on 114, only on mainnet to match 1.13, and then we'll enable this feature, or we'll re enable the code when Mainnet is fully on 1.14. But as to whether it's like attack vector for like DDOT or, like, you know, cheap transactions to spam the network.
00:37:09.654 - 00:37:26.914, Speaker E: I'd have to defer to, like, Tao. I know. Yeah. You could probably ask about it on the GitHub issue if you want. I'll probably respond. And I guess if it, if it is an issue, maybe we'll remove your comment after.
00:37:28.334 - 00:37:33.122, Speaker C: Yeah, it's just that we just got to get to 114 and then all these problems go away.
00:37:33.278 - 00:37:33.946, Speaker E: Yeah, exactly.
00:37:34.010 - 00:37:43.454, Speaker C: If all this, if all, if everyone had been on 114 when this issue happened, it would have been the 113 notes that panicked or whatever. And, you know, but we're not there yet.
00:37:43.994 - 00:37:53.614, Speaker E: Yeah, exactly. 113 was the correct behavior because that's what the majority was correct with air quotes around it here.
00:37:58.894 - 00:38:22.194, Speaker A: All right. Any other questions, topics? Okay, great. Well, thank you to Blake for joining and talking about the identity transition. And thank you to Steve for joining and talking about the panic. See you all next two weeks from now at the 11:00 a.m. Time slot, the earlier time slot. Yep.
00:38:22.194 - 00:38:24.058, Speaker A: Bye, everybody.
00:38:24.246 - 00:38:25.442, Speaker E: Hi, everybody. See you on discord.
