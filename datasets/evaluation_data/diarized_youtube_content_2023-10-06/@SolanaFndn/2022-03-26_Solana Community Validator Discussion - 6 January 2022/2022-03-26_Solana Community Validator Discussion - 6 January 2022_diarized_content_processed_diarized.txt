00:00:03.840 - 00:00:23.714, Speaker A: Cool. Awesome. Thanks everybody for jumping in here. Happy new year. I think it's setting up to be a very interesting 2022 already. Hope everybody is staying healthy. I'm getting over a little COVID myself.
00:00:23.714 - 00:00:58.762, Speaker A: I personally, I myself didn't have a whole lot agenda wise this week. It looks like we've got a couple of engineers from the Solana labs team. Anatoly may be joining us for a few minutes. I know there's been a lot of questions recently around some performance issues around the network. I know there's a topic of discussion. I know there's a lot of work underway, both on the 1.9 branch, which is not yet rolled out to Mainnet, as well as some back ports that should be coming on to 1.8,
00:00:58.762 - 00:02:18.744, Speaker A: which will hopefully alleviate some of the pain that some folks on the network have been experiencing recently. I think before we dive into that, one thing I just wanted to mention this was specifically from the Solana foundation side. The server program that we've been running and launched a handful of months ago continues to expand. We're bringing on more data center operators who want to provide validator and rpc quality nodes to the Solana community. The website, I admit, does need a little bit of a copy update to reflect the latest offerings. As of today, Equinix, Lumen, Stackpath, and Edgevana, which are all separate data center operators, and independent companies, are all offering to the validator community high quality servers with adequate bandwidth at what should hopefully be competitive pricing. And so I just wanted to make sure that everyone is aware of that for either validator nodes or individual or clusters of RPC nodes.
00:02:18.744 - 00:03:27.264, Speaker A: So just wanted to put that out there. We may have some other, there may be some other data center operators that are coming online later this year as well. So hopefully this really continues to bootstrap organic market demand as the network and the infrastructure behind it continues to diversify, as well as the number of operators continues to grow and diversify. I think it's just really great that a lot of the sort of, I'll say, more traditional Internet infrastructure organizations are really starting to pay attention, particularly as the throughput of the Solana network is quite high. The considerations for setting up the infrastructure to run this high performance network is equally as important as just the individual node or the operator, uh, you know, technical skills themselves. So just wanted to put that out there. Thanks, Ellie.
00:03:27.264 - 00:04:14.802, Speaker A: Yeah, that was mostly what I had for foundation side. Was there anyone who had either from the tech side or from the validator community side who had any comments or observations that they wanted to share regarding some of the work that's going on with the network performance network. It's just fine. Yeah, it's running okay. All. It's fine. Perfect.
00:04:14.802 - 00:04:43.414, Speaker A: Well, then we don't need this meeting, right? Everything's. Everyone knows what to do. Thanks, Victor. Yeah. I personally, I don't want to misrepresent, I don't have the deepest technical insight into what is or is not going on as far as improvements to the code base. So if there's anyone who has any specific questions or discussion points, I'm wide open, but I'm not the best reference for today.
00:05:06.494 - 00:05:07.634, Speaker B: It's all good.
00:05:10.974 - 00:05:43.854, Speaker A: Okay. Yeah. Michael, to your comment. Yeah, I know there was some issue with the, with I think like the just in time compiling and caching. I believe that is in the works in 1.9 and planned for backport to 1.8 as soon as the team believes that it is safe to do so.
00:05:43.854 - 00:06:23.504, Speaker A: I'm not sure exactly what is, what the timing for that is. I know there was a question whether it's going to be, you know, in a day or in a week. I don't know. Last I heard from, I think some estimate from the Solana labs team was probably like eight to twelve weeks before we see a 1.9 roll out onto Mainnet. I think there are a lot of changes in there, and so it's going to take, I want to make sure this is really soaked on Testnet for a good long time. I think there were some more complex feature changes that I think would be really difficult to backport to 1.8.
00:06:23.504 - 00:06:55.134, Speaker A: And so I think that is going to be a somewhat larger feature and performance change when we go to 1.9. And, yeah, I was hoping to have a more thorough, kind of bulleted projected breakpoint, I'm sorry, breakdown of the 1.8 to 1.9 changes in advance for you here. I apologize. I don't have that at the moment. So I can try and get something together, at least for the next meeting, if not sooner.
00:06:55.134 - 00:09:17.902, Speaker A: But in the absence of that, I don't have a whole lot else to cover today, unless there is something that anyone else wants to talk about or has any questions. Question in the chat, what was the best outcome from Lisbon? In my personal and unofficial opinion, I'll say, other than the incredible parties, I think it was really just like this confluence of human energy that so many people have been hustling and grinding for years at this point, really bringing Solana to be where it is today, you know, and there's always going to be challenges and there's always going to be growing pains and that's fine, it's totally expected. But I think just like the energy that individual people had coming together really just, I think it was just like an amazing thing to see how much enthusiasm that there was in the community. And a lot of people were, I think, blown away by saying like, well, oh, I knew I was excited about Solana and I knew people in my community were, but like maybe didn't have the greatest, you know, insight into the fact that this was like truly like global excitement across all different areas of the tech, all different geographies, all different levels of, you know, people want to build, people want to invest, people want to develop the network. I think it was just like a great reinforcement of kind of people's enthusiasm for the whole thing. There was some awesome partnerships that came out and announcements, but yeah, just like everybody feeling really good about things. Yeah.
00:09:17.902 - 00:10:12.234, Speaker A: Question about increasing the number of mainnet validators. Yeah, I think we would like to increase the rate of growth of the foundation delegation program. I think so. A couple things like, you know, the, sorry for those that may not have seen the question in the chat is, you know, what can we do to increase the rate of growth of the number of mainnet validators? And I just want to make sure that, you know, we understand the distinction. Like the foundation as a standalone organization can delegate, you know, according to the processes set out in the delegation program. But this itself is not, you know, really shouldn't be a gate or a blocker to anyone actually joining Mainnet, right. These are two, these are two unrelated processes.
00:10:12.234 - 00:11:15.734, Speaker A: Now I understand there is at the moment a reasonably high stake barrier to the break even point for a new validator. And a lot of people do look to receive the, you know, the approval from the foundation or receive a delegation or a potential delegation from the foundation around the same time that they would choose to join Mainnet. But you don't need one to like, you don't need the foundation to run your mainnet validator. So if a thousand people tomorrow decide they want to start running mainnet nodes, they can do that, right? The network is entirely permissionless, particularly with the more recent growth of stake pools. Marinade J Pool has launched recently. We've seen Sossian and even Lido and some other community driven initiatives to help spread the stake around. I think we're starting to, I mean, we're not quite there.
00:11:15.734 - 00:13:02.144, Speaker A: I mean it's always a work in progress. But what we're starting to see is less exclusive reliance on receiving a delegation from the foundation for a smaller validator to make that break even calculation, which is really where we want to get to that. If someone can know that if they can run a validator of a certain quality or meet certain quantitative performance metrics, then they can have a reasonable expectation that they might receive delegations from one of multiple sources of stake. Right? Not just the foundation should never be in a position where we are or are perceived to be gatekeeping anyone's access to the network. As far as running a validator infrastructure, the cost model changes that are coming in the future may also alleviate this. I don't have exact numbers, so I can't, can't make specific promises, but it is quite possible that that could reduce the recurring voting cost for validator which would then, you know, reduce the break even stake required, which could, you know, significantly reduce the barrier to entry for new and existing validators as well. Regarding the question of slashing, there's no, there's no plans for automated slashing in the roadmap at the moment.
00:13:02.144 - 00:14:12.064, Speaker A: You know, one thing that was simply a point of discussion that I think came up in the Discord channel earlier today, and it's not the first time this has come up. And again, this was just a discussion point. It's not something that's actively being worked on. To my knowledge was the idea of effectively not so much slashing, not a destruction of staked tokens, but a deactivation or effectively a cooldown or on delegation of actively staked tokens in response to poor validator performance. So those who would be delegating do not have principle at risk. But effectively if a validator is been inactive or not voting for a certain threshold of time, if the state delegated to them is deactivated, then that means that they're going to lose their leader slots. They're not going to be having as much stake weight in consensus, which really alleviates the pain of what a poorly performing validator does to the network.
00:14:12.064 - 00:16:01.124, Speaker A: Yeah, so those are some thoughts. Anyone have any thoughts or comments, questions on that? As far as incentivizing upgrading, there's nothing in the protocol really to mandate that people run on a particular software version, but this is really up to the delegators or stake pool operators or which could be a community pool, or it could be the, you know, the foundations program in this case that, you know, to move delegations around based on what they believe is, you know, an appropriate software version or remove delegations from someone who, you know, is several versions behind. Yeah. There are also occasionally, effectively like breaking feature changes. So if a validator is not on a certain minimum software version, and a feature is enabled on the network that requires minimum software, it's possible that the validator could get forked off. So while that doesn't force them to upgrade, it does effectively cause them to be unable to vote.
00:16:18.324 - 00:17:07.268, Speaker C: Hey y'all, my name's Will. I'm an engineer with Solana Labs. This upgrade problem is something I've been thinking deeply about for the last month. I'm trying to come up with a long term vision that will take, I don't know, maybe months or years to implement, where upgrading is sort of handled in a fully decentralized way with governance on the network. I still only have like a half formed idea in my head, and I'm sure there's tons of mistakes I'm making as I think through this. If any of you all who are validators are interested in discussing that further, I would love to talk to some of you, because I don't think there's any way to do this that isn't disruptive to some humans. Asking humans to, you know, be around to monitor their nodes a little bit more while during upgrades, that kind of thing.
00:17:07.268 - 00:17:21.064, Speaker C: And so this is as much a human problem as it is a software problem. I'll drop a note in the MB validators discord channel. Would love to chat with some of you more as I try to better understand this problem.
00:17:24.324 - 00:17:55.214, Speaker A: Thanks a lot, Will, appreciate that. Looks like Anatoly has joined us. Toli, would you mind giving us a little bit of, kind of the more technical insight into what's going on performance wise, kind of overall? There was also a question, if possible, to give sort of a roadmap of what are the main feature changes and upgrades to look forward to when 1.9 is eventually moved up to Mainnet.
00:17:55.624 - 00:18:15.128, Speaker B: Cool. First of all, melia, it's a dope hat. Love it. Yeah. So, performance problem. We think we've root caused it to basically be the. There's kind of two issues.
00:18:15.128 - 00:19:13.294, Speaker B: We don't know which one is causing more of a problem, but the one that's clear is that the cache for the jitted programs that are being executed on chain, that cash is getting thrashed. And we see in some patterns of access that sometimes a transaction, like every solent transaction, will require a solent program to be recompiled. And that recompilation is expensive. And when you have it occur in every validator at the same time time, that kind of basically causes delays in block processing, which then cause a leader to mister block and then create forks. And you see these like high skip rates and huge drops to capacity. So that caching. I don't know if you've ever been a programmer, but there's basically two problems in computer science.
00:19:13.294 - 00:19:39.818, Speaker B: One of them is caching. Yeah, but these are. Well we're working on a fix, I think. I don't know where. I don't want to speak for Trent and everyone else that's working on it, when it'll be ready, but as soon as possible we'll have a release. And it should be a soft fork requires. No, not a fork, just a soft upgrade without any feature.
00:19:39.818 - 00:20:14.474, Speaker B: Without a feature gate on 1.9 changes. Let's see. Basically we're trying to figure out how to increase the programmability of the network from a bunch of different perspectives. One of them is doubling the transaction size. Transactions are limited right now to about 1200 bytes. Some of the transactions that are causing this thrashing are the super big ones that are using every bit impossible resource.
00:20:14.474 - 00:21:23.606, Speaker B: We want to expand the number of use cases that cause these kinds of very hard to compute events for some reason. But these are important transactions because they are compound transactions. So they take a lot of locks and call into a lot of state, and those applications are typically not frequent, but provide like kind of rich state composition. So doubling the transaction size is one of those things. Brooks and Jwash have been working on increasing NTR on the account set, so the goal there is to allow the network to scale basically with SSD's. I think the latest numbers that I've seen, it's somewhere like a billion accounts, takes up 400gb, and I think they've run tests with over five or 6 billion accounts synchronized between all the validators. So idea is that like can we get the software to get out of your way? So when there's demand for more capacity, you can throw SSD's at it.
00:21:23.606 - 00:22:07.376, Speaker B: Users obviously pay for that storage, but that means the number of use cases, the number of stuff that the network and state is synchronizing goes up. And those are all good things. Um, the shoot, man, I've been so deep in this performance thing, I remember all the features. Uh, congestion control, that's the big one. So our runtime when we initially designed it, um, we're basically thinking that like serum was going to be the most important application. And uh, when you look at like performance of something like a trade, um, it's more much lower than a like signature verification. Like it takes less resources than it does to do an f two 5519 sig check.
00:22:07.376 - 00:22:56.116, Speaker B: So we cap the price per transaction to be basically the number of signatures that a transaction has, because that was the most expensive operation. Obviously, as like DeFi exploded nfts, all these use cases exploded in some unpredictable ways. The ways that people compose these things have now become much more interesting and rich, and we have to start charging more for transactions that use more resources, even if they only have one signature. So the cost model that was implemented in one in 1.8 does some bare bones congestion control. But the key difference in 1.9 is that transactions that want to exceed the minimum capacity, kind of like that a vote or a serum trade takes.
00:22:56.116 - 00:23:52.474, Speaker B: They need to specify how many resources they're going to use. And that means that the scheduler that picks which transactions goes into a block can then make intelligent decisions about how to pack those blocks. Some of the other things like that we are working on is in general, having much closer reflection for network throughput for quality of service based on stake weight. Typically messages, transactions, even gossip messages, everything that the network node does, it should start prioritizing things by stake weight. Most of that's true already today, but some things, like transaction flow from forwarders or TPU, that also needs to start getting prioritization by stake weight of where it's coming from. So some of those changes might land in 1.8 and some in 1.9.
00:23:52.474 - 00:24:36.962, Speaker B: If you look further down into the eventual future, that looks like a bandwidth auction. Stake represents bandwidth in the network, so the more stake you have, the more bandwidth you have. Not so much. It's slightly different from a block space auction because Solana, the way it's designed, it has blocks, but there's no gaps between them. You have this one pipe of bandwidth, and you have a leader that gets scheduled to be the block producer, but it's not asynchronous. Network produces one block, waits for that thing to synchronize, and goes to the next block. So there isn't an opportunity to run a block space auction that you do on a normal mempool.
00:24:36.962 - 00:24:59.224, Speaker B: But this looks more like a bandwidth auction. So that's the main features. And on top of a bandwidth option, you can then build your own flashbots, mempools, or whatever it is that you want to do. Network is pretty agnostic to what applications run on top of that.
00:25:16.824 - 00:25:17.804, Speaker A: Thank you.
00:25:19.344 - 00:25:37.664, Speaker B: Questions? Cool. Yeah, hit me up on discord if you guys think of something. I'm. I'm always online.
00:25:44.644 - 00:25:45.824, Speaker A: Thanks for your time.
00:25:49.364 - 00:25:50.580, Speaker B: Thank you, guys.
00:25:50.772 - 00:26:02.664, Speaker A: Awesome. Thanks, everybody. Thanks, Anatoly, for joining us. And we'll see everybody in a couple weeks. One. Thank you very much. Bye.
