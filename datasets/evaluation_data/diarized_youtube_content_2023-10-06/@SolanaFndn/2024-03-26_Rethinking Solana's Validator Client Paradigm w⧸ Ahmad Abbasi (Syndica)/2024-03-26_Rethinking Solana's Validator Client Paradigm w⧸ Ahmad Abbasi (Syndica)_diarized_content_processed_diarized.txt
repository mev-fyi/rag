00:00:08.680 - 00:00:10.302, Speaker A: Ahmad, welcome to validated.
00:00:10.398 - 00:00:11.958, Speaker B: Thanks for having me on, Austin.
00:00:12.086 - 00:00:52.354, Speaker A: So, from the perspective of the Solana foundation, this kind of came out of nowhere. I was on Twitter one day. I saw a tweet from Syndica saying, hey, we're announcing we're building a new validator client for the Solana blockchain, and we're calling it Sig. And it's called SiG because the client is built in ZiG, which is a different programming language. Original implementation is in rust, the firedancer implementation is in c. And then you guys were also building this as an RP's optimized client, a read per second optimized client, which is a different take on what the role of a validator client might be than the current two implementations of it. So I want to get into a bit of how you guys made that decision.
00:00:52.354 - 00:01:10.522, Speaker A: But also, your core business today is not building a validator client. It is running an RPC service, you know, in a fairly competitive landscape. So I want to walk through that transition you guys went through from saying, we're building an RPC business to we actually need to build a validator client to build a better RPC business.
00:01:10.698 - 00:01:39.392, Speaker B: Yeah, sure. I can kind of go into the genesis story of all this. Let's go to, like, 2021, late 2021. We had just started Syndica. I was playing around with Zig. I was pretty interested in, you know, c, c, rust, all those great languages, but felt like all of them lacked in some capacity, and so started to dive deeper into zig and quickly realized it was taking a different approach than most other languages. They were focused on more simplicity.
00:01:39.392 - 00:02:13.548, Speaker B: They were trying to not have too many abstractions. They're focusing on memory allocation, all the things that I think systems engineering requires. So got really interested in that, and then decided initially just to build out a RPC client. So we started to build out kind of the bare bones infrastructure for the RPC client. Quickly got it to a state where it was making requests, and I was like, okay, this is kind of cool. So I put out a tweet. I think Anatoly had put a quote, tweet saying, the new rust just dropped.
00:02:13.548 - 00:03:02.828, Speaker B: And I was like, okay, interesting. And, yeah, just started to get a lot of feedback from folks saying, like, oh, why? You know, why, Ziggy? Whats special about it? How do we start to use this client? And I started to see there was actually quite a bit of interest from a lot of folks that have toyed around with Zig, but not necessarily built a project in it. Start to get interested in building something more in Zig in Solana. Fast forward to 2023. I am sitting in Dubai. We had just opened our Dubai office, and I was talking to our head of data, Will, and I was telling him the response we got from this launch, and he was kind of shocked and said, wow, I thought it was going to be a side project. Didn't know that many folks are actually interested in Zig.
00:03:02.828 - 00:03:51.160, Speaker B: But I think maybe a month later, we sat down internally as a team and really sat there and thought about it in depth and said, okay, well, what is something that we can do to accelerate the adoption of Solana? And one of the most pressing, I think, issues in the ecosystem at the time was there was only one validator implementation. We decided, okay, well, it makes sense. We understand Zig. We had a few folks at our startup, at Newsig, and we decided to just hit the ground running and start building and see where it takes us. I think within maybe a month or two, we had gossip implemented and running. And so it was cool to see one language talking to another language. I mean, at the end of the day, it's just bits and bytes that are flowing through some network path.
00:03:51.160 - 00:04:02.832, Speaker B: But it was cool to see, nonetheless, that our SiG validator, gossip spy mode, was able to communicate with the rest client. So, yeah, it was cool.
00:04:02.968 - 00:04:06.296, Speaker A: So when did you guys actually start in earnest on this project?
00:04:06.480 - 00:04:13.698, Speaker B: Yeah, we started in May of 2023. Okay, so this is right around the same time that Ethereum had its buck.
00:04:13.826 - 00:04:52.754, Speaker A: Yeah, but may of 23. This is, like, depths of the bear market at this point. So walk me through a little bit of the decision making and the thinking internally to say, hey, we're a startup, we're a new team. We've raised some money, but, like, it's the bear market, capital is hard to come by. We're going to actually build an open source client in addition to our main project. Like, what about the idea of building this thing was compelling enough to you guys, from a business perspective, that you were like, it's actually worth all these engineering resources to spin off some folks from working on what was our core product and actually start building a client?
00:04:53.054 - 00:05:38.450, Speaker B: I think it really came from the need to diversify Solana's client's implementations. I think the way we looked at it was twofold. One was we realized that the current Rust Solana validator implementation wasn't really focused on RPC. And us, as an RPC provider, we need to scale that. We can't just continuously, horizontally scale the number of validators it just becomes inefficient. It's not really something we can continue, and we can't pass that extensive cost to the customer. What we decided at the time was, can we build this validator, one, that can solve this initial problem of scaling reads, and then two, help the client diversity in Solana, because if Solana fails, syndicate kind of fails.
00:05:38.450 - 00:05:45.538, Speaker B: Right to. So we knew that we had to solve both problems at the same time. So that was kind of the genesis.
00:05:45.666 - 00:06:00.866, Speaker A: So, walk me through a little bit about what you guys were seeing in the rust client that wasn't meeting your needs as an RPC provider. There's not many blockchains that have a truly separate software package, let alone one built in a totally different language, just on the RPC layer.
00:06:01.010 - 00:06:54.096, Speaker B: Yeah, sure. So what we were seeing was that there were high frequency, low RP's methods that existed in the rust client. What that basically means is there are some certain RPC calls that are called quite often, but don't actually have the capacity to be served in a scalable way. So, just to give you some raw numbers, what we noticed was that an average RPC node, with the specs that Solana foundation has laid out, they should be running one within, was getting about 60 get program accounts, calls per second, or 100 get signatures for address, 250 get transactions RPC calls per second. And these are RPC calls that are needed by dapps for their dapps to function. They're pretty critical. And so what we realized was, in order for us to get this RPC business to the next level, we also need to improve on the software layer.
00:06:54.240 - 00:07:13.208, Speaker A: Yeah, and sort of to contextualize that. I mean, the Solana network routinely runs at five to 7000 transactions per second. And so if you're saying that some of these calls were as slow as maybe only 60 calls per second, they were able to be fulfilled. There's a pretty big delta there between the performance of write optimization versus read optimization.
00:07:13.376 - 00:07:46.254, Speaker B: Yeah. I mean, just to give you a sense, for every write or send transaction call that occurs through our infrastructure, there's about 25 x more read calls required to accomplish that. Right. And that's mainly because the Solana network, or the Solana blockchain, it kind of has state kind of distributed in different accounts. Right. So in Ethereum, you have a contract which kind of holds all the state within that contract. Then in Solana, you have different accounts that can be accounts for each individual user, or a program can decide it wants to have five different accounts for whatever reason.
00:07:46.254 - 00:07:55.214, Speaker B: So it's a really flexible programming model. And so that's where the reads surpass quite a bit in terms of number of reads compared to the writes.
00:07:55.374 - 00:08:15.724, Speaker A: So walk me through a little bit of what actually goes into building a read optimized client as opposed to a write optimized client. What did decisions did you guys make differently? Or are you making differently than folks at firedancer or folks at Solana Labs who are building out a write optimized client?
00:08:15.844 - 00:09:00.924, Speaker B: Yeah, I can tell you how I think about it. And then hopefully that kind of answers your question. And so the way I think about it is accounts to me are like heap memory instead of a computer, right? And transactions are what I internally call cnps, which are cryptographically secured network packets. So if you look at transactions, they fit within the 1500 MTU of which is most routers and networking infrastructure has that MTU. So I mean, essentially it's a network packet that's cryptographically secured. And so that CMP then goes into the system and modifies that heap in some way. And so that's at the very core level how I start to frame sig in my mind.
00:09:00.924 - 00:09:44.852, Speaker B: Then what does that mean? If you think about these network packets coming into the system, what you really want to optimize for is decreasing the amount of cache misses. You're constantly writing to the account state, you're constantly reading as well. What you want to do is optimize for layer one cache hits. If not layer one, then L2 cache hits. Just to give you some sense, when you think about different operations on a cpu, the fastest cpu operation is, you can argue, is the add instruction from register to register. Past that you have an if branch statement. And if branch is maybe two x stat, floating point addition is maybe two to three x stat, and then you have the layer one cache read, which is three x.
00:09:44.852 - 00:10:43.946, Speaker B: Maybe it's just a simple ad operation, which is not that bad. And actually if you go into some other kind of operations, like if you, let's just say have the wrong if branch, where if is not true, that's actually ten to 20 x. A simple add instruction in terms of the number of cycles the CPU has to go through. And then when you get to things like an atomic operation, that's like 15 to 30 x, and then you go to something like layer three cache read, believe it or not, that's about anywhere between 30 to 70 x, a simple ad operation, right? And then what's kind of mind blowing is if you go into like a main rAm read, that's 100 X, then a kernel call, believe it or not, is 1000 X. There's actually quite a bit of disparity between these operations. What you want to do is really try to use the operations that reduce the number of cpu cycles and essentially increase the throughput of the system when.
00:10:43.970 - 00:11:34.314, Speaker A: You'Re actually building the client. Though from a high level block diagram perspective, for lack of a better term, I'm assuming this makes the client significantly worse at write operations, which is of course not what it's optimized for. But how do you think about changing the logic of how a validator operates for something that is read optimized? Because there's sort of this like, paradigm that the Solana Labs client has been built towards, right? And a lot of that is write optimization. And then the RPC package, as we were talking about, was not exactly an afterthought. But this is why there's work on RPC 2.0. This is why there's room for you guys to build this type of client. So what are the trade offs that you had to make in designing the SIG client in order to get that read optimization so high?
00:11:34.614 - 00:12:25.994, Speaker B: Yeah, so, just to give you a sense of where we're at so far, we have just implemented gossip. We are in the process of implementing accounts DB. Initial benchmarks show we have increased performance anywhere between 50% to 70% compared to the rest client. So we're actually doing some pretty good. And that's both on the reads and the writes. So I don't think that writes will necessarily suffer from us writing a read optimized validator. If you think about accounts as being deep memory, and then you think about modern cpu with 1624 cores, how can you shard that account state per core? So that way any call it a request, RPC request or transaction write that's coming into the system knows exactly where that transaction write or RPC call should land.
00:12:25.994 - 00:13:15.522, Speaker B: The way we think about it is, can we shard internally within the system in a way that utilizes like a math based sharding mechanism, reduce the number of locks, and coordinate data to and from those threads using basic math? That's kind of like the bare bones thinking behind. How do we build out this read optimized validator? When we get to the right side, which is when we start actually building out the consensus and the runtime, that's where I think we'll start to see where some of these thesis that we have internally actually translates into either better performance or worse performance on the right side. But everything that we've done so far on the REIT side suggests that the rights should not suffer.
00:13:15.698 - 00:14:19.872, Speaker A: So I want to talk a little bit about the RPC business, because most of the work that teams have been doing to optimize their RPC services are maybe not closed source, but they're proprietary. You don't see alchemy and Figment and Triton and Helios often publishing in depth guides about how they've got their infrastructure set up for the fastest performance possible. That's a competitive landscape there. But you guys are building an open source client. I'm always curious to talk with teams that are building hard foundational technology in open source that they're trying to build a business model around as well, because the classic web two or web zero version of this is build an RP's optimized client and just don't release it. Keep it internal, take your 50% to 70% performance increase and just build a better service because of that. So talk to me a little bit about the decision to build this both in public and open source.
00:14:20.018 - 00:14:28.164, Speaker B: Yeah, well, first of all, I think we're so early, we're in the 1990s of the Internet phase, right? So I think we're really early.
00:14:28.284 - 00:14:29.820, Speaker A: As blockchain in general, you mean?
00:14:29.892 - 00:14:58.496, Speaker B: Yeah, as blockchain in general. Exactly, yeah. And I think there's going to be a lot of room for a lot of competitors in this space. And the more we can enable other players in this industry, the better it is for everyone. So it's kind of, you know, instead of trying to, you know, eat the whole miniature pie, how can we grow the pie? That's kind of how we think about it. That's one. And then I think, secondly, I think what's really valuable for a startup in this industry is knowledge.
00:14:58.496 - 00:15:38.274, Speaker B: So I think the more knowledge you can accumulate, the more capabilities you have as an organization to build out different products and different feature sets that may be different from your competitors. And so I think us going through this practice of building out this validator from the ground up, literally rewriting every single component from scratch, I think will give us the most insight as an organization to build even more products and features. I think the data layer in crypto or in blockchain should be essentially free. It's a commodity. When you think about RPC, it should be a commodity really, as an infrastructure provider. It's a race to the bottom. That's how we think about it.
00:15:38.274 - 00:15:47.440, Speaker B: We might as well start to gain that knowledge and try to understand what else products and features can be built to expand from there.
00:15:47.592 - 00:15:55.320, Speaker A: Tell me a little bit more about those other optimizations that you guys are doing and where the current RPC client isn't really meeting your needs.
00:15:55.472 - 00:16:29.532, Speaker B: Yeah. One of the biggest challenges with Solana compared to Ethereum. Well, first of all, comparing to Ethereum is not really an apples to apples comparison. The rate at which data changes in Ethereum is on average every 12 seconds, whereas in Solana, data is changing every 200 to 400 milliseconds somewhere around there. Secondly, you also have a lot of indexing solutions that exist in Ethereum. Most of them are open source, some are closed source. You have things like the graph, which automatically indexes XYZ data and makes it available, which is great within Solana.
00:16:29.532 - 00:16:43.828, Speaker B: That's actually a lot more difficult even at the current amount of data that's going through the system. If you were to exclude vote transactions and you thought about just true TPS, let's just say it's around 500 tps of true user transactions per second.
00:16:43.916 - 00:16:44.260, Speaker A: Yeah.
00:16:44.332 - 00:17:17.532, Speaker B: And now you can have upwards of, I think, up to 256 accounts per transaction. So I think the problems on the RPC side or on the data layer side is pretty full. Right. We have our plate full of problems. I think building out a validator that is going to allow for ease of access to that data is kind of just the 1% and then the other 99% that an infrastructure business in this space has to solve for is really indexing and how you could make state changes available in real time.
00:17:17.668 - 00:17:24.188, Speaker A: Yeah, that's really interesting. Just the amount of state that might change at any given second is quite high.
00:17:24.316 - 00:17:24.860, Speaker B: Yeah.
00:17:24.972 - 00:17:35.652, Speaker A: So as you guys go through and are thinking about the volume of data that needs to be read in order to support these rights, that's kind of really where the impetus for a new client came from.
00:17:35.828 - 00:18:14.620, Speaker B: I think it's threefold. I think, like I mentioned, client diversity is really important in any blockchain, and so for us to contribute back to the community with building out another client, I think is super important for us. The other aspect of it is making it more accessible. What that means is today, I think anyone that goes through the Solana rust code is looking for a needle in the haystack. Right. If they're trying to find a certain issue or debug something, that's not to talk down on the rust code base, by any means. I think what the labs has accomplished is pretty monumental, and anyone could attest to that.
00:18:14.620 - 00:18:51.594, Speaker B: But I think for Solana to be more accessible for more developers, for them to understand the specifications for them to understand how can they build other modules or services on top and continuously expand and improve Solana, you have to have some kind of reference implementation that is easily readable and digestible. So I think that's the second facet, and then the third facet is, yes, 100%. We want to increase the reads capacity of the validator, potentially even make it easy for someone sitting at home with a fiber connection to be able to run a Solana non voting validator if they choose to, or potentially even a voting validator.
00:18:51.754 - 00:19:06.632, Speaker A: Yeah, there's been a lot of talk about, like, client work and sort of at home validation if they have the data layer for it. What are your views on how likely or viable that is? And does the client you guys are building get us closer to that?
00:19:06.768 - 00:19:56.680, Speaker B: Yeah, I just want to actually share a stat. We actually were successfully able to deserialize a Solana Mainnet snapshot and build account indexes, the primary indexes under 32 gigs of ram today. I know that there's many kind of requirements on the Solana validator rust client that obviously would just not allow for that to happen. So we're actually in the process of thinking like, hey, is it possible for someone to actually run this on commodity hardware, or run it literally on a desktop with maybe twelve cores or something like that? So that is something that we're thinking through. And I think us collaborating with, obviously labs and firedancer will help us answer that question more realistically as time goes. Interesting.
00:19:56.752 - 00:20:19.034, Speaker A: Yeah, I think there's been a lot of talk about, if you didn't need to validate up at the tip of the spear, could you build something at home that's maybe two minutes delayed or something like that? That requires less bandwidth and less computational resources. But it's really interesting to hear you talk about low core count machines with maybe less ram being able to keep up as well, assuming they have the network bandwidth.
00:20:19.184 - 00:21:03.024, Speaker B: Yeah, I think the end state for Solana, it needs to also include the accessibility, not just in terms of being able to read code and contribute back to it, but also being able to run it. So I think part of us building out Sig is always keeping that in the back of our minds is, okay, how can we make it more accessible? Obviously, the number of cores and desktop chips will continue to increase over time, and so maybe it's realistic that maybe Aurizon will have 16 or 18 cores, 24 cores in the future, and you can actually run Sig directly on it. Or we build out a really optimized validator that allows something as little as twelve cores to operate and run the Solana validator.
00:21:03.184 - 00:21:22.288, Speaker A: So I want to go back a little bit to zig itself. Talk to me a little bit more about the programming language. I think a lot of people who are not engineers think this is exactly what Rust is built for. So what does Zig bring that rust or c don't have?
00:21:22.456 - 00:21:55.140, Speaker B: Yeah, so Zig's general purpose programming language and toolchain for maintaining robust, optimal and reusable software, that's their catchphrase. But I think if you were to break it down, it's really, I think it's twofold. One for me is the idea that there's no hidden control flow. What that basically means is you know exactly what's going on as you read the code line by line. And that goes back to the accessibility aspect of things. I think that's really important. And then also there's no hidden memory allocations.
00:21:55.140 - 00:22:46.186, Speaker B: And so going back to the previous point where kind of mentioned a kernel call is 1000 x, a simple add call in terms of number of cpu cycles. When you need to allocate memory, you need to ask the kernel to give you that memory allocation, and that's when you can use that contiguous byte array to do operations on it or whatever you need to do. And so what zig allows you to do is keep that memory allocation aspect in front view. Anything that you write that is going to allocate memory, you will be very aware that it's going to be allocating memory. It's a very explicit language, so you have to know when you're actually allocating memory, and you have to know when you need to deallocate the memory. And so it kind of forces you to think about everything from a data perspective. So there is a really great book, I'm not sure if you ever come across it.
00:22:46.186 - 00:23:24.858, Speaker B: It's called data oriented design. I can't remember who wrote it, but it basically emphasizes the idea that you should be building data oriented systems, because at the end of the day, what you're really doing when building a system is taking some data, transforming it, and then building a business use case out of that transformation. So it's actually not really the other way around where I think that is kind of like a misnomer in the industry. And so zig, I think, helps you to think about things from a data perspective. It doesn't really abstract away all the details the way other languages do. So I think that's kind of one of the biggest, I guess, strengths around.
00:23:24.906 - 00:23:58.568, Speaker A: That it makes sense I mean, like, for folks who listening to the show for a while, like, this is a lot of how Kevin Bowers describes what he's trying to get computers to do, which is to think about data and memory flow and not committing large amounts of memory copies, because that is an inoptimal system. And that's also where the idea of CUDA comes into play, too, as a different programming or paradigm model for how you build this stuff. Is that an accurate enough analogy, or is Zig really doing something different?
00:23:58.716 - 00:24:25.660, Speaker B: I think Zig, it really models itself after c. If you write zig code, it's c compatible. If you write Z code, it's zig compatible. So really, it's as low level as you can get. That's number one. Number two, I think what it's really doing is emphasizing the idea that memory isn't cheap. And so I think it's such a simple emphasis that a programmer should probably already know.
00:24:25.660 - 00:25:08.682, Speaker B: But when you, for instance, in rust, create a vector and you are allocating some memory, you lose track of every single time you're creating that vector. And when you're reading it, it's as simple as vec new. Right. You forget that there is some kind of memory allocation happening behind the scenes. And so I think Zig, with anything that you do that allocates memory, you have to pass it in allocator, and you have to also think through the allocator. What kind of allocator am I going to use? Am I going to use the general purpose allocator? Am I going to use the page allocator, or will I use an arena allocator? There's different types of allocators that really help in optimizing your code, depending on the specific use case. I think that is probably the biggest strength I would emphasize on zig.
00:25:08.682 - 00:25:18.490, Speaker B: Also, it's a very simple language. I'm not sure how many folks have actually looked into it, but very simple. I think maybe you can pick it up in a matter of a weekend or two.
00:25:18.602 - 00:25:24.390, Speaker A: Do you think there would be applications ever for actually writing Solana programs in Zig?
00:25:24.542 - 00:25:52.110, Speaker B: Yeah, definitely. I think we have to be careful, though, because I know that rust emphasizes on safety, and not to say zig doesn't, but you have a lot more control in Zig, which can be a good thing and a bad thing. Right. And so, with Rust, I think the safety aspect is much better compared to Zig. And so if I was going to write an on chain program, I would probably use rust. That's my personal view. Right.
00:25:52.110 - 00:25:59.734, Speaker B: Now, maybe that'll change over time, but that's how I think about things when it comes to rust and sig for on chain programs.
00:25:59.894 - 00:26:45.204, Speaker A: It's interesting, because before I joined the Solana ecosystem, I worked at Bison Trails, which became Coinbase Cloud, which was an RPC and blockchain infrastructure system, and we did a bunch of client development work, but it wasn't actually on the core client. It was all on things like sidecar files and indexers and bolt ons to the stuff that other people were building. So it's really interesting to hear that you guys actually said to make our core business work as efficiently as possible, to push the bounds of what's possible, we actually have to go and build a whole client ourselves. So I just think that's a really exciting and interesting thing that you guys have chosen to do, and I don't think there's very many parallels to that elsewhere in the blockchain space.
00:26:45.394 - 00:27:10.580, Speaker B: Yeah, I definitely agree with you on that. Again, I think it's definitely scaling the RPC business, but also the way I look at it, to tell you the truth, Austin, is if we didn't do it, somebody else would. I can give myself a pat on the back, but I think technology will always find its path to the least resistant or most optimal route. And so it was inevitable, I think.
00:27:10.712 - 00:27:18.996, Speaker A: So if folks want to find out more, if folks want to actually lend a hand in building this new client, where can they learn more and where can they get in touch?
00:27:19.140 - 00:27:46.404, Speaker B: Yeah, sure. So if you go to our blog site, which is blog syndicate IO, you will see the introductory post where we talk about Sig and kind of how we think about building it. So that's a great starting point. We also have some really great technical blog posts, so you should definitely check that out as well. It is on our GitHub. It's open source. Anyone can go on there, pick up an issue, start building, report any bugs, report any performance degradations, anything that can help us make it better.
00:27:46.404 - 00:27:53.884, Speaker B: So definitely go on there. We also have our Twitter, which is syndica IO, that they can find out more information on it. Nice.
00:27:54.044 - 00:28:07.442, Speaker A: I love that you guys are starting to write more about things like gossip. I think there's a little bit of a lack of technical writing in the Solana ecosystem on how things work or how things could be improved. So glad you guys are part of the group taking up that charge.
00:28:07.588 - 00:28:28.150, Speaker B: Definitely. I think research is kind of the bread and butter of syndica, because without the data analysis, without the research, we can't really build these optimal systems. So I think it really helps when we write it down and codify these ideas and thoughts into actual papers or articles and blog posts, and then folks can also benefit from that. So, yeah, 100%.
00:28:28.262 - 00:28:30.794, Speaker A: Well, thanks for joining us today on validated.
00:28:31.174 - 00:28:32.834, Speaker B: Awesome. Thank you for having me.
00:28:36.994 - 00:28:46.114, Speaker A: Validated is produced by Ray Belli with help from Ross Cohen, Brandon Ector, Emira Valiani and Ainsley Medford engineering by Tyler Morissette.
