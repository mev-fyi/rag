00:00:01.200 - 00:00:24.990, Speaker A: Right. If you don't know Jackson, please say hi to him. He's one of the most awesome open regaris. Outgoing members of our team came up to me and said, dude, I have this new group, man. They have this new project and I'm like, bro, I have seen it all, you know, he's like, no, no, it's really good. And I said, you know, is it better than Jayz coin? He's like, it's so much better than JZ coin. He's like, these guys have done this proof of, I don't know, proof of history stuff.
00:00:24.990 - 00:00:32.928, Speaker A: And I was like, all right, okay, find, send me the white paper. And he sent me the white paper to loom and I read it and.
00:00:32.936 - 00:00:36.564, Speaker B: I was like, holy moly, this is good.
00:00:37.184 - 00:01:13.274, Speaker A: And I really enjoyed reading the white papers loom because unlike many white papers, I think they really made an effort to come up with a new idea which could go either which way, you know, it could go up or it could go wrong, or bitcoin core could merge it in, or ethereum could merge it in. So I like the science and I said, we have to have it presented. So we don't have usually a lot of non sponsored presentations. This is the first non sponsored presentation that we have for this group and I think it rises to the level of that. So with that introduction, Anatoly, please come up and tell us all about.
00:01:18.574 - 00:01:49.534, Speaker B: That was a wonderful introduction. So let's see a bit about myself. I'm an engineer, spend most of my career at Qualcomm there. Worked on operating systems. If you guys are old enough to ever held a flip phone, those like a CDMA flip phone or like a Motorola razor, they ran a software called roof. So I was a core kernel developer by object size. I wrote more than half of the kernel because they wrote the compiler that generated a bunch of the code.
00:01:49.534 - 00:02:28.274, Speaker B: So after that project, they ended up working on these dsps that are on our processor. And these things have a. So a modern mobile chip has a bunch of different components all shrunk to the same die. And if you think of it as a distributed system, how we solve our problems there is by timestamping everything. So you have a by performance pulp tower tolerance operating system or blockchain. And it's based on kind of the core component is called proof of history. And that is our way of encoding time as data and actually like passage of time as data.
00:02:28.274 - 00:03:25.684, Speaker B: And how it works is actually fairly simple. So if you guys are familiar with what a hash function is, it takes a bunch of data and then generates a piece of data that's the same size. So it can be 256 bits, could be 512 bits. But the property of a cryptographic, or more specifically a PMT resistant hash function, is that you can't predict what the output is going to be when you run it. So if we take this function and we loop it over itself, so its next input is the previous output, and kind of run it over and over, it's a process that you can't parallelize because you don't know what the value is going to be. A million iterations from now, you actually have to spend some actual time running this thing a million times. So if we sample this process as it's running, and we record the counter in the current state, we get these ticks.
00:03:25.684 - 00:04:13.876, Speaker B: So if you look at this data structure, you can actually infer that time passed somewhere, somebody to generate it. And that's kind of like the crux of this proof. So we can use this data structure also to kind of give ourselves this reconsistency of events that are happening outside. By that I mean is that like when I examine the data structure, I can look at it and I know that these two events happen like within half a second from each other without me actually having to observe. And the way we do this is we take some blob of data and we append it to this hash function as it's running to its state. And then record the counter and what we appended. So that could be in itself a hash of the data that you're actually recording.
00:04:13.876 - 00:04:37.796, Speaker B: But now our data structure contains these ticks and some data that represents an event that occurred sometime. And we know that the hash that's generated right after that occurred after this event that was created. You guys are following me. Do you want me to go like upper or deeper?
00:04:37.900 - 00:04:38.460, Speaker C: Deeper.
00:04:38.532 - 00:05:23.462, Speaker B: Deeper. So what's cool about all this is that while it takes real time to generate, if we take each one of these ticks from start to the end, we can verify each one in parallel on a separate core. So a modern GPU card has about 4000 cores. So if you have a second and 4000 ticks, you can verify those 4000 slices in parallel. And it should take about a quarter millisecond, plus or minus the difference of the CPU running times on the GPU and the CPU. So that means we have this like nice asymmetry between generation, which is is real time, and verification, which is parallelizable. And GPU's are fairly cheap these days.
00:05:23.462 - 00:06:21.994, Speaker B: And Moore's law is working in our favor, because you can add more cores, but it's very hard to get a core that's faster. So we can also make this data structure self consistent from an external user's point of view. And how we do that is me as a user that's using the service. I can take a hash as part of this historical record, record it in my data set that I'm signing, sign it, and then take that signed blob and insert it into the, into the next set, into the next hash as it's being generated. So now we have this like, stream of data that has signed blobs, which point back in time to a hash that was generated before. So now, like an untrusted generator can't reverse the order of any of these events because they would change all the hashes and then they won't appear in the previous record. So now these events have an upper bound and a lower bound of when they occur.
00:06:21.994 - 00:07:17.458, Speaker B: So we only need one core in the world to run this, and we can potentially record every event that's happening in the world with cryptographic certainty. So, like 100 years from now, you can look back in history and know that, like, this event happened now, and this event happened like 2 seconds from now. There's something really cool about that. And this is why, like, what's driving me to build this, because we can now have history that is more certain than we had history before. So some nice properties of this is we can take multiple clocks and or looms, and we can start sending hashes between themselves. So now in our record, if you have two of these generators, there's a synchronization point between them. Again, if you imagine this is like a science fiction project, you can have one on Mars and one on earth.
00:07:17.458 - 00:08:04.934, Speaker B: And if they start synchronizing now, when you merge those two records, you kind of have some cryptographic certainty of when the relative order of events occurred between stuff that's happening 24 minutes away from now on Mars. So this record, it doesn't do anything with consensus. And consensus is kind of like the problem that most people talk about in blockchain. This is simply just read consistency. So it's a self consistent order of events. So anyone can potentially generate a separate order and a separate history that's going to be completely different. So consensus comes in and selecting a unique history for that, everyone in the network agrees that that's the one we're actually using.
00:08:04.934 - 00:09:05.770, Speaker B: So when you combine this with like proof of stake, you can actually get a blockchain where consensus occurs over a historical record which can encode as many transactions or changes as like network bandwidth allows. So now we've separated the transactional throughput from consensus, and that's kind of like the key part of how we can scale to as many transactions per second as possible in the network. So what's cool about this is we can actually build a proof of stake system with high availability. You guys know what cap theorem is? You have to pick either consistent or availability. We can actually pick availability with some human bounds. And that means that when there's a network partition that we have enough human time to figure out which one we're actually going to pick with. So imagine this is kind of like what our network looks like.
00:09:05.770 - 00:10:17.308, Speaker B: There's a single generator and a bunch of replicator nodes that are replicating the state. Yellowstone blows up. And now we have two networks that are separated. So the smaller network and a larger network all start forking on a different set of histories, right? But because the smaller network has fewer verifiers that are verifying with their consensus, their proof of stake based verifications, it recovers much slower and this one can recover much faster. So now those differences in time is what gives us humans like time to figure out, okay, the smaller one is just like a faulty Internet cable, and we're not going to actually commit to using it because we know that the larger one is still there and it's going to recover much faster. So I'm not going to trust the smaller one for my transactions. So if you want to visualize that it kind of looks like that a bunch of transactions that are happening and there's a partition and one of the partitions has 1% of the verifiers left and the other one is 99% of the verifiers left because those 99% are present.
00:10:17.308 - 00:11:24.004, Speaker B: The state machine can then infer that 1% can be unstaked very quickly. And in the other partition, because the 99% are absent, it's going to take a very long time, non linear human frames like, you know, months potentially, to unstake the full 99. So we can actually take this data structure and recover it from every point. And that's something that I think might be unique to this particular proof of stake implementation like bitcoin. If you think of it as leisure, all the computers in the world can blow up and we can start doing the ledger by hand and solving the hashing problem by hand. And that's still a valid blockchain because we are continuing the protocol, right? If, if you had a simple proof of stake system like tendermin and half of the network died, you would have to effectively hard fork and figure out which one, which side of the network is actually valid. So with us, instead of doing a hard work hard fork, the generator actually has to prove that months of time passed and that there was really an absence.
00:11:24.004 - 00:12:25.460, Speaker B: And then when you look at this ledger and you compare them, you actually have availability encoded into the ledger. So when you have to figure out which fork is the right one, the one with over 50% verifiers is the right one. So we kind of have this like true height of the blockchain encoded into it. So if the, this is kind of like how the message flow from the verifiers and the generator. So one of the problems with doing like scaling this thing is that the amount of network bandwidth we have in any single point in the network is. So the total network bandwidth of how many transactions we can do is going to be limited by the smallest link in that network. So if you have one gigabit connection, every node in that graph can only process one gigabit.
00:12:25.460 - 00:13:37.468, Speaker B: So you can only do one gigabit total for the entire network, even if you have like 10,000 computers. How we can kind of mitigate some of the effects of this is by forcing the verifier. So in the initial round, if you guys look at this, some of the messages flow to the first step, and then we can do something, what bittorrent does, which is start banding out the bandwidth. So how that works is we split this bandwidth by one over n. Then this first layer of nodes can put all the data together, and now you have three times as much bandwidth. So now to serve a global network for payments, we can actually quickly fan out or multiply the amount of bandwidth available for the entire ledger for everyone to observe, using very similar methods to Brian Cohen's Bittorrent. So how we're using this proof of history thing, so we can force very quick finality and we can force the verifiers to actually do their work within a certain amount of time.
00:13:37.468 - 00:14:29.696, Speaker B: And if they don't, they simply don't get credit, because every node in the network can trust this like historical record of events without actually observing it. After a bunch of transactions, there's some still proof of history stream that's being generated. We start seeing verification events, and if they all arrive within half a second or 500 milliseconds, and they get a reward. If they don't, then they don't get a reward. So everyone that is in the network that sees this data structure, they can trust this order of events without actually having to be in the loop of observing these messages. You guys following me? Cool. So one of the problems with having a centralized kind of central generator is censorship, because this thing is our ingress point, or the point where all the messages in the the world have to be encoded into the stream.
00:14:29.696 - 00:15:28.616, Speaker B: So how do we know that it's not like censoring users? Well, how do you prove censorship? You can't, you can actually only show that there's some packets being dropped. So, because like, I don't know if I sent this packet and it actually died on the network, or this verifier actually decided that they don't like me and they dropped it. So what we can do is the users can actually forward their messages to the verification nodes randomly, and those verification nodes can forward it to the generator and then observe if this packet is actually encoded into the stream or not. And between all the verifiers, they can maintain a median error rate. And if that error rate gets too high, they can simply kick that generator off and go to the next one. And it doesn't really matter who is continually creating this data structure, because the data structure itself is our social truth. It's not tied to any particular identity.
00:15:28.616 - 00:16:51.718, Speaker B: We only really trust ledger and the data as it's encoded. So again, this solves some of the censorship problems. The other problems I think we would have to solve in like a higher levels, which are more like using CK snarks and more privacy related cryptographic applications, as well as changing the behaviors of the user. So because users can create a million wallets with a million private, you know, a million identities, and they can randomly, like when I have to pay somebody a million dollars, I don't have to pay a single transaction of a million dollars, I can pay 1000 transactions with random amounts from random wallets. And because we're like running at the throughput of just network speed, our transaction costs are ridiculously cheap. It's literally the cost of updating memory, right? And that should be like ten to the, according to my calculations using AWS, it's about ten to the minus $6 per transaction, right? So like, if you look at like McKinsey reports about payments, there's what, $40 trillion worth of volume and 2 trillion is how much we pay in payments. It's 5% tax in the world economy.
00:16:51.718 - 00:17:55.590, Speaker B: And it's absurd that we have to spend that much to transfer numbers around in the computer system, right? It should be really the cost of updating memory. In my view, that's just a huge aggressive tax on the world economy. So let's see. So, interesting thing about this is that often comes up as a question, what if somebody had a really fast computer and they could outrun the system? So let's go back to the slide. So what would happen is the generator could potentially take this 500 milliseconds and actually wait 250 milliseconds and then generate 500 milliseconds worth of proof of history proofs and reorder messages. So if they had like a verifier that was in evidence for them, they could encode their message earlier and they would get credit. That basically devolves in a system called delegated proof of stake.
00:17:55.590 - 00:18:36.774, Speaker B: So there isn't an opportunity to create a double spread, but it's an opportunity for somebody with crappier network to kind of get credit when they shouldn't. And how we solve that is by effectively having the fastest ASIC. So a modern day AMD risen core can do sha 256 round five cycles. It's actually, I think, very hard to get much faster than, than that. Because if you guys look at this green path, it's pretty long. So typically like a modern cpu will have all of these stages, pipelines. So every adder is in a separate clock.
00:18:36.774 - 00:19:30.374, Speaker B: And if you do some special instructions, you can kind of combine things. But if you try to do this whole thing in a single clock, your clock is going to get very slow, like it's going to be much longer clock speed. So unrolling this and kind of stringing it together will quickly get diminishing returns. So the first person to roll the SESIG will probably get 99% of the way to all the optimizations you can do. And we were probably going to do that because I spent twelve years at Qualcomm and I think hardware is fun. The cooler thing about rolling our own ASIC is we can build a board that can handle like a 24/7 liquid nitrogen cooler, which should give you about 30% boost, and that would be awesome. And that's cheap enough to run like actually in a real network.
00:19:30.374 - 00:19:35.734, Speaker B: So that's basically it. You guys have questions.
00:19:44.194 - 00:19:45.934, Speaker A: So we can get it recorded.
00:20:03.134 - 00:20:13.444, Speaker B: Brian Cohen, more technical. Was the cheat out Marquez more technical? I think he was, yeah, a little bit. Somebody else presenting that, they proceed.
00:20:17.624 - 00:20:18.896, Speaker A: Correct me if I'm wrong, but you're.
00:20:18.920 - 00:20:24.004, Speaker B: Saying you'll build this not for a mining reward, but just for the overall health of this network?
00:20:24.704 - 00:20:26.644, Speaker A: Yeah, there's no mining reward.
00:20:28.704 - 00:21:17.924, Speaker B: So the question was, is there a mining reward based on the speed of the clock? There's not, but the network would say set a minimum clock speed. So if you can't keep up, you would get kicked out, effectively, you wouldn't be able to keep up with the. You wouldn't be able to be selected as a generator. So can you talk a little bit about distribution of tokens? Yeah, there's no tokens, there's no security. What? But some economics play in part proof of stake system. We can actually use proof of work. It doesn't really matter what these verifications are, as long as there's some Sybil resistant way to measure votes.
00:21:17.924 - 00:22:02.270, Speaker B: If you guys don't know what the Sybil attack is, is spoofing identities, like, you know, Internet voting. I got like a million likes. You know, was that really a million people or just two people with a bunch of bots? So how, like, bitcoin does it is you buy a bunch of asics with separate cores and they mine. They spend all this time and electricity generating random numbers to figure out a puzzle. So the person that solves that puzzle first gets some credit, but because of the hardness of the puzzle, we can kind of infer how much electricity went into it. And that's your way of proving identity. You're basically voting with electricity with a proof of stake system.
00:22:02.270 - 00:22:49.264, Speaker B: You're taking some of the tokens in the network and you're saying you're not going to spend them and they're going to keep them in a separate account, and you're going to vote with those. So it's based on, like, kind of cryptographically proven identities secured by your private keys. It's in a way, weirdly recursive, because you're using the network's accounting system to vote for its own accounting system. But so in our proof of stake system, the verifiers would get the fees right and part of the mining return, and there would be some way to distribute that. Those kind of token problems we haven't simulated yet. That's kind of. That's more of the step to part.
00:22:49.264 - 00:22:53.404, Speaker B: There's a lot of ways I could go wrong.
00:22:57.784 - 00:23:16.548, Speaker D: So you mentioned that you have, like. Or you can construct this mechanism for verifiers to build consensus that, like, a generator is censoring information. Is there a way to detect cases where a generator is reordering the events relative to what event creators you're generating it?
00:23:16.596 - 00:24:01.324, Speaker B: Sure. So, like, so the generator will have some opportunity to reorder events, which is the gap between the subserved, the last thing that the user signed, and when they're entered, there's no way to really detect that. So effectively those packets are in flight. So for most transactions, like paying for coffee doesn't really matter, but for things like maybe order book settlement and the mixture exchange, that might matter. So those could be solved with use case specific solutions. For example, for an exchange, when you have a bunch of orders, you can actually decide the final order based on the random number generated in the future. So there's interesting approaches to solve problems.
00:24:01.324 - 00:24:27.464, Speaker B: Or users could actually change their behavior and generate many different small orders because it's so cheap to insert them. So then this thing has really kind of no way to figure out what the market action is happening. Please go. Can you explain again how conflicts are resolved?
00:24:27.504 - 00:24:29.224, Speaker C: Or you're saying that I'm not conflict?
00:24:29.344 - 00:25:23.034, Speaker B: So conflicts in the sense if there is a partition. So, so let's say, right, there's a partition occurs here, and maybe somebody in the network is still connected, they would see two different historical records, and their civil resistant identity has to vote for one or the other. So the conflict resolution occurs by means of voting in the consensus scheme. Right? So you have two different historical records. You as the verifier have to pick one, and you can't vote on both. And if you vote on both, then you can use that proof. If somebody sees you voting on both, they can submit your double vote on the other ledger and slash your bond.
00:25:23.034 - 00:26:45.474, Speaker B: If that makes sense. It's this process called slashing, which was introduced by Ethereum, probably before Ethereum, but they kind of maybe made it famous in their Casper proposal. Does that make sense? So if there's no, like two valid, you can have as many valid examples. Sure. So, for example, we had a. Where's our petition? We have two historical records, right? The verifiers have to pick one or the other, and when they pick, they take the last observed cash that they've seen, they sign that transaction, and they sign it with their identity, which represents some of their voting power, and they submit it into the historical record. So now they have to pick one or the other, because if they try to pick both, then anyone that presents their signed message of picking two different historical records can then present it to a historical record and cause their bonds to be slashed, which means that their voting power gets removed from the system, and that's a loss of capital.
00:26:45.474 - 00:27:25.524, Speaker B: So it's as if you're like you're hashing on both bitcoin cash and bitcoin at the same time. But then because you're doing that, your asics burn up because you can't do both. Right. You have to pick one or the other. So this is like a softer way of emulating them. So we don't like, effectively, if there's a partition, the one that gets to two thirds of verifiers is the true one, the other one gets dropped. And you as a client, you can observe these verification messages and effectively see how much consensus has been achieved.
00:27:25.524 - 00:27:27.032, Speaker B: Go ahead.
00:27:27.128 - 00:27:38.886, Speaker C: Right. How do you define which partition is going to be the dominant one? What if you have, you know, right.
00:27:38.910 - 00:27:50.874, Speaker B: In the middle, a random process? Yeah. So the verifier sees like, oh, both of these are like, near even. I'll just pick one. I'll pick one.
00:27:51.614 - 00:27:52.790, Speaker E: And it doesn't matter which one.
00:27:52.822 - 00:28:48.848, Speaker B: It doesn't matter which one, because ultimately, like me, as a capacity of the one that's invented, so those transactions effectively get dropped. And you, as, like a seller, you wouldn't, like, ship the product until you see two thirds consensus, right? So you effectively, like, you submit something as Yellowstone blows up, and you don't see two thirds consensus, you're kind of stuck waiting for that to occur. So in this particular case, let's say, like 30% of the network dropped or 34% of the network dropped, but that's like, that could occur, you know, because aws could go down. So that would cause the network to be in the state where we can still accept transactions. And you will only see less than two thirds consensus appear. It's up to you to decide whether you want to give this person an item or not. But you can then go and look what actually happened.
00:28:48.848 - 00:29:36.884, Speaker B: Is this like an actual event that matters or is this like a non event? And if this is like an event where those computers are actually gone over time, as these proofs are generated, those missing bonds would get unstaked and the network would gain two thirds percent over two, three percentage. So if it's 1%, it happens quickly. If it's near a third, it'll happen, like in hours, right? If it's more than a half, it'll take days. If it's near zero, it'll take months. So we can kind of encode this into the protocol after doing a bunch of simulations, figure out, okay, in human timeframes, if 1% goes away, it doesn't matter. If it's 20%, that's actually an event we should care about. And we should maybe spend an hour waiting for consensus again.
00:29:37.544 - 00:29:49.736, Speaker E: So if you have two networks, they're split. Eventually they're going to merge back together again because I don't want to lose my transactions because I was on the wrong side of Yosemite forever. So east coast, west coast, east coast is trash.
00:29:49.800 - 00:29:52.910, Speaker B: If there's a warm, and you said.
00:29:52.942 - 00:29:59.398, Speaker E: Earlier Mars versus verse earth, that would be separated networks, I assume.
00:29:59.446 - 00:30:01.214, Speaker B: Yes, those will be separate.
00:30:01.254 - 00:30:05.194, Speaker E: How are they going to sync up so they can, or is Mars just fucked.
00:30:07.454 - 00:30:19.082, Speaker B: Up? So those would effectively be separate networks with separate accounting systems, but they could still sync up and see relative order of events for things like maybe an exchange where you're trading things.
00:30:19.138 - 00:30:22.786, Speaker E: So they're not gonna be one combined chain.
00:30:22.890 - 00:30:27.466, Speaker B: I'm not, I don't know how to do that. Well, if you do.
00:30:27.570 - 00:30:29.050, Speaker E: No, I have no idea how to do it.
00:30:29.082 - 00:30:29.814, Speaker C: I just.
00:30:32.074 - 00:30:33.934, Speaker E: I thought that's what you were suggesting.
00:30:43.924 - 00:31:11.724, Speaker B: A 20% drop in availability would be, I think, very infrequent because that means that like if we have a thousand computers, 200 of them randomly in the world disappeared. I would say that's a fairly interesting probability about like given our Internet infrastructure now and as it's improving, like availability, availability is something we count on like constantly, right? Like my watch never gets disconnected.
00:31:12.264 - 00:31:24.044, Speaker F: Do they still make you sync up later on? Otherwise they're going to like eventually 200 goes away and after that, like another 200 goes away and slowly going to diminish down to zero.
00:31:25.824 - 00:32:19.184, Speaker B: Well typically it's random, right? For the most part for like unrelated events, right? There's some random number, computers are going to be down and when it's a small percentage then we can remove them from the voting pool quickly if it's a large percentage. Unless there's a war, Internet cables are getting cut. We can actually like start to think about how to repair those faults and like bring those networks back together. But clients that are participating in the network can ignore the smaller one because they know that the bigger one's a little. But if the, if Yellowstone actually did bit blow up and we were only left with 20% of the rarefiers, we can still continue processing. And us as humans that are like deciding to use the system can pick the 20% one and actually stick to it and eventually recover and all those transactions will become truly finalized.
00:32:22.364 - 00:32:25.484, Speaker F: If they wanted to sync up later on, they would reset their whole state.
00:32:25.524 - 00:33:03.014, Speaker B: And then start over with all those. Clients would have to resubmit their transactions because of this property here where any of the previous, any of the last caches have to be present in the historical record. Right. So if there's a partition, any new transactions out of there will be invalid on the other one. And that's also a defense against a falsified ledger. Because when I am presented in a ledger as a client that can look in its historical record and see if the hundred or whatever transactions they've done over the past six months are there. Go ahead.
00:33:03.014 - 00:33:06.818, Speaker B: So you mentioned that you wanted to.
00:33:06.826 - 00:33:17.174, Speaker C: Run your own acid of time. And I'm sure that the hardware is going to cost money.
00:33:19.254 - 00:33:26.982, Speaker B: How much of $1 million to roll your own ASIC? It's about a million for the first one. For the first one.
00:33:27.118 - 00:33:34.154, Speaker C: So do you think there's going to be a centralization of mining power? And do you see it as an industry?
00:33:34.574 - 00:34:10.260, Speaker B: No, because like, that's just for like version one. Once the pipeline is established, it's fairly cheap to version one and to go to the next fabrication level. So you're going to. Once you design the ASIC and you roll it, improvements in the fabrication process are not free, but they're much cheaper for you to leverage. You still have to do some tweaks in the routing and stuff, depending on what changes they make. So it wouldn't cost us a million dollars. And it's not a million dollars that everyone in the network has to pay, it's just some group of people can collectively decide, here's an open source project.
00:34:10.260 - 00:34:19.843, Speaker B: Let's go. Cofounded ship the SESIg, build a board that can do 24/7 liquid nitrogen cooling. And we have the fastest possible way of measuring cryptographic time.
00:34:23.583 - 00:34:29.023, Speaker C: Chia's whole point becomes as distributed as possible.
00:34:29.143 - 00:34:44.184, Speaker B: Yeah. So here in a proof of stake system, it's as distributed as people have as they want it to be. Right? It's just keys, there's no storage or anything required. Go ahead.
00:34:44.804 - 00:34:48.164, Speaker C: So you have to store array, single hash, as you mentioned?
00:34:48.284 - 00:34:48.984, Speaker B: Yeah.
00:34:50.004 - 00:35:00.332, Speaker C: And potentially you generate hashes on top of hashes. So isn't it prohibitive in terms of storage? And doesn't it get worse as hardware improves?
00:35:00.508 - 00:35:38.044, Speaker B: So now, because we can see, so we're sampling the stream, so we don't have to store every hash, we can just sample it every quarter millisecond or something like that. So we can sample like every 300,000 hashes. And when things get faster, every 3 million hashes. But storage is a problem in any blockchain that claims scale. So 25,000 transactions per second is about a petabyte of data a year. So any blockchain claim scale has to solve storage. I don't know if you guys are familiar with Filepoine's proof of storage, so I wish I should have added slides on that.
00:35:38.044 - 00:36:30.994, Speaker B: But that's an interesting problem, because how you verify that a particular chunk of memory stored is a user encrypts it, and that encrypted data can then be used as your proof. So every unique key that's used for encryption can then be verified by me as the verifier can take the data set and encrypt it and then compare the proof that the so called replicator generated. The problem with kind of doing that is that the replicator can start encrypting the data and with many identities and deleting the stuff behind them as you're encrypting it. So every proof can be streamed. So Filecoin's proposal is to reorder the blocks every time. Every time you're generating a proof. That means that me, as a verifier, I have to have the entire data set available to me.
00:36:30.994 - 00:37:12.774, Speaker B: For every replicator identity, I have to encrypt it, right? The entire block. And so for every replicator identity, I have to have the exact same amount of space as the actual thing I'm trying to store. Does that make any sense to anyone? Temporary space, right? Yeah, yeah, temporary space. But if I'm verifying, like, a million identities that are all stored a terabyte, I need, over some amount of time, access to that much storage. Right. So for us, we can actually leverage our proof of time thing. Because while you can stream, you can still encode the data in a single order.
00:37:12.774 - 00:37:53.474, Speaker B: We can force you to sample every block, let's say, one byte out of every megabyte block. And you do this faster than it takes to encrypt. And you submit these proofs to this proof of history stream. We then know that you have had all the blocks available to you. And after about six samples, the probability of you deleting a single byte in every block drops to, like, ten to the minus 610, to the minus four, some like that. So we can actually do very cheap to verify proofs of replication. So our solution to the storage problem of 25,000 transactions per second a year is using this to build a storage network that stores the ledger itself.
00:37:53.474 - 00:38:32.874, Speaker B: So all of your kind of historical record can now be in this distributed storage network that's using these cheap to verify proofs of replication to guarantee availability of that. You can't use those proofs for consensus because I could dump, like, a bunch of storage in the network. Even if it's valid, the verifiers would run out of CUDA cores to verify all of them. So effectively our network can give an economic incentive for you to provide storage and kind of get earned credits for it. But it's not used for a consensus mechanism. Make sense?
00:38:36.134 - 00:38:38.034, Speaker C: Can you go back to the Yellowstone slide?
00:38:41.054 - 00:38:45.554, Speaker B: Yeah, yeah, I love this. And then go work forwards. So.
00:38:48.094 - 00:39:11.124, Speaker G: The generators and the verifiers have vastly different hardware requirements because the generator needs to be like super fast single core and their verifiers need to be like a bunch of parallel cores. So how does one get spontaneously promoted to being a generator? Is there like an election process? And isn't that Lume B generator going to be like totally incompetent at being a generator because it's not a tuned ASIC?
00:39:11.744 - 00:39:46.434, Speaker B: So those, if we roll around ASIC, those ASICs would be cheaply available because the cost is just upfront cost to do fabrication. But then the cost per chip is like negligible so you can give them out for free. And I don't even know if it'll be faster than AMD's and Intel's like specialized chapter 56 instructions because anything we enroll we have to lower the clock speed. So the only real benefit is maybe making it easier to pull. So like there's, again, it's not even an attack that would allow you to double spend.
