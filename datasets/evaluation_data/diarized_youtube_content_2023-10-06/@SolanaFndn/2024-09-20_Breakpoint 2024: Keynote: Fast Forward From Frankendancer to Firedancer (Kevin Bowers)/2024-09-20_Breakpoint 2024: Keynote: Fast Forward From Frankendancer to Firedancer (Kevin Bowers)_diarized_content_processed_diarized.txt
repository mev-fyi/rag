00:00:14.200 - 00:00:36.665, Speaker A: A lot of lives. Okay, they already announced. I'm Kevin Bowers, chief scientist of the Jump Trading Group, but it's been two years since I presented on the Breakpoint main stage. Or I guess the left stage nowadays. Thanks for having me back. The title for today's talk is Fast Forward from Frankendancer to Fire Dancer. This talk summarizes the last two years of work by the Fire Dancer team.
00:00:36.665 - 00:01:13.435, Speaker A: The team is now quite large, so large I don't have space for all of them on this slide. You can find many of them here by their colorful shirts. We unfortunately don't get many opportunities to talk to a broad audience and we have a lot to say. To that end, we have several events this week at this conference and its siblings. These other events go deeper than we'll have time for in this talk due to the usual Tetris like scheduling challenges. You might have already seen Michael and Philip's hello World talk at Block Zero. They've given me one hell of a tough act to follow.
00:01:13.435 - 00:01:59.635, Speaker A: Let's start with a snapshot of what we said at breakpoint 22. What is fire Dancer? Fire Dancer is a new Solana Validator in development in the open at the above link, we all know the importance to the community's long term health of having multiple documented, standardized, robust and performant validators. We were honored to get this opportunity. It's like other projects we've done, but with several novel technical aspects. We thought it would be a challenge and we were flattered that people thought we were up for it. This graph of commits versus time shows that since then we've been continuously doing the work and at an accelerating pace. We've been pleasantly surprised at the level of attention that this work has received.
00:01:59.635 - 00:02:42.989, Speaker A: To get some idea of what all those commits are about, here is a slide from Breakpoint 23. A CPU nowadays is akin to many single core processors distributed over a local area network, all integrated onto a single microchip. And memory nowadays is akin to a remote file system distributed over a wide area network. High performance computing is all about streamlining data flow over physical networks and its networks all the way down. Unfortunately, there is a lot of magical thinking about computers these days. This slide is from one of our internal tech deep dives. The way people have been taught to think about hardware and behave when implementing software is detached from reality.
00:02:42.989 - 00:03:17.745, Speaker A: I don't need to read fiction anymore. I just read the computer science literature. There's more magical thinking in CS today than there is in the most outlandish fantasy. When the Fire Dancer effort started. Much of my role was organizing various wish lists into a realistic development plan with measurable deliverables targeting practical computers. Let's look at some oversimplified wish lists. First, the foundation's top priorities were an independent developed second validator and fast operational recovery from faults.
00:03:17.745 - 00:03:58.815, Speaker A: Though Anza had not yet separated at the time, the second wish list I've characterized as theirs. They've been the most vocal with us about supporting a worldwide community with a 50 billion account capacity stretch goal. The next wish list I characterize as the operator communities, but it is universal. If one of our production systems goes down, the revenue it generates stops, but the associated costs continue like nothing happened. We care deeply that our systems stay up and the costs stay down. This exact dynamic applies to the operator community. They care deeply, that chain stays up and their costs stay down.
00:03:58.815 - 00:04:56.081, Speaker A: The last wish list I've characterized as ours, but it is implied by the other wish lists. Consider a hypothetical protocol with 50 billion accounts and suppose a protocol implementation used an off the shelf single threaded hashing library that could verify accounts at a seemingly fast 100,000 accounts per second at startup. Restarting the chain would take almost a week. So it looks like we need high speed parallel cryptography for high availability. Now consider now if that implementation had a 100 transactions per second capacity when operating on a handful of cores, just touching each account once would take over 15 years. Looks like we need some high speed transaction execution to get high scale. And compensating for this low execution capacity by just naively increasing your core count is prohibitively expensive.
00:04:56.081 - 00:06:00.955, Speaker A: So it also looks like we need the high speed cores for efficiency. Now high speed is all about streamlining data flow, and data flow is the first thing that is frozen in an architecture. If speed isn't considered upfront, the only way to significantly improve it later is to throw everything away and start from scratch. One of the reasons for our interest in foundation is they groked this day one they were already the most performant chain and then by a wide margin. Our technical specialty is distributed computing at speeds people don't think is possible, much less worthwhile. But contrary to conventional wisdom, this is not a weird affectation as described at breakpoint 22 exchange matching rules require us to make high performance distributed systems just to survive in ultra competitive financial markets. We've always thought our role is to push the protocol to the limits of physics and information theory, and we have been pleasantly surprised at how much of our performance expertise has been relevant to the availability, scalability and efficiency wishes.
00:06:00.955 - 00:06:41.935, Speaker A: Now we turned these wish lists into a plan where we incrementally replaced Agave components, documenting and optimizing along the way. There were three high level technical components corresponding to the data flow of a transaction through a validator when it is the leader. This data flow was very familiar. There's a lot of similarity between a trading system and a validator. We already knew how to make a robust fast trading system from commodity and custom software, hardware and physical networks. We believed we could apply this experience to making a new validator. Our development plan was aligned with our development philosophy.
00:06:41.935 - 00:07:26.683, Speaker A: Recall the computer science decision paralysis described at Breakpoint 22 and in subsequent talks and podcasts. When nothing is optimized, AMDL bottlenecks means there's no immediate payoff to anyone optimization. Also recall Amdel's street justice when speed of light constraints meet power and heat constraints. Dedicated compute in the right location is compute that can't be used for other calculations because it is either too far away or too specialized. To do more than scratch the surface of what's possible, we must streamline data flow and then optimize everything along the path. This led to the concept of a Frankendancer. As we replaced Agave components, we would have an increasingly complete hybrid validator.
00:07:26.683 - 00:07:59.101, Speaker A: We thought that this would have many practical benefits. Agave could continue to evolve independently. We would naturally keep in sync with protocol updates. The hybrid validator would be composed of documented and standardized modules that others could replace to make their own validators and not fully appreciated at the time. We would get community production and release experience before the full Fire Dancer was ready. That was the plan as of about two years ago. To our pleasant surprise, we did not need to deviate much from this plan.
00:07:59.101 - 00:08:36.675, Speaker A: The main high level difference is we internally came to think of component 2 as a complete non voting validator and component 3 as a validator mature enough to participate in consensus. This is largely because components two and three were tightly coupled and worked on by the same people concurrently. Component one concerns performance and scale. It is easy to show benchmarks for these. Components two and three concern correctness and compatibility. These don't have obvious benchmarks, so it is easy to underestimate their difficulty. Let's look at some of the challenges.
00:08:36.675 - 00:09:30.589, Speaker A: One challenge was replicating implicitly specified behaviors. We reverse engineered and documented validator behaviors, including ones that weren't explicitly known but nevertheless required for consensus. These implicitly specified behaviors don't matter in a single client single version world, but need to be eliminated to make a multi client multi version world possible. Further, since Fire Dancer is written in a different language than Agave, validator developers can compare implementations. Rosetta Stone like to understand what is truly required for compatibility. A second challenge was there was no way to tell who's right was an issue with Fire Dancer, Agave or the protocol. We open sourced an extensible test suite programmatically generated from tens of thousands of real and synthetic mainnet and testnet transactions.
00:09:30.589 - 00:10:08.825, Speaker A: We also open sourced differential fuzzing harnesses. These go beyond the typical fuzzing frameworks which focus on detecting memory corruptions and invariant violations to establish a compatibility ground truth. Critically, these testing tools are validator agnostic. They don't care about the validator implementation details. A third challenge was replicating a complex runtime. We requested simplifications to move toward a world where most runtime logic is on chain. We also provided feedback on proposed protocol changes with the goal of making the runtime a thin shell around a streamlined virtual machine.
00:10:08.825 - 00:10:51.741, Speaker A: And yet another challenge was the protocol change proposals assumed a single validator world. We worked with Anza and foundation to improve SIMD's the protocol change process. Like compatibility testing, proposed improvements should be validator agnostic. That is they should be rigorously specified without reference to validator implementation details. Since we've open sourced all of this, developers who want to make their own validators or tune existing validators face much less complexity. They can benefit from our pathfinding, reverse engineering, testing, fuzzing documentation, security audits and so forth. In short, it is very hard to go from one validator to two.
00:10:51.741 - 00:11:38.199, Speaker A: It is much easier to go from two to many. Now with that understanding, let's get into a little more detail what we've done since Breakpoint 22. We announced the fire dancer effort mid August 22nd at Breakpoint in early November, we live demoed a signature verification deduplication block packing pipeline with a 1.2 million transactions per second ingress capacity and a 0.6 million signature verifications per second capacity on an old stock GCP host. We also demoed A hardware accelerated SHA512 packet hashing pipeline with a roughly 100 gigabits per second capacity on an old stock AWS F1FPGA. This was the first stage of a hardware accelerated signature verification pipeline.
00:11:38.199 - 00:12:37.055, Speaker A: At the time we thought it would take about two years to get to a new validator. Then in early January 23rd we live demoed that pipeline stitched into Agave, creating a proto Frankendancer. The protocol evolved to use QUIC for incoming transactions. We implemented and then live demoed our own high performance QUIC implementation integrated into the Proto Frankendancer. We next live demoed the foundation and Anza a high performance turbine implementation integrated into the Proto Frankendancer. By this time hardware accelerated ED255 19 signature verification was functional and in late June we live demoed to anza and foundation a 1 million signature verifications per second at lower cost and lower power than CPUs or GPUs on the same old stock AWS F1 FPGA from before. We also demoed 8 FPGAs in a host to scale signature verification capacity to roughly 8 million signature verifications per second per host.
00:12:37.055 - 00:13:31.265, Speaker A: Not too long after external partners started experimenting with Frankendancer, Frankendancer on Testnet was announced at Breakpoint 23 and we presented several related general innovations. This includes data parallelism techniques for elliptic curve cryptography. This reduced the core footprint required for the software signature verifications shown previously by roughly a factor of 2. Algorithmic and implementation improvements for Reed Solomon coding. This sped up the block distribution calculations by roughly an order of magnitude and sandboxing to isolate validator internals for enhanced security without sacrificing performance. We were working on Component two and three all this time and getting back to the grind after Breakpoint. We did our first component 2 live demo faster than real time Transaction Replay to Foundation and Anza in January 2024.
00:13:31.265 - 00:14:43.815, Speaker A: Sprinkled throughout this we had technical off sites with Validator developers at the Mountain Dao off site in Salt Lake City. We live demo Transaction Replay running on Asaga and Pixel phones. Then in early April we live demoed to Anzen foundation our Gossip protocol implementation, interoperating with Testnet and Mainnet and scaling to handle over 10 gigabits per second of incoming gossip to show ComponentOne's physical networking capacity. In early May we took Frankendancer to the racetrack and live demoed to foundation and Anza roughly 100 modified Frankendancers running on commodity hosts distributed over a 25 gigabit per second data center local area network and a 100 gigabit per second intercontinental wide area network. This demo required generating a massive load of simple uncontended transactions and participating in consensus required running these transactions through the Agave runtime. This demo showed a packet ingress in block distribution capacity over 1 million transactions per second. Shortly after, at the Chicago Technical off site, we discussed the necessary protocol runtime and consensus modifications.
00:14:43.815 - 00:15:28.241, Speaker A: We plan to do a lightweight version of this demo and discussion later today and again tomorrow. Given a well engineered physical network, the main bottlenecks at this point is the bandwidth to and from the accounts database. There's much room for improvement here across the entire technical stack. This was followed by yet another live demo to Anzen foundation. We integrated components 1, 2 and 3 and showed a non voting proto fire dancer that is a full non voting full independent validator client running live on testnet and as an addendum to our next demo on mainnet. By this time component two and three had come to fruition. Our next live demo to foundation and ANSA was an alpha version of the full Fire Dancer voting on testnet.
00:15:28.241 - 00:16:15.609, Speaker A: Around then we did a proof of concept port to Mac OS and FreeBSD for code linting and risk reduction. Started polishing the Frankendancers to incorporate feedback received from public external security audits and early testnet adopters. We also announced a Frankendancer audit contest and we've been pleased with the participation and feedback quality. Most recently we live demoed to ANZA and Foundation 10 gigabits per second block packing capacity for unique verified transactions in various scenarios and we just announced a frank condenser bug bounty. Just like a validator needs to catch up from a snapshot at boot, we've caught up with real time. I'm kind of proud this talk is recursive. It's like Charlie Kaufman made a PowerPoint the TLDR.
00:16:15.609 - 00:16:45.565, Speaker A: We've been busy. Now is the point where if I could, I'd fast forward for the prep video spaceballs like to figure out what I'm supposed to do next. Many people suggested I should give a proof of life, though there are dates on the community bulletin board. This is probably not what they were talking about. This is probably what they meant. This is a recent snapshot of software versions running on mainnet. Zooming in, we see a strange looking version number.
00:16:45.565 - 00:17:57.633, Speaker A: The number on the right looks a little like an agave version. The other numbers seem to have something to do with Frankendancer, but as you've already figured out, yes, Frankendancer is live on mainnet, but that's not the only proof of life we have for you. Here's another Fire Dancer is also live on testnet, but we have a bunch of other proof of lives here. And this is a bit of an open secret for many reasons, least of which is that this is not our first talk this week. More importantly, we don't have the luxury of doing a big bang greenfield rollout solo as this proof of life suggests. We've been working with foundation and Anza to roll out Frankendancer and Fire Dancer to early adopters over the last few months and going slow to minimize risk. Notwithstanding some puzzled tweets we've seen about strange software version numbers and anomalously high throughput blocks, this seems to be working.
00:17:57.633 - 00:19:02.915, Speaker A: So far. Our goal is to keep future rollouts similarly uneventful. Most of my role lately has been low level coding, ranting and making slides. My most sophisticated GUIs historically have been ASCII art with colorful terminal text. But as the team has grown to include people with GUI and web skills, we have implemented RPC support for application developers and a lot of work has gone into collecting and visualizing metrics to help operators manage and tune monitor running validators now we didn't want to risk waiting for a random leader slot in the middle of this talk, so we recorded this proof of life earlier this week. This video shows a Frankendancer validator producing four main net blocks and then shows features for monitoring the ecosystem real time. But note that Michael and Philip's hello World talk procured a fresh node from aws, checked out Fire Dancer from the public repo, built a validator from scratch, joined the newly built validator to testnet, fired up the GUI to remotely monitor it and showed it producing testnet blocks.
00:19:02.915 - 00:20:01.175, Speaker A: And they did that all live. And at the end of this talk, time permitting, we will bring up a live view as well. What does it all mean? First, we can use high performance networking to receive raw transactions requests at the limits of the physical network. We can use wide parallelism to get massive signature verification capacity and high performance computing to minimize how many cores are required for that capacity. We can use high performance computing again to schedule verified transactions for execution as concurrent as possible with high throughput. We can use wide parallelism in high performance computing for a balanced transaction execution capacity with minimal cores, and we can use high performance networking to distribute blocks at the limits of the physical network. That is, we have shown that by streamlining data flow and optimizing everything along the path, a validator can hit its ecosystem's physical limits.
00:20:01.175 - 00:21:00.465, Speaker A: Though widely available, commodity hardware can have over a 1 million transactions per second capacity. The surrounding ecosystem needs work to expose it configurable protocol limits currently capacity below what is physically possible. We've talked with Anza and Foundation how to increase the limits safely and more generally to flesh out how feature gates and releases should work in a multi client world. The most expensive real world operations are finding data within and moving data between the account store and transaction execution cores. The cheapest real world operations are transactions compute units, but the transaction compute model currently overweights compute and underweights latency bandwidth and storage. This incentivizes applications to generate transactions that spike operator costs. Aligning the transaction compute model with real world limits could reduce hardware, physical network and time to boot requirements while preserving capacity.
00:21:00.465 - 00:22:11.859, Speaker A: And for this work we analyzed many real world transactions at the bytecode level. While compute units are cheap, we also see they are not used efficiently, often wasted in unnecessary framework overheads not designed for a blockchain environment. Reducing these overheads can better utilize existing and future capacity while improving developer productivity, and this opportunity is completely independent of this validator work. Closely related, there are many opportunities to simplify the runtime and virtual machine to reduce implementation complexity and transaction execution or reducing power and capacity is ultimately limited by the underlying physical network and network protocols. And there are a lot of opportunities here. A second independent validator has benefits beyond performance, but performance is a prerequisite for reduced cost and increased capacity. As we are working with the community and we are working with the community in all these areas and as we've shown in demos, Agave can hit current protocol caps as is and it can go beyond when embedded in an uncapped high performance technology stack.
00:22:11.859 - 00:23:03.555, Speaker A: So we believe Agave can be optimized comparably and that others will be able to apply the lessons we've learned to make additional performant validators and an efficient ecosystem. Now for this there's a lot of tightropes community members are walking here. Validator developers walk a tightrope between optimizing the protocol for a specific application and supporting many applications. If you are a validator developer, you've probably already heard from us plenty. Expect to hear more about improving the whole ecosystem. Apply the lessons we've learned, use the test suites and fuzzing harnesses we use every day, and keep evolving the protocol. Though we've been able to scale capacity to physical limits, keep in mind there are very real hard constraints on latency, bandwidth and power, all too often neglected in the magical thinking machine model of modern computer science.
00:23:03.555 - 00:23:57.165, Speaker A: These constraints are not unique to Frankendancer, Fire Dancer, the protocol, or anything else for that matter. But there's nothing fundamental we found that would prevent this community from streamlining the entire ecosystem. Validator operators walk a tightrope between minimizing cost for today's needs while providing capacity for tomorrow's. Because Frankendancer and Fire Dancer have a highly configurable scalable design operators can adapt their hardware and network connectivity to community needs for the foreseeable future. Further, the tooling and monitoring we've open sourced can reveal physical network hardware and protocol bottlenecks for real world and extreme loads. So if you are a validator operator, work with foundation to try it out and expect to hear more from us about improving operational efficiency and robustness. Let us know how you want to balance the trade offs between cost and capacity now and going forward.
00:23:57.165 - 00:24:53.031, Speaker A: Application developers walk a tightrope between optimizing applications to use today's capacity efficiently and implementing new functionality to to market quickly. Neither coding raw bytecode in a hex editor nor running an unoptimized ray tracer on chain is viable, but orders of magnitude capacity is sitting on the table right now, independent of this work by optimizing applications for the same real world limits faced by the validator developer and operator. And as we've shown, there is plenty of additional capacity possible on top of that. So if you are an application developer, think big, optimize your applications and give feedback on how to improve the virtual machine runtime and programming model. Now we view this project as the computer science equivalent of civil engineering. Like putting in a larger water main or widening a highway to support a growing community. If this sounds boring, that's because infrastructure should be boring.
00:24:53.031 - 00:25:31.425, Speaker A: To the end user at least. I'm still surprised to be here on stage. We want the infrastructure to work, just work 247 with room to enable classes of applications not yet imagined or previously believed possible. Just a Suddenly replacing a lightly used country road with a multi lane highway doesn't suddenly increase the amount of traffic. A sudden increase in application capacity doesn't suddenly increase the application load. And because we can't do a big bang rollout safely and other validators will need time to apply these lessons. End user improvements will happen over time, that is We've come a long way, but we still have a long ways to go.
00:25:31.425 - 00:26:39.665, Speaker A: But all this really means is that we are still hard at work and committed to the community. In previous Breakpoint talks and early demos we showed this image of a sports car stuck in a traffic jam. This was a reminder that Amdel street justice meant we needed more than an optimized validator to have an impact. We needed to overcome the pervasive computer science decision paralysis and improve the whole ecosystem to realize the potential of modern computing technology. It's been about two years since then. How did we do in the last two years? We open sourced a tech stack for high performance distributed applications rebased the existing validator on top of it, built a second independent validator on top of it, created custom hardware for further improvements, implemented tools for developer productivity and operator efficiency, wrote documentation and testing harnesses for future community development, and released many general technical innovations. By avoiding computer science, magical thinking and designing for real world systems, we found orders of magnitude more capacity available in today's ecosystem and demonstrated how to optimize for another order of magnitude.
00:26:39.665 - 00:27:31.019, Speaker A: In our more recent demos, we've ended with this image why we now believe Frankendancer was more than a stepping stone to Fire Dancer. It is a useful modular, high performance platform for validator development. Agave can be made comparably performant, the rest of the ecosystem can evolve to expose validator capacity and the community can grow into this capacity. That is the traffic jam is starting to break up, the cars are starting to move, and there's multiple sports cars on the road. But there are still potholes to watch out for and we continue to work to fill them. Before I get off the stage, thank you to the conference and its siblings for giving us the time to speak and thank you to all the people for the work they have done over the last two years to bring Frankendancer and Fire Dancer to life. With the time remaining, let's see if we can remotely monitor a running Frankendancer.
00:27:31.019 - 00:28:25.673, Speaker A: So I think, okay, well, if we're willing to go over on time, we might get a leader slot here. But coming up here, we actually see the same monitor that was demonstrated at Block zero. This is running on mainnet. Nothing's really happening right now because it's not a leader. There's a small trickle of transactions that are coming in and as transactions move through the system, you will see when it becomes a leader generating the blocks and what happened to the transactions and where things might be getting stalled. You'll see what the core utilizations are for the various components of system as they go. And since I can't really control this from here, what you can also do with this is there's a number of options to see who's producing blocks, how long are they taking, are they causing people to skip what the current transactions are doing and just general health of the chain? We have seen a lot of people already be very excited to get visibility into how the protocol is operating in the real world.
00:28:25.673 - 00:28:41.815, Speaker A: And we think that this will be a pretty useful tool for everybody. I guess with that. Yeah. Okay, well, if we're willing to go past, I don't really know what to do here while waiting for that to get a leader slot, but thank you.
