00:00:01.000 - 00:00:38.184, Speaker A: Welcome to this month's core community call. So I have posted the agenda in the chat. So Max Flamengo will be presenting on the SIMD 46 or on the new syspar for last voted on slot. And then he also wants to talk about how quick the different tuning parameters were chosen for quick and Max just joins. I will promote you as panelists. Go ahead, Max.
00:00:43.804 - 00:00:45.304, Speaker B: Sorry for being late.
00:00:52.504 - 00:01:01.044, Speaker A: So the floor is yours, Max. You can go ahead and get started with the. Whichever one you want to get started with, either the quick tuning parameters or your SIM data you've been working on.
00:01:02.704 - 00:01:03.056, Speaker C: Yeah.
00:01:03.080 - 00:01:43.864, Speaker B: So for the quick tuning parameters, the thing that was like, my request with that was that we have someone who worked on it, you know, lead to discussion. I don't know if someone is here that worked on it. I can at least, like, I can only give the outside perspective of reverse engineering some of those things, but, you know, reverse engineering requirements is very difficult sometimes. And that was the main request, that we get someone who knows all the requirements maybe to present them. But if there's no one here, then maybe we put it for next time.
00:01:45.084 - 00:01:50.944, Speaker A: Stephen Akritch, could you talk to a little bit how y'all chose tuning for QWik?
00:01:52.744 - 00:03:01.114, Speaker C: Yeah, I can talk a little bit. Yeah. We had a somewhat specific performance idea in mind. Right. We want it to be in that, like 50 to 100k ish TPS range in the front end, or like, at most that. So, like, we try to, you know, split the stream streams that way and allow enough streams for each client to be able to fit within that. So it was mainly from benchmarking in the test environment and seeing what bandwidth we could obtain on a somewhat optimal connection, which would be the worst case in terms of TPS in like, the real validator and trying to calibrate that to what we thought, like, the backend behind Qwik could sustain or handle.
00:03:01.114 - 00:03:42.914, Speaker C: And then also, you know, like keeping the memory within something that was feasible for, like, a normal validator. So obviously, if you have more streams, you need to have more memory to hold all the packets that are kind of in flight and potentially reconstruct a transaction that might cross multiple packets and things. So the more outstanding streams you allow, the more memory you need to frame all of those incoming streams.
00:03:47.574 - 00:04:01.234, Speaker B: One limit we found that I think we were curious about was the, I think it's like 2000 transactions every 50 milliseconds. Like a leaky bucket model.
00:04:04.304 - 00:05:10.984, Speaker C: That would mean that's the Sig verify constants, right? Yeah, yeah. I mean, I think, like, we were talking about in discord, right? That's not like a hard limit. The 50 milliseconds is kind of a target for that stage to not like that's the time it's spending in sigverify and things are coming in behind it right into the channel to get backed up. So we want to make sure that we're pinging the channel like pretty frequently, like, you know, within 100 milliseconds to make sure that the channel can't back up faster than the incoming packet flow. So, I mean, it doesn't have a hard requirement, right. Like you have a machine that can say, verify, you know, 10,000 transactions in 50 milliseconds. It will clear that all of those and it will bring up the new batch and it will, it will verify it, right, but it's just a way to keep the queue from doing an exponential fill up, right.
00:05:10.984 - 00:06:07.784, Speaker C: If you have a large batch and you let it fill in behind you and that's even larger. Right, than the next batch, then you're just in like an unbounded condition and that for the memory in the validator. So there's a couple different solutions to that. But, you know, we like the, we didn't like the bounded channel solution because you can't, you know, the visibility into the channel is not very good. You can't really, you can't drop intelligently inside the channel. So we felt like it was better to just pull everything out of the channel. And we have that kind of the dumb random thing for if we're getting an extreme amount of packets that we really don't have a time to look at really anything in the packet list at all.
00:06:07.784 - 00:06:48.534, Speaker C: But if we're in a less extreme scenario where the machine can handle the flow, we do a round robin between senders kind of dropping. But we can, but that algorithm can't handle some cases of, you know, we have to handle like the case of like a really wide pipe coming in and a very small amount of compute and then a very narrow pipe coming in, a large amount of compute, right. So that all the, like those two variables can be different between across machines, right. So trying to handle any kind of configuration, I guess, of machines.
00:06:51.634 - 00:07:41.874, Speaker B: Yeah, that makes sense. I think this limit is a bit surprising, I think because at least for us it was a bit surprising. There's another one I think is the receive window size that we, I think caused some discussion a few times. I think the idea that was. Right, like you have basically based on the stake, different receive window sizes, which limits the number of parallel streams that can be one per connection. It's like eight connections per identity. And then I think depending on the stake, each connection can have a certain number of stream.
00:07:46.134 - 00:08:28.194, Speaker C: Right? Yeah. The eight connections is just, if you had clients behind, like a router or you had a race condition where you got disconnected and you needed to reconnect, you might have some connections overlapping. So just not to, like, kick you out immediately if you had a stale connection in the connection pool. But, yeah, we're using the streams to throttle based on stake. And then, of course, a budget for unstaked as well. And those are subject to. To tweaking.
00:08:28.194 - 00:08:57.914, Speaker C: I think we wanted to roll out of first version and then see kind of, you know, monitor the metrics and then update them potentially as. As we see, you know, the use in the validator. I don't think they're. I mean, they seem to have been fairly reasonable defaults, but open to tweaking, I think.
00:09:00.174 - 00:09:14.994, Speaker A: For the. Just as a question for any of the people from the fire dancer team I know that y'all have been working on. Quick, did y'all choose similar, like, receive winner parameters and tuning, or have you all not gotten to that part yet?
00:09:20.434 - 00:09:39.054, Speaker B: I don't think Nick is on this call. He'd be the best person to answer it. Let me go see if I can find Nick. Okay, thank you. I think he's on vacation today. Oh, then I probably won't be finding Nick. I don't have the answer to that off top of my head.
00:09:40.314 - 00:09:40.754, Speaker C: Okay.
00:09:40.794 - 00:09:58.244, Speaker A: It'd be probably. We probably should sink as we go further into the implementation of, like, what are the different parameters that we have seen? Because in the future, I think, as Stephen, as you said, it was a good default to start with, but what is a better performance in the long run would be nice.
00:10:01.464 - 00:10:18.474, Speaker C: So a question in the chat, just real fast, about the stake based variables. Right, so more. Yeah, more stake generally gets more bandwidth and more resources and, you know, say, of transactions into the validator. So that's the idea.
00:10:20.454 - 00:10:28.514, Speaker B: Which is really cool and, like, novel, like, a lot of innovation. We've seen that before.
00:10:35.254 - 00:10:37.270, Speaker A: Zantetsu, did you have a question on this?
00:10:37.382 - 00:11:14.514, Speaker D: Yeah, I just wanted to expand a little bit on that. Is it because the larger stake is assumed to have larger pipes, or is it because larger stake actually sort of does more in some way that requires different variables? And the only reason I ask is because if it's because larger stake is assumed as a proxy for larger pipes, then maybe it would be better to have larger pipes. Actually, be something that one could specify on the command line or something. You could say how much bandwidth you expect to be able to utilize, and then smaller validators can do more if they're able to, and larger validators can do less if they're, you know, it doesn't if that makes sense. Did I come through there?
00:11:16.654 - 00:11:34.758, Speaker C: Yeah, it's not necessarily a hardware thing, although. Yeah, more. Higher stake validators should have more hardware, more resources, I guess, and more hardware to handle a higher load, like, for.
00:11:34.766 - 00:11:49.266, Speaker D: Example, ten gigabits, but maybe jump has 100 gigabits or something. So I don't know if they may have more or less stake than I have. So I was just wondering if that stake is assumed as a proxy for bandwidth or if it's something else that causes these variables.
00:11:49.290 - 00:11:50.294, Speaker B: They need to be different.
00:11:51.594 - 00:12:41.662, Speaker C: It's not a proxy. It's just that stake is really the only civil resistant identifier that we have in the network to determine how much resources and things that you should be in control of, essentially, like how much block space. Right. Should you, should you have, I mean, I would say, you know, those overall bandwidth, those could be like a scaling, like if the validator had customized hooks for scaling the overall numbers. Right. You could do that, right. So you would have like, let's say the validator has 100,000 connections now or outstanding streams.
00:12:41.662 - 00:12:52.314, Speaker C: Maybe if you had a ten gig pipe or 100 gig pipe, you would want to make it like 10,000 or something, but you would still like, distribute those streams across the stake.
00:12:54.134 - 00:13:04.036, Speaker D: Yeah, that's kind of what I like. If there's any opportunity for validators to custom tune their numbers in ways that better match their actual configuration, regardless of their stake.
00:13:04.100 - 00:13:05.584, Speaker B: That's what I was getting at.
00:13:07.964 - 00:13:20.024, Speaker C: Yeah. There isn't today in terms of like a command line flag or anything, but that potentially could be added if it seemed to be a limiter or helpful.
00:13:20.644 - 00:13:31.314, Speaker A: And in interest of time, we probably should move on to the simd that you wanted to talk to Max. So if you wanted to go ahead and chat about that, we can get started there.
00:13:34.294 - 00:13:40.954, Speaker B: Yes, let me bring it up. Can I screen share or.
00:13:41.614 - 00:13:44.434, Speaker A: You should be allowed to let me know if you're not able to.
00:13:54.314 - 00:15:06.514, Speaker B: Okay, let's open this file. This is like the newest one we wanted to propose. Basically, the motivation here is on a high level, what do deFi protocols do when the cluster restarts? I think some protocols have built in certain slot limits for orders or things like that. But especially in lending protocols, there's a lot of first come first served. Basically when the network starts up, we need to assume that not necessarily all RPC nodes are ready yet. That's something we've seen before, is that sometimes a lot of things will not, not work on a coordinated restart. So the idea here is, well, I think right now is just a little bit random and uncontrolled how things behave, we can expose to the application developer that there was a controlled restart.
00:15:06.514 - 00:15:34.694, Speaker B: So this is currently the last hard time hard fork on the bank in the reference client. So this is basically just a very simple syscall that allows you to access the data and then implement custom logic. Maybe you want to lock down liquidations for another hundred slots until the oracles had time to update, etcetera.
00:15:36.794 - 00:15:37.282, Speaker C: Could be.
00:15:37.298 - 00:16:06.114, Speaker B: That the chain was at a very different state, or the prices of certain assets were in a very different state before the restart occurred. And so this is kind of like a. Yeah, just poor programmability. The proposal goes a bit in detail what exactly we want to expose, and I think there was some feedback already that this would be also.
00:16:08.934 - 00:16:09.414, Speaker C: A good.
00:16:09.454 - 00:16:34.054, Speaker B: Mechanism, maybe to expose other things. So just curious, before we touch it and create an implementation, if there's other data needed in just the slot, will be good to add it now to the proposals. So we kind of do one pass over it and it's everything we need.
00:16:50.194 - 00:17:06.514, Speaker A: I think Zin Tetsu had a question in the chat for you, Max. Question is, if the protocol allowed validators to snap the slot timestamp to current time on the restart, instead of having to catch up gradually, would that be another solution?
00:17:07.054 - 00:17:18.634, Speaker B: Oh, um, so I did some like analysis on like how the timestamp works right now in the last restart.
00:17:21.214 - 00:17:21.550, Speaker C: I'm.
00:17:21.582 - 00:18:16.844, Speaker B: Trying to find it, but I don't know, and post the data later. But basically you have the restart slot, then usually it's a few more slots, vote only, and only once those votes come in, actually the cluster time gets updated. So I think around like three slots after restart you'll have actually a correct cluster time, or somewhat updated cluster time, so suddenly jumps from like before the restart, after the restart. But these are the things that are kind of like can cause actually issues and protocols. Right. Slot and cluster time is supposed to move fairly connected to each other, but in those moments it doesn't. There's a real disconnect there for a few slots, and then transactions usually start piling in around that time, as well as the cluster time starts moving again, the user transactions, not both transactions.
00:18:16.844 - 00:18:55.408, Speaker B: Yeah, I think this is just one of the effects that we're seeing. Right. So that's the one you're describing. This is like one of the symptoms that we can circumvent with this measure. But I think in general, this information is a little bit more rich. Right. I think there were some ideas about lending protocols, blocking liquidations, and allowing people to deposit more collateral, kind of like a margin call scenario where you have at least two minutes to top up your balance because there's a freaking cluster restart.
00:18:55.408 - 00:19:19.624, Speaker B: You know, it's not happening every day, and it's maybe not the user's fault, right. That they couldn't manage their position. I think that was like one of the main concerns, why we wanted to be as configurable as possible and as exposed as possible to the application developer, rather than just fixing this one particular issue about cluster time.
00:19:26.324 - 00:19:47.544, Speaker A: Okay, cool. So if anybody has any questions, they can, you can voice them now until we end this. If not, there's also, I posted the link to the PR for the SIMD in the chat, so that if you can't get to it here, we can do the discussion on the SIMD as well. I'll add some time for anybody that has any questions for us.
00:19:49.324 - 00:20:44.714, Speaker E: Yeah, excellent idea from program runtime development. So, two things. One is terminology. So I think it's also written in the comments that restart slot and half of two different things. So hard work would be a scenario in which a part of the cluster on purpose tries to diverge from the rest and make up their own cluster, essentially, which is probably not meant here. The other thing is that we are trying to move away from syscall specifically, and we will be using or completely replacing them by bit and programs in the future for all the things which actually do compute anything. Right.
00:20:44.714 - 00:21:36.384, Speaker E: But in this case, this is just a lookup of some global value without any computation behind it. So that should probably go into sysystem variable, and I can't say what the jump team is going to do, but in our implementation we feed all the syscalls with system variables anyway, so that would only be like a throughput identity function. This is called kind of useless in that sense. Yeah, just saying. Probably easier to start off with Sys was. And think about what else you want to put in there.
00:21:43.244 - 00:21:47.984, Speaker B: Anyone has proposals for what else to put in? Well, here.
00:22:06.384 - 00:22:18.400, Speaker A: I think if no one has any other proposals, we can end here. We can continue the discussion on the assembly. Zantetsu, do you have a question? Go ahead.
00:22:18.472 - 00:22:49.384, Speaker D: Is there ever a value. Yeah, my question is only, is there ever value in knowing more than the last restart? Like say there were two restarts within, I don't know, 2 hours, because there was some problem that recurred. Would that be useful to know? Or is it always only the last restart is. Is useful to know? In other words, do you really need it to be a single value, or do you want it to be some like set of n values that end most recent restarts?
00:22:53.594 - 00:23:10.494, Speaker B: I think I would rather see what like adoption looks like on the single value and then go from there is a new feature, historic value. I find it hard to reason about historic values.
00:23:12.394 - 00:23:24.784, Speaker D: Well, but I mean, in particular, you're identifying a use case that you believe covers something that's important to you. I'm asking you, if you had two restarts that happened within 4 hours, would that change what you would want to know or not?
00:23:27.564 - 00:24:00.484, Speaker B: No, probably not. I think you want to get back to normal operation as quickly as possible. And I think the delay is probably within 100 slots for most applications. So the timeframes concerned are just very, very short. I wouldn't know what, 4 hours ago another restart would change on the current situation. Right. So of course, unfortunately.
00:24:00.484 - 00:24:07.168, Speaker B: Okay, thank you.
00:24:07.176 - 00:24:08.504, Speaker D: That does answer my question.
00:24:08.544 - 00:24:32.924, Speaker B: Thank you. I had a follow up for Alex. You mentioned that like the current. Right. The hard fork slot is currently synonymous for a restart, but there's other ways to figure a restart. But there's no way, there's no consensus state shared about those restarts. Right.
00:24:32.924 - 00:24:34.784, Speaker B: Or is there another way to get those?
00:24:37.864 - 00:25:13.754, Speaker E: Yeah, that's kind of the other problem, the security issue. How are you going to reach consensus about what the research. But I was really talking just about the terminology that hard work means that you are diverging on purpose because part of the cluster wants to do something else. And other words, this is just the fork. Like any other fork that will get pruned at some point.
00:25:14.614 - 00:25:16.914, Speaker B: Yeah, it's a slow block right out.
00:25:20.174 - 00:25:20.750, Speaker C: Basically.
00:25:20.822 - 00:25:44.238, Speaker B: So, yeah, I think the differentiation between like a slow block and a restart is probably not. So like on an application, it doesn't really make a difference. Right. If you have a 1 hour slow block or you have a hard fork, it's probably the same way if it took like an hour to organize that, or 24 hours.
00:25:44.286 - 00:26:01.034, Speaker E: So what? I mean, there's no terminology for this or anything, but yeah, if people, for example, say that like bitcoin and bitcoin gold, these are hard forks, or like what they did on Ethereum back in the day, because the community actually.
00:26:03.334 - 00:26:03.846, Speaker C: Switched.
00:26:03.870 - 00:26:55.274, Speaker E: To an entirely different protocol version in the restart. And this is not. I mean this obviously depends on the exact restart scenario but if you are just reloading the same version again or with only minor bug fixes and all that we aren't the same thing. And this is not really hard fork because you're not forking off from anything else. Like what people understand under the term hard fork means that then there are two blockchains and two sets of validators and two networks essentially which do different things and I mean usually restart tries to avoid that scenario of hard for that come back to one consensus of global network again.
00:27:03.734 - 00:27:28.574, Speaker A: All right just thank God because we are 1 minute over time let's bring this discussion further into the SIMD. I will post it again in the chat and then we can further the discussion on this specific symbio for 47 with Max and Alexander and more. But thank you all for coming on today on this month's core community call.
00:27:31.114 - 00:27:35.514, Speaker B: Thanks for hosting us Jacob thank you thank you thank you.
