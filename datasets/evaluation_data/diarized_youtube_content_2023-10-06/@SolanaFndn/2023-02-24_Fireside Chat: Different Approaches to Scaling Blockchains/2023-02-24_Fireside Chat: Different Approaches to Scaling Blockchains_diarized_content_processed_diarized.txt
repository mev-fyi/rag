00:00:00.200 - 00:00:19.954, Speaker A: All right, well, thank you. Thank you for coming. Thank you, Solana folks, for hosting us, Alana and team for doing all the work behind the scenes. Ryan for doing a bunch of work here, Ilad for co hosting, and everybody here for coming. Let's just kick it off with intros here. I'll start since I'm already talking. Hey, everybody, my name is Avichal, one of the founders at Electric Capital.
00:00:19.954 - 00:00:25.194, Speaker A: Nice to see so many friends in the audience. Gonna pass it down. We'll just do intros.
00:00:25.584 - 00:00:30.084, Speaker B: Yeah, absolutely. My name is Sam. I'm the co founder and CTO of Mist and Labs.
00:00:30.984 - 00:00:33.444, Speaker C: I'm Anatoly. I'm co founder of Solana.
00:00:34.224 - 00:00:39.244, Speaker D: Hi, I'm Jill. I am a co founder and head of strategy for espresso Systems.
00:00:39.744 - 00:01:25.304, Speaker A: Great. So the topic for a conversation, and we can see where it goes, and everybody here is a pro, so I'm sure we'll have a good conversation, is sort of different approaches to scaling blockchains and blockchain systems. And everybody knows there are lots of different approaches. But Sam had a good, I think, tee up for this, that maybe we can start off with, which is, when you think about scaling, usually there are trade offs that are being made. There's a set of compromises. At some point in the stack, there's a set of compromises that are being made, but the compromises are really a function of values. Maybe first question for you all, and we'll kick off the discussion is, what are your values? And how do you think about what are the things that should absolutely not be compromised? And how do you think about the things that are okay to compromise and tweak along the way? Maybe, Sam, you want to start and we'll just go down?
00:01:25.924 - 00:02:13.044, Speaker B: Yeah, absolutely. So I'll start with just one thing, because I think it's the strongest and maybe like most controversial opinion I have, which is the thing I care the most about, is atomic composability. And the reason I think this is because I think the reason blockchains matter, or like, the reason they're different and provide value that conventional computing platforms don't, is that it's a platform where you can have shared state and you can have atomic access to that shared state. And I think the value of a blockchain is basically proportional to how much shared state is there and how much people want to access it. And you want to provide, both from a user and programmer perspective, the illusion that there's this whole world of state out there. I can atomically interoperate with everything there are no borders, there are no middlemen, there's no rent seeking. I think to the extent that that's true, your blockchain is going to have value, and to the extent that it doesn't, the value goes away.
00:02:13.044 - 00:03:04.074, Speaker B: That's why I think atomic posability matters, because as soon as you start to say there's not actually one big global shared state, there's actually some piece of state where there's no walls between them, and others where there are walls that programmers are going to have to overcome by manually implementing two phase commit or that users are going to notice because there's extra latency when trying to do transactions that cross these shards or these barriers between different applications. I think that really makes things difficult. I think developer experience and writing safe code and user experience are already such big barriers to crypto adoption that you certainly can't afford to make any design decisions that aren't going to make them worse than they are today. I don't have religious feelings about a lot of other things in the space, but if I'm going to optimize, for one thing, that's something I'm not willing to compromise on. I think that rules out a lot of solutions and tells you one particular direction that you want to go.
00:03:05.854 - 00:03:44.930, Speaker C: I basically agree with Sam on that, too. I think that's pretty important. I think values wise, I think what was weird about my experience is that I thought that like, hardware is just really, really cheap. That was like a value that I had, and it's something that gets cheap really, really fast. So when I looked at the trilemma problem and I thought, okay, if you arrange the system like this, the only expensive part is the hardware. I believe that everyone else is going to copy that design because it's so obvious to me that within two years is going to be half price, in eight years is going to be a quarter price, 20 years, 1000th. Right.
00:03:44.930 - 00:04:53.984, Speaker C: So you kind of like, once you in my, like myself working at Qualcomm and like spending all this time building really, really like optimized code on tiny arm chips and then seeing in two years that, like, now you have twice as many cores, twice as much hardware, it just gave me that belief that it's just inevitable that as soon as you solve a problem and it's the only, the only bottleneck is like silicon, you're kind of done. So that was the philosophy that we started building Solana. And the things that I think Sam mentioned as the trade offs, I didn't want to pay for those trade offs because I don't need to. That's kind of like, why would I give up atomic composability when you can have it for just the price of a bit more silicon? And you kind of start thinking that way, like, why would I give up a large validator set when you can just have it with a bit more silicon and so forth? So that was really, I think, kind of where we, how we started tackling that problem. Can we take all of these really, really hard engineering protocol such a problems and just convert them into like, just add more cores or add more bandwidth?
00:04:55.244 - 00:05:54.834, Speaker D: I love this question because I think we're all coming at it, there's a lot of shared values, but I think that we're all coming at it from slightly different angles. And the angle that I think we take at espresso systems is that at its core, what makes a blockchain a blockchain is decentralization. And what I mean specifically by that is censorship, resistance. And so at the end of the day, what we're doing is we are trying to optimize for all of the other things around it, user experience, developer experience, how to achieve it without having to end up in this fully fragmented, siloed world where you lose that atomic composability, all of these things. How can we solve all of those problems where without giving up what is sacrosanct in terms of the decentralization? And again, I think decentralization is a totally overused, overburdened word. And so again, I'll define it a little bit more specifically here as censorship resistance.
00:05:55.494 - 00:06:02.310, Speaker A: Jill, you wanna talk a little bit more maybe, about the approach you guys are taking, specifically with the l two, and just give people some color on that?
00:06:02.382 - 00:07:23.958, Speaker D: Yeah, absolutely. So at espresso systems, we started out a couple of years ago, actually, with the aim of building a new l one protocol. And where we've wound up is actually quite a different place, where now we are working within the ethereum ecosystem, in part because we view Ethereum as having one of the most robust validator sets and one of the most robust approaches to decentralization and that kind of angle of security. And what we're doing is we are developing a sequencer protocol and network to enable Ethereum l two s to decentralize their sequencers, because we view that as one of the places where the ethereum ecosystem is currently falling short of that promise, which again, is our core values. Now, I'll caveat myself, and not just because I'm at Solana HQ right now and say that I think that part of the beauty of blockchains and of this space is that we are still figuring out what all of the use cases are. And so I think that it's amazing to have a bunch of different approaches, two questions of scalability, two questions of composability, et cetera. And I wouldn't be surprised at all if sort of all of these different approaches wind up having value.
00:07:23.958 - 00:07:36.514, Speaker D: But in terms of where we sit and what we're trying to optimize for, again, it's solving these problems like scalability and composability within that framework of prioritizing decentralization.
00:07:37.194 - 00:07:52.214, Speaker A: So back to this idea of these are ultimately trade offs. I'm curious how you all think about given that one or two things that are sacrosanct, how do you not, what are you trading off? What are you willing to give up to preserve this value? You start, Sam?
00:07:52.794 - 00:08:48.096, Speaker B: Yeah, absolutely like that. Jill said that decentralization is overloaded and ill defined. It's definitely a sort of thing where there's a lot of different facets to it and it's more of a continuum with the in some n dimensional space than a specific thing. And I think a lot of times folks think decentralization means you have a zillion nodes, and the barrier to operating node is that you can do it on your phone or your laptop or something like that. So I think one particular thing that I think is maybe a bit controversial, but we feel are willing to give up is the barrier to entry to operating a validator node, where we think if you're going to scale, and to me, scaling doesn't just mean pushing end transactions, it means you can throw more hardware at the problem and get more transactions. And there's no limit to that. That means in the limit operating a validator is going to be operating a lot of machines, operating maybe a small distributed system, maybe operating a large distributed system within the boundaries of a validator, that's going to cost more, it's going to require more sophisticated operators.
00:08:48.096 - 00:09:14.032, Speaker B: If that's the case, I think you have to be willing to give up this very nice value, but maybe not practical to have that. You can run a validator node in your living room or on your phone. It might be a fairly professional thing. As long as you can have a lot of those and have accessible third party replication, I think that's acceptable. But I think that's one thing that we feel like you have to be willing to give up if you really want to scale. In the true definition of scaling, that.
00:09:14.048 - 00:09:15.736, Speaker A: Sounds like something you would say, too.
00:09:15.880 - 00:09:26.244, Speaker C: Yeah, that's pretty close. I mean, basically, we think that if you have a more expensive system, it's going to be half price in two years. Who cares?
00:09:28.904 - 00:09:49.024, Speaker A: From Jill's perspective of sort of decentralization, how do you get comfortable with the idea that that's sufficiently decentralized? Is there some metric, or do you try to quantify that, or do you just accept that as if there are thousands of people doing it? It's efficiently decentralized. How do you think about the threat vectors or attack vectors? How do you get comfortable with that idea of that's an okay trade off to make.
00:09:51.004 - 00:10:23.180, Speaker B: For me, I think a lot about how do you make third party replication cheap and accessible? I do think it's problematic if running at full node is as expensive or on the same order as running a validator. And a full node isn't the only form of third party replication. But the way we usually define it, it just means doing everything a validator does, but not earning rewards and participating in consensus. Well, that seems a little bit less acceptable, but I think there are lots of alternative modes for different ways of doing third party replication. Of course, there's the classic lite client. I think there's a lot of interesting new ideas. Solana has this DIa clients idea.
00:10:23.180 - 00:11:14.174, Speaker B: We have the sparse nodes idea that's somewhat similar, where instead of replicating everything, maybe as an end user, there's an easy way to replicate exactly the state and exactly the transactions that you care about. And then if I'm a game, I can replicate all the transactions that my users are sending and replicate the game state, and then, so I care about that being trusted, but I don't really care about either replicating or the trust guarantees and other stuff. So I think giving folks the flexibility to do that individually and then collectively, you'll replicate everything. And keeping that barrier entry low is one way that you can get comfortable with that. Where you say the decentralization doesn't mean low barrier entered operating validator, it means a lot of folks can keep the validators honest, and then if they become dishonest, there's social consensus ways to get around that. And then that game theoretic mechanism keeps everyone safe. So that's one of the big things that we think about, is how do you let one thing become more expensive while making the thing that an average end user needs to do still be pretty cheap?
00:11:14.474 - 00:11:19.274, Speaker A: Jill, how do you all think about this? Is this in line with how you think about it, or do you have a different perspective on this.
00:11:19.354 - 00:12:48.884, Speaker D: Yeah, I mean, I guess I personally tend to come at this from much more kind of product or even end user perspective. And I think that we had actually, in the industry as a whole, an interesting litmus test of what decentralization means. Last summer, when tornado cash got sanctioned by the US government and that threw the ethereum community into crisis of, suddenly all of these choke points of the system came to the forefront and people really saw, oh God, if Coinbase is, uh, you know, doing all of the staking, like, can they then, and they're not processing transactions that are coming from tornado cash addresses because they're a regulated us entity, is that going to be a problem for the censorship resistance of the system? Um, you know, if flashbots is blacklisting addresses, is that going to be an issue? Um, and I think that that was a really, in a way, like, beautifully stark moment for the whole ecosystem to be able to see, like, what the choke points are in terms of where the system is still centralized. And that is kind of the very real world, concrete perspective that I try to bring to the products that we're working on building is that question of what are the choke points here that censorship could be introduced.
00:12:49.844 - 00:13:17.234, Speaker A: Gotcha. So, building on this idea of decentralization, and how do you get comfortable with the idea since the topic was originally scalability, how are your teams thinking about what exactly does it mean to be scalable? How do you think about that? It's a good value, but kind of similar to decentralization. How do you think about, is there a metric that you put against that? Is there a number that you put against that? Or what does that mean for your teams and your ecosystems to be scalable?
00:13:18.774 - 00:14:13.986, Speaker C: I mean, we're like benchmarking nerds. So we look at like atomic sign transactions as like the, it's like the nanometer like, number that intel will publish. This is the smallest useful thing that's atomic that you can do. And that's a pretty good measurement, because if you, like, cheat, then you like, have a single signature and then a bunch of batched events that this thing does, you can scale up numbers, but those aren't super useful because you're not dealing with a coordination problem of a whole bunch of users sending data that is uncoordinated. Nobody knows how it's supposed to actually fit once the system actually linearizes it. Single atomic transactions cover most of the work that the system needs to do to do something, the smallest useful thing. So that's the benchmark that we run.
00:14:13.986 - 00:15:08.788, Speaker C: It also like at least with the way Solana is designed, that plugs directly into decentralization, because the more capacity we have for these events, like to handle more single sign events, the more nodes we can have voting in the system. The more nodes you have voting, the more nodes you have to. With pagerduty they'll say, oh shit, the majority of networks just did something. It's something invalid and we need to stop and do a user activated soft work. So that at the end of the day is decentralization is like there's enough people that give a shit about the network being correct that one of them is awake and can tell you that something went wrong. So how you maximize that set so there's enough people that are awake that that never happens is really like, I think a question of product market fit. You built something useful that's permissionless.
00:15:08.788 - 00:15:48.724, Speaker C: People run these systems and they'll monitor them and they'll care that they're live. And to me it doesn't even matter if they're running them for rewards or it's a magic Eden RPC note. And they really care that that RPC note is correct because the revenue depends on that state being correct and they're not even participating in consensus, but they have somebody monitoring that machine. So those are kind of like the, the things that are important. The decentralization part is hard to benchmark, that's hard to quantify because it's not just sheer number nodes. It's like how many people give a shit that this thing isn't corrupted.
00:15:50.744 - 00:17:14.308, Speaker D: I would just echo what you said about benchmarking transactions per second. We're still in the testnet phases. And this is something that we have been grappling with a lot internally, is that there aren't great standards that exist yet in the industry about what TPS means. And I think that that's something that I would love to see evolve. And yeah, we should chat about afterwards, but I would say from like a more values perspective, the way that we're thinking about scaling is that again, it's something that we want to squeeze as much juice out of as we can without sacrificing, you know, the ultimate safety and security of the system from decentralization perspective. And for that reason, the consensus mechanism that we're building is something called optimistically responsive, which means that in sort of good networking scenarios it can run as fast as the wide area network and then if there is, you know, a vulnerability or int, and then it can fall back to a more robust, fully decentralized network. That's been kind of our philosophy in terms of approaching how we can get the system to be as scalable as possible without sacrificing, again that core value, because I think it is very hard.
00:17:14.308 - 00:17:32.394, Speaker D: I mean, for the longest time, this space has been grappling with this issue of how much can you give on the decentralization side in order to make stuff that's actually usable and that people actually are going to care about enough. So that's the experiment that we're running. That's our approach.
00:17:35.134 - 00:18:02.964, Speaker B: For us. I would say. I said by define scalability is the ability to do more with more. Basically, if you give me more cores or more machines, I can push more throughput. We care about that both for throughput and for storage, because we think a lot of interesting use cases are storage sensitive. We think about both having an algorithm and a system design that has that property. But then also you want the unit economics to work out so that if we want to ask validators to provision a new machine or add a new disk, they should be overdrawn, because it's like, well, my profit margins are good, so I'll do it.
00:18:02.964 - 00:18:26.484, Speaker B: That's the implementation and hardware aspect of this, where you both can scale from the design perspective. But there's also an incentive to scale from the validator perspective, so you'll rejoice when your system gets more traffic, instead of saying, oh, well, I hope we can make up for this by printing more staking rewards or inflation or some other thing. We really want the operation itself to be profitable, both at a small scale and as the system scales up to whatever size is needed.
00:18:27.594 - 00:18:45.482, Speaker A: I'm curious, just because you all are at slightly different stages of development, too tactically, are there dashboards that you're measuring TPS, and you're trying to. You run it like a Facebook growth team, and you're like, oh, here's the bottleneck, we should fix it. Or how do you actually think about driving scalability? Or is that even a thing that you think about?
00:18:45.498 - 00:19:15.014, Speaker C: It's just sort of subcorded. There's a giant grafana dashboard, or dashboards with metrics measuring every little part of the runtime. And myself and most of the engineers stare at those day in, day out, and then like, oh, that might be the bottleneck. And then you go run an experiment, and then it doesn't work because you're in an Amdahl's fight where improving one thing may hit another metric.
00:19:15.094 - 00:19:26.074, Speaker A: And is this instrumented all the way down? It's literally people writing to what level in the stack do you go to beyond just the consensus TPS? What's instrumented? Exactly.
00:19:26.694 - 00:19:49.222, Speaker C: So basically, everything from ingesting a transaction at a block producer validator to where it's being signature verified, deduped, goes into the runtime, then executed, then stuffed into turbine, where the block is multicasted, all of that is tract.
00:19:49.318 - 00:19:55.640, Speaker A: I see. And then was it always like that? Like, do you guys start off that way just instrument everything, or was that, did you launch it knowing. Oh, yeah.
00:19:55.672 - 00:20:02.684, Speaker C: Like, it's just part of development. That's kind of a natural process for engineers to, like, instrument the code and, like, because how else do you debug?
00:20:03.184 - 00:20:07.344, Speaker A: That's great that you guys do that early. Are you guys have something similar since everything's instrumented at this point?
00:20:07.424 - 00:20:23.570, Speaker B: Yes, we have similar stuff for that both. And we've done two testnetways and we have a live devnet. So we have all the fine grain. Like, what's the latency of this part of the system versus that part, what's the current TPS, what's the peak and things like that. So that's for the live system and then for benchmarking. I think we do something similar. We also do something similar there.
00:20:23.570 - 00:21:20.510, Speaker B: I think, as Jill alluded to, the big challenge is what's the workload? You can take real workloads from Devnet and Testnet. You can generate things that are representative in terms of how much contention they create, in terms of how many different accounts they come from, in terms of mixing code publishing versus execution and things like that. But in the space, people just talk about payments, which is a very different workload, both from what you see in practice and from what's hard to execute efficiently. Deciding how to set that variable is challenging also things like, we're not live yet, so what's our quorum size going to be? You want to make sure you perform well, and what's the size of our validator set going to be? We know we can handle, but it's sort of like market determinations of who actually has the minimum stake to become a validator. And how is that stake distributed to form the quorum size? So a lot of what we think about in our benchmarking now is trying to guess what we think things are going to look like and fixing those variables and then trying to play a lot with the things that we don't have control over. Like, okay, we're maxing out the cpu now, but when I do this, I max out the network and this infinite game of whack a mole to see how high you can go, man.
00:21:20.542 - 00:21:47.674, Speaker C: We launched blind. It was like it was early 2020. So the last tail end of the bear market where I thought, fuck, crypto is dead. And then COVID and everything else, it's dead again. We were like, we had to launch, so it was just like we didn't have time to optimize all the stuff and we're like, okay, I think the 40 validators we have is good enough.
00:21:49.894 - 00:21:51.454, Speaker A: Dylan, anything you want to add here?
00:21:51.574 - 00:22:08.034, Speaker D: I was just going to say I feel like that's the only way to ever actually get it out the door, so amen to that. I'll be taking a page out of your book, hopefully here in six months time. But yeah, no, we're still at the benchmarking phase. So, yeah, grappling with all the problems that Sam just mentioned around that.
00:22:08.934 - 00:22:34.104, Speaker A: So maybe we can move in the last section here just to kind of forward looking stuff. What are you all really excited about? Both. If you have something to pick in your ecosystem that you're really excited about that is forthcoming, that you think might drive ten x 100 x kinds of improvements, and also just more generally, as you look across all of the ecosystems, what are the kinds of things that you see that are really interesting, that are being worked on that may or may not apply to your ecosystems?
00:22:36.804 - 00:22:37.572, Speaker B: Go for.
00:22:37.708 - 00:23:23.368, Speaker D: I guess from a use case perspective, I'm excited about what's going to get unlocked as more scale comes online. Super. Genuinely, I feel like Solana unlocked a lot of what was able to happen with Defi over the last cycle because of the scalability that you guys offered for the first time. And I think that we're going to see more and more of that as more and more new chains as well as new approaches on existing chains to scalability come online. I think specifically, a lot of the very long time touted use cases that have yet to come to fruition around things like payments. I mean, you know, the whole origin of this space was founded in payments and we've yet to see a material use case there. I think that that's coming.
00:23:23.368 - 00:23:58.264, Speaker D: I think stuff in the gaming space is really exciting on that front as well. And then in the more kind of like, I guess in the weeds like infrastructure stuff that I'm excited about, I think it's going to be really interesting as more and more l two s come online to understand within the ethereum l two ecosystem, what cross l two composability ends up looking like. And even questions like what cross l two mev capture opportunities look like there's going to be a lot of wild stuff happening in that realm of the world.
00:23:59.324 - 00:24:00.204, Speaker A: Any thoughts?
00:24:00.324 - 00:24:35.484, Speaker B: Yeah, I mean, so I'm going to go with deep infrastructure answer. So whatever scaling means, I think anyone is probably going to agree that it's going to involve some notion of parallel execution. Single threaded throughput is going to max out no matter how fast your vm is and no matter how good your hardware is. I think the conventional understanding of parallel execution in the space used to be, well, I'm going to hope someone hands me this workload with lots of inherent parallelism, and then I'm going to execute in parallel and then profit. But if we look at what actual workloads look like, that doesn't happen. There's tons of contention, tons of hotspots, and real workloads. You have NFT mints where a bunch of people are trying to hit the same contract to get the same NFT at the same time.
00:24:35.484 - 00:25:29.874, Speaker B: Liquidations, where it's like there's a profit motive similar with trade volatile currencies. And so I think there's a lot of folks who are thinking about parallel execution are like, okay, we build this parallel executor, and then we bring in the workload and it's like, oh, there's not that much parallelism for it to exploit. So I think there's a lot of interesting stuff going on, both in what we're doing in SWE and in some other areas, like the Solana local fee markets, where it's like, how do we push this into the economic model? How do we make sure that we punish users who are creating contention and award users that reward users who are writing parallelizable code? And then what's the right way to do that? Is it higher gas costs? Is it higher gas prices? How fine grained can we make that? How do we have the right data model and programming model that can let users express computations that don't conflict and then make that seamless instead of really annoying? So I think that's a really exciting direction. It's super important for scaling, because if you have a great parallel execution, that's a great start. But if you can't make someone give you a parallelizable workload and continue to do that as you scale, then all that stuff isn't going to matter very much.
00:25:30.594 - 00:25:39.810, Speaker A: Where do you think we are on that piece? That's a really interesting point, actually, to call out that you could build the engine to do the parallelization, but then if the workloads aren't parallelizable, then it doesn't matter.
00:25:39.882 - 00:25:42.694, Speaker C: Everyone just wants the same NFT meant, that's all we'd say.
00:25:43.874 - 00:25:53.414, Speaker A: I mean, there's truth to that, right? If you look at the usage patterns, there's some truth to that. So I'm curious, what does that mean for the other parts of the stack? How do you think about that?
00:25:53.914 - 00:26:29.086, Speaker B: So there's this interesting Twitter debate the other day. I think both of us were part of where it's about for parallelism. Do you have this model where you declare some dependency information upfront, as you do in Swe and in a sweat transaction? You have to say, these are the objects I'm going to touch in Solana. You have to say, the accounts you're going to touch. Or is it something that you can discover after the fact? I think maybe the jury is still out on this, but my conviction on this particular thing is you have to have something like this if you want to put anti contention mechanisms in your economic model. Because if you try to discover this at runtime, by the time you discover some transactions create a contention, it's too late. You can't give it lower quality of service, you can't charge it a higher gas price.
00:26:29.086 - 00:26:55.434, Speaker B: You have to know that upfront so that you see, hey, this is touching some really hot shared object. A lot of other things are touching. I should put this in the queue and do, if it's specifying a gas price of five, I should treat that as if it's a gas price of one compared to something that's creating no contention and just how that should work. I think that's one. I have a conviction about the way that should work. There might be some interesting counterarguments, but I think that's an understanding that's just evolving and it's quite interesting.
00:26:55.594 - 00:26:57.754, Speaker C: I mean, we agree in almost everything.
00:26:57.834 - 00:26:58.454, Speaker A: So.
00:26:58.914 - 00:28:10.684, Speaker C: Yeah, I agree about one controversial take that I have on parallelism is that once you have a parallel runtime, there is no reason for l two s. I think this is something my design fight with ETH folks is that if you have a L2 that can handle parallel execution and can handle the economics of parallel execution, meaning that can isolate hotspots and preventing one from creating a global fee market spike, that single l two can eat up all the data availability in the entire l one. And then you literally have one l two that's parallel, that's getting all the transactions and one l one that's consuming all the data from that l two. Then why don't you just mix them, put them together? It kind of eliminates the need for that engineering split. But there may be other reasons why you'd have that, simply because you can iterate. If you can do that split, you can iterate maybe faster on the execution layer, and execution layers are a huge pain in the ass. To build a single bug can be catastrophic.
00:28:10.684 - 00:28:20.142, Speaker C: There's some reasons, but they're not true design. The system is better with multiple l, two s versus one parallel one.
00:28:20.328 - 00:29:14.704, Speaker A: Interesting. Maybe last question before we do, before the mics die, is actually a couple of people here sort of alluded to surprises or unintended consequences. And I mean, we like, I think like the growth of MeV is sort of like an interesting one and sort of taken a life of its own over the last 1218 months. And if you went back four or five years, it's probably not the thing that people would have talked about top of mind. So as you think about the things that are happening here from a scalability perspective, for all the things you mentioned, parallel execution, for all the things that you mentioned, are there sort of looming unintended consequences that you're looking at that you're like, oh, this is actually a thing kind of on the horizon that I don't think most people are paying attention to, that we should think about as a consequence of some of these things that are being built around scalability.
00:29:18.444 - 00:29:40.172, Speaker B: Let's see. Well, I have an answer to this. It's a very different nature. So I actually think smart contract security is the thing that's the most likely to prevent crypto from scaling overall. I mean, I think if you look at the folks who are smart contract programmers today, they're people who they really know a lot about blockchain. They're sort of early adopters, they're smart folks, they're very security minded. I think they take the responsibility that they have super seriously.
00:29:40.172 - 00:30:20.826, Speaker B: And the track record of writing safe code is still abysmal. Not because these folks aren't qualified or because they don't care or they're not smart, but just because writing safe code that manages money in an open source ecosystem where anyone can interact with it and it's motivated to, is just a really hard programming task. I think it's a much harder programming task than we would ask an average programmer to do. And so I think if you take what we have today, and now you want to bring in a game developer who reasonably, they just want to write a cool product, they're not trying to be a security expert they don't want to do audits. They're just trying to get something to market and iterate quickly. I think that problem is going to become much, much worse, to the point that folks will try it and get burned and stay out of the space or just be afraid of coming in initially because of the track record. So that's something that we think about a lot.
00:30:20.826 - 00:30:56.294, Speaker B: And that I've thought about with move is, okay, how can we really make this thing as safe as possible? It's not enough to have all this security audit firms and manual stuff after the fact. You really got to take as much off the table as you can with the programming language, and then you'll have all of that other stuff too, both in language itself and then in advanced tooling, formal verification, static analyzers, advanced fuzz testers and things of that nature. So I think if you could snap your fingers today and make blockchain platforms scale, most of them would scale fine from the techno perspective, but you wouldn't be able to find enough people to write safe code to actually have a vibrant ecosystem of real world use cases on top of it.
00:30:59.754 - 00:32:02.230, Speaker D: Yeah, I guess just from the ethereum ecosystem perspective, I think that there are so many unknown unknowns because l two s and the infrastructure around them are all just sort of emerging at once. And there's so many components to the infrastructure, whether again, it's what we're working on, which is the decentralized sequencer systems that they're going to need to rely on to, you know, the decentralization of proving systems. I think that, yeah, there's going to be a lot of these types of things that crop up, but the one that I am most focused on, which I mentioned already, is, again, what this will imply of having this proliferation of l two s for the composability between them and how we can start to solve for that problem, which you will be unsurprised to hear that. I think that a decentralized sequencer is a part of the solution to that, of sharing the sequencer amongst multiple l two s. But there's also going to be a lot more to it than that, that we've got to figure out any unintended consequences.
00:32:02.262 - 00:32:03.438, Speaker A: You want to call out things you've.
00:32:03.446 - 00:33:06.046, Speaker C: Been thinking about these things getting launched unintended. I think there is a lot of uncertainty about how people treat security of these systems, not from, I agree with Sam on the smart contract side, but in general on custody and managing keys as these things scale to normal people, because normal people don't have this understanding of this key. If I lose it, I lose my money and stuff like that. And that's a pretty, pretty hard topic and hard thing to solve. We're taking like a stab at it with mobile, like storing the keys in your phone and stuff like this, but very, very tiny. This is something that I think needs to be done by Google and Apple, really, but I don't know what's going to. Maybe we have to poke him hard enough for that to happen.
00:33:06.046 - 00:33:13.994, Speaker C: But that's one of those things that I think is like a real blocker for like real consumer adoption is just like understanding key management.
00:33:14.294 - 00:33:47.872, Speaker A: Yeah, I think, I think the one I think about there is sort of the generalized version of that is just the user experience around this stuff at the app layer is just so gnarly right now. And it's just so easy to screw things up, whether that's like signing the wrong transaction or getting rugged or phishing sites or all this kind of stuff that my outtake is like, I hope we have a three year bear market. So like all the designers can come in and we can like actually build stuff. Cause like if, if we have a, if we have a mega bull in like twelve months, we just may not be ready for it. You might get like 300 million users that come in and you poison the well because nobody, everybody just like loses all their money along the way.
00:33:48.048 - 00:33:50.976, Speaker C: Bear market with bitcoin at like 25,000 is fine.
00:33:51.040 - 00:34:12.004, Speaker A: Yeah, let's just go sideways for like three years and like build all the stuff we need to build so that when the next billion users show up, like it all actually works. It's fast, it's easy to use, but yeah, it's sort of like an extension of what you're talking about is like the UX around some of this stuff is just so bad right now. Okay, we have a few minutes. We can do Q and a. I think there's a mic that's going to float around. Anybody have questions? Amazing brain trust of people.
00:34:12.664 - 00:34:16.008, Speaker D: The mic is going to be. I'll pass it around. I have to steal your mic.
00:34:16.056 - 00:34:21.044, Speaker A: Okay, no problem. We actually should probably take a different one because that one's getting a little flaky.
00:34:21.664 - 00:34:23.004, Speaker C: We can share these.
00:34:25.123 - 00:34:41.143, Speaker E: I have questions specifically to the validators that all three of you talked about. I'm curious, how do you bootstrap the validators today? And do you wish that there is anything on the market that you could use to help bootstrap the process of building on layer one chain?
00:34:42.923 - 00:35:05.244, Speaker D: So for us specifically, we're looking at a lot of the things that are emerging in the Ethereum ecosystem to be able to bootstrap off of the Ethereum validator sets. So things like Eigen layer and restaking, these are all very interesting to us, again, from the decentralization security perspective. But, yeah, that's still in the hypothetical phase since we're not actually launched yet.
00:35:07.064 - 00:35:37.654, Speaker C: The weird thing about validators is you want them to be independent and have independent hardware and independent ops. So it's really hard to scale that. It's almost like a counter. Counterintuitively, you don't want, that's like the last thing you want scaled is you want the validators to put in their individual work and be very opinionated about security and performance and all this stuff. And out of that mass of people arguing about which setup is better, you get your social layer that's going to keep your network secure.
00:35:39.714 - 00:35:41.410, Speaker A: Is there a way to bootstrap that, though?
00:35:41.522 - 00:35:42.774, Speaker B: How did you guys do that?
00:35:43.714 - 00:35:45.694, Speaker C: We talked to all the cosmos validators.
00:35:48.594 - 00:35:57.050, Speaker E: Why do you have to find a hard machines like, can that be done by AWS? Because you talk about we have to find the people and teach them.
00:35:57.162 - 00:36:17.584, Speaker C: That's your single point of failures. You want each validator to be opinionated that AWS sucks or doesn't suck, and they, like, somebody uses equinix, somebody runs at home, somebody runs their own cables. The more diversity you have in that set and more opinionated people you have, the better you actually have of a network.
00:36:18.204 - 00:36:33.124, Speaker B: It's something we found is there's sort of a natural forcing function away from the cloud providers, because once you see the egress bill as a validator, your profit margins aren't going to be very high anymore. So it actually sort of drives people toward either independent operation or smaller things that don't try to have that particular business model for pricing.
00:36:33.284 - 00:36:34.220, Speaker A: Next question.
00:36:34.412 - 00:37:02.504, Speaker F: Yeah, so I guess to Anatoly and to Sam, you all mentioned that in the pursuit of scalability, the value you care about is not compromising on what you call atomic composability. I'm wondering if that implies that you're not very big believers in a multi chain world where composing transactions between two chains is naturally asynchronous. And if you think that sort of like a monolithic architecture, one best blockchain with atomic composability is a better take.
00:37:04.764 - 00:37:46.704, Speaker C: I mean, if both are guaranteeing censorship, resistance shouldn't be synchronous, because I know that this event is never going to fail. Right. If you trust these systems to work as designed, you can treat them as synchronous. The problem is, if there is a catastrophic failure, you're screwed between the two chains. I think we're likely going to get to a state where most of the interesting assets are going to be USDC settled off chain, because these are like the trillions of dollars of real world assets. And then it won't really matter what chain they're issued on. These are going to be execution layers.
00:37:46.704 - 00:38:10.280, Speaker C: That's my long term view in the 20 year timeline. In the short term, people really care that their NFT is not bridged. Some people care, but then some people burn their NFT and Ethereum and remit it on bitcoin. It's a free for all for those security assumptions. I don't know.
00:38:10.472 - 00:38:15.832, Speaker D: Are you saying that the off chain settlement, that's going to serve as the interoperability layer?
00:38:15.968 - 00:38:39.924, Speaker C: If we're talking about crypto winning in the post crypto PMF world, all of this, every company that's going to IPO is going to issue their stock on every blockchain. Why wouldn't they? And they're the settlement, they're the final settlement layer. They don't need to trust any chain because they're the USDC, they trust only their node.
00:38:41.514 - 00:39:14.464, Speaker B: I mean, so if I think crypto were a planned economy, the optimal setup would be just like one blockchain that has the best design and scales and has the most co located shared state and the most interoperability. But it would also be the best if we had one phone provider and one insurance provider be the most efficient. But it's not a planned economy, right? It's organic and there's not one clear best design and there's already many, many entrants. So I believe in a multi chain world. And the fact that I don't think there's going to be one consensus best design and network effects of both existing and future chains is a real thing. It's just like from a pure, this would be optimal perspective, I think one would work, but I don't think that's the world we're going to end up in. I think we'll still end up in a very good world.
00:39:14.584 - 00:39:41.784, Speaker A: Well, there's a lot of path dependency there. It's just like, were we talking about JavaScript earlier? Maybe there's a side comment. It's just like JavaScript is such a good example. It's like, who would have designed that as the programming language to win? And here we are. And so, yeah, I think there's a lot of path dependency, given that everybody had an incentive to get l one chains out the door. The reality of kind of where we end up probably ends up looking a lot messier than if there were a lot more coordination on the front end. Maybe one last.
00:39:41.784 - 00:39:45.684, Speaker A: Anybody have any last questions? Last question and then we can mingle.
00:39:46.824 - 00:39:48.280, Speaker D: How do you guys view roll up.
00:39:48.312 - 00:39:52.644, Speaker E: Tech either in your ecosystem or other ecosystems, since it's a really hot topic right now?
00:39:53.904 - 00:40:18.884, Speaker D: Big believer in it. But that goes without saying, based on everything else I've said. So make space for the others. But I think. I think, as I said, it's. There are a lot of hairy problems with it, especially as everything is kind of emerging at once. And I think that the lack of decentralization of a lot of roll ups today is a huge issue that I, you know, I think those chickens are going to come home to roost, basically at some point here.
00:40:18.884 - 00:40:33.412, Speaker D: But there's also a lot of promising companies not, you know, besides espresso that. That are working on solving those problems. But, yeah, I'm curious, actually, to know from Solana and the sui ecosystem, I.
00:40:33.428 - 00:41:09.134, Speaker C: Think that a chain that's set up for parallel execution is not going to see any fee reduction in a roll up that's built on top of it. It's basically going to run at the same fee rate as the layer one. Then you have to have some value that the roll up creates that is above just what you typically see. The roll up designs are on Ethereum. There's some reasons for that. You could have a new execution environment, or you give up some decentralization for a centralized sequencer that's just much faster or whatever.
00:41:10.154 - 00:41:40.734, Speaker B: Yeah, I think my view on this is similar to Anatolia's, where if there's something like maybe you get lower latency through some form of optimistic confirmation, or having a sequencer that has different validators that are different guarantees in the base layer. Maybe you're iterating on the programming model faster than base layer can, or experimenting with a different account model, different languages or multi languages, there could definitely be some value. But if you believe that atomic composability is the most important thing, then you won't want that at the base layer. You'll have it in the secondary layer for these experimental or features that the base layer just can't provide.
00:41:41.554 - 00:42:07.374, Speaker A: Is there a use case based case for the LT rollups beyond programmer iteration, centralized? Like, for example, should each game that wants to build in your ecosystem actually run their own quote unquote l two and then just sync down to you guys, but then they control the sequencers. Is it a use case based thing or is it a programmer desire based thing to have a different execution environment or something?
00:42:07.754 - 00:42:40.614, Speaker B: So that's an interesting question. I think maybe if the use cases composability for your particular use case is a bug rather than a feature, maybe you're protective of your brand and you actually don't want certain kinds of composition to happen with your game. That would be a good reason to put it on a roll up because you want that isolated island instead of having interoperability by default. But I think if you want that feature, then you probably want to be on the l one. I said at the beginning, I think a lot of the value from crypto comes from getting interoperability by default instead of explicit integration. I don't know how many of those things there'll be, but it's interesting to think about.
00:42:41.274 - 00:42:49.194, Speaker A: All right, friends, we're out of time. Thank you everybody for coming. Stick around. We're going to have a space for a little bit mingle. Thank you, guys. Thank you everybody.
