00:00:03.440 - 00:00:30.634, Speaker A: Okay, excellent. Hi, I'm Kevin Bowers, chief scientist at the jump trading group and the fire dancer engineering team lead. You invited me back. I'm kind of surprised. Cool. Thank you. The title of this talk is high performance computing for accelerating Ed two 5519 with data parallelism before we get into the technical nitty gritty, here's a summary of what's happened since last year's talks and an introduction to to the talks we have for you today.
00:00:30.634 - 00:01:15.352, Speaker A: Since last year, our engineering team has grown significantly. We've made a lot of progress. Dan Albert just introduced me, already did an overview yesterday and the main stage talks of the developments non technically in this area. Given the time that we have here, we're not going to cover that in detail. Instead, since this is meant for technical talks and all the presenters here are from the engineering team, our talks will showcase a few of the technology innovations we've made since last year. A question Austin Federer, the Solana Foundation's head of strategy, asked in an interview a few months ago motivated our choice of topics. Austin asked about how the decentralized compute needed for this work differed from my previous work in high performance computing.
00:01:15.352 - 00:02:26.710, Speaker A: Paraphrasing, my answer was it doesn't. High performance computing has required decentralized compute even within individual cores because cpu's have been large scale distributed systems for decades, this seems to surprise Austin and others that were brought to my attention. But a modern x 86 CPU is more accurately described as a distributed network of narrow vector cores that emulates a bunch of legacy x 86 CPU's, and a modern GPU is more accurately described as a distributed network of wide vector cores that emulates the Cuda machine model and the memory subsystem is more accurately described as a distributed file system that emulates legacy DRAM. This is because repeating a theme from last year's talks, blogs, podcasts and my previous work, the speed of light is too slow. This is a slide I first showed in 2008 and have shown many times since, including at breakpoint last year. And it still applies today. Roughly speaking, as semiconductor transistor feature sizes get linearly smaller, the number of transistors available for compute increases quadratically, while the bandwidth of feeds the those transistors increases linearly while the latency to receive data for those transistors stays constant.
00:02:26.710 - 00:03:22.496, Speaker A: This was alluded to a lot in Dan's talks yesterday about how hardware continues to grow and in performance, but the way it's manifesting tends to be in more parallelism. It doesn't manifest in a faster speed of light. As a result, computers today are more limited by how fast they can move data than by how fast they can crunch that data. Making a cpu with a ton of cores is much easier than feeding those cores and keeping them cool. And to our mild surprise, a frequent question after last year's talks was why didn't you use the latest and greatest cpu's for your demos? How fast would a modern cpu be? Though we continue to support older cpu's to address those questions, we're not going to restrict ourselves this year and as per usual, improved cpu's were released over the year that continued these trends. In fact, relatively speaking, the speed of light got slower. Here is a picture of a recent intel cpu die showing the dimensions in the high level floor plan.
00:03:22.496 - 00:03:57.076, Speaker A: This is not the high end or even the latest. This launched Q 420 22. It just happened to be a recent cpu with a good picture available online, it is targeted at desktops. It has 24 x 86 cores with a base clock of 2.2. Peak turbo clock at 5.8. Worst case, round trip between any two points on this die is 52 mm as the crow flies. Practically, it's a lot worse than this, least of all because the Manhattan distance, north south east west only, is more relevant.
00:03:57.076 - 00:04:52.188, Speaker A: That's 74 peak turbo clock speed. Light and vacuum can almost make a round trip between any two points on the die in a single clock cycle. But these signals want to be moving in air, and the more relevant silicon dioxide light can barely make it one way between any two points on the die as the crow flies, and as we mentioned, it's not going to go as the crow flies, and light in silicon itself can only travel a little over half the long edge in a single clock cycle. This very limited speed of light creates a natural parallelism hierarchy. The entire hierarchy can be seen in this picture. Data parallelism, or single instruction, multiple data, or SiMD for short, is the lowest level code often needs to do the same thing to a bunch of regular data elements. For example, shading a pixel instead of shading a pixel instead of a single instruction to process a single data element.
00:04:52.188 - 00:05:46.978, Speaker A: It is more efficient in area, time and power to have a single instruction operate on an array or a vector of data elements in parallel. This quickly uses up all those extra transistors and bandwidth, but it only works if the compute and data are near enough to keep the latency low, and we don't shuffle around the data too much to form these vectors. Data flows and layouts tend to fossilize very early in development because all the downstream code depends on it. Far from being an afterthought in the late stage development, tuning paths, data flow, and layout are the first things to consider in a new design. SSE AVX AVX 512 and an X 86 cpu and the warps in the thread block in the gpu are modern examples of data parallelism. The next level is shared memory or thread or multiple instruction multiple data or MMD parallelism right next to our data. Parallel cores are small, fast memories used in part for threads to communicate.
00:05:46.978 - 00:06:33.170, Speaker A: If we can't fit our application into these memories, there's also an interface to large remote slow memories. It's akin to hitting swap. Careful management of how threads are distributed over physical cores, how virtual address spaces are mapped to physical memory is a key to security, performance and scalability. Felix's earlier talk touched on this a little message passing or distributed memory parallelism is the next level. A network on chip akin to a local area network passes packets between components. Hardware engineers often use the term flits for these. There is also an I O interface akin to a wide area network connection, and the further apart components are physically interchiplet, NumA, node, socket, host, data center, and logically different address spaces, processes, and host.
00:06:33.170 - 00:07:27.090, Speaker A: The more we need to focus on the messaging that's going on under the hood. While programming languages make memory access look cheap relative to computation, it's not often clear when and where memory access are happening. The seemingly trivial act in code of modifying just one byte in memory is akin to resolving a symlink to a file that's the virtual to physical address translation, locking and opening the file that's getting exclusive access to the cache line containing the byte, copying the file into local memory, and that's going to actually be double cache line for most things today. That's two orders of magnitude more data traffic than you're expecting. And that's the making the local copy of the cache line pair to grab it and then actually doing the modification, which is pretty fast, and finally closing and unlocking the file. And that's the eventual cache line write back in eviction. If this sounds hellishly expensive, that's because it is.
00:07:27.090 - 00:08:05.014, Speaker A: This slide shows the messaging done for an aligned read. There's a lot of intracore messaging going on, and if a local copy of that data isn't available, we have an intra host packet storm. Note that unaligned cache line straddling reads will double this storm. And note that this slide was taken from a several years old presentation about a several years old architecture. The storm is worse today, due to cache coherence changes necessary to support even larger core counts. The way this one works, the traffic would scale quadratically. That is, the mental model encouraged by programming languages is backwards.
00:08:05.014 - 00:08:44.404, Speaker A: In the real world, compute is cheap and data motion is expensive. This mismatch causes developers to unintentionally write programs that leave orders of magnitude on the table. Computer science conventional wisdom, loved by cloud providers everywhere, is large efficiencies in code running on a single host magically become ignorable when running on thousands of hosts. Maybe there is a case for this in one off, one and done prototypes. But prototypes that work have a nasty habit of turning into large scale, mission critical code real fast. Minimizing machine footprint, minimizing I o cost, minimizing power usage. Maximizing performance is worth the developer effort for large scale, mission critical code.
00:08:44.404 - 00:09:40.152, Speaker A: So design those prototypes accordingly. Now developers hate typing. Minimizing the amount of code to type seems to be a key driver for many modern software engineering practices. This problem would fix itself overnight if programming languages evolved to make typing computation easier than typing data motion, developers would finally have the right incentives and would go out of their way to minimize data motion relative to computation. Heterogeneous parallelism is the last level big picture hardware architects have a transistor switching budget that is set by the available area, clock frequency, power, cooling, process technology, and people time, or equivalently, their design and manufacturing resources they can afford. The big question is what to do with it. CPU's traditionally allocate a large amount of their budget to caching to try to hide speed of light delays.
00:09:40.152 - 00:10:11.936, Speaker A: GPU's try to hide these delays with massive fine grain parallelism. They traditionally allocate a larger amount to compute, and this is a key reason why GPU's are faster than CPU's for raw number crunching. Now, a recent architectural dilemma is dark silicon. The speed of light makes distributing software over many general purpose cores hard, especially software with missiles. Optimize compute data motion tradeoffs. Cooling all those cores is at least as hard. Perpetually increasing the number of general purpose cores then, seems to be counterproductive.
00:10:11.936 - 00:10:57.012, Speaker A: What should we do with all those extra transistors now? Well, for a specific task, a core specialized to that task will be smaller, faster, cooler, and less power hungry than a general purpose core. We can allocate some of our budget to cores, optimize for specific purposes, and keep our fingers crossed that no one find something useful to do with all of them. That requires running them all at the same time. And just in case we'll downclock the heck out of any enterprising developer who does before they melt the chip. Now, each talk we are giving reflects an innovation at a different level of parallelism. At the end of each talk, we'll have a five to ten minute break. Last year, during our on stage Q and A, we had difficulty hearing the audience members, and vice versa.
00:10:57.012 - 00:11:53.514, Speaker A: So during these breaks, just feel free to bring your questions to us directly or other fire dancer engineering team members. You can identify them by their colorful shirts and also note that due to some scheduling constraints, Felix talk was given this morning. He's here in the audience right now to take questions also. With that out of the way, let's talk about accelerating ED 250 519 with data parallelism ED 2500 519 is a cryptographically secure digital signature algorithm based on twisted Edwards elliptical curves. It is used to verify message authenticity, or in other words, Alice uses her private key to sign her messages to Bob, and Bob uses Alice's public key to verify Alice's signature. It is computationally infeasible for Mallory, who does not know Alice's private key, to forge Alice's signature. The pseudocode for this verification algorithm is here.
00:11:53.514 - 00:12:54.984, Speaker A: See IETF RFC 8032 if you want the more details, and we're not going to cover that here, we won't need to, since all messages from untrusted sources and secure applications need to be verified. Verification limits the performance for a target hardware cost, and it also bounds the hardware cost for a target performance level, that is, to 0th order. Applications like firedance are our signature verification engines. Though ED 250 519 is considered fast for a digital signature algorithm, it is still very expensive, and critically, it was not designed with data parallelism in mind. The double scalar elliptic curve point multiplication is the most expensive part. Like many algorithms, including the AE's encryption used by quic we demoed earlier this year and the Reed Solomon used by shreds in the subject of Philips talk, this multiplication uses Galois Fields my background is more computational physics and continuous signals than cryptography and discrete math. When I learned Galois fields many moons ago, I learned it the hard way by googling around.
00:12:54.984 - 00:13:48.154, Speaker A: Cryptography is about making information hard for humans to decipher. The cryptography literature here is no exception. It isn't human readable. So typically when I Google Tech topic x, the first link is a Wikipedia page whose maintainers have a mission to keep the page exactly as they want it, incomprehensible to somebody who doesn't already know the topic. Links two through six are half years old, half written PDF lecture notes from an advanced seminar on x that, while better at explaining the topic, end prematurely when the professor got bored making slides, leaving the important bits an exercise for the reader. The rest of the links tend to parrot back this info, often incorrectly, because their authors were just as confused as me. Sometimes I'd get lucky and stumble across an abandoned geocities page with a lime green background and yellow scroll text that, while ugly enough to make my eyes bleed, does a genuinely useful x for dummies.
00:13:48.154 - 00:14:34.204, Speaker A: I didn't get lucky with Galois Fields, but understanding what we've done here requires some background. So here's a Galois Fields for dummies that I wish I had Wikipedia says a finite field of order Q exists if and only if the order Q is a prime power p to the k. What this really means is computers like them two is a prime, and that means that the number of elements in a Galois field maps well onto computers that think in binary. Galois Fields would be an academic curiosity if this was not true. Wikipedia also says that I'm not going to bother to read this text. What it really means is that it's grade school math, right down to the precocious math student pestering their grade school teacher. What happens if we divide by zero? And getting the explanation shut up.
00:14:34.204 - 00:15:09.476, Speaker A: When I started working with Galois Fields, I was constantly fretting, this feels like grade school math, but there's probably some subtle difference buried in all the incomprehensible explanations that will burn me. The special notation discussions about irreducible polynomials and residues and quotient rings and Fermat's little theorem and whatnot all give the false impression. There is something else, something much harder going on. Well, like that old expression. Nope. In fact, it is easier than grade school math for real numbers. Nothing taught in grade school works because of rounding errors.
00:15:09.476 - 00:15:40.048, Speaker A: We need to learn methods not taught in grade school if we want to do anything serious. This is not an issue with Galois Fields. There are no rounding errors. Grade school math methods work right out of the box, Wikipedia says, and I'm not going to bother to read this one either. What this says is it doesn't blow up. For example, when we add the same number over and over again, we eventually come back to where we started. For this to work, we can't use the addition and multiplication tables we learned in grade school.
00:15:40.048 - 00:16:23.854, Speaker A: All that complexity I was complaining about concerns how to generate those tables, what properties they have, and so forth. But we don't care about any of that here. The only thing we care about here is how can we do the addition, subtraction, multiplication, and division needed for ED 250 519 fast now. ED 250 519 uses a Galois field where the number of elements is a prime. Power two to the 255 -19 which is kind of why it's named what it is. If these elements are mapped onto the whole number, starting from zero, adding, subtracting, and multiplying are almost grade school math. Specifically, just do the grade school math and then take the remainder of the result divided by this prime.
00:16:23.854 - 00:16:58.606, Speaker A: Division is insanely hard to do. Binary this prime is almost all ones. This is not an accident. If we had a 256 bit processor, we could do the grade school math part directly. The almost part wouldn't be that bad either, because of all the ones in the prime binary representation. Now we don't have 256 bit processors, we have 64 bit processors. Those don't do 256 bit math, much less almost 256 bit math, much less the insanely hard division implementation.
00:16:58.606 - 00:17:26.398, Speaker A: Performance then, is largely a question of how fast can we emulate this math. Our first and primary concern is optimizing the 256 bit part. Once we have that, and that's all that we'll have time to cover. In this talk, we can worry about the rest. Note that custom hardware can implement the exact math we need, which is relevant to Cabe's talk. So we need to know what math general purpose cpu's and GPU's can do. On X 86.
00:17:26.398 - 00:18:10.614, Speaker A: We can do a 64 bit number times a 64 bit number to get a 128 bit result. Fast instructions that update more than 164 bit register at a time are not common on other architectures, and they are very special case on x 86 too. Outside of x 86, we get things like a molo instruction. This returns the lower half of that multiply. A mole high instruction is also sometimes provided for the upper half, and the power and performance implications of that should make us all a little bit sad. Languages cater to the lowest common denominator. For cross platform portability purposes, C and c only expose to the developer the concept of molo, for example.
00:18:10.614 - 00:19:08.634, Speaker A: So contrary to conventional wisdom, languages, even low level ones, do a very poor job of exposing the underlying hardware. Here we need the upper and lower halves and have often spent the area, power and time to compute them. But the language has misguidedly thrown away half of what we need, and this is the critical operation in many algorithms, including this one. That's a factor of two to four in performance in power due to the mismatch between hardware capabilities, programming languages, and developmental models. Correctly designed instruction sets, languages and application binary interfaces would allow us to use wider integer type flexibilities and efficiently return multiple values. Then, when we want to do a proper 64 bit multiply or whatever, the compiler could emit fast operations for it when available and emulate it when not. In the meantime, we are stuck with hacks like inline assembly language, nonstandard compiler extensions, and non portable vendor intrinsics to call back these capabilities.
00:19:08.634 - 00:20:01.404, Speaker A: Even then, the lack of proper linguistic and ABI support still hinders us significantly. But if I were to tell our internal trading community that we need to drop the performance of our systems several fold while spiking the cost through the roof to avoid offending the delicate sensibilities of computer scientists who have been miseducated how computers work I don't think I'd be happy with the response, given developers a good locality aware machine abstraction while modifying languages and architecting hardware to map cleanly onto that abstraction is a key factor in Nvidia's success in high performance computing. How do we emulate the math we need? With this, we're going to start simple. Multiplying two one digit numbers makes a two digit number. For example, nine times nine equals 81. We are drilled in grade school to memorize our multiplication tables. Hardware lookup tables are common for this too.
00:20:01.404 - 00:20:44.494, Speaker A: To multiply an m digit number by an n digit number, we were taught to break it down into m by n single digit multiplications and then add it up in a special order. This yields an m plus n digit output. While doing this by hand is the bane of existence for grade school students everywhere, it is a lot better than adding xy time. Adding up all the temporaries in a column requires carrying the amount that overflows into the next column. This creates a long sequential dependency chain to compute the output. Grade school often teaches a method where the carry propagate is done in two stages, but it doesn't change the actual length of the carry propagate. A basic multiplier and a processor can be implemented similarly.
00:20:44.494 - 00:21:32.416, Speaker A: For example, in a 64 bit processor, the inputs are 64 digits and the digits are bits. A single digit multiplication then is trivial, a single and gate, but the sequential dependency chain to add it all up is very long. The area needed for such a multiplier scales quadratically. A multiplier on a 64 bit processor needs roughly four times the transistors than on a 32 bit processor, and it's slower due to the longer sequential dependency chain. This is also why processors started at eight bits and walked their way to 16 and 32 and whatever. Hardware engineers don't want to make multipliers any bigger than necessary because they are big, slow, power hungry circuits. Now we can reorganize the sums to expose data parallelism we didn't learn, or at least I didn't in grade school.
00:21:32.416 - 00:22:19.248, Speaker A: Instead of doing m by n single digit multiplications, we can scale an m vector n times. But we still have that pesky sequential dependency, and if we think about digits more flexibly, we can get rid of that too. For example, the number 345 is the same as the number 123 15. Multiprecision math libraries use the term limbs for these generalized digits. Now we don't have any sequential dependencies, so the column sums can be done data parallel we only carry propagate either partially or fully when we absolutely must, to avoid overflowing the limbs. This is a lot cheaper than doing a full carry propagate every operation. Applying this, we can emulate 256 bit math with 832 bit integers and a 64 bit molo exposed by the language.
00:22:19.248 - 00:23:19.196, Speaker A: This requires eight times eight or 64 molos, and unpacking the results into its low and high parts. Since these single digit multiplications are data parallel, and there's a lot of them, this can be done about as fast as portable code can get. But since there's no room in this representation to defer carries in the inputs and outputs, we need to do a full carry propagate every time. And this part is slow, slow, slow. We can improve this by using 1026 bit digits. Storing this naturally in a 32 bit integer yields six carry bits. This is used by many ED 250 519 implementations, including OpenSSL, Dalek, and r portable implementation the most important implementation detail this is the most important implementation detail, but there is a lot of additional techniques under the hood that we will not have time to cover, and technically, these implementations use a mixed representation with some digits 26 bits and some 25 for some additional optimizations.
00:23:19.196 - 00:24:08.494, Speaker A: And while this sounds exotic, it really isn't conceptually any different than the mixed weeks, days, hours, minutes, seconds system we use for timekeeping. Both DaLeC and our AVX accelerated implementation organized the higher level elliptic curve math into groups of then independent multiplies, and do four of these at a time data parallel for additional performance. Now let's take a seemingly off topic detour. This is a breakdown of an IEEE 754 number. There's a sign bit, eleven exponent bits, and 52 mantissa bits. How it maps onto a real number is shown by the formula when we multiply two double precision numbers. Computing the exponent in sine is cheap, but computing the new mantissa requires a 52 by 52 bit to 104 bit integer multiply.
00:24:08.494 - 00:25:01.984, Speaker A: AVX 512 supports computing eight data parallel double precision multiplies in a single instruction. Recent cores can do two of these per clock with a latency of four clocks. This is instruction level parallelism and instruction pipelining, and yet more examples of our massively parallel speed of light constrained world. That is, an AVX 512 capable core is akin to a GPU MPU with a 64 byte warp instead of 128 byte warp, and then it's one that's programmed from the point of view of the vector instead of a vector lane, or CuDA thread, as CuDA misleadingly calls it. Devs have been trained to optimize the wrong thing. If you do the math out of this, the marginal cost of multiplying two numbers in optimized code is measured in picoseconds. Nowadays, the marginal cost for moving data around is orders of magnitude worse.
00:25:01.984 - 00:25:49.494, Speaker A: Now, this implies there are a ton of big, fast, power hungry 52 bit integer multipliers on an AVX 512 core. Now, we don't care about floating point here, but it'd be real nice if we could use these for other stuff. Well, hardware engineers agree. They want to find additional uses for all this area to amortize that cost. AVX 512 IFMA extensions allow us to use the vector double precision multiplier for integer arithmetic. These vector integer instructions have the same performance as their double counterparts. Unfortunately, we need to do two operations per 52 bit integer multiply because of the same molo mole high limitations we talked about before.
00:25:49.494 - 00:26:46.396, Speaker A: This inefficiency is partially compensated by the fact this also does an integer add at the same time, the seemingly bizarre hard coded masking, fused ad operation, and whatnot all makes sense given the requirements of a double precision multiply cross lane. Motion limitations and the instruction binary format limitations this capability has been under the hood for decades, but has not been usefully exposed to developers until now. This is all too common. All sorts of useful goodies exist under the hood that hardware designers and languages don't realize is useful to expose to developers, and wrongly assume that because developers aren't doing that thing that there's no demand for it. Most developers don't even know the hardware is doing it and don't even think to ask for it. This dynamic is also related to the GFNI instructions that Philip uses for his almond work. I've been waiting for this to be exposed someday, and I finally got my wish.
00:26:46.396 - 00:27:28.172, Speaker A: Let's see what we can do with it now. The documentation and the academic literature suggest the optimal way to use this functionality is to represent a 256 bit number as 552 bit digits. We can then store the digits in an eight wide 64 bit vector register. Use the Mad 52 instructions to do a data parallel single digit multiplications. So instead of needing ten times ten 100 single digit multiplications, we only need to do five times five. That's 25, and we can do five of them in parallel. With the factor of two inefficiency for the split molo mul high instructions, we end up with about ten fast independent vector operations.
00:27:28.172 - 00:28:22.610, Speaker A: For this part, though, it seems like we should have twelve extra bits available to eliminate the carry propagation two owing to its floating point origin, Mad 52 operations mask the input carry bits. We are then stuck doing a full carry propagate after every operation in a long calculation. This is partially offset by the fact the carry propagate heat chain is half the length of the obvious portable implementation, and we can bake some of that into that fused add. Still, even with those inefficiencies, the fact that we're getting about an order of magnitude fewer instructions. It's a major win for our implementation and Dalek's IFMA accelerated implementation. Converting to this representation speeds up our existing AVX accelerator implementation by low to mid tens of percent. But we are wasting three eighths of our vector lanes, and the floating point weirdness we inherited is slowing us down.
00:28:22.610 - 00:29:17.304, Speaker A: We can do better instead, we can represent a 256 bit number as 643 bit digits and store them in an eight wide 64 bit register. Now we have six times 636 single digit multiplications that we can do six at a time. We end up with around twelve fast independent vector operations for this part. Critically, we also get nine usable carry bits per input and output digit. While we need to be clever to work around the 52 bitness of these instructions, we no longer need to do a carry propagate after every operation. Better still, this representation supports a fast data parallel approximate carry propagate and a fast data parallel approximate modulo that we can use to further accelerate our elliptic curve math. Using this speeds up our existing AVX accelerated implementation by mid to high tens of percent, but we still have wasted 25% of our vector lanes.
00:29:17.304 - 00:30:35.564, Speaker A: Now, previously, we were able to accelerate our portable implementation by doing multiple gal WoW field operations in parallel, and doing this yields our current AVX 512 accelerated implementation. We represent a 256 bit numbers as 643 bit digits with nine carry bits and store them in three eight wide 64 bit vector registers. In the organization shown, we use the same Mad 52 algorithm to do four multiplies data parallel, and as before, we organize our elliptic curve math to do up to four of these at a time and minimize the number of carry propagates between stages. We get 100% lane utilization and massive data parallelism. The end to end clock for clock performance is around quadruple the existing portable implementation and double the existing AVX 512 accelerated implementation. The pull request shown here has additional documentations and tricks that we don't have time to cover. Put another way, our AVX 512 accelerated ED 250 519 implementation is over double the performance per core clock than last year's demo, but we can't use cheap cores.
00:30:35.564 - 00:31:26.108, Speaker A: This requires cores with AVX 512 IFMA support, though, we don't have time to cover this. We also applied GPU style data parallel techniques to accelerate our batch SHA 256 and SHA 512 implementations with AVX 512, and roughly doubled the performance there, too. These now have enough throughput to handle over 15 gigabit sustained equivalent Ethernet line rate per core. This includes key operations like forming Merkle tree signatures. Note that this is over double the throughput of our existing implementation based on Intel's SHA hardware accelerators. This is not because there's any issues with Intel's accelerators. Rather, it is yet another demonstration that because of speed of light delays in it is much easier to do n independent things in parallel in time t than it is to do one thing in time n divided by t.
00:31:26.108 - 00:32:06.712, Speaker A: Even when you throw custom hardware at the problem, Kavi's talk will give you a flavor of just how hard this is. Now, on this slide, we have a list of chains that, to the best of our knowledge, don't use multiplication. That is, this technique is generic. Not only can we use it to accelerate many cryptographic protocols, we can use it to accelerate large number multiplication. Generally, the techniques in the other talks are just as general. Further, we have been developing this in public under an open source license, and the source code is available on GitHub today. Now, this concludes the technical portion of this talk, but I do have some closing words that I'll give at the end of cave's talk.
00:32:06.712 - 00:32:21.704, Speaker A: That is, this is a good place for a break, and as a reminder, fire dancer team members are in the audience and available for questions during these breaks. In a few minutes, we'll resume with Philip's talk, talking about message passing, parallelism, reed Solomon coding and networking communications, and thank you for your attention.
