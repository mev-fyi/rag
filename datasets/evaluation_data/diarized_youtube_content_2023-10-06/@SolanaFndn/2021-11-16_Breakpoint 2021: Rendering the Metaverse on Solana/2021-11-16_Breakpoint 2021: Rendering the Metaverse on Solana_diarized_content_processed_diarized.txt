00:00:11.480 - 00:00:55.150, Speaker A: Hi everyone. My name is Jules Erbach. I am the founder of the Render Network, and I am here today to talk about rendering the metaverse on Solana. So, for those that aren't familiar with render, let me give you a bit of background. We started the render network based on GPU rendering on the cloud, and our mission, both personally and as a group, has been to democratize content creation. So you probably will know much of the work that's done on our rendering software. We make a popular 3d graphics tool called Octane Render.
00:00:55.150 - 00:01:50.428, Speaker A: And today you can run octane render jobs on the blockchain on render. And these videos that you're seeing are all rendered on the GPU. Netflix, HBO, Disney all utilize octane and the render network to create their content. Most recently, the Apple keynote, which I was in, showed a piece of structured content that was actually rendered on render for the keynote that Apple showed. So the reason why we have this gpu compute network is to allow the very fast turnaround of these high quality frames. Now, today, these are offline renders, but in the future, we're going to be targeting real time graphics, and the same quality that you're seeing here for these movies and films and television shows will be rendered holographically in three D and in real time. But we need a lot of compute power for that.
00:01:50.428 - 00:02:35.618, Speaker A: That's the promise of the metaverse, to have this blending of real time, shared, persistent states and quality that rivals films or surpasses them. So you'll probably also notice marvel movies that are done in octane. This is all from Ant man the Wasp. Two minutes, Captain Marvel. All of that opening sequence was done in Octane. And the reason why we have so much rendering power is that a lot of miners bought GPU's four years ago to mine Ethereum, and then that stopped making money. What we do is we take the costs of a dollar of rendering on Amazon, and we pay out either a dollar or half a dollar or a quarter of that for the same amount of work on the render network.
00:02:35.618 - 00:03:04.536, Speaker A: And on top of that, we have plugins for every single 3d tool. So there's no software that you need to go and learn to use render. If you know Photoshop, if you know blender, if you use unity or unreal, we have plug ins for all of that. It connects directly to the render network. And four years ago, we were running those jobs on Amazon, and today we're running them on a decentralized cloud, 10,000 active users. It's growing fast, and we're doing about a million tokens worth of business every month. And that's growing fast.
00:03:04.536 - 00:03:49.392, Speaker A: It's tripling. And probably by this time next year we'll see about ten X. So our very first commercial job on the render network when we launched was John Noel. He created Photoshop, he's the head of ILM, and he did a job for the Hayden Planetarium on render, and basically was the first person to tell us this product is amazing and we love it. And since then we've actually been doing so many jobs that most of the NFTs that you're seeing, including most of Beeple's work, Pac's work, these are done on the render network, and we've also added other renders. It's not just our own software, obviously towards a decentralized metaverse. We need to allow for third party software, things that we don't create to run on this system and to make it completely open.
00:03:49.392 - 00:04:35.098, Speaker A: So we brought in our 1st 3rd party renderer, Arnold, and further renders are going to be added using the render SDK. So obviously I'm here to talk about Solana and why we are so bullish on it. So let me start by saying that we started on Ethereum, layer one. We moved to Matic because the gas fees were so high, they changed the polygon and we kind of like it. But there are problems that we, that we're seeing on side chains, right? So Solana solves so many problems at once. For one thing, we want to be able to write high performance code. That means C, Rust, open shader language, we write our own compilers, and all those pieces need to run on GPU's, but also on chain.
00:04:35.098 - 00:05:08.742, Speaker A: And we can do that with Solana. The VM can basically take C or rust code and turn that into a smart contract. The other thing that's critically important, most of the forward looking work that we're doing on render is for real time. Even though we're talking about massive amounts of GPU rendering, we want to pull those GPU's together and deliver a real time stream. You'll see why that's important in a little bit. But we need to be able to change and share states for thousands of users in a scene on chain. And there is no trade off between speed and security other than Solana.
00:05:08.742 - 00:05:43.382, Speaker A: We've looked at everything, and Solana is actually delivering what we need for that. So if you're talking about powering a metaverse, where even if it's just bounding boxes, not complex physics, just bounding boxes, collisions, that's something that Solana can handle. I haven't seen that on any other chain. And if the speed is kind of approaching that, it's because you have too few nodes and security is put at risk. So for us, the security that Solana offers at this speed is incredible. That's really what sold us on this very important migration. And for us, forward looking things on render that are focused on real time are going to be done in Solana.
00:05:43.382 - 00:06:26.226, Speaker A: And while we have Ethereum and Matic and all those things, there is that bridge, that wormhole will allow us to keep that data. But there's so many benefits that we get when we're looking at Solana as our primary chain. For one thing, fiat payments are trivial, and there's another really latent and important component, and I think that's the Solana ecosystem. So I want to talk a little bit about Metaplex. So metaplex is obviously an early but really powerful and important NFT marketplace. And we are going to enable one click rendering from anything that you do on the render network to Solana and then to Metaplex. And the reason why metaplex is unique is for many, many reasons.
00:06:26.226 - 00:07:08.284, Speaker A: So let me start by describing some of the problems that artists, and we have thousands of artists on render that are making nfTs. They made about $700 million worth of NFT sales just this year. 115 might be people, but still, that's a pretty large amount. And a lot of them want to create things that are just not possible on Opensea or the marketplaces that are out there. And so as I've been sort of ingesting all of this feedback and information, we've been looking at what Metaplex is doing, and they've just been amazing. They've offered us pretty much anything that you want on your wish list, we will implement. So what do artists really want? They want to be able to create 3d metaverses.
00:07:08.284 - 00:07:35.102, Speaker A: I mean, basically these are unreal engine apps that you're forced to download and run on a pc. It's a whole kludgy thing. They want to be able to stream that which is not really easy to do on Opensea or nifty or any of those other places. And then they want the flexibility to be able to update these things live. They want interactivity, they want to be able to mix things together. And that portability is something you need to program. You need to have the storefront inside of the NFT, inside of that 3d world.
00:07:35.102 - 00:08:09.868, Speaker A: And those are pieces that we just can't create in existing marketplaces that are incentivized to sort of create and build off of the models that have been around for years. Whereas with Metaplex, it's code like we can basically do anything that we want. We can add live streaming to nfTs, and you'll see why that's really cool in a little bit. We can obviously support custom storage. Those are built into the artwork themselves. We have logic nodes, for example, pack that makes procedural renders. Those things can be fueled by oracles, they can be fueled by other things on chain, and they can be built into the actual asset itself.
00:08:09.868 - 00:08:44.737, Speaker A: The context that you buy it can make all the difference. And we can also support web like integration between different, not just art pieces, but also services like rendering as a service, AI as a service. These are all built on the render SDK, but they can interoperate. And more importantly, you can create payment flows that handle services, hundreds of different creators that are contributing each piece of a 3d model to a single NFT. And this is necessary. There's no success for NFTs in the future or the metaverse without these pieces. So Metaplex is an enormous opportunity for us.
00:08:44.737 - 00:09:22.924, Speaker A: And now I want to show a little bit about where the NFT trajectory has taken us this past year. So we have beeple, we have PAc, you know, building and creating beautiful pieces of art in octane and on the render network. And where things are sort of heading is that, as I mentioned, is streaming, right. We want to be able to deliver the high quality work that you're seeing in those films and tv shows as a precomputed light field or stream that can play back in AR and VR. And we have many partners. We have Disney and Warner Brothers and discovery and others as investors. And we've been building this content pipeline for VR for years.
00:09:22.924 - 00:10:00.028, Speaker A: But now we have other opportunities, both with the blockchain, streaming, AR and mobile. And the reason why streaming is really critical is you're looking at this back cave scene for Warner Brothers, and that is an actual render from the render network, but it's also pulling in streams, it's pulling in live web pages, it's pulling in Unity games. And you can embed a unity game like this atom demo inside of your Facebook newsfeed. That was something that we pioneered years ago. And you can then bring that unity game remotely, locally, into a text. This is what the metaverse is. This is how the metaverse is going to work.
00:10:00.028 - 00:10:26.066, Speaker A: You're going to have two d and three d spatial relationships, but they're all going to render within each other. And we've had the benefit of working with pretty smart folks like John Carmack and Facebook. Six or seven years ago, actually, we did launch a contest with Carmack and Facebook called Render the Metaverse. And here we are. So I want to talk a little bit about how developers can use the render network. So we have something called the render SDK. It's a fully portable graphics API.
00:10:26.066 - 00:10:58.372, Speaker A: It allows you to compile pretty much all the code you want. And we dogfood that. All the rendering that you saw, all those beautiful graphics, they're all done in a render that we built on this very same framework. And it runs on iPhones, it runs on anything AMD, intel, and of course Nvidia. You can create really complex pieces of code, AI ray tracing, of course. And we have our own compilers, and it's all node based. So this lends itself incredibly well to decentralized rendering, but also to decentralized composition for things like virtual worlds.
00:10:58.372 - 00:11:46.164, Speaker A: And another really important piece that we're going to be doing with Solana. One of our engineering groups has been working on GPU compiling technologies, have taken it all the way to having C code and LLVM code basically cross compile to GPU and to chain. But Rust is a really interesting language, and we are going to fund an open source rust compiler that's going to be an extension to the render SDK. So if you want to run and create really secure code, and in our case, we want to open this to everyone, right? So C isn't necessarily the best, safest language to do that with. Rust is going to be an option. And once that work is done, this is something that the entire rust and Solana community can leverage, and they can leverage it with render or without it. But it's something that we feel is so fundamentally important, we couldn't skip that.
00:11:46.164 - 00:12:16.080, Speaker A: I want to show an example of multiple renderers running on the render network. This is octane, our renderer, and in a split second, I'm switching to arnold third party render. It plugs in. The render SDK actually makes this happen in real time. You can even mix renders, you can take the Unreal engine, you can take Blender's cycles and mix these things together. So while we pioneer gpu rendering, and I think we do a great job, we're going to let every renderer in the world use this system. And you're going to see more than just these first two.
00:12:16.080 - 00:12:57.500, Speaker A: There's going to be many, many others that'll follow. And we have an open source spec called ITMF this can leverage both data and also allow APIs to leverage the rendering network for real time, offline anything and unreal and blender, because they're open source, are really interesting and we want to integrate those inside of our system in the next year. I think the iPad and iPhone portion of render is incredible. There is 100 million devices we can run an iPhone eleven. We can run render net jobs, both real time, locally on an iPad and on an iPhone. And an iPhone eleven is actually pretty powerful. It's basically about the speed of an intel integrated graphics chip.
00:12:57.500 - 00:13:21.038, Speaker A: The same thing that half of Adobe creative cloud users use for Photoshop. It can render final rendering scenes. And Apple's put a huge amount of gpu memory on most of these devices. So my iPad m one has 16GB. That's double most gpu's on the render network today. And we have real time capabilities as well that we're bringing to AR. So this is some of the scenes that you were seeing before running an Arkit on an iPad.
00:13:21.038 - 00:13:51.236, Speaker A: It looks beautiful. We want to basically deliver obviously high quality offline content. But this, this kind of stuff running on an iPhone or an iPad, and potentially in some form or another, maybe in glasses, that's really critical. And to make mixed reality work, it's more than just rendering. We need to be able to take all the data coming from an iPhone or an iPad and use that to blend and relight the scene. Now, some of those things are handled by the phone, like tracking, for example, with markers. But other things are harder, like relighting shadows.
00:13:51.236 - 00:14:32.644, Speaker A: Like you're seeing an example of an asset that's basically being relit by the phone. And the render network can send it to the cloud for rendering, or it can do it locally and it can mix those together. So this is an interesting example. This is taking that silver robot is totally rendered, it's coming from render, and it's taking that tv, which is running live, and it's doing ray tracing on that. So this is something that if you imagine social networks, TikTok, Snap, Facebook or others, having this capability where you can take all that power of the render network and blend that with reality on a phone, live or on the cloud, that's pretty good. And we've been improving this technology steadily as we've worked with Apple. We've added things like motion blur, which is pretty cool.
00:14:32.644 - 00:15:14.302, Speaker A: It's just gotten better and better, to the point where you'll be able to make pretty great movies on your iPhone, but you're going to need the render network to do that for final frame rendering. And this is an example why we have these incredible viral videos that have been done even before the NFT craze. Like this one got 10 million views. This was done on a desktop on the left. We can now do it on an iPhone, and the render network will basically make this quality happen from your local device. You can be a filmmaker with an iPhone and don't have to learn all those other 26 3D tools to do this kind of content, all of it done in our software. And while we have final rendering for these effects on the right, you're going to see the iPhone rendering at the very same scene live.
00:15:14.302 - 00:15:53.384, Speaker A: That's me holding that phone, and it looks beautiful. So, you know, we need basically this massive amount of GPU power to basically allow your phone, these AR devices, to sort of interact with the world around you and to deliver these experiences. Now, you can set up these scenes, but when you're talking about rendering massive virtual worlds, beautiful pieces of art, or IP, that's absolutely phenomenal. That's where you need more memory and you want to have this decentralized compute power. So we've been building these archives for artists like Beeple and Alex Ross to give them a way to sort of put all their content, their life's work, inside of an open metaverse. So we have a data format that is called ITMF. It stores all this data.
00:15:53.384 - 00:17:00.024, Speaker A: Artists like JJ Abrams use the same system, and we have this beautiful content that's been sort of put together this year that isn't just about archiving and creating nfts. In fact, it's not even about that. It's about allowing these pieces of art from 2d artists and be mixed together and to sort of show the way towards interoperability. And we have things like 3d scanned data, you know, paintings that we need to turn into 3d models through AI. And it's kind of a future playbook, I think, for the metaverse. Like, we need to be able to deliver tools that allow artists to sort of create things without the pressure of nfts. I mean, Beeple only does about two nfts a year, but he obviously does 365 and a quarter everydays, right? For him, the render archive, and this was something that he contributed enormously to, is all about providing a tool chain where his entire body of work is just stored for prosperity, right? And when he does something inside of our tools or on the render network, well, that data, everything, the Photoshop layers, the cinema 4d file, and of course, the rendering, when he does a post that work is there, you can then explore that piece of content.
00:17:00.024 - 00:17:38.596, Speaker A: You can edit it, you can bring it into 3d tools, you can put it, you know, into AR, which we're going to show here. And this changes the game. I mean, NFTs aren't just a 3D model or video or static file. They're the entire graph of composition that goes into a beeple piece or in the case of other artists, a painting or Photoshop layers. But all of these scenes are composable, not just in 3d space, but time and in five dimensionally. Right? If you have a thematic theme, this will work. We also have future devices like holographic displays, which I think people didn't take seriously until beepold showed human one, right, which is this physical display using oleds.
00:17:38.596 - 00:18:06.250, Speaker A: But the future pieces of physical nfts, you're going to have these kinds of displays, and so will our iPads and our phones and holographic displays. This is a simulation. It's like looking out of a window. You don't have to wear glasses, there's no head tracking needed. It just gives you a pure light field, and there's nothing like it. Now, to render on these devices is about 1000 x more than 4k. So you need a lot of compute power, either locally or on the cloud or on the render network to deliver these experiences.
00:18:06.250 - 00:18:47.680, Speaker A: But fortunately, as we see the render network grow and the cost of rendering go down, we'll be able to map these holographic display panels to render jobs and even provide solutions for real time integration. But these holographic displays, that's the future. You may not need AR glasses when you have these panels that are as cheap as oleds are today, and that will happen in the next ten to 15 years. And that brings us pretty much to the Star Trek holodeck. And the last render archive I want to show and talk about is the Roddenberry archive. It's something that I've been working on with Gene Roddenberry's son, Rod Rondenberry, my best friend for 20 years, but the last four years in earnest, we've been archiving all of his content, materials. And interestingly enough, we've also been building 3d models.
00:18:47.680 - 00:19:09.620, Speaker A: All the blueprints for the Enterprise, for example, the team that worked on it, they've been building this on the render network, and that's what Apple showed two weeks ago. So this is the Enterprise on render. It's not as good as the movies. It's so much better. Every single detail of this ship has been created. The entire solar system, every crater on the moon, all the stars. It's massive amounts of data.
00:19:09.620 - 00:19:47.738, Speaker A: It's hundreds of gigabytes. And, you know, the ship itself is just beautifully created. And this is the kind of thing where if you're going to define what the metaverse is, you don't want to build a world, you want to build an object, and you want that object itself to define how it works in the metaverse. You take another object, you take other things, you take the planet Earth. How do these things work together? Well, if we have the Enterprise and we map out its 40 year history, its world line, we map every single frame of video and tv footage to something in 3d space that we can then pull back and render, which is what you are seeing here. It looks pretty good. Thats Star Trek one, Star Trek two, that's running on an iPad.
00:19:47.738 - 00:20:22.158, Speaker A: And it's incredible, the fact that you're going to be able to go through the entire Star Trek universe, the history of that Enterprise ship, and see it from any angle and participate in it. It doesn't even matter what you can do with it. At very least, it will give you the narrative history of Star Trek with perfect fidelity. And there's another thing. It's modal reality. In other words, when you see the show, when you see Star Trek, you can also pull back into the real world and see the sets, see where it was filmed, and you have this transposition, which is pretty phenomenal. So I'm going to close out by just showing some new beautiful renders from the 1960s version of the Enterprise.
00:20:22.158 - 00:20:56.800, Speaker A: I mean, it's just massive details. This is Kirk's head, you know, this is Kirk's room. It's just incredible. And the last bit I want to show is where things are heading. So we're going to have to fill that starship Enterprise with people and characters and actors, and render will be able to do that. And this image that you're seeing is a synthetic character, and it's using a ton of GPU power, but we will be able to bring that entire universe to life, and that's the future of avatars and humans in the metaverse. It's super exciting, and the ecosystem around render is just absolutely phenomenal.
00:20:56.800 - 00:21:05.494, Speaker A: And we're very excited to have been able to share this with you, and we're super excited to bring render to Solana and metaplex. Thank you so much, everyone. It.
