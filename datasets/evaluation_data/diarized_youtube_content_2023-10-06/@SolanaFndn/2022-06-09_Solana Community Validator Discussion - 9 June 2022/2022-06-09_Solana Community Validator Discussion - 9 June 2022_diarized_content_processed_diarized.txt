00:00:02.400 - 00:00:34.514, Speaker A: All right, let's get started. Welcome everybody to the validator community call. I just want to start off by saying thank you, thank you everyone for participating in the last restart. They are never fun and it's always a bit of a fire drill, but I would say this last one was very smooth and very well coordinated. A lot of good work from validators, RPC operators and developers altogether. So good job and yeah, appreciate all of you. A couple announcements to make.
00:00:34.514 - 00:01:19.448, Speaker A: So as you know, part of the restart was a bug in nonces. Nonces are currently disabled in Mainnet and in Testnet, and that feature will not be out this week, but will likely be out sometime soon, probably next week. As part of that there will be a feature that is activated to enable this new nonce code. So just be ready for a feature activation. Especially RPC operators will need to be on the latest version and ready for the feature flag to be switched. Let's see. Yeah, so as part of that, if you are on Testnet, make sure you're on one dot, 10 dot, two four.
00:01:19.448 - 00:01:52.072, Speaker A: On Testnet we have about 90% of the stake there, but about 5% that's not delinquent, that could still update. And then if you are on Mainnet beta and you already are on a version of 1.10, make sure you can upgrade there to 110.24 only if you are currently on a 1.10 version. Any questions on all that? Okay, great. So in brief, if you're on Testnet 110.24,
00:01:52.072 - 00:02:18.702, Speaker A: if you're on Mainnet and you're on a 1.10 version, also 110.24. A quick announcement about the delegation program. So there have been a couple bugs with the program. The signup has been broken for a while, about a week or so for the delegation program signup process. It is now fixed, so you should be able to sign up without any issue. Let us know if there's any bugs that you find.
00:02:18.702 - 00:02:55.654, Speaker A: But someone has gone through the process recently and it seemed to be working. Another issue there is the stake bot for Testnet is currently down. We're investigating that and trying to fix it. It looks like an issue with the RPC we're talking to, so I'm trying to get to the bottom of that now. But stake on Testnet is not currently being updated. Any questions about delegation stuff? Okay, great. Now a couple other things I put in the agenda, a link to have a one on one chat with me.
00:02:55.654 - 00:03:50.244, Speaker A: So if anyone is interested in just talking, getting to know each other, giving feedback of any kind about, you know, how how the validated program could be improved, how the delegation program could be improved, how anything that's on your mind really, there's a link in the agenda for this meeting to have a one on one call with me, so feel free to use that anytime you like. A little background here. I've been talking one on one with some validators and found the interaction pretty useful. Just to find out what's interesting to you, how I could be more helpful. So I love it if more people took advantage and just reached out. Another thing there in the agenda, there's a hardware survey that we're sending out. Alex, who runs the server program, is trying to get a better idea and the foundation in general of what hardware you are using.
00:03:50.244 - 00:04:29.374, Speaker A: So if you could take a few minutes, even just take a few minutes right now and fill that out for Testnet and Mainnet, what cpu, what memory and what hard drive spec you're using, that would really help us out. It only takes about a minute or less, so please fill that out. Oh, and it's an anonymous survey, so no personal information is recorded, just info about your hardware. Yeah, I think that's all the updates. Any questions on any of that stuff? No. All right, cool. All right, so for a talk today, we've got Dan from Holoplex.
00:04:29.374 - 00:04:44.534, Speaker A: Holoplex is an NFT marketplace that does some really cool stuff in open source. So Dan's going to talk about the geyser plugin and their implementation of it, or their implementation that uses the geyser plugin. So Dan, take it away.
00:04:45.474 - 00:05:07.254, Speaker B: Hey, Al. Yeah, Dan, CT of Olaplex. Really excited to go over our geyser implementation today. First, be appropriate to thank Brian Long for all his help kind of getting it off the ground. He's our value provider, our RPC provider, and has been super helpful in figuring all this out. It's been a journey. We've been doing it for what, four months now.
00:05:07.594 - 00:05:09.346, Speaker C: Yeah, it has been a journey.
00:05:09.490 - 00:05:10.290, Speaker A: Yeah.
00:05:10.482 - 00:05:12.266, Speaker C: Doing everything for the first time.
00:05:12.450 - 00:06:22.778, Speaker B: Yeah. We were on it when it was still accounts db back in the, the old days about three months ago. And so I was going to talk about the overview of our approach on the consumer side. So like, what happens when the data leaves Geyser and then go over the hardware specs of that entire stack as well as why open source has been really helpful here and we're going to continue to lean into it and then talk about some of the challenges we faced before, both on the validator side and on what we call the consumer side, which is the ingesting of the data. And so if you think about the whole geyser system as an ETL pipeline, the validator kind of serves as the extraction point, and then transforming and loading are what we do on the olaplex side. So the way that sort of our approach is we try and present this like unidirectional data flow for all our applications, where you send data to RPC when you sign a transaction with Solana. But every read we no longer rely on web RPC or JSon RPC.
00:06:22.778 - 00:07:25.456, Speaker B: We go right to graphql endpoint served in front of the indexed data. And so from the validator and from Geyser, we wrote our custom Geyser plugin and we send all our data accounts, data to RabbitMQ. And then from there we have several consumer instances that read off that queue system and process the data and shove it into postgres. The processing we wrote that processor, or the consumer, we call it, that's all in rust. And we import the program code from the Solana protocols that we're indexing to use their deserializers to take apart the account data and then convert it into postgres types. And so that's been really useful because you have kind of the, you can deserialize the structs really quickly, efficiently, and then you know exactly what they contain. And then converting them into postgres types is somewhat trivial after that.
00:07:25.456 - 00:08:02.774, Speaker B: Additionally, it allows us to index the data more intelligently because we have the contract code right in our code base, and so we know what it's supposed to do, how the data is used and how other people might want to query it. And so that's been really helpful. By the way, I can post a link to the repo of everything I'm talking about here, which might be helpful. That's the indexer repo. And so we consume the data using the program code. We shared it in the postgres. And then we also have a graph QL endpoint that sits on top of everything, so you can query it readily.
00:08:02.774 - 00:09:09.618, Speaker B: I can go into hardware specs of that entire stack and post a link to those here. And so here's a link to some of our hardware specs. And so just kind of working through what we offer, what we have to work with. We have five kind of consumer instances which are m, five x arches on AWS, so 16 gigs of ram. And those have the ability to scale horizontally as the number of messages in the queue stack up. So when a validator restarts, we get every account from all time into our RabbitmQ instance. At last count, it was like 25 million messages.
00:09:09.618 - 00:10:04.444, Speaker B: Because we listen to a select number of programs, that's a lot to process. And so we use RabbitMQ as sort of like a, to kind of handle the back pressure of that so we don't have to load all those in the postgres all at once. Excuse me. On the persistence layer we have currently, right now, we have two postgres instances that have a terabyte of storage each and 64 gigs of ram. And there it's a standard kind of primary writer set up with a replica for reading. The postgres instance has 100 gigs of storage and 32 gigs of ram and, or, sorry, the RabbitMQ instance. And then we have a variety of like, logging, instrumentation stuff built in as well, like Grafana and all of that to make sure everything's kind of running smoothly.
00:10:04.444 - 00:11:22.366, Speaker B: One of the areas that I also talk about is kind of the open source approach and what's been fun there. We have a number of protocol developers coming to us having the same problem of like, hey, how can I access my data quickly? And how can I do, like, aggregate queries, like counts and complex filters, the things that make, you know, JSON RPC is a great app, and so an approach that we've taken with several of them. So strata and Cardinal and Bridgesplit is they come and they send us an open source pull request to index their protocol, and then we provide a service and charge them to serve the data back to them as it comes in, in real time. And so this has been a really nice arrangement for everyone because they have open source code that they get to keep, and then it's their protocol, so they're able to index it intelligently and efficiently in a way they want it to be queried, and then we're able to provide a valuable service. Additionally, it allows for really fast and performing cross protocol queries where you can link from one wallet address, you can see every protocol they're participating in, and that we index, which is like 20 or so. And so for a given wallet, you can see the Twitter handle they linked in the naming service. You can see the nfTs.
00:11:22.366 - 00:12:08.064, Speaker B: They, you can see, like, things they might have fractionalized with bridge split, and all kinds of other things in between. And so that's allowing for some really powerful applications to be built on top of the index data set. And all that is 100% open source, which is great in terms of challenges we face, which I think is most relevant to this group. Excuse me. Indexing non fungibles was actually one of the harder things we had to deal with. So all non fungibles are part of the solar on the Token metadata or the token account, the token keg program. If you set your validator to spin off accounts from the tokenking program, you end up getting pretty much everything on Solana.
00:12:08.064 - 00:13:20.966, Speaker B: I think initially when we launched the geyser plugin or accounts DB, it was too much bandwidth really for the plugin to support. What we do is we import the fungible token list and load it into a hash set in memory on the validator and filter out any token keg, any account who is in that list. And so all the fungibles we don't send over the wire RabbitMQ into the whole system, we just look for the metadata. So that was one challenge. I think maybe Brian can speak to a few of the others we faced, but I think we found that, like, you can run into memory pressure limits on the evaluator pretty quickly if you are doing too many writes to RabbitMQ or in the wrong order. And a big question that we get a lot of times is like, why do you use RabbitMQ at all? Why not go directly to postgres? Having the validator be responsible for acquiring a thread to write the postgres, writing to it in the background and returning it to the pool. If that doesn't happen in 400 milliseconds, then you can run into a lot of memory issues on the validator, right? Because there's going to be a new block every time you have to acquire them and return them faster than a new block comes in.
00:13:20.966 - 00:13:49.644, Speaker B: So RabbitMQ lets us kind of like have a kind of safety net and backstop you should be able to do. Most postgres writes in under 40 milliseconds, but if you have a poorly written index or developer writes a bad query on an upsert, you can kind of run out of time pretty quickly there. Those are just kind of some of the challenges, having to like take any questions or talk more about any of that, those aspects.
00:13:55.034 - 00:14:40.874, Speaker C: Hey Daniel, you guys did a great job with this thing, and I hear plenty of people in the ecosystem talking about what you've done. So kudos to you and your team for putting in the hard work and all this stuff. For the first time, the current RPC stack deals with a lot of read write convention issues where, you know, heavy reads can stop the validator from writing. Have you noticed any read write contention issues after you've moved that into postgres you know, is there a limit to how hard or how many reads you can put in before postgres starts to slow down with its updates?
00:14:43.474 - 00:15:20.472, Speaker B: So right now we're processing about like, I think it's like on RabbitMQ, it's like 600 messages a second. And we have two validators running. One we do two for redundancy issues and two types of redundancy, really. One is so we never miss a message, and the other is like typical failover redundancy if one goes down, which we yell at Brian and he fixes it. But in terms of like contention, yes and no. So you have to, because when you start up, you get every account from all of time, you need these behaviors to be atomic. Like you can just get any account at a random time.
00:15:20.528 - 00:15:20.936, Speaker A: Right.
00:15:21.040 - 00:15:59.444, Speaker B: So you have to treat it that way because it kind of works that way. And that is compounded by having two validators. So we don't, we haven't really seen that kind of contention. Sometimes when we're doing the query to find out if we need to upset or just, you know, do a new insert or do nothing at all, you can see a little bit of slowdown, but that's not really like a Solana RPC issue. That's kind of like you could fix that with a different database implementation or sort of a different way of querying your data. So we haven't really seen like that kind of, that kind of slowdown. The latency has been really impressive.
00:15:59.444 - 00:16:21.474, Speaker B: Yeah. And I think like one really cool, like, party trick we like to do is you can mint an NFT and it'll be in our database before the browser confirms it, which is kind of cool because it's so much faster than the websocket connection to our pc, which is, which is great. I don't know if I answered maybe.
00:16:21.554 - 00:16:32.014, Speaker C: No, you did. The fact that you don't see a problem is really the best answer. Great work.
00:16:33.134 - 00:17:03.684, Speaker B: I think this question in the chat was meant for me, and it's a good one. So, yeah, this requires some trust on behalf of the Dapp, which is new. If you're doing, if you're used to a traditional RPC Dapp, one of the things that helps with that is open source. It's still open source. You can see the code. Additionally, we took a really strict policy in this database. All of the data is represented by on chain data, either arway by PF's or Solana.
00:17:03.684 - 00:17:43.284, Speaker B: And so the validators and the whole stack, I'm sorry, the indexer and its stack have this really neat trick where you can blow away the entire postgres database, restart the validator and restore state exactly how it was. And so yeah, theoretically we could produce this fake data, but we really believe that open source can provide component helps with that a lot. And we haven't run into anyone like with trust issues yet. I'm sure they're out there. No one can verify that we're running source. Yeah, that's true.
00:17:44.304 - 00:17:48.872, Speaker C: Yeah. And you guys released open source software so anybody can do it on their own.
00:17:48.928 - 00:17:49.168, Speaker A: Yeah.
00:17:49.216 - 00:17:51.152, Speaker B: They can render their trust us.
00:17:51.208 - 00:17:51.512, Speaker A: Yeah.
00:17:51.568 - 00:18:24.714, Speaker B: Like that's the solution. And just to speak a little bit to that, the people who use this data, like we're under SLA and contract, it's like a more normal SaaS relationship. And so I wouldn't really want like a random developer just coming in and building on some open end point. That's an RPC feature, not so much a feature of the service. Yeah, thanks for the question. That's a good one.
00:18:27.854 - 00:18:33.954, Speaker A: I've got just more of a general question. Do you see other applications for geyser beyond nfts?
00:18:34.294 - 00:19:09.160, Speaker B: Yeah, and we've talked to some of them. So we talked to the guys at stuck really smart, one of the earlier defi apps, and Solana, I think, and they forked our code to do. It's a pretty crazy thing they want to do. They want real time fungible updates in the browser. So they're going to have some websocket system that connects directly to RabbitMQ. Much different use case than what we're doing, but great, great geyser thing where real time data is just so important. Then additionally we use it for NFT stuff.
00:19:09.160 - 00:20:04.728, Speaker B: But a lot of the protocols that use us, they have overlap with nfts but aren't explicitly about it. So we index all of Bonefita and cardinal identity protocols which allow you to link Twitter or like your sole domain to a pubkeep. And so that's been really useful. We index some of the realms and smart wallet protocols. So for DaO voting systems, so we can kind of give people more granular data about how people are voting in their DaO, which has been really interesting and I think there's a lot more to explore there because that's like a really rich data set. It's how many tokens have you staged? Like what's your voting record? How many other daos are you in? We can provide all of that in a much more performant way. Like if you wanted to do business intelligence with RPC.
00:20:04.728 - 00:20:16.144, Speaker B: It's really challenging. And so you need that index data set. We haven't dipped our toe too much into kind of the other DeFi protocols, but I think that there's a. A lot of use cases available out there.
00:20:18.004 - 00:20:19.144, Speaker A: Cool. Thank you.
00:20:20.564 - 00:20:21.784, Speaker B: Thanks for having me.
00:20:22.404 - 00:20:42.084, Speaker A: Any other questions for Dan? No. All right. Any other questions in general that people want to bring up before we end the call? All right, cool. Well, thanks, everyone, for joining and see you all in two weeks or in discord.
