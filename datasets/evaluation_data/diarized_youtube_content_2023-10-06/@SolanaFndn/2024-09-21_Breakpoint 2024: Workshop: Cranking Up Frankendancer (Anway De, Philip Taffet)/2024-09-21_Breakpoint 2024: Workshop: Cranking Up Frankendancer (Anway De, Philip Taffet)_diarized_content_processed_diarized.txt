00:00:07.640 - 00:01:01.271, Speaker A: Hi, I'm Philip and today, together with my colleague Anwe, we're going to answer one question for you. How fast can the Fire networking stack go? Since the beginning of the project, high performance has been one of Fire's main goals. But why should performance even be a goal? Well, as Kevin mentioned in his talk yesterday, having a high performance validator increases the capacity of the chain, and performance of the sort we've been working on unlocks new possibilities for what can be done on a permissionless blockchain. And plus, performance is kind of our team's expertise. So for the next few minutes, I'll talk a little bit about some of the decisions we've made to get good performance with Fire and then I'll tee up onwe's demo and then onwe will crank up Frankendanswer showing how fast Firedancer networking stack can go. Alright, diving in immediately to the architecture. If you've listened to talks or seen our presentations over the last two years, you've probably seen a lot of diagrams that look kind of like this.
00:01:01.271 - 00:01:49.913, Speaker A: Boxes and arrows and more boxes showing how data flows through our system. The reason we always depict systems like this is because that's how we think about them. As Kevin has mentioned several times, getting the data flow right for a system is one of the most important decisions in getting good performance. Here the boxes represent tiles, a Fire Dancer concept that's a thread pinned to a CPU core, running the same code over and over again as fast as possible, receiving some kind of input data, doing some operation on it, and then providing the output data to the next tile. Another big benefit of the tile architecture is that it allows us to scale stages of the pipeline independently. So for example, here you can see there are a lot of signature verification tiles. That's because signature verification is one of the most expensive operations in a validator.
00:01:49.913 - 00:02:41.865, Speaker A: So we can add extra cores to that to get to balance the overall system throughput. I'll say a word or two about each tile and some of the decisions or the changes we've made over the last few years to get good performance. We'll follow the flow of a packet as it goes in from the network. So first, when a packet comes off the networking cable, it beats the Net tile, which handles interactions with the network interface card. From there, assuming the packet is a TPU packet, a new transaction submitted by a user with hopes of it landing on chain, it goes to the QUIC tile which runs the QUIC protocol. We've improved the performance, compatibility and reliability of the quick tile over the past year, especially in the case to prevent denial of service attacks. From there, the new transaction goes to a signature verification tile, which verifies all of its signatures.
00:02:41.865 - 00:03:40.873, Speaker A: We switch the signature verification tile to use the AVX accelerated implementation that Kevin talked about at Breakpoint last year. Core for core, this roughly doubled our performance. So if you compare with the performance the demo we did two years ago, with the demo, we're about to Show we needed 72 cores then and only about 30 now. From there, assuming that the transaction is fully verified, it funnels to the dedup tile, which throws out any duplicate transactions. And then the deduplicated transactions funnel into the pack tile. The pack tile takes a look at or examines each transaction and looks at the accounts that it writes, the number of compute units, a request, the fees that it's going to pay, and the consensus limits to decide when we're a leader, which transaction should actually go into. The block pack is a pretty important component because it's one of the few parts of the pipeline that's on the critical path, but doesn't scale wide, at least not easily.
00:03:40.873 - 00:04:27.411, Speaker A: You can't just add another pack tile. So we put a lot of effort into improving the performance of the pack tile, pulling out all the stops, all the CPU tricks like non temporal memory copies and AVX accelerated bit sets. And in a recent demo, we showed the pack tile operating at 10 gigabits per second. From there, the transactions that we decided to include in the block go to one of many bank tiles named for the Agave banking stage. For Frankendancer, these actually use Agave's code. They use the vm, the transaction execution, the accounts database from the Agave runtime. There's a huge amount of complexity hidden in that little word bank, and most of that complexity in optimizing it is outside of the scope for Frankendancer, and it's just the scope for Full Fire Dancer.
00:04:27.411 - 00:05:39.895, Speaker A: So for Frankendancer, the changes that we've made, the optimizations that we've made have mostly been eliminating small memory copies and reducing the cost of data marshaling. We've actually gotten a lot of mileage out of these. We've also put a lot of effort into parallelizing or scaling out the pack tile, making it more scalable, so that assuming we have transactions that have a lot of parallelism available, we can keep this a lot of bank tiles fed, so that even if the performance of one individual bank tile isn't necessarily what we'd like, we can get good overall performance through Many of them good aggregate throughput. From there, the executed transactions flow to the POH tile, the proof of history tile, which does some hashing and basically stamps them and orders them. And it sends the incrementally constructed block to the shred tile, the shred tile which chops up the block into small packet sized pieces called shreds and sends them out. We've updated the shred tile to use the AVX or the high performance Reed Solomon library that I presented at Breakpoint last year. And we also added a new heavily optimized data structure for computing each shred's destination, which is also one of the most expensive operations in a validator.
00:05:39.895 - 00:06:33.699, Speaker A: From there, the shreds flow to the block store, which or to the net tile which sends them out to the rest of the network and to the block store, which we haven't optimized for. Franken answer and just uses Agave's code. Franken Dancer is mostly about optimizing the leader pipeline. So when we're not leader, most of the Franken Dancer tiles or the Fire Dancer tiles stay out of the way. The one big exception to this is that shreds that come in still flow through the net tile and through the shred tile where they're verified, retransmitted and reconstructed. But the rest of the pipeline, things that are not part of the leader pipeline like repair and gossip and voting, those all run concurrently in the background using stock Agave code. Okay, that was a whirlwind tour through a lot of tiles and a lot of decisions that we've made.
00:06:33.699 - 00:07:05.255, Speaker A: But I want to sum up. The keys for getting good performance are open secret with just three concepts. The first is streamlining data flow. This is a key to get right because copying data around is often the most expensive operation on a modern cpu. From there we need to exploit parallelism in the right granularity, which is as coarse as possible, and in the right spots. Parallelism is a bit like fire. When it's in the fireplace it's nice, it keeps you warm, but if it's too much or it's in the wrong place, well, it'll burn you.
00:07:05.255 - 00:07:50.031, Speaker A: And then finally, we want to optimize each of our individual components to the nth degree. Here's where all the low level CPU tricks like the non temporal memory copies and AVX acceleration and and kind of more standard things like reducing unpredictable branches or having cache friendly data layouts. All these factors in because when you can optimize each individual component to a great extent and then you Combine them with two other points of having good data flow and lots of parallelism, then you get good overall system performance. Okay, enough talk. Now let's get into the demo. Actually two demos that Hanwei's about to show. So both demos will follow the same basic pattern.
00:07:50.031 - 00:08:21.713, Speaker A: There'll be a load generator producing validly signed simple transfer transactions. These will transfer a few lamp ports from one of 7,500 source accounts to one of 7,500 destination accounts. These are, you know, it's the native system transfer. So these are native programs, they don't require spinning up the BPF execution vm. They're a little bit cheaper, but they'll still show the full execution pipeline. The load generator will send a huge number of these to the leader. When they arrive at the leader, they'll go through that whole pipeline that I described, from the.
00:08:21.713 - 00:09:23.164, Speaker A: Net tile to the shred tile, and then the leader will send out the shreds to one of nine or to all nine followers. But we wanted to do this demo with not just some network that looked like these abstract blue arrows, but with something that looked like the real Solana network. So we took a look at the current mainnet validator distribution and you can see it's something like a big concentration of nodes in North America, another big concentration in Europe, and then a smattering of nodes elsewhere around the world, including a fair number here in Singapore. Given the constraints of where we have available nodes, what we're going to use for the demo is this. We'll have the load generator and the leader running in London, and then we'll have followers running in New York, Frankfurt and here in Singapore. For this demo we're just using 10 nodes, but we've actually done demos like this, pretty similar to this, on 100 nodes. All the nodes will have a 10 gigabit network, interface cards, but the connections between the data centers will use Jump's global network, which is 100 gigabit WAN.
00:09:23.164 - 00:10:03.995, Speaker A: The actual nodes themselves have dual 40 core isolate sockets and they're using Mellanox ConnectX 5.10Gigabit Nix. Each node has 512 gigabytes of RAM, though we won't need that much for the actual demo. And the software itself is open source, publicly available. It comes from the BP demo branch. You can see the commit hash up there and it's basically similar to our main branch with just a few changes that we've made to eliminate non networking bottlenecks. So the point of this demo is to show off the networking stack's performance, but you actually can't just have a chain that's just a networking stack.
00:10:03.995 - 00:10:46.845, Speaker A: If no one votes, the slot doesn't progress and there's actually no chain. So what we're showing is almost a real cluster, but we've disabled a few components to prevent them from getting in the way of showing off the networking stack's performance. For example, we're disabling writing to the block store other than what's necessary to get the component chain off the ground. Again, the block store is outside the scope of optimization for Frankendancer and it's a non networking stack component. Secondly, we're disabling the status cache which is a part of the agave banking stage and our banking tiles. It's a component that's necessary in production, but it limits the scalability of the banking tiles. And again, it's not part of the networking stack.
00:10:46.845 - 00:11:23.511, Speaker A: Compute unit rebating is a bit of a trade off. You can choose to pack transactions based on the executed CUs, which in some cases can let you pack a little bit more in a block, or the requested CUs, which is a bit cheaper. In this case we control all the transactions that we're sending, so we're not in the case where transactions are over requesting cus. So it actually ends up about the same. And so for the sake of performance, we decided to pack based on requests in cus. We've turned off CU rebate. And fourthly, we're ignoring the replay stage performance.
00:11:23.511 - 00:12:04.157, Speaker A: Again, you can't have a multi node cluster without having a bunch of non leaders and these non leaders are running the replay stage. We didn't want to do some kind of crazy surgery to sever out the replay stage or something like that. And we didn't want the replay stage to limit our performance either since again, it's a non networking component. So we're going to ignore its performance by measuring for the followers, measuring their performance at the edge of the boundary where the Franken answer code connects to the agave banking or the agave replay stage code. So this will be as transactions or as shreds. Leave the shred tile. And finally, one more bit of fine print.
00:12:04.157 - 00:12:34.187, Speaker A: What we're going to show here is a pre recorded demo. We actually did this demo live yesterday. I think some of you were there. And we've done this demo or variants of a, I don't know, three or four times at this point, including the one that I mentioned that's on 100 notes. But given the fact that we don't have A podium, the constraints of giving a talk like this, the risk of having a freak accident like a hardware outage or a sub C fiber going down. We decided that a pre recorded demo was a better fit for this talk. You'll still be in for a treat of a demo though.
00:12:34.187 - 00:12:36.175, Speaker A: With that I'll turn it over to Anwe.
00:12:37.285 - 00:13:18.509, Speaker B: Thank you, Philip. Okay, so first I would like to show you a little bit of network statistics about the cluster where we're running this demo on. So like Philip said, these nodes are distributed all over the world. I'm going to ping a node in New York from a node in London to show you the latency between them. And we can see that we have about 60 milliseconds of latency. We're going to repeat this and we're going to ping a node in Singapore from the same node in London. And let's see, it's about, it's 175 milliseconds, which is a little bit higher, but not too bad, I guess.
00:13:18.509 - 00:14:09.524, Speaker B: And then we're going to close the circle and we're going to ping a node in New York from a node in Singapore and that is about 230 milliseconds, which is roughly the sum of the two other ones that we saw. And yeah, so we have followers running in all these locations. And then before I actually begin the demo, I want to talk a little bit about the configuration that we're going to use for the leader and the follower. So first, the leader, we have a few things like the username on the left hand side, the username, the port ranges that the Frankendancer instance is going to connect to the directory where we're going to store the ledger. The layout is particularly interesting. It is unique for the leader. You'll see that we give most of the CPU cores on the box to Frankendancer Tiles and we have about 33.
00:14:09.524 - 00:14:49.893, Speaker B: We have 33 verified tiles, 19 bank tiles, four quick tiles and five shred tiles. We're also using 7,500 accounts that are preloaded into the genesis for sending all the transactions. If you look down a little bit, we have a couple of parameters, larger max cost per block and larger shred limits per block. We set those two to false to begin with. These are consensus parameters that I'm going to talk more about when I actually set them to true for the second part of the demo. But just to note that it's there. And then we have the, we disable the status cache, we disable the block store after slot 150 so that we can do a snapshot so the followers can join the cluster.
00:14:49.893 - 00:15:18.395, Speaker B: And some more parameters, log files, RPC parameters. Now moving on to the right side, we enable RPC so that we can query for the transactions that the cluster has processed, the leader has processed. But we disable some of the extended functionalities. We do a snapshot at slot 100 so it's quicker for the cluster to bootstrap. And then finally some more parameters for the. NET tile, Quick tile shredtile. We also you can see we disabled the C rebating like mention for the pack title.
00:15:18.395 - 00:15:54.365, Speaker B: For the follower we have the starting is pretty similar. We again have user port ranges, directories. The entry point gets templated out based on where the leader is going to run. In this case it will be in London. The layout you'll see is significantly different. We actually give most of the CPU cores on the follower to the Agave runtime to the Agave replay station, everything. We don't have as many Franken Dancer tiles, we just have like two bank tiles and two verifies but we do have three shred tiles and as Phil mentioned before, that's the main component on the follower that is a part of the Franken Dancer networking stack.
00:15:54.365 - 00:16:45.445, Speaker B: And then we have some more options for the rpc and on the right hand side this file is much shorter so there's a bigger repeat. But we have, if you look at the bottom part we have the disabling block stored this time from slot one because the followers don't need to create a snapshot and the status cache is disabled as well. The two max cost and shred limits per block we set it to true. This one is a consensus limit that determines how much we pack into a block and because of that it mainly matters on the leader so I'm just setting it to true here on the follower for convenience reasons so I don't have to go and edit the file twice when we actually change it to true. OK so with that I'm going to go ahead and start up the demo. And here we go. Well I guess show you that these are the leader and follower files that we're going to use.
00:16:45.445 - 00:17:32.669, Speaker B: Actually when I recorded this I realized that this is too fast for all of you to read so I put a screenshot previously so I can explain what actually is in them. And then we have the follower one over here and then we're going to start at the leader which is just a matter of copying over the toggle file onto the host that we're going to run the leader on and then we start up the leader again. This is a host in London with the FD dev binary, which if compiled Franken should be familiar with. And then we're going to examine the cluster that we just started up with a command that most of you should be pretty familiar with, the Solana Epoch info command. This shows us that a cluster has started up from epoch zero. The slot height and block height is slowly ticking up. There's a fresh not, not mainnet, testnet or anything like that.
00:17:32.669 - 00:18:11.375, Speaker B: And we're going to join all the followers to this new cluster that we just started up. Ok, so we're going to join the followers now, which is a pretty similar process as the leader. You just go ahead and copy over the file. This time you create some identity keys and then we join it to the cluster that just started up. So this time again we'll use a command that should be pretty familiar as well, Solana Gossip. And we see it starts off with just one node, which was the leader which started off. And then the other nodes need to wait for the leader to do the snapshot, download it, bootstrap and then join the cluster.
00:18:11.375 - 00:18:49.305, Speaker B: And it's going to take a little bit of time because the nodes are in different parts of the world and so it takes, you know, different amounts of time to download the snapshot. But at the end we should see 10 nodes with one leader and nine followers. And there we have it. So now that everyone has joined, we're going to go ahead and start up the loop generator. This again uses the ftdev binary and the tile architecture that Philip mentioned. Except these tiles are going to be sending transactions to the leader and once it has started up, we're going to go ahead on the load generator box and take a look at how much transactions are we processing. If you've run Frankendancer, this output should be a little familiar.
00:18:49.305 - 00:19:31.181, Speaker B: All the tiles starting up and then we see we have a pretty steady transaction rate of 81,517 transactions per second. Just a reminder that this is actually transactions that go through the entire pipeline through the pack. Verify dedupac, it goes through the bank tiles, gets executed, sends out shreds to the followers, and we confirm these transactions through RPC and then average it out. But this number should not be too surprising. This is essentially the number of simple transfers, system transfers that we can pack into a block times 2.5, because there's about 2.5 blocks per slot.
00:19:31.181 - 00:20:18.743, Speaker B: And so this is where we're at, given what the current consensus limits are. We cannot process more than this. I want to also show you a little dashboard that we have that kind of maps the traffic that we see on these hosts. So this is mapping the amount of traffic that is going out of the network interfaces on all of the nodes we're running this demo on. And all the different colors represent the different hosts. And it's kind of stacking up to show us the total bandwidth that the network is using. And we can see that this starts up with some megabits per second when the followers first start up, join the cluster, downloading snapshots, but then it goes up to a few gigabits per second, which is essentially when the load generator starts up and we're actually sending transactions.
00:20:18.743 - 00:20:46.115, Speaker B: And again, I intended to show you what the actual amounts per host are, but that was too fast. So we'll go to some screenshots after this video is done. So this is the leader. We're processing about 724Mbps on the leader, as in we're sending out. These are the shreds that we're sending out to the followers. This is a load generator, which is at a surprisingly high 2.37 gigabits per second, way higher than all of the other ones.
00:20:46.115 - 00:21:31.343, Speaker B: And the followers are retransmitting the shreds at about 340Mbps is the bandwidth that's going up. So this is what we have right now with the current consensus limits. What we want to see is what happens when we crank these up. So what I'm going to do is I'm going to go ahead and change those two parameters that we saw before starting from the same. But before that, I'm going to stop the existing cluster because we need to make sure that everything goes down and we do a new genesis with the new consensus limits. So once all of that has stopped, we're going to go ahead and edit that TOML file, the leader one that I showed you, and we're going to edit the two consensus parameters. Not that one.
00:21:31.343 - 00:21:52.155, Speaker B: Go back. Go back to the. Yeah, those two. So we're going to edit those and set them to true, and we're going to restart the whole cluster. So again, the leader, the follower, the load generator. And once it. It's going to take a little bit of time to start up, and as it does that, I'm going to explain what those two parameters actually do.
00:21:52.155 - 00:22:53.895, Speaker B: So as it's doing that, so the larger max cost per block essentially raises the max CU limits that we have in a block from 48 million to 13 times 48 million, which is roughly about slightly higher than 600 million. And then we have another limit of 32,000 shreds, parity shreds that you can put in a block, and that we raise to about a million parity shreds per block. So those are essentially what those two setting them to true goes ahead and modifies that max limit and then we restart the whole cluster up with those new limits. Because what we want to see is how fast can the Franken Dancer networking stack go? If we've already hit the consensus limits, how much higher can we go? So the cluster has started up and I'm going to move on to show you what the actual transactions that we're processing right now are. Okay, let's see. We're going to tail the same file that we saw last time, and this time we see that we're processing about 1.04 million transactions per second.
00:22:53.895 - 00:23:45.449, Speaker B: This is so again, these are transactions that are going through the entire leader pipeline. All the different tiles that Philip talked about through the runtime being executed, sent out as threads and confirmed through rpc. We also want to see how we're doing on the followers and we want to measure the performance of the follower networking stack, which is mainly the shred tile. So the shred tile emits a metric that says how many transactions per second it processed based on the number of shreds it receives. And also, given that we know the type of transactions that we're sending it. So the top one over here is a node in Frankfurt, the middle one in New York, and the bottom one in Singapore. And we can see that they're all processing about 1.04
00:23:45.449 - 00:24:26.975, Speaker B: million transactions per second, roughly the same as what the leader was sending out. So they're all. Their networking stack is also keeping up with what the leader is sending out. It is worth talking about what is our limit right now that we have raised all the limits of? So we have a couple of points here that we are aware of. The net tile is ingesting a ton of transactions that the load generator is sending to it, and on top of that, it's sending out a whole bunch of shreds to the entire cluster and that overloads it a little bit. And we can, however, mitigate this by adding more net tiles. And that is something that's pretty easy to do.
00:24:26.975 - 00:25:14.085, Speaker B: And another bottleneck that we have is a pack tile, and I think that maxes out at about 1.08 million transactions per second. And we are actively looking into doing further optimizations to see if we can crank this limit up even higher. I also want to show you a similar bandwidth graph for this second demo. And this is the load generator starts up and we're already processing a significantly higher amount of traffic. It's going to take some time to settle down, but it seems like this is going to total out roughly to about 42 gigabits per second. Again, I'm trying to show you the points of interest here, which is again too quick.
00:25:14.085 - 00:25:44.129, Speaker B: So I'm just going to go to the screenshots. And we have the leader now processing, sending out about 5.12 gigabits per second worth of traffic, way higher than what it was before. The load generator is still at the 2 gigabits per second that we saw before. So now we know why this was the case because we were always sending all these transactions. It's just the consensus limits didn't let us process it all and send it all out. But now that we can process it, we're sending everything out and the followers are now at about 4.3
00:25:44.129 - 00:26:07.065, Speaker B: gigabits per second. So they're just retransmitting the shreds that we all saw. We can compare the two graphs side by side. On the left hand side is the one with the 81,000 TPS. It's roughly about the total bandwidth usage is about slightly less than 6 gigabits per second. On the right hand side it's slightly around 42 gigabits per second. That's a million one.
00:26:07.065 - 00:26:48.391, Speaker B: And if we take off the bandwidth that the load generator is sending out, that's roughly about 3.8, 3.7 gigabits per second for the 81001 and about 39.8 gigabits per second for the million GPS one. And we can see that that's also roughly a factor of 12, which is pretty similar to the 81,000 to a million ratio. Okay, so to summarize, we can see that the Fire Dancer networking stack can scale up to processing about a million transactions a second with system transfers. And it is also worth noting that this uses the entire leader pipeline.
00:26:48.391 - 00:27:21.835, Speaker B: So this is using the agave runtime and the banking stage and executing all those transactions. So that is performant enough to keep up with the load that we're processing here. Although the current consensus limits that we have on mainnet and testnet and also the non networking bottlenecks that Philip mentioned that are kind of necessary to keep the chain running are fully functional, these numbers can be achieved on those networks. But to kind of answer the question that Philip started this presentation with, that is how fast the Fire Dancer networking stack can go. Thank you so much.
