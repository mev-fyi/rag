00:00:00.400 - 00:00:42.794, Speaker A: My name is Anna Tolle, and I'm one of the co founders of Solana, CEO. And Solana is a high performance blockchain. And the main difference between us and Elrond and a lot of other folks is that there is no sharding. Sharding is really hard. So the things that they're working on, those are really incredibly difficult computer science problems. And we took a different approach. It's maybe easier for us because of our experience working in embedded systems and hardware.
00:00:42.794 - 00:02:05.222, Speaker A: So our approach to sharding is really to build a software stack that can get out of the way of the hardware and really utilize everything that's available in modern day semiconductors and specifically GPU's. So if you guys are familiar with Nvidia, a modern day 2080 Ti card can handle about a million, hence 50 signatures per second. So that many cryptographic signatures per second in a single GPU card, we just have to write software that can get out of the way and actually get transactions in there and send them around. But beyond just GPU programming, we have a secret, open source, secret sauce called proof of history. So what's proof history? Just to give you guys some background, around 2017, we had this huge explosion in icos. Everyone was crazy about blockchain and bitcoin. Transaction fees were dollar 60 per transaction.
00:02:05.222 - 00:02:51.904, Speaker A: Kind of insane day. I had too much coffee, and I was up till four in the morning and I realized that using the same technique as bitcoin, this chapter 56 premature system hash function, you can create a data structure that represents the passage of time. So the way it works is the technique is called a verifiable delay function. It really consists of three features. It's a puzzle and a challenge that takes a fixed amount of time to solve that it cannot be parallelized. No matter how much electricity, how much money you have, you actually have to spend real time to solve it. But the time to verify is much faster than the time to solve.
00:02:51.904 - 00:03:42.724, Speaker A: So this thing is called the verifiable delay function. And our approach is a very, very rudimentary implementation. It's incredibly unsophisticated, but it works really well on modern day hardware. So intel has a shot to instructions baked into their chips. And these are running almost near the top speed you can possibly build with a custom semiconductor. So we have a very good understanding of how much faster an attack using a custom ASIC can be. And within the bounds of that speedup, we can build something that uses this process to create a clock in a distributed system and how it works is simply take shot 256.
00:03:42.724 - 00:04:30.904, Speaker A: And you spin in a loop so its output is the next input. So this thing that runs in a loop over and over itself. Because chapter 56 is this cryptographic premature system hash function, you cannot predict what any of the values are going to be. And therefore, if you sample this data structure, you get, if you sample this process, you create a data structure that can only have been generated by using real time. Somebody somewhere on a single core, single thread and some single cpu actually took time to run this. And you can calculate how much time roughly. So imagine I had the New York Times with me and I took a picture of myself with the New York Times.
00:04:30.904 - 00:05:22.172, Speaker A: And that guarantees that I was alive after that. New York Times photo mistaken. You know, I think bitcoin, the original block, right, is a hash of some, I forget what title of the neurotype. But what's interesting is that if I take that picture and then I publish it next day in the New York Times, that guarantees that I existed before, before that second newspaper was published. Now I have an upper lower bound. So same thing with chapter 56. If we have this process that's spinning, if you take any of these samples that are generated and you reference it in a message, that guarantees that the message was created before what's generated right after this sample was created.
00:05:22.172 - 00:06:19.424, Speaker A: If you take this message and you insert it into this process, it guarantees that the message was created before the insertion because you're modifying this pre image resistant hash function in this unpredictable way. So now you have an upper and lower bound of time when an event occurred. So imagine we now have a way to record cryptographically with anything that's ever been created, right? Like any digital event was ever like, you know, done. And this guarantee is available, right? You can actually verify. So when you take a picture with a camera and that says that this picture was taken at like, you know, July 22, you know, at like 12:00 p.m. Somebody could fake that, right? They can modify that with this approach because this is embedded in this like, cryptographic process. It can never be modified.
00:06:19.424 - 00:07:16.020, Speaker A: So you have this guarantee that this is the, the true point in time. But it's not an absolute measurement of time. It's a relative measurement of time relative to all the events that occur in the structure. So what's cool about this is that we can build something very, very boring called time division multiple access. So if you guys ever, if you're familiar with radio networks, early wireless networks, people would set up towers and they would transmit over the same frequency and when they did this at the same time, they get noise, right? Because they're both occupying the same frequency at the same time. Like, those things interfere. So what they realize is if you give everybody a clock and you synchronize the clocks, you can split up the channel by time.
00:07:16.020 - 00:08:04.824, Speaker A: So every second you rotate and we get some transmit, and they call it time division multiple access. So early cell phone protocols use this. Modern day, like 5G still uses time division to some aspects, along with frequency hopping and a bunch of other optimizations. But at the end of the day, you have a bunch of folks that want to transmit something, some information, or produce a block, and they're trying to figure out who should produce a block first. Bitcoin uses this random process for the difficulty. What we can do is we can use this sequential hash function to define what time is, and therefore have these block producers scheduled at a very exact moment in time in SS. Describe the data.
00:08:04.824 - 00:09:08.812, Speaker A: Why is this cool? Is because unlike bitcoin, we never have a collision. We know exactly who the block producer is supposed to be and when they should produce. But what's really interesting about this is that none of the participants in the network actually rely on time. Like, if you are familiar with tendermint and modern day distributed systems, when messages are sent across the network, they carry a local timestamp, or they potentially trigger a state transition. And no, it's because they never trust each other, they're adversarial. They have to filter out messages that are too old or too new, or they use a local timeout before they transition state. So kind of what I like to talk about is like, you know, tendermint, imagine you are in the pre commence state, and the next block producer sends you a proposal because their adversarial or their clock is running early.
00:09:08.812 - 00:09:42.380, Speaker A: You would queue up that propose. You would wait until the secure block is finished. So imagine that instead the next block producer sends you a proof that time passed. You could take that state transition immediately. Because that proof is correct, you anticipate that everyone in the network will interpret it as correct, and it will also take the same action. Now you can move forward without actually waiting. So because of this property, we can schedule all these leaders using data.
00:09:42.380 - 00:10:33.320, Speaker A: What they're transmitting to us is proof that time has passed and the stable transitions can occur instantly. And for our test at this lab right now, this period in data is 800 milliseconds. And our goal is to ship the 400 millisecond blocks. To me, this is like the coolest thing in blockchain because now what we're talking about is like dapps that are running with the same kind of user like state transitions that are as good as AWS. Like if you use AWS using s three and a bunch of other APIs, you're likely to see about like 150 millisecond latencies. Like going to s three, getting like these rest responses, and talking to a bunch of their databases through HTTP, you can obviously do a lot faster, but like, you know, lazy. And you just kind of build something.
00:10:33.320 - 00:11:15.664, Speaker A: That's what you'll see, and that's good enough. 400 milliseconds is not there yet, but it's pretty close. Like we can start getting dapps that have the user feel that humans actually can deal with and be okay with. When you click a button that does the same transition on chain and gets some finality, if that's half a second, humans are going to be okay with it. I'd like it to be 200 milliseconds, but that's going to take a lot of blood, sweat and tears. But this is like one of the most exciting things about this approach. So what else is interesting? This is like a long presentation, so I'm going to kind of skip ahead.
00:11:15.664 - 00:12:09.600, Speaker A: So how we actually deal with BFT, the classical approach is if you're not a proof of work system, you're a proof of stake system. And I think anyone that's building a proof of stake system needs to realize that they're building a VFT. Right, subversion of the VFT, because we're still bound by cap theorem. Consistency versus availability, you know, safety versus liveness. There's no way to cheat that. We have to be a little clever about speeding up these block times, because what does it mean for you to have an 800 millisecond, 400 millisecond block? It means the network has to come to a decision that this block has some finality. And if you cut that time to be too low, you're basically cheating safety.
00:12:09.600 - 00:12:56.754, Speaker A: But there's no way for a globally distributed system to actually come to a finality decision really guaranteed within that time. So what we do is we let the system decide a very, very small amount of safety, like a very small amount of commitment in that 400 milliseconds. So when Alice proposes a block and anyone votes on it, what they're voting on is a commitment to safety for just two slots or two blocks. And that is roughly, let's say 1.6 seconds. That when Alice goes and Alice does this decision as fast as possible, because these blocks are coming really, really fast. Alice takes a very small amount of risk.
00:12:56.754 - 00:14:08.664, Speaker A: But when Alice votes again and takes this commitment again from previous commitment doubles. So the snow becomes this exponentially growing function. And this is how we're able to build a BFD system that has this pipeline approach that can generate blocks at a high frequency, but allows for validators in the network to take this risk with a very small amount of commitment to safety. By the time this thing actually gets to, you know, let's say, four blocks, this block has already been finalized to commitment of 16 slots. And what that means to this validator is in this historical record of these four blocks, they've observed other validators voting on this, right? They've observed other validators voting on this for they can make the decision that, hey, I see the super majority of the network, they're all continuously voting. Everyone is still alive. So can continue voting that can continue to increase this exponential lockdown.
00:14:08.664 - 00:14:59.940, Speaker A: So that's their very clever bit of how we're able to create this, like, kind of streaming high frequency block production while still maintaining VFT. There's a bunch more stuff, but I don't want to, like, I don't want to take an hour. So, like, questions, what do you guys want to know? We have a testnet that's up and running, and we're currently kicking off our version of Game of Steaks. It's called Tortoise soul, because, see, four of the five co founders are Iron man triathletes. I was before I had kids. So, yeah, if you guys run a validator, please join the testnet. It's going to be a lot of fun.
00:14:59.940 - 00:15:46.004, Speaker A: We're going to do a bunch of performance events and see how our software stands up to some high throughput transactions. The next stage is going to be executing a decentralized exchange contract. So, given that we're like a high performance network that is not sharded, what is the obvious thing that we can build? It's a DAX, right? Because it's a single marketplace, there's no arbitrage, and you can trade things at a near high frequency. So it becomes like a very cool application. So things like that. Cool questions? Yes.
00:15:46.954 - 00:16:01.694, Speaker B: Do you achieve absolute safety given a particular assumption of max faulty nodes relative to total nodes, or do you achieve probabilistic safety when asymptotically negligible?
00:16:02.634 - 00:16:37.842, Speaker A: So after the 32 votes, right, you have two to the 32. Number of slots at 400 milliseconds is 53 years. And we basically say that just because it's smart contract that keeps track of these votes. We don't want it to be infinite, so we fixed it to 32 votes. When you hit the two to the 32 number, that's the maximum commitment you can have, and that's when you generate a reward. So, in that sense, the protocol gives you full finality at that point. But it's from an application's perspective, depending what you care about.
00:16:37.842 - 00:16:52.926, Speaker A: Like, if it's. You may only want 400 milliseconds of confirmation. If it's a payment, you may want, like, $10 and you want, like, a few blocks. If it's $2 million transfer, wait for the whole 16 seconds, right? Yeah. Yeah.
00:16:52.950 - 00:16:58.874, Speaker B: Well, what happens in the case of a network partition where different forks vote on conflicting partitions?
00:16:59.214 - 00:17:00.230, Speaker A: Yeah. So.
00:17:00.382 - 00:17:01.126, Speaker B: Sorry.
00:17:01.270 - 00:17:32.091, Speaker A: Perfect. This is the next slide. So imagine you had a petition. You know, Alice, Bob and Charlie, Dan and Eve. So Alice and Bob are in one petition, they're voting and voting, and Charlie becomes the block producer. And because Charlie did not observe this other french, Charlie produces a fork that has this empty version of the logic, right. So Charlie simply runs their shot 256 loop and generates a proof that they got to their slot.
00:17:32.091 - 00:17:50.424, Speaker A: Right. They produce a lock. So both Alice and Bob are locked out. Right. But eventually the lockout expires, and they get rid of that state. Right. That's basically how it works.
00:17:50.424 - 00:18:17.458, Speaker A: So the longer the partition that Alice and Bob are in, the longer the lockout is. But Alice and Bob are greedy. Right. So when Alice and Bob are voting, they really don't want to vote on a ledger that doesn't have the rest of the network voting for them. So what they're. When they make that decision to vote, they're looking at their historical record. Does the rest of the network? That's commitment as well.
00:18:17.458 - 00:18:28.614, Speaker A: And that is actually our wage choice rule. When you look at two forks, you look at the statement weighted lockout of both forks, and you pick the heaviest one.
00:18:28.734 - 00:18:41.046, Speaker B: But in principle, although unlikely, it may be not resulting from rational validators. You could have two forks, which are approximately equal height for everything if there was near 55th.
00:18:41.070 - 00:19:18.034, Speaker A: They have a fringe right. But since validators are greedy, they will stop voting because they don't want to commit themselves to being locked out from the work that doesn't have super majority. So they still look for this, you know, super majority magic number. You guys use, what, two thirds plus one I mentioned as your super majority squares? Yeah, yeah, same thing. Every validator waits until they see a super mature here plus one. Great question. Yeah.
00:19:18.034 - 00:19:45.938, Speaker A: What does it work with regards to what are the incentives to not double vote for? Is there snatching for double voting? So that feminine Charlie, right? Yeah. Supposedly I vote on both petitions not because I'm badly connected, but rather because I just disconnect everyone badly. And I love it. Yeah. So slashing. Slashing sucks, but that's the only thing we can think of. We're not clever enough to get rid of sloshing like Algorand.
00:19:45.938 - 00:20:06.174, Speaker A: And I think avalanche doesn't have sloshing. So I honestly don't trust those systems. But I don't understand that you can have mistake without slashing. There's something that like, doesn't rock because you simply can't observe every possible position.
00:20:07.014 - 00:20:33.970, Speaker B: Yeah, let's just keep asking questions. So does this consensus algorithm, can you summarize the votes into transcript of assigned commits where you are like mind and you have validators at some point in time, but you don't have the ability to verify this hashing? Can you still verify the commits and assuming supermajority rational or something?
00:20:34.122 - 00:20:50.494, Speaker A: Yeah, so you can, what you can do is like effectively like a linked list where the next book is the previous hash as well. So that's what we do. It's, it's fine. Like, it's weaker, I think the verifying hash chain.
00:20:51.714 - 00:20:56.522, Speaker B: But you still get safety, right? You just don't get closer correctly.
00:20:56.698 - 00:21:07.654, Speaker A: Yeah, you don't, you don't know if like the, it's kind of like the same problem with lifelines. You don't know if the state transitions are valid. You just know that everyone voted. But it could have been because of an exploit.
00:21:08.354 - 00:21:15.454, Speaker B: But you would have the same equivocation falls where the lite client would be full of the same conditions.
00:21:20.154 - 00:22:09.282, Speaker A: So this was our private permissions deployment on a single Google Cloud 800 megabit network. What's cool is it's actually saturating the switch. And the reason why we're able to get to 185,000 p's is because we're not rotating the liter. So single leader running forever until failure. We can actually replicate that many state transitions with a new confirmation. And this is our, this is like a theoretical maximum, so it's a really cool achievement. Of course, in an open permission permissionless network that was fully distributed, this number is about a third.
00:22:09.282 - 00:22:45.834, Speaker A: So we see somewhere between 40 to 60,000 DP's. And it's going to take a lot of work to get there because like, the next set of optimizations that we need to do is effectively like ahead of time execution of transactions and like caching that state and hoping that the next block has it. So things like that, like the next level insurance. Cool. So yeah, TPS, does anybody care about TPS businesses? Yeah.
00:22:46.414 - 00:22:49.198, Speaker B: And this is what's a transaction.
00:22:49.246 - 00:22:59.394, Speaker A: Is it like so nt 50 519 signed, move from one balance to the other.
00:22:59.734 - 00:23:09.554, Speaker B: Do you think that if you were to do in a transaction ten major operations for debts or something, this would slow down substantially or is the bottom line.
00:23:13.874 - 00:24:07.514, Speaker A: So like we use a register based vm, it's Berkeley packet filter, so it can ship directly to x 86. So for like a Dex or something like that where all you're still doing is compared to prices and then just like most of the work is getting to that point in the pipeline because you're fetching disk, right. You're like setting all this up. I don't expect like ten instructions, ten more x 86 instructions that are, we have, our transaction format allows for multiple instructions to be sequenced in the same transaction and that we see about four or five x more instruction throughput. But those are all like atomic, right. So if any one of the instructions fails, we don't lose. So this is like more of a fairish comparison, but still.
00:24:07.514 - 00:24:14.706, Speaker A: This is a permission deployment on Google Cloud 800 megabit network. It's all running on the same switch.
00:24:14.890 - 00:24:17.574, Speaker B: Are you f syncing to get command?
00:24:17.954 - 00:25:18.504, Speaker A: Yeah, but we use a memory map file so to be operating system. But in these tests it's more than 10 million accounts so it's definitely getting the, it's not enough as we did. There's not enough ramp to do this all around. Have you done any kind of real world setting? Dax? So our DAX implementation can do about 30,000 price updates per second. And that's really like, it's not because it's saturating the network, it's because the bot traders like the synchronization of the state makes it very hard for you just to stimulus this probably 70. Yeah, this was like five continents deployment of 200 post nodes. That's as close as we can make it to something like, but still AWS cloud, I don't know what's going to happen with a bunch of validators.
00:25:18.504 - 00:25:42.114, Speaker A: So yeah, that's like CBD, right? Like what we do in the cloud, we have so much control because we can ensure that it's running in like a high quality system. It's like incredibly hard to corral everybody and get them all on board and get all connected and do what was working.
00:25:42.734 - 00:25:46.874, Speaker B: Yeah, you can tweak the examples right. The payments used for about as a validator for.
00:25:53.074 - 00:26:12.914, Speaker A: If there's that much demand. Yeah, our big bad is that, like, if there's, you know, if we, like when the decentralized exchange bots will eat up all the transaction space and the validators be like, hey, I can just make twice as much if I get, you know, ten gig event. Obviously, our software has to be good enough to do this.
00:26:19.134 - 00:26:20.154, Speaker B: Where are you guys?
00:26:22.654 - 00:26:26.294, Speaker A: Sorry, we're mostly in San Francisco. Okay, cool.
