00:00:00.240 - 00:00:51.430, Speaker A: Welcome to this week's Changelog, where we talk about the changes on Solana Week by week. I'm joined today by Jonas and let's get started. So this past week, Grizzly Thaw submissions were now open. You can now submit your project for Grizzly Thawne. If you're building and if you need help with like figuring out whether or not your submission is good or getting feedback on your submission, the Dev rail team at Solana foundation will be available for office hours on this coming Tuesday or the day that I believe that this goes live, so that we can help you out on tweaking your submission and make it really good for Grizzly Thon. On another note, the core community call will be happening this Friday at 19:00 UTC time. The core community call is a call between core contributors of the Solana protocol.
00:00:51.430 - 00:01:39.112, Speaker A: So this is, for example, the labs team fire dancer, Team Mango, Jitto and other core contributors will join this call, discuss whatever the agenda is which you can find on the core community. Community call GitHub and then figure out like what features or what changes will be happening on the network in the future. Now, they don't decide fully, like these are the changes on the network. Those will be decided eventually by the validators, but that is a place where they can align on changes that they want to work on. Jumping right into this week's proposal. This is a proposal on the labs client. So it's not a proposal like Assimdi across all clients, but it's called tiered account storage.
00:01:39.112 - 00:02:13.604, Speaker A: And so what this means is it's kind of just having a different storage accounts, or it's based off of whether or not you're a hot or cold account. So a hot account is a often used account versus a cold account. There's a rarely used account. And where do you store it? On the validator client. So this is for specifically using accountsDB, which is very specific to the labs client. And so basically it's just using like the storage you expect on a cache. If it's hot, you store it like on ram, if it's cold, you store it on disk.
00:02:13.604 - 00:02:21.484, Speaker A: And that is the proposal. You can check out more on GitHub. So Jonas, what resource do you have this week?
00:02:21.564 - 00:02:43.196, Speaker B: Yeah, for resource of the week I found something cool, like Nick from Helios. He built an example, like an end to end example for compressed nfTs. So it comes with everything you just started. It creates a collection, an NFT merkle tree. Then you can mint one from this mercury tree. Afterwards you can transfer it or try to redeem it. And I tried it out.
00:02:43.196 - 00:02:58.554, Speaker B: It works really well. It still says a few things that you need to chew glass on, but it's very interesting and we're going to build something cool with it. And there will also be some docs that we are writing for it so that you don't have to choose so much grass.
00:02:58.934 - 00:03:02.446, Speaker A: What exactly are compressed nfTs? Like, what do they use under the hood?
00:03:02.590 - 00:03:33.264, Speaker B: The cool thing about the compressed NFTs is that it's not all the data for the metadata on chain account is saved on chain, but instead it's cached by the validators. And there's only a proof in a merkle tree which you can use to figure out if the data that you're getting from the RPCs is correct. So it saves basically a lot of space on chain. And that's why the nfts you mint with it are so cheap. So I think you can get 1 million for five sol or something like this.
00:03:33.424 - 00:04:08.680, Speaker A: Wow. That's a lot different than today. All right, cool. And then some other things that have happened this past week is that there is a new compute budget instruction for requesting loaded account size limits. So today the limited amount of data that you can load on one time on a transaction is 64 megabytes. So what this does allows you to one request more limit for a price, whether or not be compute units or possibly a few in the future. That's a different PR that's being worked on right now, or for a lower limit for less compute usage and possibly less price.
00:04:08.680 - 00:04:16.648, Speaker A: The pricing model is still being worked out. Likely to be some kind of SIMD in the future, but pretty interesting to see that.
00:04:16.816 - 00:04:21.704, Speaker B: That's interesting. I thought it was always a limit of ten megabytes. So is it now increased?
00:04:22.164 - 00:04:36.492, Speaker A: Yes. So it's increased from ten megabytes to 64 megs in a single transaction. The amount of accounts that you can load and then you can now, based off of compute budget instruction, change that as you wish.
00:04:36.628 - 00:04:37.844, Speaker B: It's a lot of space.
00:04:38.004 - 00:04:42.024, Speaker A: It is a ton of space. And what did you see this week, Jonas?
00:04:42.444 - 00:05:09.924, Speaker B: The most exciting thing I found was the anchor update. So anchor had a big release, and I'm super happy to see that they continue working. There's now already 78 contributors and a bunch of new features. So now you can, for example, you can close your IDL accounts. You can have very large IDL accounts. They have a transfer checked and approved checked. They introduced interfaces for accounts so that you can easily manage multiple token accounts.
00:05:09.924 - 00:05:32.794, Speaker B: Which is very cool because token 2022 is there and will probably be used more in the future, I hope, which has a bunch of cool features. And then there it's also added support for version transactions and for the new bytecode loaders SPF, which is basically BPF but in a bit better version that Solana build. Is that right?
00:05:33.214 - 00:05:45.560, Speaker A: Yeah, I think it's called Solana bytecode format. I think they're going to change it to c level virtual machine. They're still working out what that looks like in the future.
00:05:45.712 - 00:06:15.532, Speaker B: Yeah. So one more thing, like someone from labs pushed something to the repository. It's a documentation of how you can debug your on chain code. So that's very nice. It's not super convenient, but there is a visual studio code extension, it's called Code LLDB that you can install. And then you just need to point it to a library called Lib LlDBso. And then you need to task JSON and launch JSON and then you can already debug your code.
00:06:15.532 - 00:06:26.668, Speaker B: It has a few limitations though. Like you can't use CPIs and you can't access on chain data. But still it's a very good step. And if someone maybe wants to improve on that, the documentation is there.
00:06:26.836 - 00:06:46.580, Speaker A: Yeah, definitely inconvenient in the current state, but definitely a possibility for someone to bundle all those steps into a single versus code extension and just make it easy for everybody. And then also if you like connect that to maybe sell on a playground, it just makes things a lot faster and a lot easier and it'd be wonderful to have.
00:06:46.732 - 00:06:50.188, Speaker B: Oh, that's a great idea, having it in playground. That'd be great.
00:06:50.316 - 00:07:30.734, Speaker A: Yeah. Looking forward to hopefully someone building it. There was one more commit that we'd like to highlight is that there's a lot of commits around, like reducing the mask shreds per slot on the cluster. They reduced it from like 557,000 shreds to about 32,000 shreds. This is a kind of a performance improvement on the cluster and it also allows the cluster to basically perform better with large amounts of transactional data. What is a shred? A shred is the smallest unit that is transferred from validator to validator that can make up a block. So it's the smallest unit of a block basically.
00:07:30.734 - 00:07:32.762, Speaker A: But yeah, that's about it.
00:07:32.858 - 00:07:33.986, Speaker B: Cool, thank you.
00:07:34.010 - 00:07:34.210, Speaker A: Yeah.
00:07:34.242 - 00:07:47.670, Speaker B: Then I would say I can't wait for all your Grissison submissions. I will look at all of them, maybe not all of them, they are like 8000 or something. But still, I'm very excited to see that and see you next week.
00:07:47.822 - 00:07:48.814, Speaker A: All right. See you next week.
