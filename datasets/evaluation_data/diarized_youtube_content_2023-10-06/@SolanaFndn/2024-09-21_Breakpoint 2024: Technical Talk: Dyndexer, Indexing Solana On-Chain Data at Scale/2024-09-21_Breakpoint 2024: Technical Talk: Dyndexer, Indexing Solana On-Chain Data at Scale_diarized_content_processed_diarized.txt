00:00:04.320 - 00:00:18.745, Speaker A: Hi guys, I'm Ahmed, the co founder and CEO here at Syndicov. And today I'm here to talk about indexing Solana on chain data at scale. So firstly, who is Syndicov? So we are building a web3 native cloud.
00:00:18.865 - 00:00:21.401, Speaker B: We primarily provide RPC infrastructure as well.
00:00:21.433 - 00:00:27.403, Speaker A: As developer tools for Solana. We also have specialized APIs like Chainstream as well as app deployments.
00:00:27.459 - 00:00:28.935, Speaker B: You guys should check it all out.
00:00:29.475 - 00:00:43.655, Speaker A: We're also the team behind the SIG validator that's currently in development. So definitely check that out on GitHub as well. So yeah, indexing data on chain is not easy.
00:00:44.995 - 00:00:47.571, Speaker B: The first issue today is availability.
00:00:47.763 - 00:00:51.915, Speaker A: So you have to run a validator to actually get that data from on.
00:00:51.955 - 00:00:53.775, Speaker B: Chain into your infrastructure.
00:00:54.455 - 00:01:45.525, Speaker A: Second is scalability. You have to be able to scale your infrastructure as the number of on chain data increases. And finally you need to make sure that data is integral, so making sure you capture all the data that you're actually interested in. So what is indexing in the first place? So here we have some on chain events. It can be a transaction, an account update, and we want to take that data, we want to transform it in some way. So in this example we have a transaction that occurred on chain, we want to take that transaction, use an anchor IDL to parse the events out and end up with a data structure like so. In this case, the event is make order with some sample data here.
00:01:45.525 - 00:02:36.013, Speaker A: And then finally, once you finally actually get these events parsed out from the transactions instructions, you want to insert that into a database. So you know what exactly is the challenges with this? So for one, you have to manage a real time streaming infrastructure and this is pretty difficult today on Solana, especially if you're streaming in intra slot account updates, as Today it's around 1 gigabit per second worth of data changes. You also need the capability to process historical events. So if you're starting from a previous slot and you want to catch up to the head of the chain, you have to write a lot of code and be able to bootstrap your database.
00:02:36.109 - 00:02:37.345, Speaker B: To be able to do that.
00:02:38.405 - 00:03:12.345, Speaker A: And then finally, as the number of on chain events in your DAPP increases, you need to be able to scale storage and retrieval of that index data. Also observability is a big thing. So you want to make sure that as your indexers are running, you want to make sure there's nothing wrong happening. You also want to be able to get ahead of any kind of issues occurring. So metrics, logging, all that. So we Developed a solution. It's called Indexer and I'm going to pass it over to Preston to talk about it.
00:03:14.245 - 00:03:50.945, Speaker B: Yeah, so like what Ahmed was talking about some of the problems with indexing, one of the things about being an RPC provider in this space is helping our customers solve some of the problems they're facing with getting data off the chain. Talking to a lot of people working on indexing, this space has also been very enlightening. And one of the things we noticed was flexibility is incredibly important. There's so many projects just being developed on Solana, there's so many protocols getting added, there's so many technologies, commodities, so many ways of representing all that data. We wanted to focus on flexibility. So one of the things we did was use a wasm runtime. So Indexer, our dynamic indexer is a wasm runtime.
00:03:50.945 - 00:04:26.475, Speaker B: You can run programs using a library and we'll kind of get into what that looks like, but it's dynamic. It lets you kind of specify how you want to observe and react to these on shade events. So Indexer is a run side platform. We call your code and we pass it every single event that you care about and we can start the data just like we mentioned with starting from a past slide, catching up to the head of the chain, you can do that with Indexer. You can replay history, you can regain state in your database. Now, all the streaming infrastructure, being an RPC provider in this space is already taken care of for you. You get verifiability, you get all the data.
00:04:26.475 - 00:04:51.649, Speaker B: We can also ensure that you don't miss any events. And all the outputs that your indexer produces, this includes logs, includes errors. All this gets tracked in our database. So you can always query this, you can regain state, you can always monitor what your indexer is doing. So you never lose track of what your indexing capabilities are. And lastly, the outputs. So we refer to the outputs of your indexer as the objects you're writing to your database.
00:04:51.649 - 00:05:15.779, Speaker B: These are things that you're interested in indexing on chain. All of this gets sent over to our storage layer. We support postgres, clickhouse, and a bunch more messaging protocols are in the works. Of course, I'm going to run through what Indexer runtime actually does. So this is kind of going back to what we talked about with indexing and the indexing 101 we gave. Everything starts on chain. We have on chain events, we got transactions, we have account updates.
00:05:15.779 - 00:05:48.173, Speaker B: The first step is getting this data off the chain and into the indexer runtime. So when you create a indexer, one of the things you're going to specify is a filter. It really just represents subset of the full Solana stream of data that you're actually interested in. This example, we've got a transaction filter. It specifies the drift protocol. So you're going to be indexing all transactions that involve the drift protocol. Once the filter has selected data that you're interested in, it gets sent over to your indexer and you can start processing it.
00:05:48.173 - 00:06:13.775, Speaker B: I'll show you an example of what that looks like in a second. Of course, we repeat this for all the data on chain. So that includes every slot and every piece of data inside that slot. So every transaction, every account. Again, this also includes historical data. So now we're going to kind of get into how indexer works, how you're going to set one up and we'll run through a quick example. So just to recap, a indexer has two components.
00:06:13.775 - 00:06:40.185, Speaker B: The first is your WASM Rust program. Of course, since we're using wasm, you can kind of write your indexer program using our library in whatever language you want. We'll have docs on that coming up soon. But for now we're going to run through a Rust example. So using a indexer, you can observe any on chain events you're interested in. The second component is the indexer itself. You can kind of think of the indexer as wrapping your WASM program, but attaching some useful metadata, that's important.
00:06:40.185 - 00:07:10.407, Speaker B: One of the most useful pieces of the metadata you're going to include is your filter. And the filter kind of reduces down that input stream to what you care about indexing. Lastly, and most importantly, one thing we really want to repeat here is you get all the data you care about, so your indexer will see that data and you will have the opportunity to index it. So let's create a quick indexer. Let's put one together for the Jupyter aggregator. I think this is a great example of how you can kind of write indexer for a single protocol. But keep in mind, it's really flexible.
00:07:10.407 - 00:07:37.957, Speaker B: You, you can index multiple protocols in the same program. So let's get to this. All indexer programs are pretty simple. They start off looking like this. You have the syndicate indexer runtime, you import the library, you just import the processor macro, and you decorate one of the functions in your project as the entry point. So this is the function that gets executed every time our runtime has new data for your indexer index. So at this point, data has already been filtered.
00:07:37.957 - 00:08:14.075, Speaker B: All you have to do is write the code that you'd like to execute whenever you have new data at this hook. So that event type is really just a dino between a transaction and an account that you're interested in. So let's get into writing the actual indexer for jupyter. So the first step here, and I know this looks a bit scary, but I'll walk you through it, is setting up anchor. So we're going to use anchor IDL to parse out the events from jupyter transactions. So the first thing we do is import the jupyter IDL JSON. We also just write out the jupyter pub key address and we set up our idlparser object.
00:08:14.075 - 00:08:44.599, Speaker B: Now, remember, the process function gets called for every single update every single time. So we use a one slot to make sure we only set up the IDL parser once and then just continue using that for every single event that gets passed to your indexer. The next step is selecting the events you care about. In this case, we're really only going to focus on transactions. So we check if the event is a transaction. If it is, we pass it to our IDL parser and we parse out the events. Once we have the events, we use the record output syscall to kind of write all of these to the storage layer.
00:08:44.599 - 00:09:12.765, Speaker B: And one thing that's really cool about this is value there. The value type is flexible, so any serializable object can be put there. So you can do much more complicated things right before you write it to your database, it will support multiple databases, multiple tables to be written from these programs as well. And that's it. Now you're indexing the jupyter aggregate. Pretty simple. Now, one thing we've had to do to kind of build this out and work on making the dev flow as easy as possible, is to build out a CLI tool.
00:09:12.765 - 00:09:28.257, Speaker B: And as you can see in this example here, we're using the Syndicate CLI indexer create command to create this indexer. And this says a bunch of things in the background. So let's walk through it while looking at the command. It's pretty simple. You give your indexer a name. This could be anything you want. We call it the jupyter indexer.
00:09:28.257 - 00:09:59.025, Speaker B: The next thing to do is point to the source code where that entry point lives. In this case, in that example, it lived in lib rs. Again, we're using Rust as an example, but since we used WASM under the hood, you can write your indexers in whatever language you prefer, as long as it compiles onto wasm. And then the last thing about this is the transaction filter. In this case, we're filtering by programs. We only care about transactions that involve the jupyter program, hence pubg. And there's a couple things that happen under the hood.
00:09:59.025 - 00:10:26.999, Speaker B: First, this compiles your index to WASM and then submits that WASM to our infrastructure. And it immediately starts executing. And there's going to be entries and events getting written to your database. So let's look at an example of how to pull that. Using the snyqua cli, you're able to get your outputs. You simply provide the same name of the index you're interested in getting the outputs for. You can then supply the slot range to specify which subset of the data you're interested in looking at.
00:10:26.999 - 00:10:48.275, Speaker B: So that way it's not overwhelming. And one thing to go along with the CLI is also the API. So if you want to write your own infrastructure around Indexer, you go ahead and do that. We have all the docs available for that as well. You can also use our platform to see what your indexers are using at any one point in time. There's a nice graph there showing you all the events coming out. You can inspect the events and see what's going on with that too.
00:10:48.275 - 00:11:18.485, Speaker B: Now, one thing I really want to conclude this presentation on is the availability guarantees the flexibility we offer through the Indexer program itself. And we can guarantee all those things by leveraging our existing infrastructure. On the point of availability, we run multiple validators. We have a lot of experience doing this. We're globally distributed. We also consume multiple geyser streams from multiple nodes in each of the regions. So this gives us availability, guarantees uptime, guarantees your indexer is never going to go down.
00:11:18.485 - 00:11:47.337, Speaker B: Scalability. So on the point of scalability, we spent this last year optimizing our hardware, getting custom hardware into data centers, improving our networking. We've got incredibly powerful NAICS guys. So as Solana scales, as more protocols get added, as there's more data to index, we're ready to scale to meet those demands. Same point on our storage. We've got plenty of storage hooks coming up down the pipeline. Clickhouse and Postgres have been some of our favorites to work with, and we're even planning on supporting SG protocols.
00:11:47.337 - 00:12:17.971, Speaker B: So Redis, Kafka, whatever you guys need integrity. Now, one of the things we've worked on is solving the problem of missing updates coming from or validators. So we consume multiple streams from multiple validators and we ensure that data reported on one but not reported on the other gets filled in. So that way your indexer isn't missing any updates. The data getting pushed to your indexer isn't coming from one validator, it's coming from multiple. And any missing data is getting filled in. The other thing and the final aspect I'll touch on is historical events.
00:12:17.971 - 00:12:43.381, Speaker B: You can restart from Genesis and see all the events you need. Catch your indexer and database data from scratch, get to where you want to be, catch up to the head of the chain and just have all your data ready to go. And currently we only support transactions, but we're working on a solution for accounts as well. We're happy to announce that Indexer is enclosed beta. We'd like to get you guys onboarded. We'd like to help you guys get started on indexing, so come over to us. Our booth is just outside and right around the corner.
00:12:43.381 - 00:12:49.197, Speaker B: If you guys are indexing or doing any indexing in the space. I'm sure a lot of you guys are. We'd love to hear your problems, hear.
00:12:49.221 - 00:12:50.165, Speaker A: Your thoughts on this.
00:12:50.285 - 00:13:09.733, Speaker B: So come find us, come talk to us. Of course. Lastly, we're also the team behind the SIG validator, so definitely scan that first QR code to check out the source code. A lot of cool things happening there. And we also have our blog. We go into a lot of depth about how our validator works, all of our design decisions. Definitely check that out too.
00:13:09.733 - 00:13:18.345, Speaker B: We have blog posts on infrastructure and crypto research in general. So yeah, thank you guys for having us. I hope everyone's having an amazing break point. Thank you.
