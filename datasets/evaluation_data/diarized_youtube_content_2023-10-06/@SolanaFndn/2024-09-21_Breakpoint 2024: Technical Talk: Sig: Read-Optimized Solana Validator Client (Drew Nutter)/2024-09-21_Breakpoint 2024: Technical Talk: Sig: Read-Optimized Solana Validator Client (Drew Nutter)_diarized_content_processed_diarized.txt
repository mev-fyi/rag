00:00:05.720 - 00:00:31.145, Speaker A: Hey everybody. I hope you're enjoying Breakpoint so far. So my name is Drew Nutter. I'm a senior protocol engineer at Syndica where we are building a READ optimized Solana Validator client from scratch. It's called sync. So who is Syndica? Syndica is a web free native cloud. We provide RPC infrastructure and developer tools for Solana.
00:00:31.145 - 00:01:34.611, Speaker A: So this includes some products like elastic nodes to scale your dapps chainstream API to stream data directly from Solana, app deployments to easily deploy your DAPPS from a repository and dindexer to convert on chain events into application specific data. So as an RPC provider we noticed a problem. Solana is plagued by slotlak. What this means is that essentially every RPC node is struggling to keep up with the latest state of the chain. This leads to poor user experience because users are not getting the latest data and they're also experiencing delays and errors when they're interacting with dapps. So we looked at the data and realized that this is called by caused by bottlenecks that occur Validator when they are receiving a large number of read requests over RPC for methods that are unable to process many reads per second or rps. So this is why we're building SIG at Syndica.
00:01:34.611 - 00:02:19.575, Speaker A: We're focused on user experience and we're going to improve that by solving slot lag with a new and faster Solana Validator client building from scratch with a focus on reads per second. So this is also going to improve the fault tolerance of Solana by introducing client diversity. And it's also going to strengthen the developer community around Solana because of our intense focus on code readability. So last year, some of you may have seen my colleagues at Breakpoint introduce the SIG project. At that time we had just finished implementing gossip, which is the entry point into the cluster, and we had a blog post up at that time. Since then we've made a ton of Progress. We've implemented AccountsDB, the blog store and the Shred Collector.
00:02:19.575 - 00:03:08.387, Speaker A: So I'm really excited to share that progress with you. But first I'd just like to go through a little bit of background about Solana. So why do we use Solana? So there's a lot of reasons you might think something like, you know, it's fast, it's cheap, it's decentralized, powers the dapps that I care about. But like, fundamentally, what is Solana actually doing for us? So society relies on information systems to coordinate just about everything that we do. Some of the responsibilities of information systems are to store information, to distribute information, and to allow changes to that information. So that's actually exactly what a database is designed to. So it's no surprise that databases form the core of all of these information systems.
00:03:08.387 - 00:03:50.861, Speaker A: So, going back to why do we use Solana? Well, Solana actually is a database because of that. It can serve as the backbone of all of society's information systems. And as a blockchain, it is inherently trustworthy by design. And due to its efficiency, it's actually more useful than any other blockchain. So, as a database, Solana needs to store information. So the first type of information that needs to be stored on Solana are accounts, and accounts answer the question, what information exists right now? So some examples could be something like, I have $100 or I'm allowed to vote in governance. The other type of information stored by Solana are transactions.
00:03:50.861 - 00:04:44.995, Speaker A: So transactions answer the question, how did information change over time? So some examples would be I paid somebody $100 or I submitted a vote for a proposal. So first I'd like to talk a little bit about the account storage that we've implemented in sig. So the component that's in charge of this is called accountsdb. So here you see an example of a read operation where the RPC client on the left might be something like your wallet trying to get your account balance. So it's going to send the request to an RPC node to get that account, and under the hood, the RPC node is going to be looking in accountsdb to get that balance and return it to the wallet. So fundamentally, AccountsDB has to support two basic types of operations. So there's like a get operation, which is the read situation that I just described, and the other is a put, which is going to save an account into the database.
00:04:44.995 - 00:05:30.583, Speaker A: So that's going to be, for example, when a transaction is executed. So every single sauna validator needs to have a complete copy of accounts DB that's going to include every single account that's on chain. So the first time that a validator starts up, it needs some way to construct this gigantic database. One way to do this is you can start from a totally empty database and just rerun every single transaction that's ever happened. So this would take a ridiculous amount of time, and it's just not typically how things work. Instead, a Solana validator will download a snapshot from another validator on the network, and it's going to only need to process the transactions that have occurred since that snapshot and that snapshot is just a copy of the database. So inside a snapshot you have a whole bunch of files.
00:05:30.583 - 00:06:22.095, Speaker A: But basically the majority of those files are just storing all of the accounts and they're ordered by the time that they were modified. So let's say you wanted to find a specific account in the database. So one approach that you could use would be you just start looking at every single byte in every single one of these files until eventually you're going to see the pub key that you're looking for and you'll know that you found that account. Well, this again is. Sorry, this again is an operation that's going to take a very long time. And so it's not practical. Instead what you need is some approach that before you ever look for an account, you're able to just look one time at the entire snapshot and create a sort of map that's going to be something you can use later to navigate the database and very quickly identify where the accounts are located.
00:06:22.095 - 00:07:04.475, Speaker A: So that's the whole purpose of the account index. So the account index essentially is sort of like a hash map. So I don't have time to explain how, how a hashmap works under the hood in this talk, but the basic idea is that it's a data structure where you are able to provide some data called a key. In our case it's a pub key. And what the hashmap is going to do is it's going to very quickly return some data to you that has been previously associated with that key. So in our case that's an account location which is going to tell you either it's in the cache or it's going to tell you exactly which file you need to look at. And then it's also going to tell you where within that file that it's located.
00:07:04.475 - 00:07:48.755, Speaker A: So the whole idea of the account index is to accelerate the process of locating accounts. So it's all about performance. And that raises the question, how do we make it fast? So the first thing is getting fast writes. So in order to save data very quickly in the index, so we did, we made these fast by reducing the number of memory allocations. So every time you want to add data into the index, you need some memory that's going to store that data. And the problem is that allocating new memory is very slow because it requires the operating system to execute some very time consuming operations. So in order to address this, we took a counterintuitive approach and we used linked lists instead of arrays.
00:07:48.755 - 00:08:43.205, Speaker A: And what that allowed us to do is recycle old memory rather than having to reallocate new memory. So this really accelerated our write performance. Additionally, if you want the entire validator to be fast, you need to be able to support multiple threads looking at the account index at the same time. Now, the problem with this is that you have one thread that's modifying a hash map, then it's not safe to have other threads using Hashmap at the same time. So the way to deal with this is by sharding the index into many different individual hash maps. So that way you can have one thread that's working on the index can just look at one very tiny portion of the index, while the rest of the index is not compromised in usability. So this has just a huge multiplier effect on the performance of of the index when it's used in a multithreaded context.
00:08:43.205 - 00:09:31.755, Speaker A: So we also want the hash maps themselves to be fast. So we were not satisfied with any existing implementations and because of that, we decided to implement our own hash map from scratch. Now, it's inspired by Google's Swiss Table. And the general idea is that you break up the hash map into chunks of 16 items. And because the metadata is structured within a single byte, it allows that data to fit within the CPU cache. And then you can also operate on all 16 items with only a single CPU instruction, which makes it faster than any other approach. In addition to eliminate bottlenecks in the validator, we've also implemented a Geyser interface with Linux pipes that exceeds 30 gigabits per second, just running on consumer hardware.
00:09:31.755 - 00:09:56.565, Speaker A: So our optimizations have paid off. Our read performance is our main focus. And for that we're seeing the account index is performing between 1.5 times to as much as 4 times faster for read operations compared to Agave. Now, we also care about writes, we want the whole thing to be fast. And over there we're seeing performance from 1.1 to 6 times faster than Agave.
00:09:56.565 - 00:10:46.845, Speaker A: Thank you. So we recently published a Deep Dive blog post about this, so I highly recommend you check that out on the Syndica website where we go into just a lot more details about all this stuff. So if you remember earlier I mentioned there's another type of data stored in Solana, and those are the transactions. So transactions represent changes to the accounts and the ledger is what contains all of the history of transactions. So the ledger is used all throughout the validator. It's used for like rpc, consensus, turbine, gossip, proof, history, like anytime you want to find the status of a transaction, it needs to look in the ledger. So the structure of the ledger, like I said, there's the transactions, which represent.
00:10:46.845 - 00:11:16.187, Speaker A: Yep. The ledger is the history of all transactions. Right. But then to facilitate consensus in a blockchain system, you need to group those in really big groups called blocks. But this kind of introduces a little bit of a problem because now there's a latency to process transactions until the whole block is available. So Solana solves this problem by dividing the block into smaller pieces called shreds. So this is a major reason why Solana is able to be as fast as it is.
00:11:16.187 - 00:12:04.675, Speaker A: So, for example, the block producer is able to start sending out these shreds early, before it's actually even figured out what the entire block is going to look like. So another way that this accelerates Solana's performance is with erasure coding. And what that means is that every single shred contains some redundant information. But what that means is that no validator needs to receive every single shred. They can receive some random subset of shreds over the network, and that's enough for them to recover the other shreds, and then they're able to move forward without needing to send out a request, ask for the shreds that were missing, and wait for that response. It just really accelerates performance there. So the block store is the name of the component in the validator that stores the ledger.
00:12:04.675 - 00:12:43.929, Speaker A: So I'm happy to announce that we just very recently finished the initial implementation of the block store. So the block store, like AccountsDB, has these two general responsibilities of reading and writing data. So the block store actually contains a very diverse collection of data. But the most important thing are the shreds. And so a really big part of the block store is just inserting those shreds. And that includes the logic for recovering shreds with Reed Solomon erasure coding and also verifying oracle trees to ensure we have well formed trends. So we've implemented all that logic from scratch.
00:12:43.929 - 00:13:30.919, Speaker A: And then on the read side, some important operations are just things like gettingblock and getting transaction, which need to reconstruct the blocks from the constituent shreds. So now that we have the block store, how is SIG different? So we've taken a little bit more of a modular design. And what that allows us to do, for example, is to have a swappable database backend. So so far we've gotten two different database backends implemented. We're using RocksDB, and we also have an alternative that's implemented with Hashmaps and in the future we plan to experiment with some other back ends that should be very easy to add support for. Like, for example, LMDB is one candidate and we're going to be testing all the performance of this to make it as fast as possible. So another way that SIG is different is that it's RPS focused.
00:13:30.919 - 00:14:20.225, Speaker A: We've been looking through the block store identifying bottlenecks, and we see a lot of examples where things can be improved compared to Agave. For example, there's a lot of linear searches that are executed redundantly that we know that we can eliminate in sig. So the block store is done and we're going to have a blog post coming out for that very soon. Now that we have these databases implemented, how do we actually make use of them in a Solana validator? Well, the shred collector is the component in SIG that is responsible for for acquiring the shreds over the network. If you're familiar with Agave, you might have seen the TPU is a similar thing. So if you recall from Accounts db, the snapshot is downloaded which represents a specific point in time which is in the past. And then we need to start processing transactions to update the state.
00:14:20.225 - 00:15:13.225, Speaker A: And so in the shred collector we're going to be receiving these shreds. And within these shreds we can pull out the transactions and, and update the databases. So Turbine is the mechanism by which Solana Network is going to distribute the shreds throughout the entire cluster. So the leader is going to produce those shreds and then it's going to send them out to its neighbors, and then they're going to send it out to their neighbors and so on in this tree like structure until the entire network has every shred. So in this diagram here, the ones highlighted in blue are part of the shred collector. And some of the responsibilities there are to receive all the shreds over Turbine to verify the signatures of those shreds, and then they get inserted into the block store. And meanwhile the repair service is running the entire time, checking the block store for any potential missing shreds that could not be recovered, and it sends out requests to get those.
00:15:13.225 - 00:15:56.177, Speaker A: So after that they get retransmit over Turbine. So we've made a lot of progress over the past year. We've implemented AccountsDB, the block store, and finishing out with the shred collector. We have a full database and we're able to run it on mainnet and it can keep up with the chain and keep all the databases in sync. So currently what we're working on is a retransmit stage of Turbine, and coming soon we're going to be working on svm, RPC consensus, and block production. So decentralized networks like Solana work best when the community contributes together as a whole. And so that's why we're developing this completely out in the open on GitHub.
00:15:56.177 - 00:16:15.545, Speaker A: So you can check out our GitHub here, it's linked on the left side, you can look there for the latest updates and if you want to make any contributions. We're also very excited to share all of our updates and everything we learn with Deep Dive blog posts that we're going to keep putting out on the Syndica website. So thanks so much for your time, and I hope you enjoy the rest of Breakpoint.
