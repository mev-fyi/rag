00:00:02.680 - 00:00:20.514, Speaker A: Awesome. Cool. Yeah. Thanks again, everybody, for joining. Good to see you all. Looks like things have been rolling along pretty good here last week or two. Good start to 2022.
00:00:20.514 - 00:01:14.144, Speaker A: I don't know how many of you guys saw my announcement in the, when I posted this event a few days ago. We have a handful of engineers from the Solana Labs team on the call today. Also, Tao Xu is here with us. He's one of the developers on Solana Labs. And I asked him to speak on some of the improvements that have been coming in in 1.8, as well as some things that are currently being worked on to look forward to in 1.9 regarding the compute unit cost modeling, quality of service, and block packing optimizations, all of which we hope shall continue to make the network run more smoothly as we implement these things over time.
00:01:14.144 - 00:01:30.034, Speaker A: So I'm going to keep hopefully myself down on mute for most of this, and I'm going to hand it over to Tao and kind of let him give an overview. And we should have plenty of time for questions and any discussion thereafter. So take it away.
00:01:31.054 - 00:02:09.264, Speaker B: All right, thanks, Dan. Hey, everybody, this is et al dev from Solana Labs. I'm based at Chicago. So I'd like to use the opportunity today to talk about what the cost model in one point, a 1.9 in testnet that does and does not, why it values and, you know, maybe touch base some of the details about how it does its job. If you have any questions or comments, please feel free to interrupt because, you know, discussion is always better than just talk. So let's get to the first nod of the bet.
00:02:09.264 - 00:03:19.536, Speaker B: What the cost model does not do is it's not fee governor. It does not set fees for transactions. What it does do is to optimize. It's a tool for the leader, for the banking stage, to optimize the block production. So the goal is for a leader to produce a block that can be reasonably, on average by any validators on the cluster within a slot time, which is 400 milliseconds. Meanwhile, to maximize blocks federalism and why it's important so early, we, especially during bots attack, leaders tend to could possibly pack in too many transactions into a block. And the cause of block in some cases takes seconds for metadata to replay and the metadata will missing its leaders slots and causing the forking routes, not rooting and all that.
00:03:19.536 - 00:04:09.374, Speaker B: It's a risk. And with this feature, with the cost model right now in mannet and the testnet aiming to do, the goal is to make sure the block is the size is reasonable enough that all the validators can replay with the slotlet so nothing will fall behind, and also trying to maximize its parallelism. And to that end, there's two limits being set for the block. One is a block limit, overall limit, that is essentially to make sure the block is not too big, not too expensive. The second one is single writable account, maximum limit. We don't want a block. All the transactions in the block writes the single account, and then we cannot really spread them out on the concurrent processes.
00:04:09.374 - 00:04:58.972, Speaker B: So what is the limit? The limit is unit is compute unit. Compute unit is a constant. It's a deterministic cost determined by executors to say, hey, this program going to cost us much. Each actions we do will cost as much compute units. You know, load the vm, compile it, execute it, calling the SysVa writing to account. So all those actions, activities per se has associated u compute units that is constant across the clusters using the compute units. Mapping to the replay time requires a conversion that is from compute unit to the microseconds.
00:04:58.972 - 00:05:53.834, Speaker B: This conversion is obviously hardware dependent. Faster machines can execute the same amount of compute units in faster times, slower once machines takes a longer time. So there's a conversion. Like I said, the goal was to get the average validators on the cluster to be replayed block within the time. So the, the conversion, the ratio is also taken as an average of the cluster. So some of the validators may be too slow, will not be able to catch up, to take a longer time, and it will miss only those slots. Few of the validators missing data starts won't cause a problem to the cluster, because that's what the cluster build for, and that actually serves a motivation for those validators to upgrade the hardware.
00:05:53.834 - 00:07:03.464, Speaker B: The other thing about the cost is programs costs are predetermined, but instructions cost varies. Instructions cost depends on how many sub instruction it has, depends on the accounts it's going to touch and how big the accounts are, so on. So, instructions costs are estimated whenever an instruction being executed, its cost, its actual cost being reported and put into algorithm to get the overall estimated cost for the instruction. So next time the instruction comes in, the cost model will use the estimation to say, hey, this instruction likely to cost this number of complete units, and we can budget that in that way. And this algorithm is pretty simple. It's just mean on the weighted moving average plus three delta deviations, because we want to get to the classes slightly on the high end to be safe. Any questions so far?
00:07:04.444 - 00:07:14.812, Speaker C: So, is this conversion ratio between compute units and like, microseconds? Yes. Is that like dynamically, like, determined?
00:07:14.988 - 00:07:50.330, Speaker B: No. So we build a dashboard. So essentially, this is a good question, though. So the reports of actual costs versus microseconds are reported by replay stage. So all the validators, when they execute a transaction, it reports how many microseconds spend on the instructions that being calculated, have how many compute units. The reports is numbers to the matrix. And on the matrix we run the aggregate queries to say, hey, cross clusters.
00:07:50.330 - 00:07:58.746, Speaker B: For this particular program, id, what is its cu, what is microseconds? And divide by that, we get this ratio average.
00:07:58.930 - 00:08:06.210, Speaker C: So, but the point is, like, the actual, like, developers have to look at these metrics and like, basically tune these, like, constants.
00:08:06.362 - 00:08:12.250, Speaker B: No, no. So the constants is statically defined in the code.
00:08:12.362 - 00:08:18.834, Speaker C: Yeah, yeah. So, yeah, so basically, like, we have to, like, look at the metrics and, like, figure out these constants.
00:08:19.374 - 00:08:20.154, Speaker B: Yes.
00:08:20.454 - 00:08:20.910, Speaker C: Okay.
00:08:20.942 - 00:08:58.650, Speaker B: And this constant should not be changed so frequently. Normally. In fact, ideally, we haven't had a chance to change it yet, or we did it once, but during the test. But ideally, this number changes because this is the only thing that connects the constant world, where the ceus are, to the real world, where the microseconds are. So anytime we change this number, we should be feature guided by. Yeah, and yeah, again, this number, this dashboard, you can run a queries on the matrix to get different networks. Ceus and the numbers should change constantly because.
00:08:58.650 - 00:09:16.450, Speaker B: Depends on the hardware. Join the clusters, they've been upgraded or not. Behaviors of the programs. Things get more complicated. It takes a longer time to secure. So, yeah, a lot of variations over there. So we sort of, we smash them together and get a single one number.
00:09:16.450 - 00:09:36.642, Speaker B: Want to be on the safer side. That's why we put a three standard deviations on top of the main. And the goal for the number doesn't need to be accurate, it just needs to be static. So the goal will be the blocks produced will be replaced by most validators on the cluster within the slot time.
00:09:36.778 - 00:09:42.364, Speaker C: And the idea is basically just that, like, hardware will, like, tend to only get faster and stuff.
00:09:42.524 - 00:10:05.212, Speaker B: Yes. That's why you cannot use microseconds as a mean to calculate the block cost or expense. Right. Because microseconds change everywhere. Cool. That's good question. Any other questions? So just want to point out.
00:10:05.212 - 00:10:06.878, Speaker B: Go ahead. Sorry, sorry.
00:10:06.966 - 00:10:48.624, Speaker A: I was just going to make a clarifying remark, because I know you mentioned this at the beginning of the talk, and I just wanted to make sure that the point got all the way around the goal of all of this is to make it, I think you maybe mentioned this is to make the replay stage for the vast majority, three standard deviations of all the validators no longer than a single slot time. Right. Because the thing is, one of the factors that we believe was driving a lot of forking was excessive replay times because the total compute packed into a single block exceeded what any replaying validator or what any validator could replay in the time given to it in the following slot.
00:10:50.244 - 00:10:51.664, Speaker B: Yep. Well said, Dan.
00:10:52.564 - 00:11:03.168, Speaker D: So do you see a lot of variation between the, I guess you're collecting these numbers. Is there a wide variation between capabilities of validators that you see or are they all packed within a pretty close range of capability?
00:11:03.256 - 00:11:32.684, Speaker B: Oh, the good question. So the hardware, the validators performance are vastly different. So the bandwidth between the high performing and some slow ones is large. So some slow machines again will miss even with this, they will miss their leader slots because of long replay time. And again, like I said, this would serve a motivations for them to upgrade their hardware for them not to miss. In the leaders.
00:11:33.104 - 00:11:54.578, Speaker D: Is this information about how a particular validator is performing something that is easily available to validators? I imagine that maybe if we looked at our logs or something we would see it or if we knew how to look at the metrics site. But since you already probably do a lot of that, is it worthwhile to make this information more available and accessible to validators so they can sort of self select better hardware?
00:11:54.776 - 00:12:43.346, Speaker B: Yeah, well, so today it's like, well, first of all, if you keep missing a little slots, you know something's wrong and you're going to do something about it. If you dig into it, probably will be seeing it. We don't have like sort of notifications or warning to say, hey, you've been too slow because slow is a relative concept. Depends on how the other elevators working. I think missing slot is a first warning sign, this flag to look at for. And if you look at the matrix on the replay stage table, I believe, and you will see on your particular host how long it takes to how many microseconds it takes to run those transactions. And if, as well as the compute units.
00:12:43.346 - 00:12:56.454, Speaker B: And if you do a division between the compute units to the microseconds and if you get the ratio much, much lower than the published ratio, then that probably a good indication that the system running slow.
00:12:56.754 - 00:13:28.234, Speaker D: Yeah, I mean that's great and useful information, but I bet that there's a lot of validators that probably just want to go right to whatever number you just said without having to do look at charts and, you know, not that, you know, only because anything we can do to sort of give, first of all, let valers understand how their performance ranks. It doesn't have to be in absolute terms, but just are you 80% as good as, you know, the average? Are you 110% as good? And then have that be easily available, might encourage validators and make them even aware that they might have something to do.
00:13:29.174 - 00:13:38.874, Speaker B: That's good suggestions. I'll just write this down and I'll look into the team to see if we can provide easier ways to notify the validators. Say, hey, you may be running slow.
00:13:39.654 - 00:14:02.294, Speaker C: And actually just out of. This is more of a curiosity question, but you mentioned three standard deviations and stuff. I'm actually curious, does it actually make sense to assume the performance of the validators like follows like a normal, normal, like distribution? Or is it like, for example, like some validator is just like super fast and it's just like. I don't know.
00:14:02.414 - 00:15:10.946, Speaker B: No, yeah, I guess it's a good question. You don't really necessarily know how the performance of each valid distributes harder layer over there. Three deviations. The way of this mean passive deviation on weighted moving average allows us to move this whole formula up and down together with this moving average. So it's just a way of coming up with simple data, right. Massaging them together into single measurements that we can tune it for the system versus, you know, multiple knobs everywhere, trying to achieve more accurate, which I don't know, it's a constant benefit discussion, I guess. So, like Dan mentioned, this whole cost model thing here is for, the main goal is for the quality of construction block and fall into umbrella of quality of service for the leader.
00:15:10.946 - 00:16:25.284, Speaker B: There are a few more things down the road to improve that. It's just refining how leader works. And one of the thing is there's a pr open on the GitHub, but when we essentially, we want to have validators have n number of percentage of stake, have this n percentage rights to say what put into the block. So when the leader put in the transactions into the block, it would do a random shuffling, but based on the stake weight. So the transaction coming from the validator have a higher stake, has higher probability to be put into the block. And that should help us to deter and excluding more bots transactions in the event of attack. And there are also pr open, they have discussion code going on to deter the bots attack by charging more fees if they allocated account without actually writing to it.
00:16:25.284 - 00:16:49.964, Speaker B: And there's, of course, still bankless leader down the road, which is not a pressing issue right now because right now the slowness usually caused by the replay value added to the replay, not the leader producing a block. But I just want to put this out so you guys know what to look in for and to put comments on the. On the prs.
00:16:51.864 - 00:16:53.448, Speaker D: Can I ask you to clarify?
00:16:53.576 - 00:16:54.184, Speaker B: Yes, sir.
00:16:54.264 - 00:17:16.402, Speaker D: So you said stake weighted by stake weight. Validator forwarded transactions will be included more readily in blocks. That is that. Is that what I'm understanding? So, but for transaction to meet that criteria, it has to first be forwarded. So that only affects transactions that first hit a validator and then that validator forwarded them. Is that transactions or that only transactions?
00:17:16.458 - 00:17:20.534, Speaker B: I think that's the most transactions, yes. I'm sorry.
00:17:21.954 - 00:17:22.634, Speaker A: Well, I just.
00:17:22.674 - 00:17:34.574, Speaker D: I know that RPC nodes also submit transactions, so those would then be, you know, lower priority. They'd first have to hit a validator from an arpeino and then get forwarded before they get higher priority. Is that what you're saying?
00:17:39.234 - 00:17:59.974, Speaker B: All right. Okay. I don't actually, I cannot clarify this right now. I think pc go through. You know what? I will have to double check this. Let me get back to you.
00:18:00.774 - 00:18:01.470, Speaker D: Okay, that's fine.
00:18:01.502 - 00:18:02.234, Speaker A: Thank you.
00:18:09.894 - 00:18:16.714, Speaker D: Where do you want to go first? You're going forward to your notes? Yeah, go ahead.
00:18:19.374 - 00:18:34.394, Speaker B: Yeah, I'll get back to you on this call. Metadata discussion over there. Any other questions?
00:18:40.494 - 00:19:02.150, Speaker A: All right, there was a, there was a quick question in the chat from Smith about any time that the hard coded constant might be changed in the future, I believe you mentioned. I just want to confirm earlier on in your chat that a future change to that value would be feature gated. Is that correct?
00:19:02.262 - 00:19:18.314, Speaker B: Yes, that's the plan. Once we found the class that has moved far away from what we know last snapshot status and we want to change the number, we will make a new release and that should be feature gated.
00:19:21.034 - 00:19:28.058, Speaker A: Great. Okay. Yeah, so tldr there. These wouldn't change willy nilly between.
00:19:28.226 - 00:19:37.054, Speaker B: Right. And then we don't expect this to change frequently. It should be rather more static than, you know, continue changing number.
00:19:48.574 - 00:19:50.150, Speaker A: Awesome, Tao, thank you very much.
00:19:50.222 - 00:19:51.754, Speaker B: All right, thank you, guys.
00:19:52.054 - 00:19:54.594, Speaker A: Looks like we might have another question here.
00:19:55.654 - 00:20:40.620, Speaker B: What happens to the transactions that you drop because they don't longer fit in the block? What if transaction being dropped? And what was the second part? I'm sorry. So what happens to those transactions that you drop? Well, transaction being dropped. Being dropped. So even without a cost model transaction will still be dropped. So what? The transaction can fit into a block, so gated by cost model, it will be put back into the retry buffer just as it is right now. And it will be either retried in the next block or be forwarded to the next leader and it will sit there until it expires. I see.
00:20:40.620 - 00:20:41.344, Speaker B: Thanks.
00:20:56.444 - 00:21:27.358, Speaker D: Can you. I'm sorry, I don't want to perseverate on this topic, but it is interesting to me to think about prioritizing forwarded transactions by stakeweight because it means that a leader with high stake will have its transaction. Anything it couldn't handle forwarded and then the next leader will prioritize those. But if it can't handle them, then they get forwarded again and the leader after that will not prioritize them. I'm not really understanding what that achieves because it almost feels like it's just happenstance of what leader happens to be running when a transaction couldn't fit in a block.
00:21:27.526 - 00:22:19.944, Speaker B: Right. Yeah. Good topic. Let me put the pr in the chat so we can, might be better just to bring the discussion to that particular pr we're working on. So find it real quick. So posted the PR on the chat. So this PR links to the next one.
00:22:19.944 - 00:23:03.414, Speaker B: Two prs work together for the functionality. We're still testing it, but what you said is a good input we should put into the test consideration. Yep, that's why it's still on the pipeline.
00:23:13.514 - 00:23:15.912, Speaker A: Awesome. Thank you. Thanks a lot, Tao.
00:23:16.018 - 00:23:16.824, Speaker B: All right.
00:23:18.524 - 00:23:36.304, Speaker A: So yeah, we're just about a time. Appreciate the input from, from the tech team. So it's not just always me blabbing at our community of wonderful validators here. So, yeah, is there any other final questions for Tao or more generally before we break today?
00:23:41.544 - 00:24:21.044, Speaker D: Sorry, I'm going to ask another one. I know that compute units, I think people often confuse transaction packing and compute unit computations with transaction fees as a mechanism for trying to discourage too many transactions being submitted. Our transaction fee, which one is likely to, like, what is the sequencing of these? Are we going to see compute unit changes before transaction fee changes? Are these, you know, like when can we expect kind of both types of changes? Because transaction fee changes could have much more wide reaching effects on all kinds of economics and I'm curious to know what the track is for those changes also.
00:24:22.784 - 00:24:58.268, Speaker A: Yeah, I can, I can take that one. So the compute unit changes, I think are very likely going to be coming in soon. Most likely, I believe in the 1.9 branch. Any changes to transaction fees is something that you're effectively tinkering with the core economics of the blockchain and the driving factors for validators and clients. So that's actually a much larger, and I would say a more sensitive consideration. It's not off the table.
00:24:58.268 - 00:25:42.124, Speaker A: I just saying this isn't going to be just patches pushed in code. This is going to be like with something that we'd want to make sure we have like a community governance vote or proposal before we would make any sort of economic changes. Some of the lessons learned from the compute unit feature additions can be used to inform potentially updated economic models and effectively creating like a fee market. But I think it's a little too early to say exactly how those two things would interact with any kind of specifics right now until more of the cost model features are implemented on Mainnet.
00:25:52.304 - 00:26:02.954, Speaker D: So the transaction features are not in 1.9. I guess there was a time when I thought that stuff was, something was happening at 1.9, but we shouldn't expect anything like that in 1.9 is what you're saying.
00:26:03.614 - 00:26:40.584, Speaker A: Yeah, I believe that's correct. I think there was some confusion around the terminology of cost model. The word cost might think certain things, but it's always been compute cost rather than actual fee cost. Again, like I said, this could be used to inform a fee market change going forward. And so it is definitely, I think, attached to some forward looking, you know, thoughts and ideas. But it is going to be a whole, a separate implementation before we see any, any changes to the current fee structure.
00:26:49.764 - 00:27:11.514, Speaker D: So a follow up, the congestion modeling for changing fees, is that going to change? Because that seems like it should perhaps be tied to compute unit instead of signature accounts, which ended up kind of being inadequate for that purpose and not allowing any congestion model to even sort of take effect. And at least as I understand it.
00:27:15.134 - 00:28:01.424, Speaker A: Yeah, I don't, I don't disagree, but I, I'm hesitant right now to, you know, make any, make any promise on that again, because of this sort of, I want to see like the full compute unit model roll out, you know, before we would update, like congestion based fee economics. I mean it's, it's a fair point, right. I think this is definitely something that we need and we want to see put together. I just want to make sure we, you know, we are judicious and careful with, with, you know, what other, what else we might inadvertently introduce. You know, if we jump into something like this too quickly.
00:28:02.564 - 00:28:22.244, Speaker E: I think that is the direction that we are heading, is to move congestion away from just sigs and fees in general, away from just sigs and base that more on a more overall approach to what load transactions put on the network, which includes both signature checking as well as compute units.
00:28:26.944 - 00:28:27.368, Speaker B: Awesome.
00:28:27.416 - 00:29:00.704, Speaker A: Thank you, Jack, for the added color. Jack is also one of the most senior engineers at Solana Labs. For those of you guys who may or may not have seen him around, well, great. Thanks, everybody, for your time. We're a few minutes over, so I'm going to call it here. Thank you, Tao. Thank you, Zantetsu and everyone else who had questions and feedback for us.
00:29:00.704 - 00:34:12.304, Speaker A: Yeah. Just really, really appreciate the engagement. So, as always, we'll see you guys in a couple of weeks. Thank you very much. Bye. It.
