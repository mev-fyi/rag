00:00:17.720 - 00:00:33.674, Speaker A: Hey, guys. Hi, everyone. I'm gonna let people settle down for a bit. So, my name is Marisol. Let me just check how this works. Okay. My name is Maurice.
00:00:33.674 - 00:01:11.228, Speaker A: I work for Hubble protocol. We're building a stablecoin in Solana, cross collateralized, yield bearing stablecoin. And we've been facing a lot of challenges building the stablecoin, building the protocol, and basically, today I'm here to talk to you about our journey and how we talk about scaling and the issues we encountered and the lessons we've learned. So, yeah, so about me, just a little bit. I've been working before going to Defi to crypto full time. I was working at Bloomberg for about eight years. I was building derivative pricing systems.
00:01:11.228 - 00:01:48.246, Speaker A: I was writing C Ocaml, and that led me to learn and love Rust. And recently, I've been working at Hubble and building the stablecoin. So if we think about the situation where we're in right now, or what we've been seeing for the past year, is that we have this new chain, Solana, which can actually finally scale. You have something, you have the opportunity to build something for the entire world. You have real people that can use it without paying a lot of fees. The UX is great. It's censorship resistant, so they don't worry about it.
00:01:48.246 - 00:02:38.686, Speaker A: And you can finally, if you can, onboard 1 million people, you need to start raising to the challenge and see what can we actually build for them. And of course, Solana allows this to happen, but it comes with constraints. So there are two main, major concerns, two major constraints we faced when building Hubble. One is the account model, the way you store state, and the other one is the compute budget. If you've written code for Solana, you've seen that it's the first three weeks, you go into a deep depression about how to write code for Solana. It's like everything is very weird, and then you start getting it, and then you start thinking different solutions, and then you hit compute budget exceeded. And then you're like, what am I doing? This is all annoying, and I need to find a way around it.
00:02:38.686 - 00:03:17.258, Speaker A: And you eventually find your way around it. It's just like you emerge from the depths of Solana, and then you start thinking about scaling, and then you have the breakthrough, essentially. So I'm going to talk about both of them and how we think about it and what solutions you can consider. So, to put everything in context, let's just consider this simple instruction. You have, obviously, a defi application. You have a borrower which deposits all and takes a loan in dollars. And it's over collateralized because we still haven't solved necessarily the under collateralization issue.
00:03:17.258 - 00:04:05.406, Speaker A: But you just hold that data, and then there's an instruction called tri liquidate. And if the deposit amount is below the collateral ratio, then you liquidate the liquidator, pays the debt, and then receives back the collateral. And if you do it at a higher collateral ratio, then you make some gain, and then you keep the system healthy. So if we look here, we have two CPI calls at the top, CPI transfer for two of them, and then you have state update. You update the user balance and the user's collateral. So this is nice, and it's like the hello world of a debt, and it's just, everything is beautiful. But then you start thinking, how do I scale? Like, how do I make this work for a million users? And why should I even think about this at the beginning? Everything, if it works for two users, maybe it.
00:04:05.406 - 00:04:31.240, Speaker A: It already works for a million. But maybe you have, like, you know, when you have a complex system, you have weird interactions that you can't foresee ahead of time. So you actually need to start thinking about testing, stress testing and so on. And as you scale for users, you actually scale with the complexity of the project. You add metadata, you add different collateral ratios for different tokens within your user. You can start adding yield, you can start adding strategies for the. And then things become more complex.
00:04:31.240 - 00:04:58.656, Speaker A: So you need to start adding more data. So you need to scale horizontally, and then you need to scale vertically for users and for data. And then you deploy, and then you have the MVP. But anything, how do I deploy in Mainnet? What issues can I have? And on Mainnet, it's pretty scary to change things because you deal with people's money. You're doing something that is quite sensitive to bugs. So that's one concern. And the second concern is that you want to scale.
00:04:58.656 - 00:05:28.312, Speaker A: You don't want to do migrations. If everyone has done ever database migrations before, I know you haven't had a good time, so you want to kind of avoid this. So these are the things that I've been thinking about for scaling Hubble. And now if you think you have basically two fixed constraints, you have state account sizes, or the fact that you have to declare them up front. This is one constraint. Compute budget is another constraint. The only degree of freedom you have is actually the way you design your state.
00:05:28.312 - 00:06:06.982, Speaker A: Like, how do you maintain state? How do you save it across instructions, and how do you reason about it, basically, and there are a few common patterns people have tried out. Padding, of course, to protect from future behavior. Pagination, queuing and splitting your instructions across transactions and basically off chain processes like bots or cranks. So these are the four things I'm going to talk about. So if we look here, for example, the easiest thing is like I have a borrower, I want to scale for a lot of users. I can allocate, let's say ten megabytes. That's already expensive.
00:06:06.982 - 00:06:27.440, Speaker A: First of all, it's like seven days old, it's already 15k. Declare up front, maybe two months ago. That was nice. But as Sol is growing, you want to be smart about how much you pay for rent. But it's obvious to think like this. You have an array, everything is great. To want to scale to more users, you start adding pagination, you paginate and then every user belongs to a page.
00:06:27.440 - 00:07:04.776, Speaker A: And then as you grow, you start scaling like that. And you see that, you notice the padding part in the borrower, you save like 1000 bytes. Those are bytes you pay upfront for users you don't have yet. And also the user doesn't pay for it, you pay for it and it's a lot of an upfront cost, which you don't want to if you're just starting. You know, maybe you don't have it. So it's a solution to maybe not a very thoughtful design. So yeah, the problem is you pay it up front, but this is, for example, this is how you would scale.
00:07:04.776 - 00:07:37.824, Speaker A: You have some bytes, you add more types to it and then you decrease the padding and this is how you can scale. But still you have the user positions array, which is fixed and you encounter the issue. So the solution is pretty much don't use arrays. You pay a lot upfront and then you have problem scaling. That doesn't mean that arrays are all bad by themselves. Arrays can be useful if they don't scale linearly with the users, but they represent some sort of queue. So for example, if you want to, you can use an array.
00:07:37.824 - 00:08:13.518, Speaker A: Sorry. The benefit for using array in this case is that it can iterate to a lot of users. At the same time you can actually loop through all the users or as much compute budget as you have, mark them as mark for liquidation, and then proceed an instruction after. So that's one cool thing you can do and we can reason about it a bit later. The better solution is don't use array at all, index by account type, but you still keep padding. You kind of get get the best of both worlds. You keep some padding, you pre allocate a little bit, but you operate on user accounts only.
00:08:13.518 - 00:09:05.462, Speaker A: No raise, so the user pays. You grow when the users are being acquired and you don't have to stress that you reach the account limit and you won't be able to add new users. This is how, for example, then let's say you run out of padding. You can create a borrower, v two that contains new metadata, and you create a liquidate function, v two that takes the new user type, and then maybe you can convert that user to the old user to the new user. And every new user comes with a new type, and then you migrate on the fly, so you might get away with it. I promised I would get back to kind of how user a. So we have a process in our protocol called redemptions, which means we need to process a lot of users at the same time, sort them and do something with their collateral.
00:09:05.462 - 00:09:45.690, Speaker A: And of course, because you can't loop over it, because compute budget is expensive and you don't want to rely on an off chain bot like a permissioned off chain bot, because it's a single point of failure, you can actually create a queue. You can fill the queue. Another bot can clear the queue once everything's done, once the requested amount and remaining amount, if they're still different, you can start the process all over again. So you can use the queue just to like a buffer, for example. This is a great way to use the queues. And serum are using queues and it's a good primitive. So now this is how I reason about state how you should grow.
00:09:45.690 - 00:10:09.904, Speaker A: You should grow linearly. You shouldn't have arrays. If you use arrays, use them for queues. Now the question, the other challenge is compute budget. If you've encountered the 200,000 compute budget limit, you've always think, think, how can I optimize it? You try to optimize it, make less clones, but then eventually you still can't scale. You still have a fixed budget. And that's just no matter how complex, no matter how smart you are, you're still going to hit that limit.
00:10:09.904 - 00:10:35.814, Speaker A: So the solution becomes split, like make it like a state machine. You can choose a user. If the user can be liquidated based on collateral ratio, then you can update the status as marked for liquidation. And then you have another bot running it later. Processing the order and doing this allows you to scale. You can do very small transactions and fast. And you're not worried about blowing out of compute budget.
00:10:35.814 - 00:11:21.482, Speaker A: But this, you know, this requires having an off chain bot to finish the processing. The bot will see, are there any accounts marked for liquidation? They will loop on like a cloud somewhere. And if it is, they're going to do a transfer and then update the state. And you can do it as you, you can parallelize it. If you don't have overlapping global state, you can parallelize it, and you have a lot of room to play around with that. And then as you grow, as you need more and more complex operations, if your protocol is not necessarily simple, you need to, it becomes more complex. You add liquidation, add order, fill order, clear order, and then it starts being more and more involved, and you rely on a lot of off chain computation, and that's fine if you can manage it.
00:11:21.482 - 00:11:58.554, Speaker A: Like off chain computation is not that expensive. You can have some sort of a mirror or a shadow library that can track the global state off chain and then execute whenever something is needed. But you need to incentivize these bots to actually do the processing, because if they don't do it, if you're the only one running it, then you're the single point of failure. Being a single point of failure is a danger to the protocol. Whatever your machine or your cloud, or you forgot to pay the bills on AWS, and they shut you down, and then you're like, your protocol is not moving forward. So you want to incentivize people to actually run the bus themselves. You want them to compete on speed.
00:11:58.554 - 00:13:12.230, Speaker A: The first one transaction gets the fee, so there's a healthy competition, and your protocol is stable. But as you add these things, your protocol grows more and more complex. So let's say someone added a liquidation order, the user is not quite yet processed, it's smart, as liquidatable, and then the user wants to do something after that. In that case, the problem is that the user is marked for liquidation, that maybe the data is not yet, the collateral is not yet moved, and that's fine, you can manage it, but you have to add more logic inside your original instructions to say, is the user active? Can I use it? Can I process it? And this basically means you're adding more and more complexity. The cost of the idea is that if you have some constraints in Solana, you engineer a way around it by adding complexity. And this complexity needs to be managed if you want to add further instructions, if you want to enhance the protocol. So as you add scale and complexity, you need testing essentially becomes, I know people have different feelings about testing, but becomes absolutely essential because you, complexity means that you can break something later on when you're trying to do something else.
00:13:12.230 - 00:13:59.074, Speaker A: And then because of that, you need to lock behavior and have test coverage and you need to make sure that everything works fine and you can. Typically people do tests in the obvious way. You write like you deploy it on the local validator, compile, deploy, write the typescript test, run typescript test, assert balances, or assert that status is okay, and then you know that you've implemented right or wrong and there's a problem with that. There's like the feedback loop is crazy. Like you wait so long for something to run, you spend, spend time doing the setup, spend time doing RPC calls, and then only later you find out if you did it right. And if you do it for two, three users, maybe you can get away with 1 minute. But if you do it for 1000 years, it just becomes prohibitive.
00:13:59.074 - 00:14:22.164, Speaker A: So, and when you're doing that integration test, you're testing a lot of things. You're testing that the validator works, you're testing that RPC calls work, you're testing that token transfer work. These are primitives that already work. Solana's job to make sure that it works. It's your job to make sure that the smart contract logic works. So you shouldn't be testing all of these things. And it becomes difficult to write tests.
00:14:22.164 - 00:14:56.414, Speaker A: And people don't like writing tests if they're annoying to write, so they will avoid writing tests. They will like, yeah, if it compiles it works. So it's not a great solution. So you need to find a way around it. And the way we thought about it about solving this complexity problem and testing problem is how do you reason about your program? So if you think about how your function actually looks, you're doing two things. You're modifying the state, you're updating the state, you're mutating the state, and then you do some side effects like token operations. CPI calls, this is pretty much what you, what all instructions look like.
00:14:56.414 - 00:15:44.104, Speaker A: So if we look, one way of us designing it is we encapsulate the state mutation inside a single function, and outside of that function we run the side effects like the token transfer. And the result of the encapsulated function are the effects that you need to run on the chain itself. So in this case, triliquidate will mutate, the user will mutate the market global state, and it will tell me that I need to do some USD transfer and some sole transfer, and I don't need to test the CPI transfer function because I know it works. I've tested it many times, and I know that it will transfer this exact amount of dollars that I told it to do. So then it's like writing an in. It's like separating the concerns. You only care about the top function.
00:15:44.104 - 00:16:37.594, Speaker A: So now, because you can do this, you can split it like this, you can take the effects you can think about. Your instruction is, what should my instruction do? What are the side effects of my instruction, and what are the mutations that my instruction needs to perform? What do I want to test? What am I actually doing in this instruction? So this encapsulation separates the state mutation from the side effects. I know functional people will call this blasphemy, like, not everything is a side effect. Here, the statement itself is a side effect. If clones and copies would not be expensive, you would get the user account and the market account as a side effect of the computation, and then you'll reassign it. But because you want to save computing, it's, you actually do the mutation inside the function. That's the compromise we reached.
00:16:37.594 - 00:16:55.946, Speaker A: So then out, you can basically test for a million users inside rust. Testing in rust is right there, front and center. You just slap an annotation on top of the function. It becomes a test. You don't need to build a new binary. You don't need to create a new makefile. It just works.
00:16:55.946 - 00:17:10.413, Speaker A: Just do cargo test and it just works. And it's extremely fast. It's blazingly fast. It doesn't run on BPF. It actually runs on your machine, and you should take advantage of that. Rust brings new tools to the smart contract problem. So you can literally take advantage of this.
00:17:10.413 - 00:17:43.922, Speaker A: That's the cool part. You have so many property based tests, libraries, you have so many benchmark tests. You can profile, you can do a lot of things. And in this case, I can create a million users. I can run liquidation effects over, I can fold over them, let's say, in a functional way, I can accumulate the results and then assert the results that are what I was expecting. These effects would be actually what the CPI call would do. So if you've designed your state well, then you don't have to write integration tests for a million users.
00:17:43.922 - 00:18:15.244, Speaker A: You can actually write an integration test inside rust, like a unit test. That's kind of our way to think about it. And this unlocks a few cool things about designing your program. You can test the scale, you can do property basis, you can do fuzzing all of them inside rust. You can take advantage of the rust ecosystem to, to test your smart contract. It runs in the blink of an eye. If you want to, I challenge you to build a million users for your protocol and tell me how long that takes, and I'll write it in rust and see how long that takes.
00:18:15.244 - 00:18:57.260, Speaker A: And the other cool thing is that because you have to go around the engineering problems to write your smart contract, and if you do a lot of complexity in rust, you can actually write the lazy implementation. I wrote one part of our protocol in like 20 lines of code, because I can do a massive for loop. I can have a hash map, I can have a lot of things. And I just test that implementation with the rust implementation, because the rust unit tests are complete, they match what the side effects would be. So I can test those two implementations against each other and assert correctness. And you have more confidence in the correctness of your program. And ultimately, of course, you can copy three lines of code, do cargo test, and it just works.
00:18:57.260 - 00:19:26.452, Speaker A: The feedback is loop is much faster. It's going to be much easier to write, to do TDD. Yeah. So this is what I was trying to, basically what we've learned from building Hubble, how you deal with state and how you deal with time complexity. Don't use arrays, use skews. Arrays make sense for queues. In Solana, if an instruction is too much, split it.
00:19:26.452 - 00:19:55.566, Speaker A: Create a state machine where multiple bots will turn the crank and push the state forward to the end state. Separate your concerns, your side effects from the mutations, and try to test the mutations themselves. And, yeah, stress test, because you don't want to test in production, you kind of want to test in development, basically, I guess. Yeah, that will be it. We're building Hubble. It's a new stable coin in Solana. We're going to launch in December.
00:19:55.566 - 00:20:02.974, Speaker A: So we're looking for devs. So if you want to write rust, come talk to me. Thank you.
