00:00:02.080 - 00:00:52.124, Speaker A: All right, welcome, everyone, to the Solana Educational Solana Validator Educational Workshop, May 14, 2024. Today we are going to go over the transaction lifecycle. So this will be kind of a high level overview of what happens when a transaction goes from somebody's wallet or DAP or whatever to the Solana blockchain and then ultimately ends up confirmed. The focus will mainly be on the first half of this agenda, so, well focus mostly on high level transaction flow ingress in the banking stage with fewer details on turbine and consensus. I think those are maybe less relevant to what validators are concerned about right now. But if you have more questions, let me know. I can see how well I can answer them.
00:00:52.124 - 00:01:27.994, Speaker A: All right. Just want to verify everyone could see the screen. Transaction overview. Yep. Okay, great. Okay, so the high level, kind of very, very thousand mile view of what's going on here is when you create a transaction, if you're a user or a user using a DAP or you're even a bot, right? There's something that's creating a transaction, a transfer, an NFT, mint, whatever. Um, typically, but not always, that transaction will go to an RPC node.
00:01:27.994 - 00:02:18.672, Speaker A: So if I'm transferring some soul in my wallet, um, my wallet will create that transaction. It will send the transaction to an RPC node, um, many RPC providers. Or you can run your own RPC node, right. Uh, that RPC node will receive the transaction, uh, and then send it to the current leader and potentially the next leader. So the idea here is, as an Nn user, you are interacting with the blockchain via RPC node. RPC sends to the leader. The leader processes that transaction, assuming it's included in the block, it is then broadcast out to other nodes in the Solana blockchain, other validators, I should say that, then confirm that transaction.
00:02:18.672 - 00:03:14.360, Speaker A: So at a very, very high level, that's what we're talking about today. Now, that's sort of a somewhat older view of the system, because currently we've got stake rated QoS. This is a view if stake rated QoS is not involved. But really what's going on right now is we have this intermediate step for some RPC nodes. So when a transaction is created, it gets sent to an RPC node. Assuming that RPC has a peering relationship with a stake validator, rather than send it directly to the leader, the RPC node will send it to the staked validator that they're appeared with. So if you're not familiar with stake weighted qos, in order to set up this relationship, an RPC node would specify a validator that it wants to appear with.
00:03:14.360 - 00:04:24.708, Speaker A: It would have to make some agreement with that validator. The decides how much stake to give to that RPC node on the network, like a virtual stake, essentially. It's not actually delegating any soul, but it's essentially saying, for the purposes of networking, my validator is going to give that RPC node this much stake that assigns some percentage of bandwidth to that RPC node, and then from that validator peer, the transaction gets forwarded to the TPU forward port of the leader. So this is the extra hop that stakeway QoS intends to add. Then, just like before, the leader processes the transaction, if it gets confirmed, it gets broadcast to the rest of the nodes, the network. Any questions about the high level so far? I think most of you are probably at least somewhat on board with all that. So now we'll talk a little bit more about what's going on in the leader itself.
00:04:24.708 - 00:05:01.118, Speaker A: Right, so all these diagrams so far are sort of a step by step of a transaction going from creation all the way to confirmation. But really, the details, or a lot of the details, end up happening here when you are the leader and you're the one receiving transactions in order to add them to a block. So maybe a lot going on here. There's sort of distinct steps that I'll talk about. So the first one is the networking layer. I touched on that a little bit already. But this is quic and sig verify.
00:05:01.118 - 00:05:59.954, Speaker A: This is essentially ensuring that the people who make connections to the TPU and the TPU forward port for some percentage of those connections, are staked. Or if they're not staked, they are sort of up to the validator to decide if they want to let certain connections to be made on their TPU or TPU forward port for the non state connections. But the networking layer essentially determines who gets to connect here. And how that works is a bit of history here. This used to all be UDP. And a lot of the issues that plagued Solana, I don't know, a year and a half ago maybe, were that in the event of big NFT mints a lot of congestion on the network, people would just flood these TPU ports with transactions, trying to get it included into a block. There's really no penalty for doing that.
00:05:59.954 - 00:06:52.654, Speaker A: So the high ingress here would cause flooding of the TPU port and end up that no transactions would get through because so much data was trying to get into that TPU port. The idea behind QWik is that we can limit the amount of ingress coming into that TPU port because there is some work that the connection has to do in order to verify its IP address. The problem with UDP is there's no actual verification that the IP address that I say I'm on is actually the IP address that I'm on. Quic has a handshake that happens before that verifies, essentially that this IP belongs to the person actually sending the request. So that's the first step. We'll talk about the networking layer, talk about sig verify. The next step is the banking stage.
00:06:52.654 - 00:07:22.010, Speaker A: And this is the stage that actually determines what transactions get put into the block. And then after the banking stage is the broadcast stage. So that's turbine. That's the thing that actually sends out shreds or sends out pieces of the block to other validators on the network. Uh, after turbine comes consensus. So all the other validators receive these shreds. Once they see the full block, they decide, uh, whether or not that block was included.
00:07:22.010 - 00:08:04.584, Speaker A: Uh, they saw the block, essentially, and then after that, um, yeah, the block gets finalized or not. All right, any, any questions on that? The high level flow so far for the leader. Thoughts? Okay, go forward. Um, so a little bit more detail about transaction ingress. Uh, I'm not an expert on quic, but the, the high levels here are maybe interesting to, um, operators. So the ingress consists of quick connections. Um, there is a quick server for the TPU port and another quick server for the TPU forward port.
00:08:04.584 - 00:08:52.858, Speaker A: After the quick server, after the packet gets sent on the QWik server, Sig verifies run. Main thing to know here is that with stake weighted qos, there's a separation between connections that are reserved for stake nodes and connections that are reserved for non stake nodes. 80% of the connections are for staked connections. So if you do not have stake on the network, you're not going to be able to use those connections. The other 500 is for anyone, any IP address. Doesn't matter if you have stake or not. Yeah, so Zantetsu has a comment there about what's going on here.
00:08:52.858 - 00:09:34.554, Speaker A: I mentioned maybe even a slide before. I mentioned that in this ideal world, a transaction is created. It goes to an RPC node, goes to a validated peer, then goes to the leader, and potentially to the next leader. Some of the issues that people are seeing is that some people who are trying to get their transaction are just spamming all validators, even the ones that aren't the leader. It's not very productive because if your next leader slot is a thousand slots away, you're not going to be able to do anything with those transactions that you're getting, that people are trying to include in your TPU port. It's just wasted bandwidth. So a lot of.
00:09:34.554 - 00:09:59.694, Speaker A: Yeah, I missed the question there. Thanks for that. A lot of the issues that you. That validators are seeing, excuse me. That are seeing right now are related to ingress that maybe isn't even sensical. Right. It's people kind of trying to get around the algorithm or get around the protocol in a way to get their transaction in, but maybe not the most productive.
00:09:59.694 - 00:10:21.054, Speaker A: Uh, there's a question about how fast the quick handshake is. I don't know the answer to that. I know that it can be a cause for issues. Right. Um, the. The idea behind the quick handshake is that it's. It's supposed to take some time, it's supposed to take some work so that you can't just spam handshakes.
00:10:21.054 - 00:11:02.974, Speaker A: But the exact number, I don't know. I don't want to get into details on that without some solid answers. All right, moving on. So, yeah, that kind of dovetails into this. Some issues that people are seeing, the quick connections can be exhausted. So if people think that they're going to get their transaction in by spamming as many transactions as they can. One issue that's happening is people are just connecting on a lot of different RPC nodes or a lot of different ips, maybe not even RPC nodes, and sending connections to the TPU port.
00:11:02.974 - 00:11:55.074, Speaker A: Once you hit that max number of connections, some connections will get evicted. And that is maybe leading to some problems. Some changes in 117, 2029 or 31 and 33 are aimed to fix that or to improve that. At least. Another issue that people are seeing, right, is that many transactions that actually make it into the banking stage are not actually valuable transactions. People are just sending the same low quality transactions over and over and over. So there is some work that Zen Tetsu and others have been investigating to try to track when a transaction or when a connection is actually leading to valuable transactions that are paying priority fees and paying signature fees.
00:11:55.074 - 00:12:52.874, Speaker A: And one other problem that maybe some of you are aware of is that the stake weighted QoS had the issue that it. It was a very low threshold in order to use those stake connections. So before, you could probably stake a few lan ports to your validator and then be able to use that stake weighted connection. Now, that's not the case. I don't remember the exact number, but you need orders of magnitude more, somewhere in the range of like 7000, 8000 sol in order to get a connection. There's a question from Zentetsu, are transactions act validator to the transaction source so that the transaction source knows the transaction was received? No, I don't believe so. So when you send a transaction to the RPC node, the RPC then sends it to the validator.
00:12:52.874 - 00:13:58.290, Speaker A: The RPC node doesn't know that it was confirmed or processed or anything changed until it gets an update from that leader. So the act is happening in the sense that shreds are getting sent out once the transaction gets included in a block, but not, there's no connection between the leader in the RPC to tell you, hey, this thing is processing now. Yeah, yeah. So Zantetsu's comment is that could lead to some issues as well, because the RPC may not realize that a transaction is processed so it can send it again to hopefully get it processed. I think in a way that's somewhat bad design by the app because you're essentially just, well, maybe the protocol is incentivizing spamming a little bit there. That might be the better way of say it. Matthew, can you elaborate there? What's the min that you're talking about? Oh, sorry.
00:13:58.290 - 00:14:15.238, Speaker A: The minimum amount of stake. Yeah. So in order to use the quick connection, you need, I don't remember the exact number, but Matthew's saying 15,000, which could be right. Yep. Think about all that. All right. Banking stage.
00:14:15.238 - 00:14:35.806, Speaker A: So that's kind of all that I'm going to get into for the networking layer. Right. You, you have the quick, back to this slide. You have the quick layer here. Then you have Sig verify. I don't think I have any notes on SiG verify, but it's essentially just verifying that the signatures on the transaction are correct. Right.
00:14:35.806 - 00:15:09.536, Speaker A: If, if you send a transaction that's not signed correctly, then it's not a valid transaction. It shouldn't go any further. So Sig verify is just the step before anything actually gets included in a block to make sure that the thing that you're trying to include actually is a valid transaction. Get back to this. So there was a question here about stakeway to QoS and. Yeah, there's also dedupe. Yeah, I'll get back to that.
00:15:09.536 - 00:15:37.314, Speaker A: So there's a question here about the 15,000 sol number and stake weighted Qos. The question is essentially, do RPC nodes have 15,000 sol in order to be able to send transactions? No. For a couple of reasons. So like I mentioned here, there are 500 connections available that are not using stake weighted QoS. So these 500 connections for the leader are not being used. Right. You have access to them.
00:15:37.314 - 00:16:31.484, Speaker A: Any RPC node or any, any IP address basically can use those 500 connections. In order to get access to these 2000 connections, you need stake on the network. It's not advised to stake an RPC node. You shouldn't try to make a staked validator, also an RPC node to send transactions to the leader. Rather, what you should do is set this up. So have a staked proxy, essentially some staked node on the network that's willing to establish a relationship with the RPC node. And both the validator and the RPC node need to make config changes in order to say that I'm sending transactions to this stake node and this stake node is overriding the amount of stake that this RPC kind of virtually has on the network.
00:16:31.484 - 00:16:47.004, Speaker A: So that way when it gets a connection from that RPC node, it allows that RBC node to use one of these 2000 connections rather than the 500. Any questions with that or follow ups, Andre?
00:16:52.624 - 00:17:24.603, Speaker B: Yeah, actually, so if I, if I understand it correctly, if I spin off my validator and stake it with one soul, that it will not be leader, it will probably never become a leader. I will be able to use my validator and, you know, for my RPC to be able to more or less successfully lend transactions from RPC. Right. So my RPC sends to my validator and validator is propagating it to, I mean, setting these transactions to a leader.
00:17:25.143 - 00:17:25.883, Speaker A: Right?
00:17:26.743 - 00:17:35.243, Speaker B: Yeah, I have to, you know, struggle to come up with 15k souls. I mean, it's, it's, it's cheaper this way.
00:17:37.183 - 00:17:58.028, Speaker A: Yeah. So I think the answer to your question is yes. In order to take advantage of that, assuming you have more than about 15,000 sol, your, the connection between your RPC node and your validator, assuming you set up stakeway to QoS, has access to those 2000 connections. Yes. Right.
00:17:58.156 - 00:18:11.036, Speaker B: I set one soul in my validator, so it will never be a leader, but my validator, I expect, will be able to forward my transactions from my RPC to a leader validator through, so.
00:18:11.060 - 00:18:27.982, Speaker A: That with only one sole stake that will not able to take advantage of those 2000 connections. A recent change essentially made that so that if you have anything less than that minimum threshold of about 15,000, it means you have, in the eyes of stake weighted qos, you have zero sole staked.
00:18:28.118 - 00:18:39.692, Speaker B: So, which means my validator should grow stake first and to become a reasonable source for propagating my own transactions from my RPC. Okay, thank you.
00:18:39.838 - 00:19:04.392, Speaker A: Yep, no problem. Just going through the comments here, does anyone know why these limits are so low? That doesn't seem like a lot of connections. I don't know the answer to that. I assume that the number of connections has something to do with the amount of resources it takes, but again, I don't know how much that's been investigated.
00:19:04.528 - 00:20:07.116, Speaker C: Yeah, if I can just elaborate. I mean obviously we can't solve this problem here, but if anyone has any idea why it's so low, I would like to understand. I think that with quick, unless it's an implementation problem, where the quick layer that's being used is inefficient enough that somehow having more connections in some table somewhere is inefficient. It seems to me like subordinating as many as you could is advantageous because if the sources are just going to keep fighting each other to get in that 500, it's incredibly wasteful because all of the work that seems like it would be inefficient to have in a larger table is just going to get done even more so in this small table with constant churn. I would think if you had say 5000 or something and it covered everybody that ever wanted to connect, then you just have some amount of time where you're holding memory associated with this remote IP peer is validated and can send a transaction and that's perfect. Attention all that's in that table. And that seems like it would be better to dedicate the memory to that than dedicate so much network bandwidth to all this churn.
00:20:07.116 - 00:20:15.824, Speaker C: So I don't know that obviously we can't solve that here. But if anyone has any reasons that they can give of why, you know, it's better to have this lower limit. I'd love to hear them.
00:20:17.604 - 00:20:31.224, Speaker A: Yeah, I don't know. My gut instinct is that if you unbottleneck those connections there will be a bottleneck somewhere else that maybe worse. But that's just speculation. I don't really know why 2500 was picked.
00:20:34.564 - 00:20:45.664, Speaker C: Well, it's the 500. I'm especially concerned about the 2000. It's like, okay, that's basically how many stake nodes there are. So that table presumably doesn't get much churn. But the 500 seems really limiting.
00:20:46.084 - 00:20:47.116, Speaker A: Yeah, that's fair.
00:20:47.220 - 00:20:56.614, Speaker C: I think maybe it was designed at the time, as we know, like a year ago there weren't even 500 unstake nodes that ever wanted to connect. Pretty much. So I know, I just wonder if it's an outdated limit.
00:20:57.314 - 00:21:02.814, Speaker A: Yeah, that's a good point. I can ask and kind of get a better answer there, but I don't know.
00:21:05.434 - 00:21:23.234, Speaker C: Also, do you know what would happen if, I mean, we locally changed that? Like, why can't I just go in and make a modification mode and say, yeah, I'll take, you know, 5000, I don't care, and maybe I can, maybe I should try that and give some feedback in the discord. Because if it reduces, it reduces the churn and it doesn't seem to cause any other problems, then why not?
00:21:23.734 - 00:21:37.994, Speaker A: Yeah, I think you could do it to your. I mean, there's nothing sort of enshrined about the number of connections. Right. So you could do it to your validator. And when you're the leader, you would get more connections to your node. I'm. Yeah, I think that will work.
00:21:37.994 - 00:22:10.324, Speaker A: Going through the comments here a bit more. Yep, I think we covered that. Nicholas has a plus one on this incentivizing span behavior. Agree. I think that's what 118 is trying to address. Don't know all the changes that are going into that just yet. Let's see.
00:22:10.324 - 00:22:28.504, Speaker A: I think we've covered that. Okay. Yeah. So let's move on to the banking stage. Good questions so far. If there's anything you want to go back to or things we want to cover again, let me know. So, assuming that we pass through the quick connection part, we pass through SIG verify.
00:22:28.504 - 00:23:12.684, Speaker A: The next thing in this pipeline is the banking stage. And the banking stage is responsible for building the blocks that eventually get confirmed by the network. The banking stage schedules transactions for occlusion, and it takes the priority fee into account. So I'm sure a lot of you know that when congestion was really high a month ago or a few weeks ago, whenever it was, priority fees were one of the tools that you could use to help mitigate that a bit. We'll talk a little bit about how that's taken into account. The current scheduler and the upcoming scheduler. The banking stage creates entries in the block, and an entry can be a set of transaction, or ticks.
00:23:12.684 - 00:24:00.584, Speaker A: And if you're not familiar, a block has 64 ticks, and each tick has 10 million hashes. So there are 10 million hashes per tick times 64 ticks in a block. And a tick is a marker of time. It's essentially what Poh is doing the entire time while your block is being built. Your proof of history is hashing everything that you're doing. There are 10 million hashes every sort of time marker, and then 64 times that is the number of hashes that a block contains. So what is the banking stage doing? We'll list the steps here, then go over maybe a little bit more visual setup.
00:24:00.584 - 00:24:28.914, Speaker A: What the banking stage does is it acquires locks for the transaction that you want to add. So if you have a transaction that's transferring from account a to account b. Right. You're going to need to acquire a readlock for the transaction that you're, or the. Sorry, the account you're deducting from. The next step is to check do checks. So you're verifying that the block hash for that transaction is valid.
00:24:28.914 - 00:24:51.246, Speaker A: The transaction is not already processed. So there's a couple steps there to verify that the thing that you want to commit is actually committable. Are people familiar with what a block hash is? Yeah, offer. Go ahead. Saw a hand there.
00:24:51.390 - 00:24:52.854, Speaker D: Sorry, that was a mistake.
00:24:53.014 - 00:25:33.004, Speaker A: Oh, no problem. Yeah. So just to clarify, a block hash is something that you need in order for your transaction to be processed. It is the current poh. So whenever you're, let's say you're sending a transaction to an RBC node, you'll ask for a block hash in order to add to that transaction. A block hash lasts for 150 slots, I believe, and then after that block hash expired, the transaction is no longer valid. So if it doesn't get committed in that timeframe, you have to request a new block hash and send it again.
00:25:33.004 - 00:25:34.592, Speaker A: Yeah, Zentetsu, go ahead.
00:25:34.688 - 00:26:16.256, Speaker C: Yeah, so if I could just restate that a block hash is essentially the hash of every block. Every block has a hash. And when you submit a transaction, you simply have to identify the hash of some recent block, and that establishes the time range in which the transaction is allowed to be processed. So I could, theoretically, if I want a particular leader to be the only one to handle a block hash, I can pick. I mean, to handle a transaction, I could pick a block hash, like 150 back. And then when I submit the transaction, if the leader does that, I'm submitting it to can't handle it, then it's no longer valid by the time it gets to the next leader, because it's already beyond that 150 span, like, sort of like time limit that's set. So there are actually ways you can use this to do interesting things.
00:26:16.256 - 00:27:00.816, Speaker C: But I think, like you said, in most cases, people want their transaction just to get handled as quickly as possible by whoever. So they just pick the most recent block cache possible, which is the RPC will tell them about the most recent block cache that it saw for the most recent block, and that establishes the longest Runway for the transaction, 150 slots worth of time to try to get it to land. And then you don't have to reauthor your transaction for 150 slots. You have 150 slots where you can keep resubmitting it, and it continues to be valid, but once that's done, you can't submit it anymore. And that's a good design choice, because it means that transaction can't just float out there forever and a year from now, potentially get resubmitted and potentially tried by someone who stored it and wants to now sort of replay it later. So, anyway, sorry, just to add some detail.
00:27:00.960 - 00:27:34.192, Speaker A: Yeah, no, thank you. That's a good addition. Yeah, so I think that was a good summary. So, checks, verify that block hash, make sure that they're valid. Next step is loading. So, after I verified the block hash, I load the accounts for that transaction. So, like I mentioned, if I'm deducting from one account and transferring to another, writing to load those accounts to make that transaction, once the accounts are loaded, then I actually execute the transaction for a simple transfer.
00:27:34.192 - 00:28:01.578, Speaker A: That is just the system program. Right. But for other programs, that could be a much more complicated set of steps to execute. If you're doing an NFT mint or if you're, I don't know, doing something with an order book. Right. There's a lot more that can go on in that execution phase. But once the transaction is executed and successfully run, the next step is to record the transaction.
00:28:01.578 - 00:28:54.018, Speaker A: So there is, um, yeah, let me go back to this diagram here real quick. The banking stage is here, and there's this Poh recorder that's running the entire time on a separate thread. Um, the banking stage, once it sees that a transaction is executed and completed, uh, it records that transactions completion to the poh thread. Um, so that's another sort of key thing with Solana, right? The, each time a transaction is added to a block, it gets recorded right away, and then turbine sends out that transaction right away. It doesn't wait until the block is complete and fully formed before it starts sending. It starts sending shreds for that transaction immediately, as soon as it's processed. Let's see here.
00:28:54.018 - 00:29:34.084, Speaker A: Banking stage check. There's a question in the check stage. I assume also that whether or not cu limits for the block will be exceeded by the transaction is also done. So I believe there are some checks about cu limits, but you don't actually know what the cus are until they're executed, right? So if I have 100 cus, maybe, I'm not positive. There, there might be some checks where you could say like there's no way that transaction could be completed with 100 cus. So, um, don't execute, uh, but for the majority of transactions. Right.
00:29:34.084 - 00:30:25.494, Speaker A: People aren't actually updating their cus properly and giving a tight estimate. So when you execute the transaction, you're actually updating the cus that were actually used versus the C's that were predicted. Uh, yeah, go ahead. Yeah, so I was wondering about, um, you're talking about cus, right? And I know that you can get the, the compute limit size of a transaction when you simulate it. And I'm wondering when you simulate a transaction without submitting it, is that going to an RPC node and is that hitting this banking stage or is that something that happens somewhere else locally? Yeah, it's actually going to the RPC node and being executed, just not committed. Cool. Yeah, so the RPC node actually runs it just like it would in the validator when you're actually committing the transaction.
00:30:25.494 - 00:30:43.438, Speaker A: So you um, by simulating that transaction and getting the cus from it, uh, you get the exact amount of ceus that you're going to be using, uh, when you commit it later. So that's, if you're building a transaction, it's useful to, to simulate it first so you know the exact amount. Yeah, Zentetsu, go ahead.
00:30:43.566 - 00:31:33.574, Speaker C: So just, I think you can bypass that. That's like a default behavior that's meant to be helpful because it prevents you from wasting a transaction fee on something that would have failed anyway. Most people would rather have the RPC nodes simulate transaction, find out that it would have failed, and then not even bother submitting it to a leader, and thus never even have the chance of paying a failed transaction fee versus just sending it straight through and then having it fail for reasons that could have been detected. Of course, there are lots of people that don't want that extra delay and don't let that simulate. And in fact, they're issuing the types of transactions which can't really be simulated because they depend upon state which is constantly changing, such as prices and stuff like that. They're trying to do some arbitrage or something and they will bypass simulate and they'll just send it directly and let the execution happen. And that's why we get so many failures too.
00:31:33.574 - 00:31:52.034, Speaker C: If you look at your blocks, there's tons and tons and tons of failed arvs and other types of transactions. And those clearly aren't ever being simulated. Otherwise they never would have been submitted. But they don't want to simulate them because it would slow it down. There was another aspect to the question I was going to answer, but I lost it. So I'll leave it at that.
00:31:52.934 - 00:32:47.138, Speaker A: Yeah, I just want to add that one thing that you can do with simulation is in the development process. If you have some sort of transaction that you always know is going to do the same thing every time, like a transfer or like a. I don't know, whatever you're doing in your code, you can send it to the RPC and simulate it so that you have the proper amount of cus that you can then send later without the simulation. So that's another way you can use simulations before you actually execute. There's also a question in here about how many shreds are composed in a block, typically. Um, so I think it fluctuates, right, because depending on how many transactions get put into a block, that, that will change the amount of shreds in a block. Um, I think an average is around 700 shreds.
00:32:47.138 - 00:33:10.502, Speaker A: 600. I review the metrics pretty regularly with, um, Anza engineers, but I don't recall the exact amount, but in. In the range of hundreds. Like 600. 700. Yep. Okay, so we've executed, we've recorded the next is commit.
00:33:10.502 - 00:33:45.674, Speaker A: So once you've recorded the change, you'll update the accounts. Right. If an account needs to be deducted or added to or modified in some way, that's the commit phase. And then the last step is to unlock. So if you make a read lock on a account, obviously you need to unlock it so that the next time you access that account, someone else could lock it and use it. So that's kind of the high level of the banking stage. Yeah, I'll come back to this and get into Cus in a bit, but let's look at it visually first.
00:33:45.674 - 00:34:38.930, Speaker A: So the way the banking stage kind of works visually is that there are four threads running, currently four threads. And each one of those threads has a priority queue of transactions. And this priority queue is determined based on the priority fee. So if you have a high priority transaction, because the priority fee is two x what it is, every other transaction in that queue, that transaction will be higher up in the priority queue and will be more likely to get included into the block. Now there are a couple issues with this, right? The main issue, or some of the issues are back to this slide. And what we were talking about earlier is the cus. So priority fees are a function of cus.
00:34:38.930 - 00:35:20.184, Speaker A: You determine how high your priority fee is as a multiplier of the cus that you requested. And if you don't set that, the default is to have, I think, 200,000 cus. So for a lot of transactions that people are sending on the network, they're requesting 200,000 cus when they don't need that much. So that is one issue that led to, or maybe exacerbated the congestion we were seeing a while ago. Each transaction can have a maximum of 1.4 million cus, and each block can have a maximum of 48 million cusp. So is there, there is a hard cap imposed by the protocol right now of how many cus can be in a block.
00:35:20.184 - 00:35:55.792, Speaker A: It's sort of an unknown right now. If that number can be increased. There's speculation that it could be, but more testing is needed. Sort of the TLDR. Yeah, we talked about this. In order to get the proper amount of ceus, you can simulate the transaction and get a better idea when you're requesting them. So one problem here, and sort of related to what we just talked about, is the priority is based on the requested cus, not the actual cus, because the banking stage can't know what the actual ceus are until the transaction is actually executed.
00:35:55.792 - 00:36:33.390, Speaker A: You don't want to execute the transaction until you are fairly sure that you're going to include it in the block. So because each one of these threads are separated, and because cus are often mispriced, you may end up in a state where the banking thread thinks that it has too much work to do based on the priority fees that are being paid for certain transactions. It doesn't think that it can fit any more into a block, but then when it actually gets executed, it actually can fit more. Yeah, Sam, go ahead.
00:36:33.502 - 00:37:25.578, Speaker C: Oh, sorry, I didn't mean you're up to you. I was going to come after you, but I think this is a place where there can be active development for improvements, that this is kind of like a competitive advantage thing people can do. You can implement something that has heuristics that tracks for a given submitter or for a given program, or for some combination of submitter and program or other things, track some historical data about how many cus do the transactions typically take, and you can better estimate. And then if you can better estimate, you can actually make better choices about what transactions to prioritize. Because if you're fairly confident that a transaction claims a lot of cus and then pays a high fee because of it, but is very likely to use a small number of Cus. That's a great transaction to conclude, because in fact, its number of its payment per actual Cu used will be extremely high. And you know, that's so.
00:37:25.578 - 00:37:45.334, Speaker C: But that kind of smarts requires heuristics and complicated algorithms, and the validator code base, as in stock form, doesn't really do stuff like that. But these are the kind of things that I wouldn't be surprised if higher end, bigger validators with resources behind them are implementing behind the scenes. And that's a competitive advantage.
00:37:45.674 - 00:38:25.724, Speaker A: Yeah, I completely agree with what he was saying. I think that the banking stage has a lot of room for validators to experiment and try new things, and there's nothing about it is really enshrined in consensus. Right. So you can change the banking stage however you want, and you could prioritize certain types of transactions or have your own heuristic, and that really doesn't affect the rest of the protocol as long as you are producing valid entries. So definitely something to look at and also something that is going to be changing in 1.18, or the option to change it and 1.18 will be there.
00:38:25.724 - 00:39:07.854, Speaker A: Is CU price playing a role in transaction prioritization? Yes. Yes. So when you are creating your transaction, let's say you are specifying the number of cus that your transaction needs. You can also specify the number of micro lamp ports per cu that you want to add as a priority fee. So if you have 100 cus that you're requesting for a transaction, that's 100 times the number of micro land ports that you're specifying in the transaction. So let's say you specified 1000, it would be 100 times 1000. B 100,000.
00:39:07.854 - 00:39:58.494, Speaker A: But on the other hand, if you're specifying a 1.4 million cu transaction, it would be 1000 times 1.4 million cus. So the ultimate priority fee that is being used to determine the relative order here of the queue is the multiplication of Cus times the micro lamp ports per cu that you're estimating for the transaction. Yeah, so I'm not positive on this one, but Zen has a comment about other things add to Cus. I think you should be able to predict your cus ahead of time if you simulate the transaction. But I'm not positive there.
00:40:01.594 - 00:40:21.962, Speaker C: Oh yeah, you should be able to predict. You're right, the simulation does help you predict. It's not perfectly accurate because things can change and you can use slightly different amounts of Cus depending upon the current characteristics at the time you executed. But that's why most people, I think, will simulate and then pick some cu limit that's like 10% higher than what they saw when they simulated, just to cover any possible slop.
00:40:22.098 - 00:40:44.598, Speaker A: Yeah, yeah, yeah. I think that's what Devrel was suggesting when congestion was bad. Right. Is to just add five or 10% to the number of Cus and you anticipate, just in case. Let's see. One thing I do is calculate the cus and then have a fixed total priority fee I want to pay, and I divide it by the number of cus to get my micro lamp ports. Yeah.
00:40:44.598 - 00:41:08.574, Speaker A: So if you have some tolerance of how much you want to pay, you can always do that, right. It's just cus times micro lamp ports equals priority fee. So however you want to arrive at that number. Is there a default cu price? I think if you don't specify it, there is none. So you're just not paying a priority fee. Right. But you can, if you do specify it, then it's whatever you, you said.
00:41:08.574 - 00:42:20.998, Speaker A: All right, so yeah, we kind of talked briefly about the issues here, but there are some problems with this. First of all, people just not pricing Cus correctly. And then second of all, the, these four queues don't really know about each other. So if this banking thread ends up getting some transactions and thinking that it's got a lot of work to do, another way of saying is because each one of these queues don't have knowledge of the other, you may end up with a queue that is just not processing many transactions for a number of reasons. If a transaction comes in that is writing to account a, and another transaction comes in that is also writing to account a, those two transactions can't happen at the same time. So one gets priority over the other. In the case of some network event, like a mint, for example, an NFT mint that everyone is trying to get in on, a queue may be starved out with a lot of transactions like that.
00:42:20.998 - 00:43:09.822, Speaker A: Right? A lot of transactions trying to take advantage of the same resource at the same time means that a certain queue might end up with just one transaction, even though lots of transactions are coming in. So that's one issue happening here, is that there could be contention within the queue which is causing starvation of that queue. It's not producing as many transactions as it could be if it were more evenly distributed. Yeah, I'm gonna move on here and then come back to the questions. So the idea with the, it's called central scheduler in 118 is that these steps are broken up a bit more. So rather than having the knowledge sort of independently in each banking thread. See if it.
00:43:09.822 - 00:43:59.712, Speaker A: I don't have a video on it. There's a separate step, which is called the scheduler that's happening before each one of these banking threads. And the scheduler has some knowledge of which transactions are going where. Based on that knowledge, it can more, it can more evenly distribute the transactions to the different priority queues. So there's less of a likelihood of starvation for any one queue, because the way the scheduler is sending transactions to those queues is with conflicts in mind. So if you're not aware in 1.118, you have the option to add the flag block production method central scheduler, and that will use this new scheduler that's available in 118.
00:43:59.712 - 00:44:47.024, Speaker A: It is not on by default, so you have to opt into the scheduler and try it out. Yeah, I sort of mentioned this, but the scheduler has this idea of a priority graph, and that graph is created per slot, so it essentially has a complete ordering of the transaction it's seeing and the ability to separate them out based on conflicting writes or reads. And then once it has that graphic, and then assign transactions to the four threads that are similar to the ones in the old scheduler, they do the same steps that we mentioned here, lock, check, load, execute, record, etcetera.
00:44:49.244 - 00:45:12.608, Speaker C: Yeah. Just to be clear, there was a question about that. I think those stages aren't exactly accurately. They're in a very granular level. Some of the locking happens after some of the other steps. You don't. If you can go back to that real quick, verifying the block hash and stuff like that doesn't require locking accounts and stuff like that.
00:45:12.608 - 00:45:38.614, Speaker C: So I'm sure that it verifies the block hash before it locks accounts. There are numerous locks involved, and I think that it's not really a single stage. I think the locks are kind of spread throughout the other stages, but the locks associated with accounts, I would expect to happen after checks and before loading. Just to be clear, like, where did you get that ordering from? And why are you saying that read and write locks for accounts happens before the checks?
00:45:38.774 - 00:46:05.054, Speaker A: Yeah, good question. So that leads nicely to this. There's a video online in the Solana, YouTube, talking with Andrew from Anza about the scheduler. And a lot of this content is from that. So if you're curious and you want to go deeper on the topic, I highly recommend this. Let me, if I can open this up here. Oh, no, that didn't work.
00:46:05.054 - 00:46:48.924, Speaker A: One sec. Yeah, here we go. Let me put this in the chat so highly recommend that. And this is where I got the breakdown of the different steps. It could have been a high level abstraction that Andrew was using. That's maybe messier in the actual code, but that's where it came from. Everyone's still seeing the slides.
00:46:48.924 - 00:47:13.744, Speaker A: The screen. Yep. Okay, cool. Yeah, so I think that kind of sums up what I want to talk about with the scheduler. The key takeaway here is that cus are important. The new scheduler is something that we encourage you to at least experiment with. Try out on Testnet, see what you know, see what the changes are.
00:47:13.744 - 00:47:55.588, Speaker A: Yeah. And ideally, this should be more productive for validators, so I encourage people to try it out. Moving on to turbine. So let me go back to this diagram real quick. I mentioned this before, but the banking stage is confirming transactions as they come in or as they get processed by these different banking stage threads. Each time a transaction is recorded, it is communicated to this Poh thread. And then once the Poh thread records it, it is sent to the broadcast stage.
00:47:55.588 - 00:48:16.464, Speaker A: So this is turbine. So I mentioned earlier, but the transactions don't wait until the block is built. They are sent out immediately once they are processed in the banking stage. So shreds are going out all the time when you're the leader, not just on block intervals. Yeah, Zen Tetsu, go ahead.
00:48:16.624 - 00:49:07.264, Speaker C: Well, I don't know if you'll know the answer to this, but if anyone knows the answer, I'd like to hear it. One thing I never did dig into the code to understand is whether or not there, like when a block is created, is it purely, is it really just this sort of abstract concept of a bunch of shreds going out, and each thread is just the transaction. So the leader is just saying, this transaction is sort of just sending out the transactions that constitute the block? Or is it also wrapping that in something of some other structure? Like does it send out a preamble or a header or something like that, to say, this is the characteristics of the block I'm creating, or does it send a terminator to say, I'm done creating the block? Or does none of that happen? It really just, every leader is just sending out transactions, and the blockchain is really just a sequence of transaction shreds and nothing else. And does anyone know that answer to that question?
00:49:07.924 - 00:49:19.484, Speaker A: I don't know the answer fully. I know that the ticks are sort of the demarcation of the block. So if you have 64 ticks, that is a block. But, yeah, maybe offer has more. Yeah.
00:49:19.524 - 00:49:44.154, Speaker D: So shreds are not transactions. So transactions are grouped into entries, and then entries are somehow duplicated with the code and data shreds. So using codes, you can basically reproduce the data shreds. So it provides redundancy. And that is being sent by tuber.
00:49:45.614 - 00:49:59.874, Speaker C: But there's like no prefix and suffix for a block. A block is just a bunch of these interleaved data entries and transaction entries in shreds, kind of like streamed out. Is that true?
00:50:00.774 - 00:50:20.514, Speaker D: Yes. So there is a header, but then you get the shreds, which are, you are combined into entries, and then you can decode them and get the transactions for replay when you are not, when your validator is not the leader.
00:50:24.114 - 00:51:08.072, Speaker A: Thank you. There's a question that I'll answer real quick about geedo bundles and how they interact with the central scheduler or any scheduler. So going back to this real quick, or maybe this one's more clear, if you're the leader and you're a Jito validator, you're not actually sending the transaction to the validator processes TPU, you're typically sending it to the relayer. Right. So a Geeto diagram would look a little different. There'd be another process, either on your same machine or the shared relayer, where the RPC node would be sending the transaction to. The relayer's job is really just the networking in SIG verify.
00:51:08.072 - 00:52:09.658, Speaker A: So it's, you know, Gao may be doing other things in the background, but as far as what it's taking over, it's really just taking over this step. So the relayer is doing this, it's doing the networking part, doing Sig verify, and then when it's going to the banking stage, it's sending the data back to the validator process. So this is the cutoff line between relayer and the rest of the validator, which means that whatever Anza is doing or agave is doing in the banking stage is also what Jito is doing, minus maybe any modifications they had to make to make this connection work. But if you're using the central scheduler in GDO, it should be working the same as the central scheduler in agave. Yep. Okay. Yeah, there's a comment there saying, I don't think it wraps maybe after 64 ticks, just a block, then 64 before.
00:52:09.658 - 00:53:00.344, Speaker A: That was the previous block. Yeah, I think that Matthew, what he's saying is true, but I'm not positive. There's a question about elaborating on a mempool and how they did that. I'm not the one to, like, I don't have enough knowledge to really talk about it. Um, like you mentioned, they disabled the mempool for sort of concerns about what people were, uh, were doing with the mempool right there. For those who dont know, the mempool allows searchers the ability to see what transactions are coming in and essentially delays the transactions to be included in the banking stage. So it gives them some time to order the transactions how they like.
00:53:00.344 - 00:53:42.612, Speaker A: Um, that was essentially resulting in a sandwich attack where, um, you can take advantage of people who are sending, um, swaps. Right. If you're swapping from one token to another, um, you could essentially maliciously wrap that transaction and, you know, steal, or, I don't know if you want to call it steal or, or what, but take advantage of that economic situation, uh, using a sandwich attack, uh, which is not desirable. That's why jito shut down the mempool. Um, yeah, and maybe I'll just leave it there. I think that it's a lot of detail to get into. I don't want to sidetrack us too much.
00:53:42.612 - 00:54:26.964, Speaker A: We're already at an hour. Um, okay, so we've got turbine, and then a couple words on consensus, and then we'll wrap it up. I think we should be done the next ten minutes or less. Um, but we'll see how it goes. Okay, so, uh, turbine is maybe visually might be now, let's go over some of the details. So, turbine is the protocol that we mentioned that, uh, sends out shreds as they get added to a block. Um, shreds are sent out immediately, like we already said.
00:54:26.964 - 00:55:08.030, Speaker A: The thing to understand about turbine is it designed to send data very quickly and efficiently. The turbine tree is precomputed. In other words, before the epoch even starts, who's sending data to whom is known ahead of time. Higher stake nodes tend to take more of that burden than lower stake nodes. Maybe the visual will help you get a better idea of what's going on. So, if this is the turbine tree, this is essentially the topology of the network for sending out shreds once they get processed. This is what it looks like.
00:55:08.030 - 00:55:54.564, Speaker A: So if I'm the leader, I'm making a transaction, I'm getting it confirmed. I will send out the shred. That shred will get propagated to a neighbor of nodes, and that neighbor will have a set of neighbors that it propagates the shred to. So it's like an exponential fan out of nodes on the network until all the nodes get the shreds. And so nodes higher in this tree will tend to be highly staked nodes. The idea is that the more highly staked nodes are more trusted, they should be able to handle the load a bit better and send it to nodes lower down the tree. Okay.
00:55:54.564 - 00:56:36.508, Speaker A: Yeah. Going to the next slide. Yeah. And a little bit more technical details here. That turbine is a UDP protocol, but you don't want the case where a packet gets corrupted. The fidelity is really important here. So each shred that gets sent out is reed Solomon erasure encoded, which essentially means that not only are you sending the shred by itself, you're also using an error correcting code.
00:56:36.508 - 00:56:53.616, Speaker A: So if the, let's say a shred packet is missed, the error code packet can be used in order to verify that missing packet, or vice versa. Right. Yeah, zan, go ahead.
00:56:53.800 - 00:57:29.912, Speaker C: Just a brief comment on that picture. I think the picture is a little bit inaccurate because it has no overlap. And I'm pretty sure that it's not the case that there's only one path from the leader to get, like, for example, looking at four over there. The only way for four to get anything in this picture is to go through zero to one to four. There's no other path to get data. I think that's not entirely accurate, and I only point that out because I think it's important that we realize that there are multiple paths for any particular node to get shreds because they can go through several different other paths. That redundancy means that you're unlikely to miss, less likely to miss things, but also means you're going to get stuff.
00:57:29.912 - 00:58:00.534, Speaker C: You're going to get the same thing many times. I think that's part of why we have so much bandwidth usage in Solana is the redundancy of just everyone's sending the same stuff redundantly to everyone else, just to make sure that it's received is part of the reason that there's a high bandwidth and higher stake. Nodes have that to even a larger degree. We fan out to a lot of nodes, which is why there's so much egress and why your egress gets bigger and bigger as you. As you go up at stake.
00:58:01.034 - 00:58:24.338, Speaker A: Yep, definitely. Um, there's also ways for you to. To get treads if you miss them. Right. But, um, that's a much slower process, so, yeah, I believe what Sam's saying is correct. Um, yeah, in the interest of time, we'll leave it at that. Um, but important to know that turbines fast.
00:58:24.338 - 00:58:54.764, Speaker A: Uh, it happens after the block is processed. It starts right away. It's a UDP protocol, and it's encoded consensus. I'm going to do a real quick hand wavy summary here. There was a consensus deep dive that was recorded today that should be out relatively shortly that I'll forward people to as soon as it's out. Let's see. Yeah, I've got a slide on that, but the.
00:58:54.764 - 01:00:06.690, Speaker A: The idea here is that once a block is processed, you've gotten the shreds for that block. A validator, a consensus validator. Voting validator's job is to vote on the inclusion of that block. So, as a Solana validator, I'm not voting like whether or not I like the block or pieces of a block, right? I'm just saying, did I receive this block? Yes or no? And then Solana uses an algorithm called tower BFT to determine if a block should be included as confirmed or not. And the reason that a block may not be included, if you saw it, is that networks inherently have issues, inherently dropped packets have things missing, have delays. There are nodes all over the world, so some set of the network might get a block that some other set of the network doesn't see. Or maybe a validator is in a part of the world where there's not many nodes and it is slow, so it takes a while to produce that block faster.
01:00:06.690 - 01:01:00.248, Speaker A: Nodes in other parts of the world may think that the block didn't exist. It may have already voted and may have moved on. So these situations are what creates forks in the network. When some set of nodes see a block and some other set of nodes do not see that block, that's a fork that gets created and that needs to be resolved via this consensus algorithm. And, yeah, that's what I was just mentioning. So, in order to resolve these forks, essentially what tower BFT does is it keeps track of something called a lockout. So, as you are voting on your fork, every time you're voting for that new block, you're affirming your commitment to that fork, and you're affirming it with this thing called the lockout.
01:01:00.248 - 01:01:55.484, Speaker A: A lockout is a count of the number of slots that you have to wait in order to switch to a new fork. So let's say you're voting on fork a. It has block one, and then you get block two, and block one is still not confirmed. Your lockout is now doubled between one and two. And, yeah, I don't have a diagram for this, so this might get confusing fast. But the key takeaway here is that as you vote down a fork, your lockout continues to double, which means that it's harder and harder or it takes longer and longer for you to switch. And that's by design, right? You don't want to vote on a fork and then immediately switch to another fork on the next slot, which would cause some issues with tower BFT.
01:01:55.484 - 01:02:52.174, Speaker A: So I'll kind of hand wave and leave it at that. I don't know if people want to add more details there or more comments, but I think the consensus video that's coming out soon give a lot more details. Any thoughts, questions? Okay, so, yeah, so like I mentioned, video is coming out soon. I will send that out to validators once it is published, but should be really interesting. And Ashwin, who's from the Anza team, goes into a lot of detail, so worth checking that out. And that's all I got. Any questions or feedback on the content? There's still a lot of talk about sandwich attacks.
01:02:52.174 - 01:03:25.440, Speaker A: All right, thanks. Plug again. If you have any topics that you want to discuss in future workshops, or if you have a topic you're passionate about, you want to lead it, let me know. I would love to do one on firewalls. I think it's a topic that's coming up again and again lately, and someone would really deep dive on firewalls. That would be great. I'm probably not the person to do it, but would love someone to lead or coordinate that or even just get excited about it and let me know.
01:03:25.440 - 01:03:34.364, Speaker A: That would be cool. But yeah, otherwise, thank you all and I will see you soon. Thanks, Tim. Thank you.
01:03:37.424 - 01:03:37.864, Speaker B: Thank you.
