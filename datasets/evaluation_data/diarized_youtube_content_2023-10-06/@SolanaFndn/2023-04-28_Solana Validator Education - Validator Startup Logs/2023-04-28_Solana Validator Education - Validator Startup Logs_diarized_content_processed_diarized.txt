00:00:01.880 - 00:00:44.384, Speaker A: All right, welcome, everyone, to the Solana validator educational workshop. Thanks for joining. Today we're going to be talking about logs as a valid, logs related to starting up your validator. So what does it mean when you see a bunch of different logs happening as your validator starts up and tries to catch up to the network? So hand this off to Steve, our labs team. Steve works on the ledger tool at Solana Labs, among a lot of other things, and has a lot of great insights on what happens when your validator starts up. He'll be working off of this gist, which I'll put in the notes as well. So let me do that right now.
00:00:44.384 - 00:01:01.270, Speaker A: Here we go. Put that in the chat, and, yeah, I'll stop my share, hand it over to Steve, let him introduce himself, and then get started. Good, Steve.
00:01:01.342 - 00:01:17.714, Speaker B: Cool. Thanks, Tim. Let me get my machine sharing. Can you guys see that? And is that big enough?
00:01:20.074 - 00:01:21.186, Speaker A: Yeah, I think pretty good.
00:01:21.290 - 00:02:07.804, Speaker B: All right, cool. Well, yeah, Tim mentioned I'm an engineer with Solana Labs. I work on the core validator, Stevie Z. As you can see, my GitHub and discord match, so they see me. But, yeah, in any case, yeah, I think there's typically, like, a good amount of questions around startup. What's happening? Why does it take so long? Why is my node not catching up, etcetera? So we have some tooling plate, like Solana catch up, and Solana validator monitor command try to address. Some of these are, like, shed some light into what's going on, but they don't necessarily paint the whole picture part of it.
00:02:07.804 - 00:03:15.144, Speaker B: It'd be really invasive to the code base to try to pipe status down to every sub function. Unfortunately, the monitor command is kind of more broad strokes, as opposed to the logs, where we can log every little minute detail. So this document covers a bunch of log snippets, and let's kind of run through, like, what? Like, you know, when I'm looking at, like, a validator restart, you know, and I'm trying to, like, figure out what's going on with mine, you know, I'll, you know, grep for these log statements, and, you know, they'll kind of tell me, like, how far along it is, you know, if I'm in a time crunch or something's not looking right. So, yeah, just these, these notes at the top are pretty much just administrative things. It's like the default Solana log. Solana equals info for the rust log. I added the hardware config in case people are looking at timing, just, you know, as a comparison.
00:03:15.144 - 00:03:44.822, Speaker B: And then, yeah, I tried to leave some, some comments here to around the log. So, you know, in this case the slides will be posted. So you know, if you come back you can look at it, but. Oh yeah. And then I guess the ls thing, the, maybe this, this is mostly a default setup in terms of accounts. Accounts are not on a ram disk, they're just on regular disk. And I also put the accounts index on disk, which in 1.14
00:03:44.822 - 00:04:21.124, Speaker B: is not the default, but moving forward that will be the default. So again, I don't think we'll be too focused on timing here. This is more about just what things are happening, but just mentioning that because I imagine people might look at timing. First one, this one's pretty simple. Solana validator, I guess, other note, just please stop me if questions come up. I'd rather answer them in line as opposed to letting a mess up at the end when we change topics. But yeah, first one, pretty straightforward Solana validator spits out the version.
00:04:21.124 - 00:04:55.890, Speaker B: So this one's useful. This will real quickly, like if you grep this one in your logs, if you see a bunch of instances of this, this means you're, you know, it's boot looping. So something's up, whether it's, you know, panicking, booming, some bad, bad flags, whatever. This is just like a good sanity check. It's also nice to, you know, if you're like upgrading, you know, I think pretty recently we've seen like a handful of people getting the 114.7 versus 114.7. So like this is just like a good sanity check.
00:04:55.890 - 00:05:49.452, Speaker B: Like this will tell you what's running. Maybe your startup script is doing some manipulation to your path or something under the hood that forget, but this is just a good sanity check. And then this also is t zero for startup process. So this next bit, there's a handful of minor things that happen. It's like some cleanup from last run, like some basic network stuff, but kind of the next big one, which a useful log that I find is this one. If you go after loading bank from full snapshot, this gets emitted right before unpacking snapshots. And the thing I like about it is it also tells you what your latest full and whether you have a incremental.
00:05:49.452 - 00:06:41.028, Speaker B: If you're familiar with the rust option, this is a sum with the path. But if you don't have any incrementals, then that will be a none. So that's just sometimes a good sanity check, especially if you're downloading fresh snapshots from the network. If you don't get an incremental, there's a good chance your node is going to be pretty far behind right out of the gate just because most operators have increased their full snapshot interval or the default is like 25k slots. So if you don't get an incremental, there's a good chance you're just nodes gonna be struggling for a while to repair and catch up that, you know, up to 25k gap. So, yeah, shortly after that, yeah, this is 706. So shortly after we get into this, the unpack of the snapshots.
00:06:41.028 - 00:07:12.982, Speaker B: Yeah. So we've recognized the latest snapshots, now we got to start unpacking them. And this just spits out how many unpacking entries? In this case, entries more or less line up with a slot. And as it is right now, you can see there's about a ten second delta. Well, there's four lines that are pretty evenly spaced. That's because there's four threads doing the unpack here. So they're each, they're each doing like, in this case they've each done like 10,000.
00:07:12.982 - 00:07:39.504, Speaker B: So this is like 40, 41,000 entries total unpacked so far. And then the reporting loop is every 10 seconds. So now, you know, this was at seven o 648. Now it's 10 seconds later at 58. Making progress. So you'll see these come up. But this is one of the first big things that takes a while, actually.
00:07:39.504 - 00:08:14.156, Speaker B: I think if you guys are familiar with the monitor command, there's the loading ledger stuff that I've heard. The feedback that there's a little bit of a black box. What's happening? This is unpacking the snapshots, the start of this, but unpacking the snapshot and getting the bank parsed from the snapshots of the beginning of the. The black box, so to speak. But, you know, in this case we're unpacking a 40 gigabyte. Like archives. You know, it's just a lot of time, but zooming ahead a little bit.
00:08:14.156 - 00:08:35.294, Speaker B: So, you know, these will keep getting emitted every 10 seconds. And then, you know, here, here we finally hit the end. So each threads unpacked about 100,000 entries. So, you know, there's four of them. So that's like 420,000. These for fulls. These are typically about on the order of an epic.
00:08:35.294 - 00:09:01.538, Speaker B: And that's just based on how often slots are updated with rent. Each account has an update once an epic if it's checked for rent. We see this took 96 seconds on my machine, minute and a half, pretty, pretty decent chunk of time. Yeah.
00:09:01.586 - 00:09:09.674, Speaker A: Talk a little bit about. Sorry, Steve, what that unpack is actually doing, is it just putting it on disk and untarring it?
00:09:09.834 - 00:09:49.926, Speaker B: Yeah, exactly. So the snapshots, you know, as I'm sure everybody, or, you know, everybody here is familiar. Snapshots are kind of like your save point. So you know, if your, your nodes running, you know, you capture a snapshot at slotox, even if your node progresses to x plus 50, if you stop and restart, you go back to, sorry, this is a, I just noticed this to do group, don't worry about that. But your unpack go, you know, you start back from slot X and it's a tarball. It's a compressed Tarbell by default, you know, like the dot tar CST. This unpacking is.
00:09:49.926 - 00:09:52.874, Speaker B: Yeah, just extracting the contents of.
00:09:55.374 - 00:09:55.758, Speaker A: That.
00:09:55.806 - 00:09:59.394, Speaker B: File onto disk in an uncompressed format.
00:10:03.894 - 00:10:04.918, Speaker A: Cool, thanks.
00:10:05.086 - 00:10:39.000, Speaker B: Yeah, Chris there. Yeah. Yes, the unpack is done when the message took or is emitted. Yeah, yep, no problem. So yeah, moving along, there's actually in this case, as I showed earlier, this line. So I have, my node does have a full and an incremental, so similar process is repeated for the incremental. Incrementals are much smaller.
00:10:39.000 - 00:11:52.294, Speaker B: If you just run ls on your machine, this will the, you know, immediately obvious to, you know, the fulls are like 40 gig and the incremental is like, you know, one gig or something. But this one, you see, it completes much faster. What the hell? There is, I'm looking, there's like, looks like I have some notes that didn't update in here, but okay, anyway that to do. I was pretty sure I addressed it, but anyways, I'll just spitball through here. So yeah, at this point we have an incremental and we have a full snapshot unpacks like we basically have what we need. So we don't have a bank yet, but we have all of the underlying files. So the next step that happens is we run through essentially all these files that are now in disk, which is like basically accounts DB, and we regenerate the state to rebuild our bank status and essentially reestablish this state at whatever the snapshot slot was.
00:11:52.294 - 00:12:45.344, Speaker B: There's actually not that many exciting logs in this area. It's just a background scan of everything we have just unpacked on disk. I think right now there's like 130gb if like if you look at the accounts directory, something on that order. So in the same vein, I don't think we're actually, we don't read all 130gb of that data. But you know, it's more like established like reading like metadata from files and there's like a piece of accounts DB, like a subset of is called accounts index. And that's kind of like a, like a map into like the underlying accounts DB stores. And so like we regenerate that and you know, just, you know, for whatever the account state size is right at the moment.
00:12:45.344 - 00:13:39.436, Speaker B: Oh, thanks, Tim. Let me just, okay, I guess let me reshare that tab if I don't see the to do. Okay, cool. Let me reshare. Sorry about this. Where is this? What the hell? Google Chrome is a lot easier to find. Yeah, they're actually, I don't want to, Chrome is getting mad at me about permissions.
00:13:39.436 - 00:13:41.404, Speaker B: Would you mind just sharing the link here, Tim?
00:13:41.484 - 00:13:42.396, Speaker A: Sure, yeah, I can do that.
00:13:42.420 - 00:13:47.418, Speaker B: We'll just, instead of having everybody watch me fumble on the call here.
00:13:47.556 - 00:14:05.262, Speaker A: Yeah, sure. Just let me know when to go up. Scroll down. 1 second. Yeah, I think you were around here ish. Yep.
00:14:05.318 - 00:14:44.944, Speaker B: So we had unpacked the incremental there. So the, that line begin the creation of initial bank accounts DB. So that's kind of what I was talking about there. So it's like that snapshot, snapshot version as emitted. There's two there, one for the full, one for the incremental and then that line like loading bank from full snapshot which is kind of similar to the log line above, but it's just getting emitted in a slightly different spot. There's a longer message that you scroll to the right, but it's not, yeah, nothing really too important there. Just the same thing, the snapshot names.
00:14:44.944 - 00:15:24.416, Speaker B: So in between there and the like the bank rebuild finished. So there's the constb index generation. That's the part I was talking about there. I actually called out, showed what some of those log lines look like where we're showing progress through uh, index generation. Um, but otherwise there's just like a lot of background stuff going. But like, yeah, we're, like I said, we're basically just scanning, um, you know, this 130gb of unpacked um, state. We do like a hash on it to make sure that the state we just re established is valid.
00:15:24.416 - 00:16:14.374, Speaker B: You know, obviously if you get like there, there's a hash that's stored within the, within the archive. So like we have that hash and then we recompute this hash for like the account state based on the scan and we compare those. If those don't match, then we obviously either have a bug in the software or a corrupted archive, and then we would not expect things to go well from there. So there's just like a lot of sanity checks that happen out of the gate. And so it takes a while, but it's just making sure that we're restarting in a good state because otherwise things will just go haywire afterwards. But in any case, that's, you know, that line there, bank rebuild finished. You know, this is like that next you see, like it took, I guess, just over three minutes.
00:16:14.374 - 00:17:05.564, Speaker B: And so that's that kind of marks like the end of, like when we've reestablished a bank and it's seemingly in a sane state. And I include that slot number there. You know that that's the same snapshot slot, the incremental snapshot slot from, for moving down. Yeah, the next one, bank frozen. So you'll see these everywhere in just steady stays too. This is when a bank, I guess, to back up when you have a slot that you're considering, if you get the block for some slot from the leader. We create a bank internally, and the bank is the term we use in the code base to represent the state and the changes that are introduced in that slot.
00:17:05.564 - 00:18:02.886, Speaker B: And so a bank is frozen when we've completed the slot or when we've completed replaying the block. A slight elaboration here. When you download a snapshot at slot x, you don't actually replay the block for slot x, like the snapshot itself is the state after the block at the snapshot slot has been played. So maybe I was a little confusing there, but hopefully you guys understand the distinction. Like the snapshot is at the end of that slot and all transactions have been replayed. So at this point we have a bank, we have a full account, state generated and like in memory and disk at our snapshot slot. So the next step is we start replaying ledger that we have locally.
00:18:02.886 - 00:18:47.784, Speaker B: So in that example I gave earlier, maybe your slot, maybe your node made it to like 50 slots past your last snapshot slot. And this is going to replay those. And that replay should fly by pretty quickly, but these logs kind of get emitted at the start and they point out that first one there, like processing ledger from the slot that's the same as the snapshot slot. And then the one after tells you the halt point for like how far it will go. So ledger holds data through slot 18 something or another. It's off the screen right now, but it's probably like 50. Yeah, 32.
00:18:47.784 - 00:19:45.212, Speaker B: Oh, okay, so in this case it's like 1000 slots, but yeah, going there, I called out that log statement bank, fork, setroot. So this is a log that will get emitted both at steady state and startup. So this is just the node identifying that like the cluster has confirmed a block. And so this is done in this case. This is identified by replaying blocks and seeing votes and knowing the stake weights of people voting. And you're able to recognize that. Tim, your question there, could you be in a case where you have a gap between full snapshot end and incremental start? No.
00:19:45.212 - 00:20:19.866, Speaker B: So like incremental snapshots are essentially like, you can think of them as like a, like a delta based on top of a full snapshot. So like you, if you need, you always need a full snapshot and then like, you know, the incremental is almost like a, you know, like a diff, you know, for how many of our slots later. So those are like just by construction, those, those are like compatible. So yeah, yeah.
00:20:19.890 - 00:20:33.730, Speaker A: I'm wondering, I guess if you download from an external validator if there's a chance of getting that gap, but it sounds like you would get both the incremental and the full from the same one that has kind of the pair.
00:20:33.882 - 00:21:00.134, Speaker B: Yeah, yeah, exactly. I think you either download both from them or like if you have a set of nodes, like I think the known validators that labs has in the like everybody has in their startup script, I think those all run with like the same intervals. So I think those are actually producing snapshots at the same slots. So you could get like a full from BV one, but then actually get the incremental from like BV three.
00:21:01.154 - 00:21:02.094, Speaker A: Makes sense.
00:21:02.714 - 00:21:47.680, Speaker B: Cool. So kind of the next log there. So yeah, that bank fork one is just a little more of an aside than anything, but after that, so that's, I'm called out the local ledger, ledger replay progress. So this is where we're actually going through the blocks and there's, you see those loglines processing ledger. So that tells you the latest slot, it tells you just some basic things, but I think maybe the interesting bit there is like the slots per second. So that just shows you that it's actually churning through these slots at a decent clip. This continues to get faster in better versions and higher versions too.
00:21:47.680 - 00:22:15.434, Speaker B: Masters quicker than 1414, is quicker than 13. So this has been an area that we've had gains in but have yet to be realized. There's also a note there. You see the, I left the m taped in there. This is where v one three restart is slow with an existing ledger. I guess we can actually, if you scroll down a little bit, Tim, past the end of the log blogs there. Yeah, that part that's kind of like, you know, waited out.
00:22:15.434 - 00:23:03.258, Speaker B: So like I think probably some operators have observed and commented that sometimes it's faster to reboot when you just completely wipe the local ledger. Or like even just like, or like wipe the rocks DB and the reason that is, is like there's, I want to call it a bug, but just like the design between 1.141.13 and 1.14 or later defers and it basically has to do with like resource reclaim. So like in 1.13 as you replaying these slots, you're accumulating these banks. You know, calling the bank is like the state for like a given slot.
00:23:03.258 - 00:24:03.374, Speaker B: And these banks, you know, they're pretty large footprint in terms of resources like IE memory. And so essentially we need to reclaim these as we go along. If you let them just pile up, we're going to memory usage will climb and maybe there wouldn't be enough slots to make you out of memory, but it's better to bound it and be sure that you won't out of memory. In the scenario, the reclaim process happens in a manner that like blocks or like the accounts background cleanup is actually what it's doing. But this cleanup process, it happens in a manner that like blocks. Replay progress. Progress in v one three, whereas in v 114 the cleanup is done asynchronously in the background so that cleanup is able to be handled while replay progress continues.
00:24:03.374 - 00:24:39.680, Speaker B: So that's just like a, you know, just a fundamental design thing where 1.14 is just superior. And I guess that, I guess one other note there, that blocking thing that happened that that's like only in the startup path. Like the, this local ledger replay at startup that doesn't happen in the like steady state because obviously that would be very, very bad. So it's just like a slight improvement that came along with 1.14 and later. Anyways, scrolling back up a little bit more.
00:24:39.680 - 00:25:38.734, Speaker B: So yeah, there's those processing ledger maybe a little bit downstream. Those processing ledgers, those get emitted it looks like every 2 seconds at the moment. And then eventually you'll see this last one, that ledger processed in dot dot dot. And that marks the end of when you've finished replaying all the slots that you recognize were already on disk. And so that it's a pretty lengthy spit out, but like it'll tell you your latest route. How many active banks you have like, you know, maybe banks that are on the main fork but have not been rooted yet, or maybe banks that are on side forks but have yet to be determined that they're not the main fork yet. Yeah, spits the time spent and yeah, after that there's just like a little more initialization.
00:25:38.734 - 00:26:48.116, Speaker B: You'll see like, but at that point basically the validator is up and ready to be like part of the network and you responding and catching up. So you see like the timestamps there are like fractional, it looks like two hundredths of a second apart from finishing the local ledger. Replay to validator initialized. Yeah, that validator initialized is just like a log right at the end of the validator struct right before new returns. Yeah, I guess I'll pause there for a sec. Does anybody have any questions about logs, these logs? Because if that, we'll move on shortly to like looking at some metrics that kind of explain, like catch up progress. Yeah, describe what means, I'm sorry, once more, describe what replay a block means.
00:26:48.116 - 00:27:25.254, Speaker B: Yeah, so yeah, replaying a block, you have a, in this case we have all the block. The blocks that we're going to replay are already on disk. So like I was saying before this example, like, you know, maybe our node was running, we created a snapshot, we progressed 50 slots pass, but then we restart. So those, those 50 slots are available on disk. So we, you know, we're not network bound. Like we are in steady state like waiting for things. But essentially we recall the block from the block store.
00:27:25.254 - 00:28:26.584, Speaker B: So there's, there's an entry point in the block store API that says go get me this slot and get me all the entire block. In the block store. Things are stored as shreds, which are like the network size pieces of a block. Those are what are sent across turbine, but essentially they're just like an implementation detail of how we propagate pieces of the blocks and then reassemble and store them on disk. So replaying a block is, you know, if we have the full block, we're just going to get all of them and we'll, you know, shreds are serialized chunks of a block. So we'll basically deserialize those back into like the underlying transactions and then we'll take that whole set of transactions for that given slot and we'll just run them back through the, like the runtime. So just basically re execute them.
00:28:26.584 - 00:29:08.890, Speaker B: Look at Mark, whatever state changes occur, which, like updating accounts or moving lamports or whatever, and we'll acknowledge all those changes kind of in, like, a temporary cache and, like, our. In our bank, that bank object I described earlier and kind of, like, store those up and, like, acknowledge that state changed. Was that clear, Chris? Or any, any other. And I might have been using. Hopefully, I wasn't using, like, words to, you know, like, I. Hopefully I wasn't using block to define block. No, I think it's clear.
00:29:08.890 - 00:29:13.134, Speaker B: I'll go back and think about it a little bit more, but that was helpful. Thanks. Okay, cool.
00:29:14.034 - 00:29:27.934, Speaker A: Maybe something that would be helpful, Steve, from our chat earlier, is, you know, I had a general idea of what restarting looks like, but clarifying a little bit more about, like, when you restart from a snapshot, you blow away accounts.
00:29:28.054 - 00:29:28.582, Speaker B: Oh, yeah.
00:29:28.678 - 00:29:31.902, Speaker A: Just start from scratch there and, like, why that happens.
00:29:32.038 - 00:30:04.128, Speaker B: Yeah, that's a good point. Yeah. So the, like, snapshots are as it is today on Solana. Like, you can think of them as, like, a safe point, you know, in a video game. So, like, you know, if you create a snapshot at some slot, that snapshot contains, like, the worldview. Yeah, the worldview of, like, Solana account states at that slot. And so, like, you know, you.
00:30:04.128 - 00:31:22.864, Speaker B: I guess when you restart just based on, like, it's kind of an implementation detail, but essentially, we don't try to start from our previous accounts directory, and it has to do, I mean, I guess, the underlying detail there is that we use a lot of memory maps, and there's just things that are in memory, things that are on disk. And if your node dies from a panic or an oom or even just a regular restart, getting the account state consistent, like knowing what was in memory and made it to disk, or, like, you know, it was in disk. Like, it's kind of like a. Just potential for race conditions. And so, I guess, you know, up to this point, it was just designed, instead of, like, instead of having to, you know, trying to be able to restart from the unpacked account state we had previously, we just blow it all away and start by unpacking the snapshot, unpacking the contents of that snapshot archive over again. As the number of accounts has increased and the size of snapshots has gone up, this is maybe something that we. It's probably not as efficient.
00:31:22.864 - 00:32:33.380, Speaker B: And I think there we have. I think both somebody external, I have heard of somebody external to labs looking into the idea of, well, both somebody external labs and I think somebody within labs have been looking at, can we adjust the code to know when the validator shuts down? Know exactly what was persisted on disk. And instead of blowing away the accounts directory, like unpacking that snapshot which you saw took like, you know, it's just like a minute and a half just to do like the Untar basically like just trying to jump straight into like the step of doing that scan over the existing account state and restarting there or you know, being able to rebuild a consistent like set of data structures from that. But yeah, that, that, that is very much like prototyping ideation. So as of now. And like, yeah, 1113 1.14, like 1.16,
00:32:33.380 - 00:32:41.344, Speaker B: it's still blow away the account state and drop in a fresh copy from the snapshot to restart.
00:32:46.164 - 00:32:47.276, Speaker A: Cool, thank you.
00:32:47.420 - 00:33:11.444, Speaker B: Yep. If there's no other questions on this at the moment, we can scroll down a little bit more. All right. So yeah, I tried to type this out so that people are looking at this after the fact. But yeah, I'm just summarizing this quickly. So yeah, after that last log line earlier. Validator new, the validator's up.
00:33:11.444 - 00:34:08.304, Speaker B: So this will be the point when if you're running the Solana validator monitor sub command, like it'll show like, you know, your current state, it will periodically ping an RPC node and it will tell you, you know, your latest slot versus their latest slot. So tell you, you know, you're this many slots behind. You know, it's kind of like that little like very, very small, like mission control like view. And so like, you know, you could just like sit there and watch monitor, you know, hopefully the numbers like, or the gap is shrinking and you eventually catch up. You know, maybe you're 2000 behind. You know, hopefully it dwindles down. But like I guess that's, you know, it gives an instantaneous view but like it's not easy to see like the entire timeline on the command line there.
00:34:08.304 - 00:34:51.844, Speaker B: And so metrics help kind of paint like a broader, a broader picture into what's going on here. So this, the sex paragraph there. Yeah, kind of just explains, but the graph there, it's a replay metric. It's just like the every, every time we replay a slot or you know, block, you know, within that slot we emit a metric that's like, you know, has a lot of timings that are useful for debugging. But in this case we're really just interested in the actual slot number that's getting emitted because as we start playing higher slots that means we're progressing. So we don't really care about all the sub timings there. Looking at this graph.
00:34:51.844 - 00:35:18.774, Speaker B: This cyan trace is from. It's like that really light blue. That was from another node that was running. That was from another node that was running against the cluster, and it was keeping up. So it's included in this graph, basically, to just serve as a reference point. The purple trace is the node that was running that. I had all the log snippets from above.
00:35:18.774 - 00:35:55.174, Speaker B: And looking at that, just a couple things to note there. It's marker one there at 713. It's discontinuous there because the node was not running previously. And replay stage starts up once the validator is up and running. So that explains the discontinuity. And then in terms of the difference on the y axis, that's basically how far we are behind the cluster. The cyan trace is with the tip of the cluster.
00:35:55.174 - 00:36:16.088, Speaker B: Unfortunately, our metrics. It's kind of goofy. We've complained to influx about this, about, like, not being able to see precise numbers. But I didn't do the trace. But, yeah, essentially, I mean, just visually, you can see that. That there's a gap. And so, something interesting to note here.
00:36:16.088 - 00:37:09.584, Speaker B: So, yeah, I dropped all these vertical markers in, and so there's like, the vertical marker one and the vertical marker two, and then I drew, like, that horizontal line across. And so what that horizontal line is trying to indicate is that. So, like, when the node is offline, it's missing out on turbine. You know, you're not. You're not getting, like, all the shreds in the, you know, or the slot. You know, the blocks, you're not getting them in the most efficient manner. And so, basically, what happens is, when the node restarts and you're behind, if you don't have those blocks, you have to use what's called repair, the repair protocol and repair where turbine is max efficiency, just blast things out over UDP.
00:37:09.584 - 00:37:39.660, Speaker B: We have some redundancy with erasure coding shreds. And then, you know, the hope is that everybody's able to collect the full slot. Um, if. If it's actually the case where you could use repair at steady state, if, like, um, if turbine doesn't work, like, you know, if there's like a hole in the. The retransmission network or whatever, um, I guess I don't want. That's like, talking about, like, turbine is like a whole nother topic in itself. So I won't go too deep.
00:37:39.660 - 00:38:16.946, Speaker B: But essentially, here, you can imagine it. Like, if your node is up and running at steady state, it's getting the blocks over through turbine. And it's like pretty efficient. But when your nodes offline, obviously if you missed 50 slots, that turbine is a one time thing. So you have to use this protocol called repair. And what repair does is you essentially just query nodes. You say, hey, how many slots or which slots should I ask for? And then you have to, you know, it's more of like a, you know, like a traditional thing.
00:38:16.946 - 00:39:04.682, Speaker B: Like, you know, you asked for, you asked for some shreds and then the node sends them to you, you acknowledge them. And so it's, I guess it's more controlled in that manner, but it's also less efficient. And so what's happening here? So from like that vertical marker one to like the vertical marker two in terms of like the progress that like our node is making, like, the purple trace, those are all slots that had to be repaired. And so, you know, repair isn't necessarily the most. It's, yeah, it's not as efficient as turbines. So like, I think we've observed in this instance like replay stage. So, you know, trying to like actually process transactions and, you know, update account state.
00:39:04.682 - 00:40:25.572, Speaker B: It's stalled out by getting these shreds from over the network again. So the interesting thing there is once we get up to the vertical marker two, we hit this point where we've repaired all the slots that came across the network while we were offline, aka the slots we didn't get. And now we're at a point where we're replaying slots that we were online or that we were online for, but they were very distant compared to our latest state of processing. So just to maybe try to help illustrate this, let's say our node starts 1000 slots behind the tip of the cluster. We have to ask for those thousand slots from other nodes and gradually replay them. But in the time it takes to ask for and replay those thousand slots, the cluster has also progressed. And while we haven't actually, we're not replaying those slots yet, those ones in the distance that the cluster is doing while we're repairing, but we at least have them on disk.
00:40:25.572 - 00:41:23.364, Speaker B: And so since we have them on disk, once we get to that marker two, we essentially have a big backlog of blocks that we can start replaying and not be essentially replay can just spin through without hitting network or replays, not starve, waiting for stuff to come over the network. It just has, like I said, large backlog of slots. So there's like a bit of an inflection point in terms of velocity there. Looks like some questions came through. Many cases I've seen the validator fall further behind the cluster while waiting for repair shreds. I guess a question there would be like, what? Like, how. How far behind was the node? Like, how far behind did it start in this.
00:41:23.364 - 00:41:29.184, Speaker B: In that instance? I don't know if you can. I think anybody can unmute, right, Tim?
00:41:32.604 - 00:41:34.228, Speaker A: Yeah, everyone should. Yeah.
00:41:34.316 - 00:41:34.948, Speaker B: Yeah.
00:41:35.116 - 00:41:59.644, Speaker C: And I've just. I've seen a lot of variation over the year of running node, so it's, you know, the behaviors change. But I've seen many times when you finish replaying and you're initialized, you'll be, I don't know, for example, 700 slots behind. And I think while you're waiting for those repair shreds, you might fall further behind, like a thousand, and then you start catching up.
00:42:00.064 - 00:42:17.304, Speaker B: Yeah. Um. Yeah, and I think, yeah, that's like, the case of, like, um. Like, your terminology was correct there. It's like you're. You're trying to repair the slots that you don't have. And that's not a quick thing, but the kind of.
00:42:17.304 - 00:43:03.822, Speaker B: Not technically sound terminology, but I'd say that period from one to two on this chart, it's, your node's just trying to hang on. It's making some gradual progress, but it's just catch up and get to the point where you're starting to replay slots that you got over turbine again. So you'll actually see, too. That period from one to two is a resource intensive area. Memory usage will be up, cpu should be up. That's a stressful point for the validator running. But once it gets to two, it's able to go full blast and it will start replaying slots much quicker.
00:43:03.822 - 00:43:50.574, Speaker B: So, like, resources are going to get, like, reclaimed much quicker and eventually to the point where you're reaching steady state. I mean, a good way of thinking about it, too is like, when you're, I mean, actually from like, marker one to marker three, all the way on the right, you're replaying slots faster than the, than the cluster, right. Because you're, you know, you were behind, but you're catching up. So, like, kind of is intuitive that, you know, you're using more resources and stressing your node more because you're doing more in less time. But that period one to two is especially kind of, like, stressful in terms of the node using more resources. And I think in previous versions that maybe weren't as robust or as efficient. You could see an out of memory happen there.
00:43:50.574 - 00:44:20.396, Speaker B: I don't think we oom as much anymore. I haven't seen any reports recently, but just in general? Yeah. That is like a, you know, a spot. It's a window. You want to be as short as possible, and you want your node to get out of as fast as possible, which is why, like, minimizing downtime is super important. And, you know, that's something that we continue to work at. You know, obviously, as an operator, you know, you can do your best to, you know, I'm sure, you know, probably all the operators in here.
00:44:20.396 - 00:44:39.974, Speaker B: You know, you stop your node and restart it immediately, you know, within seconds. And so that's good. But then more so on the engineering side, it's just minimizing the amount of time that a node is down or the amount of time that a node takes to re initialize from that timeline of all the logs that I ran through earlier.
00:44:42.154 - 00:44:58.084, Speaker A: So, is it fair to say, Steve, that between one and two, you're doing double work? You're getting shreds from turbine that are in the future? Let's say that you haven't replayed yet, and then you're also doing repair. So you're like, is that why it's so resource intensive?
00:44:58.164 - 00:45:25.194, Speaker B: There's like. There's, like, double work in the sense of like. Like you're writing the same amount of, like, stuff to disk. You're not. You're not replaying double work yet, so you're only replaying what is available and, like, on the contiguous, like, blockchain. So, you know, you're basically. You're replaying slots as you are, as they are made available to you via repair.
00:45:25.194 - 00:45:51.772, Speaker B: And so I'd say it's more resource intensive in this fact that. Like. Just like. Yeah, it's like you're. But, yeah, in that case, like I said, yeah, you point out correctly, like, you're writing shreds that are coming through repair, and you're writing shreds that are coming through turbine. It's like you're processing, like, maybe not double, but maybe like, one and a half times. The load of blocks are all being.
00:45:51.772 - 00:46:34.764, Speaker B: Those are all being Sig verified. Those are all going to disk. So there's an increase in that sense, but in terms of actual execution of transactions, if anything, that's a little lower. But once from, like, two to three, it kind of flips. So, like, from between two and three, you know, we're not repairing anymore. It's like this point where we're still getting turbine from beyond marker three, but, you know, we're no longer repairing. Or if we're repairing very minimally, you know, we're repairing at like the steady state level as opposed to like this restart level.
00:46:34.764 - 00:46:39.324, Speaker B: And so that's, you know, less load in that, in that sense.
00:46:42.984 - 00:46:57.684, Speaker C: Is there any kind of indication, like, because I've been doing get health calls to kind of monitor that process, but I don't think it's apparent there. But is there any indication in the logs when we hit number two, like repair is done?
00:46:59.504 - 00:47:41.006, Speaker B: Not, not really. I mean, not, not directly. I mean, this, I mean, I'd say like one, one log you can look at. I mean, this is a metric, but, you know, if you have metrics in your logs, you can look at it, but there's a one called shred insert is full and that one will, it will say, like how many, it will tell you like how many shreds there were for a slot. And then I'll also tell you like the breakdown of like their source. The source being whether they were over turbine, whether they were recovered. I think I mentioned there's like we do the erasure coding for redundancy.
00:47:41.006 - 00:48:27.492, Speaker B: So if they were obtained through that or if they were repaired, you know, repair being this, you know, this process of explicitly asking for them. And so like, you know, you can look at like the metrics for your own node to see or, I mean, I guess you can grep these, these logs, too. But like it's, it's a little goofy to look at because, you know, like when, when you're in that region from one to two, you're going to have like slots getting inserted for, you know, things below that horizontal red line. But you're also going to have stuff getting inserted for like where the cyan traces. Because like it's, it's like you're getting old stuff and new stuff. So, like the logs are kind of like interleaved versus like steady state. It's mostly monotonic.
00:48:27.492 - 00:48:51.694, Speaker B: You know, it's like you insert new slots. Typically you insert slot x before you insert slot x plus one, but that one to two area, you're going to be inserting x and x plus 100 and then x plus one, then x plus 101. Does that make sense? Like kind of. Why like that, the jumping around there?
00:48:53.874 - 00:48:54.858, Speaker C: Yeah, that makes sense.
00:48:54.946 - 00:49:01.654, Speaker B: Okay. But, yeah, short answer. No, like it's, that inflection point is kind of.
00:49:02.154 - 00:49:02.482, Speaker A: Yeah.
00:49:02.498 - 00:50:08.734, Speaker B: It's not clearly called out in logs. I mean, if I think if anything, like, you know, if you're trying to like monitor, you know, if you're, if you happen to be sitting in front of your computer, I mean, actually, I guess you could like, look, you can maybe do like a couple steps, right? So there's like that validator initialized log I spit out. So, you know, you could look at that and then you could try to like, cross reference that with, like, where the, you could try to cross reference that with like, where the cluster was at that timestamp. And then you could say, okay, like, I need to get to this point before my node will start getting this, like this nice rapid startup. But, you know, that's kind of a, it's, you know, it's not obvious, you know, it's not obvious that you need to do this and it's not a nice thing. So I don't know, maybe there, maybe there's an area for improvement and like the logging there. But yeah, at this point I, you know, if I'm doing this, I just grind through it because I know what to look for.
00:50:08.734 - 00:50:18.946, Speaker B: But I guess for operators, it's certainly not that obvious at all. So understand that that's something that could be of value, you know, from the logs.
00:50:18.970 - 00:50:21.494, Speaker A: What snapshot? The incremental. Was that right?
00:50:22.034 - 00:50:22.818, Speaker B: Yes.
00:50:22.986 - 00:50:35.614, Speaker A: Yeah. See, I guess you would know at least some of the info, like where it starts and then where, where your ledger was from that point, right?
00:50:35.974 - 00:51:04.642, Speaker B: Yeah. So, like, yeah, those are all called out, like in these logs. Like there's like that loading bank from dot, dot, dot. That will tell you the full snapshot slot and the incremental snapshot slot and then that one there, that's already insight. Like second from the bottom ledger processed in. That will tell you how far you made it. Actually, the one even below it, the local ledger replay complete.
00:51:04.642 - 00:51:32.374, Speaker B: That will tell you how far you made it in the local ledger replay. It's off the screen because it's a long line, but essentially it shows you the latest route. And then those are all the working banks, the slots. That has yet to be determined whether they'll make the main fork or whether they're not on the main fork, just based on the voting lockout distance.
00:51:47.844 - 00:51:49.424, Speaker A: Any other questions for Steve?
00:51:54.884 - 00:52:05.684, Speaker B: Quick question. I don't know if this is the right forum, but if, if a node enters only mode, is there something that will show up in the logs to indicate that? I mean, I imagine there is.
00:52:06.384 - 00:52:07.080, Speaker A: Yeah.
00:52:07.232 - 00:52:24.584, Speaker B: We can look for. I believe there is. Yeah. I don't remember it offhand, but I think, yeah, I'm pretty sure that happens in either. Yeah. Let me scribble on my. Somewhere.
00:52:24.584 - 00:52:37.584, Speaker B: Yeah, I will try to. I can, I should be able to look that up, but, yeah, I think there is like a, like an info or a warn that gets submitted. Okay, thanks.
00:52:43.284 - 00:53:00.212, Speaker A: All right, well, we're almost at the hour. No other questions. I'll end it there. Thanks very much to Steve for joining and walking us through all that. I'll put this on YouTube soon and then include all the notes and the chat in the link so people have some context when they're reviewing it.
00:53:00.388 - 00:53:13.904, Speaker B: Yeah, I'd say, too, like, if you give questions like, I don't like that. I think that the gist you can comment on there. So. Yeah. Oh, yeah. You can leave comments there. So if you have specific questions about that, feel free.
00:53:13.904 - 00:53:24.214, Speaker B: I think I get email notifications whenever you. Yeah. Cool. Great.
00:53:24.254 - 00:53:27.394, Speaker A: Thanks a lot, Steve, and thanks, everyone, for joining. Bye.
