00:00:03.720 - 00:00:47.854, Speaker A: Hello, breakpoint. I'm Tory green, the COO of Ionet, and I'm here today to tell you a little bit about what we're building. To begin with, let's start with a question. What's the most valuable asset in the world? Is it gold? Oil? Bitcoin. We think it's GPU's. The CEO of intel said that gpos are the new oil. In fact, China literally imported more chips than oil last year.
00:00:47.854 - 00:01:35.686, Speaker A: So what makes GPU's so valuable? In short, they're the lifeblood of AI. Machine and deep learning models are built, optimized and deployed using graphical processing units. And as we all know, the rise of AI has been unprecedented. Chat GPT recently became the fastest app ever to 100 million users. It did so in two months. That's 15 times faster than Instagram and 50 times faster than Netflix. Instacart deploys tens of thousands of models to predict delivery.
00:01:35.686 - 00:02:23.202, Speaker A: ETA's plan, the supply route of couriers, and optimize routing models for shoppers. And there are over 350,000 user generated models on hugging face. And this number is increasing by 10,000 every week. But this transformative power comes at an enormous computing cost. AI algorithms, especially deep learning models, require vast amounts of processing power to build, optimize and deploy. And this problem is only getting worse. Compute requirements for AI apps are doubling every 3.4
00:02:23.202 - 00:03:01.694, Speaker A: months. That's ten x every 18 months. As you can imagine, this has caused the cost of AI to soar. Training a model has increased in cost by 31 times per year over the last decade. And while training as a model is expensive, running it is even worse. Jensen Huang, the CEO of Nvidia, estimates that 80% to 90% of the total lifetime cost of a model are in running it. This is a process known as inferencing.
00:03:01.694 - 00:03:38.874, Speaker A: In short, companies are now spending a significant percentage of their operating expenses on AI compute. Even small startups are spending hundreds of thousands of dollars. This is going to be a trillion dollar market. But there's a problem. There's a massive shortage of GPU's in the cloud. The total demand for GPU's is around 20 to 25 exaflops, while the supply is only ten. That's a two to three times shortfall.
00:03:38.874 - 00:04:20.524, Speaker A: This creates significant problems for developers. First, it leads to long wait times. It often takes up to weeks to deploy on AWS, GCP or azure. Two, it results in limited choice. Three, high costs. The average cost of running chat GPT is $700,000 per day. Ironically, though, the shortage is primarily limited to the cloud as there are plenty of GPU's available in the broader market.
00:04:20.524 - 00:05:09.634, Speaker A: Three sources of underutilized supply include independent data centers, crypto mining farms and consumer GPU's. There are thousands of independent data centers in the US alone, representing millions of GPU's. These data centers on average operate at twelve to 18% utilization. Crypto mining farms aren't faring any better. With a switch of proof to stake, they've suffered significant losses of profitability and utilization. There are also millions of unused consumer GPU's. Basically, these are sitting in people's households and minor cloud farms.
00:05:09.634 - 00:06:03.674, Speaker A: This is where Ionet comes in. We're aggregating supply from all these sources into a single network and creating a platform that offers massive computing power at a fraction of the cost of the traditional cloud. In short, we're going to put 1 million GPU's on Solana to build the world's largest AI compute cloud. Now, this offers several benefits for the consumer. Number one, massive computing power. Engineers can log on access hundreds of thousands of GPU's with a single click. Two, flexibility.
00:06:03.674 - 00:06:36.724, Speaker A: We offer the best a with nvlink and users can customize their connectivity, security level and location. Three, speed. Unlike AWS, we don't take weeks to deploy. You can spin up a cluster in a few seconds on ionet. Four, permissionless access. There's no KYC. There's no lengthy signup process anyone can join at any time.
00:06:36.724 - 00:07:19.264, Speaker A: And five, cost efficiency. We're up to 90% cheaper than the cloud. We also offer significant benefits for suppliers, which we call workers on our network. Becoming a worker is easy. You simply need to connect to the system, install some software, and from there you can monitor your reputation, track your performance and claim your rewards. And there are multiple benefits to being a supplier. Number one, you can make up to 15 times more than mining.
00:07:19.264 - 00:08:22.284, Speaker A: Number two, you can increase your utilization rates to 100%. And number three, it's a great ux, easy onboarding, streamlined payments and extensive monitoring. In short, ionet provides structural arbitrage. By expanding supply, we can simultaneously lower costs for consumers by ten times while increasing profits for suppliers by ten times. Now, you may have heard of other protocols that are aggregating GPU's. So what makes ionet different? It's a concept known as clustering. In machine learning, clustering is the practice of taking multiple computers and combining them to work as a potent single supercomputer.
00:08:22.284 - 00:09:11.524, Speaker A: Because of the rapidly increasing requirements of training and inferencing, clustering is now essential in fact, it's virtually impossible to run today's hypercomplex machine learned models without it. And that's why everyone from OpenAI to instacart to Uber uses clustering. I want to reiterate this point and make it clear, without clustering, you simply cannot be competitive in AI. And other aggregators can't cluster. They only offer access to a single instance. But ionet can. In fact, that's what we're building the Airbnb for GPU clusters and the market is starting to notice.
00:09:11.524 - 00:09:56.974, Speaker A: We're doing the clustering for render. We've partnered with several Filecoin sp's to do the clustering for them. And we have a waitlist of over 100,000 gpu's that want to get on the network. And this is just the beginning. Ultimately, we're going to get a million gpu's to build the world's largest AI compute cloud. And we're going to do it on Solana, creating a better, faster and cheaper experience than anything that's ever existed before. Thank you, thank you.
00:09:56.974 - 00:10:08.594, Speaker A: Now I'd like to introduce my colleague, Angela Yee. She's the head of business development and she's going to give you a live demo of how it works. Thank you.
00:10:18.454 - 00:11:06.124, Speaker B: I'm Angela, head of business development at ionet and I'm really excited to share a demo presentation of the ionetwork through decentralized distributed computing. We are going to revolutionize AI compute as we know it. As Tory mentioned, global supply constraints on GPU's are declining the pace of AI innovation, but we are going to change that. Ionet offers unmatched scalability and reliability to AI startups and enterprises. Today we are announcing the launch of ionet's three core elements. So first we have I O cloud, where engineers can access our massive infrastructure at scale. Then we have ioworker, where compute providers can provide your GPU's to the network.
00:11:06.124 - 00:11:53.434, Speaker B: And finally, IO Explorer, where you can search for everything that's happening on the network, from inferences to clusters and the salon of microtransactions behind them. Each element has a simple and really intuitive interface that's inspired by Solana's user friendly and frictionless experience. And before I start, please note that everything that I'm sharing today is fully deployed and powered by decentralized infrastructure. With special thanks with our partner Solana. So now let's start with I O cloud. And this is where engineers can seamlessly deploy their clusters at any scale to power their AI compute needs. I O cloud is truly pushing the boundaries of innovation.
00:11:53.434 - 00:12:34.826, Speaker B: Engineers don't have to wait two weeks to do a KYC process, or another three weeks to access their supply. Instead, with I O, you are just 90 seconds away from deploying a 20,000 GPU cluster. We are here to serve your model end to end, from training to inference, and provide a solution to most of your problems. So let's go ahead and deploy a cluster. The very first thing that you should do is connect your Solana wallet to top up your balance, either in USDC or you can also pay through your credit card as well. Now, deploying a cluster through inet is a really simple and quick process. The first thing you should do is deploy.
00:12:34.826 - 00:13:21.664, Speaker B: Click on this button, deploy cluster, and you'll also have two options to search from today, but later on. We also have more integrations in our pipeline, including Unreal and unity for gaming applications. And to power the metaverse in this example, let's deploy a ray cluster. You should select a cluster type that fits your model, needs general workloads, training, or inference. You can also select whether or not you want to connect your devices through nVlink. And as you can see on the menu on the right, this just keeps track of all of the available supply, depending on the configurations that you select. If sustainability is important for you, you can go ahead and select for green gpu's that are fully powered by sustainable forms of energy.
00:13:21.664 - 00:14:05.342, Speaker B: After that, just choose your supplier type. Each supplier has different specifications. For example, ionet has enterprise grade chips through data centers. You can also select render network for lighter workloads and file coin for data intensive workloads. Choose one or more locations for your gpu's based on your needs. Now, for demo purposes, we're only showing a couple options here, but with our full platform, you'll see a really wide range of regions that cover all over the world, from North America to Europe and to Asia. So, for instance, you might want to bring your cluster inference close to your user base.
00:14:05.342 - 00:14:54.944, Speaker B: Or if you're running trading algorithms, you'll probably want your devices to be close to stock exchanges. Then select your silicon chip, whether it's a GPU or a cpu. And again, you only see a handful of options now. But with our full platform, you'll see a really wide range of devices, from Apple M, two Ultra's, powerful neural engines to high end RTX series, and finally, all the way up to Nvidia A. So in this example, I'll choose an 80 gigabyte, a 100, choose your level of security and compliance as a default. Your model graphs and weights are fully end to end encrypted through our platform so that no one can access them. In addition, all of the data traffic that's flowing through the worker nodes will also be fully TL's encrypted.
00:14:54.944 - 00:15:54.014, Speaker B: Now, we also offer SoC two as an option, and we've been partnering with enterprise grade data centers so that we can provide you with the highest level of security compliance that you might need. Choose the connectivity tier based on your needs, so that you're only paying for what you need. So, for instance, if you're trying to save on costs and Internet speed isn't really important to you, you can choose between our lower tiers. But if you want higher Internet speeds that are more critical to your operations, you can go ahead and choose between our higher tier options at a more premium price. Select a cluster based image as appropriate for your needs with our out of the box configuration that will be ready for your existing codebase with no additional setup from your end. I'll go ahead and select Reapp, but you can see several more integrations in our pipeline that will go live soon. Now we just have a couple of more steps left and your cluster will be ready to deploy.
00:15:54.014 - 00:16:36.624, Speaker B: Let's go ahead and name our cluster. I'll call this Solana breakpoint, then enter your GPU quantity. Again, you can just refer to the menu on the right to see how many are available. Then enter your duration. You can select your duration on an hourly, daily, or weekly process, and for this example, I'll go ahead and select 10 hours. Now you can see the average price per card based on your configurations. And towards the bottom you can see your total cluster cost that's dependent on your duration and size of your cluster.
00:16:36.624 - 00:17:48.104, Speaker B: And at the very bottom you'll see a summary of your selected configurations. Click deploy and your cluster will be ready for you all within just 90 seconds. The cluster page shows all of the clusters that you've deployed through our network, and when you click into a specific one, you'll see a summary of the specifications, the performance of each of the worker devices within it, as well as the amount that you've paid so far. Now a really cool feature is that you can go ahead and click into visual Studio or Jupyter notebook and to deploy your code through these portals. Playground is also a really cool visualization feature that we've added as well, and you can go ahead and add or remove neurons and see how that affects your performance. And again at the bottom you'll see a list of all of the worker devices that comprise your cluster. So Kubernetes is also an integration that we have as well that we're currently supporting, and it's the backbone of enterprise cloud infrastructure.
00:17:48.104 - 00:18:35.404, Speaker B: With our support of these clusters, we are opening up use cases beyond machine learning to truly decentralize the web. Any enterprise infrastructure can be deployed, from deploying a solana validator to scaling and provisioning APIs, and to managing and scaling microservices architecture like Netflix or Spotify. So that was I O cloud. Then we'll go ahead and go into our second key element, I O worker. And this is where suppliers can connect their device, monitor the device performance, as well as keep track of their earnings and rewards. Onboarding a device on your network is also a really simple and quick process. The first thing that you should do is select your supplier that you wish to be grouped under IO, render or Filecoin.
00:18:35.404 - 00:19:11.604, Speaker B: Name your device. I'll go ahead and call this Angela node. Choose the operating system that's appropriate for your device. So we currently support Windows and Linux and we have macOS coming soon. Then select your device type, GPU or CPU. Finally, you just have to run descript and install Docker in all of the devices and go ahead and run the docker command. And then once your device is ready, you can click refresh a couple times and connect will appear after a couple minutes.
00:19:11.604 - 00:20:10.694, Speaker B: Then when you go back to your devices page, this is where you can actually monitor every single device that you've deployed through us. So let's click on one of our test notes as an example. And here you can see that you can keep track of your reliability score that's driven by your uptime percent. An overview of the specs for your device, as well as all of the services and jobs that your worker has been hired for. Now, the last thing here is earnings and rewards, where you can keep track of your earnings as well as withdraw your balance. Well, while this page is loading, I'll just give you an overview of what you should expect. So once you connect your Solana wallet here, you can withdraw your balance and I'll show an overview of all of the total amount of earnings that you've earned across all of your devices, the compute hours served, as well as the jobs that your devices have been hired for.
00:20:10.694 - 00:21:31.924, Speaker B: Finally, I'll show you I o explorer and this is where you can see everything that's happening on our network, from inferences as well as the clusters and devices that have been deployed. When you click into a supplier type, you'll see an overview of key information, including the number of GPU's or devices that's within each supplier category, as well as keeping track of the average pricing for the devices in the clusters page, you can search and see all of the clusters that are running on the network, as well as how much they've earned, total compute hours served, and the number of clusters created. Now, when you click into a cluster itself, you'll see an overview of how it's been performing, as well as the amount that it's been paying so far. And again, when you scroll through the bottom, you can see all of the devices that comprise the cluster. Now, any information that's shared through our explorer is completely non identifying. Similarly, you can search and see device level information on a devices page, and when you click into a device you'll see whether it's being currently hired, as well as its reliability score driven by its performance. And at the bottom you'll see the jobs that it has been employed for.
00:21:31.924 - 00:22:30.264, Speaker B: Our last page is a super exciting feature that allows users to explore a repository of all of the inferences ever processed by our network. If you're running a model through a cluster that's deployed on ianet, any inferences executed on it will be automatically trackable through our portal. You can also see which model the inference was done on. And right now the page is showing inferences for vc eight. And this is a generative art platform that we created to demonstrate the full capabilities of iinet for engineers and how they can monetize their models by deploying through us. When you click on an inference over here, you'll see more details on the model owner, their website, the network fee and inference fee the IO network fee is very minimal and most of the inference. This fee primarily benefits the GPU supplier and a model owner.
00:22:30.264 - 00:23:45.684, Speaker B: When you click on Sol scan, you can verify Solana microtransactions and soon we'll have tens of thousands of these microtransactions, which is only possible on Solana. And so overall, this page just shows you how much traffic is flowing through our network and how many profits model owners can realize by deploying with us. So thank you so much for watching our demo presentation. And at INET, we are on a mission to deploy over a million gpu's through our network so that we can ultimately power cutting edge innovation that's made possible only through AI. Now I have some super exciting announcements to make on projects and partnerships that are going live soon after this presentation. So the first thing that you see is our early suppliers incentives program that we're launching with render and render is actually supplying up to 300,000 tokens this year, and an additional 840,000 more through 2024 to participants of our program. So all you need to do is supply fewer than 25 consumer grade gpu's and you'll be able to apply as a render node under ionnet.
00:23:45.684 - 00:24:52.954, Speaker B: Now, this program is going to run over the next eight weeks and roll out in phases, so the earlier you join, the more incentives you'll receive and you actually start earning these token admissions immediately as soon as you participate through iaventors. We're also supplying 500,000 worth of compute power for AI startups that are seeking to change the world. Our applications are open now, so you just need to go to IO ventures to apply. We're also super excited to launch our native tokens, Iacoin and IOSD, that will be released early next year. IO coin is going to be the governance token that fuels our entire network, and IOSD will be used to compensate our GPU suppliers for their contribution as well as to cover their electricity costs. We're going to release more news and updates about our token launch very shortly, so make sure to keep an eye out for that. So this is BCA, the platform that I mentioned, a generative AI platform that allows users to create these beautiful photo realistic images with high quality graphics and lighting.
00:24:52.954 - 00:25:44.564, Speaker B: Users will also soon be able to select from a range of different fine tuned models of their choosing, so you can really create any type of image that you want. Inet is powering BCH models n to n from training to inference. Through our stable diffusion and clustering capabilities, we allow BCA's computing power to reach unmatched scalability. But this also demonstrates the full usability of our platform, how as you as an engineer can actually start earning on your models by deploying through us. And finally, we have ionet summit, which is going to take place sometime next year, and it's just going to be a really awesome event where we're bringing together the worlds of web3 and AI. We're going to have a full roster of seekers and events, including a keynote presentation by our very own CEO, Ahmad Shahid. And so you don't want to miss out on this event.
00:25:44.564 - 00:26:03.044, Speaker B: Thank you so much for coming to our presentation today so you can go ahead and scan your QR code over here to contact us. We're excited to continue building on Solana, and we'll have many more developments to share at the next breakpoint. We're excited to have you on board and join our journey together. Thanks again.
