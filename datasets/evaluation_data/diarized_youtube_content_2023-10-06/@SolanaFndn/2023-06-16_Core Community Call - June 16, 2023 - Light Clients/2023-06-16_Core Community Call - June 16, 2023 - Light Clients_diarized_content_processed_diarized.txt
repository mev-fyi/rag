00:00:05.120 - 00:00:33.684, Speaker A: Welcome everyone to this month's core community call. Today we have for discussion lite clients and we have the tiny answer team, Anoosh and Hirsch, talking about different ways that they have been thinking about implementing it. The agenda can be found, as usual on the core community call repository and the specific syndi is 52 that I have just added to the chat. Anush, if you're welcome to take it away.
00:00:36.464 - 00:01:28.164, Speaker B: Yeah, thanks for the intro, Jacob. Let me just share my screen. Yeah, thanks everyone for joining and giving your time. I'm Anush from the tiny dance team and we've been working on implementing the for Solana, we drafted 70 0052 for adding transaction proof verification. So today we're going to give a brief overview of our research on it. So I'd like to start with why we're doing this. The motivation behind SMD is primarily to implement lite clients for Solana.
00:01:28.164 - 00:02:53.578, Speaker B: The reason we need like users need to verify queries that they make to the RPC, and right now they cannot do that, which is why they have to trust the RPC that they give them the correct data. These light clients need to be low hardware pieces of software, and they need to be, need to be able to run on a phone or browser. So this is really important for security for blockchain network. As we already know, more mature networks like Ethereum that have been there for longer already have tight clients. And this was a clearing problem in Solana. So the crux of this SIMD is adding something called transaction proof. So on a high level, it's just a mercury proof saying that your transaction that you sent was included in the block and it succeeded or failed, however it should have it.
00:02:53.578 - 00:04:20.344, Speaker B: Like we, the change that we're making here requires adding, requires some way of making sure that a particular signature and a status, whether it's success or failure, was included in that transaction. The block, the user needs to be able to verify the inclusion of transaction and the execution status. We would also add a RPC method that would allow anyone to just call that method and get approved for, you know, the particular transaction. So as I mentioned, for the reason that we need this is because RPC's could give you incorrect information. That transaction actually succeeded, or it was included in the block, but it actually didn't, and that's an attack vector. You can still verify this data if you have a snapshot, but obviously snapshots are 30 40 gigabits, and that's a lot of data for the MDA user. Also relating to a different SIMD about stake weight attestations, you can actually use transaction proofs to verify that a particular validator has a certain stake of chain.
00:04:20.344 - 00:05:40.030, Speaker B: As mentioned in the SPV proposal, a validator could technically use produce transaction proofs to use them as like a checkpoint to verify certain state. And as part of a different proposal that uses SPV, it can also be used for interchange verification. Here's a list of important resources that you could take a look at. This is the simd that we wrote, the SVU proposal. There's an open issue that talks about adding statuses to the bank cash, and that's the interchange SPV proposal. So this is the part that we really, there's been a lot of discussion on, and we think that there needs to be more, more discussion on from different parts of the code of community on like different ways to implement it. In our SIMD right now, we are, we are really implementing the first way, which is modifying the block hatch to include statuses.
00:05:40.030 - 00:07:02.356, Speaker B: But over the last week, after discussing with some members of the jump team and other members of Solana Labs, we also found different ways to implement it to tackle different issues that may arise while implementing the first method that is modifying the block hash. So I'm going to dive into each of these very briefly. So, modifying the block hash is pretty straightforward and again, part of the SV proposal. So all it does is you add the transaction status along with the transaction signature into the mercantry of the transactions, which is part of each entry, and that gets hashed into, that gets merkelized into the block hash and becomes part of the bank hash. Currently, the log hash is a sequential hash of all the entries, but making it a mercury would be better in terms of verification size. And obviously having the status is important for verifying if a transaction actually succeeded. Here's a quick overview of the pros and cons.
00:07:02.356 - 00:08:08.092, Speaker B: So, the pros being that on the client side, the verification is less computationally heavy. So that's good for low hardware devices. And it's part of the core consensus protocol, which means that all validators have to be do it. The downside of this being that it's quite a major change and would require feature flight activation, which means that the round trip from like implementing this to actually going like live on Mainnet would be quite long. And also there is a computational overhead of mercury entries that also needs to be taken into account. Moving on to the second method, which is adding a separate transaction tree, which would basically be a tree of all the receipts of each transaction. This would be part of the bank hash.
00:08:08.092 - 00:09:43.484, Speaker B: So basically just a tree or tree of all these transaction signatures and statuses and would just be hashed into the bank hash. The pros of this are mainly that it doesn't come the way of bankless leadership. So a leader could basically just create the entries, create the block, propagate the block, and then asynchronously update this, update the macash with the statuses so they don't need to have it before propagating the block again. This is also a major change, so comes with the challenges of the first way to implement it. Uh, and uh, yeah, so uh, this is uh, this is part of the, the second method, which is uh, uh, we were, we were told that uh, implementing, uh, changing the bank hash would somehow come in the way of bankless leaders. So uh, if we implement this, uh, which is basically has the state of block n in block n plus ten, you actually don't need to execute the transactions of block n to get the statuses and then have compute the bank hash. So you could just do that in an async way.
00:09:43.484 - 00:10:52.876, Speaker B: But this also would be a pretty major change to the continuous protocol. And that, that is also something that needs to be heavily considered if you're deciding to go with this. The last one is probably the most flexible and easy to work with because we are not really changing any core part of the validator like the consensus. So we are just using gossip or could be a different network even to just asynchronously push a mercury commitment of all the transaction receipts. And this could be done like, let's say every ten blocks or so. It's not even doing at every block. And you could then have validators that pull this change verified against their own state and then push an attestation saying that, hey, this is, this checks out with the data that I have now.
00:10:52.876 - 00:11:58.024, Speaker B: A like client could just pull that data, pull those attestations and just verify if x percent of the stake has actually confirmed that this is the correct hash and that their transaction was included in that. The pros of this is that there's no overhead to block production because stunning synchronicity, it's not part of the block hash. There's low risk of liveness failures because it's not part of the coherent census protocol and also easier to implement because it's part of corset and not consensus. The only downside would be that it's not making the core protocol, so it would be optional to implement and the validators aren't really forced to make those attestations or commitments. So this is where we end this presentation and want to hear more about the thoughts of the core dev community and open to any critiques or questions. Thank you.
00:12:05.224 - 00:12:06.124, Speaker A: Richie.
00:12:08.424 - 00:13:57.804, Speaker C: Hey, so I have to say thanks a lot for working on this. I think this is a really nice initiative and it seems like a rather easy win, complexity and wise for enabling this functionality. I would just like to voice my preference for going the gossip route, any related method of doing it this way where we don't modify the core data structures to support this feature. I think my main problem with modifying the proof of history hash would be that it's quite breaking change of the definition what the proof of history hash currently is, where it would basically go from committing to the block data contents of the current block and all blocks before that to going to committing to the state changes that this block induces. And this would matter for firedancer, for example, because we might use the PoH hash to identify the chain that fiat and Solana labs are currently on. And let's say, for example, we have a temporary mismatch in the runtime where we derive a slightly different state on both clients by redefining the proof of history hash to potentially differ on both clients. I think it would be much harder to tolerate such runtime mismatches or even detect them, as that would basically stop violence from synchronizing entirely rather than continuing replay with a slightly different state.
00:13:57.804 - 00:15:19.246, Speaker C: And similarly, I think going the fast delay route doesn't seem too elegant to me because that would basically introduce minimum latency for any lite client where they would need to wait. These n blocks be ten or so. I think regardless which way we choose, maybe I thought of splitting up the proposal into a few separate SIMD's and then it would be much easier to vote on each one specifically because it feels like if we tried to incorporate this entire feature into one, there's going to be a bit of discussion on it for quite a while. So the first one that I thought of would be just agreeing on how we actually compute the commitment for the transaction statuses only. So given a vector of transaction statuses, what goes into the hash? So do we just commit the transaction result code, or do we also hash locks in some way and then define what the actual root of the transaction status is? And then we can do a separate SIMD that says here's how you would propagate it over gossip. And then we might go back and say, well, in hindsight, gossip was a bad idea. So here's another symbiote of how we would propagate the transaction statuses or whatever other protocol.
00:15:19.246 - 00:16:02.914, Speaker C: One other way would be of course posting it on chain itself, which of course has the benefit that this kind of incentivizes validators to actually participate in this transaction status calculation. Whereas if I wanted to be mean, I could say, hey, I don't want to implement this feature in fire dancers, I'm not just going to propagate it over gossip, which would, I think, decrease the quality of service. I think we want to incentivize validators to participate in this feature. Again, thank you so much. I love to hear your thoughts on these. I'm also going to post the same feedback on this indeed itself.
00:16:06.294 - 00:17:06.584, Speaker B: Yeah, likewise, I also respect your interest in this indeed. I think that gossip is definitely a favorable route because it allows us to test a lot of the client UX of the electron, and because actually simpler and less of a breaking change, it's easier to revert back if we messed up. If we think that it's not the right way, then doing that from making a change to the bank hash and then thinking that ok, this is not the right way, and going back to yourself, and I think that regarding validators being incentivized to network, that's something that can eventually be figured out and might even be another SMD. And yeah, I think we can. I'm, I'm leaning more towards what Richard said with going with gossip.
00:17:09.284 - 00:17:22.054, Speaker C: Yeah, I totally agree regarding the incentivization. I'd be also curious whether Solana Labs has any thoughts on this proposal. Would be really cool to see progress moving on this rather soon.
00:17:27.194 - 00:17:28.654, Speaker D: I just want to say.
00:17:30.834 - 00:17:31.282, Speaker C: First of.
00:17:31.298 - 00:18:15.164, Speaker D: All, it's really good effort. It's going to help a lot, I think, to the community. One of the issues I saw regarding the transaction status is we generally been moving away from detailed transactions that is more into very broad categories of failure. And the reason for that is, for example, originally we even had the runtime errors as part of the consensus. Luckily that's not the case anymore. There's now only one error case for the entire runtime in the consensus. And the reason why we move away.
00:18:15.204 - 00:18:19.700, Speaker C: From that is that the success cases.
00:18:19.812 - 00:18:52.784, Speaker D: Are already complex, but the failure modes are so much more complex, and it really, really narrows down the implementation. You can have to get the exact precise error response of every transaction. Right? And that makes it almost impossible to change anything or to re implement it any other way. So yeah, just be careful about transaction results. Don't include too much error state or logging into them. Otherwise, we will all be stuck with exactly one implementation.
00:18:58.764 - 00:19:24.144, Speaker B: Yeah, so we really just wanted to include, like, success or failure. We. There was a suggestion to include, like, transaction logs, but we can actually avoid that by just re executing the transaction on the client side by fetching the inputs, but. So we just want successful failure.
00:19:27.484 - 00:20:08.854, Speaker C: I heard some feedback suggesting that we also add logs. I think logs are pretty scary, because right now the truncation of logs is not well defined, but maybe that's actually an opportunity to say that there's a recommendation to, for example, only do 256 byte long log lines. The concern I had there was how this would affect performance. If we hash two or more sha blocks for each program execution or so, would that limit the TPS in the future? I don't remember who it was. I think it was mango. I don't know.
00:20:13.314 - 00:20:26.774, Speaker B: Yeah, I think the only concern was, like, with the overhead added by logs. But I think you mentioned that with firearms, like implementation of Sha. I think that that might not be a problem.
00:20:31.634 - 00:20:32.194, Speaker C: And double.
00:20:32.234 - 00:20:34.494, Speaker A: You have your Henry's. Go ahead and start.
00:20:35.034 - 00:20:46.114, Speaker E: Yeah, no, I just wanted to clarify, Richard, when you actually, for both Richard and Anush, when you say the gossip route, you mean that there are going to be no consensus changes.
00:20:46.154 - 00:20:46.734, Speaker C: Right.
00:20:47.074 - 00:20:58.194, Speaker E: Like, even the. Whatever the validators are voting on regarding the state or whatever state is necessary would be in, like, a separate smart contract. It wouldn't be part of the block.
00:21:00.014 - 00:21:07.790, Speaker C: Yes, correct. It wouldn't even be in a smart contract. You'd basically just. So there's a structure called contact info.
00:21:07.942 - 00:21:08.670, Speaker B: Okay.
00:21:08.822 - 00:21:53.644, Speaker C: Yeah. So the contact info is a structure that every validator publishes regularly onto gossip. As far as I know, there's already a snapshot hash and an accounts hash. So it seems like it would be fairly trivial to fit in another hash there. So there's actually a bit more discussion around whether we should have an entirely separate hash for this, because usually if you're getting the transaction status commitments, you probably also want the account hashes. The problem with doing separate fields for this is that you'd basically have all validators sign them separately. But I think if we fit them all into the same contact info block, that might not be a problem.
00:21:55.424 - 00:22:04.964, Speaker E: They also don't need to be at the same frequency. How frequently is this contact info updated, or how frequently do validators publish it?
00:22:06.984 - 00:22:09.804, Speaker C: That would be a question for Solana Labs, I think.
00:22:10.784 - 00:22:26.644, Speaker E: I mean, that's an implementation detail anyway. But I just wanted to be clear that in the gossip route, there are no consensus. So even the transaction status or anything. The block structure doesn't really change. So tiny dancer won't be blocked in any way on that, right?
00:22:27.584 - 00:22:30.056, Speaker B: Yes. Okay. Yeah, just.
00:22:30.080 - 00:22:31.964, Speaker E: Just wanted to clarify that.
00:22:40.264 - 00:23:08.344, Speaker A: Are there any other questions for Anushk and Hirsch? Carol, I think you mentioned that the mango was. They suggested using bidirectional connection. Max, I think you're here. Did you want to speak a little bit more on that?
00:23:11.664 - 00:23:14.524, Speaker F: Sorry, could you repeat on which one should I speak?
00:23:15.224 - 00:23:31.804, Speaker A: Krill mentioned that y'all suggested you use, like, bi directional connection. I believe this was originally, like an experimental feature that you are implementing on the labs client. And it's like a client specific. Y'all want to.
00:23:33.184 - 00:23:33.736, Speaker C: Okay.
00:23:33.800 - 00:23:36.884, Speaker A: Carol says it might not be relevant anymore.
00:23:38.174 - 00:24:41.380, Speaker F: Yeah, I can give some background still for people who are interested. So what we did is when we run benchmarks on a local network, we'll enable a patch on the TPU side to give us back basically a status. Like, it's just a very short status that summarizes what happened with the transaction in the scheduler. This is very similar to, I would say, like, request tracing environment that you would see in a commercial microservice architecture deployed in a private company. Send something in from a load balancer. You want to trace for a limited amount of the requests in the network. Why are they not getting scheduled? You get a per request measurement that is actually complete, because right now the measurements we have are statistical and broad, and it's very hard to say in particular which transaction they would just see.
00:24:41.380 - 00:25:25.650, Speaker F: All five out of 100 transactions had this issue. But we don't know what's wrong with these five transactions. Why didn't they get into the block? What were the five transactions that hit the Cu limit? Or what were the five transactions that hit the, I don't know, 2000 packets per hundred milliseconds on a quick connection. Right. There's different limits in the stack, and it's very hard to identify why a certain transaction didn't pass. That was the original intention there. But I think there's a lot of pushback against this kind of, I would say, nice to have features.
00:25:25.650 - 00:25:34.874, Speaker F: We really enjoyed having it for, like, local performance testing because we get more insights. But, yeah, that's it.
00:25:41.294 - 00:25:42.234, Speaker A: Richie.
00:25:44.454 - 00:26:50.134, Speaker C: I've looked at this a bit, and it wouldn't seem that hard to support in finance and or Solana labs. And I think it also be pretty nice to have clients to get explicit feedback about why the transaction might fail or not. And it seems like in financial applications, that would seem like a basic feature to have, especially if they're user facing. If I go in my wallet and send a transaction and that gets dropped somewhere along the path. Since this is a issue that would seem to directly affect the user experience of our clients, I think it would make sense to report that. But it requires a bit of plumbing to get that data back to the networking layer because usually at the point of where you know where your transaction gets dropped, you probably already anonymized the traffic flows where you don't have the ip addresses or quick connections anymore. So I feel like there should be.
00:26:50.134 - 00:27:30.834, Speaker C: I don't see why we wouldn't move to bidirectional connections. Just have the ability to do this kind of reverse for feedback in the future, and then we could just maybe publish this in the, specifying what the protocol should be for reverse for feedback. And then, you know, I think as time goes on, we'd see more Solana labs clients adopt this and finance the clients. What was the specific feedback? Why this wouldn't be a good idea.
00:27:32.014 - 00:27:48.534, Speaker F: I think there's a couple of, like, performance questions there, right? So I think the main issue is that it kind of, like, creates risk on the security DDoS protection side. So.
00:27:51.154 - 00:28:31.262, Speaker C: Oh, I think I know where this going. So currently, every transaction is a separate unidirectional stream, and after you send it, it gets closed. So if there's reverse for feedback, maybe that can force the client to keep open these streams for longer by just saying, hey, I dropped these packets, please send them to me again. But that should be easy to fix by, for example, delivering this feedback with datagrams or with a persistent stream. I still don't really see why we shouldn't do it if this data is easily available. I see Galactus from mango prices.
00:28:31.318 - 00:29:22.504, Speaker G: Yeah, so, yeah, so actually, like, what we implemented is more like, uh, you have this, uh, like, like with Sonala client, we cannot really implement this bi directional stuff because once you get the unit, like connection, it's dropped immediately after reading. So what we implemented is a separate service where, like, a validator, uh, like validator can connect and then it can say, like, list of transactions that it, it wants, like, feedback for. And, uh, and so actually, like, whatever is executed, like, for x reason is dropped, then we just send the, like why it was dropped, actually. So it's more like a separate service. It's not in the same, even in the same, uh, uh, like TPU client, TPU server.
00:29:23.724 - 00:29:30.674, Speaker C: I see. I mean, it would seem a bit cleaner to just deliver acknowledgments.
00:29:33.054 - 00:29:33.566, Speaker D: Directly.
00:29:33.590 - 00:29:34.518, Speaker C: Yeah, yeah.
00:29:34.606 - 00:29:35.326, Speaker B: I also love.
00:29:35.390 - 00:30:09.544, Speaker G: Like I would also like to have like a bi directional channel where you just send a transaction and get a feedback like why it was dropped. But yeah, like on the validator side there are like lot of limits and connection can be dropped for like a lot of reasons and it was quite impossible just to keep the connection up. So we said like, okay, forget we'll just have a new separate service. But yeah, of course I agree, like we can have this bi directional connection of quic and then we'll just get a feedback for our transaction.
00:30:11.164 - 00:30:40.804, Speaker C: I think if we install a mechanism to signal optional features in the quick handshake itself, it should be pretty easy to trial out features like this on a real cluster without affecting reliability too much, without introducing breaking changes. But it seems like it needs a bit more time on finding out what the actual right protocol is for delivering this feedback.
00:30:41.504 - 00:30:43.392, Speaker B: Yeah, I agree.
00:30:43.568 - 00:31:05.930, Speaker G: Because right now we don't have a lot of bits remaining just to have like give accurate acknowledgement, I think while closing or like even in the acknowledgment packets. Yeah, maybe we have to find maybe another, some, another mechanism to get this acknowledgement.
00:31:06.002 - 00:31:07.094, Speaker B: I agree with you.
00:31:12.394 - 00:31:44.754, Speaker A: All right, we're a few minutes over time, so I think what we'll have to do is we'll have to continue the discussion, probably open on up the pr that's on this as well as the discussion on Anush the SIMD 52 earlier. But thank you all for coming this month and I'll see you all next month for the next call. Reminder, if you have any agenda items, make sure you do a pr to the next agenda so that we get it earlier rather than later and people can read any additional information beforehand so we have a better discussion. But thank you.
