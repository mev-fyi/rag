00:00:08.920 - 00:00:27.888, Speaker A: I'm Austin, and this is validated today. I'm talking with Togrul Maharamov, senior researcher at Scroll a zero knowledge roll up for Ethereum. If you're not deep in the crypto space, the phrase zero knowledge roll up is probably complete gibberish. But even for those of us who are deep in the space, the actual mechanics of how zero knowledge tech works.
00:00:27.966 - 00:00:29.484, Speaker B: Can sound like a cocktail of snake.
00:00:29.524 - 00:00:42.196, Speaker A: Oil and magic bean juice. But that's why I wanted to have this conversation, to define and give examples of what zero knowledge systems are, explore their current and future use cases, and most importantly, have an honest dialogue about.
00:00:42.220 - 00:00:44.904, Speaker B: The challenges facing zero knowledge systems today.
00:00:45.364 - 00:01:08.714, Speaker A: If you rewind the clock a decade, almost no one paid attention to blockchains, and even fewer people understood how they actually worked. I feel like we're at a similar point in time with zero knowledge systems. Theyre counterintuitive, hard to understand, but also foundational technology that will expand whats possible to build on blockchains. Even if youre just crypto curious and have never heard the term zero knowledge before, I encourage you to stick it out through some of the more technical.
00:01:08.754 - 00:01:10.214, Speaker B: Weeds of this conversation.
00:01:10.554 - 00:01:20.094, Speaker A: Even at the conceptual level, I think its important for folks to have an understanding of the big technological and ideological questions in web3 that have the potential to forever change the space.
00:01:25.094 - 00:01:26.950, Speaker B: Togro, welcome to validated.
00:01:27.102 - 00:01:28.422, Speaker C: Thanks for having me.
00:01:28.598 - 00:02:03.296, Speaker A: Yeah. So we've got a lot to talk about today. So zero knowledge systems and ZK proofs are technologies in crypto that we've been hearing about for a while, maybe since about 2018. And it kind of reminds me of nuclear fusion. Like, once we get nuclear fusion right, it'll revolutionize the energy industry. But it always seems like something that's coming next year or the year after that or just over the horizon. Zero knowledge has felt that way for me for a while, but now I think were finally getting to the point in time where zero knowledge systems are starting to become usable beyond a theoretical sense.
00:02:03.296 - 00:02:20.172, Speaker A: Theres the polygon zero knowledge system, theres scroll, theres aztec. Theres a few solutions coming to Solana. But before we get into any specific projects or applications, I want to start at square one. How do you define zero knowledge proofs? Like, when most people first hear about them, they don't immediately make sense.
00:02:20.348 - 00:02:28.948, Speaker C: Yeah, it's counterintuitive because you're proving knowledge, but it's actually called zero knowledge. So Schrodinger's proof?
00:02:29.076 - 00:02:29.784, Speaker B: Yes.
00:02:30.284 - 00:02:56.812, Speaker C: So zero knowledge just means that when you're proving something, you don't reveal anything about your inner state, about what you're using to prove. So let's say if I'm proving that the sum of two numbers is ten, you just need to know that I managed to fulfill your requirement of proving that the sum is ten. You don't need to know what numbers are used as an input to get the sum of ten right.
00:02:56.948 - 00:03:24.450, Speaker A: And even though it's implicit in what you just said, just to make it clear, the basic model in zero knowledge proofs is that we have two parties, the prover and the verifier. And the prover's job is to prove to the verifier that some piece of knowledge or data is true without revealing anything else about that data, like literally nothing else about that data. So what does any of this have to do with blockchains, where today all information is default public? And where does the system of proof come from?
00:03:24.642 - 00:03:44.294, Speaker C: Zero knowledge proofs were not really used anywhere before cryptocurrencies and blockchains came along. The theoretical models were there, but the applications were nowhere to be seen. But then at some point, blockchains came along and we realized that, oh, actually we need them for privacy.
00:03:44.724 - 00:03:52.516, Speaker A: Monero and Zcash are two examples that come to mind as the first blockchains that started using zero knowledge proofs as a privacy tool, right?
00:03:52.660 - 00:04:41.704, Speaker C: So Monaro started using zero knowledge proofs for range proofs, which is basically the proof that the encrypted value that you're sending is within a certain range, let's say between zero and two to the power of 256, or whatever you define it to be. And then zcash fully integrated everything into zero knowledge proof. So you could prove the signature correctness, you could prove the knowledge of the private key. And that's how we got the primitives that were then used for the stuff that we're going to talk more about today, which are roll ups and the derivatives of that. And funnily enough, roll ups. Most of the roll ups, Aztec is the only exception. Currently, they don't use zero knowledge proofs because they're privacy preserving.
00:04:41.704 - 00:05:14.864, Speaker C: They use zero knowledge proofs because they have this interesting and very convenient property called succinctness. And what succinctness is the ability of the verifier who's being convinced that some computation is done correctly to verify it in a short amount of time. So the communication complexity is not big. So essentially, instead of sending over the entire proof, you just sent this one small compressed proof that convinced you in a few milliseconds. That yeah, the computation was actually done correctly.
00:05:15.164 - 00:05:38.306, Speaker A: So we have these two different use cases for zero knowledge proofs. Their first or original use case was as a way to make blockchain transactions private. And this second almost incidental use case is for data compression, which is how zero knowledge proofs are used in l one scaling solutions like rollups. We're definitely going to break these down further. But first, what's an example of how ZK proofs actually work?
00:05:38.500 - 00:06:44.410, Speaker C: There are a lot of examples of how people try to explain what zero knowledge is. But my favorite real life example is assume that you're a store manager that sells alcohol and you need to check for an identity of a person that comes in. So instead of looking at their id and verifying that way that they're legally allowed to buy alcohol, what you instead have is this black box installed in your store, and it contains all the ids of every person in this country, for example, in the US, let's say for this example, but you're not able to view any particular id details. It's just a black box that you can't access. So what happens if I come into the store instead of showing you the iD, I can insert my id into that black box and it will just show you an answer, either yes or no, where you can see that either I'm of legal age to buy alcohol or not, without revealing anything else about me or my age or the name. Nothing.
00:06:44.602 - 00:07:31.120, Speaker B: One of the things that's kind of funny is in the example you've given the box is the all seeing, all knowing entity. That is, it's zero knowledge from the perspective of me as the store clerk. But the box actually contains perfect knowledge about everything that's happened. But with all a lot of zero knowledge systems, part of their value is that the interface there, the box actually doesn't necessarily even know what it's doing. And so I think one of the challenging things about zero knowledge systems is it's very easy to look at them and think, well, what we're really just doing here is we're creating an arbiter of truth. And that that arbiter of truth can still be corrupted. You know, we've basically outsourced the proving to something we've decided to trust.
00:07:31.120 - 00:07:33.376, Speaker B: And that can feel very antithetical to.
00:07:33.440 - 00:07:35.168, Speaker A: The ideas of blockchain, right?
00:07:35.216 - 00:07:43.764, Speaker B: That like, oh, we're going to trust this one entity to know what's going on. So what's kind of like the way that zero knowledge doesn't fall into that trap.
00:07:44.144 - 00:08:34.386, Speaker C: So there's still some trust involved. I know that a lot of people in crypto like to use the word throw around the word trustless in cases where it's clearly not trustless. So there's clearly some trust involved here you have to trust that the construction is correct. There are certain assumptions being made when a certain zero knowledge proof system is created that if broken, basically would allow the prover to convince you of something that didn't happen or some false statement. And so you will always have some trust involved. But the whole point here is that zero knowledge proofs make quite minimal assumptions. So in most cases it's some unrealistic probability that somebody breaks the underlying cryptographic system.
00:08:34.386 - 00:09:10.430, Speaker C: In realistic terms, it's almost trustless, but it's not quite there. So we still have to trust and treat it as an oracle or as a black box in a sense that if it's broken in most cases, not every case, because if you're making computation on transparent values, that's a bit different. But in, in cases where privacy is involved, it's almost impossible to verify whether something illegal was proven, something invalid was proven, or whether everything that was proven beforehand was valid all the time in blockchain.
00:09:10.462 - 00:10:18.654, Speaker B: In general, there's an assumption here that, yes, if we somehow break the underlying cryptography running on the network, a lot of things in crypto break, but we're not worried about quantum computing yet. And a bunch of these encryption technologies have been around for a very long time, so we're probably fine, right? Is sort of the general assumption there. I think what's different here is that because zero knowledge systems today are not particularly decentralized, they run on fairly centralized systems, but we expect those systems to run in a predictable way. So how does a system prove that the thing computing the zero knowledge proof has not misbehaved? Like how do I know when I submit something to a zero knowledge system that it's not actually changing what I want to do because I don't have the observability. I mean, you described it literally as a black box, right? How do we prevent situations where I even prevent situations like what is the right conceptual way to think about the black box, the role of transparency in that black box, and how zero knowledge systems actually provide privacy and security guarantees throughout that process?
00:10:19.334 - 00:11:15.742, Speaker C: It really depends on a specific use case. So for example, if you're using zero knowledge proofs for the syncness, so essentially what rollups do, you can re execute it and verify whether the state that was proven by zero knowledge proof is identical to what you computed, and in that case you can verify that it's correct. Zero knowledge proofs in the system, like zcash for example, then you're not outsourcing the proving to anyone, you compute the proof locally and just send it over. So in that case, you're just trusting that the system is correct. But specifically in the case of roll ups, the thing is, zero knowledge proofs are not used in a vacuum here. You're not just feeding your transactions into proof, and that's it. So you have a blockchain running, you have some client, be it a geth, a fork of geth, or some client that you built that has been rollupized, let's call it that way.
00:11:15.742 - 00:12:01.082, Speaker C: You first execute all those transactions. You construct a block, and then you feed the execution trace that you extracted from one transaction or a block or a bunch of blocks, and feed it into a proof system that computes the proofs and outputs the result, which is the resulting zero knowledge proof, or as we call it, validity proofs. Because using zero knowledge proofs in the context of rollups confuses people even more. So we try to avoid doing that. In that case. Essentially the only thing that you trust is that the proof that is submitted to the underlying layer. So let's say ethereum is correct and can be verified correctly by the smart contract.
00:12:01.082 - 00:12:33.564, Speaker C: So the roll up contract on the l one. Other than that, if you run an l two node, you don't care because you can just execute it and verify that everything has been correct. So it's not the same as just feeding your transactions into some random black box that just does everything. And it's not transparent. It's still transparent. It's just because the process of proving is not very efficient. At the moment, proving is going to be relatively centralized, so you won't be able to do it, let's say your phone or your laptop.
00:12:33.564 - 00:12:46.056, Speaker C: I try to mentally disintermediate the process of creating the chain versus proving the correctness of the chain. And if you see it from that perspective, it becomes more transparent.
00:12:46.200 - 00:13:07.376, Speaker B: Yeah, okay, that makes sense. And so today, primarily there's two main applications for zero knowledge systems. As you were talking about, there's a privacy and a masking function where blockchains are public by default. If you use zero knowledge systems for there, there are ways that you can ensure that the contents of transactions remain masked or private through that process.
00:13:07.560 - 00:13:36.828, Speaker A: The other application is for data compression, which we've already mentioned. A base layer like ethereum has very limited data availability today, and that affects its throughput capacity, there's only so many kilobytes available per block. But thanks to succinctness and zero knowledge proofs, rollups allow you to offload transaction information to an off chain solution without compromising post settlement security. But there are also non zero knowledge rollups. So how do zero knowledge rollups work, and how do they differ from non zero knowledge rollups?
00:13:36.956 - 00:14:19.078, Speaker C: Of course, there are two types of rollups. The one that we've been talking about is zero knowledge rollups. And there's another type, it's optimistic rollups. Optimistic rollups function in a bit of a different way, whereas in a zero knowledge roll up, essentially settling on the underlying layer, let's say ethereum is, you can call it guilty until proven innocent. So Ethereum doesn't trust you until you prove that you're actually done the computation correctly. And optimistic roll ups, it's completely the other way around. So when you settle on Ethereum, it assumes that you're correct, unless somebody submits a proof saying that, oh, actually no, he's lying.
00:14:19.078 - 00:14:51.558, Speaker C: There's a period assigned to this, a time period assigned to this. So currently most of the roll ups use seven days. So let's say I'm a sequencer for a roll up. For an optimistic roll up, I submit a block full of transactions to Dale one, and the timer starts as soon as I submit it. And then anyone within the seven days can challenge my submission and say, oh, actually it's incorrect. There's something that you submitted that doesn't match what I computed. That's how optimistic rollups work.
00:14:51.558 - 00:15:24.782, Speaker C: And the advantage of using zero knowledge rollups versus optimistic rollups is essentially you can settle on the underlying layer as soon as the proof is submitted, the validity proof. So instead of waiting for seven days, let's say if you can compute the proof in five minutes, it's settled in five minutes. There's no challenge, period. There's nothing, because the underlying contract requires you to prove that you're in a and so as soon as you prove that you're innocent, that's it, you're fine to do whatever you want with that particular submission.
00:15:24.958 - 00:15:51.714, Speaker B: Yeah. So let's talk a little bit about where that five minutes comes from, because five minutes is a confirmation time that feels similar to what we see on some l one s today. It feels much slower than we see on a lot of the sort of l one s that are calling themselves faster solutions. Right. Solana's finality time is much lower than that. Ethereum's finality time will be moving to be lower than that once a whole series of upgrades happen. Still faster than bitcoin.
00:15:51.714 - 00:16:01.814, Speaker B: But if we're looking at that kind of finale time standpoint, what are the main technical reasons today that we're still looking at that being five minutes and not, let's say, 30 seconds?
00:16:02.554 - 00:16:16.964, Speaker C: It's just that proving is not very efficient. So you could potentially scale it and do it, compute it in a minute, or let's say 30 seconds. But that would require probably a literal supercomputer to do at this point.
00:16:17.464 - 00:16:41.790, Speaker B: Yeah. So what about the operations requires so much computational power today? And is this sort of like an arrow line projection thing where we can say, look, we have Moore's law roughly running at this pace, so we can say, even with no breakthroughs, we could see these times coming down? Or is this a system where the actual amount of math you need to do needs to be meaningfully difficult for the proof to matter?
00:16:41.952 - 00:17:20.637, Speaker C: The difficulty doesn't matter per se. If you create a new system that is trivial to compute on a cpu, that'll be fantastic. So it's not like proof of work where there has to be some hardness in terms of proving, as long as the output, the proof that you produce, is secure, it doesn't matter how you generate it. What matters is whether it's breakable or not. In scroll, we just decrease the prover ram requirements from 900gb to 270gb, which is a massive decrease. And the optimizations don't stop there. I think we can push it much further than that.
00:17:20.637 - 00:17:29.645, Speaker C: This whole discussion would have been a fever dream just like four or five years ago. Starks didn't even exist back then.
00:17:29.829 - 00:17:41.786, Speaker B: Talk a little bit about the intersection or the choice between using these systems for privacy and using them for scale. What's that relationship there? Do you have to choose one or the other? How does that system work?
00:17:41.930 - 00:17:46.418, Speaker C: Right now, it's basically disintermediated from one another.
00:17:46.546 - 00:17:47.586, Speaker B: Why is that?
00:17:47.770 - 00:18:16.614, Speaker C: Because privacy is. If you're doing privacy, it's difficult to do complex computation without blowing up the complexity of your protocol, and as a result, blowing up the complexity of the proof that you're computing. And you either have privacy, but have limited functionality, so something like Austec, or you have to use it for scaling, but have essentially unlimited functionality in terms of what you can do and build.
00:18:16.774 - 00:19:00.100, Speaker B: And so what do you think the future of that is? Do you think there's a world in two or three years where we're seeing ZK tech applied both for privacy and for scaling? Or is it really a system where, for the near future, we're picking one or the other. Because I think one of the pieces that folks are worried about over the long term is that if privacy doesn't become something that is as easy to use as a standard base layer, privacy is going to become a sign that you're doing something wrong. When I was in Boston, I had an internship at a media company. And one of the stories that we covered, this is one of my favorite anecdotes from this period was, there's a bomb threat called in at Harvard during exams. And the kid thought he was so smart because he did it via tour.
00:19:00.172 - 00:19:03.012, Speaker C: Oh, I remember that story. I remember reading it about it. Yeah.
00:19:03.028 - 00:19:22.694, Speaker B: And there were only two active tour connections on the entire Harvard network at the time, and one was a computer science professor and the other was a student. And so privacy, in the absence of enough network traffic, is not privacy. It's just a flag that I'm doing something that I'm trying to hide. So how do you think these things interplay over the long term?
00:19:22.844 - 00:20:32.304, Speaker C: I think long term, but we're talking more like 510 years from now. Most of the l one s are going to adopt privacy to some degree, either through account obstruction or some other way, or just completely re engineering the core of the protocol, because I don't think if we want to go mainstream and have cryptocurrencies, blockchains to be used for salary payments, et cetera, in the real world, I don't think a lot of people would be happy if everybody else knew how much they're getting paid or how much they're spending on some stuff that they wouldn't want others to know because they're ashamed of it or whatever. And so from that perspective, it's necessary. So the question is, when does it become viable for you to build it on an l one without sacrificing either UX or the performance or both. Yeah. And I don't think we're at the stage now where it's viable. So something like built as a roll up on top of another protocol is probably the best option we have right now.
00:20:39.684 - 00:20:44.172, Speaker A: So I want to talk about something that intersects with this topic, which is.
00:20:44.308 - 00:21:59.944, Speaker B: The role of trust in these decentralized, in theory, trustless and verifiable systems. You know, we go back to the days of, like, you know, Zcash and Monero. They launched a while ago, and we did not see rapid adoption or mass adoption of either of those protocols. And a lot of the things that seem to have come up since then that are focused on privacy or sort of trust abstraction in some way. They rely on very centralized systems and entities today to, to accomplish that, even something like aztec, you have to trust the small group that's able to actually settle that to the l one from a privacy standpoint, let alone all the stuff with Intel SGX that we're talking about, even on something like wormhole, you have to trust that the Guardian network is going to do what it says it's going to do. More broadly speaking, do you see this as a shift in the industry as like a compromising of some of those original values that anyone anywhere can verify that the state of a network is true and that we haven't really just figured out what the right compromise is or that people's values have actually changed in the process of this entire space maturing?
00:22:01.244 - 00:22:47.562, Speaker C: Be honest. Are we going to talk about trustless bridges at some point? Let's talk about trustless bridges. I'm happy to talk about them. I mean, I complain about them every day, so it's basically my job at this point. Going back to what you asked, I think that the whole idea that anyone, anywhere will be able to verify it, while it's good in theory, in principle it's not very practical. So you should still strive to maximize the number of people who are capable of verifying it and validating the network. But realistically, I don't think my mother will be running a bitcoin node anytime soon.
00:22:47.562 - 00:23:29.350, Speaker C: I don't expect that to happen. If you really want to scale the network and have an ecosystem that can be used by millions and hopefully one day billions, you just can't build a system that is verifiable, but trustlessly verifiable by everyone. So you can still strive for a system though minimizes the trust requirements for even the most resource constrained user. But it has to be through the form of lite clients validating lite clients data availability sampling, it can't come through a form of everyone runs a full node.
00:23:29.502 - 00:24:41.806, Speaker B: I agree with you that even in the Satoshi Whitepaper there's a whole thing about how there isn't an assumption that everyone is going to run a full node themselves. There's an assumption that the entities or the individuals that have a use case to run a full node will do so, and that everyone else will simply have an interface point with those. I think that is a model that feels like to me like a natural end state for much of this blockchain technology, that there's a very important characteristic of a network where everyone has the technical ability to run a network, but not necessarily everyone is going to run one. I think the thing that I was thinking about more with both bridges and with sequencers and provers, is that even if it is expensive to run a full node on Solana, a block producing validator on Solana, and it will be expensive to run one on Ethereum post surge, because you're going to have to track so many different shards of this system to get an actual full copy. I think a lot of folks in the theorem community like to pretend that's not going to be the case. But I think if we're honest, when you increase the data availability per second, you increase the computational requirements to actually verify that that's the case. Right.
00:24:41.806 - 00:25:32.876, Speaker B: You've always been very honest and transparent about that. Try to be the technical capability for anyone to do this, I think is very important. I can't spin up a Federal reserve node in my house, right. As a business, I can't just plug into the ACh network. That's a permissioned application process. But for most bridges and for most l two s and roll up systems, they're not starting with the assumption that anyone who has a business use case, or anyone who has enough ether that they feel like it's in their economic interest to run a system like this is going to be able to. I think that's the piece that I seem to think is a little more problematic, that we're moving into a world where we're not only reducing the amount of self sovereignty, but we are reducing the potential for anyone to run something themselves.
00:25:32.876 - 00:25:39.516, Speaker B: Would you say that's a change you've seen, too, or is that temporary? How would you characterize that from the.
00:25:39.540 - 00:26:50.256, Speaker C: Perspective of the roll ups? I think the reason why a lot of people think that way is because a lot of projects have been working on stuff like full nodes, et cetera, internally, and haven't really publicized much for various different reasons that I can or can't go into. But, for example, in optimism, you can already run a full node, or an arbitrum. You can run a full node. The question is whether you can be a sequencer or whether you can be a challenger. And while it is a concern short term, because currently you can't just become a sequencer out of the blue, in optimism or arbitram, the reason why it's not the case right now is because they're taking a careful approach. And in a sense, that if you were to allow anyone to compute blocks and propose them or challenge it, exposes you to a lot of things that otherwise wouldn't have happened if you, or entities whom you trust operated the node, because it's in their incentive to be honest. And so, short term, I think it's an okay approach.
00:26:50.256 - 00:27:09.044, Speaker C: It's an acceptable trade off to have, because the risk of your funds being stolen in a system that is unproven and hasn't stood the test of time is higher than in a system that has been battle tested for years, or at least a year or two.
00:27:09.464 - 00:27:57.494, Speaker B: But isn't that level of complexity something that any decentralized system has to account for? I guess the piece that there's a, there's a practical answer, which is like, it's too hard right now. And there's kind of like a more theoretical answer here, which is that the difficulties and complexities of running distributed systems have always been part of their advantage. Right. Bitcoin has always been secure because it's hard to get enough power, like literally, it's hard to get enough electricity to run enough miners to attack bitcoin. Right? Ethereum has always had sort of that same approach to. To it. When we start saying that we have to make compromises in these systems in order to make them easier to run.
00:27:57.494 - 00:28:32.302, Speaker B: There is a huge spectrum between optimism or arbitrum or scroll, and a true AWS database. I'm not trying to make that equivalency, but there is a spectrum there. The l two roll up solutions are much closer to the Amazon run database than Ethereum is. And I guess part of the thing that I have always wondered about is, if you don't do the hard thing first, is anyone really going to go back and do the hard thing later? But you seem to have faith that these systems will decentralize over time.
00:28:32.438 - 00:29:39.250, Speaker C: I mean, I'm working for one and I'm doing my best to decentralize it as soon as possible. So if I wasn't doing my job, then what's the point of me being here? I think while it is true that every system, every distributed system is complex to a certain degree, the difference between, let's say, having complexity in a roll up and having complexity in Ethereum or Solana, the worst case scenario in Solana or Ethereum, if something goes completely haywire, something really bad happens. And let's say there's a bug that allows you to create 1 billion ETH out of the blue. You can always roll back and restart from the particular checkpoint. It actually happened in bitcoin and I think it was 2010, the overflow bug. So somebody managed to mint 186 million bitcoin, if I'm not mistaken, and they had to roll back, essentially, and produce a new chain, or they actually didn't, they just disabled it. But whatever, it's the same.
00:29:39.250 - 00:30:16.522, Speaker C: Still the same thing. But you can't really do that in rollups, because if something fails and the funds are stolen, they're stolen. You can't really do anything about it because you can exit to an l one and just go on with your day. Specifically in zero knowledge roll ups, it's in our interest to decentralize things because, for example, the more provers you have in the network, bigger the fruit you can get, because you can parallelize proving and assign more provers to compute different blocks. And so the more provers you have, the bigger the fruit. But assuming no other bottlenecks. But let's assume for this vote exercise that there are no other.
00:30:16.522 - 00:31:22.264, Speaker C: And the same is true with sequencers, because while it's possible for you to have one sequencer running it, your funds will be just as secure as having 1 million sequencers running. As I like to say, for rollups, decentralization is a UX problem, not a safety problem. So your fonts are still safe. The only thing that differs is whether you can be temporarily censored or, for example, lie to when the sequencer promises to include you, let's say, in block 100, but you actually get included in block 110 or in terms of mev. So it's in the interest of the protocol to decentralize because that allows you to have a system that is more robust and more rigid to failures. It doesn't have to be as decentralized, let's say bitcoin, because you derive the security of the underlying layer. But it has to be sufficiently decentralized for the system to be robust in real world conditions where failures can happen or stuff can go wrong.
00:31:22.454 - 00:31:32.260, Speaker B: You said something interesting there, which is that decentralization is not a issue about security, it's a UX problem. Why is that?
00:31:32.412 - 00:32:02.944, Speaker C: Because in the roll ups, your funds are essentially the bridge funds are secured by the proof system used. As long as the proof system is correctly working, your funds are absolutely safe. So assuming that that's the case, your funds can be stolen. The worst that can happen is that your transaction can be temporarily censored. That's not a security issue, that's more of a liveness issue. And therefore, I like to say that it's a UX issue rather than a security issue.
00:32:03.064 - 00:32:26.794, Speaker B: Definitely looking today and sort of into the next let's call it next few months in the rest of 2023. What sorts of things should users be expecting on Ethereum for zero knowledge solutions? And also from a privacy standpoint, what should developers be thinking if privacy is something they're interested in integrating into solutions they're building?
00:32:26.954 - 00:33:41.134, Speaker C: There's going to be a lot of noise about ZK rollups, but I think on top of that, we're also going to see a few solutions that concentrate more on the privacy side. So Aztec is definitely going to come up with something. They have a brilliant team, so I expect them to definitely bring something to the table. And on top of that, for example, there's a small team in Turkey that build a solution essentially for you to prove that your ternado cash transactions weren't involved in any of the blacklisted addresses or the addresses that were highlighted as north korean addresses, etcetera. The team is called chain winning system, and I think solutions like that are also going to pop up, but I'm not sure if they're going to be talked about a lot because I still feel like privacy is more of a niche in blockchains. Currently it's all about nfts, defi, et cetera, scaling. And not a lot of people are concerned with privacy, which is good from the perspective that it doesn't clog the network because privacy, privacy stuff on blockchains right now is not very efficient.
00:33:41.134 - 00:34:00.630, Speaker C: But it's bad from the perspective that I think privacy is necessary. And the less people use it early on, the more power the governments have to basically shut it down because it's difficult to shut down a system that has 150 million Americans, for example, using it and who have money in it.
00:34:00.822 - 00:34:06.324, Speaker A: Well, Toegrill, thanks for coming on to talk about zero knowledge systems. I'm sure I'll be hearing a lot more about them this year.
00:34:06.444 - 00:34:07.984, Speaker C: Thank you for having me.
00:34:10.404 - 00:34:19.524, Speaker A: Validated is produced by Ray Belli with help from Ross Cohen, Brandon Ector, Emira Valiani and Ainsley Medford engineering by Tyler Morissette.
