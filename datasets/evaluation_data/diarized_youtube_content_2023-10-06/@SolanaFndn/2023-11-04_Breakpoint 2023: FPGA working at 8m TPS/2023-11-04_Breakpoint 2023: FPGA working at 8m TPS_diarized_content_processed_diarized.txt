00:00:03.200 - 00:00:29.554, Speaker A: All right, thanks everyone. Good afternoon. I'm Kaveh. I'm a hardware architect at jump trading group. I'm going to continue our discussions about eight two 5519 accelerations, specifically about how we accelerate the signature verification using heterogeneous computing. I should remember to get out of the way. I just want to say that by the end of this talk, we will see an awesome demo.
00:00:29.554 - 00:01:38.016, Speaker A: And on this big screen, this is awesome screen. All right, so what Kevin showed basically was that with all the hand tuning and low level understanding of the underlying cpu architecture, we can get about 30,000 signature verification per second per core. Now, to the best of our knowledge, we can get about 1 million signature verification per second on the modern gpu, although they do have a power dissipation issue. Now, to get this kind of throughput, you do have to bash transactions, and that causes issues of itself. And for example, it adds latency and complexity to the end user to use it. Now, what we're going to show today is we can achieve 1 million Sig verify per second on a single FPGA. And that's using a peak power, not average, peak power of 50 watts per FPGA with a latency of 200 microseconds, not milliseconds, microseconds latency.
00:01:38.016 - 00:02:22.860, Speaker A: And a big important point here is we are streaming transactions, we're not batching. Every single transaction is being processed individually. We are not. That's basically as if you're processing transactions using cpu's. Of course, we didn't stop there, as you heard yesterday. And what we're going to demo today is eight FPGA's running processing astounding 8 million trans using the peak power of only 400 watts. Now, I'd like to point out, reaching this level of throughput on a single machine using any other off the shelf component.
00:02:22.860 - 00:02:49.358, Speaker A: And that is this old, this is a seven year old FPGA. And with this little power is near impossible. And that's what we were going to see at the end of this talk. All right, so I'm going to start talking a little bit about hardware acceleration, and I'm going to bore you. Stick with me through this. It's about five to ten minutes. And what I'm showing here is this is our FPGA.
00:02:49.358 - 00:03:17.604, Speaker A: This is the FPGA that we're targeting. It's a vu nine p from xilinx, bought by AMD. Details. So this is an x ray of the chip that we're targeting. A typical FPGA is made of a giant grid of tiny little programmable elements. And all of these little programmable elements are connected to each other with also programmable wires. That sounds weird, but it is what it is.
00:03:17.604 - 00:04:09.114, Speaker A: And while we can program these elements to perform specific tasks, for example, multiply two numbers together, all these programmable elements can be used at the same time. That's where the parallelism comes from. And in fact, we can reach very high levels of parallelism. We can program the entire chip to have so many different processing elements at the same time. Now, another key capability of FPGA's is the ability to create heterogeneous architectures, and that's what we love. Different regions of the chip can be programmed to perform different tasks, and this provides a powerful resource to maximize overall system performance that we're trying to build. For example, some parts you can build to do multiplication, division, addition.
00:04:09.114 - 00:04:33.466, Speaker A: I think I'm out. Okay. Or maybe storing data. And in fact, nothing stops us from creating our own very custom cpu on top of this fabric. That sounds a little weird, but it's actually possible you can build another processor on top of this programmable fabric. Usually that's called a soft processor because you can actually. It's gooey.
00:04:33.466 - 00:05:03.234, Speaker A: You can change that processor and it's soft. All right. Do I keep going in and out? Am I good? All right, I just keep losing my own voice. Okay. All right. In contrast, in software paradigm, we think sequentially, and our tools, that is our programming languages, our compilers and everything, are designed with sequential tasks in mind. First, we usually start with an input data, let's say a, and we transform it into an output b.
00:05:03.234 - 00:06:17.862, Speaker A: Now, when it comes to large input arrays, now we have to transform these elements one at a time. Of course, the newer SIMD architectures, the ones that Kevin talked about, do help here, but the mentality is still one at a time and very homogeneous. This leads to a very important characteristic of this system like cpu oriented system. The throughput of transforming our input array a into our output array b is dependent on the latency of this system, and that's very important, right? The faster I can transform one element, the faster my overall throughput is. That's in contrast in the hardware, because in hardware, what we do is we just create a pipeline through the chip and we transform inputs. We just shove data through that pipeline, and that pipeline transforms the data for us from a to b. Now, what that means is at any given time, we can have multiple of these input data in flight at the same time, right? That's, that is with every bit of the system, that's what we call a clock cycle, which is like a few nanoseconds, right? Every beat of the system, I can push a new data into the pipeline.
00:06:17.862 - 00:07:05.494, Speaker A: The previous one hasn't come out yet. Right? Those are also pending. But I keep pushing data into the pipeline. The output will eventually come out from the other end of the pipeline, and it may take a bit longer from a conventional, compared to a conventional cpu, as you said, as you saw, like our solution takes about 200 microseconds compared to 35 microseconds in CPU. But latency is not everything. And in a properly pipelined design, the latency of the system has little to no impact on the throughput of the pipeline because the data just flows through it like a water hose. Sometimes to implement a complex system, we need to snake the pipeline across the entire chip and results in a longer latency.
00:07:05.494 - 00:07:55.334, Speaker A: But the throughput of the system is not affected, just like a water hose, you know, the longer the hose is, the water may take longer to show up at the end, but the water will show up with the same throughput as you're shoving into the host. Right. That's a very important point here, that we have decoupled the throughput from latency as opposed to the software len in the processing lan. All right? Another key capability of FPGA's is the fact that they can be close to the wire. And what we mean by that is we can connect our data pipelines to networks directly. This can be a great advantage for data network processing applications like crypto clients. Historically, GPU's struggled with this, and they rely on the host cpu to move data in and out of the network.
00:07:55.334 - 00:09:00.682, Speaker A: All right? And that raises the question of what form factors are practical here. So what I'm showing on left is a conventional setup that the FPGA is connected to the CPU through relatively high bandwidth and a relatively low latency, like PCIe or even more recently, CXL link. Now, this setup is very robust, has been proven really nice, everybody uses it. But it does have a shortcoming, which is the FPGA does not have direct connectivity to the network, so it does limit range of applications you can do, specifically if you have network processing applications. What I'm showing on the right is another architecture that you have direct network connectivity to your FPGA. But cloud providers don't really like to give you full access to these architectures because you have direct access to the network and they don't really want you to. It's security problems.
00:09:00.682 - 00:09:41.674, Speaker A: You know what I'm talking about. Felix knows, I guess. So, for the purposes of this talk, and considering the requirements of Sigverify, we chose the AWS style, which is basically the accelerator style. And we do the demo with AWS FPGA's. All right? So that hopefully wasn't too much of deep dive into hardware design. And what I want to talk about is the SIG verify algorithm itself. So this is a pseudocode that we basically took out of the RFC.
00:09:41.674 - 00:10:11.074, Speaker A: It's simplified and we've broken it down into four sections, and I'm going to talk about each section individually. The SHA step. If you remember, we showed a demo live last year at breakpoint 2020 in Lisbon, we did the SHA, and this year we've added. This year we've added the modular arithmetic. After the SHA, this modular arithmetic keeps showing up, right. And we implemented it entirely as a pipeline. In RTL.
00:10:11.074 - 00:10:45.448, Speaker A: The latency depends on the message size. It starts from somewhere under 1 Î¼s, all the way to three and a half microseconds, depending on the packet size that's coming in, and an astounding 6 million /second we can do these sha's and mods, not just sha, sha and the mod queue. All right, so that part is taken care of. The second part is what we call the SV. Zero is to the top, right. And this part has, well, it has two point decompressions. That's good that we can relate to, I guess.
00:10:45.448 - 00:11:40.326, Speaker A: Is it cryptographically decompression of the points? But it does have a lot of various sanity checks that a lot of times I call sanitary checks. And what happens in there is that this code is very control flow heavy. And what happens in a control flow heavy code is that you only could say, well, roll up your sleeves and just implement it in RTL. Well, it doesn't really work that way, mainly because when it's control flow heavy, the development time can be really long. And also it's very prone to mistakes. You can make a lot of mistakes and it's difficult to verify and all that. So what we thought was, was there another way that we can implement this faster and more reliably? Well, there is a very fitting architecture for such code, right, for control flow heavy code.
00:11:40.326 - 00:12:04.234, Speaker A: And that is a processor. And as I said before, nothing stops us from implementing our very own custom processor on top of our programmable fabric. It's a programmable fabric. You can do whatever you want on top of it. So that's exactly what we did. We implemented a custom ECC CPU on top of the FPGA and we programmed that processor on top of the FPGA. It sounds freaky, but that's what we did.
00:12:04.234 - 00:13:14.674, Speaker A: And it actually executes SV zero part in 100 101 microseconds and achieves at throughput of 200,000 core. I do want to talk a little bit about how we program this processor. So imagine if we start with this python code, the one on the left, and it takes two parameters, a and b. If I call this function with values for a and b, what I get at the end is a value as r. But if I call this function with objects, and through it, the simple trick of operator overloading, I can actually record the sequence of operations that are applied on top of my objects. Every time an operation happens on top of a and b, a new object is created, and all it has to do is record the operator type and the previous objects that were applied upon. And then what I receive at the end is one object, one new object that gives me the entire computational graph that was applied on top of my input objects.
00:13:14.674 - 00:14:00.056, Speaker A: And out of that, from that computation graph, we can generate any machine code we want. We can also do kinds of optimizations, constant propagation, dead code elimination, register allocation, and color coding. We basically wrote a compiler and an assembler, but it was only like 200 lines ago, and it's actually in GitHub, you can go check it out. So we literally, and I'm not exaggerating here, we literally took for these three lines of code, it's very, you know, I'm showing like a reduced code here. But for those, for SV zero, we literally took the code from the RFC. There's a python code in the RFC. We literally took that code and ran it with our objects, and our objects generated the machine code for us.
00:14:00.056 - 00:14:22.358, Speaker A: And then we put that machine code on top of our Ecc CPU. That's it. That's all we had to do. And the beauty of that is you cannot get more accurate than the RFC. So if the RFC Python works, I can guarantee that it works. So that's all we had to do for verification, basically. So that's for the SV zero part.
00:14:22.358 - 00:14:54.334, Speaker A: For the next part, SV one, it's basically doing a double scalar double point multiplication of two point curves. This is basically 256 iterations of two point add operations. Point add operation on the curve has, each add operation has nine modular multiplication. These are 256 bits y, as Kevin and Philip were talking about. So these are very, this is a very heavy computation heavy step. But there's an upside. There's always an upside.
00:14:54.334 - 00:15:20.196, Speaker A: The upside is the control flow is very light. And we use that upside to basically, you know, basically we rolled up our sleeves and said, okay, let's just write the RTL. And that's what we did. We basically wrote that part in the pure RTL. And it's a full pipeline. It computes that part in 96 microseconds. And you can get more than.
00:15:20.196 - 00:15:39.912, Speaker A: Actually, more than 1 million /second for this section of the code. The last part, you have to. You have to compare the two. You end up with two points on the curve. You have to compare them. And because these points are represented on the two. So Edwards coordinates, they have Xyzt.
00:15:39.912 - 00:15:55.208, Speaker A: The t doesn't matter. For comparison, you do have to divide X and Y by their own z. So each point has XY. You have to divide it by Z and then compare. But you can always do the cross multiplication. It happens that one of the Z's is always one. So you don't have to that cross.
00:15:55.208 - 00:16:17.390, Speaker A: You can always do this cross. You only do two multiplications and you do two comparisons. And the control flow is very light. There's nothing much in there. So we just did a pure RTL pipeline as well. It literally takes 130 nanoseconds to do it. So it's just 133 million /second we can do here.
00:16:17.390 - 00:16:34.170, Speaker A: So not a problem at all. And. All right, so let's see how it looks like on a high level. So we start by. From the CPU and we send the transactions again. Remember, this is an AWS. This is an accelerator style architecture.
00:16:34.170 - 00:16:53.514, Speaker A: So we don't have network connectivity on the FPGA. So the transaction comes from the cpu through PCIe, goes to the. As soon as it gets to the chip, we extract the metadata. We put it into a queue, wait for the next step. Next step is Sha. ShA has some funny pre padding thing you have to do. We did it.
00:16:53.514 - 00:17:23.936, Speaker A: And also the SHA, because there's a variable latency. Depending on the size. Messages can come out out of order on the other side. So you need some sort of a key value store to know who's who, what. And then again, you queue it up. The next step is our ECC CPU's. As you remember, I mentioned each cpu can give us 200,000 sigma 5 /second so we threw five in there.
00:17:23.936 - 00:17:42.976, Speaker A: And again, we do need a key store in there. One thing that happens is that SV zero section. I didn't talk. Yeah, you see it up there. You see AR and R. You actually end up with eight individual numbers. 256 numbers.
00:17:42.976 - 00:18:09.720, Speaker A: So the output of each ACC CPU, for each transaction, it outputs eight, but they don't come out eight back to back. They actually come out all over the place. That's why you didn't need that key store. And they get all interleaved. And also you need that reassemble logic. And then of course there's complexity, right? And the next step is the DSDP. It's a giant, giant code.
00:18:09.720 - 00:18:40.694, Speaker A: You'll see how big it is. But it's just one pipeline. Basically what I showed in that snake, it literally looks like that and goes through it, this code, we actually ended up optimizing it too much that, you know, is there too much optimization? I don't know that. We actually clocked it a little higher. And the next step was the point equality check. We didn't have to run it any faster. It's literally nothing.
00:18:40.694 - 00:19:19.600, Speaker A: And then at the end, the transactions got reordered. Right. All around, they got reordered. So we have to put them all into a reorder buffer and shove them through PCIe again. And so this is where one of the key points of our design is that we are actually injecting the results back into the cache of the processor, the consuming processor. So if you have one tile that is basically saying, hey, compute the verify the signature of this transaction, and then that same tile can actually say, I don't want the result. That guy wants the result.
00:19:19.600 - 00:20:06.928, Speaker A: So we're not sending the result back to the sender necessarily to the requester necessarily. We're sending the result, injecting it into the cache of whoever actually needs it. And that's, it really simplifies the integration here. One thing, if you look at the pr, you'll notice the software code to use this FPGA is actually simpler compared to the software sigrify library. We actually had to comment out code to use the FPGA part. All right, so this is the overall architecture, the entire pipeline. What does it look like? If we synthesize a map into the FPGA, this is what it looks like.
00:20:06.928 - 00:20:42.290, Speaker A: And this is the same x ray I was showing before, except it's full. The blue part comes from Amazon. We can't really control it. It's fixed, it's there. It's not really full, but they're grabbing some underutilized whatever. So it really is, it pains me. And then, so the Shaw part, if you remember from last year, it was about half of this size, the Shaw from last year.
00:20:42.290 - 00:21:19.888, Speaker A: The other half is the mod. So it does, there's a lot of tricks in there. I would love to talk about all the math. And we have five cpu's here. You can see five of these cpu's is not as big as this one giant DSCP guy, but this thing is pretty fast. And that tiny little point, Nico. All right, so one thing I want to draw your attention to is the heterogeneity of our approach here, right? We targeted each step from a different angle, right? And ended up with such a diverse and colorful, you know, design, right? It's.
00:21:19.888 - 00:22:14.614, Speaker A: I mean, it's colored for a reason. Another point I would like to make is, as you can see, all different operations require actual physical space, right? That's not really how we think about it when it comes to software, right? The only thing that we really think of as like, taking space is our data. But here, our operations actually takes some space on the real state, on the actual chip, right? So this is. I'm hinting at something, right? And the hint is that let's reuse functions, right? If I had to throw like 1500 different hashes in there, right? There, there was no way I could have had like, you know, that many hashes in there, right? So let's, let's just reuse the things that we have, right? Let's not reinvent, you know, like. Okay, thank you very much. And. All right, so to summarize, I haven't shown you anything, but I will.
00:22:14.614 - 00:22:46.514, Speaker A: But what I'm going to show is on one FPGA, we can reach 1 million sigmaphile per second with peak power. Not average power, peak power, 50 watts per FPGA. And this is actually scalable. You can actually throw more fpga's into the same box if you can fit there. And apparently Amazon did. So we just used it and we just grab the machine and it has eight fpga's in it. And this approach, our approach has no problem scaling up.
00:22:46.514 - 00:23:40.624, Speaker A: We can achieve 8 million sigma per second at eight times 50 watt power. And we showed this in heterogeneous architecture. And again, I want to reiterate that if you wanted to do this 8 million sigmaphile per second with any other architecture, any other components, any other off the shelf component, we're not talking about custom custom, like asic custom. We're talking about off the shelf components. You're talking either the power is prohibitive, either the cost is prohibitive, either you get bored, it just doesn't work. This thing works, right? That's the main point. And you could argue this would have worked seven years ago, actually, because oh, one thing I did is say the FPGA is seven year old, the machine is eight years old.
00:23:40.624 - 00:24:11.104, Speaker A: Right. And yeah, we were basically freeing up cpu's to do whatever other. What is it, consensus or. Yeah, and so we're going to see the demo that will look something like this. And with that, I invite KFB on the stage with.
00:24:20.884 - 00:24:45.476, Speaker B: Yeah, before kaveh we'll show a live version of this. And is my mic on? I can just shout loud. Okay, cool. That I wanted to show another slide that I first showed in 2008 and have shown many times since, including at breakpoint last year. And that's the cruel tyranny of Amdahl's law. It isn't sufficient to just speed up one part of an application. Whatever we didn't improve will pop up, whack a mole style, and become limiting.
00:24:45.476 - 00:25:16.854, Speaker B: This causes a computer science decision paralysis. Nothing gets optimized because there's no immediate payoff to anyone. Optimization. But Cave just demonstrated something even harsher than Amdahl's law. He demonstrated the amdahl. He demonstrated the Amdahl street justice we talked about last year. Dedicated compute in the right location is compute that can't be used for other calculations because it's either in the wrong location or too specialized.
00:25:16.854 - 00:26:08.118, Speaker B: We have a very real area budget, and ED 250 519 is a beast that ate it up. Worse, as cave showed, we couldn't just make a dedicated linear pipeline that spits out a result every clock. There just isn't enough area. So in lieu of this worse still, we had to make a much more complex design that could process multiple verifications concurrently and reorder them and handle all that to compensate the pervasive never met a sequential dependency I didn't like. Mindset in software engineering has struck and struck hard. Cannibalizing area from calculations faster than the system's overall performance to speed up calculations that are slower is the bread and butter of hardware engineering. If you wanted to, say, add SHA 256 primitives for proof of history or merkle trees, we would need to take area from ED 250 519 and slow it down.
00:26:08.118 - 00:27:06.188, Speaker B: These tradeoffs get harder every time new features are added. Area used to accelerate one algorithm is area that can't be used to accelerate another. Optimizing for area and trading latency for throughput are some of the key tricks and differences in mentality between hardware and software engineers. The goal for a hardware engineer is to make a calculation as fast as it can be for the area available, and then balance the area between all calculations on the critical path to have about the same throughput to maximize overall system performance. This also gives the key to building scalable ultra high performance systems. Use a handful of massively parallel and fine grained parallel friendly algorithms and then generalize them to cover as many use cases as possible. Hypothetically speaking, assigning were tweaked assigned packet hashes instead of packet payloads, which is already supported, and ED two 5519 if various hashes were switched to use a common secure hash algorithm.
00:27:06.188 - 00:27:55.418, Speaker B: If large message hashes were switched to use a block peril implementation instead of a sequential implementation, we'd be faster, cheaper, simpler and lower area. We'd get a lot of reuse of hashing resources and or free up other area for those proof of history or merkle tree or consensus calculations. Unfortunately, while these are individually trivial changes to make, they are breaking changes, and it's yet another demonstration of what I talked about at the beginning. It's much easier to optimize data flow up front than to change it later. Now this is a mental model we frequently encounter for a server. Each socket has a bunch of cores huddled around cached for warmth, with some rough idea that these sockets can communicate with memory storage and other servers. Woe to the developer interested in the details.
00:27:55.418 - 00:28:34.124, Speaker B: These questions are strongly and actively discouraged. This results in a big mismatch between what software devs think think is easy and hard for computers to do. This in turn, results in languages, standards, and code designed for machines that don't exist. By redesigning algorithms and implementation to target real world computers and networks, we can get better security, higher performance while using less power. Unfortunately, we usually don't have the luxury of fixing existing languages and standards filled with magical thinking. We can implement them as best possible for machines buildable in this universe, though. But this is not as easy as it sounds.
00:28:34.124 - 00:29:46.758, Speaker B: Standards are often written by software engineers who've been trained to think sequentially and abhor parallelism due to the race, conditions, sharing conflicts, and so forth. Many standards are touted by their designers as being well designed for modern cpu's for custom hardware because they are built out of primitives like small lookup tables, narrow xors adds, multiplies, all seemingly trivial things for a cpu to do. At best, this is a secondary concern due to speed of light limitations. A fundamental question is where do the compute and data reside? Decentralizing compute via parallelization and optimized communication are critical and need to be at a granularity few believe is possible, much less necessary. Instead, that mindset I talked about never met a sequential I didn't like is commonplace. We need to change that mindset going forward. Now, in this series of talks, we showed that by re architecting low level computational primitives like large number multiplication and Reed Solomon coding for modern cpu's, sandboxing high performance threads, communicating, and using FPGA's for heterogeneous parallelism, we can dramatically increase the performance of key algorithms while enhancing security over what we demonstrated last year.
00:29:46.758 - 00:30:22.564, Speaker B: That being said, let's revisit yet again something we discussed at breakpoint last year in interviews and live demos and so forth. We have made independent implementations of many key components, and in doing so, remove them as bottlenecks to current end to end performance. But many other components are still in development, and without improved versions of these components, this is like putting a sports car in a traffic jam. Our end to end performance will still be limited by the slowest component in the critical path. Thus, expect us to keep hammering away at all the other components and engage with the community to help evolve the protocol in ever more efficient directions. Thank you for your attention.
00:30:29.964 - 00:30:50.332, Speaker A: Talk about the demo. All right, time for the demo. All right, so what we're seeing here is an ascii art monitor of. Let me. Okay. Oh, we got to. Yeah, there we go.
00:30:50.332 - 00:31:04.172, Speaker A: So the middle guy is our host. It's the AWS f one x 86. It's doing. I'll talk about that later. Not important. There's like. Yes, exactly.
00:31:04.172 - 00:31:33.838, Speaker A: So this is not just flickering. This is actually running on an actual AWS f one machine right now. There's a laptop right here, and it's connected to over Internet. And, yeah, this is the console. This is what we call the wire dancer monitor. And, yeah, you can go check it out. It's actually very easy to define new animations for it and new counters and everything.
00:31:33.838 - 00:32:24.954, Speaker A: So what it's doing is this number eight means there are eight tiles sending traffic over PCIe on two sides, two NumA nodes. Each side you have four FPGA's. You see four numbers, and they're receiving traffic, extracting transactions, doing the Sha mod queue, ECC CPU, doing the DSTP reordering and sending, pushing, injecting into the cache of one dedupe guy. And unfortunately, we had to turn off the dedupe because it couldn't really dedupe at 8 million. And so it's just basically showing you 8 million. It's basically saying, oh, I'm receiving 8 million. And, yeah, so we got four FPGA's on each side and receiving traffic, doing the verification and sending it back, injecting it back to the host.
00:32:24.954 - 00:32:55.894, Speaker A: And the numbers here. If you wondering why the numbers are really low here. Again, we couldn't really keep up with that much traffic, so we had to see it says broadcast. This guy is actually parsing the transactions from PCAP and broadcasting to all of these eight Sig verify tiles. So it's basically replicating that traffic. So we can actually be able to feed everyone from one source, basically. And, yeah, that's the live demo.
