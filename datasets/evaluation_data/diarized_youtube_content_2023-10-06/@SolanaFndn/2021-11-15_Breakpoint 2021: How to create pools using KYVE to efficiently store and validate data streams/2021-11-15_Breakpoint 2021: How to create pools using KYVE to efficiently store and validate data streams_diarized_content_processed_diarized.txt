00:00:15.280 - 00:00:21.326, Speaker A: All right, hello, everyone. Thank you so much for having us. I'm Fabian, the co founder of Kaif Network, together with John.
00:00:21.430 - 00:00:22.254, Speaker B: Hey, guys.
00:00:22.414 - 00:00:52.204, Speaker A: And I'm a blockchain engineer from Germany. Started out in the crypto space in end of 2019, I would say. And since summer last year, I'm full on r weave. Created my first project there called a verify, and then in February, together with John, co founded KYF. And this session, you'll get familiar with the Kyiv protocol, and you can see how you can use it in your Dapp to basically store and retrieve data really efficiently. So basically. Oh, sorry, here we go.
00:00:52.204 - 00:01:28.378, Speaker A: One fundamental problem in crypto, like around the whole ecosystem, not only Solana specific, is that basically, it's super, super hard to get access to all the data. I mean, on Solana, you guys are familiar with the RPC endpoints, not serving all the data. And this really applies to any system. Let it be avalanche or others. It's getting harder and harder and harder, and even for nodes, also more expensive, keeping all the data and making sure you retrieve it. And the solution to this is Kive. And Kive is a next generation protocol that enables data providers to really seamlessly store and retrieve the data.
00:01:28.378 - 00:02:15.078, Speaker A: And the way it works is you set up a smart contract as a storage pool, which has one uploader node and multiple validator nodes so it's fully decentralized. And then the uploader can listen to any kind of data source in the demo. You will see we hook it up to some more web, two data, so it can be used really all around the whole ecosystem. And then basically the uploader listens to the piece of data, he bundles it in big bundles, stores it onto AA with our permanent storage layer, and then makes sure that he registers the transaction id in the smart contracts. So after time, developers then pick up the transactions from the uploader coming in and then run some custom validate function against it. And for you guys, developers, it's completely custom. Whatever you want to put in the validate function, it can be an oracle based system, it can be a simple matching algorithm, it can be really anything.
00:02:15.078 - 00:02:52.412, Speaker A: And basically, then the way it works is then the validator runs the validate function, gets to a result, then takes the result and votes on the piece of data, and then if the data is found valid, I mean, it's all good, we can safely retrieve it. And if it's invalid, the uploader receives a slashing point. And the more slashing points he has, the more likely he is to get slashed. And of course all our notes are staked in the pool. So it's a proof of stake system to make sure that uploaded data is accessible and always retrievable. And we really hope that Kaif becomes the main layer between your guys dapp blockchain and ArWyf as a permanent storage layer. We can also switch to different storage layers.
00:02:52.412 - 00:03:35.384, Speaker A: Are we specific? But it's like our use case we use for the best scalability purposes. And you can imagine us basically sitting between a graph for example, and Arweave and Kaif sits in the middle responsible for data collection and making sure data integrity is basically given. Kive started out early this year as a bounty project in which we first worked on some bitcoin boundary for bridging polkadot data over to Arweave. We started out with first integration, so Polkadot was our first one. Really fast build out evm integrations and other things like that in Q two this year. We then released our testnet and funded our pre seed round. Build an amazing team.
00:03:35.384 - 00:04:23.536, Speaker A: They've given us great support over there, so that's absolutely amazing. And then we just started improving our testnet, getting more integration, so on getting more notes on and then I would say a few weeks ago closed our seed round and now really targeting to create some UI Ux improvements, make our nodes more stable. Just to give you an example, right now we have around 6000 nodes active on the storage pool for avalanche, another few thousand on the cosmos at the Moon river one. And Kive really integrates with basically all l one s around right now. So we have same partnerships with Polkadot, Solana, Celo and lots of other El Silica avalanche, you all name them because it's super, super easy to hook it up to your data source. And the cool thing is it really also works with aggregated data. You can bring the data into specific formats.
00:04:23.536 - 00:05:14.374, Speaker A: For example, we had to talk with streaming fast before it, so you can put it up into more firehose like data stream and store it and validate it. So it's really, really easy for indexers to go ahead and consume our data and then give it out as a better developer experience through the indexing systems to you guys. We will just switch into a quick demo. Give me just 1 second so I can move my c around. I think that might be it. Not yet. Do I need to switch to some sync mode? Yeah.
00:05:14.374 - 00:05:48.108, Speaker A: Ah, there you go. Awesome. All right. Yeah, you can see that's great. So in this demo we will set up a very, very basic example of how kite works. So in this demo we will use the European Central Bank API to get the USD to euro price. Basically historically from the 1 January 2000 up until now, store it, validate it, and make it available to you guys on permanent Arweave storage.
00:05:48.108 - 00:06:18.724, Speaker A: So the way it works, we start off with a very basic utility function which is just a getprice function. All of Kive's stuff is written open source and in typescript, by the way. So as you can see, we're just making a very basic get request to this API we're converting from USD to euro at a given date, and then at the end we just return the result. So we have a lot of pre built SDKs. And for our core SDK you can just import an upload in a validate function which basically need to override and then assign to the node. So just jump into the upload function. There we go.
00:06:18.724 - 00:06:56.354, Speaker A: And as you can see, you import it up here from our SDK, you just define it. And then we define our main loop, which basically sets the 1 January as our starting date, and then today, right, as the current date. And while the date we're iterating over is the same, or before today, we're going to get the price for this specific date. And then subscriber. Next is the part where Kaif comes in, pass it into the uploader. So you do have this piece of data, it's stringified, you're passing it in and then you can also specify text for easier data retrieval later on. So basically here we're just setting the date and formatting it as a text.
00:06:56.354 - 00:07:44.536, Speaker A: So you're always able to very easily get the price for the specific date then just in the loop, right. Adding another date to it, sleeping for demo purposes, because we don't want it to run through, call the main function. That's it. That's all you need to make sure your data gets uploaded through kive. You can also just specify a validate function where you're basically subscribing to this bundle of data, right? And then for each item of this proposals bundle, you go ahead, you fetch the tag again, and then we just fetching the data from it. And then down there you can see the if statement where we say, well, if the price we do get from the API does not equal the one we uploaded, well then we can just abort and call the whole bundle invalid. But if we do get through, basically, as you can see here, we're passing true for this proposal transaction to the validator.
00:07:44.536 - 00:08:14.990, Speaker A: And that's really all of the core logic behind kive. You can of course now extend this to EVM compatible data, to Solana data. You can hook up to RPC nodes, you can aggregate data from a to b, combine it in the uploader, put it into different formats and all stores through there. Then you would go ahead and deploy pool. We created a demo pool here for today. You can just see the total balance of kite tokens. So the way it works is users which are dependent on our data would fund the kite pool to make sure that the pool keeps running.
00:08:14.990 - 00:08:54.052, Speaker A: Because in every iteration where you upload some data and it gets validated, the this funding gets paid out to upload and validator as an incentive for them to be active in that pool. And so basically whenever the pool runs out of funding, it would pause. So of course every node operator and every project being dependent on Kaif data is highly incentivized to make sure that the poor always has some funding here then comes up an explore page. If you just head over to app dot kaif.net work, you can just see it here on the avalanche pool. So you can see here some changes in the stake right now and how it's all coming together and then explore. Tip oh, nice demo got still kicking in.
00:08:54.052 - 00:09:25.976, Speaker A: Usually you can see a list, of course, transaction being voted on, valid or invalid. So hopefully we can get it in somehow. And then you can also see it's a pool id. It's a smart contract, right, sorry, the pool id being here and the upload being here, both being ethereum based addresses. So our smart contracting layer is actually solidity. We are right now deploying on moonbase, so moonbeam network for that purpose. But we are able to in theory deploy a pool for specific chains just to get some benefits really across the whole solidity ecosystem.
00:09:25.976 - 00:09:55.388, Speaker A: And then basically running a node is very simple. We do have prepared some videos for that. There we go. So first, it was important that operating a node is as easy as possible. So the way it works is we have autostake and everything implemented. So you can see the node starts up, you can see the name of the node, the address, the pool is operating in, a desired stake and some version required of this integration to run. Then you can see it's automatically attempting to stake the.
00:09:55.388 - 00:10:24.374, Speaker A: Just pause the video for a second. There you go. So you can see as it's loading up all the pool demo, it's then trying to stake the tokens. Approving it, sorry, approving it first of course staking it second, and then saying it's successfully running. So basically it's super, super easy to spin up your own node, making sure it exists. And you can see now we're requesting in this while loop the API. And you can see how we basically increase our bundle size and then until we hit 50, which is the configuration value of this pool.
00:10:24.374 - 00:10:49.382, Speaker A: There you can see. So we're then creating the bundle, storing it to Arwi. If you can see the ARwi transaction id being down here. And then it just continues, of course. And it's created a new proposal here in the smart contract. And then on the validator side, this basically looks the same, really. So what you can see here, it's starting up kind of the same interface.
00:10:49.382 - 00:11:09.412, Speaker A: You can see it says it's already staked with the correct amount. So basically if you just stop your notes, start it again, it keeps continuing on that it found the proposal. There you see the bytes are matching for the payout. This is kind of important. And then you can see it voted valid on that bundle and received the reward of 0.3 kive in this one bundle. And this is the way how you incentivize it.
00:11:09.412 - 00:11:31.724, Speaker A: If you would run the uploader for a longer time, once this proposal passes, you would also see that the uploader also received a reward for that base system. And that's basically how it works. We do have multiple integrations across whole ecosystem. And then accessing the data is also very easy. Basically it's stored on arweave. There you go. Just make it a bit bigger maybe.
00:11:31.724 - 00:11:52.856, Speaker A: Perfect. So you can see this is the data we store, 1.01. It's the price data and then the tags we have in there. So the date, the value, things like that. And then this would be the data indexers or Dapp could consume. Of course, it's more beneficial for indexes because data is always bundled. So for a normal developer, it is a bit tricky to unbundle the data and then fetch the item you would get.
00:11:52.856 - 00:12:18.942, Speaker A: So of course we mostly target indexing networks, for example, to then make data available. And we have, as I said, this for EVM based integrations, Solana based integrations. We're right now building our Solana snapshot integration. We bundled big blocks of Solana data, starting early on, saving them on top of RV and making them available for others to query from. And yeah, I would say that's it. From a demo perspective, we are fully open source. Feel free to check out the code and reach out to us.
00:12:18.942 - 00:13:02.444, Speaker A: If we do have any questions, we will do a q and a session now following this. So feel free to ask any questions you have around the system. There you go. Awesome. I will sit next to John then for that. Any questions about Anthony, feel free to ask anything. Would it be data storage? For now, weave the retrieval part, the node part, something around that.
00:13:02.444 - 00:13:08.000, Speaker A: Yeah. Do we have a mic?
00:13:08.112 - 00:13:08.648, Speaker B: Yeah, I got you.
00:13:08.696 - 00:13:09.324, Speaker A: Perfect.
00:13:20.324 - 00:13:20.796, Speaker B: Hello.
00:13:20.860 - 00:13:36.424, Speaker C: Thank you for your presentation. It was really interesting. I'm not so much on the technical side. I'd like to ask about how you're looking to incentivize a testnet and reward your early adopters. What's the plan behind that and how do you plan to bring more people to your protocol?
00:13:37.684 - 00:14:09.480, Speaker D: Yeah, okay, so right now we're in testnet, not incentivized yet. Right now we have actually in the avalanche pool like 6000 validators just running nodes on their own accord, which is super exciting to see. We are planning on launching a full incentivized testnet later this year, I would say probably. And then that will be on the moon river chain. And then yeah, we're going to work on some incentivization mechanisms. Not fully 100% sure on that yet, but there will be something to do with the mainnet token there as well.
00:14:09.592 - 00:14:10.964, Speaker A: Yeah, absolutely.
00:14:11.304 - 00:14:30.764, Speaker D: But yeah, like as far as like nodes rewards are concerned, that's really configurable per pool because we really just want to make sure that like everyone can like reward the nodes properly. So like if you have an integration that's very computation heavy, then you can reward the nodes with more kive tokens than say like something that's just uploading data or something like that.
00:14:35.084 - 00:14:36.424, Speaker B: Any more questions?
00:14:40.164 - 00:14:41.704, Speaker A: Nope. This one come up.
00:14:44.524 - 00:15:01.464, Speaker E: Yeah, it's kind of a down to earth kind of question, but do you have any idea of the kind of rough pricing in terms of data? How much would it cost to store a given amount of data using kive?
00:15:01.954 - 00:15:03.026, Speaker A: Do you want to answer?
00:15:03.210 - 00:15:04.214, Speaker D: Go for it.
00:15:04.714 - 00:15:51.406, Speaker A: Okay, so I see the way it works is because we're utilizing Arweave, of course you have this base cost of Arweave storage price, right. Which right now fluctuates a bit. But I know there's maybe some mechanisms coming up to keep the price more stable right around that. And then of course the other question and that is then basically what kind of incentive does a node need to be active in that pool? So this is like kind of the additional cost of validation. Then the other question is of course what is validation worth, right? Like how much are people going to be able to put on top of it? And maybe you have a pool, you say, well, we're really desperate to get a lot of data validated so you can increase rewards artificially, right, to incentivize nodes coming onto your pool and participating in that action. But the floor price of course is our AUF based storage costs or the permanent storage cost and then the fee for validation on top of it.
00:15:51.550 - 00:15:52.754, Speaker E: Okay, thank you.
00:15:53.794 - 00:16:18.734, Speaker A: And maybe interesting about that in general is that the way we set it up is there is also treasury basically being part of the protocol. So every time fees get paid out, one part of the fee gets put into the protocol. Treasury just ensure that basically the community is able to fund pools in case a pool is running out of funding or things like that. So we really have this whole governance platform also around Kaif besides the node voting system.
00:16:20.934 - 00:16:24.674, Speaker B: Yeah, we actually had a follow up question to the one right before that.
00:16:25.134 - 00:16:25.914, Speaker A: Awesome.
00:16:27.774 - 00:16:59.124, Speaker C: I just want to ask, you mentioned that you're looking to make the validator sequence open to people who are non technical. You said that you have 6000 users already on avalanche. Do you know if those users are technical users, developers or are they just someone who's just started in crypto and they've come across and they've decided I'm going to have a go at this. And are you looking to focus on developers and experts or are you looking to bring in retail users and incentivize them with a testnet to give a validator and make it open to more public people?
00:16:59.464 - 00:17:38.810, Speaker A: Yeah, I think that's a great question because with Kaif we are in a really interesting position where of course our main users, I mean it's a b, two b of crypto if you think about it, because other indexing networks will actually use the data. But we see a huge, huge amount of people coming in being very little technica. And we see that by some questions popping up in our telegram chats. Right. And so you can see this, it's really interesting how people getting crypto and then say, oh wow, I mean, running a validator and some other, because it's super, super hard on Kaif. It's very easy. And we created videos around that, we have a documentation around it and actually lots of our community ambassadors are writing awesome articles in their own native languages to just publish it there.
00:17:38.810 - 00:18:14.732, Speaker A: And this is also where we see the, the biggest traction around it. So I would say it's very cool for us that we can focus on the node part of things, on really getting the broad crypto community in. Right. Although the product itself is very technical, I always like to say, my mom, when she ever is in crypto, would never use Kaif as a crypto project. She'd probably use some DeFi platform or some crypto platform, which then utilizes an indexer, which utilizes Kaif, which gets the data from Arweef. And this is kind of how we belong into the crypto stack. This one? Yeah.
00:18:14.748 - 00:18:16.104, Speaker D: This one behind you. Yeah.
00:18:20.404 - 00:19:03.380, Speaker F: Hi, thanks for the presentation. I'm having a question. So you guys will be a data storage framework and you guys are working with rV, but is there a reason why you guys choose rV? Because there are lots of other decentralized storage platforms like Firecoin and others. So why are we? I'm just curious. And the second one question is four gallib. Is there any reason they are not directly just making own framework like that, why they guys need the clive for them? Yeah, that's my question.
00:19:03.532 - 00:19:36.720, Speaker A: Yeah. So we choose Arwift as our storage platform, first of all, because John and I, we both emerge out of the Arweave ecosystem, we both have projects there. So which is our native place to start? But then also looking at other storage projects, you can really see that allows us the most scalability in terms of data, because querying is a lot of easier with data tagging. And then also in terms of pricing, of course it's a good option for permanent storage. Of course you could argue, well, ipfs for short term is much cheaper, right. But we do want to build a permanent product. So this is the reason we choose arweave.
00:19:36.720 - 00:20:23.254, Speaker A: But we are also, I mean, I said kind of blockchain agnostic on the storage side, so we don't have any integration on that yet. But in theory, like someone could build out integrations for ipfs, for a filecoin system and things like that. But it was really just, I would say that the nativeness of us coming from Arweave and then building on top of that, and I think the Arweave team, answer your second question, is they of course could build a kite, but it's super, super tricky to build a generalized framework into your native blockchain because it's, I mean, you don't know what kind of use cases people come up with. So I would say this is kind of the reason why we were in a good position on that, building our own project on top of our, we have to just give them the flexibility they have on the storage side, but allow us and also the flexibility to work with different integrations partners around crypto.
00:20:25.514 - 00:20:33.786, Speaker B: All right, guys, thank you. Kai, this was great. And we actually have our next presentation coming up right now.
00:20:33.850 - 00:20:34.450, Speaker A: Thanks so much.
00:20:34.522 - 00:20:35.394, Speaker B: Thank you. Thank you, guys.
