00:00:08.960 - 00:00:47.974, Speaker A: Okay, so I just waited here for a second while the slides get ready. This is going to be a continuation of the presentation we did yesterday. This is a deeper dive into fire dancer. And like we said yesterday, the overall structure of this will be to give a little bit more about the structure of it. Then we'll do a demo of the current progress thus far on the fire dancer implementation. Then Philip will come up and talk a little bit about progress we've made on improving block packing within the ecosystem. And then Kavi will come up, talk a little bit about some of the efforts to hardware accelerate critical portions of the path in the Solana validator, and also do a live demo there, too.
00:00:47.974 - 00:01:23.886, Speaker A: So this is a slide I presented yesterday. Firedancer is a new independent Solana validator. The goals are to document, standardize the existing validator to improve the ecosystem robustness, improve system performance, and it's under development. At the link above, you can look at any of the code I'm about to show today, so there won't be any tricks up my sleeves. And like I also presented yesterday, we're treating this from our perspective. It looks a lot like a production trading system, where packets come in from the outside world. In a production trading system, they go to a strategy that thinks about them.
00:01:23.886 - 00:02:09.054, Speaker A: They go through an order execution. Off to the side, there are some infrastructure that captures incoming packets. There is some replay infrastructure, so you can do various kinds of offline analysis and testing and debugging. And if you look at a slot of validator, you see the exact same structure, too, where we have the packet ingress on the front end, we have the runtime, which thinks about the packets that have come in, and then we have a consensus algorithm to essentially make those things real, exact same patterns that we see overall in the conventional trading world. And our approach for doing this is to approach it layer by layer at a high level, where there's three components to this development. The first component is optimized to packet ingress and replace that. The second component is to do an optimized implementation of the runtime.
00:02:09.054 - 00:03:10.150, Speaker A: And then the last component we expect to get to is start working on the consensus algorithms, and, you know, along the way, we'll pick up the captures and the replay kind of for free. And this led us to the concept of a Franken dancer. We didn't want to go do this project as one giant big bang project where we'd come out and say, hey, we're going to go away, put our heads down for two years. You won't hear from us, show up with a new validator that may or may not be compatible with the existing validator. So we know Solana is still under active development, changing things, we want to keep in sync with that, while not introducing too much friction between the organizations. So we are planning on just modularizing the existing validator and then replacing components as we go, which introduces the concept of a Franken dancer, where we are stitching in to the existing validator the components as we develop them to speed them up. Now, kind of a side effect of this is that we expect the early developments, even though they may be very high performance, they'll be interfacing with an ecosystem that is not very high performance yet.
00:03:10.150 - 00:04:10.554, Speaker A: And so it will be like putting a sports car in the middle of a traffic jam. But we do expect that the performance and the robustness will continue to grow as we get more and more components replaced and improved. So if we look at the architecture of Frank, and this is a slide I showed yesterday briefly, what we see is at the top we have a lot of the stuff where the pointy end of the stick is for interfacing with actual packets on the network, and a bread and butter for jump trading is interfacing with large numbers of nics with large amounts of redundancy, so that if any particular component fails, we just keep on trading actively. And all of these are very high bandwidth, like described yesterday. A modern exchange looks like a distributed denial service attack that never ends with constant line rate bursting of multiple cross connects at the same time. And you have to keep up with it to stay alive. So after we deal with that interface, we intend to go zero copy directly into a massive bank of signature verify tiles.
00:04:10.554 - 00:05:01.670, Speaker A: If you look at computationally, at least in the packet ingress end, essentially what you have is something that just needs to verify things as fast as possible. And in terms of verifying things, because of the nature of the bursty traffic and whatnot, and the overheads to set up technologies like GPU's and so forth, you can't really do this with things that are optimized for average throughput. This is one of the things I talked about yesterday. When you look at a lot of the technologies that are being developed right now, they are really trading away burst throughput capability, the ability to be prompt when data comes in, start processing it now for doing things where form really giant batches and go process them, and then you'll have really high throughput. Well, you know, as these things are coming in, they're coming in asynchronously the ability to form bursts is there. The overheads for launching a burst are there. So everything I'll be showing will be in a non burst context, and you'd be very, very prompt.
00:05:01.670 - 00:05:58.364, Speaker A: So as data comes in, can immediately be farmed out to this giant battery of signature verification tiles that go forward and start doing the necessary cryptography stuff to get through it. Now, after that comes about, there is a dedupe tile, which then deduplicates all the transactions to limit load that is going to be hitting the runtime system afterwards. There, there's also, and I'll show this in the slide here in a second, a pre de dupe stage in the signature verify. And this doesn't really exist for anything in the existing Solana ecosystem, at least since they've moved on to the QWik protocol. But if somebody was to, when you're using the high availability and you're running active active, you do expect to see a lot of redundant traffic just by nature of that. And we can filter that out just up front by saying, gee, I just saw this packet from its sister, Nick, and so I'll throw that away. And so there's a pre dedup stage that also goes on in there.
00:05:58.364 - 00:06:41.260, Speaker A: After you get through the dedupe stage, you get to the pack stage, and the pack stage does the actual block packing. And then we have a process boundary between what we're writing and the Solana validator, and that is where the blocks will be then fed into the existing runtime. If you actually were to look at the existing repo and to put some names on things that you might see, the things that are boldened, there are things that are currently checked into the repo right now that you can look at. We have the APIs for moving the stuff to the nics, zero copy. We have the actual frank application tiles for running signature verification. We have the tango API for getting it to dedupe. We have the dedupe tiles running.
00:06:41.260 - 00:07:16.964, Speaker A: We have the tango and whatnot. We are still putting together the disco libs and the rust shims. And so keep in mind, everything I'm showing is very much a work in progress. And this is essentially, we've started this work, you know, full bore, I would say, about two months ago. So kind of put frames around where we're at and how fast we're progressing. So if you delve into the details of these individual tiles, what you see on the left is a picture of a dedupe tile. With the dedupe tile, it's being fed by a bunch of data structures provided by the tango API.
00:07:16.964 - 00:07:54.590, Speaker A: It is essentially just round robin polling amongst various metadata sources to say, hey, where's the next packets? Might be for the Nics that I'm particularly interested in. Then it goes through a. Actually for the dedupe tile, this would be after the signature verify, then it goes through essentially the spam de dupe. If somebody is sending a lot of transactions to different sessions, then you can't really dedupe that upfront, you have to do it afterwards. So that essentially goes through a quick thing. Have I seen that recently? Get rid of it and then publish it downstream for that. On the right we see the SiG verify tile structure itself, similar overall structure for how the communications work.
00:07:54.590 - 00:08:38.680, Speaker A: Pull various ones, do a parse of the packet we want to parse. Because this is parallel, we expect most packets to pass parsing and parsing is pretty lightweight, so we might as well paralyze it and pick up the throughput. Do the really quick high availability dedupe on top of it. Then start doing the SHA 512 as part of the SHA 512, generate a hash that we can use for various kinds of really fast de duplications later without having to do full packet comparisons. Go into the actual looked at curved cryptography tiles using the exact same signature verification algorithms. Our current implementation is very, very fast, but we've also been very slavishly imitating the internals of the OpenSSL library for this. So as it stands right now on like a 3.3
00:08:38.680 - 00:09:31.494, Speaker A: GHz ice lake, we can do the SHA 512 and optocheraphy in about 38 microseconds to get through all that. We think there's still some slop in the implementation. Improvements in particular, we have a lot of people who are very, very familiar with optoche cryptography and have looked at the algorithms and said there's a lot of room for algorithmic improvement. And so our next phase, to try to push down the SiG verify costs there are to essentially break away from imitating OpenSSL and get very, very optimized on the more algorithmic side of the algorithm, and then hopefully maybe get another 1020 percent in performance off that. For software. If you look at the overall costs, the dominant cost are those SHA 512 and the ED two 5519 algorithm. I say about ten x is in the ED two.
00:09:31.494 - 00:10:19.034, Speaker A: Let me think how to praise this. I would say you have about low to mid tens of thousands tps per ED, 255, 19 per core that you can run. It's actually a pretty tight implementation and it's tight enough that you can use hyper threading with it to get some additional boost. The hyper threading doesn't necessarily give you a two to one speed up, because it is tight enough on the scheduling that it, you'll see some speed up, but not a straight linear scaling. You see about the rest of cost. Essentially, Sha and every other piece of infrastructure has performances that's in the millions or tens of millions or more in terms of transaction handling per second. Now, also looking at this, we've made some suggestions on the Solana discord and whatnot and potential directions.
00:10:19.034 - 00:11:25.442, Speaker A: There's a lot of areas here where we look at these algorithms, where we can see how they could be more optimized if you really wanted to, to improve performance. But right now, we are keeping this exactly compatible with what the existing validators do for that. But, like, when you look at these algorithms, I think the most common thing that you see is they never met a sequential dependency they didn't like, and processors aren't good at sequential dependencies. And so, any way you can restructure these algorithms to exploit more parallelism, be they core parallelism or ILP within the instructions, or as we'll see with some of the stuff, Kava will present parallelism with hardware, then you can get really dramatic speedups and reductions of areas. But, you know, right now, if you look at these things, you see these pipelines that are, you know, hundreds and hundreds of stages deep, and they look superficially hardware friendly in that they use operations like xors and rotates and so forths, but they are actually really quite hostile because of all those sequential dependencies. And there are other ways to be structured them for performance. And if you go through the same kind of breakdown of how these things map onto the existing things, you're looking at the repo.
00:11:25.442 - 00:12:50.154, Speaker A: The repo is structured as a number of libraries, and you can see that you have the tango library for the communications, which provide some of the objects. And in the ballet library, we have a lot of implementations of just things that are standards, and we have to conform to the standards. So I'm now going to transition to a software demo here, I hope. Can you go to the laptop? I have a sneaking suspicion that this went to sleep and it lost my connections to GCP. Yep, this is a burner laptop, too, so there'll be a lot of fun stuff. Okay, so this here is fire dancer. This has been checked out on a really vanilla GCP cloud instance in Madrid.
00:12:50.154 - 00:13:13.144, Speaker A: We picked it in Madrid for hopefully low latency in this presentation and a reliable Internet connect connectivity. This is their n two standard 80 instance, and we're running a stock red hat 8.5 image. And if you want to listen, get the rough specs of this. This is a dual socket 20 physical core cascade Lake processor, so not terribly recent. It's running at 2.8 GHz.
00:13:13.144 - 00:13:47.428, Speaker A: Hyper threading is enabled on it. It has two pneumonodes on it, and so 80 logical cores total going into it. There's nothing terribly special about this. We are not picking things for, you know, you have to use the latest and greatest intel. We're not using a whole lot of cryptographic, we're not using any actually right now, any of the cryptographic extensions on intel. Pretty much anything that supports AVX two and that includes any intel processor since mid 2013 or AMD architecture since about mid 2015 should be fine. And we aren't kicking the tires very hard.
00:13:47.428 - 00:14:38.676, Speaker A: Linux distribution so if you want to run Ubuntu or Susie or Debian or whatever you have out there, we really don't care. Our usual practice for stuff internally is we get a box, we isolate the cpu cores, we tell the OS to go away, we do it all ourselves. So we aren't actually all that sensitive to operating system things again. We are also not trying to do a whole lot of things that require people to jump through hoops to use this. There are a couple things to get the box configured for optimal performance that requires super user privileges, but we have deliberately targeted to do nothing more onerous than the existing validator does. And we've tried to be less restrictive than the existing validator so that it's very easy to install this in various environments and test it out. So if you go to a box like this, what you find is that there is not much on it, and there's no compiler, there's no git or anything.
00:14:38.676 - 00:15:39.744, Speaker A: So if you just install the basic development environment, that's fine. For this demo, I'll be using GCC, the stock GCC on the system. I'll be using the build tools like make no fancy make there or anything, and we'll get git kind of installed. We do have people like I mentioned auditing, informally verifying parts of the code, and then with that they've also been testing out their compilers like Clang and whatnot will eventually support all of that. Just in the current repos, that stuff's not checked in yet. The stock basic development package that gets provided by Red Hat here misses two important libraries from our perspective, and like I mentioned in the presentation yesterday, modern computing is all about locality, and these systems desperately try to hide locality details from you, and we tend to view that as a mistake. And so there are two libraries to install here that are just standard that allow us to essentially query the topology of the system so we can figure out how to optimize locality on that system.
00:15:39.744 - 00:16:17.006, Speaker A: The other changes that we have here are we like to lock a lot of pages in memory to get good determinism on performance, eliminate the needs of swap and so forth. And we also like to configure the scheduler for high performance critical threads to run a high priority. So we do change the limits of what a user can do in that. So there are instructions for doing that here, and then that's pretty much it for the box setup that we've done. This box here has already been primed that way so you don't have to watch me playing around with limits.com or whatnot. After we do that, we just clone the repo and you can clone it today and then we're going to build.
00:16:17.006 - 00:16:55.350, Speaker A: You can just type make, but we strongly suggest using make j because this is a proper parallel incremental build out of tree and all the other nice kind of properties. There's a number of machine targets that it will do. If you just type this vanilla, the default machine target will go to this red hat eight x 8664 and then it will do a bunch of stuff. We'll see it go through that after it's built. This creates a bunch of utilities for reliably administering. And this is something that's surprisingly tricky to do, but it's back to the operating systems desperately trying to hide locality. There are utilities that will allow you to create memory and will actually give you that memory.
00:16:55.350 - 00:17:27.304, Speaker A: It won't, you know, like, you know, basically think you're bluffing and then try to defer as long as possible. It will bind it in various places. So you could just very reliably say I want some memory here and I want it to be backed by this kind of page for TLB efficiency. And so like the configuration I've done on this box is this exact set of commands over here, and it will reserve some pages. I've already done this. And then we also created a sandbox because we are doing this with inter process models. There's a lot of stuff to set up all the shared memory objects and so forth.
00:17:27.304 - 00:18:02.434, Speaker A: And so we could also do all the asynchronous starting and restopping of applications and so over here is a command to set up a little sandbox on that. So just to go, not used to typing standing up. So if you just go through here, real brief again, just reiterating, this is a, you know, normal intel hyper threading enabled. You can see here it was processor 79, the last processor kind of mentioned 2.8 GHz. We haven't done anything to configure it. We haven't played around with any msrs or anything else like that to make this kind of run fast.
00:18:02.434 - 00:19:06.594, Speaker A: Then if I take a look at the overall thing for the system, if I run git status, it says, hey, I'm clean, I'm not doing anything here except for I did make a modification here, and if I do take a look at this modification here, this modification is really simple, is I told it to use a synthetic load generator for this purposes of this demo. The synthetic load generator is checked in, but for running this, this is not hooked up to a network yet. So we're just going to generate synthetic load that has the statistical properties very much like a Solana validator. We're going to generate it as though we are in a two to one high availability configuration with the kind of usual packet sizes and so on and so forth to actually feed into the system. So other than that, I haven't changed anything for some convenience during this demo. There are some symlinks that I have created here, because I haven't built the code yet. These symlinks aren't going anywhere yet, but I just put these here to save me some typing during the demo.
00:19:06.594 - 00:19:52.704, Speaker A: So if we take a look over here and I just type make help, it's going to tell you what it thinks it should be doing for this particular system, and so what machines it's doing, and so on and so forth. And with all this, we see our CPP flags and you can get some kind of idea of the discipline that we have internally. I'm particularly proud of these set of warnings here and whatnot. We refer to it as the brutality. But you know, I really despise the fact that narrowing conversions are allowed in C silently. We turn all of that off. I really despise that there's silent float to doable promotions, that there's a lot of problems in the type system around, types smaller than Int's and so on and so forth.
00:19:52.704 - 00:20:41.710, Speaker A: So we are really running a really, really bespoke dialect of C, which runs well for cross compiling between the Solana blockchain, running on X 86, running on various kinds of ASIC targets and or making things easy for FPGA engineers to synthesize for their hardware. Beyond that, we aren't doing a whole lot to the particular compiler build. So if I go through the make process over here and I'm not going to do make parallel first, I'll just do a serial make. Ah, I forgot to type the less. There we go. So if I, if I do the make here, what we see at the beginning of the thing is the usual stuff where it's generating dependencies for the C source and as it goes through making all the dependencies so we can do the proper incremental parallel makes and whatnot. The makefile does log what it's think it's doing.
00:20:41.710 - 00:21:19.494, Speaker A: If you're trying to debug some kind of make issues, it's a very useful thing. We see it starting to compile the various sources together. It does support mixed language development paradigms, but most of the low level stuff is written in C. And then it gets combined into a library to do the various sub things like the tango library, the ballet library and so forth. And then it creates a ton of unit tests. And if I go through and I just do a build from scratch, what we're going to find here at the build from scratch is that, you know, just take, we are real sticklers for quick build. We've just built everything from scratch, so we're done with that.
00:21:19.494 - 00:21:57.654, Speaker A: If I then go look in the unit test directory, there are tons and tons and tons of unit tests. Everything is extensively tested at levels most people aren't used to seeing. So pretty much we use a very interactive development philosophy where you write a line of code, then you update the unit test. We keep iterating back and forth. One of the reasons why we're really sticklers on really fast build times. We almost use these things big scale systems interactively. If you go over to the bin directory, there's a bunch of things in the bin directory that have appeared and these are the actual applications that we're going to start playing around with.
00:21:57.654 - 00:22:43.534, Speaker A: So if I go over to the shared memory configuration stuff, almost everything here has a little help description which will give you the kinds of commands and powers. I won't demonstrate every single one of these, but if I go over to the query over here, what we have is we have, like I mentioned, already primed the box by allocating some gigantic pages on each Numa node and how they're being used. And we've already set up a sandbox for all these shared memory objects that are permissioned for me. And we'll use these to create various kinds of stuff. So with this we can start initializing the Frank instance. So we're going to type Frank, Annette, it's going to tell me, hey, give me a name to the validator I want to run. Tell me where you want to optimize all these objects for, for locality.
00:22:43.534 - 00:23:30.668, Speaker A: I'm going to say put it near core zero, which is also Numa node zero on this box. And for now I'll just say, let's do one verifier on it and I'm going to tell it. I put all the build stuff over here in this directory and this is going to go off. And it shattered to itself. And in this chattering, what we see going on is that it has made a bunch of objects in shared memory. You see it talking about the objects that it's doing and it's recording how it laid out the shared memory and formatted it. And so if we go down to the bottom here, this is the actual, actual layout, we can then look around and we can see that in the sandbox that we have, that suddenly this thing that's gigantic pages has appeared called Frank Workspace.
00:23:30.668 - 00:24:03.340, Speaker A: And the Frank workspace is where all of our toys for running the Franken dancer have gone. So I can go maybe a little bit further on that. There's a bunch of tools to further inspect a lot of this, so you can inspect real time the state. But let's take a look at what's in the Frank workspace. And in the Frank workspace it's basically saying, hey, there's some stuff that's been allocated. A lot of this looks like file system tools. And that's not entirely unexpected, because what people don't realize is modern x 86 is not an x 86 processor anymore.
00:24:03.340 - 00:24:44.360, Speaker A: It's a network of x 86 processor emulators. And memory isn't memory anymore. Memory is actually a distributed file system that emulates DRAM. And so most people aren't used to thinking that when they touch some memory, it's actually equivalent under the hood to essentially resolving a symlink to a inode and then opening a file and then reading the entire file, then modifying the entire file, then closing it. And so when we look at all this kind of stuff and we look at programming methodologies and whatnot, and we see how inefficiently they're being done. People are doing essentially file system operations, but acting like it's a zero weight state system from 1982. So over here we see that we've already partitioned up this thing for all the various objects that are sitting around.
00:24:44.360 - 00:25:29.370, Speaker A: And, you know, with all this, let me go transition windows. And since we closed the window, I'll have to open up a new one. This will probably be hard to read, but we'll see why here in a second, because this will be extra tiny fonts. Okay, so one of the things we have here is the ability to monitor it real time from all these processes. So over here is the Frank monitor. So the Frank monitor, I just ran it. It told me, hey, tell me the frank instance that you want me to look at.
00:25:29.370 - 00:25:58.304, Speaker A: And when I run this, this has gone and inspected that shared memory region and has pretty printed what's going on inside the Franken dancer. We haven't actually started it yet. So this is just here to kind of show the monitoring stuff in a very limited sense. And so what we see over here is there's monitoring of the individual tiles in the Frankenvancer instance. So here we have the main application, we have the packing, we have the deduplication, we have the verify tile. Since they've been created, nothing's happened. You can gauge how fast I'm talking based on the time here.
00:25:58.304 - 00:26:28.924, Speaker A: They're not heart beating. They're in a state waiting to be booted. All of the counters are green. You see that there's a lot of counters for. Are these things being back pressured by other parts of the system? Are the sequence numbers increasing? How much transactions are hitting them? How many are unique? How many are failing signature verification? Is anybody overrunning me due to misconfiguration? So on and so forth. So we have complete visibility of essentially every single link in a lot of detail. And this is actually only a subset of all the things we have, but we can just do all this kind of noninvasively.
00:26:28.924 - 00:27:21.140, Speaker A: So with this, I'm going to run the monitor. I'm just going to tell it to run over here, basically printing out a snapshot every second. And so this is now going forward and it's just basically printing out snapshots. Yeah. So now with this, let's say we want to run the Franken dancer, and we're only going to run run validator right now, but that will just kind of show some of these features over here. And so when I say, like, Ben Frank run like we saw before, it's going to ask me, hey, what's the name of the instance I'm supposed to run so I can figure out where all these objects are and connect to them. And then I'm going to say, I'm going to use CPU's one through three on here.
00:27:21.140 - 00:28:01.982, Speaker A: I'm going to essentially set aside CPU zero for operating system stuff and later CPU 20 because that's the other NumA node on this box for operating. And I'm going to say just run on one through three. So now I'm running on one through three and then all of a sudden the monitor over here has gone live and it sees that there's transactions hitting it going by. And as we look, what we are basically seeing is, you know, all the tiles are in a running state. None of them are being back pressured by anything. The back pressure counts aren't ticking. We do see some failures on the signature verification, but that's deliberate in this test because we deliberately configured it to have about 1% of the transactions hitting it be wrong just to, you know, stress out those parts of the system.
00:28:01.982 - 00:28:39.802, Speaker A: We see those transaction sequence numbers are increasing. We see that, you know, the de duplication is seeing traffic. We see the verifier seeing traffic right now with the single verifier, we're getting about 30,000 transactions per second through one core. It is being de duped into about 15, 16,000 ish, 15,000 ish unique transactions per core. We are seeing, you know, the percentage of the traffic that's being de duped is consistent with the two to one high availability. 50% of the traffic is just immediately identifiable as replicated traffic from a redundant NIC. And then we also see over here that about 1% of the traffic is failing signature verification by design.
00:28:39.802 - 00:29:08.836, Speaker A: And everything's working right. No one's being flagged as a potentially thing slowing down the system. No one is being flagged as being overrun due to a misconfiguration or bug or anything within the system. Now it's important to note here, like when I'm doing this, there has been no mallocs, there's no freeze, there's no atomics, there's no hardware fences or anything in the critical path. We're using one TLB entry for everything and doing all this. And all these tiles can be put in different processes for lots of isolation and so forth. We don't really care about any of that.
00:29:08.836 - 00:29:53.580, Speaker A: So if I go over here, I'm going to stop this process over here and just control c it. As you see, with the control c, it's intersected the control c and it's cleaned itself up and it's nice and packaged up that shared memory region to be safe for doing a lot of kind of stuff. We also see that it told us that it has a log file that it made here. And so if I try to bring up that log file, let's see, seven, one, and this one won't show up very good, because it's so long. When you look at the, this is meant to be the permanent log. The log that you see at the command line is essentially meant for developers sitting there who has full context of everything that's going. But you know, typically when we're in a trading context, there's two contexts for logging.
00:29:53.580 - 00:30:50.500, Speaker A: There's the context of a developer who's sitting there, who already knows who they are, they know what hosts they're running are, they have full information. Then there's a second context, which is a regulator asked you seven years ago what were your systems doing on this day and why none of that context will exist in a log. So there's also a permanent log that gets created that has a lot more detail as to what was going on in the system. So if we look at this particular log over here, we see that there's lower priority log messages that have been committed to this logic. There's more details on the pids and tids, there's more details on the user, where the user was, what cores they were running on, the logical names of the applications, what source code and file lines they were at, and then a lot of other kinds of context that goes into that. So after we get through the kind of logging, let's go back, this monitor, if we note over here, when I controlled seed it, it basically said, hey, things were running, but they kind of stopped running. But everything seems to be ready to be rebooted.
00:30:50.500 - 00:31:22.584, Speaker A: So let's reboot it. And just for fun, let's reboot it on a different set of cores. So I've rebooted it now and everything came back this rebooted from where it left off. This isn't startingrenew. This is exactly picking up where it was at. And if we actually note here, the dedupe pack link isn't ticking, why isn't it ticking? Because in rebooted, those synthetic transactions are actually replaying from the old. So now the dedupe tile is looking at it saying, I've seen these fairly recently.
00:31:22.584 - 00:31:55.588, Speaker A: These look like they're duplicate traffic. I'm going to suppress those. So that's pretty cool. If we go over, let's do a couple other things to kind of show the limitations of this. And maybe some of the powers of this. So with this, let me take a look at what the process id here was. 617.
00:31:55.588 - 00:32:20.546, Speaker A: 657, whatever. Okay, now I've killed it from a remote process and everything was fine. It shut down normally. It intercepted, it cleaned itself up. I can then kind of come back, I can restart it again, move it around, do other things like that. So let's put it on seven through ten, whatever. So it came back up, we're kind of going fine.
00:32:20.546 - 00:32:42.664, Speaker A: I have a different process id. Six. Where did that window go? Ah, here it is. So let's do a kill minus nine. And what was the rest of it? 63. So if I kill minus. Oh, maybe not.
00:32:42.664 - 00:33:12.644, Speaker A: There we go. Okay, so if I killed it now, kill minus nine in Unix doesn't let you intercept and clean stuff up. So now this just got totally blown away in the middle. The monitor over here is going by and it's ticking and it's like, hey, the tiles think they're running, but they're not. Heart beating things look a little bit weird. If I try to bring the system back up on top of that, the system will then come up and say, wait, this validator is not safe for restart or anything else like that. This is in a weird way, I'm not touching with the ten foot pole.
00:33:12.644 - 00:34:00.304, Speaker A: So with that, let's do something a little bit bigger. How does the command work? Numanode zero. Let's give it 72 verifiers. That should work. Okay, so we built it up and now let's bring up the monitor for this and then we'll see why we had such a small font before. So the monitor, it's not even going to fit on this kind of screen. But we have a ton of verifiers ready with all the kind of tiles kind of sitting around for all of this.
00:34:00.304 - 00:34:26.618, Speaker A: And let's run it. And I need to remember the cores that I planned for this. So let's see, one, two, let's put a bunch of verifiers in here in the Numa node, and let's put a bunch here on the other Numa node. And let's put some here. And let's put some here. I think that might work. Okay, we see it coming up.
00:34:26.618 - 00:35:10.392, Speaker A: It's actually a fairly slow bring up process for this. Not because it's actually slow, but because all of the synthetic transactions we're doing take a while to generate. And as these things are coming up, we see these things coming up, we see more and more transactions growing, kind of going through the system, and we're also seeing nothing is, you know, straining under the load and so on and so forth. And so now we're basically fully saturated on top of running the system. So if I just stop the monitor, this is still running in the background, and I scroll up on the monitor, what we'll see here is getting up to the dedupe tile. The verify tiles in aggregate are doing about 1.2 million transactions per second that are going into the dedupe tile, which is then deduping the, you know, this is doing the two to one reduction.
00:35:10.392 - 00:35:29.354, Speaker A: And then the dedupe tile is muxing them all together into a consistent order. That's also, we're pretty sticklers on that for about 0.6 million transactions per second. And that's going on. Then off to the pack tile to go do its kind of business. Nothing here in the system is acting up or feeling uncomfortable. Everybody's in kind of a run state.
00:35:29.354 - 00:36:49.194, Speaker A: And then to also give you some idea of some of the high availability kinds of features over here, I can bring the monitor back up and this might take me a second, but if we, let me find a, let me find somebody to kill. Let's kill off. Let's emulate a failure here on verifier 69 over here. And so we're going to emulate a failure by just reaching into the shared memory page directly. The right place. Yes. So here I'm just going to go tell that individual command and control, just directly reach into it and tell it to shut down.
00:36:49.194 - 00:37:23.090, Speaker A: And so now it's acted like it's faulted. Let me bring up that monitor again. And we can actually overall see now in the monitor that down here, this link is now registering as being dead and it's not actually getting any kind of traffic. So you can identify parts of your hardware system that have failed. But, you know, because we had the high availability and whatnot, nothing would have broken the stride in terms of lost packets. We can also see, because we're using hyper threading, the hyper threaded pair of this one has suddenly gotten faster. And so we can actually start diagnosing a lot of very interesting performance anomalies kind of real time with all of that.
00:37:23.090 - 00:37:55.464, Speaker A: But you know, with that, that gives us at least some idea of the kind of capabilities. I'm happy to go into more details of the code, but you guys can all read that directly. And I think with that, that's a good place to wrap up this initial demo of where the software looks like for Frank. And now I want to switch back to the presentation for and have my colleague Philip join me here on the stage to talk a little bit about block packing. The clicker's on there.
00:38:00.024 - 00:38:15.800, Speaker B: Thanks, Kevin. Yes. So, as Kevin mentioned, I'm going to talk about blackpacking, and I'm actually going to talk about one specific piece of blockpacking, which is transaction cost estimation. Got it?
00:38:15.832 - 00:38:16.104, Speaker A: Yes.
00:38:16.144 - 00:38:58.330, Speaker B: So what is blockpacking in the first place? And why is transaction cost estimation an important piece? Well, so, as I'm sure most of you guys know, when these transactions come in, they have to land on chain. And space in an on chain block is a scarce resource. So the validator needs to pick which transactions actually make it on chain. And the validator's goal is to pack as many as it can so that it can maximize its fees. But there's a constraint, which is that the block has to be replayable in time by the majority of the network. Otherwise it'll lead to a fork, and then the validator won't get any rewards from that block. So there's an asymmetric risk here, where if you underpack by a little bit, you get most of the rewards, and if you overpack by too much, you risk getting no rewards at all.
00:38:58.330 - 00:39:52.358, Speaker B: So then this pushes the validators to a conservative packing, and it means that we're in the regime where the more information we have about the transactions, the better we can estimate their compute units costs. The tighter we can pack the blocks, and the more transactions can ultimately land on chain. And I guess it's also worth making a note about the compute budget program, which is relatively new, and more transactions are starting to use it. But until the vast majority of transactions use it, and even more, until the vast majority of transactions are actually estimating their compute accurately, this is still a very useful piece of the block packing process. So for all these results that I'm showing, I'll have plots like this where I took a data set of 1000 blocks just from some random day. It was like 2 million instructions. And I think these graphs maybe I downsampled a little bit.
00:39:52.358 - 00:40:35.314, Speaker B: But there's a point on the x axis is where the algorithm predicted it, and on the y axis is the actual compute unit use. So what you're looking for is a sharp line, like, you know, slope one. Y equals x, and that means that we're estimating it accurately. So here, this is the current validator algorithm. You can see there's a general trend, y equals x, you know, the diagonal line, but there's definitely a ton of spread, um, and a ton of places where we're both overestimating and a ton of places where we're underestimating. So if we zoom in here, oh, and the r squared, sorry, the r squared of this, if you fit a linear regression, is 0.65. So definitely, you know, we're not, we're getting somewhere, but there's still a lot of room for improvement here.
00:40:35.314 - 00:41:17.510, Speaker B: So if we zoom into the region with all the points we can see just a little bit more detailed, there is that clear trend line, but there's also a lot of spread. Okay, so then how is fire dancer going to do this? So, I've been working on this algorithm. The key data structure we'll use is a fixed size hash table, just like the current validator. But instead of storing each program id and using a normal hash table, we're just using buckets. Basically, we're treating this like a set of buckets. And from each instruction we'll map into a bucket. And for each instruction that's mapped into that bucket, we're keeping track of the mean and the variance and the count for those transactions.
00:41:17.510 - 00:42:05.802, Speaker B: And so then when each instruction comes through, we'll look at the buckets that it maps to add it up. And then after the banking stage, when we have executed the transaction, we know how many compute units it actually used. We go back and update the buckets with the new information. So in our experiments, we found just using, actually one hash function that takes the program id and the first byte of instruction data, which typically clues you into which instruction the program is actually executing works. And for comparison, we used 1000 buckets, which is the same as the current validator, although these buckets are actually a lot smaller. So if we did maybe an equal size memory comparison, we'd be, I don't know, 4000, 8000, 16,000 buckets, something like that. And so you can see these are our results immediately.
00:42:05.802 - 00:42:24.688, Speaker B: This is the zoomed out view. It's a lot cleaner. The diagonal line is a bit more pronounced, but again, zooming in and, sorry, our r squared here is 0.89. So from 0.65 to 0.89, if you look at it the reverse, we're making three times fewer mistakes. So big improvement.
00:42:24.688 - 00:42:47.148, Speaker B: Again, zooming in. This makes it even more clearer. So on the left you have the current validator, on the right you have r's. The diagonal line is a lot sharper. We're getting much better, much more accurate results. And even more than this, I would say our algorithm is simpler. I guess it also makes sense to pull apart the two factors.
00:42:47.148 - 00:43:38.138, Speaker B: One, we're using a better feature, as we'd call it, a better hash function. And two, we're changing the structure of the table. So if you take our new hash table structure with just the programid, which is what the current validator uses, even still, we get better than the current validator, although the difference is not as big. And plus, one nice feature of this is that we're getting standard deviations of our estimates for free. So we're getting a measure of uncertainty on our estimation, which is great, because there are some transactions that no matter how we do it, they're going to be hard to predict with high accuracy, because they could be very data dependent on a bunch of on chain accounts. And there's no way we can go and look through all these. So, for example, these ones that are circled in the red, you can see that sometimes we're underestimating or overestimating by maybe 25,000 cus.
00:43:38.138 - 00:44:13.598, Speaker B: But the standard deviation that we're getting from our table is the size, about the size of the blue section. So you can see when we make one of these estimates, we know that the standard deviation is wide. And most of the time when we're overestimating or underestimating, it's by one or two standard deviations. So we have some way of quantifying that, which then feeds back into the block packing, so that we can pack with less, you know, make less conservative packings and fine tune those packings to get more transactions on chain. Okay, with that brief note, I'm gonna turn it over to Kaveh, who's gonna talk about some really neat hardware work.
00:44:13.636 - 00:44:14.214, Speaker A: Work.
00:44:21.554 - 00:44:30.738, Speaker C: Yeah. Hello. Good. I'm good. Okay. Hi, I'm Kave. I do hardware R and D at jump.
00:44:30.738 - 00:45:02.160, Speaker C: Thanks, Philip. And that was pretty good. So, is this it? This is it, yes. So, in this quest for a second slash faster validator, and we've already talked, Kevin already talked about all these components. And so these are the components that we've looked at, studied, and we found suitable for maybe Harvard acceleration. And that's what I'm going to talk about a little bit, both from a theoretical and also from practical perspective. That's what I'm going to do today.
00:45:02.160 - 00:45:41.250, Speaker C: I'm going to talk about what it takes to and what it means to accelerate these components, at least some of them today. And also, I'm going to show a live demo on an Amazon f, one of what we've done so far. So. Ok, this slide is showing up empty. I don't know why, but so I'm going to skip that. And so when you start considering hardware acceleration, the first thing you have to consider is what form factor are you talking about? Right. Like what form factor for your hardware are you going to consider? And the very first naive setup that you can think of is the old fashioned.
00:45:41.250 - 00:46:44.894, Speaker C: You have a processor in the middle and you have your networking on one side connected through PCIe or some or newer protocols are coming, but usually you have that connectivity to your basically outside world. And then on the other side you have your FPGA or hardware accelerator connected through, and again, some sort of bus PCIe in most cases, again, better protocols coming later. But this is what you usually, right now at least you see in Amazon AWS f one s. And well, obviously you can see it has limitations because any data that gets in and out of the FPGA, your hardware accelerator has to go through your, has to go through PCIe has to go through your processor. So you're allocating a processor for at least one, one core to talk to the FPGA. And then you're limited by the PCIe bandwidth, you're limited by, you're constrained by the PCIE latency, all of that. When you're thinking about an application that is network centric, it's distributed, and your primary thing that you do is communicate.
00:46:44.894 - 00:47:20.674, Speaker C: Something like this makes more sense if you have an FPGA that's directly connected to the network. And then, so the data gets to your FPGA a lot faster with fewer hops. And then there's a processor there for coordination. You always need that processor there. So someone needs to take care of the management of the system. And then the communication between your processor and the FPGA is less of an important issue here, right? So you can rely on your direct connectivity here. And this is something that you see in Microsoft Azure, at least that's what they use themselves for their search acceleration.
00:47:20.674 - 00:48:11.324, Speaker C: Okay, so we have these different form factors that we can play with. Of course, that's dependent on the data center and all that, but we just wanted to have that out there, that the form factor and the configuration of the system makes a big difference in terms of how you can utilize your hardware accelerator. Now, looking at all of those components that we had that came, Kevin also talked about the very first thing that comes that pops out is Sig verify. Like when you're receiving blocks transactions, you have to verify, and inside the SIG verify, the first thing that pops out is SHA 512. So that's what we decided to do. We decided to just take SHA 512, implement it on Amazon, and show how fast we can run it. So what I'm showing here is a typical danger.
00:48:11.324 - 00:48:31.040, Speaker C: It actually says danger. Oh, there we go. We have a host on one side that communicates with our FPGA. This is our entire FPGA sitting inside the Amazon f one through PCie. And so we have synthetic transactions coming through. We store them in DDR for buffering purposes. They come back to the card.
00:48:31.040 - 00:49:04.120, Speaker C: We have to do some padding because Sha. And inside our SHa unit, we have the block scheduler, and we have the actual rounds that actually do the computations. Kudos to Javier on this. And then we have the reorder engine that makes sure that instruction transactions actually come out the same way that they came in, because the scheduler does reorder them a little bit. And then after that, we have this DMA engine that sends them back to the host. Now we don't have to send them back to the host if we end up consuming the. The SHA within the chip.
00:49:04.120 - 00:49:30.276, Speaker C: But. Right. For this demo, we decided to send them back to the host to validate, to see, did we do the sHa. All right, so that was important. But the numbers that I'm showing here is theoretical limits that you can get under some constraints. For example, here, PCIe, the PCIE that Amazon advertises is four times that speed. But can you get that? Good luck, because the processor is just so damn slow.
00:49:30.276 - 00:50:20.468, Speaker C: Right. That's just the way the reality of life, right? So, turns out around this, considering this is considering four blocks per transaction. So depends on what kind of synthetic data you're generating, you're going to get about 4 million transactions per second through this PCIe link. And then this FPGA, as we talked about it, doesn't have any network connectivity. But for the purposes of this demo, we decided to have an ethernet emulator inside the chip that is capable of generating 24 million transactions per second, again, assuming four blocks per transaction. And so just for the demo purposes, right, so we're buffering all of those, pushing them into the SHA. And then what I'm showing here is theoretically, our SHA can sustain 250 million hashes per per second.
00:50:20.468 - 00:50:42.724, Speaker C: Can we feed it that fast? No, we can only feed it at 60 something million hashes per second because of all the limitations. Like ingress is more limited than our SHA itself. Right? So we're good. Like, we're not the limit. So we're good on that. And then there's another limit, which is our DMA. That is, again, we're bound by the PCIe and everything in there.
00:50:42.724 - 00:51:11.170, Speaker C: So we actually just very recently this number, we doubled it, we managed to squeeze a little more data in there. So it's about 30 million transactions per second that we can send back to the host. I will show a live demo of this at the end of this presentation. What does this look like? So what I'm showing here is an x ray of the actual fPGA running on Amazon f one. And you see most of it is empty. Like all of these spots are empty spots. So we're not using.
00:51:11.170 - 00:51:31.894, Speaker C: The green is everything that Amazon provides and we cannot get rid of it. It's called the shell and you have to use it. It's always there. You cannot move it around, it's always there. And fair enough, you always end up with something like that with your own design anyway. So it takes about 25% to 30% off your chip, no problem. The pink is really our design.
00:51:31.894 - 00:52:06.364, Speaker C: So that's the entire design that does Ethernet generation, takes in the data, does the shot calculation, reordering, scheduling, everything. And also DMA pushes back to the host. So it's looking at numbers. This is about 10% of the chip and you can see that it's almost about one third of the lower one third, so it kind of checks out. And all right, so going one level back up, what do we see? We see sig verify. Sigverify includes the SHA. Have we done this yet? No, we haven't implemented this yet.
00:52:06.364 - 00:52:41.638, Speaker C: This is our design so far. And this is what we think we believe that we can hit. And what we have here is we're assuming transactions are going to come in and we're going to store them in some form of external memory, maybe SRAM based DRAM, HPM, whatever it is. And then we have to extract the transaction. We extract public key signature message, you know, all of that goes to the SHA. Some of them go to the SHA, gives us the h that goes into the ED 255 calculations and then all the other data that goes into this magic box of SIG verify scheduler. So what that guy is supposed to do is at its disposal.
00:52:41.638 - 00:53:16.626, Speaker C: It's going to have an array in the order of 20 of field multipliers. These are 256, 255 really bit multipliers that do modular multiplications. And the scheduler is going to, it's going to have a sophisticated interconnect between them. It's going to dispatch operations to them, get the results back, and it's going to parallelize across transactions because that's the only way that's the only parallelism you really have. 8250 519 is inherently, by design, is a sequential operation. You cannot paralyze it. You can run it faster, but you cannot paralyze it.
00:53:16.626 - 00:53:51.724, Speaker C: So what you can do is you can paralyze across transactions, and that's exactly what we're going to do. And then at the end, the schedule is going to give us a pass fail for per transaction. And this is our very, very conservative, by a factor of two, conservative number that we think we can hit, which is about half a million transactions per second. All right. The next thing that we considered was proof of history, replay verification. So, I'm sure most of you know more about proof of history than me. I'm new to this domain.
00:53:51.724 - 00:54:32.738, Speaker C: But at the gist of it, what it is, is, is a very long, very long sequence of hashes that you have to verify. You basically start from the dashed h, and you say, okay, if I hash guy, hash guy, hash guy, do I end up here? But to speed it up, the. The chain actually defines checkpoints here that they call ticks. So each portion of it they call a tick, which is about 12,000 hashes. So you can run all of these in parallel. The hashing that's used here is Sha 256, so it's even smaller than what I showed before. And we can hit again the same throughput of 250 million hashes per second.
00:54:32.738 - 00:55:01.934, Speaker C: We have 12,500 Sha's per tick, so we can hit 20,000 ticks per second. What does that mean in terms of transactions? We're, on average, you have about 64 transactions mixed in here. This is the protocol. I'm sure, again, you know a lot more about this than me. There's about 64 transactions mixed in at the end of each tick. So that gives us about magically 1.2 million transactions per second, the same throughput that Kevin actually just live demonstrated.
00:55:01.934 - 00:55:52.494, Speaker C: All right, with that, I'm going to switch to the demo and show if we could have the laptop brought up, that would be great. All right, I got the password, right. All right, where am I? Yep. All right, so I'm just gonna. What I'm basically doing, I'm setting up the AWS tools. This is an actual vanilla AWS F. One machine that I logged into.
00:55:52.494 - 00:56:56.358, Speaker C: It has one FPGA in it, and it's, like, basically the cheapest, smallest FPGA capable device that you can get from EC two. And I'm just setting up the tools, and it's just standard AWS stuff. And what I'm going to do is, okay, my script just moved me to the right directory automatically. So all I'm going to do is I'm going to run this process. So what I'm showing here is two components I'm showing, by the way, all of these data is getting captured live from the device. And so on the right, what I'm showing is the host has a software sender that is generating synthetic data, synthetic transactions, sending them through PCIe to the FPGA. And what it's receiving is basically telling us, yeah, I'm receiving about 24, 25 gigabits per second worth of data coming in.
00:56:56.358 - 00:57:27.726, Speaker C: This includes the overhead of the header and everything for the transactions, right. And it's basically telling me, I'm looking at the header and I'm seeing about 5 million transactions in there. That's, that's translating into about 5 million, and then it's doing the extraction. After the extraction, you see there's a drop in the data rate because the header was just extracted, thrown in the garbage. Right. So the day the data was just cut down, but these transactions are still, still the same. And inside we also have the 100 gigabits per second ethernet emulator.
00:57:27.726 - 00:57:49.698, Speaker C: Well, it turns out we overshot it a little and we're generating 128 gigabits per second. But not a problem. Airshot can handle that. So that's actually coming out to about 80 million transactions per second. If you do the math, you're like, wait a minute, this is about five times, six times bigger than that. But this is way larger than that. Why is that? Well, it's because this is generating one block per transaction.
00:57:49.698 - 00:58:23.374, Speaker C: This guy is generating random number of blocks per transaction. So the generator that we have on the hardware is simpler. It's a constant number of blocks per transaction. That's why you see this disparity between, like if you divide this by that, you don't get the same number. Basically, our Ethernet generator is generating one block per transaction back to back, no delays, just jamming the data in there. Right? Again, the hardware is, the header is getting extracted. As you can see, the relative overhead was a lot, right, compared to this guy, because there was one block and a header.
00:58:23.374 - 00:58:53.774, Speaker C: Right. So you remove the header, you cut down the data a lot, but you don't lose the transaction at all. They get muxed in and you get your 88 million transactions per second going into the SHA, and all of them come out, surprise. Right. As expected, they go into our DMA engine. Of course there's going to be a disparity here between the number of hashes going to the PCIe because the DMA engine basically says, hey, you're giving me this. So much data, I cannot push this through PCIe.
00:58:53.774 - 00:59:17.914, Speaker C: There's back pressure here. So it's dropping. It dropping transactions, hashes. Basically, all of the hashes that make it through the DMA engine go to the host. What happens in the host is we receive them into two separate. One is the Ethernet checker, one is the software checker. There's a metadata in the hash that comes back to the host that says, was this generated by the Ethernet guy or was this generated by the software guy? They know how to check them.
00:59:17.914 - 00:59:43.862, Speaker C: They are checking them, and this is the rate that they are receiving them. Why is it so slow? Because these are software checkers. They cannot keep up with this 31 million hash per second. Basically, what's happening here is it's down sampling. The checking. The data is generated at the 88 million transactions per seconds are going into the SHA engine coming out. It gets down sampled because the PCI cannot handle it.
00:59:43.862 - 01:00:13.746, Speaker C: It gets down sampled again because the software cannot handle it. And that's at the rate that they are checking the transactions. Now, you would ask, why is the Ethernet checker faster than the software checker? Like, it's a lot more transactions? Because that's just the disparity between the transactions we're pushing in. Right. The drop is happening here randomly. Like, whoever gets there first goes through. So with that disparity, you're getting more Ethernet transactions into the software, and the rest is being just dropped.
01:00:13.746 - 01:00:32.584, Speaker C: Right. So that's the demo that I wanted to show. And with that, I will invite Kevin and Philip back on the stage. We go from there. Where did the cool demo go?
01:00:35.604 - 01:00:38.372, Speaker A: I think my mic. Oh, okay, good. My mic is on.
01:00:38.548 - 01:00:39.164, Speaker C: Okay.
01:00:39.244 - 01:00:50.324, Speaker A: So we don't really have a whole lot more to say, at least formally speaking. But I know we're a little bit over on time here, but we're happy to take maybe one or two kind of quick questions, if there's any questions from the audience.
01:00:53.704 - 01:00:54.208, Speaker C: Okay.
01:00:54.256 - 01:00:55.624, Speaker A: No one's standing up. It's either really.
01:00:55.664 - 01:01:07.168, Speaker C: Oh, we have one. Yes. Okay. I wonder if Firedrasen will provide RPC interface like, one Ms. Lana client.
01:01:07.256 - 01:01:09.284, Speaker A: I'm sorry, I'm not getting the audio.
01:01:09.744 - 01:01:19.272, Speaker C: I wonder if fire density client will provide RPC interface like. Or it will be only about validation.
01:01:19.328 - 01:01:27.484, Speaker A: And RPC providers cannot here. If I come down because I'm getting a feedback loop.
01:01:34.824 - 01:01:35.880, Speaker C: There'S so much echo.
01:01:35.952 - 01:02:01.584, Speaker A: Yeah. Okay. So the question is, will firedancer provide an RPC interface like the Solana client. We intend it to be a drop in replacement at completion for Solana client. So the answer to that would be yes. A lot of the hardware that you guys are running on, actually, because of the thing. I'll just.
01:02:01.584 - 01:02:54.982, Speaker A: Yeah, they're slower speeds. Well, run on the fastest hardware you're willing to throw at us. But one of the concerns that was brought up to us in the past was, were we going to show up with the most bespoke hardware configurations that only the four richest kings of Europe could afford? And no, we are targeting essentially low end. And like a lot of this stuff, you could go very well in terms of support. Now, in terms of optimizing, we're targeting the higher end kind of things for optimization. So, like, in this demo here in the software side, you see that, you know, we got 1.2 kind of million transactions, and we were using, you know, 40 odd physical cores, 80 odd logical cores on a fairly old box, low clock speed, no turbo, things like that enabled.
01:02:54.982 - 01:03:55.794, Speaker A: Now you come along and say, no, no, no, I bought my, you know, ice lake, it's 40 dual socket. I've turned on the hyper threading, and you go up over there, you can make all those numbers twice as big. That's pretty easy. Now, I think the important message you want to get across, especially about the hardware demo, is with the kind of thing, you know, with much less footprint than buying a really beefy high end server and whatnot, a single FPGA card, and then only using like a 10th of the area on that FPGA card, we can get ten x performance over that. And so, looking at how expensive calculations like SiG verify are, that's why we're very strongly looking at the hardware acceleration. I think another thing here, which may not be completely obvious, because a lot of people aren't familiar with hardware and so forth, is that, you know, you can get high numbers off something like a GPU and whatnot, but they are really optimized at average throughput. So if you don't already have a really big batch coming in, you have to form that batch.
01:03:55.794 - 01:04:32.628, Speaker A: It eats a lot of system latency, and a lot of that takes away from the ability to use it. It makes the software hellishly complex and more bug prone and all that. So everything we're running here is in a non batched mode, including the FPGA, outside of the kind of like actual packing of the pack, as Kavi was talking about. And so if you're looking at those things, it's a lot simpler, a lot lower overhead. So those are actual numbers that you can attain much easier than you can by trying to integrate technologies like a GPU that aren't just really well designed for high performance network processing. Yeah. Oh, yeah, yeah.
01:04:32.628 - 01:04:58.104, Speaker A: Oh, yes. Sorry. Okay. You got a good question? Sure. Yeah, sure. So the question is, here is, a lot of the contracts are in rust, and then the LLVM parts, all of that's in component two runtime. Now, we've actually already implemented.
01:04:58.104 - 01:05:23.062, Speaker A: Kava could speak to this in a FPGA, a thing which can run the EPBF runtime as a thing in an FPGA. And so that's fine. But in software, that's fine, too. Like, we're going to implement a runtime like that. So any existing tools that you have, you could run the EBBF tools against that and do that. And I think as a more general thing, a lot of this low level code, we're using low level languages and.
01:05:23.078 - 01:05:23.526, Speaker B: A lot of stuff.
01:05:23.550 - 01:06:16.266, Speaker A: We're also doing tons of work on the formal verification and on the kind of code discipline stuff and whatnot I alluded to, and you can look at the code base to see that. But the big thing here is, we don't really care about languages, is as we pull into areas like the consensus areas, like the runtime areas, we have zero objections to running those in rust. The big things are when we're doing really low level interfacing, multi process, pneuma aware and all that. The thing we consistently run into is these languages aren't fully baked to kind of get down there, and we've been doing this for ten years. And then I think the other thing you look at, and this is more of a thing in terms of the discussions between Solana and whatnot, or independence of the existing validator. So we just don't like cut and paste the code and then just like scribble out anywhere. We see Solana run a sed and say, jump over here and then call it an independent validator that they didn't want us to use rust, they wanted us to use some other language.
01:06:16.266 - 01:06:46.034, Speaker A: Yeah, well, I know not everyone does. So actually, I hate c, but I hate all programming languages. It's just there's less to hate in aggregate than c. So looking at it, putting that together, it's like Solana approached a bunch of experts at high performance C C code development and said, you can't use rust. So it's like, okay, well, that was not the hardest decision for us to make in the world. So cool.
01:06:46.414 - 01:06:46.726, Speaker C: Yeah.
01:06:46.750 - 01:06:47.434, Speaker A: Thank you.
01:06:48.494 - 01:06:59.254, Speaker C: I wonder if you have performance benchmark test without NUMA and FPGH, just to compare runtime performance of Manila client. And finally.
01:07:01.354 - 01:07:20.022, Speaker A: We do have a lot of. Sorry. Yeah. The question is, do we have benchmarks to compare side by side for performance? The answer is kind of. And when I say kind of, kind of a lot of that, like, I can look at the openssl that we used as the reference. We're about, you know, to five ish faster than them. Somewhere in that ballpark.
01:07:20.022 - 01:07:57.740, Speaker A: If I run the same sig verify calculation, we can just swap out our implementation of it, put their implementation, and run it, and then see they're running at something for equivalent numbers at like the 160 microseconds. 120. It's a little bit difficult to give those benchmarks because, as per the other questioner, it's so computationally intensive, that thing, and so relatively memory poor, it really scales as. So if you're looking at it, you really care about clock speed. So you want to do apples to apples. You're saying, like, okay, there's the clock speed is the turbo mode set up and all those other things, but if you're looking at it, it's a ton faster for that kind of, like, micro benchmark side by side. Another one that's frequently used is Dalek.
01:07:57.740 - 01:08:14.236, Speaker A: Dalek. We've gone through kind of line by line. We're kind of comparable. I think the big thing there when we look at it is there's some algorithmic differences. So in some circumstances, we've seen us be a little bit faster. In some builds, we've seen it be a little bit slower. But Dalek that's currently used in the libraries is pretty darn good as well.
01:08:14.236 - 01:08:28.428, Speaker A: And we've been comparing against that and then looking at saying, like, hey, are there any tricks that we can gain from this? Instead of just turning loose our internal cryptographers and doing it de novo. Cool. Okay, I got the time warning, so.
01:08:28.556 - 01:08:31.644, Speaker C: Thank you very much. Give it up for the fire dancer team.
