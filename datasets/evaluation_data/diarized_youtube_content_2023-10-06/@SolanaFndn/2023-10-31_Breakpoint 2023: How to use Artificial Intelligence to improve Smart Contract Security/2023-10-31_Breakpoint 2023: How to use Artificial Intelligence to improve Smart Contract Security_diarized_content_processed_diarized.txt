00:00:01.160 - 00:00:23.994, Speaker A: My name is Chris Wong. For those of you who wonder that the guy on the agenda is a bald guy, right. And I do have hair. So just in case you wondered, I regrow my hair. Coming out of pandemic. I'm from sexray, a security research firm. Today my topic is AI.
00:00:23.994 - 00:00:56.082, Speaker A: In particular, new generation of generative AI and blockchain security. It's certainly a hot and pretty big topic. I'm certainly humbled by that topic. So first of all, I'd like to tell you what this talk is not about. This talk is not about what generative AI is like, transformer, tech stack, so on and so forth. Not about that.
00:00:56.138 - 00:00:56.894, Speaker B: Okay.
00:00:58.234 - 00:01:07.450, Speaker A: Second, today's talk is not about presenting a ready product. Not yet at least.
00:01:07.562 - 00:01:08.234, Speaker B: Okay.
00:01:08.354 - 00:01:55.034, Speaker A: And also today's talk is not about a success, journey or story, at least not yet. Today I'm going to focus on what we believe and sexually, we believe the opportunities, opportunities are and our efforts and the challenges and the things we have learned along our journey. Okay, so let me get started. First, for those of you who don't know about sex trade, I'd like to maybe just spend a few brief moment about us. We're a security research firm. Solana is the first blockchain. We build our product, we do a few things.
00:01:55.034 - 00:02:45.602, Speaker A: We do audits just like most security companies do. We do smart contract audit, we do front end audit, we do back end, we do web3, we do web two, we do all of them at the same time. In my opinion, it's more important to me is that we're building a suite of software products. Our goal is for every step of a dapp's lifecycle. Hopefully that we'll have a software product to feature needs. The story I like to tell is that this is, you don't really hear about like Microsoft or AWS talked about. They have just done an audit on their software, right? You don't really hear about that, right.
00:02:45.602 - 00:03:26.284, Speaker A: I think as things evolve security, they also, they do, they do code reveals and they do audit. But gradually as things software getting more complicated and more advanced, I think automated solution become even more important. That's what we're building in this moment. In particular, there's a few things that we have built and we're building right now. We built a very simple command line based vulnerability scanner called Soteria. We have since retired. It scans your source code, smart for smart contract.
00:03:26.284 - 00:03:46.312, Speaker A: It really targets five low hanging fruits such that your account is not validated after it came out. It has been downloaded so many times beyond our wildest imagination. On top of that, we built a software SaaS platform.
00:03:46.448 - 00:03:46.880, Speaker B: Essentially.
00:03:46.912 - 00:04:01.876, Speaker A: On top of that, it's called X ray. Essentially does the same thing either by uploading your source code to our platform or through GitHub actions. We expanded the scope as well. It's against about 50 type of vulnerabilities.
00:04:02.020 - 00:04:02.732, Speaker B: Okay.
00:04:02.868 - 00:04:22.952, Speaker A: And last piece we're building is called watchtower. It's really a on chain transaction security monitor. Essentially that's a data player. We take in transaction in real time and then we analyze that transaction to see if there's anything that worth alerting you for.
00:04:23.068 - 00:04:23.844, Speaker B: Okay.
00:04:26.824 - 00:04:39.896, Speaker A: I guess this is really hard to, not to read or hear about generative AI since late last year, right. Thanks to the huge success of Chaigbt.
00:04:40.040 - 00:04:40.752, Speaker C: Right.
00:04:40.928 - 00:04:57.340, Speaker A: And I looked up last night, 2023 year to day. So far there has been 15,000 pre print research paper on large models. Yes, you heard it correctly, it's 15,000.
00:04:57.412 - 00:04:58.148, Speaker B: Okay.
00:04:58.316 - 00:05:49.404, Speaker A: Just yesterday alone there was 69 papers about large language model uploaded. So this field is evolving very quickly. It's developing very quickly. Imagine every paper, there's a team behind it and there's a new finding behind it. So I suspect many of you have wondered how this new generation, new generation of AI is going to impact your product, your business and your future. We certainly answer have been thinking about that. So we came to the conclusion that this represents more opportunities for us to potentially leverage the capabilities of this AI and potentially redo all of our product to make them better.
00:05:49.404 - 00:06:16.434, Speaker A: So the first thing we did is that in May of this year, we released a model, we call it all LLM. It's an open source model, it's a web3 native model. I'll explain that a little bit. So rather than taking a foundational model out there, open sourced and built on top of that, we took a different approach. We actually built a model from the ground up.
00:06:16.554 - 00:06:16.890, Speaker B: Okay.
00:06:16.922 - 00:06:25.054, Speaker A: Block by block. I'll have a chance to talk about it, why we took that approach.
00:06:25.794 - 00:06:27.814, Speaker B: Oops. Okay.
00:06:28.674 - 00:06:55.114, Speaker A: It's actually a small model by a large language model standard. It barely clears the bar. 100 million parameters barely clear the bar to be called a large model. And it's designed to be working across different chains. So it's specifically designed for blockchain. It pre trained on blockchain transactions. It's designed to handle blockchain related tasks.
00:06:55.414 - 00:06:58.234, Speaker B: Okay, so.
00:07:00.454 - 00:07:16.230, Speaker A: Why generative AI can be helpful in blockchain security space, right? I think it's fair to say that our current landscape in blockchain security are mostly heuristics based, okay. Heuristics are great.
00:07:16.302 - 00:07:16.630, Speaker B: Okay.
00:07:16.662 - 00:07:38.882, Speaker A: Heuristics mostly are rule based, right. You, based on your past knowledge, you figure out some rules, right. It is in situations where, when you're dealing with a large, very complicated system, very complex issue, a heuristic based approach can be very practical. Many times it can get the job done.
00:07:39.058 - 00:07:39.778, Speaker C: Right.
00:07:39.946 - 00:08:17.204, Speaker A: So I had a list of other things that we actually do today, for example, auditing and human auditors looking for patterns, looking for things that, based on their past experiences. Our x ray software is another example, right. Our x ray software is largely built on heuristics. We cover about 50 types. Security issues. All those issues, we have seen them before, and we figure out what the signatures are and we build those signatures into our software, right? That's really heuristic based. Those are great.
00:08:17.204 - 00:08:47.240, Speaker A: They get a job done, especially when you don't have a full understanding or system. Just so complicated. But they're drawbacks. Number one, they require extensive human effort. Just in the case of audits, right. You have limited number of best auditors out there, right. It takes a lot of effort for them to figure out those rules, to figure out those patterns.
00:08:47.240 - 00:09:41.496, Speaker A: Our x ray takes a lot of human efforts, right. And many times those systems need to be fine tuned for different applications. In my previous life, I was a software engineer, I designed an algorithm for vehicles, right, safety systems. I remember one of the things that I really painful was sitting in the very cold winter, sitting in those vehicles trim, what we call a trim is really adjusting the parameters for heuristics to make that algorithm work for different vehicles. It's just very, very painful, very time consuming. So it takes a lot of human effort because of that, it's really difficult to scale. And also, many heuristics are derived from one particular situation.
00:09:41.496 - 00:10:07.094, Speaker A: By definition, they don't have a wide application. So you have seen a lot of security patterns in the past. You train your algorithm for that. But when you see a new thing never seen before, right, a new challenge, many times it's less efficient and less effective. Of course, it cannot. In general, it cannot be generalized. So one particular heuristic works in one situation.
00:10:07.094 - 00:10:44.384, Speaker A: So what are the alternatives? Apparently, using machine learning or deep learning for security analysis is another route. This is not new. People have been working on this approach for a long time. In our own case, our watchtower actually uses machine learning as part of the functionality. We have a so called smart bot. We actually learn from your historical transaction as we monitor your smart contract. We try to learn from your historical transaction, try to establish what is normal.
00:10:44.964 - 00:10:45.492, Speaker B: Okay.
00:10:45.548 - 00:10:50.668, Speaker A: It's not always right, but we try to learn from that and figure out when things are not normal.
00:10:50.756 - 00:10:51.384, Speaker C: Right?
00:10:51.724 - 00:10:59.664, Speaker A: So the question, obvious question is that how can large models making a difference here, right?
00:11:02.324 - 00:11:03.144, Speaker B: Oops.
00:11:04.164 - 00:11:36.208, Speaker A: We believe there are a few things, number one, in the last few years that the evidence and the research have pointed out to us that especially in the case of natural language processing and computer visions, that large models are pretty good, much better than traditional machine learning. In summarizing rules. Okay, so I talked about that it takes a lot of human effort many times to figure out the rules, right? Large models actually can help on that. They have been proven to be pretty good on that.
00:11:36.296 - 00:11:36.904, Speaker C: Right?
00:11:37.064 - 00:12:19.254, Speaker A: They can find corner rules, very hard rules to find for human being to find. It also can generalize. This brings to my next slide that emerging capability that did not exist in the traditional machine learning or deep learning model is the ability to have analogical reasoning capabilities, right. Analogical reasoning capability is really that, it means that being able to appreciate the underlying deep similarities between two sibling.
00:12:20.994 - 00:12:21.306, Speaker B: On.
00:12:21.330 - 00:13:20.754, Speaker A: The surface, two seemingly quite different things, but be able to make that deep connection and then learn from the previous knowledge and the patterns and the things, it becomes so critical in these applications, we believe. So if you leverage all these capabilities that at least promising from large models, essentially all the things we talked about so far, the drawbacks in hero heuristics, the drawbacks in traditional machine learnings, the larger models can potentially do better job, right? So, so far, I think the promises we have been talking about are largely in general, it can apply to many applications. But why is so important for blockchain or solana in particular?
00:13:20.874 - 00:13:21.514, Speaker C: Right.
00:13:21.674 - 00:14:08.334, Speaker A: We believe there are a few reasons. Number one, solana transactions in many aspects are quite similar to human language. They all exist in a limited space. This is in contrast with some image, some other type of data. They exist in a limited space and they are discrete in states. By saying that, by no means I'm trying to diminish the challenges. Solana programs in smart contract transactions are still very, very complicated with other parameters, with other function calls and so on and so forth, they can be extremely complicated and challenging.
00:14:08.334 - 00:14:14.426, Speaker A: This is all on relative basis. But in this aspect, it's very similar to natural language.
00:14:14.530 - 00:14:14.938, Speaker C: Right?
00:14:15.026 - 00:14:41.144, Speaker A: If large language model can handle natural language pretty well at this stage, that our imagination is that it should be able to add value here as well. Also, data is so important for large models. You need to have the best data, you need to have more data, you need to have quality data and blockchain. Naturally, data is out there, at least raw data for anybody who wants to work on that, at least it's open.
00:14:41.224 - 00:14:41.844, Speaker C: Right?
00:14:42.784 - 00:15:15.088, Speaker A: And thirdly, as a security company, we like to argue security is so important. It's a lot more important in this case if you can crack the knot here. So what is our approach again? Our first effort is a proof of concept foundational model called all. I'll explain that a little bit. I touched base a little bit early, but here we built from the ground up. It's using the same transformer tag stack. It's trained on transaction data, nothing else.
00:15:15.216 - 00:15:16.004, Speaker B: Okay.
00:15:16.424 - 00:15:25.272, Speaker A: So rather than try to understand human knowledge, human language, human tags, this one is specifically designed to understand blockchain transactions.
00:15:25.408 - 00:15:26.244, Speaker B: Okay.
00:15:27.504 - 00:16:11.054, Speaker A: To prove the concept we actually did, out of convenience, our first downstream task is using that pre trained RLM large model. And try to pick a task is though, can you tell me which transaction is the MEV transaction on this particular dataset? I don't claim that works across the board. It's all about data. So on this particular data set, it proven to be pretty accurate 95% of time. We continue to work on this model. It's an iteration process. Right now, we're building a much bigger, there's a lot more parameters and pre train on a lot more data.
00:16:11.054 - 00:16:23.626, Speaker A: This brings to the watch tower. Remember I talked about the watchtower? Remember the opportunity that we're thinking that to redo every of our product leveraging large models.
00:16:23.690 - 00:16:24.226, Speaker C: Right.
00:16:24.370 - 00:16:50.444, Speaker A: Watch tower, it seems to be an ideal candidate for us because rather than using traditional machine learning or heuristics to tell you, is there something might be wrong that worth alerting you now we can use larger models to do the job we're in the process of doing that. I'd love. I wish I could tell everybody that we have a Reddit product we don't have yet.
00:16:50.524 - 00:16:51.144, Speaker C: Right.
00:16:52.364 - 00:16:57.436, Speaker A: But more here, I'd like to talk about the challenges we're facing. It's turned out to be quite challenging.
00:16:57.500 - 00:16:58.084, Speaker C: Right.
00:16:58.244 - 00:17:09.584, Speaker A: So all we see this is, we read on the news, like AP OpenAI, they are so successful. Everybody is so, so successful. But very few people are talking about the engineering challenge, nitty gritties, how they get there.
00:17:09.624 - 00:17:10.204, Speaker C: Right.
00:17:10.584 - 00:17:40.372, Speaker A: So there's a lot of pitfalls or a lot of challenges that I like to share with people here that, you know, some of the challenges we had, you know, first is data preparation. Even though the data, raw data, is open out there. But it's not an easy task collecting those data, parsing the transactions especially for Solana, right. To parse Solana transactions to convert those long hacks into something meaningful.
00:17:40.428 - 00:17:41.024, Speaker C: Right.
00:17:41.684 - 00:18:16.504, Speaker A: Similar. Or you can make analogy to human language. It turns out to be not an easy task on this. I'm very happy to report that we team up with a great team, Solana FM. And we're also very grateful to be awarded a Solana foundation grant to parse all the historical transactions. And hopefully in the next few weeks we'll have a data set fully parsed out there for everybody. It's going to be open source for everybody who wants to build in the AI space to use.
00:18:16.504 - 00:18:33.300, Speaker A: There are also challenging tokenization. You need to have a vocabulary, you need to have a dictionary that transfer all those parcel transactions into tokens so the machine can be trained upon.
00:18:33.372 - 00:18:33.940, Speaker C: Right.
00:18:34.092 - 00:18:36.476, Speaker A: And there is a balance you need to strike.
00:18:36.540 - 00:18:37.144, Speaker C: Right.
00:18:37.524 - 00:19:10.500, Speaker A: Doesn't matter which way you tokenize it, you're going to lose some information. And that vocabulary. In theory it can be very large, right. But you don't want to be too large because the larger that vocabulary list, the more difficult for the model to be trained and it's more costly. Also, there's a pre training. For the pre training, you want to make sure your pre training doesn't break all the things you have learned. And also you want to make sure the training converges.
00:19:10.500 - 00:19:41.868, Speaker A: You want to make sure indeed develop that intelligence you're looking for, right. That capability to be able to reason. I think I'm right out of time, so let me directly jump to those last slides. The progress so far. Okay, this is the result is from a few weeks back. So we're training on a new set of data. This time the task is really picking out which transaction is malicious out of this new data set.
00:19:41.916 - 00:19:42.504, Speaker C: Right.
00:19:44.874 - 00:19:54.026, Speaker A: The position a few weeks back that we have achieved is about 70% and the false positive is about 8%.
00:19:54.130 - 00:19:54.898, Speaker B: Okay.
00:19:55.066 - 00:19:59.962, Speaker A: Both are not ideal, both are less than what we wanted to achieve. But it is an iterative process.
00:20:00.058 - 00:20:00.694, Speaker C: Right.
00:20:01.314 - 00:20:27.474, Speaker A: I guess we stay very hopeful that I think this is really generative AI represent a great opportunity for blockchain security. I think in the next few months we'll continue to work very hard on bringing this to the market. All right, thank you very much for listening. I appreciate having this stage. Thank you very much.
