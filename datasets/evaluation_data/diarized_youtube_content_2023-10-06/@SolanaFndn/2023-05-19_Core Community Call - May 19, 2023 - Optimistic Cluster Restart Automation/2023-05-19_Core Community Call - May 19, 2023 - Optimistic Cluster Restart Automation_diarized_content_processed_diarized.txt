00:00:00.400 - 00:01:05.814, Speaker A: So welcome, everyone, to this month's core community call. There is a couple things on the agenda. So the first thing we're going to hear from will, before you win, will, about the upcoming release changes that they have or that they're pushing, and then we'll hear from win on SIMD 46. Before I get started, I wanted to first call out that like, if it, if you ever want to see the agenda for the meeting, you can always go to the core community call repo, which I just linked in the chat. And then there's two SIMD's that would be, that are getting close to consensus on. So it'd be great if people could find time to give your review on them. The first one is 33, which is the timely vote credits, and then the previous one, which we actually already merged in but would love to have any extra eyes on it would be the 15, which is partition epoch rewards.
00:01:06.234 - 00:01:06.906, Speaker B: And.
00:01:07.050 - 00:01:09.322, Speaker A: Go ahead, will. You can go ahead and say your.
00:01:09.338 - 00:01:12.074, Speaker B: Spiel real quick, will.
00:01:12.114 - 00:01:30.214, Speaker C: Yeah, thanks. So, as you probably all know, the upgrade of mainnet to one for 1417 is underway. We asked for 25% beginning of this week. We're currently at 27%. Thank you. We appreciate the overachievement there. Everything's looking good.
00:01:30.214 - 00:02:08.406, Speaker C: Our plan is to ask for general adoption at the start of next week, so you can anticipate that request. In case anyone hasn't seen it yet, I would encourage you to read the outage report from the February outage. At the time, it was widely believed that that outage was caused by the 114 upgrade. I'll drop a link to that report in just a moment in the chat here. It's also linked in the MB announcements channel on Discord. We also have some audit reports that we'll be publishing later today related to 114. Feel free to peruse those.
00:02:08.406 - 00:02:23.424, Speaker C: There's nothing scary in them. If there were, we wouldn't be shipping the release, but it might give you some peace of mind and some interesting reading. So yeah, thanks everyone who's already upgraded and looking forward to getting the rest of the cluster on 114 soon.
00:02:27.284 - 00:03:00.038, Speaker B: All right, wen, you're up. Okay, let me share my slides first, then I can present it. Slideshow and share screen. Sorry, what? I share screen. Oh my. So, hello to those who I haven't met yet. I'm wen, I joined Solana Labs last September and personally I'm working on consensus.
00:03:00.038 - 00:03:37.974, Speaker B: Personally, I'm interested in, like, high performance and high reliability system design. So how come I don't see, I don't see my slides to share oops, no, sorry. Why don't I have my slides to share? I have it previously, and let me do it again. Sorry for wasting time. I don't see my slides to share. I can share my whole screen. Maybe that'll work.
00:03:37.974 - 00:04:04.488, Speaker B: Okay. Do you see the slide now? Yep. Okay, sounds good. So, motivation and requirements. We are interested in making Solana more reliable, of course. So for high reliability, there are many things you can do. First of all, you can write very high quality code.
00:04:04.488 - 00:04:51.412, Speaker B: Then you have less outages, but no code is ever perfect. So you need testing to improve it. But testing couldn't catch all problems, so you need monitoring to tell you what problems you have. And when monitoring tells you there is a big outage, you kind of need outage handling to get the cluster back to a same state. So there are a lot of efforts inside Solana to improve code quality or testing or monitoring and other stuff, and auditing and formal verification. And today, the proposal is only about outage handling. So, first of all, let's look at how we handle outages now.
00:04:51.412 - 00:05:55.664, Speaker B: So when I'm talking about outages, I'm talking about, like, the last outage where the whole cluster just couldn't make routes anymore. It's not making progress. And so somehow what we needed to do is we need to restart validators, give them a same state to start with so the cluster can continue function again. So this is called cluster restart, which I have a link here. It's very different from, like, sporadic single validator restart, which happens all the time, and that doesn't impact reliability at all normally, because you still have a lot of validators functioning. So what we do now, first of all, we would try to find the highest optimistically confirmed bug. So optimist confirmed means you have a block which got the votes from majority of the validator when we use two thirds here.
00:05:55.664 - 00:07:02.926, Speaker B: So when the block is optimistically confirmed, ideally we shouldn't row it back because it may contain user transactions. If you roll it back, it will have economic impact, very big economic impact. So the whole design goal in this proposal is to, to try not to roll back, optimistically confirm block if possible. So in reality, today, we also try to do the same. But since today we don't have really autonomous process to do this, today we use what we call social consensus. So when there is outage, and people would gather in the discord channel and they would first confirm, yes, there is outage, all the validators are not making progress, then they'll try to, if they decide to go for restart, because the cluster doesn't seem to be recovery, then they'll try to see where to restart from. So we need to have one block which everyone restarts from.
00:07:02.926 - 00:08:03.796, Speaker B: And to make sure we're not rolling back user transactions, we need to find this highest optimistic confirm block where we agree we will start with. And what we do now is we would normally do a consensus or voting in Discord channel. Say my local confirm block is x, and what do you see? So if most people vote for x, then go with x. So that's how we do this now. And after that, after we decide which block to start from, the validator monitors, operators would stop the validator. And sometimes if there is a bug which could lead to like outage again, then we might install new binary, but that doesn't happen very often. And next, because we decided we need to start from this block.
00:08:03.796 - 00:08:37.496, Speaker B: So everyone needs to have the same block. So you would create a snapshot with a hard fork at this block we decided on. And if you don't have that block locally, you would download it from a trusted source. And after that you would restart validators with the following arguments. Two arguments. One is wait for super majority at slot x. And next is, I think the bank hash at this slot is blah.
00:08:37.496 - 00:09:58.294, Speaker B: So this makes sure you restart with the correct snapshot, you have the correct block and correct hash, and then you would wait for 80% of the people to reach the same state as you. Then the whole cluster begin to function, you start to make new blocks again, and you start to vote and all things go to go normal. So there are a lot of problems with this current restart process. Maybe the biggest problem is it takes a long time. So the whole cluster restart takes about, it could take about like several hours, which would make your reliability not very good. So in the future we might have other efforts to make it faster, but today we're just trying to solve a small problem. So we are trying to improve the process of finding highest oblivious confirmed block, because it's really hard for human to poll like 2000 to 3000 validators and see what each one voted for and then decide what the block is.
00:09:58.294 - 00:10:46.482, Speaker B: This is really a job better for machine. So we're trying to see whether we can develop a protocol so that the machines can automatically find the highest confirmed block without human intervention. And so, oops, go back. Okay, that's fine. So the main design goal here is not to have cross pot negatives. That means if a block was confirmed before the restart, try not to roll it back because that will have very, very bad impact, and. But false positives might be okay.
00:10:46.482 - 00:11:31.168, Speaker B: It means maybe some block is not confirmed before the restart, but somehow we think it is confirmed, which is fine. Let's say there's a. So our. Our limit is at 67. Let's say there is a slot which got 66% of the votes, and there are no competing blocks. So this is the only one we know which got 66% of the votes. So it's okay if we, the 80% of the validators in restart, decide that we should start from here, even though it's not confirmed, it's okay to confirm more, but it's not okay to roll back, like confirm block.
00:11:31.168 - 00:12:33.454, Speaker B: So, in all the choices we have after this, we will see that we try to not have false negatives at all and force positives might be fine in the design choices. So the new approach, we think, because we now want to have machines automatically negotiate, what the. What is the block we want to restart from? Sorry, let me turn that off. So we would still have human in the room, they would still restart validators with an argument, let's say optimist restart, but they don't need to tell you which slot and which hash they need to start from. That's for machines to decide. And we would add additional phase. The silent repair phase here is where the validators negotiate well themselves.
00:12:33.454 - 00:13:17.714, Speaker B: What is the block they should restart from? So, because they are in this repair phase, no one would make new blocks anymore. So there would be no turbine traffic at all. The goal here is to just converge as soon as possible so nothing changes here. Every vote should be, you should still stick with the vote you had before the restart. Don't change your votes. So the current proposal, we would use gossip to exchange most information in this phase. And because people are worried, because we do restart, people do restart.
00:13:17.714 - 00:14:09.074, Speaker B: Like, some people restart earlier, some people restart later. So in the cluster, it's possible to have some validators in restart and some which are not. So we are worried that these two gossip message will pollute each other's internal data structure. So here we would probably use a new shred version to form a new gossip group. So the validators in restart talk among themselves while the others form a different gossip ring. And we would send two new gossip messages. One is last voted for slots because the goal here is to have everyone share the same metadata so they know, so they repair the same data blocks.
00:14:09.074 - 00:15:03.954, Speaker B: So at the end of silent repair phase, everyone should have the same data and same metadata so that they can make the same decision on the same block. And start from here. So last voted fork slots is where we share the last vote before the restart. And because, like, validators might be on different slots, like, some people vote faster, some people vote slower. So just one last vote slot is normally not enough. So we would send 9 hours of slots on the same fork so that people can get the whole fork, people can get a better view of the metadata. And also after you repaired everything and you have all the metadata, then you send out your new vote.
00:15:03.954 - 00:15:54.110, Speaker B: We don't call this vote to distinguish between normal vote and this vote. So here we call it havis fork, which is actually just a vote. So you send out after all the repairing is done, you have automated data, what you think, where we should restart from. And also we, you also send out how many heaviest work you received from other people. Because we need to decide when we would exit this silent repair phase. After we see that 80% of the peers received this heaviest fork message from 80% of the people, we check that everyone agrees on the same block, same slot, and hash. If yes, then we exit this phase and proceed to real restart.
00:15:54.110 - 00:16:20.474, Speaker B: So we do what we currently do. Otherwise, if anything happens that makes you can't proceed, just stop, print all debugging information and hot. And so human can intervene or maybe switch back to the old restart method. How much time do I have? I'm probably a little bit slow.
00:16:20.634 - 00:16:25.934, Speaker A: Yeah, we have eleven minutes for both finishing questions, but we can see how far we can get.
00:16:26.554 - 00:17:31.700, Speaker B: Okay, that's fine. So I put all the information, I put all the details in the SM and e, and I put more details in this slide. So you are welcome to read the slide. So I'm just going to generally introduce the silent repair phase and then we can go proceed to questions. So, first of all, when you restart a validator with this new argument, immediately send last voted fork slots, which is what you voted last, and all the slots on this fork. And after that, everyone aggregates the last voted fork slots from all the other restarted validators. And you could start repairing a slot if you think this slot could potentially have been confirmed before the restart, and we could draw a line somewhere to say, these are the candidates which could have been confirmed before the slot and the other blocks, I don't care.
00:17:31.700 - 00:18:12.694, Speaker B: So you repair all the blocks, you care. And after repair repaired all of them, you aggregate all the last votes in the last voltage fork slots and choose your heaviest fork. And so I think we are at 20 minutes now. I don't know whether I've introduced the new method enough so everyone has good grasp. But we could, we could see if anyone has any questions at this point or I can proceed if no one has questions.
00:18:13.514 - 00:18:26.184, Speaker A: Does anybody have any questions currently on the current approach? Or do we, should we just continue? All right, go ahead and continue then when?
00:18:26.964 - 00:19:21.024, Speaker B: Okay, so to exit silent repair phase, I think the most, one of the most important thing in outage handling is to make sure everyone is on the same page. Otherwise you think everyone's on the same page, you single handedly enter the restart and start making new blocks. And while everyone still is repairing blocks, that would be disastrous and another eye audit might be happening. So here we are very careful. When we exit the silent repair phase, we would count whether enough people are ready for action. So the current check is whether 80% of the validators receipt the heaviest fork response from 80% of people. So we cut 80% here because this is a current.
00:19:21.024 - 00:20:08.104, Speaker B: This is the current line we draw when we do a restart. We wait for 80% to join the restart, then we proceed. And also, even if we see that 80% of people got response from 80%, we also linger for. Maybe we haven't totally decided the numbers here. We would say linger for two minutes because gossip message propagation takes time. So linger for two minutes so that my heaviest fork, which contains how many response I got, can read everyone and then you can perform security checks. Check one is did everyone agree on the same block, which means same slot and same hash and whether.
00:20:08.104 - 00:20:57.210, Speaker B: So I also have my local optimistic confirm block before the restart, right? So because my local confirm block means before the restart, I saw two thirds of the votes on this block already. So you would imagine this block should be on the selected fork. So it should be the ancestor or it should be the selected block. If it's not the case, then the security check fails because something's wrong. My local confirm block is getting wrote back. Then you would also exit and halt and wait for human inspection. If every check succeeds, then perform the current restart logic.
00:20:57.210 - 00:21:46.924, Speaker B: We probably will clear the gossip crds table so that we don't carry the old restart messages into the new environment. And then we would automatically start snapshot creation. In contrast to what we are doing today is we manually ask people to manually do it using ledger here we might start earlier. Once once we find out everyone agrees on the same block, then we execute the same logic. What we are doing now in these arguments, I have a few links. One is a Google Doc. I started and this proposal was just starting.
00:21:46.924 - 00:22:25.296, Speaker B: It contains many, many details and a lot of designs. We rejected. And why? Because I'm currently modifying both the Smid draft and this Google Doc. So this Google Doc might be outdated and so some design choices might be different. If that's the case, SmId is the newest proposal and the actual Smid draft here as well. And this is why I'm asking whether you have any questions again. So I haven't decided, I haven't described the details of the algorithm.
00:22:25.296 - 00:23:01.944, Speaker B: I have backup slides at the end to talk about. There are basically two algorithms. One is how we calculate which blocks I should repair when I have all the heaviest slot, last voltage fork slots, and the other is how do I select a block in the heaviest fork. So these two algorithms are in the Smids. And I also have some description here. Let me know if you have questions.
00:23:02.804 - 00:23:03.984, Speaker D: Can you hear me?
00:23:04.444 - 00:23:05.316, Speaker A: Yes, we can hear you.
00:23:05.340 - 00:23:05.956, Speaker B: Yep, yep.
00:23:06.020 - 00:23:28.634, Speaker D: Yeah. So actually I'm from Mango team, and like, we are currently implementing SMD 47, which is like the last restore slot. So I'm wondering if this silent repair stage should come also, as should be considered also restart slot, because silent repair can.
00:23:29.934 - 00:23:48.066, Speaker B: So sorry, I haven't checked that document in detail. So I think your proposal, correct me if I'm wrong, your proposal is to expose the last restarted slot somehow through ledger, right?
00:23:48.210 - 00:23:49.334, Speaker D: Yeah, exactly.
00:23:49.794 - 00:24:08.740, Speaker B: Yeah. So here, I think it's orthogonal, but once the silent repair phase is over and we know where we are restarting from, we could also like connect to your code and expose this information somewhere, right?
00:24:08.892 - 00:24:19.412, Speaker D: Yeah, exactly. So like silent triple should be considered as a restart, right? Because it may take time to restart the slot silently, right? That's my question.
00:24:19.588 - 00:24:46.344, Speaker B: So when it's, when it's in the silent repair phase, we don't know for sure it's a restart. They are negotiating, but they don't know whether they can decide on the same block. So in that case, I don't think we will expose anything. And as that case, that phase is over and we know we are really entering the restart, then we can expose that restart slot. Did that answer your question?
00:24:47.204 - 00:24:49.164, Speaker D: Okay, yeah, it makes sense.
00:24:49.284 - 00:25:01.464, Speaker E: There shouldn't actually be any changes to that proposal, because we're going, once we've committed to restarting at the coordinated restart slot, there's going to be a hard fork anyway. And so that'll update this as far.
00:25:02.084 - 00:25:02.516, Speaker B: Okay.
00:25:02.540 - 00:25:03.504, Speaker D: Okay, that's cool.
00:25:04.444 - 00:25:10.424, Speaker B: Yeah, so that works nicely together. Any other questions?
00:25:14.244 - 00:25:18.956, Speaker A: You have a question? Zan Petsu, I unmuted you, so you can.
00:25:19.100 - 00:25:21.196, Speaker E: So I just. Can you hear me okay?
00:25:21.300 - 00:25:21.764, Speaker B: Yep.
00:25:21.844 - 00:26:10.508, Speaker E: Okay, sure. So as a validator operator, I want to say it's very encouraging to see how much really good thought design seems to have gone into what you're proposing here. I predict that what would happen during another restart event is that there will still be humans that will get alerting and there'll be a lot of confusion and talk. So, and if we're kind of racing some automated process, just keep in mind that you may want controls there as certainly messaging from the validator to sort of let us be very aware of what's going on so that we can make decisions about, oh, you know, because if someone believes that there's a bug or a reason that the restart shouldn't proceed, allowing something to run away with, you know, a whole cluster restart that we want to pause, just having controls and informational messages to help us understand what's happening are very important. I just want to emphasize that.
00:26:10.636 - 00:26:32.668, Speaker B: That's all. Yes, yes. I think maybe later there will be, I would try to get more feedback from the operators because this really impacts how you operate during outage. Right, right. So it helps to get more feedback there. So I think, first of all, I totally agree. The current approach is opt in.
00:26:32.668 - 00:27:11.550, Speaker B: If you don't restart your validator with that flag, nothing would happen. We would keep the current approach, if you feel more comfortable with that. And also, of course, the outage handling is mostly to assist people, it's not to replace people. And we'll, of course, give ways to inspect what's happening inside and ways to decide, no, this automatic restart is not working. I should do something else that's totally doable. It will all be command line controlled. Did that answer question? Yes, it did.
00:27:11.550 - 00:27:13.794, Speaker B: Thank you. Sure.
00:27:14.174 - 00:27:29.634, Speaker A: Thank you, everyone. Thank you, Wen for presenting today. If there's anybody else that wants to ask further questions on the sim simD I posted in the chat, we can take the discussion there. And thank you all for joining another core community call. You all have a good month. Thanks.
