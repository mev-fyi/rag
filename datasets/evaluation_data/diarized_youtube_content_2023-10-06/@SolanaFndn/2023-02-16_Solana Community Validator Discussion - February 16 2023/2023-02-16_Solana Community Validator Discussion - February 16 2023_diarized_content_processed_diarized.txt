00:00:01.600 - 00:00:30.936, Speaker A: All right, welcome everyone to the Solana validator community discussion. February 16, 2023. Today we're going to talk about a few things. We've got some updates on validators. There is a issue with high disk IO on 114 that's not a blocking issue. So for the curious, there's details in a discord channel that I'll mention. But again, it's not blocking the 1.14
00:00:30.936 - 00:00:51.844, Speaker A: rollout right now. And then we'll finish this call up with some updates on Firedancer that I'm excited to talk about. So if you haven't been keeping up with fire dancer, a lot of really exciting stuff going on there. Let's get into it. Okay. First up, Testnet 115.2 is now the recommended version for Testnet.
00:00:51.844 - 00:01:22.620, Speaker A: Beginning to roll out on Testnet in general. So the minimum requirement for delegation program is 1.15. Pretty soon we should have close to 100% on a 1.15 in Testnet. For those of you who haven't checked out the release notes, I'd highly recommend it for 1.15.0. Gives a really good overview of all the major changes that are coming up in 1.15. I think labs engineers did a really good job of organizing it and kind of giving a lot of high level important points for each section of the validator.
00:01:22.620 - 00:01:51.832, Speaker A: So again, highly recommend checking out this link 1.15.0, which has the most detail on different sections that change in 1.15 for the validator on Mainnet. The 113.6 release is still recommended for Mainnet, but we are rolling out 1.14. So right now, 13% of stake is on 1.14. There should be a call relatively soon to get 25% of stake on 1.14.
00:01:51.832 - 00:02:26.124, Speaker A: Again, it's not called for yet, but I think it'll happen soon. And if you're already on 1.14 or will be upgrading when the call goes out, make sure to monitor your node closely and report any issues that you see. Again, the idea is to hopefully get the whole cluster on 1.14, uh, soon. However, this rollout goes, um, part of the issue that we've been experiencing lately that slowed down the 1.14 rollout is a high disk IO.
00:02:26.124 - 00:02:48.174, Speaker A: It seems to be happening on startup. So when you restart your validator on 1.14, you might have experienced this. Uh, there's a discussion of this in the Discord Channel debug disk I o catch up issue. So sort of a TLDR. This flag disable accounts disk index is the likely fix. For now.
00:02:48.174 - 00:03:02.614, Speaker A: There should be a new release out soon. That makes that the default. So you don't have to add this flag. But if you like, you can just add the flag. Now if you're experiencing long startup times and high disk I o. Again, this should not be a blocker for the 1.14 rollout.
00:03:02.614 - 00:03:58.734, Speaker A: Just making this flag disabled future release will hopefully mitigate that issue. Any questions on, on the 1.14 update or any testnet questions? Yeah, so I should mention that Zantetsu has a question. Disabling disindexes does use more memory, so if you're experiencing high memory usage after this, you'll essentially have to experiment with it on your own. This particularly affects people that have both accounts and ledger on the same disk. So if you have two different disks, one with accounts and ones with Ledger, potentially you're not running into this issue. But for more details, check out this debug disk I o, catch up, Discord channel, and experiment on your own.
00:03:58.734 - 00:04:12.854, Speaker A: Does a high disk IO extend beyond startup? I don't believe it does. If one of the engineers on the call wants to comment, I pretty sure it's just a startup issue, but.
00:04:19.634 - 00:04:21.374, Speaker B: Sorry, I was eating lunch, but.
00:04:24.554 - 00:04:24.914, Speaker C: The.
00:04:24.954 - 00:05:17.714, Speaker B: Contention that we observed, or like the, the highest point of contention where we're seeing like, I zero eight, like things are still now for disk operations is only a startup. We have like some graphs in that channel where you can see like, the node will stall out or not stall out, not stall completely, but it won't be moving as fast as the cpu's good. But after startup, like after unpacking snapshot and generating the index that that ends and I guess something else. Comment about memory this, the accounts index was in memory in 1.13. So essentially we were trying to move things to disk to be more ram friendly. We see the high contention when everything is on one physical volume. So this is kind of reverting back to v 113 behavior in the sense.
00:05:24.234 - 00:05:43.682, Speaker A: Thank you, Steve. Any other questions? Yeah, great info. Thank you. I echo Zantetsu's comment. Okay, cool. All right, so the remainder here is going to be a fire dancer update from the firedancer team. So I'll let you all take it away.
00:05:43.682 - 00:05:47.998, Speaker A: Looks like yippee yappy is the one.
00:05:48.046 - 00:06:08.334, Speaker C: Yeah. Thank you very much. I hope you can hear me right. Yeah, excuse the silly name. I joined from a meeting room, and we have also generated names for that, and I don't really know how to change that. Anyways, thank you for the opportunity to speak. I want to preface that with what I want to say is no official guarantee of any timelines.
00:06:08.334 - 00:07:45.744, Speaker C: I just want to give you a kind of snapshot of where we're currently at and things that may be interesting to the validators on this call. Yeah, I guess I'll start off with the things we've delivered so far. So if you haven't watched our presentation of the milestone one pointer, two demo yet, I'd encourage you to take a look. This is the first, and so far the only actual component that can be deployed with a live validator, and that involves the replacement of the transaction in this stage with optimized stage for signature verification and some heuristic algorithm to schedule transactions before they actually land in the bank to kind of optimize the behavior during a large part of incoming transactions. While you are a network leader on the currently work in progress side, we have a large number of ongoing fronts so far, and we've seen the team grow quite nicely. The first one and the next one to deliver is probably a reputation of the quick protocol. So I think like half a year ago or so, I don't remember the exact time the Solana protocol switched over from UDP to quic as the transport protocol for the transaction protocol, which is used to get unconfirmed transactions over to the leader.
00:07:45.744 - 00:08:59.116, Speaker C: And of course, we have re implemented that protocol in fire dancer. It is mostly working so far, but the plumbing is just missing to have it in a production ready state yet. So I think during this exercise of implementing this protocol, we've also identified a good number of inefficiencies with this protocol. For example, mandatory encryption of packets, you don't really notice any performance loss at ten gigabyte line rate or so, but if validators want to push the performance of that network further, for example, on development networks, and that might become an issue in the future. And just things like the packet encodings are slightly overcomplicated, and we hope to send a few SIMD's up that improved that suggest changes to the quick protocol, though that might have some implications, because that is already a standardized protocol, so it might have to go up the IETF track. And I also see there's a bunch of questions, if you don't mind, we can cover them later. And since I'm also joining from this meeting room thing, I can't really see the questions.
00:08:59.116 - 00:09:09.784, Speaker C: So, Tim, I might have to ask you to read me these questions later on. All right, next. Currently, sorry, go ahead.
00:09:10.764 - 00:09:21.470, Speaker A: Oh, there's one question that came in. So Zantetsu is asking, is the operating system going to be Ubuntu 20.04 or where there'll be another operating, excuse me.
00:09:21.502 - 00:10:04.654, Speaker C: Operating system required financer has no dependencies on the operating system except for the kernel version. And I believe that Ubuntu 20 and 2204 should be near enough. But we mainly use RHEL and, you know, kind of fit all our ecosystem. I would just encourage you to check out the file answer tree and just run make. I think there are next to no dependencies currently, so I would expect that to succeed on Ubuntu 20. If you see a failure, please put up a GitHub issue and we'll address this.
00:10:07.594 - 00:10:16.154, Speaker A: Next question. Is, is it possible to do benchmarking against hardware at this time? I guess in testnet, I suppose.
00:10:16.974 - 00:10:25.434, Speaker C: Oh, so is the question benchmarking on like a specific piece of partner, the Solana labs implementation versus the finance implementation?
00:10:27.014 - 00:10:38.868, Speaker A: I guess maybe MCF, if you can elaborate there, fire dancer. Yeah, so I think he's just looking to test out firedancer on his own, I believe.
00:10:38.966 - 00:11:13.204, Speaker C: Okay, yeah, well, we don't have a full implementation of the validator yet, so any benchmarks would most likely be micro benchmarks. So, for example, how many signature verifications per second can you do things like that? We do have a artificial load generator that tests the pure packet relay performance without any SiG verify, and you should be able to feed it with packet capture files. I think we have a bunch of guys in the room for that.
00:11:17.304 - 00:11:40.984, Speaker A: So there's a question about is there any way to spin up fired answer on Mainnet testnet with ease. I put a Twitter thread in the chat, so if you refer back to that, there's some explanation of where fired answer is and also the code base. So I'll sort of defer to Richard, but I think just checking out the code is what you should do.
00:11:41.724 - 00:13:03.514, Speaker C: Yes. So to put it shot, we're not ready to, like, the modules we've published are not ready to be deployed on Mainnet. The main reason for this is that file answer uses OpenSSL's version of at 250 519, and that is known to have some differences in behavior and edge cases between the Dalek implementation, which is what Rust uses, and this could be exploited to create invalid blocks, which of course force the validator of the network, unless you of course just accept your own transactions. But either way, that's not going to serve a benefit to the mainnet yet. Obviously we are working on resolving this, but instead of just pushing our fix and hope that it's working, we're working with our security auditing firm runtime verification on solving this with approaches like abstract interpretation and fuzzing and all these kinds of stuff. And once that is ready, we will put out a further announcement. And another point is probably that the Frank Condenser architecture that uses the Solana Labs components to spin up this kind of prototype validator was not designed to be production ready.
00:13:03.514 - 00:13:12.414, Speaker C: This is more like a test feature that allows validators to get a feeling for deploying the validator.
00:13:15.114 - 00:13:18.054, Speaker A: Great, thanks. That's all for questions in the chat right now.
00:13:18.494 - 00:14:18.744, Speaker C: Okay, cool. I have four or five more quick component overviews I want to go over. So the next one is a reimplementation of turbine. We expect no larger changes there, though we do use a special network interface of the Linux kernel, and that is a reason why we require more recent versions of the kernel called aFXDP. So this is basically a standard Linux feature to allow bypassing the Linux kernel networking stacks and netfilter and all these other various components and sockets, and get data directly from the network interface through XDP to fire dancer. And this is again useful to reduce cpu usage and allow reliable operations at high line rates. So this is getting interesting once you cross into 100 gigabit networking range, but even at ten gigabits, it should offer significant cpu usage reduction.
00:14:18.744 - 00:15:44.468, Speaker C: Turbine optimization is another area we've looked into in parallel, and this would require breaking changes to the turbine protocol. So this goes into things like changing the erasure coding FEC ratio, or like using a different erasure coding, um, algorithm and looking into improved block propagation strategies. Um, I'm not too familiar with, with that, um, side of the protocol myself, but um, as, as with all these um, updates, we'll publish a full overview and potentially blog posts once that is live. Um, and finally, I think, uh, the most interesting to validators probably sig verify hardware optimization using FPGA's. So that is a completely optional technology, but it allows offloading the cpu using hardware implementations of SHA 512 and 5519 curve operations, which are used in the signature verifications of shreds and transactions. Again, it's useful for high bandwidth operations or just if you're being flooded. There's a slight problem with the use of the QWik protocol though, because ideally what you want to do is you send all the network traffic directly to an FPGA and process it there, and only then forward it onto the cpu.
00:15:44.468 - 00:16:52.696, Speaker C: But the QWik protocol is complex enough that it's not easily realizable in hardware. Therefore the production deployment will look something like you still have network traffic coming in through a regular network pod, like an Infiniband or Ethernet port, you process it via XDP to separate the system and the Solana traffic flow. So XDP is just what we use for flow stream, which can be deployed on Smartnix or just on regular hardware, because it runs on any cpu basically. And then once that traffic is identified and the quick processing has completed, you then send the traffic back to the FPGA, do the signature verification there, and then send it back to the validator. So this additional network of going through the cpu first makes things a bit slower, but certainly at the rates of which Solana Mainer currently operates, that's no problem at all. POH is also pretty interesting. So Shaofact twelve, sorry, SHA 256.
00:16:52.696 - 00:17:35.974, Speaker C: So the smaller version of SHA two is significantly easier to implement on hardware. And this component, we notice it's not particularly useful for PoH leading. Therefore it also doesn't break any of Solana's PoH security model. However, for Poh replay, you can do a really nice parallelization optimization that Solana currently does with GPU's. Basically, it removes a lot of CPU time that's currently spent on verifying Poh by offloading that to an FPGA, and that can run in the same FPGA that also does sig verify. And then finally. Actually two more things.
00:17:35.974 - 00:18:30.224, Speaker C: We have a reimplantation of the account CB that is currently work in progress that has not been checked into GitHub yet, but we hope that we can drastically reduce the memory requirements here. It's no secret that most validators are starting to deploy 512gb up to a terabyte of DRAM. We hope that we can improve this, but we don't want to be overconfident before we have actually tested the account CB in a live validator, where you have the realistic read and write workloads that they have on a current validator. We haven't tested that component in testnet normal main yet, so that's not to come here. And then also a reimplementation of the runtime. So things like reimplementing the EPPF virtual machine. But for now we're just starting out with simple transactions.
00:18:30.224 - 00:19:18.554, Speaker C: All of these parts would be technically ready and we expect like two or three months out. But there's so much security considerations that we want our auditing team to have a thorough look at it and also define a model for easily running formal verification and constrained analysis over things like native programs deployed in the c level virtual machine. So again, that is probably going to be very far out. Yeah, I'll stop ranting here to save everyone's time. I think the next things to look forward to is the quick re implementation and potentially turbine. And that's going to be packaged again in the frank condenser model that we've used for the last milestone transaction.
00:19:22.254 - 00:19:28.154, Speaker A: Great. Thank you, Richard. Any more questions for fire dancer? For Richard and the fire dancer team?
00:19:35.114 - 00:19:45.534, Speaker C: If you have any questions after this call, please also feel free to send them to the discussions tab of the file dancer GitHub, and we'd be more than happy to answer them there.
00:19:47.674 - 00:19:50.654, Speaker A: Great. Let me grab that link real quick.
00:19:54.534 - 00:19:55.474, Speaker C: Thank you.
00:20:00.814 - 00:20:16.830, Speaker A: All right, I'm going to add that link to chat. There's a new question. Have you ever considered making quick window size proportional to latency to account for the fact that throughput is already inversely proportional to window size? Yes.
00:20:16.862 - 00:21:07.254, Speaker C: So I think this problem mainly arises from TCP. I think we haven't seen a problem with window size on quic, mainly because it's a very pipeline protocol where you can. I think the window size is basically large enough already that you don't need to wait for any acts to send a single transaction, and all transactions are sent in separate streams. So I don't think we've run into any window size issues yet, but all the quick transport parameters are actually fully configurable, or will be when the code is released. I think we're sitting on 35,000 lines of unpublished code right now, but if you see any window size problems, I'm not an expert in that part particular, you can just change that number in the file answer configuration file.
00:21:12.394 - 00:21:16.134, Speaker A: And just another shout out to say, amazing work. Thank you for the feedback.
00:21:19.324 - 00:21:21.984, Speaker C: Thank you very much for the interesting questions.
00:21:26.724 - 00:21:45.104, Speaker A: All right, thank you, Richard. Thank you, fire dancer team, and thank you to Steve for chiming in. Also, next week, the validator or not? Sorry, next week, two weeks from now, the validator call will be a late one again. So for all those in North America, South America, it'll be later your time. But anyway.
00:21:45.144 - 00:21:45.448, Speaker C: Yep.
00:21:45.496 - 00:21:47.444, Speaker A: See you all in two weeks. Thank you.
