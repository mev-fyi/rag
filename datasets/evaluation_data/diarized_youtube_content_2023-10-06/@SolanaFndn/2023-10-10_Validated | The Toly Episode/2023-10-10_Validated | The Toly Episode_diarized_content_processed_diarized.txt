00:00:04.760 - 00:00:07.190, Speaker A: And it's Oli. Welcome to validated.
00:00:07.382 - 00:00:09.194, Speaker B: Hey, great. Good to be here.
00:00:09.494 - 00:00:13.754, Speaker A: This is a show that you started that I took over that now you're a guest on.
00:00:14.574 - 00:00:16.834, Speaker B: What happened to the no Sharding podcast?
00:00:18.574 - 00:00:53.786, Speaker A: Uh, it seemed too adversarial. Yeah. This has been a long time coming. We chat a lot, which is great. We do Twitter spaces fairly regularly. We got breakpoint coming up. But I thought it'd be nice to sit down and kind of just slow things down a bit, have a bit of a longer conversation about some of your thoughts and experiences in the last year, about getting the network to the state that it is now, and sort of where we've got probably line of sight to a bit more stability.
00:00:53.786 - 00:01:00.634, Speaker A: What are some of the big ideas that you're thinking about for the next year or two of development on the Solana network?
00:01:02.334 - 00:01:04.994, Speaker B: Awesome. Yeah. Where should we start?
00:01:05.574 - 00:02:03.524, Speaker A: So I guess I want to start going back a little bit to, let's call it the early q one of 2022. And this was probably in a period where, you know, we'd had that first major outage since people started using the network in, like, September of 21, and there was just suddenly it felt like the network was congested every day. I'm kind of curious, like, less on the, like, we'll get into the technicals of, like, what some of the solutions were, but, like, as someone who was one of, like, the original four people who really conceived of this network and sort of started building it, like, what was that process like for you, of sort of seeing ideas that you guys had thought might work or systems you thought you designed well enough, like, fall over in production? That's not usually a conversation you get to have with founders.
00:02:05.264 - 00:03:10.804, Speaker B: This was actually a spot of the design that we left, almost, like, intentionally not well defined. We had a solution that we thought would work sufficiently well, and it did for a bit, and then it fell over. And I think it was really important to. I think if you try to solve all the problems all at once, you end up with something that either doesn't work or takes so long to build, or is very extremely conservative. And, um, the latter part is what a lot of layer ones end up with that launch. They kind of build something that is robust but very conservative. And that means that once the system's up and running, it becomes really, really hard to make the kind of changes that are necessary to, like, to move it to that next level of performance.
00:03:10.804 - 00:04:00.672, Speaker B: And these are really hard trade offs to make because you really want to ship a good product you want it to be fast, you want it to be cheap, and you want it to be stable and over. Optimizing on the last one early on will box you in. Design wise, we really needed to see what fails and how it fails. And obviously, if we knew that these were failures, that could cause an outage. The interesting part was that the things that caused an outage wasn't the load itself. It was problems in the code base that couldn't handle the load at that specific performance. They basically blew up in memory.
00:04:00.672 - 00:04:36.184, Speaker B: And that memory failure is what caused the validator to fail. It wasn't the load itself. And at one point, we fixed a bunch of these issues and the network was more or less stable. It would get congested and block production would slow down, but consensus wasn't falling over. You still would land transactions. But you get these really annoyingly, you have to retry a bunch. If you're a DeFi protocol, you get failures and stuff like these kind of ux problems that make it a very unpleasant experience.
00:04:36.184 - 00:04:47.144, Speaker B: So that reliability part, that last part is what needed an actual good solution that was robust all the way through and through.
00:04:47.884 - 00:05:21.664, Speaker A: Yeah. I want to go back to something you said, which was that there's a trade off there that had to be made in the early days between doing something new and doing something conservative. And it seems to me like that was a trade off that a lot of founders were making 20 years ago. And it seems like this is not a web3 thing, but this has sort of fallen off the spectrum that you're allowed to launch a v zero product or a v one product, and it's allowed to occasionally fall over to push the bounds of what's possible. And there seems to be a lot of, like.
00:05:22.904 - 00:06:01.184, Speaker B: So that you're screaming, no. I think the thing is that people pretend that they launched the product that is not a V zero product, but all of them are v zero products because they literally just went to production. And it takes multiple years to figure out in a complex system, if it took two years to build, it's going to take at least two years to stabilize. That's kind of like how long it took for Solana. Like, it's just no matter what, and anything that complex, you need, it needs time in the hot seat, so to speak, to, like, kind of stress test it from every possible angle.
00:06:01.964 - 00:06:08.224, Speaker A: How did you get comfortable with that? Because I think today most founders are still not comfortable with that idea.
00:06:08.924 - 00:06:46.662, Speaker B: It's not. You don't have the option, like, well, you can lie. Yeah. I mean, everyone that every engineer that's been had the experience of working on big systems that they had to bring up from zero to one that took multiple years to build knows this. Exactly. Things go wrong and it takes a long time to stabilize something. This is why you want to really optimize for shipping small changes as fast as possible that are incremental and that really you can kind of roll out one feature at a time and test things out.
00:06:46.662 - 00:07:53.664, Speaker B: But when you're building a new database or a new operating system or a new blockchain, you have to solve multiple hard engineering problems at the same time. And if you don't innovate on them, the problem that you don't innovate on ends up being kind of your Achilles heel that becomes too slow or too expensive or whatever. So that kind of prevents you from going to that next generation of technology. So this is kind of, I think why a lot of other layer ones that in my view, kind of take this with a grain of salt. But folks, that copyDBm or just like modified one part of the ethereum stack, you just really cant take. Its not going to be a next generation blockchain because you only fixed one problem. But Amdahls law means that if you have seven other hard problems, fixing that one problem is only going to give you marginal benefit, like ten, 5% improvement overall.
00:07:53.664 - 00:08:36.858, Speaker B: You have to fix all of them. Fixing all of them, theres a 50 50 chance that you get it right. You dont ship a bug. So if you have eight hard problems, each one of them has a 50% chance of having a bug. The probability of you not having any bugs is one in like 256, just virtually zero. So you kind of know coming into this that like, okay, there's going to be some bugs and some of them are going to be hard to fix and problematic, but that's just part of like, you just have to get through it. I think we got pretty lucky that the bugs were just stressed, like heavy load issues.
00:08:36.858 - 00:08:40.014, Speaker B: That's actually. I think we got pretty lucky there.
00:08:40.794 - 00:09:07.302, Speaker A: So Solana has made a few design decisions that sometimes are criticized and other times are held up as the reason it can be as performant as it is today. Can you walk us through a little bit of why those decisions were made at the very base layer? I'm thinking of the way a virtual machine is thinner, if that's the right terminology, than many other blockchains between the VM and consensus layer.
00:09:07.478 - 00:10:05.924, Speaker B: Yeah. A lot of this design came from trying to figure out what problem are we solving what are these technologies good at? And I thought that there was already a narrative that Ethereum is working on settlement and bitcoin is this store of value. And none of those problems were just interesting to me. Not that they're not important, right. It's just not something that I got excited about. What I thought was cool was the idea that you can use this same byzantine fault tolerance systems to give you guarantees that information is synchronized around the world in a fair way and as fast as possible. And like providing those guarantees that everyone has an opportunity to submit data and everyone has an opportunity to read it kind of at the same time simultaneously around the world, and then making that system be as fast as possible as physics allow.
00:10:05.924 - 00:11:12.058, Speaker B: That's the engineering challenge. And the benefit of having fair, open kind of information engine is that it's a really, really good spot for finance, for lots of stuff, for like financial rails and trading and all these other things. It solves a lot of problems that centralized systems do. So my thought wasnt that were building a competing settlement layer to ethereum or a competing store of value to bitcoin. It was that if we get this right and there is this open platform for synchronizing state around the world at the speed of light, thats competitive with very important businesses like CME, NYSE, all of trading, all of finance can run on this, and it's not going to displace them. It hopefully becomes the infrastructure layer that glues them all together. And they all know that they're getting a fair, that no one is clobbering them on rights, no one is getting ahead of them unfairly on rights, or withholding data, and getting a bit of an information edge ahead of them.
00:11:12.058 - 00:12:16.714, Speaker B: All those things are great, good for those. So this is what solving that problem and keeping that in mind, that we need to make the system synchronize data as fast as possible and allows data to be submitted as fast as possible. Those two problems were just always in my mind. So this is where the design comes from. It's just trying to always solve those problems in that context. Why a lot of folks argue on crypto Twitter is that if you have this system that can synchronize information and guarantee that it's globally synchronized, it has feature overlap with things like settlement and store of value, just simply because once you solve this information, fair information, symmetry, whatever you want to call it, you solve that problem. You also have to, by definition, implement something that does settlement of state, or could be a store of value, because it's the token that runs on this thing, like, prevents spam in the system.
00:12:16.714 - 00:12:59.764, Speaker B: What makes it a store of value? It's a meme, but it exists, and people could use it as a store of value if they wanted to. There's overlap there between what we're building and the features that Ethereum and bitcoin provides, but we are solving a specific problem. Everything flows from that. So design wise, for that system to exist and for it to handle more bandwidth, that bandwidth is only going to come from parallelizing how information flows, and that means how it flows through the smart contracts or through the propagates around the world and stuff like that. All the design decisions kind of came from that.
00:13:00.584 - 00:13:16.684, Speaker A: Yeah, it's kind of interesting to think about what analogies do and don't exist in the rest of software. Like, most global systems are not synchronous, and most synchronous systems are not global.
00:13:17.624 - 00:13:40.394, Speaker B: But Google Spanner, there are systems like that. People have built them because they solve a large class of problems for a lot of applications. And developers then don't have to, like, go build, re implement a whole bunch of algorithms so that there's like, once you build it, like, it's a very, very powerful tool.
00:13:41.774 - 00:14:19.454, Speaker A: Right? And maybe I'm curious if you'd agree with this or not, but it feels like most global synchronous systems are actually not particularly concerned about how quickly they synchronize information. Like, yes, the YouTube video you uploaded needs to be in every YouTube data center, but it doesn't like, the speed in which it arrives in every data center is not the defining factor of whether YouTube is successful. I'm currently curious what class of problems really need both of those two things, like very fast propagation of the synchronicity, for lack of a better term, as well as also a global state.
00:14:20.994 - 00:14:58.602, Speaker B: Data structures that are shared between a bunch of users you can actually look at. Even things like that are simple as a newsfeed on a Facebook or a shared file system like Dropbox, stuff like that. You really start getting into weird UX problems that if they're not synchronized within some time, guarantee you get like, your 0.1% of users get these weird bugs and they'll start complaining. And 0.1% may seem small to people, but it's very large number to DevOps engineers. If you have a million users .1%
00:14:58.602 - 00:15:12.354, Speaker B: is like 10,000 complaints. Yeah, that's bad. You all of a sudden get 10,000 tickets. That's a bad. That's a very bad day. That's a bug that you need to fix. So, like, that's.
00:15:12.354 - 00:15:36.738, Speaker B: And that's 1 million users is not even like that big of a size. It's pretty big, right? It's a product with very strong PMF. And when you scale it to 100 million to a billion, like things like Facebook and Google, you start really, really caring about places wherever these inconsistency failures will bug or annoy users.
00:15:36.886 - 00:16:18.934, Speaker A: So kind of looking at that work that's been done since Q one of 2022, there's a huge amount that has happened since then. It feels like just in the last six months, something has changed. And I'm sure it's actually 100 things that have all slowly been changed and that it adds up to the culminative experience we have. But you saw 116 be released and hit a majority adoption on mainnet beta. And the response from the community was like, wow, something's new here, something's different. What was that two years, year and a half of engineering work to make that moment happen?
00:16:20.154 - 00:16:49.304, Speaker B: It is a lot of optimizations. We only had 20 people to build Solana, and only twelve of them, including me, were engineers. So when we launched, we wrote the code as well as we could have, but it's obviously not going to be as efficient as 40 engineers that are optimizing it for 20 years. And in the two years since. Or like, how long has it been? One and a half years or two years since launch?
00:16:50.244 - 00:16:53.868, Speaker A: Three years, March of 2020?
00:16:53.956 - 00:17:39.974, Speaker B: Yeah, it's been three years since launch. We've had the. The blessing of being able to see what works, what doesn't in production, and be able to optimize a lot of the code paths. I think this last release, the accounts index that is synced to disk, I had the prototype that I handed to Jeff Washington two years ago. I think at this point where I wrote a prototype implementation of the syntax and I benchmarked it and it was going to disk. And I showed him, hey, look, if we do this, we can save a bunch of memory. And it literally took him two years to land it into a main net release where it's on by default.
00:17:39.974 - 00:18:51.854, Speaker B: Because these problems are hard, right? When you're building something like this, you get a lot of, if there's some bug in the implementation, it'll chew through the disk or it'll open too many files and a whole bunch of other. These weird problems come up when you start trading off these resources between ram, which is very easy to use but expensive and limited to disks, which are somewhat slower but much, much cheaper and they're for plentiful and space, but also have trade offs. Like, they wear out. Like, RAm doesn't wear out. So you have to balance all of these weird physical limitations of the hardware that runs digital systems and math, which supposed to be limitless. When you map it to hardware which has limits that are physical, based on physics and manufacturing limits, you start getting into all of these edge cases, which is where engineers, systems engineers, are going to be employed, making sure that AGI systems don't fall over for millennia.
00:18:52.794 - 00:19:11.218, Speaker A: Do you find that empowering or annoying? Like, when the math runs into the real world and not the real world in the sense of like, oh, there's a bug in the code, but like, oh, fundamentally, like, computers are still physical things in the real world at some level.
00:19:11.306 - 00:19:12.600, Speaker B: Yeah, they overheat.
00:19:12.762 - 00:19:13.464, Speaker A: Yeah.
00:19:14.604 - 00:20:21.236, Speaker B: I think those are really cool, fun problems, because it's like, if you've ever played resource management games where you have, like, and you're like, building a civilization or whatever, and you gotta balance all of these things. Those are fun optimization problems. You're trying to schedule a bunch of work and then make sure that you don't overuse one resource or another, and, like, you're optimally distributing the work and stuff like that, and load between all of these different things, and those are fun. My job at Qualcomm was managing that on a much smaller scale. On a single chip, I was balancing between the DSB and the arm and the camera sub processor that had a faster cache. And you're trying to place this algorithm to, you know, measure six degree of freedom changes from the camera across all of these parts of the chip, and, like, minimize latency and optimize speed. Those are, like, fun engineering problems.
00:20:21.236 - 00:20:26.824, Speaker B: You can kind of, like, go deep on forever. It's a. It's a nerd snipe.
00:20:28.284 - 00:21:05.164, Speaker A: Yeah. So we had Kevin Bowers on a while ago, and one of the things he spent a good amount of time lamenting was that in the transition to cloud, there's been a focus on creating software abstractions that are designed to make it so it doesn't really matter what you're running your code on. It's just running on abstract math in the cloud. Solana historically been hard to run in the cloud, both for business operational reasons about the way clouds charge, but also just the hypervisors sometimes get in the way of some of the work that's trying to be done.
00:21:05.654 - 00:21:06.318, Speaker B: Yep.
00:21:06.446 - 00:21:20.834, Speaker A: I want to hear your sort of thoughts on, like, that as a design decision for Solana, but then also kind of going forward, do you see other classes of software where our reliance on abstractions are actually hurting our ability to make cool new things?
00:21:22.254 - 00:22:21.414, Speaker B: Cloud is pretty bad, but it's pretty good at, like, if there's strong business, like if there's strong product market fit for a specific kind of computation, the engineers at those places will optimize and plough through the layers of abstraction to do the right thing. It just takes time. But you will see that, I think with AGI, you will rapidly see all the red tape abstraction stuff be blown away and have very optimized systems that are good at the things that people want them to do. It's just hard to make it easy to program and high performance at the same time. Like, it's just all the high performance benefits come from really understanding the physical limitations of the machine and then eking out performance everywhere you go.
00:22:21.954 - 00:22:32.046, Speaker A: How much of your work at embedded systems companies like Qualcomm was foundational in actually being able to come up with something like Solana, obviously.
00:22:32.070 - 00:23:37.434, Speaker B: There's, I mean, a ton, obviously. And the fact that most of the early team members were like the folks that I worked with for over a decade at Qualcomm, it's just that knowledge of kind of having that instinct that this stuff, these particular optimizations and writing code this way will make it faster was. It's just become second nature after ten years. So I think it was really, really important to have those folks and to have us kind of start like that. I almost like, this was like a weird metaphor that I had, is that like. Because, you know, we had this like two years, whatever to build it before we launched, my instinct was to like, this is kind of opposite of what people should do, but take as much technical risk as we could. That while still launching in two years, that would set us up for the fastest possible kind of design.
00:23:37.434 - 00:24:38.808, Speaker B: And this is again, kind of fighting that idea that if you have eight hard engineering problems, there's a 50 chance you'll have a bug in each one. It's very, very unlikely that you'll be bug free. But there's kind of like, if you come into that, knowing that the opposite side of that is if you have eight hard engineering problems and you just fix one at a time and the rest are slow, it actually becomes very, very hard to fix them one at a time. Like, you get stuck with legacy code and legacy dependencies and legacy APIs. What I really didn't want to happen is us getting stuck with some legacy thing that was just really hard to fix. Like really hard to incrementally change. So what we got lucky with is that the implementation was actually pretty decent, only fell over under extreme load, not even due to design, but just due to bugs.
00:24:38.808 - 00:24:47.384, Speaker B: And that we didn't build anything that was, like, excruciatingly slow, that you cannot incrementally get around.
00:24:48.684 - 00:24:51.104, Speaker A: What were some of those risks you took?
00:24:52.924 - 00:25:39.178, Speaker B: Those are actually like the articles that we wrote on the seven or eight innovations of Solana. And youre kind of dealing with the major hard problem is just propagating the data around the world. And turbine was a pretty novel idea at the time. So we used erasure coding, and blocks are transmitted in chunks, not in one block at the same, not block one block from peer to peer at a time, which is how bitcoin does it or ethereum does it. And I think most layer ones do it that way. So you basically propagate between peers. You send a block out the entire block from pier one to peer, two, three, and four, and then they fan it out through gossip kind of in this random way.
00:25:39.178 - 00:26:29.630, Speaker B: And it's very robust and very fault tolerant, but it's very slow. So, yeah, what we did was optimize block propagation, and it has to stand up with, like, adversarial nodes, and we did a whole bunch of testing. It also has to work on open Internet, not just, like, in a kind of cloud optimized environment. We made some assumptions in the v zero that broke as soon as we tried to run it on the open Internet. So stuff like that. SVM itself is a pretty big risk. Building a new virtual machine, or even though we reused a lot of existing technologies, we used Berkeley packet filter as the bytecode and LLVM as a tool chain.
00:26:29.630 - 00:26:39.354, Speaker B: You still have to create this runtime environment that kind of glues all these things together. That was a lot of work and a lot of code.
00:26:39.694 - 00:26:52.342, Speaker A: So that's a classic one where it's like you don't really touch the VM unless you have to. What were the reasons that convinced you guys to say, this is a big thing to do, but we got to do it?
00:26:52.478 - 00:27:48.284, Speaker B: Well, if we didn't optimize the eVm, then all these other optimizations would just disappear, would become very marginal. EVM speed would kind of dominate the fact that a single threaded would dominate. Similarly, if there was a mempool, that would dominate the performance costs. So then if you do everything else, but you still keep EVM in a mempool, you get maybe 20, 30% faster, and then you just can't differentiate like that. So if you're gonna, if basically, like, I would say that this probably goes to product, consumer product design too. If you're solving a problem for the user, like figure out what are like. If it's a new novel problem, ship it as fast as you can, then users will overcome any friction.
00:27:48.284 - 00:28:12.854, Speaker B: They won't care about performance or price. Means you've caught lightning in a bottle. But if you're not solving a new problem, and I don't think layer ones are like Solana, we weren't solving a new problem. Ethereum already existed and there were a bunch of other alternatives. You have to pick a specific thing that you really, really care about and then kind of Hail Mary, throw everything at it to differentiate.
00:28:13.634 - 00:28:24.586, Speaker A: Yeah, I think that's a really good kind of analogy there. I mean, it took Solana being 1000 times faster to get anyone to notice it.
00:28:24.690 - 00:28:25.778, Speaker B: Yeah, exactly.
00:28:25.946 - 00:28:32.114, Speaker A: Yeah. If you think of it like, oh, if it were twice as fast as EVM on Ethereum, would anyone really have noticed?
00:28:32.234 - 00:28:33.894, Speaker B: Yeah, I don't think anyone would care.
00:28:34.794 - 00:29:05.096, Speaker A: Kind of running the clock forwards now, you mentioned that there's a bunch of work that was done sort of since launch, but also since that kind of spring of 2022 timeline. How much of that work was stuff that was sort of originally somewhere in the original docs that were like, oh, we got to get this eventually, and now is the time we got to it. And how much of that was really net new work that was determined by seeing the network actually operating in the wild?
00:29:05.280 - 00:29:33.354, Speaker B: I mean, there's stuff that was new, like adding new system calls for cryptographic operations that we just didn't anticipate people needed. Or if they don't need it, we're not going to add them, right, until, until somebody really, really wants it. A lot of the work has just been fixing bugs, and that includes performance optimizations. You can think of them as bugs. This code is too slow or runs uses too much memory. That's a bug, go fix it.
00:29:33.694 - 00:29:34.486, Speaker A: Yeah.
00:29:34.670 - 00:30:21.806, Speaker B: So that kind of work is just work that is almost always going to continue because hardware changes and those physical limitations change from each version of hardware. Hopefully they all get better, but that's not even always the case. Sometimes they get worse. Magnetic disk drives can write a lot more data than SSD's. Stuff like that you have to keep in mind and keep adjusting your implementations. Things like local fee markets and Qos. That was a net new thing that needed to be built to address a problem that was obvious only after the system was in production and under load that exceeded the expected load.
00:30:21.806 - 00:30:33.086, Speaker B: Like we have to actually see it get stressed to a point of breaking to understand the problem, which is kind of obvious in retrospect now, but you really just didn't anticipate it.
00:30:33.270 - 00:31:02.212, Speaker A: A lot of software over its life span, no matter how sort of targeted and small the project begins, bloats as it gets bigger. I think consumers are used to this, developers are used to this. You add new features, new libraries, new et cetera. How do you balance the need to add new things to the Solana network, like local fee markets and stake weighted qos, while also keeping the network performant and fast over the long term?
00:31:02.388 - 00:32:13.414, Speaker B: The reason code gets added is because it's better than the previous code. Every release is better than the previous one. And what makes it better is it solves some problem for validators, reduces memory or bandwidth, or makes the software more stable for developers, it gives them an API or removes a previous limitation, like how much memory they can allocate, or being able to grow programs and stuff like that. These are all requests that people ask, hey, can you fix this, increase this limitation, or fix this bug? And this is why the releases are basically non controversial. You have a change between 114 and 116. The changes are cosmetic from a very bird's eye view. Does anybody understand or care if program allocations are now allowed to grow or not after this update? After this feature is enabled, it just increases the programmability or stability or performance of the network.
00:32:13.414 - 00:33:07.154, Speaker B: Those are effectively the things that push forward. Some of the stuff that is more controversial, I think generally has to do around like economics and things like that on the network. I would say fees for writable accounts, stuff like that. There's a lot more discussion on whether these things should go in or how they improve the developer experience or impact validator economics. And people argue about that, and then eventually, if it doesn't get pushed back from enough people, it goes through. If people want it or not. It's the old school Linux open source model.
00:33:07.154 - 00:33:12.926, Speaker B: If this makes the software better, then it gets shipped. If it makes it worse, if it doesn't.
00:33:13.070 - 00:33:17.676, Speaker A: Yeah, it's a pretty good heuristic for figuring out what to ship and whatnot.
00:33:17.870 - 00:33:30.164, Speaker B: But ideally you don't want to churn. There's just obvious stuff that you don't want to do. Change APIs or break backwards compatibility. These are no nos, and there's a lot of tooling and stuff in place to make sure that never happens.
00:33:30.664 - 00:33:38.960, Speaker A: Yeah, but then there's also stuff like Token 2022, which is the next generation of token program coming to the network.
00:33:39.072 - 00:34:32.021, Speaker B: So that's not part of the network itself. These are programs that anybody could ship their applications effectively. And ideally you move as much of the functionality that you're working in the code to the application layer, so the applications can decide as much of the policies and decide how things work. It's not always possible. There's just, that's kind of how I think of it from a design perspective. Like if you're building an operating system, you would want it to be like as flexible to the developer and as robust that if you have a single os running five applications, they work as well as if you had one application per operating system instance. It just doesn't matter to the developer how it's deployed and how it's configured.
00:34:32.021 - 00:34:51.303, Speaker B: It's just kind of like maybe you may make those decisions, but from the software perspective that it's equivalent to, and that's very hard to do. Job of the OS to balance all the resources and make sure all that stuff works. You can never get it right, it'll never be perfect, but it can be within one 10th of a percent.
00:34:51.683 - 00:35:06.703, Speaker A: Yeah, it's interesting to think about that. So where is the level at which something belongs in, to use your term here, the OS versus belongs in the application layer on a blockchain network.
00:35:07.814 - 00:36:12.954, Speaker B: There's some things that it's just very like may take a very long time to do it all in the application space. So for example, in theory the runtime could be as fast as native code. It could be so optimized that the just in time compiler and all the code transformation it does actually takes the user supplied programs and runs them at native speed. But that may not be the case. That sandbox that runs user supplied code has limitations on it that the native code doesn't. So if you have a cryptographic operation like you need for elliptical curve or zero knowledge or something like that, it may be really really tough to do it in user space. And people actually do you do it, they implement it in user space, it becomes an expensive operation to call, but they get demonstrated, hey, this works, there's an application that needs it and then you can take effectively that code and add a system call or something like that, that becomes enshrined in the runtime.
00:36:12.954 - 00:37:35.934, Speaker B: So this is how I think of it, which is I think if you read the Vitalik's article on enshrining versus not, he kind of goes into it from a very different perspective, more about the, the social, not the social layers. Decentralization like, should they enshrine a liquid staking solution into the protocol or not? My view is that a lot of these problems come from some bug in the programmability of the layer that has caused this very uneven distribution of power in the network, and for example, with liquid staking and stuff. Solana is proof of stake. Ethereum is proof of stake. There's a very big difference there. The way Solana is designed is that all the components of that staking, the validator account, their vote key, their withdrawal authority, the stake that's delegating to that thing, the withdrawal key on the stake, all these things are separate programmable keys that can be taken over by a program address or by a user key at any time. So these are data structures that were created to be maximally flexible.
00:37:35.934 - 00:38:31.568, Speaker B: And because of that, we have many different stake pools. None of them are very dominant. Not only that, these stake pools overlap validators. So it's not like Ethereum, where 132 ETH node is a single validator that can only go into one stake pool. So you have these weird inefficiencies that are hard to get around as a software engine with code in Ethereum, I think that create these kind of centralizing effects because they're not as programmable. I don't know if that's always the case. There's obviously lots of factors there, but my feeling is that if you make it very flexible and very programmable, it's actually very hard for a piece of software to accrue power, because it's kind of like very easy to compete with.
00:38:31.568 - 00:38:42.964, Speaker B: There's no moat there. And if there is a mode, for some reason, I think it's kind of typically, to me, indicates there's something that's hard to program in the underlying layer.
00:38:44.124 - 00:39:26.564, Speaker A: Yeah, it's interesting to look at it that way because that's a real example where a lack of enshrinement has created a positive effect for Solana. I think it's also possible to look at it on the user level and say a lot of people would prefer if metaplex were enshrined and were not an independent entity. And it's not necessarily that they don't trust the metaplex foundation. It's just that they view this as something that is almost mission critical beyond the point of being owned by someone at this point. What do you think about those sort of arguments that on the network level, 100% on the program level, Solana actually has potentially a little bit more program control than you see on Ethereum?
00:39:26.944 - 00:39:38.722, Speaker B: Yeah, this is, I think the, I wouldn't call this the enshrined versus non enshrined. These are like public good versus commercial good kind of question.
00:39:38.898 - 00:39:39.610, Speaker A: Yeah.
00:39:39.762 - 00:40:12.148, Speaker B: And, like, when you have, like, what was really cool about DeFi summers, you have these companies like Compound Aave and stuff. They built commercial products, which was like the lending protocols, but they also built a bunch of public goods, which is the governance modules and things like that. So those government. So those public goods were given out for free to anyone to fork and use even their competitors. Right. Even though those are very powerful public goods. I would say that compounds governance is more successful than compound.
00:40:12.148 - 00:41:20.298, Speaker B: Even the compound itself was very successful, too, but the governance was a very successful contract. And this is a dilemma on Solana because of some of the inefficiencies in the programming model. Where you don't have interfaces, you have programs that become very dominant due to their implementation. There's positives and negatives there that once you have a dominant program, it's very tempting to try to commercialize it and try to make sense. Does this thing a product market fit? Is there a business model there? And how that works out, I think, is really hard to predict. I can't say that all pieces of software on Solana that are these low level components should be pure public goods, that there is no business model that works for them, because I don't think that's true. I think you actually can build business models into them that perpetually fund them.
00:41:20.298 - 00:41:36.814, Speaker B: But we'll see either way, that inefficiency in the programming model, like lack of interfaces, that's being solved. Once it's solved, you'll see a lot of these modes go away and we'll see what happens. Maybe the public good interface thing will win out in the end.
00:41:37.194 - 00:42:10.384, Speaker A: One of the main differences, I would say, on a program level, and I'm curious if you agree with this, is that Solana does not require users to redeploy contracts every time they want to interact with a contract. There's a lot of efficiencies that come from that. Like the price of NFTs on Solana is astronomically lower, in part due to the fact you don't have to deploy the contract code. But that does mean that most folks are very comfortable and accustomed to using other people's contracts. Do you see that as sort of the long term, like socially, folks will figure it out?
00:42:10.804 - 00:43:26.674, Speaker B: I mean, that's like a huge benefit. You have a single implementation of the token program or the stake pool program or the NFTs that has many eyes on it that people have audited to death and have looked at over and over and solves a lot of different problems. And that code is not enshrined because it's user space code, but it is socially enshrined amongst the developers that they're not going to go build another NFT program or another token program unless there's a very, very nuanced reason to do so, a very specific problem. But it solves, like, that's great. You know, honestly, like, it's not a bad outcome if the dominant top programs are these, like, single implementation that are extremely robust, that everyone reuses, that have no bugs, and everyone because they've been audited to death, code reviewed to death, it becomes that, like, you know, selling point for, like, correctness. That's a very, very good outcome for contracts and for network efficiency.
00:43:27.134 - 00:43:56.834, Speaker A: So you've been tweeting a bunch more about AI. Solana Labs launched an AI grants initiative program and has sort of this, like, chat GPT reference implementation. I'm curious where you see AI going in blockchain, but also sort of around the trust component here, because I. A thing you hear a lot of the time when folks are talking about AI is like, oh, well, you can't trust the code who's written by a machine, but we trust code written by humans every day, and humans make tons of mistakes.
00:43:58.254 - 00:44:39.894, Speaker B: I think people will get around to trusting code written by a machine. I think that because AI can also be used to verify the code. Like, you can have effectively a different context, verify the code or correctness. And you can build these adversarial models where you build something, some other AI instance tests it, third one verifies it. All that stuff will get there, actually. I think AI is awesome for code generation. I think this is an amazing toolkit that's going to make software much, much faster than before software development.
00:44:40.474 - 00:45:07.420, Speaker A: Yeah, I mean, I was spending a lot of time looking at how much it's aided physical design. There are part generative systems now that you just give it the design specs and it gives you six or seven different versions of a part that looks very different than a human might ever design. Have you been playing around with AI generated code at all? Does it look reminiscent of code you expect, or is it sort of like in this mechanical engineering parody where it's just completely different?
00:45:07.572 - 00:45:37.366, Speaker B: No, it actually looks like Cody to expect. And it's buggy right now, it generates buggy code. We're very far from having an AI equivalent. L three engineer AI is really good at tasks right now. You can give it short tasks, unit test dysfunction, that kind of thing. It's very bad at like something that I'd expect a new grad to solve in like two weeks. They just can't do that.
00:45:37.430 - 00:45:38.074, Speaker A: Yeah.
00:45:39.054 - 00:46:33.670, Speaker B: Interesting. The question is, can it make the new grad solve that in like a day? The problem that they would otherwise take, and take him two weeks? And I think that's close. And that's a very, very big change, I think, in software development. So like, I think software is going to get cheaper to write, like cheaper to produce across the world, which will have, I think, more dramatic effects. Software is going to eat more and more of the world and crypto is trying to eat the finance and governance part of the world, which is very resistant to being eaten. This is where humans really feel strongly that this should not be eaten. I think out of all other aspects of life, but it's slowly happening.
00:46:33.670 - 00:46:41.794, Speaker B: I just can't imagine 20 years from now that most of the functions that banks do is fully automated on chain.
00:46:42.094 - 00:46:52.614, Speaker A: What are some applications for blockchain technology that are not obvious today that you're thinking might appear as use cases in the future?
00:46:53.154 - 00:47:40.150, Speaker B: I'm actually more focused on trying to do stuff that's obvious today. I think that's just like really important to try to get users to go do the things that already work at scale. And those are payments, very, very simple kind of thing. Let's send money to each other in a cheap and fast way without, you know, like around the world. I think there's a lot of subtle things that happen on the web and it's kind of became super apparent to me. Everyone is in crypto, Twitter just like me. I signed up for the rewards program and it's a very painful process to actually get through stripe.
00:47:40.150 - 00:48:32.774, Speaker B: You got to send a bunch of identification, give up all your addresses and names to get 20, $30 a month from accident rewards. And its a pain in the butt to use and your money is sitting in stripe pain to withdraw it and actually get to use it at the same time. This developer built this USDC gated email thing based on my tweet called Solent, where I just post an email address and people send me email. If it doesn't come with a $10 USDC tip, I don't answer it. If you haven't heard of it, it's called solinked.com and I'm totally@solinked.com so if you want to send me $10 in an email that you really, really need me to respond to.
00:48:32.774 - 00:48:34.350, Speaker B: That's the way to do it.
00:48:34.502 - 00:48:36.206, Speaker A: I've been thinking of using it for you, too.
00:48:36.270 - 00:49:01.562, Speaker B: Yeah, it takes ten minutes to set up. Like, not even ten minutes. It was like seconds. Because you connect to server and detects your wallet. You just enter the email that you want and the email that you wanted to forward to your normal email, and you're done. And everything is already set up. All the financial rails are there, and it's just such a better experience for anyone that generates user content.
00:49:01.562 - 00:49:37.806, Speaker B: So the problem, though, is that because it's such a pain to deal with finance on the web, these platforms are able to aggregate it, aggregate power, by creating these rails for a large number of content creators. So this is how they get their moats. They own the content, they own the creator's financial relationship between the creator and the user. They own that rail and they own that connection. And once they own it, the creator doesn't want to leave. It's a kick in their revenues. Right.
00:49:37.806 - 00:50:04.770, Speaker B: They're not working for themselves or working for Twitter or for TikTok or whatever, because all of the financialization happens through these platforms that creates all of kind of the segregated, heavily adverbial web that sucks. Like, open a website without an ad block browser. Today, it's impossible to read.
00:50:04.802 - 00:50:05.534, Speaker A: It won't.
00:50:06.954 - 00:50:18.554, Speaker B: It's just insane. I don't understand. Who uses the web? Who is this targeting? It can't be human. So I don't know.
00:50:18.634 - 00:50:20.482, Speaker A: Yeah, that's kind of interesting when you.
00:50:20.498 - 00:50:21.694, Speaker B: Think about it that way.
00:50:22.664 - 00:50:25.364, Speaker A: Do you think advertising will eventually come for web3?
00:50:28.264 - 00:50:47.764, Speaker B: I mean, brave is trying to do it. I don't know. It's hard. It's hard to see right now, but that's because cryptoweb3 users are the early adopters, weirdo tech people that already use adblock and stuff like that.
00:50:49.604 - 00:51:01.144, Speaker A: All right, last questions here. What do you think? What would you like to see more of in the Solana community? And what would you like to see less of?
00:51:02.044 - 00:51:20.986, Speaker B: I think less. We're actually doing pretty good. We don't have a ton of infra projects. We could actually use some more of people trying to build, like, I think. But you kind of see it. As soon as I post something, there's a team that's picking it up already. They tell me, hey, we're working on.
00:51:21.010 - 00:51:21.626, Speaker A: Yeah, like roam.
00:51:21.690 - 00:51:48.314, Speaker B: Like the shit? Yeah, like roam sequencers. That kind of stuff. I think it's worth. Solana should have one or two of those, but not be too overly focused. On infra. I think what's important is the hard problem, which is like, find a problem that consumers have that is so important to solve that theyre willing to overcome the friction to use crypto. If you find those problems, you caught lightning in a bottle.
00:51:48.314 - 00:51:52.434, Speaker B: Do more of that on Solana. Nice.
00:51:53.134 - 00:51:58.582, Speaker A: Well, totally. Thanks for joining us on the no sharding podcast, for sure.
00:51:58.758 - 00:51:59.494, Speaker B: Glad to be here.
