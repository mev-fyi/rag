00:00:02.160 - 00:00:12.414, Speaker A: GM. Gm. Good morning. Welcome. Breakpoint day two. Who had a good time yesterday? Yeah. And you guys are here bright and early.
00:00:12.414 - 00:00:39.486, Speaker A: Thank you for being here. If you don't know where you are, you're back at Pateo DeGale. This is the laboratory for deep dives workshops. Dev, focus time. I see a couple people already with the, the silent disco noise canceling headphones which allow you to focus on the content up here on stage with all that's going on out here in the crowd. So thank you. If you need to pick those up, they're at the front.
00:00:39.486 - 00:01:00.678, Speaker A: Borrow them for the day, return them on your way out. Once again, just like yesterday, we're going to feed you. So there's food all over the venue. Breakfast, lunch, enjoy. Bathrooms over on this side, we've got an incredible slate of content for today. Some really wonderful workshops. If you were here yesterday, it was impressive.
00:01:00.678 - 00:01:33.334, Speaker A: The lineup of speakers panels, I learned a tremendous amount, and I'm not a dev, but by the time this three day period is over, maybe that will change. Watch out, colleagues. I'm coming for you. All right, so our first panel of the day. It's not a panel, actually. Our first workshop of the day is Niko from Elusive, who is going to take us through how to build your own ZK app on Solana in minutes. So get excited, get ready and welcome Niko from Elusive.
00:01:33.334 - 00:02:28.274, Speaker A: All right, so good morning from you again. I'm, as you mentioned, Nico from Elusive. And what we're going to be talking about today is how to build your own ZK privacy app on Solana in minutes. And, yeah, as mentioned, it says ZK there. But we're going to be focusing more on the privacy aspect of it today. And so if we're going to be talking about this, probably a good question to answer first would be why is privacy even important in the first place? That we're dedicating a whole workshop to it in web3. And the current status quo of how transactions work on most blockchains at the moment is that you have this sort of pseudonymity, which means that if you're on a blockchain, people, if they look in the block explorer, can't see, okay, Niko sends so and so much money, but they can see a certain array of bytes, send so and so much money to a different array of bytes via accounts.
00:02:28.274 - 00:02:57.946, Speaker A: And this works pretty well for most cases as long as you're doing stuff where you're not directly involving your real life identity when interacting with other people. So, for example, you know, trading in Defi or something like that. But then as soon as you start transferring and trying to connect with the real world, payments is a great example for this. You start running into problems just because. Yeah, consider you go to a coffee shop, you pay with, you know, you pay with Solana pay, and suddenly the coffee shop can see, okay, this is Niko's account. He has so and so much money on it. He spends so and so much money.
00:02:57.946 - 00:03:45.714, Speaker A: This is his income. Information that most people wouldn't even be comfortable sharing with their close friends or even family is suddenly visible to everyone in the world without the other people even asking if they're allowed to see that information. So for that reason, privacy is just incredibly important. If we want web3 to go mainstream and simply have a more similar UX to web two and get more normal users on board, aside from just general crypto users. And, you know, the first use case, as mentioned, is already simply that most people aren't really comfortable sharing all this information with everyone in the world. But the second use case is, of course, that can actually bring significant dangers with it if you don't have that privacy built in. Again, coming back to the example of the coffee shop, assume you go to a coffee shop in the morning, you buy yourself a coffee for $5 with Solana pay.
00:03:45.714 - 00:04:15.482, Speaker A: But some bad actor is scheming outside and wants to rob someone. And suddenly, if you didn't have this privacy, that bad actor could go in the coffee shop. Before you pay for a coffee, memorize the coffee shop's address and just scheme outside the coffee shop. Wait for someone with a large enough balance to walk in and buy a coffee and rob that person as they proceed to go out, which is something you want to avoid. So for that reason, privacy is quite important on that front. The other aspect of that as well is probably defi. It's a very important use case as well.
00:04:15.482 - 00:05:18.404, Speaker A: As we saw, with things like three arrows capital in the past, having every single one of your positions, especially leveraged ones, visible to the whole world, is not always a good thing. Just because if you're running a company and have competitors, or if you just have a lot of enemies for some reason, if everyone can see your leverage positions, then they can see, hey, if Sol falls to so and so price, you get liquidated, they can actively trade against you, which is not ideal. So, yeah, and of course, plenty of other use cases for that as well. So you might then be thinking, okay, so, Nico, if privacy is so important for web3, why isn't it more widespread yet and for that, we've identified three main reasons. The first one, in the past, has been cost. So if a normal transaction, especially on Solana, is very cheap, very fast, you know, a fraction of a couple of cents, but then suddenly, to use a private transaction, that's way slower, way more expensive, especially on other chains where transactions are already more expensive as it is, users are not going to be willing to use it again. Buying a five dollar coffee and paying a $20 transaction fee, not ideal.
00:05:18.404 - 00:06:04.168, Speaker A: So the way we circumvent that with elusive is on one hand, we have the silver bullet called Solana, which simply has super cheap, super efficient transactions by default. So through that, we can already get the cost way down. And on top of that, simply by doing a bunch of optimizations to our on chain proof verifier, we're able to get the cost down to, you know, less than a cent, or around a cent. Okay, next. So this is not as cheap as what you conventionally have on Solana, but it's definitely way more acceptable than multiple dollars. So, next problem. Okay, if we get the cost down, what else is holding people back from using privacy in their daily lives? So, in past systems, the way privacy systems would be set up is that you might even have to switch to a different chain altogether, like Monero or ZCash or something like that.
00:06:04.168 - 00:06:39.310, Speaker A: Or at the very least, have to go to different websites, require people to go to elusive.com, and then on elusive.com, you can make a private transfer. Again, this is not ideal. There's a reason why Solana is the most used chain, because it has such great UX, has such great protocols, cheap fees, everything. If we then tell people, hey, you have to choose between using Solana with its great UX and great apps already built on it, or you have to choose between using that and privacy on a different chain, which isn't as nice to use as Solana is. Most people will probably choose the UX, and this also works on a smaller level.
00:06:39.310 - 00:07:23.534, Speaker A: Like if we say, okay, you can stay on Solana, but you have to go to the elusive website, you can't do it from your wallet directly. Most people have their favorite wallet. They love using soulflare, for example, and has sleep. It works well, and they can just do ascend just like that, instead of having to go through the whole extra steps of going to an external site and just for having privacy built in. So for that reason, we take the approach of just making privacy as composable as possible with standard Solana protocols, meaning we don't have an extra elusive, elusive address or elusive whatever. It can directly interact with other Solana accounts. And beyond that, what we'll be talking about afterwards is we have an elusive SDK with which you can build privacy directly into your existing app, instead of having those two have to be separate features.
00:07:23.534 - 00:08:10.740, Speaker A: And finally, this one might not be as sexy as the previous two, but in our opinion is just as important, if not even more important, especially when talking to other developers. And you guys might be familiar with this as well. The big risk with privacy in the past, especially when you think about things like tornado Cache, has been that you want to add this cool privacy feature into your app, give that access to that for your users. But then you sort of have those thoughts in the back of your mind, what if someone uses it for bad? What if because of adding this one feature, I get into huge trouble that I don't want to deal with my protocol? Well, if that's the case, maybe I won't add the privacy at all, just to be on the safe side of it. And for that. But of course we don't want to build a system where everything's like you have to kyc yourself to use it or something else. So we need to find a good middle ground.
00:08:10.740 - 00:09:04.074, Speaker A: And what we do for that is inside our SDK. As I'll show afterwards, we give developers directly the option to choose the level of compliance that they want to have in their specific implementation. So that means for the rogue cipherpunks amongst you who wants to say full decentralization, nothing at all, you have that option. But for people who prefer to be a little bit more on the safer side, and if you need to do that because of your legislation or whatever, you have that option as well. So yeah, before we get into the coding side of it, I think what might be interesting to think about a little bit as well is just for those of you who might not be as familiar with how these privacy systems work, it's just a quick overview of how can we add privacy to a chain that's not private by default. And the way we do that is with a two step process. Basically you have your normal Solana balance, which anyone can see, and you have a private balance that's stored on chain and managed by the elusive program.
00:09:04.074 - 00:10:06.220, Speaker A: And the way you add to your private balance is simply you generate a couple of secret values that only you know that you don't share with the world. And you hash those to a certain cryptographic hash. Then you take that certain cryptographic hash along with the amount of money you want to top up your private balance with, and you send that off to the elusive program saying, ok, I want to top it up with Tusl, and I want to have it linked and managed via this specific, specific hash. Cool. So now you have a pride balance on chain, but what does that do for you? Well, most interesting you can do with it is send money to someone else, obviously. And the way you do that is using zero knowledge proof, which is, just as most of you probably already know, a mathematical argument with which you can prove knowledge of a certain value without actually revealing what that value is. In this case, obviously, the idea would be, is you have these commitment hashes stored on chain, which are hashes of secret values, and you want to prove to someone else the program, in this case, that you know what these secret values are without actually revealing them to the world.
00:10:06.220 - 00:10:36.724, Speaker A: And for that, you use zero knowledge proof. And then once you've generated that zero knowledge proof, you send that off to the elusive program. It verifies the proof. You know, if it runs smoothly, it says, okay, I now know that those funds belong to you. And because of that, I'll allow you to, you know, send two of your ten sol and your private balance to someone else. One thing that might come to mind at first, if you think about this, is like, wait, so that means if I go to the block explorer, what you'd see is Niko topped up his private balance. Niko submit a zero knowledge proof, which led to two Sol being sent.
00:10:36.724 - 00:11:17.866, Speaker A: In that case, you wouldn't have gained any privacy at all, just because everyone can see I submitted that zero knowledge proof. So for that reason, what we actually have is a little intermediary step, which are relayers, which means when I generate my zero knowledge proof, I submit that zero knowledge proof via HTTPs to a relayer. He encodes it in a Solano transaction and submits it for me. And because of the way zero knowledge proofs work, he can't actually adapt any of the values inside of that proof without making the proof fail. So through that, we've just achieved privacy, because in that case, if you would check in the block explorer, you'd see, okay, Nico topped up his private balance with tensoul. And, you know, amongst all these transactions, one of those probably came from me, but they all were submitted by relayer number two, for example. So through that, we can.
00:11:17.866 - 00:11:50.594, Speaker A: Yeah, achieve privacy. So now on to the more interesting stuff, which is how you can build privacy into your own app. You can do this with elusive SDK. And this is yeah, rather straightforward. Basically, it's a three step process. And let's walk through what we do if we want to create the most simple app, which is just a little app with which we can manage an on chain private balance and send funds to someone else from our private balance. So the first thing you do is you generate your elusive instance here, which has, you know, the last two parameters I think will be familiar to everyone.
00:11:50.594 - 00:12:18.142, Speaker A: The standard connection. Use your favorite RPC and, you know, the key pair that's supposed to be the owner of that. The first part of this, the seed here, is what we actually derive those secret values from. So you can think of this sort of like how on a wallet you have a little pin or something like this. This is similar to that, just a little cryptographic parameter from which we can derive values that should only be known to the user. Okay, so now we create our elusive instance. What can we do with it? Well, let's create a private balance with it.
00:12:18.142 - 00:12:59.254, Speaker A: So what we do is we call the build top up transaction. We say how much money you want to submit and what token you want to submit it with. We specify what token we want to submit it with because currently elusive supports three tokens which are lamports or Sol, rather USDC and USDT. So yeah, just specify how much you want to top your private balance up with and how much you want to send and what type of you want to send. Sorry. Yeah, then you sign that transaction and simply call elusive elusive transaction. And with those four lines of code, suddenly you've just created an on chain private balance from which you can privately interact with the world, which is pretty cool.
00:12:59.254 - 00:13:27.502, Speaker A: Yeah. Next, what can you do with your on chain credit balance? Well, the most straightforward thing that you can do is send money to someone else. And again, pretty straightforward as well. You have your elusive instance here. You build a send transaction. You specify how much money you want to send, who you want to send it to, and what token type you want to send it off. Between those two steps, it does all the ZK stuff generates the zero notch proof, encodes your data, all that stuff.
00:13:27.502 - 00:14:01.012, Speaker A: You get a nice little package out called sendata. And again you call it elusive sendillusive transactions transaction. Send it off. And yeah, you can add some logic afterwards as well to verify a transaction I went through. And if all of that works, you've just created a private transfer with which you just privately send money to someone else. And all of that in five lines of code, which is pretty cool. We also have some other smaller features that might be helpful as well if you're integrating this inside of your app, because your users probably will want to know what their current private balance is.
00:14:01.012 - 00:14:41.166, Speaker A: And, you know, there's the top up feature with which you can top up your private balance. Maybe someone wants to withdraw from the private balance so, you know, you can build a withdrawal transaction. Yeah, same interface as always, elusive dot send, elusive transaction, you know, pretty straightforward stuff. And again, so if you do all this stuff, what you'll basically have built is a simple private sending app. And that would qualify for the two criteria I described at the beginning. This would be, you know, low cost, just because it's on Solana and because of the systems running underneath. And it would be, you know, quite composable and easy to use just because it's quite easy to implement and, you know, implement your own specific app.
00:14:41.166 - 00:15:20.226, Speaker A: But it doesn't satisfy the third constraint I specified yet, which is compliance. Like, you might build this, but then, you know, before you deploy it to Mainnet, make it go live, integrate directly in your protocol. You're like, ah, what if, you know, someone uses this for bad and end up getting in trouble for this? Or maybe not, but just in case. And so for that reason, we have compliance systems in place. And you can think of this as sort of a compliance ladder. So to say we have the lowest level, which would be the ground, basically, where you just ignore the rest of this talk and just ignore this part and simply just do. You're happy with the first part, but if you want to have a little bit of compliance, the lowest step, the lowest rank we can provide for you is ownership proofs.
00:15:20.226 - 00:16:42.776, Speaker A: And the idea of an ownership proof is, as the name basically describes, is given a certain send transaction that you can see on chain, you can generate a proof if that transaction came from you, obviously, that you're the person who submitted that sent transaction, which under the hood, basically all you're doing is you're generating the proof, again, that you own the secret values and that created the commitments that were used for that specific sent transaction. And the interesting thing to notice about this here is you're just proving ownership of a specific transaction, but you're actually not leaking any information about your private balance, or more importantly, of the public key that we use to create this private balance. Which is good, because that means you just prove ownership of a specific transaction, but also has a slight drawback, which is why you might be wondering why there's like some date time stuff in the constructor there, which is since it only proves ownership of the transaction, and not much more than that. If I would generate that proof and give it to you, and you would give it to someone else, you could just pretend that you're me, just because the person would be able to verify, okay, you know, this guy, since he generated this proof, he must have been the owner of it. Yeah. And since we want to prevent these imposter situations, what we actually have encoded inside the proof as well is the time at which it was created and a time at which it should expire. So that means when you create this ownership proof, you might say, okay, I want this proof to be valid for one day, or I want this proof to be valid for 1 hour, something like this.
00:16:42.776 - 00:17:11.527, Speaker A: And then when the verifier, then, you know, verifies this proof, he has to specify the current time it is by him. And when the proof is then verified, it checks. Okay, is the time that the proof was created with smaller than the current time. And if that's the case, then the proof passes. Otherwise it says, sorry, expired proof. And yeah, note this, is that something that's encoded inside of the zero notch proof, not inside of our top level SDK, obviously. So that means if someone were to use some other proof verification system, it would still not work.
00:17:11.527 - 00:17:57.764, Speaker A: You wouldn't be able to get around it so easily. But let's say second case, your counterparty is a little bit more strict. They don't just want proof that a certain transaction came from you, but they want, you know, they want to know what the public key behind it was, what your private balance at that time was. In that case, we have, you know, the second rang of the compliance ladder, which would be the viewing keys. And the idea of the viewing keys is somewhat similar. It's the idea that you can generate a viewing key, as the name implies, for a certain on chain private transaction, and you can generate that viewing key and pass it to someone else. They can use that viewing key and simply put in the transaction that you told them belongs to you, using that viewing key, they can decrypt your private data to some extent so they can see, okay, this was the person who sent it off.
00:17:57.764 - 00:18:32.312, Speaker A: This was their private balance at the time that they sent it off. Yeah. This is of course a much stronger step than the previous one, because if you give someone a viewing key, they will know the actual key behind it and reveal a lot more about your privacy. But as mentioned, we want to build an SDK with which developers have the option to choose how many, whichever level of compliance they feel is appropriate. And in some cases, this level of compliance is more appropriate or is needed. Yeah. And the final level of compliance, sort of the peak of the ladder, so to say.
00:18:32.312 - 00:19:43.184, Speaker A: So we call trusted third parties TTP. It's not on the slides here because it doesn't fit like this exact format here. But basically what you might have noticed about these two is that both of these require the user's seed, basically. So that means only the user is able to generate these ownership proofs and these viewing keys if he so chooses yourself. So it means if you're the owner of the protocol, you can't actually force your user to generate this unless you hold his private keys, obviously, and his seed. Yeah, and in some cases, obviously, in 99% of cases this is good, but in 1% of cases this is bad, which is if someone uses your wallet to make an illegal transfer. And in which case, the idea of a trusted third party is at the very beginning, when you instantiate the elusive instance, what you provide beyond the normal parameters is an additional parameter, which is a trusted third party key, and which means from that point on, every single transaction that is privately submitted that is privately encoded also basically has an additional key that can be viewed by the trusted third party, which means you don't only have one person who has access to this privacy yourself, but also a trusted third party.
00:19:43.184 - 00:20:27.494, Speaker A: Again, this is probably the largest type of trade off and in most cases might not be fully appropriate. But in some cases, where legislation demands this or for whichever other reason, it's optional, you can do that as well. So again, to reiterate, the main idea here is not to force any compliance on anyone who doesn't want that to have, or rather give developers the option to choose the type of compliance that they think is ideal for their specific use case. Yeah. So, yeah, and that's pretty much the main idea of what you can build with elusive. You can easily build private transfers, you can easily build compliance into it. If you want to learn more about it, you can check out elusive privacy on Twitter or on our website.
00:20:27.494 - 00:20:56.114, Speaker A: Yeah, right now we're on devnet. We have a private beta going on right now. So if you want to try out our SDK or if you have some cool idea to build with it, reach out to us. We'd love to give you access to the SDK itself, and documentation should be very cool. In addition to that as well, if you check out our Twitter, you'll see we are actually hosting an event later today, I think at 03:00 p.m. Where you'll be able to attend, have a chat with us, have some free drinks, grab some free merch. Should be cool.
00:20:56.114 - 00:21:38.754, Speaker A: Yeah. And so for the remainder for the next five minutes, also, quick Q and A. What any other questions you might have about elusive or privacy on Solana in general? Yes, sorry. Hello. Thank you. That looks amazing. I'd like to ask how much.
00:21:38.754 - 00:22:17.834, Speaker A: How much does it take to actually do a transaction? Like, you had a wait there, but what is behind that? Like maybe 100 transactions or something like that, and also generating the data for a transaction, is it like a hard time ask, and can this be done on a mobile? Potentially. Thank you. Okay. If I understood your question correctly, the first part of your question was like, how many transactions does it actually take to do the pride set? And the second part is, can you do that on mobile? Right. Okay. Yeah. So for the first part, how many transactions does it actually take? So for the top up, that takes about five transactions.
00:22:17.834 - 00:23:09.988, Speaker A: And for the proof verification, for the actual sending, that takes around 30 to 40 transactions. The reason it's 30 to 40 is because we optimize the proving system in a way that, depending on the proof parameters, it might take a few transactions more or less. But the cool thing is that the way we have our SDK set up is that through this relayer system, you don't actually have to worry yourself about submitting all those three transactions and making sure all of them actually went through and all that stuff. The idea is really that you leverage the relayers for that, that you send one transaction, your zero notch proof to the relayer. He takes care of submitting the initial transactions to initiate the proof verification, and also making sure that the further 30 or 40 transactions actually go through and your proof is actually verified and your private send goes through. The second part of your question, whether this works on mobile as well. We haven't tested as much on mobile yet.
00:23:09.988 - 00:24:10.630, Speaker A: However, we are confident that with the developments in, we're using gross 16 proofs, which has a ton of tooling for it already, which is one of the most popular tooling ZK tooling systems at the moment, which has been optimized a lot for all types of systems, which makes us quite confident that we can get this working on mobile quite well as well. Hey, there. I was wondering, is the code open source, and can we go check it out somewhere? Could you repeat that? I didn't quite understand your question. Yeah. Is the code for the contracts and everything open source? And where can we learn more and sort of look at the code and stuff? Is the code open source? Was the question. Yeah, okay, sweet. Yeah, so yeah, the code for this is currently not open source because.
00:24:10.630 - 00:25:05.194, Speaker A: Yeah, we're closed beta. We're currently on Devnet and we will be launching on Mainnet end of the year. But at the moment when we will be launching on Mainnet, all this code will be open source and you'll be able to check it out simply on our GitHub. Yeah, right. Cool. I don't think I see any further questions or am I missing someone? Let me ask one more then. What is the kind of, can you just dig a bit into what is the kind of zero knowledge math? Is there maybe how it does compare with bulletproofs or ZK snarks or something? Yeah, for sure, we use ZK snarks.
00:25:05.194 - 00:26:17.778, Speaker A: And the proving system that we use is graph 16 over bn 254 curve. Yeah, the reason we use zksnarks is simply for the case that zksnarks have the cool property of having the prover. You know, the proving system is a little bit more intensive, but the verification system runs basically an o of one, which is super cool for multiple reasons. Because if you essentially have a proof that, whose verification steps are pretty much the same no matter what the actual proof encodes, that opens up a lot more opportunity, which is also something that we're working on as well. Because at the moment our main proving system supports, as I showed in the presentation, privately transferring funds. But the cool thing is, since we've optimized this prover and we've built this whole infrastructure, since the verification process is pretty much the same, no matter what the proof encodes, whether that be a proof that encodes a private swap or private lending or even a private NFT trade, we can reuse pretty much all of our infrastructure for that as well, which is something that we'll be shipping earlier next year, where you'll be able to submit just a zero notch proof circuits, which in most cases is very short. For example, in our case for this, the zero notch proof circuit itself is probably like 300 lines of code or something like that.
00:26:17.778 - 00:27:43.592, Speaker A: And then just leverage your infrastructure to verify any type of zero notch proof that you would like on chain. So yeah, that's the rationale where we chose ZK snark specifically for that. So would I be able to use this to build something like Dark Forest, like the game Dark Forest? Could you use this to build something like the game dark Forest, you said? Yeah, yeah, yeah, for sure. So you would be able to use this again. At the moment, the main process is for basically just privately sending and receiving funds. So you would have to do like, some extra hacks around it, so to say, to build something like dark forest. But yeah, as mentioned earlier, at the beginning of next year, we'll be shipping general purpose zero knowledge proof verification vms, which will basically allow you to define what the zero knowledge proof should encode and simply use our infrastructure to verify it.
00:27:43.592 - 00:28:22.904, Speaker A: So I think in that case, with that specifically, building dark forest should be very straightforward or not very straightforward, but definitely possible. Yeah. How do you, or what are your plans for the trusted setup for the Zksnarks? Yeah, so our plans for the. That's a good question. So our plans for the trusted setup is. We actually plan to. Yeah, at the moment for the devnet, we just have a local, we have a trusted setup where basically you have to trust us to do it correctly because we're not using real funds for that yet.
00:28:22.904 - 00:29:07.044, Speaker A: But beyond that, it's simply going to be again when we launch on Mainnet, we'll allow anyone who wants to sign up participate in the trusted setup and, yeah, be a little piece of elusive history and contribute to the trusted setup. All right. I don't think I see any more questions. Sweet. All right, well, thank you for your time, everyone then. Thanks, Nico. That was awesome.
00:29:07.044 - 00:29:29.800, Speaker A: So we're not going to start the next panel until 1040. There might be people in transit that are trying to get here in time for that. And so we're running a little early. So you have about a five to ten minute break. Now is a really good time to grab that breakfast, breakfast spread behind the stage here in the next room. I just had a little bit. It's fantastic.
00:29:29.800 - 00:30:03.742, Speaker A: You could also visit Ui dot asics.com. Get a load of these light mode and dark mode sneaks that are only for sale for a couple more days. Get your USDC ready. Are there any dgods fans in the audience? Dgods dust labs. Yeah. So there is a secret dust labs badge Nft that comes with every purchase. It's not advertised, but once you've made your shoe purchase, it pops up on your order confirmation screen.
00:30:03.742 - 00:30:24.760, Speaker A: So take a look at the asics UI dot asics.com site. Get your USDC ready. Make that purchase, you get two nfts plus the chance of a step in. NFT airdrop of the shoe. So physical shoe, digital shoe all in one package. Yes.
00:30:24.760 - 00:30:51.184, Speaker A: We have about five to ten minutes. We'll be back with the next panel then. Thanks, everybody. All right, all right. All right. Welcome back. Hopefully you are hydrated, well fed, ready for some more deep learning? We now are going to bring up a wonderful gentleman who delivered a very impressive, well attended talk at the Paris hacker house.
00:30:51.184 - 00:31:20.124, Speaker A: Anybody remember the Paris hacker house? Yeah. All right, so this is Olivier, CTo from Sonarwatch. And this talk is a dive into Solana's data structure. A deep dive into Solana's data structure. So backed by popular demand. Olivier, come on up, my friend. Hello, everyone.
00:31:20.124 - 00:32:03.722, Speaker A: Olivier from Solanawatch. And today I'd like to talk with you about Solana data structures. So let's dive in. Okay. First thing first, if you are not really familiar with the Solana blockchain, what we call programs, also known as smart contracts, on other blockchains. So as soon as I will speak about program, it will be kind of a smart contract. So on the Solana blockchain, we have three kinds of accounts.
00:32:03.722 - 00:32:52.140, Speaker A: The first one is the data accounts that will store all the data that you need to run your programs. The other type of accounts that we have is the program account that will store the executable code. So basically, the program accounts are the smart contracts, and then we have the native accounts. But the native accounts is for the Solana blockchain working well. So we don't really care about it right now. We are more focusing about the data accounts and the program accounts. So below here we have the program account, any program account.
00:32:52.140 - 00:33:43.054, Speaker A: So a smart contract is owned by the system program of Solana. And on the blockchain, it will say that it's an executable account. It means like, it can execute some code, and the data is stored on the data property, and it's the executable bytecode that the blockchain will use to execute the smart contract, the program. On the other side, we have data accounts. So this is, this one, the data accounts belongs to a program account. So let's say any program account can create data accounts that will store the data. So the owner of the data account is the program account.
00:33:43.054 - 00:34:21.014, Speaker A: And we can see also here that it's not executable, which means that it's only data. We are not running any code on that type of account. It's just for storing the data. And here we have also the data on this example is just a counter set to one, just another example. Here we have the token program. The token program and the token program can create two different types of data account. It can create the tokens that we have here on the left.
00:34:21.014 - 00:35:17.908, Speaker A: On the right, we have the token accounts. So let's, let's, let's. We can see here that we have the USDC mint, which is a data account owned by the token program, and we can see some data on it, we can see the supply, the supply of the USDT, we can see the decimals, we can see the freeze authority and some other data. And any token owned by the token program is an account of 82 bytes. So this is for the tokens. And let's say you have to hold, maybe you want to hold some USDC or USDC on your Solana wallet. You need to create a token account which is 165 bytes.
00:35:17.908 - 00:35:59.088, Speaker A: And this account data will store some data about who is the owner of this account. So the account GxJT, the owner of this account is hack h. So this is my demo account that I use for hacker houses. So this is why it's a vanity address. And then we have the means, so the mean is EPGF. So this is the same address that we can see here. So it means that this token account will hold some USDC, and under that we have the amount.
00:35:59.088 - 00:36:56.522, Speaker A: The amount here is 75,000. But you need to keep in mind that you will need to divide this number by ten six, because USDC have six decimals, because usually we don't want to deal with decimals on the data that we store on the blockchain. So that's why we use big number that then we divide it by ten six to do not deal with decimals. And you have, you have some other accounts, let's say here we have a second account, also owned by me, but the mint, the mint is different, is the mint of the msoul. So if you don't know the amsel, it's a liquid token, liquid token on the Solana blockchain. So here we can see that the amount is 500. But you need to keep in mind that you will need to divide this number.
00:36:56.522 - 00:37:44.222, Speaker A: So here is how the data is stored on the blockchain we can see here. So here is the structure of the token account. So the token account was this, sorry, was this part on the right. So the data is stored this way. So here we have the 165 bytes, each square is a byte. So we have our 165 bytes, and this is how the data is structured. 1st, 1st, 32 bytes are dedicated for the mint, next 32 bytes are dedicated for the owner pub key.
00:37:44.222 - 00:38:38.864, Speaker A: And then we have the amounts, we have also some other information that we don't really need to understand right now, but we have the mean, the owner, and then the amount. So this data use the borch here, the borch standard to be encoded and decoded. So yeah, you need to keep in mind that all the data stored on the Solana blockchain, you need to use the Bosch standard to decode it. And if you want to put some data on it, you will need the Bosch standard to put data on the blockchain. Yeah, so one tool that I really like to use is the solver decoder. It's a website that will help you to decode the data that you can find on the Solana blockchain. So you will paste your data account address on the website.
00:38:38.864 - 00:39:29.434, Speaker A: It will show you. So the address, it will show you the owner of this data account. So here GxJT is owned by the token program and next we have the data. So here it's a buffer, but the solver decoder will help you to decode this buffer of 162 bytes. So we are telling to Solgebolge decoder website that the first property is the mains, it's a public key. So a public key is 32 bytes on the Solana blockchain. So you are basically putting the same fields that we have here and the solver decoder tool will decode this for you.
00:39:29.434 - 00:40:29.490, Speaker A: Now we'll see some examples on how to directly fetch the data on the blockchain using the Solana websterii JS NPM package. So this is a bit more take slides. But basically what we have to do is to create a connection. The connection will handle the communication between you and an RPC server. Then you need to create a public key. This is the address of the token account that you want to fetch. And then you just need to use the get info method from the connection object to get the data stored on the blockchain.
00:40:29.490 - 00:41:11.174, Speaker A: And here we can see on the output that we have a data, a data field which is a buffer. And we can see here like the buffer start with c six FA and it's exactly the same one that we have here on the previous slide, c six Fa. So yeah, this is, we can also see that it's not an executable. So it means that this is really data. It's not a program that executes code, it's only data. And we can also see the owner which is at the token action program. So this is how to fetch the data from the blockchain.
00:41:11.174 - 00:42:13.132, Speaker A: But as you can see here, the data is not really usable because it's a buffer and you would like to have a real object that we can use. So you need to decode this buffer. So to decode this buffer you can use whatever you want, but here in the example we'll use the metaplex bits NPM package that will help us to decode the buffer that we had previously and it will use the Bosch sender to decode the buffer. We will create an account structure with the properties that we've seen before and then we'll be able to decode our buffer. So this is here. So this part of the code is the same as the previous one. So we are creating a connection, fetching the account info.
00:42:13.132 - 00:43:00.712, Speaker A: So here on this object we have buffer and then we have a buffer on the data property. And then we use the token icon struct that we just created on the previous slide to read the buffer. Here is the buffer and basically the post account will be this object. And here we do not have a buffer anymore. We have a real object that we can use to trying to understand what's happening on the blockchain. So here we can see that the amount, as we seen before, the amount is 75,000, but you need to divide it. This is what I am doing here.
00:43:00.712 - 00:44:26.954, Speaker A: I divide it by ten to have the real amount. And so yeah, that's a good example, but what we'd like to do is on the previous slide you can see that here, it's the address of my account and we paste the address here. But let's say you don't know the address that you want to fetch because this address like it's hard coded and usually you don't know this address. So one thing that we'd like to do is let's say the token program have a lot of token accounts and one thing that we'd like to do is maybe fetching all the token accounts belongings to specific user. So let's say you want to fetch all the token account belongings to this owner. One thing you'd like to do is okay, please may I have all the token accounts of 165 bytes and with the field owner set to this public key. So this is a thing that you can do by using the get program accounts method.
00:44:26.954 - 00:45:29.666, Speaker A: And basically you will be able to filter all the accounts created by the token program with specific keys and specific bytes. So I did create a workshop with some exercise and example to show you how to fetch this kind of information. So this is available on this link. You can also scan the QR code and by following this workshop you will be able to fetch any data that you want to fetch from blockchain and you'll also be able to decode this data. So if you are a tech guy. And you really want to try to understand how, how the data works on the Solana blockchain? Definitely. You can, you can go on the GitHub repository, train the exercise and.
00:45:29.666 - 00:46:00.424, Speaker A: Yeah, that's, that's it for me. I don't know if you have any question. I think we have. Yeah. Five minutes left. Yes, hello. Yeah, what do you guys do at Sonarwatch? Thanks first of all for the great introduction about Solana's data structures.
00:46:00.424 - 00:46:49.730, Speaker A: But I would love to know in context, what do you guys do at Sonarwatch? Yeah, so what we are doing at Sonarwatch is aggregating all your DeFi position. So it's a defi dashboard that tracks all your defi position. So let's say you've put some tokens on any DeFi protocol, let's say orca, Solan radiom, or any protocol on the Solana ecosystem. Your token are not anymore on your wallet. So you are not able to see them on any wallet you are using. And it's really convenient to have a single web page where you can see all your DeFi investments. So that's what we are doing.
00:46:49.730 - 00:48:08.884, Speaker A: Instead of having, let's say you want to track your investments on Solan or car radium, you'll need to open three tabs to track your investments. So what we are doing at Sonarwatch is showing all the information on a single web page so that it's really convenient for you to track you invest your investments and your performance on the DeFi. Thanks for the great talk. I would like to, I asked that yesterday already. Are you facing get program account limitation problems with what you're doing at Zona watch at the moment? And if yes, how are you solving them or how are you bypassing them? Yeah, really, really good question. Because it's like a day to day program problem that we have basically get program account that we, that I explain explained here is really heavily heavy tasks for the RPC's because as I said, it will try to filter all the token, let's say on the example of the token account, it will try to filter all from all the opened token accounts. So it's basically millions and millions of token accounts.
00:48:08.884 - 00:49:00.570, Speaker A: So it needs to filter all of that and get you back what you've asked with your filter. So yeah, it's a really heavy task for the RPC's. So right now we are not doing this. We are not running our own air PCs. We are using some providers and usually what they need to do, the provider needs to index all the account data because otherwise it will be too heavy task to run in real time. So yeah, basically they are using some, I think it's the JZer plugin that you can use to help with the program Archons method. Well, thank you.
00:49:00.570 - 00:49:41.038, Speaker A: I would like to ask, how does the geyser and Google bigtable API change the game of querying this account data? Could you speak a little? How does the gates park in for the validator to invest their data and also the Google bigtable API change? How do we get this data? You mean the data of the. Let me. This kind of thing. The structure. The structure. Like for the token account, the structure is open source. Yeah, it's a really good question because to decode the data you need the structure.
00:49:41.038 - 00:50:27.414, Speaker A: So for the token account it's easy because the code is open source and the structure is available for everyone. You just need to check the GitHub repository. But usually what you can do is maybe trying to reverse engineer the structure. But otherwise, if the program is using encore, you can generate a file known as an ideal. And the ideal will gives you all the structure that you need to use. So yeah, if the program use anchor, you will have access to the IDL and the IDL will give you the structure. So it's really, really convenient to have the ideal.
00:50:27.414 - 00:51:05.248, Speaker A: The ideal can be seen on the ABI on EVM blockchains. I think that's it. Thank you very much guys, for listening to me. Well done. I love that talk. I knew 165 was an important number, and to have it visualized like that with bytes, really, it hit home for me. Oh, got this podium coming up.
00:51:05.248 - 00:51:42.268, Speaker A: One note on housekeeping. The daily happy hour at the square. The Teatro Capitolio is at 04:30 p.m. I think it was advertised in a couple places as 06:00 p.m. But that's today at 430, sponsored by Masari. All right, so for our next talk, how to squash sim jacking, we're going to bring up Thomas, Thomas Olufsen, who is the CTO at Fyeo. So this is a good half an hour deep dive.
00:51:42.268 - 00:52:16.472, Speaker A: We will have time for questions at the end and then we'll have a little break at the end of this session leading into the rest of the deep dives for today. So as I've said before, breakfast is being served behind me. Bathrooms are out here to my right. The headphones, which are very popular today, I see, can be picked up at the front of the house right when you walk in. If you want to be able to just hear the presentations and the speaker mics very, very clearly. I know there's a little bit of echo in here, so. Yeah.
00:52:16.472 - 00:53:19.948, Speaker A: Are we almost good to go? Okay, we need a couple more minutes. Any questions? Anybody want to come up and tell a joke with me? Anybody want to know more about these beautiful ASICs GT 2000 shoes? They are limited edition. We're only going to make as many as you guys buy, and there's only two days left to get them. This is the best possible souvenir you can have from breakpoint this year. No, I mean, really, why we did this partnership with ASICS was to demonstrate the power of Solana pay. So if you weren't able to see my talk yesterday with Joe, the director of Web three at ASICS, this is a global distribution USDC on Solana is the only way to pay for these shoes. And it really demonstrates the power of what can be done when you have a global brand, a global, fast, cheap payments network, and everybody aligns on a vision for what the future of commerce could look like.
00:53:19.948 - 00:53:36.656, Speaker A: So give that a chance. Ui dot asics.com. Honestly, the more shoes we sell, the more we can prove to the world that Solanapay and payments on the blockchain are here to stay. So no pressure, guys. All right, I think we're good. Everybody's set up. Tech's all working.
00:53:36.656 - 00:54:16.134, Speaker A: Thomas. Give Thomas from Fyeo a very warm welcome. Thank you. Hey, guys, welcome to my little talk. We did this demo a couple of weeks ago for black Cat to normal security conference. This day's talk is going to be a bit more web3 focused. And I'm going to explain to you how the bad hackers have managed to pull off quite large heist against some of the largest crypto exchanges, including crypto.com,
00:54:16.134 - 00:54:57.654, Speaker A: bitfinex. Everyone has been hit by these attacks. So basically what we did at FIO was to, like, try to understand how do we protect against these types of attacks by actually researching how did they break in, how did they take over accounts, and how easy is this to actually do? So you, from a, like, builder perspective, can actually protect against these attacks, which are happening a lot lately. Lately. So a bit about myself. My name is Thomas Olufsen. I'm an ex hacker that I've been working with protecting computer systems for a very long time by actually breaking into and hacking them actively.
00:54:57.654 - 00:55:41.240, Speaker A: The reason why we do that, it's called, like, penetration testing to actually bash the system. And that's now what we have Fhir are doing at web3 as well, to actually break the systems and test the security before the bad guys actually do that. I'm sort of a digital nomad, so I work mostly from Tenerife and from Sweden. And also fire is very global company, so we work from all over the world. I'm also founder of Sect.org comma, which is a large scandinavian computer security conference, which teaches this. And I used to do capture the flag games, which is like legal hacking, so also won capture the flag in Las Vegas a couple of times.
00:55:41.240 - 00:56:22.054, Speaker A: And CTFs are actually a very good way to actually get into hacking without ending up in jail because you can actually go there and hack stuff legally and earn a lot of money. So except for that, I'm into climbing, diving and all other safe sports, motorcycles. And my current interest in web3 is actually going into decentralized identities. I think that's one of the space after Defi. So just a quick bit about what we do at FiO. So we do basically three things. We do domain intelligence and protection of domains, as well as web3 security audits and security checks.
00:56:22.054 - 00:56:48.524, Speaker A: But what we're here to talk about today is that text message or two factor authentication. If you don't want to see more of the talk, you can go now. It's like it's broken. It's totally broken. But I'm going to try to prove that today by showing you guys how you actually can send your own text messages and do all of this proxy and bypass everything. And actually, because everyone's like, oh, you have to have two fa. If you have two fa, you're safe.
00:56:48.524 - 00:57:27.746, Speaker A: And as we've seen on some of the large exchange hacks, no, with text messages, you're not safe. You should actually force your users into other, more forms of two factor authentication because text messages are just stupid. Yeah. So, yeah. Setting the scenes, the last couple of years, there has been a lot of really big attacks where people have, like, taken over accounts. They have sort of empty the account via phishing or smishing. How many of you know what smishing means? Have you heard the term smishing? Okay, smishing.
00:57:27.746 - 00:58:14.044, Speaker A: How many of you have got an unsolicited text message to your phone during the last couple of days which you didn't receive? Oh, yeah, most people have, because there's been an increase of like seven times 700% over the last year in what's called smishing. And that is like phishing, phishing links, but via SMS. And the reason for this, it's super effective because people, for some weird reason, are much, much more likely to click an SMS message than they are in email because we're so used to email spam that we're like, oh, this is just email spam. It's like, oh my God, it's an auditioning. But it's like, it says like, hey, this is from the police. You have people click because it's on their phone. And for some reason, people do believe that it's secure.
00:58:14.044 - 00:59:15.556, Speaker A: And then we've seen hackers actually using this technology to actually bypass the two factor authentication on some of these exchanges. And I got really interested because no one in the media said how they did it, for obvious reasons, because maybe you don't want to teach the criminals, but then again, the criminals will know this. So we actually went through some of the larger hacks and actually looked at the evidence files and find out how they did it. And I mean, basically we want to research how do they do it? What were the attack vectors? What do you need to do? This is an expensive operation. Do I need to buy a small country to pull this off? Do I need to hack a telco operator? Or can I do it with small means? And then again, how can we help other builders like yourself protect against this? So one of the biggest one was crypto.com that we saw getting targeted by this. Their two fa bypass exceeded 30 million in withdrawals from accounts.
00:59:15.556 - 01:00:01.220, Speaker A: Their bypass, they're forcing, of course, refunds from crypto.com. But again, not only crypto.com has been targeted by this. Most exchanges have, most exchanges have now implemented controls against this because obviously it's a bad thing. So basically, SMS phishings, we are seeing a lot of the message say there is a seven times increase. And the reason for this, it's really hard to verify the integrity of the sender, of the messages. People just assume that you can only send messages from your own phone number.
01:00:01.220 - 01:01:02.990, Speaker A: However, that's not the case if you're looking into the smishing attacks that we were investigating, less than 35% actually realized that the text messages that they clicked were not from the right sender. So most people that get targeted by smishing attacks don't even understand that they have been targeted by a smishing attack. They think that their text messages are, are legit even though they've been hacked. So they like, hey, why is my binance account empty? It's like they don't expect that the SMS message were not from binance because we're so used to that being a security. And the reason for this, as I said, is that for some weird reason, it has higher implicit trust of looking at a SMS message that comes through than your emails are because you never trust your emails. And we have seen a much higher success rate in these attacks and the mobile functionality. I'm going to do some demos of this, so we're going to see how the actual mobile works.
01:01:02.990 - 01:01:37.106, Speaker A: And the mobile providers, they've been really good at picking up these text messages, so they will actually autofill them for you. So I'm going to demo that later on. But if I spoof a text message, the browser will pick it up, auto fill it for you, you just log you in. It's super nice. Oh, and then we want to research, how do these hackers find my phone number? So we actually did a lot of research together with Michael Bistrom to actually find a lot of phone numbers online. So we have indexed about 22 billion leaked passwords in our database. But now we also started looking at the telephone numbers.
01:01:37.106 - 01:02:14.026, Speaker A: How many telephone numbers can we find? So we have found 1 billion telephone numbers leaked online that we can tie to an email address. I'm going to demo that as well. So that's how the bad guys are, like, oh, say, oh, this is just a very, very targeted attack. No, it's not, because like, we have your telephone number, your telephone number, your telephone number. We have like maybe a third of all telephone numbers that are active in the world are already leaked on the darknet or in data dumps. So that's like a source that, yes, keeps giving. So basically, to do this research, we did what you can call like digital dumpster diving.
01:02:14.026 - 01:03:09.118, Speaker A: So we're looking through the really nice places on the Internet where you really want to go, like the russian hacking forums, the leak database trading files, where a lot of hackers online actually trade in this stolen data. The stolen data has become like a commodity online and there is so much of it that they nowadays give it away for free. If it's less than three months old, it's not worth anything. So then you just publish it for free. So if you into digital Amsterdam, there's a lot of funny stuff you can find. And also we have all the ransomware, so we generally go and hang out where the bad guys hang out, especially ransomware sites. They're like people that get hacked, like local governments, they don't have very good it budget, but they do have your phone number, etcetera, and they get hacked and they can't pay, so they just publish all their data online.
01:03:09.118 - 01:03:54.742, Speaker A: So we went there, scooped it up, bridge forums, and also there is like a new thing called like sales conference. Have you seen these mails? Like, hi, do you want to buy the attendee list of the breakpoint conference? I mean there's all of these lists that you can just buy. I have no idea how they get the data, but we did buy them. Some of them are accurate and some of them contain phone numbers. So it's like, and if you collect this, if you're a threat actor, don't want to hack the crypto community, having the breakpoint attendee list with phone numbers would be a great way to actually bypass people to fa. So there is actually these dump sites, they look like this and they have like all of these dumps just free for download. Anyone can register here, just go in.
01:03:54.742 - 01:04:34.616, Speaker A: There is darknet sites, but there is also on the normal Internet and these in the green there is how many leaked credentials they have. So it's like 800,000, 700 million. It's like it's quite large numbers of leaked credentials they just running around. So we index, as I said, 24 billion of them with about 1 billion telephone numbers. And this is just clop leaks is one of the ransomware groups. These are the data that's currently published now on their Darknet site. So we see like, yeah, we see a lot of.com
01:04:34.616 - 01:05:16.164, Speaker A: names and there is also a lot of crypto sites that are getting hacked, that's not public, that are just published on these sites with usernames and phone numbers. So that's interesting. So one of the biggest thing when we index this, facebook.com was a good give. They got hacked and they leaked 509 million, which we could extract 139 million phone numbers from Facebook verifications. IO, 68 million US extended cell phone feeds, 90 million phone numbers that we can tie to email addresses, people, data labs, LinkedIn. And then we have like breakpoint where we found 16,000 phone numbers.
01:05:16.164 - 01:06:01.696, Speaker A: So I mean these are the sources that we're looking for when building this database. So basically all your numbers are belong to us. So I am going to do a demo of this. As you see, we're coming into the demo space now. But so basically Brian unfortunately had to go home. We've been normally using, he's our CEO, co CEO of the company and he's a very good, like he worked a long time in tech, so he's been on a lot of sites, he likes the Internet, he's a very good test user. So we use his Gmail account to see how many phone numbers do we have for Brian? We have three.
01:06:01.696 - 01:06:41.060, Speaker A: And this is just also for GDPR perspective, I use Brian because I have permission to use him. But this is how much data we have on people and this is how quickly it is to get his phone number. So you could also do, if you want to search for Brian, you can also search for his phone number and see what other email addresses he has tied to that phone number. So you can do search phone number pie. Then we see we can do another phone number. Let's take this one. What was that one I did? Yeah.
01:06:41.060 - 01:07:19.154, Speaker A: So I'm going to search for this one. And there you see he has a lot of other accounts like Adam Gail. So we can actually tie people with multiple email address to one phone number. So why don't we send Brian a text message now, because I know he's back in the US and probably wouldn't mind. So boom. Now we send Brian a text message saying basically the same thing as this one does. It's like Google, suspicious account activity on your account, please verify your account, etcetera.
01:07:19.154 - 01:07:44.106, Speaker A: And obviously this doesn't go to, this goes to a site that we have set up, which proxies Google. And we can log in here with breakpoint. And I hope I cleared the cache. So I'm going to log in. And as you see, this looks really good. And this is not Google site. This is what we have, a proxy site.
01:07:44.106 - 01:08:53.203, Speaker A: So what's happened in the background here? If you look in the other tab, we actually logged in with two factor authentication and hijacked all the login data, including the two Fa password, which were sent and autofilled by the browser. The reason why we logged in now directly is did this. So I have to clear the cache for this to work because I already had a login session, which I did just behind, but it will autofill the two FA password along with the session cookies and actually send it to the attacker. This is the only thing you see being targeted by this. If you click the SMS, it's like unable to sign in, you signed in, you're unable to sign in and the hacker has your two FA password as long as your session take over your session from any browser. And that's how easy the attacks are. So basically they're made up by one spoofed SMS messages proxy service that automatically does a man in the middle attack between the targeted site.
01:08:53.203 - 01:09:35.144, Speaker A: And I have too many screens. There we go. So where we used to think about credentials as a username password or a hash, we now start to think about credentials pairs like everyone. Now how many has two Fa enabled on their crypto. So today we need to start thinking about credentials and leaked credentials being username, password and telephone number. The problem is not only us doing that, the attackers are doing the same. So if you have your username, your password and your telephone number, you can still do these things by redirecting you and sending the telephone numbers to the sites where you know you've been leaked out.
01:09:35.144 - 01:10:04.540, Speaker A: So we're currently able to detect one in ten email addresses and tie that to a phone number. In our research, yeah, when we did the slides, we had 500 million. We now have 1 billion telephone numbers indexed. But yeah, if we can do this, so can the bad guys. And I mean, this is the thing that is leading to losses. Crypto.com says we have a small number of users reporting suspicious activity.
01:10:04.540 - 01:10:47.242, Speaker A: On their count, they lost 34 million from 436 accounts. I wish I could say that that was a small number in my budget, but OpenSea has had, this has happened to multiple times as well. Of course, phishing is missing. Conjunction with nfts has been rife, especially before the NFT tanked, we saw a lot of attacks, again against OpenSea and where you have a fake site. And we're also tracking when they're registered because they register very similar domains. You have open e and opensees and you have so many sites that are really similar that these smishing attacks are tied to. So they have the phone numbers.
01:10:47.242 - 01:11:33.808, Speaker A: You go to OpenSea, IA instead of IE, etcetera or IO, and then they steal your two factor tokens, they steal everything and all your nfts are gone. And actually, in the funny thing that with this attack against OpenSea, they actually sent out the SMS in the mail to everyone saying, hey, we have a new contract address. The guys copied that email. It looked exactly, the SMS mess were exactly the same. And they did a copy of the contract, just pointed it to their own address. So everyone that went in and connected their wallet against the new Opensea contract did so to the attackers. All the nfts belongs to us.
01:11:33.808 - 01:12:26.804, Speaker A: They came in and wiped everyone by a smishing campaign. And then after, when we wrote these slides, this happened as well. So the most popular ways of doing this, there's two things. When looking at all these big attacks, the most popular way is the reset password is not working as intended. So a lot of the times if we have looked at Coinbase attack, it was actually that you go in, you reset your password, and then you could bypass to Fa because people haven't had, wasn't good enough at penetration testing, the sites so there were ways with like these security questions. Well, they could do a password reset without actually having the two FA code even though you had two FAA enabled. That's obviously been fixed.
01:12:26.804 - 01:13:11.160, Speaker A: So the account recovery process, change of phone numbers and then again SMS injection into already existing login. So let's say you login to buy, you first say to people like, hey, click this SMS and log into the site. Then you actually send them an SMS login and they will get the SMS and send it to you via one of these like proxy man middle that I'll just demonstrate on Google. Actually, I had the permission from Google to present that on Google. I don't have the permission to prove it on any of the. I can't do a fake crypto exchange site because they won't let me. But Google actually let me demonstrate on Google.
01:13:11.160 - 01:13:44.768, Speaker A: So this works regardless of the site and the protections we come into protection. And yeah, as I showed you smashing and phishing proxies against real site saving, the session cookies and the two FA codes. And then we have had Sim jacking, cloning porting in the US. It's easier than in Europe. You can basically just call people and say, hey, I would like to sign up for new. If I know that you have a lot of crypto, I don't know your phone number, I can say, hey, I'm currently on XYZ. I love to go to T Mobile.
01:13:44.768 - 01:14:27.480, Speaker A: This is my number, can you port it? And I pay. So we have seen a lot of incidents where you actually port over. The real users, especially like high stakeholders, have had the numbers hijacked by people social engineering themselves and doing what is called like sim jacking. So someone takes over your SiM card by saying that, I love to buy this T mobile plan. And the salespeople there, they shouldn't really do it, but they really want the sale. So they actually port over your number to your new line and then you can bypass two of so short stories. Two Fa is terrible for this.
01:14:27.480 - 01:15:04.736, Speaker A: And even if you look at Google, if you look at the account recovery process, they're normally flawed. This is how much data from my personal account is given out. Like, oh, he has another email address which is this, oh, he has another email address with this. Oh, his phone number ends with zero. Zero. So just by initiating a password reset forgot phone number for any account, I can also tie them to the other phone numbers that we have in our database and verify them through the account recovery processes. So, yeah, short history of SMS.
01:15:04.736 - 01:15:33.770, Speaker A: It was actually designed in 1985. That's not a new technology. And there was some data left between the voice packets in GSM, which they decided maybe we can do short messages here. So basically, the first message that was sent was Merry Christmas. And that's probably what it was intended for, to like send short messages, not to have it as a security function. There is no sender verification whatsoever. There's no check from which number it comes on the network.
01:15:33.770 - 01:16:07.004, Speaker A: So if you get access to the network, you can do that two ways. You can either have a phone and send it from via the cable. You can use any phone, more or less, or you can use gateway providers. There is a lot of SMS gateway providers that you can get access to. So it is technically as legal to setting your phone number to Santa Claus. So I'm going to try to show you how to send some text messages. Let me see.
01:16:07.004 - 01:16:41.064, Speaker A: So let's say I want to send an SMS message to my phone. I prepared some here. So let's say I want to send a text message from Google. Let's see if this works. Let's hope it works. It's always fun when it works. So that was Brian's, let's say here, normally smooth.
01:16:41.064 - 01:17:09.554, Speaker A: So let's say Binance. I want to spoof Binance because that's fun. And here we go. Binance. Your account has been blocked. And this is, of course, the funny thing is this shows up. As you can see, I do have binance, and this shows up in my real binance like log of messages, including my real two fa codes that actually come from Binance.
01:17:09.554 - 01:17:39.994, Speaker A: So it's very hard as an end user to see that this is fake. Okay, now the domain that I've done is like demo dot smishmash.net comma. And smishmash is what we're calling this technique. So of course this is going to be fake. But still, it's very hard for you as a user to actually see that this is not from binance, that this is actually from a fake account. And boom, if you go there and actually log in to, I have another one which is called crypto something which actually points right.
01:17:39.994 - 01:18:17.324, Speaker A: And if you actually go there and input your details, they will be hijacked by the attacker. And I mean, obviously if you do this live, you do a better domain than like demo dot smishmesh.net, because you do like a domain that's very much more similar to binance. And that's what we've seen. So, yeah, can you send basically from any. So we have developed some tools and these are published as well on a public repos. You can play with these tools as well.
01:18:17.324 - 01:19:01.784, Speaker A: So you can, as you say, just put the sender to anything you want here and just send SMS from any sender. That's how easy it is. You can link in the phishing site that hijacks the data and then login on your account. Let's see. So, and then it's like when we're looking at this, if the guys that are doing this professionally, they don't use like just one phone that I'm using. There is a lot of fun stuff you can buy from China. We hadn't had time to order, but there is like $160 you get, you see what that is? It's a lot of cables that you plug into a lot of Sim cards.
01:19:01.784 - 01:20:08.304, Speaker A: You can automate this and just run it on a professional scale. So the hackers that we're seeing, I love this one, like $508, you get like, I don't know how many phones that is, but it's literally a rack mounted phone that you can just use as a gateway and like spam SMS with basically the same thing as sending it from your phone. But they also come, they actually come with the marketing like 64 port bulk sending SMS device supporting IMI change by Atikmon and imie. That's like the sender identity of the phone. So these are chinese devices that you can reprogram to send from any device id known whatsoever. So yeah, SMS, this is what the bad guys are doing. They are buying one of these devices and then they find a dumb file for one of your targeted sites and they email and send SMS to all your users and send them to a similar site.
01:20:08.304 - 01:20:50.018, Speaker A: And this is quite funny because actually this is on the sales side for these devices. They have like your Google verification code is. So this is actually on the marketing material for these sites. The marketing them is like this you can do illegal activity with. So how this works in reality is you have a phished user, you have an adversary in the middle that is pointed to by this that has the decrypted password as I show you on screen and then goes to the real website and actually injects and steals all the two FA codes and the session cookies. So we already done the demos. So thank you.
01:20:50.018 - 01:21:29.040, Speaker A: I have 1 minute left for questions and answers, but I'm just going to talk about protection really shortly. Three major ways that you can protect against this. If you're running a, b, two, c, exchange, et cetera. Recaptcha has a hidden field where you can say that if someone goes this, if you tried this on crypto.com today, they're using recaptcha to solve against this. There is also cloudfront cookies that will mess and show that the site is being manned in the middle attack because as you saw on the sites, there is no certificate. Everyone's like, oh, but HTTPs, that protects me against this.
01:21:29.040 - 01:22:18.368, Speaker A: We set up servers that in real time request all the new certificates via let's encrypt or free enrollment services. So there's basically no way to see this unless you have the recaptcha or the cloud front and you could sort it with course headers. But unless you protect against these, your end users will not see that they're not on the real site because the proxy servers are so good today. Thank you very much for attending. Wow, that was amazing. I'm simultaneously terrified and yet I feel safer. So thank you so much.
01:22:18.368 - 01:22:38.916, Speaker A: Those attacks are getting very, very sophisticated. Yeah, Thomas. Okay, so now we have a 15 minutes break. Can use the restroom, grab some food, breakfast is still being served. If you just got here, we'll be back. And in 15 minutes with a deep dive from wormhole. I'm Josh fried.
01:22:38.916 - 01:22:55.644, Speaker A: That's it for me today. David will take care of you guys the rest of the day. Thanks. Enjoy. Breakpoint, day two. Welcome back. Excited to be here again at the workshop stage for day two of breakpoint.
01:22:55.644 - 01:23:25.166, Speaker A: I was here yesterday. My name is David. I worked on the grants team at the Solana foundation, so working with a ton of projects in the ecosystem to support public goods on the network. And so I'll be your emcee this afternoon and really excited to keep the programming going. It's been an amazing day and a half with some incredibly insightful presenters. The one tip I'll give you, I noticed a lot of you have these headphones that you can pick up at the back of the audience here. The room is a bit echoey, so it helps you just focus in on the content.
01:23:25.166 - 01:24:03.492, Speaker A: I would strongly suggest that you can pick them up right back at the back where you walked in. But let's keep the party rolling. I'm really excited to introduce our next presenter who's going to talk to us about how to build x dapps. How to build dapps that are cross chain in a really effective way way. So please join me in welcoming Chongor Kiss, who's a contributor to Wormhole Changor. Hey, everyone. I hope you're having as much fun as I am.
01:24:03.492 - 01:25:17.926, Speaker A: My name is Chongur case. I work at JMP, I'm a core contributor to wormhole, and today I'm going to be talking about XStaP frameworks, actually. So, yeah, so the first question is, what is an Axdap? So next app is an application composed of smart contracts that live on multiple chains. And this doesn't sound very profound, but actually it has a lot of implications on the way you want to design your smart contracts. Because, you know, if you have a Dapp that lives in a single chain, you have a lot of nice properties that are guaranteed by the smart contract runtime on that chain. But the moment you start going cross chain, there's going to be, you know, asynchronous communication, all kinds of different synchronization issues. So what I kind of want to talk about today is what is a good way to architect an Axdap? So before going into XDAP, let's just quickly review some of the properties of traditional smart contracts.
01:25:17.926 - 01:26:45.142, Speaker A: So here's a transfer function written in a language that looks a little bit like, and we've all seen programs like this. You know, most smart contracts kind of have these have these workflows where you have a function that users can call in a transaction and the contract does, you know, in this case it's a transfer function. So the contract takes two arguments, the from address to address and some amount, and the contract keeps on chain storage representing the balances of each user. And in the transfer function, you basically subtract the amount transferred from one user and add it to the balance of another user. This is a very simple program, but it already shows the usual workflow when you write smart contracts on a blockchain, which is that the balance's data structure is a shared object between all invocations of your smart contract. So this is a piece of storage that's globally available, and it's a globally shared resource. So in traditional smart contracts, execution is synchronous, thanks to the log based concurrency model of the underlying blockchain.
01:26:45.142 - 01:28:01.344, Speaker A: Now, Ethereum takes a global lock on the whole storage, and that means that transactions are sequenced. A single transaction is executing at any given time, and once that's finished, another transaction can start executing. Solana has a slightly more fine grained concurrency control, because the account model of Solana kind of statically exposes the resource acquisition structure of your transactions, which allows the runtime to execute transactions in parallel. But at the end of the day, you still avoid risk conditions because Solana is very careful about taking the right looks on your resources. Transactions are also atomic in a traditional smart contract runtime environment, which means, you know, when you start execution, the smart contract instructions, if there's an error or if there is, the transaction reverts, then none of the state changes are persisted on chain. So just the whole thing rolls back and you can try again later. And finally, there's a shared state between transactions, which means on chain storage is globally available between any two transactions.
01:28:01.344 - 01:29:09.852, Speaker A: So you could say that traditional smart contracts are easy. And of course there's a lot of subtleties to writing smart contracts, but from the perspective of resource management, they're pretty easy to think about, because you don't have to conceptually think about concurrency issues or any kind of race conditions, memory sharing, that kind of stuff. But xdapps are concurrent, and concurrency is very hard. So why are X Dapps concurrent? An Axdap is basically you have multiple traditional smart contracts operating on different chains from each other. So let's say you have a smart contract on Solana and a smart contract on Ethereum. And the way these smart contracts communicate is you run a smart contract transaction on one of the chains and then you maybe emit a message that gets picked up by the smart contract on the other chain. But the moment a transaction finishes on your source chain, all those state changes are, all the partial state changes are committed.
01:29:09.852 - 01:30:29.054, Speaker A: But maybe your workflow hasn't completed fully yet, because maybe you want to do some sort of cross chain swap and you're looking locking up funds on the source chain, and then you want to perform some action on the target chain by sending funds through. But you know, the target chain transaction might not succeed. Or, you know, if, if the transaction succeeds, you might have to think about synchronizing the smart contract state back and forth. So it's a very concurrent programming environment that I think currently we don't have a very good mental model for tackling. So yeah, cross chain smart contracts, cross chain communication is fundamentally asynchronous, fundamentally concurrent. Partial state changes can occur and workflows may be non atomic. So then the question is, are there any frameworks that can help us build safer xdapps or more principled xtaps? And the reality is that up until very recently, with these newer blockchains like Solana and Aptos, avalanche blockchains that have very fast finality.
01:30:29.054 - 01:32:08.094, Speaker A: Up until we had these fast finality blockchains, it didn't really make so much sense to, to architect smart contracts in a way where your different components and different chains communicate with each other because you just had to wait too long for a transaction to finalize on one of these chains and the user experience there wasn't so great. But now if you're communicating between contracts on these fast finality chains, the user experience can be improved because the latency is much much smaller, much lower. But because it's a very early days of cross chain development, there aren't really any cross chain XDAP frameworks yet. And so what I'd like to present now is not so much of an actual tool that you can use or actual framework that you can use to write safe axstaps, but more of a mental framework for thinking about xstaps and thinking about resources in a cross chain environment that is fundamentally concurrent and such that the concurrency can be handled in a principled and safe way. So here's a mental framework for a cross chain architecture. So contracts on different chains communicate with message passing. So message passing is a very nice primitive because it's fundamentally you run a transaction on one chain that emits a message, and the actual mechanism for emitting this message will require some cross chain interoperability protocol.
01:32:08.094 - 01:33:50.062, Speaker A: So wormhole is an example of a cross chain protocol that supports message passing in a generic way, which means your application can emit a message with some arbitrary payload targeting another chain, and wormhole will then take care of taking that message and delivering it to the destination chain of your choice. So an example of a cross chain message is, or an example of an application that uses cross chain messaging is the portal token bridge, which is built on top of wormhole. And the portal token bridge messages are responsible for for essentially describing token transfers that are to be interpreted in a cross chain environment, which means on the source chain you look up some funds, emit a message, which then gets picked up on a target chain, and then the message will allow you to mint wrapped assets corresponding to the locked up assets on the source chain. So that's a very clean message passing mode model. And the benefits of message passing is it's pretty agnostic to the runtime environments of your smart contract. So if you think about concurrency, concurrency control models of more traditional programming languages where you, let's say typescript, in typescript you have async await style concurrency. But the problem there is that it kind of assumes that you're operating in a sort of homogeneous computation environment where you can execute some action and then wait on the result.
01:33:50.062 - 01:35:15.204, Speaker A: In the same computation context, message passing is more agnostic to what's actually happening on the receiver side. And in particular, that's very beneficial for cross chain communication because different blockchains might have very, very different calling conventions as far as their smart contract entry points are concerned. Memory models, you know, the whole ecosystems might be, might be very different and so communicate with message passing. A very important thing for a cross chain application is that you carefully design the state machine of your user interactions. So if you have multiple components on multiple chains communicating with each other, let's say in like a ping pong style fashion where you're sending messages back and forth, you have to be really careful about designing the error cases of what happens if you're sending a message through that maybe represents a token transfer to a recipient chain, but maybe the smart contract on the recipient chain has a revert condition that doesn't allow the user to unlock the funds. Like what do you do then? You need to handle that error case very carefully. And when you're going back, provide some mechanism for the user to be able to unlock the funds that they put in.
01:35:15.204 - 01:36:10.518, Speaker A: If you don't think about these error cases, then users lose, lose their funds and it's going to lead to very bad user experiences. So try to keep interactive structure simple. That's another very important thing to keep in mind. And what I mean by this is it might be. So again, going back to a typescript example where you use async await style concurrency management. Usually what ends up happening is your program is going to make a lot of asynchronous requests and block on them all the time. And the interactive structure there is very much kind of a ping pong, lots of pings and lots of pongs style model where the problem in a cross chain environment is that you have to wait for finality on all these chains.
01:36:10.518 - 01:37:43.912, Speaker A: You have to think about state changes and all these change all these chains. So try to minimize the number of messages that you're sending across chains and try to do as much in a single transaction as you can. A very important next property is managing states locally to a session. So cross chain communication, the way I like to think about it is you have a very well specified protocol that describes a state machine of the different states of your transactions or the different states of the user workflow. And in each step of the way you might want to make some state changes, store some variables, store some local state in the transaction, like the on chain component. But a clean way to think about this is each interaction that users kick off should be represented by some sort of session session identifier that keeps track of the whole cross chain interaction. And when you're storing local variables on any of the connected chains, you want to make sure that the local variables are scoped to that session identifier, and you're not sharing resources between different parallel user interactions.
01:37:43.912 - 01:39:23.714, Speaker A: So that way you can avoid race conditions very cleanly. So you want to then once the workflow completes, and that's up to your application, what you're working workflow looks with, like in case of a cross chain swap, the workflow might be you send some funds through, perform a swap, and then maybe send the confirmation back. Once the workflow completes, then you want to commit sort of this local state, local state into the global storage of your application. And so these kind of, these kind of, this framework that I described is a nice conceptual framework, but it actually corresponds nicely to a principled concurrency framework that's called the actor model. And the actor model is a nice and principled way to think about concurrent, very inherently concurrent applications, which has clean denotational semantics, which means that it's very well understood and easy to reason about. So your application, a good way to think about structuring a cross chain application is that the components are structured as actors, which then communicate with message passing. Actors have internal state, internal state that's private to them, and they handle errors gracefully.
01:39:23.714 - 01:41:06.152, Speaker A: So a programming language, for example, that has an actor model built in is Erlang or elixir. And these programming languages have been, have been, you know, proven to have proven to be very scalable in a highly distributed environment, which is exactly the use case that cross chain application development takes place in. So just to think about the presence of cross chain development, I think that the existing smart contract models are currently inadequate for describing asynchronous interactions. So if you think about solidity, there isn't really any, or like even Solana Rust, there isn't really any nice native way to describe a smart contract logic that sends a message to another contract and then cleanly manages local state in a way that doesn't interfere with other sessions. So right now, you, as a programmer who's building a cross chain application, you kind of have to think about rolling these primitives yourself. So for example, in a Solana context, maybe you keep track of sessions that are happening between different user interactions, and maybe you localize all your storage interactions to PDA's that are derived from the session id and something like that, just to ensure that there's no overlap between your contract interactions. Because, you know, sharing global state between concurrent processes is just a recipe for disaster.
01:41:06.152 - 01:43:18.190, Speaker A: Like you have race conditions, double spending issues, those kind of problems and you know, that's the present, the future. Depending on how prevalent cross chain applications XDApps are going to be, I think the future, in the future there might be a potential for building out kind of domain specific programming languages that handle this cross chain actor model interaction pattern neatly, potentially with multi party session types, which is a family of type systems that allow you to reason about reason about the sort of the concurrent control flow of your applications in a way that you can write down what your application is going to do and then what you expect a distributed set of components to do before coming back to your control. So I think there's a lot of potential in building out these domain specific languages in the same way that, for example, move is gaining a lot of traction now where move, the purpose of move is to provide a principled way of describing assets or like, you know, just application specific resources in a safe way that guarantees no duplication and no copying. So similar to how move solves the problem of application specific resource management, I think there's a lot of potential in the future for building a language or maybe extending one of the existing smart contact languages with native native concurrency control primitives like actor models or recession types. And that's kind of the overall vision of what mental framework looks like, at least in my head. And yeah, thanks very much. Thank you very much.
01:43:18.190 - 01:43:55.418, Speaker A: I think building cross chain is something that every single project talks about and thinks about. So it was amazing to get that framework from Changor. So we're going to keep the train rolling really quick here. I'm really excited to introduce Josh from anchor to talk about again, a really important topic, which is optimizing nodes. So please join me in welcoming Josh. All right, good to meet you guys. My name is Josh Durath.
01:43:55.418 - 01:44:19.582, Speaker A: I work for a company called Anchor. We're going to talk about nodes today, so I want to do a quick audience poll here. How many of you have ran a Solana node yourself? Okay, good amount. Good. And how many of you are just curious like how nodes run some of the challenges? Maybe you don't know that much. Awesome. Okay, great.
01:44:19.582 - 01:44:44.742, Speaker A: And then I'm just curious how many of you provide node service as like a commercial offering? We got a few of those guys. Awesome. Okay, great. So you can talk about optimizing nodes here and what really peak performance looks like in our experience. So let me give you a little background on anchor. We are primarily, we're an EVM based RPC provider. Since then we've expanded into liquid staking.
01:44:44.742 - 01:45:29.180, Speaker A: We were first to market on Ethereum liquid staking, compete with Lido on that. We do gaming, SDKs, a lot of developer tools. And last year we got into, and we had a lot of clients coming to us in 2021 asking us to support Solana. And so we started this journey of figuring out what it would take to build on Solana at scale and offer that to our developers. So we began to do that, but it wasn't an easy journey. So we have an RPC for Solana. It's the fastest growing, was pretty, pretty, we had a lot of errors early on, a lot of bugs, but recently we've grown like 500% over the last three months because we fixed a lot of the issues that were on our RPC.
01:45:29.180 - 01:45:57.484, Speaker A: And that's kind of why we got invited here to do the talk. Today we do the main net, the Devnet, archival support, and we focus on geo distribution. And we're lowest latency in many emerging markets like Brazil, India, etcetera. So that's what we do. I'm going to talk a little bit about, like, Solana, and what are some things we need to consider as a community here. As you guys all know, Solana is really probably the fastest layer one out there. Right now.
01:45:57.484 - 01:46:43.240, Speaker A: Current transactions per second are 3300. Now obviously, theoretically, Solana promises 60,000 transactions per second, but right now we're hovering around 3300. So there's this kind of trade off that we get when we, as a layer one blockchain, when we do these higher transactions per second. Ultimately the node hardware needs to be better and have more resources in order to perform at that kind of level. So in many cases, you know, you look at like other layer ones I compared to here, binance smart chain started at 20 transactions per second. They forked Ethereum. Now it's grown to 200, give or take.
01:46:43.240 - 01:47:42.406, Speaker A: It goes up and down a little bit. Ethereum is limited at 25, but these chains need a lot, those other chains need a lot less on the node infrastructure in order to run that. So as a community, we basically have a trade off here by having that higher transactions per second. We need a lot more resources on our servers in order to support that kind of high transactions per second. So there's like this whole debate that many of you are aware about right now over, like, how much decentralization do we need, should we have? But ultimately, there's some challenges that kind of come into this play, because if the nodes are harder to run, they use more resources, they get a little bit more, less people are going to run them effectively. So, like Ethereum you can run from a raspberry PI, basically, right? You'll never run a Solana node on a raspberry PI many of you guys know here. So talking a little bit about that.
01:47:42.406 - 01:48:16.682, Speaker A: So, generally, Solana hardware needs about seven times the hardware that, like other EVM chains need, which introduces a lot of complexity. So last year, as Solana started booming, all the DeFi, the NFT summer, all of these kind of movements started really putting a lot of traffic in a load. And I think a lot of you remember the RPC issues that we saw as a space last summer, in 2021. I mean, like a year ago last summer. And since then, it's gotten better. But there's a couple reasons it's gotten better. It's by no means perfect.
01:48:16.682 - 01:49:23.632, Speaker A: So the first thing is that the Solana server program was really a game changer for this community. So I want to talk a little bit about that and recognize them because it was helpful for us and was helpful for other RPC providers. So, effectively, because of the pandemic, there was a huge chip shortage, which still is in somewhat in effect right now. But it was very, you know, 2021. It was really, really bad. If you were trying to get a server that met Solana's specifications, you were looking at like a six to eight month lead time minimum, right? So what the Solana foundation did is they went out and they worked with all these different data center providers to make servers readily available to this community, right? So they went out, they signed long term contracts, they worked on getting the right specifications, and then they made those available on a month to month basis to RPC providers like ourselves or individual developers. Right? So I think that this single handedly really changed the game, not just for Solana, but for many other different chains.
01:49:23.632 - 01:50:29.768, Speaker A: Because up until this point, a lot of the data center providers out there, even the cloud providers, were really, really skeptical of Web three use cases because miners for a long time had really abused, you know, server providers. Right? So recently, you probably heard about, I mean, this happened a couple days ago. The whole situation with Hetzner banning, like, the validators, which they have long talked about doing, just not that many of us are paying attention to it, but that's really because they got abused from the Chia protocol on their disks. Right? And so a lot of these providers were these bare metal server providers. These data center providers were very, very skeptical of working with any web3 companies, but this program really made them pay attention to the size of the market for us. Right. What I started seeing kind of after this program was launched after Solana started getting these servers for this community is all these other providers started coming into the market and building specific web.
01:50:29.768 - 01:50:48.956, Speaker A: Three teams to address Solana and other chains as well. So it's really, really amazing opportunity. These are just some of the logos. I've talked to all these providers. We personally work with latitude a lot. They're great. Zen Layer's vp told me that web3 is the fastest growing for them.
01:50:48.956 - 01:51:21.412, Speaker A: Same with vulture. Ovh now has a web3 team. They're all trying to figure out how to support the needs of our ecosystem, which has been really amazing that they're giving that kind of focus. So let's talk a little bit about the hardware, and then I'm going to talk about optimizing software here in a bit. So if you're not familiar with these components, this is like a standard server. The Solana official documentation. You can see the official docs here.
01:51:21.412 - 01:52:02.674, Speaker A: 16 core, 256 gigs of ram, NVMe one gig network. Pretty common for a server, but that was really based on like 18 months ago. And since then, the needs of the hardware to really optimize your node have increased dramatically, I'd say. And so now we're looking at an even bigger set of requirements. So I'm going to talk through these here and give you guys advice, especially if you're trying to run a node, maybe work with an RPC provider. These are all things that you need to know. So generally, like on the cpu, we see intel and AMD are usually the most common options.
01:52:02.674 - 01:52:34.054, Speaker A: We tested intel early on. It does have the performance, it does work, but we saw dramatically better performance with the AMD third gen cpu's. And it's not saying that intel can't be better than that, but for the kind of the cost performance trade off, usually in our experience, AMD was really the way to go. The other thing is too, the clock speed is very, very important. So I would not run Solana on anything less than saying like 2.8 GHz. That's really like a bare minimum.
01:52:34.054 - 01:53:11.244, Speaker A: It needs to really be more like four, to be honest. But you can run it at 2.8. And then the multi core is very important. So if you're familiar with other chains, like EVM chains, they run mostly on like single threaded. But Solana does benefit from like the multiple cores and multi threading. And then with the ram, we started out at 256, you might be able to still boot a node at 256 gigs of ram. But if you're doing any kind of high traffic, you're going to need up to 1 tb, right? So 512 is kind of the new minimum, you know, and that's going to keep growing over time.
01:53:11.244 - 01:54:04.226, Speaker A: So all these tests are done at 650 requests per second over not just one client IP, but, but hundreds, right. So that's kind of what it looks like for an RPC provider. Or if you're building your own node to support this network, and then on the disk you absolutely need Nvme storage, you need at least two drives, you could maybe get away with one. We use four in raid zero. I'll talk about in a second about why that's important. But the actual network stats, I just pulled these off our monitoring portal, receiving 200 megabits per second, sending up to 300 megabits per second within that one gigabit per second recommendation there. So if you look at these requirements, the average person is really not going to be able to run one of these nodes in their home.
01:54:04.226 - 01:55:11.404, Speaker A: And that's what we see in other chains, right. The decentralization you can run, I wouldn't recommend it, but you could run an ethereum validator right, from your house. I know some people that do, I don't personally do that, but you really need to be running these kind of requirements in a professional data center or some cloud, cloud provider, right? And so that kind of what we need to think about as a community is like what things do we need to do to improve the decentralization? So if people are not able to run these in their homes, how can we work with managed service providers, data center providers, cloud providers to really improve that decentralization, to get more nodes on more networks in more places? All right, to kind of go into the network side where you host your Solana node matters a lot. In fact, a very lot. And there's a couple reasons for this. So as you guys know, blockchain is peer to peer technology. So the nodes, both the validators and the RPC nodes are all talking to each other in the peer to peer protocol.
01:55:11.404 - 01:56:10.314, Speaker A: And so because as I mentioned here, you need that one gigabit per second kind of connectivity between to the network that starts mattering a lot when you're using your node, all right? So if you think about it, a node that you operate needs to talk to, a node that someone else operates through that system, and that receiving and sending of the bandwidth starts to factor into that. So what ends up happening here in the BGP and the layer three Internet services is if you're next to a cluster of other Solana nodes, you get benefit. You get a lot of benefit from that. But if you're in kind of one of these edge markets, it's going to be really hard for you to stay at block height and for your node to perform, which, in my personal view, is a huge issue for our community. Right? So here are some of the best locations. Us east, we have, like Virginia, Ashburn, Virginia. It's like the headquarters of the cloud.
01:56:10.314 - 01:56:44.918, Speaker A: You got AWS there, you got Google Cloud, all the others, right? New York, Atlanta, all locations where nodes perform on Solana really, really well. West coast is reasonable. Solana does a lot with Google Cloud. Google Cloud has a lot of regions over there, too. Singapore, reasonable. Europe, reasonable. But as we start thinking about bringing Solana to India, to South America, even to Southeast Asia outside Singapore, it becomes very, very difficult to keep a node at block height.
01:56:44.918 - 01:57:49.460, Speaker A: We are constantly trying to optimize the network between our nodes in those locations, but what we really need to do is just get more people running Solana nodes in those locations, which will ultimately improve the performance of the network and the distribution of network. So it will help your node out, too, but it will also help us just as a general community for that decentralization and that distribution. So one of the things that my customers ask me all the time, and you guys probably have the same question, is, why is Solana more expensive than, say, ethereum or some other network? So here are some of the challenges we face as a node provider in this space. So there's such little documentation on best practices. We've been lucky to kind of share information in forums like this with other providers, like Triton one and others. But most of what has been learned, I think, from my understanding, is just in, like, tribal knowledge. It's trial and error that people have done.
01:57:49.460 - 01:58:22.926, Speaker A: It's not documented on the discord or the official documentation. So what we can do is we can start documenting this stuff on the community space, is to do better and to share knowledge. Also, the hardware keeps increasing. Like I said, best practices, server specs. I think we don't have a lot of information on that. There's also issues, Solana, for its archive data, for its historical data, the Solana foundation uses Google Bigtable, which is great, but there's some issues with connecting to that. We fix this.
01:58:22.926 - 01:58:57.626, Speaker A: I'm going to provide you to a link to a pull request where you can get our code to do that. It's very simple. Every time Solana has had outages, as we've all been aware, that comes back to the node providers. We have to recover those nodes very quickly, which is hard. And sometimes in some instances, our data, our historical data has blocks missing when that happens, and we have to go and manually backfill those. As I already talked to you about the geodistribution, the geo distribution of peering issues comes in and that plays a fact. And then the updates are frequent, which is awesome.
01:58:57.626 - 01:59:57.106, Speaker A: Love that we have a lot of updates, but we have to stagger them because the nodes take a long time to restart, so it uses more DevOps resources. So for all those reasons, Solanza, we have to end up passing that cost off to our customers in order to at least break even or make money on a chain like Solana. I want to talk a little bit about the cost of supporting archive data, the rough cost. So to run a bigtable instance with any kind of meaningful traffic volume, it's really going to cost you about $8,000 per month that you're going to be paying to Google Cloud to do that, just in comparison to other chains. With Ethereum, go Ethereum under $1,000, you could run a server that has every block indexed right there. With Aragon, less than $250. Something you might not know if you're not in the EVM space, is that there's multiple node clients for Ethereum, and some perform better, others for different things.
01:59:57.106 - 02:00:34.594, Speaker A: So Aragon is like a rising star in the community and does really, really well with handling that archive data. So I think there's more solutions we could do as a community in this space. Now to talk about some of the principles of optimal performance. On the software side, we've already covered network and hardware. Solana tends to be, from the node perspective, generally stable. But what ends up happening is because of the speed of the chain, if you're trying to bring nodes to block height, they really can't be receiving traffic while doing so. So it's very difficult.
02:00:34.594 - 02:01:09.214, Speaker A: It's very hard to bring a node all the way to block height so it's usable while it's also serving any kind of traffic. Of course it could. There's many types of requests that can be still served while not at block height, but you need to turn off traffic when that node is recovering. And so this can be done with a very simple script. If the node isn't at block height, stop sending traffic to it, let it recover, test the block height again, and then resume sending traffic. That is what you're going to need to do to really serve traffic. And then, as I spoke to you earlier about having four NVme disks in raid zero.
02:01:09.214 - 02:01:58.600, Speaker A: If you can have a bigger drive on your node, you will definitely not need to make as many archive recalls, those that kind of expensive archiver calls to Google bigtable, which helps out a lot. So a general rule is 1 million slots, 750gb of disk space. So we personally, on our local nodes, use like six or seven terabytes of data just on our Solana nodes. So we don't have to always be going to the archive nodes or the archive service to, you know, to do that. So I want to have a few minutes for Q and A. So where do we go from here? How do we get better as a community at supporting our nodes? And I think, you know, a lot of these principles still apply to the validator community, not just the RPC community, because, you know, we get better node clients, we get better performance, especially on the hardware side. That does impact our validators, too.
02:01:58.600 - 02:02:30.890, Speaker A: One of the things, or one of the concepts that has been really, really valuable in the Aragon node client is the RPC daemon, or Daemon is run separately from all the peer to peer processes. And we've seen really good performance off that. I don't think that's implemented in the Solana node client, as far as I can see. But we think it would really help if we could get there as a node client. So that's my recommendation to the Solana team to do that. And we also need to start seeing alternatives to bigtable. So right now, if you want archive data, the only option is to use bigtable.
02:02:30.890 - 02:03:25.444, Speaker A: I actually love bigtable, but it's using 80 terabytes, which is costing us eight grand a month. And we'd love to be able to run that locally without having to query bigtable. And so I think getting the support from the Solana foundation on both those things would be very helpful to node providers, very helpful to our community. And then think about it. One of the things we've been working on with other chains, specifically with binance chain, few others, polygon avalanche, is this whole concept of side chains. So, you know, as we continually scale Solana, we're already really at the hardware limits at 3300 transactions per second. So as we grow as a community, as more people come into the ecosystem that is only going to go higher, how do we keep scaling with that effectively, we might want to start thinking about, you know, offloading some of that traffic to a Solana side chain.
02:03:25.444 - 02:04:16.964, Speaker A: That's just a recommendation, I think future focus, like for the future Solana roadmap, that we might start wanting to look like as a community. Okay, so one of the things, if you run a node and you can't, if your connection to bigtable drops, you can go to the GitHub for Solana Labs, go to pull request 26217, and we have a fix. It's just a simple one line code fix that will help you. It has not been merged yet into the official repo, but you guys can use that if you want to get better performance on that archive service. So that's what I wanted to talk to you about. We're excited to keep supporting developers in the Solano ecosystem and beyond, but I'd love to leave it up for Q and A. If anyone has any questions about the RPC service around performance, please come up and ask a question.
02:04:16.964 - 02:05:05.978, Speaker A: I think you have to go to the mic. Hey, thanks for all the insight. So do you guys publish performance data of your nodes, like a graph over time? How many slots you're behind or ahead of the rest of chain? The. We have it internally, we're working on an external tool that's going to make it, it's an open source monitoring service for nodes that we're going to publish to the community. So it's not available yet, but we do collect that internally. So, you know, it is a hard chain to keep up with. But it's just, we've been trying, you know, all the guys out there, quick note, alchemy, Genesis, go, and it's kind of crazy.
02:05:05.978 - 02:05:45.878, Speaker A: They're always running behind, and then our apps are like flip flopping old state, new state and stuff like that. Right? Yeah, yeah. Like I said, there's a lot of issues that, you know, as node providers, we're all dealing with. So I think the recommendations here can help us get better as a community. But yeah, we can absolutely publish that and we have a tool to do so that'd be awesome. Awesome. How do you measure the latency of your RPC nodes and how do you publish, like, how behind are you in processing those blocks through the RPC call? Sorry, really hard to hear you.
02:05:45.878 - 02:06:42.034, Speaker A: Okay. How do you measure the latency of your RPC service? Yeah, yeah. So we do measure our latency of our RPC service. What we do is we do a combination of different calls, both like stuff that regular node and the archive service will do. And we use little monitoring networks from like Google Cloud, we have like little scripts running in Digitalocean AWS, and then we just look at like the latency across all of our user and client data as well. And we have a formula for pulling that together to show it so keep in mind that when you look at a Solana latency RPC that is testing the latency from a point a. Right, which might be if you're running like, I don't know, like you really want some neutral data on that because it would be very easy to skew that data if the RPC provider is running the test in their own data center or something like that.
02:06:42.034 - 02:07:19.954, Speaker A: So you really need some like beyond like one point to test the latency. Thank you for that clarification. I do have a follow up question. Okay. I mean, from the moment a block was produced and the moment that block was usable by an RPC server, how do you measure that latency? Yeah, we have a tool for doing that, but we also have an enterprise service that we do where we have a special focus on exactly that, on making sure the accurate data is relevant to those users and most up to date. Yeah. Okay, one more question.
02:07:19.954 - 02:08:19.236, Speaker A: I think I'm going to get chased off the stage. Yeah. Thanks, first of all, for this presentation, and I really appreciate that you were emphasizing the problem of decentralization. And I would like to have like a sense of the future that you can imagine. For example, you were like saying that we can use side chains like afterwards, but could it be like possible in the future to, like, if there is like a problem of decentralization, that to lower down, like the requirement for like running a road, a node, so that I can road, I can run a node at home, for example, is that possible to imagine or not? It would require. So the question is, what is the future of a potential side chain? So one of the things we've been working on with polygon is the supernets avalanche with the subnets and then binance application side chains, which we were the majority contributed to. So really what the concept of side chains is, is we might say NFTs are going to be broken off into their own side chains, chain gaming traffic broken off into its own side chain.
02:08:19.236 - 02:08:54.656, Speaker A: And then those side chains will have a set of hardware that can run those. So what we would want to look at is can we bring down the hardware requirements on the side chains? And that would really help us with that decentralized effort. And on the main chain, is that possible? I'm sorry? On the main chain, like on. It's going to be hard, I think, you know, a lot of re architecture, but probably not for the foreseeable future. All right, thank you all so much. Enjoy the conference. Awesome.
02:08:54.656 - 02:09:50.994, Speaker A: Thank you so much, Josh. Great presentation. And again, something that everyone cares about which is operating nodes efficiently and effectively. While we're just getting set up here, I thought I would mention, as I said earlier, I work on the grants program at the Solana foundation, so I am not a developer. I work with developers all the time. But it's been amazing to be emceeing this stage, because I think the content here, even if you're not a developer, you can learn a lot from. But if you are a developer, it is incredible to hear from the people working for the top companies in the space who are solving the newest, biggest challenges, just to be able to hear, for example, yesterday from Dune analytics, etherfuse, Solana FM, all the presentations, presenters today, it's been really eye opening and really helpful for me, even as a non developer, and for all of the things that happened yesterday, all the incredibly exciting announcements and presentations.
02:09:50.994 - 02:10:56.954, Speaker A: I think what I saw on Twitter at least, as being the biggest, the one that got a ton of excitement from everyone in the Solana community, was the announcement of Fire dancer, which is the jump crypto. Crypto new validator client for Solana. Because I think that even though we know that Solana is an incredibly fast chain, as Anatoly said yesterday, having two validator clients means that reliability, speed, just everything is sort of taken to another level. And so it's been a pleasure to get to know the team at JMP and learn about the incredible things that they're doing. And just as we get finished, the setup here, I think there may be a few more of these headsets that you see people wearing around in the audience. I know this room is a little bit echoey, very beautiful vaulted ceilings here, but if there are some more back there, I would strongly suggest grabbing some because you can hear the presenters a lot more clearly and not have so much sort of background chatter noise. Even though, again, this space is absolutely beautiful.
02:10:56.954 - 02:11:37.052, Speaker A: And I should also say that there's refreshments behind me and restrooms off to the right of the stage. Also, we're going to be having a 45 minutes break right after this session, so there'll be time for lunch, time to step outside, time to stretch your legs, and then we'll have five more sessions in the afternoon to sort of finish it up. But this one will be the last session before the break. So I appreciate everyone's panic, patience and attention, because I know it can be. You can get a little antsy sitting in these chairs for a super long time. I think we're just getting the demo all set here. It's always exciting to do live demos, because there's always a little uncertainty.
02:11:37.052 - 02:12:31.782, Speaker A: But really excited for the presentation, and I think we're just about ready to go. Cool. Cool. Well, while we're fixing some of this presentation here, I also just to talk a little bit more about the grants program, which I help run at the Solana Foundation. I mentioned earlier that we focus on funding public goods, but I think a lot of projects that apply for our grant funding, they think that that means, like, bringing a lot of new users to Solana or helping people make money on Solana. And while those things are really good, or can be really good, that's not really what we mean when we say public goods at the Solana foundation. What we mean by public goods is things that make it easier for other people to build on the Solana ecosystem and help develop the network.
02:12:31.782 - 02:13:29.756, Speaker A: And so a very clear example of a public good would be like developer tooling, something that may not have a clear commercial reason to exist. So no one's going to pay a ton of money for that system to exist or that tooling, but if it did exist, it would really help a lot of developers do their best work and help everyone develop. So when we're giving a grant, we're not just giving a grant to that specific recipient, but we're giving a grant that is going to have an exponential effect. And so the way that we actually do that is by giving out milestone based grants. So if we're going to give a $100 grant, we're going to give like, 20 or $25 upfront, and then the rest over the series of completion of milestones, so that we make sure that things are mutually beneficial for the ecosystem and for the foundation. So if you're interested in grants, just please find me afterwards. I'll be right off by the stage over there.
02:13:29.756 - 02:14:05.972, Speaker A: Happy to chat with any of you about our grant program, but I think we're all ready to go. So it is my pleasure to introduce Kevin Bowers, the chief science officer at jump trading, to talk about fire dancer. Really excited for it. Please join me in welcoming Kevin. Okay, so I'll just wait here for a second while the slides get ready. This is going to be a continuation of the presentation we did yesterday. This is a deeper dive into fire dancer.
02:14:05.972 - 02:14:50.218, Speaker A: And like we said yesterday, the overall structure of this will be to give a little bit more about the structure of it. Then we'll do a demo of the current progress thus far on the fire dancer implementation. Then Philip will come up and talk a little bit about progress we've made on improving block packing within the ecosystem. And then Kavi will come up, talk a little with bit about some of the efforts to hardware accelerate critical portions of the path in the Solana validator, and also do a live demo there, too. So this is a slide I presented yesterday. Fire dancer is a new independent Solana validator. The goals are to document, standardize the existing validator, to improve the ecosystem robustness, improve system performance, and it's under development.
02:14:50.218 - 02:15:15.078, Speaker A: At the link above, you can look at any of the code I'm about to show show today you. So there won't be any, you know, tricks up my sleeves. And like I also presented yesterday, we're treating this from our perspective. It looks a lot like a production trading system, where packets come in from the outside world. In a production trading system, they go to a strategy that thinks about them. They go through an order execution. Off to the side, there are some infrastructure that captures incoming packets.
02:15:15.078 - 02:16:08.180, Speaker A: There is some replay infrastructure, so you can do various kinds of, of offline analysis and testing and debugging. And if you look at a slot of validator, you see the exact same structure, too, where we have the packet ingress on the front end, we have the runtime, which thinks about the packets that have come in, and then we have a consensus algorithm to essentially make those things real, exact same patterns that we see overall in the kind of conventional trading world. And our approach for doing this is to approach it layer by layer at a high level, where there's three, three components to this development. The first component is optimize the packet ingress and replace that. The second component is to do an optimized implementation of the runtime. And then the last component we expect to get to is start working on the consensus algorithms, and along the way, we'll pick up the captures and the replay, kind of for free. And this led us to the concept of a Franken dancer.
02:16:08.180 - 02:16:59.960, Speaker A: We didn't want to go do this project as one giant big bang project, where we come out and say, hey, we're going to go away, put our heads down for two years. You won't hear from us, show up with a new validator that may or may not be compatible with the existing validator. So we know Solana is still under active development, changing things. We want to keep in sync with that, while not introducing too much friction between the organizations. So we are planning on just modularizing the existing validator and then replacing components as we go, which introduces the concept of a Franken dancer, where we are stitching in to the existing validator the components, as we develop them, to speed them up. Now, kind of a side effect of this is that we expect the early developments, even though they may be very high performance, they'll be interfacing with an ecosystem that is not very high performance yet. And so it will be like putting a sports car in the middle of a traffic jam.
02:16:59.960 - 02:18:17.848, Speaker A: But we do expect that the importance and the robustness will continue, continue to grow as we get more and more components replaced and improved. So if we look at the architecture of Frank, and this is a slide I showed yesterday briefly, what we see is at the top, we have a lot of the stuff where the pointy end of the stick is for interfacing with actual packets on the network, and a bread and butter for jump trading is interfacing with large numbers of nics with large amounts of redundancy, so that if any particular component fails, we just keep on trading actively. And all of these are very high bandwidth, like described yesterday. A modern exchange looks like a distributed denial service attack that never ends with constant line rate bursting of multiple cross connects at the same time, and you have to keep up with it to stay alive. So after we deal with that interface, we intend to go zero copy directly into a massive bank of signature verify tiles. If you look at computationally, at least in the packet ingress end, essentially what you have is something that just needs to verify things as fast as possible. And in terms of verifying things, because of the nature of the bursty traffic and whatnot, and the overheads to set up technologies like GPU's and so forth, you can't really do this with things that are optimized for average throughput.
02:18:17.848 - 02:19:00.523, Speaker A: This is one of the things I talked about yesterday. When you look at, at a lot of the technologies that are being developed right now, they are really trading away burst throughput capability, the ability to be prompt when data comes in, start processing it now for doing things where form really giant batches and go process them, and then you'll have really high throughput. Well, you know, as these things are coming in, they're coming in asynchronously. The ability to form bursts is there. The overheads for launching a burst are there. So everything I'll be showing will be in a non burst context, and you'd be very, very proud. So as data comes in, can immediately be farmed out to this giant battery of signature verification tiles that go forward and start doing the necessary cryptography stuff to get through it.
02:19:00.523 - 02:19:44.534, Speaker A: Now, after that comes about, there is a dedupe tile which then deduplicates all the transactions to limit load that is going to be hitting the runtime system afterwards. There. There is also, and I'll show this in the slide here in a second, a pre dedup stage in the signature verify. And this doesn't really exist for anything in the existing Solana ecosystem, at least since they've moved on to the Qwik protocol. But if somebody was to, when you're using the high availability and you're running active, active, you do expect to see a lot of redundant traffic just by nature of that. And we can filter that out just up front by saying, gee, I just saw this packet from its sister Nick, and so I'll throw that away. And so there's a pre dedup stage that also goes on in there.
02:19:44.534 - 02:20:25.288, Speaker A: After you get through the dedupe stage, you get to the pack stage, and the pack stage does the actual block packing. And then we have a process boundary between what we're writing and the Solana validator, and that is where the blocks will be then fed into the existing runtime. If you actually were to look at the existing repo and to put some names on things that you might see, the things that are boldened, there are things that are currently checked into the repo right now that you can look at. We have the APIs for moving the stuff to the nics. Zero copy. We have the actual frank application tiles for running signature verification. We have the tango API for getting it to dedupe.
02:20:25.288 - 02:20:58.022, Speaker A: We have the dedupe tiles running. We have the tango and whatnot. We are still putting together the disco libs and the rust shims. And so keep in mind, everything I'm showing is very much a work in progress. And this is essentially, we've started this work, you know, full bore, I would say, about two months ago. So kind of put frames around where we're at and how fast we're progressing. So if you delve into the details of these individual tiles, what you see on the left is a picture of a de dupe tile.
02:20:58.022 - 02:21:29.324, Speaker A: With the dedupe tile, it's being fed by a bunch of data structures provided by the tango API. It is essentially just round robin polling amongst various metadata sources to say, hey, where's the next packets might be for the nics that I'm particularly interested in. Then it goes through a. Actually, for the dedupe tile, this would be after the signature verify. Then it goes through, essentially, the spam de Dup. If somebody is sending a lot of transactions in different sessions, then you can, you can't really dedupe that upfront, you have to do it afterwards. So that essentially goes through a quick thing.
02:21:29.324 - 02:22:24.854, Speaker A: Have I seen that recently? Get rid of it and then publish it downstream for that. On the right, we see the Sig verify tile structure itself, similar overall structure for how the communications work. Pull various ones, do a parse of the packet we want to parse because this is parallel, we expect most packets to pass parsing and parsing is pretty lightweight, so we might as well paralyze it and pick up the throughput, do the really quick, high availability de dupe on top of it. Then start doing the SHA 512 as part of the SHA 512, generate a hash that we can use for various kinds of really fast de duplications later without having to do full packet comparisons. Go into the actual looked at curved cryptography tiles using the exact same signature verification algorithms. Our current implementation is very, very fast, but we've also been very slavishly imitating the internals of the OpenSSL library for this. So as it stands right now, on like a 3.3
02:22:24.854 - 02:23:17.654, Speaker A: GHz ice lake, we can do the SHA 512 and optic curve cryptography in about 38 microseconds to get through all that. We think there's still some slop in the implementation. Improvements in particular, we have a lot of people who are very, very familiar, familiar with optic curve cryptography with, and have looked at the algorithms and said there's a lot of room for algorithmic improvement. And so our next phase, to try to push down the SiG verify costs there are to essentially break away from imitating OpenSSL and get very, very optimized on the more algorithmic side of the algorithm, and then hopefully maybe get another 1020 percent in performance off that. For software. If you look at the overall costs, the dominant cost are those software and the ED two 5519 algorithm. I'd say about ten x is in the ED two.
02:23:17.654 - 02:23:59.714, Speaker A: Let me think how to praise this. I would say you have about low to mid tens of thousands TPS per ED two 5519 per core that you can run. It's actually a pretty tight implementation. Tight enough that you can use hyper threading with it to get some additional boost. The hyper threading doesn't necessarily give you a two to one speed up, because it is tight enough on the scheduling that it, you'll see some speed up, but not a straight linear scaling. You see about the rest of cost. Essentially, ShA and every other piece of infrastructure has performances that's in the millions or tens of millions or more in terms of transaction handling perception.
02:23:59.714 - 02:25:02.204, Speaker A: Now, also looking at this, we've made some suggestions on the Solana discord and whatnot, and potential directions. There's a lot of areas here where we look at these algorithms, where we can see how they could be more optimized if you really wanted to improve performance. But right now, we are keeping this exactly compatible with what the existing validators do for that. But like, when you look at these algorithms, I think the most common thing that you see is they never met a sequential dependency they didn't like. And processors aren't good at sequential dependencies. And so, any way you can restructure these algorithms to exploit more parallelism, be they core parallelism or ILP within the instructions, or as we'll see with some of the stuff, Kava will present parallelism with hardware, then you can get really dramatic speed ups and reductions of areas. But you know, right now, if you look at these things, you see these pipelines that are, you know, hundreds and hundreds of stages deep, and they look superficially hardware friendly, in that they use operations like xors and rotates and so forth, but they are actually really quite hostile because of all those sequential dependencies.
02:25:02.204 - 02:26:10.904, Speaker A: And there's other ways to be structured them for performance. And if you go through the same kind of breakdown of how these things map onto the existing things, you're looking at the repo. The repo structured has a number of libraries, and you can see that you have the tango library for the communications, which provide some of the objects. And in the ballet library we have a lot of implementations of just things that are standards, and we have to, you know, conform to the standards. So I'm now going to transition to a software demo here, I hope. Can you go to the laptop? I have a sneaking suspicion that this went to sleep and it lost my connections to GCP. Yep, this is a burner laptop too, so there will be a lot of fun stuff.
02:26:10.904 - 02:26:58.810, Speaker A: Okay, so this here is fire dancer. This has been checked out on a really vanilla GCP cloud instance in Madrid. We picked it in Madrid for hopefully low latency in this presentation and reliable Internet connect connectivity. This is their n two standard 80 instance, and we're running a stock red hat 8.5 image. And if you want to get the rough specs of this, this is a dual socket 20 physical core cascade lake processor, so not terribly recent. It's running at 2.8
02:26:58.810 - 02:27:31.796, Speaker A: GHz. Hyper threading is enabled on it. It has two, two pneumonodes on it, and saw 80 logical cores total going into it. There's nothing terribly special about this. We are not picking things for, you know, you have to use the latest and greatest intel. We're not using a whole lot of cryptographic, we're not using any actually right now, any of the cryptographic extensions on intel. Pretty much anything that supports AVX two and that includes any intel processors since mid 2013 or AMD architecture since about mid 2015 should be fine.
02:27:31.796 - 02:27:58.424, Speaker A: And we aren't kicking the tires very hard. The Linux distribution. So if you want to run Ubuntu or Susie or Debian or whatever you have out there, we really don't care. Our usual practice for stuff internally is we get a box, we isolate the cpu cores, we tell the os to go away, we do it all ourselves. So we aren't actually all that sensitive to operating system things. Again. We are also not trying to do a whole lot of things that require people to jump through hoops to use this.
02:27:58.424 - 02:28:51.362, Speaker A: There are a couple things to get the box configured for optimal performance that requires superuser privileges, but we have deliberately targeted to do nothing more onerous than the existing validator does. And we've tried to be less restrictive than the existing validator so that it's very easy to install this in various environments and test it out. If you go to a box like this, what you find is that there is not much on it and there is no compiler, there is no git or anything. So if you just install the basic development environment that is fine. For this demo I will be using GCC, the stock GCC on the system. Ill be using the build tools like make no fancy make there or anything, and well, git kind of installed. We do have people like I mentioned auditing, informally verifying parts of the code, and then with that they have also been testing out other compilers like Clang and Whatnot will eventually support all of that.
02:28:51.362 - 02:29:51.646, Speaker A: Just in the current repos, that stuff's not checked in yet. The stock basic development package that gets provided by Red Hat here misses two important libraries from our perspective, and like I mentioned in the presentation yesterday, modern computing is all about locality, and these systems desperately try to hide locality details from you, and we tend to view that as a mistake. And so there are two libraries to install here that are just standard that allow us to essentially query the topology of the system so we can figure out how to optimize locality on that system. The other changes that we have here are we like to lock a lot of pages in memory to get good determinism on performance, eliminate the needs of swap and so forth. And we also like to configure the scheduler for high performance critical threads to run a high priority. So we do change the limits of what a user can do in that. So there are instructions for doing that here, and then that's pretty much it for the box setup that we've done.
02:29:51.646 - 02:30:26.634, Speaker A: This box here has already been primed that way so you don't have to watch me playing around with limits.com or whatnot. After we do that, we just clone the repo and you can clone it today and then we're going to build. You can just type make, but we strongly suggest using make j because this is a proper parallel incremental build out of out of tree and all the other nice kind of properties. There's a number of machine targets that it will do. If you just type this vanilla, the default machine target will go to this red hat eight x 8664 and then it will, you know, do a bunch of stuff. We'll see it go through that after it's built.
02:30:26.634 - 02:31:03.716, Speaker A: This creates a bunch of utilities for reliably administering. And this is something that's surprisingly tricky to do, but it's back to the operating systems desperately trying to hide locality. There are utilities that will allow you to create memory and will actually give you that memory. It won't basically think you're bluffing and then try to defer as long as possible. It will bind it in various places. So you can just very reliably say I want some memory here and I want it to be backed by this kind of page for TLB efficiency. And so like the configuration I've done on this box is this exact set of commands over here, and it will reserve some pages.
02:31:03.716 - 02:31:36.504, Speaker A: I've already done this. And then we also created a sandbox because we are doing this with inter process models. There's a lot of stuff to set up all the shared memory objects and so forth. And so we could also do all the asynchronous starting and restocking of applications. And so over here is a command to set up a little, little sandbox on that. So just to go, not used to typing standing up. So if we just go through here, real brief again, just reiterating, this is a, you know, normal intel hyper threading enabled.
02:31:36.504 - 02:32:24.810, Speaker A: You can see here it was processor 79, the last processor kind of mentioned 2.8 GHz. We haven't done anything to configure it. We haven't played, played around with me, msrs or anything else like that to make this kind of run fast. Then if I take a look at the overall thing for the system, if I run git status, it says, hey, I'm clean, I'm not doing anything here except for I did make a modification here, and if I do take a look at this modification here, this modification is really simple, is I told it to use a synthetic load generator for, for this purposes of this demo. The synthetic load generator is checked in, but for running this, this is not hooked up to a network yet. So we're just going to generate synthetic load that has the statistical properties very much like a Solana validator.
02:32:24.810 - 02:33:15.530, Speaker A: We're going to generate it as though we are in a two to one high availability configuration with the kind of usual packet sizes and so on and so forth to actually feed into the system. So other than that, I haven't changed anything. For some convenience during this demo, there are some symlinks that I have created here, because I haven't built the code yet. These symlinks aren't going anywhere yet. But I just put these here to save me some typing during the demo. So if we take a look over here and I just type make help, it's going to tell you what it thinks it should be doing for this particular system is, and so what machines it's doing and so on and so forth. And with all this, we see our CPP flags and you can get some kind of idea of the discipline that we have internally.
02:33:15.530 - 02:34:08.694, Speaker A: I'm particularly proud of these set of warnings here and whatnot. We refer to it as the brutality, but you know, I really despise the fact that narrowing conversions are allowed in c. Silently we turn all of that. I really despise that there's silent float to build promotions, that there's a lot of problems in the type system around, type smaller than Int's and so on and so forth. So we are really running a really, really bespoke dialect of C, which runs well for cross compiling between the Solana blockchain, running on X 86, running on various kinds of ASIC targets, and or making things easy for FPGA engineers to synthesized for their hardware. Beyond that, we aren't doing a whole lot to the particular compiler build. So if I go through the make process over here, and I'm not going to do make parallel first, I'll just do a serial make.
02:34:08.694 - 02:34:37.990, Speaker A: I forgot to type. The less here we go. So if I do the make here, what we see at the beginning of the thing is the usual stuff where it's generating dependencies for the C source and as it goes through making all the dependencies so we can do the proper incremental parallel, the makes and whatnot. The makefile does log what it's think it's doing. If you're trying to debug some kind of make issues, it's a very useful thing. We see it starting to compile the various sources together. It does support mixed language development paradigms, but most of the low level stuff is written in c.
02:34:37.990 - 02:35:25.270, Speaker A: And then it gets combined into a library to do the various sub things like the tango library, the ballet library and so forth. And then it creates a ton of unit tests. And if I go through and I just do a build from scratch, what we're going to find here at the build from scratch is that, you know, we'll just take, we are real sticklers for quick build. We've just built everything from scratch, so we're done with that. If I then go look in the unit test directory, there are tons and tons and tons of unit tests. Everything is extensively tested at levels most people aren't used to seeing. So pretty much we use a very interactive development philosophy where you write a line of code, then you update the unit test.
02:35:25.270 - 02:36:11.684, Speaker A: We keep iterating back and forth. It's one of the reasons why we're really sticklers on really fast build times. We almost use these things big scale systems interactively. If you go over to the bin directory, there's a bunch of things in the bin directory that have appeared and these are the actual applications that we're going to start kind of playing around with. So if I go over to the shared memory configuration stuff, almost everything here has a little help description which will give you the kinds of commands and powers. I won't demonstrate every single one of these, but if I go over to the query over here, what we have is we have, like I mentioned, already primed the box by allocating some gigantic pages on each Numa node and how they're being used. And we've already set up a sandbox for all these shared memory objects that are permissioned for me.
02:36:11.684 - 02:36:38.128, Speaker A: And we'll use these to create various kinds of stuff. So with this we can start initializing the frank instance. So we're going to type frank init. It's going to tell me, hey, give me a name to the validator I want to run. Tell me where you want to optimize all these objects for, for locality. I'm going to say put it near core zero, which, which is also numa node zero. On this box and is for now I'll just say, let's do one verifier on it and I'm going to tell it.
02:36:38.128 - 02:37:25.400, Speaker A: I put all the build stuff over here in this directory and this is going to go off. And it shattered to itself. And in this chattering, what we see going on is that it has made a bunch of objects in shared memory. You see it talking about the objects that it's doing, and it's recording how it laid out the shared memory and formatted it. And so if we go down to the bottom here, this is the actual, actual layout, we can then look around and we can see that in the sandbox that we have, that, you know, suddenly this thing that's gigantic pages has appeared called Frank Workspace. And the Frank workspace is where all of our toys for running the Franken dancer have gone. So I can go maybe a little bit further on that.
02:37:25.400 - 02:37:54.166, Speaker A: There's a bunch of tools to further inspect a lot of this, so you can expect real time the state. But let's take a look at what's in the Frank workspace. And in the Frank workspace it's basically saying, hey, there's some stuff that's been allocated. A lot of this looks like file system tools. And that's not entirely unexpected, because what people don't realize is modern x 86 is not an x 86 processor anymore. It's a network of x 86 processor emulators. And memory isn't memory anymore.
02:37:54.166 - 02:38:38.714, Speaker A: Memory is actually a distributed file system that emulates Dram. And so most people aren't used to thinking that when they touch some memory, it's actually equivalent under the hood to essentially resolving a symlink to an inode, and then opening a file and then reading the entire file, then modifying the entire file, then closing it. And so when we look at all this kind of stuff and we look at programming methodology and whatnot, and we see how inefficiently they're being done. People are doing essentially file system operations, but acting like it's a zero weight state system from 1982. So over here we see that we've already partitioned up this thing for all the various objects that are sitting around. And you know, with all of this, let me go transition windows. And since we closed the window, I'll have to open up a new one.
02:38:38.714 - 02:39:24.492, Speaker A: This will probably be hard to read, but we'll see why here in a second, because this will be extra tiny fonts. Okay, so one of the things we have here is the ability to monitor it real time from all these processes. So over here is the Frank monitor. So the Frank monitor, I just ran it. It told me, hey, tell me the frank instance that you want me to look at. And when I run this, this has gone and inspected that shared memory region and has pretty printed what's going on inside the Franken dancer. We haven't actually started it yet.
02:39:24.492 - 02:39:49.378, Speaker A: So this is just here to kind of show the monitoring stuff in a very limited sense. And so what we see over here is there's monitoring of the individual tiles in the Franken dancer instance. So here we have the main application, we have the packing, we have the deduplication, we have the verify tile. Since they've been created, nothing's happened. You can gauge how fast I'm talking based on the time here. They're not heart beating. They're in a state waiting to be booted.
02:39:49.378 - 02:40:26.814, Speaker A: All of the counters are green. You see that there's a lot of counters for. Are these things being back pressured by other parts of the system? Are the sequence numbers increasing? How much transactions are hitting them? How many are unique? How many are failing signature verification? Is anybody overrunning me due to misconfiguration? So on and so forth. So we have complete visibility of essentially every single link in a lot of detail. And this is actually only a subset of all the things we have, but we can just do all this kind of noninvasively. So with this, I'm going to run the monitor. I'm just going to tell it to run over here, basically printing out a snapshot every second.
02:40:26.814 - 02:41:27.226, Speaker A: And so this is now going forward and it's just basically printing out snapshots. Yeah. So now with this, let's say we want to run the Franken dancer, and we're only going to run run validator right now, but that will just kind of show some of these features over here. And so when I say, like Ben Frank, run like we saw before, it's going to ask me, hey, what's the name of the instance I'm supposed to run so I can figure out where all these objects are and connect to them? And then I'm going to say, I'm going to use CPU's one through three on here. I'm going to essentially set aside cpu zero for operating system stuff, and later cpu 20, because that's the other NumA node on this box for operating. And I'm going to say just right, run on one through three. So now I'm running on one through three, and then all of a sudden the monitor over here has gone live and it sees that there's transactions hitting it going by.
02:41:27.226 - 02:42:00.660, Speaker A: And as we look, what we are basically seeing is, you know, all the tiles are in a running state. None of them are being back pressured by anything. The back pressure counts aren't ticking. We do see some failures on the signature verification, but that's deliberate in this test because we deliberately configured it to hit have about 1% of the transactions hitting it be wrong just to, you know, stress out those parts of the system. We see those transaction sequence numbers are increasing. We see that, you know, the deduplication is seeing traffic. We see the verifier seeing traffic right now with the single verifier, we're getting about 30,000 transactions per second through one core.
02:42:00.660 - 02:42:36.764, Speaker A: It is being deduped into about 15, 16,000 ish, 15,000 ish unique transactions per core. We are seeing, you know, the percentage of the traffic that's being de duped is consistent with the two to one high availability. 50% of the traffic is just immediately identifiable as replicated traffic from a redundant NIC. And then we also see over here that about 1% of the traffic is failing signature verification by design. And everything's working right. No one's being flagged as a potentially thing slowing down the system. No one is being flagged as being, being overrun due to a misconfiguration or bug or anything within the system.
02:42:36.764 - 02:43:20.564, Speaker A: Now it's important to note here, like when I'm doing this, there has been no mallocs, there's no freeze, there's no atomics, there's no hardware fences or anything in the critical path. We're using one TLB entry for everything and doing all this. And all these tiles can be put in different processes for lots of isolation and so forth. We don't really care about any of that. So if I go over here, I'm going to stop this process over here and just Ctrl C it, as you see with the control C, it's intersected the control C and it's cleaned itself up and it's nice and packaged up that shared memory region to be, you know, safe for doing a lot of kind of stuff. We also see that it told us that it has a log file that it made here. And so if I try to bring up that log file, let's see, seven, one.
02:43:20.564 - 02:43:59.990, Speaker A: And this one won't show up very good because it so long. When you look at the, this is, this is meant to be the permanent log, the log that you see at the command line. Is essentially meant for developers sitting there who has full context of everything that's going. But you know, typically when we're in a trading context, there's two contexts for logging. There's the context of a developer who's sitting there who already knows who they are, they know what host they're running are, they have full information. Then there's a second context, which is a regulator asks you seven years ago, what were your systems doing on this and why none of that context will exist in a log. So there's also a permanent log that gets created that has a lot more detail as to what was going on in the system.
02:43:59.990 - 02:44:52.974, Speaker A: So if we look at this particular log over here, we see that there's lower priority log messages that have been committed to this log. There's more details on the pids and tids, there's more details on the user, where the user was, what cores they were running on, the logical names of the applications, what source code and file lines they were at, and then a lot of other kinds of context that goes into that. So after we get through the kind of logging, let's go back. This monitor, if we note over here, when I controlled seed it, it basically said, hey, things were running, but they kind of stopped running. But everything seems to be ready to be rebooted. So let's reboot it, and just for fun, let's reboot it on a different set of cores. So I've rebooted it now and everything came back this rebooted from where it left off.
02:44:52.974 - 02:45:11.940, Speaker A: This isn't starting renew. This is exactly picking up where it was at. And if we actually note here, the dedupe pack link isn't ticking. Why isn't it ticking? Because in rebooted, those synthetic transactions are actually replaying from the old. So now the dedupe tile is looking at it saying, I've seen these fairly recently. These look like they're duplicate traffic. I'm going to suppress those.
02:45:11.940 - 02:45:52.806, Speaker A: So that's pretty cool. If we go over, let's do a couple other things to kind of show the limitations of this and maybe some of the powers of this with this. Let me take a look at what the process id here was. 617, six, seven, whatever. Okay, now I've killed it from a remote process. And everything was fine. It shut down normally.
02:45:52.806 - 02:46:18.524, Speaker A: It intercepted, it cleaned itself up. I can then kind of come back, I can restart it again, move it around, do other things like that, put it on seven through ten, whatever. So it came back up. We're kind of going fine. I have a different process id six. Where did that window go? Ah, here it is. So let's do a kill minus nine.
02:46:18.524 - 02:46:45.036, Speaker A: And what was the rest of it? 63. So if I kill minus. Oh, maybe not. There we go. Okay, so if I killed it, now, kill minus nine in Unix doesn't let you intercept and clean stuff up. So now this just got totally blown away in the middle. The monitor over here is going by and it's ticking and it's like, hey, the tiles think they're running, but they're not.
02:46:45.036 - 02:47:22.054, Speaker A: Heart beating things look a little bit weird. If I try to bring the system back up on top of that, the system will then come up and say, wait, this validator is not safe for restart or anything else like that. This is in a weird way, I'm not touching with the ten foot pole. So with that, let's do something a little bit bigger. Let's. How does the command work? Okay, Numa node zero. Let's give it 72 verifiers.
02:47:22.054 - 02:48:01.744, Speaker A: That should work. Okay, so we built it up, and now let's bring up the monitor for this, and then we'll see why. We had such a small font before. So the monitor, it's not even going to fit on this kind of screen, but we have a ton of verifiers ready with all the, the kind of tiles kind of sitting around for all of this. And let's run it. And I need to remember the cores that I planned for this. So let's see, one, two, let's put a bunch of verifiers in here in the Numa node, and let's put a bunch here on the other numa node.
02:48:01.744 - 02:48:30.352, Speaker A: And let's put some here. And let's put some here. I think that might work. Okay, we see it coming up. It's actually a fairly slow bring up process for this. Not because it's actually slow, but because all of the synthetic transactions we're doing take a while to generate. And as these things are coming up, we see these things coming up, we see more and more transactions growing kind of going through the system, and we're also seeing nothing is, you know, straining under the load and so on and so forth.
02:48:30.352 - 02:49:06.172, Speaker A: And so now we're basically fully saturated on top of running the system. So if I just stop the monitor, this is, is still running in the background, and I scroll up on the monitor. What we'll see here is getting up to the de dupe tile. The verify tiles in aggregate are doing about 1.2 million transactions per second that are going into the dedupe tile, which is then deduping the, you know, this is doing the two to one reduction, and then the dedupe tile is muxing them all together into a consistent order. That's also, we're pretty sticklers on that for about 0.6 million transactions per second.
02:49:06.172 - 02:49:39.604, Speaker A: And that's going on. Then off to the pack tile to go do its kind of business. Nothing here in the system is acting up or feeling uncomfortable. Everybody's in kind of a run state. And then to also give you some idea of some of the high availability kinds of features over here, I can bring the monitor back up and this might take me a second, but if we. Let me find a, let me find somebody to kill. Let's kill off.
02:49:39.604 - 02:50:50.404, Speaker A: Let's emulate a failure here on verifier 69 over here. And so we're going to emulate a failure by just reaching into the shared memory page directly. It's not the right one. So here I'm just going to go tell that individual command and control, just directly reach into it and tell it to shut down. And so now it's acted like it's faulted. Let me bring up that monitor again. And we can actually overall see now in the monitor that down here, this link is now registering as being dead and it's not actually getting any kind of traffic.
02:50:50.404 - 02:51:32.394, Speaker A: So you can identify parts of your hardware system that have failed. But because we had the high availability and whatnot, nothing would have broken the stride in terms of lost packets. We can also see, because we're using hyper threading, the hyper threaded pair of this one has suddenly gotten faster. And so we can actually start diagnosing a lot of very interesting performance anomalies, kind of real time with all of that, but with that, that gives us at least some idea of the kind of capabilities. I'm happy to go into more details of the code, but you guys can all read that directly. And I think with that, that's a good place to wrap up this initial demo of where the software looks like for Frank. And now I want to switch back to the presentation for and have my colleague Philip join me here on the stage to talk a little bit about block packing.
02:51:32.394 - 02:52:13.620, Speaker A: The clicker's on there. Thanks, Kevin. Yes. So as Kevin mentioned, I'm going to talk about blackpacking, and I'm actually going to talk about one specific piece of blockpacking, which is transaction cost estimation. Got it? Yes. So what is blockpacking in the first place? And why is transaction cost estimation an important piece. So, as I'm sure most of you guys know, when these transactions come in, they have to land on chain.
02:52:13.620 - 02:52:55.598, Speaker A: And space in an on chain block is a scarce resource. So the validator needs to pick which transactions actually make it on chain. And the validator's goal is to pack as many as it can so that it can maximize its fees. But there's a constraint, which is that the block has to be replayable in time by the majority of the network. Otherwise it'll lead to a fork, and then the validator won't get any rewards from that block. So there's an asymmetric risk here, where if you under pack by a little bit, you get most of the rewards, and if you overpack by too much, you risk getting no rewards at all. So then this pushes the validators to a conservative packing, and it means that we're in the regime where the more information we have about the transactions, the better we can estimate their compute units costs.
02:52:55.598 - 02:53:50.894, Speaker A: The tighter we can pack the blocks and the more transactions can ultimately land on on chain. And I guess it's also worth making a note about the compute budget program, which is relatively new, and more transactions are starting to use it. But until the vast majority of transactions use it, and even more, until the vast majority of transactions are actually estimating their compute accurately, this is still a very useful piece of the blockpacking process. So for all these results that I'm showing, I'll have plots like this where I took a data set of 1000 blocks just from some random day. There's like 2 million instructions. And I think these graphs maybe I downsampled a little bit, but there's a point on the x axis is where the algorithm predicted it, and on the y axis is the actual compute unit use. So what you're looking for is a sharp line like slope one.
02:53:50.894 - 02:54:29.634, Speaker A: Y equals x, and that means that we're estimating it accurately. So here, this is the current validator algorithm. You can see there's a general trend, y x, you know, the diagonal line, but there's definitely a ton of spread and a ton of places where we're both overestimating and a ton of places where we're underestimating. So if we zoom in here and the r squared, sorry, the r squared of this, if you fit a linear regression, is 0.65. So definitely, you know, we're not, we're getting somewhere, but there's still a lot of room for improvement here. So if we zoom into the region with all the points we can see, just a little bit more detailed. There is that clear trend line, but there's also a lot of spread.
02:54:29.634 - 02:55:14.688, Speaker A: Okay, so then how is firedancer going to do this? So, I've been working on this algorithm. The key data structure we'll use is a fixed size hash table, just like the current validator. But instead of storing each program id and like using other a normal hash table, we're just using buckets. Basically, we're treating this like a set of buckets. And we'll from each instruction we'll map into a bucket, and for each instruction that's mapped into that bucket, we're keeping track of the mean and the variance and the count for those transactions. And so then when each instruction comes through, we'll look at the buckets that it maps to add it up. And then after the banking stage, when we have executed the transaction, we know how many compute units it actually used.
02:55:14.688 - 02:55:54.684, Speaker A: We go back and update the buckets with the new information. So in our experiments, we found just using actually one hash function that takes the program id and the first byte of instruction data, which typically clues you into which instruction the program is actually executing works. And for comparison, we used 1000 buckets, which is the same as the current validator, although these buckets are actually a lot smaller. So if we did maybe a equal size memory comparison, we'd be, I don't know, 4000, 8000, 16,000 buckets, something like that. And so you can see these are our results immediately. This is, you know, the zoomed out view. It's a lot cleaner.
02:55:54.684 - 02:56:13.144, Speaker A: There's a lot, the diagonal line is a bit more pronounced, but again, zooming in. Oh, and sorry, our r squared here is so from 0.65 to 0.89, if you look at it the reverse, we're like making three times fewer mistakes. So big improvement. Again, zooming in. This makes it even more clearer.
02:56:13.144 - 02:56:39.990, Speaker A: So on the left you have the current validator, on the right you have ours. The diagonal line's a lot sharper. We're getting much better, much more accurate results. And even more than this, I would say our algorithm is simpler. And so I guess it also makes sense to kind of pull apart the two factors. One, we're using a better feature, as we'd call it, a better hash function. And two, we're changing the structure of the table.
02:56:39.990 - 02:57:42.026, Speaker A: So if you take our new hash table structure with just the programid, which is what the current validator uses, even still, we get better than the current validator, although the difference is not as big, big and plus one nice feature of this is that we're getting standard deviations of our estimates for free. So we're getting a measure of uncertainty on our estimation, which is great because there are some transactions that, you know, no matter how we do it, they're going to be hard to predict with high accuracy because they could be very data dependent on a bunch of, on chain accounts. And there's no way we can go and, like, look through all these. So, for example, these ones that are circled in the red, you know, you can see that sometimes we're underestimating or overestimating by maybe 25,000 cus. But the standard deviation that we're getting from our table is the size, about the size of the blue section. So you can see, when we make one of these estimates, we know that the standard deviation is wide. And most of the time when we're overestimating or underestimating, it's by one or two standard deviations.
02:57:42.026 - 02:58:17.818, Speaker A: So we have some way of quantifying that, which then feeds back into the block packing so that we can pack with less, you know, make less conservative packings and fine tune those packings to get more transactions on chain. Okay, with that brief note, I'm gonna turn it over to Kaveh, who's gonna talk about some really neat hardware work. Good. Am I good? Okay. Hi, I'm Kave. I do hardware R and D at jump. Thanks, Philip.
02:58:17.818 - 02:59:00.254, Speaker A: And that was pretty good. So, is this it? This is it, yes. So, in this quest for a second slash faster validator, and we've already talked, Kevin already talked about all these components. And so these are the components that we've looked at, studied, and we found suitable for maybe hardware acceleration. And that's what I'm going to talk about a little bit, both from a theoretical and also from practical perspective, that's what I'm going to do today. I'm going to talk about what it takes to, and what it means to accelerate these components, at least some of them today. And also, I'm going to show a live demo on an Amazon f, one of what we've done so far.
02:59:00.254 - 02:59:57.704, Speaker A: Okay, this slide is showing up empty. I don't know why, but so I'm going to skip that. And so when you start considering hardware acceleration, the first thing you have to consider is, what form factor are you talking about? Right. Like, what form factor for your hardware are you going to consider? And the very first naive setup that you can think of is the old fashioned. You have a processor are in the middle, and you have your networking on one side connected through PCIe or some newer protocols are coming, but usually you have that connectivity to your basically outside world. And then on the other side you have your FPGA or hardware accelerator connected through, and again, some sort of bus PCIe in most cases, again, better protocols coming later. But this is what you, right now at least you see in Amazon AWS f one s.
02:59:57.704 - 03:00:59.806, Speaker A: And well, obviously you can see it has limitations because any data that gets in and out of the FPGA, your hardware accelerator has to go through your, has to go through PCIe has to go through to your processor. So you're allocating a processor for at least one core to talk to the FPGA. And then you're limited by the PCIe bandwidth, you're limited by your constraint, by the PCIe latency, all of that, right. When you're thinking about an application that is network centric, it's distributed, and your primary, primary thing that you do is communicate. Something like this makes more sense, right? Like if you have an FPGA that's directly connected to the network, and then, so the data gets to your FPGA a lot faster without like with fewer hops, and then there's a processor there for coordination. You know, you always need that process are there? So someone needs to take care of the management of the system. But, and then the communication between your processor and the FPGA is less of an important issue here, right? So you can rely on your direct connectivity here.
03:00:59.806 - 03:01:45.294, Speaker A: And this is something that you see in Microsoft Azure, at least that's what they use themselves for their search acceleration. Okay, so we have this, these different form factors that we can play with. Of course, that's dependent on the data center and all that. But we just wanted to have that out there that the form factor and the configuration of the system makes a big difference in terms of how you can utilize your hardware accelerator. Now, looking at all of those components that we had that Kevin also talked about, the very first thing that comes that pops out is Sig verify. Like when you're receiving blocks transactions, you have to verify them. And inside the SIg verify, the first thing that pops out is ShA 512.
03:01:45.294 - 03:02:08.732, Speaker A: So that's what we decided to do. We decided to just take SHa 512, implement it on Amazon, and show how fast you can run it. So what I'm showing here is a typical danger. It actually says danger. Oh, there we go. We have a host on one side that communicates with our FPGA. This is our entire FPGA sitting inside the Amazon f one through PCie.
03:02:08.732 - 03:02:40.168, Speaker A: And so we have synthetic transactions coming through. We store them in DDR for buffering purposes. They come back to the card. We have to do some padding, because Sha. And inside our SHA unit, we have the block scheduler, and we have the actual rounds that actually do the computations. Kudos to Javier on this. And then we have the reorder engine that makes sure that instruction transactions actually come out the same way that they came in, because the scheduler does reorder them a little bit.
03:02:40.168 - 03:03:11.978, Speaker A: And then after that, we have this DMA engine that sends them back to the host. Now, we don't have to send them back to the host if we end up consuming the SHa within the chip. But for this demo, we decided to send them back to the host to validate, to see did we do the shot. Right, right. So that was important. But the numbers that I'm showing here is theoretical limits that you can get under some constraints. For example, here, PCIe, the PCIe that Amazon advertises is four times that speed.
03:03:11.978 - 03:03:38.754, Speaker A: But can you get that? Good luck, because the processor is just so damn slow. Right. That's just the way the reality of life, right. Turns out around this, considering this is considering four blocks per transaction. So it depends on what kind of synthetic data you're generating. You're going to get about 4 million transactions per second through this PCIe link. And then this FPGA, as we talked about it, doesn't have any network connectivity.
03:03:38.754 - 03:04:19.084, Speaker A: But for the purposes of this demo, we decided to have an Ethernet emulator inside the chip that is capable of generating 24 million transactions per second, again, assuming four blocks per transaction. And so just for, again, for the demo purposes, right, so we're buffering all of those, pushing them into the sha. And then what I'm showing here is theoretically, our SHA can sustain 250 million hashes per second. Can we feed it that fast? No, we can only feed it at 60 something million hashes per second. 2nd because of all the limitations, like our ingress is more limited than our sha itself. Right. So we're good.
03:04:19.084 - 03:04:40.214, Speaker A: Like we're not the limit. So we're good on that. And then there's another limit, which is our DMA. That is, again, we're bound by the PCIE and everything in there. So we actually just very recently this number, we doubled it. So we managed to squeeze a little more data in there. So it's about 30 million transactions per second that we can send back to the host.
03:04:40.214 - 03:05:06.334, Speaker A: I will show a live demo of this at the end of this presentation, what does this look like? So what I'm showing here is an x ray of the actual fpga running on Amazon f one. And you see most of it is empty. Like, all of these spots are empty spots. So we're not using, the green is everything that Amazon provides and we cannot get rid of it. It's called the shell, and you, and you have to use it. It's always there. You cannot move it around.
03:05:06.334 - 03:05:39.226, Speaker A: It's always there. And fair enough, you always end up with something like that with your own design anyway. So it takes about 25% to 30% off your chip, no problem. The pink is really our design. So that's the entire design that does Ethernet generation, takes in the data, does the shot calculation, reordering, scheduling, everything, and also DMA pushing it back to the host. So it's looking at numbers. This is about 10% of the chip, and you can see that it's almost about one third of the lower one third.
03:05:39.226 - 03:06:10.306, Speaker A: So it kind of checks out. And. All right, so going one level back up, what do we see? We see sig verify. Sig verify includes the sha. Have we done this yet? No, we haven't implemented this yet. This is our design so far, and this is what we think we believe that we can hit. And what we have here is we're assuming transactions are going to come in and we're going to store them in some form of external memory, maybe SRAM based, DRAM, HPM, whatever it is, and then we have to extract a transaction.
03:06:10.306 - 03:06:54.460, Speaker A: We extract public key signature message, you know, all of that goes to the SHA. Some of them go to the SHA, gives us the h that goes into the ED 255 calculations, and then all the other data that goes into this magic box of SIG verify scheduler. So what that guy is supposed to do is at its disposal. It's going to have an array in the order of 20 of field multipliers. These are 256, 255 really bit multipliers that do modular multiplications. And the scheduler is going to, it's going to have a sophisticated interconnect between them. It's going to dispatch operations to them, get the results back, and it's going to parallelize across transactions, because that's the only way, that's the only parallelism you really have.
03:06:54.460 - 03:07:32.746, Speaker A: 8250 519 is inherently by design, is a sequential operation. You cannot paralyze it. You can run it faster, but you cannot paralyze it. So what you can do is you can paralyze across transactions, and that's exactly what we're going to do. And at the end, the schedule is going to give us a pass fail for per transaction. And this is our very, very conservative, by a factor of two, conservative number that we think we can hit, which is about half a million transactions per second. All right, the next thing that we considered was proof of history, replay verification.
03:07:32.746 - 03:08:13.134, Speaker A: So I'm sure most of you know more about proof of history than me. I'm new to this domain. But at the gist of it, what it is, is it's a very long, very long sequence of hashes that you have to verify. You basically start from the dashed h, and you say, okay, if I hash guy, hash guy, hash guy, do I end up here? But to speed it up, the chain actually defines checkpoints here that they call ticks. So each portion of it they call a tick, which is about 12,000 hashes. So you can run all of these in parallel. The hashing that's used here is Sha 256, so it's even smaller than what I showed before.
03:08:13.134 - 03:08:48.094, Speaker A: And we can hit again the same throughput of 250 million hashes per second. We have 12,500 shahs per tick, so we can hit 20,000 ticks per second. What does that mean in terms of transactions, where, on average, we have about 64 transactions mixed in here? There's a protocol. I'm sure, again, you know a lot more about this than me. There's about 64 transactions mixed in at the end of each tick. So that gives us about magically 1.2 million transactions per second, the same throughput that Kevin actually just live demonstrated.
03:08:48.094 - 03:09:37.130, Speaker A: All right, with that, I'm going to switch to the demo and show. If we could have the laptop brought up, that would be great. All right, I got the password right. All right, where am I? Yep. All right, so I'm just gonna. What I'm basically doing, I'm setting up the AWS tools. This is an actual AWs, vanilla Aws f.
03:09:37.130 - 03:10:21.020, Speaker A: One machine that I logged into. It has one FPGA in it, and it's, like, basically the cheapest, smallest FPGA capable device that you can get from EC two. And I'm just setting up the tools and just standard AWS stuff. And what I'm going to do is, okay, my script just moved me into the right directory automatically. So all I'm going to do is I'm going to run this process. So what I'm showing here is two components. I'm showing, by the way, all of these data is getting captured live from the device.
03:10:21.020 - 03:11:02.728, Speaker A: And so on the right, what I'm showing is the host has a software sender that is generating synthetic data, synthetic transactions, sending them through PCIe to the FPGA. And what it's receiving is basically telling us, yeah, I'm receiving about 24, 25 gigabits per second per second worth of data coming in. This includes the overhead of the header and everything for the transactions, right. And it's basically telling me, I'm looking at the header and I'm seeing about 5 million transactions in there. That's, that's translating into about 5 million. And then it's doing the extraction. After the extraction, you see there's a drop in the data rate because the header was just extracted, throwing the garbage.
03:11:02.728 - 03:11:21.148, Speaker A: Right. So did they. Data was just cut down, but these transactions are still the same. And inside we also have the 100 gigabits per second Ethernet emulator. Well, it turns out we overshot it a little and we're generating 128 gigabits per second. But not a problem. Airshot can handle that.
03:11:21.148 - 03:11:44.648, Speaker A: So that's actually coming out to about 80 million transactions per second. If you do the math, you're like, wait a minute, this is about five times, six times bigger than that. But this is way larger than that. Why is that? Well, it's because this is generating one block per transaction. This guy is generating random number of blocks per transaction. So the generator that we have on the hardware is simpler. It's a constant number of blocks per transaction.
03:11:44.648 - 03:12:25.562, Speaker A: That's why you see this disparity between, like if you divide this by that, you don't get the same number. Basically, our Ethernet generator is generating one block per transaction back to back, no delays, just jamming the data in there. Right again, the hardware is the day the header is getting extracted. As you can see, the relative overhead was a lot, right, compared to this guy, because there was one block and a header. So you remove the header, you cut down the data a lot, but you don't lose the transaction at all. They get muxed in and you get your 88 million transactions per second going into the SHA, and all of them come out surprise. Right.
03:12:25.562 - 03:12:55.506, Speaker A: As expected, they go into our DMA engine. Of course, there's going to be a disparity here between the number of hashes going to the PCIe, because the DMA engine basically says, hey, you're giving me this. So much data, I cannot push this through PCIe. There's back pressure here. So it's dropping transactions hashes, basically all of the hashes that make it through the DMA engine go to the host. What happens in the host is we receive them into two separate threads. One is the Ethernet checker, one is a software checker.
03:12:55.506 - 03:13:30.046, Speaker A: There's a metadata in the hash that comes back to the host that says, was this generated by the Ethernet guy or was this generated by the software guy? They know how to check them. They are checking them and this is the rate that they are receiving them. Why is it so slow? Because these are software checkers. They cannot keep up with this 31 million hash percentage. Basically what's happening here is it's down sampling of the checking. The data is generated at the 88 million transactions per seconds are going into the SHA engine coming out. It gets downsampled because the PCI cannot handle it.
03:13:30.046 - 03:13:51.150, Speaker A: It gets down sampled again because the software cannot handle it. And that's at the rate that they are checking the transactions. Now you would ask why is the Ethernet checker faster than the software? Software? Check it like it's a lot more transactions. Because that's just the disparity between the transactions we're pushing in. Right. The drop is happening here randomly. Like whoever gets there first goes through.
03:13:51.150 - 03:14:24.542, Speaker A: So with that disparity, you're getting more Ethernet transactions into the software and the rest is being just dropped. Right. So that's the demo that I wanted to show. And with that, I will invite Kevin and Philip back on the stage and we go from there. Where did the cool demo go? I think my mic. Oh, ok. My mic is on.
03:14:24.542 - 03:14:42.974, Speaker A: Ok. So we don't really have a whole lot more to say, at least formally speaking. But I know we're a little bit over on tip time here, but we're happy to take maybe one or two kind of quick questions, if there's any questions from the audience. Okay. No one's standing up. That's either really. Oh, we have one.
03:14:42.974 - 03:15:13.674, Speaker A: Yes. Okay. I wonder if fired will provide RPC interface like one in a client. Sorry, I'm not getting the audio. I wonder if firedrange client will provide RPC interface like. Or it will be only about validation and RPC providers cannot. Here fire is going to come down because I'm getting a feedback loop.
03:15:13.674 - 03:15:52.864, Speaker A: There's so much echo. Yeah. Okay. So the question is, will fired answer provide an RPC interface like the Solana client? We intend it to be a drop in replacement at completion for Solana client. So the answer to that would be yes. A lot of the hardware that you guys are running on, actually because of the thing, I'll just. Slower speeds.
03:15:52.864 - 03:16:41.152, Speaker A: Yeah, we'll run on the fastest hardware you're willing to throw at us. But one of the concerns that was brought up to us in the past was were we going to show up with the most bespoke hardware configurations that only the four richest kings of Europe could afford? And no, we are targeting essentially low end. And like a lot of this stuff, you could go very well. It's in terms of support. Now, in terms of optimizing, we're targeting the higher end kind of things for optimization. So, like in this demo here in the software side, you see that, you know, we got 1.2 kind of million transactions, and we were using, you know, 40 odd physical cores, 80 odd logical cores on a fairly old box, low clock speed, no turbo, things like that enabled.
03:16:41.152 - 03:17:41.944, Speaker A: Now you come along and you say, no, no, no, I bought my ice lake, it's 40 dual socket. I've turned on the hyper threading, and you go out over there, you can make all those numbers twice as big. That's pretty easy. Now, I think the important message you want to get across, especially about the hardware demo, is with the kind of thing with much less footprint than buying a really beefy high end server and whatnot, single FPGA card, and then only using like a 10th of the area on that FPGA card, we can get ten x performance over that. And so, looking at how expensive calculations like sig verify are, that's why we're very strongly looking at the hardware acceleration. I think another thing here, which may not be completely obvious, because a lot of people aren't familiar with hardware and so forth, is that, you know, you can get high numbers off something like a GPU and whatnot, but they are really optimized at average throughput. So if you don't already have a really big batch coming in, you have to form that batch.
03:17:41.944 - 03:18:19.764, Speaker A: It eats a lot of system latency, and a lot of that takes away from the ability to use it. It makes the software hellishly complex and more bug prone and all that. So everything we're running here is in a non batched mode, including the FPGA, outside of like the kind of like actual packing of the pack, as Kabi was talking about. And so if you're looking at those things, it's a lot simpler, a lot lower overhead. So those are actual numbers that you can attain much easier than you can by trying to integrate technologies like a GPU that aren't just really well designed for high performance network processing. Yeah. Oh, yes, sorry.
03:18:19.764 - 03:18:54.208, Speaker A: Okay, you got a good question? Sure. Verify. Can you talk a little bit on that? The rust side and contracts. Yeah, sure. So the question is, here is, a lot of the contracts are in rust, and then the LLVM parts, all that's in component two runtime. Now, we've actually already implemented, Kave could speak to this in a FPGA, a thing, which can run the EPBF runtime as a thing in an FPGA. And so that's fine.
03:18:54.208 - 03:19:20.236, Speaker A: But in software, that's fine, too. Like, we're going to implement a runtime like that. So any existing tools that you have, you could run the EPBF tools against that and do that. And I think, as a more general thing, a lot of this low level code, we're using low level languages and a lot of stuff. We're also doing tons of work on the formal verification and on the kind of code discipline stuff and whatnot. I alluded to, and you can look at the code base to see that. But the big thing here is we don't really care about languages.
03:19:20.236 - 03:20:04.908, Speaker A: As we pull into areas like the consensus areas, like the runtime areas, we have zero objections to running those in rust. The big things are, when we're doing really low level interfacing multiprocessors, the thing we consistently run into is these languages aren't fully baked to kind of get down there, and we've been doing this for ten years. And then I think the other thing you look at, and this is more of a thing in terms of the discussions between Solana and whatnot, for independence of the existing validator. So we just don't, like, cut and paste the code and then just like, scribble out anywhere we see Solana run a sed and say, jump over here and then call it an independent validator that, you know, they didn't want us to use rust, they wanted us to use some other language. Yeah. Yeah. Well, I know not everyone does.
03:20:04.908 - 03:20:33.614, Speaker A: So actually, I hate c, but I hate all programming languages. There's less to hate in aggregate in c. So looking at it, putting that together, it's like Solana approached a bunch of experts at high performance C C code development and said, you can't use rust. So it's like, okay, well, that was not the hardest decision for us to make in the world. So cool. Yeah. Thank you.
03:20:33.614 - 03:21:05.080, Speaker A: I wonder if you have performance bench test without Numa and FPGA just to compare runtime performance of Manila client and final. We do have a lot of other. Sorry. Yeah. The question is, do we have benchmarks to compare side by side for performance? The answer is kind of. And when I say kind of, kind of a lot, a lot of that, like, I can look at the openssl that we used as the reference. We're about, you know, five ish faster than them.
03:21:05.080 - 03:21:46.458, Speaker A: Somewhere in that ballpark, if I run the same sig verify calculation, we can just swap out our implementation of it, put their implementation, and run it, and then see they're running at something for equivalent numbers at like the 160 microseconds, 120. It's a little bit difficult to give those benchmarks because, as per the other questioner, it's so computationally intensive, that thing, and so relatively memory poor, it really scales as clock speed. So if you're looking at it, you really care about clock speed. So you wouldn't do apples to apples, you're saying like, okay, is the clock speed, is the turbo mode set up and all those other things, but if you're looking at it, it's a ton faster for that kind of like, micro benchmark side by side. Another one that's frequently used is Dalek. Dalek. We've gone through kind of line by line.
03:21:46.458 - 03:22:15.738, Speaker A: We're kind of comparable. I think the big thing there when we look at it is there's some algorithmic differences. So in some circumstances, we've seen us be a little bit faster, faster. In some builds, we've seen it be a little bit slower. But Dalek that's currently used in the libraries is pretty darn good as well. And we've been comparing against that and then looking at saying, like, hey, are there any tricks that we can gain from this instead of just turning loose our internal cryptographers and doing it de novo? Cool. Okay, I got the time warning, so thank you very much.
03:22:15.738 - 03:22:42.834, Speaker A: Give it up for the fire dancer team. Thank you so much, guys. The fire dancer team will be over in the corner. If you have any more questions, they're happy to answer them. We're going to take a quick 15 minutes break. We're going to come back at 02:00 just to try to stay on schedule. There's food in the back here, so let's break and see you back here at two.
03:22:42.834 - 03:23:11.646, Speaker A: Hope everyone had a good break and got some food. Got to step outside. I know it was a little quicker than we planned, but appreciate everyone coming back so fast. Also, just want to mention that all of the talks you see here are going to be available on YouTube within an hour of them actually occurring. So, for example, the last talk with fire dancer, very technical, long demo. So if you didn't catch something, go check out YouTube you'll be able to see it there. But I wanted to introduce our next amazing speakers here.
03:23:11.646 - 03:24:06.274, Speaker A: We're going to do a panel discussion about something that probably, probably isn't talked about enough within Solana, which is mev, maximal extractable value for validators. So the panel is going to be moderated by Tushar Jain from Multicoin Capital, and he'll introduce the rest of the panelists. So please welcome Tushar and our panel. All right, hello, everyone. I'm really excited for this panel. Today. We have an amazing group of people to talk about what I think is going to be one of the driving forces that shapes how blockchains evolve, which is Mev.
03:24:06.274 - 03:24:35.556, Speaker A: Before we get started, I just wanted to ask each of you to maybe give a little background about yourselves and introduce yourselves, starting here. Awesome. So, I'm the founder of DFLO, which is a protocol that's building decentralized order flow auctions. Spent a ton of time thinking about Mev, so I'm pretty stoked for this talk. And prior to this, I was a quant researcher at an HFT firm. My name is Lucas Breuder. I'm the co founder and CEO of Geo Labs.
03:24:35.556 - 03:25:14.152, Speaker A: We're building high performance Meb infrastructure for Solana, help Solana scale to the next billion users. Hi, I'm Eugene Chen. I'm the co founder at Ellipsis Labs. We're building a new limit order book, decentralized exchange protocol on Solana. Previously, I used to do Mev research focused on PBS, on Ethereum. All right, so today we're going to cover some of the thorniest questions in MeV, and we're going to get into the weeds of it. So we're assuming the audience has some context about what this is.
03:25:14.152 - 03:26:25.614, Speaker A: Otherwise, there's a lot of resources online to learn a little bit more. But the first question I wanted to ask each of you is, what do you think of the various tactics being taken by l one s and l two s across the ecosystem to try and reduce, or potentially, in their words, eliminate Mev? Do you think it's possible? Do you think there's positive or negative externalities? Examples of this include, you know, the stance that arbitrum has taken around how the sequencer works or how osmosis randomizes transactions within a block. Yeah, start. So, I guess, you know, in terms of directional things like, I think l one s and l two s should definitely try and eliminate mev to the extent that it's possible. I think eliminate is maybe a little bit of an overloaded term. I think what generally people have come to somewhat of a consensus on is that you can sort of redistribute and move Mev around, but you can't necessarily just like get rid of it because it's just a human problem. Like, you know, contention for state is essentially something that it boils down to.
03:26:25.614 - 03:27:27.666, Speaker A: Yeah, I think that, you know, the FIFO and the pharaohsuken swing, things are kind of interesting. I think they have some negative externalities from spam, and it kind of turns into this like high frequency trading game that we see in traditional finance. I think that my take is that we should try to minimize it through like applications and try to minimize users exposure to meV through like aggregators and things like Nitesh is working on at D flow. And for everything else, it should be efficiently extracted through state auctions. So, you know, on Ethereum with pbs and flashbots is working on suave now, which would be very interesting. You kind of have like the highest payer bidding for state. I think that makes the most sense to me.
03:27:27.666 - 03:28:20.670, Speaker A: On Solana or on Ethereum, it's you're bidding for the entire block on Solana because of the way the account model works, you can actually have auctions for individual pieces of state, which seems to be the most efficient way to track the. Maybe on there. Yeah. When we see some of these FIFO mechanisms proposed, you often see them coming from teams running protocols that are completely centralized, where you do have full control over the ordering. I don't think FIFO transaction ordering is really possible in a decentralized, permissionless, validator world. Compared to something like optimism or arbitrum, which run completely single sequencer centralized, they can go for those sorts of options. At the end of the day, MEV extraction, efficient extraction is going to be the most economically efficient outcome for the network itself.
03:28:20.670 - 03:29:22.414, Speaker A: And I think it's up to defi protocols and other applications that are built on chain to build them in a way that is MEV aware, where the MEV surface that's exposed is as small as possible. And then it's up to the underlying l one to come up with an MEV policy that's both economically robust and able to redistribute the proceeds of MEV auctions back to the appropriate stakeholders, whether that's the end users or the validators or the stakers. Yeah, just to chime in on that a little bit more, I think that these methods being used to try and reduce or eliminate MEV have a lot of negative externalities. Specifically centralized sequencer. It's like, well, might as well trade on NYSe. Like, what are we doing here? I think, what's the point of all of this? You need permissionless validation of blocks. And if you have permissionless validation of blocks, you can't enforce FIFo, like you said.
03:29:22.414 - 03:30:25.924, Speaker A: And then I think, you know, MeV is inescapable because time doesn't stop between blocks. And so, you know, things are going to happen in the world and someone is going to make a trading profit off of that thing happening. And, you know, doing things like randomizing transactions within the block just actually incentivizes censorship, where the validator can say, only the MeV searcher's transaction will enter the block, and I won't let anyone else's transaction in because they're getting randomized in order within the block. So generally, I think it's unavoidable. I would just add one thing to that, where I agree that time doesn't stop, and therefore, MeV is something you can't really stop. But there are other, I guess, technologies that can contribute. And I think probably the biggest set of technologies is privacy preserving technologies, which do actually a lot in terms of, I guess, revealing and hiding the right information at the right time.
03:30:25.924 - 03:31:06.266, Speaker A: And that sort of coalesces into MeV being reduced at certain stages for very specific applications. I think a general solution is probably not really possible. Yeah, there's a lot of interesting things happening here in the cosmos ecosystem. I'm not sure how familiar this crowd is with them, but like Nitesh mentioned having some sort of threshold encryption on transactions. This is what Penumbra zone is working on in Cosmos, where DeFi trades are only are not revealed to the validator creating the block. They're only revealed in the consensus mechanism. And this makes a ton of sense.
03:31:06.266 - 03:32:19.564, Speaker A: It makes it very difficult to front run and censor for the validator. And then there's some other interesting things happening where if you have an app chain, say you have a uniswap chain on Cosmos, it doesn't really matter that your liquidity providers are getting, as long as you still have a robust MeV auction, because if that value is getting returned to the l, one token in this case would be the app chain token. You can just redistribute those profits to the auction revenue to the LP's who are losing the money there. Yeah, I think every however you extract MeBA, there's always trade offs associated with it. I think it'll be very interesting to see how it evolves on Solana, just because it's such a high performance chain. Like, I'm sure a lot of us were watching the fire dancer talk earlier. So, you know, if we start to, depending on the way that we try to, like, hide MeV or handle Mev on Solana, it could potentially impact the performance of the chain.
03:32:19.564 - 03:33:46.784, Speaker A: So it'll be really interesting to see how MEV kind of evolves on Solana. All right, so the next topic I wanted to cover with you guys is around centralization and how MeV can contribute to it. There's been a lot of talk about how MEV leads to validators wanting to co locate with the block builder in order to get the lowest latency and have potentially the highest amount of MEV that they can earn. There's also a really fantastic blog post written by Vitalik called Endgame, which talks about cross chain MeV leading to centralization of validation between various blockchains, where you would have Solana nodes running on the same box as an ETH node, as an osmosis node, as an aptos node, as, you know, whatever else, to maximize the possibilities of cross chain MeV. How do you think this will evolve? Like, will MeV be a centralizing force for all blockchains forever? Yeah, I'll kick us off again on this. I think to answer that question specifically, MeV is already a centralizing force on proof of stake networks. And I think the catalyst for what makes Meb a centralized enforce on these networks is private order flow, essentially.
03:33:46.784 - 03:34:43.558, Speaker A: You know, it's sort of a self fulfilling prophecy where blockbuilders that dominate, tend to dominate recursively and become resource starving entities to other blockbuilders. And essentially this boils down to a situation where there's like a centralized choke point on the network. And so I think part of your question was also, like, what does a solution to this look like? And I think the canonical solution here is decentralization of private order flow, which is something, you know, we think is really important in what we're working on. Yeah, I think. I think about centralization, like, two fronts. There's kind of like the validator searcher centralization, which I don't think will be the best long term solution. I don't think that an MeV searcher will be able to build the best block, given the data.
03:34:43.558 - 03:35:48.144, Speaker A: So, you know, could be a combination of arbitrage, statistical arbitrage, liquidations, NFT. I don't think that any one person will be able to do all of that and make the most money for stakers so that's kind of why we're building this kind of decentralized way to have multiple participants participate in block building. I think the other point about centralization is the location centralization. So on Solana, there's a lot of stake in Frankfurt and Amsterdam, which causes a lot of searchers to kind of go to that region to participate, and then you have kind of more stake going there. So that's something that we're working on a lot at Jito to kind of move stake out of Europe. And I think that l one s need to think about these, like, latency games very hard so we don't end up with validators all in the same data center. That data center going offline and causing the network to essentially halt.
03:35:48.144 - 03:37:13.438, Speaker A: Yeah, I think I agree with these guys that MeV is naturally very centralizing. And I think this is a theme we see all across the ecosystem, where decentralization is often trading off against economic efficiency. And so you need to push for decentralization wherever you can. And I think we see the, just earlier this week, we saw some of the benefits of decentralization, where if a data center suddenly kicks off a bunch of your nodes, is your, is your blockchain going to be robust to that? And because only 20% of the stake was on Hetzner, Solana was able to keep running through that. And in a world where the stake is much more centralized, that might not be the case with MEV, assuming you have no infrastructure for extraction whatsoever. It's very centralizing, because validators that can more efficiently extract MEV, which is a high skill game, they're going to be able to offer better returns to their stakers, so they're going to be able to accumulate more and more stake weight, which is obviously pretty centralizing to the network. There's many other forms in which MEV causes centralization, even when you have democratic access to mev extraction infrastructure, such as Cheeto in Ethereum.
03:37:13.438 - 03:38:55.166, Speaker A: Today, a lot of people in the mev world are worried about centralization caused by latency advantages. And this is in a world with twelve second blocks, where if you're a blockbuilder on Ethereum, and you get to see just a little bit more of the mempool than someone else, or, you know, the fair price on binance, just a little bit better than another blockbuilder, you're going to be able to build a slightly more efficient block. And these effects are much more pronounced in Solana, where block times are significantly faster than 12 seconds. And especially when you cut up a block into smaller slices. These forces are very real. At the end of the day, we're not going to be able to depend on the benevolence of a few centralized actors to maintain decentralization for the sake of the network, we need robust economic systems that mean that decentralization is actually the most efficient outcome where no one is incentivized to defect and centralize. Do you think that proof of stake can be that economic motivation where stakers realize it is not in their best interest or the network's best interest to have all of the validators centralized in the same data center or other types of centralization around blockbuilders, et cetera? And so the stakers actually force the decentralization because that's maybe short term economically irrational because they could earn a higher yield, but long term economically rational because the network is more resilient and more useful.
03:38:55.166 - 03:40:16.134, Speaker A: I think this is, it's possible, but it's a little bit dangerous because it's really the largest stakers who are going to be incentivized to spread their stake geographically. A smaller staker is going to rationally conclude that they have no effect on, on the total decentralization of the network. And so if you're relying on the benevolence of large stakers here, I think you're still running into some issues on decentralization. I think stake pools are pretty popular on Solana, and there's reference bots that are available that let stake pools move stake around depending on some criteria, one of which is you have like the them ASN centralization or location centralization. And I think stake pools are really good about kind of handling this for the user so that they don't necessarily need to know how, how to decentralize it. They kind of trust the stake pool to do it for them. But ultimately, the nice thing about proof stake is that if you don't like what a validator is doing, or you think that you can go earn more money elsewhere, or you don't like what this validator is doing with Mev, then you can just de stake and move it in like two days.
03:40:16.134 - 03:41:09.564, Speaker A: I think it's fun to be optimistic that stakers will be sort of morally about not going to these toxic validator nodes on the network. But I think the right approach is for us to design a system where we have economic incentives that prevent that. And I think Lucas is right. Stake pools is a good example of that, doing things like completely decentralized state space auctions and order flow auctions is how we avoid that sort of killing crypto. Got it. On the same note, I wanted to ask you all about the various types of malicious mev that exist in the world. Everyone has heard of these sandwich attacks, which are basically a front running attack where someone sees a transaction in the mempool and they try and front run that transaction.
03:41:09.564 - 03:42:40.204, Speaker A: What do you think is the long term equilibrium here? Will the validators that facilitate malicious MeV be the winners because they earn slightly more yield and people will stake to them and so they continue to accrue more yield? Or will there be a different equilibrium, maybe where stakers are exercising more discretion in order to protect the users of the network? I think stakers will probably go to where the highest yield is at, assuming that the validators like not totally messing with the network, like doing time bandit attacks, changing history, things like that. I think as these systems evolve, we can't continue to let people get sandwiched on chain. And I think that like, better app design and having apps consider what they expose their users to with respect to NeV will play a very important in removing how much it happens. Like if you go on like Orca or something and you set your slippage to 10%, it will warn you that you're going to get front run, or you could get front run. So I think apps need to handle that more, and I think users will go to where the highest stake is. But as I said earlier, stakers can move their stake wherever they want. So I guess we'll see what happens on the long time horizon.
03:42:40.204 - 03:43:36.730, Speaker A: I think it's going to be tough to ever come to a consensus on what forms of MeV are malicious and what forms are benign. For example, a lot of people feel like front running attacks that result in a worse fill for the user sending the transaction are malicious. Well, what about passive lp in a pool getting arbitrage against, is that malicious? Is that benign? Should stakers move their stake away from validators who are allowing those types of trades? I don't think it's very clear. I think Lucas is right. At the end of the day, a lot of this has come down to application design. If you're designing a front end for some trading protocol and you're not showing big red walls, warning lights when the user is typing in 50% slippage, I think it's kind of on you. When it comes to these transaction generated Mev.
03:43:36.730 - 03:44:25.584, Speaker A: For example, an oracle update, front runnable Dex trade. I think it's very possible that the equilibrium here is we end up with order flow auctions, where this is going to happen completely upstream of, of the protocol itself. So these transactions are never going to reach the Solana validators. And so Solana validators are not going to have any economic interest in them. And instead, as a user, you're going to send your transaction to some private RPC. This is going to happen completely off chain, and that service is going to auction off your order flow to independent searchers. And so instead of the revenue from the auction going to the validators and to the stakers, it's just going to go back to the user.
03:44:25.584 - 03:45:24.464, Speaker A: And so in this way, instead of searchers competing to give the user the worst fill, they're actually going to be competing to give the user the best fill by way of bidding for the right to mev the transaction. Yeah, I also agree with Lucas in terms of stakers will go where the yield is. It's just, you know, it's sort of up to us to create like the community to create incentive systems that economically reward stakers more than handshake deals between people you know, or people who know each other can reward those. So like, we just got to get to a point where who, you know, isn't an edge anymore in things like financial applications. NFT is just everything. And once we get there, I'm optimistic that stakers will go to the place that is least toxic in terms of validator nodes. Got it.
03:45:24.464 - 03:46:46.454, Speaker A: What are some more things that you think apps can be doing to more intelligently design to reduce harmful mev to their users? You mentioned a couple big red warning signs when you put your slippage for like 50% on the hand pool or order flow auctions, which I think make a lot of sense. Do any other ideas come to mind? Yeah, I think aggregators are really good about reducing mev. They essentially leave almost no ARB after the user's order is finished. Which means that the user, users, it could have exposed mev and a searcher or a validator could have taken it, but the user is getting more money back. I think one thing that I'd like to see is if you're aping into some coin and you set the slippage super high, having that reset on you after you kind of ape into it, but you are seeing like orca, uniswap, sushiswap on Ethereum have warnings about high slippage. So I think users need to be aware. I think that you have to kind of lean on the Dapps to do it for users right now.
03:46:46.454 - 03:48:19.856, Speaker A: And I think we should like keep doing that where dapps are warning users about exposure? Mev? Yeah, I think Mev aware protocol design is very dependent on the particular application. One example here from the Ethereum world where there was just like a strong improvement made to a protocol, is with Euler finance, with these traditional borrow lend protocols, like AavE, like Solend, for example, when a position is allowed to be liquidated, there's a fixed liquidator surplus that's paid to a liquidator or mev bot to take on the debt, to take on the, to basically pay back the borrow from a user that's getting liquidated. And Euler finance instead essentially encodes an auction into their own protocol. And so less of the surplus is going to the validators and more of it is staying in the hands of the user. So this is a pretty big step function improvement, and I think it highlights a general design pattern that can make sense for some, but not all, protocol developers, which is throwing a multi block auction implicitly into your protocol. Obviously, it does not work for opportunities that are exposed as sort of. In a sort of step function way where if you want to have an auction, you're going to have to have it outside of the protocol, like in a Jito auction, for example.
03:48:19.856 - 03:49:20.754, Speaker A: But this is just one example of how a protocol is going to be designed to be more MEV aware, which is going to be protecting their users, essentially letting less of the MEV make it all the way to the validator level, trying to keep it within the, the complex of the protocol and its users. I think the key here is to have a system where a lot of this stuff is impossible. For example, like a big red warning sign when you set your slippage to 50. I mean, it's nice, but it's treating the symptom, not the disease. And in my opinion, I think we get 95% of the way there by just having for this sort of problem order flow that is accessible not through handshake deals, but just through permissionless auctions. Yeah, that's my takeaway, is treat the disease, not the symptom, for a lot of this stuff. Got it.
03:49:20.754 - 03:49:57.604, Speaker A: All right, so we have time for one last question for you all. And Eugene, you mentioned a point earlier that I thought was really interesting. You talked about how amm app chains can redistribute MeV to LP's, basically. And so I guess I want to start with you. Like, what do you think are the fairest ways to distribute MeV? Who should get paid? Is it like the LP's and amms? Is it the layer one token stakers. Is it the RPC endpoints who should get paid? I think should is a very. This world is doing a lot of work in this question.
03:49:57.604 - 03:51:02.022, Speaker A: At the end of the day, I think it's going to be the entities that have the economic bargaining power who are going to get to decide where the MEV goes. For example, in the current world, on Ethereum or on Solana, the users really have no say where the MEV goes, because once their transaction hits the mempool, once their transaction hits the validator, they've lost all control over it. And they can take take back this control by auctioning off their order flow privately before it gets to the validator layer. With app chain in particular, I think the protocol designer ends up with a lot of freedom as far as designing rebate type mechanisms. And at the end of the day, today, it seems like LP's are pretty okay. AmMLPs seems pretty okay with just like losing money on uniswap, it's like on the order of hundreds of millions of dollars per year, they just lose money and they keep providing liquidity. So maybe the market there is saying that aMMLPs don't need to get paid.
03:51:02.022 - 03:51:56.598, Speaker A: And if you're uniswap and you're collecting that MeV revenue, you can kind of do whatever you want with it. If you end up in a more competitive world where some LP's are getting rebated and others or not, then I think you might see some movement in terms of where people are willing to provide liquidity. And so I think you kind of just have to let the market decide, because there's multiple redistribution mechanisms that all make sense and that are all fair, or at least not provably unfair. So I tend to answer like, yeah, let the market decide here. Yeah, I think that MeV should be distributed to the users in the community. So ideally that would be through proper app design and trying to minimize MeV exposed there. But for everything else, I think it should be distributed to stakers and validators.
03:51:56.598 - 03:52:52.698, Speaker A: For the l one, that was really important to us when we were working on Gedo, and something that we built from the start was being able have the ability for validators to share MEV with their stakers. So anyone can kind of participate in the system. Yeah, I think the right answer here is that it just depends on what the application is and who the users are. Like, if it's two quant prop firms that are battling it out on some decentralized order book, MEv should probably go to whoever's facilitating that exchange. But if it's like, you know, a retail trader who's like, opening up a wallet and trying to swap the MEV shouldn't go to the validator or blockbuilder. I think it should go back to the retail trader. Like, it's a simple sort of, you know, intuitive action they're trying to do, and it shouldn't be so complicated to just make a swap, you know? Got it.
03:52:52.698 - 03:53:32.894, Speaker A: Well, awesome. Thank you, everyone, for joining us, and thank you to the panelists for sharing the knowledge. Thank you very much. What an amazing panel. Definitely learned a lot about a subject I did not know too much about. So, but while we get the stage set up here, our next speaker, Stephen Lucher from Solana Labs. I had the privilege of going to the Vancouver hacker House a couple months ago, my first hacker house, and if you haven't been to one, I strongly suggest going to a Solana hacker house.
03:53:32.894 - 03:54:18.066, Speaker A: The schedule just got published and it's on the monitors around here. Going to be doing them all over the world next year. But Stephen is from Vancouver, and it was amazing to see how he was able to work with so many different types of projects to just help them understand mobile much better. And if you heard Anatoly speak yesterday, he talked about how mobile is such an important focus for web3, for Solana. So I'm really excited to introduce Stephen to talk about how you can develop on mobile. So give it up for Stephen Lucher. All right, thank you so much for being here, everybody.
03:54:18.066 - 03:55:22.094, Speaker A: My name is Steven Lucher and I work on the core team that's building the saga phone at Solana Mobile. I work on JavaScript infrastructure projects there. So that's things like wallet adapter and the wallet adapter specification for J's and for react native and also at Solana Labs on JavaScript infrastructure projects like web3, JS and wallet adapter. And this talk today is called the Gateway drug to mobile. It's a story about how you maybe did one or two lines of JavaScript, maybe in college when you were experimenting, and one thing led to another and pretty soon you woke up like this incredible mobile developer. Because right now, what's the situation, right? Nearly every website on Solana right now, nearly every application is a website, and it's a website written in JavaScript. And there's a good reason for this.
03:55:22.094 - 03:56:03.454, Speaker A: The reason is sort of threefold. It plays into the strengths of the web. One of the first strengths of the web is that it's a great rapid prototyping environment you can set something up really fast. You can go from idea to implementation in just a few hours, throw something on your local host or publish it to Vercel and send someone a URL for feedback. And speaking of feedback, the web is amazing at shortening iteration cycles. You can invest in the stuff that you see working, double down on that stuff, and you can just abandon all the features that aren't. And it's also the ultimate zero install distribution platform.
03:56:03.454 - 03:56:55.646, Speaker A: You can just send someone a URL, you can just send someone a URL. And users always get the latest version of your application with all of the latest features. So I think that the hypergrowth cycle of apps that Solana has enjoyed for the last little bit has been in large part due to the strengths of the web. But I have got to believe that the next hypergrowth cycle is going to owe itself to the shift to mobile, and it's going to owe itself to a shift to mobile, not for the same reasons as the web, but for rather different reasons. First reason out of three I want to present to you is that people have got these things. They have phones and they've got a lot of them. Let's do some statistics.
03:56:55.646 - 03:57:38.018, Speaker A: This is focused on the United States, a place called the Pew Research center. The percent of us adults who say that they own a cell phone is all the way up to the wall at 97%, as compared to folks who report owning a desktop or a laptop computer, which is only at 77. And if we get a little bit more nuanced about that and take another slice and say, okay, well, how many own just smartphones? It still exceeds desktop ownership. Now you're looking at this and you're like, I'm not convinced. That's only eight percentage points away of the market. It's not enough of a market for me to build a mobile app. I'm going to keep focusing on desktop.
03:57:38.018 - 03:58:40.580, Speaker A: I'm going to keep shipping apps on desktop, web. Well, access to the devices is not the only thing. The second thing is that people who have phones use them, and they use them a lot in terms of attention. The percent of smartphone folks who say that they go online mostly using their phone, this is people who say when they're on the Internet, they are mostly accessing the Internet through their phone. 46% of folks across the United States and another 23% say, eh, you know, I go on the Internet on my desktop and my phone roughly equally. So you add those two numbers up, you know, that's about 70% of people who say that at least half of their time on the spent on the Internet is on their phone. And if you take a different slice of this and you look at, just say, 18 to 29 year olds, those numbers go way out of whack.
03:58:40.580 - 03:59:31.140, Speaker A: 60% of folks say, when I'm on the Internet, it's probably on my phone. That is a massive market of attention. If you're not shipping a mobile app that looks beautiful, works well, delivers value to people on their phones, wherever they are, you're missing out on a huge part of their daily attention. And that's a market that I think you want to capture. Now, the third reason is that not everyone has a desktop computer. If you take a look at the number of us adults who say, I have a phone, but I do not have Internet connection at home, that's 15% of us adults. At current numbers, like, what is that? Like 45 million people? And this doesn't even cut equally.
03:59:31.140 - 04:00:37.108, Speaker A: If you take a look and you ask, Hispanics report at a rate of 25% that they have a phone connected to the Internet but no Internet connection at home. If you look across the country and you say you look at people who are making $30,000 or less, 27% all of their time spent on the Internet is on their phone, because they don't have a desktop connection, a broadband connection at home. We call these mobile only users. So we've got this saying in crypto, right? We have this aspirational goal that we're all here to build the products and services and networks that are going to onboard the next billion people. And I have got to tell you, if we want to have a hope of onboarding the next billion, we are going to have to meet them. Where they are and where they are is overwhelmingly on mobile right now. The interoperability story between mobile phones and mobile wallets is kind of a story of heartbreak.
04:00:37.108 - 04:02:44.824, Speaker A: Anyone who's tried to sign a transaction, do a mint on the go knows that it's kind of this jumbo of different protocols, different operability stories, different applications, and different strategies for being able to connect to wallets to regular websites. We're hoping, with the advent of the Solana mobile stack, to create a set of interoperable standards that will abstract away all of this complexity, so that there's a high probability that when you go on your phone to do something in crypto, that it. It just works. Now, if you're buying what I'm selling so far, and you, like me, believe that there's going to be a massive shift to mobile next year, let's go through. Who's this talk for? Should you be worried about this shift to mobile? And then what is it that I want you to do by the time that this 20 minutes is over? So maybe you're this, maybe you're a developer, you understand your problem space and you're used to executing really, really well on desktop web. You might be wondering, are my skills obsolete now? Am I going to need to retrain and learn Kotlin or Java or objective C or Swift and become this mobile developer and leave everything behind? Or maybe you're a product manager and you have a team of those folks and you're thinking, ugh, do I need to hire a new mobile team and retrain them on our company's products and policies? Or am I just going to need to get more funding, increase my burn rate so that I can deliver the same amount of value on now fundamentally two different platforms? Or if I can't get that cash, am I just going to have to settle for whatever I can do? Am I going to have to settle for less because I'm splitting my attention between two different platforms? Well, not usually an optimist. Today I'm here to make the claim that the answer to all of those questions is no.
04:02:44.824 - 04:03:40.124, Speaker A: And here's the proof of why that answer is no. All of these applications that you're seeing on the screen right now connecting to brand new mobile wallets, like the new mobile wallet adapter compatible Phantom and the new mobile compatible solflare that came out, they're roughly about two weeks old. All of these applications and operations are applications written in JavaScript. There is no native mobile code harmed in the filming of these videos. And these videos were filmed straight off of me, my saga device. So two strategies for you. If you're a JavaScript developer and you want to dive into mobile with us, the first strategy is just to take your existing web app and make some small modifications.
04:03:40.124 - 04:04:15.828, Speaker A: The modifications look something like this. First, make it render well on small screens, which is harder than it sounds. The second you got to use the new SMS Solana mobile stack mobile wallet adapter. What does it mean to render well on small screens? Sounds obvious. Not always. It means you have to employ responsive design principles. Mobile phones come in all shapes and sizes, resolutions, nonstandard portrait or landscape orientations.
04:04:15.828 - 04:05:38.164, Speaker A: You need to make sure that when your website is rendering on one of these devices of unknown size and resolution, that it is readable, that it's beautiful and that it's functional. And you're web developers, so you already know how to do this because you know CSS, you know CSS layout, and you might be familiar or might not be familiar with CSS technology like media queries, where you can deliver different layouts to different devices depending on their viewport, width and height. And this one is so, so, so important and not to be missed. You have to start designing for touch. You have to eliminate mechanics in your applications that depend on hover states because there's no cursor mobile. If a cursor is like a small surgical knife that's really good at hitting tiny little targets and dragging tiny little sliders, these things on the end of your arms are like sledgehammers. You have to make sure that every button and every slider and every text area in your application can be hit by one of these ten sledgehammers that the hit targets are, are large enough so that folks can actually do the things that they need without making errors.
04:05:38.164 - 04:06:29.174, Speaker A: All right, the second thing you have to do to upgrade your desktop website to run on the mobile web is you've got to get on mobile wallet adapter. But what do I mean by that? Well, some of you in the audience are using a library already called Solana Wallet adapter react. Throw up your hands while I'm talking. If you're using Solana wallet adapter react already, and you guys know that you're using it to create a button like this where you connect to a wallet. And when people mash that button, it connects to a browser extension wallet on their desktop computer. But you also have experienced that when you mash that button on multiple, the story is largely, the behavior is largely undefined. It's not really clear what's going to happen there.
04:06:29.174 - 04:07:34.550, Speaker A: Well, what I'm here to tell you today is that we have an upgrade to that particular library. Jordan Sexton and I have done awful, terrible things to wallet adapter react so that you don't have to, so that you can enjoy all of this new, new mobile functionality with very little effort. But it's not no effort. This is going to be one of the world's largest code mods. We need every application on Solana to take this upgrade and integrate it into existing websites so that when all of your customers think, ooh, I'm going to go visit my favorite website on this new phone that I built, that things just work. But it's going to take a massive effort from all of us. We all need to go to every one of the Solana sites out there, every one of the Solana applications, and make sure that we have mobile wallet adapter capability built in and that we've tested it on a handful of Android devices.
04:07:34.550 - 04:08:21.490, Speaker A: The reference implementation right now on Android. So I'm going to outline the steps of this massive, massive code mod that we all have to do together to enter the mobile era. And it looks something like this. Just upgrade to the latest version. All of this stuff is in the box. If you're on mainline wallet adapter react, you just have to bump to the latest NPm version and you get all of the mobile wallet connections, signing services and transaction sending services for free with the MWA wallets that are compatible today and forever in the future. All right, so get going with this upgrade.
04:08:21.490 - 04:09:18.350, Speaker A: And remember that slide that I showed earlier of all of these websites? You know what they say in crypto, if you're not early, you're late, right? All of these folks have already done the upgrade. And if you visit them on a saga, if you come to the saga booth or you visit them on an Android phone with the brand new version of Soulflare and Phantom, you should be able to connect and do transactions on these sites. And they're not alone. All of these other folks have also made the upgrade. And from all reports, they say that it took them somewhere in between the neighborhood of 30 minutes to, you know, maximum 4 hours if they had a lot of customization in their app around wallet adapter. All right, so that's what it means to use mobile wallet adapter in your traditional web app. Get that new Solana Wallet adapter react or do a custom integration.
04:09:18.350 - 04:09:58.114, Speaker A: We're always here to help you on the Solana mobile discord. If you have troubles, just reach out on GitHub or on Discord. So if that's strategy number one, take your existing web app and bring it into this mobile world. Here's strategy number two. Maybe you want to go a little bit further and you really want to create something that is mobile native. You want to use device APIs on the phone to create incredible new experiences. There is a technology, if you're familiar with building websites using react, you can also build mobile applications with something called React Native.
04:09:58.114 - 04:10:52.494, Speaker A: This is a way to build a cross platform native feeling app using device APIs, and you can reuse your existing code most importantly, and your existing skills and your existing team. Everything is the same. Hooks work, suspense works. All of the reactions that you're used to using and building a native feeling app with only react JavaScript and CSS still in this paradigm lets you access device APIs like vibration and location and the camera and NFC scanning and all of that stuff. And the mobile wallet adapter, just like on the web, written in JavaScript, works also in React native to be able to connect to mobile wallets. Here's the interesting part. You should by all rights be able to reuse as much of your web code as possible.
04:10:52.494 - 04:11:51.034, Speaker A: Any of your JavaScript and react business logic we're talking about network fetching and transaction signing and data management should be reusable, as is in react native. Your state management architecture, whether you're fetching data from graphQl or a rest endpoint, or you're managing data in redux or relay or SWR, all of that too should work in react native without modifications. And also the Solana web3 J's client works in React native. You can use all of the usual signing services and RPC communication services that you're used to using on the web web. Also in react native. Okay, so what is the bad news then? What is the stuff you can't reuse? And it comes down to this. If your application is really, really business logic heavy, you're in luck.
04:11:51.034 - 04:13:08.084, Speaker A: If the meat of your application is creating transactions, making calculations, fetching and sending data to the network, all of that mostly reusable as is. On the other hand, if your app is really heavily weighted towards the user interface, if you have a ton of beautiful charts and animations and user controls and transitions in your app, a lot of that stuff is going to need to be rewritten, none the least of which because you're on a different, fundamentally different display and display size and input devices, but also because the layout paradigms, with the exception of CSS, are fundamentally different on mobile. But I believe you can do it. And with that comes to the last part of this presentation. What am I here to ask you to do? I'm here to ask you to do these three things. The first thing is, if you own a web app on Solana, make the upgrade to wallet adapter. You can shoot this QR code leads you to a URL with an upgrade guide that includes instructions on how to upgrade and a test plan to run to make sure that you've got it right.
04:13:08.084 - 04:14:01.516, Speaker A: And you're not the only one. If you have an app that you love on Solana, and you want to make sure that when you get a mobile phone like saga that it just works, go and hound them. Go and ask your favorite apps if they've made the upgrade and send them the upgrade guide. The second thing I want you to do is I want you to at least study the react native app that we've written to see how your J's react paradigm translates to writing a mobile app. And I think you'll find that it's not at all scary and it's very interesting in that you can build extremely powerful things with it. And then the last thing is just join the Solana mobile discord. We're always here to help, to answer questions, whether you're building a wallet or you're building an application and trying to use the mobile wallet adapter SDKs just hit us up anytime.
04:14:01.516 - 04:14:27.474, Speaker A: So, JavaScript ers, welcome to Mobile. Thank you so much. And come and visit us at the saga booth with any questions you have. Right in the back corner corner there. Thanks for your time. It's been a pleasure. It was an insightful talk, as I expected.
04:14:27.474 - 04:14:56.604, Speaker A: Amazing to hear from someone who just understands mobile to the nth degree. Every possible detail. So if you see Stephen walking around, you have questions about mobile. He's a perfect person to ask. Really excited for this next panel. I know we're running a little bit behind schedule, but we're just going to do our best to stay on track as much as we can while giving the presenters the time that they need to share all of this incredible information. Kind of staying with the same theme, though, of mobile.
04:14:56.604 - 04:15:50.534, Speaker A: I know that Stephen touched on the mobile wallet adapter and that's something that I know a ton of people are excited about. About. And at the saga phone launch earlier in the year, Armani over at Coral talked about how there's this new concept of ex NFTs and what that means for the Solana ecosystem. And so I know since then there's been a lot of interest in learning more about that in understanding how to build XNFTs, understanding what the folks at Coral are cooking up in terms of launching backpack, their new wallet wallet. So that's what we're going to hear today. I'm super, super excited for that, because, again, it's an area that I've heard so many people asking questions about. But the great thing about this stage is that you get to hear from the best minds in crypto who are working on the true cutting edge technology that is going to make the difference for Solana over the long term.
04:15:50.534 - 04:16:41.124, Speaker A: And I know we're having just a few minutes here to set up these slides, but I think what Stephen presented was so important because, as he mentioned, the next billion users are not going to be the users that we see in the room today. They're going to be people who are using their mobile phones first. And that's the blocker that I sort of hear about a lot of the time when I talk to friends, family about using crypto is that it's not just that existing tools are desktop focused, it's that they're designed with that type of user in mind. And so when you design things for mobile, that means that you have to simplify, you have to make them more sort of engaging to the user because they have a smaller screen. So, as I said, very related to our next speakers here. So without further ado, let me introduce John and Tom from Coral, the makers of backpacks. So give them a hand.
04:16:41.124 - 04:17:16.920, Speaker A: All right. Hi, everyone, I'm John, this is Tom. We're working with backpack. We're at Coral and. Yeah, so we're both developers. We're both developers, yes. I'm going to talk through what Tom's doing on his laptop.
04:17:16.920 - 04:18:04.070, Speaker A: So we're going to show you how to build an XnFT. But for anyone here who doesn't know what an XnFT is, we'll just give you a very brief rundown of like, how to set up backpack. So, yeah, firstly, we're an open source repository, so you can go to GitHub.com Coral XYz backpack and then we have the releases page. So one thing to note is we've been giving out invite codes to people who are like really good at our waiting room games and they managed to get the answers right. But everyone here has a cheat code. You can go to the pavilion and we've got a booth there and we're giving out invite code.
04:18:04.070 - 04:19:03.564, Speaker A: So if you go and find the backpack booth, there's a QR code, you scan, they'll tell you what to do. We're going to use a local build. And the thing that's different about the local build is you're getting features that aren't in production yet, so you'll get the latest and greatest stuff, but you won't have the username stuff and you're definitely going to want the username stuff in the next few weeks. We've got a lot of things coming around the corner. Use the edge build locally if you need to run it locally for now, but I recommend you get an invite code. So Tom's just going to download the build edge version and then to install it, you go either in chrome or brave, you want to go to Chrome. So instead of like HTTP Extender, and once you're in there, there's a toggle at the top right, and you enable developer mode.
04:19:03.564 - 04:19:39.350, Speaker A: So now we've done that, we can get the zip file we just downloaded, unzip it and then drag the directory onto there and boom, you've got backpack locally. So now, yeah, once we click there, Tom's just pinned it in to the extension bar at the top. Recommend doing that, because otherwise you'll have to keep opening it up. So if we go to create new wallet, we're going to create with a recovery phrase. There's not going to be any sol in here, so try stealing it. But there's nothing to take. Click next.
04:19:39.350 - 04:20:12.714, Speaker A: And yeah, this is one of the big features of backpack right here. We're multi chain, so already we're sporting Ethereum and Solana. Obviously, today we're going to talk about Solana stuff, but we're constantly going to be adding new blockchains to it, as well as being able to run XNFTs on the different blockchains. So we agreed to the terms of service next, and we're set up. So now we've got backpack installed. You'll see there's three tabs at the bottom. It looks like a standard wallet view.
04:20:12.714 - 04:20:32.514, Speaker A: Right now. That's what you expect. You've got your balances, but this middle tab, this is our special one, this is our secret source and it says no xnfts. So that's sad. So we should go and have a look at the XNFT gallery. And if Tom connects backpack here. So this website is just Xnft GG.
04:20:32.514 - 04:20:57.494, Speaker A: So now we're connected. So what can we do at this point? Well, basically, it's like a store of, like, all the ex nfts. Everything is free right now. We're going to have creative royalties, all these things, but it's still very early, so. Yeah. Which one should we show? Flappy bird? Yeah, it's the most popular one. Tristan tried to show flappy bird technical gods weren't playing along, so we're hoping they'll play along with us.
04:20:57.494 - 04:21:28.164, Speaker A: So there you go. That's an insanely fast flappy bird. I think our high score is about three on this. We wanted to make it challenging and as you can see, Tom's popped it out as well, so he can, like, browse the web and stuff while he's using it. The other thing to note is, at the moment, we're recording the scored locally, but with the username system, we're going to have, like, high score leaderboards, we're going to have invites, a whole sort of like, multiplayer gaming lobby. Basically. Right.
04:21:28.164 - 04:22:20.524, Speaker A: So we've done that. Now the next thing we want to do, if you have backpack already installed and you had an invite code, you'll see that when you're on the XnFt page. There's an app on there called Simulator Tom's because we're using the latest and greatest version that hasn't been approved for the Chrome store yet. We have this toggle that we switch on because the simulator was confusing some users who aren't developers. So they're like, what is this simulator? So we put it in the settings, you into preferences, and then developer mode, and then you toggle it on, then close that, go back, and now we have an Xnft installed. It's a simulator. Okay, so what's the simulator? Basically, it's something that runs inside backpack, and it's watching like one of your local ports.
04:22:20.524 - 04:22:50.070, Speaker A: We use port 990, complete the arbitrary. It's just the port that we all had free. You'll be able to change that if you want, if something's on there. But what's cool about this, we've got a Cli. So if Tom writes and everyone that's got node installed can do this, you won't have to download things and install them. You literally write MPX, xnft, init. And in Armani did a tweet where he just wrote this LFG.
04:22:50.070 - 04:23:08.314, Speaker A: So that's the name of our project. So we're just going to go with it. So yep, that's installed. So great. So now if we go into the directory, the LFG directory, and we run what it tells us to. So yarn, you don't have to use yarn, we're just using yarn. It's in our project.
04:23:08.314 - 04:23:46.484, Speaker A: Yarn installs all the dependencies. This is pretty like apologize if there aren't technical people here, but you'll be able to see what's going on. So yeah, once that's installed, we'll be able to run yarn dev. And what yarn dev will do is essentially run like RXnft locally on a port, but it has hot reloading. So as you make edits to it, if you've used like any react or vue or svelte frameworks, you'll be very familiar with hot reloading. So that's running now. So if Tom now goes back to the simulator and opens it.
04:23:46.484 - 04:24:15.344, Speaker A: Hello world. Yay. Like the classic developer thing. So, okay, great. We've got like a really, really boring looking window here. Like, there's nothing particularly interesting, but what's the interesting part well, we can start, start to make some edits. So now we're inside this Xnft context, we can actually fetch the user's public key or their username.
04:24:15.344 - 04:24:49.712, Speaker A: And we don't need to install any third party dependencies yet. So we don't need to install Solana web3 j's, or if this is an Ethereum, XNFC, we wouldn't have to install ethers. We'll just have everything provided directly in. And there's, like, a few reasons for that. Like, one of the big ones is to simplify the development experience. But another reason is just if these APIs and things change, if everyone's using, like, the same sort of thing when they have to upgrade the xnfts, hopefully it will all work. So.
04:24:49.712 - 04:25:04.856, Speaker A: Oh, Tom's changed it to hello, Lisbon. I missed that. That's cool. So the. Yeah, I mean, another user useful thing to have in this xnft is the public key. So we've created some custom hooks here. We've put use public key.
04:25:04.856 - 04:25:27.156, Speaker A: So let's see if this pops up for us. I mean, it's not the most exciting thing yet, but we're getting there. Vim under pressure, everyone. Okay, cool. So that's our public key. Now we have a public key. Like, we know we're, like, connected.
04:25:27.156 - 04:26:14.968, Speaker A: The context is all fine. If we changed our, if we added, like, another wallet inside backpack, it would have the other public key when we're inside it. So we can do a lot of stuff with this, obviously. I think the first thing that we'll do is, like, sign a message, maybe. So message signing for anyone who's not familiar, it's a way of proving that you, you saw a message and you said, yeah, I agree with that in its simplest terms, but because it's cryptographically, it's a cryptographic proof, basically, it's a way of guaranteeing that the person that did see that and say, yeah, that's. That has that address, they have the private key to that address, but they don't need to give away any of that technical detail. Like, it's just a secure way of doing things.
04:26:14.968 - 04:26:47.062, Speaker A: So we actually use this message signing in the background when you sign in with your username into backpack, because we don't want to store anyone's password. We don't want to have any sort of leak news or anything like that. So, yeah, we just use this message signing. So Tom's now made a button. And here you can see we're using Window Xnft. Solana signmessage so if we were using Ethereum, it would be window XnFt, ethereum times. We're going to try to have like consistent APIs.
04:26:47.062 - 04:27:47.356, Speaker A: It's still fairly early days with this stuff, but now if Tom clicks sign, okay, so what's happened here? We haven't created any of this API, any of what you're seeing now in this xnft. This is actually backpack. So this is rxnft talking in the background to backpack saying, I've got a message to sign, I want you to sign it and then come back to me and ask the user to approve it. And this approval part is very important because we don't want people to create like, I don't know, like scam X nfts where it's doing things on behalf of the user without the user knowing. So every time we do something like this, it's going to present up to the user and say, oh, like this, XnF wants to do this. We're hoping like in all cases, like it's something that the users clicked. I think this might only come up from like clicks, but once click, once Tom sees this, he can either say, okay, no, I don't want to do that, I'll cancel that, I'll give it a terrible review on the App Store.
04:27:47.356 - 04:28:17.264, Speaker A: I don't know what it's doing, or you can approve it. And yeah, basically once you approve it, you'll be able to do like login systems, you'll be able to do gaming high scores. You could do like dial votes type things. The other nice thing with just message signing is it's instant. So if we're doing this on Ethereum or some other chains that have got longer block times, we can still do some instant things. So I think the next thing we said, are we going to do a payment? Yeah. Okay.
04:28:17.264 - 04:29:02.096, Speaker A: So Tom's just, we're going to make simpler APIs for this too. But Tom's just a bit of a legend and he's going to write the code to generate a new payment. So you can imagine, again, if this was like, I know, some sort of like social chat app or it was a game or something and you wanted to be able to integrate payments into it, you'd write a few lines of code. Tom's doing great. So yeah, once this comes up, basically he's going to click the button. It's going to say, do you want to send a payment? I mean, it's one lamp port. It's a minuscule amount, but it demonstrates.
04:29:02.096 - 04:29:27.162, Speaker A: So there you go. So this again, we've not imported any Solana. Oh, we have at the top, actually, just the solana types. Yeah. So yeah, we're going to fix that. But yeah, so once this goes through, that would actually send a payment and it's approved the transaction and it's gone through. What were we going to do after this one? So we're both developers, we're not going to make you something pretty.
04:29:27.162 - 04:30:09.804, Speaker A: I'm sorry. Like we should direct them to the docs in the. What? Sorry, the docs, the documentation for the. Oh yeah. So if you go to Xnft GG, if you're interested in doing any of this stuff, we have a docs page at the top, it's just on the header. Everything we talked about here is in there. You'll see that our API that we use for the react x NFT stuff, it looks very, if you've used react native or react three fiber or something else that uses react through the reconciler, which is you're not just saying like Dave or h one or something like that.
04:30:09.804 - 04:30:49.164, Speaker A: We have custom elements and the reason we have these is because we know it's like very early with backpack. We don't know what the future holds for us, but we do want everyone to come along for the ride. So if we decide like, oh, Backpack's gonna have like a custom theming solution, people are gonna say, I want all my stuff to be pink or something. Or we just think, oh, like backpack doesn't look right at this size, we're gonna make it bigger. As long as you've used these custom elements that we provide, your Xnft will automatically adapt and adjust with us. So it's always going to look like it's part of backpack. So yeah, we definitely recommend the react Xnft route.
04:30:49.164 - 04:31:41.724, Speaker A: We're looking to support other frameworks as well. But there is one other way that you can run an Xnft if you want to just get something up and running super quickly, or if you have an existing service and you think, oh yeah, it would be cool if I could, I have say a big one for us is games. So a lot of people are like, okay, I've got this game, it's like in the Unity web player, I can do the JavaScript, I can talk between Unity and JavaScript, but I don't really want to have these custom components. So we made this little helper for the Xnft Cli. And if you run, okay, can I get back up again? Yeah, so if you run MpX Xnft dev space and then Dash. Dash iframe. And then you just put an iframe URL in there.
04:31:41.724 - 04:32:08.464, Speaker A: Like, we don't recommend this route, but it's good for games. At the moment, we're still working out, like, the best way to do games. But, yeah, Tom, if you open that, you'll see that Xnft is like embedded. Again, this is an open source version of Cross hero. That Tom sucks. Yeah, it's on GitHub, but yeah, any kind of game and stuff you can imagine, like what this would be like or any other kind of game. Like, we were playing Pac man earlier.
04:32:08.464 - 04:32:33.360, Speaker A: What else were we doing? We're doing like, calendar, apps. Anything's kind of open to it. Yeah. Was there anything else? So there's also the publishing flow. If you end up producing an Xnft, you can go through a publishing flow on Xnft GG to get it listed up here so people can discover it and start using it. I think that's. That's all we've got.
04:32:33.360 - 04:32:51.900, Speaker A: Yeah, I think that wraps it up. So we've got a discord. If you come to either the booth where we're at at the main pavilion, we're just in the front, on the left. Come and get your invite code. Come and get your username. You're going to want one of those soon. We've also got like, an NFT collection coming out and that would be very useful for that.
04:32:51.900 - 04:33:18.637, Speaker A: That's all I'm saying. And, yeah, any questions or anything, either look for us, look for Armani or Tristan or any of the backpack people, and we'll gladly help you out. Yeah. Good luck, everyone. Thank you. Awesome. That was a great presentation.
04:33:18.637 - 04:33:52.714, Speaker A: I love how practical all of the presenters are. I saw so many people taking pictures, trying to follow along for themselves, because there's so much that you can do, actually, with the advice that you're getting on this stage. And, yeah, reminder that all the talks are recorded. They're all available on YouTube, probably about an hour after they actually occur. So if you want to check something out, applies to every single stage, so you can follow along there as well. Our next speaker is going to be Brandon from inflect, a popular company, apparently. Yes.
04:33:52.714 - 04:34:28.744, Speaker A: Inflect is one of our amazing partners in running the server program at the Solana foundation. So they have a booth just back there along with our other server providers. But if you're a validator, this is something you know very well, that the Silvana foundation supports validators with provisioning servers and inflect is a really crucial partner in that. So really excited to introduce Brandon from inflect. Thank you. Hey, guys, can you hear me? All right, my name is Brandon. I'm from inflect.
04:34:28.744 - 04:35:08.444, Speaker A: My position is the head of customer engineering. Basically, my job is to help my partners and customers optimize their infrastructure. Awesome. So I've been with inflect for about six months now, doing that. A couple years before that, I worked at Equinix Metal helping run the Solana server program at that level. In that time, when we started setting this up, we found that there was a couple different hardware types that we were utilizing. One of them was really designed for rpcs and the other was designed for validators.
04:35:08.444 - 04:36:20.614, Speaker A: The RPC specific node was really similar, but the main difference between the two came down to really in the processor. So the processor of the rpcs had 32 cores and was a bit slower of a clock speed, whereas the validators were lower core counts, but higher clock. What we started to find though, was that essentially we were dividing the pool into two separate groups, and if we could find a way to make those acted the same, that we would give validators and rpcs more options. So the real goal was to figure out, is there a way to make these slower running RPC nodes catch up with the faster? If you're not familiar, most modern processors are limited by what they call the TDP or the thermal design power. Essentially, there's a maximum amount of heat that can be generated in the processor and it can't be exceeded. If the server is configured properly, it'll actually throttle the clock speed to maximize that performance. So the theory that I had was, if we can turn down the number of cores in that system, we end up allowing more of that power threshold to be applied to the clock speed.
04:36:20.614 - 04:36:55.668, Speaker A: So I took a system and I turned it from 32 cores down to 24, and quickly realized that I didn't have a way to benchmark it. So the reason that I should benchmark Solana is necessity. So when I was getting started, I just knew that I wanted to push the processor as hard as possible. And there's an old tool that some of you may know called Prime 95. It's an old open source project that Linux guys use. It was designed for calculating large prime numbers. What it does is completely hammer processors.
04:36:55.668 - 04:37:28.693, Speaker A: It doesn't touch ram, it doesn't touch anything else, but it'll destroy the processor. So I stood up a system, made the BIOS changes, deployed the operating system, installed Prime 95, ran it, and my first try worked. So I took a system that was designed to run at 2.6 GHz would usually run up to closer to 2.8, and was easily hitting 3 GHz, which was our target number. So we took those systems, took those BIOS changes, rolled it out to the entire fleet, and gave them out to the community. And what we found was they didn't see the same results that we did.
04:37:28.693 - 04:38:12.401, Speaker A: And my key takeaway from that was that workloads really, really matter, right? So in theory, by hammering the processor, we should see kind of a similar result. But what we found is that prime 95 is very well multi threaded. If you put it on a system with 128 cores, it would hit every single one of them at its full capacity. Solana, by comparison, for all intents and purposes, is more single threaded. And so what ends up happening is a single core consumes most of the power, while the others sit idle. What we know about thermal design pressure or power, is that whenever we're using less power, it ups the clock speed. So I lucked out in the sense that the validators weren't seeing the same results that I'm seeing, but they were actually seeing much better results.
04:38:12.401 - 04:38:49.843, Speaker A: So dodged a bullet. But the reality was that I knew that I needed a better way to test. And so I went out and started doing research into what are the best ways to actually kind of benchmark Solana or hardware. I went through and kind of identified three potential options. The first was simply just run it in testnet. The theory was that it's free money, you don't have to worry about the actual economics of it working, and it should have what I would think to be similar to real world traffic. The reality is that the traffic is not real or similar to the real world, and so it just doesn't function the same.
04:38:49.843 - 04:39:34.430, Speaker A: The next thing I considered was just running it in Mainnet. That solves the traffic problem, but the other thing that it introduces is there's significant financial costs of actually running the server. And the other main concern with running in mainnet is that the traffic isn't consistent. So I know that I could hit the server hard, see the same results that end users are seeing, but I couldn't do it consistently. And I couldn't say that the changes and results that I'm seeing were necessarily directly the results of the work that I was doing, as opposed to just it happening to be a busier or slower time on net. And then the third thing that I discovered was the Solana benchmark utility, which is free, it's open source, and it's really designed to do this. It's definitely not the perfect solution, but it runs the same every single time.
04:39:34.430 - 04:40:25.044, Speaker A: And a lot of the main concerns can actually be offset with a better process. So I went ahead and used this launch benchmark utility. My process with this was essentially just do a fresh reinstall every single run. I let it run for 2 hours, so the system would become completely heat soaked, so it would kind of stabilize and then make a minimum of five runs, so that every time there was a weird result, we could kind of average that out and make sure that we were getting more consistent. And that was really when I kind of found the power of this repeatable process. So at that time, I took this position within Synflect, really, to kind of focus on the work that I was doing up until that point was with a single provider on a single series of processors. If I could go a little bit farther up the chain, I could start looking holistically at the Solana network and benchmarking.
04:40:25.044 - 04:41:22.158, Speaker A: And so I did. And when I got here, what I really found was that the first thing that stuck out to me, I should say, is that a large majority of the Solana network runs on AMD currently. And that felt really odd to me because I knew that intel had recently invested tens of millions of dollars into developing their ice lake generation processors with the sole purpose of trying to break into the web3 and cryptography world. And so it felt really odd that this major company would invest a ton of money and build this processor, and then it would not be reflected within kind of the makeup of the network. So I kind of felt like, why? And so I took this process, and I got my hands on some ice lakes, and I started benchmarking, marking, and it wasn't even surprising. What I found was that these systems were outperforming even the latest generation AMD by sometimes 25% to 30%. But that ends up bringing up an even weirder question, is then, if these are better and they're the same price, why aren't people using them? And what I found was maybe a little bit more discouraging.
04:41:22.158 - 04:42:04.054, Speaker A: It's just a byproduct of how people kind of get started in this, right? If you're new to validating, you get on discord, you get on Reddit, and you just try to find things. What ended up happening in a lot of these cases was simply people were finding old posts that just said things like, AMD is better. And so they would go out and they would find an AMD processor, and they wouldn't test it. So that kind of brings me back to the original question, why do all this work? Right. For Solana to fully realize its potential, it needs to be decentralized. And I think we use this as a buzzword in the industry a lot, you know, talking about data and business structures. But the reality is much, much bigger than that.
04:42:04.054 - 04:42:46.610, Speaker A: We need to be decentralized from the standpoint of hardware providers, from telco providers, from geographic locations, because of, you know, hurricanes or tsunamis, there's geopolitical issues. Like, we really do need to push this all kind of across the entire spectrum. And having everything fundamentally pushed onto AMD can be problematic. On top of that, it also has to be performant. We can't just be decentralized. It has to actually do what we need it to do. And what makes that really odd to me is kind of a byproduct of decentralization, is that there's no way to enforce or tell somebody to do something.
04:42:46.610 - 04:43:24.410, Speaker A: Right. So if we say we need more make up to be intel, we can't just go buy more intel. There's no authority structure by which to do that. And so in this decentralized world, what ends up happening is that information becomes a market force, right? So by doing this work, by benchmarking, by having these conversations, we can actually put that information out there and make a compelling argument as to why people would want to do these things, and it ends up helping the entire ecosystem. That's all I had to talk about. If you guys want to catch up with me, we actually have a booth in the back, so please feel free to come chat. I know this is really high level.
04:43:24.410 - 04:44:06.710, Speaker A: I'd love to get more down into the weeds and talk if you have questions about specific processors or processes. So thank you. Thank you so much. Really appreciate that. It's amazing to have partners like that present and people that you actually know and you work with every day. And inflect is definitely one of the most, technically, sort of have the highest technical expertise of the partners that we work with. Just in terms of how do you configure servers for Solana validators? So it's great to hear from them.
04:44:06.710 - 04:44:39.624, Speaker A: Them. And now we've got our last session of the day. So I'm so pleased to introduce Ahmad from. Excuse me, Ahmad from Syndica, who's going to be discussing a really, really important topic that we deal with all the time at the foundation, which is developer tooling, developer infrastructure. So please give it up for Ahmaud. Hey, good afternoon, guys. Can you guys hear me? Fine.
04:44:39.624 - 04:45:11.684, Speaker A: All right, cool. I'm Ahmad. I am the co founder CEO here at Syndica. We provide web3 developer tools and infrastructure for the Solana ecosystem. So I'm here to talk a little bit about Syndica and what we do for the Solana ecosystem and how you guys can use it. So what we're really building is the cloud of web3. We started off initially with just RPC infrastructure, but really quickly started to go into other areas of the development stack.
04:45:11.684 - 04:46:08.556, Speaker A: We're backed by some of the best venture capital out there, which provide us immense amount of resources and network to continue to build in this ecosystem. Today, we're used by some of the biggest projects in the Solana ecosystem, whether that's zero one exchange, we have play to earn games like Star Atlas, Stepn, Mango markets, bridge split, and so much more. So this is going to go a little bit into how we kind of got here. When we first started syndica, we knew that the RPC layer was really, really difficult in the Solana ecosystem. So we started first with best in class hardware. We knew that in order for our infrastructure to scale the way we want to, the only way we could do that was really to have the best hardware out there. So we kind of sourced the different chips in this Solana ecosystem through the server program as well as through our own vendors.
04:46:08.556 - 04:46:53.444, Speaker A: We benchmarked all the different chips out there, and we kind of came to a conclusion of what ships are best for RPC nodes. Then we geolocated all these nodes strategically across the globe to make sure that all the customers of ours can easily access Solana RPC, no matter what location they're in. And then we kind of built out a really interesting architecture called elastic node. And the idea really behind elastic node is that you're provided a single endpoint. With that single endpoint, you can easily scale your Dapp infinitely. You don't have to wait, worry about one node or ten nodes, or five nodes. It's really, you just have that single endpoint and you know that your DaP will work and kind of scale infinitely.
04:46:53.444 - 04:47:45.128, Speaker A: Aside from that, we've done some really cool stuff around intelligent caching. I think one of the hardest things to do in any kind of blockchain is to cache data properly, because the nature of blockchain is such that you need some of the most readily available data in order to make sure your transactions succeed. We had to do a lot of work to understand what data can we actually cache, which ones can we not cache? And build out an API gateway that is intelligent enough to be able to decipher those different things. And then one of the biggest pain points in the Solana ecosystem, especially when I first came into it, was the websockets. The websocket support was pretty poor. We did some really cool stuff around websocket multiplexing. The idea is that you connect with our syndicate infrastructure, and then we multiplex your websocket connection to many nodes.
04:47:45.128 - 04:48:24.962, Speaker A: And so that way you have high availability websockets. So yeah, all this kind of really was done by a bespoke API gateway, so we kind of had to build that from the ground up. Most other providers in this space will use kind of the traditional load balancers, like haproxy Nginx, which I think is really good for restful kind of applications. But anything that's using JSON RPC, you kind of need a really specific type of API gateway to handle those loads. And so we guarantee 99.9% uptime with our elastic node architecture. We have a very fault tolerant network.
04:48:24.962 - 04:49:02.860, Speaker A: This kind of describes how dapps make RPC calls to our infrastructure. They call our API gateway. The API gateway essentially takes the data from the JSON RPC payload. It populates the analytics so you can use that downstream for your own intelligence, and then it load balance that request to many RPC nodes out there. And our customers love us. So these are some of the messages that our customers have sent us, letting us know how happy they are with Syndica and using Syndica thus far. So we have had 99.99%
04:49:02.860 - 04:49:39.548, Speaker A: uptime since launch. So our infra uptime is best in class. To give you an understanding, that's roughly less than, I think, an hour and some minutes of downtime. So we've done, I think, a phenomenal job of making sure the RPC infrastructure is always up to date and ready to go go. And we have 99.999% RPC call success rate, and so our dapps rarely ever occur any kind of issues with the RPC calls themselves. And so this is kind of a screenshot of our platform.
04:49:39.548 - 04:50:20.744, Speaker A: So we offer true observability. What that really means is you get to dissect your RPC calls and gather and tell in a way that you really can't if you were to lose that data. And so when your customers make requests to our RPC infrastructure, we capture that data and we're able to then generate these really great analytics that you can use, one you can understand what geographies are using. Your dapp you can understand what are the top cities if you're trying to do in person events. If you're an NFT project that wants to understand where your users are, this is really, really helpful. You can understand what percentage is desktop versus mobile. And I think this stuff is super powerful, especially when you're trying to build a project from the ground up.
04:50:20.744 - 04:50:59.056, Speaker A: There's a term called guerrilla marketing, and in order to do guerrilla marketing, you really have to know where your customers are. And so the way we kind of structured our whole app and our infrastructure is this concept of stacks. So stacks essentially allow you to house multiple dapps multiple services, your front end, your back end. However, you need to structure your application within a stack, and it's kind of like synonymous with app stack, right? You have a stack for your application. We kind of took that and put that into our own infrastructure. And so under a stack, you have many different resources. You have elastic nodes, you have your dedicated nodes.
04:50:59.056 - 04:51:46.408, Speaker A: You can spin up a dedicated node within Syndica really easily. You have the ability to have different, different access tokens and logs and analytics, which I'll go into next. So within logs, I think this is one of the most powerful feature at Syndica. A lot of dapps, initially, when they launched in the Solana ecosystem, were operating in the blind. They weren't even sure if they were down. And so a lot of Dapps would come to us and say to us like, hey, we love best in class RPC infrastructure, but we'd love to have some visibility as to what's actually happening behind the scenes. Are these RPC calls failing? If they're failing, why are they failing? Can I inspect the request body? Can I dissect down and understand what's going on at the app layer? You can obviously get on chain data to tell you why the transactions are failing on chain.
04:51:46.408 - 04:52:34.116, Speaker A: But in order to understand from the DAP to the actual RPC node, you need to have some kind of observability, and we power that through our application. You also have really great filtering mechanisms, so you can filter by IP addresses, request origin countries, RPC ID by methods, whatever way you want to kind of dissect your data. It's really, really easy to do that. And we also have access tokens. So we know that many teams have many developers working on many projects simultaneously. So sometimes you may have a back end service that needs access to certain RPC methods and another service that needs access to other RPC methods. You don't want to be overcharged or overbuild you want to kind of limit access, you want to segment the access to your elastic node infrastructure, and you can do that really easily with access tokens.
04:52:34.116 - 04:53:33.262, Speaker A: And we provide that kind of out of the box. We also have that concept of expiring access tokens. So you know that your access tokens are never leaking and people are not kind of hijacking your access token using it, and you're incurring a bill essentially for that. And I think this is probably one of the most powerful pieces of syndica, which is analytics. I think being able to see really easily what RPC methods are working, which ones are slowing down your applications, which one are performing well to kind of fine tune the user experience. I think this really helps a lot to be able to see that data really, really easily and to be able to gather intelligence in a way that you really couldn't if you didn't have analytics platform like this. So to kind of sum it up, you know, I think Syndica really provides you with a great, great RPC infrastructure, but on top of that provides you with really great logs and analytics, which is the only way you can iterate on your product and make it better.
04:53:33.262 - 04:54:13.586, Speaker A: If you can't measure it, you cannot improve it. So that's kind of what we enable through our platform, and then we built really cool APIs. One of these APIs is called the Chainstream API. Today we are essentially wrapping the Solana validator geyser plugin and exposing it through a websocket connection. And so what you can do is you can stream block transaction and slot updates in real time from the validator into your own environment. And there's a couple reasons why this is super important. One is, is at the validator RPC level, you can gather a confirmed commitment level transaction from it, but you cannot get a process level commitment transaction.
04:54:13.586 - 04:54:57.414, Speaker A: Process basically means that this transaction has been processed by the validator, but yet is yet to be confirmed by the cluster. And this is important for a couple reasons. One, if you're trying to front run or if you're trying to snipe an NFT, if you're trying to execute a transaction that needs to be be prioritized, or are you trying to get the best execution speed or success rate on your actual orders, this is really important for you. You can get that information as quickly as possible, get your transaction ready to execute. The second that transaction is confirmed, then you can execute your transaction. So I think it's a really, really interesting API. We actually had Solskniper who's one of our customers, tell us that their limit order success rate has improved by three x, which I think is pretty phenomenal.
04:54:57.414 - 04:55:55.618, Speaker A: And so I want to leave some time to talk about another aspect of web3, development and tools and infrastructure. And so before I get into that, I want you guys to kind of use this opportunity right now. You can scan this QR code, you can use the promo code, breakpoint 22, and you essentially get dollar 250 in credits with Syndica. And so you can start testing us today, let us know if things are going good, if they're not going good, and then, yeah, just give us a spin and let us know how it goes. And I want to leave this up here while I talk about a couple other things. I think we have been in Asana ecosystem now since April of 2021. I officially didn't launch the company until September, but in April of 2021, you know, the salon ecosystem was very different than what it is today, and it's expanded, it's grown for the better.
04:55:55.618 - 04:56:30.184, Speaker A: And so many dapps are now entering the Solana ecosystem coming from other blockchains. And we obviously see there's a clear winner in terms of high performance blockchains in terms of adoption so far. And Solana is it. You know, I think as a web3 developer, you have to provide the best user experience you can. Otherwise, people will kind of go to the next competing product. And that's really obvious when you first think about it. At first glance, you're like, well, obviously, if I don't provide the best user experience, they're going to go to another product.
04:56:30.184 - 04:57:01.610, Speaker A: But I think it's more important in web3 because everybody has access to the protocol. Nothing is gated. Anybody can take, take that protocol and build a user experience on top of it. That may be better than the actual team that built the protocol. And that's the point of web3. The point is to iterate. The point is to provide the protocol to any developer out there, let them create whatever user experience they see best fit for the users, and let the users decide what actual user experience works out best for them.
04:57:01.610 - 04:57:32.082, Speaker A: And so a lot of times we see that in web two, everything is really gated. The only way that you can interact with a web two protocol is through an API. And those APIs are throttled. They're rate limited. There's some kind of permissions that the company has to give you. Again, in web3, all that doesn't exist. I think what's really going to matter at the end of the day is your ability as a developer to provide that amazing user experience, and then aside from that ability to market that user experience.
04:57:32.082 - 04:58:13.584, Speaker A: And so in order for you to market and build that great user experience, one, you need to have some kind of observability. You need to be able to know what is working in your dapp and what isn't. Where are my users coming from? How can I cater to them? And all this data is kind of lost at the RPC layer, and we make that really readily available through Syndica. The other piece of it, I think is really important from marketing perspective is you need to be able to measure your success. If you push out a feature, how many more transactions are your users sending? Not just the ones that are succeeding, but just sending in the first place to the actual cluster. And you can't really measure that unless you have some kind of analytics at the RPC layer. And so I think Syndica is really powerful in that regard.
04:58:13.584 - 04:58:41.454, Speaker A: It kind of provides you that, and it gives users that visibility into the Dapp, which really doesn't exist. And so, yeah, I'll end it off there. I think I wanted to dive into that a little bit. I know we have a few minutes. If anybody has any questions, I'm happy to answer it. Yeah, there's two mics right here if you guys want to come up and ask any questions. Don't be shy.
04:58:41.454 - 04:59:31.914, Speaker A: Hi, I have a question from your experience. What's, like, the simplest thing that adapt can do to kind of improve those RPC requests that you would recommend everybody just do at the start? I'm going to come down to hear that. Sorry. What is the most common mistake in terms of, like, optimizing RPC calls? Got it. Got it. Yeah. So the question was, what is the most common mistake that developers do to optimize RPC calls? I think there's, I don't know if you guys know about the get program account call, and so that essentially is really, really heavy on the RPC node when you make that call, it's basically getting all the program accounts under that one specific program.
04:59:31.914 - 05:00:08.470, Speaker A: And so using something like program derived accounts is a much better approach to fetch the specific accounts that you need. I think those are some of the ways that you can, can improve it. Also, I think there's a lot of improvement in the websocket space. So right now, you are connecting to a Websocket connection, subscribing to one account, and that account may not be updated for a very long period. So instead of using up that one Websocket connection, you can just kind of set up a polling mechanism and actually fetch the data every x amount of seconds or x amount of interval. And so I think things like that will help a lot. And the way you can do that, again, is you need to have some kind of observability.
05:00:08.470 - 05:00:59.850, Speaker A: I'm not just saying this because I want to sell syndicate to you guys, but I'm saying it because you can set that up today, right? If you know how to set up Grafana, if you know how to set up a time series database, you can put that data and capture it and then gather some kind of insights. And I think it's super important. So, yeah, that was a great question. Thank you. Hey, yeah, since I was looking also, and I was attending the one other presentation that was kind of related to the Google capabilities and their offering to the Solana itself, then how do you differentiate, for example, their bigquery offering to your monitoring challenges? Yeah. So the question is, how do we kind of compare to Google's bigtable instance, essentially. Right.
05:00:59.850 - 05:01:19.554, Speaker A: The bigquery. Bigquery, yeah. So I think it's really interesting what Google is doing in the Solana ecosystem. I'm really happy that they've joined. Right. It validates us in what we do because we have a big cloud provider like Google coming into the space and saying, hey, yeah, we believe in Solana and we know it's going to succeed long term. In terms of bigquery.
05:01:19.554 - 05:01:55.038, Speaker A: I think we're going to have to wait and see what they come out with today. We have the whole Solana blockchain history inside bigtable, which I think is a super, super important piece to make sure that we have that data accessible. At Syndica, we're working on probably the world's fastest indexer, and we're going to be releasing that here in the next month. And you'll be able to try that out and tell us how you feel about that. When Google comes out with their bigquery indexed version of the Solana history, we'll definitely be trying it out. But I think there needs to be more than just one solution. It shouldn't just be syndicate providing an indexer or Google providing an indexer.
05:01:55.038 - 05:03:00.286, Speaker A: There should be many, many RPC providers, many, many infrastructure providers in the Solana ecosystem that provide that to you, because that's the point of decentralization. We kind of need that layer. Otherwise it's centralized again and it's not really helpful for anybody. And so, yeah, to answer your question, I'm really excited about what Google's doing and how they're going to enter this space. Cool. Any other questions? Yeah, well, since I'm more into infrastructure development thingy and public clouds, knowing also the fact that there hasn't been any big announcement from other public cloud players like Amazon and Azure, Microsoft, what do you think? What are your, I don't know, feeling about them in the moment when they first have something also to offer? Would you find that better or worse? Because it is kind of big, you know, enterprise going into something that in the end of day might be more centralized for their own favor? You never know. Yeah, it's a good question.
05:03:00.286 - 05:03:40.152, Speaker A: So do I believe that other cloud providers, like Azure, GCP, AWS, getting into the space is good or bad for the ecosystem, plain and simple. I think the more players there are in the Solana ecosystem, the better. You know, I think everybody that can provide some kind of infrastructure is really helping the Solana ecosystem. We all know the pains of the developer ecosystem right now in Solana, there's a lot of improvement, there's a lot of ways we can optimize development. And I think cloud providers like AWS, GCP is organic, going to be super powerful. I'm super excited. So, yeah, 100%, I think it's a great thing, but it looks like we're right out of time.
05:03:40.152 - 05:03:58.188, Speaker A: I appreciate you guys listening in. Thank you for staying until the last talk. And yeah, enjoy the rest of your day, guys. Thanks. Awesome, great job. Thank you, Ahmad. And yeah, as Ahmad just said, that concludes the programming for today.
05:03:58.188 - 05:04:11.064, Speaker A: Today on the stage, please join us for a happy hour in the back. In just a few minutes there's going to be some refreshments and drinks and stuff back there, but otherwise, hope you had a great day and I'll see you tomorrow for day three. Thank you.
