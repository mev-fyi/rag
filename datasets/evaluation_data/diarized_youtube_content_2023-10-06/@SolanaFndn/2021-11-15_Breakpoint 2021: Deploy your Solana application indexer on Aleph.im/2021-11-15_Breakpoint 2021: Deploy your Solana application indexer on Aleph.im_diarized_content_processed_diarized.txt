00:00:18.160 - 00:00:42.722, Speaker A: So, hey, hello everyone. So I am Jonathan Shamul, aka Moshe Malawar on Twitter, telegram, et cetera. That's just a nickname. My real name is Jonathan. I'm the founder of Aleph Im. Today we'll speak on how to deploy your Solana indexer on our network. So first and foremost, thank you very much, Solana, for inviting us.
00:00:42.722 - 00:01:24.290, Speaker A: It's a really great conference and I'm really thrilled to be here. A bit about us so what's Aleph? I am it's a decentralized cloud computing platform. We provide storage, as in file storage and database storage and computing as in serverless computing. You can think of it as something like AWS Lambda or some similar things. So in a nutshell, it's like a decentralized AWS, Google Cloud, or whatever cloud platform you might see. Our network is not a blockchain. We don't have blocks, but we accept messages coming from blockchains.
00:01:24.290 - 00:02:16.534, Speaker A: A message from a blockchain is like a private key, gives you a public key, gives you an address. So if a message is signed by, like any of the accepted address chains, it is accepted by the network. We currently accept addresses from Ethereum, Polkadot, Substrate, Cosmos, Cosmos, SDK, Polygon, like all the EVM chains, and obviously Solana. Our network works with core channel nodes, which are the controllers of the network for which you need to have skin in the game to run one of these. And they are currently doing a part of the storage stuff. But in the near future, everything will go toward the resource nodes. The resource nodes are those that are actually providing resources, as in storage resources, computing resources, etcetera.
00:02:16.534 - 00:03:17.404, Speaker A: The computing resource node will go live sometime next month, and the storage resource node will go live in early 2022. So let's zoom in a bit on the computing resource node, because that's what is interesting to us. For the indexing part, we have two kinds of load balancing. One which is cloud load balancing, one which is virtual, where you ask the network to find which vm can answer your request, and then on demand, a micro virtual machine is started with your own root file system. Unlike your code inside, your code can access ipfs, all kinds of blockchain data, decentralized databases, our own database system, or external sources here. For the blockchains, obviously we're contacting Solana. And for the storage, we will also support other kind of storage in the near future, like Arweave, Filecoin, etcetera.
00:03:17.404 - 00:04:31.724, Speaker A: Now, about our subject today, so what is an indexer? Why would you need to index your data? In general, when you talk, when your dapp accesses a website, ok, you have a front end, your front end, we can host it on ipfs, whatever. But then you need to contact RPC servers, that's cool. But then if you have millions of users you start mirroring them. So an indexer allows you to have some kind of firewall between your users under RPC's because the state will be stored in in a database and you can also add stuff to your status. You could add prices from Coingecko, status from another smart contract, whatever. And most importantly you can have history like trade history, transaction history, price graphs, whatever. And then it replaces a centralized backend here in our case, because most of the DApps do that on a centralized server, here we are doing it in a decentralized way.
00:04:31.724 - 00:05:38.564, Speaker A: So how does our indexing work on Nf Im? So here we have zoomed in even a bit more inside our micro Vm. So when you start your applications it will index data from the blockchain. Like when you start it, it gets the current state and then goes back in time to get history. But when you launch it instantly it will get the current state and then go back for historical data to store all this inside the node local file storage which is persisted between multiple launches of the same node and then it will get the data out through graphql. We can also have a snapshot system to restart from a previous point if we want to get faster. Let's say your application has a spike in demand. The network will spawn multiple instances through the whole network for you.
00:05:38.564 - 00:06:25.584, Speaker A: So to do a quick demo, we didn't release yet our full open source framework for indexing for Solana. So so what we will do today is to make a very simple indexer, self contained indexer. All the source code will be made available on our Twitter. Later we will send a link. So we create a simple counterapp which has a value that we can increment or decrement in a Dapp or for a program, and we log the state as a log inside the program so the history is kept and we don't have to look at all the account state all over the time. So let's whoops. So here is the code of our very simple program.
00:06:25.584 - 00:07:38.734, Speaker A: We are using anchor, obviously. So we have free function, we create our counter, we increment our counter and we decrement our counter. The very important part here is the log that we are adding on this log is quite interesting because we will parse it later. I know that anchor is working on some kind of serialization for the logs. So in the future you will be able to use the standardized serialization. So now how do we build our indexer? When you create an indexer with our framework, here itself contains, so like everything is there, but normally you will only have boilerplate code. You make a parser for the instruction, you explain when there is a new request that comes, how do I know what function has been called and what are the arguments, et cetera.
00:07:38.734 - 00:08:20.184, Speaker A: We need to decode the data part of the transaction. Then we define the program id, we define the logic, and we prepare the graphql output. So let's have a look a bit more in detail to that. So in the domain, this is where we process the transaction. So once we have a transaction that has been parsed, we will look into it and we will parse what we had before, which was split it by an equal number. So we split by it, we set a counter and we return a state. Our state is counter timestamp.
00:08:20.184 - 00:09:25.174, Speaker A: And in the process, whenever we receive a new transaction, we are verifying that is the timestamp superior to the previous one that we have? If yes, we set the current state. So whoops, parsers programs here is how we define the parser for our program. We have a program name. We have the instruction code, which is the start of the data of the transaction that would then map to create increment decrement. We have the instruction data layout, which has the authority for the create and nothing for the increment and decrement because we have no arguments. And then we explain what are the accounts that are called here and there. Again, if you are using anchor, all these can be made way simpler, anchor on the client side.
00:09:25.174 - 00:10:23.452, Speaker A: Then we set a schema where we explain what are the constructors, et cetera, that we have. We have only one data type here, which is the pulse state, which is a counter on the timestamp. And inside our schema we say what we have. So we have two types of call in graphql, one which is pulse, and one which is pulse history, which is the actual history on the data. Then we have the resolvers that get you all the data out here. For pulse it will just get you the state, and then for the history you have a limit reverse on some arguments, and then it gets inside the whole database and get you for each transaction your data out. So let's try this.
00:10:23.452 - 00:11:13.388, Speaker A: So I have already, I have it already running here. I have a little script that will write on the smart contract and do increments and decrements and let's have a look. So here in my graphql I do two queries, one which is one which is pulse history as we have seen. So this one gets us the current state and the other gets us like the history, and we can see all the history of the latest pulse if I rerun it. It is changing in real time. We did a very simple UI that is showing the current state at the moment. So all this is in local, it works, but it's pretty centralized so far.
00:11:13.388 - 00:12:01.844, Speaker A: Now, I won't upload it on the Aleppo IAM cloud right now, because with the Wi Fi uploading a whole node program, I'm a bit aware of the demo effect, but I uploaded it just before I came here. So it's a very complicated URL. Alep sh or Alep cloud. It's two kind of domain, two different load balancer that gets you toward your micro vm. And we will also support in the near future custom domain names as well, so that your application on your world back end can run inside the cloud on demand. It will spawn up your micro VN with your code, which is the code that we have seen just now. And it works.
00:12:01.844 - 00:13:35.300, Speaker A: This indexer is now stored on the left AM network. So this indexer is decentralized right now. So a bit then about the whoops. So as is, it feels like it's a bit complicated or whatever, but it's really simple because you will just write your queries, you get a simple result out, you can get arguments, etcetera, and then you can do things that are much more complicated than that. And we have been working on a few other indexers, like for example radium dot in Fo, which is an indexer for the radium AMm. The good part with that is that you can see the world trading history, you can see all the swaps, add of liquidity, removal of the liquidity, and in the same way all this data is available as GraphQL and then we can do queries here to get all the data out and all the data is open and available to the general public. So if you are doing on am, if you are querying data from Solana, we will likely already have some kind of data endpoint available for you.
00:13:35.300 - 00:14:22.864, Speaker A: So I've shown radium, we also have orca dashboard, sabre dashboard like this one. The one that we are pretty proud of is the serum markets dashboard. And if I don't have a demo effect because of the Wi Fi, okay, so you can see all the data, so all this data is open. If you want, you can get all this data out. This one is available. This one is actually open as well. In the same way, for all this theorem data, you can get all the tokens, all the transfer.
00:14:22.864 - 00:14:59.634, Speaker A: You can get, oh, lcv data for the pairs on serum, even if they are permissioned or not, etcetera. So that's for the indexers. We are also working on indexers for a lot of other projects. Currently, we have a partnership with port Finance, with whom we are building an indexer on many more up and coming that will be announced in the next week and months. So thank you very much. I will perhaps I still have a bit of time, so I will show you a little video that explains how the network works.
00:15:01.614 - 00:16:18.066, Speaker B: The Internet is broken, flows through centralized networks controlled by just a handful of companies. But there's a change coming, a monumental shift, and at Aleph IM, we're supercharging the transition to a new, decentralized era. An era that provides fully decentralized cloud storage, computing and identity services powered by incentivized self hosted servers built from the ground up. Our unique architecture blends both off chain and on chain technologies to support an ever growing constellation of blockchains and companies. We've built the tools that allow you to store any file of any size on our proven decentralized network. Protect your precious nfts securely on our network for eternity with our pioneering backup dapp. Unleash the power of our decentralized virtual machines, allowing you to run dapps in any language that can also read and write to blockchains, or be triggered by often on chain events.
00:16:18.066 - 00:16:27.894, Speaker B: Be empowered to store, build, and launch incredible things in this new era of decentralization. Join us at Aleph Im.
00:16:33.714 - 00:17:05.770, Speaker A: Thank you so thank you very much for your attention. We also have a dev hub that is available. We have a lot of documentation. We have tutorials coming up for the micro virtual machine as well. And you can also find us on Twitter and telegram. And we will release all the code that has been shown before as open source in the coming hours. We will put it on Twitter.
00:17:05.770 - 00:17:06.954, Speaker A: So thank you very much everyone.
