00:00:03.680 - 00:00:40.182, Speaker A: Thank you for being here. I see a lot of friends and very friendly faces in the audience. The topic RPC 2.0 I found, especially over the last few days, in having conversations with other people here at breakpoint, that phrase means different things to different people. And it turns out that I picked the perfect panel for this discussion. And so, briefly from my left. Yeah, my left all the way down to the end, Max is here from mango markets.
00:00:40.182 - 00:01:34.584, Speaker A: And mango markets has been working on an RPC 2.0 spec, mostly on high performance for Defi and for traders. So it's interesting because it's a different point of view than maybe some of the other folks have when we think about RPC 2.0. Noah is here because he likes to rant and he's had some pretty good Twitter rants and some are very much on topic about RPC 2.0. He brings a different perspective, which is more of a front end dev, somebody who's trying to think more creatively about the applications that he's building and what he needs from the infrastructure layer. And then finally Nick from Helios, he's the guy who actually does stuff at Helios. And so he's got an excellent background of building up systems at large scale.
00:01:34.584 - 00:02:07.934, Speaker A: And then, so he can help give us some extra balance and blending of can you actually build this thing? Right. And so with that, the conversation that we're going to have is going to be extremely free flowing. We're going to start with a couple questions. Questions just to start the conversation, get things going. And once we go off on a tangent, I'm just going to let it roll. So what I have then I did bring some notes because I've got Noah's rant.
00:02:10.514 - 00:02:12.858, Speaker B: Yeah, I can't reproduce that. It comes from the heart.
00:02:12.986 - 00:03:01.234, Speaker A: It does come from the heart. Right. And so we're just going to take the tweet thread one at a time and just have some fun with it, that we're all here to have some fun. So tweet number one, dapps shouldn't need to run their own Geyser webhooks and centralized Web 2.0 infra. What happened to the dream of writing a decentralized UI that anybody can deploy? It's a good start to a rant, isn't it? I do feel, though that we should probably just kind of do some level setting of definitions and things like that. Max, maybe you could start out, talk a little bit about what is Geyser, and then we can all just kind of have the basic context and then get more advanced from there.
00:03:01.974 - 00:03:47.488, Speaker C: Yeah. Geyser was this, I think, the big cure to the initial issues with get program accounts. Remember, in 2021, everyone was having issues. We had to have a lot of nodes, RPC nodes in particular, to run these queries. They can take up to 3 seconds, can lock out the node completely, make it fall behind, and then the solution was, well, what if we could take every write to an account and stream it out of the validator into some other system and then run get program accounts on that cache of the original data, and that system was first called, I think, accountsDB plugin and then renamed to Geyser.
00:03:47.536 - 00:03:47.960, Speaker A: Yep.
00:03:48.072 - 00:04:03.344, Speaker C: Yeah. And then over time and has evolved, and there's more things you can get now. You can get the blocks you can get, I think, also the transactions itself inside the block, and probably a few more things I'm not aware of, but that's the rough.
00:04:03.424 - 00:04:08.244, Speaker A: Great. Awesome. Webhooks, Mister Webhook.
00:04:10.544 - 00:05:04.166, Speaker D: So yeah, webhooks. I mean, I can talk about two things because I think first off, there's the. So webhooks are a concept in software engineering, basically, where when data needs to be sent, like, it's an event driven architecture that where we push the data to a consumer, say via like an HTTP post. There's also web sockets, which I think deserve a bit of a to be brought up as well, which is another mean of another means of communication where you can push data. And the Solana API has some native websocket methods, but one that's kind of recently been added through powered actually indirectly by Geyser, is also the ability to do transaction websockets. So you could do either of those really when we're going back to the original tweet and the talk about Das, which I think I'm sure we'll loop back to in a second.
00:05:04.310 - 00:05:14.196, Speaker A: Yeah, for sure. Yeah, we'll definitely loop back to the DOS API and how complex that is. Noah, anything to add from that initial tweet there?
00:05:14.380 - 00:05:55.396, Speaker B: Yeah, I mean, I think so. I came to web3 from data infrastructure, so my job was all kind of deploying all this web two infrastructure, tons of pipelines. And it was beautiful when I came to web3 because it was like, wait, I can just go and I can just write a front end. And it's not just using my stuff, it's using Bonfeed as nave service, all these other things. And it was very composable, and anybody could just go fork it and they could deploy it. I was really, I loved that idea. And then Solana grew and the accounts grew, and suddenly things like get program accounts just stopped working.
00:05:55.396 - 00:06:15.988, Speaker B: And then Geyser came out, and it was great because it was like, now we actually can solve these problems. But I have now found myself very squarely back into the same hole that I had crawled out of from data infrastructure, which is beating data pipelines with a wrench. Again, I had gotten out of being a plumber, and now I'm back.
00:06:16.076 - 00:06:57.304, Speaker A: Yep. Yeah, yeah. I distinctly remember those tipping points from the very beginning when the account db was relatively small. And then all of a sudden, serum is starting to take off, and the number of accounts that serum had really started to explode, and performance just went straight down. And it was a beast to the point that we actually had to disable. We would not index serum for most servers unless it was a trader that actually needed serum. And then soon thereafter comes candy machine, and the exact same thing.
00:06:57.304 - 00:07:15.714, Speaker A: A standard setup would be, okay, we're not indexing serum, we're not indexing candy machine, and if you need that, it's special handling. So, yeah, it's pretty crazy how that happened. Let's see. Let's go to the next tweet. Bytes. Bytes everywhere.
00:07:16.414 - 00:07:17.318, Speaker B: It's like a poem.
00:07:17.406 - 00:07:49.614, Speaker A: Do any ETH devs care about encoding schemes? Probably not. 99% of Solana projects are using anchor with idls, and that defines the structs. Why bytes? Can one of you guys be comfortable maybe playing devil's advocate here? Why would it be better? Or why do we have the bytes everywhere in the first place? Chesterton's fence. Right? A fence exists for a reason. Why is it there? Why are we dealing with bytes? So do you guys want to.
00:07:49.994 - 00:07:52.746, Speaker D: We're talking about in the context of GPA or.
00:07:52.930 - 00:08:04.090, Speaker A: Yeah, just the idea that there's no advanced parsing layers happening with RPC right now. Now. And that the results you get back is basically just base 64 or whatever.
00:08:04.202 - 00:08:29.960, Speaker D: I think it ties back to what we were just talking about with the size of the accounts by having a byte structure. So the way that you query, you're querying by a byte offset. So you need to know the schema, but then it means that the actual way it works is now schema less. It's easier to operate at scale. And it's really. The answer is it's easier, I think. So what's happened is it's basically pushing the problem down to the consumer.
00:08:29.960 - 00:08:43.744, Speaker D: And I think that's why people like Noah are frustrated, because they don't want to deal with that, but it works for now. From that perspective, from our perspective, the RBC provider's perspective, even then, it doesn't really work that great, actually. So it still hurts.
00:08:43.904 - 00:09:24.150, Speaker C: I mean, if you ever developed an EVM, it's just like simulators, too slow, because on EVM, you just send a simulation of a contract call and get your parse data back according to the contract API. And no one, I think first there were no return codes for instructions. That was the first thing that had to be resolved. Now there are return codes, but no one's using them. But also, if you were to simulate an instruction or transaction that returns something, it would just be completely slow. No one uses simulation at large scale. And I think that's right.
00:09:24.150 - 00:09:25.726, Speaker C: That's a problem of RPC.
00:09:25.830 - 00:09:58.784, Speaker A: Yeah, yeah, for sure. Yeah. Traders don't use simulations. You lose too many milliseconds, you lose the ARB. Then Jari Zhao had kind of, he had replied, he said, I agreed with the sentiment, but he thought that the RPC interface should always stay as raw as possible, which means bytes, and it gives you a more universal interface. But let's dig deeper into that then. So what would you add into the base RPC layer? How would we build that?
00:09:58.944 - 00:10:37.498, Speaker B: Yeah, I think this is always interesting, especially when you talk to the people that are kind of around the core Dev of Solana, because they very much want to focus on the hard problems and they want to stay out of user land as much as possible. Like, that's your problem. It's all pushed downhill. But my start in development way, way, way back when was kind of in ruby on rails land, and the beautiful thing about rails was it actually took opinions on things. People were like, no, we're not going to be opinionated about everything. But rails was like, actually, we are going to be opinionated about everything. We're going to take sane defaults that you can kind of break out of.
00:10:37.498 - 00:11:08.186, Speaker B: And because of that, it was just an absolute joy to work with. It was very great to work with. Solana is very much on the other end of that, where it's not because they haven't taken an opinion on everything, but in a weird sense, it kind of has. Like, most people use anchor. Most people are using abortion coding, for better or for worse. Most people don't publish their idls, but they should. And if they did, you could parse all of these accounts and you can actually see this in the Solana Explorer.
00:11:08.186 - 00:11:49.726, Speaker B: If you go look at helium smart contracts, it's actually very easy to figure out what's happening with them because the IDL is there. But we have people in the helium community that have never touched Solana before and they're trying to get our accounts back and stuff, especially if they're not working in JavaScript. How do I parse this? What is a discriminator? What is this encoding? All of this is just crufts that people really shouldn't have to worry about. So I don't know exactly how you implement this at RPC layer. Maybe there's a layer on top of the RPC. But if we actually start taking some opinions on things, we can improve the developer experience and we can get rid of things like get program accounts. Like get program accounts.
00:11:49.726 - 00:12:10.376, Speaker B: What if you add a variable length encoding? So now you have a vector. Well, sorry, if you put anything after that vector, which, sorry, if you didn't know you were going to add a field to your account after the vector, sorry, you just can't filter on it, you're done, you're cooked. I think if we get a little more opinionated we can make these things better, right?
00:12:10.560 - 00:12:37.264, Speaker A: Yeah. And in rails they call that convention over configuration and just do it this way and it will work. And there's probably more than a few rail staffs or former rail staffs. Yep, there you go. I am too. I totally believe in convention over configuration. So maybe there's, maybe there's an opportunity to try and somehow get more of that spirit, maybe even enforce it a little.
00:12:37.264 - 00:12:41.064, Speaker A: Nick, any thoughts on that?
00:12:42.204 - 00:13:31.898, Speaker D: I think I agree with the sentiment, but it's kind of like it's where do you draw the line on where you separate the boundary between the platform layer? Or if you make an analogy to like the network stack, you have, you know, the various, various layers. So like if you like rails very much is like almost like a toolkit, it's a framework, right? So we're talking about, yeah, sure, you have an opinion in framework, but should you have an opinion in RPC, should you have an. I'm not too sure, I'm not convinced. I think probably what you want is you want a richer anchor framework, you want stuff sitting on top of all this that gives you all that stuff. And then for every normal developer that wants to be build an application, you're just using that and it says, okay, here's how you do it. No, we'll never let you add a variable, a field after a variable array. It just won't let you.
00:13:31.898 - 00:13:43.814, Speaker D: If you choose to do that. And if you want to go hard mode and go raw, then yeah, you can do some stuff if you want, but most people won't. And that would, I think, behave very similar to rails and give the same experience.
00:13:44.274 - 00:13:58.690, Speaker A: Okay, Noah, do you think that would be an optional layer that, let's say Max and his team, they want to go hardcore and they're going to go for the bytes and just don't want any of this other stuff.
00:13:58.882 - 00:14:00.930, Speaker C: The problem is when you fork anchor.
00:14:01.042 - 00:14:01.986, Speaker A: Go ahead, go for it.
00:14:02.050 - 00:14:09.014, Speaker C: Yeah. When you start forking anchor and adding variable length fields and stuff. Support for variable length fields in your fork.
00:14:09.394 - 00:14:09.706, Speaker D: Yeah.
00:14:09.730 - 00:14:11.202, Speaker B: Where you get zero copy.
00:14:11.338 - 00:14:13.314, Speaker C: Yeah. And you use zero copy, both.
00:14:13.434 - 00:14:15.810, Speaker B: Yeah. It gets real dicey real quick.
00:14:15.922 - 00:14:31.998, Speaker C: It's very, very interesting code, I think, but I didn't understand it. But I also wouldn't like everyone need to support that. Right. We have all these dynamic fields in the mango account structure, and that's very specific parsing code that we use there.
00:14:32.166 - 00:14:33.254, Speaker B: I think that's fine, right?
00:14:33.294 - 00:14:40.062, Speaker C: Yeah, most people wouldn't need that. But if you use the IDL, you're going to parse like 20% of the count and the rest is garbage because.
00:14:40.158 - 00:15:16.020, Speaker B: Yeah, and that's fine. Right? Convention over configuration rail is very much like people who develop Audig will tell you, as soon as you start straying away from the happy path, it starts fighting you. You're like, damn, this framework got hands. And that's fine too. Solana can start fighting you. In fact, I actually already developed this way on anchor. When I find myself in a situation where I need to start using zero copy or have these giant accounts, usually I end up switching the design around such that I just have more accounts or different accounts.
00:15:16.020 - 00:15:50.246, Speaker B: Usually you get into the situation when you're storing large arrays of data on accounts, and I find that that's because Solana acts like a NoSQL database where point queries are best. You're actually better off just adding more pdas to the situation. So I still think we can absolutely get away with it. And actually, I was listening to the explorer's talk earlier and our Armani was there. And it's funny, he actually put in the original anchor IDL. You could put an index into the macro of account. You could say, index this account, and it would put it in the IDL that you wanted this account indexed.
00:15:50.246 - 00:16:23.720, Speaker B: And they eventually removed that. But that was the spirit. He knew when he designed it, and he's very, very good at understanding what makes things easier for devs and cutting out that cruft. And he knew, he was like, yeah, this is what we need we need a clean indexed version. And actually there are startups on Solana working on this problem, but then you have to worry about vendor lock in and what happens if that startup dies and things like that. So there's something very beautiful about. We talk about not doing this and we want to stay raw and we want to stay in the bytes.
00:16:23.720 - 00:16:43.004, Speaker B: But the digital asset API dos is beautiful. It is a work of art and it runs on all the major rpcs and it's a pleasure to use and it is very opinionated, but it's great. And I'm just saying, like, just do that for all the accounts, do it for everything. Make my life easier.
00:16:44.224 - 00:16:55.192, Speaker A: Yep. If you're building for a particular program, though, for helium, do you actually care about all the other programs or do you really just need to focus in on your own?
00:16:55.368 - 00:17:35.398, Speaker B: It depends if you compose with them, right? Like, if you compose with switchboards, you maybe care about the switchboard accounts. The big example is all of our hotspots are nfts and so they need to compose with bubblegum. And we actually attach PDA's on top of these nfts so that we can put on chain metadata on there. But then this is actually a problem that we end up putting our plumber hat on and actually running indexers for. Which is like, what if I want to get all of the, all the NFTs in a given wallet with their associated on chain metadata? That's now a join from my program to somebody else's program.
00:17:35.486 - 00:17:36.314, Speaker A: Right, right.
00:17:36.894 - 00:17:45.230, Speaker B: And you can. Yeah, you can say with these things. Well, sure, you're helium. You're huge. Of course you've gotten into these horrible, complex use cases. That's your problem. And that's fine.
00:17:45.230 - 00:17:59.544, Speaker B: Maybe that's the answer that we go with. But I think more and more you're going to see these indie dev teams that just. They want to build a UI and have it deployed anywhere. And it's so much easier for them if they don't have to do the DevOps work if we've got this stuff figured out.
00:17:59.844 - 00:18:07.984, Speaker A: Yep, yep. Let's see. I'll let Nick respond to this one. We're going to go on to the next tweet.
00:18:09.084 - 00:18:11.628, Speaker B: Oh God, this one.
00:18:11.756 - 00:18:17.252, Speaker A: I'll use an angry voice. I'm not sure if that's what you intended, but it might be good drama.
00:18:17.308 - 00:18:18.976, Speaker B: I hadn't had my coffee, so.
00:18:19.160 - 00:18:19.752, Speaker A: Oh, okay.
00:18:19.768 - 00:18:20.404, Speaker B: A little.
00:18:20.864 - 00:18:39.724, Speaker A: You may be a little hangry. Yeah. If you just decoded accounts on a program with a well defined IDL and provide a nice API over these accounts, you'd remove all GPA use cases and most custom indexer use cases. Nick, how would you deploy that? How would you build that?
00:18:40.424 - 00:19:01.374, Speaker D: Well, I think the first clarification I want to ask, and maybe, I don't know if that's your thoughts on this has changed, is are you looking to have indices on the properties of your schema? Essentially. So like your data model, actually application specific indices that you can query that's very different from the current GPA.
00:19:01.714 - 00:19:24.254, Speaker B: Yeah. Let's say you're building a governance module and you have proposals on it, and those proposals have names. Like if you just want to do a simple query like you would do in postgres, like an ilike, I want proposals that have a name like this. There is literally no way to do that on Solana right now. Not possible on Solana npos.
00:19:26.474 - 00:19:43.094, Speaker C: Yeah, I mean, searchability, right? Just being able to index a field, a text field, and search in it is like a common feature in front end right now. Everyone just downloads all the possible accounts they want to search through. And then there's a client side. Right.
00:19:43.214 - 00:19:45.994, Speaker B: Yeah. Why is my phone bill?
00:19:47.494 - 00:20:26.344, Speaker C: Exactly. I think if you want to load realms, it's something like 16 megabytes of data from vote. So you cannot really vote on 3G, you need at least 4g. But yeah, I agree that there is this. Having a proper querying API is extremely difficult to specify before you know, all the use cases. I think I remember the first time I saw the MongoDB basically query API. I was very impressed that these guys managed to take something that was a standard called SQL.
00:20:26.344 - 00:20:52.000, Speaker C: Everyone was using it and they were bold enough to bring their own, and it was. I think there are some use cases that are really hard to express. Right. Once you go out of the declarative space. And so I try to have a similar, very reduced query API for the event spec that I wrote. And you just get into a lot of edge cases that you cannot express. Right.
00:20:52.000 - 00:21:28.804, Speaker C: Like some things, I don't know, give some queries. We really like looked at it and we're like, okay, can we do this query? Can we do that query? Can we do that one? All these things that are currently running on custom SQL caches, basically. And I would like that we can get a discourse going and come up with something that makes universal sense. But I'm not sure what even the starting point for that would be. Do we want to take something like MongoDB security language? Do we want to roll our own? Do we want to support full SQL.
00:21:29.744 - 00:21:59.640, Speaker B: I mean, what we've done for helium is we just literally take Helios webhooks, decode the accounts and insert them straight into a postgres table. It actually works disgustingly well. There's a startup called vibe that actually they did this and they presented like a Graphql API on top of it. And it was fantastic, actually. I don't know where they stopped. It was probably just like they weren't able to do the business development to get enough teams to use it. Or I've also been wrong many times before.
00:21:59.640 - 00:22:06.920, Speaker B: It could be I'm just wrong. And this is the way I'm designing things. I need this occasionally, but other people don't. I'm not sure.
00:22:07.112 - 00:22:40.740, Speaker A: Yeah, there's one danger. Kind of my hot take on indexing in particular, like Armani allowing for an index index, this attribute statement. My hot take is this. Most devs don't know how to build a proper database index. And then we end up with a situation where we've got people pushing that into production, but they don't realize how much damage they just caused on the back end. And they don't have to pay the cost of that. Somebody else does.
00:22:40.740 - 00:23:02.044, Speaker A: They point at Nick or they point at me and say, you suck or I suck. You've been there before. I know everybody blames the RPC and so that's going to be hard to solve. Is somebody in the middle to say, wait a minute, that's a really bad way to do this?
00:23:03.224 - 00:23:40.546, Speaker B: I think what we're missing is, and this is maybe a little bit crazy, but crypto, right? It's all about incentives. And the incentive model there is completely messed up because I can push crap downhill and just completely overload the RPC. But what if we somehow made it so that reads also had a cost? And if you have some horrible, horrible thing that you're indexing that is expensive to read, you're paying for that. That would be cool. I think there's like a, I think the graph on ethereum does something like this, but I haven't dug too deep into it.
00:23:40.730 - 00:23:42.174, Speaker A: Yeah, I haven't either.
00:23:42.674 - 00:25:18.954, Speaker C: Infura has credits. I mean, I'm sure you can buy them with metamask somehow, but like classical web, two credits, right? And yeah, you need to optimize your, you need to start using all these weird, I don't know. Multicall is a very popular one. So when I build like oracle infrastructure or things like that, where we need to keep ethereum endpoints over RPC, I'm always also surprised by how much inside knowledge you need, how experienced you need to be to do very simple things like I just want to know the price of rapid coin versus ETH on uniswap v three and I would like for us to kind of foresee those things a little bit and from a standard point come up with something that is actually both deployable but then also generic enough for people to use. I'm very like once you go into this question of indexing, for me the biggest cost is at least indexing what? Because in theory you can run a query without any special purpose index, but it might be very slow. That's basically what we have right now with GPA and filter that just runs in linear time over accounts DB and tries to find all accounts that match the description. If you want o log n lookup for instance, or a faster search query with a premade index, you need to aggregate that data.
00:25:18.954 - 00:26:35.224, Speaker C: You need to store that index data somewhere, which usually is roughly the same size as the data you're searching on, right? And so the more of those search indices you have, the more data you actually need to aggregate, sort through and store. And so I think two RPC providers might not have the same index. So this then brings up the for me, that makes sense in some way, right? To say, well, my application runs everywhere, if I run it on a public RPC, it will probably be horribly slow or will be rate limited and will not even work at all. There might be too many errors or I work with a custom indexing provider, then it has the same API. At least I think we utterly failed if they will be different. So we should make sure it has the same API, but then also that we have different offerings, the target to the most common use cases and we have recipes for people to set it up. I think 99% of the developers want to have for the application some form of indexing on some form of NFT collection.
00:26:35.224 - 00:27:46.654, Speaker C: Like usually they want to have all the accounts owned by a certain program and they want to have all the token accounts of the respective wallet or user that's interacting with their program or never interacted with their program. And those are the most common curies. And we know the token program, the token owner lookup is actually already in the validator as a special purpose index, right? So there is a strong indication that this is something most people want to have. Once we have a way to easily set up the same index in RPC tool outside the validator, we should be able to remove to that from the code make the validator faster, which is the part I'm so excited about there, actually. Right? We want to get it at the same time, make it more feature complete so that people can actually, I remember we had this discussion, we were like, hey, should we just fork the validator and put another index in for gpas on the mega program? Like, why don't we do that? And I remember I proposed this to Solana, why don't we add a command line flag, or like the user index? And people were like, oh my God, no, probably, yeah, yeah, yeah, yeah.
00:27:46.954 - 00:28:04.186, Speaker B: I like it though. Like, that's great. Like you define an API, it's going to be the same API. And actually this already happens with GPA. If you like, hit the public RPC with an unindexed GPA, it's going to suck. But then you go to Triton and they can index it for you. But it's the same API, so you still have the dream.
00:28:04.186 - 00:28:40.344, Speaker B: You can still deploy the code once and everywhere. And it also works really, really well for small startups. If you have a couple hundred users, your thing doesn't need to be indexed, it can be a range query. But if it has an API that's querying on well structured data on a well structured field, and not just bytes, that's good. If you're doing crazy, crazy stuff like you probably eventually have, like, yeah, you're going to have to run your own geyser or webhooks and index it, but we can probably solve for 99% of these use cases.
00:28:41.084 - 00:29:16.804, Speaker A: Yeah, yeah, I think so. I think so. I want to give Nick a chance to jump in here and let me do this. So I'll read you some comments I had from Steve Lusher earlier today. He's emceeing over at a different stage, so I asked him ahead of time if he had any comments for this particular panel. And so the thing that he said that he's most passionate about is what RPC will look like at the outer edge. And I said, joe from Labs has written a graphql schema to the network to answer the question.
00:29:16.804 - 00:29:39.384, Speaker A: Let's see. Oh yeah, that I had asked the question, what would the ideal API look like? He thinks that's graphql and that they actually wrote that to ride on top of the existing JSON RPC API. So I don't know if any comments of that or.
00:29:40.444 - 00:29:41.652, Speaker D: Yeah, I keep getting riff.
00:29:41.668 - 00:29:42.732, Speaker A: However you want to write, I keep.
00:29:42.748 - 00:29:44.428, Speaker D: Getting the ones I don't agree with. Really.
00:29:44.556 - 00:29:45.804, Speaker A: All right, great, good.
00:29:45.884 - 00:30:16.124, Speaker D: Yeah, there's going to be the contrarian here. I'm not a huge fan of graphql to begin with. I get what people like it, especially front end devs. And for people who don't know Stephen Lucher, he's the author of web3 j's. So it's a typescript library. GraphQl, the way it works is that you can basically get a bunch of different data in one spot with one kind of query, but it allows people to do some pretty crazy stuff. If you're thinking RPCs with graphql against my server, it's like, oof, no, that's scary.
00:30:16.124 - 00:30:48.394, Speaker D: You could nest stuff, you can nest data and it's going to result in insane queries. I think I agree with the principle that people are going to want that. And the way to do it though is it's about having a way to pull the data into some sort of indexed state where then you can actually have optimized graphql queries running against that instead. So it's like, how do we build a tooling to let people actually construct that and hold it there instead of having graphql converting into the existing RPC queries? And then that would just be a disaster.
00:30:50.314 - 00:30:51.094, Speaker A: Nice.
00:30:52.554 - 00:31:00.818, Speaker B: Someone wants to have it be an intermediary layer where you hit the graphQL and then it forms a GPA and.
00:31:00.986 - 00:31:03.130, Speaker D: Oh, that's what I'm concerned about.
00:31:03.162 - 00:31:04.586, Speaker B: Yeah, yeah.
00:31:04.610 - 00:31:11.618, Speaker D: That's terrifying. Exactly. It's like one call making thousands of GPA calls.
00:31:11.666 - 00:31:18.414, Speaker B: Yeah, yeah. What I would love is like if it was actually well indexed data. So you could go and make the graph.
00:31:18.494 - 00:31:41.244, Speaker D: Exactly. So we've talked a lot about how do we build these indexes? And it's like we talked about, say, Triton has their, you can, oh, index this field for me. So if we're talking about structured data, then GPA already acts like a filter. So you could have structured data with the filter. That's pretty easy to do. Generally, that's really just your anchor idl. And then you have, then you have the structures you can filter, and then if you want something optimized, then you.
00:31:41.244 - 00:32:20.402, Speaker D: Yeah, you have an index. And that I think works. But then if you want to go even more complicated, it's about how do we set up common tooling to pull the data somewhere else and then build a query of, say a graphql API? And that also talks about things like GRP, streaming tools, or other sort of ways. We can kind of have that sort of common interface to stream it out. And then maybe it's also, how do you run that locally as a developer? How do you run it yourself in a smaller mode? It shouldn't have to always depend on Helios or Triton. If you wanted to use that graphql API, it's only us two that offer it. Well, then now you kind of have some form of vendor lock in, which I guess is somewhat good for me, I suppose, but long term it isn't.
00:32:20.402 - 00:32:23.734, Speaker D: So we have to think about aligning the incentives long term.
00:32:24.474 - 00:32:51.598, Speaker A: Yeah. Let's think for a moment about going the other way too. So we're talking about doing things at scale and trying to get performance at scale. But if we bring this down to local host and Dev is just working in a local host environment, is this stuff going to be easy to set up and run it on your laptop and expect that it's just going to work once you push it to production?
00:32:51.766 - 00:32:53.114, Speaker B: Didn't happen with us.
00:32:53.814 - 00:32:54.554, Speaker A: No.
00:32:57.614 - 00:33:31.876, Speaker D: I think it could be. Yeah. If you're running, say, even if you're running your own validator or say you're doing something on local net, you could have a sqlite database and as long as there's some sort of open source tooling to go and it'd be the same core packages, that's the thing. It's just about what is it pushing it to? If it's just pushing it over localhost to your own sqlite database, then that should behave in principle the same way. And the only ways that's going to behave differently is this. Problems of scale. But if you don't want to deal with that, then you throw that at us and we'll take care of that.
00:33:31.876 - 00:33:39.264, Speaker D: That's really our job. Like Helios and Trident is just to deal with the painful scale stuff so you don't have to. But locally it should absolutely work. Yeah.
00:33:40.724 - 00:33:47.020, Speaker C: I think there's very early Internet times where we had this thing called the lamp stack.
00:33:47.132 - 00:33:49.024, Speaker A: Oh yeah, I remember that. Yeah.
00:33:49.324 - 00:34:19.324, Speaker C: I think something like that very basic package is enough. I mean, if people want scaffolding and setting up index or configurations when they create a new account, type an anchor and that sounds like a rail style kind of development workflow maybe that we want to lay on top. But for. Yeah, for the beginning, I think in the testing phase and initial development phase, we can probably run with a lamp style, right?
00:34:20.404 - 00:34:40.184, Speaker A: Yeah. I mean, there really does feel like it needs to be a requirement no matter what. Right. No matter what we do with RPC 2.0 that we have to be able to run it on localhost and then know that the modules are going to behave the same once we throw it up into production. Even if it's running SQL lite locally or postgres or whatever, it's still going to behave the same.
00:34:40.484 - 00:34:47.344, Speaker B: Yeah, there's really like three stages of development, at least for me. I know my brother just pushes straight to Mainnet, but for me.
00:34:50.553 - 00:34:52.601, Speaker A: Yeah, he's the guy we're afraid of.
00:34:52.697 - 00:35:22.236, Speaker B: Yeah, exactly. But for me, I start, I develop things on localhost, I run tests against it, and things need to work in localhost. And then there's a stage after that you're running it on Devnet. And man, so many developer tooling companies just forget about Devnet. Why aren't you just using Mainnet for it? And then you finally go to Mainnet. But this mirrors somebody working on a startup. They're going to start localhost just messing around with it.
00:35:22.236 - 00:35:46.944, Speaker B: Maybe they haven't even raised funds yet. So obviously they're not going to be paying an expensive RPC to index this stuff. But it doesn't matter. They've got a few tens of records and then you get the Devnet or even maybe Mainnet, but they don't have that many users. So they can use the public RPC. And then finally when they've proven they have a little bit of PMF, then they can pay for the RPC to have the indexing. But as long as the API stayed the same the whole time, it's perfect.
00:35:47.064 - 00:35:49.164, Speaker A: Yeah, yeah, yeah, for sure.
00:35:49.624 - 00:36:12.484, Speaker C: Because also I remember we started there was like the public one, then there was project serum. Like you could ask to get access to it with your dapp. And I think they were running an absurd amount of RPC nodes. And then at some point you would outscale that as well. And then I remember contact with Marco or something. He was running in his basement here in Amsterdam somewhere.
00:36:15.544 - 00:36:27.174, Speaker A: In Rotterdam. And he, he has a. And this is cool, by the way. Talk about Max nerd. There he is right there. There's Marco. He's got a data center in his house with a one gigabit fiber connection.
00:36:27.174 - 00:36:53.614, Speaker A: And he also d dosed himself once. I remember in the early days of Solana and his kids were pissed because they couldn't game. Yeah, yeah, that was, that was definitely crazy having to deal with the way that, that scaled and the issues there. Let's see, we've got. Boy, only a few minutes left. This has gone quickly. So let's jump ahead a little bit.
00:36:53.614 - 00:37:11.954, Speaker A: Going back to Nick. Let's start to talk a little bit about Dos API and account compression indexing in general. Could you give people a sense of how difficult that was to bring that up and how difficult it is to run it?
00:37:12.654 - 00:37:56.614, Speaker D: Yeah, it's pretty difficult. And I think like, yeah, there's a lot of issues with it, but we're kind of. You're adding an extra layer of complexity over the same panes you've already talked about as soon as you go into compression. Because in compression what you're doing is that when instead of just indexing the current state of accounts, you can never recover that by saying, what do accounts look like? You have to go back, traverse the whole ledger and reconstruct your current state. And then if you have problems, you have to find a way to look back and be like, it's not easy to figure out what's missing, especially when with compression, everything's all hashed and by nature hashes are not feasibly reversible. So yeah, it's quite challenging. And to add to that too, when you're talking about Das, you're actually going one layer up the stack even further.
00:37:56.614 - 00:38:25.902, Speaker D: So we have compression in there, but now we're also talking about almost like an application or your user space, you could say, because you're talking about now like nfts, right, and how they behave and actually metadata as well. So like collections, craters, all the rules around that. So all that's all tied together and all of it can break each other. So I personally don't love that model. I think that it's the same sort of things we're talking about here. It's about just building out the right layers on top of each other. So if you make a.
00:38:25.902 - 00:38:48.394, Speaker D: I would like to see. And there's work that's going to happen here. I'm basically making compression just easily easier to use. In general, the core components just handled for you. And then that way when you're in your user space, you can just build at the user space and not have to worry about what's a Merkle tree and how do I deal with that, and how do I recover my data when I've missed x transaction? You shouldn't have to worry about those things.
00:38:50.174 - 00:39:24.464, Speaker A: Yeah, and like you said, to be able to bring that back down to localhost too, and then know that, that you can run locally of a version of DoS API that's only for just that limited test data set that you're working on definitely hard. We've got less than a minute left. Maybe we'll do just real quick, boy, what do we want to do? Just super quick thoughts on how long is this going to take to build? What do you think? A year? More than a year. Really quick. We've only got 30 seconds.
00:39:26.564 - 00:39:43.744, Speaker C: I mean, all the extra API fluff on top, I don't think there's any timeline on that. We're happy to hear your input. For the basics that I'm looking at right now, I think it's maybe six to twelve months for an initial rewrite of RPC.
00:39:44.524 - 00:39:45.292, Speaker A: Okay.
00:39:45.428 - 00:40:05.036, Speaker B: No, I think it depends on how much discussion you want to do. If you want to do proposals and proper discussions and community forums, it's going to take five years. If you want to shove like five devs into a basement and force them to code it, maybe you could do it in six months, but it probably wouldn't be the best API. You got to get trade offs.
00:40:05.100 - 00:40:07.052, Speaker A: Sounds like the Solana ethos there.
00:40:07.228 - 00:40:12.504, Speaker B: Yeah, Solana really does love the shove some beds in a basement.
00:40:14.394 - 00:40:17.338, Speaker A: Nick, final word. We're out of time, but I'll give you the final word.
00:40:17.426 - 00:40:33.658, Speaker D: I think pretty similar. I think maybe six months. But again, it's going to be a bunch of phases. So I think we just have to break it up into phases and knock one at a time. And I think all those phases what we're really envisioning. Yeah, we're probably looking more at the past one year mark to have the dream talking about.
00:40:33.746 - 00:40:37.434, Speaker A: Nice. What do you think, guys? Good job. Thank you.
