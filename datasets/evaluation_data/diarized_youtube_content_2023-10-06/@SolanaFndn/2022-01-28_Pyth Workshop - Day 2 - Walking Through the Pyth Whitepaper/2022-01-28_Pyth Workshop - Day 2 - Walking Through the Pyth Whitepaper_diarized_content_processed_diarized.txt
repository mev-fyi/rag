00:00:09.160 - 00:00:47.576, Speaker A: In this talk, I'm going to be talking a little bit about the designs and mechanisms in our white paper. We released our white paper yesterday. So if you're interested in learning more about this, I encourage you to check that out. And, you know, the goal of this white paper is really to make PIP into a sort of self sustaining and decentralized protocol. Going forward and to understand why we're proposing this white paper, I think it helps to start from an understanding of where we are with PIP today. And so I have this kind of simplified diagram that shows the architecture of PIP today. So right now we have a great network of publishers, and they have estimates of some price.
00:00:47.576 - 00:01:26.180, Speaker A: Let's say this whole thing represents the bitcoin us dollar price feed. So they have an estimate of the price, and they are running a publisher which is constantly submitting Solana transactions, which is publishing those prices and making them available on chain. And then the on chain program aggregates those prices into a single price and confidence interval, where that data can then be read by consumers, which are downstream defi applications or what have you. So this is kind of just a sketch of what Pip does today, right? We publish prices. They're available on chain, people can read them. And, you know, this is being used by a lot of applications. That's not to say that this doesn't work, but there are some issues with this.
00:01:26.180 - 00:02:04.492, Speaker A: The first problem that I'll point out here is that this system has no way for publishers to monetize their data. Now, this is a problem because the publishers are actually paying to submit these transactions, right? So there's a cost required to run the system. So in the long run, this is unsustainable unless there is some way for publishers to monetize their data. So we need to figure out how to do that in order to make the system work. Now, I'll say that monetizing the data is a little bit challenging, just due to the nature of blockchain. Once the data is published on chain, it's publicly readable. There's no way to have, for example, a subscription model or something like that, which you might have would say, Bloomberg.
00:02:04.492 - 00:02:52.460, Speaker A: In the real world, that business model is off the table. So we need to think of something more creative in order to solve this problem. A second problem that we want to think about, and we actually get a lot of questions about, is from publishers asking, how do I know that I'm providing good data? And if we take a step back, that might be like, we really want to attract the highest quality data providers to PIp, we want to attract the best exchanges, market makers and all of that. And so we really want to come up with a way to preferentially reward these high quality data providers so that they are motivated to come and provide prices to our network. So this is another problem that we want to solve. And then finally, whenever you're building in crypto, you have to be concerned with like, adversarial behavior. You know, we can't assume that everybody who's participating in the protocol is an honest actor.
00:02:52.460 - 00:04:08.718, Speaker A: And so we just need to be careful to make sure that there aren't any, you know, exploits or things like that where people can come in and break the protocol by behaving in ways that we don't expect. So these are kinds of the three challenges that we're going to try and face in this white paper, and we're going to try and design around. And just to give you an overview of the mechanisms in the white paper, we're going to start with the same data feed mechanism that we had before, where people are publishing prices and they're getting aggregated and consumers are reading them. And our kind of critical idea is that we're introducing a new mechanism which we're calling data staking on top of this, which enables consumers to optionally pay a fee for their data feeds. And in exchange for that fee, they essentially get a hedge or added security, which is saying that if the data feed is ever wrong, then they will get a payout, a monetary payout in PiP tokens. So this is like, we think this is an interesting service because if you're a crypto protocol or something like that, it's a high value application and you really want to be safe and secure and you want to make sure that your users funds are protected. And so, um, this mechanism kind of gives you an added level of security that you wouldn't have with just an oracle by itself.
00:04:08.718 - 00:05:03.514, Speaker A: Um, so this is kind of our, our key new idea, and this is how we're going to monetize the data. So the way this mechanism, um, well, I'll talk a little bit more about how the mechanism works in a second. Um, and then now this mechanism, you know, has a way for consumers to pay fees and those fees get sent to delegators, but it doesn't actually solve the problem of monetization because right now nothing is being sent to publishers. So what we're going to do is we're going to add a third mechanism here, which I'm calling reward distribution, which is going to take a share of the fees earned in this data staking mechanism and share them amongst the publishers. So this provides the means for publishers to monetize their data and this mechanism is going to do some quality scoring and things like that to preferentially reward the highest quality data providers. So this is how we're going to solve that second challenge of just, you know, incentivizing high quality data providers. And then, you know, I said adversarial attacks.
00:05:03.514 - 00:06:03.080, Speaker A: We'll kind of think about those attacks as we go forward. But some things you might want to think about are, for example, consumers could, you know, try to pay the data fees and then like try to trigger a payout, you know, even if the oracle feed is accurate. Or for example, you know, maybe in the long run we have a big network of publishers and some of the publishers try to join the system and they don't have any pricing information, but they want to try and earn a share of the rewards. So these are like some of the kinds of attack vectors that we want to think about when we're designing. So what I'm going to do in this talk is I'm just going to kind of go through the three major mechanisms here and then I'll talk a little about governance, which isn't shown here, but it kind of controls all the parameters of the protocol. And what I'll try to do is I will stop after each segment and maybe take some questions and then we can kind of continue onward. So the first mechanism I want to talk about is actually maybe the oldest mechanism that we have, which is just aggregation.
00:06:03.080 - 00:06:52.606, Speaker A: And so this mechanism is responsible for taking the prices contributed by publishers, the prices and confidence intervals, sorry. And combining them into a single aggregate price and confidence. And, um, this mechanism is, you know, we already have a version of this mechanism, but I want to talk about it because I do think its properties are important for the security of the protocol as a whole. So, um, in thinking about this mechanism, there are a couple things that we want this aggregation algorithm to have. Um, and the way I'm going to talk about these properties is I'm going to show you some graphics that kind of depict some scenarios that you might want to consider when you're building aggregation. And the way I'm depicting these scenarios is using these, I don't know exactly what you call these plots, but basically each color represents a publisher and the circle represents their price. And the width of the confidence interval, or the width of the bar here represents the confidence interval.
00:06:52.606 - 00:07:32.484, Speaker A: So in this scenario you have four publishers publishing a price at 110, and you have one publisher at 80. So the first property that you would want in this algorithm is you want it to be robust. And what I mean by that is that you don't want a single publisher or a small group of publishers to be able to move the aggregate price by themselves. Because if they could do that, then publishers could join the system, and then maybe they take out a futures contract or something on the other side on the price, and then they can try to move the price for their own benefit. This is kind of like the Libor scandal in pit the world. So we kind of want to prevent that behavior from happening. And that's what this robustness property guarantees.
00:07:32.484 - 00:08:03.564, Speaker A: Oh, I want to say one more thing about aggregation, which is throughout this section, I'm going to talk about the publishers as if all publishers are equal in the long run. And you'll see this in a second. When we get into data staking, the publishers are actually going to have different amounts of stake. So the publishers will actually be stake weighted in all of these things. It's just a little bit hard to depict that in these graphics. So just assume for now that everybody has equal stake, and everything will translate to the stake weighted case as well. Okay, so we want this robustness property.
00:08:03.564 - 00:08:50.834, Speaker A: The second property that we want is confidence weighting. So the reason for this is that our publishers are actually different, and they have access to, like, different levels of information about the price of any given product. So, for example, some of our publishers are exchanges, and exchanges have different levels of liquidity, and the less liquid exchanges tend to have wider bid offer spreads. Right? So if you have a wider bid offer spread, you have a less precise estimate of the price, and that gets represented in pit as a wider confidence interval. So what that means is you can end up in situations like this, where you have two publishers that have a really wide confidence interval, and then you have some publishers that have a really small confidence interval. But it's consistent with the wider confidence interval. And in these kinds of situations, our intuition is that we want to trust the publishers with the tighter confidence intervals more.
00:08:50.834 - 00:09:14.984, Speaker A: Right. They have more information about the price as sort of our intuition. So in this kind of scenario, what we want is for the aggregate price to be somewhere here and maybe not here. Okay. So this is the second property that we want. The third property that we want is just, we need to set confidence intervals on the aggregate price. And there are a couple different cases that you might want to consider for how these confidence intervals should work.
00:09:14.984 - 00:09:54.514, Speaker A: The first case is what happens if there's a bunch of publishers who are publishing approximately the same price at approximately the same confidence interval. Now, in this setting, if you did sort of a standard statistical approach, what would happen is your confidence interval on the aggregate would kind of narrow as you got more publishers, right, your variance sort of shrinks as you add more things to your average. But we actually think that's the wrong thing to do, because typically what this scenario represents in our case is that there's a bunch of exchanges, for example, that have the exact same bid offer spread. Do we know that the price is the midpoint? No. Right. We just know it's somewhere in that range. So in these situations, we don't want to average down the confidence.
00:09:54.514 - 00:10:39.892, Speaker A: The second scenario that you might want to consider is what happens if our publishers are publishing just widely different prices. So this scenario I've depicted here, where they're like, at 80 to 110, this is probably never going to happen in real life. But there are situations where the prices across exchanges can diverge, because maybe the layer one has congestion and you can't arbitrage or something like that. So we need to think about what's the right thing to do in this situation. And we think the right thing to do is to publish an aggregate price that's somewhere in the middle, but a wide confidence interval to basically reflect the fact that there's a lot of dispersion in the price. So this is kind of the whole selling point of the confidence interval, if you think about it. It's that we can model situations like this that you can't really model with just a price.
00:10:39.892 - 00:11:13.210, Speaker A: Um, so these are just two properties of the confidence interval that you might want to think about when we're, uh, trying to design this algorithm. So, um, we thought about this problem a lot, way more than I care to admit, because it seems like a very simple problem. Um, but we came up with an algorithm, at the end of the day that I'm happy with, and it's a really simple algorithm, so I'm not even gonna write it down. I'm just gonna tell you what it is. So the algorithm is this. For every publisher, we're going to give them three votes. We're going to give them a vote at their price, their price plus their confidence interval, and their price minus their confidence interval.
00:11:13.210 - 00:11:51.624, Speaker A: We're going to take the median of all those votes, and that's going to be the aggregate price. Then for the aggregate confidence interval, we're essentially going to use the 25th to 75th percentile. The problem with that is, it's not symmetric. So we're just going to make it symmetric around the median by taking the maximum of the distances so that it's a symmetric confidence interval. So that's our algorithm. Now you probably hear that algorithm and you're like ok, I get how it works, but why is that a good idea? Seems kind of random. So I'll tell you why this is a good idea, but to understand the motivation here, I'll have to take a step back and compare it to some other approaches that you might consider.
00:11:51.624 - 00:12:37.264, Speaker A: So if I gave you this problem to, I don't know, an undergrad, they would say take the average, take the mean of all the prices. Now we can't actually take the mean because the mean is not robust. If one person publishes a price of infinity, the average is infinity. Right? So that's not going to work. But let's pretend we're going to use the mean and let's think about what that does. So one way to understand the mean is that we're going to take the data points in our sample and we're going to draw a little parabola around each data points, and then we're going to add up all these parabolas and the minimum of that summed parabola is the mean, right? The mean is the point that minimizes the sum squared error of your sample. So um, we can kind of visualize that as like, you know, minimizing this parabola here.
00:12:37.264 - 00:13:17.704, Speaker A: But there's a bunch of them, one for each point. Um, okay, so another way we could solve this problem is we could take the median. And it turns out that the median has uh, analogous interpretation where instead of putting a parabola around each point, you put an absolute value function around each point, and then you add them up, you take the minimum, that's the median. So what our algorithm actually does is it finds the minimum of a similar kind of objective function, except the function looks like this. And so it's similar to the median, except it's got this hinge in it. And the location of this hinge is actually based on the confidence interval. So if you have a narrow confidence interval, the hinge is over here.
00:13:17.704 - 00:13:47.784, Speaker A: If you have a wider confidence interval, the hinge is over here. Um, and so this is basically an adjustment to the median objective function that accounts for the confidence interval. In addition, um, it might not be obvious that the thing I told you minimizes a subjective function, but it does, trust me. I can tell you how to prove it later if you want. Um, so this is kind of the intuition. And hopefully by, by looking, thinking about it this way, you can understand that it's not just some like ad hoc thing. There's actually like a deep reason that it works.
00:13:47.784 - 00:14:16.892, Speaker A: And just to confirm that it works, I'll show you the results on our four scenarios from before. So we had this robust case where we said we didn't want to be distracted by that point, and we kind of get the right answer here. We wanted to be biased towards these points, which we do. Sorry. This top bold red star is the aggregate here. You can see that we keep the width of the sort of confidence intervals down there. And here we kind of represent that price dispersion.
00:14:16.892 - 00:14:54.532, Speaker A: So you can kind of see that those scenarios that we wanted to capture are captured well by this objective. Okay, that's the end of the aggregation part of the talk. Maybe I'll take questions on aggregation before I continue. Yeah. So the question is, why should the confidence interval be symmetrical? Um, I don't think theoretically we care that it's symmetric, but there's a couple reasons. One is that getting people to adopt even the symmetric confidence interval has been hard. So, like, if we now have to tell them that it's asymmetric, it's going to be even harder.
00:14:54.532 - 00:15:34.112, Speaker A: Um, and second, that's also what we've been doing. So just like in terms of having something that's compatible with the existing interfaces, it was easier to do it that way. Yeah. Um, yes. Yeah, that's a good question. So the question is, how should I use the confidence interval in my application? And I think the way that I would use it is I would think about the price as a probability distribution. And then I would say, like, I would only do things if it was very likely that the price, you know, justified it.
00:15:34.112 - 00:16:01.870, Speaker A: Right. So if you think the price is like 50,000 plus or -1000 and then someone could get, I don't know, liquidated at 49,000, maybe you don't liquidate them. Right. But if it's like 100,000 plus or -1000 then it's fine, right? So I do some check like that. Yes, yes. We currently require at least three publishers before we do the aggregation. Good question.
00:16:01.870 - 00:16:37.164, Speaker A: Yeah. Yeah. It's because, it's because these guys here basically drag up the confidence interval. Right. I don't know if that's like ideal in this situation. There's kind of some trade offs. Like, we did a whole bunch of case analysis and like, this was sort of the best we could come up with, but I kind of agree with you that my intuition would say that in this case, you do want the confidence interval to be narrower.
00:16:37.164 - 00:16:56.164, Speaker A: Yeah. And actually, if you look at it, if you look at what would happen, the 25th percentile is actually here. And it's that symmetrization that's causing us a problem, because this is like the 75th. Right. And that's like, so if we did an asymmetric one, it would fix it, but like. Yeah, so, good question. Thank you.
00:16:56.164 - 00:17:53.532, Speaker A: Yes, the publishers, that's actually going to factor in later in the talk. Also a good question. Okay, I'll keep going. So, for the next part of the talk, I want to talk about this data staking mechanism, which is kind of our new monetization thing that we're building on top of pipe. Um, this is kind of our, this is kind of our, we're calling this mechanism data staking. And this is kind of the most complicated mechanism. And I think the best way to understand this is to think about first just the mechanics of the actions that people can do, and then to think about the dynamics, like, what are the incentives, and what do we expect to actually happen if people, you know, play the game to like, sort of optimize their profits or whatever? Um, so the mechanics of this system are as follows.
00:17:53.532 - 00:18:25.918, Speaker A: For every single product, we're gonna have two pools, we're gonna have a stake pool, and we're gonna have a fee pool. So there's, this is gonna depict like, that's depicted here for like, say, bTC USD, but there'll be a separate stake pool for ETH USD and a separate fee pool for ETH USD. Right. Everything works on a per product basis. And the way this mechanism works is that for a publisher to publish prices for that product, they will have to stake a minimum quantity of pit tokens. And that's depicted here in these stake weights. So these p one, p two, p three, those are the publishers.
00:18:25.918 - 00:18:57.804, Speaker A: And the first component is like their stake. And it's going to be some fixed amount of stake per product. Okay. And then we're going to have this mechanism, the staking mechanism, where delegators are going to be able to essentially stand behind the accuracy of the data feed. So the way that they do that is they stake pit tokens and they delegate those tokens to publishers. So that's kind of depicted here with these arrows where the delegators are staking. And then some of these tokens that are kind of backing the publishers come from the delegates.
00:18:57.804 - 00:19:54.254, Speaker A: And so what this means at the end of this process is you're going to have some stake pool and that's going to have a total number of tokens that are staked for that particular product and some of those tokens will be allocated to each particular publisher to give them a weight. Now on the other side of this mechanism we have the consumers, and the consumers, they're the people consuming the data feeds, but this is an optional thing that they can do where they can essentially choose to pay fees into this fee pool. And oh, I need to tell you something before I tell you this, which is this whole protocol is going to run in epochs. So everything's going to run in discrete time where an epoch is one week. So the way to think about it is there's a fixed amount of stake for that week and then next week there'll be a different amount of stake, right. If you change your stake this week, it only takes effect next week and same for the fees. So if you paid your fees this week, it only takes effect next week.
00:19:54.254 - 00:20:38.264, Speaker A: So these consumers will be able to pay these data fees and you can pay any amount that you want. And we can permit payment in a variety of tokens, like maybe USDC or PIP. We can have these approved by governance, but you basically pay the fees in some token. And what will happen is if you pay the fees, you will essentially obtain a payout if in the next, say four epochs, the Oracle publishes an inaccurate price and you're going to ask how do we know if it publishes an inaccurate price? And we're going to define a process that detects that, which is this claims process. And I'm going to talk about that later. Okay. For now, just assume we can detect if there's an inaccurate price.
00:20:38.264 - 00:21:16.834, Speaker A: So the consumers are going to get a payout if this process decides that there's an accurate price. Right. And the payout that they get is going to be basically we're going to take the total stake that's been staked and delegated, we're going to slash that from the delegators and the publishers and we're going to pay basically the proportional amount of the stake to the consumers. So let's say there's like 100 pip tokens staked and there's two consumers and they're both paying $1 each. Right. Then if there was a payout, each consumer would get 50 pip tickets. Does that make sense? It's just a proportional amount to the total fees that you're paying.
00:21:16.834 - 00:21:48.074, Speaker A: So those are the mechanics of the mechanism. So what are some properties of this? Right. Why is this a good way to do it. Well, I'm going to make a couple claims. The first one I'll say is it's a market for hedging against this oracle inaccuracy risk. And the way to think about that is to think about what this looks like from the perspective of the delegators. So from the perspective of the delegators, there's a fee pool, there are fees coming in and there's different fees coming in for different products.
00:21:48.074 - 00:22:28.184, Speaker A: And the delegators can essentially look at the products and decide where to stake their tokens. So they might look at BtCUsD and they might look at Ethusd and they might say, well, the fees on BTC USD are higher, right. But on the other hand, you know, by delegating their tokens, by staking and delegating their tokens, they're at risk of getting them slashed if the oracle is inaccurate. Right. So the delegators are kind of doing this risk reward trade off where they're looking at the fees that they could get by staking versus the risk of getting slashed. And it's going to be a competitive market. So people are essentially going to stake more tokens towards products that they feel are lower risk and higher reward and vice versa.
00:22:28.184 - 00:23:20.364, Speaker A: So the end result of this process is that you should get some sort of fair market price for these data fees. The ratio of fees you pay for the data and the amount of money you would get in a payout should be roughly proportional to the risk of the product actually having a problem. Right. The second property that I want to mention is that this mechanism kind of aligns the incentives of the publishers and the delegators. And the way we do this, the problem that you might see here is that the delegators are essentially staking tokens on the publishers. The publishers, if they're wrong, could cause the delegators tokens to be slashed. So this is why we require the publishers to stake some tokens as well, so that everybody's incentives are kind of aligned toward making sure that we have a robust price feed.
00:23:20.364 - 00:24:24.070, Speaker A: The third property, I'll say, which is maybe the most interesting, is that the delegators are actually going to improve the robustness of aggregation. And the way they're going to do this is through those stake weights that I showed you here. So when the delegators are actually staking and delegating to publishers, these weights, the resulting stakes of the publishers plus their delegated weights are what we're going to use in aggregation to weight the publishers. But the interesting thing about this mechanism is that the delegators are going to have their stakes slashed if the aggregate price feed is ever incorrect. So they're not optimizing for picking the best publisher, they're actually trying to optimize the distribution of weights such that the price feed as a whole has the lowest risk of failure. And the reason we did this is because this is actually a little bit of a hard problem to figure out. There's a lot of intangible factors in publishers like you can look back at their historical record, or maybe it's a big name and you trust them.
00:24:24.070 - 00:25:17.016, Speaker A: But on the other hand, having a diversity of publishers and having the stake distributed across many, many different parties actually makes it harder for any single person to break the price feed. So there's this kind of tension between delegating more stake to high reputation publishers versus diversifying across everybody. And so this mechanism kind of provides a game for the delegators where it's kind of their job to figure it out. So those are the kinds of the three properties of this data staking mechanism. I want to say one more thing, which is just, I want to talk a little bit about the claims process, which is what we're going to use to decide if the oracle price is inaccurate. So remember that there's a payout that happens if this claims process decides that the oracle has published an inaccurate price. And this is actually a little bit of a hard problem to solve.
00:25:17.016 - 00:25:53.894, Speaker A: In some sense, this is the same problem as building an oracle in the first place. Like if we knew what the price in the real world was, we wouldn't have to do any of this work. Right. But, you know, we need to do something here. So our, so our idea is, oh, and there's maybe two competing proposals, and neither of them are perfect for how to build this. So the first proposal would be something like, well, anyone can show up and say, hey, I think the price was wrong at this time for this product and we raise that. And Pip token holders look at that proposal, that claim, and they say, yes, I think this is correct, or no, I think it's false.
00:25:53.894 - 00:26:31.218, Speaker A: And this would be kind of how you do governance and all these kinds of things and all these mechanisms. So it's a very natural thing to think about. The problem with this proposal is that a lot of the people who hold PiP tokens are also the people who would get slashed potentially if a payout happened. So they're kind of financially motivated to reject all the claims. So that seems bad. On the other hand, we could just find some uninterested judges and ask them, like, what they think the answer is. And this would be great, except the problem is someone could try to bribe those judges, right? They have no interest one way or the other.
00:26:31.218 - 00:27:06.574, Speaker A: So you could show up and be like, hey, here's $10. Can you vote to approve my claim? And then now you're going to get a false payout when there shouldn't be one. So we're kind of trying to balance the tension between those two approaches. And basically what we've done is we've kind of combined them in a way that I think addresses kind of the limitations of both individually. And so the way we've combined them is going to work like this. We're going to produce a software package that anyone can download on the Internet that's going to, essentially, you run it and it's going to tell you whether or not your pith claim should be approved. So you're going to put in the product that you think was wrong.
00:27:06.574 - 00:27:39.900, Speaker A: You're going to put in the time when you think the problem happens, and it's going to give you a yes or no answer. And the way it's going to do this is we're going to use human protocol, which is kind of like a, you know, it's a way to recruit humans from around the world to, like, you know, look up data in the real world and post it on chain. It's kind of like a mechanical Turk that runs on chain, if you're familiar with mechanical Turk. And so we're going to use human protocol. Human protocol is going to go out. They're going to find a bunch of judges all around the world instantaneously. And those judges are going to collect, basically data, right, price data from reference exchanges and historical data from PIP.
00:27:39.900 - 00:28:30.490, Speaker A: Everything we need to know to compute on, to validate whether or not this claim should be approved. And human protocol will validate that data and post it on chain. And then our software package will download that data, analyze it, and decide whether or not the claim should be approved. There's kind of a question of what exactly that algorithm is. There's more details about that in the paper, but the intuition is that basically, if Pyth publishes a price and confidence interval such that all the reference exchange data is highly improbable, according to our published price and confidence interval, then the claims package will say, yes, this is a valid claim. So if it says yes, it's a valid claim. The way that the claim actually gets paid out is that you post the output of this software package on chain and then Pip token holders vote to ratify whether or not this claim should succeed.
00:28:30.490 - 00:29:39.060, Speaker A: And the reason we did it like this is because this serves as sort of like a final check to make sure that no one has tried to like manipulate the process. If the PiP token holders feel like someone has tried to bribe the judges, they can vote no. However, on the other hand, the output of the software package, like we designed this software package and the use of human protocol and all that to give a very accurate answer, like it's supposed to be as hard to manipulate as possible. So if someone runs the claims package and the output of the claims package says yes, and then the PIP token holders vote no, the output of the claims is really strong evidence that maybe this whole system doesn't work. So that's basically going to make it so that the PIP token holders, if they vote no, it's basically going to destroy all the value of the protocol because no one's going to pay anymore. They're basically going to have evidence that this is never going to pay out. So why am I paying the data fees in the first place? So it's kind of like a social pressure almost mechanism that like, well, social pressure that everybody can put on the pit token holders which will translate into financial pressure on the pit token.
00:29:39.060 - 00:30:09.536, Speaker A: Right. So anyway, hopefully that made sense. I don't know, that's the claims process. So that's the end of the data staking piece. I'll take questions on that. Yes. Yeah, yeah.
00:30:09.536 - 00:30:51.408, Speaker A: So the question here is how do the human protocol judges have access to the data and the ability to validate it? Yeah, so for access to the data we'll need some publicly accessible price feeds, like from exchanges. This is historical data though, so that is more accessible than like, you know, real time data, even like, you know, equity data is available delayed on 15 minutes or whatever. But that is a good question. It is something where we'll have to be, you know, very clear that we are allowed to use this data in this process. The other thing is that in terms of expertise and judging the task that they will be given is like find the number at this time and put it in the box. It's not like, hey, was this right or wrong? Right. So there's no real judgment there.
00:30:51.408 - 00:31:43.872, Speaker A: It's just a matter of like reporting the state of the world so that it can be put on chain. And once that information is on chain, the claims software package will actually do all the analysis. So there's no judgment required by these people, they literally just need to copy paste data. Oh, no, no APIs. It'll have to be like, look at a chart. Yeah, yes. Yeah, yeah, yeah, yeah.
00:31:43.872 - 00:32:25.724, Speaker A: So the question is basically who are the reference exchanges? So I'm going to give you an answer, which is sort of a non answer, but I think it's sort of the best you can do, which is we're going to have governance pick the reference exchanges per product upfront, and there can be more than one. So I agree with you that it's not like there's one exchange that represents the truth. Right. So what we should really strive to do is list as many reference exchanges as we think are necessary to get a good estimate of the overall market. And the process will only decide that the pit price is wrong if it's inconsistent with all of the reference exchanges. Right. Yeah.
00:32:25.724 - 00:32:58.576, Speaker A: Okay. Okay. So the question is, basically what happens if there's no demand for the product, for a particular product? Like, if there's no demand for equities, this is a good question. And in some sense, like, if you were looking at this from a sort of product perspective, you'd say, like, well, if there's no demand for it, maybe you shouldn't list that product in the first place. Right. I think in your particular case, you know, people want cryptos now because only the crypto people use crypto. Right.
00:32:58.576 - 00:33:24.214, Speaker A: But, like, you can imagine that's going to change going forward. Right. So I'm not sure that there's, like, no demand for equities in the long run, and it's just in the short run, it seems like people want cryptos. Yeah, I'll go over here. Because you have his handwritten verified. You should ask the human protocol. Eli overheard that question during his talk.
00:33:24.214 - 00:33:54.262, Speaker A: I'll do this one first. Yeah, I think. I don't think it would ever have happened in the last three months. Oh, sorry. The question is how often would these events have happened that would trigger these payouts? In the last three months? In the last three months, I don't think it ever happened. Our goal should be to make it never happen. Right.
00:33:54.262 - 00:34:47.590, Speaker A: The fact that it never happens doesn't mean that you don't want to be extra safe, though, right? Like, you still buy fire insurance even though you don't think your house is going to burn down. Right? So. Yeah, yeah. So, you know, I agree with that. I mean, the reality is that they're. The question slash comment is that there isn't really a single price. And, like, I agree with that.
00:34:47.590 - 00:35:18.208, Speaker A: You know, we're trying to do our best to like take all the contributing prices and give a range. That's essentially what the price and confidence interval will do. And in terms of the actual claims, it's like, well, is that range completely inconsistent with everything we see in the real markets? Yeah. Ok, there's a lot of questions, but I don't really want to get stuck here, so, sorry. Why don't we continue and if you still have questions we'll circle back. Ok. All right, so the next mechanism I'm going to talk about is this reward distribution mechanism.
00:35:18.208 - 00:35:58.748, Speaker A: And recall that what this mechanism does is it takes a share of the fees that are paid to this sort of fee pool and it's going to divide them amongst the publishers. And in doing that we had a couple of goals, and those goals are basically this. First of all, we wanted to preferentially reward the highest quality publishers. This is one of our goals from the very beginning. This is the mechanism that's going to be responsible for doing this. Um, then there were a couple of, you know, attacks or maybe perverse outcomes that we realized could happen here and we kind of wanted to avoid those. So, um, the second one, incentivizing honesty, is kind of getting in a problem with other oracle mechanisms that have been proposed.
00:35:58.748 - 00:37:14.242, Speaker A: Um, and the problem is that typically oracles have worked by rewarding publishers for agreeing with everybody else, right? So like everybody publishes a price, we take the median price and then whoever published the price closest to the median gets a point. And this is not a good way to build an oracle, we think, because what this actually incentivizes is for all the publishers to kind of predict what everybody else is going to say and then play that same number. So if everyone was trying to maximize the amount of money that they could make, we would lose the dispersion amongst the prices and things like that. We'd lose this sort of representativeness of the market, which is kind of what we're going for. So we really wanted to build a system where the best thing for publishers to do in terms of maximizing the money that they can make is to just take whatever they think the price is and put it on chain. We don't want to gamify it too much beyond that. The third thing that here is just, we wanted to make sure that, you know, we were safe in the case where as we grow the network of publishers, you know, maybe people who are less reputable join the network and they try to game the system and they try to maybe earn rewards without actually knowing anything about the market.
00:37:14.242 - 00:37:48.074, Speaker A: Right. And this is actually also a problem with that agreement system because, like, the price of a product doesn't change that much over time. So if you just look at the price, like the last, like last Solana slot, that's a pretty good estimate of the price of the slot. So if you knew nothing about the market, you weren't trading at all, and you just looked at the PiP prices, you might be able to earn money in that system. And so we thought that was a bad outcome, and we kind of wanted to prevent this from happening. We really only want people who have, like, new information about the market price to be able to make money. So those are kind of our three properties in this reward distribution system.
00:37:48.074 - 00:38:19.824, Speaker A: And so we kind of came up with a metric that we're going to use to distribute the rewards. And you're basically going to get rewards proportional to this metric. And the metric is going to be a product of three different factors. The first factor is the stake weight, which comes out of this delegation or data staking mechanism. And you can think about this as just being the baseline. Like, the baseline way you distribute the rewards is proportional to stake. So what we're going to do is we're going to take this baseline stake distribution, and we're going to adjust it a little bit with two other factors.
00:38:19.824 - 00:38:55.636, Speaker A: So the first factor is going to be what I'm calling the quality score. And the quality score is trying to get at whether or not a publisher's price series is contributing new information about the price. So one way you can operationalize this is you can take the publisher's prices and you can see if their price series predicts changes in the future price. Right. Intuitively, changes in price are new information. So if you are predicting those changes, then you're contributing that information to Pip. Maybe an idealized way to say it is imagine you always predict the price one slot ahead.
00:38:55.636 - 00:39:33.336, Speaker A: Then everybody else isn't contributing any information. You're telling us everything we need to know about the price. We can ignore everybody else. So the way that we're actually going to measure whether or not you can predict the future price changes is we're going to train a regression model on chain that takes some features of the publisher's price series and predicts the change in the future aggregate price. And we're going to take the correlation of the predicted change and the actual change, and that's going to be your quality score. So there's a couple different ways to visualize this. One way to visualize this is we can just plot the predicted price changes and the actual price changes.
00:39:33.336 - 00:39:57.520, Speaker A: So I've done that on the left here for two different publishers. And you can see here that what you want to see is a linear relationship. You want to see a tight linear relationship. So you can see for this top publisher, we're getting a higher score than this bottom publisher. This is like a little bit more of a cloud. So that's one way to visualize it. Another way to visualize it is that you can plot kind of the inputs to the regression model versus the output.
00:39:57.520 - 00:40:28.792, Speaker A: So I've done that on the right here. So we're going to have two features that go into the regression model. The features are essentially the publisher's change in price from the previous slot. So you can imagine if the publisher plays 100 and then they change their price to 101, that's a prediction that the aggregate price should go up, right? So that's one of the features. That's this x feature here. And then the y feature is the difference between the publisher's price and the aggregate price. So again, if the aggregate price is 100 and you say it's 101, you're saying the price should go up, right.
00:40:28.792 - 00:41:01.864, Speaker A: So that's the y axis, and the coloration here is actually sort of a z axis. So what you want to see is like a smooth gradient, right? You want there to be a reli. Like, you want all this data to lie on a plane, and a smooth gradient corresponds to a plane here. So you can kind of see that this publisher has a nice smooth gradient. This publisher has a bunch of random points floating around. So that's another way to visualize it. Um, there's some more details on, like, the math and, like, how we're going to train the regression model and stuff like that in the white paper.
00:41:01.864 - 00:41:35.682, Speaker A: Um, okay, so this is the quality score. Um, the third. The third factor is the calibration score. And the calibration score is going to get at whether or not the publisher's confidence interval is an accurate estimate of the uncertainty in their price. And the way we're thinking about this is that the publisher contributes a price and a confidence interval that's effectively a probability distribution. It's a probability distribution that tells us where the real price of the product is. There isn't really a real price of the product, but we'll use the aggregate price instead.
00:41:35.682 - 00:42:15.656, Speaker A: We're basically going to compare the distribution of the aggregate price to the distribution that the publisher said, the aggregate price, where it should be. There's two probability distributions. We just need to compare them. And the way we can do that is we're going to compute a histogram where we take the difference between the publisher price and the aggregate price, and we're going to bin it by confidence intervals. We're just going to divide by the confidence interval. And what that's going to give you is a histogram like this purple one here, which tells you essentially how many confidence intervals away from the median, the publisher's prices. And then if you use some probability distribution, it will tell you the actual theoretically correct count in each of these bins.
00:42:15.656 - 00:42:44.982, Speaker A: So we're using a Laplace distribution, and the points are kind of depicted here, and we're just going to do a distance metric between these two histograms. So that's going to be our calibration score. You know, I think on this calibration score, like, I think we might do something where it's like, as long as it's greater than 0.9 or something, everybody gets full credit. Like, this isn't meant to be, like, a really strict test. Right. It turns out that this score works pretty well.
00:42:44.982 - 00:43:18.754, Speaker A: Most of the publishers are pretty well calibrated today, or you're off by a factor of 1.2 or something. You can just scale all your confidence intervals. So this is not a very strong penalty in today's data, but we just want to make sure that people aren't publishing confidence intervals of zero or a penny on the bitcoin price when it's 50,000. That's the cases we're trying to avoid here. Um, okay, so that's the calibration score, and the. The publisher's overall score is just the product of all three of these scores.
00:43:18.754 - 00:44:09.874, Speaker A: So, um, I'll tell you a couple of. Well, actually, why don't we just check how we did? So, uh, does this reward publishers who are contributing higher quality prices? I think the answer is yes, because we have this quality score that preferentially rewards you for contributing prices that predict future price changes. Um, and we also have this calibration score, which encourages you to set the confidence interval sort of appropriately. Right. Um, and then does it incentivize honesty? Well, I wouldn't say it necessarily incentivizes honesty, but it disincentivizes dishonesty. So if you think about the agreement system, the regression model actually captures all of those factors for you, so you don't have to worry about it. If you're, like, always a constant factor off from the aggregate price, there's no reason for you to adjust your price, because the regression model does that automatically.
00:44:09.874 - 00:45:02.034, Speaker A: So a lot of the things that you would kind of use to tweak your prices to get a better reward, the regression model takes care of. So you kind of aren't as strongly incentivized to try and cheat. The third property is it discourages uninformed participants. And the way it does this is that the quality score will only be greater than zero if you can produce a price series that predicts the future changes in the price to some extent. So if all you have is historical data and you believe that markets are efficient on sort of a small timescale, this isn't something that should be easy to do. And I know some of the people in this room do this, but if you could do this, you'd probably rather trade on that information than go play with pith, right? So, seems like a reasonable security guarantee there. Okay, so those are kinds of the properties of this reward distribution mechanism.
00:45:02.034 - 00:45:41.484, Speaker A: That's the end of this part of the talk. I'll take questions on this. You're asking, can you see the stakes of the different publishers? Yes, good question. So right now we don't have the staking implemented, so there is no stake. Everyone has uniform stake, but in the future, the stakes will be on chain. So it's public information. So you will be able to see that by publisher Key, all the publishers are pseudonymous.
00:45:41.484 - 00:46:08.164, Speaker A: The second question about the metrics. Yeah. So we're actually working on a page on the website that will actually show you these metrics. If you find Daniel around here, he might show you a preview. He's over there. But actually this plot is from Daniel's dashboard, so you'll just be able to pull this up for any symbol, any publisher on the website. Yes.
00:46:08.164 - 00:46:42.684, Speaker A: Sorry, I couldn't quite understand that. Oh, yes, sorry. Good question. Yes. So what we're going to do is we're going to compute the scores per epoch, and then we're going to distribute all the fees for that epoch at the end of the epoch. Yeah, good question. The question is, are there any constraints on how often you have to publish your prices? Yeah, so actually this is.
00:46:42.684 - 00:47:27.682, Speaker A: Yeah, I probably should have said this, but the quality score actually counts up time. So you actually only get like, there's sort of one possible prediction for every time the aggregate price changes, and then you get a score for every time you actually made a prediction by like, publishing a price. So the quality score is actually a combination of, like, how good your thing is, times, like how often it's up. So it's kind of implicit. Yeah, good question. Yes. Yeah, so I didn't put that in the talk.
00:47:27.682 - 00:48:16.640, Speaker A: There's some discussion of this in the white paper. There's kind of a question of exactly how that mechanism is going to work. At the highest level we can basically take extra pip tokens and put them into the fee pool and distribute them. So that's an easy way to do it. But there's a question of how many tokens and where and all that. Yes. You're asking the question is if there's data on a blockchain with a lower block time, are we going to publish that same data at high frequency? That probably doesn't make sense, but I haven't really thought about it.
00:48:16.640 - 00:48:56.862, Speaker A: Yeah, well, ok. So there's maybe two answers here. One is imagine there's a publisher who has a 15 2nd time and there's another publisher for the same symbol with a much shorter time. I guess there's a couple publishers with a shorter time. That slower publisher isn't going to earn any money so they're eventually going to be kicked off the system. We've actually seen this for some of, we have some on chain publishers right now and they are actually slower than the other publishers out there and they actually get a lower reward because of them. I think that's fine from the perspective of the reward distribution.
00:48:56.862 - 00:49:39.074, Speaker A: Now there's a question of like if it only updates every 15 seconds, are we going to waste the money to keep updating the price feed? And I think that's more of an implementation question and we can cross that bridge if we have to. We could say this is a feed that updates slower and maybe the prices just stick around for longer. They're valid for, instead of being valid for 25 slots, they're valid for 200 slots or something. You can just update it slower. We can figure out how to deal with that, I think. Oh sure, sure. Yeah.
00:49:39.074 - 00:50:26.120, Speaker A: I mean we're really trying to build. Sorry. So the question is what happens if it's like even lower frequency data like once a week? And we're really trying to build something that works for like prices which are updating all the time. So like you probably want a slightly different solution for that lower frequency stuff. We've talked about different ways to do it, but we haven't committed to doing anything. Yeah. So we have some like the questions what happens about things that close like equities, right.
00:50:26.120 - 00:50:41.624, Speaker A: And we actually publish like equities. And what we do is we just like stop the price at the end of the day. And like you can use the closing price if you want, but like whatever. Yeah, yeah, yeah. Okay, I'm going to continue. I'm almost done here. I'm just going to talk about governance.
00:50:41.624 - 00:51:21.894, Speaker A: Our governance mechanism is pretty standard. We're going to do a sort of token voting mechanism where if you've staked pit tokens, you get to vote. There's going to be a bunch of different actions that governance is going to be expected to do. Here's a listing of some of them, like approving what tokens can be used in the fee pools, listing new products, approving updates to the program, that kind of thing. Control all the parameters of the protocol, that kind of stuff. Basically, we don't really want to have a lot of power given to governance, but there's some things that we can't do with these automated mechanisms and require a little bit of human input. And so that's kind of where these things come in.
00:51:21.894 - 00:52:06.334, Speaker A: But this is a pretty standard way to do governance. We're not innovating much here. Okay, so that's the end of the talk. Just to sum up, I presented some of the mechanisms and designs in the white paper that we just released. Our core problem that we're trying to solve is to come up with a way to take these data feeds that PIp publishes and come up with a way for publishers to monetize that data. And what we've done is we've added this data staking mechanism up here that gives consumers a reason to pay fees for the data feed, which we can then share with publishers. And we've done some kind of clever things in how we distribute those fees amongst publishers to encourage them to contribute the highest quality data and really get high quality data providers to PIP.
00:52:06.334 - 00:52:23.514, Speaker A: So that's kind of the summary of the mechanisms. If you're interested, please check out the white paper. And I'm going to be around all afternoon, so please come talk to me. Happy to have more conversations. Really want to know what people think. And we'll definitely take your input into account as we kind of build things going forward. So thank you very much for your attention.
