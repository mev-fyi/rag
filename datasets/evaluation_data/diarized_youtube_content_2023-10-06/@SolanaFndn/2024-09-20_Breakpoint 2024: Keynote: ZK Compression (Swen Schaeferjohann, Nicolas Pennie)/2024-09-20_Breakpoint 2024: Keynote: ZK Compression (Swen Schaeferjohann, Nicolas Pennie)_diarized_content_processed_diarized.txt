00:00:03.880 - 00:00:13.045, Speaker A: Hey everyone, I'm Nick and I'm one of the co founders of Helios. Helios is a developer platform and RPC provider for Solana.
00:00:13.465 - 00:00:20.685, Speaker B: My name is Sven, co founder at Lite Protocol and Light is a protocol for scaling Solana with zk.
00:00:21.425 - 00:00:41.875, Speaker A: So you might be wondering why are two different businesses here right now? And we basically partnered to build an end to end scaling solution for Solana. Today we're going to talk about Zika compression, what it is, how it works, and most importantly why it matters. We'll do some demos and we'll also go and have a Q and A at the end if you have any questions.
00:00:44.015 - 00:01:28.205, Speaker B: Awesome. So first question, what is ZK compression? Now to understand that answer, we'll provide a little bit of context first and then we'll just recursively go deeper over time. And by the end of the talk, hopefully you'll all understand what ZK Compression is. First thing to know about Solana is that compute on Solana is very cheap. But actually data storage is very expensive. So if you want to create 1000 token accounts that cost you 300 bucks. Now the thing is, if you want to scale your application, right, you have an application, you hit a lot of users, that cost becomes really basically economically prohibited for you.
00:01:28.205 - 00:02:06.009, Speaker B: If you want to create a million token accounts, that's for $1,000. And so that cost just goes up with the price of Sol as well. So that's a big problem, number one. Now there's a second problem which is the state growth problem, which all stateful blockchains have. The thing about Solana is that it has a lot of accounts. So the concept here is that if we store data, we store it in accounts and when we create an account we have to pay for rent and rent is expensive. And still there are 500 million accounts on Solana right now in the total state.
00:02:06.009 - 00:02:40.955, Speaker B: And this number is growing. So each day we have net new, we add about 1 million new accounts per day. Now those are two major problems. And ZK compression basically fixes this. We have on one hand 1,000 times cheaper accounts for Solana. Then we have a solution for state growth which is ZK compression. Then lastly, this is a little bit of a side note, we'll get to it at the end of the talk is ZK Compression is also a foundation for for ZK Compute on Solana.
00:02:40.955 - 00:02:54.663, Speaker B: Alright, just to hammer it in once more, we have a thousand times cheaper accounts, we have a solution to state growth and we have a foundation for ZK Compute on Solana.
00:02:54.719 - 00:02:55.315, Speaker A: Now.
00:02:57.055 - 00:03:51.035, Speaker B: Alright, so if we take the words ZK compression, what does it actually mean? Right, so we have state compression on one hand, then we have, we add ZK0 knowledge to it. So let's first look at state compression. It's basically four major points at the high level. First point is what we do is we take millions of accounts on Solana and we compress them, we hash them together into one small fingerprint. Then we take that fingerprint and we store the fingerprint in an on chain account and we store the actual underlying account data off chain, for instance on the Solana ledger. Now lastly, we have a proof mechanism that lets us verify on chain the validity of these underlying off chain accounts using the fingerprint that we stored on chain. So that is the high level of what state compression is.
00:03:51.035 - 00:03:58.575, Speaker B: And then ZK compression is just using zero knowledge proof snux for the last element, which is the.
00:04:00.375 - 00:04:34.921, Speaker A: Yeah. And so now we'll talk about why CK compression is the right solution as a state compression kind of primitive. And what I'm going to do is outline the key features that essentially make it the right solution. So first off, what I really want to hammer in is that compressed accounts are essentially analogous to regular Solana accounts. They are very, very similar. Which means that you can apply the same sort of Solana, you know, development techniques that you do today with the ZK compression system. It will feel the exact same, which means the barrier to entry for developers is very low.
00:04:34.921 - 00:05:17.417, Speaker A: And it just essentially will make this whole system easier for you to get started with. If you look at the, for example, the API that the indexer exposes, which we'll talk about more in the future, you can actually just see how similar it is. You can see that for example, getaccountinfo just maps to getcompressedaccount and, and this sort of like one to one mapping continues for essentially the whole RPC API. Now another really important point that we want to hammer in is that compressed PDAs are supported. So PDA is a program derived address. And essentially what this is is it's a deterministic address. It's a way that given a certain program plus a seed, you're able to get consistently the same account out of that.
00:05:17.417 - 00:06:41.235, Speaker A: Now this is actually so important that we decided to rebuild the entire compression system into ZK compression for this very reason. And it's powered under the hood by the zk, which is a bit out of scope of this talk, but essentially it's the reason why we did all of it and the way that compressed PDAs work is that basically you take the program ID and say you have a domain name service like a DNS and you want to take that key, you can then map that to one single account every single time, which means you don't have to worry about having race conditions or collisions and accidentally creating two DNS value accounts for the same key. This is a super powerful concept that if you've done a Salon development, or if you've done just development in general, you'll understand how this is like a basic primitive that needs to exist in any system and it's fully supported with CK compression. So if you think about where this will take Solana, what I want to emphasize is that because it has P and because there's also this ability to compose programs together is you're able to have these sort of more complex architectures. You have multiple programs, different PDAs, the sort of development experience, you're used to building applications, you're used to building on Solana, but you're going to be able to do this with a thousand times less cost for your accounts. That is huge. This will open up use cases that we probably haven't seen before because people are essentially just priced out of it.
00:06:41.235 - 00:07:16.031, Speaker A: So there's a few things I really want to hammer in here that are quite important. First off, everything is executed on Solana. It's not an L2, it's not a validium. Essentially you have the data availability automatically through the Solana ledger because it's executed on Solana. It means that it's verifiable and it also means that it's fully composable. Now, for those of you that are familiar with the old compression system, I I just want to make a quick little call out. That one improvement we've done here is that the Zero Knowledge proofs have this feature where they shrink the proof sizes down to a consistent 128 bytes.
00:07:16.031 - 00:07:49.337, Speaker A: What that means is you now have a lot more room in each transaction for essentially just whatever else you need to do, which means easier composability and just more room for activities. There we go. Another important feature is decompression. This is really important because it essentially prevents lock in and allows for simple interoperability. Any Solana account, today or in general, can be compressed. And when you do that you get the rent cost back. You can also take any compressed account and decompress it.
00:07:49.337 - 00:08:29.435, Speaker A: And you obviously need to put your rent payment in there. So that means you can go between the two so if you ever have a compressed program, say a compressed token, I need to decompress it. You can then decompress it and feed it into say like Jupiter, do a swap or whatnot. It also allows for a sort of back and forth between hot and cold state. So you're actually able to say you want to create like a video game where you have say a bunch of different cards and items that are all compressed. You can decompress those when you go into a battle, have quickly iterating state and then you can compress it afterwards, saving the final result, say like the hit points change and whatnot. So again, very sort of powerful concept.
00:08:29.435 - 00:08:55.525, Speaker A: The other thing too is I want to emphasize this is a fully generalized solution. It's not tied to a single program or business logic or application. It is essentially just Solana accounts and compressed with a low cost. You can do anything you want to do today. It has out of the box indexing support, support. It's fully open source production ready. And yeah, that's pretty much the main point I want to make there.
00:08:55.525 - 00:09:38.515, Speaker A: But going even further, we've also added just in the first release, compressed tokens. And we built compressed tokens using the exact same tools you would build to build any application that leverages compression. It's no different. And so you obviously get the thousand times cost savings, but you also. I suppose what I want to say is that the compressed token program we've built mirrors the Solana token program pretty much one to one, which means it feels the exact same to work with. We're also going to include token extensions which will be live at the beginning of 2025. And as I mentioned before, you get all these other benefits like decompression so you can airdrop tons of tokens and then decompress them if you want to use them in say defi.
00:09:38.515 - 00:10:01.405, Speaker A: So now we're just going to do a demo so you can actually see how it works and just see how essentially it mirrors the Solana experience pretty much one to one. And so what we're going to do here is we essentially have a, we're doing this all locally and we have a test validator similar to the Solana test validator. And we're going to go and essentially create a token and mint some tokens.
00:10:02.465 - 00:10:53.391, Speaker B: So that's what we do here. And one thing I want to highlight here is that ZK is thought of as like this holy grail of like, you know, very expensive computation. But what you see here is that actually the transfer and we'll do it right here looks a lot like a regular SPL token transfer. And that is because we've shrunk down the proving time to like milliseconds. And so we think that's pretty cool because it's like a production grade consumer ZK right there for the of token accounts. And so what we have here is we have 42 minted token accounts in compressed form. Now we transfer 12 of them to another person and we see that it takes right there, we see that it takes like, you know, similar to a regular Solana transaction.
00:10:53.391 - 00:11:57.713, Speaker B: And then we check the balance and we see that indeed we have sent 12 tokens, compressed tokens to another person. Awesome. So this demo looks cool, but how does it actually work under the hood? Now to recap, you notice slide, we have these four points at a high level about what state compression is, right? So to recap, we have millions of accounts. We fingerprint them, we hash them together, we store the fingerprint on chain, we store the underlying account data off chain, for instance, on the Solana ledger, which has the same safety guarantees as Solana itself, we use on chain accounts. And then lastly, we have a proof mechanism that allows us to, in a smart contract, verify this off chain data on chain. Now, if you dig deeper, there are five major components in the overall system and I'll just walk you guys through it. So the first thing that is very critical to understand is the concept of state merkle trees.
00:11:57.713 - 00:12:27.911, Speaker B: And we call it the forest of State Merkle trees. And I'll get into why it is a forest in a bit, in a minute. But basically this concept helps us understand how we create this fingerprint that I mentioned earlier. So say we have four accounts. What we can do is we can hash them and then we can recursively hash them together into this upside down tree structure. This is what's called a Merkle tree. Now the most the root on the top.
00:12:27.911 - 00:13:13.097, Speaker B: So this hash, the final hash on top is a 32 byte hash. Now basically what this does is it gives us a cryptographic guarantee about the underlying account data of all the different accounts. And what that means is that it ensures integrity. So it's really easy for us to verify that a given account state is actually part of this merkle tree by verifying it against the state run. Now there are two relevant programs here in the protocol. There's a light system program which is sort of an on chain vm and it mirrors the Solana system program. So all it does really is it writes to the Merkle tree, it enforces the generic account model that Solana also does have.
00:13:13.097 - 00:13:53.255, Speaker B: And then it also deals with verifying the uniqueness of PDAs. Next up we have the compressed token program. And as Nick already mentioned, compressed token program mimics the SBL token program. So what it does is it just enforces the SPL data layout on top of the account model that we have for compression. Now, there's one other important concept that I want to highlight here in this section, and that is the concept of the forest of state knuckle trees. Well, there's some nuances there, and that's why we have forester nodes. So in short, foresters manage the light forest.
00:13:53.255 - 00:14:38.653, Speaker B: And what that means is when we look at the compressed state changes, what actually happens when you update a compressed account is that you actually add a new account state to the tree, you append it, and then you zero out or nullify the old state. There are two implications of this. One implication is, well, whenever there's a state update, the root changes, right? Because the state underlying changes and therefore the root must change as well. The changes trickle up the upside down tree. Now there's a second facet to this which is, well, the trees fill up over time, eventually they get full. And so that's where the forest of light trees comes in. Forest denotes basically maintain a state route.
00:14:38.653 - 00:14:56.525, Speaker B: So they update the state route in a synchronous process and then they also roll over full state trees. Now I should note that for your own state trees, running your own forester node is permissionless. And lastly we have the indexer.
00:14:58.065 - 00:15:36.401, Speaker A: Yeah, so I'd like to next introduce Photon and essentially it's the open source indexer for ZK compression. The purpose of the indexer essentially is to listen to what's happening on the chain with respect to compressed accounts, getting updated, created, mutated, etc. And this is essentially it is caching the current state and it's also responsible for generating the pro proofs that you can then use to verify the data or to also mutate the data, if you so choose. So in terms of features, it's fully open source and it's production ready. And we've intentionally tried to make some improvements and take the learnings we've had.
00:15:36.433 - 00:15:37.857, Speaker C: From working with the first variant of.
00:15:37.881 - 00:16:09.255, Speaker A: Compression to make this indexer much easier for everyone to use, regardless of whether you're an individual developer or if you're an enterprise or RPC provider. First off, local development is far easier. You have a cli. It's like a one click thing where you Just run it and it will automatically work with your local development. We also have an explorer that works locally, so it's more of a dev focused explorer. So you can kind of see your compressed accounts visually and understand what's happening, look at the transaction history, etc. Next, I want to talk about something important which is snapshot support.
00:16:09.255 - 00:17:02.903, Speaker A: It will generate snapshots at a daily period, which is really important because it allows anyone else to start the photon indexer from a snapshot and they no longer need to go and re index from Genesis, which means their boot times and start times are a lot faster. And as you can see here, we are able to catch up in about 15 minutes or less. Also, the advantage of having snapshots is it increases your sort of replication factor or your sort of, yeah, I guess your replication factor. It's like the. It makes it easier for anyone to run it because if say any RPC provider stops wanting to offer support for compression, you just need to go grab that snapshot and you can run it yourself. There's no real risk anymore because we can offer these snapshots and store them on somewhere decentralized like filecoin. And lastly, I wanted to point out that I already mentioned this, but it really is built for everything from individuals to enterprises.
00:17:02.903 - 00:17:34.071, Speaker A: So if you want to index your own data, you can run photons as only indexing a subset of the data means you need much lower hardware requirements, smaller database, et cetera. You can run it with SQLite and one CLI command. Or if you want to index everything as an RPC provider, it also of course facilitates that. So how do you use it? Well, it's available today on any Helios plan. You can also, as I said, run it yourself, or you can just talk to your other RPC provider and just ask them to start offering it. Awesome.
00:17:34.263 - 00:18:30.485, Speaker B: So one other major thing to highlight is developers, Developers, Developers, developers, developers. We are focused on developers and so there's three major things that we've built and that we're building out and improving on. And the first thing is the SDK which is like Web3js, if you use Solana Web3js, but for compression. So what that means is if you have your compressed token transfer, all you need to do is you need to first fetch your compressed token accounts. Then you can use those token accounts to fetch a validity proof from your RPC or a dedicated prover node. And then lastly, you can just build your instruction like you would with the SBL token program. So you build the instruction, you Tell it what you want to send, how much, which meant, right, and to whom, and then you can go off and build your regular Solana transaction.
00:18:30.485 - 00:19:16.041, Speaker B: All right, the second thing is you saw it in the demo as well in the beginning and Nick highlighted it, which is the full local development setup. So we have this test validator that comes pre initialized with all you need for local development. So this is all the programs. The photon indexer runs locally, the prover node runs locally, so you have all of that ready to go. And then lastly anchor macros. So if you are into Solana program development, you've probably heard of unused anchor. And basically our goal is to make developing with compressed accounts feel exactly the same as developing with regular accounts.
00:19:16.041 - 00:19:24.175, Speaker B: So this is what this looks like. If you have written an active program, you will feel the similarities.
00:19:25.035 - 00:20:09.435, Speaker A: All right, so next I want to talk about Airship. This is something we built to essentially let people start using ZK compression today for users essentially. And what it is is it is a mass airdrop token. Essentially it's very easy and cheap to use and you have either a UI you can use or a CLI and everything's open source. You can go use it, you can fork it if you want and make changes as you need. But essentially what I want to point out I guess is that in the best case you airdrop decompressed tokens and over time the ecosystem starts using compressed tokens for all sorts of different applications. But say you just want to immediately convert that, you can't, you can just decompress it right away.
00:20:09.435 - 00:20:42.565, Speaker A: We've also built that into the U. So essentially like your worst case is that using Airship would basically be a pre built airdrop tool that you could use and it just would be the same as using normal tokens. The other thing I want to point out is that it automatically supports airdropping to Solana mobile holders. It can also airdrop to any token. So holders of a certain token or an NFT collection, you can also provide your own CSV. If you have a regenerated list of users you want to airdrop to. Probably the easiest way to go over it is just to show a demo.
00:20:42.565 - 00:21:18.535, Speaker A: And in this demo what we're going to do, we're going to use the UI and we're going to use a CSV in this case. And then once you import that, you're going to go and specify the token you want to airdrop and the amount, confirm everything and then you can run it. Now one thing that's nice to is that both The CLI and the ui. If say you lose Internet connectivity or something goes wrong, you can resume your drop from wherever it is midway through. It keeps track of that state, which is quite nice because some of these airdrops might take a little longer, like 30 minutes or 45 minutes if you're doing a lot.
00:21:19.995 - 00:22:19.413, Speaker B: All right, so the TLDR ZK compression scaled Solana and we hope that you've all now gotten a glance and high level overview of how exactly it scales Solana. But then there's, you know, that's basically cheaper accounts, right? And then solving a state growth problem. And one note on this is basically the state growth problem, right? The way that ZK compression solves this or gives a solution to solve it is that you can store the. You basically only store the final root hash of all the accounts in the active validator memory, right? So that's effectively how we approach the state growth problem. Now there's these two scaling problems that ZK compression solves. But ultimately what ZK compression does is we think it's creating a new design space for apps. And there's two types of apps basically and we have listed a bunch of ideas on the next slide and there's two segments.
00:22:19.413 - 00:23:01.377, Speaker B: So one is just like dealing with cheaper state. So I'll just run through lists and there's a lot more that we've seen people do and that we like to see play out. We have SBL token compressors, 1 billion meme coins. I think that's an interesting one. And then prediction markets for Twitter posts. You could also use state compression with ZK to create identifier PDAs for deep end networks where you basically register your different nodes on chain. And then there's the second section which goes into the ZK part and into the verifiable compute part.
00:23:01.377 - 00:23:37.955, Speaker B: And three things I want to highlight here that I find very interesting are verifiable reward calculations. So you have some sort of like verifiable off chain computer. Just in a previous talk you've heard about bonsol, that's one approach to do it. And then you could plug into and actually write that data to the Merkle tree roots that ZK compression maintains. Now there are two other ideas. For instance, trust minimized bridges where you basically sync Merkle roots again and then ZK identity protocols. I think that's also something that's pretty interesting.
00:23:37.955 - 00:23:51.045, Speaker B: So there's a bunch of these ideas. We have a lot more out there in the documentation as well. But yeah, this should just give you sort of a sense of, like, how broad the applications are for ZK compression.
00:23:52.785 - 00:24:19.659, Speaker A: So this is live on mainnet today, so there's nothing stopping you from getting started. We have docs, we have demos, everything to start. There's also DevNet, LocalNet, TestNet support, whatever you need. We also are hosting a hackathon. You still have time. There's up to $45,000 in prizes. And also, I guess as we wrap up, I also want to give a special thanks to all the people who are behind this project.
00:24:19.659 - 00:24:35.955, Speaker A: We have Pedro Kuhn who worked on the Helios side, and then we have Yorit, Michael, Sergey and Tilo from the Light Protocol core team. And we have about five minutes left, so we can run through a Q and A if anyone has some questions.
00:24:37.695 - 00:24:38.143, Speaker B: Awesome.
00:24:38.199 - 00:25:02.875, Speaker A: And I believe we will have some staff out here who have mics that they can give to people who have questions. If that doesn't work out, then I'll just go around and ask directly. Does anyone have questions? You can raise your hands. I'll just go around then. Okay. Yeah, let's start here. Yep.
00:25:02.875 - 00:25:04.967, Speaker A: What's the incentive to run?
00:25:05.151 - 00:25:05.471, Speaker C: Sure.
00:25:05.503 - 00:25:10.343, Speaker A: So the question is, what's the incentive to run the forester nodes? Take that one.
00:25:10.399 - 00:25:36.145, Speaker B: Yeah. So running a forester node, the way that we see it, it's very similar to like running RPCs. Right. And running photon indexers, is that when you care about your own compressed state, as an app developer, you have an incentive, natural incentive, to make sure that your state trees are being maintained. And so we have an incentive to either run a node yourself if you want to self host, or you could pay another provider for it.
00:25:37.965 - 00:25:41.445, Speaker A: Awesome. Anyone else? Maybe over here I can jump down and the gray shirt.
00:25:41.485 - 00:25:42.629, Speaker B: Oh, we got a mic. Awesome.
00:25:42.717 - 00:25:43.985, Speaker A: Yeah, that would be great.
00:25:46.445 - 00:25:50.387, Speaker B: Okay, so when you're compressing states, what's.
00:25:50.411 - 00:25:57.235, Speaker A: The difference with the existing state compression program, like on the library.
00:25:57.395 - 00:26:04.615, Speaker B: And the other question is, like your token program created by you, is that.
00:26:05.075 - 00:27:19.523, Speaker A: Supported by any exchanges? So creating tokens in your token programs does not compatible with the ECC ecosystem. How do you fix that? Okay, yeah, I'll start with the second question, which is essentially, if I get it correctly, you're asking, is the compressed tokens, are they compatible with exchanges and defi, and if not, how do you rectify that? So essentially the answer is sort of yes and no. They're not directly compounded compatible because they need to actually support them, which they can do, but you can decompress them. So if you have a compressed token, you want to Decompress it, you can convert it into a normal token, regular SPL token and then you can use it in defi the exact way you normally would. So there's no real lock in. But we do hope that basically because you get to keep the cost savings if you don't decompress and it is very easy to use, we'll see more and more people basically building applications that leverage the actual compressed state. Now, I guess while we're here, I will mention that if you want to do rapid fire trading or something, decompressing is a better way to go.
00:27:19.523 - 00:27:42.397, Speaker A: Compressed state is best for cold applications and regular accounts are good for hot state. And that's kind of one of the big important things of this whole system is that we have that state growth problem. Right. So essentially there's tons of state there that is just cold. And now we're going to be able to move that to a compressed state which has all the benefits we talked about. Awesome. Any other questions?
00:27:42.501 - 00:28:29.759, Speaker B: I think there was another question as well asked by the first part. Yeah. So to repeat it, I guess the question was around why don't we just use the regular SPL account compression program and what it changes? And Nick has highlighted this in the beginning of his slides. There's a couple of improvements that we've made where we basically figure out we have to rebuild this from scratch. And this includes the support for PDAs, right? So what that actually means is proving not only that your state is part of a mercatory inclusion, but also being able to prove in an efficient way. So within the transaction size limit, basically that given address is unique. So what that means is that you have something that's called an address tree.
00:28:29.759 - 00:29:24.391, Speaker B: And this address tree is very similar to a state merkle tree, but with one nuance. What you actually do is by proving inclusion in a leaf, you prove exclusion of a certain range of numbers, right? And so what you can do is you can actually display or handle the whole address space. In our case it's 248 bit address space within much smaller merkle trees. And so that's how you get the uniqueness property. And this is something that we added that we basically had to rebuild from scratch. And then I guess the fundamental difference as well is we're using zero knowledge proofs snarks to keep the proof size consistent independent of, you know, how many accounts you want to update, read or write in one transaction and then how many inclusions or exclusions you want to prove. And this is very important because Solana transactions are very small by nature, so like 1.2
00:29:24.391 - 00:29:41.877, Speaker B: kilobyte. And the way to achieve these like fast proving times to generate these proofs you have to actually use a different hash function for these state merkle trees. And so therefore we could use the original SPL account compression program and then build our system program and compress token.
00:29:41.911 - 00:30:03.913, Speaker A: Program on top of that. I think we have time for one more quick question if anyone else has one in the back there are there any limitations around the concurrency that's happening inside a single tree during a slot? I'll take that one.
00:30:04.009 - 00:30:52.075, Speaker B: Sure. So the question is what are the limitations in with regards to concurrency when using compressed accounts? And Nick mentioned it in his one answer, which is that if you have account updates to the same account, like you write to that same account many times in a block, you should not use compression for that. What you can do is you can just keep that specific account permanently decompressed as a regular on chain account. For instance, if you have an AMM pool account, you want to keep that one decompressed, right? And then basically all the other state that is colder than this like hot account can. You know, it's designed, the system is designed in a way where you can then interact with these decompressed accounts and mix and match within one transaction.
00:30:54.615 - 00:31:09.199, Speaker A: Awesome. I think that puts us at the end of time. So we'll wrap up here, but thanks for the questions and yeah, thank you. Thank you everyone for having us the applause going.
00:31:09.327 - 00:31:14.235, Speaker B: Keep it going. And welcome to the stage Jacob from RISC0.
00:31:21.575 - 00:31:57.429, Speaker C: Nice to see everyone excited to continue to like expand on the stuff going on in Solana. So the title of the talk is Boundless Execution on Solana. First of all, let's do a little bit of a history recap. In general, blockchains are kind of in this dilemma of security, throughput and decentralization and everyone kind of makes trade offs. So you can have higher hardware requirements, more or less node operators, different sort of execution limits, and different sort of block frequencies. In general, when you think of execution on a blockchain, you're constrained by the smallest node.
00:31:57.517 - 00:31:57.765, Speaker A: Why?
00:31:57.805 - 00:32:06.645, Speaker C: Because that person has to keep up, continue to execute transactions. With ck, we get boundless execution. Instead of being restricted by the smallest.
00:32:06.685 - 00:32:08.585, Speaker B: Node, we become a sum of them all.
00:32:09.405 - 00:32:46.635, Speaker C: With ck, you can have really brief verification that someone's done an execution correctly. So if someone wants to generate and has 30k GPUs, they can do 30,000 GPUs worth of verifiable execution. If someone has 100 then you can do 100. So you kind of get this really nice property that you can just do a lot more execution. Let's do a really brief history of ZK in general. I think ZK has been talked about this holy grail technology more on the Ethereum ecosystem side since 2015, 2016. I think it's kind of obvious that there's not many applications live today.
00:32:46.635 - 00:33:04.629, Speaker C: Real quick, just mention here. When people mention CK normally, they're normally talking about verifiable compute. That's the syncness reverification factor of it. You can add privacy on top as well. So why are there no ZK applications?
00:33:04.717 - 00:33:05.325, Speaker A: Right.
00:33:05.485 - 00:33:28.503, Speaker C: Well, ZK is a little bit broken. Performance has not been there. There's been a lot of research around it. It's been inaccessible. The example I always give is currently you have to write these like ZK circuits. Most of you can probably code the game of chess in a high level language. If I asked you to code the game of chess represented by polynomials, probably less accessible.
00:33:28.503 - 00:34:05.389, Speaker C: And then there's also like an ongoing maintenance and security issue. If you ever need to change your ZK program, you normally have to change a large portion of your program. These circuits also really hard to one audit and make secure in the first place. So RISC0 fixes ZK development. All you need to know about this is we essentially made a CK circuit that looks like a cpu. We can take Rust code, compile it down to a RISC V binary and generate a proof that this code has been executed correctly. So what does this mean for a developer? Right, it becomes really usable.
00:34:05.389 - 00:34:29.469, Speaker C: Okay, so you just write programs in Rust. You can use 70% of the most popular Rust creates inside the ZKVM. Your development time is a thousand less. You don't have to worry about. You don't have to worry about the security as much and the maintainability. If you want to change your program, you just change the line of Rust and you're getting going. You don't have to get your circuits audited and then you get this boundless.
00:34:29.517 - 00:34:31.105, Speaker B: Proof part of it.
00:34:31.445 - 00:35:02.843, Speaker C: We have this other innovation called continuations. We can kind of just generate proofs for infinity. So we kind of like stitch them on the back of each other. So let's talk right? Like why should a developer care about becoming boundless? I'm going to share some examples that are all being worked on today by partners. First you get like boundless skeuomorphic execution today. Execution in ZK is cheaper than anything on chain. We can verify a dex swap for a tenth of a cent inside the zkbm.
00:35:02.843 - 00:35:18.095, Speaker C: So you know we have like roll ups. There's some people on Solana looking to do it using our ZK from network extensions coprocessors. Unfortunately this video is not loading. We actually ran the original version of Doom inside the ZKVM with rendering.
00:35:19.115 - 00:35:19.795, Speaker A: There's the picture.
00:35:19.835 - 00:35:56.565, Speaker C: Now next you could do some more like complex trading strategies. We have one partner, not specifically Tokamak, but you could track a moving price average over a month across multiple assets, across multiple chains, doing way more expressive things. And lastly we can start to bring in web2 infrastructure on chain. So here's a good example. We can do sort of like ZK login with the Gmail we generate a proof that you actually own a JWT token. We can also do this with oauth tokens Twitter logins. You can find this example on our website.
00:35:56.565 - 00:36:17.975, Speaker C: So if you would like to figure out more risk 0 is you can find all the ZKVM documentation on the left. And then we're building a protocol. We're calling it the verifiable compute later. It lets any chain elastically demand on execution. You can think of it a lot just how chainlink brought price feeds to every chain. We're going to bring ZK to every ecosystem.
00:36:19.155 - 00:36:19.963, Speaker A: Perfect.
00:36:20.139 - 00:36:20.595, Speaker C: Thanks guys.
