00:00:02.800 - 00:00:43.184, Speaker A: All right, welcome, everyone to the Solana Validator Educational workshop. Topic today is validator failover discussion. So the main point here is going to be to have us all discuss how each individual operator handles failures and what their course of action is for different types of failures. I'm going to introduce the topic a bit, but again, the point is to generate discussions. So, you know, at any time, if you feel like you have a point to add, please interrupt me, and, you know, we'll go from there. Just opening up the comments? Yeah, there's a question in the comments about previous workshops. I'll make that clear in just a second.
00:00:43.184 - 00:01:39.504, Speaker A: I'll copy it into the notes. So the agenda, we're going to overview the current techniques and resources for failover, talk very briefly about consensus and tower BFD, and then leave it open for discussion on how each operator handles, handles these cases. Let me just copy a link into the chat real quick. There we go. All right, so the general idea is operators have the ability to run a second validator as a failover option. You run the second validator as a non voting validator, so it is unstaked. It has an identity that you essentially just throw away.
00:01:39.504 - 00:02:12.970, Speaker A: It doesn't have any stake on it. And then at any time, you can designate that backup validator as the primary validator. The backup validator will start voting after the primary validator stops voting, either by shutting that validator down or switching that validator to a different identity. Key pair. The key thing here is you don't want to be double voting. You don't want to have your primary and your backup voting at the same time. So you have to be a little cautious with how you handle that.
00:02:12.970 - 00:03:13.542, Speaker A: Had this in your setup. The reasons you want to do this I think are maybe pretty clear. But the idea is to limit downtime, right? For some very obvious planned reasons, you might have downtime if you want to upgrade your software, you can have hot backup that takes over voting while your primary upgrades to the new version, and then switch back to the primary after you've upgraded. Lee Jenkins, I think you've got your mic on if you can mute. Thanks. Other reasons you might want this, right? If the data center decides to block traffic for whatever reason, if you have a hardware fault, there are many issues why you might want a second validator to fall back to. This is more of an overview of how the transitioning works.
00:03:13.542 - 00:04:06.224, Speaker A: Let me copy this here. Pumpkin pool, who I believe is on the call, made these docs, and I've done a workshop in the past a workshop in the past on this as well, but I think his docs are great. So go check out the docs on how to do a validated transition. But the point here is more just to talk about conceptually what's going on. So like I mentioned, you have a primary validator, and secondary validator. If you have a planned outage like a software upgrade, you are going to wait for that validator to stop producing blocks. So when you have a point in time where you're just voting at that point, you will switch that primary validator from the current identity, which is states, to a non voting identity.
00:04:06.224 - 00:05:06.418, Speaker A: There's details on how to do it in the pumpkin pool docs, but essentially you'll just have a second identity that doesn't have any stake on it. Once you've transitioned to that second non voting identity, you're going to copy the tower file to your backup validator, and then once you've copied that over, you designate your secondary validator as the new voting identity. So you essentially switch, um, what identity that validator is using, and then that validator goes on voting and producing blocks for you while your primary is down. I already put this in the notes, but this is the demo that kind of goes through it. And I've also put the previous workshops, there's also a previous workshop in doing the demo. Any questions about this live transition setup? Let me get this. I don't know why this keeps going.
00:05:06.506 - 00:05:35.734, Speaker B: There we go. I just want to point out that's like the happy path. So there are problem situations that will work slightly differently. I don't know if you're going to get to them, but they'll involve instead of, you know, sort of changing the primary and copying tower. Sometimes you just have to kill the primary because something is wedged beyond recovery, and your goal is to make sure the primary is not going to vote while you're making the secondary into the validator. The voting validator. Will you cover that later? Do you have a slide for that?
00:05:36.114 - 00:05:45.494, Speaker A: Yeah. So I'm not going to cover it directly. I'm hoping that's where the discussion comes in so we could talk about that. Now I'm going to talk a little bit about why you want to be sure you're doing those things.
00:05:46.194 - 00:05:52.114, Speaker B: We can help you go through. I just want to make sure that that point isn't missed and then we can circle back to it.
00:05:52.454 - 00:06:32.314, Speaker A: Yeah, yeah. So, I mean, the key point I think that Zantetsu is making here is all these things work great if you don't have like a real fault, right? If you're just doing this when you're upgrading software, when everything's working, the problems come in when like, let's say this primary validator is not reachable, you don't have access to this tower file there. You know, you have to consider what you want to do in those cases. And just a little background on what this tower file is doing. So this is going to be very high level and hand wavy because I'm not an expert on consensus. I think there's others on the call that would know more. But essentially, every time you vote on a fork, the tower is keeping track of those votes.
00:06:32.314 - 00:07:15.704, Speaker A: And it's also keeping track of something called lockout. And lockout essentially is signifying your commitment to a fork. So let's say you vote on fork a, and you vote on the second block in fork a. Each time you vote on that fork, you're saying that. I'm more and more confident that that fork is going to be the fork that is actually included in the ledger. And so the reason we keep track of this thing called lockout is so that validators can't just vote on multiple forks at the same time. When you're locked out from voting on a different fork, you're essentially saying, for this many slots, I can't switch my fork.
00:07:15.704 - 00:07:58.718, Speaker A: Yeah, anything. Want to stay here? So the key point here is that this tower file, this thing that keeps track of lockout, that is essential on your backup validator, because you could be in a situation where you have a long lockout right now. You bring up your new validator. If you don't copy over this tower file, your validator, your secondary validator could very happily go and violate lockout because the primary validator has a long lockout. But the secondary validator doesn't know about that lockout. So copying over this tower file and making sure you avoid lockout violations is critical. Any conceptual things to add there?
00:07:58.766 - 00:08:16.518, Speaker B: People want to add a subtle issue, but going back to my previous point, the ideal is that you copy the tower over and that it's seamless. But there are times when your primary is dead. I've had this happen. It's dead. You cannot get to it. It's the hardware fault. It's gone.
00:08:16.518 - 00:09:16.162, Speaker B: How are you going to get the tower out? Some people advise, well, you should copy it over periodically and often, and that'll reduce the window of possibility of a lockout violation. But it won't eliminate it because there could be just those few slots that happened since the last time you copied it over that are the slots that would let you secondary know that it shouldn't be voting. So that's. I wouldn't say you shouldn't do that because it's better than nothing, but I, but there are times when you basically have to, like, make a choice. Do I enable my secondary to vote knowing that it does not have an fully up to date tower, or what do I do? And I, and my rule of thumb is, is this. The stock salon software will work very hard to never go into a lockout longer than 256 slots. So if it's been at least 256 slots since your primary voted, your secondary is, even if it doesn't know anything about what your primary did, you're very unlikely to violate lockouts at that point because the primary very, very unlikely to have gotten itself into a situation to be that far locked out.
00:09:16.162 - 00:09:48.526, Speaker B: The only difference is that there for a couple minutes, couple votes that the, that you didn't realize were sent out, that would have extended that or something. Then, you know, maybe you could go, you could violate lockouts. But I think that 256 slots is kind of like the target. If you know that it's been 256 since you're primary voted, you should be able to just say, well, I lost my tower. I'm just gonna, I'm just gonna pick up, you know, pick up. Now, if you start before that, there's a chance you can violate lockouts. Additionally, I would like to point out that I run this thing called the Vodalyzer, which basically watches for lockout violations.
00:09:48.526 - 00:10:25.374, Speaker B: It watches RPC nodes and looks at all the votes they're receiving, even if they don't end up on chain. And seeing if any validator submits a vote that would violate lockout, which you wouldn't normally see if you're looking at block history, because only the surviving votes show up there. But so, you know, I see when there are lockout violations and they happen occasionally, and I think the answer is that it's because people are, you know, not copying their tower file over during a set identity or blowing it away on restart or somehow losing it. And when they restart, the violet outcomes, it's very rare. It's like once a month or less, but it does happen. Someday that'll be slashed. But for now, that's just like a slap on the wrist.
00:10:25.374 - 00:10:30.286, Speaker B: We let people know that it happened. Okay, I'm sorry to have taken over.
00:10:30.310 - 00:10:48.734, Speaker A: There, but that's great. Yeah. So there's some questions related to that. First question is, what is the effect of double voting, both nodes will crash. No, like Zantetsu saying you can violate lockout and right now there's no consequences for it other than just social consensus. Right, but if enough people were to do that, that would.
00:10:48.854 - 00:11:13.854, Speaker B: But if they were both live and voting, they will see each other's votes and they will both crash. If one of them's dead and the other one violates lockouts and the other one that's dead isn't still voting, then the one that's violating won't even know it did it. But if they're both live and voting, they will see each other's votes and they will both crash. And it sucks. I've had that happen, and it sucks to have both your primary and secondary both decided just exit because they don't like the fact that they're both running. That's what will happen if you run them both at the same time.
00:11:15.354 - 00:11:46.664, Speaker A: Yeah, and then Zantetsu mentioned it a bit as well. But there's no slashing right now. But the fix for this long term is slashing. Right? This is one of those clear cases where you're breaking the protocol. For whatever reason, malicious or not, you should be slashed. And that's an upcoming change that the consensus team is working on, but no timeline for arrival yet. And then there's a question from dragon decentralization.
00:11:46.664 - 00:11:59.664, Speaker A: Is there no sequence as to when to copy the tower file, like before or after changing to non voting? I would assume you want to stop the primary and then copy the tower file in the happy path.
00:11:59.744 - 00:12:22.810, Speaker B: That's the sequence you had it in that previous slide. Like you had that sequence. You stop the primary to non voting or kill it or whatever you have to do to make it stop voting. Preferably you're setting it to identity so that it can keep going. You can come back to it if you need to. But anyway, you first stop it from voting, then copy the tower over, then start the new one voting. That's the sequence you have to use, right?
00:12:23.002 - 00:12:47.034, Speaker A: Yeah, because the counter, right, is if you copy the tower file over and then stop your primary, there could be more lockout that you aren't keeping track of in the secondary, right. So you want to stop the primary first, have no more votes go out from the primary, and then copy over that tower file to the secondary so that the secondary can go off from the same state the primary was on.
00:12:49.254 - 00:13:14.524, Speaker B: And it doesn't take long. I've got it scripted that my, I have a script that basically tells the primary to, you know, stop copies the tower over tells the secondary to go and it takes maybe a second and a half to 2 seconds. And most of that time is probably waiting for the primary to complete its set identity. So it doesn't take long if you script it, if you do it manually, you probably miss a few more seconds typing things. Let it go.
00:13:16.464 - 00:14:04.784, Speaker A: Makes sense. Some more resources I can copy over. There's a question about sharing scripts, so if people are curious just about this topic in general, there's some decent docs on tower BFT algorithm in the Solana repo, so I'll copy that here. And Helios actually did a write up somewhat recently on consensus as well, that I think is a good read. It goes into a, I guess a bit of like clarifying what some things are and are not. It talks a bit about Poh, but generally speaking it gives you a good overview of consensus. So worth read as well.
00:14:04.784 - 00:14:37.782, Speaker A: That's pretty much all of the content I had planned. I think now is a good time to talk in more detail, maybe answer questions about different people's setups, why they do things they do, and how they handle the non happy path. The drawbacks here are kind of what we've been talking about, right? There's a risk of double voting the hot that the primary may not be reachable, so you can't copy over the tower. And in those cases, what do you do? How do you handle those scenarios?
00:14:37.918 - 00:15:16.316, Speaker B: So I've been through like everything you can be through I think, on this. So as pumpkins, we've been doing this for a long time and seen all the sort of traps and edge cases. I think if you're going to start, just start with the simple case of having a script that can do the copy and the happy path that'll cover 80% of whatever happens. Because mostly it's very helpful to do during software upgrades, because you can do a software upgrade with zero downtime, it's great. And in that case, you're in the happy path. It works really well. Then when you try to use that in some situations in which there are problems, you'll start to see the edge cases and I can describe them all, but you know, it's, it's hard to like until you've been through them, it's hard to really understand them.
00:15:16.316 - 00:16:00.154, Speaker B: Mostly they surround the primary being unhappy in a bad state and not being able to get the tower over properly, and the secondary not liking something about how it's running without a tower or without starting up without having had a tower. Most especially if you have an old tower file sitting on the secondary, which easy to accidentally leave it there. If you switch over the secondary in the past to do a software upgrade, switch back to the primary. The secondary now has written its own tower file with the same identity that it had while it was acting as the primary. That file is poison. If you don't get rid of that file and you try to set identity later, and especially if you haven't, in this situation where you haven't been able to copy the new fresh tower over because your primary was dead, the secondary will just die. It will be like, I can't.
00:16:00.154 - 00:16:32.370, Speaker B: You didn't give me a new tower. This tower that you have sitting here is way too old. And rather than just ignoring it, it'll just die. It should just ignore it, but instead it just dies. So one of the things you have to do is make sure that you script checking the stick, you know, in the situation where, you know, you're not copying a tower over properly or something's going weird, check that old tower on the secondary and make sure there isn't something really old there, because you will crash if you do. Now, demon has made a patch which fixes this. And I think it's awesome and I wish, I hope they take it and that it happens.
00:16:32.370 - 00:16:41.494, Speaker B: But until that happens, you're going to have to worry about this. It's the edge case. It happens to me once every six months to a year. It's not, it's not like a common thing, but it can't happen.
00:16:47.954 - 00:17:05.704, Speaker C: Yeah, it would be really nice. I don't know how to pronounce. I thought it was d man or D Mon. But if they, if they merge his pull request, that'd be great because that's. It's such a pain in the butt to accidentally forget you have that tower there. And to get the tower to old error and thing just dies when you need it the most.
00:17:06.884 - 00:17:37.326, Speaker B: Yeah, that's disheartening. You got this beautiful setup, you're ready to like, be like, I'm awesome. Yeah, right over. And then it dies and you're like, oh my God, it's, it's the worst thing. So that you don't want. And those who don't have a primary secondary, you do not understand the sense of like confidence that it gives you to know that you've got that, to know that like you can reach, you can switch over the secondary and then you can do whatever you want to your primary. It can crash on restart, it can have some weird problem that you didn't realize was going to happen and you're like oh, that's all cool, I'll fix all that for 1020 minutes, half an hour, hour, doesn't matter.
00:17:37.326 - 00:18:01.554, Speaker B: I'll bring back to my primary when, when everything's fine. Whereas if you don't have a secondary setup then you're on your live primary restarting it, not knowing if you've just about to enter hell for like you know, an hour or 2 hours while your primary isn't voting and stuff really sucks. It's great to have a primary secondary if you can make it happen. It is expensive and you know, more, more management, but I cannot recommend it highly enough. It's awesome.
00:18:01.974 - 00:18:32.294, Speaker C: Yeah, I would echo that especially for um, just experimentation because especially when you're a new validator, you want to try new scripts, you want to try and provision machines. I'm sure there's all kinds of ideas that people have, but they're afraid to try them. Um, you know, even if you could try it on testnet, you don't necessarily want to just roll that out to mainnet without testing it. So having that secondary just gives you so much more confidence. And if something goes wrong, it's like Zan said, it's not an issue at all because it wasn't voting in the first place.
00:18:35.834 - 00:18:53.444, Speaker A: One thing I've been curious about if anyone's tried, which I've been meaning to try, is has anyone tried using like a temporary secondary, like spinning up something on a cloud provider, having that run while you do the upgrade or some other happy path thing and then switching back to the secondary or to the primary, getting rid of the cloud server?
00:18:54.944 - 00:19:28.622, Speaker B: No, I used to have a setup like that before set identity even existed. And I had an Amazon like AWS node that was, you can like set it up so that all you keep persistent is the disk and it costs you a couple bucks a month until you need it, then you spin it up and you pay hourly. So I was like, oh, whatever, it cost me $5, $10 for an hour or two of cpu time. That's great. Back then it was really, I was doing a shut down the primary and restart the secondary as quickly as possible because there was no set identity method. And it worked like that worked and I think it would work really well now. I think it would work even much better now and I think that's a great technique.
00:19:28.622 - 00:19:59.094, Speaker B: I haven't personally set it up, but I think anyone that at least if you can't have a fully hot secondary, having a cold secondary like this, that costs you little but that you can spend on when you need it is a good second choice. And if anybody has done it, please give your, you know, your feedback, because I would love to hear about it. This is an opportunity for someone to contribute, you know, figure out how to do it and make, make a document and share it with everyone.
00:19:59.954 - 00:20:17.294, Speaker A: Yeah. From the foundation's point of view, I'd love to amplify that. If anyone works on a doc or wants to contribute to the docs in general, that would just be a helpful way to like, you know, have a low cost backup in the case of upgrades or maybe some outages.
00:20:18.714 - 00:20:41.234, Speaker B: I mean, this is something I'm probably going to do eventually because my primary secondary are in the same cabinet in a data center. So the data center goes offline. I'm not protected at all. My data center has never gone offline in three years, but it's always possible. So eventually I plan to have a tertiary, which is something like this set up elsewhere, but I haven't done it yet.
00:20:44.654 - 00:21:20.028, Speaker C: Another benefit, I guess I'll add. I don't know if it's been discussed. I was trying to help with the testnet thing, but especially when labs, I think it's called Anza now, will call for 5% of stake to upgrade to a new version. Well, in the past, obviously, it's a new version. It hasn't been, I mean, it's probably been on Testnet, but maintenance different, you don't know what's going to happen, and people have been burned by upgrading and hitting an edge case bug with a hot spare. I always just upgrade the hot spare and let it burn in for a week or two before pushing it to my live validator. So that if you do hit one of these edge case bugs, it's totally a non issue.
00:21:20.028 - 00:21:26.864, Speaker C: You can just report it, roll back if you need to, and you're helping move the cluster forward.
00:21:29.804 - 00:21:51.084, Speaker B: Yeah, that's a great point. Running different versions on your primary and secondary is another form of redundancy. Cause if there's a problem with one software version, having another one ready to go can be helpful. It is more management load, it is something, it introduces more possibilities from mistakes or whatever. But if you're doing in a very conscious manner, like Blake is suggesting, I think it can work really well.
00:21:53.424 - 00:22:04.984, Speaker A: I'm curious, Blake or others have you run into issues where running a version on the secondary that's not staked and then switching over and staking that secondary, you hit a different code path, found an error.
00:22:08.924 - 00:22:35.650, Speaker B: Not a long time, when there was some new when votes aid update happened. And then Ashwin at Solana Labs made a couple more changes. He made some changes where he was trying. The secondary now will watch the I don't know if you're all aware of this, but the tower is stored locally in a file. But everybody's vote state is also stored in a vote account. That's what your vote account is. It's like the same stuff that's more or less in your tower.
00:22:35.650 - 00:23:10.164, Speaker B: You store locally a copy in the tower that's more up to date that your value knows everything it's done. What's in the vote, you know, account is the stuff that's landed on chain that came out of that. And so Ashwin implemented something where it's like, well, if you don't have a tower, it's better to at least get what you can out of the vote state. So if I'm the secondary, I'm kind of constantly watching. If I get swapped over to and I don't have any state at all, I try to pull it from, you know, what's on chain. And that had a couple bugs at the beginning that caused when he first introduced this. Maybe it was eight months ago that caused a couple crashes for me, but that hasn't happened in eight months.
00:23:20.264 - 00:24:17.372, Speaker A: Any other thoughts or questions people have? Maybe just a like show of hands or you know, in the chat? Are there other people here that have run hot spares? I'm just curious what the kind of the breakdown of the room is. Looks like. Not. Yeah, I'd encourage to experiment with it, even if it's cost prohibitive. Right now it's, you know, knowing how it works and being comfortable with it, I think is useful. Yeah, yeah. Zen Tetsu is planning to post some scripts in discord, so be on the lookout for that, I assume.
00:24:17.388 - 00:24:48.204, Speaker B: And MB validators, I mean, my setup is so esoteric. I'm not. I mean, maybe you'll be able to look and see a couple hints, but I. No one's going to be able to copy it and use it because I have a bastion system that's between the two validators. It's very complicated setup and that's why it has to be so complicated. But one thing I did do is I put a script on both of them that lets me on that validator easily set it to either be voting or non voting with a couple different parameters to tell it. Expect a tower fire, don't expect a tower file.
00:24:48.204 - 00:25:23.856, Speaker B: And I can do that in case the primary is dead. I can just go over the secondary and say okay, become the voter now, please. But from my third system I have it scripted so that it basically goes it ssh is the primary says stop being the validator using the script, and then when it's done, copies the tower over and then Ssh is the secondary and says okay, here's the, here's the tower and I'll become voter. That's my setup and I run it from a third system that kind of talks to both. But some people have direct SSh access between, from one to the other. In fact, most people probably from one, from the primary to Ssh directly into the secondary or vice versa. And it's much simpler to just copy it over that way.
00:25:23.856 - 00:25:39.484, Speaker B: But I don't have that because I there, I don't know, maybe I'm too paranoid on security, but they can't even talk to each other that way. So my setup is more complicated and then many people would have to deal with. So I'm not sure my scripts are going to be that helpful, but I'll post them.
00:25:41.664 - 00:25:54.844, Speaker A: There's a question about hardware specs, which I think is a good one. Is your secondary less specs? Do you have a lower end machine for the secondary? And I guess why, if you do, mine are.
00:25:58.224 - 00:26:18.164, Speaker C: I don't know if I can really advise that though. So I have a strange setup that's kind of like Zan. And that's another reason why, because my scripts have evolved over time. So the docs are, the docs are the basic. Here's basically how it functions with set identity. It just gives people a basic explanation of how it happens. And that evolved over time.
00:26:18.164 - 00:26:49.554, Speaker C: Um, like that, my setup has evolved over time. So I have a couple servers that are intel servers, which are not recommended. They keep up just fine for the most part. Um, I use those as my hot spares. So your mileage may vary. In my experience you can get away with using not quite as strong as hardware because most of the time it's not you using know bandwidth, as much bandwidth. It's not actively participating in consensus.
00:26:49.554 - 00:27:01.034, Speaker C: But it's very hard to tell you what the specs for that would be. So I don't know that I can get a recommendation, but I do. For both of my hot spares, they don't have near as good a hardware as my primaries.
00:27:04.894 - 00:27:07.622, Speaker A: I have a question, I'll add there. Oh, sorry, go ahead.
00:27:07.718 - 00:27:34.454, Speaker D: Yeah, so if I had a cold spare, I guess, why not have a cold spare? Is there a lengthy process to get it updated or up to date before you make it primary. I don't understand, I'm new to this and haven't set one up. I'm just thinking out loud that if I had a cold is the reason I don't have cold and rather a hot, because it just takes so long to go get the latest state.
00:27:34.914 - 00:28:05.798, Speaker B: Well, one of the reasons is, I mean, you're increasing your operational load to have two validators. When a software update happens, you have to update them both. If you have monitoring, you have to monitor them both. There are many ways in which you're doubling your load. A cold is less load because if it just sits out there, like I said, if it's an AWS node, that's most of the time, not even on. But the problem is you don't want that thing to not be accessible or to be a problem the moment you need it. So you have to treat it for those intervals where you would do things to the secondary, such as upgraded software.
00:28:05.798 - 00:28:58.180, Speaker B: You have to spin the second up, secondary up upgraded software and shut it back down again. You have to like basically it's actually even probably more loads and keep that thing, you know, up to date because there's an extra spin up and shutdown involved all the time. And you're basically doing that just so you don't have to pay for it while, you know, it's otherwise would be sitting there idle. And it's not going to be that helpful if there's a crash on the primary, it's mostly just a bridge during a software upgrade, because if there's crash on the primary and it's going to take you ten minutes to get your secondary started up and you don't have a snapshot and maybe you forgot to update the software version or you forgot whatever, you're unlikely to be just this instantaneous swap over situation, which is what saves you from having a long delinquency. So there are many ways in which having a secondary and even is extra load and a cold secondary I think was even more load. But the problem. But it just saves you money if you don't want to pay for a fully running secondary all the time.
00:28:58.180 - 00:29:07.604, Speaker B: And so I think the only reason not to have it is you don't want to pay for it and you don't want to do it. There's no other reason not to have it. And I think it's worth the time and the money personally.
00:29:09.664 - 00:29:10.444, Speaker A: Yep.
00:29:11.904 - 00:29:17.192, Speaker D: I was thinking like planned maintenance. It does make sense, but I, you know, the unplanned is what you have to plan for.
00:29:17.248 - 00:29:17.584, Speaker A: Yeah.
00:29:17.664 - 00:29:35.794, Speaker C: That's where you'd get burned for sure. Yeah, for, I agree. For planned maintenance, a cold spare sounds like a great idea for the. Yeah, for bugs. Anything that's encountered where your validator goes down for any reason, it could be a problem with the data center. Could be you hit a code path that, that caused it to. To halt.
00:29:35.794 - 00:30:01.666, Speaker C: That's when the hot spare really pays off. And I will add another thing that a couple people, including myself, have been caught out by is that, like zan said, when you're using a hot spare for mainnet, it's basically a main net validator. Because I've seen a couple of people do this, and I did it at first too. You have zero monitoring for your hot spare. You're like, ah, it's just a hot spare. And then it dies at some point, you don't know. So then when you finally need it, you, since you weren't monitoring it, it's totally dead.
00:30:01.666 - 00:30:16.334, Speaker C: And then you get burned by that too. So you really do have to treat it as, like, I have, uh, the exact same alerting on my hot spare as I do on my primary. Because, I mean, if you think about it, it's, it basically could be a primary at any time.
00:30:18.314 - 00:30:55.540, Speaker B: One other side benefit, and this is really fringe. I had a couple crashes that I reported that were real Solana labs bugs that, because my primary and secondary were configured the same or running the same, they both crashed on the exact same slot. So that was a very clear indicator that there was a real problem. Whereas if you, if you only have one machine and it crashes, you're like, I don't know, maybe it was a hardware fault. Maybe it was, you know, who knows what caused it? Maybe it was something specific to the system. But it's almost impossible from a, you know, from a probability perspective that two validators that are completely separate crash on the same slot, unless there really is a problem at that slot with that configuration. So that's a fringe benefit.
00:30:55.540 - 00:31:10.564, Speaker B: It can help sometimes in diagnosing or debugging issues, although it sure sucked to have them both die at the same time. That was once again where I was like, oh, my God, I spent all this work setting this up, and now, like, they're both crashed. That just sucked.
00:31:17.904 - 00:31:20.324, Speaker A: Any other thoughts or questions for the group?
00:31:25.104 - 00:32:10.914, Speaker B: Don't get confused by the ETCD thing. A lot of people will talk about people who start researching this when we do Google search or whatever, and they'll find the ETCD instructions, which was an early attempt at having this where people would run this secondary network of this ETCD server that communicates with multiple nodes and tries to keep this tower in sync between all of them. And the problem with it is it would gate voting on having an upgraded, making sure the tower was distributed out to everybody, which slowed down the primary's voting a lot to the extent where the latency was so bad that people like, couldn't really effectively use it. And maybe someone could explore how to make that work someday. But in the short term I wouldn't recommend even trying it. Make sure you use the set identity method for, you know, for this, not the ETCD method.
00:32:18.374 - 00:32:37.034, Speaker A: I put the docs there in the chat just for completeness. Those are the docs that Zentetsu is referring to. And as far as I know, no one is using that setup should probably just be removed from the docs or updated to be more in sync with what people are actually doing. In practice.
00:32:42.254 - 00:33:33.184, Speaker B: I think that can work. I talked to someone, a friend of mine who's a little bit more of an expert in these distributed systems and eTCD, and he said it's not really a good idea to gate every vote on that. Instead it should be like, you know, sort of like sync every five minutes or something like that. And have the secondaries understand that if they have to restart and they know they haven't gotten synced in five minutes, then they just have to wait, you know, until the danger window is over. But you've reduced, like, you've allowed ETCD to sort of like do its thing and work really well. Most of the time you just have introduced a potential, you know, area where, like, you may have a problem, but most of the time it'll work fine because it won't, it will only sync very, very infrequently. But to be honest, I think we'll probably never, that's probably just a dead branch and it's never going to go anywhere because the set identity just works, so why not just use it?
00:33:34.124 - 00:33:34.940, Speaker A: Yeah.
00:33:35.132 - 00:34:27.991, Speaker B: Another problem with set identity is it's very hard to automate. And I suppose another question that comes up a lot is people are like, well, can I automate this? Can I just like, have it? So I have multiple validators and they all just know which one should be voting and just swap to the right one at the right time. And I don't even have to manage it and I can go on vacation and not even think about it because, you know, if I problem, they'll just, you know, figure it out amongst themselves. That is very, very hard because you know, the situations in which a validator cannot be accessed. You know, if you can't talk to a validator over the network, you can't know if it's still voting and, you know, in some network direction that you can't see that the votes are out there and hitting places and you can't assume that it's dead. And so it's very hard to do. Like, even when you do it yourself in these problem situations, you have to make judgment calls about what's the least likely thing to do that's going to cause harm here.
00:34:27.991 - 00:34:52.510, Speaker B: And to try to automate that is very, very, very difficult. I've tried it and I've given up. I've tried it a couple times and given up every time because I always work my way into education. I'm like, I have no idea how to make this work reliably. So if someone comes up with a really great proposal for how to automate this, that would be fantastic. But I have never seen anyone automate it successfully. And I know that there are edge cases that would be very hard to make.
00:34:52.510 - 00:35:11.314, Speaker B: Right, because you're basically like trying to achieve consensus among a set about who's the voter, which is the form of consensus in and of itself. And so now you're subject to all the same problems of like, you know, of a blockchain and trying to achieve consensus, but in a more smaller environment. And it's really hard to get it right.
00:35:14.794 - 00:35:21.894, Speaker D: Appreciate the insight, Dan. It's months of pain that I feel like it's going to really be helpful, for me, at least. Thank you.
00:35:27.354 - 00:35:36.734, Speaker A: Great. Yeah, I appreciate all the insight. It seems like questions are dying down, so unless people have questions, we might end it there.
00:35:40.654 - 00:35:51.274, Speaker D: Tim, I had a question in general, been watching all of your videos here in a short amount of time. Are the source video, are the source presentations available anywhere?
00:35:52.374 - 00:35:58.486, Speaker A: I have them. I'm uncertain how much value it would have to post them, but I can if you think it's useful.
00:35:58.590 - 00:36:19.634, Speaker D: The biggest thing often is you'd copy the links into the chat and those are not available for us watching later, either if we put them in the comments or like. I was just thinking, if I had the source, I'd see your links right there on the source. That was the only thing, I guess. Biggest thing I found was just having to screenshot the link and type them in.
00:36:20.854 - 00:36:29.678, Speaker A: Yeah, I'll double check all the videos to see what makes sense. Usually I put like an important link section in the notes for the video, but I might miss some.
00:36:29.766 - 00:36:31.340, Speaker D: Yeah, there were some. You're right.
00:36:31.502 - 00:36:32.564, Speaker B: Appreciate it.
00:36:33.744 - 00:36:39.324, Speaker A: Yeah, no problem. I can go back and make sure those are all up to date.
00:36:42.704 - 00:36:44.684, Speaker D: Who wants to go restart a cluster?
00:36:46.744 - 00:37:02.686, Speaker A: Yes. All right. I think that's all. Thanks, everyone. We'll have another educational workshop in about a month. If people have topics they'd like to discuss, let me know. Love to get feedback from community on topics people are interested in.
00:37:02.686 - 00:37:11.614, Speaker A: So please ping me in discord or in the validator calls or wherever you think is appropriate. Thanks.
