00:00:04.520 - 00:00:14.057, Speaker A: Hello everyone, I'm Tim Garcia, validator relations lead for the Solana Foundation. I'm joined today by Michael from the Fire Dancer team. And today we're going to talk about Fire Dancer. So Michael, what is Fire Dancer?
00:00:14.241 - 00:00:45.667, Speaker B: Yeah, so at a high level, Fire Dancer is a new validator client for the Solana blockchain. The validator client is a software that runs on a bunch of machines distributed around the world. They talk to each other over the Internet. And the validator software is doing two primary things. It's occasionally getting a chance to be leader, which means it's receiving transactions from users, producing a block and broadcasting it out to the rest of the machines. And then most of the time it's in the state where it's a replay. So it's receiving these blocks from the machines that were leader and verifying that the work that the leader did was correct.
00:00:45.771 - 00:00:47.419, Speaker A: And what are the goals of Fire Dancer?
00:00:47.587 - 00:01:11.387, Speaker B: Yeah, so there's three high level goals, no particular order. We have security independence and performance. Security is about making sure that the blockchain is secure. So this software is running on, you know, 3,000 plus machines. It's responsible for a lot of important functions of the network. It's taking transactions from users, verifying that the signature is correct. It's talking to all these other validator nodes.
00:01:11.387 - 00:01:45.573, Speaker B: It's performing consensus and making sure we agree with all the other nodes to keep the network moving forward. So it's a target for attackers. If they can get onto these validated machines, they could attempt to do a bunch of different things. They could try and forge a transaction or accept transactions with signatures that aren't valid. They could try and steal the validator's identity key and drain the funds from the validator, or they could just try and mark with consensus, cause the network to halt. And so finance are really a very important goal for us is to prevent any of these attacks from happening. We take a very unique approach to security.
00:01:45.573 - 00:02:09.171, Speaker B: We have a very, very strict security sandbox. And so that's been a really a strong focus for us. Number two is performance and it's probably the primary reason for building Financer. The performance of the network. Very important. The more transactions users can send, you know, the faster the network is going fire. We're aiming to be on the same hardware as an existing agave validator, about two orders of magnitude faster.
00:02:09.171 - 00:03:08.543, Speaker B: And so if the network were able to process, let's say 6,000 transactions a second today what we're talking about is a few hundred thousand transactions a second, maybe a Million. So finance is built by a team with a lot of low latency trading specialization. And so the team is really coming at this problem with a lot of experience and trying to bring all that experience to be on this problem of how do we make Solana faster? How do we make the performance of the validators faster and have the network running faster without requiring any upgrades to the hardware, without requiring specialized data centers? The third thing we talked about, the reliability and independence, especially independence means having multiple validator clients. Essentially, this is a very important goal for the blockchain. If you have one validator client, you're susceptible to certain kind of problems. A bug in that validator client, a protocol deviation could occur on all of the nodes at once and bring the network down. And so if you have two validated clients, if one of them has a bug that causes those nodes to go down, the other nodes may keep running.
00:03:08.543 - 00:03:55.403, Speaker B: And so having multiple validated clients running at the same time can increase the reliability of the network. And then in terms of independence, this a little bit ties back to security. It's about preventing certain kinds of attacks. So one attack that's a little bit pertinent lately is a supply chain attack where an attacker tries to compromise parts of the build tool chain or parts of the dependencies that the program relies on. Solana uses a lot of different cryptographic libraries and things for the protocol. And so if an attacker can get access to one of those dependencies and insert some code there, it may be that sometime later, when Solana or the Agave validator takes an update of that code, that they're not compromised. And so for Financer, one of our really key goals was to use a different language, to use an entirely new set of dependencies, to use a different build tool chain in finance.
00:03:55.403 - 00:04:02.555, Speaker B: So we actually take a goal of minimal dependencies. So we've pretty much rewritten almost all of the stack from scratch. Yeah.
00:04:02.675 - 00:04:10.643, Speaker A: And today we're going to talk about Fire Dancer V0, lovingly named Frankendancer. So talk a little bit about Frankendancer and kind of how that's architected.
00:04:10.779 - 00:04:33.809, Speaker B: Yeah. So Frankendancer is kind of a stepping stone to a full flight answer. The Solana protocol is quite complicated. There's many different components. There's this entire stack when you are the leader to receive transactions from users, process them, put them into a block. There's this entire stack when you're replaying, to receive blocks from other validators to verify that what they did was correct. There's repair.
00:04:33.809 - 00:04:59.827, Speaker B: You need to talk to other nodes in case you missed out on some data or you're trying to catch up to the network. You can ask them for data, there's gossip. You need to talk to all these other nodes on the network and say, where are you? How do I send transactions to you? How do I receive data from you? And that's just a handful. There's many, many more. And so reimplementing all of this at once would be a pretty big undertaking. So Franken answer is a stepping stone to a full replacement of Agave where we're just reimplementing one part. It's the pipeline when you're the leader.
00:04:59.827 - 00:05:31.573, Speaker B: So it's the part that receives transactions from users, decides what to put into a block, produces a block and broadcasts it out to the rest of the network. The reason we've picked that part to replace. You can kind of assume that when someone is not the leader that replaying the work that the leader did is a little bit faster than how fast we could produce the work. And so it's possible the chain could see some benefit from FITENSW immediately if we're able to produce blocks a little bit quicker and put a little more information in them. So that's currently been the focus of Frank and Answer.
00:05:31.669 - 00:05:43.265, Speaker A: Yeah. So at a high level, can we break down on the board what parts are the TPU and what parts are still agave? Or maybe a way to rephrase that is what parts are firedancer v0 what parts are still agave?
00:05:43.845 - 00:06:00.295, Speaker B: Yeah. So Firedancer, at a high level, you can kind of say there's this system here. Right. This is the Firedancer process. Right. So Validator launches this on their machine. And the first thing to note is when you run Fire, you're also running Agave.
00:06:00.295 - 00:06:22.395, Speaker B: So inside this Firelancer process. Well, actually, it's a separate process, but we spawn it for you as a child. There is Agave. And Agave here is running with a lot of the existing functionality that it has. You know, one of these things is gossip. So this is the part that talks to all the other machines on the network and figures out where they're located. One of them is Replay.
00:06:22.395 - 00:06:58.463, Speaker B: So what Replay is doing is when some other node on the network is the leader, it's receiving the data from them and verifying to make sure it was correct. There's a bunch of other functionality here. Repair. Actually, it's easy to describe what's happening in Agave as everything that we haven't implemented for Frank and Answer or Finance of V01. So the part that's implemented here is this so called leader tpu. And this is just what happens when we're leader. And if you imagine, well, what would a node be doing when it's leader? Conceptually it's kind of simple.
00:06:58.463 - 00:07:41.721, Speaker B: There's a whole network of, we call them RPC nodes, but these are kind of people on the Internet that are receiving user transactions. So I'm running a box somewhere, a user says, I want to transfer this amount of Sol from A to B. They send this to some known computer that they're talking to. Or maybe they click a button on a web browser and it goes to one of these nodes. Those nodes look at the network and they say, hey, who is the current leader or who is about to be leader? And they kind of format this transaction in a special way and they send it to them over the Internet. So this leader in the lead up to their leader slot, so for a few seconds before they become leader is receiving a lot of these kind of transactions. And then they're gonna put them through a little pipeline here of work.
00:07:41.721 - 00:08:19.433, Speaker B: So at the very first layer of this pipeline is what we would call quic. This is a network protocol that talks between this RPC server and the leader and just receives a transaction so we can get them onto the leader's machine. This is a very important part of the pipeline cause it's very performance sensitive, right. If let's say there's a token mint and a lot of people are trying to generate transactions really quickly, they're gonna be spamming this. So the performance of this is particularly important. Once we receive transactions from quic, they go into this next step called signature verification or verify. Transactions can be invalid.
00:08:19.433 - 00:09:00.591, Speaker B: Right? I may want to be sending someone Solana tokens that I don't actually own. So signature verification is how we check that the transaction is signed by its rightful owner and actually valid to be processed by the network. From there we go to a step called Dedupe, which filters out duplicate incoming transactions. Often users will send the same transaction many times in a row to try and get it on the chain a little quicker. So it's important to filter those out. From Dedupe we have a step called Pack. So what Pack is doing is it's taking all of these incoming transactions and putting them in a software buffer, basically a long list of the transactions.
00:09:00.591 - 00:09:39.799, Speaker B: And in some way it's ranking them by how much we want to execute them. For the validator operator, they get fees for executing transactions. And so they want to execute transactions that are likely to generate the most fees for them. Those transactions are going to be kind of weighted between two variables. There's transactions that have a high fee, but they're very expensive to run, and there's transactions that have a low fee but are very cheap to run. The transactions you generally want to pack are those that have a high fee but are cheap to run. So this pack piece of infrastructure is kind of looking at all the transactions and ranking them by, you know, how easy will it be to run this transaction versus how many fees will I generate?
00:09:39.927 - 00:09:44.671, Speaker A: And just for clarity there, when you say cheap to run or expensive to run, you mean in terms of computing time, right?
00:09:44.703 - 00:10:29.705, Speaker B: And computing resources, right? Yeah, essentially in terms of wall clock time. To run on the machine, our leader has to produce a block within 400 milliseconds. And so it's really just how many of these transactions can we fit into that 400 millisecond slice of time? So this tile is, this piece of code is determining those transactions. And then once it's kind of ordered them and decided they're ready to execute, it sends them to something called a bank. This is really just an executor. And so this is receiving a lot of transactions in the order it should be executing them and it actually runs the instructions in the transaction. So Solana has this virtual machine, right? The users that are sending transactions aren't just sending us something saying transfer from A to B.
00:10:29.705 - 00:11:21.165, Speaker B: They could be running somewhat arbitrary code and instructions to do multi signature verification and things like that. So these banks actually execute those transactions and then from there they send them to this component can extend that box here called POH or Proof of History. This is an interesting part of the Solana network. It's a little detailed to get into, but at a high level, this is proving that our validator has been making forward progress. You know, as we generate these transactions out of pack and execute them, this proof of history tile is constantly stamping these transactions that are being executed with a hash. And periodically it kind of re stamps the new hash. And this is important so that other people that are replaying the block can verify that our work was done correctly.
00:11:21.165 - 00:11:56.355, Speaker B: If we have time, we can get into that. So once we get done with proof of history, we come to this tile called Shred. Shred is responsible for taking the output of the banks so the results of the executed transactions and distributing those results to the rest of the network. So Shred Tile implements Turbine, as it's called. This is Solana Protocol for determining how the data gets sent to the network. The Solana network we mentioned, there's 3,000 nodes, so it'd be a lot for this one validated machine to send the data to all of those nodes so it gets fanned out in the network. So this red tile is deciding where it goes.
00:11:56.355 - 00:12:12.695, Speaker B: And finally, there's a storage component. So once we've sent the data to the network, we also send it locally to the hard drive or the SSD or memory, so that it's stored permanently. And if the validator crashes or needs to be restarted, then the data's there, ready to be used.
00:12:12.995 - 00:12:28.595, Speaker A: Okay, great. So, yeah, it sounds like this is a good overview of the leader pipeline and places where fire v0 takes over instead of Agave. Maybe taking a step back, we could talk a little bit about how Firedancer does things differently than Agave. What's different about the implementation?
00:12:28.755 - 00:12:58.669, Speaker B: Yeah, so at a high level, there's a bunch of interesting things about finance, especially when we think about the lineage of it. So the team that's building Financer is very specialized at building low latency, high throughput systems. Some of the team has a background in the trading space. And so if you look at the trading space, I think it's one area that's probably different than a lot of software development, where it's. I think traditional startups take this approach that it's better to ship first. Right. You want to ship first and optimize later.
00:12:58.669 - 00:13:20.711, Speaker B: It's enough to have some code doing, anything at all. And then once you find, you know, market fit, you can make it faster in the trading space. It doesn't really work that way. If you build as quickly as you can a product and you start running it, you just build a product that's going to lose money really quickly. And so the goal with these systems is to design them from the start to be able to execute as fast as possible. Right. We have to actually be very intentional with the design.
00:13:20.711 - 00:13:46.175, Speaker B: And so some of the things that are different if you want to completely utilize the machine. Remember, we're trying to execute two orders of magnitude faster than the existing client on the same hardware. We need to be very, very careful with our design choices. And so a few things that are interesting, we don't really use threads. Right. The second you use a thread, the operating system is now deciding, okay, I'm going to schedule a thread here, I'm going to schedule a thread there. You've lost control of where your work is running on the computer.
00:13:46.175 - 00:14:01.735, Speaker B: So Firedance is very explicit. We assign one thread to each CPU core. Firedancer doesn't allocate memory. Right. The second you call Malloc, okay, now you're in, you know, the kernel memory manager. Some pages are getting faulted in. Think about it.
00:14:01.735 - 00:14:45.903, Speaker B: If, you know, for example, if we were using threads and the kernel decided to context switch our thread out for some other thread on the machine, that context switch maybe takes 10 milliseconds, 20 milliseconds. If we're trying to execute a million transactions a second and we get swapped out for 10 milliseconds, we're now doing 980,000 a second. Right? And then the cache got invalidated, all these other instructions came in from some other thread. And so it's actually a pretty high cost. And so we need to ensure that these things that I think a lot of programs take for granted aren't happening in finance. One other thing we do that's kind of interesting, we think of the CPU of the machine as a distributed network of computers, basically. Right? So let's say you have your CPU here and.
00:14:45.903 - 00:15:23.831, Speaker B: And it's 32 cores. Really. What you have here, if you want to use this machine optimally, is you have a network, some local area network connecting 32 distinct pieces of hardware. And Fireancer kind of takes this approach of treating each of these cores as a little microservice. So we run a separate task on each core, and then they communicate with each other by sending data from core to core. So this is sympathetic to the hardware. So some of the types of tasks we would run, for example, signature verification, each of these steps is actually assigned to a part of the CPU core.
00:15:23.831 - 00:15:37.239, Speaker B: And then we're very intentional about what data flows over the CPU chip and what data is moving on the network. Managing this data flow is actually more important for us really, than managing like, how busy a CPU core is or how much any individual core is utilized.
00:15:37.407 - 00:15:49.833, Speaker A: Okay, really interesting. And maybe for layman terms for just a second, the CPU cores here, this is, let's say I buy my intel, whatever, and it comes with consumer grade hardware. Probably like six cores, right?
00:15:49.849 - 00:15:50.545, Speaker B: Yeah, yeah.
00:15:50.585 - 00:15:59.481, Speaker A: So you're essentially splitting that chip up into six different mini CPUs, and that's combined into one chip that people use.
00:15:59.513 - 00:16:00.177, Speaker B: Right? So that's.
00:16:00.281 - 00:16:06.813, Speaker A: Yeah, okay, sounds good. Let's talk a little bit about the architecture of fire dancer v0 and what a tile is.
00:16:06.889 - 00:16:38.015, Speaker B: Mm. Yeah. So we talked a little bit about. We have all of these independent pieces of work that are getting assigned to cors, and we Actually kind of laid out here exactly the different kinds of work we have. So if you think about these as microservices that are different pieces of code running on each of the cores inside your cpu financer takes each of these pieces of work we've decided here and assigns them onto these cores in an optimal way. So let's say there's 32 of them. We might say we want two of them running this QUIC service.
00:16:38.015 - 00:17:24.087, Speaker B: So we assign QUIC to these first two cores and we might want actually in practice we want a lot of them running. Verify. Signature verification is a really expensive operation, something on the order of like 20 to 100 microseconds per signature. And it's also free for users in the sense that if a user sends us a transaction that fails to verify, they don't get charged because we don't have an address we can take SOL from for them. And so if someone is spamming a lot of fake transactions, typically we're going to see signature verification fail. But we still had to go and do this cryptographic operation to verify it. So this is an area where we want to assign a lot of cores firelance that can handle about 50,000 signature verifications a second on a modern Intel CPU core.
00:17:24.087 - 00:17:46.995, Speaker B: So let's say we had six of these. We could be handling about 300,000 transactions a second. And so we kind of go on like this through all of these different pieces of work. Right. Deduplication actually this is really, really fast. We just compute a hash of the transaction. So this only runs on one core pack by its design has to run on one core because it's kind of looking at all of the transactions.
00:17:46.995 - 00:17:59.205, Speaker B: So it has to have a view of everything. These executors though can be quite parallel. Again the same as verification in practice. You might want quite a lot of these executors that are executing non conflicting transactions.
00:17:59.325 - 00:17:59.797, Speaker A: Interesting.
00:17:59.861 - 00:18:10.677, Speaker B: So if transactions are writing to the same account, they probably can't execute at the same time. But if we are able to find independent sets of transactions to execute at the same time, finance will do that. Cool.
00:18:10.821 - 00:18:19.503, Speaker A: Is there a difference there in the way Agave is implemented? I know they have four threads running and essentially four banks. Is that similar here?
00:18:19.599 - 00:18:45.647, Speaker B: Yeah. So Agave is somewhat similar to finance and actually the finance TPU pipeline is a little bit inspired by Agave. So Agave implements something of a similar work structure. They have a quick implementation that they're using that runs on some threads. They have signature verification threads. The difference I think there's A few things. So one, finance is very intentional about the threads and where they run.
00:18:45.647 - 00:19:12.475, Speaker B: Agave I would call something of a thread soup. Charitably, if you attach a debugger to Agave and list all the threads, there's probably more than 1,000. And there's really no intentionality behind where or when they run. It's all left up to the kernel. And so the kernel may just decide randomly, oh, we don't want to run what an Agave has called this unprocessed transaction storage. This is how they determine what to schedule on the banks. The colon may just unschedule that thread because.
00:19:12.475 - 00:19:47.135, Speaker B: Because it decided to do some other work. And then the entire Agave pipeline stores Agave. Also, the way they communicate between these, between their threads is very different to Fire Lance. So we use a very high performance, low latency design that's very tied to the CPU architecture you're on. It's based on sending these small cache lines we call fragments between the CPU cores. Whereas Agave kind of sometimes just copies whole data packets between cores. Sometimes it's, you know, using primitives that aren't very sympathetic to the cpu.
00:19:47.135 - 00:20:46.593, Speaker B: Sometimes if there's no data, for example, Agave will do basically asleep inside the operating system kernel. So if this channel, let's say, between their signature verification and what's deciding what to execute, if there's no data incoming, they'll spin for a little bit saying is there new data? Is there new data? But then eventually they'll go to sleep. So what that means is they call into the Linux kernel this syscall called futext, which is basically wait until a certain value changes indicating this new data. We talked about a few things that are no no's in file answer having threads being one allocating memory, another one is making system calls. If you make a system call in the Linux kernel, you have a huge overhead. All of a sudden, right? This is unacceptable for us. And so whereas Agave is going to be wasting a lot of time doing these sleeps and wakes and blocking inside the kernel finance, we just spin all of our threads and because we know the exact work they're doing, we know we can have them running on that core fully utilized.
00:20:46.593 - 00:21:17.027, Speaker B: We never essentially never need to enter the kernel. And so all these little small differences add up. And so even the pipeline looks somewhat similar. The final answer one is I would say a lot more performant. And then parts of the structure of the pipeline are very, very different between firearms and Agave. So one of the main things that's Interesting to look at. That's different in Agave, these kind of three steps here are implemented kind of with locking.
00:21:17.027 - 00:21:57.291, Speaker B: And so locking for high performance is another big no, no. Right. If you lock something, you've prevented any other thread or any other worker on the system from using that data at the same time. Financiers, essentially lockless. We don't use locks, we just have message passing between all of these parameters. Agave, when they're implementing this pack and bank situation, the way this kind of works is there's a pool of transactions and they have four banking threads. Each of those threads is running in a loop, constantly saying, oh, am I done processing my last transactions? Let me lock, essentially, let me lock this list of transactions and take some data out of it that I want to execute and let me check that these transactions are available to me.
00:21:57.291 - 00:22:17.895, Speaker B: So if two of their banks are awake at the same time trying to pick new transactions to execute, they might pick the same transactions and try to lock that transaction's accounts to say they can execute it. So this is incredibly slow. It moves data between cores on the system, it causes these tiles to block and be doing nothing. So there's a huge difference there as well.
00:22:18.355 - 00:22:27.593, Speaker A: Maybe just one level down for us laymans. How do you, how do you handle that problem if you don't have locking? Is it. Yeah, go ahead, I'll explain.
00:22:27.689 - 00:23:03.855, Speaker B: Yeah. And so really, you restructure the system to model the data flow more accurately. Right. So in finance, that's why we've introduced this separate packing stage, so our data actually flows into the system. It comes to pack and then pack decides what should execute where. So it's receiving a stream of transactions and it's looking at them and saying, okay, these 10 transactions can go on this banking tile, this banking thread. If we find 10 other transactions that can execute without conflicting with this first set, we send them to this second thread.
00:23:03.855 - 00:23:59.485, Speaker B: So by having this additional core spinning, doing this work, we can kind of ensure that these cores never need to take a lock. They never need to say, oh, is another thread trying to use the same accounts? Because it's already been guaranteed for them by this dispatcher of work. So in Agave, this dispatcher kind of doesn't exist. Or maybe it's been implemented recently. But so if you imagine you didn't have this component, this pack component, and after signature verification, all of the transactions got sent into this queue, and this queue is just kind of managed by a bunch of locks. And so all of these threads have to take this locked, you know to pull transactions out or say, is anyone else using these transactions? So it's kind of this a way of redesigning the system. Right? One other way we remodel, the data flow is when transactions go from these banks to be, so called, stamped, they have a hash computed from the transactions.
00:23:59.485 - 00:24:38.925, Speaker B: In Agave, let's say you have your verification here, you have your bank executing transactions here, and you have, we'll call it Shred, which is Turbine. It's sending data out on the network. In Agave, they model this part as a pipeline. So data flows pretty cleanly between these. But proof of history is basically implemented as a lock. So proof of history has to be done sequentially. Even though you have a bunch of banks, they all have to order their transactions one by one to make sure that the hashes are sequential between the groups of transactions.
00:24:38.925 - 00:25:34.097, Speaker B: So in Agave, when you're done executing a group of transactions, the validator actually goes out to this special proof of history component and it takes a lock on it and says, hey, will you compute the hash of these transactions and send them back? So you can imagine if you have a bunch of banks doing this, they're all going to get in line, right? And if they all want to compute a proof of history at the same time, they're completely blocked from doing any other work. They can't receive new transactions because they're essentially stalled. Not only that, but as we talked about, well, we have to do a memory allocation to get these transactions across and the operating system may decide, oh, that memory doesn't exist. Let me fault it in. And when we gotta take this lock, if it's already taken, we're gonna spin for a second and as we talked about, then we're gonna enter the kernel to wait. And so all of this costs are kind of combining together and adding up in a way that can be pretty disastrous for performance. So fired.
00:25:34.097 - 00:26:14.695, Speaker B: Answer. We actually take pains to not have any data flow like this, where data moves back and forth between the same components. Everything is sequential. And so when banks are done executing transactions, they just send them onto this new service that's responsible for stamping those groups of transactions with a hash and then sending it onto the network to be distributed. You might say, well, that's pretty straightforward. Why doesn't Agave do that? There's actually a lot of technical gymnastics involved in doing this. One thing that is interesting, when the bank is done executing the transactions, it has to commit the results, which means it has to say, okay, these account balances are now changed from A to B, I have to save those balances.
00:26:14.695 - 00:26:50.537, Speaker B: But if you then send the transactions over here and they fail to commit the hash in this proof of history, for some reason, you know you're in a tricky situation because you've already saved the results of executing them and now your database is wrong and do you go and roll it back then? That requires going backwards in time. So Agave takes this approach where they wait to make sure that the hash could be computed successfully before they have the data committed to the accounts database. That's why they need to do this logging. So Finance goes to, I would say, almost pain to avoid this situation. So we've made this operation infallible. It can't fail.
00:26:50.641 - 00:26:53.505, Speaker A: What were the cases where they were failing before Engabe?
00:26:53.625 - 00:27:28.877, Speaker B: So proof of history is kind of the timekeeper of the network. It makes sure that you're only taking 400 milliseconds to execute your block. And so the main case where this operation would fail is if executing the transactions took so long in this banking tile that by the time you went to compute the hash, your 400 milliseconds had expired. And now you have to throw that work away. That's the main reason things can fail. There's kind of a couple others, but this is really the main one. And so in finance, we prevent that by we essentially slow down time a little bit.
00:27:28.877 - 00:27:48.115, Speaker B: As we kind of reach towards the end of the 400 milliseconds, we keep ticking slower and slower, but we never quite reach it because we know that banks are still executing. So that's one way we make sure that this operation can't fail is by ensuring there's always enough time left to be able to mix in the results of these operations.
00:27:48.535 - 00:27:58.231, Speaker A: Makes sense. And then is there more sort of differences on the shred and store parts or is it fairly similar operations there?
00:27:58.303 - 00:28:37.071, Speaker B: Yeah, and I would add that we say, well, Agave has somewhat similar pipeline. All of the parts of this pipeline are implemented from scratch in Firedance. So signature verification is an interesting example. Agave has a signature verification code. I think it's importing a standard cryptographic library to do this. Flightancer has a custom written from scratch AVX512 Implementation of signature verification that runs probably an order of magnitude faster than the agave one. So we can handle 50,000 transactions per second per CPU core dedicated to verification, whereas Agave is much, much less.
00:28:37.071 - 00:29:10.381, Speaker B: And this is generally true throughout our pipeline. So we've designed Financer from the beginning to be able to handle a million transactions a Second. So these components that can't be distributed across cores are written to be capable of a million transactions a second deduplication. Pretty straightforward. The pack component's interesting. Well, it has to be keeping these lists and ranking transactions and scheduling things onto all these downstream consumers and waiting to see when they're done executing. This component actually is written in a very, very high performance, low latency way, so it can handle that load million transactions a second.
00:29:10.381 - 00:29:47.265, Speaker B: And likewise with these other components. I mean shred we've written a custom read Solomon implementation can handle, I think, hundreds of gigabits a second, at least from the benchmarks we could find. It's the fastest read Solomon implementation that has been written. Store is an interesting one. Financer doesn't rewrite the data storage of the transactions. And so this part is the only part of this pipeline that still comes from agave code. But yeah, so in general, we've tried to make every single part of this pipeline as fast as possible, make all of the communication as fast as possible.
00:29:47.265 - 00:30:30.103, Speaker B: And when we look at where the bottlenecks are in here, there's really two places that are still utilizing agave code. So one is this storage, as we just mentioned. The other one is the banks. The bank infrastructure is written on top of finance primitives, so it communicates with these other parts of the system of a Financer. But the actual transaction execution, implementing the virtual machine, the runtime, like actually taking these transactions and saying what do all these instructions do? That part is also agave code. And so within these tiles we have a little bit of FFI that calls out to the agave sub process essentially. And so those two parts are somewhat the bottlenecks of the Finance system right now.
00:30:30.103 - 00:31:14.489, Speaker B: But if you kind of go through it and say, well, how do we hit our performance goals here? It's, you know, let's imagine you have a 32 core system, which is probably a typical of slightly slow hardware for an operator. Today we can look at the performance of each of these parts of the system that we've written. So our QUIC component can handle pretty over a million transactions a second. But it can also scale and it scales somewhat linearly. So as we add more CPU cores to quic, it can scale above this. But for our purposes right now the network is receiving much less transactions than this. So we say, okay, we have one quic tile Verify again on a modern intel core can handle about 50,000 a second.
00:31:14.489 - 00:31:41.805, Speaker B: So you know, we would need about 20 of these to do a million transactions a second. Deduplicating is very, very fast. We, I think we can handle a million transactions a second and only use about 10% of the CPU core for this deduplication operation. So we dedicate one core to that pack. We only need one core. Again, it's tuned to handle about a million transactions a second, not much more. This is a very, probably one of the hardest parts of the pipeline, again, because it can't scale.
00:31:41.805 - 00:32:30.565, Speaker B: And these banks are very interesting. In Agave, transaction execution is quite fast, but all of the bookkeeping around transaction execution is kind of slow. One thing that's involved in executing transactions is you have to save somewhere the result of the transaction and say that you actually executed it. So you can think of this. There's like a map from the hash of the transaction to a Boolean saying, like, was this transaction executed? And if you have 10 of these threads and they're all trying to lock this map at the same time, Agave uses a concurrent map, but it's actually still incredibly slow. So this we found to be a bottleneck in finance. If we use the Agave bookkeeping for the transaction execution, it limits US to around 2, 300,000 transactions a second.
00:32:30.565 - 00:32:43.161, Speaker B: But in benchmarking, we replace this with a custom map implementation and we can have these banks then run somewhat linearly and they execute 40 to 50,000 transactions a second as well.
00:32:43.313 - 00:32:44.553, Speaker A: Per bank or per bank.
00:32:44.609 - 00:33:21.151, Speaker B: Okay, so if we have 20 of these, we're also doing, assuming somewhat linear scaling, which we can get if we replace a little bit of the bookkeeping, proof of history, relatively quick as we've rewritten it, we just need one of these and then shred also one of these is enough to handle and transaction a second. The block store is pretty complicated. We use the Agave block store. It does not handle close to a million. I think it currently can do about 300,000 a second. You kind of only need this for certain. You need this to run on a live network.
00:33:21.151 - 00:33:59.191, Speaker B: But in benchmarking, you don't really need it because you don't need to store the data. You can kind of just have all the data in memory and keep the chain running. So if we run without actually storing any data on disk, we don't need any of these. If we do have to run with this, which we do in production currently, it limits us to about 300,000. But so if you kind of add all this up, you know, assuming we could have this data storage run a little bit faster, we have, you know, 20 tiles here, 20 CPU cores here, that's 40 plus, you know, 4, 5. So we're at 25. We have a few other parts of the system that aren't called out here that use a CPU core.
00:33:59.191 - 00:34:25.305, Speaker B: We actually have a dedicating networking layer which we can talk about a little bit as well. This is using a CPU core, we have some metrics infrastructure and we have a special security device that uses a CPU core called our signing tile. So we get to about 32 cores. And so on those 32 cores you can see we're able to execute about a million transactions to second. So that's kind of the design goal of Fire Answer.
00:34:25.685 - 00:34:38.025, Speaker A: Cool, then maybe that's a good segue to talk a little bit more about security. So you mentioned that security tile for verification. Is there other sort of architectural designs that Fire used to improve security?
00:34:38.445 - 00:35:22.279, Speaker B: Yeah, so fireancer is. It turns out that if you write something for performance, you get a lot of really, really nice security properties as well. And one example of that is we talked about how making syscalls is kind of a. No, no. If you look at the fired answer system from Quic, actually almost from the networking subsystem which uses direct mapped memory, so it doesn't make system calls, but from the network tile all the way to the shred tile, essentially we don't make syscalls. If you go and look at this verification tile, what's it doing? It's reading incoming memory. So this quick tile is writing transactions to memory somewhere.
00:35:22.279 - 00:36:11.855, Speaker B: This verification tile reads that memory, it does a bunch of CPU work to verify the signature, and then it writes it to some outgoing memory for this downstream CPU core to read. And so there's really no need to touch the operating system or the kernel at all here. And so when we say, well, these services don't make system calls for performance, it's also great for security. Linux has a facility called seccomp. So what that's doing is letting you blacklist system calls from your process. And so Linux has, I think somewhere in the order of 200 plus system calls. I believe Google Chrome originally pioneered this idea of, well, a lot of the vulnerabilities in software like that come from an attacker getting code execution and being able to then leverage a Linux system call to get root capability on the box and install something or.
00:36:11.855 - 00:36:50.519, Speaker B: And so what if we could sandbox these processes to not be able to make any system calls even if you got code execution? If you can't actually read or write a file, if you can't open a file, if you can't talk to the network. If you can't really essentially do anything, then even having code execution on the system, all you can do is read and write memory. You're pretty restricted from being able to actually affect the box in any way. And so file answer takes us to the extreme. This verification tile isn't allowed to make syscalls. It actually is allowed one, which is a logging related syscall. So it can write specifically to a log file that it's allowed to write to, and all it can do is append data there.
00:36:50.519 - 00:37:08.975, Speaker B: It can't do anything else. It can't open, can't talk to the network, it can't query what is the operating system. It can't. Right. And so it can't even generate random numbers, for example. And so this is a very secure sandbox. Even if an attack is able to get code execution on here, it's unlikely they would be able to leverage that to do much else.
00:37:08.975 - 00:37:58.159, Speaker B: This combines with another part of our performance architecture, which is that every piece of work, say verification or deduplication, happens on its own CPU core. So not only are these one thread per core, we've made it one process per core. So each of these pieces of work is its own process on the machine, and it gets its own security sandbox. So these processes can't talk to each other except over shared memory that they've mapped together. So for example, verification has a bunch of private data that it uses. It needs to know, it might need to know how many other verification tiles are there, so it can distribute work between them. This deduplication CPU core can't access the number of verification tiles, for example, it can't read any of that memory.
00:37:58.159 - 00:38:28.435, Speaker B: All it can read is the memory that we've allowed it to read. That is this channel that sits between these two pieces. And now what else is interesting here is this deduplicator gets its own security sandbox. So it's running in its own address space. It has its own list of allowed system calls, which turns out to be exactly the same ones as verification. This guy just needs to write to a log file as well. But so the entire system is actually comprised of a bunch of separate processes with separate security mechanisms that can't talk to each other except over memory that they're allowed to read and write to.
00:38:28.435 - 00:39:03.655, Speaker B: And the only system calls or ways that the system actually interacts with the operating system at all really are at the edges. The networking tile, I think, sometimes needs to call, send and receive to notify the kernel that there might be new data to read or write and networking buffers. So that's allowed. This Agave subsystem is kind of a wild west. Agave, because of its design, can't use a security sandboxing system like this. And so Agave really can do whatever it wants. So when we host this Agave piece inside, this is not inside of our sandbox.
00:39:03.655 - 00:39:12.215, Speaker B: So this, this sub process is allowed to make a lot of system calls. And you know, this storage component runs inside of the Agave address space.
00:39:12.595 - 00:39:15.975, Speaker A: Is there shared memory that Agave uses as well or is it?
00:39:16.515 - 00:40:07.931, Speaker B: Yeah, so we, when we do communicate with Agave, for example, when we are sending data to be stored on disk, this storage tile I drew in a bad way. But this CPU core lives inside of the Agave address space within this process and it does map. This memory that is being written by the CPU core gets mapped as an input memory here. So yeah, when we talk to Agave, it's also over the same channels. Makes sense. And so if you think about, well, let's say an attacker is able to create a corrupt packet that our parser doesn't accept properly and they get code execution at our networking protocol layer. What can they do? Well, they can create a transaction and send it to be verified, but unless they have the right signature, they're not actually going to be able to get it verified.
00:40:07.931 - 00:40:38.165, Speaker B: They can read and write memory on these input output buffers that have been set up with a networking device and send it, but that's not particularly useful either. They can't get access deeper in the system. They can't get access to the execution engine, they can't get access to the keys, for example. So this is another interesting thing to talk about with our security architecture. None of these parts of the system have access to the private key. They couldn't read it if they wanted to. They can't hopefully induce the kernel to read the key.
00:40:38.165 - 00:41:21.707, Speaker B: One of these cores we mentioned earlier that is kind of a weren't called out here is what we call assigning core. We have just one of these and it runs off to the side and it receives requests to sign payloads and signs them and returns them. So this CPU core has access to the private key, but all it has is one input channel and one output channel for the payload to be signed. And it verifies these payloads quite thoroughly. So there's only certain types of signing operation that are allowed, signing an outgoing gossip message or signing a shred to prove that we are the one that produced this data. So the shred tile, when it's ready to send data, will ask the signing tile to sign it. And the sign tile will check who is asking me to sign this.
00:41:21.707 - 00:41:39.377, Speaker B: Right. If the quick tile and attacker gets code execution here and says, hey, signing tile, can you sign this shred for going to the network? The signing tile is going to say, no, you don't have authority to do that. And so all of these roles are also petitioned in terms of what they can sign and how they can access the signing machinery.
00:41:39.571 - 00:41:43.837, Speaker A: And just for clarification there, when you say the private key, you're talking about the validator's identity key, right?
00:41:43.861 - 00:41:45.157, Speaker B: Yeah, the identity key.
00:41:45.261 - 00:41:46.025, Speaker A: Got it.
00:41:46.325 - 00:42:23.617, Speaker B: Cool. Yeah. And we contrast this with Agave, where all of this code is running in the same address space. So there's One process has 1,000 threads inside it. And so somewhere in this address space, probably in many places, because I imagine the key is copied, but in at least one part of this address space is a private key. And if an attacker can get code execution from any part of this system, they're gonna just be able to read this memory and then they can make system calls arbitrarily and send this key wherever they want. So Filencer, you know, I think in some ways doesn't have some of the security benefits that Agave does, because it's written in Rust.
00:42:23.617 - 00:42:34.847, Speaker B: You know, you can't have some simple buffer overruns and things like that. But in other ways, because of its design and its unique architecture is able to offer a lot of security protections that aren't available to the traditional Agave value.
00:42:34.991 - 00:42:45.515, Speaker A: All right, so that security overview sounds great. Could we talk a bit more about how Firedancer talks to Agave? You mentioned the shared channels with memory, but I'm sure there's other integration points.
00:42:46.415 - 00:43:19.023, Speaker B: Yeah, so this is an interesting one. When it came to kind of running the Agave code within flight. Answer. There are a few ways you could think that we would do this, right? One is take the parts of the Agave code base that we want to use and lift them them somehow into finance and call out with them. Probably the primary goal we had for this integration was to have it be extremely minimal. Anytime we're changing this Agave code base, we're doing something novel with a code base that really isn't fully owned by the Financer team. And so we want to make sure we're doing very, very minimal cut here.
00:43:19.023 - 00:44:00.281, Speaker B: And so what we've done is said we're going to host almost the entire Agave code base as it is as a child processor financer. And the only integration we're really gonna do is these little snips at the beginning and at the end. So Agave, when it talks to the network, has their own version of QUIC that you know, is binding to some sockets on the machine. So it's listening through the network device for incoming packets. We just intercept those packets before they reach Agave. So we have a little channel here for the technically minded. We install an XDP program that filters that traffic before it even reaches the kernel and we redirect it over to filencer.
00:44:00.281 - 00:44:25.995, Speaker B: So all of this Agave code is actually still running. It's still there. We haven't touched it, we haven't deleted code, we don't have a huge refactoring that we've done. We've just kind of taken this little snip and we intercept the data and it goes over to Filencer. And then right at the end, as we talked about earlier, we have another SNP right at the end here. We take the data that we've produced and we insert it back into this Agave block store. So we have another little snip here.
00:44:25.995 - 00:44:57.445, Speaker B: There's a Finance, a commit stack, which changes certain behavior in Agave, but it's very, very minimal. I think we have about 10 of these types of little changes. One of them is around executing transactions, right? As we mentioned, we call Agave code to execute transactions. So we have another little interface for that. But for the most part, the Agave validator is reproduced as is. And so if you're an operator of the validator, looking at, well, what's different in finance, a lot of things are the same. A lot of the configuration options you can pass to the validator are the same.
00:44:57.445 - 00:45:26.585, Speaker B: Although Finance uses a slightly different configuration language, it gets passed through to Agave in the end. A lot of the functionality that you come to expect from the Agave validator is the same. So we have the same RPC service. In addition to hosting these services, Agave continues to host rpc. And that should continue to work exactly the same as it is today. We're using the Agave accounts database. So all the account storage is stored in the same place on disk in the same way it was before.
00:45:26.585 - 00:45:47.877, Speaker B: The accounts database is actually compatible with the Agave because of this. And so if you're switching from an Agave validator to a Finance one, your existing data will continue to work. You don't need to delete it, you don't need to generate something and you can just cut right over. And so there's a lot of benefits to this kind of stepping stone design that we have that allow validators to start transitioning very smoothly over to Fire Dancer.
00:45:47.981 - 00:46:00.637, Speaker A: Yeah, that's great for operators. Anything specifically about RPC that would have to be different for RPC operators, I'm thinking in an RPC operator's perspective, I'd imagine that would be a great thing to try out. Right?
00:46:00.781 - 00:46:30.907, Speaker B: Yeah. So because we haven't touched rpc, you wouldn't expect to see a huge benefit from switching to flight. Answer if you're an RPC operator. We have rewritten this shred tile, which is again receiving the blocks via turbine from other nodes. So you may expect to see this part of the system work a lot better and be a little bit faster. And that ultimately stores the data that is used by rpc. But the actual RPC implementation itself is not going to be very different and you probably wouldn't see a huge benefit there yet.
00:46:30.907 - 00:46:31.147, Speaker B: Great.
00:46:31.171 - 00:46:34.535, Speaker A: So are there any other parts of the system you want to cover before we finish up?
00:46:35.135 - 00:47:15.535, Speaker B: Yeah, I think a couple. One place that's interesting to talk about, especially Solana Network lately has been focusing a lot on improving the performance of the networking layer. Firedancer contains a custom written networking stack. So we use a Linux specific technology called xdp, I think it stands for Express Data Path. And what that's gonna do is allow us to skip a lot of the kernel processing. So if you think about what happens when a packet is coming in off the network, we have the Internet, some Ethernet cable coming into a networking device. This could be an intel device running this ICE driver.
00:47:15.535 - 00:48:05.537, Speaker B: And the typical way that this works is the networking device waits till it's received the packet and then the Linux kernel pulls this device or something and it allocates a buffer in the kernel called a skb, some like socket buffer, and it copies this data into this buffer and then it waits for the application to make a system call. As we remember, it's kind of a no, no, the application's gonna call receive and this is gonna kind of go into the kernel and say, okay, is the data ready to be read on this socket? And return this socket buffer back to the networking user and then they now get to process it. So there's a few things that have happened here that are bad. One is we've copied data. This is somewhat slow. It'd be nice if we could just read the data directly. Another thing that's bad is we've made a system call and Actually, we're making essentially a system call per packet.
00:48:05.537 - 00:48:56.497, Speaker B: So if you're doing a million tps and you have to do a million system calls a second, that's probably gonna be prohibitive. So there's this new technology or somewhat new technology called Express Data Path, which allows us to kind of change the architecture of this a bit. So firedancer installs a small little program on the kernel. Sometimes it kind of gets installed in the networking device, but let's say it's in the kernel and it allows us to just read packets directly out of the network device and there's no system call needed. This networking device actually just has a buffer of packets and fired answer is kind of in this spin loop saying, while true, while true, check this little buffer. And all we're doing is reading memory. So we say, are there new packets in this memory? Yes or no? And the device driver, this networking driver is going to update some counter here saying if there's new data or not.
00:48:56.497 - 00:49:30.223, Speaker B: So when we talked earlier about how it's really fast if you can manage your communication between cores to just be kind of overshared memory, it's the same setup we're trying to do with the device driver here. They write memory, we read memory, there's no copies. And so this is a nice functionality of the Linux kernel. It also lets us implement this little program that filters data coming to the Agave validator too. So this is how the finance and networking stack is implemented. This allows us to hit over a million transactions per second per what we call a networking core. And these are also scalable switch.
00:49:30.223 - 00:49:49.885, Speaker B: Networking core can bind to a different channel of the network device. So modern network devices have a concept called queues, where they have kind of separate packet buffers internally that you can create. And so we can run one core per each of these queues to parallelize this work pretty linearly.
00:49:50.225 - 00:49:53.965, Speaker A: Great. How many cores typically are running a networking tile?
00:49:54.305 - 00:49:55.049, Speaker B: Just one.
00:49:55.137 - 00:49:55.425, Speaker A: Okay.
00:49:55.465 - 00:50:30.779, Speaker B: I mean these networking tiles are very, very quick. We've designed a lot of the system to scale beyond what is necessary today. If you look at this line of network, what it's processing, I think is around 6,000 transactions a second in that range, something like that. So when we're looking at, well, you're running a fired answer on mainnet beta today, let's say do you need to be doing a million transactions a second for some parts of the system? No. When you're executing transactions, when you're leader, certainly not. The most transactions you would ever need to execute is around 6,000 a second. For these earlier parts of the pipeline, potentially you would need to execute a lot more.
00:50:30.779 - 00:50:46.913, Speaker B: The reason is these aren't the transactions that are making it onto the network. These are the transactions that are getting sent to you. And there could be a lot more of those than are actually getting packed. Right. If you're getting dosed or you're getting spammed, it's helpful to have these be able to scale. And so we have the capability to scale these. Right.
00:50:46.913 - 00:51:17.665, Speaker B: If you were receiving over a million transactions a second, then you could run a lot of these networking cores and you could run a lot of these quick cores and you could run a lot of these verify cores. Right. And you could have 160 core box and be handling, you know, 6, 10 million transactions a second of spam coming in. Would you see that realistically on the network today? Probably not. And so the operator can maybe dedicate a little less hardware to these jobs. And for current purposes, it seems like one networking tile and one quick tile is more than enough.
00:51:17.825 - 00:51:26.009, Speaker A: Makes sense. All right, thank you, Michael, for the overview. And if I'm an operator or someone interested in Fire Dancer, where do I go to learn more about progress?
00:51:26.177 - 00:51:55.761, Speaker B: Yeah, sure. So the firelancer team, I think our goal, we're heads down right now getting stuff shipped. So the best place to learn more, the GitHub, you can follow along with the commits. We welcome contributions if you have any. We have an operator guide linked on the GitHub that explains how to set up and operate the validator. We've tried to make it really easy and even though we're using a lot of modern and special technologies, seccomp, xdp, we've really tried to simplify and make this easy for operators. So we'd welcome, if you wanted to give that a try to run a validator too.
00:51:55.913 - 00:51:56.625, Speaker A: Great, thank you.
