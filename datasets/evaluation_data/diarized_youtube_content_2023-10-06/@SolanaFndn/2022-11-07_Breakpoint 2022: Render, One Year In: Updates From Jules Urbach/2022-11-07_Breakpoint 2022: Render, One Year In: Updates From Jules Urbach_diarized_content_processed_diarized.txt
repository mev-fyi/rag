00:00:03.960 - 00:00:26.114, Speaker A: Hi, everyone. It's great to be back at breakpoint. It's been an amazing year, and I really love breakpoint. You know, it was. It was an amazing experience for myself and for render. Let me get these slides back here. Here we go.
00:00:26.114 - 00:00:46.522, Speaker A: So I'm here to talk about what's happened in the last year since we were at breakpoint. And I should start by just talking a little bit about what we do, what render does. Now, I have a company called Otoy. I'm the CEO. Founder, yes. That's the company that did all the visual effects work that our CCO went to Academy Awards. For.
00:00:46.522 - 00:01:13.684, Speaker A: This kind of work, you're seeing digital doubles visual effects. But really, what we're probably known for the most is GPU rendering. We revolutionized film rendering about ten years ago with a piece of software called Octane. It's now used for major films and motion pictures. It also is something. This is a great slide from an artist at Rafael Rao. It basically made art democratizable.
00:01:13.684 - 00:01:48.572, Speaker A: It brought final frame rendering to cheap gpu's. And over the last, I guess it's been out almost twelve years since Octane has been out. We tackled initially gpu rendering for images, animations. Finally, around 2016, all of these movies, including all the HBO shows, were rendered on the gpu. And now we're heading into a world with real time rendering AI. It's really kind of hard to predict how massive gpu computing is going to change things. And if you look at the last ten years, this is me on stage with Jensen, the CEO of Nvidia, announcing cloud rendering.
00:01:48.572 - 00:02:19.624, Speaker A: Because we always knew that there was a problem. You want to have more gpu's available on the cloud. And ten years ago, there really wasn't any GPU's. Amazon was the first ones to deliver that. And we went on stage with Jensen, and we showed a scene from the Transformers movie rendering in octane, on octane, render cloud and 80 gpu's. And it was a very proud moment for us because it really was the beginning of what became the render network. Today, what we've discovered is that even with Amazon providing those gpu's, or Google and Microsoft and others, we don't have enough to satisfy everyone.
00:02:19.624 - 00:02:50.726, Speaker A: And we do have partners. Google came in as a partner. Eric Schmidt's on our board of advisors. She's the one that made that happen, Microsoft as well. And it still turns out that decentralized GPU computing is a much larger addressable space than what you can get on the public cloud. And that's because it's expensive to put GPU's in a data center. Now, when we look at the next ten years, when you think about how crazy things have gone in the last ten years, and we went from having that data center to be able to render some of those things on a phone, and I'm looking at these important trends.
00:02:50.726 - 00:03:28.376, Speaker A: So unlimited computing, AI rendering, we're seeing some important steps now with AI, art, generative art. And of course, I'm here because we strongly believe that crypto and web3 and provenance is going to be a huge part of how art and content creation gets made. And also mobile. Mobile is a really important piece because we're going to have millions of artists using all of this extra compute power, right? We can't put a high end 300 watt GPU in an iPad. Well, mobile matters. And in fact, when you go back to that talk I gave ten years ago with Jensen, again, those 80 GPU's rendering, that starscream scene, I loaded it on our just released iPad app, and it renders on the iPad. That's crazy.
00:03:28.376 - 00:04:06.814, Speaker A: So one of those high end GPU's is now not as fast as what you can put in an iPad M two, but we still need more compute power. Now we need to render at holographic display resolutions. Those displays are coming, and in fact, we're working with a company called Lightfill Lab. As big as we think the AR space is going to be, people probably won't want to wear glasses if they don't have to. So the Star Trek holodeck is a huge inspiration for render. Light Lab is building these incredible panels that are about a gigapixel per square meter, ten gigapixels per square meter, and you can chain them together and you can build the Star Trek holodeck. This is how we simulate content on the displays.
00:04:06.814 - 00:04:44.290, Speaker A: We basically have to track our eyes. But on the real displays, you're shooting at so many rays of light that you do get back, effectively a white light hologram. And if I think about the future and I think about how people are going to experience anything on a screen, it will be holographic. And it's really just a question of scale, right? Because these things are hundreds of thousands of dollars now for a panel, but they'll be hundreds of dollars in 15 years. We have to think about that when we're planning something that is meant to last decades. And render is meant to be that multi decades long process. This, by the way, is a holographic display panel, of course, being filmed on a 2d camera doesn't give you the full effect, but you're seeing that specularity.
00:04:44.290 - 00:05:18.286, Speaker A: It's like nothing you've ever seen before. And that's the thing. That very simplitude that you get from holographic panels is really what, when people are talking about presence in VR, that's what you want. It feels natural, and there's nothing quite like it. So the Starfleet holodeck was supposed to be 300 years in the future, but as it turns out, it might be much sooner than that. And in fact, the holodeck is a huge inspiration to the work that we're doing, because ultimately, if you're going to be rendering reality right, this is probably the best experience. And in fact, one of the interesting things about the holodeck was that it wasn't just rendering anything you wanted in a room.
00:05:18.286 - 00:05:59.766, Speaker A: It was also the last episode of the original nineties series. Turns out that the entire history of the original Enterprise was being run in a simulation in a later Enterprise. Right? So if you question base reality, you question the story. Is Star Trek true and complete? If you just have a future Enterprise that is playing back the stories of the older one, how does that even happen? Fans were a bit miffed in 2005. How do you know what Archer did? How do you know all these things work? And it's clear that when you think about rendering and you think about something like the holodeck and how magical it is, you need to have something like AI that interpolates. It fills in the blanks, and that's what AI is great at doing. It's not great at just giving you everything unfiltered, because it's really just pulling together things from a training data set.
00:05:59.766 - 00:06:25.890, Speaker A: You need to have that training data set be really good. So let's talk about Solana, why we're here. I have to say that since last year, as I've gotten to really know and work with Raj and Toli, I mean, I love this family. I love being part of the Solana family. And we're here because we really want to share our vision for where we both. So really, both parties see all of this going. Render started out as built around my company, Otoys, GPU.
00:06:25.890 - 00:06:57.974, Speaker A: Render, it is massively different. This year for us has been about decoupling Otoy and even our own Otoy's tech from the render network. Render now has a foundation it just put in place. So governance, dao, all these things will be handled in this pretty decentralized way. In fact, we have votes going on now for governance, for a new token model, which is likely going to be Burnham into equilibrium. That was suggested by one of the members. And with Helium's announcement, I think we've got a very good model that is likely to be the one that we follow.
00:06:57.974 - 00:07:21.960, Speaker A: And that's great. Obviously, Helium's working great on Solana. I think that that's going to be super helpful, because the render network itself, right, is still in some ways limited, especially for nfts. And this is why Metaplex is such an important partner as well. You know, gas and these things are a bit walking, a bit crazy. The other decentralization really isn't on the crypto side. It's also about partners that can use the render network besides us.
00:07:21.960 - 00:07:44.554, Speaker A: Now, we've built 26 different integrations. These have been there forever, that allow you to take your 3d scene, send it to the render network, and it spits out this open source format and it renders an octane. But a year ago, we announced we're going to bring in not just other renders, but other 3d formats. Some of them are closed source. So it's not like blender, where you can just go and drop, drop it in. That's complicated. Running that on decentralized nodes is very complicated.
00:07:44.554 - 00:08:03.906, Speaker A: But we did it. A huge number of users use cinema four d. And I would say the other GPU renderer besides ours, we love our octane so much, is redshift. Redshift is a great renderer, and it's a GPU render like octane. It was created back in the day, and we're almost really the only two that started out on the GPU side of things. But it is very different than octane. It does bucket rendering.
00:08:03.906 - 00:08:36.486, Speaker A: It is a bias renderer, but it is the best in that. And so if you combine that with octane and redshift and sun Arnold, you have a pretty powerful system running on the render network. And it's more than just what we've started and contributed. So I want to show you the very first render job that's not just running redshift, but also running a c 40 file. This c 40 file you could just drop onto the render network and you don't have to export anything, it just goes right in there. And when we're talking about driving users adoption on the render network, this is going to help enormously. This is redshift bucket rendering.
00:08:36.486 - 00:09:02.150, Speaker A: It's not progressive, you get back little squares, but it's mapped to octane bench and it works and you get back the image and what's amazing about this 15 seconds you've just seen is that all of this software, none of it is otoys. Right. So the render network is now supporting somebody else's 3d file format. Somebody else's renderer. Yes. There's a tax on the render power that you'll spend to use redshift and cinema four, d and maximum. But this is a model for others to follow.
00:09:02.150 - 00:09:24.878, Speaker A: We have Autodesk. Autodesk is actually an investor in my company. We'll bring Arnold as well. Arnold's the other major renderer out there. The other critical thing is we're not just bringing these renders willy nilly. We're working with Arnold Redshift epic Unreal to come up with standards for the metaverse for how data can translate between two different renders. You're seeing here Arnold and Octane switching between 3d model of the Enterprise.
00:09:24.878 - 00:09:41.930, Speaker A: More on that in a moment. Seamlessly. Also something that hasn't really been done. And there's a lot of talk about the metaverse and open standards for how this will all work. It's very important for render. It's actually necessary. And I think by solving that, we're going to be doing the same for a lot of the other open problems in the Metaverse.
00:09:41.930 - 00:10:05.334, Speaker A: There have been some great movies being rendered on the GPU. I'm very proud of one of them, which is the Star Trek motion picture. It was filmed in 79, but it's been redone with effects. Redone for four. It was rendered in octane. And you're seeing major movie studios that are not just rendering in octane, but also using the render network, tier two and tier three. These are average people's machines to process jobs.
00:10:05.334 - 00:10:42.456, Speaker A: Not only did we work and have our software used on the Star Trek film and productions, but we also were doing a project for the Roddenberry estate, which is essentially the story of Star Trek. And it's using all these incredible assets. When we did that, Apple saw what we were doing and invited us to be in the keynote, which is crazy. I was in the apple keynote last year showing the work itself, the Roddenberry archive work. Thank you, CB's, for letting us do that. What's interesting is this was running in octane. These files were all created on the Mac, but when the frames were being done and they were sent out as layers, there were render nodes that were rendering those files for the Apple keynote video.
00:10:42.456 - 00:10:59.504, Speaker A: It's crazy. And since then we did another video short. This one came out in August. And that also was just featured in the Apple keynote, which I'll talk about. The new one, which I'll talk about in a minute. Let's watch that video. Everything you're seeing here at CGE was rendered on the render network.
00:10:59.504 - 00:12:31.216, Speaker A: That was pretty cool. We're going to talk about all of that now. Here's the thing. The rendering was on the render network. But the way this was filmed, the way this was made, is completely different than traditional movie making as we were working with CB's and Apple and others, showing how we built this. When you see the production that went into this, you'll understand how render is more than just shooting rays around. I mean, the tools, the way that we went about this is just completely.
00:12:31.216 - 00:13:03.746, Speaker A: And that's why we ended up in that second Apple event where, oh my God, you can actually create these shots on an iPad. Wait a second, how does that work? Remember I was showing that video we did with Jensen ten years ago, transformers, movie rendering and all that on those cloud gpu's? Well, you cannot do this on the iPad. And the iPad's not a very fast gpu, but it's good enough, right? It's good enough to get you started. This is huge. On the Apple website right now, you see our app, it's the rendered logo. It's called Octanex because Octane's forefront of this. And here's the video from the Apple keynote and also on their page.
00:13:03.746 - 00:13:27.442, Speaker A: And it shows the very first shot that you guys saw on the iPad so you can do final frame rendering. This is huge for the render network because. Why? Well, first of all, the iPads, now there's tens of millions more people buying an iPad that can actually load these scenes. And it only has one gpu, but they can at least load them and they can send them to the render network to do final rendering, right? That's very important. This is an iPad. This is moving that shot around. It's very grainy, very noisy.
00:13:27.442 - 00:13:58.178, Speaker A: You need to render this in 4k, you need to render it with 1000 samples. Render network solves that problem for you. But the other thing that's wild is that 10 million iPads, some of them have 16 gigs of memory. That's a lot, right? I mean, they can load scenes, full production scenes, even if they're an 8th the speed of a 4090, there's 10 million of them out there. And if those scenes become render nodes, that can change the entire composition of the network. That means a lot more gpu computing power. If you look at that, one of the shots you just saw, that desert scene that is running on the 16 gig iPad.
00:13:58.178 - 00:14:19.842, Speaker A: I can move that around. I can set it up if you look to the shot next to that. I'm also using AR as well. All of these incredible shots load in 16 gigs of memory and render slowly on an iPad GPU. But they're fast enough for me to be able to set up the shot. And of course, the iPad, unlike the computer, does have Arkit built in, right? Same thing with a phone. And of course, the app that we put out on the iPad is coming to.
00:14:19.842 - 00:14:47.148, Speaker A: The phone is coming out to the Apple and others. And you can see here, we've also come up with ways where you can use the render network to precompute essentially a hologram. That's where the light field is. Now, there's more sophisticated versions called nerfs, which you might have seen because they're used a lot in AI generative art. And with these pieces, with these switches, we also have a real time renderer called brigade on the phone. In AR, you can actually do really fast, noise free rendering. And we have those two modes.
00:14:47.148 - 00:15:39.854, Speaker A: And this is also another important piece for how we imagine the render network being used, not just by professionals and even artists that have learned how to do this like people, right? Taking a pc has a laptop, uses octane for cinema 4d, but there's a whole generation that just has a phone, and those phones now can do production rendering. We've made it simple enough now to be able to almost just have one button, right, especially on the phones. You don't want to learn blender, you don't want to learn all these crazy things. 30 years of legacy 3d software, you want that power, but you need to be able to deliver that to people in a way that's really easy. And the amazing thing for us is that we're living in a time now where AI is probably going to make that happen, both at the very high end, professional level and for the average user. And we'll show that as well shortly. As I said, every single app that's going out for the iPad and soon for the Apple set top box as well as the.
00:15:39.854 - 00:16:10.846, Speaker A: And the new one has an a 15 is a node, right? It's different than the render nodes now, which basically collect jobs. This can do live rendering. You can take your Mac app, your render app, and you can use that to connect to a pc and vice versa. You can sling your own render power to your own devices. And of course you can then do that on the blockchain as well. So this is how we're going to expand rendering and distribute it in really remarkable ways. So not just offline rendering, not just live rendering, but also you're seeing blender.
00:16:10.846 - 00:16:46.366, Speaker A: Blender is streaming to the new app, and the app is delivering the rendering power. So if you have a phone in your pocket, but you're using blender on the pc, or vice versa, you can sort of bridge these together. You can also mix that with different 3d packages. We also have been developing technology to stream in terabytes, gigabytes of data. Normally, to render the moon in octane, you need all that data to be in memory. With nanite, which is based on technology that unreal developed. And we've adopted that for production rendering, you can stream in 10gb of geometry over a 500 megabit buffer.
00:16:46.366 - 00:17:19.442, Speaker A: And that means that even if you have low memory or you're on an endpoint with not a lot of bandwidth, we can still send this data as you need it, based on what's being rendered. And it works great, of course, within SSD, but it also works over the Internet. And this is also going to expand significantly the amount of render nodes we have and also how easy it is to send data to the render network. We have Delta syncing, we also have an SDK. And we've been bringing on partners to create various different tools and modules. A lot of what you saw in that Roddenberry piece was done. In fact, these are some of those scenes, right? We used them.
00:17:19.442 - 00:17:55.756, Speaker A: They're done in cinema four d and world creator, which is a tool that we bundle rendered, of course, in octane. The artist's name is Aaron Westwood. He is an incredible artist and was more than happy to help with the Roddenberry project. But the tools to create these scenes are pretty straightforward. And once we get onto the iPad and iPhone world, it's our job to really make it so that these kinds of things, while they're easy to render, because you can just load these on an iPad and they will render just like this. We have to make it easy to create. And all of the AI prompts and all the things you're seeing, those will help, and we're working on those right now.
00:17:55.756 - 00:18:22.324, Speaker A: The other thing that's significantly important for render is assets. We need people to be able to, one, claim ownership of a 3d model. Their face light stage is our scanning system, right. We have hundreds of people that have contributed their face and that's owned by them. So if there's a remix or re render, we know what to do with that. We can basically turn that into something that is licensed. We also have tools for generating physics, lighting, fire sculpting, and again, unreal virtual production.
00:18:22.324 - 00:18:59.200, Speaker A: For the Star Trek piece, we did a lot of work in unreal. This is running octane in Unreal. We can bring in out other renderers, and no matter what tool you're coming from, how you're using three D, the render network will be there to support that. Right. We're also working with Nvidia Omniverse. And as mentioning earlier, AI is such an important part of how all this comes together, especially for the next 10 million, 100 million billion artists. So CEO of Google posted this tweet a few days ago, and it's showing, if you haven't seen this, it's basically showing a block of text, and the video is following along, and it's AI rendered video.
00:18:59.200 - 00:19:28.734, Speaker A: And what's really happening is it's rendering about three frames a second, and there is basically interpolation that's making that look really good. And AI is great at interpolating. You can give it a blurry image and it will give you back a photo. But the truth is, that's not exactly what you want for the holodeck. You don't even want that, really, for high end art. What you really want is something like this, which is what we've been building. So every one of those nodes, those little colored boxes, represent something, an asset in the 3d scene geometry, a texture, the sky displacement, those things we've been training AI on.
00:19:28.734 - 00:19:49.624, Speaker A: And we can also load in straight up, just stability diffusion. Right, if you want to do that, and feed those into the, into the scene. And we built an SDK. I was talking about neural objects, Nurfs. We can render directly to nurfs in a very optimized way. And that's important because really, AI doesn't work in triangles and textures. It works in latent space.
00:19:49.624 - 00:20:26.272, Speaker A: It's almost like a dream version of what a 3d model is. And it's actually somewhat less precise than a good 3d scan, but it can be very convincing. And what's important about this is that when you're doing an operation, if we're sending the starship enterprise, and this is an AI remembering what the enterprise looks like, we can mix that and blend it together in ways that you can't really do that easily with 3d data normally. And that's where AI tools are very powerful. As I mentioned before, one of the job types we're adding to the render network stability diffusion, right. They just raised $100 million. They're running on 5000, 6000 a 100s, that's about two to 3 million octane bench.
00:20:26.272 - 00:21:05.118, Speaker A: We have that on the render network every day, right? And we can deliver that and many more types of AI jobs at a fraction of the cost of what it takes today. And that's also super important as we're talking about scaling this to tens of millions of people on an iPhone or iPad. These are showing the different AI things you could do within a normal 3d scene. You can use scattering, you can generate AI for procedural data, not just an image or not just in painting. And I think that it looks beautiful. I mean, we're seeing really the need for having a material library is going away, even geometry, which is tough. So a lot of the simple text to mesh things are too simple, right? I mean, we need something that is much more domain specific.
00:21:05.118 - 00:21:33.994, Speaker A: That's why we're focused on faces. When you look into how we did that Star Trek production, you'll see how we've been applying that. And on the iPad and the iPhone, where there's just minimal interface, we really do need something like voice input. Right. That helps a lot. And now that we have text to, you know, image geometry, object scene, it really is going to be really easy for a lot of people to pick these tools up. Let's talk about nfts, because really, Metaplex is a big part of why we're so bullish on Solana.
00:21:33.994 - 00:21:58.784, Speaker A: You know, we've talked about before why Metaplex is such an important partner. There are things you can do on Metaplex and on Solana that are just very different than other ecosystems. And I would say that one of the main goals that we've had from the beginning is we want to deliver live streams to the render network. Right. You saw some of the streaming that you can do peer to peer. Well, you can do that for an NFT as well. And we want to be able to deliver finalization of your scenes, your data, but also a live stream that allows you to edit that.
00:21:58.784 - 00:22:27.320, Speaker A: Doing this has been really tricky, but the Metaplex team, Solana, they've been really helping us at a protocol level, but also opening these things up to opensea and others. And we're going to show some examples of how this is going to work. So nfts have been created on render network forever. I mean, Pax, I don't know, I think he's made $100 million of nfts on multiple hundred million on render. Beeple, of course, as well. And there's just tons of great artists that are using the system to do final rendering. But also to create prominence.
00:22:27.320 - 00:22:58.030, Speaker A: Everything that PAc does is record. And this chain is not just a flat data point. It can be used for remixing when we add live streaming. So this is an NFT that was created for streaming and it's all running in a browser, right? So there's nothing to download, it just runs on a node on the render network. And the way that it works is there is a bit of staking, right. If you do an offline job and the job is done, your reputation score goes up and then you're allowed to run a real time node and the finalization of that node is handled based on your reputation score. We can also, by the way, stream app.
00:22:58.030 - 00:23:15.614, Speaker A: So this is the same app that we have on the iPad. If you don't have an iPad, you're on an Android or Solana phone. You can get it through a web browser and you can get it through the render network. And now let's talk about Beeple. Now Beeple, we have made a special case around a lot of his work. I mean I've known him for ages. He's actually a member or partner of our advisory team.
00:23:15.614 - 00:23:41.314, Speaker A: And we've been building something called the Beeple Archive. We've also been doing the Roddenberry archive, the Alex Ross archive. But with Beeple, all of his 5000 or so everydays are done at c 40, which we now support directly on rendering. It's not about selling those nfts, it's about his work as an artist being represented on chain and the composability around that. So I asked him, we've been talking about how to leverage the Beeple archive. The Beeple archive exists, it's done. It's like Alex Ross's physical paintings.
00:23:41.314 - 00:24:20.230, Speaker A: They're out there, right? The everyday collection that was sold for 79 million is a snapshot of all that. So I asked him for this scene. It's one of my favorite scenes, it's one of his everydays. And we're playing around with what does it mean if you own that NFT? Now people owns the NFT. But if you were to transfer that, does that mean I can load it on the iPad app and edit it or in the web browser and change it? And I think that we decided, well maybe, maybe that does is an interesting option, but we need to make it so much simpler. Not everybody knows how to use these tools at that level. So we ended up with, for this test that we're about to show and this is something that was preannounced is you cannot remix a beeple entity on the render network with live streaming.
00:24:20.230 - 00:24:50.672, Speaker A: And it uses all of the things we've been showing, even the live rendering tricks, mixing that with different pieces. And this is the output. People can make their own nfts. People doesn't have to sell an NFT. You can take one of his everydays, you can make one and remix it, and they look really good. They are rendered on the render network, but instead of using cinema 4d or loading his files, you get a stream. You have a certain amount of time, and in that time stream this is running live in a browser, you can make changes, and depending on the artist, this is going to be a stream for, let's say, visiting the world of Geonronberry.
00:24:50.672 - 00:25:28.152, Speaker A: It's not going to be this, but if it's people, it will be this. And this is where the tools and the things that we're creating don't have an analog in the professional 3d space. We want to create something that is really simple and that leverages what the render network is built for and lets people create beautiful art on demand. And with Metaplex, we now have the ability to create a system where that can be sold, tickets to these streams can be offered. And it's remarkable. I think this is going to fundamentally change the way that people think about nfTs, because now you don't have to just create one thing. Anybody can take somebody's last creation and remix it.
00:25:28.152 - 00:25:56.934, Speaker A: And this is not some simple svg file. I mean, this is a full 3d scene that gets rendered in a full production renderer. And as far as we're going to get with iPhones and iPads, and that is important for AR, this will work in a browser and the render network will power it. Cloud rendering is expensive, especially when it's live, and it can also handle all the different things you might want to do. You can have generators, you can do procedurals, it can be rarity. There's all these different models. We'll put these slides out so people can look at them later.
00:25:56.934 - 00:26:24.828, Speaker A: But it's really exciting and we're super excited. Mike's excited as well with this technology. It's going to be sometime next year when we put this out there. But this is how we both see the future of NFTs developing. I want to talk about something important, which is open standards, right, the open metaverse. We're part of a group called idea, which is standardizing everything we're doing on render that is part of an even larger group called the Metaverse Standards Forum. And we definitely don't want to have one company.
00:26:24.828 - 00:26:54.626, Speaker A: We're all here because decentralization is important. Control how this goes. And already you can see that because Facebook is called meta, Nvidia is not going to call the omniverse the metaverse. Apple's not going to call it the metaverse. Right? There is this bifurcation of what it means to call something the metaverse. In my opinion, it's just the Internet, right? It's just pulling up 3d data. But there's also a deeper analog here, which is when you look at the work of Alex Ross, another top artist and a good friend of mine, I mean, the amount of pieces that his work touches is all of pop culture, right? I mean, it's enormous.
00:26:54.626 - 00:27:51.558, Speaker A: It even crosses over into Warhol and Star Trek. And so as we were collecting and building the Alex Ross archive, which is almost everything in the 20th century, pop culture mythology, from comics to movies, we came up with an indexing system. And when we're doing a render job on render, we can attach it to a specific universe, a specific episode, and you start to create a chain. And this is what we have in this token called damage for metaverse, which allows you to create a provenance for stories and for narrative fiction. Remember the holodeck? How do you know if what you're seeing in that past was really the story of Star Trek? Literally, the Roddenberry archive is telling the story of Star Trek for the future holodeck, with everybody that's involved, trying to make sure that we get this right and that providence for physical art, for storytelling. I mean, heck, even the story group at Star wars or Star Trek has to go to memory Alpha or Wikipedia to find out what's what. And so we have all of these documents, all of the free files we've been creating for the Roddenberry estate.
00:27:51.558 - 00:28:34.464, Speaker A: And I want to close out by showing the Roddenberry archive project. Now, you saw the video that we showed earlier that was also featured by Apple. But this is the behind the scenes part, which is really almost a fascinating, deep insight into the meta commentary of Star Trek, not just the story, not just the actors. So one of the goals of project is we have the in universe version of Star Trek, what happens in the universe, as it were, versus the eleven foot model of the Enterprise and the Smithsonian. And we are building the life size Enterprise digitally, but it is a digital twin, and that's what Apple showed in that first keynote, right? But you know, you have Kirk's coffee mug, you have everything that went into that ship. We do have the plans. We hired all the people that worked on the original mister shots guide to the Enterprise.
00:28:34.464 - 00:29:11.138, Speaker A: So we know how the ship is supposed to be built, how it looks, and we've even built a simulated universe to run it in. Right? I mean, all these planets are not texture maps, they're all procedurally generated, the interiors, as we get inside the ship. Of course, now we have to think about people. They all work like, the entire ship runs according to the laws and physics of Star Trek. And that also isn't going to be radically different when we do marvel stuff or DC stuff, which we've done a ton of work for DC with Batman and the like. And so when you think about coming up with standard 3d asset formats, but also a standard physics format, a standard way of linking stories, narrative events, information theory. Right.
00:29:11.138 - 00:29:47.044, Speaker A: It becomes really important to understand all of that through projects like these. And just the concept of visiting the world of Star Trek or Star wars or things that you loved is what the metaverse, I think, in some ways promotes or could deliver. Obviously, these are pretty incredible scenes. This is what we use to film the actual videos that you saw on a virtual production stage on an led wall. But you can pull those down from rendering, you can get a stream, and you can't change the story of Star Trek. That's really up to the Rodenberg archive, the estate and everyone CB's as well. But you can certainly explore it, and you might be able to create derivative content from it, just like you can from people.
00:29:47.044 - 00:30:24.164, Speaker A: And when you talk about derivative content of Star Trek, Gene Rodenberry was guilty of that almost more than anybody. He had 20 different versions of the Enterprise. And of course, the Enterprise itself changes enormously over time. From the 1960s, it has a 40 year history in universe, and you can see this beautiful time lapse showing all the different stages of it being built. So again, if you're visiting the world of Star Trek, which, even if it's one single chair that captains chair, that changes significantly over time, there's the physical props, there's the digital props. All of it coming together really is part of the fabric of the storytelling of Star Trek. I'm out of time, but I'm going to go a little bit over because I have a few more videos to play, and I think it's worth showing.
00:30:24.164 - 00:30:55.970, Speaker A: The scanning of these assets is something that leads back into the digital world. So when you're talking about digital collectibles, Providence on chain, we scan these objects in. We have a team that is able to then rectify them and put them into a system where we know what the carpet was, we know what the materials were, and you can then visit the actual location. And it's all real. It's as real as you're going to get. Second to having the physical asset in a museum, which does exist in certain cases for these pieces. But all of this works, right? The interior of the Enterprise is there.
00:30:55.970 - 00:31:17.004, Speaker A: It's visible. And when we came to tell this story, we started with the very first episode, the Cage. These are all the sets in the cage. It was filmed November of 1964. And we built these first because we wanted to tell the first story of Star Trek on a virtual production shoot. And this is where this came from. The director of the cage, Robert Butler, is alive.
00:31:17.004 - 00:31:47.620, Speaker A: He, in fact, was director of many pilots, not just Star Trek, but Hill Street Blues and Batman, the 66th one. So when it came time to put together all of this, bring him in in March, it was incredible. And we had to then think through, how do we get costumes? How do we get characters? Characters are an important part of making that world come to life. We cast two people. I'll play this video, and then I'll talk more about the casting. I was aware of the size that this show has become. I saw the size there.
00:31:47.620 - 00:32:10.954, Speaker A: I saw the size here. Testimony to the quality. It's been really rewarding to see the feedback from those that actually worked on the show. Coming in and seeing. Seeing the world of Star Trek as they remember it brought back to life. That's where the magic of the work that we've done really kicks in. We're at the point where we're looking to rebuild and reconstitute these uniforms digitally, physically scanning them, putting a performer.
00:32:10.954 - 00:32:36.336, Speaker A: So what you're seeing there was shot a lot like the Mandalorian. We built the CG sets. We had the director, Robert Butler, come in, as we were, to refilm some of these scenes. And we did a lot of work. We did do a lot of physical work with the hair and the costumes. Then we used marvelous designer, which is a fashion tool used for a lot of fashion and avatars. But we have the physical and the digital there, and you can't even tell what's 1964 or what's 2022.
00:32:36.336 - 00:33:32.300, Speaker A: It's that good. But what's crazy is that these things are digital. These are digital assets. When we have performers and we have real objects, but they're scanned in, and then AI rectifies all that to allow us to deliver these kinds of experiences from those assets, all rendered on, rendered by the way of course. And that world exists. You know, we've been building out all the structure of history for so long that we now can tell those snippets and the stories. When it came to the last piece, Spock, you saw that at the end, they got a lot of gasp, and it was something that really did garner a lot of attention in the industry as well.
00:33:32.300 - 00:33:50.506, Speaker A: Last year at bright point, I showed this very slide saying, we talked to Adam Nimoy. We're going to do a CG. Spock. We didn't end up going for that. We ended up hiring two performers who played Colton Spock, which are the two characters from the original pilot. And in the case of what we're doing, we're aging them up through decades. And we found that actors really needed to perform these roles.
00:33:50.506 - 00:34:18.754, Speaker A: Even if we put a ton of prosthetics on them, which we did for Spock, it didn't quite get us all the way to something that looked as close to what you saw. So the eyes, for example, needed to be CG. And we actually built a CG model. We sculpted it out of the light hounds that we had done from previous subjects, and then we applied that live as we were filming the two actors. Spock. This is what I've seen through the viewfinder with the digitalized overlay live, and using AI to track all those moves. This isn't a deep fake.
00:34:18.754 - 00:34:36.410, Speaker A: It's something else altogether. It's AI assisted rendering, and it's being done while I'm filming. If you imagine that being applied to things on the iPad, all those different pieces, this is insanely valuable for a filmmaker. This is as well. What I'm showing here, is the two of them as they were being filmed. They were scanned in. Right.
00:34:36.410 - 00:35:01.300, Speaker A: And I have an asset. I can even load this on the iPad where I can move the camera around. I can get to final frame rendering. But these are people, right? This is a snapshot of what they were doing on stage, and it's completely digitized to the level where I can render it, and it looks like I filmed it. This is incredible. This has never really been possible before. What we went to Academy Awards for was a static version of just heads, not two bodies in a performers doing this very same thing.
00:35:01.300 - 00:35:19.376, Speaker A: And what we learned from this, and this is really recent, cutting edge tech, is that maybe we don't need a green screen or an AR wall with a mandalorian. Maybe that tech is outdated and there's something better on the other side of this. And that's where render comes in. That's where we did all this. It's like, let's shoot it on a green screen. Let's do this shot three different ways. Ar, green screen.
00:35:19.376 - 00:35:39.514, Speaker A: And then let's scan her in and put her in a render job. And we've seen there on the bottom, it's so much easier. It looks so much better than anything done with green screen or even an AR wall, which has severe limitations. This you can do on an iPad. More impressively, once she's scanned in, right in costume, this is like a statue. It's scanned in. It's from a pose, but it could be relit, and it looks perfect.
00:35:39.514 - 00:36:04.602, Speaker A: But what we did is that her walk cycle could be trained. So as she moves, as a clothes move, we can then apply that to a performance that just filmed with a normal camera, and then it gets applied in the short. That's what you're seeing in that very first piece. This is how AI is going to work on rendering. This is how it's going to work in virtual production. And when you look at how good these results are, it's incredible. We're just at the early stages of all this, so we're so excited for how the future of content creation is going to work.
00:36:04.602 - 00:36:15.434, Speaker A: We think that render and, of course, everything we're doing with Solana and Metaplex is at the center of that. Thank you so much, everyone. I love Bravepoint. It's great being here, and hopefully I'll see you around. Thank you so much.
