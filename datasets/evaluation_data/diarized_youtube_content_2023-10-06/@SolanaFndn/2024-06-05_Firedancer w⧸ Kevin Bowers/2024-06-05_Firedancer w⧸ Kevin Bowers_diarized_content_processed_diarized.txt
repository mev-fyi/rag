00:00:05.840 - 00:00:08.062, Speaker A: Doctor Kevin Bowers, welcome to Validated.
00:00:08.158 - 00:00:09.198, Speaker B: Thank you for having me.
00:00:09.286 - 00:00:46.794, Speaker A: Thanks for coming back. So I want to start off talking about fire dancer today. The process of getting there, what it actually means for the network, what the various stages of the project and the rollout actually are. Uh, we'll talk a bunch about other things in computer science and technology and, you know, blockchain in general. But I think the first thing that people are most excited to hear about is firedancer. Okay, so this was a project that sort of got its beginnings back in 2021 2022. You know, big news at breakpoint that originally came out.
00:00:46.834 - 00:00:47.482, Speaker B: Yeah.
00:00:47.658 - 00:01:00.624, Speaker A: Walk us through a little bit of what the rollout and roadmap for the project actually is. It's a pretty ambitious goal of getting Solana up to a million TPS over the next few years. Where are we on that journey today?
00:01:03.284 - 00:01:49.436, Speaker B: We're very far along on that journey. I and my team had been asked to look at the code base to say, hey, is, you know, what can we do to help? And a lot of discussions kind of started there. And so we sat down and we chatted a lot about what the foundation wanted, what different communities in the foundation wanted, and everything was on the table. But there were also clearly priorities with things on the table. And so if you look at Solana foundation, what they wanted was a second validator. They wanted it to be really robust and really well documented, and a foundation on which other people could do a validator. They also wanted to be performant and use hardware and a bunch of other kind of things and kind of a wish list.
00:01:49.436 - 00:02:51.956, Speaker B: That framework is what a lot of people, at least at Solana and whatnot, have seen, and maybe a couple cases on Twitter of a bunch of components in fire dancer. And the way we approached it is very similar to what we've done in a production trading sense. A lot of times, we have a trade group. They've come up with some algorithms, some code, some ideas that are very, very clever, but they want it to be higher performance, more stable, so it can survive in the general markets, be competitive against them, be robust, and so forth. And so then often our team gets called in to say, hey, go in and shore this up, make this much more performant, whatever. And so we approached it with the same attitude of that, and a lot of the ways we do that is just essentially take the existing code base, start separating it into layers, and then work from the bottom up. Essentially, put the entire upper layers up on jacks, rip out the bottom layer, put in a new bottom layer and then go up and just incrementally work through.
00:02:51.956 - 00:03:38.998, Speaker B: If we look at that in the Solana codebase, that is reflected in the development plan we've had with the three components. Component one is essentially put it on top of a high performance computing stack, networking stack, so that we'll have a lot of capacity and use a lot of technologies. We've proven internally for doing that and doing that at scale and doing that robustly. And then, you know, put the next layer and essentially replace the runtime within the salon. That's component two. And then after that, you know, kind of replace consensus on top of that. And then along the way, we had a lot of milestones within those areas to say like, okay, hey, we can do the networking at this speed, and we can do the signature verification at that speed.
00:03:38.998 - 00:03:49.994, Speaker B: We can run replays and match Bain caches at whatever performance levels, whatever robustness levels, and work our way through the stack. I can keep talking.
00:03:51.494 - 00:04:22.140, Speaker A: You described these three different levels to the project. The first level is replacing the existing networking stack with a net new networking stack. The next piece is replacing the runtime with a net implementation. The last piece is a net new consensus implementation. So talk to me a little bit about why that was done in that order, and sort of what are the sort of cascading performance increases that come as you move from phase one, which is the sort of Franken dancer version, to the full fire dancer version?
00:04:22.332 - 00:05:36.164, Speaker B: This also creates some opportunities for performance, and I think it's best to think in terms of the performance, about the capacity of the system that kind of comes into play. And this also gets to some of the, I think, kind of surprising things from our perspective with the kind of early stages, which is like, as I think you already know, but the audience may not. We've already demonstrated running the leader pipeline of Solana's thing on top of our high performance computing stack, and have been able to show over a million tps kind of level performance. Now, that's not everything, and that there's not a million tps out there to do today, but to the generic kind of point is that the Solana components, two and three, are actually quite performant already, with some minor tweaks and whatnot. The low layer networking stack fixes a lot of just problems in computer science and language and the way people approach problems which just create massive performance drains throughout the entire system. And so if you like, look at how we started the project. We knew that if you don't have a good foundation for doing things, then it kind of doesn't matter.
00:05:36.164 - 00:06:05.902, Speaker B: So we started there. We also started there because that's what we already had. And so if you like, if we want the reverse direction, okay, we don't have a consensus layer, but we already have a high performance. So we're kind of working in the wrong direction. If we want to show incremental progress to the community, start with what we know and what we knew really well as high performance. We knew that would be a strong foundation for all of this. And then we could put the other components that were less familiar with, but that Solana already had.
00:06:05.902 - 00:06:20.634, Speaker B: And I think at this stage, once you get rid of a lot of the performance saps that are just pervasive in the various environments and whatnot, what Solana has for their component two and three is actually quite respectable in performance as well.
00:06:21.224 - 00:06:50.864, Speaker A: You mentioned that in that component one, the networking stack of the existing Solana labs and agave clients. A lot of that stuff is inherited from modern paradigms in computer science, networking ideas that maybe you and the folks at jump don't really agree with. So walk me through a little bit of the delta between what the way that networking stack was implemented in the existing client is versus the way you and your team approach networking stacks.
00:06:51.244 - 00:07:46.976, Speaker B: So what I find in most development environments, in most languages, modern languages and whatnot, is the mental model that they present to the developer of how a computer works is just wrong. It's backwards. What they think is cheap is expensive. What they think is expensive is cheap. That's both on the technical side for the computer and economically in terms of how developers interact with the code base. A lot of this, in my view, like when I trace it back, these are the same problems I was dealing with in high performance computing in the past, the national labs or in biochemistry or whatnot, which is that fundamentally, everything is limited by speed of light and everything is oriented around data flow, if you really want to be optimized. But most operating system languages, APIs go out of their way to hide locality and data flow from you.
00:07:46.976 - 00:08:16.952, Speaker B: They treat it as something like, oh, the kernel scheduler will figure that out. The network will figure out. You don't got to worry about that. The thing that's expensive for you is the operations count on your computer. The thing that's expensive for you is the physical act of you typing keyboard code. And then when I actually look in the world that I operate in, which I think actually everyone operates in, but they don't appreciate as much as they're limited by, but at least definitely the world I operate in, the operations are virtually free. Everything's about dataflow.
00:08:16.952 - 00:08:52.884, Speaker B: If I don't have the information, the right place, the right time, I can't move it there fast enough because I'm limited by speed of light. I have to be architecting around data flow, and I have these languages and paradigms that do not expose that. So I have to fight against everything which tries to pretend like that's a minor cost. It's like, no, it's your dominant cost, and you've hidden it from the developers. They can't make intelligent decisions about what to do in their code. Likewise, when it comes to getting stuff in really high performance, where do I spend the most of my time? Do I spend my time the physical act of touching a keyboard? No. I spend my time trying to reason about code, trying to understand what it's doing.
00:08:52.884 - 00:09:09.196, Speaker B: A lot of times it's reading my own code that I wrote a few months ago and trying to like. But, you know, it's even worse. You're reading somebody else's code. I don't know what my mindset was then. It's even worse. I'm trying to understand somebody else's mindset. And so what is my time? My time is spent on understanding readability, debugging.
00:09:09.196 - 00:09:30.220, Speaker B: And so if you look and you say, what should you be prioritizing? You shouldn't be prioritizing. How fast can I get functionality crammed in? It's how much understanding, how minimal can I make the cognitive load on the developers reading the code? And then how much can I optimize data flow? And so virtually everything we see today is just like. It's backwards, and it's very frustrating.
00:09:30.412 - 00:09:38.624, Speaker A: So how did this manifest itself in the existing client, and how have you sort of corrected for that in the fire dancer stack?
00:09:39.684 - 00:10:15.804, Speaker B: I don't know where to start. So one of the things about the entire area, I'm not from the crypto area or whatnot, and I don't know a lot about the ecosystem, but one of my first questions, and it's related to this data flow question, was like, okay, who's the most performant one out there? Like, Solana. I'm like, okay, cool, why are they most performant? Well, they designed for performance from day one. I'm like, okay, who else has cared about performance designed for performance from day one? And the answer was nobody. And so I'm like, okay, cool. I don't got to worry about anybody else. Solana's the only people that have the right mindset from the get go.
00:10:15.804 - 00:11:14.474, Speaker B: So one really high level answer to your question is actually, Solana, this is reflected in some of the reasonable results that I was talking about is conceptually sound. It already has some notion of the right data flow through the system, and with the way things like verification is done, or the VM and execution runtimes are working and so forth. That's not to say it's perfect. It too is written in terms of these modern APIs and languages and whatnot, by developers who have been trained to essentially optimize the wrong thing. So it's not an issue of like, we need to conceptually rethink Solana. It's more an issue of we have to undo the implementation practices that are bad now. So, like looking at that, a lot of these implementation practices are things which, interestingly enough, are the same implementation practices you use for high availability.
00:11:14.474 - 00:11:55.634, Speaker B: But to kind of give an example of one that most people think is an absolutely crazy thing to say, except for when you start reading the literature on high availability or whatnot, is you never use dynamic allocation for anything ever. And you look at how a lot of languages treat it, it's like, oh, don't worry about it, memory's infinite, it can go on forever. And the operating system will magically figure out how to get these resources and all that. And kind of like an example I'll use. And there are actually some precedences like this in various areas. If you're interested in engineering disaster literature. To kind of figure out things that can go wrong is think about those optimization priorities and the kind of economic priorities people have been taught.
00:11:55.634 - 00:12:15.086, Speaker B: And imagine somebody is supposed to do logging for some avionics in an airliner. So most people think logging is this really minor thing. It's like, oh, just call printf everywhere. It's okay. So they go and they call printf everywhere. And they're like, oh well, printf is not good enough. You should go use one of those logging frameworks.
00:12:15.086 - 00:12:59.846, Speaker B: So they download a logging framework, and that logging framework is out there and it's calling a little Malloc every time under the hood to get a buffer. And the developer thinks they did the right thing. They linked to some library that's been written and tested by everybody else, and they already deployed it in their embedded mission critical kinds of system. And the thing is flying, and then it's logging all the events so that people can optimize or diagnose or do whatever kind of post mortem fault recovery is or whatnot. And then in the middle of flight, all of a sudden it runs out of memory and kicks the Om killer or whatever else. It doesn't have the option to say, gee, you didn't provision enough memory. And it turns out when you go beyond like 16 hours of flight time that you're going to fill up your logger and then all of a sudden the plane blows up in middle of the air and crashes and whatnot.
00:12:59.846 - 00:14:07.926, Speaker B: It's like, okay, was the priority of saving your time by calling this and pretending that you had an infinite amount of memory the right priority? It's like, no, if you want to make a mission critical system, you don't use things like Malloc, but also you want to make a performance system, you don't use things like Malloc. Cause those same mechanisms under the hood where you have essentially skimped on doing the engineering of your system when you've tried to basically say, I don't want to figure out the resource requirements and limitations and how to work around them, I'll just kick that off to some magical library, some operating system thing that the mechanisms for doing another hood are really, really slow because all of a sudden you call Malloc and the operating systems, like, I don't know if they really mean it, it's a bluff. And, oh, they're actually touching the memory now I need to go find some stuff. Oh, shoot, I can't do that. Maybe I just need to start blowing up things on the system to try to recover from this situation I got into, because people haven't actually planned around their resource or their actual needs. So you end up with like fast systems that are really, really robust. If you actually start paying attention to performance needs, resource limitations, they're actually very similar problems.
00:14:08.070 - 00:14:49.394, Speaker A: So one of the advantages of dynamic systems is you don't actually have to think about what's going on on the hardware under the hood. So what you're talking about are system design paradigms where you have to think about the actual hardware you're running on. You have to think about locality that feels like that's something that's much harder to scale, that it doesn't have the same, I can just drop any code into AWS and it will run medium quality. Good. How do you actually think about bringing that paradigm to something like a decentralized system where you don't necessarily have the same level of control over what everyone's system configuration is on a network like Solana?
00:14:49.774 - 00:14:50.886, Speaker B: I can answer that. A lot of different.
00:14:50.910 - 00:14:52.350, Speaker A: Or do you disagree with the entire paradigm?
00:14:52.382 - 00:14:53.918, Speaker B: I mean, some of it I disagree with the paradigm.
00:14:53.966 - 00:14:55.154, Speaker A: Let's go. Let's start there.
00:14:56.204 - 00:15:26.136, Speaker B: So does it run medium good on AWS? Look at what I've described. People basically say I make a small pile of garbage, but it's okay because I can scale up that small pile of garbage if I'm willing to pay a bunch of operational expenses to AWS. It's like, oh, okay, cool, you now have a big pile of garbage that you're paying for continuously. Like maybe you should have invested your time upfront to do something more efficient.
00:15:26.200 - 00:15:33.576, Speaker A: But that is sort of the paradigm that many startups operate in. Yeah, they do just hit some amount of scale. Don't worry about the garbage, we'll fix it later.
00:15:33.640 - 00:15:36.704, Speaker B: Yeah, and that's a lie. I mean, it's a lie they tell.
00:15:36.744 - 00:15:38.808, Speaker A: Themselves because they never fix it.
00:15:38.856 - 00:16:13.250, Speaker B: Yeah, because they never fix it. And you know, most startups go out of business, so maybe they don't. It's not, it's not really a problem. Meanwhile, AWS is making plenty of money off the venture capital that's coming in from the startups that don't have a future. But a lot of times if they are successful, they never go back and fix it because people are too afraid to fix it. You have all this technical debt, then you have a lot of inefficiency and you have the kind of ridiculous situations you see now where the level of performance that people achieve is ludicrous. Compared, you're looking at three orders of magnitude plus.
00:16:13.250 - 00:16:56.672, Speaker B: That's just sitting on the table if people take the time to understand this. And it's not that you have to understand every single level of detail. My big issue here is the abstractions that people have. People are coding stuff wrong, and if you fix that, you get a big impact. So I can kind of flip this around, which is, let's look at a very, very successful company lately, which has been Nvidia. Nvidia has. What's their big secret? And there's a couple, I'm happy to get into technical details, but I think their biggest secret is that one, they didn't treat performance as a second class citizen.
00:16:56.672 - 00:17:41.492, Speaker B: When you're a developer, you go to the Cuda zone developer page, they'll have drivers and whatnot available for everybody under the sun. And so they're not going to give you an attitude that if you're not running the cool kids version of Linux or whatnot, it's like you're running Windows 98 se. It's like cool. They're going to have a driver for you. The first example program, they give is how do you do a dense matrix multiply fast? Which is a problem near and dear to my heart, given my traditional stomping grounds. But beyond that, it's a problem near and dear to everyone's heart and machine learning. But it's also a problem that's really representative of a problem that's important that you want to get good utilization of your system on.
00:17:41.492 - 00:18:44.162, Speaker B: And that in the past, whenever people would show up with their language du jour, I'm like, this language is very, very cool looking at least, but to me, understand how to use it. Like, show me how to do a dense matrix, multiply with it. And the answer would always be kind of like, oh, you want to use this for that? And I committed some horrible faux pas. And so at which point I'm like, okay, I don't know what I would use this language for, so go have fun with it. But I'll stick with my old c that I actually kind of despise, but I don't have any better alternatives. So what does Nvidia do? The first thing is we'll give it to you on everything available. And then the first problem we're going to show you is a performance oriented problem, like take your dense matrix multiply, take the four lines of code, the three nested loops, and here, run it on your x 86 processor, and then, hey, add these little annotations and put this thread index thing here and this block dim here and whatnot, and then run it again under the compiler.
00:18:44.162 - 00:19:21.292, Speaker B: And then all of a sudden they go from gigaflop scale performance to teraflop scale performance. And people get really, really excited and a lot of interesting institutional dynamics kick in. But suffice to say, everyone wants to port their code to a GPU. Now, what has Nvidia really done? Nvidia, with those little annotations has made it possible for a developer to express locality. All of a sudden I can say, this is near that. Then the compiler tools and whatnot can go in and put things near each other and be intelligent. They now know from the developer what things are related, what things aren't.
00:19:21.292 - 00:20:14.354, Speaker B: The developer, once they see that they get this kind of performance and that they can get these big improvements, will then spend their time figuring out how to further optimize their locality and push stuff. Now, when I tried to do that in any other ecosystem, including low level assembly on, like, x 86 processors or whatnot, everything fights you. It tries to pretend like, don't worry about it, it's all in a cloud somewhere. And it's all going to be good. It's like, no, locality is the defining feature of high performance computing and to the overall kind of cost structure. What I think we're talking about with AWS, like, you know, I think it's like a sign something's going horribly wrong when people are like training LLMs and whatnot on these systems and they're talking about like, we need to take all the geothermal power from Iceland or we need a nuclear reactor to train this. And I'm like, you know, somehow a baby, you know, with a, you know, a couple exposures to language and playing peekaboo can figure out language.
00:20:14.354 - 00:20:30.842, Speaker B: And so like, like, things are a little bit off here. Like, you need to be paying attention to this. And a lot of this is, is this very concerted effort to try to pretend these features aren't things to care about.
00:20:30.978 - 00:20:43.066, Speaker A: So we're talking about these concepts of locality and how important they are in high performance systems. So how have you taken that? And you and the team have taken that and adapted it to this networking stack in firedancer.
00:20:43.210 - 00:21:52.704, Speaker B: So like a lot of this, you go into the code base itself. There's a, like, every codebase has a utility layer, and the utility layer largely exists to establish a development environment as best we can with the generally portable tooling like C and Unix like and PoSix and so forth, these concepts. So one of the very beginning is just like a notion of, as a developer, I don't want to think about the details of which cores are running my thing, but I do need to know that there are cores running, and I need you to have some kind of notion of locality, kind of like an Akuda sense. So one of the very basic things is just like saying, hey, there are some physical execution resources that get allocated somehow by the operator of the system, by whatever configuration, topology, manager, whatnot is written for the system. But as a developer, you don't really care about that. That's not your job. What you just need to know is how have those been enumerated to me as a developer? Just like Nvidia, you're not coding to the details of a GPU, you're coding to this abstract model of like, tell me where the threads are and so forth.
00:21:52.704 - 00:22:39.244, Speaker B: And so this is also not that different from pre Nvidia, an MPI like model where most people get their time thinking about like the communication calls and whatnot. The most important part of MPI is just that it tells you you are like the 7th process within this collective and so you know, hey, whatever the 7th process in that collective is supposed to do, I can write the code for that process and so forth. So that's one of the very first things that's there. It's just a way of abstracting the execution resources there. Then there's a bunch of things built on top of that to like, like, you know, allow developers to flexibly allocate jobs and so forth. I want this execution resource over there to run this function, I want this to do that. So there's like thread pool models, there's a lot of other stuff.
00:22:39.244 - 00:23:41.686, Speaker B: There are similar things where all of the fighting of these misguided APIs and operating systems and whatnot, like the concept, I want some memory near me, you can say. If you try to say that in a modern operating system, everything fights you, you can't really say it cleanly in POSiX, you can't really cleanly say it in c, you can't really cleanly say it in system five or Linux or whatnot. Everything is trying to say, no, no, no, we know better, we'll figure out how to place your resources and whatnot. It's like, no, I need this resource near me. Don't second guess me, don't think it's a bluff. Create it, make sure it's real, make sure it's backed by DRam, make sure it's now in this numa node, whatever kind of feature. So it has those similar notions, like a topology manager can lay out the resources, can acquire the resources, lay them out for you, do it all in advance so you're not in a situation like I described before, where you decide, I'm actually going to use the resources I requested earlier and then discover the system going unstable as, as the Om killer or whatnot starts blowing things away.
00:23:41.686 - 00:24:36.002, Speaker B: And so then you are on a just stable substrate. Like I have these resources, they have been enumerated in a logical way that is independent of however they got procured at job startup. They're real, they're there, they're not going to get swapped out and whatnot. That just goes a long way to like fixing almost all the problems. Just kind of like having the threat index in a CUDA or having the MPI rank in an MPI thing goes a long way of just like you can start thinking about the system in distributed sense now when you go into the more specific details, then you're starting to look and say okay, cool, I have the ability to express, I need this amount of resources for that, or this amount of memory. For this other thing, you can start going through the data flow of what happens in a validator. So what happens in a validator? Well, the leader pipeline is essentially the most important time, and so transactions are coming in from the world.
00:24:36.002 - 00:25:04.388, Speaker B: You don't know if you should trust them. So you need to spend a lot of time cryptographically verifying them. And after you've cryptographically verified them, you're going to spend a lot of time trying to figure out like which ones with the resources I have available for execution are the best ones to execute. So I'm going to figure out how to pack up those transactions and then I'm going to send them off to my execution resources. You have a scheduling problem there. If you try to run everything sequentially, it can be quite slow. So you're going to do something which is akin to what happens on a super scalar risk processor.
00:25:04.388 - 00:25:44.106, Speaker B: You're going to look through the instructions and try to figure out which ones are independent and have them execute in parallel again. And then you're going to go back and then retire everything in order and then present the block that you've created to the world in this. Then if you're a developer, you're going to step back and say, okay, cool. For what we think is a reasonable platform for the community to use hardware whatnot, like a million TPS at the kind of speed at which you can verify a transaction kind of implies we need tens or whatnot of physical cores. Okay, cool. We'll allocate tens of physical cores at jobs startup. We will dedicate tens of physical cores on that.
00:25:44.106 - 00:25:51.260, Speaker B: We don't do something where we just create a bunch of threads and just hope the operating system will find a way to tetris them together. We'll do a really poor job of that.
00:25:51.402 - 00:26:26.200, Speaker A: Right. And this is like a fundamental, like for people like me who sort of jump in and out of this periodically and learn different pieces of it. Like the fundamental nature of a processor between, like when I was building like a gaming computer in 2008 and now like a server grade chip is like fundamentally different. And you have so many more cores to play with now that instead of having to rely on just thread management at the OS level or something below that, you can actually say, we're going to take this piece of code and it will always run on this core which we know is physically here in this cpu. Die.
00:26:26.352 - 00:27:08.394, Speaker B: Yeah, well you can do that if you are willing to fight the good fight against the operating system. The operating system doesn't let you now the low level APIs in firedance or Franken dancer let you do that. But these are also the same APIs we've used internally, the same APIs I've written in past lives for big giant supercomputers which are similarly distributed, or at least similar flavor, or for that matter the ones in CuDA, or for that matter the ones in MPI. It's not that it's a new concept, it's that people think it's new. As one of those signs that things have gone really off the rails of computer science. And you can't ignore this, there's a very real concurrency budget on a system. I can just step back and say, okay, you're talking about all these cores.
00:27:08.394 - 00:27:35.854, Speaker B: There's a certain number of instruction pointers. That's how many things you could be running concurrently. So I should be thinking how to allocate those instruction pointers to what I want to do. If I just say, okay, hey, I'm going to create a bunch of programs that are way more than that, then some magical entity that does not have the context or understanding of the application that I have, not the hardware, the application that I have. You're expecting them to do a better job of resource management than you would. It's like, no, it's going to do a worse job.
00:27:36.014 - 00:28:01.816, Speaker A: It's so interesting because the way, like in common parlance, the way multicore processors and multi threaded processors are talked about is, oh, you can now break up jobs and execute them in parallel across multiple cores, but that's not really what you're talking about. You're really talking about almost building discrete pipelines based on the physical layout of the cores and what you need them to accomplish in a certain amount of time. Is that accurate?
00:28:01.880 - 00:28:44.648, Speaker B: Well, it's reasonably accurate. Like this goes back to, there's not a language out there that lets me express data flow the way it needs to be expressed, but these topologies that I'm describing is like, okay, cool, we're going to have a core which takes data off a network interface, and that's what it's going to specialize in. It's going to fan that out to a bunch of cores that are going to specialize in doing signature verification. There is no sense in letting those things be flexibly allocated between different kinds of resources because I'm going to be running at load, I'm going to have all these things coming in. I know I always need them. If I have them constantly being context switched and moved around, I'm just incurring tons and tons of penalties where I'm wasting the cash, I'm thrashing, and I'm doing all the other things. They're like, no, I have a concurrency budget.
00:28:44.648 - 00:28:57.284, Speaker B: I'm going to allocate whatever fraction is appropriate for the application. Now this means I have to do resource planning upfront. I just can't do the kind of Hail Mary people are encouraged to do. Say, I'm just going to create a bunch of threads and the operating system will sort it out. It's like, no, it won't.
00:28:57.704 - 00:29:19.608, Speaker A: It's an interesting paradigm change too, because in this case a Solana validator, it is doing one job. You are not also running Microsoft office on this machine as well. So you can really like all these things that are made for sort of consumer level productivity app context switching, where you never know what the job is that's going to be run on a machine at any given time. That's not the world we're talking about here.
00:29:19.776 - 00:29:27.240, Speaker B: No, it's not. These are servers. Not only that, they are servers that are dedicated for purpose.
00:29:27.312 - 00:29:27.832, Speaker A: Yeah.
00:29:27.968 - 00:30:01.126, Speaker B: And that purpose has very like, you want a capacity engineer for your capacity. Don't just assume that somehow it magically appears. And so that's exactly what these tools at the kind of base layer are doing. So you set up the topology, there's cores that are dedicated towards like verifying stuff, and that's all they'll do. And they can be very specialized and very efficient. And not only that, because the processor architectures, their caches are always hot on the instructions you need. They're never thrashing the page table to access memory because you've pre allocated all the memory.
00:30:01.126 - 00:30:20.402, Speaker B: Everything can be hyper deterministic, hyper reliable and hyper fast, which is the exact properties people want. Yeah. So it's like, okay, cool, why don't people do this? Why do they think that they can ignore this problem, kick it off to something that's not designed for the space and not a weary application needs, and expect it to do a good job? It's like, no, it's going to do a terrible job.
00:30:20.558 - 00:30:41.054, Speaker A: Yeah, interesting. So let's talk about tiles. We've talked a bit about how there's these low level frameworks that you guys have built out that allows different pieces of code and different sort of execution bits to be assigned to different cores. How does this map to the tile architecture? And what's that long term vision look like?
00:30:43.714 - 00:31:29.734, Speaker B: So I think there's a bunch of concepts that are worthwhile to maybe start pulling apart. There's and like already mentioned there's a very real concurrency budget. There's so many instruction pointers, and if you're designing these systems, you want to be thinking what should each instruction pointer be doing and how you're going to assign it. And you also want to be finding ways to abstract this away for your developers. You want to give them the concept of here's an execution resource, but not be too much into the details. So tile can take on a lot of meanings here, like the lowest level, we can talk about processor architectures, and you look at how like a modern x 86 looks like it's a bunch of tiles that are replicated on dye. Many of them are general purpose processing cores.
00:31:29.734 - 00:32:07.576, Speaker B: Some of them are specialized for like memory controllers and whatnot, which is why we use the term, because it is very evocative of how modern systems actually work. And you want to map tightly on the hardware, give people an abstraction which actually is mappable tightly on the hardware, but then you actually get into it from an administrative point of view. And a operational point of view is it's a form of resource management. And the goal here is to encourage developers to do the right thing. Just like I mentioned before, CuDA allowed people to express locality. MPI allows people to express locality. It's here to encourage people to have a notion that locality is a thing.
00:32:07.576 - 00:32:42.092, Speaker B: And because it's a thing, like you need to be thinking, how many tiles are there? How many do you need? How are you going to allocate them? You don't want to know the details of the operating system, or is this an x 86 or RISC or an arm or whatever else. You just want to know that I have some execution resources. I can express that concept at a high level, and I can do the kind of resource planning upfront that people aren't encouraged to do today. And so I know this is what I need. I know I can allocate it when I start up a job. If I have what I need, it's going to run. If I don't have what I need, I'll be told before I'm running that this job is not feasible.
00:32:42.092 - 00:33:29.220, Speaker B: And then you can adjust whatever you might need to do in your configuration and so forth. Encourage that kind of resource planning that's not being done. Then you get into what the developer writes. A lot of what the developer writes is not any different from what you would write in a really like simple embedded microcontroller. You typically in a tile have a loop that just say, okay, while I'm running, occasionally I'm going to do some house cleaning off side, maybe set some diagnostics on some people, maybe check if I'm supposed to keep running other little things like that. The usual kind of things that you need to do. But then the bulk of the stuff is, okay, get inputs from my other tiles that are in part of my topology.
00:33:29.220 - 00:33:56.982, Speaker B: And this is also encouraging developers to think about the data flow through their system and then do whatever I need to do with those data flows. If I'm like talking a signature verification tile in firedancer, I'm pulling the network thing. Do you have a new unverified transaction for me? When it says yes, it's okay, cool. Then it does the verification. Then after it does that, it's like, okay, it passed. I'm going to forward that onto the next tile. Now, one thing that isn't as appreciated beyond doing the right thing.
00:33:56.982 - 00:34:27.090, Speaker B: It's not just doing the right thing in terms of resource allocation. It's not just doing the right thing in terms of thinking about data flow. It's doing the right thing by naturally creating internal security boundaries. These tiles themselves can be in their own address space, they can be their own processes. You can even start and stop them asynchronously with respect to other parts of the system. And so if you're looking at that, it allows you to also have things like, hey, here is a piece of code that's sensitive. It's dealing with private keys or whatnot.
00:34:27.090 - 00:35:23.438, Speaker B: We're going to put that on a tile in its own address space isolated to the side, whereas, oh, here's a tile that's running like an example we use internally is kind of quick, it's complicated, it has attack surface that takes a lot of effort to really pin down saying, you know what, let's not take the chance and have that piece of code running in the same address space, in the same security permissions as the piece of code that is running vm code that might be dealing with transactions or whatnot. We'll create these internal barriers. And this is a level of security that just doesn't exist as far as we can tell any applications today outside of maybe you can see some hints of this and things like the virtual machines and like a, a web browser for like Mozilla or Chrome or whatnot. But this naturally comes about having this, like I have well defined, these are the execution resources. They have a well defined barrier. Here's where I'm going to get my data. Here's what I'm going to do with it.
00:35:23.438 - 00:36:04.154, Speaker B: Here's how I've allocated it. And developers are now just doing the right thing naturally, and they're actually not paying attention to when they're writing their code. They're just in a logical space. This thing operates on this. They can write some generic code and that code will run fine on arm, it will run fine on a RISC V, it will run fine on x 86 or whatever architecture du jour. But the point is, none of this is anything which is requiring developers to understand the low level details. It's more exposing them what they need to do to be able to write performance systems that are portable, that are high, reliable and very, very robust.
00:36:05.254 - 00:36:21.068, Speaker A: The future of that is, correct me if I'm wrong, the ability for someone to maybe even bring in custom tile code and integrate that into a system, like if you needed to pull data flows out to dump it to some reason, you could do something like that. Or a Geeto system could interface with a tile.
00:36:21.196 - 00:37:02.408, Speaker B: Yeah. A really important part of the firedancer system is the messaging layer on top of that which effectively implements a multi producer, multi consumer queue. But it's also completely block free and it has a lot of features for ultra high performance and preserving, ordering, and a lot of things which are provided in other languages, like rust for the kind of analogy, something cross beam like. But this is designed day one to integrate with ultra high performance. These tile systems use no atomics, be portable, multiple architecture provide a lot of strong guarantees. Don't do any operating system locking or whatnot. And importantly, to your question, it's effectively an ABI for tiles.
00:37:02.408 - 00:37:47.912, Speaker B: So somebody just comes along and says, hey, I read messages this way, I write my messages this way, I will be as performant as any other part of fire dancer. The only thing I got to worry about is what my little operation needs to do here. So somebody wants to add on something, they think that they have some interesting parallel calculations they can run off to the side, or ways to augment or whatnot. An example of this would be things like an RPC server. Like we've implemented one, it sits off to the side, it's in its own address space, it communicates via these APIs into the system. We don't have to worry about saying how much do we have to audit this, versus say, auditing the parts that deal with the VM. Because we have natural isolation internal.
00:37:47.912 - 00:38:01.114, Speaker B: And that also allows us to bring in other developers, potentially use other tool change languages. We don't really care if you speak the ABI, you can just be a tile allocate one of your core resources to it and go to town, do what you want with it.
00:38:01.234 - 00:38:03.494, Speaker A: Yeah, that's pretty cool.
00:38:04.714 - 00:38:05.654, Speaker B: Thank you.
00:38:07.594 - 00:38:24.484, Speaker A: So right now, the tile implementation system, you're talking about verifiers, that's largely a signature verification, and then the scheduling and ordering of transactions. And then that's sort of where the Franken dancer project and the fire dancer, phase two, phase three separate, right?
00:38:25.104 - 00:38:26.248, Speaker B: I think reasonably, yeah.
00:38:26.336 - 00:38:49.964, Speaker A: Yeah. So let's walk through a little bit of that. So Frankenancer is part of what you showed last year at breakpoint, was shipped to testnet, is currently operating in some capacity on Testnet, and that's using all the fire dancer code on the networking side, but then the runtime and the consensus is the agave code underneath.
00:38:50.044 - 00:38:50.476, Speaker B: Exactly.
00:38:50.540 - 00:38:55.224, Speaker A: Yeah. Where is that project right now in terms of its transition to Mainnet?
00:38:57.044 - 00:39:14.558, Speaker B: Pretty far. There is a continuum here, and a lot of this is not a question of the technical capability. It's the comfort level. When we think that we can deploy this on Mainnet and have a really boring experience.
00:39:14.756 - 00:39:15.610, Speaker A: Boring is good.
00:39:15.682 - 00:39:47.964, Speaker B: Boring is good. I view this as a utility project. It should not be exciting when I turn on a light switch, the light should just come on here. It should not be exciting when somebody sends a transaction. The transaction should just run. A lot of what's going on. There is just massive amounts of auditing, both internal and external, and tuning and whatever else, to get us to a place where we can think we can give a, a very robust performant turnkey experience to the existing community.
00:39:47.964 - 00:40:06.940, Speaker B: The biggest thing that we're worried about is we jump the gun and our excitement, we say, okay, everyone go loose, and then a validator operator gets out there. In aggregate, there's a massive amount of stake that's suddenly running on it. And then there is something we didn't consider, and then things blow up, and it just devalues the whole project, devalues the change.
00:40:07.012 - 00:40:25.484, Speaker A: And as you're saying, this is sort of a little bit of a boring rollout. Initially, it's not like day one, we have Franken dancer on main net at the end of December, and suddenly Solana's transaction capacity has forexed. That's not what people should be expecting, correct?
00:40:26.064 - 00:41:25.124, Speaker B: No. Well, if things go like I'm expecting, yes, they should not be expecting that. The capacity might go up, but I would expect it to go up slowly for a myriad of reasons. Like, one, if the deployment is a boring deployment, like we're hoping for, you have a small amount of stake that's running on it that will grow with time and as people get confident, likewise, it's not like there is a million TPS of applications that are waiting to run, and the moment it turns on, those will appear. So even if you could somehow big bang this safely, and I don't believe you could, and just deploy everything, day one and everything, and agave has been upgraded using our techniques and whatever else, all those, all those things that like, okay, cool, you'd still probably see TPS about where it's currently at. You might see less congestion, you might see more reliability, but then what I'd actually expect is what you'd get in the real world, not big bang. You get some people running it, doesn't catch on fire.
00:41:25.124 - 00:42:09.514, Speaker B: More people start running it, more stake is backing it, hopefully fewer issues with congestion or any other kinds of faults or forks or whatnot on the chain. And then people can look and we can see what we've done in the lab environment on internal wans, internal lans and whatnot, and then say they can also see the code, it's open source. And look at it and hear my rants about tiles and data flow optimization and all those things, and then start saying, hey, we've kicked the tires of this, it seems to be running in the wild. And then you might see some of those larger scale applications start to materialize, pilot projects for them slowly. But I'm viewing this. This is a utility. It should be boring.
00:42:09.514 - 00:42:17.022, Speaker B: My dream for day one goes live on Mainnet is no one notices and it's a really, really quiet day.
00:42:17.118 - 00:42:55.154, Speaker A: Yep, that makes sense. So we asked just people on Twitter what were some of the questions around fire dancer, fire dancer rollout, Franken dancer, Franken dancer rollout, how these things will all integrate. And we got a number of questions that were some level of how will the existing agave client be able to keep up? Won't that sort of still be a limiting factor here? How does the process of actually scaling, let's just say we're at a place where there's very high confidence in firedancer. How do we move from the world right now of maybe four to 6000 tps to a world of 20,000 tps?
00:42:55.654 - 00:42:59.274, Speaker B: So that world doesn't scare me, actually.
00:43:00.654 - 00:43:01.794, Speaker A: What scares you?
00:43:02.174 - 00:43:35.842, Speaker B: Well, what scared me before was that question. I'm not scared of that question anymore. That was a question we were worried about day one, like, what would happen if we were able to suddenly show up with an order of magnitude and increased capacity for this. But nothing else got that increase in capacity. And I think a lot of, at the time, there was kind of a reaction that was like, oh, well, we'll just link in the fire dancer library and our stuff will be fast, too. Now, one of the big things I keep talking about is you need to think about dataflow and optimization. Day one.
00:43:35.842 - 00:44:18.396, Speaker B: It's not just a simple thing. My reaction at the time was, ok, the community is probably overestimating how easy it will be to upgrade the existing infrastructure on that. This is going to be a real problem. We saw a lot of models floated. Some of them were like, well, maybe fire dancer is the dominant. Other one kind of runs in the background or whatnot, kind of side by side. But one of the nice things about the Franken dancer model, one of the models that we kind of hypothesized might happen is other Franken dancers might come to be and people coming along saying, hey, we'll just use their networking stack and we'll write our own verification layers or our own things to do the transaction packing or this, that, the other kind of component.
00:44:18.396 - 00:45:10.854, Speaker B: And we are totally supportive. The tile architecture we are designed from day one is already well situated to do that. And when those ideas were initially kind of kicked around, the reaction was very, very chilly to. It was kind of like, no, it's going to be too difficult to do a mix and match. I think that as things become more real and people start realizing that this is not just a bunch of crazy academic talk or whatever, that they're going to start thinking maybe, maybe we should be looking at using the Franken dancer kind of model for that. So I think one, we're going to see more validators, we're going to see more mixing and matching of components between things, like a Solana or an agave or an Anza or our validator stuff. And I think kind of going beyond that, one of the things here, and I already alluded to this, was that conceptually, Solana is very sound.
00:45:10.854 - 00:45:59.140, Speaker B: They did design it with performance in mind from day one. So a lot of the real, real nasty conceptual faults just weren't there. And so in the process of making Franken dancer, where we are using the agave runtime and the agave consensus, we've been able to get that performance at the million TPS level. Now, when I say million tps, I'm going to put a lot of qualifications around that, because right now, today, the protocol does not do a million. So, like you gave the 200,000. 200,000 the protocol can do where it starts running into problems is there is an arbitrary compute unit limit at around 81,000. And so it's like, okay, hey, even if stap our fingers, Franken, dancer, fire dancer, agave are all upgraded, all million tps capable, the protocol itself will cap you at 81,000.
00:45:59.292 - 00:46:00.796, Speaker A: Okay, and that's just the Cu limit.
00:46:00.820 - 00:46:14.248, Speaker B: Yeah, that's just a c limit. Okay, so let's get rid of that Cu. There's other protocol limits that are implied by, like, how shreds are packed and distributed and whatnot. Those are, I think, around 430,000. Okay, you start ripping those things. Okay. Okay, now you can start doing that.
00:46:14.248 - 00:46:34.024, Speaker B: But, like, first you have to go through some essentially community governance to upgrade the protocol to get beyond that. So, like, getting to that 20,000. Okay, not really a problem. Getting to the 80. Beyond 81,000. There is a very clear need to tweak the protocol. Getting above, you know, a few hundred thousand requires we need to tweak the protocol again in a slightly different way.
00:46:34.024 - 00:47:04.820, Speaker B: But both of these, again, the protocols conceptually sound, these are fairly minor changes. These aren't like going back and saying, we need to rethink the entire model. It's just like, oh, no, change this enum here to that. And then all of a sudden that capability is possible. If the technology is present now, Franken dancer will be able to exploit that day one, whenever those limits get lifted. Agave, we sat down, we went through exactly what we did. What kind of changes are necessary? We think they'll be able to make those changes, too.
00:47:04.820 - 00:47:26.420, Speaker B: So we expect instead of the kind of thing before, like, what's going to happen, when it's going to share, it's like, well, no, I think what's going to happen now is agave is going to be plenty performant, and we're going to be plenty performant, and other people will be able to take their components and re implement in areas where they think we didn't do it right, do their own thing, and they'll be plenty performant. And so I think we're actually very, very well situated going forward for that. Right.
00:47:26.452 - 00:47:39.274, Speaker A: So it's like the lowest common denominator of performance among the clients sets the ceiling for which you can raise it to. And so if you're up here and agave's here, and suddenly they bring it up to here, the whole protocol can come up to that level.
00:47:39.314 - 00:47:40.106, Speaker B: Yeah, exactly.
00:47:40.210 - 00:47:48.362, Speaker A: Yeah. Which. And that is what the SIMD process is for. Right, exactly. We've changed many things through community votes, through that process already.
00:47:48.458 - 00:47:49.074, Speaker B: Yes.
00:47:49.234 - 00:48:01.822, Speaker A: So let's talk about fire dancer. The process then of taking this net new networking stack, adding the net new runtime component, adding the net new consensus component. Where is that project right now? In its development site.
00:48:01.878 - 00:48:56.584, Speaker B: So it's quite far down the path there as well. We've already done many demonstrations that we can run our VM interpreter at performance levels and match blocks on Mainnet, match blocks on Testnet, and so forth. This too is another one where it's a comfort level and you want to get to a point where you can say, okay, we think the risk and the rate of potential bank hash mismatches is adequately low. A lot of this is very, very, it's straightforward, but it's tedious exercise of really pinning down every single little behavior that are just easy to miss. Like, one of the pet peeves I often have is there's like, syscalls in the VM, and a lot of them have an argument of size. And then you can ask, okay, what happens if I pass a size to this of zero? And the answer is, no one really knows. Literally no one knows.
00:48:56.584 - 00:49:12.288, Speaker B: It's just like it wasn't really considered when a piece of code got deployed. And so some places will be like, oh, that's invalid. Oh, that's fine. It's a no op. Oh, no. Maybe it's a fault if the memory address for the zero thing points to an out of bounds region or whatnot. It's like, okay, there's a lot of different interpretations, but there's no consistent one.
00:49:12.288 - 00:49:47.284, Speaker B: And it's what I think of as an implicitly specified behavior. Yes, and what I mean implicitly specified, it's like, there is a defined behavior you have to match. Yeah, but no one knows what it is. And so if you're going through that component two and the component three, it's to surface all of that and get to a point where like, okay, we think we really have pinned down every single last bit of this protocol and what the behaviors are and that we'll match. And so, you know, it has kind of one of those, like, exponential like curves where it's like, okay, you know, we got 80%. Okay, that's pretty quick. Okay, we got 90%.
00:49:47.284 - 00:50:18.260, Speaker B: That's a little bit longer. Okay, we got 99%. That's a little bit longer. And then, like, you know, at what point between this trade off between time and comfort, do you say, okay, we think that this is, you know, production ready, turn the world kind of loose on that. And so there's a massive amount of work, and people can have already see how much we've been contributing, like the differential fuzzing and all the other kinds of stuff to help the community in general. This is also a service to the community. Other validators will benefit from this, from the, from our having to go through and essentially specify all of these areas of the protocol.
00:50:18.260 - 00:50:31.396, Speaker B: But it's a time consuming process, but it's also one that depends on essentially a comfort level. I've been staring at a lot of code lately. I have lots of examples of pinning down implicitly specified behavior.
00:50:31.500 - 00:50:46.284, Speaker A: So, yeah, yeah, so kind of zooming out and looking at the project in its totality, we're, I don't know what you'd say for percent wise through, but what are the components that you found were easier than you expected? What was harder than you expected?
00:50:46.824 - 00:51:50.464, Speaker B: So I really, really big picture, I have been shocked at how I think lucky we've been and maybe not luck is the word, but I don't want to characterize anything else in that. Also, we have all those incremental deliverables so that people can actually see we're doing the work. And those have been kind of hitting. Now you can actually look at those and start drilling down and saying, okay, which one seemed to be more effort or less effort than we anticipated back around breakpoint 2022? And so the area where we definitely underestimated the amount of effort was around the quick protocol implementation. At the time, our kind of spidey sense was like, this protocol is probably not what people want, but at the same time, we didn't have any credibility with the community. There was a lot of good faith, like Google clearly would have done a good job at the Qwik protocol and all that. And we really went into there to say we want to do a ultra high performance, ultra high security, ultra robust implementation.
00:51:50.464 - 00:52:57.146, Speaker B: And it's been a ton of work, and at the same time, it's been an area where we keep finding, like, it doesn't really solve the problems that a validator has. And so it's taken us a lot of time, and it's also now we are spending a fair amount of time in community discussions around essentially, let's the community independently has come to the conclusion that, yeah, Qwik actually is maybe not the right thing. It has some features, a very few number of features that might be useful, but generically, if you're looking at the kinds of problems of being a validator, it's not the right protocol. So there's a lot of effort now to essentially saying, hey, let's rethink this from ground up. Bring in people from our team, especially ones with low level hardware, networking experience, who can really come in and say, if you really want to implement a protocol that you can do really, really fast and really, really high performance and really, really stable, this is what it should look like. That's an area where it's been a lot more work than we expected. I would say that there has been a lot of complexity in the vm, that it's mostly been in the implicitly specified behaviors.
00:52:57.146 - 00:53:30.716, Speaker B: Just surfacing. All of them has been a lot of work. And that's probably the area where I'm like most concerned right now, is making sure to pin all of those. So that's probably an area where you under mess data, the amount of work, pleasantly surprised. A lot of stuff around databasing. I think gossip is an area of the protocol, or the gossip protocol where I wouldn't say it was more or less work than we thought. It's that I don't think people anywhere have a clear notion of the boundaries of that protocol.
00:53:30.716 - 00:53:48.636, Speaker B: So if I kind of look at where. Where. What our expectations for what it was and what other people's expectations for what it was, that's been an area where. Where I think there's probably still a lot of opportunity there, but at the same time, we've been able to go quite far with it. Yeah. So, yeah, I don't know. I can think of other areas of.
00:53:48.660 - 00:53:53.162, Speaker A: Project, but no, it's interesting. So I'm going to do a hard pivot now.
00:53:53.218 - 00:53:53.410, Speaker B: Sure.
00:53:53.442 - 00:54:23.492, Speaker A: Okay. Had some other community questions that had come in. Crypto is as much a technical project as it is sort of a social and philosophical project. You mentioned before. This is really your first exposure to the technology. You've obviously gotten very deep on the technical side, on the cultural side. Is that something you found interesting? Some of these very libertarian, self custody, frictionless money that moves anywhere around the world sort of components.
00:54:23.492 - 00:54:27.184, Speaker A: Is that like personally appealing to you as you've gotten more into the space?
00:54:28.164 - 00:54:49.596, Speaker B: I'd say. I'd say it was somewhat appealing in the past, too, as well. I mean, the thing I've always been hyper focused on, this goes to, like, very, very early stuff. Like, I saw the first white papers on bitcoin. I kind of read it. So that's kind of interesting, but I'm not really sure, like, what's it there for? Right. It's going, okay, that's a bunch of cool technologies, but I'm not really sure what it's about.
00:54:49.596 - 00:55:44.260, Speaker B: And then, you know, as things, you know, progressed and looking at, like, other stuff, like it's clearly stuff exploding. I'm plenty busy over on the traditional side, but kind of looking at it, I'm like, okay. You know, there's a lot of parallels to stuff that goes on in traditional finance. And so I'm like, okay, cool. What's here is unique. And the thing that I kept coming back to was just like, okay, what I call the trusted public ledger. I don't know what the hip term would be in crypto, but I'm just like, okay, at the core of finance, you have this base infrastructure layer of clearing, and we're going to do these transactions and this happened and whatever else that's a core at the crypto layer.
00:55:44.260 - 00:56:17.584, Speaker B: Now, if you go look in the traditional areas, okay, you have a lot of these ledgers floating around. And it's an evolved system. It's evolved over literally hundreds of years. And working in this area, it's like, I know a lot of the internal mechanics of that system. And so it's like, okay, cool. So I can kind of see how this is parallel to that, or this is maybe not parallel to that, but I'd keep looking at things and saying, okay, there's clearly areas in traditional finance where it's like, this is inefficient. It could be done better.
00:56:17.584 - 00:56:47.404, Speaker B: It can often blow up in very public kind of ways that people don't fully understand why it blow up because they don't know or are that all that interested in the internal mechanics. They just have some generalized distrust of it. And then you see over in the crypto area saying like, hey, we're going to go build an alternative to it. I'm like, okay, I can get why you might have these distressed. Some of it's justified, some of it's not. But, you know, I get where you're coming from, especially from the point of view that you have. Because the point of view that they have is the point of view I had before I worked.
00:56:47.404 - 00:57:06.092, Speaker B: Like I'm saying I'm not native. I'm also not native finance. I'm like, like native scientific computing. So I can have the outsider perspective on crypto. Have the outsider perspective. I can see why they view certain things the way they do in some areas. I was like, okay, yeah, yeah, that is an area that could be improved.
00:57:06.092 - 00:57:44.832, Speaker B: Now, the question I keep asking myself when I look at a lot of these things, is it an improvement? Is this actually a legitimate improvement? The thing I keep coming back to in this area which is like, okay, I can find examples in the traditional area of like a trusted ledger, but it's not public. Or I can find a public ledger but it's not trusted. But I can't find a trusted public ledger. At the same time. If you really want to be disruptive in this area, what is necessary is you need to have a performance which is competitive with the traditional areas. Otherwise it doesn't matter. And so I'm very supportive, at least at high level.
00:57:44.832 - 00:58:26.196, Speaker B: A lot of what their goals are, a lot of the details is like, okay, this area I think is kind of misguided or whatever else, but everyone's going to have those kind of opinions. But the area I'm like is like, it doesn't really matter. Like, like you're going back to, this should be like a utility should be really boring. You know, somebody coming home from a day of work and they stopped by the convenience store to go, you know, buy a pack of cigarettes or, you know, get some cola or whatever else and, and they swipe their credit card, you know, they're not going to wait for 15 minutes to finalize on that transaction. Like, you could say all you want about DeFi or DPN or any other, other kind of trends there, but it's like, like to be competitive, it has to have a level of reliability that that will just go through.
00:58:26.260 - 00:58:26.864, Speaker A: Yeah.
00:58:27.374 - 00:59:01.410, Speaker B: And so I'm like, okay, now I know how big the world is. I know about how often people do transactions and start extrapolating. And I'm like, if this kind of project that we're doing is going to matter at all, you really need to be thinking of a capacity in the millions of transactions per second. You need to be thinking about that with huge numbers of accounts to support, you know, these large scale applications. And you need to be doing that for both. Just the practical thing of, you know, people can find it usable and frictionless and the other things that you mentioned, but you also need it for just kind of availability. If like the system goes down, you need to be able to bring it up fast.
00:59:01.410 - 00:59:33.518, Speaker B: Like outages happen. Like a lot of these things, they happen all the time, but the systems have already built in resilience. And part of that is being performative enough that when things are booting up from scratch, you don't wait a week, you don't wait a year for it to come back online. It just is there very, very quickly. And so with all of this, like, I've just been hyper focused on performance. There's also why I'm hyper focused on Solana because I look at the other ones and I'm like, they didn't care. They've been raised in what I've described as the computer science magical thinking, where performance doesn't matter.
00:59:33.518 - 00:59:49.930, Speaker B: You can just kind of throw it at the cloud and scale wide, and somehow the problems go away. And it's like, no, that's not true. And this is actually a really hard problem, and it's one that is quite impactful, I think, if you can make a dent at it. And that's, you know, at least where my personal motivation interest kind of comes into play.
00:59:50.122 - 00:59:53.362, Speaker A: Doctor Kevin Bowers, thank you for joining us today on validated.
00:59:53.458 - 00:59:55.306, Speaker B: Again, thank you for having me. It's been fun.
00:59:55.410 - 00:59:57.882, Speaker A: Yeah. And we will see you in Singapore.
00:59:58.018 - 01:00:00.214, Speaker B: Yes. Guess you will.
01:00:00.834 - 01:00:01.674, Speaker A: See you then. Excellent.
