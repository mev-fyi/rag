00:00:09.200 - 00:00:51.092, Speaker A: I'm Austin, and this is validated today. I'm talking with Sam Williams, CEO and co founder of Arweave, a decentralized permanent information storage system. Sam is equal part software developer and philosopher. Although we get into some of the unique tech behind Arweave, like how its core block weave architecture works, most of our conversation explores something more fundamental. Why is permanent information storage important in the first place? And why is permanence important? This is a deceptively complex question. Sam has deep concerns about todays information space, specifically how it can be manipulated to distort perceptions of the truth. To quote George Orwells 1984, who controls the past controls the future.
00:00:51.092 - 00:01:30.692, Speaker A: Who controls the present, controls the past. I know this sounds like a sophomore seminar, but stick with me. As we know, authoritarian regimes and misinformation campaigns are not the stuff of fiction. They exist in the here and now. Sam is hopeful that censorship resistant, immutable storage systems like Arweave can play a part in making the truth, or more accurately, an objective record, more accessible forever. Currently, Arweave has user curated archives documenting the Russia ukrainian conflict, China's zero COVID policy, the 2022 crypto crash, and more, all of which offer permanent, high def glimpses into these events as they were unfolding by the people who witnessed them.
00:01:30.828 - 00:01:36.424, Speaker B: Let's jump in, Sam, welcome to validated.
00:01:36.804 - 00:01:38.228, Speaker C: Thanks so much for having me on.
00:01:38.356 - 00:01:40.784, Speaker A: Yeah, really excited to have you here with us today.
00:01:41.724 - 00:01:44.180, Speaker B: I want to talk about the role.
00:01:44.212 - 00:01:54.596, Speaker A: Of decentralized storage, the concepts behind it. But before we get into that, Arweave is a decentralized permanent storage system. What exactly does that mean and how.
00:01:54.620 - 00:01:55.744, Speaker B: Does the system work?
00:01:56.674 - 00:01:59.442, Speaker D: So Arweave is essentially an experiment, fundamentally.
00:01:59.498 - 00:02:11.666, Speaker C: In building the world's first, we think, truly permanent collective memory for humanity. The idea is to create a ledger of historical artifacts, essentially.
00:02:11.730 - 00:02:14.202, Speaker D: And that can be really anything. It can be your blog post, it.
00:02:14.218 - 00:02:18.770, Speaker C: Can be a video or audio, really? Anything at all. Yes.
00:02:18.802 - 00:02:20.122, Speaker D: And storing that in a way where.
00:02:20.138 - 00:02:22.034, Speaker C: It is distributed around the world such.
00:02:22.074 - 00:02:28.620, Speaker D: That there is no single point of failure, and then backing that up economically with an endowment based structure.
00:02:28.692 - 00:02:30.372, Speaker C: So essentially what we do is when.
00:02:30.388 - 00:02:34.732, Speaker D: You put a piece of data into Arweave, you pay for the storage of that piece of data for 200 years.
00:02:34.828 - 00:02:35.892, Speaker C: At the present price.
00:02:36.028 - 00:02:37.236, Speaker D: So cost of storage declines.
00:02:37.260 - 00:02:42.220, Speaker C: That 200 years expands out and sort of strange mechanic, but if the cost.
00:02:42.252 - 00:02:43.684, Speaker D: Of storage declines at a rate higher.
00:02:43.724 - 00:02:45.892, Speaker C: Than half a percent per year, you.
00:02:45.908 - 00:03:09.636, Speaker D: Actually end up with more storage purchasing power at the end of that year than you had at the beginning. Yeah. So its essentially an endowment, the backed by the deflationary of effect of technology. And so with this system, weve built a mechanism where you can store data perpetually all around the world in such a fashion thats never forgotten. Its really building actually on the first ever use of a blockchain. Funnily enough, Satoshi in the first block.
00:03:09.660 - 00:03:13.524, Speaker C: Of bitcoin, embedded a headline from the.
00:03:13.564 - 00:03:14.892, Speaker D: Times in the UK, I think it.
00:03:14.908 - 00:03:20.004, Speaker C: Was chancellor on brink of second bank bailout, which is sort of fitting.
00:03:20.384 - 00:03:21.416, Speaker E: But what they were trying to do.
00:03:21.440 - 00:03:29.104, Speaker D: With that was to show and sort of timestamp a piece of information in such a fashion that it could never be changed in the future. We thought that was a really powerful.
00:03:29.144 - 00:03:32.808, Speaker C: Part of blockchains, but when we started, no one was looking at how do.
00:03:32.816 - 00:03:39.488, Speaker D: We scale that up to fit arbitrary amounts of data inside. So that's essentially what we did with our weave. And now inside the system is about.
00:03:39.536 - 00:03:42.542, Speaker E: 450 million pieces of information spread all.
00:03:42.558 - 00:03:44.150, Speaker D: Across the world, available in people's web.
00:03:44.182 - 00:03:46.614, Speaker C: Browsers, everything from text to video to.
00:03:46.654 - 00:03:52.574, Speaker D: Audio to even full decentralized web applications. Something that we realized later about the.
00:03:52.614 - 00:03:54.718, Speaker C: System is that when you have permanent.
00:03:54.766 - 00:03:58.234, Speaker D: Information storage, you also have permanent application storage.
00:03:58.574 - 00:04:00.654, Speaker C: That means that you can create web.
00:04:00.694 - 00:04:04.446, Speaker D: Applications which are analog to smart contracts.
00:04:04.470 - 00:04:08.740, Speaker C: In some sense, that exist without a centralized controller or owner.
00:04:08.892 - 00:04:10.772, Speaker D: So an obvious example of this is.
00:04:10.788 - 00:04:12.756, Speaker C: That you can build, many people have.
00:04:12.900 - 00:04:16.380, Speaker D: Decentralized social networks on top of the system that store your posts and so.
00:04:16.412 - 00:04:22.188, Speaker C: On all around the world in such a fashion that no single person controls them. And with this, you can build, I.
00:04:22.196 - 00:04:30.876, Speaker D: Would say, credibly neutral web services in a way that's just not possible on the permanent or on the centralized web, and indeed is not really the focus.
00:04:30.940 - 00:04:37.880, Speaker C: Of smart contract based blockchain systems. So I weave, in that sense, is quite. Yeah, it stands on its own.
00:04:37.912 - 00:04:39.240, Speaker D: It's in its own kind of niche.
00:04:39.272 - 00:04:40.560, Speaker C: If that makes sense. Yeah.
00:04:40.592 - 00:04:44.816, Speaker A: There's two interesting parallels I'm thinking of here. One of them is the.
00:04:44.960 - 00:04:46.800, Speaker B: The concept of Moore's law, I think.
00:04:46.912 - 00:04:48.200, Speaker A: Folks are very familiar with.
00:04:48.232 - 00:04:48.424, Speaker B: Right.
00:04:48.464 - 00:04:58.884, Speaker A: Computational power roughly doubles every 18 months. And so, uh, you know, most people think about that as an acceleration of what's possible, but it's equally valid to look at that as a cost reduction curve.
00:04:59.224 - 00:05:00.324, Speaker C: Right, right.
00:05:00.754 - 00:05:04.770, Speaker D: I must say, I'm much less excited about Moore's law than Crider's law, which.
00:05:04.802 - 00:05:07.410, Speaker C: Is the sort of storage based opposite.
00:05:07.562 - 00:05:09.730, Speaker A: Could you just define Crider's law for us?
00:05:09.882 - 00:05:10.578, Speaker C: Yeah, sorry.
00:05:10.666 - 00:05:12.562, Speaker D: So Crider was, I think he was.
00:05:12.578 - 00:05:15.410, Speaker C: Like the CEO of Seagate or something like this.
00:05:15.482 - 00:05:17.962, Speaker D: He essentially made some assertions about the.
00:05:18.058 - 00:05:29.446, Speaker C: Speed at which hard drive costs would decline, and particularly not hard drive costs, but hard drive storage costs. So the gigabyte cost per hour, what's.
00:05:29.470 - 00:05:32.310, Speaker B: The function of the time component there?
00:05:32.502 - 00:05:44.114, Speaker A: I think most people would think about it and say, oh, the cost per storing a gigabyte is decreasing, but there's a time in there. You mentioned the cost of storing for an hour. What is that time coefficient?
00:05:44.534 - 00:05:50.462, Speaker D: Right. So an important part of this is to work out what the gigabyte hour cost is, because when you have a.
00:05:50.478 - 00:05:52.334, Speaker C: Hard drive, it has a certain price.
00:05:52.414 - 00:05:54.398, Speaker D: It has a certain capacity, and critically.
00:05:54.446 - 00:05:57.182, Speaker C: It has a certain drive life. Right.
00:05:57.278 - 00:06:06.438, Speaker D: A mean time between failure, we call it. That's fundamental for working out, well, how much does it actually cost to keep gigabyte of storage online? And so that's kind of the central.
00:06:06.526 - 00:06:08.142, Speaker C: Metric in the Arweave system.
00:06:08.318 - 00:06:16.310, Speaker A: Yeah, it's funny, I remember I was talking to someone who works in film production, this was probably five or six years ago, and they were actually saying.
00:06:16.342 - 00:06:19.542, Speaker B: That the cost of the storage of.
00:06:19.558 - 00:06:24.138, Speaker A: A Hollywood movie has actually gotten significantly more expensive when they switched from film.
00:06:24.186 - 00:06:26.002, Speaker B: To digital, because you used to be.
00:06:26.018 - 00:06:32.814, Speaker A: Able to shove a climate controlled room and you could stick the film reel in there and sure, there was heating and cooling costs, but fundamentally that thing.
00:06:33.114 - 00:06:35.114, Speaker B: Was a, you know, is a static cost.
00:06:35.234 - 00:06:39.890, Speaker A: And now with digital systems, there's actually a ton of maintenance work they need to do to make sure that the.
00:06:39.922 - 00:06:45.294, Speaker B: Digital copies of these multi, probably petabyte movies are kept up to date.
00:06:45.594 - 00:06:45.978, Speaker E: Right.
00:06:46.026 - 00:06:49.946, Speaker D: And that's one of the interesting things about using AOI. If you pay once to pay into.
00:06:49.970 - 00:06:52.704, Speaker C: The endowment, then you don't need to.
00:06:52.824 - 00:06:59.552, Speaker D: Essentially do the upkeep required to move the data from system to system over time. The network does that for you, which.
00:06:59.568 - 00:07:01.920, Speaker C: Is actually a pretty major total cost.
00:07:01.952 - 00:07:07.288, Speaker D: Of ownership saving when you look at the multi decade horizon, because you can say, okay, so I'm going to store.
00:07:07.296 - 00:07:10.928, Speaker C: The piece of information today on Amazon. Sure.
00:07:11.056 - 00:07:15.000, Speaker D: But seven years from now, they might change the offering. So you end up moving from Amazon.
00:07:15.032 - 00:07:17.022, Speaker C: To who knows what's new then.
00:07:17.038 - 00:07:24.046, Speaker D: If you want to store a piece of information for 100 years now, you've got 20 different times that an engineer has to come along and move it.
00:07:24.070 - 00:07:25.182, Speaker E: From system to system.
00:07:25.278 - 00:07:26.714, Speaker D: It's actually quite expensive.
00:07:27.374 - 00:07:27.990, Speaker C: Yeah.
00:07:28.102 - 00:07:30.438, Speaker D: It's a surprising quantity of the total.
00:07:30.486 - 00:07:31.966, Speaker E: Cost of ownership of a piece of.
00:07:31.990 - 00:07:40.078, Speaker C: Data over the century horizon just to move it around. Arweave is a protocolization of that system.
00:07:40.126 - 00:07:41.222, Speaker D: For you, so you don't have to.
00:07:41.238 - 00:07:42.746, Speaker C: Worry about it just happens.
00:07:42.770 - 00:07:45.866, Speaker D: And we incentivize miners to contribute storage.
00:07:45.930 - 00:07:48.802, Speaker C: At the lowest achievable cost in the.
00:07:48.818 - 00:07:52.426, Speaker D: Same way that you essentially do with bitcoin. What bitcoin is fundamentally doing as a.
00:07:52.450 - 00:07:58.002, Speaker C: Mechanism is maximizing the number of SHA 256 hashes you can do for a.
00:07:58.018 - 00:08:01.162, Speaker E: Certain cost, for a certain expenditure, essentially.
00:08:01.218 - 00:08:06.210, Speaker D: In terms of block reward. But with Arweave, Weve changed that SHA 256 hashing, which is arguably not very.
00:08:06.242 - 00:08:08.836, Speaker C: Useful for useful storage of peoples data.
00:08:09.010 - 00:08:14.096, Speaker B: So the origin of Arweave, there's a lot of tough problems in blockchain and.
00:08:14.120 - 00:08:22.724, Speaker A: Computer science to look at solving what, on a philosophical level, spoke to you about this particular problem of permanent storage?
00:08:23.264 - 00:08:26.904, Speaker C: Yeah. So the project got started, started thinking.
00:08:26.944 - 00:08:30.552, Speaker E: About it in, I guess, early 2017, which was just after the election of.
00:08:30.568 - 00:08:36.558, Speaker C: Trump in the US, and also after a couple of, let's say, destabilizing geopolitical.
00:08:36.606 - 00:08:37.438, Speaker D: Events around the world.
00:08:37.486 - 00:08:40.942, Speaker C: And ID always been fascinated, I suppose.
00:08:41.038 - 00:08:43.726, Speaker D: By the works of George Orwell and a few others.
00:08:43.830 - 00:08:46.422, Speaker C: And id been watching this, I would.
00:08:46.438 - 00:08:56.238, Speaker D: Say, technological dystopia slowly emerging around us. And also the opposite of that, this promise that the Internet had of producing a freer society.
00:08:56.366 - 00:08:59.510, Speaker C: And so there was these strange strands.
00:08:59.582 - 00:09:02.326, Speaker D: Going in parallel, and he wasnt really sure which way the world would go.
00:09:02.350 - 00:09:05.114, Speaker E: And I still, frankly, think thats the case today.
00:09:06.164 - 00:09:12.092, Speaker D: So I was looking at the destabilization of things, and I was thinking about how authoritarian regimes get started.
00:09:12.148 - 00:09:13.572, Speaker E: I was reading some books about how.
00:09:13.628 - 00:09:18.184, Speaker D: That happened in the Soviet Union, also in Nazi Germany and in North Korea as well.
00:09:18.884 - 00:09:21.540, Speaker C: I got a broad sense that what.
00:09:21.572 - 00:09:22.916, Speaker D: Needs to happen is that you need.
00:09:22.940 - 00:09:25.628, Speaker C: To, at some point, as an authoritarian.
00:09:25.676 - 00:09:28.220, Speaker D: Regime, gain control of the information space.
00:09:28.372 - 00:09:32.512, Speaker C: You need to gain a total grip.
00:09:32.688 - 00:09:35.208, Speaker D: On the information that people have access to.
00:09:35.296 - 00:09:37.168, Speaker C: And part of that is changing the.
00:09:37.176 - 00:09:38.696, Speaker D: Way that they think about the past.
00:09:38.800 - 00:09:42.336, Speaker C: Yeah, because George Orwell nicely outlined this, right?
00:09:42.360 - 00:09:49.848, Speaker D: He said, person that controls the present controls the past. Person that controls the past controls the future. And the whole idea there is, well, if you can change the way people.
00:09:49.896 - 00:09:51.816, Speaker C: Think about the past, and that is.
00:09:51.840 - 00:09:53.560, Speaker D: A consequence of the information they have.
00:09:53.592 - 00:09:56.832, Speaker C: Access to, then you can change the.
00:09:56.848 - 00:10:01.828, Speaker D: Way they act in the future. You change this sort of frame of reference, of understanding the world around them.
00:10:01.916 - 00:10:03.436, Speaker E: Which makes them act differently.
00:10:03.580 - 00:10:06.124, Speaker C: And so I'd been watching blockchain since.
00:10:06.284 - 00:10:17.964, Speaker D: Very early on, like 2010, 2011, I think, although I frankly didn't think much of it, I wasn't. I thought it was like magic Internet money, right? But I saw that there was that thing there, and that's pretty interesting.
00:10:18.004 - 00:10:21.140, Speaker C: This globally replicated ledger that can't ever.
00:10:21.172 - 00:10:23.292, Speaker D: Be changed, and it's cryptographically verifiable.
00:10:23.388 - 00:10:25.196, Speaker C: It's actually a pretty powerful idea.
00:10:25.380 - 00:10:26.956, Speaker D: And I'm sort of fond of this.
00:10:26.980 - 00:10:28.972, Speaker C: Philosophy that says that in life you.
00:10:28.988 - 00:10:36.972, Speaker D: Should find the single small problem you are capable of solving, and then you should set yourself to doing that. And then if everybody were to do that, then we would actually solve all.
00:10:36.988 - 00:10:38.524, Speaker C: Of the big problems pretty quickly.
00:10:38.684 - 00:10:43.092, Speaker D: But if we all set about thinking, oh, look at these enormous problems, we'll.
00:10:43.108 - 00:10:44.284, Speaker C: Never get anything done.
00:10:44.444 - 00:10:46.148, Speaker D: So I realized that my niche, I.
00:10:46.156 - 00:10:48.436, Speaker E: Was doing a PhD in distributed systems.
00:10:48.580 - 00:10:51.020, Speaker C: At the time, and I understood enough.
00:10:51.052 - 00:10:53.984, Speaker D: About blockchains, and I could see there's.
00:10:54.524 - 00:10:59.312, Speaker C: The, the blueprint or the prototype blueprint.
00:10:59.488 - 00:11:02.792, Speaker D: Of what a permanent information storage system could be in bitcoin.
00:11:02.928 - 00:11:04.912, Speaker C: And I thought, well, hey, we could.
00:11:04.928 - 00:11:06.496, Speaker E: Go build that and turn it into.
00:11:06.560 - 00:11:10.592, Speaker D: Something that's a proper ledger of history that can't possibly be changed by anyone.
00:11:10.648 - 00:11:20.980, Speaker C: Or any group on earth such that we can maintain a collective, verifiable, permanent record of humanity's history and culture.
00:11:21.112 - 00:11:24.540, Speaker B: I love this topic because there's all.
00:11:24.572 - 00:11:27.724, Speaker A: These allegories throughout history of a desire.
00:11:27.804 - 00:11:32.364, Speaker B: To collect all of the world's important information.
00:11:32.524 - 00:12:14.940, Speaker A: And I want to come back to that word important later, because I think that really is the crux of a lot of this. But you look at something like, obviously the library of Alexandria, or you even look in fiction and the Asimov foundation series, an initial major plot point of that is building an archive of all of human knowledge. So when the inevitable fall of this science fiction civilization happens, theyll be able to restart again. At the same time, if you look at paleontology, if you look at archaeology, 99.999% of all information ever created, whether that information is DNA, whether the information is written word or something along those.
00:12:14.972 - 00:12:20.466, Speaker B: Lines, has been destroyed and lost. And I think there's an interesting tension.
00:12:20.530 - 00:12:21.134, Speaker A: Between.
00:12:22.834 - 00:12:33.922, Speaker B: The fact that our digital material allows us to hypothetically store anything forever. But realistically, the Wikipedia page history is.
00:12:33.978 - 00:12:54.904, Speaker A: Probably one of the only records of a changelog on the Internet. There's webarchive.org and stuff like that. But even in most modern computer science systems, you throw out 99.999% of your log data. Very little log data is actually stored, unless you're working in financial trading or something like that. How do you think about, in its.
00:12:54.944 - 00:13:01.072, Speaker B: End state, how a protocol like Arweave conceives of important data well, fortunately, the.
00:13:01.088 - 00:13:09.444, Speaker D: Protocol doesn't have to. The protocol charges people for 200 years worth of storage, which is not as expensive as you might think, but it's also not that cheap.
00:13:09.844 - 00:13:13.396, Speaker E: Half a cent per megabyte, something like that. So it's a simple system that just.
00:13:13.420 - 00:13:20.748, Speaker C: Says, well, look, if a person thinks it's worth storing at a rate of half a cent per megabyte, then it.
00:13:20.756 - 00:13:22.652, Speaker E: Can be put into the system.
00:13:22.828 - 00:13:27.492, Speaker C: One of the things about arweave that few people, I guess, have thought all.
00:13:27.508 - 00:13:34.308, Speaker E: The way to the end of the thought on is that it's a democratized archive. So if you wanted to store information.
00:13:34.476 - 00:13:43.346, Speaker C: For hundreds of years previously, you had to have tens of millions of dollars on hand, at least. And then the likelihood that something goes.
00:13:43.370 - 00:13:46.970, Speaker E: Wrong is actually pretty high somewhere in that storage period.
00:13:47.042 - 00:13:54.890, Speaker C: Right? And most of these archives, they work on consistent donations from patrons, basically.
00:13:55.082 - 00:13:56.762, Speaker E: That's a pretty scary system in a.
00:13:56.778 - 00:13:58.454, Speaker C: Way that will fail.
00:13:58.954 - 00:14:03.762, Speaker E: It's just a matter of when. Whereas in Avius case, we say, look, you can pay for 200 years worth.
00:14:03.778 - 00:14:06.102, Speaker C: Of storage, and that 200 years is.
00:14:06.118 - 00:14:07.902, Speaker E: Very likely to expand out to thousands.
00:14:07.918 - 00:14:11.590, Speaker C: Of years worth of viable storage length, and you don't have to do anything.
00:14:11.622 - 00:14:17.954, Speaker E: Else with it, and you don't have any gatekeepers between you and the ability to store that information.
00:14:18.454 - 00:14:20.166, Speaker C: So, yeah, we think that that's a.
00:14:20.190 - 00:14:23.886, Speaker E: Pretty profound change in the way that this works. And it really opens it up to.
00:14:23.910 - 00:14:25.982, Speaker C: Well, I don't know what should be.
00:14:25.998 - 00:14:28.878, Speaker E: Stored for that length of time, and I don't really think that anyone in.
00:14:28.886 - 00:14:33.882, Speaker C: The world knows, but we give it to the user to say, well, you.
00:14:33.898 - 00:14:38.218, Speaker E: Decide, basically it's democratizing in the same way that the Internet is. I think that's really powerful.
00:14:38.346 - 00:14:39.034, Speaker B: Yeah.
00:14:39.194 - 00:14:50.810, Speaker A: I guess the reason I asked if Arweave distinguishes important information is because I'm curious how and if this impacts the system design. So, for example, you use a metric of cost of storage for 200 years.
00:14:50.882 - 00:14:51.970, Speaker B: I want to talk about where that.
00:14:52.002 - 00:14:53.114, Speaker A: Comes from in a second.
00:14:53.234 - 00:14:57.598, Speaker B: But I'm sure there's an implicit trade off in the ratio between the cost.
00:14:57.646 - 00:15:00.126, Speaker A: Of storage and the number of replicas, right?
00:15:00.270 - 00:15:01.646, Speaker B: Like you look at a rate array.
00:15:01.710 - 00:15:15.470, Speaker A: The number of redundancies of hot spares that you need increases the total system cost, which effectively increases your cost per gigabyte of usable space. If you want one replica, you need one extra drive. Two replicas, you need two extra drives, which cost money.
00:15:15.622 - 00:15:16.934, Speaker B: How do you think about that model.
00:15:16.974 - 00:15:21.390, Speaker A: In terms of arweave? How many replicas are enough? And how does that factor into the pricing model.
00:15:21.542 - 00:15:21.846, Speaker C: Yeah.
00:15:21.870 - 00:15:25.050, Speaker E: So at the moment, all we've used is 45 replicas as a base amount.
00:15:25.182 - 00:15:26.618, Speaker C: And we used 200 years.
00:15:26.666 - 00:15:28.094, Speaker E: This is a bit of a funny one.
00:15:28.594 - 00:15:31.546, Speaker C: So we were thinking about, like, what.
00:15:31.570 - 00:15:41.570, Speaker E: Are sensible parameters for the endowment, basically, and we settled on 200 years because we figured that there's a certain proportion of the population that is very skeptical about what will happen in the future.
00:15:41.722 - 00:15:43.906, Speaker C: And to that proportion of the population.
00:15:43.970 - 00:15:48.290, Speaker E: They don't tend to believe that anything is predictable more than three generations away.
00:15:48.402 - 00:15:49.746, Speaker C: So we say, okay, well, fine, if.
00:15:49.770 - 00:15:56.726, Speaker E: The cost of storage never declines again, then we have payment for approximately three human generations of your data storage.
00:15:56.870 - 00:15:59.766, Speaker C: But to everyone else that is more.
00:15:59.790 - 00:16:01.150, Speaker D: Optimistic about the future, you can see.
00:16:01.182 - 00:16:05.634, Speaker E: Very clearly how that would, well, cover the storage for thousands of years at least.
00:16:06.734 - 00:16:20.234, Speaker C: We set these parameters because we believe that it's really important that the system simply works rather than being cost efficient. I have this hypothesis that most of the areas where you want to use.
00:16:20.274 - 00:16:23.586, Speaker E: Permanent storage, you're not really that cost sensitive or not.
00:16:23.610 - 00:16:28.690, Speaker C: Like in the realm of whether it costs two cent or whether it costs.
00:16:28.722 - 00:16:30.330, Speaker E: $0.05, you don't really care.
00:16:30.362 - 00:16:34.186, Speaker C: What you care is what you care about is the system works, and so.
00:16:34.250 - 00:16:37.018, Speaker E: You can kind of jiggle these risk parameters.
00:16:37.106 - 00:16:38.242, Speaker D: But arweave is set up in this.
00:16:38.258 - 00:16:42.866, Speaker C: Way that says, look, let's use the most conservative risk parameters we possibly can.
00:16:42.890 - 00:16:48.736, Speaker E: To make the system economically stable for as long as it possibly can. And then after that, we expect that.
00:16:48.760 - 00:16:53.064, Speaker C: Basically at some point the last block in the network will be mined, but.
00:16:53.104 - 00:16:59.256, Speaker E: It will be a useful historical record at that point, and someone will teleport it into the next archival system, whatever.
00:16:59.280 - 00:17:00.168, Speaker C: That happens to be.
00:17:00.256 - 00:17:02.032, Speaker E: But really, the job of the economics.
00:17:02.048 - 00:17:04.080, Speaker C: In the system would carry it for.
00:17:04.112 - 00:17:06.480, Speaker E: That period and make it prominent enough.
00:17:06.592 - 00:17:09.392, Speaker C: That the people will essentially hoover it.
00:17:09.408 - 00:17:10.628, Speaker E: Up into whatever's next.
00:17:10.776 - 00:17:13.036, Speaker B: So what are the use cases for.
00:17:13.060 - 00:17:21.460, Speaker A: Arweave today in actual deployment? How are individuals, companies, groups using Arweave in the wild?
00:17:21.652 - 00:17:24.012, Speaker E: Yeah, well, the big one turns out.
00:17:24.028 - 00:17:26.156, Speaker C: To be nfts, at least during that.
00:17:26.180 - 00:17:32.364, Speaker E: Boom, it was huge use of the system. It was the majority for a while. And that makes sense, because if you.
00:17:32.404 - 00:17:39.218, Speaker C: Have digital assets that you're going to trade on, you need to make sure that that data doesn't disappear.
00:17:39.306 - 00:17:49.130, Speaker E: It's a nightmare. You buy a hundred thousand dollars JPEG, and then the worst thing that can possibly happen is the JPEG is gone. It's a real disaster waiting to happen.
00:17:49.202 - 00:17:56.794, Speaker A: Yeah, this is the old problem of a lot of the first generation nfTs, their media URL was just a link to Amazon Web Services.
00:17:56.954 - 00:18:01.042, Speaker C: Right, right, exactly. So are we was used in, I.
00:18:01.058 - 00:18:12.246, Speaker E: Think in Solana, 60 or 70% of the nfts that are minted. Same in other chains. But there's also other uses of the system, like social media, for example. Lens is starting to use Arweave to.
00:18:12.270 - 00:18:14.726, Speaker C: Store a lot of its data, same.
00:18:14.750 - 00:18:19.462, Speaker E: With a bunch of other systems. Meta uses arweave to store their NFT data as well.
00:18:19.558 - 00:18:20.006, Speaker A: Oh, really?
00:18:20.070 - 00:18:23.022, Speaker E: Kind of a. Yeah, yeah. We see a big growth in usage.
00:18:23.078 - 00:18:26.558, Speaker C: Around decentralized social media one way or another.
00:18:26.606 - 00:18:31.910, Speaker E: And that's pretty exciting to us because what got us into this in the first place was fundamentally allowing people the.
00:18:31.942 - 00:18:40.784, Speaker C: Right to, or guaranteeing people the right to free speech in cyberspace and to be remembered to have their artifacts, whatever.
00:18:40.824 - 00:18:48.600, Speaker E: It is that they think is important to be remembered over time. And social media is how people communicate on the Internet today. And so we see a lot of.
00:18:48.632 - 00:18:50.544, Speaker C: Use in that area, and that is.
00:18:50.704 - 00:18:52.288, Speaker E: Buoying to the team, I would say.
00:18:52.376 - 00:18:57.880, Speaker C: Like Mira, for example, uses, all of the crypto blog posts of the last.
00:18:57.912 - 00:19:01.044, Speaker E: Two years are stored on Arweave. And this kind of amazing history, actually.
00:19:01.454 - 00:19:01.830, Speaker C: Yeah.
00:19:01.862 - 00:19:10.678, Speaker A: Are you seeing use cases that are sort of more overtly social or have a more overt political bent to them? Like, NFT storage is obviously a great.
00:19:10.726 - 00:19:12.630, Speaker B: Use case, but it doesn't speak as.
00:19:12.662 - 00:19:14.870, Speaker A: Much that sort of memory hole problem you were describing.
00:19:14.902 - 00:19:17.942, Speaker C: Right, right, yeah. I must say it was quite a.
00:19:17.958 - 00:19:23.838, Speaker E: Strange thing when people started using the system for that. It wasn't what I'd anticipated to begin with, but it makes sense.
00:19:23.966 - 00:19:26.022, Speaker C: And it was actually a great sort.
00:19:26.038 - 00:19:29.256, Speaker E: Of forcing function to make sure that the system scales.
00:19:29.390 - 00:19:35.212, Speaker C: So the architecture of our, we've scaled since about version 2.0, which was released.
00:19:35.268 - 00:19:43.228, Speaker E: Two and a half years ago. And when I say scales, I mean it scales without limit, arbitrarily, horizontally. But of course, the technical implementations have.
00:19:43.236 - 00:19:44.544, Speaker C: To actually get up to speed.
00:19:45.084 - 00:19:48.956, Speaker E: Yes. And so when the NFT boom on.
00:19:48.980 - 00:19:53.668, Speaker C: Solana happened in like September 2020, I.
00:19:53.676 - 00:19:54.660, Speaker D: Guess it was 2021.
00:19:54.692 - 00:20:01.388, Speaker C: 2021, yes. Gosh, time flies. So, yeah, when that happened, we saw.
00:20:01.436 - 00:20:05.260, Speaker E: A huge growth in usage of bundling systems on the network, which is essentially.
00:20:05.292 - 00:20:08.980, Speaker C: How we get around the kind of scaling the double spend problem. Right.
00:20:09.012 - 00:20:10.276, Speaker E: So essentially what you would do is.
00:20:10.300 - 00:20:12.332, Speaker C: You would top up some tokens with.
00:20:12.348 - 00:20:14.116, Speaker E: A bundler, and then the bundler takes.
00:20:14.180 - 00:20:16.548, Speaker C: Data from all of these other users.
00:20:16.636 - 00:20:22.804, Speaker E: And it settles it as a single base layer transaction. And because it's permanent storage. We can do this in a hyperscalable.
00:20:22.844 - 00:20:25.454, Speaker C: Way, which actually we think we're very.
00:20:25.494 - 00:20:30.470, Speaker E: Lucky in the sense that we have a problem in the blockchain space that.
00:20:30.502 - 00:20:33.194, Speaker C: You can create a scalable solution to.
00:20:33.574 - 00:20:34.582, Speaker E: In a way that you don't have.
00:20:34.598 - 00:20:36.590, Speaker C: To make too many, do you say.
00:20:36.742 - 00:20:40.990, Speaker D: Too many major compromises, which is not really the case on the compute side.
00:20:41.142 - 00:20:47.926, Speaker E: But anyway, we had this growth in NFT usage and that really forced the infrastructure to improve.
00:20:48.070 - 00:20:59.134, Speaker C: And then it was just five months later that there was this massive push in the Arwiv ecosystem to archive records, mostly social media, but other sources as well.
00:20:59.294 - 00:21:01.078, Speaker E: From the start of the Russia Ukraine.
00:21:01.126 - 00:21:04.366, Speaker C: War and the community archived, I think.
00:21:04.390 - 00:21:08.486, Speaker E: About 80 million pieces of data in the first month and a half. 70 to 80 million.
00:21:08.630 - 00:21:08.886, Speaker C: Yeah.
00:21:08.910 - 00:21:14.718, Speaker E: And thats a pretty amazing archive. Its stored in so many places around the world, and its cryptographically verifiable.
00:21:14.766 - 00:21:18.338, Speaker C: Its really, its almost unfathomable that it.
00:21:18.346 - 00:21:19.706, Speaker E: Would ever get lost at this point.
00:21:19.810 - 00:21:22.306, Speaker C: And it has huge amounts of information.
00:21:22.490 - 00:21:27.826, Speaker E: Probably evidence of war crimes. And certainly its now a sort of event in history that was catalogued in.
00:21:27.850 - 00:21:30.234, Speaker C: High death and wont be forgotten.
00:21:30.354 - 00:21:32.266, Speaker E: So when we think forward like 100.
00:21:32.330 - 00:21:36.986, Speaker C: Years or 200 years, its pretty incredible. You should be able to look back.
00:21:37.130 - 00:21:44.806, Speaker E: At the start of that conflict with the same level of definition that you saw on, on social media as it was happening live.
00:21:44.806 - 00:21:48.586, Speaker C: The, that's something that's really powerful about the system over time. Yeah.
00:21:48.610 - 00:21:51.450, Speaker A: It's also a nice collapsing of the problem space.
00:21:51.562 - 00:21:51.954, Speaker B: Right.
00:21:52.034 - 00:22:00.786, Speaker A: The goal of Arweave is not to determine if a war crime took place in this certain situation. It's to collect and catalog evidence that.
00:22:00.810 - 00:22:04.454, Speaker B: Would allow someone to determine if that did take place.
00:22:04.794 - 00:22:05.266, Speaker C: Exactly.
00:22:05.290 - 00:22:16.800, Speaker E: It's a neutral ledger of information fundamentally, so it makes no assertions about what is true and what is not. The data is the only thing that exists, if that makes sense. They are ledger entries, which means that.
00:22:16.832 - 00:22:20.000, Speaker C: You have the piece of data, but you also have who put it there.
00:22:20.152 - 00:22:21.536, Speaker E: What was the time that they put.
00:22:21.560 - 00:22:24.392, Speaker C: It there, and these arbitrary tags associated.
00:22:24.448 - 00:22:25.776, Speaker E: With it, where they can add all.
00:22:25.800 - 00:22:27.328, Speaker C: Sorts of other metadata.
00:22:27.376 - 00:22:33.000, Speaker E: That's useful for working out whether it's true or not in the future. It's more like Arweave is an open.
00:22:33.072 - 00:22:41.048, Speaker C: Data lake for a start. What that enables is composable data. You can recall all of these archived.
00:22:41.096 - 00:22:49.408, Speaker E: Pieces of information, whether they be nfts or whether they be tweets about what happened in Ukraine, into any user interface you want. And this is permissionless.
00:22:49.456 - 00:22:54.432, Speaker C: You can just pull the data and remix it essentially. But one of the things you can.
00:22:54.448 - 00:22:58.528, Speaker E: Do on top of that is start contracts atomically on top of the data itself.
00:22:58.576 - 00:23:02.648, Speaker C: So you have the piece of data, it has an identifier, and then inside.
00:23:02.696 - 00:23:04.464, Speaker E: That piece of data it can have a contract.
00:23:04.624 - 00:23:07.522, Speaker C: One of the things that permafax, a.
00:23:07.578 - 00:23:08.922, Speaker E: Project in the army of ecosystem is.
00:23:08.938 - 00:23:14.146, Speaker C: Doing is taking this and saying, okay, can we make truthiness markets on top.
00:23:14.170 - 00:23:16.242, Speaker E: Of the data itself? So you can upload the piece of.
00:23:16.258 - 00:23:17.730, Speaker C: Data and you can say, hey, here's.
00:23:17.762 - 00:23:21.202, Speaker E: What I'm seeing in the world. Here's an assertion about what is happening.
00:23:21.338 - 00:23:22.098, Speaker C: Is it true?
00:23:22.186 - 00:23:36.210, Speaker E: And then people can interact with that in economic fashion in a sort of derivation of a prediction market, essentially to give the viewers. So for a start, the person that takes part, of course they make money if they're correct, but it also gives.
00:23:36.242 - 00:23:38.724, Speaker C: The users of the system sort of.
00:23:38.764 - 00:23:40.604, Speaker E: Richer information about what does the crowd.
00:23:40.644 - 00:23:43.704, Speaker C: Believe about the truthiness of this piece of data.
00:23:44.164 - 00:23:55.692, Speaker A: It's interesting because this was one of the original, if you go back to 20, 1718, people were really, really big on token curated registries as a way of potentially addressing this. The idea of a token curated registry.
00:23:55.748 - 00:23:58.020, Speaker B: Was there'd be a whole bunch of.
00:23:58.052 - 00:24:39.044, Speaker A: Entries in a database somewhere and you could basically attach your tokens to either things you'd like to see happen or things you like to think are true, or it was basically a system of voting on what was and what wasn't. I think this is really interesting in context of the Russia Ukraine war, because of course there's a massive disinformation campaign going on by Russia to limit the kind of access to information, but also just to create false narratives around what actually is occurring there. And the ukrainian side is producing propaganda as well, not disinformation as much, but they have a very excellent video editing team that's producing a lot of materials there. How do you think about.
00:24:41.184 - 00:24:47.688, Speaker B: The citizen journalism trend is very much a mixed pack, right?
00:24:47.816 - 00:24:56.704, Speaker A: And part of the way disinformation campaigns have been so successful is because they're able to take advantage of these social graphs that we have that sort of.
00:24:56.824 - 00:24:58.360, Speaker B: Fake aversion of truth.
00:24:58.512 - 00:25:01.328, Speaker A: And I'm curious how a project that.
00:25:01.496 - 00:25:08.704, Speaker B: Involves, for lack of a better term, voting or predicting on the validity of a piece of media that also seems.
00:25:08.744 - 00:25:13.284, Speaker A: Ripe for some form of manipulation. How are you thinking about problems like that?
00:25:13.864 - 00:25:15.968, Speaker C: So I think that, okay, for a.
00:25:15.976 - 00:25:26.656, Speaker E: Start, in the case of permafax, what's happening is that you can vote one way or another. And people can see who is voting. So there's a sort of transparency layer on top of this as well.
00:25:26.800 - 00:25:28.392, Speaker D: So you can quite easily detect if.
00:25:28.408 - 00:25:31.232, Speaker E: There are bot farms. And also you can farm money out.
00:25:31.248 - 00:25:33.706, Speaker C: Of those bot farms, which is kind of interesting.
00:25:33.850 - 00:25:34.962, Speaker A: How would that work?
00:25:35.138 - 00:25:37.874, Speaker E: Well, because they have to stake tokens on one side or another.
00:25:37.914 - 00:25:40.650, Speaker C: The mechanism is derivation of a bonding.
00:25:40.682 - 00:25:45.746, Speaker E: Curve on either side of true and false. But basically if you have some tokens.
00:25:45.770 - 00:25:47.138, Speaker D: On that side and they buy in.
00:25:47.186 - 00:25:53.450, Speaker E: Then you can sell your tokens for their liquidity. So there are certain mechanisms that make.
00:25:53.482 - 00:25:56.074, Speaker C: That, let's say disincentivized as well as.
00:25:56.114 - 00:25:58.562, Speaker E: On top of that, you can just analyze the social graph and you can.
00:25:58.578 - 00:26:03.132, Speaker C: Very easily see yourself well, and you can build systems that highlight what might.
00:26:03.148 - 00:26:16.908, Speaker E: Be a platform or not. But I think that, to the general point of citizen journalism, I personally believe that this is one of the most important and most powerful changes to society that's happened probably since the invention of the printing press.
00:26:17.036 - 00:26:20.348, Speaker C: I believe that essentially what the Internet.
00:26:20.396 - 00:26:22.572, Speaker E: Started to enable was something closer to.
00:26:22.628 - 00:26:30.294, Speaker C: Peer to peer communication at distance. And then that really sort of hatched into. We saw the.
00:26:30.454 - 00:26:32.150, Speaker D: So there was like forums and stuff.
00:26:32.182 - 00:26:41.430, Speaker E: Like this, but it was quite niche. And then that hatched into something bigger with social media, and particularly Twitter, which was an attempt to build fundamentally peer.
00:26:41.462 - 00:26:44.726, Speaker C: To peer communication at distance, which is.
00:26:44.910 - 00:26:48.934, Speaker E: You got to think about, how does the information architecture of the world work?
00:26:49.014 - 00:26:56.162, Speaker C: Right before that, it was basically, if you had control or power over the.
00:26:56.338 - 00:27:01.106, Speaker E: Machinery, fundamentally that pushed information to large groups of people, you could control the.
00:27:01.130 - 00:27:09.626, Speaker C: Way that they thought about the world. And with peer to peer media, that started to shift. There's this invention of the retweet in.
00:27:09.650 - 00:27:14.642, Speaker E: Twitter, which is so underrated, it's incredible. It's this idea that I can lend.
00:27:14.698 - 00:27:18.662, Speaker C: My voice to an idea so you can have a crowd of people completely.
00:27:18.718 - 00:27:20.974, Speaker E: Horizontally, sort of scaled, if you will.
00:27:21.134 - 00:27:28.086, Speaker C: And they can not like cleanly, but.
00:27:28.110 - 00:27:30.958, Speaker E: They can come to consensus, broad, rough.
00:27:31.006 - 00:27:35.726, Speaker C: Consensus about information about the world, like what is true just by lending their.
00:27:35.750 - 00:27:44.958, Speaker E: Voices to things instead of everyone shouting individually, which is a system that just doesn't work. I think that that is an unbelievably powerful change in the way that the world works.
00:27:45.006 - 00:27:46.932, Speaker C: And I think that there are, don't.
00:27:46.948 - 00:27:47.996, Speaker E: Get me wrong, it's not like the.
00:27:48.020 - 00:27:50.064, Speaker C: Crowd never gets things wrong.
00:27:50.964 - 00:27:56.316, Speaker E: And of course, we've seen look like the platforms themselves. It turns out that Twitter was not.
00:27:56.340 - 00:27:59.100, Speaker C: Peer to peer, it was peer to peer asterisks.
00:27:59.252 - 00:28:08.684, Speaker E: Apart from if you own the platform. So there are obviously problems, and that's what the decentralization of social media is coming for. But I think that you really have to look at what the old system was like.
00:28:08.764 - 00:28:35.228, Speaker C: It's like, okay, so, sure, there were problems with the new system, but ultimately, if you look at the record of centralized broadcast media over time, you see it's staggeringly bad. So let's think back to the last major theater war, 2003, Iraq. Well, the broadcast media in the west uniformly told everyone that that was because.
00:28:35.276 - 00:28:38.436, Speaker E: Of weapons of mass destruction. That fundamentally did not exist.
00:28:38.540 - 00:28:41.454, Speaker C: The Gulf of Tonkin, incidentally, what actually happened there?
00:28:41.534 - 00:28:44.686, Speaker E: Well, it's absolutely not what you read on the front page of the New.
00:28:44.710 - 00:28:46.150, Speaker C: York Times, that's for sure.
00:28:46.222 - 00:28:55.574, Speaker E: Oh, and then, you know, the start of World War Two, what was on the front page of the New York Times the day after Germany invaded Poland?
00:28:55.694 - 00:29:00.918, Speaker C: Well, it was that Poland had attacked a radio tower in Germany.
00:29:00.966 - 00:29:04.126, Speaker E: It wasn't true. They got that from nazi propaganda, and.
00:29:04.150 - 00:29:05.966, Speaker C: They repeated it on the front page.
00:29:05.990 - 00:29:09.260, Speaker E: Of the New York Times. This is not like a conspiracy theory.
00:29:09.292 - 00:29:10.636, Speaker C: It's just objective facts.
00:29:10.660 - 00:29:13.292, Speaker E: You can go read the archives. So what we are up against is.
00:29:13.348 - 00:29:16.628, Speaker C: The system that was fundamentally not very good.
00:29:16.676 - 00:29:18.764, Speaker E: But you didn't hear about it because they.
00:29:18.804 - 00:29:21.684, Speaker C: Well, the same people that were espousing.
00:29:21.724 - 00:29:33.068, Speaker E: What was essentially disinformation were also the people that controlled the ability to espouse whether they had spoken disinformation. And so it remained a fringe interest.
00:29:33.236 - 00:29:38.940, Speaker C: To look at media accuracy fundamentally. I think Noam Chomsky has written about.
00:29:38.972 - 00:29:44.060, Speaker E: This in the most powerful way. And one of the observations he makes is you don't need there to be.
00:29:44.092 - 00:29:57.196, Speaker C: Some conspiratorial cabal to have a propaganda system. What you need is just that people promote people like them for a certain period of time. Say, do that for 2030 years, and.
00:29:57.220 - 00:30:17.096, Speaker E: Then you end up with basically all the people around the technology, so around the machines that produce the broadcast of information to the rest of the population. They are all of one mind. And so it's not that, you know, why didn't they write this story in such and such a fashion that was maybe freer thinking. It's that they wouldn't have been in.
00:30:17.120 - 00:30:22.804, Speaker C: The room if they had that other opinion, and so it didn't get printed.
00:30:23.544 - 00:30:26.240, Speaker E: And so, yeah, I really passionately believe.
00:30:26.312 - 00:30:39.102, Speaker C: That the decentralization of the information systems of society will be the single most democratizing event in the last at least 200 years. And I think it's happening right now.
00:30:39.278 - 00:30:42.894, Speaker E: But it's messy as hell. But if we kept, like, a bigger.
00:30:42.934 - 00:30:45.886, Speaker C: Picture approach, I think it's really exciting.
00:30:46.030 - 00:30:46.430, Speaker D: Yeah.
00:30:46.502 - 00:30:56.946, Speaker A: The messiness is one of those things that I think is unavoidable, though, because there's this great book, stranger than we can imagine, by this guy, John Higgs. And it's an alternative history of the.
00:30:56.970 - 00:31:00.410, Speaker B: 20Th century, told not based on events.
00:31:00.442 - 00:31:06.362, Speaker A: But based on ideas. And one of the really interesting pieces there is the collapsing of the sense of reality.
00:31:06.458 - 00:31:06.898, Speaker C: Interesting.
00:31:06.946 - 00:31:09.134, Speaker B: And this is very closely tied to.
00:31:09.434 - 00:31:14.014, Speaker A: Discoveries in physics and mathematics, but also the machines of war.
00:31:14.314 - 00:31:24.826, Speaker B: And that the idea that there is an objective truth pretty much stops existing by the time you get to the.
00:31:24.850 - 00:31:30.954, Speaker A: Vietnam war in most of western society. And that this starts in World War one, it goes through World War two.
00:31:31.034 - 00:31:39.294, Speaker B: But we can never know enough to know what happened. And I think the place we're in.
00:31:40.154 - 00:31:43.682, Speaker A: Truly, objectively know what happened. But the place we're in right now.
00:31:43.778 - 00:31:47.206, Speaker B: As a culture of information is everyone's.
00:31:47.230 - 00:31:54.310, Speaker A: Just given up on trying to know what happened. This is how these disinformation campaigns are so effective. It's so in doubt, is that they.
00:31:54.342 - 00:31:57.406, Speaker B: Take advantage of that fact, that there.
00:31:57.430 - 00:31:59.894, Speaker A: Will always be counterpoints to every point.
00:32:00.014 - 00:32:07.126, Speaker B: And that the larger scope of what happened. You have to zoom out to actually.
00:32:07.190 - 00:32:20.980, Speaker A: See the details now. And it's very interesting because what you were talking about, one of the classic problems with Twitter, as you were talking about, are these sort of engagement farming accounts where they'll tweet something that's pro a project, and then they'll tweet something.
00:32:21.012 - 00:32:24.460, Speaker B: That'S anti a project, even though those.
00:32:24.492 - 00:32:26.380, Speaker A: Two things both exist in technically a.
00:32:26.412 - 00:32:29.556, Speaker B: Perfect information space that's completely public, the.
00:32:29.580 - 00:32:37.004, Speaker A: Linkup is not done. So because the algorithm feed, people only see one of those two perspectives. But the thing you've described is potentially.
00:32:37.044 - 00:32:41.184, Speaker B: The first system where truth is cheaper than fiction.
00:32:42.014 - 00:32:42.830, Speaker C: Interesting.
00:32:42.942 - 00:32:44.990, Speaker E: Yeah, I mean, I see that what.
00:32:45.022 - 00:32:46.150, Speaker D: We want to do with the data.
00:32:46.222 - 00:32:52.078, Speaker C: Is decorate it with as much sort of useful metadata as we possibly can.
00:32:52.126 - 00:32:54.590, Speaker E: That helps people decide whether it is true or not.
00:32:54.702 - 00:32:58.942, Speaker C: So the obvious first place is, well, who said it, right?
00:32:59.078 - 00:33:06.238, Speaker E: That's pretty important. And we have various systems that people have built in the arm weave ecosystem, like vouch Dao, which allow you to.
00:33:06.326 - 00:33:09.590, Speaker C: Vouch for someone's real world identity and.
00:33:09.622 - 00:33:11.286, Speaker E: Attach that on chain if they want.
00:33:11.310 - 00:33:12.646, Speaker C: To, don't have to use it, but.
00:33:12.670 - 00:33:17.302, Speaker D: You can if you want to. And so there are lots of systems like this, and there's permafacts which can.
00:33:17.318 - 00:33:20.102, Speaker C: Be attached to any piece of data on the network.
00:33:20.278 - 00:33:22.206, Speaker E: This is like a data protocol.
00:33:22.310 - 00:33:23.230, Speaker C: So you're uploading something.
00:33:23.262 - 00:33:25.878, Speaker D: You can just add one of these markets, or if something's already uploaded, you.
00:33:25.886 - 00:33:27.214, Speaker E: Can add a market to it.
00:33:27.334 - 00:33:36.194, Speaker C: We think that people will fundamentally need to come to terms with the fact that we cannot see things clearly and.
00:33:37.814 - 00:33:39.518, Speaker D: In a simple fashion, because the world.
00:33:39.566 - 00:33:41.038, Speaker C: Just isn't like that.
00:33:41.126 - 00:33:45.206, Speaker D: And I personally actually believe that there is an objective reality, but that actually.
00:33:45.270 - 00:33:48.062, Speaker C: Human brains are not capable of, would.
00:33:48.078 - 00:33:49.902, Speaker D: You say, perceiving it objectively?
00:33:50.038 - 00:33:50.334, Speaker C: Right.
00:33:50.374 - 00:33:51.598, Speaker E: Like, if you look at the machine.
00:33:51.646 - 00:33:56.374, Speaker D: That is a human brain, it's a relativistic machine, but that doesn't mean the world is relativistic.
00:33:56.454 - 00:33:59.974, Speaker C: It's just that our interpretation device is relativistic.
00:34:00.054 - 00:34:01.502, Speaker E: Now we have to deal with that.
00:34:01.518 - 00:34:05.682, Speaker C: It's just true. It's what. That's what it is. And so one of the things that.
00:34:05.778 - 00:34:11.594, Speaker D: We'Re really passionate about enabling is giving people all the tools they need to.
00:34:11.634 - 00:34:14.810, Speaker C: Sort of see the complex mass of.
00:34:14.842 - 00:34:18.498, Speaker D: Information about the world and try and understand it the best that they can.
00:34:18.626 - 00:34:38.550, Speaker C: But with a reasonable level of, let's say, certainty in their own beliefs. Because I just don't believe we used to live in a sort of quite a monopoly, but a quasi monopolistic information space where there was a certain set of facts, inversely commas, that were repeated.
00:34:38.622 - 00:34:39.710, Speaker D: Over and over again, and we just.
00:34:39.742 - 00:34:42.326, Speaker C: Believed them to be 100% true.
00:34:42.470 - 00:34:44.350, Speaker D: Actually, the weapons of mass destruction thing.
00:34:44.382 - 00:34:46.006, Speaker C: Here, I think if you asked american.
00:34:46.070 - 00:34:48.022, Speaker D: Population, it was almost 100% belief that.
00:34:48.038 - 00:34:49.606, Speaker E: That was true before it happened or.
00:34:49.630 - 00:34:51.758, Speaker D: Before we, of course, found out that wasn't true.
00:34:51.926 - 00:34:53.278, Speaker C: Yeah, we used to live in that world.
00:34:53.326 - 00:34:57.628, Speaker D: And I think what the Internet has done is it's. It's opened up the ability to see.
00:34:57.676 - 00:35:02.404, Speaker C: The complexity and the depth to things.
00:35:02.524 - 00:35:03.772, Speaker D: And now what we need to do.
00:35:03.788 - 00:35:05.236, Speaker E: Is give people the best tools we.
00:35:05.260 - 00:35:09.428, Speaker D: Possibly can to sort through that in the minimal amount of time and in.
00:35:09.436 - 00:35:13.668, Speaker C: The most transparent fashion, and that will sort of lead us to a, I.
00:35:13.676 - 00:35:15.044, Speaker D: Would hope, a brighter future.
00:35:15.204 - 00:35:15.612, Speaker C: Yeah.
00:35:15.668 - 00:35:17.596, Speaker B: You know, it's funny, because every bone.
00:35:17.620 - 00:35:18.876, Speaker A: In my body wants to agree with.
00:35:18.900 - 00:35:23.292, Speaker B: You, but at the same time, what we've seen is, like, the reason conspiracy.
00:35:23.348 - 00:35:25.260, Speaker A: Theories exist is because they usually have.
00:35:25.292 - 00:35:32.684, Speaker B: Some shred of truth to them, and its one or two leverage points that are true.
00:35:32.844 - 00:35:34.572, Speaker A: And then the rest of the conspiracy.
00:35:34.628 - 00:35:36.796, Speaker B: Theory is the conspiracy part.
00:35:36.900 - 00:35:41.156, Speaker A: But usually theres a few grounding pieces of information that are hard to refute.
00:35:41.300 - 00:35:48.864, Speaker B: And of course, that doesnt mean the conspiracy theory is true, but that information can keep it alive for years. It seems to me like you believe.
00:35:48.904 - 00:35:53.600, Speaker A: The more information we have, the more true we as humans will be able to perceive the world.
00:35:53.712 - 00:35:54.844, Speaker B: Is that accurate?
00:35:55.384 - 00:36:00.896, Speaker D: I think the meta brain of human brains can perceive the world. The alternative approach is someone has to.
00:36:00.920 - 00:36:02.584, Speaker C: Say, what's a conspiracy theory?
00:36:02.704 - 00:36:06.584, Speaker D: And I can give you an example that is now sort of out of the conspiracy zone, let's say.
00:36:06.664 - 00:36:08.192, Speaker B: Are we going to talk about UFO's?
00:36:08.328 - 00:36:14.154, Speaker D: No, no, no. Although that is an interesting one. No. Let's talk about the origins of the coronavirus.
00:36:14.344 - 00:36:14.694, Speaker C: Right.
00:36:14.734 - 00:36:18.702, Speaker D: So back in January to February 2020.
00:36:18.878 - 00:36:25.062, Speaker C: There was a concerted push by certain parts of the scientific establishment to label.
00:36:25.118 - 00:36:26.702, Speaker D: Anyone that was asking the question of.
00:36:26.718 - 00:36:33.718, Speaker C: Where the coronavirus came from as a conspiracy theorist. And this meant that essentially the social.
00:36:33.766 - 00:36:42.204, Speaker D: Media companies shut down any conversation about this for 18 months. Then they only unblocked conversation about this.
00:36:42.294 - 00:36:44.864, Speaker C: At the time that the US president.
00:36:44.984 - 00:36:49.352, Speaker D: Announced that there would be a major inquiry into where did the coronavirus come from.
00:36:49.448 - 00:36:52.928, Speaker C: And now it seems that the consensus.
00:36:52.976 - 00:36:54.456, Speaker D: Belief is something like, it may have.
00:36:54.480 - 00:36:56.320, Speaker C: Come from a lab, it may have.
00:36:56.352 - 00:36:59.800, Speaker D: Been natural, we don't know for sure. And it seems that over time, things.
00:36:59.832 - 00:37:03.816, Speaker C: Are tending more in the lab direction. But that is at least the part.
00:37:03.840 - 00:37:05.680, Speaker D: That seemingly everyone can agree upon.
00:37:05.832 - 00:37:20.150, Speaker C: If that is the case, let's say it's 50 50, which is approximately the consensus belief. If that's the case, and according to the WHO, 15 million people died, 15 million as a result of the coronavirus.
00:37:20.342 - 00:37:24.674, Speaker D: And there's a 50% chance that we accidentally engineered the virus ourselves.
00:37:25.574 - 00:37:27.006, Speaker C: That seems like something we should at.
00:37:27.030 - 00:37:32.902, Speaker D: Least have the ability to have a conversation about in a democratic, open, western society.
00:37:32.998 - 00:37:35.106, Speaker C: But we weren't able to do that.
00:37:35.270 - 00:37:46.334, Speaker D: And as a consequence, we didn't have the societal conversation about it. And so now we are continuing doing that kind of research. For example, in Boston recently, they made a chimeric coronavirus.
00:37:46.714 - 00:37:50.922, Speaker C: I'm not a biologist, I won't try and talk about the details, but essentially.
00:37:50.978 - 00:37:53.706, Speaker D: Added the fundamental parts that were more.
00:37:53.730 - 00:38:02.854, Speaker C: Dangerous about coronavirus v one to the transmissibility of Omicron. So he called it Omicron S. And.
00:38:02.894 - 00:38:13.846, Speaker D: They did this in a lab. And it's like, look, we didn't have a conversation as a society about whether that is a good idea as a result of information censorship on the Internet, and it killed 15 million people last time.
00:38:13.950 - 00:38:18.902, Speaker E: Could there possibly be a brighter warning.
00:38:18.958 - 00:38:39.962, Speaker C: Sign that says, look, allowing people to talk about things is important, and if we don't, we will make catastrophic errors as a society. I think that, like the. I understand the impulse towards you could say, monopolistic broadcast information systems, because it.
00:38:39.978 - 00:38:41.034, Speaker D: Makes the world simpler.
00:38:41.154 - 00:38:45.330, Speaker C: We can all believe a collective, single myth, but ultimately, the reason that we.
00:38:45.362 - 00:38:49.274, Speaker D: Chose democracies and we chose for there to be free speech inside those democracies.
00:38:49.314 - 00:38:52.938, Speaker C: Is because over millennia, we've learned that.
00:38:53.066 - 00:38:55.762, Speaker D: You have to allow the dissenting voices.
00:38:55.818 - 00:38:59.130, Speaker C: To speak, and it harms, like, it.
00:38:59.162 - 00:39:04.666, Speaker D: Hurts the collective myth. Like, if you believe the collective myth and you're exposed to this information that.
00:39:04.690 - 00:39:07.930, Speaker C: Says that it's not true, it's literally painful.
00:39:07.962 - 00:39:09.826, Speaker D: The brain rejects it to begin with.
00:39:09.930 - 00:39:11.778, Speaker C: But we have learned over such a.
00:39:11.786 - 00:39:14.654, Speaker D: Long period of time that that is better than the alternative.
00:39:21.174 - 00:39:27.606, Speaker A: How do you, in your calculus, weigh the difference between outcomes and truth?
00:39:27.710 - 00:39:28.862, Speaker B: What I mean is this.
00:39:28.918 - 00:39:31.406, Speaker A: Let's go back to the lab leak hypothesis.
00:39:31.550 - 00:39:32.782, Speaker B: This was something that had a very.
00:39:32.838 - 00:39:43.238, Speaker A: Close Venn diagram with folks who had an opinion that COVID wasn't serious, or folks who had an anti chinese bias. Inquiries about the origins of COVID are obviously valid and important.
00:39:43.366 - 00:39:46.402, Speaker B: But given the initial correlation, for people.
00:39:46.458 - 00:39:49.010, Speaker A: Who believed the lab leak hypothesis without.
00:39:49.082 - 00:39:52.986, Speaker B: Evidence at the time, what do you think of the implications of this line.
00:39:53.010 - 00:40:02.362, Speaker A: Of inquiry in the middle of a pandemic? I suppose the alternative is an overly simplistic narrative, a lie, if we're going to call it a lie, that can do more good than the truth, at.
00:40:02.378 - 00:40:03.514, Speaker B: Least in the short term.
00:40:03.674 - 00:40:09.138, Speaker D: So what comes to mind is, in the very first weeks of the pandemic, some people don't remember now, but it.
00:40:09.146 - 00:40:11.372, Speaker C: Is the case that people were told.
00:40:11.428 - 00:40:18.412, Speaker D: To wear masks is sort of morally wrong, because what really needed to happen was there weren't enough masks, and they.
00:40:18.428 - 00:40:19.532, Speaker E: Needed to go to the people that.
00:40:19.548 - 00:40:24.172, Speaker D: Were tending to people that had the coronavirus all the time, so that they didn't pick up the coronavirus, but they.
00:40:24.188 - 00:40:28.424, Speaker C: Were told that masks didn't work, when that obviously wasn't true.
00:40:29.044 - 00:40:33.104, Speaker D: I personally believe that you shouldn't lie to people.
00:40:33.524 - 00:40:34.904, Speaker C: I really believe that.
00:40:35.484 - 00:40:36.974, Speaker D: Life is very complex.
00:40:37.164 - 00:40:42.450, Speaker C: There are myriad different ways of looking at things. But trying to cover up the truth.
00:40:42.602 - 00:40:44.322, Speaker D: Generally doesn't have the effect that people.
00:40:44.378 - 00:40:47.362, Speaker C: The positive effect that people want, because.
00:40:47.458 - 00:40:50.090, Speaker D: They can't model what all of the.
00:40:50.122 - 00:40:54.466, Speaker C: Elements of the truth are like. One of the outcomes of these events.
00:40:54.490 - 00:40:55.930, Speaker D: During the pandemic, where sort of people.
00:40:55.962 - 00:41:00.258, Speaker C: Were told things that turned out not to be true for the collective good.
00:41:00.306 - 00:41:02.362, Speaker D: Say, is that now nobody believes the.
00:41:02.378 - 00:41:05.678, Speaker C: People that said those untruths. And that is something thats going to.
00:41:05.686 - 00:41:08.354, Speaker D: Haunt society for maybe decades.
00:41:08.654 - 00:41:12.034, Speaker C: And that loss of trust is a real cost.
00:41:12.494 - 00:41:14.302, Speaker D: I guess I believe a democratic system.
00:41:14.358 - 00:41:17.510, Speaker C: Is best because I believe that information.
00:41:17.622 - 00:41:19.554, Speaker D: Hubris is a real danger.
00:41:19.974 - 00:41:33.192, Speaker C: We may think that we know, but the likelihood that we actually do is quite slim. Theres this whole idea of truth decay, which I don't really like that phrase, because it implies that things were true.
00:41:33.208 - 00:41:33.920, Speaker D: And then they weren't true.
00:41:33.952 - 00:41:39.392, Speaker C: It's more that we just believe stuff that isn't true all the time. And I think that, well, in democracies.
00:41:39.448 - 00:41:41.664, Speaker D: Essentially, the whole principle of the idea.
00:41:41.704 - 00:41:43.824, Speaker C: Is we say, okay, well, yeah, it's.
00:41:43.864 - 00:41:45.720, Speaker D: Really, really hard for humans to know the truth.
00:41:45.752 - 00:41:50.096, Speaker C: So instead of having a cabal of people that do know the truth, that.
00:41:50.120 - 00:41:52.256, Speaker D: Choose everything for everyone else, we delegate.
00:41:52.280 - 00:41:53.640, Speaker E: The decision making to the group.
00:41:53.712 - 00:42:03.540, Speaker D: Even if the group may get things wrong sometimes, it's still better than the alternative. And to some extent, they at least live and die by their own mistakes then, rather than the mistakes of other.
00:42:03.572 - 00:42:06.916, Speaker C: People, which is a sort of basic form of respect, I think.
00:42:07.060 - 00:42:08.356, Speaker B: So going back to some of the.
00:42:08.380 - 00:42:13.420, Speaker A: Underlying technology that powers arweave. Arweave is technically not a blockchain.
00:42:13.492 - 00:42:14.564, Speaker B: It's a block weave.
00:42:14.684 - 00:42:17.740, Speaker A: Tell us a little bit about how blockweaves work and how they're different from.
00:42:17.772 - 00:42:20.052, Speaker B: Traditional blockchains, like what can you do.
00:42:20.068 - 00:42:23.784, Speaker A: On Arweave that you can't do on ethereum or bitcoin or Solana?
00:42:24.244 - 00:42:24.668, Speaker C: Yeah.
00:42:24.716 - 00:42:32.530, Speaker D: So essentially, Arweave is built such that you loop old pieces of information in the network into the production of new blocks.
00:42:32.642 - 00:42:58.498, Speaker E: And this forces miners essentially, to make replicas of that old information. And through fairly complicated mechanisms, we can prove that as you make more replicas of the data, you replicate more information, you are more likely to mine a new block. And so essentially, we move all of that proof of work to useful proof of storage of information and accessibility of that information, which is really the core of the system.
00:42:58.626 - 00:43:11.186, Speaker A: You talked a bit about the endowment model for Arweave. The endowment's really interesting to me because when you're trying to build stuff for a hundred year time horizon or 200 year time horizon, in your case, there's a lot of economic assumptions that go into that.
00:43:11.370 - 00:43:13.066, Speaker B: Obviously, one of those is that the.
00:43:13.090 - 00:43:17.162, Speaker A: Cost of storage will at worst stay the same and almost certainly decrease.
00:43:17.298 - 00:43:20.282, Speaker B: That's somewhat straightforward. But in terms of the endowment, if.
00:43:20.298 - 00:43:22.042, Speaker A: You look at the dominant currencies in.
00:43:22.058 - 00:43:23.740, Speaker B: The world, that can change on a.
00:43:23.772 - 00:43:29.196, Speaker A: Multi decade or multicentury time horizon, how does the Arweave token figure into the endowment right.
00:43:29.220 - 00:43:38.588, Speaker E: That's an interesting one. So there's a very fundamental reason we don't denominate in us dollars, which is that if you look at reserve currencies of the world over the last 500.
00:43:38.636 - 00:43:41.508, Speaker C: Years, you'll see that I think it's.
00:43:41.556 - 00:43:44.172, Speaker E: Something like four or five of the.
00:43:44.228 - 00:43:46.084, Speaker C: Seven are now worth zero.
00:43:46.244 - 00:44:14.014, Speaker E: So if you're thinking over long time horizons, you just can't use a fiat debasing currency. What we do is we denominate in Arweave tokens because they have some value when you want to store some information. Theyre also just a fundamentally scarce asset associated with this massive collection of human knowledge and history, and we think that has a value of its own. And then we have these enormous safety margins. So the actual declining cost of storage.
00:44:14.054 - 00:44:15.702, Speaker C: Over the last 50 years has been.
00:44:15.718 - 00:44:25.128, Speaker E: About 30% on average, and were expecting 0.5%. Which means that when you store a piece of information in Arweave, it's highly likely that most of the tokens that.
00:44:25.136 - 00:44:26.968, Speaker C: You put aside will never be released.
00:44:27.056 - 00:45:06.236, Speaker E: And so there's sort of deflationary effect on the token supply that happens, yeah. And as a consequence of this, so if the token price plummets, say, one day, well, then that 61 x safety margin becomes like 30 x or 15 x. And so we have these parameters set such that it's a risk model fundamentally, but it's open, transparent. You can go even use simulations if you want. There's some on the Arwiki that helped you run it for yourself and see what would happen if this happened to the token price or if that happened. And in the vast, vast majority of cases, this thing ends up being massively deflationary. But the alternative, if you were to denominate in dollars, is that things would die sometime in the next couple of.
00:45:06.260 - 00:45:08.652, Speaker C: Hundred years, long before the payment you.
00:45:08.668 - 00:45:12.268, Speaker E: Put aside for storage could be used up by the literal buying of hard drives.
00:45:12.436 - 00:45:14.196, Speaker A: Do you think a decentralized network like.
00:45:14.220 - 00:45:20.286, Speaker B: Arweave can survive those capitalist tendencies to throw a gatekeeper and a fee model on it?
00:45:20.390 - 00:45:32.674, Speaker E: Well, that's why we don't have a big dao, which can change the nature of the protocol as a result of coin voting. We designed Arwiv very, very specifically to minimize the effect of.
00:45:33.374 - 00:45:35.846, Speaker C: Well, really to design the rules of.
00:45:35.870 - 00:45:39.550, Speaker E: The game upfront such that they never change again. There doesn't need to be any governance.
00:45:39.582 - 00:45:41.302, Speaker C: Or debate about them, because that is.
00:45:41.318 - 00:45:42.094, Speaker E: Just what, ay, viz.
00:45:42.134 - 00:45:44.614, Speaker C: If you don't like it, go somewhere.
00:45:44.654 - 00:45:53.110, Speaker E: Else in the same way that bitcoin is like, okay, there's 21 million bitcoin. I don't care how many bitcoin Michael Saylor has, he cant vote to make 2 million bitcoin.
00:45:53.182 - 00:45:54.478, Speaker C: It doesnt work like that.
00:45:54.646 - 00:46:15.538, Speaker E: My rights as a user of bitcoin are protected by the network in the same way that in Arweave, my rights are protected by the network. Its never going to be anything other than permanent storage of information with the certain set of risk parameters that are baked in. Weve been talking about them for six years as a community, and everyone just agrees that theyre reasonable places to go.
00:46:15.686 - 00:46:21.042, Speaker C: And so it's the nature of immutability of the system outside of, you could.
00:46:21.058 - 00:46:57.066, Speaker E: Say, capitalist control in some fundamental form that gives it its power in the first place. I wouldn't trust a company, and as a consequence, I wouldn't trust these, like, v one daos, which are kind of direct democracy companies, really. Unfortunately, in a lot of cases, and doesn't, wouldn't necessarily end that way. But that is what we have today in 2023. I wouldn't trust them to run a permanent information storage system because there's no way to incentivize them to make decisions that are going to make sense 500 years from now, today, and particularly if it is based on who owns the tokens today. Well, they're going to find a way to extract value for themselves today, not in 500 years.
00:46:57.170 - 00:47:00.650, Speaker C: And so instead, what we've done is say, okay, let's come up with a.
00:47:00.762 - 00:47:09.414, Speaker E: Set of reasonable math at the beginning. Let's make it open, transparent, auditable, and then immutable. Fundamentally, just like bitcoin, it should never change.
00:47:09.794 - 00:47:14.582, Speaker A: Where can folks learn more about Arweave? Get involved in the project?
00:47:14.778 - 00:47:16.154, Speaker B: Run a node themselves?
00:47:16.574 - 00:47:23.814, Speaker E: Arweave.org is the, I would say, traditional homepage of the Arweave project, but also just go to wiki dot arweave dot.
00:47:23.854 - 00:47:26.438, Speaker C: De V, which is itself a perm web application.
00:47:26.486 - 00:47:35.366, Speaker E: No one owns it or controls it, and it's a wiki of all the information about Arweave that you need to understand it top to bottom, that is becoming the new focal point.
00:47:35.470 - 00:47:38.262, Speaker A: Well, Sam, thank you so much for coming on the show.
00:47:38.398 - 00:47:40.554, Speaker E: Yeah, thanks so much for having me. That was fun.
00:47:41.394 - 00:47:50.274, Speaker A: Validated is produced by Ray Belli with help from Ross Cohen, Brandon Ector, Emira Valiani, and Ainsley Medford. Engineering by podglomerate.
