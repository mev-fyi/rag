00:00:04.720 - 00:00:45.080, Speaker A: I'm Austin, and this is validated today. I'm speaking with Chris Greco and Lauren Culbertson. Greco, my co host for this conversation is Amira Valiani, head of public policy at the Solana Foundation. Chris is general counsel at Rain, a fintech company providing corporate cards for web3 based teams. But before that, he worked in the Department of Justice, where he led the department's review of major tech platforms and the investigation of section 230. Lauren is the ex head of government affairs at Twitter, where she led Twitters north american public Policy team, including Twitter's legislative positions on things like free expression and section 230. Aside from the fact that today's guests are a couple, one of the common threads here is section 230.
00:00:45.080 - 00:01:33.766, Speaker A: Section 230 is a law that limits tech platforms liability for user generated content. It's the only piece of surviving legislation from the Communications Decency act of 1996, and it's played a foundational role in creating the Internet as we know it today. Without section 230, we wouldn't have things like Facebook, YouTube, or Twitter. We also wouldn't have things like online ISIS recruitment or fake news. The rise of controversial content on social media platforms over the last decade has led to two recent Supreme Court cases, Gonzalez versus Google and Tamina versus Twitter, cases, which call into question the extent of section 200 thirty's protections for Internet service providers. In these cases, the court upheld section 230, but these challenges are becoming more common. Ive been wanting to take an in depth look at section 230 on this podcast for a while now.
00:01:33.766 - 00:02:35.110, Speaker A: I think theres a parallel to be drawn between how it enabled the web one honestly web zero revolution, and the place we find blockchains in today. They dont fit neatly into any existing category, just like the Internet didnt. I hold a minority opinion that section 230 could apply to blockchains. A straight reading of the text seems to cover exactly this type of interactive computer service that blockchain is, although it's often not invoked as a legal defense in the industry. At the end of the day, smart contract blockchains are messaging and compute platforms, no different from an Internet service provider or a cloud computing platform. But moreover, does blockchain need its own equivalent of section 230? And if so, what would that even look like? Where does the absence of a comprehensive section 230, like crypto law leave us in the United States? Chris and Lauren get into things like how and why the law emerged, different readings of the law, and how liability applies to the free speech of individuals as well as intermediaries in non Internet contexts. There's a lot to get into here.
00:02:35.110 - 00:03:02.000, Speaker A: So let's dive in. Lauren, welcome to validated. And, Amira, thanks for joining me to co host this one. I want to start off with a little section 230 history lesson, and then we can get into what the law means and its impact on the growth of the Internet. So where did this law come from, and what's the context it emerged out of?
00:03:02.152 - 00:03:11.262, Speaker B: I nominate Chris to give you a little bit of the legal landscape leading up to section 230, which was the impetus for this landmark law to go into effect.
00:03:11.448 - 00:04:18.524, Speaker C: Okay, so, well, my favorite part about section 230 is that it all comes back to the Wolf of Wall street and Jordan Belfort. So, for those of you who remember the movie Jordan Belfort, Leonardo DiCaprio's character is running some illegal operations. Basically, the company's called Stratton Oakmont. And the kind of seminal case that caused section 230 to come into existence is based off of a lawsuit where Stratton Oakmont, that same company, decided to sue Prodigy for some defamatory content that was posted on their platform. Prodigy had a policy of potentially removing that content. And unlike other situations where courts had found that the hosting of content did not impart liability for defamation, the court in this case found that Prodigy was, in fact, liable to Stratton Oakmont for the content, despite the fact that, you know, in hindsight, the defamatory content was about fraudulent IPO issuance, which is clearly seen in the movie. So the reality is that what they were accusing them of being defamatory was actually factual, which is usually defense for defamation.
00:04:18.524 - 00:04:24.220, Speaker C: But needless to say, some legislators saw this judicial outcome and wanted to fix it.
00:04:24.412 - 00:05:02.732, Speaker B: Yeah. And that leads to what was happening in Washington. You have two House, one Republican, one Democrat, and that would be Chris Cox from California and Ron Wyden from Oregon. They had been having these lunches and talking about, we want to work together. We want some sort of bipartisan legislation on emerging technology. Well, fast forward. Chris Cox is sitting on a plane, and he reads about the Stratton Oakmont case, and he says to himself, if the Internet, if these platforms are going to be sued every single time this happens, we're never going to be successful, we're never going to foster this growth.
00:05:02.732 - 00:05:53.056, Speaker B: And so he went back to Ron Wyden. He said, I've got this great idea. They're like, let's go. Meanwhile, in the Senate, on the other side of the Capitol, you had a senator by the name of James Exxon, and he was hellbent on getting rid of indecent content on the Internet, particularly content that children could be exposed to. And so, fun fact, he used to walk around the Capitol with a binder of Internet porn that was printed out so he could show people and say, hey, look at this. We've got to protect the children. And so when Congress was passing a rewrite, so, an overhaul of the 1934 Communications act, which became the 1996 Telecom act, they included a provision from Senator Exxon called the Communications Decency act.
00:05:53.056 - 00:06:54.332, Speaker B: So the Communications Decency act was really aimed at getting rid of tamping down on indecent content, particularly content that children could be exposed to. And Ron Wyden and Chris Cox said, we actually have this more pro speech approach to online content. And that was section 230 of the CDA. And their proposal was really this, I think, genius construct where it provided Internet companies Internet platforms with both a sword to go after content that's problematic or harmful, and the shield for liability for hosting that content. So it has this kind of balanced approach that, again, has a very pro speech, pro civil liberties aspect to it. And so that's how 230 became part of the CDA, which was part of the Telecom act of 96, which is still the kind of most significant piece of Internet telecom law that's in. In effect today.
00:06:54.508 - 00:07:26.322, Speaker A: Yeah. So I want to kind of go back a little bit, because we're setting the scene of stuff that was happening, like, very much in the early nineties. And that is interesting in its own right, because the state of the Internet today is so much more different than it was back then. Section 230 is, like, very simple, right? No provider or user of an interactive computer service shall be treated as a publisher or speaker of any information provided by another information content provider. It's like one of the most simple pieces of legislation you can read. Very human readable. I think most people reading this intuit probably 80% of what the impact of this sort of thing is.
00:07:26.322 - 00:08:05.966, Speaker A: But this wasn't the case beforehand. Right? As you were saying, like, if the New York Times, your classic example of a publisher, writes as an article on its website, something that is not true and defamatory, they are open to a lawsuit. Still, the classic example right now is Fox News and Dominion voting, where Dominion was able to, in a settle out of court, but was able to basically put a pretty good case together of a defamation case against them from Fox News. This is like the same thing that makes it possible for someone to go on Twitter and say something that is objectively false and Twitter to not be liable. But this was not really the state of the Internet before this went into effect. Right.
00:08:06.070 - 00:08:50.604, Speaker B: Well, I think you also have to look at beyond 230. You had the first amendment in place, and I like to say it's first for a reason. It's a fundamental foundational law that we have in this country. I think what section 230 does is it provides an additional layer of protection on top of that. And that's really the beauty of it. Of course, yes, you have the publishers like the New York Times and Fox News, but what Ron Wyden and Chris Cox realized was, given the rise of the Internet and online speech, you have so much speech out there that if these platforms had to have an editorial process for hosting the speech for every single piece of content, then it just wouldn't be scalable or sustainable.
00:08:50.764 - 00:09:02.308, Speaker A: Sure. I mean, you could see email as being something that would be completely impossible without a carve out of something like section 230, let alone the comments section on a news website or any sort of modern social media.
00:09:02.436 - 00:09:22.654, Speaker B: Yeah. And what I think people forget is section 230 also protects kind of your average Joe Internet user for when they retweet something or when they're doing some sort of curation of content on their webpage. If anyone had a geostidies back in the day, they'd appreciate section 230.
00:09:23.114 - 00:09:54.038, Speaker D: Section 230 has been the foundation of a lot of what we know is the modern Internet. I know you both have different views on that. Chris, when you were at DOJ, you worked a bunch on re examining section 230 and ideas for how we might want to reform it, or some of the issues that might be baked into this law. And I'd be curious to hear, from your perspective, one, if we do a retro of section 230, how do you personally feel about it? How might you think about the issues baked into it? And two, how you might think about changing it. If you think we should change it.
00:09:54.206 - 00:10:28.128, Speaker C: Yeah, that is a super difficult question. We spent, gosh, 912 months doing a real deep dive, talking with people and coming up with some potential legislative solutions, all of which are still available on the DOJ website. I mean, I think my biggest concern is, I agree with Lauren. Right. Like it is, it is fundamentally a protection for First Amendment speech. And the fact that section 230 exists does amplify speech, which I generally think is a good thing. And I do think it's probably been taken a little bit too far.
00:10:28.128 - 00:11:08.986, Speaker C: So, you know, we can get into this later. But section 230 is the only part of the Communication Decency act that remains. The rest of it was thrown out in court as being a violation of the First Amendment. And so this one provision has remained and really been the sole thing that survives from this law. And then the tech industry did a really good job of strategically litigating and defending section 230. I think, not to speak for Lauren, she'd say, defending what it says, I would argue potentially expanding what it says. I think my qualms with some of the provisions are that it wasn't passed in a vacuum.
00:11:08.986 - 00:12:04.364, Speaker C: The entire title of section 230, at least the parts that everyone relies on, are protection for good samaritan blocking and screening of offensive material. And tech companies these days don't use it to balkan offensive material. And, in fact, many people don't use it for good samaritan protection. There's plenty of websites that kind of solicit illicit material or kind of unsavory material, whether it's revenge porn or harassing material, those things should not be entitled necessarily to what is essentially a carve out from normal liability protection. And I think the other thing that people forget about section 230, if remove section 230, that does not mean that, like, a lawsuit is automatically going to win against the company that's being sued. They still have any of the normal protections that they would have as a civil litigant. They just don't get this added benefit of, like, a get out of court free card.
00:12:04.784 - 00:13:03.830, Speaker A: It feels to me like a lot of these various pieces of legislation that govern or don't govern how information moves in digital spaces are. To say that they're outdated is, I think, obvious, right? Like, when you look at the way these things are structured, none of them are built with the understanding of mass peer to peer communications in mind, right? You've got something like section 230, which, at the end of the day, the context of 1996, Internet is one where the most user generated content is like a static geocities page, or it's like the beginning of the idea of a post on a. It's not a peer to peer messaging system. It's not Twitter, it's not Facebook. It's certainly not blockchain. But, you know, a lot of the things that we may consider to be objectionable content in some form on the Internet are completely legal to write down on a piece of paper. For example, I can write a letter full of defamatory material, and if I don't distribute that, that is not illegal for me to have.
00:13:03.830 - 00:13:29.644, Speaker A: No one can sue me for writing a document that says lots of mean things about Amira. But it's a problem then, if I go and post that online, that transition there, when something moves from being something that is fine to potentially not fine because of distribution is very interesting to me as a content switch. What is the sort of thinking there, or basis for when distribution becomes something that changes the nature of information?
00:13:30.024 - 00:14:09.384, Speaker B: Austin, you raise a really great point around content amplification, but I think at the heart of section 230, it's really about putting the liability on the spot speaker instead of the intermediary, which I think still stands the test of time, because it's still kind of zeroing in on who's doing the speaking, who's doing the hosting. But you raise a really interesting point, and that is about targeted recommendations, algorithms recommending content. That's something that the Supreme Court recently looked at. It's something that a lot of different states and countries are looking at.
00:14:09.764 - 00:14:20.904, Speaker D: Maybe now's a good time just to lay out those two court cases. Does one of you sort of want to review them quickly and their backgrounds and what the court said, because these were kind of like, they turned out to be nothing. Burgers is my perception.
00:14:22.204 - 00:15:32.696, Speaker C: They certainly did. I mean, the details of the case are a little bit irrelevant. It's the Gonzalez case and Tamina case. But basically, the cases are essentially premised on the fact that these social media companies have made algorithmic recommendations that have caused some kind of harm in the real world, and whether or not they are liable for that. So, for the past 20 years, these social media companies have basically said a recommendation, whether it's made by an algorithm, whether it's made by our editors on a blog post or whatever, we are protected by section 230. I think in recent years, there's been a real question about how much is the algorithm an editorial choice versus just pushing content to people that's increasing viewership or whatever, and where that lies in terms of section 230 liability. If you've read the cases or listened to the cases, the arguments were extremely convoluted, and in my opinion, like very poor representations of the anti section 230 side or the anti tech company side in those cases, and you heard the justices wrestle with some really difficult questions.
00:15:32.696 - 00:15:49.834, Speaker C: I mean, one of the arguments that the plaintiffs were making were essentially that, like, the little thumbnail YouTube videos that were recommended by YouTube are not protected by section 230, because YouTube is coming up with those themselves. But, like, the actual content is protected.
00:15:50.134 - 00:16:00.482, Speaker A: Which is amazing, because, of course, from a technical standpoint, YouTube re encodes all video you upload. If that was seen as an editorial judgment, effectively a CDN is now a publisher.
00:16:00.638 - 00:16:31.696, Speaker C: Yeah, I mean, I think it's. It's. It was a really hard case to make. I'm not sure how they got into that very minute argument. One of the things that I think echoes through both these cases and some of the other cases that have come up previously about this is like they're making algorithm recommendations that have, like, these far reaching consequences. Most of the consequences of their actions would get eliminated by a court further down the line in the court case. And so in this case, it was terrorist incidents or bad outcomes happen to somebody in the real world, that they may not be able to prove liability in a normal situation anyway.
00:16:31.696 - 00:17:02.560, Speaker C: But they get knocked out on 230 grounds in these cases that the Supreme Court decided that the anti Terrorism act didn't apply to the situation, which was the underlying complaint, so they didn't have to rule on the section 230 grounds. But I think it is a really interesting question of when does algorithmic amplification and kind of forcing people towards certain content? When is a company liable for the ramifications of that content in the same way that, like, a product liability lawsuit, would leave you liable to the outcomes of your faulty widget.
00:17:02.672 - 00:17:17.840, Speaker D: And to be clear, I think the answer we got from the court is we don't have an answer for you yet. Right. There's basically no clear limit in terms of amplification or sort of what it means for an algorithm to recommend a certain video or tweet or whatever it is. Is that a fair interpretation?
00:17:17.992 - 00:17:52.138, Speaker C: I think that's fair. The justice is really punted on this issue. I think everyone coming in thought it to be a much closer case. Justice Thomas, some others had written in previous cases about wanting to look at this issue on denials of cert petitions and whatnot. And then it turned into kind of a nothing burger because of this underlying anti terrorism act statute that they wind up ruling on and not section 230. And the reality is. So it's just the status quo and all other takeaways that the justices had such a difficult time dealing with this technological issue and really getting their heads around this.
00:17:52.138 - 00:18:10.130, Speaker C: And you could hear in our oral arguments, they basically were conceding that if they made a ruling on this that went against the tech companies, they thought they were going to destroy the Internet. So I'm a little skeptical that we're going to see some big overreaching case on this in the future that doesn't have this anti terrorism act issue underlying it.
00:18:10.282 - 00:18:10.586, Speaker B: Yeah.
00:18:10.610 - 00:18:16.226, Speaker D: My favorite quote from all the oral arguments is Justice KagAn saying we aren't the nine greatest experts on the Internet.
00:18:16.290 - 00:18:17.634, Speaker B: I'm pretty sure in the middle here.
00:18:17.674 - 00:18:19.248, Speaker D: In the arguments, Lauren, you want to.
00:18:19.256 - 00:18:55.576, Speaker B: Chime in from a practical, technological standpoint. I mean, algorithms, power, everything. Even the reverse chronological feature on a Twitter feed, which, from a consumer standpoint, it's giving you the tweets in the reverse chronological order. So, like, it's not actually amplifying necessarily, but it's still using an algorithm to provide that content to users. So then the question becomes, if you're going to regulate it or you're going to put some liability on it, where's the line? And so I don't think practically there are ways to go about that, even if the courts or Congress decides to do something there.
00:18:55.720 - 00:19:21.356, Speaker D: I just want to make sure that I understand what the law is at today, and then we can shift into sort of how this all might apply to blockchain, or how to think about blockchain in these regards. So Austin writes a letter and says, like, really awful stuff about me, and he writes it down and puts it in a drawer. Austin writes a letter with really defamatory stuff about me and sends it to a billboard company and has it published on a billboard. Where does that fall? Is that legally problematic?
00:19:21.540 - 00:19:39.932, Speaker C: Yeah, so that's an interesting question. I don't know about the billboard example. So, traditionally, before section 230, all of these things were decided under the common law. And first of all, that's state by state. It's not a federal statute. So each common law in each state was a little different. But generally, you had kind of different categories of liability, and each one was different.
00:19:39.932 - 00:20:18.368, Speaker C: So there's like, editorial board liability. It was like kind of one caveat where you're picking what you're putting in the newspaper publisher. Some people call that publisher liability. I think that others consider that slightly different. But then you had, what's in a bookstore, like, if you're carrying a book that has defam, well, you can't be required to know what every single word in every single book in your bookstore is. So that's a different kind of set of liability. And in some states, that required you to actually have knowledge that there was defamatory content in that book, and then further down the line, as you get further and further away from the defamatory content and what the actual knowledge standards you would have to have about whether it was defamatory or not.
00:20:18.368 - 00:20:34.714, Speaker C: So, for the billboard, I don't know what the answer is, but the reality is that under the common law, there was all these different kind of sliding scale standards, and those have basically been, at least for online content, that's user generated. That's all been obliterated by section 230, which is just this standard law for.
00:20:34.904 - 00:21:09.374, Speaker B: Online speech everywhere, and also relevant. You know, right before 230 became 230, there was cubby versus CompuServe, in which the court said because CompuServe didn't moderate content, then they weren't liable. They were using kind of that bookstore precedent to make that decision. And then, of course, you have Stratton Oakmont versus prodigy. And the distinction and difference there was that prodigy moderated content. It was part of their business practice. And so that was a distinction that Chris Cox and Ron Wyden were looking at when they developed 230.
00:21:09.374 - 00:21:10.202, Speaker B: Cool.
00:21:10.258 - 00:21:20.338, Speaker D: Okay, so, like, in the case of a billboard, certainly Austin is personally liable in that case. And there might be some liability from, like, the billboard publishing company.
00:21:20.506 - 00:22:01.214, Speaker C: I think that's right. I don't know exactly. It partially depends on what knowledge of how defamatory the statement is on behalf of the billboard company, whether they're liable or not. That's one of the things, actually, we've called out in the DOJ proposal for section 230 was, you can let a company know that you have a court order that something is defamatory, and the tech company can still use section 230 as a shield to prevent any liability and refuse to remove it. So that was, like, one of the first things, like, well, if you have a court order that says this is defamatory speech, and you can show it to the tech company, like, they shouldn't get section 230 immunity for knowing that this is a defamatory thing that they're not taking down.
00:22:01.834 - 00:22:33.592, Speaker D: Right? So then the distinction is, like, Austin writes, like, an obviously defamatory letter about me and post on Facebook. And what section 230 does is basically say, like, Austin might be personally liable, but Facebook is not liable. And that's the rub, right? And then the thing that the court basically didn't give us an answer on is, like, what if that defamatory letter goes viral and Facebook decides, like, this thing is getting a lot of engagement and just shares it with a billion users? That is also something the court has not given us an answer to. And right now, still seems like liability is limited for Facebook.
00:22:33.648 - 00:23:11.398, Speaker A: For that, I kind of want to take a bridge here and talk about blockchain, and they also want to set up a premise before we go into this, which is that fundamentally, what we've been talking about is communications technologies. Right? The liabilities associated with a type of communication. If you read section 230, it's talking about information. Information is a very broad term that can mean everything from a Twitter post to a Venmo transaction. Right. From a technology standpoint, they are both just information that is transferring bits and data. The content of that information may be text or video versus the transfer of a dollar.
00:23:11.398 - 00:23:21.014, Speaker A: Right. And those are treated as two different things from a regulatory standpoint. So what about section 230 does or does not apply to things that have monetary value?
00:23:21.174 - 00:23:55.904, Speaker B: Here's a really important point to make as we're going through this thought exercise. Section 230 is really focused on intermediary liability, meaning what liability is on the intermediary? So not the speaker, the person who is the intermediary between the speaker and the audience. At its core, blockchain technology is really intended to decentralize and remove the behemoth intermediary from the process. I think that makes it a little bit difficult to try to translate 230 to blockchain.
00:23:56.244 - 00:24:50.448, Speaker A: So I guess the piece I want to dig into there a little bit is the intermediary in the case of, like, if I try and send a Venmo transaction for something that is illegal, right. I'm trying to donate to a terrorist organization via Venmo. I think the general assumption is Venmo has a certain degree of liability there, even though they are simply acting as the intermediary between me and the terrorists I'm trying to fund. Or, you know, in the case of a bank transaction, a bank wire, right? And this goes into, like, anti money laundering laws, and there's a whole other sort of regime around that. But I guess the thing that I've always been a little bit confused at is if you just read section 230 straight up, no provider or user of an interactive computer service shall be treated as a publisher or speaker of any information provided by another information content provider. That. I mean, at the end of the day, nothing in there says that the user and the recipient can't be liable.
00:24:50.448 - 00:25:27.306, Speaker A: Right, as you're saying, it's talking about that intermediary area. But if you are facilitating something that is inappropriate or illegal, even if you have no knowledge of facilitating it, that seems like that could fall under section 230. But then we look at something like the tornado cache case and tornado cache. There is a intermediary. It is an interactive computer service, and it is not considered immune under section 230. I'm wondering again to that same sort of thing before about when does the type of information and the distribution of information collide into something that is either covered or no longer covered under section 230.
00:25:27.490 - 00:25:32.074, Speaker D: Can I just clarify the question? Austin? So, tornado cache, you mean, like the actual code.
00:25:32.194 - 00:25:34.514, Speaker A: The actual code program, not the users.
00:25:34.554 - 00:25:37.914, Speaker D: Of the program or the team. Right, exactly.
00:25:37.994 - 00:26:29.992, Speaker C: Right. Yeah. I mean, so I think, listen, the tornado cache is a fascinating case, one that I think the government is likely to lose, albeit not on speech grounds. You know, we've heard all these talks, but publishing code that just doesn't do anything, or is. There is speech, and there's certainly instances where that's true, but there's also just as many instances where, like, if you write something that creates a product that does something, you are, you know, liable for that product. So I'm very skeptical of the free speech arguments related to the tornado cash case. I am much more bullish on the cases that are focused on the sanctions themselves being extra statutory or exceeding OFAC's authorities by either id ing folks that are not related to it, or that they're not identifying property or other things that are specific within OFAC sanctions.
00:26:29.992 - 00:27:05.672, Speaker C: But kind of getting back to your bigger question on how it relates to section 230, the Venmo example. Like, Venmo is actually doing something. They're not just publishing information. Venmo is facilitating the transfer of cash from person a to person B. And so that's an action. There's always this interesting, like, law school hypothetical that you go through, and free speech is like, when does speech become an action? And when is, you know, like an expressive form of speech, an act that's no longer, you know, purely speech. And it is like, a very interesting thing when you get into the case law, which, you know, admittedly, I haven't done in a very long time.
00:27:05.672 - 00:27:11.862, Speaker C: But the reality is that very few things are seen as pure speech and protected. I hate to drag you back to.
00:27:11.878 - 00:27:45.250, Speaker A: Law school, but I think the thing we're talking about here is speech versus action. Generally, speech is protected more than actions. And it's also understood that YouTube distributing a video is speech. But venmo facilitating a financial transaction is an action. But from a technological standpoint, that's a very silly distinction. The amount of infrastructure that YouTube runs is orders of magnitude more than the amount of infrastructure that Venmo runs. And they're doing everything from automatically determining how much bandwidth your computer has and what quality video file to send you.
00:27:45.250 - 00:27:53.214, Speaker A: Those seem more like actions than speech. And the amount of technology and actions powering Venmo is minuscule in comparison.
00:27:53.714 - 00:28:39.902, Speaker D: I'm going to disagree with that analogy a little bit, or poke at that a little bit. So the line between what is speech and what is action isn't sort of directly correlated to the amount of technology that goes into supporting it, right? Like you could say you have a bank branch and you have a newspaper, and a newspaper requires tons of people to go through and do editorial content. They have to go off traditionally and like ship and produce the newspaper and hire delivery people all over the country to deliver that. That's like a pretty big operation compared to like maybe just running a bank branch. But fundamentally, the laws applying to money and the movement of sort of like, value are different from the rules sort of governing speech. And so the question that I think, like Chris raises, which is like, when does speech become sort of action? Is a fair one here. But technology isn't necessarily like the line that you draw between the two.
00:28:39.902 - 00:29:44.812, Speaker D: I think the question of something like tornado cache, which is we have a piece of code, we're deploying it online, and this piece of code could be used to basically obfuscate where sort of value is coming from. And going to is maybe more, well, analogized to something like someone uploading the blueprints for a 3d gun online, and like the opportunity to print it using a 3d printer. And so I guess that's the question, right? When do you go from, here's a piece of code or blueprint that any average person could take and translate into something that facilitates maybe a legal action versus something that is very clearly falls into money services or money transmitting authority. And there's sort of a difference there. Digging into this, like, what is the extent to which section 230 might apply to something that's like on GitHub or on like a 3d printing repository website would be pretty interesting to get into. Like where, where do we draw that line? And specifically, like, if we think about just code that's uploaded to GitHub, where does that fall under what is and isn't protected?
00:29:44.948 - 00:30:47.064, Speaker C: I assume that GitHub would have protection for stuff that is uploaded to there in terms of the actual material itself and whoever the person is that wrote it. Again, I think that's a really tricky decision to make, and I hesitate to speculate a little bit about. I think the gun case is like a fascinating example, right? So the 3d gun cases where people ten years ago even were putting up blueprints to 3d guns and the Commerce department tried to sue them and stop it from being distributed by saying it was a violation of sanctions and weapons trafficking, they won a bunch of those cases until they ultimately lost. And as far as I know, those are still net losses for the government on trying to get those removed. But, yeah, I mean, that's like someone can take that diagram and build something that does an action, but, like, the actual code itself is speech. And so I think that's one of the issues with the OFAC complaint as written, was that they were targeting things that they shouldn't have been able to be targeting, that are not normally within the sanctions regime.
00:30:47.404 - 00:31:34.574, Speaker B: Yeah, and typically, the section 230 really is tied to the action that a platform is or isn't taking. That is really what it comes down to. So I'll let you all kind of sort out the actions, but let me add just another element to this equation, because I think it's relevant to the discussion we're having. And this was actually raised in the Supreme Court cases, is how about generative AI? And I think that's also kind of the next big debate around section 230. In fact, I've had conversations with a lot of folks in Congress about this, and a lot of people agree that it's a very gray area that could need some clarifying pretty quickly. And so I think you'll also see the debate shift in that direction in the coming months.
00:31:41.214 - 00:31:44.318, Speaker A: Do you think section 230 could have gotten past today?
00:31:44.486 - 00:32:16.792, Speaker C: I don't think anything could have gotten past today. I mean, that's when we talk about crypto, like this idea that we're going to have some huge, overarching crypto bill. I'm extremely skeptical just because in the eight years since I started working on the hill for the second time, we barely passed any significant legislation. It's really hard to pass section 230 and the Communications Decency act, they did it for the kids. So if you can pass something for the kids, maybe you can get it passed as a side project to the overall bill, crypto for the kids.
00:32:16.928 - 00:32:18.104, Speaker B: How do we frame that?
00:32:18.184 - 00:32:46.338, Speaker C: Yeah, so I don't think anything like section 230 could pass today. In general, social media and the actual fact of section 230 existing have become very partisan. Even though at the time it was a bipartisan bill as part of a pretty aggressively bipartisan overall bill in the CDA, passing anything as comprehensive as that, as that would impact so much of our economy, just seems. I'm skeptical.
00:32:46.426 - 00:33:23.896, Speaker B: Yeah. Well, and, you know, when I was at Twitter, we put forth some principles to provide a solution, an alternative to eroding 230, because, you know, I'm an absolutist. I think 230 is really brilliant. It strikes the right balance. But we put forth a proposal that was really focused on four principles, that being increased transparency, choice for consumers, due process, and privacy. And we're starting to see legislative proposals. I mean, mind you, it's going to be hard to pass anything right now, but we're seeing proposals that are incorporating those elements, or companies are going ahead and starting to incorporate those elements.
00:33:23.896 - 00:33:50.724, Speaker B: But going back to the blockchain and crypto piece, I think that you're starting to see, because of a lack of us action, countries in other places of the world are noting this, and they're starting to make moves, and they realize that having, again, that certainty and that clarity is really good for business and that brings jobs and innovation to your market. And so I think the US is at a risk of losing out in this space if they don't take action.
00:33:51.144 - 00:34:26.471, Speaker A: I get that something that's more holistic reform might be off the table. And I think we've all been pretty frustrated with the last six to eight years of Congress's inability to take action on really many issues that feel like they're important to the future of the United States. That being said, are we sort of in this de facto state for a while now where we're relying on an old legislation to defend or allow new technologies? And if so, are there areas that you think might have practicality or applicability here that have not been explored in.
00:34:26.487 - 00:34:29.071, Speaker C: Terms of section 230, in terms of crypto, or both?
00:34:29.207 - 00:35:22.494, Speaker A: In terms of crypto in general. Right. Like, if we're looking at an area now where we say, like, an example of this, right, is like I look at something, section 230, I am not a lawyer, right. I'm not trained to say how this could apply, but I read this thing and I'm like, yeah, this should apply to blockchain technologies. Like, fundamentally, at the end of the day, blockchain is a system for managing and distributing information between folks and as an intermediary, that seems to me to be very similar to Amazon Web Services or, or an ISP in that the being involved in that place should afford the same section 230 protections. That is not something that seems to have been something anyone has necessarily tried to push, and there may be very good reasons for that. But are there areas you see that are open to sort of a different interpretation that might carve out an area for blockchain to have more certainty around operations, assuming we're not going to get some comprehensive legislation from Congress?
00:35:22.664 - 00:35:27.450, Speaker C: Well, I do think, I mean, getting back to the comment about the ISP, I think that is a very interesting analogy.
00:35:27.482 - 00:35:27.674, Speaker A: So.
00:35:27.714 - 00:36:20.708, Speaker C: Right. You've got a lot of people that are pushing now for, we can regulate blockchain at the app or Dapp level, but that the underlying layer one is analogous to an ISP HTML or whatever, and we don't regulate that. And so I think that is an interesting analogy as it relates to how the Internet evolved too, in terms of section 230. That took some fights to get those things to be kind of the status quo that we've experienced for the last 30 years. I don't think that was a given. I think the other advantage that blockchain has in this space is that if blockchains are truly decentralized, and they are decentralized enough that they are enforcement resistant, if not enforcement proof layer ones themselves theoretically shouldn't be able to be regulated, even if they want it to be right. And so you will have to regulate them at the DAP level rather than the layer one level.
00:36:20.708 - 00:36:46.704, Speaker C: Now, that doesn't solve the problem that we've seen over the last few days with what all of these tokens are going to do in the meantime before they are truly decentralized and how they deal with civil enforcement from the SEC or whatever. But I think it does point to an avenue that we can look to in the past, which is what early folks on the Internet did to preserve base layer neutrality of our underlying technology.
00:36:47.044 - 00:37:35.470, Speaker D: Yeah, I think it's interesting. One of the, I think biggest curses of blockchain is sort of the first killer app had to do with sort of the monetary use cases, because at its core, blockchains are decentralized communications protocols, or at least sort of. That's the way I view them. And I think a lot of sort of folks who are proponents of the technology. And so it feels logical that we would think about sort of regulation at the sort of protocol versus application layer, the same way that we think about sort of regulation of ISP's versus the applications built on top of them. It's interesting when we think about what enforcement looks like in this case, if we're looking at some kind of enforcement that has to do with something, that's just like in web two on the open Internet. To what extent are different layers of that stack liable? Right? So we've talked about what layer at which a platform might be liable, like Facebook and the speaker themselves.
00:37:35.470 - 00:37:48.256, Speaker D: But how far deep does the rabbit hole go? Do we think about like AWS's liability or comcast liability? Or like, you know, the people who are running the damn fiber optic table? Like, where does that stop and start? When we think about sort of web.
00:37:48.280 - 00:37:55.144, Speaker B: Two tech, Abira, you are bringing back flashbacks of net neutrality wars and also.
00:37:55.224 - 00:37:56.712, Speaker D: A big issue back in the day.
00:37:56.888 - 00:38:20.612, Speaker B: Oh yeah, still a big issue. You know, it always comes back. Yeah. And that's how I got into all of this. And the way that we like to kind of distinguish is ISP's and telecom services. Those are kind of the infrastructure. Those are sending packets, whereas the Facebook's, Google's, twitters of the world, that's customization of that information, of those packets.
00:38:20.612 - 00:38:40.472, Speaker B: And again, that's a layer on top that those are the edge providers. It's a different set of regulations, different set of technologies and how information is disseminated. So I almost want to say that you've got to look at telecom, look at the kind of web, two companies like the ones we've discussed.
00:38:40.648 - 00:38:40.976, Speaker A: Yeah.
00:38:41.000 - 00:39:26.832, Speaker D: I mean, there's sort of the, I think, technological build and what people who I think deep in the technology often believe. And then there's sort of the practical implications of what is likely to win an argument in front of a court or gain traction with legislators. And, Chris, I know something we've chatted about is, like, you can talk about ideas like, basically or neutrality as much as you want, but when push comes to shove, like, really, you're, you're putting something in front of a bunch of legislators who are over the average age of 60, well over the average age of 60, or judges who don't spend much time in this space at all. And so we also have to build for and think about the practical implications of policymaking in an era where we have 30 years of Internet policy to learn from. And not all the sort of initial stage setting is universally popular today.
00:39:26.968 - 00:40:21.292, Speaker C: Listen, the idea that the 80 year old senators are going to be really crypto savvy and understand what this technology does or how it works, I think is pretty naive, and we have to start with that. And the judges thing is even worse. I think these folks don't have nearly the interaction with people that legislators do, either in terms of staff or people just coming in and talk to them about stuff. And so really, like, one of the things I am working on is trying to figure out a plan for educating judges on web3 and just kind of the basics of how it works, because I really do think that, like, if we're not going to have a legislative solution in the future, which I'm skeptical of, and the last two days shows us anything, that a lot of this stuff will be decided by the courts ultimately, then we need to make sure that we're educating and convincing the judicial branch on how this technology works.
00:40:21.468 - 00:40:56.052, Speaker A: So do you think being hopeful about a section 230 for crypto carve out is a little naive. The reason I ask is because section 230 is one sentence that literally enabled the creation of the modern Internet in the United States. In contrast, recent stablecoin legislation that's been introduced is around 130 pages. And my understanding is that's pretty short for modern legislation. Can we tackle big problems with comprehensive, concise legislation anymore? Like the Social Security act passed in 1935 was just 29 pages, and it's one of the biggest decisions Congress ever made.
00:40:56.148 - 00:41:19.792, Speaker C: My take on all of these is that no one wants to waste the capital on doing something small because it takes so much energy to get it through that we wind up with these massive bills in the hopes that at least some portion of them will get through, if not all of it. And the reality is that it makes the process even more complicated because there's 15 parts to this bill instead of one focus thing.
00:41:19.968 - 00:41:20.256, Speaker A: Yeah.
00:41:20.280 - 00:42:12.684, Speaker B: And to that point, I think that there's a lot of education happening in Washington, but I think equally important, there needs to be some grassroots work done at the state level and local level to kind of bring together, you know, people who are using this technology or interested in this technology to the policymaking. So those members, when they go home, they hear from their constituents, hey, we really want you to do something about this. And I think that that's also missing from the advocacy piece to the degree it needs to happen in order to push things along. I think across the pond, we might see some things like we've seen with GDPR sparking the privacy debate and DC. Granted, it's still not going anywhere, but it's still influencing the process. So I think also some of the international initiatives might help at the federal level.
00:42:12.984 - 00:42:30.810, Speaker D: Yeah, I'll say there's a few things that can galvanize Washington, and one of them today is China, and another is privacy legislation that's getting traction. A lot of it is focused on minors. We're almost at time. I want to just give us a little bit of time to wrap up and any sort of big thoughts that you both have to close out.
00:42:30.962 - 00:43:08.430, Speaker C: I just want to caveat. This might be a little squishy here, but I will caveat my skepticism of a big overarching crypto bill with the statement that that comes before these two recent cases by the SEC against Binance and Coinbase, which I think actually could. They're so kind of far reaching that I do think that they could spur something in Congress into action as a response. But no, overall, I think we need to be looking at strategic litigation in the courts to really champion some of the things and the base layer protections for crypto that we've seen for the tech companies for the last 30 years.
00:43:08.622 - 00:43:40.854, Speaker B: And I would say the initial impetus for section 230 was global competition in the Internet space and technology space. And I think that that still rings true today. And I think that making the argument of why this matters for the US and for, you know, the competitive space and crypto, I think that that's also critical and going to be an important catalyst to whether us is successful or not in facilitating, again, a clear and predictable regulatory framework for crypto.
00:43:40.974 - 00:43:44.094, Speaker D: Those are two very tight closing thoughts. Thanks so much to both of you.
00:43:44.214 - 00:43:45.934, Speaker B: Thanks for having us. That was fun.
00:43:46.014 - 00:43:46.830, Speaker C: That was fun. Yeah.
00:43:46.902 - 00:43:58.654, Speaker A: Thank you. Validated is produced by Ray Belli with help from Ross Cohen, Brandon ector, Emira Valiani and Ainsley Medford. Engineering by Tyler Morissette.
