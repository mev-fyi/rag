00:00:02.800 - 00:00:03.380, Speaker A: Awesome.
00:00:04.214 - 00:00:37.204, Speaker B: Cool. Thanks, everyone for joining. Appreciate your time, as always. Love to see everybody showing up. A couple things I wanted to go over today and leave some time for questions. I know there's some spicy conversation happening in the discord and a couple channels this morning, so first thing, I would like to welcome Tim Garcia. He has recently joined the Solana foundation full time as our new validator relations lead.
00:00:37.204 - 00:01:05.550, Speaker B: So he's going to be working closely with myself and Ella, members of the Solano Labs team, and most importantly, the validator community to make sure that our growing network of validators is resourced and has educational content and kind of community resources so that everyone can succeed and kind of run the best node possible. So let's, yeah, give them a nice welcome.
00:01:05.582 - 00:01:09.574, Speaker A: And Tim, you can feel free to say hello. Jump in here. Sure.
00:01:09.614 - 00:01:09.830, Speaker B: Yeah.
00:01:09.862 - 00:01:46.974, Speaker C: Hi, everybody. I'm excited to get started and work with all of you. Just getting my hands dirty with validator stuff. Now, I've not been an operator in the past, but I have a technical background. So a little bit about me. I was a developer, worked at Amazon, done lots of low level, like, system admin things before, but learning the validator world in Solana. So the point, I guess, is for me to figure out these things as I go and document pain points and also just be a sounding board for all of you when you get stuck on something or need help.
00:01:46.974 - 00:01:51.144, Speaker C: Like I said, I'm happy to help and looking forward to working with you all.
00:01:56.324 - 00:01:57.544, Speaker A: Awesome. Thanks, Tim.
00:01:58.044 - 00:03:07.944, Speaker B: Yeah, we're excited to have you. It's been a long time coming. Sort of no secret that I think some of our validator education materials have been a little sparse in the past. So for anyone who has any particular requests or pain points, you know, as it relates to sort of node operation, we're really looking at. Looking at this sort of as like, very similar to like, the developer relations initiative, that kind of the Solana ecosystem and other projects have to provide, like, a community of support, but also sort of a library, a compendium of resources for potential new and existing operators to kind of learn best practices, be able to kind of find their way through the resources, have like a more dedicated kind of liaison, point of contact when. When there's questions or missing documentation or, you know, kind of changes to recommended workflows and. Yeah, so we're really kind of, really, really looking forward to that.
00:03:07.944 - 00:03:46.924, Speaker B: So first thing wanted to go over. There have been some questions about win 1.9 on Mainnet. It's looking like our hope is that I'll get a green light from the labs engineering team to recommend 1.9 for Mainnet around the end of this month. So that should be another ten days from now. You know, I think there's still a couple little things that are soaking on Testnet on 1.9
00:03:46.924 - 00:04:01.660, Speaker B: branch now, but that's sort of the tentative timeline and. Yeah. Tim, do you want to kind of give a brief overview of what the new features are, what some things we can validators should be aware of for what's coming down the line?
00:04:01.692 - 00:04:03.268, Speaker A: 1.9? Sure.
00:04:03.356 - 00:04:32.764, Speaker C: Yeah. So I'll put a link in the chat here, and I'll also add it to discord after this. So the doc goes through the things that I've noticed as big changes. The main three there are that 1.9 is going to contain incremental snapshots. So the reason for that is snapshot size has gotten very large. As most of you probably know, it's in the 20 to 30 gigabit range, and this should help alleviate that problem and alleviate some startup time costs.
00:04:32.764 - 00:05:37.598, Speaker C: So there's notes in there about what you want to look out for. The main thing is once you switch to incremental snapshots in your validator, you're sort of locked into 1.9. You don't want to downgrade again to 1.8. So the recommendation in there is to try out 1.9 without incremental snapshots, make sure you're really happy with the performance, and then once you're comfortable with it, start activating incremental snapshots with the incremental snapshots flag. Another feature that's landing is validator identity transitions, or I'm not sure if that's the best way to describe it, but essentially the feature allows you to keep your validator up and running while you're doing maintenance or restarting a validator or anything else on the primary validator. So if you want to take your primary validator down, there's some notes in the link for what's the optimal time to do that? When the validator goes down, you could very easily switch the vote account over to your secondary validator that's running.
00:05:37.598 - 00:06:21.488, Speaker C: So there's a link here in the docs that you should check out and run through the steps and just play with that. And then the final change is more RPC related. So previously, as you might know, that there's a flag that you can add for minimal RPC. If you previously have the minimal RPC enabled, that is now the default after 1.96. And if you want the full RPC, you will need to add the new full RPC flag. So essentially the switch here is old nodes, nodes that are running before 1.96, full RPC is the defaults.
00:06:21.488 - 00:06:41.004, Speaker C: Now after 1.96, minimal RPC is the default. So just be aware of that. The minimal RPC flag is going to be deprecated in favor of the full RPC flag. So anyway, you can read about there. If you have any questions, comments, feel free to reach out to me on discord or Twitter or wherever you want chat.
00:06:46.584 - 00:06:47.844, Speaker A: Awesome. Thanks Tim.
00:06:48.264 - 00:07:40.396, Speaker B: And yeah, one just a clarifying point. I'll just note that the minimal RPC, this is for nodes that just want the ability to serve snapshots to requesting nodes. So it just provides access to the RPC calls necessary to get a snapshot, or rather to serve a snapshot from that minimal RPC serving node. This is really just a sort of like enforcing best practices by default, which currently is not the case when it comes to leaving your RPC port wide open. There's a number of nodes across the networks that have open RPC ports that may not necessarily need them to be open, which can leave them open to various sorts of unpleasantries if someone starts spamming the open RPC port on a.
00:07:40.420 - 00:07:43.496, Speaker A: Voting consensus node, which is something that we want to avoid.
00:07:43.660 - 00:07:53.284, Speaker B: So we're just really going to make this really just making the defaults harder to accidentally screw up your node.
00:07:56.024 - 00:07:56.800, Speaker A: Question in the chat.
00:07:56.832 - 00:09:32.504, Speaker B: Do I understand correctly minimal RPC reduces the load on the node? Not necessarily really if you choose to leave an RPC port open. So the real answer is you really should not leave a fully open RPC port on a voting node. It's very easy for someone to just hit a voting node with, get program accounts a million times, and cause your node to fall off the tip of the cluster, which impacts your vote performance and your block performance, and it's kind of a bad time overall. So in general, the recommendation is you either have an RPC node which is not voting and has no stake, but has an open RPC port, or perhaps a a token gated RPC port, which is what some of like the RPC provider projects do in whatever flavor, or you have your voting validator with stake voting and consensus that does not have any open RPC. The soft middle ground there is voting validators can still provide snapshots to other validators that are starting up for the first time and need to get a snapshot from someone else on the network. And so this minimal RPC flag enables a voting validator to provide snapshots to those that are looking for it, but does not respond to any other RPC requests, such as get transaction history or get program accounts or anything like that, which is a much heavier read load on your node, so it won't impact your node's performance as much.
00:09:45.014 - 00:09:49.594, Speaker A: Cool. You're very welcome.
00:09:50.934 - 00:10:13.572, Speaker B: Yeah, any other questions on kind of the 1.9 upcoming features or transition there? We're still going to kind of continue to use the same like upgrade communication and workflow. You know, often it's the Solana Labs team who's producing these, these builds, and so they'll kind of give their blessing when they believe that it would be.
00:10:13.588 - 00:10:15.424, Speaker A: Appropriate for the community to move up.
00:10:26.184 - 00:11:06.664, Speaker B: Yeah, interesting suggestion, being able to flip on minimal RPC at runtime. Yeah, I'm not sure that seems like a good idea. I'm not sure what else is required in the validator boot up process at the moment that might block that. Or maybe nothing. Yeah, that's a be great feature proposal. There's a question on Testnet. Looks like you chewed through 20% wear and tear in two months on your shiny new NVMe.
00:11:06.664 - 00:11:15.820, Speaker B: I know there's been some questions floating around about excess UDP traffic on Testnet.
00:11:15.852 - 00:11:17.764, Speaker A: Which we'll kind of look, look separately at.
00:11:20.144 - 00:12:05.544, Speaker B: Yeah, so, snapshots. So one of the things with kind of the transition to 1.9 incremental snapshots should, I believe should reduce the write load on the SSD's as well as make your validator restart time a lot quicker. The other thing, if you're just running a single 1 tb disk, you may want to think about moving. If you have two SSD's and you can put the accounts DB on one disk and the ledger on a separate disk, you're effectively going to divide the write load accordingly. And now you've got twice as many blocks that your blocks of memory that you're writing to. So you effectively have double the or half the, half the write load across.
00:12:12.544 - 00:12:22.832, Speaker A: Welcome. Cool.
00:12:22.968 - 00:12:57.914, Speaker B: And I don't know if there was a. I think Tim put a question out to the community a couple days ago. I was out. So I apologize if I'm behind on this one. We're looking for one or two node operators who consistently run a node on testnet who are willing to leave the minimal RPC port open on their validator to serve up snapshots to new nodes that are joining the network. We have a couple of the known validator pub keys that are outdated in the official docs. And I would love to have a little more community support there that we could point new node operators to.
00:12:57.914 - 00:13:16.620, Speaker B: If you're willing and planning on keeping your node running on Testnet for a long time and are willing to keep your minimal RPC port open and have that included in the docs, please reach out to Tim and we will hook that up and just.
00:13:16.772 - 00:13:24.344, Speaker A: Yeah, that would be super nice. Cool.
00:13:26.044 - 00:14:32.484, Speaker B: Other thing I wanted to touch on and this is still, I'm still looking into this, so I don't want to kind of over promise anything. I have been seeing or catching up on some, what looks like some really kind of challenging or unfortunate behavior that's going on in the foundation's delegation program. There have been some complaints of people perhaps behaving unfairly or taking advantage of the way the delegation program automatically orders people for their kind of onboarding time from testnet to Mainnet. I've heard a number of different perspectives on this from a few people in the community. So me and Ella who's here on the call are going to be looking into this. But if there is a need for us to sort of update the way that we run the program, or if people are found to be really blatantly kind of taking advantage or really kind of trying to cheat the program here, we're going to take some corrective action. If I need to bring the hammer down on a lot of people who are trying to cheat the program, that I will.
00:14:32.484 - 00:14:50.964, Speaker B: I don't want to. Yeah, I'm still looking into this, so I can't say for sure, but it is on our radar. It's something I. Yeah, I'm taking really seriously. So I appreciate everyone's thoughts that you've expressed in discord.
00:14:58.084 - 00:15:00.104, Speaker A: Question here. One nine supports.
00:15:03.924 - 00:15:44.172, Speaker B: Right. So this. The question is about switching to a standby server during maintenance. This is actually not a 1.9 specific feature. It's really that one of the labs engineers, Mike Vines, put out kind of like a best practices guide for how you can prepare and switch over to a standby server with minimal notice. So if in the event of planned or unplanned problems with your primary server that goes down, that you could still move over to a backup, whether you got to go to a cloud node or a cheapo node for some temporary amount of time while you get your main guy back up.
00:15:44.172 - 00:15:57.548, Speaker B: It was really just to provide a guide on how to do this. I don't believe there's any features in there that are 1.9 dependent. So if you wanted to test this out today, on your, on a 1.8 node or on your Testnet 1.9.
00:15:57.676 - 00:15:58.824, Speaker A: You can go right ahead.
00:15:59.164 - 00:16:25.714, Speaker C: Can I add a little bit there please? So in the 197 there actually is a feature that helps enable that. It's essentially the ability to switch the primary vote account from the validator that is running to the validator that's not voting. So you need two validators and you need one 9.7 to get that working. But the steps are mentioned in the second bullet point in that link I sent.
00:16:27.374 - 00:16:28.158, Speaker A: Got it.
00:16:28.286 - 00:16:32.270, Speaker B: Thanks for, thanks for keeping me honest, Tim. No problem.
00:16:32.462 - 00:16:32.854, Speaker A: Cool.
00:16:32.894 - 00:16:54.912, Speaker B: Yeah. And so that exists I think as a markdown file in one of Mike Fine's repos right now. But we'll work on getting that kind of best practice suggested operational procedure. That and other kind of similar flavors of good, good node operator knowledge base kind of built built into the restructuring of the docs that we're working on.
00:16:54.928 - 00:16:57.164, Speaker A: Doing over the next couple months here.
00:17:08.664 - 00:17:30.034, Speaker B: Yeah, a couple questions about people running modified binaries on Testnet. Yeah, we're going to look into this. I haven't myself looked at the data in enough depth to comment right now. I know this is a big concern for a lot of people and we're looking at it straight away.
00:17:47.634 - 00:17:48.454, Speaker A: Awesome.
00:17:50.434 - 00:18:22.554, Speaker B: Anything else anyone wanted to cover? Any other questions? Happy to open up the floor for further discussion. All right, well if not, I guess we can wrap this one a little bit early today.
00:18:33.754 - 00:18:34.494, Speaker A: Great.
00:18:35.074 - 00:18:39.778, Speaker B: Thanks a lot everybody. Have a great Friday, have a great weekend and I'll see you in a couple of weeks.
00:18:39.866 - 00:19:23.154, Speaker A: Take care. It, it.
