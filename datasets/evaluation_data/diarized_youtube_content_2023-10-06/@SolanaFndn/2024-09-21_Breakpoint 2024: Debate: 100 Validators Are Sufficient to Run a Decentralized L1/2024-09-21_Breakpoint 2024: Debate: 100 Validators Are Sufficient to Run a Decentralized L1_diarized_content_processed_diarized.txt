00:00:17.320 - 00:00:18.004, Speaker A: Oh yeah.
00:00:19.385 - 00:01:06.001, Speaker B: Good morning. My name is Dan Albert, I work with the Solana Foundation. And the debate motion for this morning is are 100 validators sufficiently decentralized to run Solana's L1? I'm joined by Brian Long, co founder of Triton1, one of the early validators and largest RPC operators on Solana. And everybody's famous favorite founder, Anatoly CEO of Solana Labs. So I'm gonna invite these guys to open. Brian is gonna be arguing in favor of the motion that 100 validators are enough to have a decentralized L1 totally is going to be arguing in opposition. So Brian, your opening statement please.
00:01:06.073 - 00:01:42.007, Speaker A: Great, thank you. I'll focus on the financial and performance aspects of the proposal. Future of Solana with 100 validators is inevitable and we should embrace it. The race to zero commission for inflation, rewards and MEV tips has led validators to share their transaction fees with stakeholders. That creates a situation where economies of scale matter more than anything else. To run a validator, the larger validators simply have more fee revenue available to share with their stakeholders. The stakeholders will always do better.
00:01:42.007 - 00:02:42.215, Speaker A: There for illustration, a validator with 200,000 soldiers delegated stake might roughly break even when their transaction fees after voting costs they don't have anything left to share with their stakeholders. Whereas a validator with 2 million SOL active stake will have roughly 30 SOL per epoch that they can share with their stakeholders or about 390 SOL per month. And so it's impossible for that smaller validator to ever compete with the larger validator that has scale stake moves to where it's treated best and it will start to concentrate onto the larger nodes. That's why I say it's inevitable we're going to end up with 100 validators. But that's not bad. That's a good thing. The reason is that those 100 validators are now financially healthy and then they can afford to invest in advanced infrastructure for the validator of the future.
00:02:42.215 - 00:03:23.821, Speaker A: The hardware stack for a validator will get more complicated. We'll be dealing with FPGA network ingestion hardware. We may have a future where we have multi GPU setups for advanced computation or separate databases to manage a larger accounts. Db I don't think anything gets easier for the validator of the future. But the good news there is the performance will be amazing and we will have for like transaction delivery, we'll have 90% of our transactions landing same slot. We will have confirmation times less than 1 second. The user experience will be fantastic and smooth.
00:03:23.821 - 00:04:18.429, Speaker A: Web3 will feel like web2. Network congestion will also go down. And that helps too for some of the one I just described, actually for the transaction delivery that will work better in the new world. And then finally these validators will have enough stake and enough skin in the game and being financially healthy that they would be in a position to actually move validators south of the equator. So we're lacking validators in the southern hemisphere. And a validator in a multiproposer world may decide that it's actually a great idea to put validators in Brazil or Argentina to be able to service the Latin American community. So in summary, we're building the world's fastest globally distributed state machine.
00:04:18.429 - 00:04:26.105, Speaker A: We need financially healthy validators with the sophistication to run complex hardware in professional data centers. We can do this.
00:04:26.485 - 00:04:34.075, Speaker B: Thank you. Brian anatoly, why are 100 validators insufficient?
00:04:35.295 - 00:05:33.495, Speaker C: Because economic security is a meme. So I make that joke on Twitter. What I'm trying to actually say is that to actually have any kind of meaningful economic security, we need actual, maximized physical security, like the actual boxes and the links and how all of these things connect around the world have to be secure to such an extent that these other information systems that are effectively virtual can give us guarantees that we rely on. And I want you guys to try to remember your a bit of computer science or if your comp. Sci networking classes. So there's this thing called Cap theorem, and it is a trade off of maintaining consistency and availability in the presence of partitions. So if you have a partition, you have to pick one, either consistency or availability.
00:05:33.495 - 00:06:31.605, Speaker C: And kind of the other side to that coin is if we can avoid partitions, if partitions are impossible, that system can maintain both consistency and availability. And from an outsider perspective, from a person, from a user, a system without partitions is indistinguishable from one that is the most decentralized. With maximum decentralization. Whatever you may imagine decentralization means to you, if there's no partitions, the thing that you're dealing with, you cannot test it and show that it's not decentralized. That's really, really important because in the modern Internet, it's not magic, it's made by people, people like Brian, but it's actual people you can go talk to. And they live in places and they maintain these physical systems that run the Internet. It is literally wires, fiber optic cables, switches, things that need power and stuff like that.
00:06:31.605 - 00:07:19.885, Speaker C: And there is expectation of service from these things. And if you remember Networking. There's this thing called the OSI layer stack and the Lore layer that you're in the physical layer. The higher is the expectation of service. And from a very high level you can think of it, if there's a problem very, very low on the bottom at the physical layer, more people get woken up at night to go fix it because all the applications start breaking, all the RPC providers start breaking, the network starts breaking. All those people get woken up and eventually somebody at AT&T realizes that they've misconfigured a route or literally a switch is on fire and they have to go F. And that is actually kind of somewhat predictable.
00:07:19.885 - 00:07:59.349, Speaker C: Usually these outages on the physical Internet get resolved in about four hours. So we have this real world expectation that partitions get resolved in about four hours, no matter where they happen. And you can kind of look at some history of AWS and Google and Facebook. That's about the time somebody gets woken up and they're like, what the hell is going on? I'm going to go debug it. And then they can switch whatever system's over. And the fact that you can reach that person and talk to them like that means that there is another channel, there's another way to send communication between the real world and whoever that person is. So a partition actually never even existed.
00:07:59.349 - 00:08:45.032, Speaker C: We can always reach somebody in the world to go fix the Internet. In a very academic sense, there's enough data for us to be able to talk to each other, to go figure out what happened. So we have to kind of create this world that where partitions are impossible through work. And we need to maximize that security to have any sense of economic security on top of it. So we need the largest number of validators we can possibly get, because all of those boxes and the intermediaries between them, they generate links. They create that sense that, that when Solana doesn't make a block for 10 minutes, a lot of people get paged and they go fix it. So there's a very high expectation of service on solana.
00:08:45.032 - 00:09:22.811, Speaker C: That's every 400 milliseconds there's a block. And the more validators we have around the world, the more people are looking at that expectation of service from every different point in the world. That means they're connected through Internet service providers that are in the US In England, in Ireland, in France and Argentina or whatever. Those companies are part of the government. There's dudes with guns that protect that infrastructure. And if something goes wrong, somebody observes it, somebody gets woken up stuff gets fixed and partitions are impossible. And we have this expectation that we can actually build economic security on the system.
00:09:22.811 - 00:10:00.065, Speaker C: But you still need a very, very large validator set. And the larger it is at a gut level, the network is more secure. But an academic level, the bigger the set of nodes, the easier it is to guarantee that honest nodes as a minority of that set can always have a minimum spanning tree that can always reach each other. And that doesn't even mean at the protocol layer. It is literally people talking on the phone. Like the fact that people can get into discord or, or IRC or call each other on a cell phone. That is us resolving a partition and figuring out what's wrong.
00:10:00.065 - 00:10:08.053, Speaker C: The more people we have, the easier it is for us to guarantee that partitions are impossible. And I think I'm going to cut.
00:10:08.069 - 00:10:10.725, Speaker B: You there totally so we can get to some Q and A.
00:10:10.765 - 00:10:13.797, Speaker C: Okay, thank you for that. One last thing.
00:10:13.861 - 00:10:14.661, Speaker B: Go for it.
00:10:14.813 - 00:10:47.265, Speaker C: I think as crypto matures and gets bigger and bigger, just the sheer fact of it creating value for the world on board all the businesses, all the merchants, all the banks and there's people that work there and they're all going to connect these things and they will pay for the expensive hardware because it is now part of their business. It's not a cost for the vote fees. It is just simply like infrastructure costs at aws. And the network will grow and become very large and physically secure if it has product market fit and people use it.
00:10:47.815 - 00:11:26.421, Speaker B: Great, thank you. So, Brian, I'm going to ask this question to you. You mentioned around the economics of. You claimed that 100 validator set is inevitable and therefore should be embraced. Totally made the argument that maybe economic security is a meme. We can argue about that, but physical security and distribution is paradise paramount. How can we, you know, how would you argue or defend that? The network is, I'll say, sufficiently physically decentralized.
00:11:26.421 - 00:11:54.055, Speaker B: Right. The motion is, you know, a decentralized L1. If the requirements to run a validator, you know, are so high that the stake has amassed? You know, a common criticism we've heard of Solana for years is around the economics of operating a validator. And so how do you feel that those economics do or do not contribute to Solana's centralization or decentralization?
00:11:54.395 - 00:12:06.295, Speaker A: Sure, yeah. The. Actually the largest expense for. Did my mic just. Mike's back. Okay. Largest expense for a validator is actually the voting cost.
00:12:06.295 - 00:12:45.601, Speaker A: And so it far exceeds the cost of hardware right now anyway. Not in the future, but right Now. And so it's important to understand that running with low stake, you know, lots of thousands of nodes with really low stake, they still can't cover their monthly operating cost, which might be between 20 and 30 sold just for the voting cost. So the, that's how I'm kind of looking at that as saying, you know, it's not just the cost of the hardware, because that's actually the one of the lower cost items, it's the economics of running the validator and the high cost of voting.
00:12:45.753 - 00:12:56.325, Speaker B: So totally, and we've talked about this, the cost of voting, the economic barrier, how do you see, you know, pushing a large number of validators forward into the future, given the economics?
00:12:58.305 - 00:13:47.741, Speaker C: So we're kind of screwed because as you lower the financial barrier to entry, you lower it for everybody, including dishonest participants. So in that imaginary world where you have a very large number of validators, if an attacker can arbitrarily create a very large number of validators themselves, they can prevent that minimum spanning tree from happening at the protocol layer. And that would create a partition. And when you have partitions, you can't guarantee consistency or availability. And that basically means double spend attacks are possible. And no global financial institution is going to use these networks if double spend attacks are happening. Not if they're possible or not, but whether they're happening.
00:13:47.741 - 00:14:42.927, Speaker C: Because from a service provider's perspective, that becomes a very, very annoying outage. Even if circle doesn't lose money, they get into a state where they have to go potentially fork the chain to redeem a whole bunch of money in their own state and the rest of the chain view of what a dollar is becomes completely screwed up. And that's a bad outcome for the ecosystem as a whole. So we have to have some economic incentives or barriers for nodes to enter, simply because we can't allow attackers to create infinite number of validators because that would create partitions. So here's me arguing why 100 nodes is enough. It's simply me, a circle or a financial operator that has a lot at stake at the network. I don't even want to trust the network or its network topology.
00:14:42.927 - 00:15:09.017, Speaker C: Instead, what I want to do is trust my own infrastructure and I could run my own hundred nodes anonymously around the world that create the same statistically consistent view of the network, that if an attacker tried to double span and create a partition, my nodes at least are very likely to detect both partitions and tell me, circle, that something is wrong and I should halt deposits and withdrawals and figure it out. Later.
00:15:09.201 - 00:15:13.881, Speaker B: So this is an argument for institutions that they should run their own infrastructure.
00:15:14.073 - 00:15:57.501, Speaker C: They do anyways. And the amount of nodes that they run is scales with the amount of risks that they're taking. So if you're in some distant future, if you're a Bank of America and you have a trillion dollars worth of assets that are issued on Solana, hopefully you don't want to trust Solana, you want to trust your own infra. And because you have so many assets that you've issued for a financial kind of profit motivating reason, you can afford to run 100 nodes because those are just boxes. You already have a team to manage all this stuff. You can constantly sample the network and make sure that from your perspective that there's no partitions are possible. And that actually is very, very hard to break.
00:15:57.613 - 00:16:16.583, Speaker B: One thing I want to clarify here, and Brian, I think you might have some thoughts, is, you know, when we're talking about staked nodes, right in the, in this 100 validators, right. These are, these are staked nodes. These are block producing nodes. Right. They're writing to the blockchain. Yep, totally. It sounds like you're advocating for a very large number of nodes at least to be able to self verify.
00:16:16.709 - 00:16:16.971, Speaker C: Right.
00:16:17.003 - 00:16:36.975, Speaker B: And so to bring the verification cost down. And so you know what we're seeing now with projects like Mithril and other verifying light nodes, these are not block producers, they're not staked, but they are allowing for greater independent verification. So I'm going to take this opportunity and here it comes.
00:16:40.115 - 00:16:42.541, Speaker A: Lean into it, Dan, lean into it. Come on.
00:16:42.723 - 00:16:59.617, Speaker B: And Brian, I'm going to ask you to, if you could, why would you argue that 100 validators, particularly staked block producing validators, is not sufficient for Solana to remain decentralized?
00:16:59.761 - 00:17:24.675, Speaker A: Yeah. So I'll flip positions here and the idea of better performance sounds fantastic, right? Unicorns and rainbows. We're going to have really fast transaction times, smooth experiences, global coverage. Got to love it. That's fantastic. Until something breaks. And the truth of the matter is that a smaller validator set leaves the network fragile and subject to breaking.
00:17:24.675 - 00:18:28.375, Speaker A: I'll try to give some examples though, but not in a CSI context. I don't have a CSI degree, so I'll just try to explain it a different way. But if we think about 100 really wealthy validators with a lot of stake on them, those become really ripe targets for bad actors. Whether it's state actors, hackers, the bribery, corruption of some sort, it's easier to go after 100 nodes instead of 1000 and especially 1000 that are actually smaller targets because they don't have as much stake. So that's one way that a larger network could be more robust and more reliable than a smaller validator set. Regulatory risk is very similar that we need validators in as many jurisdictions as we can around the world. And just in case that there is trouble with any governments in any part of the world that they could flip overnight and that we would want to know that we have very broad coverage of our validator set.
00:18:28.375 - 00:18:30.539, Speaker A: And I'll turn it over.
00:18:30.627 - 00:18:41.055, Speaker B: Thank you. If you have any thoughts. How much harder is it to partition 1000 validators versus 100 in defense of 100 validators being enough.
00:18:41.395 - 00:19:23.089, Speaker C: So it's very hard to even get AT and T to create a partition as a normal person, as somebody that's not part of a three letter agency or working at AT&T. It's human. Normal security and activity is actually pretty good and people are honest and it's hard to go do that. And when you have a network like tendermint with 100 nodes, typically you're up to 160 that are geographically distributed. You're talking about partitioning 1020 ATT level organizations. It's extremely hard. Like it is something that I expect NATO would have trouble coordinating because the amount of bureaucracy that needs to all agree to go do it at the same time.
00:19:23.089 - 00:19:44.623, Speaker C: So there is a very practical diminishing returns as the validator set increases. And we'll probably never see 100 validator network running. Tendermint or any other consensus mechanism actually have a double spend attack. We've never seen one out of all the thousand instances. It's very, very unlikely that we'll ever see one ever.
00:19:44.799 - 00:19:58.045, Speaker B: So under this argument, in the few seconds we have remaining, if 100 is sufficient by these standards, how low can we go? What is the minimum number that still qualifies the safety?
00:19:58.665 - 00:20:05.285, Speaker C: Yeah, that's a good question. EOS is 20 and they've never had a double spend attack.
00:20:06.905 - 00:20:10.405, Speaker B: And if 100 is not enough, how high should we go?
00:20:11.825 - 00:20:28.435, Speaker A: Yes, somewhere bigger than a bread box, smaller than a VW bus. And I would argue that to get global coverage we'll need at least 500. Maybe somewhere between 500 and 1500. That's a top of the head answer.
00:20:28.855 - 00:20:31.855, Speaker B: Perfect. Thank you both very much. Thanks everyone for your time.
