00:00:36.454 - 00:01:22.134, Speaker A: Thanks so much for coming. So I am Raj from Solana. We really appreciate this has been a fantastic turnout. The event sold out really quickly. This is actually our first event ever that Solana has hosted and we appreciate you so much. If you are interested in more events, we will be doing more things on scalability, probably on different Dapps, namely decentralized exchanges, and we will likely be doing a hackathon within the next, I'd say three months, but don't quote me on that to our CTO. To learn more about all these you can go to our website solana IO, and you can sign up for the email list.
00:01:22.134 - 00:02:28.894, Speaker A: Also, as should always be done, you should join our telegram and what else we got. Yeah, the topic today is scalability. So I won't get too much into what kind of a problem scalability is in blockchain, or what it is holding back or how we solve it. I think our panelists and in their presentations, they'll do quite a good job of explaining the problem. But I think what we wanted to focus on is gathering a community, getting everyone who has a stake, either as a builder or someone who is helping to solve the problem, or someone who might use blockchain and might suffer from or be affected by lack of scalability, to come together and talk about how we solve the problems, when they'll be solved, what some of the risks might be, kind of how we can expect things unfolding over the next year or two. So we've got a really great panel, really great event today. We've got four speakers.
00:02:28.894 - 00:02:45.294, Speaker A: First, I believe is actually dfinity. So little update. Manoush from Dfinity was not able to join us today. Unfortunately. She is sick. Thoughts and prayers. She's not like that sick or anything.
00:02:45.294 - 00:03:28.332, Speaker A: But we instead have Dominic Williams, who is the chief scientist and president of dfinity, and he is presenting remotely. He was generous enough to step in at the last moment, so he'll be starting. We also have Monica from Kadena, we have Sina from Truebit, and we have Brendan Curtis from radar relay. So Brennan is filling in for Manoosh on the panel because it might be weird to do remote panel stuff. So yeah, with that, I'll let you take it away, Dominic. And let's hope that we get the AV right so maybe we turn up the volume.
00:03:28.508 - 00:03:38.544, Speaker B: Absolutely. Thanks. Yeah, thanks for having me. So would you like a kind of high level view on scalability at the protocol level?
00:03:41.024 - 00:03:41.408, Speaker C: Yeah.
00:03:41.456 - 00:03:42.324, Speaker A: Ten minutes.
00:03:42.824 - 00:03:44.864, Speaker B: High level. And if you have slides.
00:03:45.024 - 00:03:46.164, Speaker A: Oh, you need a.
00:03:47.144 - 00:03:48.112, Speaker B: Give me a slide.
00:03:48.248 - 00:03:57.176, Speaker A: Yeah, so ten minutes is good. And if you have any slides you'd like to share, go ahead and present them. High level on scalability is great.
00:03:57.360 - 00:04:48.676, Speaker B: Yeah, if you can hear me clearly, I thought I'd just run through a sort of brief history of the scaling challenge without any slides because I think it's something that's worth talking to. There are several different kinds of scalability that need. That you need that have to be addressed when you're devising new blockchain protocols. The first kind of scalability is one that allows your protocol to be run by a very large number of nodes. So, you know, traditionally computer science has these things called consensus protocols. And there are different kinds of consensus protocol. Ones that we're interested in are fault tolerance, called byzantine fault tolerant.
00:04:48.676 - 00:06:09.962, Speaker B: And that means that some number of the parties running the protocol can be faulty and malicious, and the protocol continues running. So, you know, back before bitcoin was created, there were a number of people in that kind of cypherpunk community who were looking at using traditional consensus protocols to create centralized networks. So, in fact, I believe Nick Salvo wrote a bunch of blog posts on unenumerated, discussing the shortcomings of traditional consensus protocols, but also trying to find ways they could be used. The problem with those traditional consensus protocols is they have what's known as quadratic message complexity. And that means the number of messages that the network as a whole has to process increases with the square of the participants. And as you can imagine, if you have a million nodes participating in the protocol, you have far too many nice messages. In fact, as soon as you run anything like 100, 200 nodes, these traditional protocols stop working.
00:06:09.962 - 00:07:00.714, Speaker B: They just involved. They generate too many messages and the messages clog up. Decentralized broadcast network, a particular broadcast network. So those traditional protocols clearly didn't scale and people set about finding better solutions. Satoshi Nakamoto was the first person to propose a solution that would enable a network of almost unlimited size to come to agreement. So, you know, in the Nakamoto consensus, you have all the participants in the network race and puzzle to solve. The current puzzle can broadcast a block.
00:07:00.714 - 00:08:07.460, Speaker B: And the nice thing about Nakamoto consensus is because you can adjust the difficulty of the puzzle, you can ensure that only one block we found in expectation every ten minutes. And even if you had several hundred million participants in the protocol, you could adjust the difficulty so that, in principle, just one of them would get to block every ten minutes. And there's some variance. Of course, we all know that as well. Now, of course, the trouble with that scheme is that it's very slow. And people recently have been seeking ways to run to create consensus protocols or agreement, not really consensus protocols, but agreement protocols that can enable all of the participants in a large network to agree on something, but they run much quicker. So as some people have gone back to the traditional consensus protocols, and an example of that would be tendermint.
00:08:07.460 - 00:09:09.860, Speaker B: So tendermint have gone back, and they've taken a consensus protocol from 1988 and adjusted the structure of their network so they can use it. So in the case of tendermint, they have a sort of select set of validators, and it's a pretty small set. Because they've only got a small set of validators, it becomes more practical to run these traditional consensus protocols. So in principle, with tenement, you can have many, many nodes in the network, and they could all agree upon something. But that's largely because the number of participants actually running the consensus protocol is pretty small. It's a filtered set of the overall network, and they sometimes they go round robin, and they have a bunch of participants in the set become leaders in this protocol one after the other. Now, that is one way of doing things and speeding things up.
00:09:09.860 - 00:10:01.634, Speaker B: But the problem is you don't get very good security properties. And a lot of these traditional consensus protocols have other shortcomings, too. Not only can you not have everybody participate, participating in creating this agreement, so potentially, it's very easy to launch a DOS attack on that small number that are selected to do it. In addition to that, if the network becomes very jittery, they can fail to reach consensus. The consensus protocols fail to reach conclusion, and in this case, output a new block, say. So. Dfinity, anyway, represents some years of work in this field, and we have a novel approach which looks like it solves the main problems encountered in the field today.
00:10:01.634 - 00:10:55.622, Speaker B: We first of all use cryptography, a thing called unique deterministic threshold signatures in a chain to generate what's known as a random beacon. A random beacon is a sequence of random numbers, and it's a random sequence of numbers that you can't predict. And dfinity uses this cryptography to create the world's first unmanipulable, decentralized random beacon. So the dfinity network generates a sequence of random numbers in a way that is completely unstoppable and can't be corrupted and can't be manipulated. And it's actually a very efficient process. Even with a network, doesn't matter how big the network is, you can produce the next random number by broadcasting about 20 data, tiny, tiny amount of data. So each new random number is generated by broadcasting 20 data.
00:10:55.622 - 00:11:53.268, Speaker B: But the beautiful thing about it is we're able to bring an absolutely massive network to agreement on this sequence of random numbers very efficiently, without any chance of manipulation occurring without running a consensus protocol at all. We just used the power cryptography and sort of magic math if you like, and I won't try and explain it, but there's a thing called BLS and unique deterministic threshold signatures. It was created at Stanford, a guy called Dan Bonnet, he heads secure systems there is involved with Ben Lin, who's the l from BLS. He 3 meters to my left, working at the moment on this laptop. And by using this cryptography, we can create these random numbers without running a consensus protocol. That's the key point. Now once we have these random numbers in a network of any scale, so we can have a network with millions of nodes, and that's what definitis all about.
00:11:53.268 - 00:12:35.302, Speaker B: We're trying to create cloud 3.0 with massive unbounded capacity. So we anticipate there being eventually tens of millions of nodes in our network. And using this thing called threshold relay, we can create this sequence of random numbers using cryptography without running a consensus protocol. And that's the magic. Once we've got this sequence of random numbers, we can then use it to drive other protocols. So we can use the sequence of random numbers that are created without running a consensus protocol, just using magic cryptography.
00:12:35.302 - 00:13:09.818, Speaker B: And then we can use those random numbers to drive agreement on other things. So we have another protocol called probabilistic slot consensus. And we demonstrated this in October last year with a network of only 500 computers, but they were distributed around the Internet and we were achieving finality in just 1 second. So to put that in perspective, bitcoin requires six confirmations of ten minutes. That's 3600 seconds to reach finality. And there's a lot of variance in that. Sometimes bitcoin can take hours to reach finality.
00:13:09.818 - 00:13:59.786, Speaker B: Sometimes it happens in just 20 minutes or something. Ethereum has less variance, involves about 37 blocks block times shorter. But in expectation, the thing will take ten minutes, approximately ten minutes to reach reality. That's 600 seconds. So dfinity, by bringing this down to 1 second and is running 600 times faster than Ethereum today and 3600 times faster than bitcoin. So that gives you an idea of the kind of progress that's been made creating consensus amongst a very, very large network of nodes. And the beautiful thing about threshold relay and probabilistics what consensus is that? It's not like a traditional consensus protocol.
00:13:59.786 - 00:15:00.778, Speaker B: Somebody can dos the network and it will slow down the analysis, that the protocol will run slower, but nonetheless, it keeps progressing. And this is a very, very important property for an open, decentralized network, because the open Internet is a very, very adversarial environment. And if you create a decentralized network, a real decentralized network, and once the dust has settled and the hackers come out and realize they can short tokens and then attack the network to make money, you know, many, many protocols out there will understand the full nature of this. So, you know, threshold relay, pluralistic slot consensus work like Nakamoto consensus, they continue progressing even when the network's being dosed. Yet they provide even more speed than traditional consensus protocols. And in networks of massive size. The last piece of this whole kind of jigsaw is you can also use random numbers to create scalable validation schemes.
00:15:00.778 - 00:16:00.104, Speaker B: So this is something described back in 2015. We have validation powers and validation trees, and these are other protocols that are driven by the random beacon, driven by the sequence of random numbers created by threshold relay that can validate massive amounts of state transitions, that is, changes to the data that a blockchain computer represents, created by the execution of transactions function calls, effectively. And so these managers, these random, this random sequence of this random beacon, this sequence of random values is actually a very magical thing. You can use it to drive agreement, you know, in a very, very large network. You can also use it to scale out validation. And so this kind of summarizes where the affinity is. And, you know, you can stay abreast by following us and looking at the website.
00:16:00.104 - 00:16:13.504, Speaker B: We're going to push more information, like soon. I believe there's like a set of basic white paper on threshold, really unprofessional consensus that's been put on the site, website. And there's more information coming.
00:16:22.304 - 00:16:22.952, Speaker D: Thank you.
00:16:23.008 - 00:16:23.724, Speaker C: Awesome.
00:16:24.624 - 00:16:31.084, Speaker A: That went off, I think, with fewer hitches than I expected. Thank you so much for joining us.
00:16:33.264 - 00:16:33.664, Speaker D: Yeah.
00:16:33.704 - 00:16:49.894, Speaker A: So maybe we have time for like, two questions for Dom. If anybody has a question, you can raise your hand, or if there are none, then he can get back to work. There you go.
00:16:49.934 - 00:16:50.634, Speaker D: Okay.
00:16:53.134 - 00:16:55.094, Speaker E: Hi, could you talk a bit about.
00:16:55.134 - 00:16:57.406, Speaker D: How dfinity achieves the higher scale compared.
00:16:57.430 - 00:16:58.834, Speaker C: To some of the other blockchains?
00:17:05.274 - 00:17:05.562, Speaker D: Yeah.
00:17:05.578 - 00:17:12.894, Speaker A: So the question is, can you explain how dfinity achieves such high level of scalability compared to other blockchains?
00:17:16.674 - 00:17:56.034, Speaker B: Well, there are different kinds of scalability, and we use cryptography to generate a sequence of random numbers. It's called threshold relay. And the sequence of random numbers is agreed upon, can be agreed upon by a network of any size. Because it uses the magic of cryptography. It uses advanced number theory rather than a consensus protocol. So we can generate a sequence of random numbers in a network of almost unlimited size extremely quickly and extremely efficiently. Everybody in that network can agree upon that sequence of random numbers.
00:17:56.034 - 00:18:38.674, Speaker B: And when you then use that sequence of random numbers, which is created extremely efficiently without a consensus protocol, and can be done in a network of almost unlimited size, we use that sequence of random numbers to drive other protocols. So, for example, we use it to drive probabilistic slot consensus to bring to create a very, very fast blockchain. And we can also use that sequence of random numbers with techniques such as validation trees and validation tabs, which I won't try and explain now, that allow the blockchain computer to create a scale out, a horizontally scalable validation there that can process potentially billions of transactions a second.
00:18:40.934 - 00:18:48.526, Speaker A: Cool. All right, let's do one more question. How do you foresee dfinity interacting with.
00:18:48.550 - 00:18:57.096, Speaker E: The larger blockchain ecosystem, such as working with Ethereum, for instance? Or do you foresee dfinity pretty much.
00:18:57.160 - 00:19:00.264, Speaker A: Being the master network that runs most.
00:19:00.304 - 00:19:01.524, Speaker E: Things in the future?
00:19:04.504 - 00:19:05.728, Speaker A: Did you hear that, Dom?
00:19:05.896 - 00:19:06.440, Speaker C: I think so.
00:19:06.472 - 00:19:07.760, Speaker B: Could you just repeat it?
00:19:07.952 - 00:19:18.604, Speaker A: How do you see dfinity interacting with other blockchains, like Ethereum? Or do you see Dfinity being like the master blockchain?
00:19:20.784 - 00:20:10.016, Speaker B: Well, firstly, with respect to Ethereum, we have a lot of big Ethereum fans on the team, and I think Ethereum and Dfinity are slightly different things. Dfinity is really trying to be cloud 3.0. It's trying to be a tamper proof supercomputer. Now, in order to scale out dfinity, we have to have an asynchronous smart contract model. And that means that contracts make asynchronous calls to each other. A bit like in JavaScript when you call into an XML HTTP object or something like that. So there are some differences between the programming models of Ethereum and affinity.
00:20:10.016 - 00:20:36.752, Speaker B: Ethereum, I think, is more suited to a certain kind of smart contract than Dfinity. Dfinity is really optimized for performance and scaling, so there's slightly different things. Also, Dfinity has a governance system. It's an algorithmic governance system. It's a thing called the blockchain nervous system. And this is really the complete opposite to the codex law. The blockchain nervous system can make dynamic updates to users state.
00:20:36.752 - 00:21:10.624, Speaker B: It can update economic parameters, it can help users patch broken software systems, mitigate hacks, freeze mystical systems, upgrade the protocol, all kinds of things. So Dfinity is very different to. Ethereum has a different programming model, it's much more focused on capacity and speed, and it's designed to access cloud 3.0 and it also has this governance system. So I think, you know, when the dust settles, people will see that definitive different things and they address slightly different needs.
00:21:12.724 - 00:21:13.148, Speaker C: Cool.
00:21:13.196 - 00:21:14.744, Speaker A: Well, thank you so much, Dominic.
00:21:20.684 - 00:22:10.020, Speaker F: This was originally a 35 minutes talk, which I then cut down to a 20 minutes talk, which we're going to do in ten minutes. So here we go. Kadena. We were originally a private blockchain project that grew out of the JPMorgan research labs, and we developed a super awesome smart contract language on top of a really fast awesome private blockchain. And nobody cared because nobody cares about private blockchain. So we took this fabulous smart contract language, which is called pact, and we are putting it on top of our technology that we're calling Chainweb. And that, combined with scalable BFT is going to make us this really great one stop shop platform for business products on a blockchain.
00:22:10.020 - 00:22:42.148, Speaker F: And so if you have this awesome idea to make securitized real estate improvements on a public blockchain, or you are boeing and you want to have all of your mechanics. Whoa, that's a dog. All of your mechanics have all of their entries in a log on a private blockchain. You can use the same smart contract language to do both of these things and have the same underlying technology. Right? So here's the idea. We have smart contracts impact. We have a development tool suite that has all of the bells and whistles.
00:22:42.148 - 00:23:05.364, Speaker F: We have error messages. Our smart contracts can be upgraded because they come with built in governance protocols. We're actually unpacked version two, 8.4, and version three is rolling out in two months. So it's a lisp. It's actually a language that developers actually want to write in. We actually had a insurance company come to us and say, hey, we rewrote our entire project in your language.
00:23:05.364 - 00:24:10.438, Speaker F: Can we get a beta license? They did it in two weeks. So unlike other smart contract languages that were maybe a good first try and maybe should have been upgraded a few times before they went out into the wild, Pact has had some very serious development on it, but we're talking about scalability today. So we're going to talk about chain web. Chain web is how our public network is going to work under the hood, we are doubling down on proof of work. We believe that if proof of work could handle all of the transaction volume that everybody wanted to do, nobody would be wasting their time with proof of stake. That if bitcoin could process the kind of transaction volume that we needed, nobody would be trying to throw away proof of work, because that kind of truth assured cryptographic security is something that throwing that away and instead coming up with some complicated, like, voting algorithm based upon the amount of stake that you have in the network. But please don't collude secretly behind your back, like, don't man in the middle attack, like, all that stuff.
00:24:10.438 - 00:24:56.114, Speaker F: If we just had throughput on proof of work, then we wouldn't have these problems. Chainweb, we have many parallel proof of work chains, all progressing all at the same time, that then share their roots so that they can stay in sync with each other. So imagine 10,000 bitcoin chains all making a block, all at the same time. And then when I complete a block, I pass my solution on to my peer chain, and that's how we all stay in sync. So because we have all of these chains, then you can have this serious scalability. If you have a killer app on a few chains, it's not going to clog the rest of the network. And then you can spread it out and deploy your smart contract to many different chains.
00:24:56.114 - 00:25:26.190, Speaker F: Because we have all these chains braided together, then all of these shared routes make a very tightly networked proof, which is very difficult to attack. And because we have such tight security, it means that you have lower confirmation times. So we get speed. So we probably don't need to do what is a blockchain. So we can probably skip this. You guys know what blockchain is. You probably know what proof of work is, so we're going to skip that, too.
00:25:26.382 - 00:25:27.154, Speaker C: Cool.
00:25:27.454 - 00:25:53.758, Speaker F: This is how we're going to get through all these slides. So this is your base case. Bitcoin, where you have your base graph, is one. You only have one chain. You make your first block and then you make your next block, which references the header from the previous block. You're passing your route. So chain web runs many bitcoin, like, proof of work chains graded together.
00:25:53.886 - 00:25:54.294, Speaker G: Ah.
00:25:54.374 - 00:26:12.888, Speaker F: So we are not one of these projects that you make some sort of a side chain, do some stuff, and then, like, merge the chain back in. All of the chains are created at Genesis. They are fixed. They are all real chains. There's no main chain. There's not like cosmos. You don't have to, like, do some stuff and then come back to the main web.
00:26:12.888 - 00:26:28.884, Speaker F: Like, every chain is a real chain. They're fixed at Genesis. If we decide that we need more because the network has gotten saturated, then we hard fork up to a larger configuration, which the miners will want to do because it means that they can make more money.
00:26:30.594 - 00:26:31.374, Speaker E: Yes.
00:26:32.474 - 00:27:03.706, Speaker F: Here we go. So this is two chains. We have a lot of two chains. The rapper jokes in the office because of this slide. So this is the idea is if you could have two blockchains that passed each other their roots as they both succeed, you can have one get as far ahead from the other one as one before you have to start waiting in this particular configuration. Because the diameter of this graph is one, the degree of this graph is also one. The diameter of this graph is four.
00:27:03.706 - 00:27:30.084, Speaker F: The degree of this graph is three. This is a 20 order chain graph. So the way that this, if you can sort of see, is that one and 15 talk to each other and then 15 and ten talk to each other. So one is never more than two hops away from ten. So at this point, one can get two blocks ahead of ten. But then if ten is lagging behind, then it has to wait. But miners are going to want to distribute their hash power in order to keep the network all moving at the same time.
00:27:30.084 - 00:27:54.544, Speaker F: So. Yeah, right. So header references. The solution to every block is referenced by the peer chains. That's the whole, like, ten and 110 and 15 talk to each other. The peer chain is referenced in the next block. So I finish a block, I take my solution, I give it to you, and then you include it in your next block.
00:27:54.544 - 00:28:51.594, Speaker F: So it's always past referential. This braiding structure means that instead of your traditional Nakamoto 51% attack, where it's just like, wait, four blocks, hope that somebody's not cheating, you have to beat a whole lot more blocks. Because the proof, if you're trying to attack this block in the middle at block height zero, if this is the one that you're trying to attack at block height one, not only do you need to replace that chain, you need to replace the n number of chains that directly reference that one. I should probably say d because it's the degree of the graph, it's the degree number of chains. And then the next one, it's the degree number times the degree number of chains in the graph. So the number of blocks that you have to attack becomes exponentially more difficult, which is how we get the security. And then the security gives you the speed mining many nodes mine all of the chains.
00:28:51.594 - 00:29:28.288, Speaker F: The network is designed to be both adapted for large mining pools and for the individual miner. There's no point in trying to believe that we live in some sort of crypto anarchy wonderland in which everybody has a GPU in their closet. It just doesn't exist. That's not a world where it exists. We have large mining pools because electricity is inefficient and you want people to pool their resources. So if you want to have a GPU in your closet, you can replicate one chain. And if you want to have a mining pool and then have a network, mine all of the chains.
00:29:28.288 - 00:29:57.354, Speaker F: You can do that, too. All the chains. Mine the same coin. If you have a wallet on one chain and you want to pay somebody on another chain, you just do a transfer from chain to chain by simple payment verification. How am I doing on time? Cutting it quick. Okay, let's not talk about simple payment verification. If you want to hear about it later, you can read these slides again, or you can ask me afterwards.
00:29:57.354 - 00:30:33.084, Speaker F: Transferring money, load balancing, smart contracts. Anyway, okay. Lots of chains, all talking to each other. We. I want to see us work with all of these other L2 projects, too. I think that projects like Lightning Network and the Thunderella and all these other projects that are giving additional throughput and scalability solutions, I think that we should all be working together. I want to see us be the backbone of everybody else's project.
00:30:33.084 - 00:30:52.124, Speaker F: That's my dream. We have an awesome smart contract language. I think everybody's going to want to play with it, and I think that that combined with this underlying architecture is a way that we can keep the dream of proof of work alive. Anyway, so that's the short story. If you have more questions, ask me later.
00:30:56.264 - 00:31:16.962, Speaker A: Next up is Sena Habibian. So we're going to save questions for after. The thing is Dominican. You know, he was remote. It was easier to ask questions during the presentation. We'll have a panel. The panel's gonna be, you know, the centerpiece of all this, and you'll have questions at the end of it.
00:31:16.962 - 00:31:43.982, Speaker A: So Sina Habibian is lead engineer at Truebit, another scalability play in the space. Sena actually used to work with me and Eric, so I guess there's sort of a mafia thing going. I don't know. Sina's great, and he has only four slides. He has one slide, and so this is going to go really great.
00:31:44.078 - 00:31:44.990, Speaker D: Thank you.
00:31:45.182 - 00:31:46.594, Speaker A: Sena Habibian, everyone.
00:31:47.094 - 00:32:31.424, Speaker E: Thanks, Raj, everyone. Thanks for having me. So apologies if you saw this present. Or a similar presentation last week at the San Francisco Ethereum meetup. But I'll try to give a quick overview of what we're doing at Truebit. So if you think about how a blockchain operates, a blockchain that allows you to run computation like Ethereum, when you run, what do you do? You write a piece of code, say, in solidity or viper, you compile it down into bytecode, and then you send it to the network, at which point it's stored in the blockchain state. From then on, anyone can send a transaction to the address at which this code is stored and trigger its execution.
00:32:31.424 - 00:33:00.748, Speaker E: And this execution is replicated around the network in a way that's decentralized. It's trustless. Basically, all of the nodes on the network, all of the miners, execute that piece of code and come to consensus on the results of it. They don't execute it at the same time. There's usually one leader elected, the person who creates the block. But eventually, all of the miners on the network need to execute the code and come to consensus on the state transitions that it causes. So that's how blockchain works fundamentally.
00:33:00.748 - 00:33:40.778, Speaker E: And if you think about it, there's a lot of redundancy there. Basically, all of these nodes are replicating the work. When you send a transaction, triggering some codes to run, you're using this shared resource, where basically these transactions are run one after another in a single threaded type of a way. And any time that your computation takes is time that the entire network needs to run. So computation, because of this, is very expensive on blockchains. So ethereum, for instance, has this concept of gas, which needs to exist in every blockchain, like dfinity has a similar concept. Polkadot has a similar concept where you pay per instruction that you're running.
00:33:40.778 - 00:34:14.044, Speaker E: So if you think of a program as a series of instructions, when you compile it down to the virtual machine level per instruction, you're paying a cost and then per byte of memory that your program uses. So if you initialize more variables, the more you need to pay. So it's very expensive. And then Ethereum and other blockchains again have this upper bound on the size of computation. It can fit totally within a block. So ethereum has this concept of the Ethereum block gas limit. And so even I won't get into why this exists, but it's kind of a fundamental property of the blockchains again.
00:34:14.044 - 00:34:56.471, Speaker E: And it basically means that even if you were willing to pay, say, $100 to execute this large piece of code, you wouldn't be able to do it because of this upper bound. So, computation is expensive and bounded, and we're envisioning all these things that we want to use blockchains for. There's decentralized autonomous networks, there's prediction markets, there's peer to peer lending platforms. All of them involve running pieces of code on the network in a decentralized way, and most of them can't be achieved given where things are right now. So this is the problem that truebit is going after. We're basically scaling computation on the network. So how does it work? Let's talk through a little bit of the protocol.
00:34:56.471 - 00:35:48.060, Speaker E: So, Truebit is a crypto economic protocol, meaning we use both cryptography and the economics that come when you have digital scarcity in play to build this system. And it essentially allows people to run computation off chain outside of this giant network while being able to trust that it executed correctly. So how does it work? So, Truebit will be one smart contract that's deployed on any host blockchain. So on ethereum, there will be a truebit smart contract. And then let's say you have another contract over here. Let's say it's Aragon that's trying to tally up the results of some autonomous voting that just happened. And there's an array of 1000 votes that have come in, and they need to iterate through, calculate the results, and make some decision based on that.
00:35:48.060 - 00:36:18.884, Speaker E: And they can't do that on chain because it's expensive. So instead of running that inside their own contract, they make a call to this truebit contract. So they call this function, and they create a task on truebit. And to create a task, you really need to provide three pieces of information. The first is the actual program that you want to run. So in the case of the example, this program will be a function that takes an array and tallies up an array of votes, tallies it up and gives you a result back. That would be the function.
00:36:18.884 - 00:36:55.334, Speaker E: The other thing you need to pass into truebit is the actual inputs you want to run through this program. So the inputs would be the actual array of votes. So you have the program, you have the inputs, and then you also provide some reward. So this is some amount of ETH say, that you attach to this task as a reward for someone to run it for you. So at this point, the task is created on the Truebit contract. Now there's a network of truebit miners. So this is a free entry system similar to the other decentralized networks anyone can install the client on their own computer, on a server and become a part of this network.
00:36:55.334 - 00:38:01.304, Speaker E: And when you're running this client, it's basically listening for new tasks that get created in the system. And if you desire, you can basically download the task, the program, and the inputs, run it locally beginning to end, and submit the solution along with a deposit of your own money, your own ETH. So now you have skin in the game behind the answer that you gave. So in the expected case where the solver doesn't lie, this person who just ran the computation, basically this timeout on the contract passes down, and once it gets to zero, their answer is deemed correct, and the Truebit contract makes a callback to the original smart contract being like, here's your answer, so go ahead. And only one person ran it on their own computer. So that, but that's not the interesting case. What happens if this person actually provided the wrong answer, ran it on their computer, submitted something, but it's the wrong solution? So anyone else in the network who saw this solution come in can submit a challenge.
00:38:01.304 - 00:38:41.866, Speaker E: And to submit a challenge, you also deposit your include your own deposit along with it. So you also have skin in the game. So if a challenge comes in, then from the point of view of the contract, there's the original solution, there's a challenge, and there's two deposits sitting locked up in there. Now it needs to figure out which, what's right or wrong, because it has no idea yet. It needs to resolve this dispute. And this is a part of truebit that, in my opinion, is pretty interesting. So basically, the challenger is responsible for playing out this interactive game, which is a back and forth between them and the solver, which basically resolves to a point where you find out the correct, the truth.
00:38:41.866 - 00:39:39.242, Speaker E: So how does this game work? The challenger goes first. It's their turn. And they use the knowledge. They know that both them and the solver, they ran the program beginning to end, right? They know that at time zero, when they started running the program, they must have agreed on everything because they had the same program, the same inputs, and the same empty virtual machine. So at the very beginning they agreed, and then they ran through all the instructions in the program, and then at the very end, they disagreed because they got different answers. So the challenger uses that knowledge and asks, like, sends a call to the contract querying for the solver state at the halfway point of execution. So let's say there were 100 instructions in this program, and the, the challenger asks for the state of the solver at instruction 50, halfway through the solver.
00:39:39.242 - 00:40:22.292, Speaker E: Then it's their turn. They calculate the state, their state at that point, run beginning to step 50, and they put all the mutable parts of their machine into the calculation of the state. So all their memory, their stack, their registers, they calculate the hash of that and submit it as a representation of their state. Now, the challenger, it's their turn again. They calculate their own state at step 50 and compare it with what the solver said. Now, if these two values are the same, that means that both the solver and the challenger agreed from the beginning up to step 50. Like beginning to midpoint, everything was the same, and the error must have happened in the second half.
00:40:22.292 - 00:41:06.418, Speaker E: If the two states are different, it means that the disagreement already happened in the first half. So using that knowledge, then the challenger either queries for the midpoint of the first half or the second half of the computation. Let's say the two routes were the same. They query for the midpoint of the second half. So it's like, what was your state? At the 75% mark, the solver calculates, it responds, the challenger does the comparison again, decides to query for the right or the left half. Queries solve a response, and basically, if you think about it, the challenger is in each round narrowing down the possible split space in which the disagreement happened to half of what it was before. So at the beginning, you had this giant program with a number of instructions and each back and forth.
00:41:06.418 - 00:41:37.394, Speaker E: You narrow it down to half of the size and progressing through. So this is a binary search and an olog n time, with n being the number of instructions in the program. You narrow it down to the points where you're down to a single instruction where the solver committed to a state. There's a single instruction, and they committed to the state after. And the challenger is basically saying, I disagree with this. At this point. Truebit basically runs that single computation on chain, and it compares the result of it with what the solver had said.
00:41:37.394 - 00:42:27.434, Speaker E: And if the two numbers are different, the solver gets slashed, they lose their deposit. So effectively, what we're doing is we're narrowing down the computation to one single step, and then we're using Ethereum, we're using like the base blockchain and its security as kind of a final judge to adjudicate whether that single instruction was done correctly or not. And what this means is that in the worst case, you have to run one instruction on chain, and you only ever need one honest verifier, one honest challenger in the entire network to catch an error. And that's in the case that things go wrong. In case things are going right, which is the expected case. Only one person runs a computation on their computer and you get the result back. So I'm out of time, but that's effectively truebit, and happy to answer any questions after.
00:42:30.814 - 00:42:39.734, Speaker A: Greg Fitzgerald is the CTO of Solana, and he's going to be our last presentation, and then we'll have a panel.
00:42:39.894 - 00:43:48.092, Speaker C: So scalability is the problem in blockchain to solve this year, and I think Vitalik really said it best. If you tried to build an Uber on unscalable Ethereum, you're screwed. Our blockchain, Solana tries to solve these issues, but then goes another step further. We ask, what is the maximum transaction throughput possible on blockchain? What if the only real constraint was network bandwidth? Could blockchain host the most demanding centralized systems, like the Nasdaq exchange? And that's what we sent out. The answer. We found the answer to be yes, but with some changes to the blockchain format, if we added a clock and a few optimizations on how transactions are processed in the transaction pipeline. Excuse me, over here, we could create a blockchain that would allow for over 710,000 transactions per second.
00:43:48.092 - 00:44:42.274, Speaker C: So we'll put that into perspective here. The Nasdaq processes about 500,000 transactions per second. Or we could host something like Google's ad traffic, which comes in at about 350,000 transactions per second, with more than enough room to spare for all of visa's transactions at peak load. Now, the reason we're able to pull this off is because we've assembled an ideal collection of experts for exactly this task, to build the world's fastest blockchain. We have a veteran team comprised of former Qualcomm systems engineers. Anatoly Akavenko, our CEO, is an expert in distributed systems and operating systems design. I come from Qualcomm's office of the chief scientist and the LLVM compiler team.
00:44:42.274 - 00:45:13.664, Speaker C: And Stephen. Stephen Akridge, our magician with the GPU. Anything in transaction processing that seems expensive, Steven will split it over 10,000 GPU cores and make it negligible. It's kind of amazing. He's right there if anyone's looking for autographs. So 710,000 transactions per second. Without sharding, how is that possible? It's not just Stephen hacking on the GPU, though, that has been very significant.
00:45:13.664 - 00:46:14.790, Speaker C: The big innovation, the thing that forced us to define our own blockchain format, is what we call a proof of history. It allows us to implement a whole slew of optimizations, because time is the gold standard of distributed systems. If you have it, optimization is easy. If you don't, you're forced to use more sophisticated algorithms that take longer to execute, like proof of work or the practical version of byzantine fault tolerance. So how do we define time on an adversarial peer to peer network? Well, just like dfinity generates randomness and stores it on chain, we generate a notion of time and store that on chain. We do it by continuously running a Sha 256 hash over itself, forming a chain. Each link in the chain represents a duration of time.
00:46:14.790 - 00:46:58.784, Speaker C: It's like a very small proof of work. By gluing them together, you get a longer duration. And when you put together enough of them that it's more than a typical network latency, you get an upper bound on a voting time right, so you can implement proof of stake that's not only simpler, it's much faster. The voting rounds are much faster, and that implies a faster finality time. But to get to sub second finality, it's not enough to form consensus quickly. You have to quickly validate these massive blocks of transactions, and you need to quickly replicate them across the network. So we'll talk about the local processing first.
00:46:58.784 - 00:47:49.564, Speaker C: Right out of the gate, we knew that signature verification was going to be a bottleneck, but also that it's this context free operation that we could offload to the GPU. But even after offloading this most expensive operation, there's still a number of additional bottlenecks, such as interacting with the network drivers and managing the data dependencies within smart contracts that limit concurrency. We needed to find a way to keep all hardware busy all the time. That's the network cards, the CPU cores, and all the GPU cores. And to do it, we borrowed a page from CPU design. We created a five stage transaction processor in software. We call it the TPU, our transaction processing unit.
00:47:49.564 - 00:49:00.704, Speaker C: So by the time the TPU starts to send blocks out to the validators, it's already fetched in the next set of packets, verified their signatures, and begun crediting tokens between the GPU parallelization and this five stage pipeline. At any given moment, our TPU can be making progress on over 50,000 transactions simultaneously. And this is all with an off the shelf computer for under $5,000, not some supercomputer. So with the GPU offloading and our pipeline TPU, we have single node performance pretty much covered. The next challenge is to somehow get the blocks from the leader node out to all the validator nodes and to do it in a way that doesn't congest the network and bring throughput to a crawl. For that, we've come up with this replication strategy that we call avalanche. With avalanche, we structure the validator nodes into multiple levels, where each level is at least twice the size as the one above it.
00:49:00.704 - 00:49:50.834, Speaker C: By having this structure, these distinct levels, finality time ends up being proportional to the height of the tree and not the number of nodes in it, which of course, is far greater. So every time the network doubles in size, you'll see a small bump in finality time, but that's it. Log. Nice sub second finality times at effectively any network size. And since avalanche gives us these amazing scaling process, excuse me, these amazing scaling properties, we've rolled it into our storage solution as well. After validating a block, the network rewards a subset of the nodes that are willing to continue hosting the block. So, like a torrent network, nobody needs to maintain a full copy of the ledger.
00:49:50.834 - 00:50:42.024, Speaker C: But a full copy is always available. Anyone in the network can help store it, and they're rewarded for that service with a portion of the transaction fees. So, what's next for Solana? Our testnet is under active development, and we'll be making our third major release next week. Just to give you an idea of the trajectory that we're on, in our first release, just two months ago, we demonstrated a 7000 transaction per second single node network. And now, last month's release, we upped it to 35,000 transactions per second. Next week, we'll be launching our first multinode testnet, and we're expecting it to process well over 100,000 transactions per second. After that, we'll be turning our attention replication and storage.
00:50:42.024 - 00:51:23.708, Speaker C: And as I mentioned earlier, Avalanche is expected to scale cleanly to 10,000 nodes without degrading finality times. Now we'll prove it. We're aiming to have our full testnet up in June and launch our main net by the end of the year. All right, one last thing for application developers. We're going to build you a smart contract platform unlike any other in the blockchain world, one where you choose the language, the one that's best suited for your domain and your expertise. Now, what we currently have is a fairly minimal language tuned for performance. And while we'll continue to support that, what we'll be implementing next is far more general.
00:51:23.708 - 00:52:04.464, Speaker C: It's not a language at all. It's just an interface to a safe execution environment. See, from the network's perspective, it doesn't care about very much of your contract at all. It cares about who you're trying to spend your token, who you're trying to send tokens to. From the network's perspective, it's like a black box. Events come in, the black box changes state, and when it has acquired enough information, it tells the network where to send the next set of tokens. You should be able to write your contract in any language that targets this safe execution environment.
00:52:04.464 - 00:52:56.946, Speaker C: And since we'll be using an LLVM backend to execute these contracts, that means you'll be able to implement your smart contract language in nearly any mainstream programming language. You implement the interface, compile it for our environment, you're good to go. Now, in the short term, our existing contract language is fairly expressive, and because it's very small, it's quite straightforward to ensure that it's secure and that it performs well. And because we built it ourselves, it's easy for us to extend for the needs of our early partners. If you'd like to be one of those early partners, to be one of the few that gets custom tailored solutions built into the blockchain, please contact Ellenu. To learn more about our blockchain, check out the Solana Whitepaper online. Or if you have any technical questions, please jump in on our Telegram channel.
00:52:56.946 - 00:53:02.094, Speaker C: We love what we built and we're more than happy to talk your ear off about it. Thanks everyone.
00:53:04.754 - 00:53:20.894, Speaker A: It's time for the panel. Exciting. I've already introduced Sam. Yeah, just to reiterate, VP of Facebook, VP of product of Facebook. He's at fin exploration company now. He founded it. It's an amazing AI and virtual assistant type of thing.
00:53:20.894 - 00:54:01.528, Speaker A: Also major contributor at the information. If you ever see an information chart, like something that is a quadrant matrix or a graph, probably written by Sam, and probably only half accurate, philosophically accurate. And then we have Brandon Curtis, who is filling in for dfinity. So he is the director of research at Radar Relay, which is a relayer. They are in the decentralized exchange space. They're more on the who is using blockchain that is highly scalable. And then the rest you already know because they're wonderful people that already presented.
00:54:01.528 - 00:54:04.264, Speaker A: I will turn it over to you, Sam. Great.
00:54:04.304 - 00:54:05.192, Speaker E: Thanks for having me.
00:54:05.248 - 00:54:26.964, Speaker D: Take care. Thanks for having me. So I thought we'd start out. We heard introductions, obviously from all the panelists, but getting a little sense of who's in the room. Like, are you guys. So, can I ask you, I'll do three quick hand raise you questions. The first is how many of you guys are actively developers working on crypto projects right now? Okay, it's a pretty good turnout.
00:54:26.964 - 00:54:56.214, Speaker D: How many people are developers or engineers that are working in other parts of the ecosystem but kind of secretly want to be working in crypto? Okay, a few. And then how many people are, I don't know, like investors? Other, other angle. What is the other category? Buyers. Ah, fair enough. All right. Fair enough. How many of you are token speculators? Fair enough.
00:54:56.254 - 00:54:56.454, Speaker B: Great.
00:54:56.494 - 00:55:24.482, Speaker D: Okay, well, it's good to just understand kind of a little bit who's in the office? In the room. Oh, carefully. So, you know, the thing I think just a little comedic relief there. The thing I figured we could kick off the panel with, which I think it would be really cool to just hear a little bit from each of you, is you guys each gave very impassioned pitches for your version of scalability and why proof of stake is crap and proof of work, whatever the thing is going to be.
00:55:24.538 - 00:55:26.186, Speaker F: I did not use the word crap.
00:55:26.330 - 00:55:36.834, Speaker D: That was my word. But I'd actually love to hear you. If you were going to give a short argument against yourself, what would it be?
00:55:37.294 - 00:56:14.614, Speaker H: So I'll go first. As I was already introduced, I'm the director of R and D at radar Relay, and so we're based on the Ethereum network right now. We allow trustless exchange of ether and Ethereum tokens. And we're in a unique position where we don't care who wins the scalability wars or the cross chain wars, we just need to see more technologies that are in production that we can build on. Because right now we are limited by the scalability of Ethereum and we're limited by the lack of connectivity between these networks. So we just want to see more technology get developed. That's why I'm here.
00:56:16.754 - 00:56:18.906, Speaker D: So I'd like to hear an argument for proof of stake.
00:56:19.010 - 00:56:47.348, Speaker F: Ok, sorry. Actually, proof of stake is great for the things that proof of stake is built for. We have a proof of stake consensus mechanism. It's called scalable BFT. We use it in our private network. And it's exactly what you want. When you have a semi trusted environment, you have people that you know who they are and you've let them through the gates, but they might be adversarial and that's exactly what you want a byzantine fault tolerance system for.
00:56:47.348 - 00:56:55.148, Speaker F: That's what proof of stake is built for, and I think that we should keep using it for the things that it's good for. So, but that's not an anti Kadena statement.
00:56:55.196 - 00:56:57.052, Speaker D: Not quite. So you want to keep going? Go for it?
00:56:57.068 - 00:57:28.318, Speaker F: Yeah, sure. I mean, I think the biggest problem that we have is that there are so many projects right now, and we get a lot of, like, well, Cardano. And we're like, no, no. Just because we're in New York and we're a Haskell shop and our name kind of sounds like that does not mean that we are Cardano. Stop it. Stop it. So, like, how do we differentiate ourselves in a space that is becoming increasingly crowded with a bunch of really interesting projects? I still don't know if I know the answer other than just trying to keep spreading the word.
00:57:28.318 - 00:57:31.310, Speaker F: Like, try our stuff, come look at our things.
00:57:31.422 - 00:57:32.074, Speaker E: But.
00:57:32.414 - 00:57:40.434, Speaker F: So I guess that's my biggest. Like, who should. Why would you care about Kadena? I'm like, well, I don't really know, except that we have a really great project.
00:57:41.614 - 00:57:42.594, Speaker D: Fair enough.
00:57:44.374 - 00:58:55.254, Speaker C: All right, so our radical wager, I guess, right now, is that we can do enough transaction processing without sharding to meet the needs of the entire blockchain industry right now. If that turns out to not be the case, or, say, the big moneymaker in the space, I guess, is something that needs a million transactions per second, and realistically, sharding becomes the right solution for that. Maybe your contracts are simple enough that it doesn't matter if they're split up across shards, or maybe you're really good at writing in continuation passing style, which I know our Haskell guys would be able to do. All right with that. And that this happens, this need for this many transactions happens before network speeds increase. Because once we have 40 gigabit network and 400, our architecture is planning to be IO bound. So that need has to come before the hardware can support it.
00:58:55.254 - 00:58:58.274, Speaker C: That's what we're betting on. And if we're wrong, we're out.
00:58:59.774 - 00:59:00.754, Speaker D: Fair enough.
00:59:01.734 - 00:59:05.918, Speaker E: Hey, so question was, what? What are some of the big problems I see with truebit?
00:59:05.966 - 00:59:16.894, Speaker D: Well, I go even further, which is, if you're wrong, it turns out you look back in ten years, and you're like, man, that was a drunk test. Years. I was totally off in Crazyville. Like, what happened? Why? Like, what did you get wrong?
00:59:18.514 - 01:00:15.678, Speaker E: I think this answer that I think about, I think it applies to a lot of the projects in this space, and that these projects generally start from an interesting point of research or technology, or they have their birth in academia. And at the end of the day, it's like a massive technical problem. You need to build all these new pieces of technology, but is there going to be product market fit when you launch? And it's diff. You know, when you're building a normal product, you start from the user and you work backwards, and the technology really doesn't matter. You need to deliver value. So how do we think about that in this space, when you're taking so many fundamental bets on the technology and you actually delivering a working thing is like many months ahead because you need to build such an infrastructure. So I think that applies to everyone, and there's multiple parts to it.
01:00:15.678 - 01:00:59.816, Speaker E: I think one, you need to have kind of a strong thesis on where the space is going and be able to project into the future, and you do have to kind of take a bet. On the other hand, I think what we're trying to do in truebit is so our users are developers or applications that are trying to run on Ethereum or other blockchains and can't due to the computational limit. So we've basically pinpointed, like, three or four of these, and we're using them as first use cases, like developing true bit, the interfaces, like the actual experience of it in partnership with them to make sure that it works for their use case and then kind of viewing them as a proxy for this upcoming wave of decentralized applications and where that will go.
01:01:00.000 - 01:01:17.762, Speaker D: Yeah, I mean, I actually want to build on that question a little bit for all of you, which is, I think everyone's kind of hit on this question. What is the use case? Like, where are we going? Where you need scalability? I mean, I would be. You guys can push back on this. In the camp that bitcoin occupy, proof of, I would say store of value occupies an interesting point.
01:01:17.818 - 01:01:19.106, Speaker C: Other than cryptokitties.
01:01:19.210 - 01:01:57.004, Speaker D: Well, other than to push on that, collectibles is a real thing. But I would say take like, if you believe as I do, you can push back on this. Please do that. For instance, bitcoin, while completely unscalable, is in its current version plenty scalable for store value ish. Right? And there's obviously a lot of speculation going on, but if you had to bet, and it sounds like you might have some very specific bets if you're working with a few partners on this, what are the cases where all of a sudden the different forms of scalability you are pursuing because they're all a little bit different, come into play in a meaningful way from a consumer experience?
01:01:57.724 - 01:01:58.036, Speaker C: Sure.
01:01:58.060 - 01:02:54.152, Speaker E: So I can take that. So I think at the baseline, what this technology has achieved to this point is that you can quote unquote, keep states across, you know, the Internet. And what this means is you can for the first time have like money and like financial value transfer built into the fabric of the Internet. And you know, if you think about it, you can do a video stream with someone on the other side of the world, but you can't send them money. And you can't with Ethereum, for instance, you can literally write a function that is like take the money in an insurance contract, take money in every month when there's report of an incident paid out to the person. So I think these kinds of things as a baseline I see happening. Beyond that, I think with this technology, as it was with say the Internet, the most interesting things will be ones that are uniquely enabled by the tech.
01:02:54.152 - 01:03:39.834, Speaker E: And some places that I think are interesting are decentralized autonomous organizations. It's really weird to imagine what this could look like. But I think if you kind of read a bit of Sci-Fi, you can see these types of decision making agents that are distributed across multiple computers. They can't be shut down, they have their own learning processes. And if you really want to, like if you were 30 years ago and you were trying to project like what the Internet would have done to the world, it's hard to see. But I think those are the big, big use cases that would like the big impacts that will have on the world. And I think something, I mean, this is not the bet we're necessarily making a true bit, but you need computation to let this technology achieve its full potential.
01:03:42.854 - 01:04:39.704, Speaker C: I'll just add to that that one thing that, you know, an example of how it's hard to see what's possible is that, you know, we're looking at what's on blockchain as these discrete events. And so we're looking for applications where requires more discrete events. And we can come up with some ideas for that. Like anything that's an exchange, the idea of anything that currently right now is off chain to do it on chain. The one that interests me most is the idea of all the bids and asks in a transaction that those all be recorded on chain. So you can really see what market price actually means with respect to the number of bids and asks in the system at the time. That's interesting stuff, but continuous functions is something I think that we haven't really looked at at all yet.
01:04:39.704 - 01:05:14.454, Speaker C: Think about the forming of calculus. You know, you have these discrete events and they get closer and closer together. And whoever thought to talk about derivatives or integrals when discrete math is all that we had, you know, so, you know, by bringing the scalability to this whole other level, it's just, it's where people can start to dream again at a different level. So it's pretty cool to be a part of that and see just what comes out of it.
01:05:18.114 - 01:06:11.214, Speaker F: I think limiting ourselves to money is like, not. We should think bigger than that. There are a whole lot of other non monetary assets, not just cryptokitties that we have an opportunity to data that can belong on a chain that currently is not. I'm not saying that, like, okay, real estate improvements, but not for like your class, a big shiny office building. There's a bunch of tranches of real estate improvements that I used to work in real estate once upon a time that for like the sort of mediocre office parks in like, central Maryland, those things, you can't really secure a loan against those things because there's not enough concrete information on them about when they were last financed, when they were built, when they were created. If we can actually record that information in a way that allows you to have clear title search, suddenly you've unlocked like a trillion dollars of potential liquidity.
01:06:11.294 - 01:06:39.544, Speaker D: But just to push you, because that's my job on the panel more than anything else. I mean, I think there's this other massive living problem, which is like the oracle problem, right? Which is you can come up with all you want in digital space, right, and all sorts of really interesting, scalable things out there. But if really what you're talking about is mirroring the complexity of the real world into a digital space and then dealing with the complexity of the real world in digital space, you have this massive interface problem which none of your solutions really speak to. Right? I mean.
01:06:43.364 - 01:07:12.852, Speaker F: Right? Attestation only works if you have somebody to attest to the thing. There's no point in tracking attestation until you have somebody that you're actually willing to have attest to a thing. But I think that there's actually an appetite for people wanting to have proof of ownership for, like, for example, I lived in Peru for a while. And in Peru, like, people would love to be able to actually have title to their homes, but they don't. So there's like no mortgage network in Peru, and people would register their things if they had a way to do so.
01:07:12.908 - 01:07:17.908, Speaker D: But how? Just to push you because we can move on to talk because it's not specifically about technical scaling, but, like, I.
01:07:17.916 - 01:07:18.584, Speaker E: Would just.
01:07:20.324 - 01:07:30.882, Speaker D: In the end of the day, if it turns out that the model of transitioning between the physical world and the digital world is going to be some other framework. My question has always been how do you decentralize that?
01:07:31.018 - 01:07:32.314, Speaker F: Decentralized what?
01:07:32.434 - 01:07:43.414, Speaker D: Decentralized the actual, like I claim I own this place. Whatever. You very quickly end up back in a centralized world where like this point of centralization is the interface between digital and physical.
01:07:44.194 - 01:08:09.430, Speaker H: But, yeah, so you don't always need to. And so. Sure. All right. So I'm interested in the tokenization of everything, not just because I'm in sort of the exchange business, but because I see a lot of systems out there that currently have lots of middlemen involved and lots of transaction costs, things like issuing municipal bonds. It's very expensive to do this. Banks are involved.
01:08:09.430 - 01:08:46.333, Speaker H: It takes a long time. You can't have an ICO for a local park because the money you have to raise with municipal bonds is so much larger than that that you just raise $100 million and throw it in the general fund. If you could have an ICO for local park or local improvements, local transit, those sorts of things, maybe people want to buy those bonds at three point something percent because they get a community improvement along with that. And right now we don't need to decentralize that entirely in order to improve it drastically by moving it into a token of some sort.
01:08:47.113 - 01:08:48.093, Speaker D: Fair enough.
01:08:50.813 - 01:08:52.621, Speaker F: I actually have to go to the airport.
01:08:52.797 - 01:08:53.713, Speaker E: I'm sorry.
01:08:55.973 - 01:09:02.589, Speaker F: Yeah, I have a flight back to New York. Thank you so much for having me. I'm sorry for ducking out early. This was really great.
01:09:02.661 - 01:09:03.553, Speaker G: Thanks, guys.
01:09:03.893 - 01:09:05.593, Speaker C: Thanks for being here, Monica.
01:09:07.853 - 01:09:53.363, Speaker D: All right. I've never had a panelist leave. Mid panelist must be going great. So let's go back a little bit more and talk a little bit more deeply technically now, rather than getting into the whole how do we interface with the real world thing, let's forget the real world. When you think about the different scalability solutions you guys are each proposing in different ways, some of them seem pretty symbiotic, and there are obviously places where you look at projects that are competitive or in real tension with each other. How much do you think that the ultimate scalable idea of decentralization is going to end up being centralized around a few players versus kind of an interconnected web of different scaling solutions?
01:10:00.863 - 01:10:37.612, Speaker E: So I think one of the core parts of this ecosystem is that all the code is open source. Right. Because they are distributed networks means that different people from around the world need to go and download this piece of code and run it on their own computers to become a part of the network. And because people won't trust you know, won't run untrusted code on their computers. It needs to be open source. And because it's open source, it means that it can change, it can evolve. So I think that's one part of the puzzle.
01:10:37.612 - 01:11:16.684, Speaker E: And then I think we need to explore with a whole bunch of different models in order to find which ones work. That doesn't mean that eventually, if there's, like, four or five protocols that run the entire network, that value will have accrued to, like, four or five teams, because I think networks that can kind of get off the ground and don't get forked out will have an equitable distribution of, like, their tokens or like, whatever the fuel is that operates that network. So it doesn't mean that five teams will dominate the space, but there could be five protocols that eventually run the entire thing.
01:11:25.344 - 01:11:25.728, Speaker B: Sure.
01:11:25.776 - 01:12:12.874, Speaker H: Yeah, I can jump in. So this was touched on a little bit in some of the. In some of the earlier presentations that there are a lot of different ways of measuring decentralization. Are you interested in who's committing code? Are you interested in how many nodes are being run? Are you interested in how censorship resistant? There are a lot of different ways of measuring it. And I think that for different applications, whether you're talking about collectibles in the blockchain, whether you're talking about currency, whether you're talking about securities that may be issued by a centralized authority of some sort, there are of different levels of need for decentralization. And so I think it's highly likely that we see a variety of different solutions that do very well for themselves because we've only just scratched the surface of the applications of these networks.
01:12:15.054 - 01:12:59.064, Speaker C: All right, so what we want to do with Solana, I guess we think that probably this consolidating of power, I guess, would likely, eventually happen, and it could happen quicker. Anytime there's a successful fork, it goes into the wrong hands with maybe the wrong governance model. And so what we've done is we actually picked up one more special co founder. Is Doctor Eric Williams. Here. Where did he go? Oh, there he is. So, Doctor Eric Williams, there he is, a former particle physicist from CERN.
01:12:59.064 - 01:13:41.854, Speaker C: He's a hunter of the Higgs boson. And we're having him basically model and simulate every conceivable token economy that we can think of in search of what game theory calls the Nash equilibrium. It's this kind of wonderful place where all incentives are aligned. And even though we have all these individuals acting in their own self interest, that they all conclude the decision that's actually best for everyone. So we're hoping to basically have this nice, decentralized thing go for as long as we can, and if it doesn't work out, we're going to blame Eric.
01:13:44.874 - 01:14:01.534, Speaker H: My counter to that would be look at the world around you. How many systems do you see operating at a Nash equilibrium in the real world? So that's why I fully expect if things right now are messy, we're only getting started. I have a feeling they're going to get even messier going forward.
01:14:02.314 - 01:14:45.084, Speaker D: Yeah, I mean, that makes a lot of sense to me. I mean, I think there's so many wide variety of incentives, right. And not all players are rational, self interested parties, I would argue when you look at the whole cornucopia of the world, but it's an interesting question about how it'll play out. So I want to ask, there was a poll done actually, I think before this event about what kind of the big use cases around scalability would be, and there are kind of three that pop out and then one that I have to talk about because of my background. I'm curious, people, the three that popped out as kind of the big voting areas were one, decentralized exchanges. Right. Pretty obvious, I think, why people would be interested in that.
01:14:45.084 - 01:15:31.590, Speaker D: Although if someone wants to articulate it as a member of the hell, that's fine. Two is payments, again, pretty obvious. Three is games, again, pretty obvious. Right. Like there's a straightforward path why you, why that's a valuable or an interesting thing, although it's not clear to me, and I'm curious if you guys want to push on this outside of the store of real value stored in games, how much value you really get from decentralizing games and what the points of centralization versus decentralization are on it. Are you really worried about most of your games being disrupted? But the fourth one I want to get to and then we can get back to that, is that defacebook made quite a strong showing in the poll. So given as a former Facebook group, I'm actually really curious.
01:15:31.590 - 01:15:45.404, Speaker D: Do you guys, as people who think a lot about scalability and decentralization, are you in the camp of we're marching towards decentralized Facebook, Airbnb, Uber, Lyft? Or do you take a more circumspect or a different approach to that question?
01:15:50.824 - 01:16:47.484, Speaker H: Yeah. So I think that depending on what you're trying to do, there may be more or less demand for decentralization. If you ask people what would you improve about Facebook, I don't think that many people would vote right up at the top, that they wish it was more decentralized. So I really think that it's going to depend when you're talking about a currency. People are interested in things like stability, and I think that they're, for many applications, willing to suffer a certain degree of centralization to get the other features that they want. I think that going forward, I don't think that 20 years from now, every single thing on the entire planet is going to be on a blockchain. I think that some things that make a lot of business sense are going to be on blockchains, and plenty of other stuff is going to stay centralized.
01:16:50.304 - 01:17:49.394, Speaker C: I think that hatred is building for centralization of data. You can't see what data is being held. You don't know anything about the security protecting that data. And occasionally there are these massive data leaks and you lose everything. And you had no idea that they even knew that about you. So what I think is that at some point, that the customer will demand that if they're going to use an app, if they're going to participate, that the data that they contribute is part of this decentralized system, so that they know something about that. I don't know what form that'll take exactly, if that'll be like building a Facebook on top of a decentralized platform.
01:17:49.394 - 01:18:04.634, Speaker C: Maybe it all goes that way, but that's definitely one use case motivation, I suppose, that will have very, very big things, something as big as Facebook trying to launch on a decentralized network.
01:18:06.614 - 01:18:48.384, Speaker E: Thanks. So I do actually think Facebook being centralized is one of the sketchiest things about the whole situation. This is just like a massive data silo that only Facebook has access to. So on the one hand, it carries with it immense power and knowledge about each one of us and what we're doing in our personal lives. But more than that, the Internet is a very insecure, broken place. Things get hacked all the time. So think about all this data that's being accumulated everywhere, sitting there, and if it gets into the wrong hands, at some point, we're all kind of like doomed.
01:18:48.384 - 01:19:46.802, Speaker E: But I also have a tough time seeing how Facebook itself could be decentralized, because so many of these products rely on hyper customization and machine learning that's run across a large number of people. So if each person's data is encrypted with their own keys, and no central party can learn from all the data and come up with some scheme, how would you build Spotify if everyone's listening history was encrypted you would have no way of being like, oh, this person listens to this music. This person listens to similar music. So let's refer these, like, across their listening habits. And there's some schemes that try to get at that, like multi party computation and homomorphic computation, but I don't know how close that is. On the other hand, I think, like the big part, like the part of Facebook that I use the most myself is messaging. And I think messaging is already.
01:19:46.802 - 01:19:54.570, Speaker E: There's already tools out there like signal that are fully encrypted, like peer to peer, and there's no centralized party. So I think those things are already happening.
01:19:54.602 - 01:19:55.898, Speaker D: You don't need crypto for that.
01:19:56.026 - 01:19:56.466, Speaker E: What's that?
01:19:56.490 - 01:19:57.546, Speaker D: You don't need blockchains for that.
01:19:57.570 - 01:20:01.934, Speaker E: You don't need blockchains for that. Yeah, yeah. You don't need blockchains for everything.
01:20:05.014 - 01:20:07.230, Speaker C: Let me just throw one more out there, actually.
01:20:07.302 - 01:20:08.462, Speaker H: I guess a bit of a plug.
01:20:08.518 - 01:20:43.194, Speaker C: For our friends at origin. Very cool. What they're doing and the idea that say, like, if you want to put up a Airbnb or sell your bike on Craigslist or something, you don't necessarily have to give one private party this information and expect them to do the marketing that would stay with them. Instead, you know, you just want to sell your bike. You just want to put it out there that I want to sell my. I mean, not me personally, I don't sell any of my bikes. All right, but that was funny, guys, come on.
01:20:43.654 - 01:20:44.678, Speaker D: We thought you were serious.
01:20:44.726 - 01:20:46.326, Speaker F: You don't have no bikes for sale.
01:20:46.470 - 01:20:48.118, Speaker D: It was just information for us.
01:20:48.246 - 01:21:14.062, Speaker C: But anyway, with all these market. With all these markets platforms, how cool would it be if the consumer had the option or the seller to just put it out there? I want a ride. You don't have to tell Uber you want your ride. You just want to say you want a ride to this location. And any service, any application could build on top of that and pull the data from the chain and offer you a ride.
01:21:14.118 - 01:21:35.534, Speaker E: The lowest bidder, I think that's one of Zero X, for instance. The shared pool of liquidity is a really powerful force which a centralized exchange would never offer. So by having all the orders, like, shared in this decentralized network, you can potentially get them cleared faster and arrive at more efficient prices.
01:21:39.554 - 01:21:42.954, Speaker D: We were just talking about origin. You want to jump in and answer? Talk about origin a little bit.
01:21:43.074 - 01:21:44.082, Speaker C: Is origin here?
01:21:44.178 - 01:21:47.690, Speaker D: Yeah, here. So sorry, I don't know your name.
01:21:47.762 - 01:21:52.454, Speaker I: Did I misrepresent Coleman partnerships at Origin?
01:21:53.974 - 01:21:54.422, Speaker C: Welcome.
01:21:54.478 - 01:21:55.474, Speaker I: If anyone has.
01:21:58.814 - 01:22:01.714, Speaker D: Decentralized and what doesn't matter.
01:22:03.134 - 01:22:52.154, Speaker I: Well, I guess the kind of witty one liner I like to use specifically about origin is that peer to peer marketplaces shouldn't be peer to giant asshole corporate monopoly to peer marketplaces. So if we can figure out a way to architect it where it's actually peer to peer, and we're building on open source standards that can be shared, and we're going to enable a lot fairer marketplaces, more open marketplaces. And a lot of the gripes people have with things like Uber or Airbnb is because of centralization. There's arbitrary rule changes or people don't feel like they're being treated fairly by the platforms themselves.
01:22:53.934 - 01:23:45.042, Speaker D: I mean, the question I would ask all of you guys with this kind of narrative, I get the narrative. It's a super cool and interesting one. I would argue if you kind of unroll the clock 25, 30 years and ask and redid this panel at the earlier days of the original Internet, of the Internet we currently use today, you would have had a bunch of similar people saying, we can all of a sudden talk to each other with no middleman. This is unbelievable. We can now build all these great things where we don't need to talk to at and t in the middle or whatever if we want to send a message around, or you don't need to use these central authorities. I actually think most people will be shocked. Not everyone, but a lot of people will be shocked that what ended up happening was a massive consolidation of power based on what fundamentally the fundamental innovation is encrypted end to end communication from any peer to any peer blockchain.
01:23:45.042 - 01:24:21.634, Speaker D: If you believe that a lot of the stuff you guys are working on and you're trying to scale is just, all it is, is if we had secure, encrypted end memory, I'm sorry, communication, we're just adding memory to that. We're saying now we also have a store of memory, not just the communication over the wire that way, what's the probability that when you look at the crypto yuan and a lot of things going on, that actually when the scalability starts working on this, in 30 years, a panel kind of like this once again is going to be up here saying, I can't believe those guys thought this was going to be decentralizing or going to give power back to the people. And actually this is the most centralizing thing in history.
01:24:22.534 - 01:25:08.994, Speaker H: Yeah, I mean, a lot of old people still use Yahoo. So I think that oftentimes people sort of underestimate the sort of the power of inertia to shape the way that things go in history. And so that's why I'm just sort of of the position that I think that a lot of things are going to change, a lot of very important things are going to change. But as much as they change, many things will remain the same, if only because of inertia. And so when we talk about, you know, the time skills over which these things are going to happen, a lot of these protocols, a lot of these ideas that we're exploring now are going to mature over the next decade, two decades. But in terms of how long it takes for a good idea to sort of spread through a market, it takes generations.
01:25:12.574 - 01:25:56.884, Speaker C: I'm afraid. This is a powerful force that we're playing with. You know, like Facebook. Did you guess they'd sway an election? Maybe they didn't. But with crypto and privacy and the ability to hold wealth in a whole bunch of different public keys and nobody knows who has it, I think we know that generally the rich know how to get richer. And so if they're able to hide that wealth, then this is a bit of an enabler. It can be.
01:25:56.884 - 01:26:19.154, Speaker C: I think we need to be thinking about that, thinking about what we can do to create the future that we want to live in. I'm not sure what that should be right now, what that should look like, but I could definitely tell you that I think fear is justified in this arena.
01:26:22.014 - 01:27:19.148, Speaker E: Yeah. So I think that's a really interesting point. And there was this book I read when I first got into the space. I think it was called, like, fall of information empires or highly recommended, but basically it talks through the history of all these different communication tools like telegram, the radio, the Internet, and how every time they kind of entered the world, people thought it would democratize voice and allow people to communicate to each other freely. But then it somehow becomes a more centralized system as it consolidates. And I think there are some things that are really risky about crypto. On the one hand, the most promising ways we have of voting on chain or proof of stake, they're all kind of stake based systems, right? So if there's this global thing that's governing what's happening in the world and it's stake based, then it means it's kind of like governance by the rich.
01:27:19.148 - 01:28:03.532, Speaker E: And a lot of the existing coins, like bitcoin and ethereum are heavily concentrated in how the wealth is held. So that's one thing that I think we need to figure out. Another is that if all of your transactions are public on this transparent ledger. That's also a very negative situation. So I think having cryptography built in like Zcash and Monero do, is hugely important. And it's kind of scary to think about this stuff hitting real world adoption before that's baked in, because then you could pinpoint someone's full historical financial life to them and learn, learn so much about them.
01:28:03.668 - 01:28:49.084, Speaker I: So, yeah, I do think the development of communication protocols over time has democratized things further. You know, like, maybe it's the platforms themselves that there's certain points of centralization, but I think more than ever, people have a voice. And I think if. If things like identity and payments and reputation, these kind of protocols were built into the Internet in the early web days, things would have developed quite differently. Web 2.0 probably would have looked a lot different. And I think blockchain is exciting because we have the opportunity to write these new protocols to make them decentralized and open source and kind of go back to the early philosophy of the Internet.
01:28:58.684 - 01:30:40.624, Speaker C: Hi. Great job, guys. I've got a question around engineering philosophy, and this is more relevant to the people doing decentralized protocols. To what degree is this building a rocket ship where you need to do all this upfront testing and build this kind of system that needs to go and kind of go in perpetuity? And if you make a mistake, then you make a mistake versus shipping something smaller, iterating in public, often with financial stakeholders in the mix, and kind of learning live, how do you balance the two? Do you think about one side versus the other in terms of how you're designing your protocols themselves? I'd say it's a major challenge in that if you take a very organic welcome, Ellen, if you take a very organic approach, you're taking a slow approach and leaving yourself vulnerable to being passed by someone who doesn't, someone who doesn't take on those customers and goes heads down and builds the code and just. Just makes the technology happen. And so we are definitely aggressively growing ourselves and willing to go heads down, I think, for a little longer because we would prefer to be those people that do the passing rather than be passed.
01:30:46.284 - 01:31:16.964, Speaker H: So I'm not building a protocol. I'm just evaluating a lot of different protocols to build things on top of. But I think that, you know, maybe not 2018. Maybe 2019 will be the year where we see the first multibillion dollar failure, where something may be built on proof of stake, potentially. Proof of stake does have a lot of failure mechanisms. That proof of work does not have. And so we may see something fail catastrophically.
01:31:16.964 - 01:31:32.534, Speaker H: I think that will be hilarious. Other people, other people probably won't, but, you know, I mean, what's it going to be? Oh, no way. Okay. I do want to keep my job. So.
01:31:34.834 - 01:32:24.324, Speaker E: Yeah, I think that's a really good question. And I mean, ideally, I think what historically has been shown to work well with engineering is if you take kind of an iterative approach and you build an MVP and then put it out where there's users and you iterate on it, I think that's at the rate that that happens in the web, it's hard to do that because even if. If you're talking about the contracts or the stuff that's stored in the blockchain, you can't. Like, the whole point is that no one can change them after they're deployed. So you need to deploy a new version and get people to use that. And if you're talking about the clients, like the off chain code that people run on their computers, this is some kind of a distributed network where people have to, of their own volition, go and upgrade their client. So you can't move that fast.
01:32:24.324 - 01:33:25.318, Speaker E: I think probably the approach that we're taking at Truebit is first trying to build everything in a very modular way. So separating the stuff that deals with incentives and money and that is going to go through a more rigorous process of security analysis and formal verification and these types of things. And the parts of the code that could cause less catastrophic failures and that you can iterate on more quickly will be closer to a normal engineering type of development. And then I think the other point is that in the early life of a project, people kind of have this flexibility. Like, the community has this flexibility of, like, this is a v one, we're going to use it, then we're going to upgrade together to v two. I think Zero X, for instance, is one of the best examples in my mind of doing this well. So you launch the network, it works, but it's not optimal.
01:33:25.318 - 01:33:43.878, Speaker E: And then you continuously communicate with your community around what you're up to. So you do these like, monthly updates and that actually, like, forces you to ship code. And then, you know, once a quarter or like, however much long, when you hit the next milestone, you just roll it out. Yeah. So that's what I think.
01:33:43.966 - 01:33:46.314, Speaker D: Fair enough. Other questions.
01:33:55.974 - 01:35:03.434, Speaker G: Hi, guys. Awesome panel. My question is, how do you, what's your view on the long term sustainability of your protocols, each of them, because once the token economy is in place, this protocol has to be viable on long term? And for example, if thinking about truebit, first of all, how do you make sure, for example, that verifiers and challengers are interested going forward and in the long term to verify and to challenge? I guess if, for example, of the verifiers that keep submitting the right answers, why challengers will be interested in challenging their interest going forward? This is more like short to midterm. And long term is why is it going to be interesting for them overall? Because I guess you will have a native token, right? So why is going to appreciate in value? Or if not, why are they going to still be interested in doing this? And same goes for Solana, for all the other protocols, like how do you ensure, how do you think? I guess you don't have the exact answer now, but how do you think of the long term sustainability of protocols? Because if they are to bring this scalability, they have to survive for five and ten years and longer.
01:35:09.874 - 01:35:51.778, Speaker E: All right, so I can take that first. So you hit on an interesting point in the protocol, which is if the solvers are always telling the truth, why would these verifiers show up and challenge them? And this is actually a core part of how the protocol works, which I didn't get to talk about. So there's multiple variants of this that we're exploring, the one that's kind of written up in the white paper. It works as follows. So the argument is solvers always telling the truth. People, the verifiers, never win any money because they never prove them wrong. The verifier stops showing up, then the solvers can start lying, then the verifier starts showing up, so the system doesn't have a stable equilibrium.
01:35:51.778 - 01:36:40.484, Speaker E: And the way we solve that is that the protocol itself forces the solver to give a wrong answer probabilistically. So, one, you know, let's say the probability is 1%. So one out of 100 times, the solver will actually be forced by the protocol to give the wrong answer. And then the verifier who challenges that solution receives a jackpot payout, which is 100 times larger than their normal payout. So what this means is that if, as a verifier, you know that even if everyone tells the truth, and you just catch these forced errors because it's 100 times larger and it's a 1% probability, your expected profit will be positive. So that's how truebit gets around this problem. Now why we need a token.
01:36:40.484 - 01:37:15.148, Speaker E: So it's one of the reasons ties into this jackpot itself. So the jackpot bounds the security of the system. So in the trivial case, think about the point where the jack. Okay, let me take a step back. So how does this jackpot come to exist? Because it's like paying out to people. And in normal cases, there's rewards going to the solvers. So where does this thing, like, come to existence from? Basically, every time a task giver creates a task, a part of that reward is sliced off the top as tax and put towards this jackpot.
01:37:15.148 - 01:38:06.160, Speaker E: So this thing is like being accumulated over time. And when there's a forced error, the jackpot gets paid out. So in the trivial case, let's imagine, because this is probabilistic, what if there's like, you know, this happens multiple times and the size of the jackpot shrinks to zero? That means that even if verifiers know for a fact that there's a forced error, they won't even bother checking anymore. And the size of the jackpot basically determines how much work verifiers are willing to do. So if we have our own token, one of the things we can do is actually use the jackpot as a, as a minting mechanism and as a time to introduce new tokens into the network. That's one thing. Another thing is we're building truebit as a L2 token or as a L2 system, meaning that we wanted to work on Ethereum, Dfinity, Polkadot, all these different systems.
01:38:06.160 - 01:38:38.744, Speaker E: You have the same network of miners, you deploy the entry point contract to any chain and then people get access to this, to this system. So you need some kind of a, you know, you need a system that's independent from any one blockchain native to the system to pay people out to you to carry governance through all these kinds of things. So that's our kind of answer. But it's, to be honest, like, the token mechanics are still a topic of ongoing research, but there's reasons why it needs to exist.
01:38:42.744 - 01:39:58.234, Speaker C: So I spoke about Eric before and his data science, that he's going to be applied to find exactly these types of solutions, to really look and model these token economies and find where the incentives are aligned. And maybe it's helpful to kind of bring that down a level and just talk about some of the variables at play. I think I mentioned in that avalanche architecture that we have at least double the size between levels. At least is not a known number, right. The bigger the number changes, the finality times that might bring more players into the game. How much to pay people to store a piece of the ledger to go up or down from that, maybe cause some group of people to want to fork the whole blockchain, to be able to put themselves in a situation where they get the value that they want, that's most profitable for them. So I think using these game theory techniques to find these equilibrium points is really important to the stability of the future of the chain.
01:39:58.234 - 01:40:00.660, Speaker C: We're working on it.
01:40:00.692 - 01:40:09.012, Speaker D: Can I offer something else, though? I mean, the question is, why is SMTP still exist? Right? I think, why is email still around?
01:40:09.108 - 01:40:14.204, Speaker C: That's the simple mail protocol. Transport, transport protocol.
01:40:14.244 - 01:40:22.516, Speaker D: There's like all this. And I think the answer is because it has users, right? At the end of the day, things continue to exist because people are getting value for them and they're users.
01:40:22.580 - 01:40:46.124, Speaker H: It's also not rent seeking. So that's the thing. Some of these protocols are out here, and they're not seeking to extract rent just for using the protocol. And basically, to summarize everything that's been said so far, it comes down to economics. The projects that get the economics right to incentivize people properly, are going to do well and they're going to persist. The projects that get the economics wrong are going to fail eventually or be forked out.
01:40:47.784 - 01:40:51.964, Speaker C: That's much better than what I was trying to say. Thank you. Thank you for that.
01:40:54.284 - 01:41:24.844, Speaker D: All right, I just got whispered in the ear as the moderator, and this is a poll. We have built some consensus here. We'll see how long it takes, which is, do you guys have more questions and want to keep going or want to do open networking? No hard feelings either way. So raise your hand if you have questions and want to keep going or want us to keep going. And then first. All right, well, that's a pretty easy answer, which is, there's one more question, and then we're going to do networking. Go for it.
01:41:33.424 - 01:41:42.176, Speaker A: So just to repeat the question, so what's the master. What is the master plan for all economics?
01:41:42.200 - 01:41:46.964, Speaker C: And for us, it was definitely to hire a particle physicist.
01:41:48.384 - 01:42:13.664, Speaker D: I'll offer one thing, and then you can give a serious last answer, which is, have you guys all seen the episode of South park with the underpant gnomes? It's a classic, and I think anyone in this community should watch it, but they have a three part plan. Step one is steal underpants. Step two, step three is profit. So that's the under pandome strategy.
01:42:14.244 - 01:42:36.098, Speaker H: So I think that for all the master plans that have been put forward by different projects, the real master plan at work here is evolution. So all these different projects are out there. There's a lot of money flying around in the space. People are trying a lot of different approaches. Some of those approaches will work really well. Others will fail catastrophically, and people will iterate and will move forward.
01:42:36.236 - 01:43:08.450, Speaker C: And I say we're going to do way better than that. We're not going to just butt heads and see who wins. We're going to do science. And Selena, I, as a long term qualcomm, low level programmer, I don't have quite the appropriate expertise to manage these multivariable systems. To me, eigenvalue is a pretty intimidating word. And so we got a guy that can handle this. You know, there's a couple different solutions.
01:43:08.450 - 01:43:24.490, Speaker C: We can do the differential equations. Maybe we can build a neural network to kind of manage these multivariable systems. There are definitely techniques to manage this complexity, and we're going to try, that's for sure. We're going to try to predict the future and make the one we want.
01:43:24.562 - 01:43:36.174, Speaker D: Well, that sounds great, I would say. Everyone, thank you for a wonderful panel. Thank you for a great audience. Thank you to the Solana guys for hosting. I almost called you by the wrong name.
01:43:37.474 - 01:43:39.434, Speaker A: And thank you all for attending.
