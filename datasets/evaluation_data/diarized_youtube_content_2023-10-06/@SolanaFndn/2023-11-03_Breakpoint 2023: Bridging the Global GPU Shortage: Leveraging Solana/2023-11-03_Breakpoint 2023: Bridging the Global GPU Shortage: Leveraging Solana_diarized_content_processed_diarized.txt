00:00:01.320 - 00:01:01.404, Speaker A: I'm short, and I'm here to talk about a critical challenge, and that is the global shortage of graphical processing units, otherwise known as GPU's. So, as everyone knows, we live in a world where AI is pretty much everywhere. It's used throughout the industry, in every sector, helping with innovation, solving problems, and driving efficiency. But this progress is heavily reliant on an increasingly scarce resource, the GPU's. And this is not just a minor inconvenience, this is a significant roadblock in realizing the full potential of what artificial intelligence can bring. But what if we can turn this around? Today, I'm excited to introduce to you Nosana. We're an innovative project that leverages Solana technology to bridge this gap, and we're building the world's largest decentralized compute grid to do this.
00:01:01.404 - 00:01:52.762, Speaker A: By directly connecting GPU's and AI users, we are really revolutionizing the way we can utilize compute power. So, yeah, first, I'll be going more into this problem, the GPU shortage and its implications, after which I will introduce you to our network. What Nosana is, how it works. What are the benefits of using Nosana? I will also showcase a few screenshots of our platform. We are currently in a private beta, which we are very excited about. And lastly, we will have a look at how the future will look with Nosana. So, yeah, the global demand for artificial computations.
00:01:52.762 - 00:02:37.470, Speaker A: Artificial intelligence computation exceeds the available on demand supply. There are basically not enough GPU's to run artificial intelligence computations at this point in time. So to quote Elon Musk, for example, GPU's are considerably harder to get than drugs at this point in time. Referring to acquiring a bunch of GPU's for one of his ventures. This is an article from the New York Times, a recent one, the desperate hunt for the AI boom's most indispensable price. Also referring to startups trying to get their hands on affordable GPU's and having a really hard time doing so. Nvidia themselves.
00:02:37.470 - 00:03:37.066, Speaker A: Nvidia, as you all will know, is a supplier of GPU chips. They're having a hard time to keep up with the demand. They basically cannot build enough GPU's for the needs of the demand. And then a final tweet referring to the current pricing that you're now paying if you would rent a GPU in the cloud nowadays. So this tweet refers to the h 100 chips, GPU's which are really on the higher end of the GPU's that you can use in the cloud but just to rent this for a month, you would pay $72,000. So just think about how much you would pay for training a model like chat, GPT, or another model that requires a lot of GPU computations to train it. And then if we look at where is all this compute needed for.
00:03:37.066 - 00:04:28.524, Speaker A: Of course, it's not just AI. The GPU's are not used for just AI, but it's a very significant part of the industry. And in this graph, you see over time how much compute is used in nowadays artificial intelligence models. And then on the axis going up, laser's not working. You can see it's growing exponentially. So compute is usually measured in what is called floating point operations per second. And you see not only the amount of models increasing, but also the amount of compute needed for those models to train them over all the categories, ranging from games, language, speech and vision for artificial intelligence.
00:04:28.524 - 00:05:22.004, Speaker A: So, yeah, to summarize the implications of this GPU shortage, then specifically on the AI industry is like, yeah, the innovation is stifled. It goes less quick because of the shortage we have with GPU's. Also the cost increases for GPU's, which makes it very inaccessible for a lot of people that would try to participate in the AI industry. Inequitable access to GPU's. Yeah, of course, large enterprises with more money will have an easier access to GPU's than the smaller people. And overall, there will be a slow down progress in artificial intelligence research. What can we do about this? Very excited to introduce you all to Nosana.
00:05:22.004 - 00:06:15.684, Speaker A: We've built a network, a decentralized network, where we allow miners, gamers, MacBook owners, MacBook owners with a GPU inside. So basically, anybody with a GPU to lease their GPU on the network through the Solana network, and allowing AI users to execute AI computations on their devices directly. And this is, of course, all done in a secure, cost effective, and efficient manner. And before I'm going to show you, or explain to you a little bit how that works, first, let's have a look at what's actually needed to run AI. Right, so, roughly speaking, there's two versions of AI. First, you need to train a model, and after it's trained, once it's ready, you can use it. Well, training is called training, and then the usage of a model is usually referred to as inference.
00:06:15.684 - 00:06:53.892, Speaker A: And especially, the training is most computationally expensive. To give an example, chat GPT four, which is the most recent one used nowadays, did cost $100 million just to train it. And they're predicting that Chet, GPT five will need an approximate of 25,000 a 100 GPU's to train it. So we can only guess how much this is going to cost. But there's really a lot of compute needed for these types of models and algorithms. Inference on the other hand, is a lot cheaper. You do require a GPU for it most often.
00:06:53.892 - 00:08:00.404, Speaker A: Not always, but most often you do. But the problem is that cloud GPU's, so the AWS, Azure, Google Cloud, the GPU's they offer, are not really optimized to run inference, as I will also show you in a minute. But to give an example, stable diffusion, which is an open source model which converts text to images, takes roughly around 15 seconds per image to generate, which is really a long time. So we actually did a research on this matter and compared the consumer version of this gpu. So we took the GTX 3080 from Nvidia and compared it to the equivalent car they have in cloud, which is the t four. And as you can see, so the average cost for 1000 images running this stable diffusion algorithm is like 85% cheaper if you would do it on a consumer card than in the cloud. And it's much faster as well.
00:08:00.404 - 00:08:45.198, Speaker A: So bottom line, consumer devices, retail devices, the ones we buy for at home or to mine for example, are great for inference. Yeah. So here's an example of an RTX card, but also nowadays MacBooks have very powerful GPU chips inside them. But how do we connect these GPU's with users? So yeah, this is a technical diagram of how it works. So in the middle you see the Solana blockchain represented by blocks. And in the bottom you have your AI developers or the users of any given AI algorithm. And what they do is they on chain, they post a job to our network, which we.
00:08:45.198 - 00:09:20.524, Speaker A: Yeah, we have a number of Solana programs making up for the network. It's all open source. And then there's nodes listening to these, or they're. Yeah, they're pulling the blockchain looking for these jobs to execute. And then, yeah, once they finish the run the algorithm, run the inference, they post back the results back to the users and get paid for it in a direct transaction. And that's it really. It's a decentralized way for connecting users with GPU's, and it's an open marketplace.
00:09:20.524 - 00:10:22.764, Speaker A: So yeah, Nosana allows you to earn a passive income by renting out your hardware. If you may have bought expensive GPU for gaming that you do not use all the time, you could opt for, you know, maybe at night or when you're not using it, rent it out on a network, let other people use it to run AI algorithms and get your investment back for your card, for example. And then on the other hand, our platform enables companies, or really anybody, to run AI inference on your devices. So yeah, to summarize all those points, the benefits of using Nosana is that it's up to 85% cheaper than using cloud compute compared to AWS, Google Cloud, or any big cloud provider. It's an open compute marketplace. So prices are fluctuating depending on demand and supply. Of course, it supports many workloads.
00:10:22.764 - 00:10:57.038, Speaker A: Yeah, it's in private beta, we see it's highly performant, it's quick. That's also why we chose to build our programs on Solana, of course. And for each different algorithm, we're building what we call connectors. So these connectors allow you, which I will also show you in our demo in a minute, to seamless interact with the algorithm. So just have the results of AI. And then of course, we also have the web. Three major benefits.
00:10:57.038 - 00:11:33.476, Speaker A: It's transparent, everything's on chain, everybody can see what's going on over there. So let's have a look at our demo. It's a few screenshots that we did also in a workshop on Wednesday, which was a great success. So what we've been working on is what we call a browser node. Oh wait, let's first have a look at this image. Yeah, so this is an example of a stable diffusion result. So the prompt that was given is a photograph of an astronaut riding a horse, and then the algorithm returns exactly this.
00:11:33.476 - 00:12:15.290, Speaker A: That's nice, right? So, yeah, this is what you can do on the network. So this is one of the things we've built. We call it our explorer, which basically gives you a looking glass into each job that has been run on the network. For example, you can go into each job, see what has been computed, for whom, what has been paid, how many nodes are active on the network, and it's live right now. Most of the jobs are currently run on Devnet, but yeah, please have a look. We're very happy with it. And this is the thing we showed in the workshop a few days ago.
00:12:15.290 - 00:13:31.044, Speaker A: So the most conventional way of running a node on our network and share your gpu would be on a containerized environment. But in addition to this, to make it more simple for everybody just to generate some passive income on the side, we've also created what we call the browser node. And with the browser node, it's fairly easy just to open a tab login with your favorite wallet or generate a key pair on the fly, and then it will load the specific AI model that it's trained for in your browser and you can opt for the compute grid that you want to participate in. So like I said, we're still in private beta. So this was just like a small pilot we did in the workshop, but it was a great success because everybody in the audience that we had there plugged in their device and we run whisper, which is another AI model which convert speech to text. So basically you would send it any given text memo and it would return the text of that memo and it works just like that. So I hope I'm not standing in front.
00:13:31.044 - 00:14:21.660, Speaker A: So what will then happen is it will automatically enter a queue where it waits for a job to execute and once it's in, you will process it, post it back to the original user, and afterwards get paid for it. So yeah, I left mine open for a while. It's on Devnet, so it's not actual money, but yeah, I was able to earn some tokens just by doing this. So yeah, that's it. The future with Nosana. So how does it look like? Set out to democratize compute power. We want to make compute as accessible as possible and not let all the big cloud providers be the only providers of it.
00:14:21.660 - 00:14:54.414, Speaker A: Our network is more cost efficient, as I showed, than cloud providers. By doing this, we increase our resilience against the GPU shortage. And it's really nice just to build this global grid like a collaborative network to solve all these problems and accelerate the AI progress. Again. That's it. If you're interested in joining the private beta, please reach out. Our whole team is here.
00:14:54.414 - 00:15:13.452, Speaker A: I will be here also. This is my telegram handle. We're also doing a race currently. We're conducting a seat round, so if you're interested in that, please also reach out. I will be here or on telegram. This QR code will set up a meeting with us. Yeah.
00:15:13.452 - 00:15:17.764, Speaker A: So get in touch. So yeah, thank you for your attention.
