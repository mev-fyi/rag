00:00:01.120 - 00:00:29.614, Speaker A: Hey, everyone, thanks for joining me today. I will be speaking about how we scaled NFT compression all the way through to production. My name is Nicholas Penny. I'm one of the co founders of Helios and also the CEO. And I take the lead on all of our engineering for compression. And do I need the slide? Thank you. That's important.
00:00:29.614 - 00:01:01.344, Speaker A: Perfect. Okay, there we go. Okay, perfect. So, a bit of background on myself. So I have a background in distributed systems, and prior to Helios, I worked at AWS, specifically on RDS, which is relational database services. And essentially, what I want to talk about today is compression. A little prelude to that is you're probably familiar with some of these companies, and the one thing they all have in common is that they use NFT compression.
00:01:01.344 - 00:01:52.380, Speaker A: Of course. And you might be curious about specifically, what is this talk about? And there's been quite a few talks and content about compression in general, but what this talk specifically will be about is about all the work that happens behind the scenes to make compression work at scale. And the journey that we've taken from when it started to all the way until today, where it's now quite stable in this diagram, is quite true. The early days were quite painful, like a lot of things in Solana, and now we're quite in a good spot. So I'm just going to take you through that entire journey from start to finish. So this is the first time that I learned about compression, more or less, was December 2022, when Mert essentially told me that this is going to be big, and he shared this initial white paper. And I'm not going to go deep into compression itself because there's been quite a few talks.
00:01:52.380 - 00:02:37.292, Speaker A: I don't have the time to explain it fully, but this is when I first basically learned about it, and this is the original kind of proposal to explain the algorithms that make it possible to happen on Solana, specifically like operating on the blockchain. And so I'll talk a bit about what compression is just for people who aren't familiar with it. Essentially, it allows people to go and use accounts, but a lot more cheap, specifically for, say, minting nfts. It allows you to mint nfts for a far cheaper rate. The rough sense is about $150 for about a million nfts. So that's significantly cheaper than it usually would be. Why is it cheap? Is because traditional nfts use three accounts.
00:02:37.292 - 00:03:20.314, Speaker A: You have your mint account, your metadata account, and your token account. And with compressed nfts, you have none of these accounts, making it a far bit, a fair bit cheaper. Now, compressed nfts are securely stored on the ledger. So instead of storing it in account space, you're storing it with transactions. And it uses these things called merkle trees to kind of group them together and allow you to verify that it's correct on chain. Now, merkle trees have a unique property that they produce a single cryptographic hash. This hash kind of acts as a digital fingerprint, which lets you take data from, say, an index or service like Helios or Triton, and then you can verify that the data they have is actually correct.
00:03:20.314 - 00:03:57.836, Speaker A: So what happens? You basically have these transactions that hold all your historical state of these nfts, and then you can verify the current state against the merkle root using these things called proofs. This is a rough diagram of what a merkle tree looks like. All the NFT data is hashed. It becomes the leaf nodes, and then everything's recursively hashed to produce that root hash, which is your digital kind of fingerprint. So I did kind of lie. There are accounts involved, but there's only one, and it's the Merkle tree account. This account looks roughly like this.
00:03:57.836 - 00:04:39.424, Speaker A: If you look it up on an explorer, it has a few key properties. The ones I really want to point out here is you have, of course you have your root, which is that fingerprint I talked about. And you also have the number of leaves, which is the current number of nfts that are minted to it. And you also have, which is the one I really want to point out is the sequence number. The sequence number is a counter that increments every single time you do something on the tree. So if you mint the new NFT or you modify the NFT or anything like that, this number always goes up and it never goes down. This allows us to handle things like ordering out of order updates and basically gives us a sense of time as things happen on the tree, which will come into play later in this talk.
00:04:39.424 - 00:05:30.040, Speaker A: The rough flow of how you use this stuff is the first thing you do is you're going to create a merkle tree account. It'll be an empty tree, and from that empty tree, you can then basically start going to mint your nfts with this program called bubblegum. The NFT is hashed and then appended to the tree, and then the properties I showed you before within the the tree account get updated. And ultimately, that mint transaction will always contain all the information you need about the NFT. So now that we've kind of explained a rough background of how compressed nfts work. I'll go back over to kind of our history and around January, and the timelines might be a little off here, but I'm giving just a vague sense of the kind of journey we've taken with this. So the digital asset specification API and system was released by Metaplex.
00:05:30.040 - 00:06:20.010, Speaker A: And this here, this diagram, you don't need to follow it. I will explain stuff a bit later. This is just the original diagram I ever saw when I was first learning about this system. And the specification is essentially a unified view set of APIs that you can essentially get your data about nfts, whether it's compressed or non compressed, kind of all in one place, and that's what it serves. But it also comes with this reference implementation that allows people to index decompress nfts, as well as regular nfts, along with all the necessary API code as well. And that's kind of the starting point of where I first learned roughly how we can go about building these compression indexers and systems. This is roughly what das looks like if you break it down in a really high level.
00:06:20.010 - 00:07:15.126, Speaker A: You run a Solana validator with Geyser, and geyser is basically a plugin system for the validator, lets you stream data out, and it streams the accounts or transactions to a message broker. In our case, we use redis, which then is picked up by the ingesters. So you have a bunch of concurrent, basically servers that are going to be processing the data and then updating all the data into a database. In our case, we use postgres, and then of course you have the API servers, which then exposes, exposes all the data out. So February 2023 is when we really started talking more and working more with customers who wanted to go live with compression. At the time, these two, helium and dialect, were both getting ready to be like the first adopters of compression, especially at scale. Helium, as you probably know, was migrating from their own blockchain to using Solana, and they were going to use compressed nfts to manage their hotspots.
00:07:15.126 - 00:08:12.666, Speaker A: Dialect is a messaging app, and they were going to use compressed nfts to do exchangeable and tradable stickers. So, you know this messaging stickers, essentially, they made all of those compressed nfts, which is only possible with this technology because of the mass scale you're going to be using. And so once these customers want to use it, it really put pressure on us to get ready to run the system at scale on our infrastructure, which we run globally. So this kind of leads us to the main point of my talk, which is kind of just discussing the journey that we took at Helios to kind of basically put all this stuff in production and run it operationally in a way that becomes low latency, reliable, and if customers have any issues, it's how do we quickly identify what are the problems and how do we repair them for them. I'll go into this a bit more specifically to be more clear. First off, there's one really, really important key thing about compression indexing. It's that data can never ever be missed.
00:08:12.666 - 00:09:10.364, Speaker A: Of course, if you're a data provider, you never want to miss data, period. But the consequences are a lot higher with compression indexing. And the reason is that because you have everything grouped in a tree and you need these proofs essentially to be able to work, developers need the proofs to be able to work with the nfts if something goes wrong with the indexing for one NFT, that can have a blast radius of larger parts of the tree, which means that you really, really need to make sure that everything is working smoothly at all times. Otherwise the data won't be as reliable for your customers. Some other issues that can lead to this as well is that validator nodes are not always stable, especially when you're running a custom plugin. The kind of load on the server is a little bit different than it usually would be, and there's a few things you have to keep in mind. So Solana validators have major version upgrades, they have feature gates, and you also have to factor in things like network connectivity or other just problems.
00:09:10.364 - 00:09:40.938, Speaker A: You need some form of redundancy, even against your own potential, like issues with your plugin or anything else you're doing. So what's the solution? The solution here for this part is pretty straightforward. The first thing we did, of course, is just run two geysers. So you're running two nodes and streaming it into the message bus. So you're now running two x traffic. That helps add an extra layer of redundancy. Now, the other thing that we need to do is have a way of reliably backfilling nfts for a certain time period.
00:09:40.938 - 00:10:31.744, Speaker A: So say, for example, you had an issue, say, in network one a couple hours ago, and you need to reprocess data for that 1 hour time range. You need a way to say, I want all the nfts, mutations and stuff that happen between point a and point b. So what we did is we reused one of our existing systems that we call transactionstream that we used to power the Helios webhooks. And the rough architecture is that it's concurrently pulling a large number of validators. We run a pretty big RPC fleet. And so essentially what happens is that this system is pulling all those blocks and then basically breaking them down, filtering out all the relevant transactions, and then pushing it ultimately to the consumer, which in the case of compression, of course, is going to be the message bus. But the same system can work for anything else, really.
00:10:31.744 - 00:11:10.008, Speaker A: And the reason this is really beneficial is it provides extra redundancy against Geyser. So if there's ever any problem with your plugin or say there's a problem with a newer version of Solana, you at least have kind of some redundancy against that. And you're also distributing across the whole RPC fleet, so you have a lot more reliability, because if there's any issue with a data center or anything like that, you're no longer at risk. Of course, the consequence of this setup is it's higher latency than Geyser, because you can run your geyser node in the same data center as where you're running your whole indexing system. So that's very low latency. So you want both of them together. So you run Geyser for the low latency.
00:11:10.008 - 00:11:46.814, Speaker A: And this is essentially your fallback. That can also be used as a job to execute and fill a certain data range. So we've covered a few key things, and it's kind of like, say, now it's about March, and we're kind of starting to go to prod. And people are starting to use this a lot more. So the question is, what could go wrong? Well, you got stuff like this, people saying, hey, I can't transfer this NFT. I have a broken proof, what's going on? And we got quite a few of these. And this is a really tough problem to have because someone's saying, hey, something doesn't work, my proof is wrong.
00:11:46.814 - 00:12:21.714, Speaker A: But the thing to think about is that when you're talking about hashes, they are, by design, irreversible. So there's no easy way to go back and figure out what actually went wrong, what's going on here. You need to kind of look back through the entire history and the mutations and kind of figure out what led to this circumstance. And there's quite a few problems with that. If you want to go look at, say, your database table and figure out, okay, what's wrong. The first, with time I was looking at this, you're presented with this, which is unless you're a robot, it's not very helpful. And so say you want to go and look at this NFT now and you want to find its history.
00:12:21.714 - 00:12:59.234, Speaker A: Like say what happened at point a, b, c, you can't do that either because there's no actual account. So if you use get signatures for address, you can only do that against the Merkle tree account. And the Merkle tree might have a million nfts and consequently might have 5 million transactions. And you're not about to go parse through 5 million transactions to figure out what went wrong. So you're now in a very painful spot where you need to somehow you have angry customers who have broken proofs and you have no easy way of figuring out why the proof is broken, or at what point in time something was missed, or if there's a bug somewhere. Everything's all mixed up. This is quite painful.
00:12:59.234 - 00:13:40.598, Speaker A: So you're left with a few very difficult questions, which I've already answered some of them, but the main thing is how do you know the tree is indexed with valid data? Like how do you know what data is missing? And then if you have that miss, you know where the data is missing, how do you go and repair that? And how do we get the history of a single asset? So the solutions for this is a bit of operational tooling. A few key things. First off, you need a way to check the validity of a tree. You also need to see if updates were missed for a tree. And then lastly you need a way to find and submit all the missing transactions to repair the data. And I'll walk you through roughly what that looks like. So this is just for history.
00:13:40.598 - 00:14:41.704, Speaker A: One of the first scripts I wrote just to just check the tree. You don't need to read all the code, but on a high level, all it's doing is looping through all the leaves in the tree, checking all the proofs and verifying they all make sense. And this is a quick Sandy check, so that if a customer says, hey, my proof is broken, I can run this tool and at least just double check that it is in fact broken and it's not a bug on their end. Now, the main things that were done that allowed us to handle these situations better is, first off, one of the other RPC providers who was working on compression at the time, Triton, they added an audit table. I do want to mention that the whole compression system, all the stuff, was quite a big collaboration across quite a few players in the community, mainly metaplex trident and ourselves, which I think is awesome. So they added this audit table and a tree status tool, which I'll explain in a bit. And then we added a getsignatures for asset method, which lets you get the history of an NFT so we're able to solve that problem.
00:14:41.704 - 00:15:13.196, Speaker A: And then lastly, we kind of built along with our, in our own kind of system, just a way to blend all these things together so that we could run these jobs that can run at scale and repair full trees. So say there's a bunch of gaps, spots wrong in a tree. We can run this job and it'll fill everything up. I'll explain now and this will maybe even make a little more sense. First off, you have the audit table. This is what it looks like. What you'll notice here is unlike the previous table I showed you, it has the transaction and it has the instructions.
00:15:13.196 - 00:15:58.436, Speaker A: And remember how I talked about the sequence number a little while ago? You'll notice here that we're actually seeing the linear history of this tree. So sequence 1234, this is actually the history of what happened for this tree. So the first ten transactions here were all mints. Now the reason this is useful is that say for example, sequence four was missing. Then you know you've missed an update. And because you have sequence five and three and you have those transactions, you now have a boundary between like the before and after. So you now only have to scan the range in between there, find the corresponding transactions, submit it, which now makes it a lot easier to go and repair when you have missing data, the getsignatures for asset method, this is the data that's returned from it.
00:15:58.436 - 00:16:18.776, Speaker A: It basically gives you the history and the transactions of a single NFT. So putting these together, we're able to build these jobs. And due to the fact I only have 20 minutes, I can't go super in depth over all these things. And I will say again at the end, if anyone has any questions, you can always come talk to of me. The after the day talk. I'll be hanging out. But this will give you the full history.
00:16:18.776 - 00:17:06.212, Speaker A: And in case you're wondering, this is also what some explorers, like x ray, use to power the history view of compressed NFT. And so now we're kind of finally at a peaceful place because we have things running, we've identified few bugs, we've collaboratively fixed them with the ecosystem, and we have ways that now when a customer comes to us and says there's a problem, we're able to actually go and fix it and detect where it is quite quickly. And so a few good things happened. In March 2023, dialect launched their stickers successfully, and in April, Helium did their large launch. And that rolled out, to my relief, very, very smoothly. But there's still always some more complications that can happen. As many people here know, you might have heard of drip and drip.
00:17:06.212 - 00:17:36.342, Speaker A: Basically, they found out that you can mint a lot of nfts for cheap. And they said, okay, well, now we can mint millions of nfts, distribute them across a lot of people as a new kind of modern distribution channel. And so this is early days when they airdropped three hundred fifty k. And so everyone loves this because they get a lot of nfts straight delivered to them and it's cheap and easy. But for us, that can cause some problems. What we see instead is we see this, we see that's the steady state traffic. And this is when drip runs.
00:17:36.342 - 00:18:13.298, Speaker A: And they do this about like a few times a week. So it's just quiet, quiet, quiet, and then just drip just destroying us for like 30 minutes. And then it keeps on going back to flat. And this is what can happen sometimes when you have these super high periods of load is things are going fine, and then you have, they're no longer going fine. And this red here is locks. And this isn't from our actual database monitoring. That stuff is no longer, basically, the retention period for our data is expired by now.
00:18:13.298 - 00:19:04.034, Speaker A: But these locks, what I want to talk about is they're not dead locks, but they're called exclusive locks. And what's actually happening is because you have so much write traffic going to the same table, trying to update the same rows, is that they're basically waiting on each other to be able to have the time or access to be able to go right to that row. And part of the reason that's happening is I talked about the redundancy earlier. We're three x processing all our compressed nfts, and we're two x processing all our regular nfts. Now, every single CNFT operation, specifically the mint, does about six up certs. Now, the reason it does six upstarts for one mint is that you need to deal with some out of order indexing. So say, like, say you have transaction one, two and three, and you get three before one.
00:19:04.034 - 00:19:27.862, Speaker A: You need to handle those cases. And it's a bit out of scope. But the point is that we do a lot of operations. And so you have basically 24 operations happening right now for one mint. So if drip is minting 1 million nfts, we have 24 million updates happening to this table. The solution is pretty straightforward. It's basically doing a form of deduplication which comes with a few extra complexities.
00:19:27.862 - 00:20:06.076, Speaker A: But you essentially are adding, and this diagram is a little simplified, but it's basically adding a layer to figure out if those transactions have already been successfully processed. If they have been, then don't reprocess them. And you use something kind of like redis something really low latency. So you can do it super fast because will come in all at the same time. And this helps us basically reduce the load. So on average, we're only processing each thing at least once, which makes things quite a bit simpler. So in summary, what we've been able to do is reduce a lot of the complexity by adding these verification and recovery tools.
00:20:06.076 - 00:20:37.804, Speaker A: And we also added a lot of redundancy by adding extra geyser nodes and this transaction string. And then finally the fix for the problem that comes when you're running so much extra load is the deduplication. So you might be wondering what's next for cnfts and compression in general. First off, there's updatable med data. So right now, every time you mint a compressed NFT, your med data is static, you can't change it. And that's coming soon as well. So if you want to change the name or something, you'll be able to do that.
00:20:37.804 - 00:21:17.974, Speaker A: That. The other thing I really want to talk about that's quite interesting is general indexing solutions. So this doesn't have to do anything with compress nfts, it has to do with compression in general. What I've talked about today, a lot of the problems will impact anyone who wants to go and build some custom compression solution. So say you wanted to do like a domain name service and you want to use compression to reduce your cost. You'd be facing all the same problems of having to cover missing data index, the merkle trees, etcetera. The approach that we need to do here and the kind of roadmap is to basically make that layer a lot more robust and so that these common problems aren't being faced by every developer over and over again.
00:21:17.974 - 00:22:02.610, Speaker A: And actually, on that note, John Wong will be doing a talk, I'm not too sure which day, but he's doing it during this conference about general indexing. And John Wong's also one of the guys foundation behind the compressed NFT project. So I highly recommend you go listen to us talk. And finally, if you do want to learn more, I'd say we have some great blogs that explain the compression stuff in further detail. So if I went too fast over all that, you can learn it there. And we also do have docs covering all of the DAS methods and the different ways that you can interact with compressed nfts and the APIs and different tricks and tips for for things like pagination. And of course, if you have any questions or you want to learn more, I'll be hanging out afterwards and I'll be here all week.
00:22:02.610 - 00:22:12.474, Speaker A: I'm always happy to talk about anything compression nfts or just helios related. Feel free to come interrupt and come talk to me. I'm happy to nerd out. And thank you so much.
