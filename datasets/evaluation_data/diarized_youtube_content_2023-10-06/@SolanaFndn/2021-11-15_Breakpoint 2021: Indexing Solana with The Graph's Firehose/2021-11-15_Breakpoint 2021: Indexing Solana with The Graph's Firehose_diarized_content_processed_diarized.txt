00:00:06.720 - 00:00:19.910, Speaker A: Whoa, great. Hear a little clap there. So. So. Hi, everyone. My name is Alexandre Bourget. The fair, fair assumption there, I will do the demo in the emacs, and that's not true.
00:00:19.910 - 00:00:44.566, Speaker A: So I'm Alexand Bourget, CTO at streaming fast, one of the core devs of the graph. I'm really stoked to be with you guys today. I'm in that shitty place here, but I'm. I'm super happy I can bring that firehose thing to you guys. I think it'll be great. So, first of all, I want to say that people in this room, our team is there. So if you have any questions after the conference, please go to them.
00:00:44.566 - 00:01:07.826, Speaker A: They know the whole thing, and it'll be awesome. Okay, let's start here. The fire hose is what? Okay, this is our team. Thank you. The firehose is a means of data extraction and replication. Let me give you a little context here. We're attacking the problem of blockchain data and indexing.
00:01:07.826 - 00:01:56.852, Speaker A: The goal of the graph is to index the data so that you can have queries and do things with the data that lives on the blockchain that transits through the blockchain. And so firehose is a radical approach to tackling that issue that works across blockchain has been proven to work on many other protocols. This thing runs on 20 other blockchains here. Different implementation, different languages. And there's a few principles I want to announce, to sort of to tell you here, so that you feel it with me, and maybe I can announce here that the firehose, therefore the graph, because this is technology that's going to underpin the graphs. Indexing tech is coming to Solana. So all these Solana folks, all of you guys, will be able to index a lot of things, because there's a lot of data there on Solana with the graph protocol.
00:01:56.852 - 00:02:24.900, Speaker A: Okay, so to give you a little idea, I've got technical folks here in the room. This is dev segment. So our goal is to take blockchain nodes and extract the data out of it in the most efficient way, to store it in a way that is easily retrieved and easily parallelized. And to the tech folks, I'd like to see the room. That'd be awesome. I see myself here. So the data to be easily parallelized and so that it can be consumed at high speeds.
00:02:24.900 - 00:03:14.064, Speaker A: It is a files based and streaming first approach. What does that mean? So we take the nodes, we extract the data, put them in files so that it can be consumed from files files are the delight of data scientists, the delight of anyone working with data, because of the things that scale the most easily. There's no CPU machine overhead and SSD's, these things have been optimized like crazy. So we took that approach to have files on disk, but at the same time to have the fastest streaming system. So you have a node and imagine it's outputting the data of each and every transaction execution. And the goal is to log an output. Imagine printing to standard output, okay, have a node Solana validator.
00:03:14.064 - 00:04:06.040, Speaker A: It's outputting everything to the standard output. Any transaction being executed, any instruction being executed, and extracting thereby any state changes, but all in context of a block, of a transaction of instruction and of its tree, and then outpouring that and then providing a nice and clean interface to the next consumer. Like this allows for subsystem like massively paralyzed operations. And so I'll give you a few technical details there. The whole shebang has been coded using GRPC and protobuf. So we have the deepest representation, that's about richest data around here, the deepest data representation of the different chains and the richest. So you have all for each protocol, even more so than the protocols do themselves in their own node.
00:04:06.040 - 00:04:45.710, Speaker A: Because we don't compromise on, for example, block rounding, like we hate block rounding. Because when you have a block and there's few things that happening in mid block, you would as well be interested in what's happening here. Not necessarily like 75 transactions tweaking the same balance. If you're analyzing what happened here, you want to know the truth at that time. So we'll provide all the rich data so that your stream can be absolutely congruous and not round things. If you're getting the state at the end, you're getting the summation of all the operations in between. Okay, so that's about the richest data.
00:04:45.710 - 00:05:16.536, Speaker A: Now, the firehose is also the only thing on earth that I've observed, that I've seen that handles forks gracefully and in a guaranteed way. It has guaranteed linearity. That means that. Let me share my screen, something very ugly that I just whipped off this morning. It's okay, this thing, right? You have a fork, like you have a block here, another block, and then we have some forked blocks. They get eventually forked. And this is the canonical chain here.
00:05:16.536 - 00:05:56.562, Speaker A: Okay, so what happens is that the stream will send you this block and this block and this block. But then it knows, it learns through the protocol that it needs to be forked out, and it'll send you a cursor for each message. It comes with a cursor. And that cursor, if you bring it back to the connection, it's a GRPC stream. So you get disconnected, bring back the cursor, and it will continue exactly where it left off. So if it was undoing this for you and it disconnected right in the middle here of those two blocks, it will continue doing the undo exactly where it left off, giving you sort of a guaranteed linearity of the stream. Nothing does it like.
00:05:56.562 - 00:06:30.004, Speaker A: No websocket protocol. You connect, sometimes you disconnect while it's reorganizing, whatever, and you'll miss some information. There's just no way the node will tell you that, because there's no distinction between the node and what's happening on the network. Here, the fire hose will cobble from different, for example, Solana validator nodes, and merge all the fork information into what we call 100 blocks files. We'll merge them and we'll have all the views of the perceived forks. So you can always continue from where you left off. And also it allows the systems to be detached from the Solana validator node.
00:06:30.004 - 00:06:58.094, Speaker A: Think of the whole thing as a master slave replication protocol. You have MySQl, it's writing operations, but then there's that bin log replication protocol that sends it to a slave for having increased read queries. And that's similar here. The firehose is sort of a thing to extract and have the least burden on the node. Right? The least burden on the performance. So you're not hitting that node with crazy RPC calls. No.
00:06:58.094 - 00:07:30.244, Speaker A: First you output the data, and you can feed just the right data to perhaps another subsystem, like a graph indexer that's putting data in the right spot. Or another system that would think about, like, a very lightweight system that only tracks the state for one contract, keeps it in memory, like super high speed, and then you can scale that because the firehose replicates. He's scaling the read thing here. You're not putting more burden on the load on the Solana validator. That's doing right. So think of it as replication protocol. I think it's fair thing to say.
00:07:30.244 - 00:07:59.060, Speaker A: Okay, so we have crazy perfs, file based streaming. What else is there to say? I want to say that we've introduced the firehose in the graph ecosystem. So we were joined to the graph ecosystem. And in here, if you search that, introducing the firehose, you'll have a lot of depth. There's a document that is linked in here, it has all the spec, all the principles on which we base ourselves for designing this thing. I encourage you to just search that. Introducing the fire hose.
00:07:59.060 - 00:08:19.504, Speaker A: You'll find it right away. This, doc, is what we are bringing to Solana. Oh, you weren't seeing that, right? That's crazy. I was showing you this here, the fire hose dock. Introducing the fire hose, like search that. It has all these details and the links so you could find the principles and all that. This is the thing we're bringing to Solana.
00:08:19.504 - 00:09:11.022, Speaker A: So right now there is a pr open in Solana land, and our goal is to have that merged so that this principle can arrive and we have it working. The thing can be piped to a graph node like in a week, let's say. And I think that's going to change really the face of what we can do on Solana. So give me now a few details. What are specifics to Solana? Unless obviously there are questions in the audience. I love if someone stand up, would stand up and then would just scream out some, but someone would need to get that audio back to me, otherwise we'll have questions from the folks on stage there. So what's specific to Solana here we have, first of all, full history.
00:09:11.022 - 00:09:39.286, Speaker A: Now, do you know what that means? You know how crazy that is? Here we're talking about full history with the richness of the data. That includes transaction and structured call tree and state change. I'm going to show you the Protobuf model, because you all are geeks. I'm going to show you the depth of the data here. This is the Protobuf model. It's in this repository, proto Solana, tick, tick, tick. And you have that block.
00:09:39.286 - 00:10:09.264, Speaker A: Each thing that streams in our system is a block, and it contains all the things you would expect from a block. Also the transactions here, they're a little bit more enriched. Let's say a transaction will include again, a few of the fields you're familiar with, the id, the first signature, additional signatures, beheader, whatever. But log message and instruction, this is where it changes a little bit. And each instruction. Whoops, whoa, whoa, whoa. It's crazy here.
00:10:09.264 - 00:10:32.088, Speaker A: I'm new to Mac manual, so instruction. Wow, wow, wow. Always. What is that? I'm in edit mode. Okay, forgive that, that's crazy. Message, instruction. So we have here, like all the data that is needed to recreate the call tree.
00:10:32.088 - 00:11:27.702, Speaker A: And when you get that information, you get it in context, you get the block, the transaction, it's in the instructions. You can go from the instruction to the transaction and see what was, I don't know, the first signature there and then instruction and the instructions related to one another. So you can actually model bunch of instructions as something happening, like a serum dex operation is happening. You have all that information to create the tree and these little sweet, sweet things, the balance change and the account change. And you can see here, you'll see the effect of an instruction before and after. You'll get the previous amount of lamp ports and the next amount of lamp ports for a given key. And this too here, you'll get the previous data and the full account data of what's after and remind you, you get that at the instruction level and you could squash out in a transaction level, but most systems right now just rely that as being cobbled up to the block level.
00:11:27.702 - 00:12:07.818, Speaker A: So actually this is the only way. The serum index, using serum dex, we're able to extract all of the orders and fills. It's only by getting the order book, the previous one extracting, because we have domain knowledge of how the thing is laid out in the account. Found it from the list here, because it's sort of a cue, extract what was there and then the other one, the new one, and do a diff and say, oh, this instruction added that specific fill. It's the only way. It's the only way to get something reliable. And once we have you guys, you'll say, this is a lot of data.
00:12:07.818 - 00:12:39.870, Speaker A: It is true, and we will, once we have, we're talking about history. We will go and back process segments of history. That means taking those epochs, two days, crunching the ledger. You know that the ledger is unexecuted transactions. We don't know if it's failed or not. We will go and crunch that and extract that rich data out, put it in files, perhaps once and for all. Once that's done, and you don't have any other information to extract, it's in files, so easy to paralyze, to stream out.
00:12:39.870 - 00:13:11.694, Speaker A: And so we will have that for all the pieces of history so we can take it reduces, crunch it, pipe that into the graph indexer to make sense out of it. Okay, so, tech, I haven't seen your face there, but I'm confident that you're with me here. Okay, so, yes, we'll have full history. Now, the firehose itself is, let me show you the, the protocol. So just the query. That's the query you're sending. It's very simple.
00:13:11.694 - 00:13:48.234, Speaker A: So you have that in proto, in the streaming fast proto here, it's b stream. Eventually it renamed to firehose. That's firehose. B stream is firehose. You do a simple request that has a start block and a start cursor and eventually a stop block and different forking steps. Let's say you want the new only things that pass certain level of irreversibility and you have a few filters in there that you can select to shrink the data a little bit and the response is an agnostic. Any field that will encode any blockchains, things like that.
00:13:48.234 - 00:14:27.106, Speaker A: Same principles apply to all these protocols. They'll get step here. So is it a new, is it an undone, an undo signal or is it you've passed the reversibility plus that cursor and if you give it the start block of one it'll just pipe in your face the whole history. And if you each time you have a message there you get a cursor, you can interrupt the stream and then start again with cursor. It'll continue exactly where it left off, give it a starbuck of minus ten. It'll start ten blocks from head a stop block. You can give it to stop at some point so you can get segments to process things in parallel.
00:14:27.106 - 00:15:15.174, Speaker A: The interface is very simple and the richness comes from the absolutely exhaustive data model that exists for each protocol in particular Solana. And that's what makes it so that you don't need to. There's a nice and beautiful trade off here between the flat aspect of files and a running beast with a lot of SSD's that you have a hard time just rebooting to catch up and all these things. Or it consumes a lot of memory that you will query for it in turn to go and query a key value store somewhere and it's going to work for a certain period of time that's really cumbersome to work with. It's not the thing data scientists like to work with flat given flat files. Okay, stop block, dooba doo doo bada. Do you have any questions there? No.
00:15:15.174 - 00:15:44.918, Speaker A: You'll send them over after, right? Okay, I still have four minutes. There's so much to be said about that I'd like to see. You're cheering up here. I'll let you write, I'll let you read these things later on. Did we have 20 minutes? So until 50. Right, I see you now. How are you loving it right now? Show me the audience.
00:15:44.918 - 00:16:41.624, Speaker A: How are you loving it? Wave. Okay, so maybe one question is when to expect that in the coming weeks this is being worked on actively but the team is in the room. I don't have a fixed announcement date, but if we have a lot of cheer up and a lot of need for that, I think we can converge on effort and making habits sooner than later. I wanted to tell one thing, reduce risk. Okay. Okay. Why don't you take questions from people in the room there? Two minutes.
00:16:41.624 - 00:17:22.743, Speaker A: Yeah, right here. Yeah, you can come up. Cool.
00:17:32.923 - 00:17:33.783, Speaker B: All right.
00:17:34.363 - 00:17:37.102, Speaker A: Yeah, hi, I hear you. Cool. Yeah, go ahead.
00:17:37.238 - 00:18:04.904, Speaker B: Nice. First thing for the talk, it was very technical. I could see well that this must be probably pretty big on performance, like all the streaming. That's fine, but I didn't get the gist of how would I actually use your service as a front end or backend engineer? Are there any interfaces, frameworks to actually interact with it?
00:18:05.244 - 00:18:33.590, Speaker A: Very good question. So I've talked about the firehose because this is the underpinning of what will be served as the graph. So you want to use the graph the moment integration of firehose is done. Then you'll be able to use very standard the graph ways of indexing. There'll be mappings, you'll be able to write that assembly script, and then you'll be able to map those things to entity entities with meaning and fields. So that's standard the graph. I highly encourage you go to thegraph.com
00:18:33.590 - 00:19:03.530, Speaker A: dot, look at the docs, these things for Ethereum right now for near we just shipped. And so that's the sort of experience you'll be able to use to have a decentralized network index your things for you. So that's, that's the value proposition of the graph. Now, the firehose in and of itself can provide value also only for, if only for inspection and things like that. But the streaming aspect of firehose will be there. You could hook up a lot of things to that for a bunch of different purposes. And I've shown you the interface is GRPC interface.
00:19:03.530 - 00:20:00.024, Speaker A: That's how you would connect with a HTTPS socket to some people in the graph ecosystem. We're going to serve this information eventually, who knows? That becomes an IETF spec that all blockchains output data in a similar consumable way, so that we can even index bridges here and there using the firehose output of a chain in a much more reliable way than what we have right now. The firehose, all of that is also high availability ready multiple, let's say solenoid validator nodes that all contribute to one or two firehose endpoints. And that little service is a thing that can plug up from files and stream directly, and you join from files to the real time stream so that you don't see anything happening. You see archives and then real time stream. And from an end user's perspective, is just extremely reliable data from a chain, and that can be all hidden. I don't know if that answers the question, does it?
00:20:00.644 - 00:20:06.704, Speaker B: Direct way to interact now with firehose would be through your graphql API or through the graph, right?
00:20:07.084 - 00:20:34.000, Speaker A: So most direct GRPC and then the graph, we're going to be shipping that part where you'll actually write mappings and you won't query the thing, you'll just have it shipped in your face. And so there's a small wasm vm here. You'll get the data in, it'll execute, and you can store entities to a database. And then you can query it using graphql. You can query it using GraphQL, right. That's the second part. First part for a host is GrPC.
00:20:34.112 - 00:20:36.444, Speaker B: All right. Yeah, that makes it clear for me. Thank you.
00:20:36.824 - 00:21:04.594, Speaker A: Hey, bring on the other questions. I appreciate, it's fun to be able to, even though remote, to interact with you guys. Come on, come on. Who had challenges with Solana indexing? Whatever, raise your hand. No one had challenges. I don't believe you. Yeah, yeah.
00:21:04.594 - 00:21:04.994, Speaker A: You there?
00:21:05.034 - 00:21:05.854, Speaker C: Another question.
00:21:06.154 - 00:21:07.026, Speaker A: Yeah, go ahead.
00:21:07.130 - 00:21:24.984, Speaker C: Hi. So my question is, suppose I want to create a new index and I want to data to be indexed from like three months ago. How much should I wait for that. So like, how fast can you like, process the transactions? Suppose I want to create a new index on all the serum events, for example.
00:21:25.444 - 00:21:51.712, Speaker A: Yes. Okay, so we have come to the graph ecosystem because we were able to make an 800 x performance increase. Why? Because the firehose is highly parallelizable. Because you have a history, it's on disk. You have six months of history. It's on disk. So you can run many processes to take chunk of history and extract them and then cobble that up into one unified view.
00:21:51.712 - 00:22:37.968, Speaker A: So that's what we've done initially, and that's why I think the attention has been brought to us and all that happened. So that can be done also eventually using the graph. We're currently working on parallelism in the graph so that we could bring that sort of crazy throughput. But then after that, just think it, you spin off more machines and then eventually that gets trickled down to the decentralized ecosystem. So maybe if you have, like, I don't know, if you have a petabyte of data. You'll pay a little bit more, maybe for the initial, for additional sync, but at least it'll be possible, right, to get it, to get it out of there, because the technology allows it. Right now, most systems will go linearly, and they'll just go through the information linearly, and that's going to take absurdly amounts of time.
00:22:37.968 - 00:23:06.102, Speaker A: If we don't have a system like that, where we decouple data and we don't have a possibility for parallelism, we're dead in the water. Blockchains are ever growing things. Social database, everyone writes into your database, so you need to be ready for crazy scale. I think the firehose pattern there, applied to different chains that we have experience with, that is the solution. And I've seen other things in the ecosystem, trying to wire in directly into the Solana validator node, writing to databases. I think it's great. I'm happy that this happens.
00:23:06.102 - 00:23:30.014, Speaker A: It's innovation. I've seen that time and time over again that the Solana validator, being directly linked with some other database systems, is inevitably going to be brittle. You want that distinction, you want that clear data centric interface between systems makes things much easier, you decoupling. So maybe I use your question for a tangent, but does that answer the question?
00:23:30.174 - 00:23:41.194, Speaker C: Yes. So, but in order to code, in order for the code to work, like, faster, I should make sure that my code doesn't depend on the linear, that the execution is linear, right?
00:23:41.654 - 00:24:10.644, Speaker A: Yeah. Well, you know, we took a subgraph that was based on the fact that it's not linear, doing sums and whatever, and we made it in stages. So that summation at one stage would be then, in a relative way, for a small segment of history, was then cobbled up in an absolute way at the next stage. So there's ways, you know, slice and dice. If you have files, you can smash slice and dice in different directions to increase performance. So I don't think that would be a necessary compromise.
00:24:10.804 - 00:24:13.544, Speaker C: Okay, thanks. That was real pleasure.
00:24:18.484 - 00:24:24.394, Speaker A: Come on, we got one question over here on the. Go ahead. Go ahead. John.
00:24:25.174 - 00:24:25.870, Speaker C: Hi.
00:24:26.022 - 00:24:58.834, Speaker D: Thanks for the talk. It was great. Have you thought about when Solana is working with other chains? So, like, with metaplex storing arweave data and stuff, you're getting like a snapshot view of what the world was like at that moment in time. But if it's like a mutable NFT and you're querying past events on Solana, but the data's changed on Arweave, at the point of query, are you thinking about how to represent like external dependencies.
00:24:59.854 - 00:25:33.838, Speaker A: Yes. Okay, so the firehose is not in play here. Firehose gives you data of what happened in the richest way. Now the graphs ecosystem is working on that issue because obviously an NFT won't store everything on chain and you will want to have these things synced. So there's a design that's put out right now and it's being worked on. So that rweaves, ipfs and other things can be synced in a deterministic way, which is crazy, but you'll be able to index things that are. So you'll say, receive a signal on chain in an account says, oh, this is the pointer on Arweave.
00:25:33.838 - 00:26:08.044, Speaker A: Well, you'll be able to go and fetch it and bring it into the, bring it into the indexing flow so that certain fields that depend on it would be present still being in a centralized network. So these things are being worked in. But right now you can use IPfs fetchers in the subgraphs that go through Ethereum. But there's more challenges than meet the eye here. But I think you're aware, right, this is actively being worked on and it's going to come up together. But one is needed, the other is needed too. There's no history right now, there's just zero history.
00:26:08.044 - 00:26:40.836, Speaker A: How do you get the history? You pull the node as it's going. So that's one thing. Pulling nodes at a fast pace is crazy. It's absurd. You're going to miss some data. The only way to work that is to pipe it out to make sure you miss nothing and have fixed commit at the end of each block, even though if in the end Solana nodes output only one block out of five, and you have five nodes contributing streaming blocks. And so the firehose can be the one that sort of gets the fuses, the different contribution of data extraction if we're worried about performance and things like that.
00:26:40.836 - 00:27:00.404, Speaker A: So there's many things, when we decide to decouple data from the nodes, there's many, many things that we could do to speed up data systems and filter and reduce and whatever. Okay, what else? So we're about to wrap up. We're running short on time here, I'm not sure. Yeah. So thank you so much for joining us today.
