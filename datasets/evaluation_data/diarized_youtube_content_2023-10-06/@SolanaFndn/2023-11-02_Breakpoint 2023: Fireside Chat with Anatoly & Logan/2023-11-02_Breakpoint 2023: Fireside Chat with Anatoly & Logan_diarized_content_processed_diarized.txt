00:00:01.520 - 00:00:21.114, Speaker A: Awesome. Well, Anatoly, really excited for this chat. We've done a couple podcasts now kind of deep diving Solana's tech stack. But I would love to maybe start this conversation and framing Solana in your vision in the end state, maybe starting there, we can work our way back.
00:00:22.094 - 00:00:54.602, Speaker B: Sure. So, like, when I first had the idea of what we wanted to build, I thought what was really interesting about the space is that it's a smart contract platform. It's an open platform for developers to write new kinds of software that didn't exist before. And two is that it synchronized global information as fast as possible, because that also doesn't exist. You have all these marketplaces around the world. No matter how fast they are, they're very localized. Kind of.
00:00:54.602 - 00:01:35.634, Speaker B: The faster they are, it's kind of actually worse for a hobbyist or any external user because, you know, there's somebody that's paying $10 million, you know, to Nasdaq to get priority access, and you don't have the opportunity to do that even, even if you had the money. So, like, to me, that felt like that's an unfair environment and blockchain, byzantine fault tolerance. It can actually enforce fairness through code, and that's really, really cool. So that's primarily the two constraints in the system. How do we make it as fast as physics allow at synchronizing information? How do we make it an open developer platform?
00:01:36.414 - 00:01:55.900, Speaker A: Amazing. So being we're at the developer stage, we're definitely going to dive into the tech, kind of starting with synchronizing that world's information. Solana does all to all communication, and I think that's really kind of Solana's secret sauce. Can you go a little bit further into that?
00:01:56.092 - 00:03:14.390, Speaker B: Yeah, this was also, like, kind of obvious to me, but not obvious to a lot of our, a lot of folks that we were competing with. So basically, there was kind of like, 2017 cycle of a lot of consensus optimizations. So, like, even if you could magically solve consensus, if you have, like, a limit order on Openbook or Phoenix or whatever, and you want that order to be fairly distributed to everyone that cares about that order, you gotta propagate the data around the world. So even if consensus had zero cost, you're dealing with the application information that needs to be fully propagated. So to me, I thought, like, what does it matter if you optimize consensus if you still have to build this pipeline for propagating, like, all the messages from all the applications to everyone else, and you still have to build a pipeline for receiving messages from all the applications in a high throughput way, as fair as possible. So if you solve those two problems, to me, it seemed trivial to build the consensus messaging layer as just another smart contract, because once you have that all to all communication pipeline solved, like, consensus is trivial to implement. It doesn't matter how many messages it costs to the network, because we made messages cheap.
00:03:14.390 - 00:03:43.766, Speaker B: That's kind of the whole point. If they were going to be expensive, the system was not going to work anyways. We were not going to be able to solve the use case that we really set out to solve. So kind of like some back in the envelope napkin math. This was 2017. We needed four gpu's for a single machine to saturate one, a gigabyte worth of traffic. The cost of that box was like $1,300, like a month.
00:03:43.766 - 00:04:34.234, Speaker B: And even at that price, it was like if you actually priced the transactions fully saturated at that pipe, I think it was something like ten to the minus $6 per transaction. So in my mind, I was like, okay, why optimize? Why even worry about the academic problem of consensus? Let's just build the software in the way that we know, like with systems optimizations, and let it rip. So that was basically kind of the premise. And a lot of the I kind of, it was like night and day. The reactions that I got from Qualcomm engineers when I showed them, like, this is my plan, this is how we're going to do it to, like, academics that were working on optimizing, like, layer ones. So, like, all the Qualcomm folks were like, oh, yeah, this is obvious.
00:04:36.854 - 00:05:00.584, Speaker A: And really the unique property that Alta all allows is kind of that real time market data and being able to propagate that around the world. Can you talk a little bit more about that and how that even kind of contrasts to other ecosystems, how it kind of logically separates the two?
00:05:00.964 - 00:05:45.896, Speaker B: So if you're like a trader, you're receiving information from a market, and it's all these offers and stuff like being streamed to you. Like normally, if the market runs in New York, you want to co locate in New York, so you're physically closest. So you have the shortest link between the marketplace and your machine that makes trading decisions. But the world is big. And actual events that move markets happen anywhere in the world. You can have some newsworthy event happen in Singapore. That newswire has to travel all the way to New York before that machine sees it, and then combines it and synthesizes that with market data, decides that prices are different and should be different than what they are today.
00:05:45.896 - 00:06:29.214, Speaker B: Sends an order, but that information still has to travel all the way to Singapore. So if you kind of, like, think of that problem that way, all you really need to solve is like, how do I synchronize all of the world's information? It doesn't matter where it is, such that a user that has that news event or transaction can submit it somewhere and start propagating as fast as possible. So this is kind of where this design idea came from. Let's build a global network. Let's rotate leaders around the world. Let's try to add multiple leaders at the same time. So then your distance to the leader is shorter, and, like, increase that number until it becomes shorter and shorter to where, you know, your closest physics allow.
00:06:29.214 - 00:07:15.594, Speaker B: And, like, that's. That information synchronization problem is the one we're trying to solve. Because if we can solve it with a global network, it means that by the time a trader sees that newswire and that trader sitting in New York, there's already a transaction moving through Solana that reflects a price movement, because somebody in Singapore sent it to the block producer nearest to wherever they live. And there's no arbitrage. It's not like Nasdaq is any better just because they trade sub one millisecond. So this is like that, like, hyper world of, like, there's one giant state machine, everything's perfectly synchronized, and it's massively scalable to house all of finance.
00:07:17.414 - 00:07:19.382, Speaker A: Beautiful. I mean, and that's.
00:07:19.478 - 00:07:40.214, Speaker B: It's a science fiction thing, right? Like, you gotta, like, I keep saying it, and I feel like everyone just believes it's gonna happen, but it's a lot of work and a lot of really, really hard engineering problems. And in my mind, it's still like science fiction, but, like, we're getting slowly and closer and closer to making it real.
00:07:40.374 - 00:08:10.594, Speaker A: So let's break apart some of the steps that we need to get there. Ultimately, Solana today is doing more transactions than all EVM networks combined, all L2s, even when you remove the vote overhead. That was really put forth by a lot of the innovations of Solana with turbine, sea level, etcetera. But there's new things ultimately coming about on the roadmap, and I would love to dive further into those, maybe perhaps starting with concurrent leaders.
00:08:11.454 - 00:08:52.246, Speaker B: So, yeah, so there's a couple problems that we need to solve. One is, right now, there's a single leader at a time in the network. So the probability of you being close to that leader is pretty low simply because it's a global network and it's randomly picked. Users, I assume, are pretty random, too. So the way that you can kind of solve this problem is you have two concurrent leaders, and there's really nothing magical about that. It's just all they're doing is they receive data, they linearize it, they pick the order of events, and they transmit it out as fast as they can, and they don't actually have to execute anything. They just receive the data, order it, and send it out.
00:08:52.246 - 00:09:31.528, Speaker B: And two things can do that at the same time. And it doesn't matter if they, like, submit a double spend, because the actual order is not the order that the leader makes. It's the order that the validator receives both of those blocks or four of those blocks. And then the validators decide in some deterministic way how to play this all back. So the difficulty here is that the leaders, if they cannot communicate with each other, and because we want the network to be really, really fast, they will not be able to do so. They don't know what the other leader knows about the network. They don't know what fork they're going to pick.
00:09:31.528 - 00:10:08.954, Speaker B: And that means that they can have divergent views of the state, so they can start picking different forks. And if both of them pick different forks, that creates a fork of in and of itself. And that kind of forking explodes out, right. Permutations of what the heaviest fork is kind of grows exponentially. So you have to have a way to kind of clamp that growth down and resolve it. And the way that we're doing that is kind of in a similar way as forking is resolved at the physical layer today. And this is actually like a very lucky, clever optimization.
00:10:08.954 - 00:11:40.584, Speaker B: You rarely get lucky like this, but the way that erasure codes process propagate around the network is when you're receiving this data, it's a very similar high level idea to data availability sampling. You receive a bunch of ch ch ch ch chunks of your block from a bunch of different validators. And if you can reconstruct the block, it's very likely that everyone else can reconstruct the block because you received a very large number of samples of this block from different nodes. So you kind of get this, like, effectively sampling that you're doing in the rest of the network, and you're sampling erasure codes, and that gives you real time guarantees that everyone else is also receiving the data. And they can also reconstruct that same data structure, because it doesn't really matter like, for folks who don't know what erasure coding is, it's kind of this magical polynomial algorithm that folks discovered in the seventies, where you take any chunk of data, doesn't matter what it is, and you like, if you remember from high school, you can draw a polynomial through a bunch of dots, and then you can basically figure out the function of that polynomial just from three points. Well, you take any chunk of data, you draw the polynomial through those points, you can figure out the function of this polynomial, and then you can extend it with more dots. And if you receive any number of these dots right from this polynomial, you can reconstruct the function, and therefore, you can reconstruct the data.
00:11:40.584 - 00:12:23.864, Speaker B: So it's this beautiful mathematical trick, and it allows you to have very high data loss, but recover the entire data set. And because of this trick, when you recover the block, you have guarantee that everyone else can recover the block, and that eliminates the forks at this, like, physical layer. So as long as there's enough erasure codes, you can actually avoid forking in the network. And the only thing that you need to have more erasure codes is more cores, more cpu's. And if there's not enough cores today, there will be twice as many two years from now. So we kind of get this very lucky break. Ironically, it's almost the same thing that occurred with CDMA.
00:12:23.864 - 00:12:34.582, Speaker B: They also use erasure coding and this kind of magic trick. So, in some ways, I'm replaying Qualcomm's journey through an 880, but through later.
00:12:34.638 - 00:12:48.674, Speaker A: Ones, the wireless network background definitely is very applicable to blockchain, ultimately, with the multiple leaders. Can you also chat through how that affects MeV?
00:12:49.574 - 00:13:56.784, Speaker B: So, MEV is this kind of inherent problem to public networks, and you can't really get rid of it at all, but we can make it as competitive as possible. And my feeling is that solutions that try to hide it kind of push the MEV to some other layer, that if that layer is corrupted, can extract 100% of it. And my feeling is that if you really want a totally permissionless network where you assume that there is no layer that has remained, honestly, that you need to maximize that competition, because competition will effectively drive the extraction to the minimum amount. So the way MEV works is that with a single leader, if all the data arrives to that leader, that leader can reorder events. They can insert their own events ahead or after, and those events could be trades. And if you're receiving a bunch of buys and a bunch of sells, I don't know if you've ever had this experience in the nineties with a bank that reordered your withdrawals and deposits to create an overdraft. That's Mev.
00:13:56.784 - 00:14:39.296, Speaker B: So if you were a student in college, in the whatever, late nineties, early aughts, I'm sure you had this happen to your bank account in college, that's effectively meV. The bank is screwing you because they want to make sure you get an overdraft. So this is what malicious blocks block producer can do. But if you have two of them, all of a sudden there's now competition for this. And that means that you as a user can select whichever block producer you know ahead of time is the least or the most honest, or offers the best rebate. And you kind of have this economic competition, which is great. And that competition, because it's a competition within well known entities that are publicly advertised at stake.
00:14:39.296 - 00:15:21.664, Speaker B: They have brands that are trying to attract stake on the network, that actually creates a nice market. And markets tend to do a really good job when they're competitive at creating fairness. So that's really like the foundational goal there is to create as fair of an environment. And in the true implementation, with multiple concurrent leaders, both leaders have full functionality. There's nothing different between them. They have full control of which fork they can pick, which ordering they pick, and that means there's no like distinct. The user can actually pick one or the other, and neither one can affect that choice.
00:15:21.664 - 00:15:58.796, Speaker B: So that really creates a fair environment. There's other solutions that try to do proposer builder separation. In fact, I proposed one as well, like a design that's similar to that. Those all have caveats. If there is like a proposer that is malicious, they can reject builders. And then if they're colluding with a builder, they can reject the honest builder and they have the colluding builder extract all the MeV. And these things are probably unlikely right now in a world where I think most operators in the world on permissionless networks are honest, but we have to kind of plan for the worst case.
00:15:58.796 - 00:16:07.154, Speaker B: So I would like to build a system that is like worst case resilient, but that takes more work.
00:16:08.174 - 00:16:25.394, Speaker A: Multiple blocks or multiple leaders are ultimately just one of the few technologies that are on the roadmap. Another one of those being really a pretty big upgrade to sea level. Can you talk about some of the things that are going into sea level and the upgrades there?
00:16:25.814 - 00:17:29.014, Speaker B: Yeah, the way that the we built the runtime was to maximize parallelism. And we had a small team, we literally took Berkeley packet filter and used that as a virtual machine because it was battle tested at Linux, it's provably high performance and there was like a C compiler for it. So we thought that was good enough, even though I kind of shuddered at the thought of people writing smart contracts on C. So we invested a lot of work to get rust to work and build tooling for that. But you're still dealing with a very limited byte code. So some things that people take for granted as higher level developers like the ability to introspect the code, like actually know what the names of the structures and fields are, that's missing because the code is not typed. The ability to call between programs, like in most modern day virtual machines, you can actually call functions between different environments.
00:17:29.014 - 00:18:04.816, Speaker B: In Solana right now, you have to write these, pack and unpack instructions and actually serialize the data, then deserialize it. And that's a huge pain in the butt. So all these improvements that we know we had to do, we just didn't have the time. And now the team is big enough to where they're working on adding a full type system system to the Berkeley packet filter. What's cool is that we don't have to invent it. Berkeley BPF type format, which was developed by the Linux folks, is something that they're adding to the kernel as well. So we get to again stand on the shoulders of giants.
00:18:04.816 - 00:18:49.492, Speaker B: The work that we're doing is the robust rust integration. Also working on adding move support. So like another high level language that can leverage those types. And once that's ready, you get this really full linkage. So when a transaction is, when the virtual machine for the transaction is created, all the programs can call each other without any memory copies, because we know the exact types of every exported symbol from every program. And during that linkage you can actually verify that the register refers to the right type from of the right account that is referenced by the transaction. So no memory copies and safety and developer experience.
00:18:49.492 - 00:18:58.660, Speaker B: You really rarely get all three like improvements with one change, and we are getting all three of those. And it's really, really awesome.
00:18:58.852 - 00:19:21.680, Speaker A: It is amazing in terms of kind of trade offs. Often there's no free lunch, but one unique upcoming feature to the Solana ecosystem will be asynchronous program execution, where you really have kind of an amazing benefit, really at no cost. Could you talk about that?
00:19:21.792 - 00:19:54.924, Speaker B: Yeah, this is another proposal. Like one of the problems that all the core devs are working towards. And the reason for this is that like right now, the way Solana works is a block producer. When they're proposing the block, they actually execute transactions as they come in and they stream the block out. So they're doing both the work of execution and creating the block and scheduling. Technically, the only thing that they need to do is just scheduling. So they just need to figure out how to pack the block and transmit it out.
00:19:54.924 - 00:20:27.144, Speaker B: They don't need to do any execution. That means that a block producer doesn't need to even have the account state. They can actually just guess which transactions are valid. And guessing is pretty easy to build, to have very high fidelity, like high percentage, like hit rate. And the system should be tolerant with invalid transactions, because that's just dead bytes, and it doesn't really matter if a few bytes are dead or not. So that could make the block producer separate it from the rest of the validator. It could run as a separate process.
00:20:27.144 - 00:21:24.198, Speaker B: It could run on a totally separate network card, like a totally separate interface, which creates a lot of redundancy, because a block producer is what's receiving untrusted traffic from the rest of the Internet that's usually malicious and can spike. And it's much, much easier to build a very highly reliable system if the component of the system that's receiving untrusted traffic can be killed and restarted without any state and moved around the world like arbitrarily. So that's just a huge safety improvement and performance improvement on the receiving side. When you're receiving this data, the only thing that consensus really needs to do is to pick a fork. It actually doesn't really care about state execution, what the value of the transactions are. None of that stuff matters. Did you receive the data? Is the data that you receive the same as everyone else? If so, then you can pick the fork.
00:21:24.198 - 00:22:29.282, Speaker B: And that decision, if we made the validator, only make that decision to pick forks, it means that execution can run asynchronously to fork choice, and can effectively also run on a separate box if you wanted to do that. But most importantly, it can run in the background, and sometimes it could take longer than 400 milliseconds, sometimes it could take shorter. And that allows the rest of the system kind of much more leeway in making sure that blocks are exactly 400 milliseconds or 300 milliseconds. Because the only thing that we're tuning there is data propagation. We're not dealing with like cache misses in the account state or memory contention, all this other stuff that comes with execution, which makes it a lot easier, like a much, much easier programming challenge. So those are like kind of the two things that we want to make asynchronous. Can we make block production separate from running a validator? If so, the system's going to be much safer.
00:22:29.282 - 00:23:10.084, Speaker B: It's much. That node can be much, much faster at picking blocks. It could spend more time doing Cheeto math stuff or building, building a faster scheduler. All that stuff is good. If the blocks are propagated and received and the nodes vote without actually executing them, it means there's never some crazy, miscalculated instruction that takes way more time than the compute units estimated. That makes blocks lower. There's no jitter like, the system can be much, much more finely tuned, and it's much safer to go from 400 milliseconds to 200 and then maybe 120.
00:23:10.084 - 00:23:34.404, Speaker B: So those are really the fundamental challenges that we have in getting to that speed of light, is that we're running on permissionless systems. Anyone can bring whatever box they want. That means there's going to be differences in latencies and all these other things. So we want to eliminate as much of that as we can from the synchronous block propagation changing leader to make that asynchronous and kind of run in the background.
00:23:36.984 - 00:23:43.816, Speaker A: It really is amazing. I hope you appreciate the level of engineering and thought. It is truly remarkable.
00:23:43.920 - 00:24:48.974, Speaker B: So all I really do is I talk about these things, and I write docs and I try to convince Richard from fire dancer and the folks that are fighting fires and building stuff and trying to fix bugs and optimize things to pull their heads up and be like, okay, these are good ideas. How do I like, as I do the work, the day to day work, how do I make sure that we're moving in that direction? And eventually these ideas become the low hanging fruit and, like, incrementally, like, there's a release that all of a sudden turns on bankless block producers or asynchronous execution. And it's not going to be like these big rewrites. Like, this is like the. It's really, like, it's really important that we don't have those because those are super risky, and it's really fantastic that we're in a place where we don't need them, that engineers are able to incrementally get there. It means that we have stability between releases, and releases actually happen on time, and every improvement is clearly better than the previous one.
00:24:49.914 - 00:25:12.516, Speaker A: And just to double click on that, ultimately not having to rewrite the system, throw additional bandwidth at the network, throw additional cores, that really is an addition to the secret sauce. But it's predictable scalability and it doesn't rely on some future ZK magic or other blockchains on top of blockchains on top of blockchains to scale.
00:25:12.580 - 00:26:00.594, Speaker B: And like, validators can do this without our help. This is kind of like, I think the most important part of, I think in my mind of a permissionless, scalable system is that the scalability is like outside of the core developers, we just built the protocol. If there's so much demand that literally Solana is currently saturated, every validator under the sun is going to go order more cores from Amazon and just double the bandwidth because they're making more money. Everyone's happy. That's the best part to be in is when there's so much demand that you should be adding hardware to the network. I'd love for that to happen someday, but hardware gets twice as powerful every two years. It's an exponential curve, and human population does not grow that fast.
00:26:01.734 - 00:26:23.384, Speaker A: So you've painted this beautiful picture, I guess. What, in terms of next steps do we have to get there? Obviously, fire Dancer has been making a lot of progress. We've kind of outlined some of the upcoming network upgrades. But are there anything specifically outside of blood, sweat and tears on the engineering side that we need to do to get to that invention?
00:26:24.364 - 00:26:54.354, Speaker B: There's already, I mean, just yesterday Andrew posted like a blog post on bankless leaders and the roadmap to get there. And there's a couple SIMD's out to relax some of the constraints on what a valid block looks like to get us there. There. So these are incremental changes that are all kind of happening in parallel. I don't know if there's like, there's no like, big thing besides those folks, you know, like understanding the design and the goal we're trying to get and kind of slowly moving there.
00:26:54.774 - 00:27:04.866, Speaker A: And can you maybe just briefly touch upon in your mind how big of a leap fire dancer is from today to the original Solana Labs client?
00:27:04.990 - 00:27:52.954, Speaker B: So what's awesome is that I don't think there's any core protocol changes that the fire dancer folks are doing at all. So somehow we got most of the design right, like during those two years that we were building, which is pretty cool. It's a cool feeling. And they're rewriting this from the ground up. They kind of see the, you know, how the system's built. They can look at our code and be like, okay, why did you guys do that? But understand the goals we're trying to have and rewrite it in their beautiful hardware scaling tiling system that they've built with shared memory that just magically scales across as many cores as you have. They get to do this in c, which is super high performance.
00:27:52.954 - 00:29:05.098, Speaker B: They never pay a language penalty, they never use a library that has hidden allocations or anything like that. If I had infinite resources, I literally did start building Solana and C the first week, and I realized that there's no way I would finish, no matter how many engineers we hired. So we switched to rust just to be able to finish it. But now I get to see that vision come to fruition, and I think engineers that are world class, and it's awesome. The performance that they're really demonstrating is going to be, I think, groundbreaking in the sense that it's going to reduce the cost of the hardware. For the current loads, you should be able to run on a system that's much, much lower requirements than what Solana runs on today. So when folks talk about, oh, Solana is expensive at $350 a month, I don't know if it'll run in like, a digital ocean droplet, but, like, close to that, maybe, like, if you can get a Solana validator for sub $50, the whole argument that it's too expensive to validate just goes away.
00:29:05.098 - 00:30:08.752, Speaker B: So that's really exciting, just from the optimizations they're doing to the software. The other side of it is that if they can really demonstrate that all you need is scores, and they've really solved all those problems to, like, the practical limit, like 20 gigabits, I honestly think we're basically done. There's no more real protocol development that needs to happen outside of improving the developer experience. And hopefully a lot of that stuff is now happening in the compiler application layer where you don't need to make any core changes. And that's, I hate to say it's ossified protocol, but, like, that's, it's ossified because it's done, not because we're, like, scared to make changes. That's a really, really awesome place to be. Hopefully that means that, like, the third client is much easier to write and the fourth client, like, I joke about this, but, like, if you have three clients in two years, I should be able to tell chat GPT five, go write me a fourth client.
00:30:08.752 - 00:30:11.564, Speaker B: Here's three examples of the protocol, and we're done.
00:30:13.074 - 00:30:20.014, Speaker A: Awesome. Well, that is all the time that we have. Anatoly and Slana can sense us at the speed of light.
00:30:20.874 - 00:30:24.314, Speaker B: Thank you all. Thank you.
