00:14:22.715 - 00:14:36.589, Speaker A: Hello everyone. Nice to meet you. I'm Anthony. I'm going to be your MC for the day. It's extremely bright here and they've not prepared me properly. Forgive my squinting as I look into this fantastic audience. I work for something called Club.
00:14:36.589 - 00:14:57.675, Speaker A: We're an education community. We train developers. Some of you might have gone through our programs over the years. We run a very good Solana Rust bootcamp. I encourage you all to sign up. That is my shilling done. We have a packed schedule of talks today, technical talks, workshops, product keynotes, and we're going to keep it fast and furious throughout.
00:14:57.675 - 00:15:13.139, Speaker A: Now I'm supposed to do a five minute intro but I don't know what to talk about for five minutes. I'm going to save you from British jokes. I am British. My accent is rather thick. So forgive, please forgive me in advance. We're going to get started with our first talk. That is from Ariel Seidman.
00:15:13.139 - 00:15:29.885, Speaker A: He's going to talk about Hive Mad, where it's going to be a technical talk for about 15 minutes or so. Well done for being the first ones in. I'm sure there's some heavy heads. Of course, not at all. You're stone cold. So ready to learn some fantastic things. So please join me in giving a round of applause to Ariel.
00:15:29.885 - 00:15:54.823, Speaker A: Good morning. Good morning. My name is Ariel. I'm the CEO and co founder of hivemapper. I've been an entrepreneur in San Francisco, Silicon Valley area Now for about 10, 15 years, building out a series of mapping technologies. My latest is hivemapper. So let me.
00:15:54.823 - 00:17:22.785, Speaker A: This talk is going to be a little bit different. I'm not just going to talk about hivemapper. Obviously I'll share a little bit about hivemapper, but I actually want to talk about the seam associated with how do these types of products get funded in this kind of environment? Alright, I will start by just kind of explaining what hivemapper is at a super high level. Really what we're doing is building this decentralized global map, right? And so think how do we build a fresher, more detailed version of something like Google Maps that can be used by autonomous vehicles, that can be used by logistic companies, that can ultimately be used by all of us in a consumer navigation experience. And so the way that we do this is that we built this purpose device that we call the High Mapper Dash cam or the next one is going to become the B and people are just driving around with this device, right? We're not asking people to go and drive in specific locations we're just saying deploy this, put this on your car, mount it on your car, and just drive like you normally do. And associated with that, based upon your driving patterns and your driving habits, you're helping build this map, right? So in real time, this map is getting created as you're driving around, which is really fun and really cool. And you're being rewarded for that, right? So all of these people all over the world are being rewarded for just driving and doing something that they already do with this purpose built device called the hivemapper Dashcam.
00:17:22.785 - 00:17:49.910, Speaker A: So let's see how we've been doing. So we launched about two years ago. It was total mayhem. FGX was blowing up literally the day that we launched. But fast forward two years, we have now mapped 10 million unique road kilometers. Sorry, 10 million unique miles. Okay? If you compare that to Google Street View, that took them 10 years.
00:17:49.910 - 00:19:13.367, Speaker A: So we are growing 5x faster than Google Street View. Now behind this is the incentive mechanism that we have, which is rewarding people with this honey, which is our cryptocurrency, and we're rewarding people relative to what they're contributing, which is a very, very, very powerful mechanism. Okay? So we've accomplished all this and we've accomplished it at a moment in time, in a moment in period where effectively there was like two moments, right? There was before interest rates, interest rates were effectively zero percent. And then after interest rates start to get hiked up to about 5%, right? And the world changed dramatically for all startups, including hivemapper. We had to be incredibly efficient, right? You can't just go out and raise $100 million anymore or $200 billion like you could in 2020 or 2021. And so what this means is that there's effectively been kind of two different kinds of startups, right? Those that have been incredibly efficient with their dollars and those that maybe weren't always efficient. Here's some basic stats in terms of VC funding, right? If you just look at VC funding over the last three or four years, it's dropped off a cliff not just in terms of total dollar size, so sorry, total dollars invested, but also the number of deals that we're just getting done on a monthly or weekly basis here.
00:19:13.367 - 00:19:47.769, Speaker A: You kind of see like most of the dollars now that are happening in the VC world are basically flowing into large language models, right? These kind of the open AIs, the anthropics of the world, that kind of stuff. You know, you could argue over here. So this is the number of unicorns that were born by year. So a unicorn is a. Is a startup that's worth a billion dollars or more. And so 2021 was probably a little bit extreme, if we're being honest with ourselves here. But you see, now we're back to 2017, and so we've come down this really, really steep cliff.
00:19:47.769 - 00:20:57.811, Speaker A: And what that means is that for a lot of these companies and a lot of these startups, especially those that operate in the physical world, like hivemapper does, the number of dollars available has just dried up. So if you guys want to take you back to 2010 timeframe, right, 2010 time frame, avocado toast were plentiful. You could take an O Uber to your Airbnb and then order dinner on DoorDash for, like, $50. And that was all subsidized by venture capital dollars, right? When zero. When interest rates were effectively 0%, VCs were encouraged to take riskier and riskier bets, and they were willing to wait longer and longer for the return on that investment. And so a lot of these companies that you see here, that of the world, the Airbnbs, they all got funded in that time frame. Look, if you take Uber, if you take Airbnb, and you take DoorDash, collectively, they burn through $30 billion of cash before they turn profitable, right? That's just like a massive amount of money.
00:20:57.811 - 00:21:41.305, Speaker A: Now, look, these things are really important products. Really. I mean, all of you probably use these products on a daily, if not weekly basis, right? So the investment ultimately paid off, and it's a very powerful set of products that came out of this time frame. Okay? The reason all of this, and kind of just go back to the term zurb that you guys may have heard about, is the zero interest rate phenomenon. All of those companies were born out of that era, in that moment in time, in that moment in history that made capital, especially risk capital, accessible to these types of companies that were operating in the physical world. The physical world is expensive. The physical world is messy, and it does take capital to grow.
00:21:41.305 - 00:22:28.847, Speaker A: If you guys see this video, you guys should check it out. It's pretty funny. All right, I'm going to hone in on Tesla here for a second. So Tesla started in 2004, it went public in 2009, and then they were. By 2019, they had already burned through $6 billion of cash, right? By 2024, they had kind of turned the tide. Why did they turn the tide? They turned the tide because they started to scale up Model 3, they start to scale up Model Y, and So they start to generate plenty cash. But look at that, from 2004 to like 20, 20, 21, they were effectively in the red.
00:22:28.847 - 00:23:18.251, Speaker A: That's a long, long period of time. And so look, all of these companies, whether it be Tesla, whether it be Uber, whether it be Airbnb, as a society, we are better off with these products and services. And so the question really is, how do we ensure that the next Ubers, the next Teslas, right, the next Airbnbs, that they get created, but they get created in a different kind of environment, and that we have a new financial model to support those. So Deepin is actually a new option for capital formation, right? So this is actually a new way. It's a little bit of a joke here. That's not actually what it stands for, but I think it is a little bit accurate. All right, so Deepin is decentralized physical infrastructure networks.
00:23:18.251 - 00:23:50.911, Speaker A: When HiveMapper got started, this truth did not exist. The term that existed was actually proof of physical work. I kind of prefer that, to be honest with you, because I think it better describes what we're doing. But here we are, we're going to live with Deep in. It's a fun little term. And what it effectively means is that the collective of people, right, in the case of Hyper, all these people all over the world help coming together and building this map together and sharing the economics together. And Helium is another great example of this.
00:23:50.911 - 00:24:50.993, Speaker A: You deploy all of these different wireless networks, all these different wireless antennas all over the world. So rather than the AT and T's and the T mobiles and so forth, going out and finding real estate, leasing that real estate, they're effectively monetizing that real estate. Your own real estate, I should say. And so it's been a very, very, very powerful kind of new trend. But when we started, it was not sexy at all, right? I think there was effectively helium, multicoin Union Square inventors, Like, there was like four of us, right, that were kind of into this idea. But look, if you go back further in time, especially in the agriculture world, this idea of how do you pool together products and services, right, from multiple constituents towards a common goal. So, for example, Land OLEX is a co, op, right? All these farmers get together, pool their products so that they can reach a scale where it gets interesting to a large commodity buyer.
00:24:50.993 - 00:25:34.087, Speaker A: So that's effectively what these Deacons are doing now. They're doing it with the advent of crypto and the blockchain. And there's a couple of really, really key capabilities. One is this Transparent ledger of rewards and revenues. Right? So whether you're driving, in our case, five kilometers a day, or somebody else is driving 100 kilometers, another person is driving 1,000 kilometers per day, all of that is visible, obviously, anonymously, but all of that is visible on the blockchain. And how much rewards that person got for driving 1,000km in this region or 550km in that region, that's all visible. And once the tokens are awarded to you, that's yours.
00:25:34.087 - 00:26:15.307, Speaker A: And you would think this is a small idea, but it's actually a very, very, very powerful idea, right? Because then you can't. It's not like people are pointing fingers at Hiveabra. It's all out there. And either you can choose to participate, it makes sense for you, or you choose not to participate because it doesn't make sense for you. The other mechanism here is obviously the incentives, right? The crypto token in our case, honey, which we use to balance supply and demand. The problem with maps is, like, if everyone is mapping in Los Angeles and you have 10,000 people mapping in Los Angeles, you can have a really great map of Los Angeles, but that's it. And so the number of buyers you have for that product is pretty small.
00:26:15.307 - 00:27:07.065, Speaker A: So the question really is, how do you ensure that you have enough drivers in LA and San Francisco and Seoul here in Singapore, and you make sure that you're balancing the supply so effectively the map duration meets the demand that you're seeing from customers on the other side. So let's talk about demand. The big knock on deep in the last couple of years has been, yeah, yeah, yeah, you can grow supply, but there's no demand. Well, let's look at this. Here's Helium. Helium now has over 100,000 mobile subscribers on the consumer side, and they're offloading, or I should say large mobile carriers are now offloading their data onto Helium Mobile. So this idea that, like, demand doesn't exist in the wireless world is just utter nonsense, quite frankly.
00:27:07.065 - 00:27:39.495, Speaker A: In the case of hivemapper, we are now serving three of the ten largest global mapmakers. And there's more on the way. And then there will be automotive companies and autonomous vehicle companies and logistic companies and FedExes of the world and so forth. So this idea that there isn't demand for these products is just not true anymore. Right? Just the same way that we kind of looked at the Ubers of the world, they said, ah, they'll never make money. We said the same thing about Facebook ads. It's cute, it's fun, but they'll never make money.
00:27:39.495 - 00:28:22.817, Speaker A: Well, these things take time to mature. So the other big knock on deep in is, well, it doesn't really use the entire crypto technology stack. Let's take a step back. Let's look at something like Uber. Uber was certainly enabled by sparse balance, but did it use every single technology, every single feature within smartphone platform? No. Why would it? It used the technologies and the features that were important for its specific use case that would enable it to grow faster, that would ensure they could offer a better product at a lower price than the taxi service. Right.
00:28:22.817 - 00:29:18.997, Speaker A: That's what it was trying to do. It was effectively disrupting the taxi business through, yes, technology, but more importantly through business model. And I think that is the lesson that we have to take with DPINs, which is we are disrupting existing players not so much through technology, but through business models. And I actually think that, like, although we like to talk about technology and so on, it's interesting and so forth, that is much, much, much harder for Encompass to respond to. Like, Google or somebody else can very quickly throw a bunch of engineers at a problem and respond to a technology. It is much harder for them to respond to a fundamental change in their business model. And so look this moment in time, I think we're going to look back maybe 10 years from now and say this is where we were at.
00:29:18.997 - 00:30:13.573, Speaker A: Like, if you go back to look at Uber in 2010 or 2012, if you go fast forward to like let's say 10 years from now, we're roughly at that time frame. So we're still very, very early. And these products, these networks, they take time to fully mature, but once they mature, they start to generate tremendous amounts of cash and tremendous amount of networks. Because you're dealing with a physical world which is incredibly complicated, incredibly messy. But once you install, whether it be helium in the antennas or in our case the dash cams and a lot of the other tools, it's very, very, very hard to rip that out. So I hope to see many, many, many more products and projects launching on Solana. It's exciting to see the next generation of products coming in and I think that we should all be a little bit patient to in terms of letting these things play out.
00:30:13.573 - 00:30:34.065, Speaker A: Right. If we're all very quick to jump down Uber's throat or Airbnb's throat or Facebook's throat and say, you don't have this, you don't have this, and they're only two or three years into their maturation, I think we're doing ourselves a disservice. And I think we're doing ourselves. We're keeping it very, very shortsighted. So thank you for your time. Appreciate it. Have a good day.
00:30:34.065 - 00:31:08.305, Speaker A: Fantastic. What a fantastic talk there from Ariel. Making maps section. I never knew they could possibly be right. This is my favorite bit because I get to be the voice of God, which is an untold power I will yield. Right now, our next talk is a product keynote about Marinade V2amarketplace for Solana staking. This is by Michael Repetny, so please welcome him onto the stage.
00:31:08.305 - 00:31:42.363, Speaker A: Price. Good morning. Good morning. I'm Michael. I'm from Marinade, and today I'm extremely excited to announce Marinade V2, which is the new and improved version. So we got a long way going. Marinate has been around since 2021 when it launched the first liquid staking solution on Solana, completely bootstrapped, with no VC money.
00:31:42.363 - 00:32:23.147, Speaker A: And today we're taking those three years of learnings and we're baking it in this new Marinade to shake up the staking industry once again. But why Marinade? Me too. To solve problems. So first, the number one reason why people don't want to use liquid staking solutions like Amsom is still the smart contract risk. They don't want to transfer the SALT to a smart contract and worry about potential risk. And that's why if you combine all LSTs together, you get about 6, 7% adoption compared to total state market capital. Next, valators.
00:32:23.147 - 00:32:58.611, Speaker A: If you're staking to a valator, they go offline or change commission. This is going to affect your staking rewards. In other words, your staking rewards as of now are not protected. This big exchange validator went offline for five hours in that one epoch. For those five hours, their stickers lost more than 700 SOLs for that time. And finally, hidden commissions. So validators would earn from multiple revenue streams ranging from MEP tips, inflation block rewards.
00:32:58.611 - 00:33:47.713, Speaker A: So it's very hard to tell. And quite often they make more rewards than stakers realized, which is why some big stock holders would end up running their own private nodes to capture it all. Or you have hundreds of the ledgers running at 0% commission because it makes sense for them, it's still profitable. There are other revenue streams. So combining it all together, how does V2 solve that? So starting with smart contract risk, Marina V2 introduces a new product called Marinade Native, which is completely non custodial. Here's how it works. So unlike liquid sticking, you don't get to set your soul anywhere with Marinade native, you keep control of your soul and you only delegate the stake authority to the Marinade's delegation strategy.
00:33:47.713 - 00:34:23.954, Speaker A: So this way you don't need to forego, you don't need to send your so anywhere, you keep it with you and only using the delegation strategy. Next, validators. So let's touch on that. Protected Staking rewards is this new cool thing that we are introducing. You can think of it like an insurance for your staking rewards. So if you're staking to a expecting 8% yield goes offline for five hours, that yield just drops to 7.2%. So with marinates via star system, any Solana ballator to get staked from Marinade, they need to set up a bond also.
00:34:23.954 - 00:35:29.073, Speaker A: So if they go offline or they change, commission this bond that covers any potential loss of the rewards, making sure that your rewards are 100% protected, meaning 100% uptime. And finally you would be thinking that you get the best staking rate by delegating to 0% commission villager or even running your own. And you'd be right. Until today. Today we're happy to announce the final missing piece of Marinit V2, which is the stake auction marketplace. The way it works is that Merid puts all the delegated SOL up for an auction to all solar validators and the validators bid each other using all revenue streams, MEV tips, block rewards, inflation rewards, you name it, to get the best yield possible on the market out there. This allows stakers to tap into the highest rewards possible without them needing to go to stake with 0% commission balloters or run their own node.
00:35:29.073 - 00:36:07.379, Speaker A: So once again there is no second best right now. Super excited about that. And finally, thanks. Finally I'm happy to announce that Marinade is entering a strategic partnership with Zodiac and Copper. So as of now, their customers can be using Marinade V2, this new version right in their custody platforms. Meaning that it's very easy and even more secure for their users to use Marinade V2. So, welcome to Marinade V2, right? The non custodial, protected and highest rewards way to stake your swan.
00:36:07.379 - 00:36:43.309, Speaker A: Thank you for your time. Right, well done, Michael. Fantastic talk there. This room has filled up. Remarkably, you all have woken up, had coffee and breakfast and are ready to go. I've been told to fill a bus stop for about a minute or so, so please forgive me. Our next talk will be a product keynote.
00:36:43.309 - 00:37:17.225, Speaker A: My obligates to our Margarita. Perhaps that name will trigger a few of you after last night. This will be with Daniel Killenberger and stop running and indeed. So please join me in welcoming them. Thank you very much. Hello, Breakpoint. Great to be here.
00:37:17.225 - 00:38:21.309, Speaker A: We're Daniel and Nadine from Obligate and we present you today in Margarita Finance. We at Obligate, we issued structured products for quite a long time on chain, and we decided to bring it to a next level with Margarita Finance while leveraging Obligate's legal and tech capabilities. But before we jump into the presentation, let me ask you a question. Are you bored and tired from choosing between low staking yields or high risk meme coins? If so, I have great news. With Margarita Finance, you can choose your own risk return profile for your own investment and mix your own yen booster directly on Solana. So why we are doing this? We think Defi needs to grow up. If we look at the ChatFi world structure, products have a volume of over US$7 trillion as of today.
00:38:21.309 - 00:39:07.245, Speaker A: That's a lot. But these products are very complex, not accessible for the crypto community and to these products running on outdated rails. So that means the settlement cycles are very long and the fees are very high. If we look at Defi, we have here the staking yields, which are not very exciting. Or we have really high risk, high return products, but also very intransparent. So we decided with Margarita Finance, we take the best from both worlds and combine it. So you can choose the quote and trade it directly.
00:39:07.245 - 00:39:36.785, Speaker A: Your own investment contract on chain with us. It's your choice. So how will you do that? First you choose your underlying cryptocurrency. This could be Sol Bitcoin. Or then you decide how spicy your investment should be. So choose your apy. And also what's the lock up period of your investment? You want to have it locked up for one week, one month or three months.
00:39:36.785 - 00:40:24.525, Speaker A: And then also you decide how much you want to chip in. Totally your call. Everything is executed directly on Solana and comes into your wallet. But you know what, enough about the theory. I would say let's jump into the tool, into Margarita Finance and our CTO Daniel will show you one trade live here on stage. All right, so we're going to choose a new trade here and I'm Margarita Finance, I'm. So I'll be investing 10,000 margarita bucks and I'm choosing, with my very high risk appetite, quite a bit of yield that I'm ready to accept.
00:40:24.525 - 00:40:49.975, Speaker A: And today I'm going to go with slightly above 72%. That's this per annum. And I'll be Paid back in a week. So I'll earn like 1.5% over that week in yield. I'm going to be requesting the quote. And in the back end, our trading partner, STS Digital, is going to price up the new yield booster.
00:40:49.975 - 00:41:59.985, Speaker A: And I can tell now for 72% yield per annum, I'm going to be having a buyer at 115 price for SOL, which just means that if SOL stays above $115, I'll get the 10,000 plus plus yield and if it falls below the barrier, I'll just get sold at today's price for the 10k that I put in. But I still get the yield anyway. So it's kind of like dollar cost averaging with last week's price. So I'm going to buy this now. And in the back end, we're now setting up the escrow contract. We're setting up the structured financial product which is a bilateral agreement between you, the investor and the issuer, STS Digital. And we're setting up also a payoff smart contract which at the end of lujurity will, based on Oracle price, determine if the barrier has been broken and determine what the the payout will be.
00:41:59.985 - 00:42:18.005, Speaker A: So at maturity based on these parameters, you'll just be paid back one of those two amounts based on what the pricing contract says. And this was a trade already done. I have this in my wallet and I'll be paid back next week. And we have one more slide if we can.
00:42:20.785 - 00:42:21.201, Speaker B: Yeah.
00:42:21.233 - 00:42:53.085, Speaker A: So last but not least, you have to go next. We need to slide again. Ah, here we go. Here you have the QR code. If you want to join our Margarita Finance community, feel free to scan the code and be part of us. So thank you for listening. Fantastic.
00:42:53.085 - 00:43:18.751, Speaker A: We keep it going. We've got more keynotes and rapid talks coming right now. So next up is a product keynote about governance on Solana. This is by Dean Papas from Realms Today. Please join me in welcoming him. Hello. So we're talking about governance.
00:43:18.751 - 00:43:54.825, Speaker A: The governance ecosystem on Solana. Governance has three main components. Measuring sentiment, how we come to decisions together as an organization, stakeholder engagement. Why do I even care to be an active voter? And treasury management. How do we actually do things with these assets? On most chains you've got one option in each of those quadrants and they kind of clunky speak to each other. They're not all on chain. It works on Salon because of composability.
00:43:54.825 - 00:44:35.705, Speaker A: Realms touches every single one of these protocols and we have multiple options across them. One of the ones that I'm a very big fan of outside of realms. Obviously is metadao on the sentiment side and I am a huge fan of symmetry on the treasury management side. And of course, Jupiter and Bong have done wonders in voter participation. Right now, Solana Governance is breaking records. We have the most voter participation across all daos, across all ecosystems. We have two of the biggest DAOs by TVL and with greed we had the most proposals of any dao ever in history.
00:44:35.705 - 00:45:20.281, Speaker A: Yeah, right now there are only five voting types I have on this slide, but trust me, there are many more. As long as you have a token or an NFT or some sort of on chain marker, you can create any kind of voting system you can think of. Most of this is done on 97% of DAOs on Solana are on realms. $1.5 billion worth of TVL resides in realms treasuries. Here you can make a dao within one minute and you can set up proposals that will execute a transaction the moment the vote is completed. You can see that this is all done because of composability.
00:45:20.281 - 00:45:51.943, Speaker A: Look how many different integrations there are. They're not all there, there's many more. So how does this all work? You can create multiple wallets with different rules, different roles, different permissions for each wallet. All of it is transparent. You can see every single voter, what they're doing, what they've voted on, how they feel about their different proposals, and you can even hide it if you want to. Readable instructions. This is very important.
00:45:51.943 - 00:46:37.259, Speaker A: You need to know what you're voting on. We have an ever growing library translating what the instructions actually do. And you can have warnings on them so that way voters know that if they vote on this one, it's going to do something critical to the dao. Of course, there's certain governance features that are only possible on Solana. One of my favorite links. You can vote directly from Twitter, you can set up vesting grants and if someone isn't doing their job or hitting their milestones, the dao can vote to claw it back. This is one of the pet peeves I have and I love this feature set.
00:46:37.259 - 00:47:03.975, Speaker A: If you are in the dao, please for the love of God, stop doing swaps through multisigs. It's not decentralized and it's clunky. We have two ways to go into the future with Realms. One, the Mango delegate account. You set up a treasurer, they can trade whenever they want and they can only withdraw back to the dao. Say you want to get even more decentralized get rid of a treasurer. The DAO votes on portfolio management through symmetry.
00:47:03.975 - 00:47:31.119, Speaker A: It automatically rebalances the portfolio. It automatically puts the assets defi yield. This is the most decentralized way for a DAO to manage its treasury. Of course, the coolest experiment in governance is Futurkey. That's also only on Solana. Metadap and Helium and Jupiter have shown that. You know what, if we don't have the tools, we can easily build them ourselves.
00:47:31.119 - 00:48:03.845, Speaker A: With their own custom built governance of Mango, an exchange has a really cool way of doing a token listing. If any of you have a token and you want to get it listed on an exchange, you know you have to pay a token fee, usually a listing fee. Here you set up a dao proposal. Ask the Mango Dao. Hey, do you guys want our token? So if you have a token and you want to get listed on a big exchange, you should look at Mango. Bonk. They've used the Realm's new voter UI to increase voter participation.
00:48:03.845 - 00:48:39.657, Speaker A: 10x and marinade. Michael was here before. If you are a Marinade governance holder, you can choose how Marinade actually delegates its stake across the different validators. There are a lot more features. This is all I could fit on this slide before making it unruly, literally. We're outside of the area of trying to build new tooling. We're now in the exploration area, where we need to work with universities, where we need to work with policymakers to try to start experimenting on what's possible.
00:48:39.657 - 00:49:39.109, Speaker A: Athens now is going to happen this year again where we only talk about governance. It's going to be in Athens, Greece in January. Now I want to show you a quick slide, a quick video of some of the new UIs coming out in Realms later this year so you can start building today. The discord is like the treasure trove of all of our documentation. Also, we have docs Realms. Today I'm going to be around. All this is open source, so reach out, start building.
00:49:39.109 - 00:49:54.365, Speaker A: Really. Solana is the home. Thank you. Excellent. Right, on to our next talk. It's another product keynote. It's about Street Flow.
00:49:54.365 - 00:50:59.473, Speaker A: It's by Militia Imperfect. That is not his actual surname. Introducing Militia, please. Thank you very much. Hello everyone. Malisha from Streamflow. Thank you for taking the time to join this early.
00:50:59.473 - 00:52:08.907, Speaker A: I know last night there was a lot of. There were a lot of parties, so I really appreciate it. For those of you that know what Streamflow is and what Streamflow does, Streamflow solves some of the biggest problems in crypto scams rug pools, bad token launches, bad incentives, bad distribution. And how does Streamflow do that? We do that through secure, easy to use and powerful platform that helps projects to align incentives among different stakeholders. Streamflow launched three years ago at first the Breakpoint and as of now we have over 600 million in TVL, roughly 600 million processed in different tokens. Over 1 million users have used Streamflow and over 4,000 different tokens have gone through the platform so far. The most known BAS projects out there use Streamflow in some capacity or community tokens, Defi, Deepin, Gamify, you name it.
00:52:08.907 - 00:52:43.705, Speaker A: Most likely they use Streamflow in some capacity for airdrops, vesting payroll token locks. And we are just getting started. Actually this is where we've been so far. We're just getting started. We are launching permissionless token staking as a service. All of us love to stake Solana, right? What if you could stake any token out there? I'll show you the demo. So this is launching in October, very simple.
00:52:43.705 - 00:54:09.093, Speaker A: First version where you can select any token to create a staking pool, add any token as a reward, set the release rate and boom, in just a few clicks you're good to go. The stake pool is created. Any token project can do that permissionlessly on Chainflow platform as a staker, individual staker, you go to the platform, add your stake, create a stake account, it's deposited as rewards accumulate, you can claim them and once the lockup period ends, you can unstake. So this is one thing we're launching in October, another thing we're launching in October and we are very excited about is aligned unlocks. What are aligned unlocks? It's a novel mechanism that we're bringing to Solana and things so unlocks happen relative to some milestone or KPI. Let me give you an example. A good example is of use of this line unlock is you come to, you set a target price for the token to unlock and once there is, the unlock is supposed to happen.
00:54:09.093 - 00:55:24.319, Speaker A: Depending on the target price and the current token price, a certain percentage is unlocked and released at that time frame. But this applies to airdrops, vesting contracts, locks, payment streams, you name it, across the stack. And this unlocks a whole new opportunity for the experimentation. And we are very, very excited to see what users are going to do with aligned unlocks, milestone based unlocks. So, and we are not stopping here either. Like even bigger thing after these two launches in the next few weeks is Streamflow v2. What is version two? Well, it's a way and A bigger plan for streamflow to decentralize and to turn users and community into stakeholders to coordinate this network and to open source the protocol and allow others to build on top of streamflow and no better way to achieve coordination Invite others to build and decentralize than the token.
00:55:24.319 - 00:56:08.991, Speaker A: And the logical question is we're launching Staking in October, we're launching Allied Unlocks in October and we're launching Token in October. Follow us on Twitter. Join our community in the upcoming days and weeks. There will be many, many more news on this front. Thank you very much, ladies and gentlemen. Right, thank you very much. Our next talk is actually a debate.
00:56:08.991 - 00:56:37.611, Speaker A: It's going to get a little bit feisty in here. It's about the trade off of institutionalization. Thank God I got through that word. The trade off of institutionalization is network centralization. So I didn't get it. The second time on this debate are two individuals, Eva Lawrence from Figments. They'll be moderated by hani Rashman of 21 and Company and joined on this panel by Hassan Ahmed of Coinbase.
00:56:37.611 - 00:57:35.831, Speaker A: Please join me in welcoming them. Thank you for being here. We have a fun and exciting debate today. We're going to be talking about a little bit of the nuance behind centralization and decentralization. Joining us are two very important but very different players within the crypto ecosystem. Why don't we do very quick introductions and then we'll jump right in? Sure, yeah. Hey everyone, my name is Hassan Ahmed.
00:57:35.831 - 00:58:11.525, Speaker A: I'm the country director for Singapore Coinbase. Moved to Singapore about five years ago to start the franchise over here and never looked back. Hi everyone, my name is Eva Lawrence. I am the head of EMEA for Figment. We are the largest independent staking provider covering around 38 networks and have offices in London City, Singapore, Toronto and New York. And yeah, looking forward to discussion today, guys. Looking forward to it.
00:58:11.525 - 00:59:07.817, Speaker A: We'll try and keep it friendly with some spicy points here and there. First things first, I think when it comes to seeking, for example, we've been hearing that the institutions are here for five years now. Where are the volumes? How do we think about it with respect to institutions? Are they really here? And if they're not, what is, what is going on there? Yep. So the institutions are here. When it comes to staking, they are staking. We have 15 billion in AUS and what we're seeing is institutions not wanting to run everything themselves right now. So they're working with leading providers like Figment and others in the space for staking.
00:59:07.817 - 01:00:12.095, Speaker A: They're Working with leading custody, custody tech providers, all of these things. They want to want to work with the best in class, but they don't have the resources internally to run it themselves. So what we're seeing is maybe you don't know that a large institution is staking their soul, but they're doing it in the background. Maybe they're also purchasing ETP products, maybe they're also looking at trading on the side. The institutions are here, but I would say it's not always obvious to everyone because they're not ready to be as public about it yet, especially when it comes to staking. But I think the next step will be the large institutions and by that I mean banks, some of the world's largest asset managers looking at offering these products to their end customer. And at that point it will become more visible.
01:00:12.095 - 01:01:52.397, Speaker A: And where do you find the break between the highly decentralized liquid staking protocols, the things that may not certainly pass risk and compliance committees of large institutional holders versus what you see? So I mean I think when it comes to staking, staking on Solana is pretty expensive to run, right? To do it well, you need to have bare metal setup, right? You don't use cloud AWS type setup. You need to have a lot of infrastructure to run the service to support Solana staking. Now we have 130 people across 17, 18 countries now and we have both technical expertise and also. So the infrastructure supports a lot of staking. And to do that well, I think it's very difficult for say a smaller firm to do that. It takes a lot of financial power backing in that sense. So I think when we see kind of institutions coming into the space and looking to work with staking providers, their first port of call is okay, which large provider can I work with where I don't have to lose ownership of my assets and I am not creating a liquid stake token on top of it.
01:01:52.397 - 01:02:44.705, Speaker A: For a lot of them, it's just about, they just about get past their risk and their management teams to explain to them what staking is. Native staking on Solana, whether it's not custodial protocol staking, you introduce a liquid staking element and then a lot of these large institutions, their risk and compliance teams go slow down there. That's way too much for me. So we've seen them, the institutions go to the large providers who have the support and the tech infrastructure, service, etc. To unstaking and are doing it in a non custodial way, which focuses. Should we make this a little more spicy and make it a bit of a debate format. So I'm happy to give my opening arguments until I have maybe three or five minutes.
01:02:44.705 - 01:03:50.751, Speaker A: All right, thank you. So I assert that institutionalization does not lead to network centralization. Institutions are a big group of entities from asset managers, market makers, hedge funds, regular crypto exchanges, banks and subreddits, and so on. It's a segmentation to differentiate them from individuals and developers that are building on chain. Institutions also have very disparate jobs to be done, risk appetites, and then also there's a level of sophistication. So the first thing I would disposit is that institutions can indeed participate and engage in the network, especially at the application layer, without compromising any decentralization. So if you look at PayPal's BYUUSD that was launched recently, they hooked onto the Token Extensions program that was launched and for them it was great because they were able to use the composability and the expressiveness of the platform in order to be able to provide compliance with their risk posture that they wanted to do without compromising the consensus layer.
01:03:50.751 - 01:04:42.563, Speaker A: I think there's a lot of exciting developments that are happening on the institutional side on Solana. I think we in the last few days we've had announcements around SocGen deploying their stable. There's also Franklin Templeton that's deploying their MMF. There's also BlackRock that's moving their fund over and they all see the potential for the on chain economy on Solana and being able to really harness the performance at the execution level. The stronger version of my argument is that institutions can actually help promote decentralization. So Sol is the third most traded asset on Coinbase, and one of the ways that we help decentralize is to increase distribution of the native asset. So over the years, Coinbase has helped hundreds of thousands of users be able to access this native token as well.
01:04:42.563 - 01:05:22.195, Speaker A: We also help with issuance. So we recently launched CB BTC on base. Our users love Solana and so do we. So we're very excited to announce that we will be bringing native CBTC to Solana as well. Just to close out two more things on this. So I think the most impressive display of institutions helping with diversification is with Jump's work on Frankendancer and Fire Dancer. Client diversity is one of the core components of the Nakamoto Coefficient and currently everyone's really running on Bonzo.
01:05:22.195 - 01:06:24.677, Speaker A: So once Frankendancer and Fire Dancer are alive, this is a huge upgrade on Savannah's Network decentralization as well. And then lastly, I would even posit that Solana foundation is an institution as well. And one of the cool things that they have done very recently is their delegated staker program where they've been subsidizing vote fees for individual validators and stakers. And I think that's been a resounding success and it's really helped just bring individual and power user participants at the consensus layer as well. Congrats. First of all, do you ever think that Coinbase is trying to do too much, try to be involved in too many points on the value chain? That would be my question for you. In terms of execution, trading, custody seeking base.
01:06:24.677 - 01:07:16.525, Speaker A: Now, is there a point where Coinbase says we're doing too much, we need to leave some to other institutions, or we're creating a centralization issue by having so much under the Coinbase umbrella. Is that something that you discuss internally and something that you focus on? Yeah, that's a great point. Our mission really is to increase economic freedom around the world. The power of crypto. And I think the reality is that when Brian and Fred started the firm, there just really was nothing in terms of core infrastructure. And I think we really consider ourselves as a core crypto company. So as we have grown to provide just more services to consumers, institutions and to developers, we are very open to partnering on some of these solutions with folks like fit.
01:07:16.525 - 01:08:41.840, Speaker A: But sometimes we just find that given that we've got like 12 years of experience, we have the right security posture, we're the largest asset custodian in the world, sometimes we just have to build it ourselves to push the crypto economy forward. Eva, do you think you can do both? Do you think you can, as an institution, be both decentralized and centralized? To go to your question, I mean, centralization is a sliding scale, right? It's not absolute, it's not. You're either decentralized or centralized in our ecosystem anyway, right? What we're seeing with firms, even if they say they are decentralized, they're often as decentralized as they can be. So I think that what we need to do as an industry and as institutions within this industry is figure out where we on that sliding scan of decentralization. Now, I think that what we're seeing with all of the institutions coming in to coming into the industry looking to invest in builds, build projects on Solana is institutions beget institutions. And so in bringing in these institutions, you are actually making awardees. You have more firms running trading, running, staking, offering, Stable coins.
01:08:41.840 - 01:09:26.255, Speaker A: But all of these aspects are bringing more institutions in. As a result, we are seeing some more decentralization because you're not just relying on one firm and you're not just going to one firm to custody of Solana to trade Solana to buy Solana options. You are diversifying because you are working with different providers. And our view at figment is that we are the leading staking provider, but we don't go into the custody space. You won't see us. We have no plans to start our own wallet infrastructure. We are focusing best in class for staking and that's our focus.
01:09:26.255 - 01:09:52.533, Speaker A: And I think that we lend our expertise to other institutions coming into this space. The other issue that you have is centralization is not just about institutions. We see it in the staking space. There are definitely some out there who are individuals or groups of individuals who are offering staking and they're pushing as well.
01:09:52.549 - 01:09:53.625, Speaker B: We're not an institution.
01:09:54.515 - 01:10:55.915, Speaker A: But if a large proportion of the stake is centralized with one group of individuals or one individual, whether they want to call themselves a dao, whether they want to call themselves a foundation, a group of Solana entrepreneurs, whatever they want to call themselves, that's still centralization. Whether or not it's centralized at Coinbase, whether or not it's centralized figment, having that centralized stake around one company is still centralization. So I think what we should focus on is one, figuring out whether we are moving the centralization versus decide decentralization, sliding scale slowly towards decentralization and not just blaming this on institutions. Right. It's not institution centralization is bad. Everything else is fine. It's more decentralization should be the goal that we are all striving towards.
01:10:55.915 - 01:11:43.205, Speaker A: Yeah. So I agree with Eva that decentralization really is a journey, that it's non binary at all. And I think one of the important things as we're thinking about this is it really depends along with just hard incentives on what is the culture and values of the ecosystem and whether the community, the core developer community, also cares about it. Because I think that also acts as a powerful social construct in terms of setting up norms for any kind of actor on the ecosystem to behave accordingly. So I think we see that in spades on the Solana ecosystem. There's other ecosystems that are also exhibiting similar behavior, but it's not always the case. And I think we all know in the room that there's a lot of networks and network founders that just built up service.
01:11:43.205 - 01:12:56.331, Speaker A: Let me push you on that point a little bit because arguably one of the things that coinbase brings to the table is greater trust that allows people to, especially institutions, feel more comfortable coming in. Certainly institutions that are not using Uniswap are using, are using coinbase. And yet when it comes to see stablecoins, which Eva brought up, coinbase's offering, which is far more centralized, far more controlled, you can close someone, it's regulated, etc. Is far and away way more successful than the decentralized alternatives. So how do you think about the mismatch between these two? Yeah, I would just say that coinbase has its set of centralized and regulated services, but we also have an equally robust set of on chain and self custodial products and services as well, like base, like WindBisWallet and so on. I think that the reality is that you have to take the institutions on a journey and they're going to start out with something that they're familiar with, where they can test the waters and understand what it looks like before they go deeper. I'm actually pleasantly surprised at this cycle.
01:12:56.331 - 01:13:57.909, Speaker A: I would say the institutions are here. If you even look at our financials, the institutional segment is healthier on a quarter over quarter basis. That is the consumer segment. And we release products like Prime Web 3 wallet that lets them start custodially and then start deploying their funds and capital on chain as well. Eva, do you buy that argument or do you think that at some point the institutions have to touch the asset class, whether it's decentralized, centralized or not, and it's just a matter of. Yeah, I mean, I think that the institutions will want to take more control in the future. The way that, you know, the way that they're approaching it is come up with an idea, come up with a strategy, test it with a partner, test it with their own internal setup, touch the asset class by offering it to their customers.
01:13:57.909 - 01:15:05.587, Speaker A: I think they're on this journey whereby we'll see their involvement grow as they get further along the line. I do think that they will, the large institutions will very much be involved with the asset directly. We've already discussed today. Franklin Templeton, you've seen be very involved. There was the Velocity event here this week. We saw those institutions and many more very well represented. And that is because they are seriously considering what products should they be offering on Solana, should they be building something on Solana? So I don't think that there is a world where institutions say, you know what, I'm going to outsource everything, I'm only going to work with Partners, but it's more that at the stage they are in their journey towards offering products or offering more products, they're starting off with partners, but in the future they will very much be offering products themselves.
01:15:05.587 - 01:16:23.259, Speaker A: So I think one of the biggest benefits of centralization is being able to call someone, explain the problem. I think we call you guys a lot when there's a problem. Liquid staking protocols, decentralized stablecoins, etc. The fact that you are not able to have recourse, whether legal or actual, do you think that is, do you think that it's here to stay in static or do you think there's an opening for insurance or more programmatic features of these technologies that can mitigate some of that pain? Yeah, I'd say that one of the most encouraging signs that we've seen in this cycle is that most institutions have consensus that building on open public networks is the force that they want to take. And I think that was very different from a couple of cycles where it was enterprise blockchain and dlts and whatever works out that people wanted to use. So I think it's a recognition that look, they've taken the time to study what's going on, they've done their due diligence and you know, for institutions as big as BlackRock to now start moving size on chain, on Ethereum and Solana is a huge testament. And I think that the rest of the industry will also sit up and take notice.
01:16:23.259 - 01:17:06.531, Speaker A: Eva, I'll give you the last word. Yeah, I mean, I think that when it comes to things like, like liquid staking, defi and setups where there is no central point of contact for an institution, I think that we're not there yet. We need these institutions to be able to ease themselves in. And from a risk and compliance and legal perspective, a lot of these firms can't get comfortable with that kind of a setup yet. If they're a smaller firm, then yes, that's fine for them. But large institutions, institutions do need a clear framework, a clear point of contact and an ability to resolve any issues. So I think that will be what we'll move towards in the future.
01:17:06.531 - 01:17:32.580, Speaker A: But it's not something institutions are ready to take on yet. All right, last thing. Five second answers. Five years from now, staking and exchange volumes. What percent decentralized, centralized. Oh, I think the future for Dex is bright. I definitely think that with all the improvements going on, we'll see most trading watch in 5 years.
01:17:32.580 - 01:18:16.551, Speaker A: Sorry, the percentage of. Yeah, what percent of staking and exchange volumes in total. So Uniswap versus centralized exchanges in 5 years. Decentralized versus centralized up. Still think maybe 5 years time 60, 40 to centralization, I would say. Thank you guys so much. So sorry, but I'm sure you're capable of taking a break with your own judgment.
01:18:16.551 - 01:18:51.745, Speaker A: We're going straight in. It's another workshop, it's with Alabama at Tencent Cloud. We've got a double header of speakers. Diwa Chen from Tencent Cloud will be haggling the first 15 minutes or so, then passing the baton, the proverbial baton, to Kosmin Gama from Beware Lab. So let's start with David and give him a lovely hello. Hello. Hey guys.
01:18:51.745 - 01:19:41.073, Speaker A: My name is David and I'm from Tencent Cloud. So I'm happy to be here to present to you guys. And Tencent Cloud is a worldwide cloud computing services providers and it's happy for our team to be here and present to the SolarNight ecosystems. So we are based in China and our international team is headquartered in Hong Kong. So maybe a bit of background introductions to you guys about Tencent. So Tencent is the largest public listed company in China and we also built two of the largest social media platforms in China, so called WeChat and QQ. So for WeChat we have about 1.3
01:19:41.073 - 01:20:25.225, Speaker A: billion monthly active users and for QQ we have about 5 million. So that's where we're coming from. Right. And with the social media platforms we also built games around it. So perhaps you guys have played owners of Kings and PUBG Mobiles and we also invested into 300 gaming studios worldwide, including Riot Games and also Epic Games. So all of these applications, games and social medias are actually built on Tencent Clouds. And we went international about five years ago and not only we are serving enterprise, we also serving consumers worldwide now.
01:20:25.225 - 01:21:05.623, Speaker A: So only in the international markets, we have about 1.2 billion users worldwide now. So on the one hand we serve those consumers with retail products and on the other hand we connect enterprise with those consumers we have in the international markets. So where we are, right, so we are the international team. So we serve anywhere outside of mainland China. So currently we have about 21 regions and 58 availability zones worldwide. As for myself, I look after the strategies for international markets.
01:21:05.623 - 01:22:03.208, Speaker A: So if you guys are actually from some of the other regions, feel free to talk to me about more info in the local markets. So as I said, Tencent is a very diverse company. So when we actually extracted the technology capabilities from the Tencent group and output it to the various industries, we work with all of these different industries worldwide, name it, be gaming, social medias and finance. But actually in Web three we work with most of the leading players now. So we work with most of the top crypto exchanges, we work with gamefi projects and also work with different sets of service providers like wallet solutions and staking providers. So maybe focus back onto Web3. Right.
01:22:03.208 - 01:22:53.581, Speaker A: So in the international markets we have about two over 200 products and services. But for Web3 we actually categorize it into three tiers. Cloud security and innovations. For cloud we provide cloud computing services and infrastructures to empower different blockchain applications. For securities we provide user safety and also fraud preventions to protect the different applications. And for innovations we bring immersive customer experience and also super application capabilities for different applications. So in the following time let me perhaps give three product use cases specifically for Web3.
01:22:53.581 - 01:24:22.785, Speaker A: So the first one we have is the super application platform. It is basically a very low code technical framework that can be easily embedded into any applications and then transform it into a super application. Think of WeChat, right? And also what Telegram is doing now, it's basically attracting the different applications into one super app and enhance the ecosystem growth in the longer term. So one of the Web3 leading gaming technology companies is actually working with us with these products. So basically they integrate this framework into their existing applications and that allows the different GameFi projects to transform their existing GameFi projects as mini apps and they get embedded into this grand super app applications and the different functions that will support supported in the mini program applications framework are different wallet systems and NFT minting capabilities and also cross chain capabilities. The second product I would like to maybe introduce to you guys is our full sets of Media Services SDKs. So the client we're working with is actually one of the top five centralized exchange in both spots and derivatives.
01:24:22.785 - 01:25:39.379, Speaker A: This client built their live streaming and also video on demand platforms 100% on Tencent Cloud's media service SDKs. So with these products they actually built the most vibrant online crypto communities that have attracted thousands of Kols to share their insights and perspective on their live streaming platforms and allow the platform to acquire more users across the globe. The last product use case I would like to pitch to you guys is actually the cloud computing infrastructures. So we are working with one of the top three staking providers worldwide. These providers, they are European based, they have about US$5 billion asset under management. So basically we provide GPUs and CPUs for them to run the different blockchain nodes, for example in Solana, in Ethereum, in Celestia, in Polkadot, you name it, all of the mainstream tokens. So with our infrastructures they are able to uphold the same 99.9%
01:25:39.379 - 01:26:38.713, Speaker A: uptime and also we support them to cut their running IT infrastructure cost by about 50%. So that would be a wrap to my introductions for tencent cloud internationals and also some of the product use cases we have. I'm happy to introduce you guys to Beware Labs. So together we are building some products for the Solana Ecosystems. The solutions are an archive data service and also in the future we are building some RPC node surveys only tailored for the Solana Builder ecosystems. The solutions are easy to employ, it is very cost effective and it is 100% tailored for the Solana builders. So next on let me invite Cosmic from Beware Labs for introducing more about the Beware Labs and also the new product launch.
01:26:38.713 - 01:27:27.015, Speaker A: Thank you all and let's welcome Cosmic. Hello everybody. First thank you David for introducing me. I'm happy to see you here in Singapore and I hope you're having a great time. My name is osmin, I'm part of spiralabs and today I would like to present you our ARP solution which enables data availability for Soar Builders. My presentation is going to a little more technical, but bear with me. First let me give you some context about Pure Labs.
01:27:27.015 - 01:28:14.595, Speaker A: We are focused on providing one of the best, one of the highest performance infrastructure for Web3 services while also having one of the best prices on the market. We focused on growing our business horizontally, meaning that we serve a lot of networks and we have very quick time integration plans. We can accept new ones, we can expand our services a lot faster. But in order to get to the next level we decided to agree an acquisition offer for Alchemy. So I think together we can improve the Solana ecosystem and the Web3 ecosystem as a whole. Let's talk about the main two problems that we tackled in this solution. First up, the data availability.
01:28:14.595 - 01:29:05.419, Speaker A: I mean you can get access to this data, but you either have to use a provider that already got access to it and you have to pay it, or you can source Solana foundation to give you access to their Big Table instance. In cases you don't know the Solana historical data is stored in a Google instance in a Big Table in the instance and it has like around 1.5 petabytes of data. And once you get access to it, you will find out that it costs a lot to move it away. So egress traffic is amazing. It's huge. It will take you around 130k to migrate the data to another storage because you have to use Google tools, you have to use data flow and the data and data flow only allows you to data to Google storage.
01:29:05.419 - 01:29:40.991, Speaker A: And if you want to store 1.5 petabytes of data in Google storage, it goes through around 60k or 90k for the storage and egress traffic is the rest of it. It's like 40k. It's huge. We managed to lower the cost with more 30k by using Tencent's cloud infrastructure, like cloud object storage for quant storage or a longer. And we built our own internal synchronic programs that move the data around. They're built in house.
01:29:40.991 - 01:30:01.787, Speaker A: So we get a lot, we got a lot less. The price will lower a lot. Let's talk about deliverables. First. You get access to data, of course you can. It's publicly available, it's a tension bucket. And you also get data syncing mechanism.
01:30:01.787 - 01:30:41.965, Speaker A: Once you get your data, you have to keep it in sync because the Solana network is a very high throughput network and it generates a lot of data. So you have the mechanism to keep it in sync and data. And you also have some infrastructure automation tools like terraform skills just to make it easier for you to spin up your nodes and everything. And once you get access to the data, you can deploy your own cluster to serve data from it in house. You can build RPC services, you can build index servers or anything like the world. It's your Romania. But first we have to talk about storage.
01:30:41.965 - 01:31:06.675, Speaker A: As I said, Solana is a very big network and it has a lot of data. So. So we have to use Tencent cloud lowering storage as a cold storage in order to lower the cost and have long term storage. But there's a problem with that. You can't query it. And if you want to serve data, you have to query it. So in order to fix that, we have a HBase cluster.
01:31:06.675 - 01:31:41.825, Speaker A: It's the edge based cluster. It's a distributed system, just like Bigtable, it's open source and then I can use it, you can host your own. And so you introduce the data from cloud W storage into AWS on demand. You can build cache systems, everything you want. You can use the whole data if you want to have the whole 1.5 petabytes of data and you can manage that, or you can digest just a few blocks, ranges, everything you like. The first process of syncing the data was to get all the data from the BigTable into cloud.
01:31:41.825 - 01:32:38.029, Speaker A: So we used BigTable as a data source and cloud object storage as a destination source. We then built our cluster of programs. It's a distributed system, it's a cluster of multiple machines, meaning that we process the data distributor and in parallel on each machine after we solve them, we have to keep it in sync. And we know that Sonana nodes have a thing called Geyser plugin. That Geyser plugin plugin can export the data from the blockchain directly from the POD without having to use some standard RPC codes. So we get the data from the node that runs a Gezer plugin, we read that and we upload it cloud object storage to keep the hardware in sync. But as I said, you have to query it.
01:32:38.029 - 01:33:21.101, Speaker A: And if you want to use only these two part of the solution, you'll get some delays because you have to upload it in host and that cloud object storage and then you have to download it from host and load it into your railways faster. And that takes time. But we know that Solana nodes are able to communicate with. So all you have to do is basically create an adapter that mimics Google BigTable's interface and you connect the Solana node to it. And the Solana node will think that he's talking with a bigtable instance. But in reality the adapter writes and reads to your HB pass through. You can then connect a Solana Lite RPC node like the one built by TextureLabs.
01:33:21.101 - 01:33:44.823, Speaker A: We use that one. Thanks guys. And you can serve data from there. The last process is ingesting the data from the original data from the cloud storage. You have to sync your HS and then connect the live syncing process to it. This is how the final architecture looks like. On the top right corner you have a Solana node communicating with the live syncing process.
01:33:44.823 - 01:34:37.145, Speaker A: On the lower right side, the live syncing in process uploads the data into cloud object storage. And on the left side you have the initial archive syncing process that uploads the original data into cloud storage. In the middle you have the adapter and the ingestor sync that update your hbiz cluster. But if you want to use all of this, if you want to use the product, you don't need all of this. You initially just initially need the ingestors to sync the hbase cluster with the data that you want. You can then throw it away if you don't use the synchronous anymore and you need a Solana node with an adapter, hook it up to hbase to your hbase cluster and you're done. You can then maybe build a cluster of Solana Light nodes or whatever you like to serve data from there.
01:34:37.145 - 01:35:07.873, Speaker A: And finally we have some decent deployment automation like Terraform Scripts. You can deploy your own Solana node, your own Solana Light node and the hbis cluster just by running some Terraform scripts with different configs. They will spin up intensive so you don't have to think anything about all of the complicated stuff. That was all. I hope this project helped you. And if you have any questions, feel free to contact us. Check out the git file we have.
01:35:07.873 - 01:35:36.805, Speaker A: The repo is publicly available, there are multiple repos and fill fit them. We will be next page if you have any questions. Thank you. I guess we Voice of God which is our favorite thing to do. We have another product keynote for you. Wonderful title in the Blink of an Eye and this is by Chris Osborne of Dialect. Please join me in welcoming.
01:35:36.805 - 01:36:24.473, Speaker A: This is a huge stage. Okay, Good afternoon everybody. I'm Chris Osborne, the founder and CEO at Dialect. In June we launched Blinks, a new technology in collaboration with Salona Foundation. Links or blockchain links change the way that we discover share that again. Links or blockchain links change the way that we use Share and discover the web. 3.
01:36:24.473 - 01:37:02.803, Speaker A: Internet the atomic unit of shareability on the Internet is the URL. Blinks give URLs superpowers and let your users take action right from where the link is shared. Instead of having to go to a website to take an action, Blinks let your users take action right from where the link is shared. They teleport the action right to where the link is shared. As a part of the launch, we teamed up with the wallets and some of our favorite dapps to do something incredible. We put crypto right into crypto. Twitter is my mic going out? Can you hear me? There we go.
01:37:02.803 - 01:37:58.585, Speaker A: On launch day you could Swap tokens and LSTs with Jupiter, Sanctum, Meteora and Phantom. Buy and mint NFTs with truffle and Tensor, vote on governance with Helium and Realms and subscribe to premium content with Access protocol among other things. And you can do it all directly from your timeline. This was made possible right on desktop with integrations from your favorite wallet extensions like Phantom and Backpack. No new downloads were required to make this possible. We launched in June with 12 blinks and two wallet integrations. Today there are supporting wallets and over 400 links have been built by hundreds of developers, from the biggest teams to a wave of creative new startups shipping Blinks first before even building their first apps.
01:37:58.585 - 01:39:07.005, Speaker A: We've been absolutely blown away by how much you all have been enthusiastic, by your enthusiasm, by your enthusiasm about this new technology. We also shipped a host of new capabilities and have others coming soon like SignMessage, optional transactions and external Blinks. Blinks are a whole new way to take your product to market, to do it at lightspeed and to share it everywhere. I'm here today, however, to tell you about what's next with Blinks and how they go so far beyond Twitter and the Web two platforms and create a new kind of hyperportable, hyper composable Internet. Blinks construct, deliver and distribute the things you want to do on the Web three Internet, whether that's signable transactions or messages, but they do it in an interesting way. Instead of delivering transactions via a website or a mobile app as is typically done today, or require developers to integrate and understand your SDKs directly. Links deliver ready to sign transactions directly from URLs.
01:39:07.705 - 01:39:30.391, Speaker C: This simple restructuring has enormous consequences and opens up a wide array of creative new opportunities. So let's go through some of them. Yes, Blinks let you deliver experiences directly to Web2 platforms like Twitter, but you can also teleport buttons directly between Web3Dapps, creating a more seamless, composable and immersive.
01:39:30.463 - 01:39:32.183, Speaker A: Experience for your users.
01:39:32.359 - 01:39:55.685, Speaker C: Drip, one of Solana's leading creator platforms, uses Blinks to bring functionality from their partner sites into their platform directly. Mallow Blinks let you bid on auctions and Tensor Blinks let you buy NFTs on their marketplace. With both of these you do not have to go to the underlying site. All of this happens happens right on Drip.
01:39:56.985 - 01:39:59.185, Speaker A: Mobile wallets are the front door to.
01:39:59.225 - 01:40:36.787, Speaker C: Crypto for millions of users. Blinks will transform them into super apps with on chain experiences sprinkled throughout the entire product. Deploy your crypto to earn more using DeFi, list your NFTs or buy new ones, take action on timely alerts and maybe even buy a Keystone hardware wallet. Blinks teleport the whole blockchain right into your wallet and will bring a new generation of users more on chain. Similarly, innovative social fi products like Bags are bringing crypto right into their chats, giving users more to do with their friends.
01:40:36.971 - 01:40:38.459, Speaker A: I also see I'm almost on time.
01:40:38.507 - 01:41:27.335, Speaker C: Here, so I'm going to go through these a little more quickly. They transform the way that we can do security, empowering teams like Blowfish to do more interesting things in security analysis. And as I mentioned before, Blinks dramatically speed up your time to development working with third party providers. The last point I want to make here is that beneath all of this, blinks are an ambitious new way to build experiences in your actual product. Blinks can now make your product experiences shareable, literally the buttons inside your dapp. Every experience I talked about before is capable if you build your product with blinks today, Crypto promises to create a composable Internet in which we own our identity and can take action from anywhere. Blinks are the language of that composability and the technology that will finally take it to market.
01:41:27.335 - 01:41:29.515, Speaker C: We can't wait to see what you build with them.
01:41:31.095 - 01:41:55.489, Speaker A: Thank you. Awesome. Well done Chris. We're going to keep the talks coming. We have another product keynote. It's about Clickhouse and Gold sky by Crypto House or concerning Crypto House. Derek Chia from Clickhouse will explain more and I want a very overzealous round of applause to welcome him right now.
01:41:55.489 - 01:41:57.525, Speaker A: So please welcome Derek.
01:42:26.635 - 01:42:28.455, Speaker D: Hi Ron, how are you doing?
01:42:28.915 - 01:42:29.995, Speaker A: Good, right?
01:42:30.035 - 01:42:57.665, Speaker D: Thank you. So I'm from Clickhouse and today I'm going to introduce to you Crypto House. So Crypto House is a free blockchain analytics service powered by Clickhouse and ghostguide. So before I talk about Crypto House, let me tell you more about Clickhouse. Clickhouse is an open source analytical database developed since 2009. We have over 36,000 GitHub stores with over 500 releases. It is column oriented and we do very best, very well in aggregations.
01:42:57.665 - 01:43:42.515, Speaker D: People said you can use this in a single binary or you can set up in a distributed fashion. We support replications with shards and you can do this across regions. If you have any analytical use case then Clickhouse is the best database for you. If you need something managed then please visit Clickhouse Cloud for a service. So why is Clickhouse ideal for blockchain analytics? First it speaks SQL which means that most SQL compatible URL applications dashboards would work out of the box including services like Dune. We support a lot of writes and this means that you can have several million writes per second coming into one single server per server. And for reference the transaction row reads for solana it's around 4,000 per second.
01:43:42.515 - 01:44:22.087, Speaker D: It is distributed. Many people set up Clickhouse in production in a distributed fashion with applications for high availability and with shots. The largest known cluster is over 4,000 servers and I'm sure there are larger cluster that is unknown to us. It is also highly efficient. What this means is that we offer encodings and compression options, for example and uncompressed CSV you get to enjoy over 20 times the compression rate. This means that you save money in terms of storage. The next one is most important because Clickhouse runs blazing fast queries.
01:44:22.087 - 01:45:11.949, Speaker D: We scan and process billions of rows per second and you will need this in order to get real time analytics coming out from blockchain data. Last but not least, most of the transactions and blocks data are structured and is suited for OLED modeling. What this means is that you can have joins, you can use joins for in depth analysis and we do support it as well. So Clickhouse is very honored to be trusted by multiple companies in the blockchain analytics space today. I quote Aaron Chu, co founder of coinhaul and I paraphrase, Clickhouse is significantly outperformed other databases. We tested and delivered 40 times the cost savings and kickhost is a clear winner. Thank you Aaron and congratulations on the acquisition yesterday.
01:45:11.949 - 01:45:38.515, Speaker D: They are being acquired by Jupiter Exchange and I just learned about it yesterday. So Crypto House is not just about Clickhouse. We partner with goalsky that provides us the data through their real time data pipeline. The core product is called Mirror. It is REOC Aware, supports Mountain multiple sync and also it's easy to configure. So you can visit crypto house@crypto.clickhouse.com over there you can write SQL queries against blocks, transactions, tokens, tables.
01:45:38.515 - 01:46:30.685, Speaker D: Basically all the tables that you see there, the results that you see are returned real time, computed real time. There's no batching, there's no waiting for answers. We also have simple visualization for you to consume the data data. We put in quotas at the database level to ensure fair usage. We welcome you to contribute queries over at our GitHub repository. And now let's cut over to the demo. All right, so this is Crypto House.
01:46:30.685 - 01:46:59.199, Speaker D: On the left you see a list of tables available for you to query on the next tab. These are some of the pre made queries that you get to run. So let's have a look at the number of transactions. All right, we have over 300 billion rows all stored in Clickhouse. Let's have a look at some of the sample blocks over here. This is the editor you are free to put in your query, like what I'm doing here. And let's have a look at some of the latest blocks.
01:46:59.199 - 01:47:33.507, Speaker D: So yes, we get to see the latest blocks being ingested into Clickhouse. There's also charts for you to view the data and let's take a look at one of the more expensive queries. Let me just give it a try. So this is a query that runs and looks for the most active account within a day. As you can see, we are reading over 72 million rows, but actually we are processing over 1.2 billion rows at the back end because we are doing an array join within. Wow, seven seconds, we already got the top account that have the most number of absolute balance changes.
01:47:33.507 - 01:47:57.695, Speaker D: Pretty good. If you like to, you can also share your query or save your query at the browser. Let's cut back to the last slide. Okay, so for more information, if you mean if you have any issues with quotas or if you like to lay your hands off the data, feel free to contact Gosky. Otherwise, I welcome you to experience clickhouse@crypto. Clickhouse.com. thank you.
01:48:06.685 - 01:48:23.189, Speaker A: Hello. The room is getting even fuller. Thank you very much, Derek, for that excellent presentation. Now we've got a very good one. It's a technical talk. It's about the past, present and future of products on Solana. Now, when I introduce this chap, you're going to give him a lovely round of applause.
01:48:23.189 - 01:48:39.255, Speaker A: You may streak. Streak. That's not all you want to do at all. Please do not do that. Shriek and scream. I will do that if this gets any worse. So please join me in whatever way you'd like to in welcoming John Wong of the Solana Foundation.
01:48:46.235 - 01:49:17.585, Speaker C: Hello, everybody. It's wonderful to be here in Singapore. It's wonderful to be here for my fourth break point. It's an incredible time to be in the Solana ecosystem and today I'm going to be talking a little bit about the past, present and future of product on Solana. Let me start with a quick introduction and that's all the Chinese are going to get out of me today. I did not pay enough attention in Chinese school to do any more than that. But again, great to be here in Singapore and meet so many new people.
01:49:17.585 - 01:50:24.305, Speaker C: For the people who don't speak Chinese, my name is John Wong. I run the engineering team at the Solana foundation and part of my role is to spend time with all of you, all the hundreds of teams, the thousands of builders building on Solana to understand exactly how they're approaching building their products and businesses and figuring out both. How do you use the existing state of Solana as well as what new things we should create to help facilitate new interesting products on chain. And in light of that, instead of the past, you know, present and future of product, which sounds like A roadmap sounds like very, you know, definitive things. I do want to talk a little bit about the insights that actually I've learned over the last couple of years working with all of you and a lot of the folks in the broader crypto ecosystem. So instead of the talk that you came here for, the past, present and future of products, the talk you're going to get is how to build better products, comma, on chain. And I will talk about everything that you need to know about products.
01:50:24.305 - 01:50:57.035, Speaker C: So first off, we have to remember why we're here. Before we can understand the how, we have to understand the why. Why would you want to build products on chain? Well, there's some components of being on chain that are just really valuable that help you do really interesting things. I'll go through this quickly because I'll touch on a specific couple of them. First, trustlessness, censorship, resistance. This is sort of the obvious one. It allows you to transcend the platforms that lock in our data and force you to do different things.
01:50:57.035 - 01:51:47.083, Speaker C: Security and provenance allow you to confirm both what and when something happened. Ownership and self sovereignty allow you to own digital assets and be able to transfer them in different ways. Interoperability and composability allow you to work with others collaboratively on this giant public, slow but still public database and programmable money allow you to handle things like value transfer and allow you to really interact with the broader world. Let me touch on two of those things because I think it's really important to understand what the answer to why blockchain is for every single one of you, your products and your businesses. Composability is a really interesting attribute that allows you to have different opportunities for operational leverage. I just said a lot of words. I'll explain exactly what that means.
01:51:47.083 - 01:52:27.837, Speaker C: I'll give a very concrete example of one of the best teams and one of the best examples of composability in the Solana ecosystem. There's a team named Baxis. What they do is they take high end whiskeys and they vault them, tokenize them as NFTs and allow you to buy, sell and trade them on their marketplace. But what's so interesting about this is just the fact that they have that particular NFT allows them to interact with the broader Solana ecosystem. If they want to, you know, handle a secondary marketplace, they can post it on places like Tensor and Magic Eden. Their team does not have to go and figure, figure out all the, you know, idiosyncrasies of building a marketplace or contract. They have an NFT and they can use that contract from someone else.
01:52:27.837 - 01:52:56.965, Speaker C: If you want to collateralize that NFT against anything else you want to collateralize alone, you can use something like Bridgeplay. All of these things are on Chain and allow you to reuse all of these different components. And from a company standpoint, it means that they can stay super small, nimble, able to respond to different things without having to build all of these extra pieces. Also, efficiency gains are pretty innovative too. I'll explain that. So there's a couple components to this. Here's a typical hotel reservation.
01:52:56.965 - 01:53:38.341, Speaker C: You'll book a hotel, hopefully a few months in advance, but you don't really get that payment to the hotel. The hotel gets paid when you stay, or even later than that because the credit card company has a settlement window of 30 days. So there's a lot of where that money is sitting around basically not being particularly useful. With crypto, you can actually squeeze that down and do something. I heard this term yesterday. I really liked it, the stablecoin sandwich, where in this case, on both sides of it, the hotel is getting Fiat, the credit card company is paying Fiat, but in the middle, the hotel is actually willing to give you a 10% discount because you're paying instantly. And that instant payment happens through a liquidity pool.
01:53:38.341 - 01:54:17.055, Speaker C: Right. These are the kinds of efficiencies that are pretty tough in the web2world because of how expensive payments are, but are dead simple in the web3world. Okay, so we've talked a little bit about some of the interesting components of building products on chain, but why should you build that on chain? On Solana? Well, let me also back up a little bit there. Why do we count in tens, 10, 20, 30, 40, 100? Well, we have 10 fingers. It's really easy. Form follows function. You have the tools available to you, so that's how you end up, how that informs, how you end up building things in the real world.
01:54:17.055 - 01:54:46.375, Speaker C: For the last seven years, Smart contract development on chain has kind of looked like our life with horses. You spend a lot of time, you love this horse, they grow with you, but sometimes they have a bad day, you can't make it to the, to the farm. You got to deal with having this horse and sometimes you're just not going to make it right. And hey, sorry, I can't visit you. It's going to take three days to get there. And obviously this worked great. You had a lot of people building around horses and getting from A to B in weeks.
01:54:46.375 - 01:55:14.071, Speaker C: But then came something called the car and it was a little rough. Right. Everything was bumpy. There was no paved roads, who knows what gas is, all that sort of stuff. But it was sort of very clear that this car was just infinitely better than the horse on Solana. What we are, we're the car, right? And over the last couple years, we've spent a lot of time building infrastructure. The infrastructure is ready.
01:55:14.071 - 01:55:45.085, Speaker C: This is where we are today. We have these highways. We have all of the infrastructure around it. And now instead of thinking about your horse, Roach, instead of thinking about your horse, now you're just thinking about, how do I get from A to B as quickly as possible? Form follows function. And when the form is fast, easy, interoperable, and on Solana, cheap, then you start building more interesting products. As a result, the technology falls away and you can do the thing that you want to do. So back to the talk.
01:55:45.085 - 01:56:15.265, Speaker C: So we talked about, okay, how do you build better products on chain? I'm going to start this off with the most important framing for how you build products on chain. And this is going to come from the goat himself, Steve Jobs. You've got to start with the customer experience and work backwards to the technology. You cannot start with the technology and try to figure out how you're going to sell it. This is super critical. You must talk to your users. You must talk to people and understand exactly what they're dealing with.
01:56:15.265 - 01:56:40.721, Speaker C: In order to solve those things, I'm going to give you a couple tips on how you can use the form of Solana to really inform how you build and deliver value to your users. First thing is deliver value immediately. And there's a couple steps here. How many times have you gotten to a site? And the first thing you see, it's completely beautiful. And right in the middle is a Connect wallet button. Now, this implies a lot of things.
01:56:40.753 - 01:56:40.903, Speaker A: 1.
01:56:40.936 - 01:57:14.145, Speaker C: 1 is you have to be on desktop, right? You have to have some sort of extension or maybe in your mobile wallet. It also implies what happens when you connect to it and it doesn't have Solana support. There's so many problems with this. These are each friction points and your end user is going to drop right off, right? So what if instead you took advantage of the fact that we have this public blockchain, we have all this extra data. Why not preview another public key? I kind of want to see what it's going to look like. Show me the value that you're going to provide to me. Before I have to get all this stuff set up, I need to, you know, scramble for my desktop.
01:57:14.145 - 01:57:56.871, Speaker C: I should be able to See it on my mobile browser and really understand what's going on. Or maybe use a browser stored session key and graduate to self custody later. Why not just keep it there and say, hey, look, you've gotten to a certain point, you've gotten a little bit of value. Now you can graduate, get out into your own, your own system that you have pressed. You might also think about getting rid of the paywall, gas tokens, protocol tokens, even you know, Solana, the native token itself, they're all blockers to someone getting started. Because guess what, I accidentally connected my wallet that has nothing in it and now I can't use your app. You just lost a user and there's not that many of us to begin with.
01:57:56.871 - 01:58:00.783, Speaker C: So use systems like the transaction relays on Solana.
01:58:00.879 - 01:58:01.827, Speaker A: Very easy.
01:58:01.991 - 01:58:06.043, Speaker C: Use the new technologies like ZK compression and across the two of them you.
01:58:06.059 - 01:58:35.625, Speaker A: Can get on Chain for less than a tenth of a cent. Right? And at that cost point you can sponsor those interactions both for first time users and maybe throughout the entire lifetime of their interaction with your application. Also to take advantage of there's other information on the chain. You already know who I am when I connect my wallet and let's say I have all of those things figured out. You already know who I am. I don't need you to. I don't need to upload a second profile photo.
01:58:35.625 - 01:59:03.865, Speaker A: I got a bunch sitting in my wallet already. You know that I like to listen to Hachi Mugen. That should just be the background music, right? Thinking about these different components that are already on chain and taking advantage of them in the context of your application. Okay, so you've delivered value immediately. Someone has connected their wallet, everything is great. They're really figuring out what's going on. The next thing that I would recommend is to meet people where they are.
01:59:03.865 - 01:59:38.535, Speaker A: For those of you here earlier you heard a little bit about what links from Chris. But one of the things that we think about is how do we push crypto to the edge. You shouldn't only need to use crypto in a place where there's a connect wallet button. You should be able to use it on any website in the world. Links or blockchain links allow you to bring that on Chain World, the one that we're all building, and push it to every edge, every website in the world. It means that you can take a fun game like Checkmate and be able to render it right on x.com and be able to do it right when there's a new game that's happening.
01:59:38.535 - 02:00:16.475, Speaker A: So instead of having to go figure out what the website is, because I don't remember what it is, you can play it right there and play it with your friends in your DMs and you can see that call to action immediately. Second thing is to build for intent. One of the things that I don't see a lot of teams doing that they really should think more about is give me the thing that I need to do if I'm about to get liquidated. Give me one button that gets me out of dodge, please. Or you know, hey, enough multi signers have signed this proposal on the Dow. Or maybe the vote went through. I should just be able to execute it straight from my wallet instead of going through all the hoops to figure out exactly where I am.
02:00:16.475 - 02:00:53.445, Speaker A: Same with subscriptions, same with NFTs and games. These are opportunities for you to serve your user and tell them exactly what they need to do and they will do it. It will be frictionless because they're using blockchains. And it's also important to note that certain, when we say meet people where you are, some of those people are working at big businesses and enterprises. We've seen a lot of institutions joining the Solana ecosystem over the last couple of days. And a lot of what we have on Solana with things like token extensions, allows them to express the permissioned nature of their product in the broader permissionless network. The opposite is impossible to do.
02:00:53.445 - 02:01:17.703, Speaker A: But with these smart contracts, you can actually design really complex products like these institutional finance products that need that capability. Okay, so we've delivered value immediately. We are building with intent. We're building, building wonderful, great products and you're all set up for success. This is great. So this is the current state of the world today. This is what you can use in the technology space today.
02:01:17.703 - 02:01:56.559, Speaker A: But what I want to really dig out here is actually what kinds of products should we be building next? What are the areas that we should look to see? What problems exist to make sure that we can solve them? So where do we go from here? I'm going to show you two tweets. I'm not going to read them in detail, but just a couple of notes on what they're mentioning here. The final chain, abstracted experience, social recovery for wallets, buying any asset on chain. Being able to have a dead man switch or something like that. Being able to have subscriptions. You can do 90% of this on Solana today. This is not a fantasy.
02:01:56.559 - 02:02:36.287, Speaker A: It's not something that is on a roadmap for 12 months. Or years or anything like that. I guarantee you, if we sat down for three weeks with the people in this room, we could figure this out. We have the technology, we have the infrastructure, but we have to have the thought process to really understand what the product is, what is the problem and how we can solve it. A second area that we might want to look into are things like our identities. I mentioned this before with things like NFTs, but in this world outside of Web3, our identities are increasingly becoming both the mixture of the physical and the digital. A lot of our digital experience, though, and our digital identities are associated with platforms.
02:02:36.287 - 02:03:35.743, Speaker A: TikTok, YouTube, Twitter, and they're all separate. And being able to express yourself, your own identity, as an amalgamation of those things is critical to both making sure that we can transcend different platforms, but also to be able to express ourselves better, right in this Internet age. In my mind, the chain, the blockchain itself is the social graph where we can, as more things come on chain, you can have better things to express yourself with that. And lastly, in the world of things like AI, blockchains have the potential to be the ground truth of the source of data or the source of bias. And that might be something like provenance for something that's created in a digital form or might just be an objective or social consensus around a specific fact that has happened out in the real world. So let me leave you with this. When you're building products on Chain, you have to talk to people, you have to solve real problems.
02:03:35.743 - 02:04:03.017, Speaker A: You have to understand that not all our problems involve blockchain. And in fact, they might not need blockchain at all. But where blockchain is very valuable, you should absolutely take use of it and make sure that you guide them through the entire process of really understanding who they are. I'm glad that we've had Breakpoint here in Singapore for all of you to understand as well that we have a global audience. Our products are not just crypto. Twitter, unfortunately. So I think that you need to talk to more people.
02:04:03.017 - 02:04:30.655, Speaker A: You need to really understand what their needs are and make sure that you're solving their problems. Thank you. Awesome. That was excellent. It's your friendly neighborhood voice of God here and I'm about to introduce our next keynote. It's a product keynote from Finternet. Siddharth Chetty will be leading the way and please welcome him over zealously on stage right now.
02:04:30.655 - 02:05:26.973, Speaker A: It's truly an honor to be with all of you today and I'm really grateful I have a chance to Share this Internet with this audience. Let me dial back a bit and talk a bit about the journey and why are we talking to people building on blockchains and a lot of this infrastructure? Over the last decade and a half, we've been building identity systems that are used by 1.5 across the world, 1.4 billion in India. And really it was a journey of giving people agency over their digital identity, giving them an attestation back. We then architected a payments protocol called the Unified Payment Interface, which was a way to transact value from one store to any other that is used 500 million times a day by 500 million people. And then we built a data sharing system.
02:05:26.973 - 02:06:19.191, Speaker A: Again, much of what the crypto ecosystem talks about as giving control of data back to individuals and that has over 2 billion financial accounts live. None of this uses a blockchain. But very quickly over the last three, four years, what we realized is we were building highly purpose specific infrastructure. And when you build purpose specific infrastructure within the financial system, it did one thing and did it really well. But when we wanted to extend it to a larger universe of developers, larger universe of entrepreneurs, different types of asset classes, different countries, this is where it became really, really difficult. You had very high transaction costs, These were siloed systems. It was difficult to achieve interoperability, it was difficult to have composability between different asset classes.
02:06:19.191 - 02:07:21.255, Speaker A: And so over the last few years we started to look at how do we reimagine this? And so we came from the traditional financial world and started to look at the cryptographic developments that were happening. And about a few months back, me and my colleagues Nandan Nilikeni, Pramod Verma and Augustin Karstens from the bank for International Settlements released a paper in Washington D.C. known as the Finternet. And there's a second follow up paper that you should also take a look at and read about on Finternetlab IO. But the core idea behind the Finternet is one, how do we give every individual and every business complete agency over any asset that they have? These might be regulated assets like money, publicly listed securities, might be registered assets like land vehicles. It might be attested assets like carbon credits by a third party auditor, so on and so forth. Or user controlled assets like NFTs.
02:07:21.255 - 02:08:18.697, Speaker A: So how do you give people control? How do you do that in a manner that is unified for them rather than having to go to different places, different silos? And how do we build that on the back of highly universal infrastructure, but the flows depending on the asset type may be governed and may be regulated. And so if I walk you through very quickly, the high level architecture behind the finternet, central to it is putting individuals and businesses in the focus in the center of all of these activities. Second, they should have the ability to use a diverse range of applications that are out there. So these are technology companies building out these different front end experiences. This is where a lot of the traditional institutions come in. These might be commercial banks, central banks, private companies that you've started, asset managers. They should have the ability to tokenize and issue electronic representations back to these individuals.
02:08:18.697 - 02:09:04.845, Speaker A: And when they tokenize and create these electronic representations, they may, depending on the type of asset, continue to be in control of the flow, to set certain limits, KYC norms, capital control norms, flow limits, so on and so forth. And so therefore token managers play a vital role even when you think about recoverability, consumer protection, dispute resolution. And so this is really a way to bring a lot of the use of vocabulary here. Real world assets on chain. And when it comes on chain, we have been big believers in it being issued straight to the use. And they could host that on any universal ledger that they may have. These ledgers would fundamentally be interoperable with each other.
02:09:04.845 - 02:09:44.271, Speaker A: So we don't expect the whole world to be building on one ledger infrastructure. They're going to build and tokenize on multiple and therefore those multiple ledgers should be interoperable and these assets should be different types. It might be, as you see here, regulated assets, registered assets, so on and so forth. And you may have different third parties getting added to the flow. These might be escrow providers, insurers, guarantors, basically adding additional trust every time a particular transaction happens. And all of this happens within the confines of a well governed framework. In our view, a lot of the existing laws, regulations can be leveraged.
02:09:44.271 - 02:10:33.871, Speaker A: People may create additional enhancements to this. And so the real idea is how do we access accelerate the transition towards a user centric digital economy? How do we leverage the cryptographic primitives that are coming out? Because for example, digital signatures allow you to have portability, the ledger infrastructure allows you to work across different asset classes and still have finality of settlements, so on and so forth. So I highly urge you to take a look at the work that's happening in fininetlab IO read the papers and do reach out. There's a lot of work that each of you are doing at different parts of the stack that can actually fit into the finternet as we call it. So thank you so much. I had Five minutes. So it's a brief overview of the finternet and look forward to hearing from a bunch of you again.
02:10:33.871 - 02:10:51.221, Speaker A: Take care. Fantastic. We keep it moving. Next up we have a technical talk. It's a double duo. It's about dying. I should have asked him how to say this.
02:10:51.221 - 02:11:15.405, Speaker A: Dindexer. That's alright. Which is about indexing Solana's on chain data at scale. I said it's a duo and it's Ahmad Abassi and Preston Narceti from Syndica. The last, the last round of force was subject but we're going to make this one, this one a really, really overpart round of applause for our two speakers. Thank you very much. That was excellent.
02:11:15.405 - 02:11:52.375, Speaker A: Hi guys, I'm Matt Hood, the co founder and CEO here at Syndico and today I'm here to talk about indexing Solana on chain data at scale. So firstly, who is Syndico? So we are building a web3 native cloud. We primarily provide RPC infrastructure as well as developer tools for Solana. We also have specialized APIs like Chainstream as well as app deployments. You guys should check it all out. We're also the team behind the sync validator that's currently in development. So definitely check that out on GitHub as well.
02:11:52.375 - 02:12:43.995, Speaker A: So yeah, indexing data on chain is not easy. The first issue today is availability, so you have to run a validator to actually get that data from on chain into your infrastructure. Second is scalability. You have to be able to scale your infrastructure as the number of on chain data increases. And finally you need to make sure that data is integral, so making sure you capture all the data that you're actually interested in. So what is indexing in the first place? So here we have some on chain events. It can be a transaction, an account update, and we want to take that data, we want to transform it in some way.
02:12:43.995 - 02:14:06.673, Speaker A: So in this example we have a transaction that occurred on chain, we want to take that transaction, use an anchor IDL to parse the events out and end up with a data structure like so. In this case the event is make order with some sample data here and then finally, once you finally actually get these events parsed out from the transactions instructions, you want to insert that into a database. So what exactly is the challenges with this? So for one, you have to manage a real time streaming infrastructure and this is pretty difficult today on Solana, especially if you're streaming in intro slot account updates, as Today it's around 1 gigabit per second worth of data changes. You also need the capability to, to process historical events. So if you're starting from a previous slot and you want to catch up to the head of the chain, you have to write a lot of code and be able to bootstrap your database to be able to do that. And then finally, as the number of on chain events in your DAPP increases, you need to be able to scale storage and retrieval of that index data. Also, observability is a big thing.
02:14:06.673 - 02:14:49.597, Speaker A: So you want to make sure that as your indexers are running, you want to make sure there's nothing wrong happening. You also want to be able to get ahead of any kind of issues occurring. So metrics, logging, all that. So we developed a solution, it's called Indexer and I'm going to pass it over to Preston to talk about it. Yeah, so like, while I was talking about sort of promise indexing, one of the things about being an RPC provider in this space is helping our customers solve some of the problems they're facing when getting data off the chain. Talking to a lot of people working on indexing this space has also been very enlightening. And one of the things we noticed was flexibility is incredibly important.
02:14:49.597 - 02:15:26.559, Speaker A: There's so many projects just being developed on Solana, there's so many protocols getting added, there's so many types of commodities, so many ways of representing all that data. We wanted to focus on flexibility. So one of the things we did was use a wasm runtime. So Indexer or dynamic indexer is a wasm runtime. You can run programs using a library and we'll kind of get into what that looks like, but it's dynamic. It lets you kind of specify how you want to observe and react to these on chain events. So Indexer is a one time platform where we call your code and we pass in every single event that you care about and we can start the data.
02:15:26.559 - 02:15:58.571, Speaker A: Just like we mentioned when starting from a past slot, catching up to the head of the chain, you can do that with Indexer, you can replay history, you can regain state in your database. Now all this streaming infrastructure, being an RPC provider in the space is already taken care of for you. You get verifiability, get all the data on here. We can also ensure that you don't miss any events and all the outputs that your indexer produces, this includes logs, this includes errors. All this gets tracked in our database. So you can always query this in regain state. You can always monitor what your indexer is doing, so you never lose track.
02:15:58.571 - 02:16:22.269, Speaker A: Of what your indexing capabilities are. And lastly the outputs. So we refer to the outputs of your indexer as the objects you're writing to your database. These are things that you're interested in indexing on chain. All of this gets sent over to our storage layer. We support Postgres, Clickhouse, and a bunch more messaging protocols are in the works. Of course I'm going to run through what indexer runtime actually does.
02:16:22.269 - 02:16:51.101, Speaker A: So it's kind of going back to what we talked about with indexing and the indexing 101 we gave. Everything starts on chain. We have on chain events. We've got transactions, we have account updates. The first step is getting this data on, off the chain and into an indexer runtime. So when you create an indexer, one of the things you're going to specify is a filter and really just represents a subset of false monastery of data that you're actually interested in. In this example, we've got a transaction filter.
02:16:51.101 - 02:17:15.362, Speaker A: It specifies the drift protocol. So you're going to be indexing all transactions that involve the drift protocol. Once the filter has selected data that you're interested in, it gets sent over to your indexer and you can start processing it. I'll show you an example of what that looks like in a second. Of course, we're going to get this for all the data on chain. So that includes every slot and every piece of data inside that slot. So every transaction, every account.
02:17:15.362 - 02:17:43.909, Speaker A: Again, this also includes a sort of data. So now we're going to kind of get into how the indexer works. We're going to set one up and we'll run through a quick example. So just to recap, an indexer has two components. The first is your WASM Rust program. Of course, since we're using wasm, you can kind of write your indexer program using our library in whatever language you want. We'll have docs on that coming up soon, but for now we're going to run through our Rust example.
02:17:43.909 - 02:18:18.686, Speaker A: So using a indexer, you can observe any on chain events you're interested in. And the second component is the indexer itself. You can kind of think of the index server as wrapping your WASM program, but attaching some useful metadata, that's important. One of the most useful pieces of the metadata that you're going to include and the filter kind of reduces down that input stream to what you care about indexing. And lastly, and most importantly, one thing we really want to repeat here is you get all the data you care about. So your indexer will see that data and you will have the opportunity to index it. So let's create a quick indexer.
02:18:18.686 - 02:18:35.535, Speaker A: Let's put one together for the Jupyter aggregator. I think this is a great example of how you can kind of write an indexer for a single protocol. But keep in mind it's really flexible. You can index multiple protocols in the same program. So let's get through this. All indexer programs are pretty simple. They start off looking like this.
02:18:35.535 - 02:19:15.145, Speaker A: You have a syndicate indexer runtime, you import the library, you just import the processor macro, and you decorate one of the functions in your project as the entry point. So this is the function that gets executed every time Our runtime has new data for your indexer index. So at this point, data has already been filtered. All you have to do is write the code that you'd like to execute whenever you have new data at this hook. So that event type is really just an enum between a transaction and an account that you're interested in. So let's get into writing the actual indexer for Jupyter. So the first step here, and I know this looks a bit scary, but I'll walk you through it, is setting up anchor.
02:19:15.145 - 02:19:47.213, Speaker A: So we're going to use anchor IDL to parse out the events from Jupyter transactions. So the first thing we do is import the jupyter IDL JSON. We also just write out the jupyter PUB key address and we set up our IDL parser object. Now, remember, the process function gets called for every single update, every single time. So we use a one slot to make sure we only set up the IDL parser once, and then just continue using that for every single event. That again pass your index. The next step is selecting the events you care about.
02:19:47.213 - 02:20:12.879, Speaker A: In this case, we're really only going to focus on transactions. So we check if the event is a transaction. If it is, we pass it to our ID on parser and we parse out the events. Once we have the events, we use the record M syscall to kind of write all of these to the storage layer. And one thing that's really cool about this is value there. The value type is flexible, so any serializable object can be put there. So you, you can do much more complicated things Right before you write into your database.
02:20:12.879 - 02:20:39.233, Speaker A: It will support multiple databases, multiple tables to be written from these programs as well. And that's it. Now you're indexing the Jupyter ip. Pretty simple. Now, one thing we've had to do to kind of build this out and work on making the dev flow as easy as possible is to build out a CLI tool. And as you can see in this example here, we're using the Syndica CLI indexer create command to create this indexer. And this is a bunch of things in the background, so let's walk through it.
02:20:39.233 - 02:21:02.325, Speaker A: But looking at the command, it's pretty simple. You give your indexer a name. This could be anything you want. We call it the jupyter indexer. The next thing to do is point to the source code where the entry point lives. In this case, in that example, it lived and lived rs. Again, we're using Rust as an example, but since we use WASM behind under the hood, you can write your indexes whatever language you prefer, as long long as it compiles out of compiles.
02:21:02.325 - 02:21:27.883, Speaker A: And then the last thing about this is the transaction filter. In this case, for filtering by programs, we only care about transactions that involve the Jupyter program. Hence WebKit. And there's a couple things that happen under the hood. First, this compiles your indexer to wasm and then submits that WASM to our infrastructure. And it immediately starts executing. And there's going to be entries and events to your database.
02:21:27.883 - 02:21:54.219, Speaker A: So let's look at an example of how to pull that. Using this nipple cli, you're able to get your outputs. You simply provide the same name of the index you're interested in getting outputs for. You can then supply the slot range, specify which subset of the data you're interested in looking at. So that way it's not overwhelming. And one thing to go along with the CLI is also the API. So if you want to write your own infrastructure around Indexer, you can go ahead and do that.
02:21:54.219 - 02:22:24.429, Speaker A: We have all the docs available for that as well. You can also use our platform to see what your indexers are using at any one point in time. There's a nice graph there showing you all the events coming out. You can inspect the events and see what's going on with that too. Now, one thing I really want to conclude this presentation on is the availability guarantees the flexibility we offer through the indexer program itself. Now, we can guarantee all those things by leveraging our existing infrastructure. On the point of availability, we run multiple validators.
02:22:24.429 - 02:22:53.763, Speaker A: We have a lot of experience doing this. We're globally distributed. We also consume multiple guides or streams from multiple nodes in each of the regions. So this gives us availability guarantees uptime guarantees your indexer is never going to go down scalability. So on the point of scalability, we spent this last year optimizing our hardware, getting custom hardware into data centers, improving our networking. We've got incredibly powerful NAICS guys. So as a lot of scales, as more protocols get added, as there's more units index, we're ready to scale to meet those demands.
02:22:53.763 - 02:23:21.909, Speaker A: Same point on our storage. We've got plenty of storage hooks coming up down the pipeline. Clickhouse and Postgres have been some of our favorites to work with, and we're even planning on supporting messaging protocols. Redis, Kafka, whatever. You guys need integrity. Now, one of the things we've worked on is solving the problem of missing updates coming from certain validators. So we consume multiple streams from multiple validators and make sure that data reported on one, but now report on the other gets filled in.
02:23:21.909 - 02:23:51.775, Speaker A: So that way your indexer isn't missing any updates. The data getting pushed to your indexer isn't coming from one validator, it's coming from multiple. And you're missing data is getting filled in. The other thing and the final aspect I'll touch on is historical events. You can restart from Genesis and see all the events you need. Capture index or database data from scratch, get to where you want to be captured to the head of the chain and just have all your data ready to go. And currently we only support transactions, but we're working on a solution for accounts as well.
02:23:51.775 - 02:24:12.735, Speaker A: We're happy to announce that the indexer is enclosed beta. We'd like to get you guys onboarded. We'd like to help you guys get started on indexing, so come over to us. Our booth is just outside and right around the corner if you guys are indexing or doing any indexing in the space. I'm sure a lot of you guys are. We'd love to hear your problems, hear your thoughts on this. So come find us, come talk to us.
02:24:12.735 - 02:24:34.331, Speaker A: Of course. Lastly, we're also the team behind Sync Validator, so definitely scan that first QR code to check out the source code. A lot of cool things happening there. And we also have our blog. We go into a lot of depth about how our validator works. A lot of our design decisions definitely check that out too. We have blog posts on infrastructure and crypto research in general.
02:24:34.331 - 02:25:09.755, Speaker A: So yeah, thank you guys for having us. I hope everyone's having an amazing break point. Thank you. You can not leave. Leave as you deem fit. We want you back here in exactly five minutes and ready to go for our next talk, which will be and forgive the clunkiness here cranking up Franken Dancer, whatever that means. See you back in five minutes or so.
02:25:09.755 - 02:31:39.257, Speaker A: It it Philip Taffet and Anwe Day from Fire Dancer will tell you all about that. Please a nice zealous hi, I'm Philip and today together with my colleague Onwe, we're going to answer one question for you. How fast can the Fire no Team Stack go? Since the beginning of the project, high performance has been one of Fire's main goals. But why should performance even be a goal? Well, as Kevin mentioned in his talk yesterday, having a high performance validator increases the capacity of the chain, and performance of the sort we've been working on unlocks new possibilities for what can be done on a permissionless blockchain. And plus, performance is kind of our team's expertise. So for the next few minutes I'll talk a little bit about some of the decisions we've made to get good performance with Fire and then I'll tee up onwe's demo and then onwe will crank up Frankendancer, showing how fast Fire Networking Stack can go. Alright, diving in immediately to the architecture.
02:31:39.257 - 02:32:30.825, Speaker A: If you've listened to talks or seen our presentations over the last two years, you've probably seen a lot of diagrams that look kind of like this. Boxes and arrows and more boxes showing how data flows through our system. The reason we always depict systems like this is because that's how we think about that. As Kevin has mentioned several times, getting the data flow right for a system is one of the most important decisions in getting good performance. Here the boxes represent tiles, a finance or concept that's a thread pinned to a CPU core, running the same code over and over again as fast as possible, receiving some kind of input data, doing some operation on it, and then providing the output data to the next tile. Another big benefit of the tile architecture is that it allows us to scale stages of the pipeline independently. So for example, here you can see there are a lot of signature verification tiles.
02:32:30.825 - 02:33:27.025, Speaker A: That's because signature verification is one of the most expensive operations in a validator, so we can add extra cores to that to get to balance the overall system throughput. I'll say a word or two about each tile and some of the decisions or the changes we've made over the last few years to get good performance will follow the flow of a packet as it goes in from the network. So first, when a packet comes off the networking cable, it meets the NET tile which handles interactions with the network interface card. From there, assuming the Packet is a TPU packet, a new transaction submitted by a user with hopes of a landing on chain. It goes to the QUIC tile, which runs the QUIC protocol. We've improved the performance, compatibility and reliability of the QUIC tile over the past year, especially in the case to prevent denial of service attacks. From there, the new transaction goes to a signature verification tile which verifies all of its signatures.
02:33:27.025 - 02:34:26.033, Speaker A: We switched the signature verification tile to use the AVX Accelerated ED2 5, 919 implementation that Kevin talked about at Breakpoint last year. Core 4 Core. This roughly doubled our performance. So if you compare with the performance only about 30 now. From there, assuming that the transaction is fully verified, it funnels to the dedup tile, which throws out any duplicate transactions and then the deduplicated transactions funnel into the pack tile. The pack tile takes a look at or examines each transaction and looks at the accounts that it writes, the the number of compute units, a request, the fees that it's going to pay, and the consensus limits to decide when we're weird which transactions should actually go into. The block pack is a pretty important component because it's one of the few parts of the pipeline that's on the critical path, but doesn't scale wide, at least not easily.
02:34:26.033 - 02:35:01.545, Speaker A: You can't just add another pack tile. So we've put a lot of effort into improving the performance of the pack tile, pulling out all the stock, all the CPU tricks like non temporal memory copies and AVX accelerated bitsets. And in a recent demo, we showed the pack tile operating at 10 gigabits per second. From there, the transactions that we decided to include in the block go to one of many bank tiles named for the Agave banking stage. For Frankenhancer. These actually use Agave's code. They use the vm, the transaction execution, the count status database from the Agave runtime.
02:35:01.545 - 02:36:15.735, Speaker A: There's a huge amount of complexity hidden in that little word bank, and most of that complexity in optimizing it is outside of the scope for Frankendancer, and it's just the scope for Full Fire Dancer. So for Frankendancer, the changes that we've made, the optimizations that we've made have mostly been eliminating small memory copies and reducing the cost of data marshalling. We've actually gotten a lot of mileage out of these. We've also put a lot of effort into parallelizing or scaling out the pack tile, making it more scalable so that assuming we have transactions that have a lot of parallelism available, we can keep this a Lot of bank tiles fed, so that even if the performance of one individual bank tile isn't necessarily what we'd like, we can get good overall performance through many of them, good aggregate throughput. From there, to execute a transaction, flow to the POH tile, a proof of history tile which does some hashing and basically stamps them and orders them. And it sends the incrementally constructed block to the shred tile, the shred tile which chops up the block into small packet sized pieces called shreds and sets them out. We've updated the shred tile to use the AVX or the high performance Reed Solomon library that I presented at Breakpoint last year.
02:36:15.735 - 02:37:18.825, Speaker A: And we also added a new heavily optimized data structure for computing each shred's destination, which is also one of the most expensive operations in a validator. From there, the shreds flow to the block store, which or to the NET tile which sends them out to the rest of the network and to the block store, which we haven't optimized for. Frankendancer and just uses the McAve's code. Frankendancer is mostly about optimizing the leader pipeline, so when we're not leader, most of the Frankendancer tiles or the fragrancer tiles stay out of the way. The one big exception to this is that shreds that come in still flow through the net tile and through the shred tile where they're verified, retransmitted and reconstructed. But the rest of the pipeline, things that are not part of the leader pipeline, like repair and gossip and voting, those all run concurrently in the background using your stock agonic code. Okay, that was a whirlwind tour through a lot of tiles and a lot of decisions that we've made.
02:37:18.825 - 02:37:50.435, Speaker A: But I want to sum up. The keys for getting good performance are open secret with just three concepts. The first is streamlining data flow. This is a key to get right because copying data around is often the most expensive operation of a modern cpu. From there, we need to exploit parallelism in the right granularity, which is as coarse as possible, and in the right spots. Parallelism is a bit like fire. When it's in the fireplace, it's nice, it keeps you warm, but if it's too much or it's in the wrong place, well, it'll burn you.
02:37:50.435 - 02:38:35.205, Speaker A: And then finally, we want to optimize each of our individual components to the degree. Here's where all the low level CPU tricks like the non temporal memory copies and AVX acceleration and kind of More standard things like producing unpredictable branches or having cache on of cache friendly data layouts. All these factors in because when you can optimize each individual component to a great extent and then you combine them with two other points of having good data flow and lots of parallelism, then you get good overall system performance. Okay, enough talk. Now let's get into the demo. Actually two demos that Hanwei is about to show. So both demos will follow the same basic pattern.
02:38:35.205 - 02:39:03.575, Speaker A: There will be a load generator producing loudly signed simple transfer transactions. These will transfer a few land ports from one of 7,500 source accounts to one of 7,500 destination accounts. These are, you know, it's the native system transfer. So these are native programs. They don't require spinning up the BPF execution vm. They're a little bit cheaper, but they'll still show the full execution pipeline. The load generator will send a huge number of these to the leader.
02:39:03.575 - 02:39:57.446, Speaker A: When they arrive at the leader, they'll go through that whole pipeline that I described, from the. Net tile to the shred tile, and then the leader will send out the shreds to one of nine or to all nine floors. But we wanted to do this demo with not just some network that looked like these abstract blue arrows, but with something that looked like the real Solana network. So we took a look at the current mainnet validator distribution and you can see it's something like a big concentration of nodes in North America, another big concentration in Europe, and then a smattering of nodes elsewhere around the world, including a fair number here in Singapore. Given the constraints of where we have available nodes, what we're going to use for the demo is this. We'll have the load generator and the leader running in London and then we'll have followers running in New York, Frankfurt and here in Singapore. For this demo we're just using 10 nodes, but we've actually done demos like this, pretty similar to this on 100 nodes.
02:39:57.446 - 02:40:49.125, Speaker A: All the nodes will have a 10 gigabit network interface cards, but the connections between the data centers will use JMP's global network, which is 100 gigabit WAN. The actual nodes themselves have dual 40 core isolate sockets and they're using LNQ Linox Connect X5 10 gigabit NICS. Each node has 512 gigabytes of RAM, though we won't need that much for the actual demo then. The software itself is open source, publicly available. It comes from the BP demo branch. You can see the commit hash up there and it's basically similar to our main branch with just a few changes that we've made to eliminate non networking phone packs. So the point of this demo is to show off the networking stack's performance, but you actually can't just have a chain that's just a networking stack.
02:40:49.125 - 02:41:31.885, Speaker A: If no one votes, the slot doesn't progress and there's actually no chain. So what we're showing is almost a real cluster, but we've disabled a few components to prevent them from getting in the way of showing off the networking stack's performance. For example, we're disabling writing to the block sorter other than what's necessary to get the chain off the ground. Again, the block store is outside the scope of optimization for Frankendancer and it's a non networking stack component. Secondly, we're disabling the status cache which is a part of the agave banking stage and our banking tiles. It's a component that's necessary in production, but it limits the scalability of the banking tiles. And again, it's not part of the networking stack.
02:41:31.885 - 02:42:15.811, Speaker A: Compute unit rebating is a bit of a trade off. You can choose to pack transactions based on the executed CUs, which in some cases can let you pack a little bit more and lock, or the requested CUs, which is a bit cheaper. In this case we control all of the transactions that we're sending, so we're not in the case where transactions are over requesting cus. So it actually ends up about the same. And so for the sake of performance we decided to pack based on requested CUs. We've turned off CU rebate and fourthly, we're ignoring the replay stage performance. Again, you can't have a multi node cluster without having, you know, a bunch of non leaders and these non leaders are running the replay stage.
02:42:15.811 - 02:42:53.243, Speaker A: We didn't want to do some kind of crazy surgery to sever out the replay stage or something like that. We didn't want the replay stage to limit our performance either since again it's a non networking component. So we're going to ignore its performance by measuring for the followers, measuring their performance at the edge of the boundary where the Franken answer code connects to the agave banking or the agave replay stage code. So this will be as transactions or as shreds. Leave the shred tile and finally one more bit of fine print. What we're going to show here is a pre recorded template. We actually did this demo live yesterday.
02:42:53.243 - 02:43:37.505, Speaker A: I think some of you were there and we've done this demo or Variance of it, I don't know, three or four times at this point, including the one that I mentioned that's on 100 nodes. But given the fact that we don't have a podium, the constraints of giving a talk like this, the risk of having a freak accident like a hardware outage or a sub C fiber going down, we decided that a pre recorded demo was a better fit for this talk. You'll still be in for a treat of a demo though. With that I'll turn it over to Anwe. Thank you Philip. Ok, so first I would like to show you a little bit of network statistics about node cluster where we're running this demo on. So like Philip said, these nodes are distributed all over the world.
02:43:37.505 - 02:44:30.841, Speaker A: I'm going to ping a node in New York from a node in London to show you the latency between them. And we can see that we have about 67 milliseconds of latency. We're going to repeat this and we're going to ping a node in Singapore from the same node in London. And let's see, it's about, it's 175 milliseconds which is a little bit higher but not too bad I guess. And then we're going to close the circle and we're going to ping a node in New York from a node in Singapore and that is about 230 milliseconds, which is roughly the sum of the two other ones that we saw. And yeah, so we have followers running in all these locations. And then before I actually begin the demo, I want to talk a little bit about the configuration that we're going to use for the leader and the follower.
02:44:30.841 - 02:45:13.771, Speaker A: So first, the leader we have a few things like the username on the left hand side, the username, the port ranges that the Franken Lancer instance is going to connect to the directory where we're going to store the ledger. The layout is particularly interesting. It is unique for the leader, you'll see that we give most of the CPU cores on the box to Frankendenser tiles and we have about 33. We have 33 verified tiles, 19 bank tiles, four quick tiles and five shred tiles. We're also using 7,500 accounts that are preloaded into the genesis for sending all the transactions. If you look down a little bit, we have a couple of parameters. Larger max cost per block and larger shred limits per block.
02:45:13.771 - 02:45:46.741, Speaker A: We set those two to false to begin with. These are consensus parameters that I'm going to talk more about when I actually set them to true for the second part of the demo. But just to note that it's there. And then we have the. We disable the status cache, we disable the block store after slot 150 so that we can do a snapshot so the followers can join the cluster. And some more parameters, log files, RPC parameters. Now moving on to the right side, we enable RPC so that we can query for the transactions that the cluster has processed, the leader has processed.
02:45:46.741 - 02:46:12.393, Speaker A: But we disable some of the extended functionalities. We do a snapshot at slot 100 so it's quicker for the cluster to bootstrap. And then finally some more parameters for the. Net tile, quick tile, shred tile. We also, you can see we disabled the CU remaining like Phil mentioned for the pack title. For the follower we have the starting is pretty similar. We again have user port ranges, directories.
02:46:12.393 - 02:46:48.083, Speaker A: The entry point gets templated out based on where the leader is going to run. In this case it will be in London. The layout you'll see is significantly different. We actually give most of the CPU cores on the follower to the agave runtime to the Agave replay station, everything. We don't have as many Franken dancer tiles, we just have like two bank tiles and two verifies. But we do have three shred tiles and as Phil mentioned before, that's the main component of the follower that is a part of the frankendancer networking stack. And then we have some more options for rpc and on the right hand side this file is much shorter so there's a bigger repeat.
02:46:48.083 - 02:47:24.737, Speaker A: But we have, if you look at the bottom part we have the disabling block block stored this time from slot one because the followers don't need to create a snapshot. And the status cache is disabled as well. The two max cost and shred limits per block we set it to true. This one is a consensus limit that determines how much we pack into a block and because of that it mainly matters on the leader. So I'm just setting it to true here on the follower for convenience reasons so I don't have to go and edit the file twice when we actually change it to true. Okay, so with that I'm going to go ahead and start off the demo. And here we go.
02:47:24.737 - 02:48:01.103, Speaker A: We'll I guess show you that these are the leader and follower files that we're going to use. Actually when I recorded this I realized that this is too fast for all of you to read. So I put a screenshot previously so I can explain what actually is in them. And then we have the follower one over here. And then we're going to start with the leader, which is just a matter of copying over the tol file onto the host that we're going to run the leader on. And then we start up the leader again. This is a host in London with the ftdev binary, which compile Frank should be familiar with.
02:48:01.103 - 02:48:29.765, Speaker A: And then we're going to examine the cluster that we just started off with. A command that most of you should be pretty familiar with, the Solana epoch info command. This shows us that a cluster has started up from epoch zero. The slot height and block height is slowly ticking up. This is fresh, not maintenance tested or anything like that. And we're going to join all the followers to this new cluster that we just started off. Okay, so we're going to join the followers now, which is a pretty similar process as the leader.
02:48:29.765 - 02:49:09.001, Speaker A: You just go ahead and copy over the file this time you create some identity keys and then we join it to the cluster that just started on. So this time again we'll use a command that should be pretty familiar as well, some on our gossip. And we see it starts off with just one node, which was the leader which started up. And then the other nodes need to wait for the leader to do the snapshot, download it, bootstrap and then join the cluster. And it's going to take a little bit of time because the nodes are in different parts of the world and so it takes different amounts of time to download the snapshot. But at the end we should see 10 nodes with one leader and nine followers. And there we have it.
02:49:09.001 - 02:50:05.931, Speaker A: So now that everyone has joined, we're going to go ahead and start up the load generator. This again uses the FDEF binary and the tile architecture that Philip mentioned. Except these tiles are going to be sending transactions to the leader and once it has started up, we're going to go ahead on the load generator box and, and take a look at how much transactions are we processing. If you run Frankendancer, this output should be a little familiar. All the tiles starting up and then we see we have a pretty steady transaction rate of 81,517 transactions per second. Just a reminder that this is actually transactions that go through the entire pipeline through the PAC Verify Dedupe Pacific, it goes through the bank tiles, gets executed, sends out a shreds to the followers, and we confirm these transactions through RPC and then average it out. But this number should not be too surprising.
02:50:05.931 - 02:50:40.895, Speaker A: This is essentially the number of simple Transfers system transfers that we can pack into a block times 2.5, because there's about 2.5 blocks per slot. And so this is where two, this is where we're at. Given what the current consensus limits are, we cannot process more than this. I want to also show you a little dashboard that we have that kind of maps the traffic that we see on these hosts. So this is mapping the amount of traffic that is going out of the network interfaces on all of the nodes we're running this demo on.
02:50:40.895 - 02:51:15.101, Speaker A: And all the different colors represent the different hosts. And it's kind of stacking up to show us the total bandwidth that the network is using. And we can see that this starts up with some megabits per second when the flowers first start up showing the cluster downloading snapshots. But then it goes up to a few gigabits per second, which is essentially when the load generator starts up and we're actually sending transactions. And again, I intended to show you what the actual amounts per host are, but that was too fast. So we'll go to some straight screenshots after this video is done. So this is the leader.
02:51:15.101 - 02:51:48.653, Speaker A: We're processing about 724 Mbps on the leader, as in we're sending out. These are the shreds that we're sending out to the followers. This is the load generator, which is at a surprisingly high 2.37 gigabits per second, way higher than all of the other ones. And the followers are retransmitting the shreds at about 340 Mbps is the bandwidth that's going up. So this is what we have right now with the current consensus lines. What we want to see is what happens when we crank these up.
02:51:48.653 - 02:52:25.581, Speaker A: So what I'm going to do is I'm going to go ahead and change those two parameters that we saw before, starting from the same. But before that, I'm going to stop the existing cluster because we need to make sure that everything goes out and we do a new genesis with the new consensus limits. So once all of that has stopped, we're going to go ahead and edit that TOML file, the leader one that I showed you, and we're going to edit the two consensus parameters. Oh, not that one. Go back, go back to the. Yeah, those two. So we're going to edit those and set them to true, and we're going to restart the whole cluster.
02:52:25.581 - 02:53:26.115, Speaker A: So again, the leader, the follower, the load generator, and once it. It's going to take A little bit of time to start up. And as it does that, I'm going to explain what those two parameters actually do. So as it's doing that, so the larger max cost per block essentially raises the max CU limits that we have in a block from 48 million to 13 times 48 million, which is roughly about slightly higher than 600 million. And then we have another limit of 32,000 shreds, parity shreds that you can put in a block, and that we raise to about a million parity shreds per block. So those are essentially what those two setting them to true goes ahead and modifies that max limit and then we restart the whole cluster up with those new limits. Because what we want to see is how fast can the Frank Intent Answer networking stack go if we've already hit the consistent limits, how much higher can we go? So the cluster has started up and I'm going to move on to show you what the actual transactions that we're processing right now are.
02:53:26.115 - 02:54:16.749, Speaker A: Okay, let's see. We're going to kill the same file that we saw last time, and this time we see that we're processing about 1.04 million transactions per second. This is so again, these are transactions that are going through the entire leader pipeline. All the different tiles that Phil talked about through the runtime being executed sent out to shreds and confirms our PC. We also want to see how we're doing on the followers and we want to measure the performance of the follower network and stack, which is mainly the shred tile. So the shred tile emits a metric that says how many transactions per second it processed based on the number of shreds it receives.
02:54:16.749 - 02:54:51.935, Speaker A: And also given that we know the type of transactions that we're sending in. So the top one over here is a node in Frankfurt, the middle one in New York, and the bottom one in Singapore. And we can see that they're all processing about 1.04 million transactions per second, roughly the same as what the leader was sending out. So their networking stack is also keeping up with what the leader is sending out. It is worth talking about what is our limit right now we have raised all the limits up. So we have a couple of points here that we are aware of.
02:54:51.935 - 02:55:42.795, Speaker A: The nettile is ingesting a ton of transactions that load generator is sending to it, and on top of that it's sending out a whole bunch of shreds to the entire cluster and that overloads it a little bit. And we can have her mitigate this by adding more nebtouts and that is something that is pretty easy to do. And another bottleneck that we have is the pack tile and I think that maxes out at about 1.08 million transactions per second. And we are actively looking into doing further optimizations to see if we can crank this limit up even higher. I also want to show you a similar bandwidth graph for second demo. And this is the load generator start sub and we're already processing a significantly higher amount of traffic.
02:55:42.795 - 02:56:23.387, Speaker A: It's going to take some time to settle down, but it seems like this is going to total out roughly to about 42 gigabits per second. Again, trying to show you the points of interest here, which is again too quick. So, so I'm just going to go to the screenshots and we have the leader now processing sending out about 5.12 gigabits per second worth of traffic, way higher than what it was before. The load generator is still at the 2 gigabits per second that we saw before. So now we know why this was the case because we were always sending all these transactions. It's just the consensus limits didn't let us process it all and send it all out.
02:56:23.387 - 02:56:50.935, Speaker A: But now that we can process it, were sending everything out and the followers are now at about 4.3 gigabits per second. So they're just retransmitting the shreds that we all saw. We can compare the two graphs side by side. On the left hand side is the one with 81,000 TPS. It's roughly about the total bandwidth usage is about slightly less than 6 gigabits per second. On the right hand side it's slightly around 42 gigabits per second.
02:56:50.935 - 02:57:29.495, Speaker A: That's the. And if we take off the bandwidth that the load generator is sending out, that's roughly about 3.8 3.7 gigabits per second for the 81001 and about 39.8 gigabits per second for the million GPS one. And we can see that that's also roughly a factor of 12, which is pretty similar to the 81,000 to a million ratio. Okay, so to summarize, we can see that the Fire Dancer networking stack can scale up to processing about a million transactions a second with system transfers.
02:57:29.495 - 02:58:07.485, Speaker A: And it is also worth noting that this uses the entire leader pipeline. So this is using the agave runtime and the banking stage and executing all those transactions. So that is performant enough to keep up with the load that we're processing here. Although the current Consensus limits that we have on mainnet and testnet and also the non networking bottlenecks that Philip mentioned that are kind of necessary to keep the chain running are fully functional. These numbers can be achieved on those networks. But to kind of answer the question that Philip started this presentation with, that is how fast the fairer answer networking stack can go. Thank you so much.
02:58:07.485 - 02:58:54.485, Speaker A: Fantastic. Right, final talk before lunch, so please stick around and then fill your bellies with this. And then actual food it is. Scenes from a Fire Dancer validator. We have Ayman, Jane and Liam Heger, all from the Namesake at Fire Answer. So please last four lunch, last of my session, one more big round of applause. Hello everyone.
02:58:54.485 - 02:59:13.465, Speaker A: Waiting for the slides. Here we go. Hello everyone. I'm Liam Hager. I'm a blockchain engineer working on Fire Dancer. Today I'm joined by Aryaman Jain, another engineer working on the team. This is the Fire Dancer team's last Talk of Breakpoint 2024.
02:59:13.465 - 03:00:25.415, Speaker A: You've hopefully had your Philip talks from us, but now it's time for the dessert. For our last presentation, we'll be showing off the full Fire Dancer client. But before we get into that, we're excited to announce again that the full Fire Dancer validator client is live on Chestnut. That is to say, this is the full Fire Dancer client, written purely in C, contains no Rust code, and by extension shares no code with a copy provider known as State, actively participating in consensus and producing blocks. We'll get into the specifics, how we got to this point, but first let's touch on what Firedancer has been up to on testnet. Just the last few months, we've gone from not operating on testnet at all to building tens of thousands of blocks and voting on over 10 million blocks, all the while growing to 1% of stake. Continuous uptime has gone from a few minutes to several days, and similarly, runtime discrepancies between Firedancer and Agave used to appear once a day and now appear less than once a week.
03:00:25.415 - 03:00:57.059, Speaker A: All this has come together in just the last 12 weeks. Over the last two years, though, we've built dozens of components into Fire, from networking to runtime. We've been extremely busy engineering, integrating and testing all these pieces. It would not have been possible to get where we are today without the village of engineers, staff and external partners working on Fire. And so. But enough of that. The goal of this talk is to show you that we indeed have a functional Validator client and that it performs well on testnet.
03:00:57.059 - 03:01:40.465, Speaker A: So let's get into that there are two acts to today's presentation. The first act will show you proof that Fire answers are behaving as a good validator client on testnet. To be a good validator, a node must be able to vote on blocks in a particular manner and produce blocks when it is the leader. We also stipulated that having a usable RPC interface is crucial to the functional validator, as that service fulfills many functions for operators. The second act will show Firedancer's performance in two different environments. First, we'll show off a few interesting blocks that Fire builds on testnet. Second demonstration will show what Firedancer is capable of when pushed to the max.
03:01:40.465 - 03:02:17.261, Speaker A: Those experiments were run on a globally distributed cluster composed exclusively of Firedancer nodes. So without further ado, let's look at how our validators performing on testnet. This site may be familiar to a lot of you. This is validators app by BlockLogic. There's a plethora of useful information about Solana Validator Set. This information is useful to both operators and stakeholders alike for determining the effectiveness of any given validator. Here you can see our Testimonials page and just a note, there will be QR codes throughout the presentation linking to many of these things we're looking at here.
03:02:17.261 - 03:02:51.895, Speaker A: Don't worry if you miss any of them and there will be a final QR code at the end with everything in it. If you scroll down on our page there are four charts for our validator. Each of these charts describes an important metric for all validators on the Solano network. Two rows of charts match up to the two measures that define an effective validator. Let's look at that first row. Here we see two metrics, root distance and vote distance. You can think of these metrics as both as slightly different measures of how far behind the latest block we are.
03:02:51.895 - 03:03:43.905, Speaker A: Effective validators have a lower root and vote distance and are therefore closest to the tip of the join. They also vote for as many blocks as possible. Not only does this ensure that validators contributing in a healthy managed consensus, but it also means that they are voting at the necessary cadence to earn as much of the voting awards as possible. And as a sanity check, here are those same statistics up against several of our peers on testnet. The hypothesis is that if we are ineffective at participating in consensus, we should have metrics at least on par with or or better than our peers. So there we are towards the bottom of the list and above and below the Fire Dance and Validator are healthier. Calvin nodes leave the Final judgment to you on how similar they look in his second, but to me they look pretty similar.
03:03:43.905 - 03:04:18.585, Speaker A: Okay, so let's look at the second row of entries. The autographs tell us how the Fire Dancer boundaries perform, but it is the block producer. The left graph measures the percentage of our nodes missed opportunities to produce a block during our leader slot. The right graph is similar, but it shows the rate at which the leader after us was skipped. The top line of both graphs is the cluster average skip rate. Missing many blocks or being the cause for someone else to miss their blocks leads to less blocks visibility in the whole cluster. It also can result in reduced profitability.
03:04:18.585 - 03:04:57.299, Speaker A: Profitability for the validator or other cluster participants. For those reasons, it's also good for these numbers to be as low as possible. Again, I'll give you a second to look at this, but if you look at some of our peers, you can see we're pretty much in line with their current skip rates. Okay. And so you can see we're doing pretty well. So there's one last metric we actually have to look at here, and this is vote latency. Vote is the number of slots between when a validator has generated a vote transaction and the slot that it's actually included in.
03:04:57.299 - 03:05:31.181, Speaker A: A block. Validator's voting lane is not keeping up with consensus. This results in their votes being less valuable for resolving forks and finalizing blocks at a time. And again, for this metric, latencies closer to one are always better. Last, on the critical items for validators of rpc, here's a video of the Solana Explorer connected to the RPC service within our Fire Dancer validator. Normally the site is connected to a public RPC endpoint. We redirected it to R Node to demonstrate its capabilities.
03:05:31.181 - 03:06:11.735, Speaker A: You can see that we're populating all the information that's pretty interesting about Speedpoint, from cluster statistics to block and transaction information. Just for fun, we also tried Salon fm, another Solana Explorer. You may have noticed some missing information on both sites. This is because the RPC service is not perfect yet. We're still working on RPC support and only recently started really putting it through its basis. So, based on these three markers of a good validator, you can see we've done a pretty good job so far in all of them, participating consensus in block production in an effective manner and preserve data from the node that is useful for end users. Now let's talk about those performance demonstrations.
03:06:11.735 - 03:07:34.525, Speaker A: Up first are three demonstrations of large blocks Firedancer built on testnet. All three of these blocks were Built by sending transactions to a specific type to Fire Answers Transaction ingest service While we were the leader, the purpose of this test is to demonstrate Firevents could perform well when running on an existing protocol and on existing clusters of Majority, Agave and Frankdancer nodes showing both transactions per second and compute units consumed per second for these blocks Quick Refresher Compute units are a resource that transactions request and are consumed whenever a transaction does something. Slot and blog producers have limits on the size of blocks that they can produce. For example, the maximum number of compute units the block it consumed is capped at 48 million or about 120 million per second. So with our transaction generation script, firedancer built this first block a few weeks ago for just under 36,000 transactions. Almost entirely composed of what we are calling basic transactions, these transactions don't really do anything beyond request a minimum Compute units set a price for the transaction and pay the fee to the block producer. Despite not doing anything, these transactions still consume some number of compute units.
03:07:34.525 - 03:08:20.895, Speaker A: This is because these transactions still invoke the compute budget program twice. This block summary page only shows the compute units consumed by smart contracts, not the total for the transaction. So let's look at one of the transaction detail pages. As you can see, the transaction consumed 350 units, 150 for each 50 budget instruction. We can get the transactions per second of this bot by taking the total number of transactions and multiply by 2.5. This is because blocks and Solana as you guys may know are 400 milliseconds. This brings us to just under 90,000 transactions per second for a block containing mostly basic transactions, say mostly because we're still actually including many vote transactions and other transactions that users have sent us.
03:08:20.895 - 03:09:09.155, Speaker A: Next we try the same tactic with the token program. Token program is used everywhere on Solana where tokens involved. It is also token accounts themselves also account for most program drive accounts owned. Mainnet this test we get transfers performing the same calculation for this block token transfers we get just under 19,000 transactions per second. Take note of how many payments were consumed just under 4004 just over 4400 components. Lastly carried a block full of nanotoken programs in grantrend program transfers. Nanotoken is an experimental, highly optimized variant of the token program Raymond Rust.
03:09:09.155 - 03:09:55.605, Speaker A: It's been developed by KV over at Temporal. There will be a link to his GitHub repository at the end of the presentation. So if we run the numbers again you can see this blocked an effective TPS of 54,000 transactions per second and each of these nano token transfers only consumed 48 compute units. Recall, token program transfers took over 4,000 compute units, whereas nano consumed just 48. There's a massive reduction in the number of compute units consumed for what is effectively the exact same behavior. Now I've paired that up with the fact that token program is the most evoked program on mainnet. Hopefully you can see for yourself the amount of shit action throughput that has been left on the table.
03:09:55.605 - 03:10:29.055, Speaker A: Here are the final numbers from those experiments. Please note these are individual samples that are not continuous block production rates possible to test them. They're single blocks built at one point in time, so look at the results. Something strange does show up here. We'd expect to compute units per second scales directly with transactions per second. Oddly, this doesn't seem to be the case. This is largely due to Solana's cost model overvaluing the cap and underweighting the cost of increased account access.
03:10:29.055 - 03:11:20.565, Speaker A: Lastly, testnet is a very adversarial environment for validators trying to make large blocks like this. The cluster is actually larger than mainnet and has many less capable nodes, more dealing with state at a high skiff range. It's a hard place to pull off an experiment like this as we not only have to make Fire Dancer perform well, pull our punches to work within the constraints of weaker agambere that Franken Dancer nodes. In that sense, this is also a test of acknowledging Frankendancer on Tesla. Okay, now onto the final performance demonstration. This last demo was conducted on a globally distributed test cluster composed solely of Fire Dancer nodes. Before I hand it off to Arman to do the demonstration, I want to spend a moment discussing the test poster.
03:11:20.565 - 03:12:04.645, Speaker A: The Coaster is composed of 12 servers located in New York, London, Frankfurt and Singapore. Each machine has a dual socket Intel Xeon with a total of 80 cores and 512 gigabytes of RAM. In realism, these nodes are placed in similar metropolitan areas to the rest of the Solana validator set. Machines are networked with a dedicated optical fiber network on a junk global network. This gives cluster unique characteristics that are not seen on the public Internet. To illustrate some of that, let's look at some of the dimensions of this network. Here I'm running a set of tests from one node of the cluster to the other.
03:12:04.645 - 03:12:49.505, Speaker A: This first test is a bandwidth test and the latter test is a pig test. The results we get contain a lot of information, but only two measures concern us today. Jitter and Bing time. The jitter of a link is simply the mean Difference between packet arrival times and their expected arrival times. Put another way, jitter is the measure of how unpredictable the time of packet's arrival is being down or round trip time is the time it takes a packet to go from one machine to another and then back again. Big time is analogous to distance travel on the Internet. Low jitter connectivity is essential for distributed applications like Solana, as any jitter that is introduced into the network induces more latency.
03:12:49.505 - 03:13:28.105, Speaker A: So here are those big times we measured that are laid out on the map. As you can see the inter metropolitan area round trip times around three orders of magnitude smaller than the Intercontinental Mint times. The jitter measurements are another three orders of magnitude smaller than those intra metro pick times. To illustrate the magic this difference, let's zoom in on the map. The variance and packet arrival times on these links is less than 1 microsecond to 4 microseconds. At 1 microsecond I could travel 200 meters. What you're seeing here is the venue we're in.
03:13:28.105 - 03:14:03.615, Speaker A: 200 meters is roughly a distance from here to the center of Suntec. But let's zoom out and once more. In comparison to the jump global network, the jitter between my laptop and a server in France was on the order of 665. And that time light travels 120 kilometers. And let's zoom out one more time, one last time. Compared to the total distance light has to travel over the Internet from one continent to another, it's incredibly small variance. Dare I say, negligible.
03:14:03.615 - 03:14:46.425, Speaker A: All right, with all that out of the way, I'll hand it off to Armand to present this last demo. Thank you, Liam. Before I dive into the details of the demos, let me quickly go over the setup. We will be running 10 validators and two transaction generators. A cluster is set up such that there is one leader in London and nine followers across the globe. This is done largely to be consistent with the configuration that Flag and Dancer showed off in the previous talk. The transaction generators are set up create transactions whose account accesses maximize parallelism as much as possible to push the runtime to its limits.
03:14:46.425 - 03:15:36.525, Speaker A: The validators have been parameterized to have maximum block level compute limits 50 times larger than Solana mainnet. We have disabled the transaction cache and at the transaction rates we will show our machines cannot meet the memory requirements to retain that number of transactions. The network has 5 gigabits per second of bandwidth between nodes and is thus unburdened by large transaction nodes. This is not the largest demo setup we have ever constructed. In the past, we have set up 100 node clusters with 100 gigabits per second links for the wide area network and 25 gigabits per second to each host. For all of the demonstrations, the followers are configured identically, each with 32 CPU cores assigned to the runtime. The leader was configured similarly, except the cores allocated to the runtime are variable.
03:15:36.525 - 03:16:07.925, Speaker A: Depending on the demo scenario. We will be showing you four scenarios today. A small note, the demos are pre recorded and not live. This is not for a lack of wanting to show you live demos. Rather, doing them live is complex and time consuming and it would involve a fair amount of waiting around on your part. The first demo will show us handling 1 million transactions per second of basic transactions. These basic transactions are identical to the type Liam described for the first testnet block.
03:16:07.925 - 03:16:45.515, Speaker A: Okay, let's start with the first demo. Here we will show you the leader coming up and starting to keep up with 1 million transactions per second. On the left is the leader node having just started from Genesis. Right now it is only processing one transaction per slot. I also start up our RPC service so we can start receiving transactions from the transaction generators. Next, I'm going to start up this handy script which will show us the TPS numbers averaged over the last five seconds. When it starts up, it shows us two transactions per second as expected.
03:16:45.515 - 03:17:28.395, Speaker A: Finally, I will be starting up the simple transaction generation scripts on the right which will generate over 5,000 transactions per second of traffic per node, which is why we have two and send it to the leader. They start up by creating and funding accounts and then start sending basic transactions. We will slowly see the TPS numbers go up in the middle pane as the initial transactions start going through. As promised, the generators are producing over 1 million transactions per second combined. You can see this on the right. We will wait a little bit and the numbers are already up to over 900,000 transactions per second. As you can see in the middle pane.
03:17:28.395 - 03:17:58.045, Speaker A: We will let this run for a few more seconds for everything to start running at capacity. If you can bear with me for a second There, we have 1 million transactions per second being produced by the leader number. You can see that in the middle. Onto the full cluster setup. Now we have the leader node on the left and the nine followers in the grid on the right handily synced up. You will see the leader running and the followers connected and keeping up. We do all the same setup here as we did for the leader.
03:17:58.045 - 03:18:53.883, Speaker A: We now start up our scripts to measure the transactions a second on the Leader and the followers off screen the transaction generators have already started and the transactions start landing slowly. You will see that from the measurements going up on all the panes. An important thing to note is since the leader and followers are not synced up here, there is a slight difference in when the scripts print the metrics, but by and large the numbers are the same. It takes a few seconds for us to hit the desired numbers as the fired answer clients need to warm up and the traffic starts to reach all the nodes. You will see that the numbers have increased in all the bins and in sync we will wait just a few more seconds. Bear with me once more and there we have 1 million transactions per second being produced by the leader and being handled by all of the followers. And that was our first demo.
03:18:53.883 - 03:19:24.811, Speaker A: Here's a zoom in on the numbers of the leader. You can see the 1 million transactions per second being produced. Handling this many basic transactions is critical to establishing the maximum capacity of fire dancer. The second demo shows Firedancer handling over 1 billion compute units per second using the memo program. The Memo program is a simple Solana program that like token program consumes several thousand compute units. I go to the demo itself. The setup remains the same and you will see our script.
03:19:24.811 - 03:19:55.277, Speaker A: We started up again but with a little twist on the leader. We measured compute units per second, a good measure to show the amount of compute activity happening. The followers continue to show the DPS but this is not meant to be a showcase for that and is not optimized for it. On the left you will see the Compute units per second climb up to over 1.2 billion per second. You'll also see that the followers keep up with the leader as the slot numbers continue to increase in lockstep. And that is our second demo.
03:19:55.277 - 03:20:22.793, Speaker A: 1.2 billion compute units per second. Here is a close up of the measurements from the leader. Compute unit throughput is one of the primary limiting factors for Solana. Based on the existing protocol limits, this is almost 10 times the amount of compute activity possible on Solana mainnet. The third demo shows an interesting metric which is 3.5 gigabits per second of block space when handling no OP programs with fixed size payloads.
03:20:22.793 - 03:21:16.445, Speaker A: No OP program is an SBPF program that does nothing excepting arbitrary data in the transaction. On our now familiar setup, I would like to draw your attention to the block space per second metric shown in all the pins. As the generators start the numbers start going up and you will see them increasing right about now in all of the and they go up to over 3.5 gigabits per second. The followers and leader keep up and remain in sync while handling all of that traffic. Here are the highlighted block space rates for one of the followers Block space throughput is critical for data availability solutions on chain proof systems and for handling large transaction payloads such as NFT Mint events. Firedancer provides an excess of this block space bandwidth which is necessary for both bursty high demand events today as well as for the future growth of the chain.
03:21:16.445 - 03:22:18.065, Speaker A: Finally, we will be demonstrating Firedance and validators handling over 500,000 transactions per second of Solana virtual machine executions using the aforementioned nanotoken program. We once again have our trusted setup of validators running with the generators starting up. Notice that with the Nano token setup we see one additional transaction landing during the setup phase giving us three transactions per second. As the generators reach maximum capacity, we see the TPS numbers climb up and you will see them approaching 500,000 transactions per second both on the leader and the followers. We let it run for a couple of seconds to show that all the transactions are being handled at that rate. Zooming in on the lead validator shows the 500,000 transactions per second of Solana virtual machine executions. In an earlier slide we showed packing a block on testnet at around 50,000 transactions per second with narrow token transactions and here we see a 10 times increase on this Fire Dancer cluster.
03:22:18.065 - 03:22:51.713, Speaker A: Based on these demos you can see that we have done a good job of optimizing and tuning the performance of the Fire Dancer client. This is not to say however, that there is no more room for improvement. We are hard at work optimizing even more. I will hand the stage back over to Leonardo. Thank you Aravan. You've now seen the first glimpses of what Fire Dancer has to offer. We showed you that the Fire Dancer validators had parity with the Solana protocol, executing blocks, voting on them and even producing its own blocks.
03:22:51.713 - 03:23:39.475, Speaker A: We also showed what the full Fire Dancer client that the full Fire Dancer client can perform both on testnet and at scale in a cluster. Before we finish up, I want to thank the wonderful people at BlockLogic, Solana FM, Solana Labs and KB for the tools and programs we showed today. This presentation and for that matter, Fire Dancer would not be possible without the community builders on Solana. What's next for Fire? Well, we still have a lot of work ahead of us. Fire Dancer is not ready for general use in production and still needs many more features optimization, organization's audits, etc. Before we ready that Being said, we do have one last surprise before we head off the stage. Here's a list of nodes currently participating in gossip on Solana mainnet.
03:23:39.475 - 03:24:17.865, Speaker A: Excuse me, One node that has a squared version number attached to it. That is the full Fire Dancer client. Yes, you heard that right. The full Fire answer client is live on Maina. It is in a non voting mode, but it is doing everything that the Jestnet validator that you just saw also does. So we've been running in this mode on and off in mainnet for almost two months with and without that special first number. Here's a video about mainnet node running in the terminal window.
03:24:17.865 - 03:24:53.919, Speaker A: Notes. Just a node passively participating in the network is not voting or producing blocks, just listening on terabyte and replaying blocks in real time. And lastly, here's the explorer running against the Firedancer maintenance of RPC service. It's a pre alpha version of Fire Dancer. It's largely unaudited and in many places the code needs a lot of refinement. There's a long journey ahead of us to get it into production for testing and hardening to object and quality of life improvements. So that's all from us, this Breakpoint sites and tools we referred to in this presentation are available at the QR code on the screen.
03:24:53.919 - 03:25:21.089, Speaker A: Stay tuned for more from us next year. Well, two brilliant presentations that takes us to lunch. We've already headed the game. We'll be back here in an hour. I won't be there. It's been lovely to speak to you all. I've been Anthony Beaumont.
03:25:21.089 - 04:29:36.483, Speaker A: Have a lovely lunch. See you in an hour. Thank you very much. Sa it it it it it it it it it it it. All right, hope you're all enjoying the food. We've got a ton of technical talks, product demos, live cool stuff happening here for the next hour and a half. And to kick us off, I'd like to welcome to the stage Drew Nutter from Seneca.
04:29:36.483 - 04:30:04.345, Speaker A: Ready to talk to you about some validator client biz for Solana. Welcome him to the stage please. Hey everybody. I hope you enjoyed Breakpoint so far. So my name is Drew Nutter. I'm a senior protocol engineer at Syndica where we are building a read optimized Solana validator client from scratch. It's called Sync.
04:30:04.345 - 04:30:59.255, Speaker A: So who is Syndica? Syndica is a Web3 native cloud. We perform RPC infrastructure and developer tools for Solana. So this includes some products like elastic nodes to scale your DAPPS Chainstream API to stream data directly From Solana app deployments to easily deploy your dapps from a repository and a dendexer to convert on chain events into application specific ban. So as an RPC provider we noticed a problem. Solana is plagued by slotlack. What this means is that essentially every RPC node is struggling to keep up with the latest state of the chain. This leads to poor user experience because they're not getting the latest data and they're also experiencing delays and errors when they're interacting with dapps.
04:30:59.255 - 04:31:59.333, Speaker A: So we looked at the data and we realized that this is close by bot that occurs validator when they are receiving a large number of read requests over RPC for methods that are unable to process many reads per second or RPMs. So this is why we're building sync at Syndica. We're focused on user experience and we're going to improve that by solving slotlak with a new and faster Solana Validator client building from scratch with a focus on reads per second. So this is also going to improve the fault tolerance of Solana by introducing client diversity and it's also going to strengthen the developer community around Solana because of our intense focus on code readability. So last year, some of you may have seen my colleagues at Breakpoint introduce the SIG project. At that time, we had just finished implementing Gossip, which is the entry point into that cluster, and we had a blog post up at that time. Since then we've made a ton of Progress.
04:31:59.333 - 04:32:48.297, Speaker A: We've implemented AccountsDB, the blog store and the Shrine Collector. So I'm really excited to share that progress with you. But first I'd just like to go through a little bit of background about Solana. So why do we use Solana? So there's a lot of reasons you might think something like, you know, it's fast, it's cheap, decentralized powers of the dapps that I care about. But fundamentally, what is Solana actually doing for us? So society relies on information systems to coordinate just about everything that we do. Some of the responsibilities of information systems are to store information, to distribute information, and to allow changes to that information. So that's actually exactly what a database is designed to do.
04:32:48.297 - 04:33:31.655, Speaker A: So it's no surprise that databases form the core of all of these information systems. So going back to why do we use Solana? Well, Solana actually is a database. Because of that, it can serve as the backbone of all of society's information systems. And as a blockchain, it is inherently trustworthy by design. And due to its efficiency, it's actually more useful than any other blockchain. So as a database, Solana needs to store information. So the first type of information that needs to be stored in Solana are accounts and accounts answer the question, what information exists right now? So some examples could be something like I have $100 or I'm allowed to vote in governance.
04:33:31.655 - 04:34:14.735, Speaker A: The other type of information stored by Solana are transactions. So transactions answer the question, how did IPS information change over time? So some examples would be I paid somebody $100 or I submitted a vote for a proposal. So first I'd like to talk a little bit about the account storage that we've implemented in sig. So the component that's in charge of this is called Accounts db. So here you see an example of a read operation where the RPC client on the left might be something like your wallet trying get your account balance. So it's going to send a request to an RPC node to get that account. And under the hooded, the RPC node is going to be looking at accountsdb to get that balance and return it to the wallet.
04:34:14.735 - 04:34:58.514, Speaker A: So fundamentally, AccountsDB has to support two basic types of operations. So there's like a get operation, which is the read situation that I just described, and the other is a put which is going to save an account into the data. So that's going to be for example, when transaction is executed. So every single sauna validator needs to have a complete copy of accounts DB that's going to include every single account that's on chain. So the first time that a validator starts up, it needs some way to construct this gigantic database. One way to do this is you can start from a totally empty database and just rerun every single transaction that's ever happened. Placement take a ridiculous amount of time, and it's just not typically how things work.
04:34:58.514 - 04:35:46.323, Speaker A: Instead, a sauna validator will download a snapshot from another validator on the network and it's going to only need to process the transactions that have occurred since that snapshot. That snapshot is just a copy of the database. So inside a snapshot you have a whole bunch of files, but basically the majority of those files are just storing all of the accounts and they're by the time that they were modified. So let's say you wanted to find a specific account in the database. So one approach that you could use would be you just start looking at every single flight in every single one of these files until eventually you're going to see the pub key that you're looking for. And you'll know that you found that account. Well, this again is an operation that's going to take a very long time.
04:35:46.323 - 04:36:37.185, Speaker A: And so it's not practical. Instead what you need is some approach that before you ever look for an account, you're able to just look one time at the entire snapshot and create a sort of map that's going to be something you can use later to navigate the database and very quickly identify what file where the accounts are located. So that's the whole purpose of the account index. So the accountant text essentially is sort of like a hash map. So I don't have time to explain how a hashmap works under the hood in this talk, but the basic idea is that it's a data structure where you are able to provide some data called a key. In our case it's a pub key. And what the hashmap is going to do is it's going to very quickly return some data to you that has been previously associated with that key.
04:36:37.185 - 04:37:17.881, Speaker A: So in our case that's an account location which is going to tell you either it's in the cache or it's going to tell you exactly which file you need to look at. And then it's also going to tell you where within that file that it's located. So the whole idea of the account index is to accelerate the process of locating accounts. So it's all about performance. And that raises the question, how do we make it fast? So, so the first thing is getting fast writes in order to save data very quickly in the index. So we made these fast by reducing the number of memory allocations. So every time you want to add data into the index, you need some memory that's going to store that data.
04:37:17.881 - 04:38:08.973, Speaker A: And the problem is that allocating new memory is very slow because it requires the operating system to execute some very time consuming operations. So in order to address this, we took a counterintuitive approach and we used linked lists instead of arrays. And what that allowed us to do is recycle old memory rather than having to reallocate new memory. So this really accelerated our write performance. Additionally, if you want the entire validator to be fast, you need to be able to support multiple threads looking at the account index at the same time. Now the problem with this is that you have one thread that's modifying a hash map, then it's not safe to have other threads using hashmap at the same time. So the way to deal with this is by sharding that index into many different individual hash maps.
04:38:08.973 - 04:39:03.765, Speaker A: So that way you can have one thread that's working on the index and just look at one very tiny portion of the index, while the rest of the index is not compromised in usability. So this has just a huge multiplier effect on the performance of the index when it's used in a multithreaded context. So we also want the hash maps themselves to be fast. So we were not satisfied with any existing implementations, and because of that we decided to implement our own hash map from scratch. Now, it's inspired by Google Swiss Table and the general idea is that you break up the hash map into chunks of 16 items. And because the metadata is structured within a single byte, it allows you to. It allows the data to fit within the CPU cache and then you can also operate on all 16 items with only a single CPU instruction, which makes it faster than any other approach.
04:39:03.765 - 04:39:38.848, Speaker A: In addition to eliminate bottlenecks in the validator, we've also implemented a Geyser Inter interface with Linux pipes that exceeds 38 gigabits per second, just running on consumer hardware. So our optimizations have paid off. Our read performance is our main focus, and for that we're seeing the account index is performing between 1.5 times to as much as 4 times faster for read operations compared to agame. Now, we also care about rights. We want the whole thing to be fast. And over there we're seeing performance from 1.1
04:39:38.848 - 04:40:19.101, Speaker A: to 6 times faster than it got made. Thank you. So we recently published a Deep Dive blog post about this, so I highly recommend you check that out on the Syndica website where we go into just a lot more details about all this stuff. So if you remember earlier I mentioned there's another type of data storage in Solana, and those are the transactions. So transactions represent changes to the accounts, and the ledger is what contains all of the history of transactions. So the ledger is used all throughout the validator. It's used for like rpc, Consensus Turbine, gossip, history.
04:40:19.101 - 04:41:10.565, Speaker A: Anytime you want to find insurance or status of a transaction, it needs to look in the ledger. So this trick of the ledger, like I said, there's the transactions which represent. Yeah, the ledger is the history of all transactions, right? But then to facilitate consensus in a blockchain system, you need to group those in really big groups called blocks. But this kind of introduces a little bit of a problem because now there's a latency to process transactions until the whole block is available. So Solana solves this problem by dividing the block into smaller pieces called shreds. So this is A major reason why Solana is able to be as fast as it is. So, for example, the block producer is able to start setting out these shreds early, before it's actually even figured out what the entire block is going to look like.
04:41:10.565 - 04:41:56.335, Speaker A: So another way that this accelerates a lot of performance is with erasure coding. What that means is that every single shred contains some redundant information. But what that means is that no validator needs to receive every single shred. They can receive some random subset of shreds over the network, and that's enough for them to recover the other shreds and then they're able to move forward without needing to send out a request, ask for the shreds that we're missing, and wait for that response. It just really accelerates performance there. So the block store is the name of the component in the validator that stores the ledger. So I'm happy to announce that we just very recently finished the initial implementation of the block store.
04:41:56.335 - 04:42:50.911, Speaker A: So the block store, like a casdb, has these two general responsibilities of reading and writing data. So the block store actually contains a very diverse collection of data, but the most important thing are the shreds. And so a really big part of the box store is just inserting those shreds. And that includes the logic for recovering shreds with Reed Solomon erasure coding and also verifying oracle trees to ensure we have well formed shreds. So we've implemented all that logic from scratch. And then on the read side, some important operations are just things like getting block and getting transaction, which need to reconstruct the box from the constituent trends. So now that we have the block store, how is SIG different? So we've taken a little bit more of a modular design and what that allows us to do, for example, is to have a swappable database backend.
04:42:50.911 - 04:43:28.055, Speaker A: So so far we've gotten two different database backends implemented. We're using ProcsDB, and we also have an alternative that's implemented with hash apps. And in the future we plan to experiment with some other backends that should be very easy to add support for. Like, for example, LMDB is one candidate and we're going to be testing all the performance of this to make it as fast as possible. So another way that SIG is different is that it's RPS focused. We've been looking through the block store, identifying bottlenecks, and we see a lot of examples where things can be improved. Compared to agave, for example, there's a lot of linear searches that are executed redundantly, we know that we can eliminate in sync.
04:43:28.055 - 04:44:12.985, Speaker A: So the block store is done and we're going to have a blog post coming out for that very soon. Now that we have these databases implemented, how do we actually make use of them in a Solana validator? Well, the shred collector is the component in SIG that is responsible for acquiring the shreds over the network. If you're familiar with Agave, you might have seen the TDU as similar thing. So if you recall from AccountsDB, the snapshot is downloaded, which represents a specific point in time which is in the past. And then we need to start processing transactions to update the state. And so in the shred collector we're going to be receiving these shreds. And within these shreds we can log the transactions and update the databases.
04:44:12.985 - 04:45:10.563, Speaker A: So Turbine is the mechanism by which Solana Network is going to distribute the shreds throughout the entire cluster. So the leader is going to produce those shreds and then it's going to send them out to its neighbors, and then they're going to send it out to their neighbors and so on in this tree like structure until the entire network has every shred. So in this diagram here, the ones highlighted in blue are part of the shred collector and some of the responsibilities there are to retrieve, receive all the shreds over Turbine, to verify the signatures of those shreds, and then they get inserted into the block store. And meanwhile the repair service is running the entire time, checking the block store for any potential missing shreds that could not be recovered and it sends out requests to get those. So after that they get retransmit over Turbine. So we've made a lot of progress over the past year. We've implemented accounts DB in the and finishing out with the shred collector.
04:45:10.563 - 04:45:56.305, Speaker A: We have a full database and we're able to run it on mainnet and it can keep up with the chain and keep all the databases in sync. So currently what we're working on is a retransmit stage of Turbine. And coming soon we're going to be working on svm, RPC consensus and block production. So decentralized networks like Solana were best when the community contributes together as a whole. And so that's why we're developing this completely out in the open on GitHub. So you can check out our GitHub here, it's linked on the left side, you can look there for the latest updates. And if you want to make any contributions, we're also very excited to share all of our updates and everything we learn with Deep Dive blog posts that we're going to keep putting out on the Syndica website.
04:45:56.305 - 04:46:38.485, Speaker A: So thanks so much for your time and I hope you enjoy the rest of Breakpoint. Let's give it up again for Drew, everybody. Awesome. Okay, you're stuck with me for the next five minutes. I'll do some setup. So I guess we talk about the first eight year registration. Did anybody catch the robots? The snack robots? Okay, I know somebody over there.
04:46:38.485 - 04:47:04.779, Speaker A: Yeah. Okay, we got at least one person. I was actually apologizing to them because I felt so bad, honestly. There was a lot of people in the hall and we were all, all trying to get our badges and whatnot, but these robots were working really hard, you know, Like, I don't know, I saw it in their eyes. They actually make faces. They're not here anymore, as far as I can tell. But they would actually talk only in Singapore.
04:47:04.779 - 04:47:28.385, Speaker A: Right. The last few breakpoints we didn't have that, but they'd be like, hey, we heard the way. Can you move, please? And I was like, oh, I'm sorry. I wanted some ships. Go ahead. Yeah, it was crazy. But anyways, I think we're done and it's way faster than five minutes, so thanks for humoring me with my robot story.
04:47:28.385 - 04:48:39.795, Speaker A: Let me introduce here we got Joe Caulfield and Lucas Sternagel here from Onslaught to talk to you about svm. Give it up, y'all. Hello, everyone. We're here today to talk about the SVAM and how developers can use it beyond the Solana blockchain source. I work on compilers and runtime. And I'm Joe. I work on itching programs and runtime as well.
04:48:39.795 - 04:49:19.021, Speaker A: And we both do svm. So as you guys know, SVM has been heating up pretty quick. So there's a lot of projects that have been kind of like rising up for all different use cases and niches to basically use SBAM outside of the validator. Right. So here's just like a couple of the teams that you might have heard of. So as you can see, there's a lot of different use cases already that have come up. There's going to be probably more, so some permission environments, a few different kinds of roll ups that maybe roll up to different chains or roll up to Solana, which there's been some debates about already, and then some other stuff too, like ephemeral rollups.
04:49:19.021 - 04:50:03.025, Speaker A: Right. So like some of these new co. But what exactly is svm? What are People building with and what exactly are they able to do with it? And we're going to talk a little bit about this today, but first we're going to talk about what is this in the actual validator in order for us to understand what the SVM is, to understand where it came from. So let's think about how transaction processing pipeline works in Agave. First we have a transaction schedule beside what transactions can be executed in a batch. And then bank, sort of the AGATHA components also comes to the database. The transaction processor performs all the checks to make sure the transaction is valid.
04:50:03.025 - 04:50:58.345, Speaker A: Then we have a message processor which will serialize all the accounts and paste the data and pass the data to the virtual machine. And after that happens, we have what we call the commit stage. We backtrack all the results down from the virtual machine to the account cp. So we return to the message processor the program execution result, we deserialize the accounts in the message processor and then the transaction processor verifies that none of the accounts constraints are violated, like balance and even if the written so read only accounts. And then if bank decides these accounts, once successful, we commit all the results to the database. But we notice that this is a massive entanglement in our repository. And we decided to clean, fix up and build a new product out of that.
04:50:58.345 - 04:51:38.817, Speaker A: We call this on a virtual machine, the svm. The SVM consists of transaction processor, message processor and the virtual machine. And it connects directly to the account DB and bank. So this is like a pretty simplistic design. Of all the things that bank does, I mean there's even more, but you can kind of see that the transaction pipeline that Lucas was just describing is sort of here on the bottom. But then there's like all these other things that if you've ever had to fork this or use it, you have to kind of just like get rid of. Maybe you need it, but most of the time you probably don't.
04:51:38.817 - 04:52:20.195, Speaker A: So the API that we're going to talk about today actually lets you do like just this. So you can just give it some inputs, you give it a list of transactions, you get that entire pipeline with the cache, like the vm, all these things for free, and then you get some results and you even get some metrics, right. So this is kind of like the way that we've carved this out of the rest of the runtime. Here's just a low level overview for any of you guys who don't know exactly what I'm talking about. This is what each of these transactions kind of does and looks like. And you can see those components from the SVM right here. So there's the program ID that's going to be used to load from the cache and it will eventually come from an account.
04:52:20.195 - 04:53:05.743, Speaker A: At some point you got some account that are read, some are write and some instruction data. And then the BPF loader is going to load up that executable and the VM is going to be provisioned for you and you'll execute your program right. So you get all of this inside this new API. And we have the code DSPM from Agave, but now provided as a separate crate. We can just include it in Cargo Dome and developers can use from it. It has its own public API and no external dependencies in Agave. And now the question remains how can we use the svm? So we have a set of items that users need to provide.
04:53:05.743 - 04:53:53.535, Speaker A: One of them are overridden traits you need to implement because we made them generic over a canvas. So anyone can just implement for their needs and to data structures the transaction process environment and the transaction processing machines. But also we have just go back from the slides. We also have the items Agave provides, but they are overrideable. So one of the set of built inside can be the built in functions which is built in programs like System Program and the State program. And we have class these virtual variables programs can access during execution. Developers can just import them or develop their own solutions for these.
04:53:53.535 - 04:54:42.475, Speaker A: Now the overriding trades, we have two that are the main ones. The first one is a transaction processor callback which is just basically an accounts provider as function for the SM to retrieve accounts from the database. I add new accounts and just check if a set of owners match those accounts. And then a foreground trait which let us find the relationship between two slots and then the two data structures. The first one is the transaction processing firewall. Contains variables necessary for processing a transaction like the active feature set and how many LAMP boards to charge per signature. We also have the transaction processing config.
04:54:42.475 - 04:55:25.027, Speaker A: This is just a set of configurations for how we want to execute a batch of transactions. So some of the settings we have there are if you want to collect logs from the program and if you want to record the return data from the program. And now what we are calling the SVM is actually the transaction batch processor. This is another data structure. It contains among other things the CSR cache and the program cache. And if you want to use the SVM you need to create this data structure in a rust code. And the SVM Entry point is a load that executes sanitized transactions, those five arguments.
04:55:25.027 - 04:55:55.701, Speaker A: So you need to pass on the callback, transaction processing, callback training, implementation transactions. You need to process the check results which tells the SVM which transactions are valid for execution. Transaction process environment, transaction processor configure the two trended data structures I mentioned in the previous slides. So what can you build with this? Exactly. You saw some of the examples earlier. These are like real lab projects that are here probably in this room. And so here's just a couple like general examples.
04:55:55.701 - 04:56:24.255, Speaker A: The list is honestly bigger, but maybe some off chain services. Right. So you can emulate transaction processing outside of like the actual protocol. Some light clients, state channels and roll ups which we were talking about a little bit earlier. Subnets sounds like maybe avalanche and stuff like that. And hopefully in the future some TK proofs, some projects are already doing this as well. So this is kind of like what we would like to have as our Hanza SPM roadmap.
04:56:24.255 - 04:57:07.379, Speaker A: So as we're talking about today, we've decoupled this SVM API which has been super nice to use and soon we want to probably try to do the same thing to the scheduler. And then also we want to have like native support for RES0 and for using K Proust, maybe using something else. And then of course we want to have lots of reference implementations for you guys to look at, one of which we're going to show today. But there's also a few more that you can check out as well. And now we have a live demo, so we're going to flip over to a live demo here. But you guys can look at any of our examples already right here. This is just the Agave repo, which most of you should know where it is hopefully.
04:57:07.379 - 04:58:00.435, Speaker A: And then you can just go to SVM examples and you can find all the reference examples that we're going to put in there. So there's a couple right now there's an RPC service and then there's this state channel that we're going to go through. But there will be more soon in the future. Yeah, so if you look at the Acabbas repository, we can find the SVM in the SVM folder, we have a docs code for documentations and the examples folder we want to talk about now about the pageview example, but we also have a PR for another example which is a JSON RPC server built with the svm. So basically the way that this example kind of works is this is a pretty straightforward payment Channel, which many of you guys might know what this is. But. But basically this is an off chain environment where two or more parties are going to be allowed to just transact.
04:58:00.435 - 04:58:24.017, Speaker A: And this is a simple one. You can actually do this with a lot of different kinds of state changes. But the most simple way you can do it is just token balance changes. So like native, solo token or whatever other token you might be working with. That's the way we lost. That's what this is going to basically do. The people in this channel, like the parties in this channel, will transact with each other.
04:58:24.017 - 04:59:42.235, Speaker A: And then at the end right here, you can see that that's where the final resulting balances will settle to the main ch. So as Lucas was talking about earlier, like this trait is one of the core components you can use to implement and then you can actually power your instance of the SVM API. So right here I just created this basic cache that is going to just pull accounts from rpc. And it's kind of like a really simple dummy implementation. But the point is, this is a custom way to load accounts that I'm just using to implement that trait and give this module a way to load accounts. And you can do this however you want and however it makes sense for whatever it is you're trying to build. So this is just implementing that trait.
04:59:42.235 - 05:00:23.565, Speaker A: And then here is where I'm actually going to bootstrap the processor. So I put it all in this function to make it a little bit easier to call from the internals of the example, but essentially walking you through what's happening here, this is that forkraft trait that we were talking about. It's just a state channel. So there are really no forks, there's no like actually really no blocks or anything like that. So I just mocked this out here. But the actual creation of the processor here, you can see you set this thing up and then you set it up with the proper fork graph and then you start configuring like the runtime environment and you configure the cache. And so we want to be able to transact with SPL tokens.
05:00:23.565 - 05:00:54.165, Speaker A: So here I'm going to actually set up the program cache with the SPL token, like the SPL token program. And so you can get this program binary from like wherever you want. In this case, I'm just going to get it from the account, like I'm just going to use the callback and then you set this up with the cache. And then we also want to be able to transfer like actual SOL too. So we're going to add the system program and then the token program is owned by the deprecated V2 loader. So we also want to add that. But that's it.
05:00:54.165 - 05:01:34.235, Speaker A: That's how you set up the cache and that's how you set up the processor. And there's even like further configurations you can add. Like we're not even doing like sysbars or anything else here. So you can kind of configure this however you want. And then you basically have yourself a working processor that you can use to process transactions. And now we can take all this wind bonus them together and create the entry points for the page view. So in this function pageview process transactions, we create the account loader, since it's the training implementation for the transaction processing callback.
05:01:34.235 - 05:02:21.345, Speaker A: We create the batch processor. Then our SCM set up the environment and the configurations, giving the transactions function receives, we convert them to the SVM format because page completed a slightly different format for the state channel. And then we call the load execute transactions the SVM entry point. We have the results here. And then we call settled, which will set only the account balances in the Solana blockchain. So let's write an example test with SPL tokens. So in this example we should create the accounts and the associated token accounts.
05:02:21.345 - 05:03:25.305, Speaker A: And we start the validator of these accounts. We create our paytube with again the accounts we just created. And then we define some transactions will execute in the state channel. So Bob transfer these transfers of two tokens and Bob transfer five to wheel additions due to Bob and wheelchairs want to list all these transactions and then pass transactions. And what this will do is that will execute all these transactions the SVM and then we will only submit to Pad blockchain the account balances, the minimum set of transactions we need for the accounts to get their final balance. And at the end of the test we query the validator to check if the balances are correct. Now if we execute the test, we can ensure that this is happening.
05:03:25.305 - 05:04:15.465, Speaker A: Okay, so here we've got the logs. This is what has happened in PSVM inside paycheck. So there were four transactions in four transactions. And then when we settle in the blockchain, we only have one transaction and three instructions for transfers. Yeah, so basically this is like the crux of this innovation here is like you can have a ton of transactions that you might use in a state channel. But as you can see, this is a pretty simple example to go from four to one. But imagine you're going from like millions of transactions to like a few thousand, right? Like you can greatly reduce the costs of settling this information on the L1 in something like a state channel.
05:04:15.465 - 05:05:49.545, Speaker A: And like, as we showed you, like this is really all you need to do to set this entire thing up. And with just this bootstrapping method, like just using this API, you can literally just bootstrap an entire roll up or anything that you want to do just using these few pretty simple steps and not a very big code base. So you can kind of focus on exactly what it is that you're trying to build and what you're trying to innovate with whatever project it is that you're trying to spin up. Thanks guys. IT SPF rules. All right, I'd like to introduce and bring to the stage Jason Lee with a product keynote. Welcome to Jason, everybody.
05:05:49.545 - 05:06:26.505, Speaker A: Hello everyone. Ah, the slides are wrong. Just noticed to you. Hi, can I update the slides please? Can I update the slides please? Yeah, yeah. It's not updated. No. Just to resolve the technical issue.
05:06:26.505 - 05:08:52.901, Speaker A: Apologies about the slides issue. I'm just going to do it without the slides and then you can appreciate the logos in the meanwhile. Thank you guys very much. So Jason here from Solaire. Right. So I'm going to cover the most positive topic that's happening in crypto right now. And that is a lot of you might be wondering what is risk taking and what's the value of risk taking? How do I even get yield and are there sufficient yield? Are there enough AVS to pay for the yield right now on the market? So that's the questions you guys are wondering.
05:08:52.901 - 05:09:26.775, Speaker A: But because I only have five minutes, I think I have three or five minutes left, three or four minutes left. Right now I can't cover them all. So I'll save that for the next episode if I have the opportunity. But one argument I would like to make is that imagine re staking is a thing where would it happen? And I would argue they will only happen on Solana. And the reason is because base chain. What's the significance of the base chain GEO risk taking metric? It's the stake movement, the underlying chain. In this case Solana is moving all the stickers.
05:09:26.775 - 05:10:23.475, Speaker A: And in that situation the application builder wants to have a chain that's cheaper, that's faster because they don't want to pay for all the costs. And the underlying chain you don't want to pay on Ethereum, $5 today and $10 tomorrow with your smart contract, maybe $20 whenever a user moves a stake. So if you think from that perspective restaking has to happen also on that and how is structure restaking is for Solana native exo layer restaking. If you think about from an eigo layer perspective, it's about securing off chain applications. And there is also another component that we've added because we're not Solana native which is securing on chain applications. If you try to understand the word restaking, it's about using stake in other forms other than the native pos, you know, sort of yield. So how do we secure on chain applications? And that's by providing stake weighted quality of service.
05:10:23.475 - 05:11:25.975, Speaker A: And the second thing about having anything that you bury in the native form is its liquidity. And Soldier has come up with an innovative mechanism where we have pooled liquidity for the asset. So even if your AVS token only has $1 in issuance, you're able to get the same liquidity as everything thing issued on solar combined. So that's the innovation that we have when it comes to solving the liquidity problem for AVS tokens. When you secure applications on Solana by having their AVS token by sort of holding their AVS token which automatically translates to stakeholder quality of service for that particular application. And then the second thing we have is very similar to Eigonair in terms of support in terms of securing off chain applications. But the only difference is there is what I said being faster, cheaper and you have almost instant finality and users don't really know a thing when they are actually using a race taking network because application builders esports are end customers.
05:11:25.975 - 05:12:43.219, Speaker A: Now the third thing I want to mention is that as you think about race taking assets in general, you are thinking about what can I do? Like what should I restake? What do applications want to restake? They want to have their own tokens restaked because that's in their economic interest. And therefore solar protocol does allow you to have your own token to be part of the restaking asset for your own protocol application. But we're also thinking about what else do we want to add to the basket. And because everything is volatile, as long as you're a token, you're not a stable asset. So we're also thinking about adding stable assets to the basket. And therefore today I don't have the slides here but we're announcing Solaire USD which is a yield bearing token that's backed by RWA assets in collaboration with Open Enid. It's another, you know, really good friends that for a long time they have been pushing RWA assets to Ethereum for a long time over hundreds of millions of dollars in their treasury right now and we're working together to bring yield bearing stablecoins that's restaking back to Solana and we've termed it s USDC and there are a few interesting properties with this.
05:12:43.219 - 05:13:57.275, Speaker A: There has been no self rebasing assets on Solana and we've done a lot of homework to make it self rebasing. So for this particular asset you but yield does not cause the price to increase, right? So for anything even so right now the price will increase against so that's how the value accrual works. But right now we have adopted the token 22 standards with IB extension interest bearing extension such that the token price will always be kept at one bar but the amount will reflect the increase and the curl of the value in the token. So I think that is a very first on Solana and we hope to see more interest bearing assets or interest bearing extensions to be used to allow easier accounting and also it's a much better user interface. So like to say that you know we are announcing SUSD today which is USDC restaking and you guys will be able to see it in one to two weeks time and with that I'll conclude my presentation here today. Thank you everyone for your attention and check out Solaire, a lot of new stuff coming out very soon. Thank you.
05:13:57.275 - 05:15:00.495, Speaker A: All right, give it up again for Jason. I'd like to invite Vijay Chetty to the stage from Eclipse. Welcome him everybody. Hey what's up guys? It's great to be here with you today. My name is Vijay and I'm the CEO of Eclipse. So what is Eclipse? Eclipse is a general purpose L2 that combines the best parts of the modular stack from first principles. So what does that mean? We're taking the speed and scalability of the Solana VM which we all know well.
05:15:00.495 - 05:16:15.385, Speaker A: We're inheriting the security properties from settling to Ethereum the liquidity and network effects of ETH as gas token. So a big part of this is kind of the moneyness of ETH and then we're taking the bandwidth and verifiability capabilities of Celestio as the data availability layer as well as ZK fraud proofs from RISC 0. So Eclipse is currently on developer mainnet which means that we're open for builders, developers and infra partners like Python Backpack to integrate with Eclipse. And I'll talk a bit more about what's next for us but I wanted to touch on why did we choose to use the svm. So for me, I've spent the last five years building an Ethereum defi, right? So bonus DYDX Uniswap X, which is the RFQ system on Uniswap. And if you look at the direction that a lot of Ethereum Defi has got in, it's increasingly moved compute off chain. So you've lost a lot of the benefits of liquidity and network effects, transparency and composability that were a big part of the early promise of fully on chain defi.
05:16:15.385 - 05:17:31.175, Speaker A: So with the sb from day one, it's always been able to support multiple concurrent hotspots capped at 25% of the network. And so you've always been able to have multiple central Net order books, RFQ systems, high throughput consumer deepin apps, all coexisting on the same L1 with Solana, right? So unbundling the SVM and applying it to settlement on Ethereum brings those same kicks capabilities over to the Ethereum user asset base. And so we're looking to kind of share the great capabilities that Solana L1 has always had and bring those to Ethereum users and Ethereum assets. Now, Eclipse, uniquely, as an L2 can do something that the L1 can't, right? Which is really lead into high throughput and aim for very high tps. And that's what we intend on doing, especially with the latest developments from Fire Dancer. I think Eclipse inherits the security of Ethereum and so we can focus on the qualities of liveness and censorship resistance and really lead it to what makes eclipse unique as L2. So that's kind of a lot of the thought process that went into us using the svm.
05:17:31.175 - 05:18:48.885, Speaker A: And then of course, when you're talking about language and sort of developer awareness of Rust, there's a huge Web two audience, a great builder base around Rust, and so really providing them on ramp to come in and build for their users and assets as well. So where are we today? So as I mentioned, Eclipse is currently on developer Mainnet and we are currently the number one, one data user of Celestia. So by volume of data that's been published in blocks of Celestia, we're at the top of the leaderboard there now, of course, a lot of that right now. The transaction hashes include additional things like votes and a variety of other things that we're looking to really streamline over time, but we think it's a good early data point. And then of course, as we move to public Main Net, you'll start to See a lot of that be actual transactions. We also have SVM execution with a decentralized sequencer, so four sequencers with three separate entities and building with force inclusion as well. So this is a little bit of a reveal that we're doing today, but it's a sneak peek at the Eclipse ecosystem that we've been hard at work partnering with and helping to onboard.
05:18:48.885 - 05:19:59.445, Speaker A: So the three main verticals that we're focused on right now are DeFi, fully on chain DeFi, particularly central mid order books and intent systems which are high through use cases that can really demonstrate the potential of Eclipse and then consumer, particularly kind of gamified consumer experiences along the lines of say a friend tech, which was a great early example in that space. And then lastly is infrastructure. So you can see the Oracle and the Wallet partners that we're working with among others as well as the RPCs. And then we're also working with a handful of deep end projects to expand to the Ethereum user base. Now a few names to highlight here on the Defi side. So excited to bring on orca, the leading AMM on Solana L1 Solend, which is rebranded as SAVE, some leading borrowband protection protocol as well as Mango which is doing a relaunch in partnership with Manifest, which is a rebuild of OpenBook, which we're super excited about. So excited for users and all you guys here to be able to access these apps in the coming weeks as we head to public mainnet.
05:19:59.445 - 05:21:02.435, Speaker A: So I wanted to hone in a bit on Defi on Eclipse in particular, what we're doing with liquid restaking tokens. So we built what we think is kind of a unique best in class and pioneering experience around that, which centers around a unified ETH restaking token which you know, we're calling URT for short and we're hoping to bring this same capability over to Solana LRTS as well, or Sol lrts. And so the goal is to make Eclipse the home for LRT 5. Obviously there's a ton of activity around this right now both in Ethereum and Solana and we think if we can help to aggregate the yield opportunities around both these asset bases, that's a very interesting value proposition for users out there and something that doesn't really exist right now. Right. For those of you who are around for the DeFi Summer of 2020 year was kind of this yield aggregator. So we're building the same experience for LRT assets on Eclipse.
05:21:02.435 - 05:22:00.483, Speaker A: So we've launched the ETH URT and then we're in the process of working on a version for Sol as well. And so excited to combine that to create some interesting opportunities around SOL and Eth Defi. So what's next? So I'm sure a lot of you have seen talks create around raising the bar for L2s on Ethereum, right? So this is something that we take extremely seriously in Eclipse. So as part of that road to building a trustless L2, the next features in the roadmap are to build trustless deposits with chain derivation fraud proofs with forced inclusion like I mentioned earlier, and then a sequencer rotation protocol with governance as well. And just to hone in on this point, like I was saying, Eclipse as an L2 can really lean into a much higher TPS, potentially even than Solana L1 can over the long term. Right. And this is something that we're very focused on.
05:22:00.483 - 05:22:31.173, Speaker A: And working with a single rotating sequencer is a way to achieve this. Right. Or what we call thick or beefy sequencers. And so our division and goal over the long term is to bring high throughput, defi and consumer back on chain. Right. In Ethereum a lot of that has moved off chain and we want to bring those same capabilities that Solana's had back to Ethereum users assets. And then we're also focused on onboarding mainstream and institutional users with the best in class UX or product experiences.
05:22:31.173 - 05:23:16.737, Speaker A: Right. This is no surprise to us in this room, but one thing that we're particularly excited about is being able to bring a unique block Explorer experience to Eclipses. Well, that we're hoping to give back to Solana Alpha and something that we're excited about. And then lastly, Public mainnet is coming in October. So towards the end of October we're excited to start rolling out a lot of the front ends and start talking about the app experiences that are launching and then what's next for Eclipse after that answer. So thank you guys. Let's go.
05:23:16.737 - 05:23:37.105, Speaker A: All right, give it up again for PJ everybody. How's everyone doing? Nice, Nice. Yeah, Eclipse is cool. Let's see here. I think we're gonna get ready for yet another product keynote. How's everyone doing? With the right brand, there's a lot of right brain happening in here. Yeah, pretty good.
05:23:37.105 - 05:24:12.335, Speaker A: Let's see here. The next talk is going to be about the best execution here. And as Omar gets his laptop set up, I'm curious, who here has done the boxing downstairs with Bonk? Has anyone done that yet? No, no, Ryker's boxed. Okay, all right, that makes sense. Has Anyone plugged the otter? I believe it's. Is it chiton? No. Oh man, that was so right.
05:24:12.335 - 05:24:41.695, Speaker A: Curve. All right, what? Who am I? Well, I'm a nerd like everybody else here. Okay, I think we're ready. Let's give a round of applause for Omar Zaki, everybody. Hey guys. So I'm here to present Mantis. So what is Mantis? We're a Solana Network Extension L2 focused on processing user intents.
05:24:41.695 - 05:25:28.315, Speaker A: So when you sort of think about what does the actual apple of crypto look like? The non custodial apple of crypto, you need three main in our opinion, you need a solid application layer where users can express themselves. That's what we call intense. You need a processing layer and you need a settlement layer. So for us, our architecture has all these different components. We have the Mantis rollup which has solvers directly plugging in. It's completely segregated from the Solana mempool, but everything that occurs on the Mantis roll up cells of Solana. We also have the Mantis protocol, so solvers directly tap in and are able to process these intents and come up with different solutions.
05:25:28.315 - 05:26:16.075, Speaker A: And then we also have the Mantis application. So a little bit about the Mantis rollup. What is the Mantis rollup? It's a natively IBC enabled SVM rollup on Solana. Its sole purpose is to ensure that users are processing or able to submit intents from both Ethereum and Solana and serve as a processing center for those intents. So simultaneously users are able to bridge this roll up and have a Mantis account and directly earn native yield. This yield is a bit similar to how people earn yield on the last Sylvanas staking and also lending protocols, primarily margin fi for stablecoin yields. So I'm just gonna.
05:26:16.075 - 05:27:19.935, Speaker A: I'm gonna show later on that the Mantis rollup is live and producing blocks. And we also have this native IBC connection already. Why is IBC important And what is IBC? It allows for interoperability between the Solana L1 and the Solana L2, but also with Ethereum and other Ether ecosystems that we've connected over ibc, including the Cosmos ecosystem. So what is the Mantis protocol? So for us, the intent is if a user wants to buy Solana every Friday for the next eons and eons, that's an intent. And so you can get as sort of general as you'd like and you can submit these intents from Ethereum or Solana. The first use case that we're focused on is sort of like the Uniswap x Cow swap experience, which is on Ethereum and currently processes billions of dollars of volume. And how we're facilitating this is we're able to tap into our network of solvers that either can come up with solutions to the two routes.
05:27:19.935 - 05:28:12.805, Speaker A: So routing a bit similar to Jupiter, they can come up with cows so matching orders between different chains and on the single chain and also coming up with their own solutions where they directly solve these solutions themselves. So all these different solutions are provided to the Mantis rollup and auctioned in the Mantis roll up through the Mantis protocol and then settled on these different chains. So I'm just going to show a quick demo of how that actually works. So just like this, let me just reload. So this is similar to what the application will look like. So you connect your backpack. You already have integrated the mainnet connection with backpack.
05:28:12.805 - 05:28:57.165, Speaker A: You want to swap specifically amount of slurs for usct. And so now I've submitted my intent, I'm signing. It's going to find the solver that's able to solve the solution. The solver has picked up the solution and is now executing the transaction. It should wrap up in a second and then there it is. So that's kind of like the simplest UX that we imagine where people can literally just click these different buttons and copy trade people and instantly be able to get things done in a single click. So just going back to my presentation.
05:28:57.165 - 05:30:10.437, Speaker A: So where are we at right now? Users can directly deposit into the Mantis Pre fund your account phase the applications. It already sort of gives you a good overview of what the application is going to look like. From there we'll be introducing mainnet and people will directly be able to start swapping across these different chains from day one. So like I said, the rollup's already live, I've already demoed it and looking forward to putting this into mainnet production over the next few weeks. Please head over to our booth for any other information and feel free to scan this QR code and pre fund your account to await the Mantis launch. Thank you so much. Okay, coming up here, we got another product keynote and this is a PhD.
05:30:10.437 - 05:30:23.745, Speaker A: We got someone big brain here. The biggest of the brains for the right curve stage, it is Dr. Gemma Green and she's here to tell you about Power Ledger. So let's give it up for Gemma.
05:30:29.815 - 05:30:31.435, Speaker B: Good afternoon everybody.
05:30:31.735 - 05:30:32.871, Speaker A: My name is Gemma.
05:30:32.983 - 05:31:16.045, Speaker B: I'm very excited to be at Breakpoint in Singapore. Thank you so much. To Solana for inviting me. I am going to announce that Power Ledger is expanding onto Solana mainnet today. So we're very excited to be here to share this great news. We hard forked Solana two years ago so we've been using the technology for our products since then and we've really seen the benefits for us and our clients. And the logical next step for us is migrating our products onto mainnet.
05:31:16.045 - 05:32:26.355, Speaker B: And so in doing that we can also be a bigger part of the Solana ecosystem. So I'll just talk with you about the next steps for us in doing that. Firstly, we'll be publishing a light paper in the coming weeks which provide all the technical details. We will be in addition to the Power Token which is an ERC20 token that will remain but we'll lock up a portion of those and that same amount will be issued on Solana. So Power Token will be available on Solana, which we're really excited about and we'll be spinning up a liquidity pool in Raydium so that anyone can buy and sell Power Tokens on Solana. And finally we'll be migrating our products from the Power Ledger blockchain onto Solana mainnet. And you can find out more and track our progress on this on Telegram on X and also on our website.
05:32:26.355 - 05:33:42.611, Speaker B: And for those of you that are not so familiar with Power Ledger, allow me just to provide you a little bit of information. We set up the company in May of 2016 so we're not really OGs, we're more dinosaurs and we have been pioneering peer to peer training of electricity since then. We make software for electricity utilities that want to make new products for their customers to empower them to take control of their energy future. And we're trusted by major brands that you can see here. We also make software for tracking and trading electricity and emissions so that companies can do their reporting but also know how many offsets they might want to procure. And we have a marketplace for environmental attribute certificates like renewable energy certificates and carbon credits. And we've successfully delivered more than 30 projects in 10 countries and we've been named as one of the top 50 companies of crypto Valley in Zug, Switzerland where our head office is based.
05:33:42.611 - 05:34:25.715, Speaker B: We were originally an Australian company, but a year ago we moved to Switzerland where we're now head office and where I am based as well. And there's I think a few other points that I wanted just to share in terms of the benefits both for powerledger and also for the Solana ecosystem in Solana in powerledger, migrating onto Solana mainnet. The first thing is about Deepin. We're a Deepin project and we've been working in Deepin before. That was actually a technical term. But what actually is Deepin, it's about connecting real world assets into Web3. And that really is what Web3 is about.
05:34:25.715 - 05:35:26.917, Speaker B: And we're a founding member of the DPIN association and the Deepin community within Solana is really active and growing and we're super excited to be a part of that and contributing to that. The second point I want to make was that it's important for the Solana foundation that the ecosystem as well as the protocol is as green as it can possibly be. And with our products and also the power token being on mainnet, we're able to interoperably connect with applications and projects. For example, projects like validators might want to track their energy usage or purchase offsets and they'll be able to seamlessly do that. And I think that will make interacting within the ecosystem much more streamlined. And assets like digital assets like carbon credits, in addition to being used for their primary use case, they can also.
05:35:26.981 - 05:35:28.741, Speaker A: Be used for secondary uses like in.
05:35:28.773 - 05:36:17.785, Speaker B: Defi pools as well. So we're super excited to be expanding into Solana mainnet for all of these reasons in addition to the amazing technology. And then finally, we've made this limited edition NFT and you can get a free copy of that by scanning the QR code here. And then in doing so, you'll also qualify for a power airdrop for power tokens on Solana. And we have a few T shirts with this on the back of it. If you'd like, you can find me at one of the powerledger team as well. So powerledger is about making software for the democratization of power and sustainable energy for all.
05:36:17.785 - 05:36:26.199, Speaker B: And we're really excited to be expanding onto a Solana mainnet and being part of and contributing to the ecosystem with you all.
05:36:26.287 - 05:36:50.497, Speaker A: Thank you. Give it up again for Dr. Gemma Green. Cool. Okay, for our first talk of this new hour, we have Lucas Bruder from Jito Labs and he's right backstage here. So I'm glad. Let it come on.
05:36:50.497 - 05:37:30.299, Speaker A: Give a round of applause. Hello everyone, My name is LucasFruter. I'm a CO founder of Geno Labs. I'm here to talk about restaking and moving beyond the hype and the implications of Geno Restaking for Solana protocols. A little bit about myself. Some of you might know me as Buffaloo on Twitter, contributor to Solana core. Between finding bug bounties and the Solana validator to contributing to Jito and building a lot of MIF info on Solana.
05:37:30.299 - 05:38:11.143, Speaker A: My background is in computer engineering and firmware. So I'm going to talk a little bit about Solana. Why are we here? Why do you guys like Solana? I think it's the, the best place for builders. Solana loves builders shared vision. You know, there's not a ton of politics on Solana, you know, that everyone can get behind Anatoly's vision of propagating information at the speed of light around the world. Everyone's kind of rowing in the same direction towards the same thing. There's an extremely rapid iteration if you use the network a few months ago to using it today.
05:38:11.143 - 05:38:32.285, Speaker A: It's like night and day. There's been a ton of improvements at the network layer. You know, Fire Dancer is coming online or I guess it's online today on Mainnet. So things are just moving very, very fast. You can do more on chain. Look at some of the order books on Solana that, you know, you can't do that on Ethereum or some of the other networks. So.
05:38:32.285 - 05:39:14.001, Speaker A: So because of this, all these things, Solana is the best network for builders. Now let me talk about stakenet and like I think this is like only possible on Solana protocol. Stakenet is the brain of the geodosoul stake pool. So maybe some of you saw last year we talked about stakenet and it's live today. It's been live on Solana for a few months now. And stakenet is responsible for operating the jitosoul stake pool and it's completely decentralized. It's running the state machine on Solana where it will delegate stake to different validators.
05:39:14.001 - 05:39:39.905, Speaker A: If validators start performing poorly or there's better options, it will remove the stake and move it to a different validator. All of the scores and metrics that you see on the screen here, those are stored on Solana. So you can see this like bonk validator that JITO runs. You can see the score of 97.63. You can see all the MEV Commission scores. All this data is on Solana. Solana is the database.
05:39:39.905 - 05:40:14.115, Speaker A: This other chart is the state machine. So you can see if you go to jito.net, you can see this running. It will run and then periodically rebounds the stakeholder. Sticknet's pretty cool, but there's some issues with it. You can see here, this is the x axis, here is the epoch, the Solana epoch and the Y axis is just different properties of different fields. So you can see on the software version there's a few epochs where the version wasn't uploaded.
05:40:14.115 - 05:41:41.035, Speaker A: You can actually see the network on here going from one version to the next version and the upgrades of validators on the network. There's also been some bugs in the cranker where it will upload the wrong data on a permission feed and a lot of the data on Stigma is available on chain but there's a few things that are available on chain and so it's essentially a permission to Oracle. So yeah, there's, you know, better uptime. Would like to see better uptime on some of these things. If it's going to run a state pool which Currently today is 13, roughly 13 million SOL and continue to grow, the standard needs to be very reliable. There needs to be incentives for kind of cranking these keepers and there needs to be punishments if you upload the wrong data or you're late or your keeper goes down. Yeah, jito, it's a pretty technical protocol between the mev, the liquid staking and now kind of the restaking and we have a few different products at various stages of decentralization and we, we realized that there's kind of a common theme between a lot of these things and there's something missing that was required to decentralize things much more than they are today.
05:41:41.035 - 05:42:15.945, Speaker A: And so this is the main reason why we created the restaking protocol. For those that don't know, restaking is essentially just the assets of. It's the process of staking assets to secure a network. And I put the here because Judo Restaking lets you stake any asset. How does it work? So you have a few different parties in the restaking protocol that all kind of work together. So you can think of networks. It's like a rule based protocol, decentralized.
05:42:15.945 - 05:42:58.249, Speaker A: These can be running on Solana. The most obvious example here is in Oracle or perhaps like the safety net keeper that I was just showing you. And this network defines the rules for how the rewards work. How are validators and operators punished? You have operators as well. Operators are running the software for the networks. This can be an L2 can be a keepers that I just mentioned and then you have stakers and these are the people that are staking to these operators. And one of the main things with restaking is that assets can be shared across multiple networks.
05:42:58.249 - 05:43:30.791, Speaker A: So you can do shared, you can do isolated networks. The JITO restaking protocol is very customizable and very Flexible. So yeah, JITO restaking, it's a multi asset restaking protocol for NAT consensus networks. It's SVM native, so it runs on Solana. Spent a lot of time optimizing the code. A lot of zero copy and very efficient asset agnostic so you can use a variety of assets. Yesterday I was on stage with Jeremy announcing Renzo's Easy usdc.
05:43:30.791 - 05:44:00.125, Speaker A: So there will be networks that can work with usdc. You can sync that and it's also very flexible. You can change the arrangements of networks, what assets you support. If you're building a network, you can launch it with Geno Sol or Sol or usdc. Once your governance token is out, you can start to kind of move some of the security over to that. And so it's very flexible. Another main primitive of this is the vault receipt tokens.
05:44:00.125 - 05:44:44.425, Speaker A: We kind of saw what was happening with LRTS and especially liquid staking tokens. You can think of vault receipt tokens like liquid staking tokens on Solana. So you are staking to the vault and you get back a receipt token that is a tokenized representation of your pro rata share in the pool. And so this is built into the program. You do not need to build a vault receipt token or lrt. The protocol just has it built in and can continue to use it in defi go on exchanges, borrow all that stuff. So yeah, beyond the hype, I think it's important to highlight some of the use cases of restaking.
05:44:44.425 - 05:45:36.237, Speaker A: First one is stakenet. I showed you guys the data. This is actually a copy paste of a graphic that I put on the slide last year. So I've been thinking about this for a while. Being able to kind of decentralize some of the data fields for StakeNet so it can be truly permissionless and Judo, sol and other LSTs that want to use it can run forever without requiring centralized services. One of my favorite use cases of this free staking protocol is Squads Squads as you guys know, largest multisig on Solana and they are building Squad's policy network. This will act as another signer on your multisig and it allows for very, very custom and flexible arrangements for signers on the multisig.
05:45:36.237 - 05:47:00.895, Speaker A: So you can think of like automation and thresholds and things that are really hard to calculate and perform on chain can be offloaded to a network of. And these are examining the transactions and checking the policies and they all come to consensus on signing, acting as a signer on the multisig. Something else that we're in the process of designing and building. And this will be the first network that we build and it's kind of a precursor to other networks is the reward network. And this will allow, if you are building an NCN or a vault and you want to distribute rewards to stakers and operators, this will be a network that will handle that for you. So essentially what you can do is you build your network all your on chain information, let's say Oracle for instance, and you want to distribute it to different operators based on some criteria. Then you can write this off chain logic that will run on a variety of operators and that will essentially all compute a merkle root and a merkle tree and they will upload it on chain and that will come to consensus on how rewards are distributed.
05:47:00.895 - 05:47:33.995, Speaker A: So we're actively dogfooding this technology and the platform. I think that's the only way that these platform kind of plays can succeed. And there's certainly some other things that we're working on that be sharing soon. So yeah, wrapping it up. I think Solana loves builders. It's the best place to build, you know, compared to a year ago to where we are now. It's incredible to see the progress that's made on the actual technology, technology, the ecosystem and all the builders that are coming in.
05:47:33.995 - 05:48:29.935, Speaker A: The reason we created this restaking protocol is because we think that Solana is the best place to do restaking and it solves a real problem for jito. We want to make stakenet truly decentralized. We want to decentralize a lot of products that JITO builds and this is the way that we think is the best way to do it and expect to see JITA restaking live in later 2024 and some of the network starting to come online later this year and early next year. That's all I have. Thank you guys. So I'm going to be the MC for the next two hours as we close off the dev shut at Dev Stage and Breakpoint. So a bit about myself.
05:48:29.935 - 05:48:52.313, Speaker A: I'm Nero. I'm the CEO and founder of Multichannel Advisors. We are a Web3 management consulting firm. We work with the likes of Wormhole, Zis Network, Helio, Incubator, Shark and a bunch of other companies. So get ready for the next two hours. We got some exciting product launches, product releases, fireside chats and workshops. So enough about me.
05:48:52.313 - 05:49:27.365, Speaker A: To start off the next segment we have Matthew from Layer Zero Labs. He's going to be doing a workshop on Omni chain programs. What's up Solana? Breakpoint, how are you Guys doing that was pretty weak. How are we doing? There we go. I'm going to try and keep this a little bit high energy. I know you guys have sat through and listened to a lot of really interesting presentations, so without further ado we can hopefully jump into a couple more. Next slide please.
05:49:27.365 - 05:49:39.333, Speaker A: Let's see if we can see them. Okay. Of these up. Yeah, exactly. Let's see here. Not able to get the slide moving forward, but that's okay. Today I'll be talking to you a little bit more.
05:49:39.333 - 05:50:37.697, Speaker A: What LayerZero Labs is if you guys aren't familiar, LayerZero Labs and OmniChain interoperability protocol. What does that actually mean though for you app developers? LayerZero is basically a way for you to permissionlessly interact with any chain that we support. Currently at the moment we're live on 80 plus different EVM networks and we launched recently live on Solana as of two months ago. And with Layer zero, one of the things that we're really interested in seeing you guys build out and work on is, you know, new novel use cases for interoperability. One of the big selling points that Layer 0 has over other interoperability solutions is that you're able to configure not only the type of validator set that you use for verifying messages between chains, but also configure the size and scope of those validators. Currently, layer 0 has what's called a decentralized verifier network system, where as an app developer you essentially have control over your own channel between point A and point B of your network. The way that this fundamentally works is pretty interesting.
05:50:37.697 - 05:51:38.597, Speaker A: You're able to basically configure any type of these verifier networks that are currently live today. At the moment, there are 30 plus different verifier networks to choose from. Currently some of the popular ones are Polyhedra, a ZK Light client, Layer zero Labs, Google Cloud, nethermind, and a bunch more that are currently being activated under on Solana today. And really the main benefit that you guys are going to be able to activate from this as an application developer is the ability to say I'm not forced to accept a one size fits all model for my application security. I'm actually able to control and parameterize what type of security I need on a per pathway basis. What that means is there are no more examples where I subscribe and put my trust into to one monolithic verifier network and that verifier network fails and outright destroys not only your application due to invalid message delivery. That's malicious, but it no longer means that every application has to subscribe to a single verifier network.
05:51:38.597 - 05:52:41.095, Speaker A: Ultimately for us, what that means is that you're able to not only unlock more and more use cases for not only bridging, because it's able to be customized based on your own use use case, but then it also leads to you being able to activate much, much more execution and overall environment changes for you as the app developer. Let's see if we can get the slides Moving forward here, I'm not sure if we can do next. If not, no worries. It's going to be a bit more of an informal talk then. I think one of the big things I was going to talk about here, regardless of the slides or not, was basically going through kind of understanding where we've gone from bridge infrastructure from 0 to 1. As some of you may be familiar with, the way that bridge infrastructure historically worked as an app developer was that whenever I wanted to move to a new chain, I had to figure out a very complex problem. And that problem was how am I able to not only send messages from chain A to chain B, but also be able to not only ensure that the validity of that message was maintained in a cross chain environment.
05:52:41.095 - 05:54:01.393, Speaker A: When I say validity of a message, what I mean is that you need to be able to send not only whatever your input state changes are on a source blockchain, say Solana, but then ensure that that state change that has occurred is then transmitted correctly to the destination. Historically, the way that this worked was that you would have some type of bridge security, whether that was a centralized provider, whether that was a single network collection of ciders, or even a middle chain bridge that would basically validate that this message delivery occurred on the source blockchain network and then deliver that message through to the destination chain. One major problem with this type of approach though, was that what happens when that validation fails, whether there's a back door into the cider clients that you have, where the centralized entity turns on you, or whether the relaying between a middle chain bridge just simply doesn't have enough. No diversity. The major issues that could occur from your centralized or validator set failing was that suddenly that attacker could take advantage of everything on the destination chain. If you're able to pass any type of message through as valid, with no security checks on who can pass that message, it means that you're now fundamentally hurting not only the application that subscribes to that validation network. You're hurting every Single type of transfer that's happening between point A and point B.
05:54:01.393 - 05:54:34.163, Speaker A: Historically, this has accounted for around four out of the five largest bridge hacks and, you know, losses of monetary value in blockchain history. Overall, around $2.3 billion have been lost to bridge security related hacks. Which means that as a user you really need to make some stark trade offs around how you determine your application security when it comes to moving data between chains. For us, this is where we feel that layer zero comes in and provides a really valuable utility. For Layer zero. We're built on three principles in particular.
05:54:34.163 - 05:55:24.107, Speaker A: One, that the protocol is permissionless. Any application built on any of the networks that we support can uniquely go and access not only the layer 0 endpoint smart contract and send a message to any of the pathways that we support, but two, once a message has been emitted out from the source chain, layer 0, the protocol has no ability to strike down or censor messages once they're in flight. This unique censorship resistance is really, really valuable in a cross chain environment. If someone was able to censor packets while they were off chain being moved to the destination, it would mean that fundamentally your protocol wasn't secure. Ah, there we go. Are we able to see that? Okay, I'm going to try and catch up really quick, but we're at a pretty good spot. So again, permissionless, censorship resistant and next up here, immutable.
05:55:24.107 - 05:56:07.055, Speaker A: One of the big pieces for Layer zero as a protocol is that our smart contracts and the protocol contract standards that we put out are immutable. We ask you, the developer, to not only deploy your own Solana programs, but we make sure that the core protocol infrastructure can't be changed or upgraded by us as centralized entity. We want to make sure that once you interact with this interface, it is immutable and unchanging. That's very, very important for you, the app developer. So let's get into a little bit more of what makes Layer zero bridge infrastructure. One of the things I talk about a lot is this analogy to the real world. Bridges are really, really important because not only do they connect one isolated groups of people, but they also connect the INF that flows between them.
05:56:07.055 - 05:57:20.363, Speaker A: And that information can be really, really valuable because you're not only able to coordinate the transfer of goods and services, but using that information, eventually financialize those assets and enable these isolated parties to exchange information. Catching up a little bit here, we understand that cross chain is reality. Different chains have different value propositions, both in terms of speed, cost, potentially communities, funding, etc. And we see developers moving to more and more chains from the Electric Capital Developer report back in 2023 and overall understanding kind of this, you know, infrastructure that I mentioned before, this idea that token transfers and movement of information on a single blockchain is relatively straightforward, but movement on more than one chain becomes infinitely more complicated. This idea of needing to invoke a call on a source chain that eventually changes the state on the source and then correctly relay it to the destination to trigger some other function call. Again, as I mentioned before, taking advantage of these cross chain transfers, if you don't have proper security in place, can lead to really, really not so nice outcomes for the average user. And that's where this kind of idea of monolithic bridge security comes into play.
05:57:20.363 - 05:58:22.223, Speaker A: Monolithic bridge security, in my opinion, as I mentioned before, isn't really all that secure because if your security fails, not only you, the single application, but every application on that network is fundamentally now at risk. And as I mentioned previously, this is why we have such large exploits in this space, because every application falls prey to a single validator set failing. Ultimately, it's because of this channel basis where every app is forced to use the same security channel for their bridge infrastructure. And for us, what that means is that app developers like yourself are forced to make a really nasty trade off. Network A might be providing a very low security threshold, but much cheaper for bridge infrastructure or Network B might be providing you with a really, really high cost of security, but at the same time a really, really high level of security to go with it. Obviously this is a bit of a simplification, but the main takeaway is that the switching cost between these different bridge providers is really high. Typically it requires core contract changes, different function methods to be invoked, and ultimately redeployments potentially on your application.
05:58:22.223 - 05:59:05.405, Speaker A: That's why I like to say if Ethereum is a dark forest, you can think of cross chain as being a dark ocean. You have all these isolated islands of connectivity, and if some malicious actor gains control of the pathway between them, the islands face an existential threat. Ultimately, if the actor also destroys the port of entry, it also faces existential threat to the island itself. So for layer zero, what we kind of see as a solution to this issue of monolithic bridge security is what we call our on the chain protocol stack. The idea that applications built on Layer 0 are able to access these immutable Layer 0 endpoint contracts and actually configure the type of security that they use. And more importantly, there's the type. Also the scope of security the number of validators that you have in your stack.
05:59:05.405 - 05:59:56.303, Speaker A: Beyond that, we also provide a broad function call layer, the ability for message execution to be done automatically on behalf of the application that is calling it. Also delivering native GAS tokens and other composable function calls that might be necessary for a destination transaction. And finally, broad smart contract application standards such as the oap, which is a way for you to move any type of bytes, input or data between chains. And the oft the Omnichain Fungible Token Standard, which basically extends just the native SPL token model on Solana and combines it with ERC20 on the destination. At the moment ONFT is grayed out. Right now we are working with the Metaplex guys on getting that set up for Solana, but ERC721 is currently live on all 80 chains that we support. Getting a little bit diving in and really looking into the message workflow.
05:59:56.303 - 06:00:45.251, Speaker A: When you send messages with layer zero, the way that it works is that the application defines a message in bytes. They want to send a destination chain as an endpoint identifier and a send library configuration. This configuration determines the type of security that you use. In this case, there are four DVNs configured here and the executor that you want to use. From there, those decentralized verifier networks will validate on the destination chain and compare against the reception receive library config that you, the app developer have set, which specifies the finality that you expect them to wait, the number of deviants to provide the signature, and the threshold for verification for a message to be considered verified. From there, an executor, whether it's one that we run or a permissionless one, one that you interact with yourself, can deliver the message automatically through. For Solana, this is a little bit different.
06:00:45.251 - 06:02:06.365, Speaker A: The way that it works is that we commit a message and basically say that it's been verified from the DBN set, and then two this message gets executed through via the destination app and then there's a quick little relay through cross program invocation down to the destination app. So super interesting. Without getting too much into it, the way that the layer 0 endpoint works is that it basically defines this interface and this channel setup for your app itself, which allows us to configure not only the type of secure that we want to use via these appendable message libraries, but it provides you with a really interesting app use case where if as layer 0 becomes more developed and there are better validation techniques that come online, you're able to suddenly extend existing applications by opting into new configuration parameters and setups. So really, really unique here. And what that means for an app developer like yourself is that when we look back at security cost curve, not only does layer 0 support point A and point B on this curve, but it supports every single point in between. And more importantly than that, as new validation techniques come online, you're suddenly able to unlock better security over time by opting into it yourself as the app developer. And if you prefer the existing setup that you already might have, you're able to continue running the same configuration as is with no changes to application.
06:02:06.365 - 06:02:55.119, Speaker A: So it gives you the best of both worlds in terms of what can be a decentralized verifier network. There are currently 30 plus that are live today. At the moment, some of the ones that you can configure between are Polyhedra, a ZK Light client, Lagrange Cryptographically Secure State Proofs, or an existing provider like Axelr, CCIP or Google Cloud. And what's really interesting there is that it means that you can not only control the scale of your verification, but you're also able to now have different permutations of these different providers. And on top of that, add yourself as a verifier to provide redundancy. And what that means is that layer zero is broadly scalable for verification, diving in a little bit deeper into kind of the execution side of the workflow. Traditionally, the way that execution works on an EVM compatible blockchain is that messaging is atomic.
06:02:55.119 - 06:04:26.409, Speaker A: So either everything succeeds the execution level or reverse. And that might be useful in a lot of cases, but for cross chain messaging, it can be a little difficult for the app developer because if, let's say on the destination chain you want to send a token, swap that token and then stake it all within one workflow from the user perspective, if one of those calls fails, the entire transaction reverts. And realistically, what that means for you, the app developer, is that you need to think of more creative ways to build your application that might be unintuitive for a user who's interacting with it. Layer zero built this concept of horizontal composability, a way for you to basically get around this issue of atomicity by for a single receive call defining all of the inputs that you want to change. So in the token transfer example, let's say the first call in that stack is receiving tokens and then execute a second message back up to the endpoint that pings the executor on the destination chain and executes an entirely separate workflow transaction called LZ Components. And what that means is that when you're delivering, you know, data or any type of transaction in a cross chain environment, you can separate now your workflow. So in the example I mentioned previously, swapping tokens, let's say in that second call the swap fails, you can know that the token transfer succeeded on destination without needing to worry about the entire transaction stack reverting, giving you a lot more broad configurability and kind of how cross chain workflows behave beyond just kind of the execution types that you have available.
06:04:26.409 - 06:05:40.009, Speaker A: LayerZero also supports gas drops down to the destination chain. So for example, with Solana, if I wanted to have a user interact on Solana itself, they could make a call to the smart contract that I've deployed and say that I want to purchase a certain amount of GAS and deliver it to a destination app. Or as a developer, I can specify that I want this gas to be transferred to an end user's address along with the cross chain message. And so it means that not only can you help ease the onboarding process for your users as they move from chain A to chain B, but also ensure that there's automatic execution that you don't run into any weird GAS issues around with that. So everything that I've talked about in the last few minutes, hopefully you've caught up. Basically builds this idea that omnichain applications allow any smart contract on any blockchain network that LayerZero supports to not only control the validation that they need for every pathway that they want to move to, but also control the ability to move between not only chain A and chain B, but have relays of transactions between every network that they want to be on. Ultimately diving back into that omnichain transfer that I mentioned before, this allows you to not only control again, the security setup itself, but feel confident that you're able to work through.
06:05:40.009 - 06:06:35.577, Speaker A: And if a validation network fails, know that you're not going to be at risk of having everything within your application lost and potentially having to worry about what happens to your user base as well. All of this kind of builds to the idea of what we think of as an omni chain tech stack. The idea that any application with any broad use case, whether that's DeFi, SocialFi, Gaming, NFTs, Oracle feeds, etc. Can interact with the layer 0 endpoint smart contract and be comfortable knowing that not only are they able to kind of meet the broad needs that they have for their application, but Be sure they're able to not only solve the problem areas that they need and really, really work well within the configurable set. So I think if you go to QR code right now, I believe right now it goes to a couple example design patterns that we have if you're interested. On GitHub, I went a little bit fast to kind of catch up on the slides. So happy to kind of go through and answer any questions that there might be from the audience too.
06:06:35.577 - 06:07:13.295, Speaker A: But yeah, thank you guys. Oh, probably no questions then, sorry, I'll meet you after that. Thank you guys. Next up, we use product keynote by Sunny Agro from Osmosis Pliers, cross chain router and all change. Let's give it up for Sunny. Hey all, thank you all for coming. I'm excited to be here at my second break point.
06:07:13.295 - 06:07:52.511, Speaker A: So I'll be talking today about a new product that we just announced last week called Polaris. But before that, let me tell you a little bit about how we got here and why we're building this. So for those who don't know, I'm the co founder of a project called Osmosis. Osmosis is a Dex app chain, which means it's a Dex built as its own standalone blockchain. It's currently the dominant Dex in the Cosmos ecosystem. And you know, it's connected to over 100 different blockchains. It's the primary liquidity venue for most of the major Cosmos ecosystem assets, tia, akt, dydx.
06:07:52.511 - 06:08:40.779, Speaker A: And we're growing beyond that as well with a big focus on Bitcoin liquidity and a lot of the, you know, one of the main things that a lot of our community loves us for is our commitment to like Stellar ux. And we do that by a number of different ways, getting updated on the app chain features, building things like one click trading. You can trade rapidly without having to sign with your wallet every single time. The Osmosis chain lets you pay your gas in any token, including usdc, Bitcoin, Sol, whatever you want. We have native cross chain swaps. You can swap from one Cosmos chain to another Cosmos chain without making any transactions on Osmosis and working on native privacy stuff so you can stop prying eyes from copying their trading strategies. But we have a problem.
06:08:40.779 - 06:09:39.875, Speaker A: We built a lot of these amazing features into Osmosis that have created this great trading experience. But we face this thing we call the great chain divide, where the problem is liquidity is very fragmented across all these ecosystems. And while we created this amazing product for The Cosmos ecosystem we struggled to how do we reach users who are elsewhere? And the problem today is almost every application. Most of the infrastructure in crypto, wallets, liquidity, venues and interfaces that service them are designed in a very chain centric way. Right? But the problem is users are not chain maxis, right? The apps might be chain maxis right now, but the users say that want to trade everything and we want to build apps for users, not for the chains. And so we were looking at, okay, how do we solve this problem, right? Users have wallets that they like to use and you know, liquidity is very sticky in the chains that they're built on. So we realized the best place to handle this is at the interflip base layer.
06:09:39.875 - 06:09:58.925, Speaker A: So this is why we built Polaris, the token portal. That's like a web portal. It's a single interface that allows you to tap into liquidity across all these different venues and chains. Instead of telling you more about it. I'll show you. So you log in, you come in. The first thing you do is you go ahead and connect your wallet.
06:09:58.925 - 06:10:37.513, Speaker A: When you connect a wallet, you can choose which ecosystems you want to connect it for. So I'll connect my phantom wallet for both Solana and Bitcoin. You can also choose to filter by ecosystem. So I'll see like, okay, what are the EVM wallets I can connect? I'll go ahead and connect my Metamask and there, cool. So I have my phantom wallet connected for Solana and Bitcoin and Metamask connected for a slew of EVM chains. Now I can go ahead and open up the portfolio drawer. This is one of the most useful features for me personally because you can now see all your balances across all the networks aggregated and it even aggregates like kind assembly.
06:10:37.513 - 06:11:16.895, Speaker A: So for example, I can come over to Bitcoin, it will collapse my, you know, it'll have both my BTC on Bitcoin Mainnet and my WBTC on Ethereum. Same for something like Ethereum, right? It doesn't matter whether I'm holding my ETH on Ethereum or on an L2, whether it's held as wrapped eth or even as an lsd. Right? It collapses all of those into one place. If users really want to see the more network centric view, they can do that. We will break down the assets by network. It's on. And when you're trading as well, you can choose, hey, where do I want to receive my Bitcoin? Do I want to receive it on Bitcoin? Mainnet on Ethereum as wbtc, on stacks as btc.
06:11:16.895 - 06:11:53.665, Speaker A: But we really believe that the end state of crypto is that users should not really have to even be thinking about what network they're receiving on. So let's go ahead and make a trade on a sell and then you open up this ledger. You can use the network selector on the left. But once again, we really want people to try to use this all networks view which just aggregates everything. So I can go ahead and search for the token I want to buy and you'll see, hey, cool, I can buy this token on base and it will show up in my MetaMask wallet. Let's try to do something even more challenging. Instead of buying Degen, let's go ahead and buy something.
06:11:53.665 - 06:12:40.813, Speaker A: Like, you'll notice I don't actually even have a TON wallet. So what can I do? You know, I can go ahead and it will give me the option of connecting a TON wallet and there's a bunch of ton wallets that you can install. But I don't want to go install a new TON wallet, right? So instead we have this option called Polaris Vaults. So Polaris Vaults are one of the unique features that we built. So what essentially happens is there is a Polaris vault is a way of expanding, letting your existing wallet support networks that they don't already support. It will use an MPC service run currently by eight independent nodes, but eventually by the osmosis validator set to create a new key that's custody that's controlled by your existing wallet. You can also choose to deactivate Polaris vaults and do it manually.
06:12:40.813 - 06:13:20.409, Speaker A: But the problem is that you have to sit there and sign every single transaction ban. So instead we have this autopilot feature built into the player's vaults, where what it will do is it will secure the vault with your existing wallet and you'll see that here's a very complex trade, right? To go from SOL on Solana to Ton on the TON network, you have to do many steps. We're going to swap to USDC using Jupyter. We're going to bridge to Osmosis with cctp. We're going to swap to USDT on Osmosis. We're going to bridge the USDT to Ton and they're going to swap to TON on DNS, right? These are like five different steps and if you were going to do this by yourself, you'd have to go to five different sites. You probably don't even.
06:13:20.409 - 06:14:06.065, Speaker A: I bet most of you probably don't even know what ton bridge to use. Right? Like, but instead what we do is we aggregate all of this in one. It's all run by autopilot. You know, all you have to do is two transactions. One is you go ahead and prepay your gas fees, right? So part of the problem is what you're doing this you might have figured out how to get to USDT on time, but oops, you can't pay your gas fees. That's why what we do is we automate all the gas fee payments for you using our gas station and then once you've initiated the autopilot, you can log off, go do your own thing, go buy something else and this whole transaction will run in the background automatically and you're not going to skip forward and you'll end up with time in your, in your Polaris vault. So yeah, that's.
06:14:06.065 - 06:14:39.915, Speaker A: Well that's Polaris, your token portal. A place where you'll be able to trade all tokens across all chains in one app. If you want early access, check over go to the site Polaris app and you can sign up for the early access or you can follow us on Twitter alarisapp. Thank you. Next up we got a lot of keynote by Zeus Networks, one of MCA's clients. We got Dean Little and Jim, both co founders of Zeus network. Let's give it up, give him a hand.
06:14:39.915 - 06:15:38.319, Speaker A: Thank you everybody. So I'm Jim, co founder and CTA with the same word. A lot of people are asking what we're doing as this network to simply put WGBBS we going to bring Bitcoin to Solana so we're building a Solana point 5 which is powered by Zeus node and ZPL aka Zeus program library to bring Bitcoin to Solana. So there are three exciting things I would like to share with you guys. The first thing is we have launched Apollo which allows users to deposit BTC to earn yield. Ever since our launch one month ago we've seen over 40,000 users already trying to use Solana and the number is still going up. So we're so proud to announce our second product which is zeuscan a bitcoin Solana interaction explorer which allows you to see what's happening under the hood in our Zeus node and DPL and it's completely lagged now.
06:15:38.319 - 06:16:25.975, Speaker A: Just Google Zoolscan IO so first we will probably announce Zeus Theory which is our next killing product. It is a bitcoin and solar infrastructure so there are actually multiple steps you can see on the diagram I would like to point out one most critical step, which is we need to pack a lot of BTC from Bitcoin side to Solana side via a program called Simplify Payment Verification Program. So this is a people asking questions. So we are planning to launch Q4 this year. So obviously there are a lot of things to be achieved, million things to be resolved before. So one of the very first challenges we need to resolve is how do we build a robust BSV program Solana. So I would like to help my chief scientist to talk more about that.
06:16:25.975 - 06:17:20.155, Speaker A: In section 8 of the Bitcoin white paper Satoshi detailed Simplified payment verification. This is a means for light clients to be able to have efficiency sync proof of inclusion of any transaction in the Bitcoin blockchain by simply following the longest chain of proof of work. All that's actually required to implement this on Solana is a library that's capable of efficiently generating and validating these proofs and it works even better when you shave 20 cus off of the standard implementation of SHA256 then you come to the problem of data. How do we get transactions and proofs on chain really fast? Because with ordinals, inscriptions and other kinds of exotic taproot scripts, some of these transactions are quite large. So we created Chatbuffer. Now Chatbuffer is actually an SBPF assembly optimized parallel data buffer that enables you to put huge amounts of data on chain really fast. And this is also utilized by other teams with big data problems like squads.
06:17:20.155 - 06:18:12.395, Speaker A: Finally we have the actual implementation of the Bitcoin protocol itself. Logos is a library that will be Open Sourcing in Q1 of 2025 written by yours truly that implements the entire Bitcoin protocol in Rust with three compiled targets, native Rust, SBPF as well as WASM enabling you to utilize Solana to make truly full stack bitcoin apps. This is powered by our currently Open source Solana SCCP 2x6k1 library. Why? Because we could have just left it at Merkle Tree as an ECDSA verification. These are data supported by Solana, but we wanted to tackle the two new cryptographic primitives introduced by Taproot, that is Tweaked public keys and schnorr signatures. These require easy multiplication and easy addition 5 million and 400,000 cus each. A naive implementation of this might be that we hack the ecdsa signing algorithm itself.
06:18:12.395 - 06:19:35.035, Speaker A: We start by negating the mod inf k by feeding in age nonce of 1 and then we negate the hash scaler by feeding in a hash of 0 to reduce the algorithm of generating a signature down to rkmod n, a big number operation, enabling us to extract a public key that we can then add to another public key on chain 3 EC edition for 400kcus and that unlocks tweaked addresses. However, what if I told you we can do way better? See, when you actually look at why this works, if you look at the math under the hood, the right hand of this is a negative z mol g, negative Z being our hash scalar. If we negate the hash scalar, we're able to use this as a mol g and then turn it from a negative into a positive, creating an easy add to the point R on the left hand side, reducing the algorithm down to p k mol g. The reason for this is because the way it was implemented on Solana, it does not hash the output of the public key, giving it superior cryptographic properties. Another thing we can do is we can actually decompress and invert public keys using this function. This enables developers to decide if they would like to trade 25kcus for 32 bytes of precious instruction data. But the most important thing is actually if we negate the scalar on the right hand side by feeding in a z a z of zero, what we actually end up doing is creating an arbitrary easy multiplication of smol r on the left hand side.
06:19:35.035 - 06:19:54.155, Speaker A: This actually unlocks everything. And when I mean everything, I mean everything. Schnorr signatures, ring signatures, Peterson Commitments bulletproofs. You could theoretically build the entire Monero and see Cash privacy stack on Solana without introducing any new cryptography today. Thank you all for listening. Only possible on Solana. Only possible on Solana.
06:19:54.155 - 06:21:15.025, Speaker A: Hello everyone, I'm Alex, the co founder at dbridge and in dbridge we like to say something that everyone who has come to Breakpoint can already feel in their bones. We are saying Defi does not win. And let's test something we here at breakpoints and at telegram message today, raise your hands. Don't worry, I'm not with Interpol. Yeah, literally everyone. And now imagine if the message that you sent would take 20 minutes to be delivered to recipients. Or if they reply that a sandbag would take 20 minutes to hit your inbox.
06:21:15.025 - 06:22:59.835, Speaker A: Or if the content of the message delivered would be slightly different from the one that was sent, what would be the opportunity cost? Like even for this conference, all the people you did not meet, all the connections you did not make, all the parties you missed. What am I getting at here? What I'm getting at is that Internet makes magic happen by connecting people in real time. And that's why in dbridge we believe that Defi should be connected in real time too. And we're solving this by building DeFi's Internet of liquidity, the trustless layer that connects information and liquidity in real time for users and builders on Solana and beyond. And the first step of doing this was launching a zero TVL intent based model for bridging at the beginning of last year, enabling virtually instant transfers of information and value with guaranteed rates. And at that time lots of people told us that this model will never work, that liquidity pools with synchronous execution is the only solution for breaching that waiting for 15 minutes till the transfer is settled while losing on slippage is an acceptable status quo. But Solana, you guys were the first to understand this vision and to believe that Defi could do better.
06:22:59.835 - 06:24:16.655, Speaker A: We started to build on Solana back in summer 2021 after receiving a small grant from Solana foundation. And we launched in mainnet on Solana in the middle of the last year and it's with huge joy. I'm proud to say that D Bridge has become Solana's favorite bridge. We help more than 117,000 users to bridge more than $2 billion of liquidity between Solana and other blockchain ecosystems. We are working with partners who share our vision on DeFi, Jupiter, Birdie, Zoneflare, wallets, Zeta markets and many other incredibly hard working teams and stupidly smart guys who work 24, 7 to make DeFi better. Solana has already become the chain with the best user experience. And together with our partners and community our goal is to make it extremely easy to do anything in Defi through Solana.
06:24:16.655 - 06:25:41.085, Speaker A: And here is what's coming next. We'll help to make Solana become a global standard for payments and stablecoin transfers and connect Solana to Tron Ton and many other blockchain ecosystems, opening up access to more than $60 billion of stablecoin liquidity. Native Bitcoin is coming to Solana through D Bridge, the most secure cross chain infrastructure today. Imagine you'll be able to trade any liquid on chain asset to native Bitcoin instantly with settlement happening on Solana thanks to its fast finality catless bridging, same chain intents and social fi apps will be among other things that will be powered by tbridge. But before that, now it's time to you to take control over dbridge. It's time for DPR and today we'd like to answer the question that we were asked most during the last month when token DPR goes live on October 17th, 8am UTC time with Jupiter LMG launch happening 48 hours prior to that. Thank you and let's go.
06:25:41.085 - 06:26:54.911, Speaker A: Hello everyone, I'm Ian. I do developer relations and partnerships at Tools for Humanity supporting the worldcoin ecosystem and I'm here to tell you about how we are bringing World ID to Solana. So how do you know who or what is a real and unique person online? Right, this is a big tough question. Captchas don't really work super well anymore and we don't want to have to go around scanning everyone's IDs just to know that they're actually a person and not a bot. World ID is built to be the answer to that question. With World ID you can know that your users a real human doing something only once without them revealing to anyone not even a world coin which unique human they are. And for that we use this device called the Orb.
06:26:54.911 - 06:27:41.495, Speaker A: The Orb is a trusted hardware device that checks to make sure that you're human. It takes a photo of each of your irises. On the device calculates an iris code, a digital representation of the pattern of your iris which is only used to check and make sure that you only verify the orb once and that it verifies your World id adding your world public key to a Merkle tree On Ethereum Mainnet we built World ID to be privacy first, decentralized and open source. And there's a lot of different use cases for World id. Right? Bot resistance and civil resistance make sense for a ton of different use cases. Personally I think social media is a very obvious one, but a very applicable one. But my personal favorite is customer incentives.
06:27:41.495 - 06:28:42.005, Speaker A: I remember back when we had the two week free trial of Netflix and I definitely spun up more than a handful of email accounts to keep getting those free trials. And then Netflix got rid of it because people like me kept using Netflix without paying for it and taking advantage of the lack of civil resistance. But World ID can fix that. We've built World ID to be super easy to integrate across the web mobile applications and on chain applications. But so far those on chain applications have mostly been in the Ethereum ecosystem. Right? But we've had a few different Solana community applications use World ID off chain. Recently Discover launched there World ID integration so you can with World ID and quite a few months ago Drip integrated World ID to offer a sort of sign on bonus of droplets to users who are verified with World id.
06:28:42.005 - 06:29:43.395, Speaker A: And what we saw is that these two integrations have been some of our most successful World ID integrations. We trialed both of these, highlighting them to users in World App, our mobile application, and we saw tens of thousands up into the hundreds of thousands of users verifying with World ID on these apps. We said, well, why don't we have World ID available on Solana? It turns out that was because of the BN254 syscalls which are now available or will be in the near future. So we hit up Wormhole Labs and gave them a grant to implement our smart contracts for Solana and to bring bring World ID over from Ethereum Mainnet to Solana so you can use World ID natively on chain. What they built is currently being audited, but will be available in production in hopefully the coming weeks, maybe months. Depends on the audit. But I'm incredibly excited that you'll be able to use WorldID on Solana.
06:29:43.395 - 06:30:23.093, Speaker A: So with this right, World ID is proving that your user is a unique human, doing an action only once or however many times you want them to, with a tamper proof message called the Signal. And from the user's perspective, they'll scan this QR code displayed in your front end in the World app. They'll generate this zero knowledge proof, proving that they're actually verified in the orb. And then your application will verify this zero knowledge proof. So this widget on the left is what the user sees. This is IDKit, which you integrate into your front end. We have a few different libraries available for it.
06:30:23.093 - 06:31:38.305, Speaker A: Any of them will work depending on what your front end looks like. In WorldApp, they'll see this prompt to verify for your application, right? But then after that, what do you do? This single call is all you need in order to verify the World ID proof on Solana. And because you know, copying code off of a slide deck is pretty hard, this QR code will link you to the template repository that Wormhole Labs built to use World ID on Solana. So thank you all. Let's build some great stuff together. And if you have any other questions, you can find me on Twitter. Hey everyone, my name is Juan and I'm here to talk about how the filecoin community and the Solana community are coming together to store all of Solana's data in the filecoin network.
06:31:38.305 - 06:32:27.345, Speaker A: Filecoin is the world's largest decentralized storage network. We store on the order of two to three petabytes a day, and it's exabytes in total the Solana ledger, all of it not sustainable transactions terabyte total. That means that every day filecoin stores about seven times all of Solana's data. That's awesome. Means full verifiability over all the data, full random access to all the data and zero knowledge proofs verifying that SPS are continuing to store all of it. So we truly mean we're ready to help you scale to store everything and make it easily random access. But why are there still problems in terms of accessing people's data? It's about the software connecting the FALCO network to the Solana network and the applications.
06:32:27.345 - 06:33:38.915, Speaker A: So today already a bunch of applications in the Solana community use IPFS and dotcoin to store and distribute their data. People use either IPFS storage and pinning services to store all kinds of things from random objects they need in contracts. NFTs meet content for dapps. We also have really cool functionality where username sol their entire operation, all of the assets that represent the entire website and any media they might link to completely stored in a fully way by the entire IPFS network. The way you do this is you first publish a website or media to ipfs, you potentially back it up in filecoin or any other pending services, you set the CID in your records for the sole name and third your application can either request it directly from the IPFS network by doing the resolving logic or you can use a transparent proxy like forsol xyz. This is built by the SNS team, the Floor Everland team and the broader IPOs community. So a lot of coordination across the networks.
06:33:38.915 - 06:35:04.683, Speaker A: The thing that I'm most excited about is being able to back up the entire Solana ledger and then make it accessible in random access with all kinds of indices. There's been an enormous amount of cool work on this in the last year and a half, primarily done by Triton Descent and a number of other folks. Earlier in the year we had some really cool announcements about Project Yellowstone and Old Faithful. This has already consumed the vast majority of the Solana data and is backing it up, archiving it in a fully decentralized setting and providing it for access. You can today already as you store your data in the Solana chain, either hitting the state or not remaining in the state and being in the ledger. All of this is getting archived by Old Faithful and stored in filecoin storage providers, your applications and whatever client can already access all that data through RPCS in noospateful or if you get access to the CIDS directly through contacting the storage providers directly or any other node in the IPFS network. One of the things that's coming soon that we're really excited about is being able to provide indices and random access to all of the compressed data and all the ZKIT compressed data, so you can be able to store and retrieve all that information directly from your applications, whether it's back end stuff or front end code, directly from whatever client you have, whether it's servers, mobile or browsers.
06:35:04.683 - 06:35:58.751, Speaker A: We then want to be able to bring in things like NFT media and websites to be fully automatically backed up as they enter the Solana chain. So ideally, as you publish information and data in Solana, that should get backed up and replicated and made randomly accessible for you. Another really cool thing there is that you can, through the IPFS gateway, you can provide all of this data accessible to any browser. So this is going to be a super cool way of accessing all of the data that you're storing directly in Solana through the IPFS network. A key thing to make this happen is going to be able to be constructing indices for accessing your data. And we're working on figuring out cool native ways by which you can write those indices directly into your code running in the SVM to then automatically produce the indices over there. So a lot of cool stuff coming together again.
06:35:58.751 - 06:36:32.299, Speaker A: The FALCO network is here to help the Solanac community scale. Already storing a huge amount of data for the whole community, NFTs, media applications and so on. We're already backing up a huge fraction of the ledger and making it accessible. Next is all of the CK compressed data and regular compressed data. And beyond that, all of the objects, websites, media and so on that your heart desires to store. Thank you. Hey guys, welcome back.
06:36:32.299 - 06:37:12.042, Speaker A: So next up we got a few fireside chats and technical talks. So we have Zach, which is actually a subsidiary of AD Group and Web3 division of Alipay. So let's give it up for Colby from Zan. Hello everyone. It's a great pleasure to be here to share about our Web3 Innovations from Zan. My name is Kobe Dal, CEO of San. For those people who are not familiar with San, we are new brand for Drama Art Group for Web3 Innovations.
06:37:12.042 - 06:37:58.615, Speaker A: We are studying from last September. It's almost one year till today. So we have served more than 1 billion people in China and also another 1 billion people in the ACPC areas for the web tour. And we built a lot of blockchain infrastructure already. But now we are trying to bring this technology to be available to be shared with you guys. So we want to build a faster, higher, stronger infrastructure together with you. So we all know that web3 massive adoption we need a greater infrastructure not only for tokenization, for applications for their so we will share a few use cases here.
06:37:58.615 - 06:38:40.385, Speaker A: First we will talk about the faster zero target proof. We all know that usability is just like love. You have to listen, you have to care and you are willing to change. And one of the key factors for durability for ZKROJ is about performance speed. So we want to build a faster ZKP service for you. So we all know that ZKP is great but it has lower performance and it's fragmented. You have a lot of options and computing complexity involves a lot of complexity to the developers.
06:38:40.385 - 06:39:37.055, Speaker A: So we launched Xen Power Zebra. Zebra is short for zero. The knowledge has become really accelerated or simple. We provide fg, GPU and software co optimization for the underlying operators and give you a simple interface to do the Deku service. So we have recently released a few performance table on our data account for reference for GPU, MSM and also ASIC, MSM and GPU HPoly we all have been released the best performance ever in the world. In case you are interested you can check out Twitter and also have a live demo to try it out. So we provide different form of the service to you guys.
06:39:37.055 - 06:40:23.023, Speaker A: It could be FPGA in case you are interested in building your own Async chips. We provide GPU libraries to be deployed on your own machines and we also have a SaaS option just ran on demand with TPU virtual machine on the cloud. The second is about security. So we believe that webstream object project is only as strong as security secured. So we all know that there's a lot of innovations around smart contract review. So there will be automated tools and also expert manual audit for this smart contract. But both method is kind of a lag behind the involving threats.
06:40:23.023 - 06:41:58.735, Speaker A: So on the day of the release you actually checked all the potential threat about smart contract. But as time goes by things will change, there will be new threats rise and you will find new flaws of this smart contract. So how do we deal with this? So we notice that a lot of smart contracts actually forked from the same origin. In that case we can understand that attack impact is amplified across fox code which is very straightforward. So we all know about vampire right? So if someone is bite by a vampire and then he will bite another one and then it goes to become a vampire family and the reason is we have an original vampire if he is killed and then the entire family tree will be killed. So with this observation we want to find that if we continuously scan the code of all the Web3 smart contract and also to analyze the bloody correlations between using lab models and also the code analysis technologies and we also have on chain monitor about the ongoing threat happened in the web3world. So we know that KYT know your transactions about all these threats and to combine these two together, we actually can improve this security of smart contract.
06:41:58.735 - 06:43:03.605, Speaker A: So that's what we have released the Zen AI scan. Basically it continuously protects smart contract with SAS service. So it's very close to the experiment audit in complex and code. So with manual audit from experts at the beginning and AI scan continuously scan this smart contract, it actually improves the security a lot. The third piece is we will talk a little bit about anti AIGC kyc. So I think a lot of progress recently is about how we meet the compliance and the regulatory requirements for three applications. And this is very interesting because recently when we noticed all the recent progress of learning art language models and the aicc, it's actually been a big threat about whether to identify your face, identify the image and identify your identity card.
06:43:03.605 - 06:43:46.885, Speaker A: This is a famous word from inception. To break down this kind of threat you have to go deeper. We all know about aitc. So with AITC you actually can generate a vivid image of anyone that you have ever saw on the Internet. You can make this, this person make any actions or reactions as you want. And with this it could be a presentation attack or injection attack to your mobile devices. Actually it can fake, it can be anyone that you use on the Internet.
06:43:46.885 - 06:44:43.239, Speaker A: So how do we deal with this? So especially for those exchange wallet with bio verification of your face, how do we deal with this? So we have upgraded the KYC solution from Zen Violoft to solve this AITC attack. So previously our real ADID basic actually served more than 1 million people in China and also in SAPC areas. But we find it still a little bit behind what we saw the new thread from AITC and deepfake. So we have upgraded our product with the yellow highlighted body features. For example for this badge generated document detections. So for ERTC we generate a batch of documents. They are pretty similar in terms of the background, in terms of the features they have.
06:44:43.239 - 06:45:55.061, Speaker A: So we have a cloud, the feature vectors doc to identify this kind of new threat. And we have a reinforced learning problem framework to have generated all the deep fake videos and to train a model how to deal with it. And also we have a lot of effort to protect identify the root of hook device and rigs in case of the injection attacks. And also there will be some blacklist from the device design or the behavior data image feature data. So with this upgrade we actually notice that our capability to deal with AIGC deep fake attacks is about 90%. So here is a quick summary of all the products we have the three products that we only example so we basically focus on tokenization, decentralized applications and the key smart content review application chain load service. So in case you are interested you have visited our website and try it out by yourself.
06:45:55.061 - 06:46:33.131, Speaker A: So all the technology is from Ant group and Alipay. So we are powered by Antchain open apps. So in case you are interested in Italy collaborations for the research and paper feel free to visit our official site from underschell Collapse. One more thing here. So we love Sonana so we provide some exclusive offers for smart content review identity, KYC and Power Zebra and Sandlot growth plan. So in case you are interested we have a telegram. Take a picture.
06:46:33.131 - 06:47:57.249, Speaker A: Let us stop here. You can take a picture and join the telegram group and you can claim your coupon there. And also in case you are interested to have communication with us on Twitter and website. Feel free to thank you all and restaking. So yeah, give it up for Gino and what's up guys? I'm Zano from Jito. I've been in this space as an engineer for a couple years now. Really got into it because Defi is just amazing.
06:47:57.249 - 06:48:26.467, Speaker A: I think financial sovereignty is just. It's an amazing concept. Working on Geno for the last three years, we build MEV software along with a couple other things on solar and yeah, I mean it's been wild. Yeah. How's it going everyone? I'll introduce myself. I'm Ben. So my background is all in like theory in the weeds, computer science, mathematics, high performance computing style stuff.
06:48:26.467 - 06:49:14.855, Speaker A: Studying math and CS in school for a few years, interned at two high frequency trading firms, Acuna Capital and Citadel Securities. Became really fascinated by trading like mortgage structure and stuff like that. But at the same time I was doing a lot of research on blockchain systems and just high performance distributed systems in general. Became very interested in the intersection between the two and as we'll get into MEV considerations are very, very relevant to that. And yeah, excited to chat a little bit more. What is it like what does mev? Seriously, everybody's throwing this around, this obscure mysterious acronym. What does MEV do? You It's a good question.
06:49:14.855 - 06:50:08.581, Speaker A: MEV is sort of this intrinsic property of blockchains that is essentially the concept that in blockchains where you rotate proposers such as Solana or Ethereum, you've got time when where proposers have a lot of power, they have the sole power to actually build and produce a block. And in the process of actually building a block, there's a lot of value that you can actually capture in doing so. Right. So one of the most infamous examples is say you've got an NFT Mint, for example. And it just so happens that in Solana, for example, my leader slot is the later slot where that NFT mint actually starts. I am in the ultimate position to actually capture every single nft. When that mint goes live, I just snap all up just like that.
06:50:08.581 - 06:50:29.559, Speaker A: Right. There's many other types of map related to markets and stuff like that. But yeah, we can, we can get into that a little more. I just want to ask you, like, what are kind of your overall thoughts on mep? Right. Like, a lot of people see it like a negative light. Is it necessarily a bad thing? Can we get rid of it or should we? Yeah, I mean, that's a great question. I see this get thrown around a lot.
06:50:29.559 - 06:51:19.235, Speaker A: Let's remove mev. I don't, I think that comes from a good place. I don't think MEV is necessarily a malicious phenomenon. I think it's a naturally occurring property of blockchains in any system that requires consensus so on the sequencing of events as they happen. I think during, like, I think it comes from a great place, right, that sort of like, let's get rid of mev. I think the way you need to think about it is how do we give users the best pricing and how do we optimize for the best user experience? And what I think is good user experience is an example, is just I push a trade out to the network and I don't get filled at the worst price possible. And the worst price possible is, let's say you put 5% slippage and you get filled at 5% slippage when you could have gotten filled at 0.1%
06:51:19.235 - 06:51:58.373, Speaker A: slippage. I think in order to be competitive or harmoniously competitive with centralized exchanges, we need to do just as good and just as well as centralized exchanges. Today we're seeing about half a billion dollars in ME generated for the network on Solano. It's a big number, but I think it's actually quite a small number in the grand scheme of things over the next decade. I Expect this to be a multi billion dollar sector in defi. What do you think, Ben? We're at half a billion today. What's the state of MEV on Solana? How would you address that? Yeah, I mean I do get the feeling we're going to go higher.
06:51:58.373 - 06:52:21.043, Speaker A: 500 billion is still a lot. And if I'm not mistaken, that is, that's value that's been captured by JITO alone. Right. And this is a lower bound on how much MEV is actually in Solana to begin with. Right. And you know, I get the feeling that especially with all the exciting announcements and all the users that will continue to trust Solana, that this number is probably going to go a lot higher. Right.
06:52:21.043 - 06:52:53.575, Speaker A: So what we're seeing right now is, what we're seeing right now is that you've got like it's a. For example, the users will send transactions directly to the validators to their tpu. Right. Hence one thing that we're seeing right now is the validators have realized that the information that the user has actually sent them is pretty valuable. Right. It's like a transaction that hits the tpu, it actually hasn't landed on the chain yet. Validators will sometimes pass this information down to them searchers.
06:52:53.575 - 06:53:47.999, Speaker A: They're essentially people that bid on these MEP opportunities and compete to return the most value to validators and stakers as possible. Right. So what you're describing here, how do you see is this malicious or how do you describe it? Yeah, so like there's some types of MEV that I can definitely describe as more as, you know, more native mev, such as truck running for example. Right. And sandwiching is the really infamous one that people talk about a lot. This is when I'll have a user send me a transaction and I'm going to pass this transaction, I'm going to pass this transaction onto searchers or maybe I'll just myself, I will insert my own front run transaction before that retail transaction, insert the retail transaction and dump on them. But essentially this is I am ensuring that the user gets the worst price possible.
06:53:47.999 - 06:54:34.995, Speaker A: Right. And I would call this a negative type of mess because the validator is behaving in a way that ultimately results in the user getting the absolute worst price possible in the market. Right. So I think one thing that I wanted to touch on next is, you know, as a staker, this is probably something that you want to be aware of. You'll have some validators that will just consistently engage in behavior of leaking retail flow to sandwichers and then you'll have some validators that might be actually trying to keep that flow private and actually give users better prices. I'm not sure who coined this exact phrase, but something I've heard thrown around quite a bit is that staking is explicitly political. Right.
06:54:34.995 - 06:55:01.975, Speaker A: In a way that stakeholders like Bitcoin or like a proof of network or network, it's not. What do I mean by this? Right? Like how might. As a staker, how might I actually help mitigate this problem? Yeah, I mean, I think Samani coined the term. I'll give him credit because he's good at scoring new phrases. But yeah, us networks are explicitly political. I think what the way I interpret that. Well, I'm just getting back.
06:55:01.975 - 06:55:49.029, Speaker A: I think proof of work networks, any sort of system where you can influence large groups of people do incentives to act in certain ways are going to naturally be political. POS networks have manifested out to be even more political. The way I interpret it is you have some capital as an individual and you direct that capital to validators. By doing so, you use that validator power over the network network to propose blocks. You were talking about proposing blocks earlier. We were talking about sequencing of events and every slot sauna slots are just units of time. Every unit of time there is a single arbiter of transaction ordering and that is proportional explicitly to your amount of stake or delegations that you have.
06:55:49.029 - 06:56:14.933, Speaker A: So as a staker, you believe that certain activities are malicious and you should not stake to validators or stake pools that allow that specific type of activity. That's where it gets political. Right. And then you might be a staker that doesn't care. You just want to optimize for yield. That's fine, everybody. You know, that's the beauty about these networks is whether or not we think things are malicious.
06:56:14.933 - 06:56:44.555, Speaker A: They are open networks. And if you're not, if you cannot mitigate something through cryptography or incentives, then you then so be it. That's the problems that we're trying to figure out here. Those are the problems that teams like mine and yours are trying to solve here. I think stake bulls help with this problem a bit. As a staker, you probably don't have to know how. As an individual, you know, retail user, you probably don't want to be monitoring your investment.
06:56:44.555 - 06:57:26.585, Speaker A: Did this validator rub me on the yelp? Did this validator, you know, right at the end of the epoch, crank the commission up to 100%? Or did this validator sandwich users where I'm against that sort of activity? What you do is you Offload the task of monitoring validators to stake pools. And stake pools are aligned with. They stake pools only spread staker around validators that are explicitly aligned with them. Some stake pools optimize for yields, some stake pools optimized for other things. Right. So as a staker, you should be staking to stake pools. Stake pools will then take on the burden of monitoring the validators.
06:57:26.585 - 06:58:15.695, Speaker A: Why do you think it's important that stakers think about what they're staking to, who they're staking to, and whether or not they're aligned? Why is that important? Right. Yeah, I can, I'll make an argument to every staker in this room and you know, I can make this argument to every staker on Solana that if you're, if you're trying to actually ensure long term prosperity on the network, you actually want to engage in behavior that benefits your users. These users are the customers of the network. Right? Users bring value to network. They pay fees to the network and execute their transactions. Those fees ultimately accrue two stakers. Right? So as a staker, you want the maximum number of users on the network long term.
06:58:15.695 - 06:59:02.903, Speaker A: And as a staker, if you're staking to validators that are just leaking transactions and sandwiching users left and right, you're probably going to drive those users away from the network. If I'm a retail trader, for example, I'm sending orders on Solana and I am always getting filled at literally the worst price I can get filled at. That is the very definition of an inefficient market. Right. Especially in the short to medium term, what we really want to see is stakers, validators in the greater Solana community actually work together, figure out why so many users getting sandwiched and figure out how we can put a stop to it. Right. And that starts by, you know, again, like stakers need to know their validators, they need to have a relationship with them.
06:59:02.903 - 06:59:28.833, Speaker A: Right. They need to actually be vigilant. And as a Solana developer community, Right. I have a feeling that people are going to put out a lot of tooling relatively soon. That's hopefully going to give us a lot more insight into which validators are behaving maliciously leaking user transactions, which eventually leads to those users being frontrun. Right now this is kind of short to medium term. I think there's some more things that we can do in long term.
06:59:28.833 - 07:00:25.975, Speaker A: What have you heard, for example? I mean, there's many proposals out there that's super interesting. The one that interests me the most is async execution. I think Solana was the reason why I got Solana build and I think a lot of people, whether you know it or not, is because slow's block propagation through turbine is the blocks get streamed out as opposed to, you know, a validator building an entire block and then posting it out once the block's built. I think that was a step functioning through previous networks and I think the next improvement is going to be async execution where consensus essentially gets faster. I think that in combination with multiple concurrent proposers helps with a lot of things we're talking about here. I just want to know, you know, we've been talking a lot about sort of like the malicious things and things like that. But like on a more positive note, I think Solana's got some of the best validator operators in the ecosystem or across crypto period.
07:00:25.975 - 07:01:10.271, Speaker A: I've been, I've been in the room, I've been in the war room with some of these guys like and I'm sure you have too, where you know, we've had outages and that sucks. But when you can coordinate people across seas and oceans to coordinate restarts, you got people in Europe where it's 5am their time, people in the U.S. everybody just coordinates together and we're pushing out patches. You don't see that. I don't see that in any other ecosystem. So I mean kudos to those guys and honestly just wanted to highlight that I'm pretty optimistic. What do you think? What's going to happen in the future? How do you see the next decade or two? Find out.
07:01:10.271 - 07:02:08.443, Speaker A: Yeah, so I think also to kind of continue what you said, I think that these are like the fact that some validators are behaving maliciously and maybe even some stakers, this is, this is a conscious trade off of decentralization in tradfi and in the centralized world. We solve a lot of these problems with things like law and regulation. Say if you are front running people on your own exchange, that is definitely illegal in almost every jurisdiction. Part of the value proposition of decentralized networks is can we actually solve these problems without having to trust anyone. And I'm pretty involved in the core community and a lot of the core stuff that's happening and we're working on a lot of solutions that will hopefully solve these problems longer term. Right. Because even if stakers, I think it's really important for stakers to be extremely vigilant about who they stake and to know their validators and you know, again to keep in touch, make sure.
07:02:08.443 - 07:02:27.157, Speaker A: Their validators are behaving maliciously. Right. But beyond that, we're working out a lot at the core L1 level to actually solve these problems longer term. Right. There's a few proposals that are floating around and discussions as to which proposal is the best is ongoing. Right. So the first thing you need can actually be solved at the application layer.
07:02:27.157 - 07:02:59.749, Speaker A: You actually just need better, better Dexes. Like better Dex mechanism design. Right. Like I think XY K kind of creates this fundamentally flawed market structure, right, where it's really, really easy to atomically front run and then back run someone. What you actually want are Dexes more like order books or, or auction mechanisms or RFQs or something like that, where you can actually have multiple people that are competing to give users the best price possible. Right now where this breaks down is if L1 is not censorship resistant. And that's where multiple concurrent proposers, for example, helps a lot.
07:02:59.749 - 07:03:33.927, Speaker A: Another thing that I've heard thrown around a lot and I know the Fire Dancer team is a really big proponent of this, they think that we can get slot times down to like 20 milliseconds or even lower than that. Sheesh. We actually get slot times that fast. We get a really, really high degree of censorship resistance as well. And I think some combination of lowering slot times and hopefully in the future figuring out a mechanism, at least in my opinion. I like multiple concurrent movements a lot, in my opinion. I think there's a way that we can get this on slot long term as well.
07:03:33.927 - 07:03:59.765, Speaker A: And, and this will enable a market structure that I think is a lot friendlier toward users and that can, I think, be maximally competitive with centralized exchanges long term. Ben, you're an absolute gigabrain. I hope everyone in the ecosystem continues doing what they're doing and we continue working together to solve these hard problems. And my final thoughts are just higher. I think we just go extremely high. That's it. Thank you.
07:03:59.765 - 07:04:55.063, Speaker A: Them yet. You should. So we have Krim, the co founder of D. Exciting stuff. And then this will be the last product keynote that we will be having on the stage over here on the dev stage. So definitely stick around. Hey everyone, I'm Karam.
07:04:55.159 - 07:04:57.911, Speaker E: I'm one of the co founders of Dubba.
07:04:58.063 - 07:05:07.895, Speaker A: Our mission is to bring the next billion users online to the Internet. We've been an Internet service provider for the last eight years.
07:05:07.975 - 07:06:04.355, Speaker E: We used to power Google, WI Fi. We helped draft a lot of the telecom policy relating to WI Fi in India. And I'm going to talk to you about why that really important Fairly soon. I'm here to share one really simple idea and it's one really quick thing and I think it's a slightly interesting way of looking at the deep end landscape, the way that it is at the moment. And that's to push the idea of a layer 0 deep end. So I think a really helpful classification of how deep ins are at the moment is sort of in two layers, right? Right at the top you've got deepen services that offer storage, compute databases and they're primarily software. One level below that you've got hardware that plugs into other hardware which is say for example, helium, right? You buy a helium hotspot, but you've got to plug it into a broadband connection that's already there.
07:06:04.355 - 07:07:00.045, Speaker E: You buy a hive mapper, but you already have that, have a car to plug that into. So there's a lot of those. I want to propose the idea of a layer 01 which is sort of foundational physical infrastructure in a place where that infrastructure doesn't exist so that all of these other layers and projects can sit on top of them. Layer 0 deepen is a really strong concept because first thing is of course is their primary and the foundational things sit on top of them. The second thing is it's really useful that they are foundational because that means that they are resilient and long lived. A very practical example is if you build a highway in a place where there's no highway before, it's going to take at least, you know, a couple of decades before that highway degrades in any way. So infrastructure that is built for the first time lasts a long time.
07:07:00.045 - 07:07:45.015, Speaker E: As a result of these two characteristics, the returns on layer 0 deep ins, things that are built as primary resources, are good and they provide long term value, a place to look for layer 0 deep ins. And these are up and coming. This is an entirely new concept. So it's going to take a while for them to roll out. The places to look are emerging markets and this is fundamentally the. Because these are markets where the kind of infrastructure for a helium or a hive mapper or that sort of thing doesn't really exist, right? One layer below has to be built. So you're going to see layer zero deep ins being built out in emerging markets and they'll cover areas like roads, water, energy and of course things like what we do, which is connectivity.
07:07:45.015 - 07:08:23.391, Speaker E: So I want to give you a really practical example of what a LEO 0 deepen looks like in the real world. And I'm going to use our stamp as an example of it. So we operate in India. And although India is amazing, it's massive economy, great adoption of crypto, it's got a ton of mobile Internet connections. Like we have something like 800 million 4G and 5G connections, which is insane. But access to broadband, high speed WI fi is absolute garbage. If you look at that graph, that little line at the bottom that barely exists is where India stands in terms of broadband connectivity.
07:08:23.391 - 07:08:45.473, Speaker E: And our role is to help change that. To give you a sense of scale, the US has about 150 million broadband connections. China has something like 700 million. India's only got 30 million of them. And we want to do something about that. So, so we've started this process. We've rolled out thousands of connections, thousands of hotspots.
07:08:45.473 - 07:09:17.963, Speaker E: This is what our explorer looks like. You can see the connectivity that's there. We've connected tens of thousands of users, we have terabytes of data flowing through the system. And the thing to note in all of this is, all of this is infrastructure that is paid for. Because the thing about layer zero, deep ins and emerging markets is you're bringing a crucial service to people that are willing to pay for it, right? It's not an exercise of build it and they will come. In fact, in a country like India, where there's an overwhelming amount of demand for data, we cannot build out our infrastructure as fast as we need to.
07:09:17.979 - 07:09:19.135, Speaker A: Be able to do it.
07:09:20.035 - 07:10:28.529, Speaker E: The way that we do this is India's going through this really interesting transition where traditionally we've had something like 150,000 tiny local cable operators, right? These guys are the ones responsible for distributing cable TV across the country. So picture a 20 to 40 male who's got a tiny satellite dish pointed at the sky with a couple of hundred subscribers that he's delivering cable TV to. But these people are going through a massive change because everybody has shifted to streaming services, right? Nobody wants cable TV anymore. So all 150,000 of these operators, which serve hundreds of billions of cable customers, now need to transition into becoming Internet service providers, because that's what their customers want. So we're helping them change their infrastructure from cable to fiber optic. And the way that really works is anyone anywhere in the world buys a hotspot and some connectivity hardware. We deliver that to a local cable operator in India, who then installs that broadband connection in a paying customer's home or office.
07:10:28.529 - 07:10:42.025, Speaker E: The person that bought the hotspot gets dubba tokens as a reward. And the way the value accrues to the whole system is we take the paying revenue from the customer and we use it to buy and burn DABA tokens.
07:10:42.105 - 07:10:43.765, Speaker A: So it's a fairly simple system.
07:10:44.345 - 07:10:58.619, Speaker E: We think that this is what sort of true deep ins look like. Serve demand rather than building out supply. They do it in places that are needed the most and they provide rewards and returns to people that help create.
07:10:58.667 - 07:11:00.455, Speaker A: That infrastructure in the first place.
07:11:01.275 - 07:11:30.429, Speaker E: If you have the chance, please take the time to scan the QR code. We've got amazing campaigns on Zealy. We're doing epic things with Drip House. There's a whole bunch of activities that we're doing to build out a and help people get involved in this process of building deep ins that actually deliver valuable things and hopefully get rewarded in return. So if you scan the QR code, you're going to definitely find significant alpha that will pay off for a long period of time.
07:11:30.597 - 07:11:31.421, Speaker A: That's about it.
07:11:31.493 - 07:11:32.385, Speaker E: Thanks so much.
07:11:40.695 - 07:14:13.155, Speaker A: This is the end of the Dev stage presentations. Now if you guys can all go over to the left stage, we'll be closing off the breakpoint ceremonies and presentations and there's going to be the final debate which is should Solana foundation be dissolved? Thanks again. I'm your MC over here at Neural Crypto Ape on Twitter and I'll see you guys on significant IT.
