00:00:00.160 - 00:00:50.214, Speaker A: Um, yeah, so I'm here to talk a little bit about local validator state. I know something about this. It's entirely likely that some people in the audience know more about various parts of this, so please chime in if you have things to add either in the chat or, or feel free to just interrupt and jump in. Also jump in with questions. Yeah, so first I want to talk about why this is important. The various public clusters have like a few thousand nodes on them. And if the goal was just to have a few thousand servers running a piece of software, most engineers with cloud experience can, can set that up over the weekend.
00:00:50.214 - 00:02:12.434, Speaker A: That's pretty straightforward, and there's a wide variety of tools for doing that. That's clearly not what we want here. The goal of a decentralized ecosystem is to have a few thousand nodes with a few thousand people behind them and all of the associated resources and those people essentially all confirming the veracity of the transactions that happen on the network. In order for that to happen, every node operator basically needs to be taking personal responsibility for the voting decisions of their node. And the voting decisions are based to a tremendous degree on the local state. So in the simplest version, a transaction shows up that wants to send tokens from account a to account b. Well, does account a have those tokens? And has that transaction been assigned? Has, has that transaction been signed by the correct key to be allowed to transfer tokens out of that account? The only way that the node can make accurate decisions on those questions is if it has accurate local state.
00:02:12.434 - 00:03:16.000, Speaker A: And when things go wrong on any of the clusters, there's often a feeling of urgency to get things back to right. And sometimes that leads to people deleting their ledgers. And I'd like to encourage you to never, ever, ever delete a ledger if you absolutely have to move your ledger and download a new one, but hold on to the old one. But really, it should be extremely rare that you have to even move your ledger. Your ledger represents your ability to verify the state of the network. And so my plea to you is to take that seriously, and to today I'm hoping to give you some information that will make that easier. So I'm sharing a couple of screens here on the right.
00:03:16.000 - 00:03:29.724, Speaker A: I have a gist with a few commands that are helpful, and then I've got a terminal up. First I want to just talk through at a really high level. What are the pieces of local state that your validator maintains?
00:03:30.624 - 00:03:35.334, Speaker B: Hey will, real quick, would you be able to increase the text size a bit for, for both.
00:03:35.994 - 00:03:45.562, Speaker A: Absolutely. Thank you for, whoops. Yeah, I hate that.
00:03:45.578 - 00:03:51.614, Speaker B: It also makes the window bigger on Max. That's great.
00:03:52.114 - 00:05:20.264, Speaker A: Okay. Yeah, thanks for the tip there. Okay, so starting with just running through, what are the pieces of local state that the validator maintains first? And maybe the one that we talk about the most is the ledger. So I'm going to talk about paths at various points, like directory paths that's entirely configurable and your node might be different, but hopefully the paths that I'm working with are reasonable enough that you can sort of figure out what that maps to on your node. So the ledger path ends up frequently containing a whole bunch of other directories of the things we're going to talk about here, again, depending on your configuration, but there is a rocksDB directory that would end up usually inside, at the root of your ledger, and that's mostly what we're talking about when we say ledger. The ledger is essentially a list of transactions. There's a little bit more to it than that, but basically transactions have been submitted to the cluster, they've been verified by a leader, packed into blocks, voted on by the rest of the validators, and then every node gets those transactions.
00:05:20.264 - 00:06:05.926, Speaker A: And depending on your configuration, stores those transactions for some amount of time. I think a pretty normal retention would be that you're storing maybe like a day or two of transactions at a time entirely configurable. You in theory could set up a raid and store transactions indefinitely, although it would be a pretty substantial amount of data. Accounts represents the current state of all the accounts on the network. This is essentially a giant hash map. The pub key is the key, and then there is data in the account. It's in raw form, and we have a bunch of tools to parse and serialize it to put it into human readable form.
00:06:05.926 - 00:06:54.870, Speaker A: We'll look a little bit at that in a few minutes. The accounts index. This is an index into that same data. It can be a little perplexing. Why do we have an index if it's a hashmap? Because hashmaps are constant time read and write, backing up a little bit. What is the ledger for? That's a fair question. If we are processing transactions and moving along, why do we need to store any amount of history? If you were in a mood to experiment, you could probably reduce your ledger retention pretty dramatically from the defaults, and your node would still work.
00:06:54.870 - 00:07:54.324, Speaker A: I don't have a sense of how dramatically you could reduce it, but I think you could probably retain a very small amount of ledger data. It is useful for nodes on the network to retain some amount of history because the use case that I'm familiar with. Thanks, Steve. The use case I'm familiar with is a new node starts up maybe from a snapshot, let's say if it's on a public cluster, it's going to have to be from a snapshot because genesis was too long ago. So it has a snapshot. That's for argument's sake, let's say an hour old. In order to catch up to the tip, it now needs to know what has happened in the last hour and it gets that information from, hopefully many nodes, and those nodes are responding to those pull requests from their local ledger.
00:07:54.324 - 00:08:42.756, Speaker A: So it's in the interest of the cluster and everyone that most or all nodes have a record of recent transactions. To make it easy for nodes that are starting up to catch up. There may well be other use cases that I'm not thinking of right now. If anyone has examples of those, feel free to chime in or leave comments in the chat. Let's just see. Did I miss anything else? Oh yeah, two questions that I've missed here. I'll do the first one.
00:08:42.756 - 00:09:46.974, Speaker A: First, do I consider it primary secondary when one deletes a ledger but gets it from the other one to be an acceptable management solution? I don't see any problem with that. I'm really curious if other people want to chime in if anyone objects to that or thinks that there's risks involved. From my perspective, if an operator is maintaining a secondary and feels comfortable that that node is, is accurate, then you know, it, it's, it's none of my business how many, how many nodes you're using to maintain your trust in your own ledger. Is it what RPC queries draw from? Yes, and among other things. So, yes. So there's obviously a lot of RPC methods, but yes, some of the RPC methods draw directly from the ledger.
00:09:48.434 - 00:09:59.214, Speaker C: Zan, quick question. So you mentioned primary secondary. You mean like copying stuff over from one node to another when you switch?
00:09:59.754 - 00:10:42.264, Speaker D: Well, I mean like as an example, recently my disks had filled up because of whatever bug is causing certain files to be retained. So I feel very free since I have a primary secondary, to simply like go to the secondary, shut it down, delete all the files, delete the whole ledger if I want to, then download a snapshot from the primary and then restart and let it sort of rebuild itself. I guess the part I didn't realize is that the catch up process after the snapshot has been acquired and used requires going out and getting things from the network, and it's not necessarily coming only from my primary, so maybe that's the part I didn't quite realize.
00:10:43.044 - 00:10:44.204, Speaker C: Yeah, that's correct.
00:10:44.244 - 00:10:44.396, Speaker A: Yeah.
00:10:44.420 - 00:11:06.592, Speaker C: Like, if you copy the snapshot, yeah. You'll need to repair all those blocks from the snapshot slot down to like, when your node becomes online and starts getting turbine shreds again. So, you know, there might be, if your nodes offline for five minutes, there's like 150 slots a minute or so. So you might miss, you know, somewhere in a 500 to 1000 slots, and so you'll have to repair those from other peers.
00:11:06.768 - 00:11:26.644, Speaker D: Is there, is that the. There are probably, and I should know this better, but I've never had to mess with these options because I just found something that worked and never touched it again. Is there a way to have my ketchup only draw from my other node, like from my primary, save the secondary, catch up only from the primary, and not even bother the rest of the network with, with repair requests?
00:11:27.834 - 00:11:59.596, Speaker C: That is not an option. That is an interesting idea, though, especially if an RPC provider that's managing a fleet, they might know they have quicker latency or they might open. There's throttling on repair to make sure that repair requests don't overload a system. So they might be more okay with a higher limit. It's an interesting idea, but the answer is no. Right now you just pull from everybody. Yeah.
00:11:59.620 - 00:12:10.304, Speaker D: One of the advantages would be then I could implicitly trust what I'm getting, since I'm getting it only from my other node, which I already had perfect trust in, I suppose, would be one potential advantage.
00:12:10.684 - 00:13:05.218, Speaker C: Yeah, definitely. I mean, yeah, obviously not a good solution, but you could also just like, you could do a few ledger tool commands and copy a tarb all over. But yeah, something baked into the client would certainly be more robust and potentially worth considering. And then I guess I'll chime in to the with. Is it what RPC queries draw from? So, yeah, like in the RPC code path, you know, you could think of like the local block store as a cache and then the, like, bigtable as like, you know, the disk, you know, for making the analogy to a computer, you know, a system, if your node is backed by bigtable. So, you know, if you don't have bigtable, then yeah, whatever you have on disk is what, you know, you could serve. So, like, if you call, get blocked for something from 2022, not gonna, you know, you're not gonna get a hit, but if you call get blocked for something from 2 hours ago.
00:13:05.218 - 00:13:09.854, Speaker C: Yeah, like you're probably gonna get it. And, you know, that's certainly quicker than the bigtable request.
00:13:16.254 - 00:14:26.034, Speaker A: Cool. Yeah, thanks for the questions and all the engagement there. Zen, to your point about sort of being able to trust, if your secondary were to pull all of its repair requests from your primary, the difference between sort of repair requests and normal shred propagation starts to get not blurry in terms of like the code pads, but blurry in terms of when are you trusting other members of the network. I don't think it goes from like, you know, never having to trust anyone to she's suddenly having to trust. It's just a, it's a step along a continuum. I realize, of course, that, you know, that. I'm just wanting to clarify for, you know, everyone else.
00:14:30.774 - 00:14:34.310, Speaker B: Okay there when you were talking, will, just as an FYI.
00:14:34.502 - 00:14:55.980, Speaker A: Oh, okay, thanks. Let me know if that gets worse and I can, I can try switching. Hopefully this one keeps going. Okay, so back to just a quick run through of the local state. And thanks for the questions. Keep them coming. Either unmute or in the chat accounts index.
00:14:55.980 - 00:16:00.546, Speaker A: So you would normally think that if your data structure is a hashmap, you don't need an index because hashmaps are constant time read and write. Um, but basically it gets more complicated when you are, uh, trying to maintain multiple states because, uh, because of forks. So remember that it takes, uh, 32 slots for a block to be rooted, which means that at any point in time, there are multiple potential, uh, sources of truth. As nodes navigate, adding new blocks to the chain, the index helps solve some problems there. I have never had to dig into it, and so I will not be able to go into more detail than that. If people have questions, hopefully there's someone on the call that understands the accounts index more than I do. Shots.
00:16:00.546 - 00:17:08.844, Speaker A: So we're probably all familiar with snapshots at some level databases, you can think of snapshots like backups. We search, and this is basically a bank, is sort of like a not yet completed block. The file name for snapshots includes. There's a question about node counts on the networks. I am not going to try to do node counts from memory, but you can run Solana gossip and pipe that into WC l and that will give you pretty accurate node counts for, for the different clusters. Somebody wants to do that.
00:17:11.224 - 00:17:12.992, Speaker B: It's pretty chappy again for me.
00:17:13.008 - 00:17:26.063, Speaker A: Will share the results. Finally. What's that?
00:17:26.363 - 00:17:31.411, Speaker B: Will? Can you hear me? You're getting pretty choppy for me again.
00:17:31.507 - 00:17:36.703, Speaker A: Yeah, I couldn't hear that last one moment. I'm going to push network.
00:17:43.264 - 00:17:51.364, Speaker C: To answer Will's question. It is the slot number, not the block number like the time slice slot, not the actual block height in the file name.
00:17:53.744 - 00:18:18.054, Speaker A: Awesome. Okay, I have switched networks. Hopefully my connection is better. Apologies. Thanks for the clarification there, Steve. There is also a hash in the snapshot name, and those attributes of the snapshot can also be verified with the Solanaledger tool verify command. I think.
00:18:18.054 - 00:19:04.074, Speaker A: I don't have the help up in front of me, but if you do Solana ledger tool help, you'll get the list of commands. Yeah, so hopefully everyone is familiar with snapshots, at least at a general level. Maybe the one piece of relevant information here would be that the incremental snapshots build off of a full snapshot. So you would need a full snapshot or a full snapshot and one incremental snapshot. The incremental snapshot file names have the full snapshot slot number in the file name so you can relate them back to each other. If you just. Actually, I've got a, whoops.
00:19:04.074 - 00:19:37.064, Speaker A: Got a shell window up here. I might as well just show this while I'm talking. And again, this is probably review for nearly everyone on the call, but if there's anyone here who isn't familiar with this, hopefully you'll learn something here. Whoops. Okay, so this is a node that I have running on mainnet right now. There are two full snapshots. I think this is the standard retention, although again configurable.
00:19:37.064 - 00:20:30.122, Speaker A: And it looks like I am in the process of writing a third snapshot. Once this third one gets written, my older full snapshot here will get deleted. Again, configurable. Then I have a bunch of incremental snapshots. The interesting thing here is if you look at this number, the one that ends in 736, this is the slot number that goes with my older full snapshot. So this incremental snapshot would pair with my older full snapshot here, and then all of these other incremental snapshots pair to the newer full snapshot. We sometimes get confusion around testnet restarts, uh, about, uh, there being extra snapshot files in the, in the directory.
00:20:30.122 - 00:21:21.844, Speaker A: Um, in situations where there are extra files that are causing problems, uh, grabbing the most recent full and the most recent incremental, um, and pulling them off into a different directory and, and using that as the source for creating a snapshot should work and, and solve your problems. Again, strongly recommend moving rather than deleting anytime you're in a situation where you think files are causing problems. Let's see. Catching up on the chat. Cool. Thanks for the counts, Tim. Yeah, so, a question about where accounts are stored.
00:21:21.844 - 00:22:06.824, Speaker A: Completely configurable, of course, on my node here. So this is my ledger directory and I have an accounts directory. This is an NVMe and the accounts are stored on disk. You could, I think, put accounts in memory with something like temp fs. We don't currently recommend that, but it probably is the right thing for, for some nodes with some configurations you would want to have a fair bit of memory. Let me check. I forget what the, I forget what the current size is, but it's not small.
00:22:06.824 - 00:22:28.604, Speaker A: Is my screen share halfway usable? But the network I'm on has much slower uplink, so hopefully this is coming through with not terrible latency.
00:22:30.944 - 00:22:32.600, Speaker E: It might be helpful to scale your.
00:22:32.632 - 00:22:35.004, Speaker B: Camera well and just do the presenting.
00:22:36.384 - 00:23:09.218, Speaker A: Great point. Thank you. Yes, I totally forgot that I was still showing you all my face. So yeah, accounts, it looks like on this node, which is on main net, beta accounts is 321 gigs. So you could put that in memory, but it's going to need to be a reasonably beefy node to have that in addition to everything else. Okay, let's see. Sorry, catching up on the chat here, we're getting some great questions.
00:23:09.218 - 00:23:33.762, Speaker A: Is the snapshot folder of any use? Certainly, yes, it's important. It's especially important if you need to maybe shut your node down, for example, to update the software version and then restart it. The snapshot is part of that process. So yeah, I guess we can go.
00:23:33.778 - 00:23:51.994, Speaker B: Into more detail or maybe a different question. The question I have, which maybe is the question he has also there's incrementals and there's full snapshots, and there's also a folder called snapshots, and I think that might be relatively recent, and I don't know what that folder is for. Do you have any question?
00:23:52.074 - 00:23:54.454, Speaker E: Yeah, that's the question, yeah.
00:23:56.274 - 00:24:09.018, Speaker A: Thanks for clarifying. I don't know, let's have a look and see what's in there. Yeah, I think that's, it's been like that for a while.
00:24:09.066 - 00:24:09.898, Speaker E: I've noticed that too.
00:24:09.946 - 00:24:17.654, Speaker A: There's a snapshots directory, but I think the main snapshots we are concerned with are never in that snapshot directory. That's a good question.
00:24:20.374 - 00:24:42.994, Speaker E: Yeah, especially when we have like we had the desknet restart instructions, it mentions, you know, use your snapshot folder or your snapshot directory. And I was wondering, is it the ledger directory? Or is it this subdirectory called snapshot? You know, so a little bit confusing. So that's why I'm asking.
00:24:44.454 - 00:25:02.834, Speaker A: Yeah, a totally fair question. I don't know the answer. I'm going to go learn about this unless anyone happens to have the answer right now on the call. And I will. The next time we do a testnet restart, I will make sure the instructions are more clear and more up to date for this kind of issue.
00:25:03.734 - 00:25:27.844, Speaker C: I know that when you download a snapshot it goes to a different path, and the distinction there is that when you download one you want to do more validation versus a snapshot that you create. You trust it because you created it, but there's, that might be the remote folder too, so I can dig around, look at what the snapshot folders, or try to find what snapshot folder is used and paste it in the chat if I find it.
00:25:29.144 - 00:26:16.550, Speaker A: Awesome, thanks. Yeah, the last time I downloaded a snapshot I think it went to remote, but it's been a little while since I've played with that. Okay, I think I still have a backlog of questions here. There's a request to repeat what I said about about snapshots. So just to recap, the incremental snapshots here have two slot numbers. The first slot number is the slot number of the full snapshot that this incremental builds on top of, and then the second slot number is the slot number of this snapshot. So this file here basically contains all the necessary information to go from this 736 slot up to this 310 slot.
00:26:16.550 - 00:27:24.724, Speaker A: So this snapshot is only really useful if you have the full snapshot with the corresponding number to go with it, in this case the 736 one. In general, probably what you want in a situation where you need snapshots is your most recent full snapshot and the most recent incremental that goes with it. But that wouldn't be universally true, and, well, I guess I'll just keep talking about that right now. The key question is, what point in time are you trying to get to? So frequently you are trying to get to the present, but sometimes you are trying to roll back. So, for example, when we have a cluster restart, there's the challenge of trying to figure out what was the highest optimistically confirmed slot, and hopefully everyone has snapshots that get them really close to that. But that's not always going to be the case for everyone. And it might be the case.
00:27:24.724 - 00:28:37.672, Speaker A: I can't remember how the snapshot code works here, but it might be the case that a node for some reason keeps running beyond the cluster, reaching consensus on anything, and writes snapshots past that optimistically confirmed slot, I don't think that's the case. But anyway, it's conceivable that you could have snapshots that are in vote only mode and no longer useful, don't contain any user transactions. And so in those cases you would want to figure out, you know, what slot are you actually targeting, and find the full and incremental snapshots that get you close to that slot. But definitely you need to, your snapshot needs to be earlier than the slot you're targeting. So if you're trying to, if you're trying to get to like slot 100, a snapshot at slot 101 does not help you because you can't go backwards. But a slot, a snapshot at slot 90 is fine because you can start at 90 and then use repair to go from 90 to 100, assuming that other nodes have those, have those blocks to share. Hopefully that answered that question.
00:28:37.672 - 00:29:44.104, Speaker A: I'm going to move on, and if there's more questions, drop them in the chat. Okay, so let's see. I have quite a backlog question about whether whether all accounts are just backed by the disk, or if there's a subset in memory, there is an accounts cache. I don't know anything about it, so I'm not going to try to answer that question, but maybe somebody else can. Yes, you hold all the accounts locally. Basically every node should have its own full copy of accounts. And in fact there's a recently activated feature recently activated on Mainnet beta where all of those accounts get hashed once per epoch, which helps catch situations where maybe a disk has a bad sector and you might have an account that got corrupted.
00:29:44.104 - 00:30:44.084, Speaker A: But if that account never gets written to, you might not catch that corruption for a very long time. So accounts all get hashed once per epoch now, so that you would catch corruptions accounts in Ramdisk. Why is it not currently recommended? It's just a resource issue. If you have the RaM and want to do it, go for it. But in general, account read and write latency has not been a performance bottleneck for most nodes in most configurations, so it's not the sort of the default recommended configuration. Your use case might be different, your resources might be different, you know, make your own decisions there. Is there a way to enable incrementals to be made from the last increment instead of the full snapshot? I am not aware of a way to do that.
00:30:44.084 - 00:30:48.204, Speaker A: I don't think that's supported?
00:30:48.744 - 00:31:25.714, Speaker C: Yeah, that's not how they work. The full snapshot has the entire account state, and the incremental is a diff, so it's the diff from the full. It's not like a diff on top of the last incremental, because if it was a diff on top of the last incremental, then you'd have to retain all of your incrementals along the way. That's also why you see, like, the incrementals get larger. You know, if you have like a full snapshot interval, like 25,000 slots, you know, an incremental, 1000 slots from the base will be smaller than one, like 10,000 slots away, because you just accumulate more, you know, as you're replaying more slots.
00:31:29.074 - 00:32:01.454, Speaker A: Cool, thanks for that. And, yep, confirmed, it looks like on this node I'm getting 3.7 gigs at the end of the interval where I'm about to write a new full, but only 2.6 at my current spot in that cycle. I think we're more or less caught up here. Yeah. Okay.
00:32:01.454 - 00:32:27.444, Speaker A: I think we're caught up on questions. Keep them coming. This is great. So I was hoping to do more prep of this sort, but I wanted to show at least a couple of. Oh, apologies, I'm not quite finished going through local state. So the final one that I have here on the list of local state is the tower. The tower is basically a record of how your node has voted.
00:32:27.444 - 00:33:20.544, Speaker A: As the nodes all discover and navigate forks before consensus is reached, your node might vote on a block that it later decides was not the right block to vote on, and it will backtrack and choose a different fork. And there's a whole mechanism for how that happens, but it is important for your node to keep track of how it has voted, and so the tower does that. I don't know much about towers, except for what I just said. Generally it's not. I don't see a lot of issues around, like tower management. So from the perspective of being good stewards to the cluster, this isn't something that I'm worried about. But who knows, maybe there's an issue here that I'm not aware of.
00:33:20.544 - 00:33:51.764, Speaker A: Feel free to chime in if. If you're aware of anything. Okay, so, Solana Solano ledger tool is incredibly powerful, does all kinds of stuff. Highly recommend playing around with it, getting familiar with it. Oh, yeah, thanks for that. Yes. Don't delete your tower file.
00:33:51.764 - 00:34:28.784, Speaker A: Yeah, you can't get it from somebody else, it's specific to you. I think it has one of your pub keys in it. I think it's your node identity. Key is in the file name. Yeah, don't delete that. So Solana Ledger tool, great way to explore some of this local state in general. I think if you have time to explore your local state, the more you look at it, the more you will understand it, have intuition around it and reason about it, and be not quick to delete it when things go wrong.
00:34:28.784 - 00:35:12.204, Speaker A: So here's an example. Ledger toolbounds. This is just going to show me how much ledger I am currently retaining. Looks like I'm retaining 356,000 rooted slots and it gives me the range here of which slots I have ledger for. Can you run a ledger tool against a running validator? Is that recommended? I just did it. But bounds is a very cheap command, depending on what commands you're doing and how often you need to do them. Yes, it's possible, but keep an eye on resources.
00:35:12.204 - 00:35:28.064, Speaker A: If it's for, you know, one off things, it's probably fine if you're doing them all the time. You might want to have a separate node that's keeping the workloads as separated as possible.
00:35:31.164 - 00:35:58.584, Speaker C: Yeah, a little more detail there. So yeah, like anything that just reads the block store, not a big deal. There are a bunch of commands that will actually unpack a snapshot and like a snapshot archive and then like replay that includes like verify, create, snapshot graph and a couple other ones. But I would not run those against a running validator. But anything like bounds or slots or anything that's just hitting the block store shouldn't be a problem.
00:36:01.644 - 00:36:39.394, Speaker A: Cool. Thanks for the extra info there. Okay, so there are far too many Solana ledger tool commands for me to even attempt to run through them all here. I highly recommend running through, you know, a bunch of them on your own. It's interesting the things you can do and the things you will uncover and learn, but there were a couple here that I wanted to show you. So slot this just gets information about a specific slot and you can give it basically any number that's in this range here. And it takes a little bit to run.
00:36:39.394 - 00:37:25.302, Speaker A: So I'm going to show you a show you some output of one that I ran earlier. Whoops. So I will share a link to this gist and I'm going to keep adding to this gist. I want to create like a list of ways to explore your local state of note here on this slot command is that you can pass verbose multiple times. I think one and two are the number of verbose flags that have a meaningful impact here. So this is verbose. One, this is a summary of this particular slot.
00:37:25.302 - 00:38:01.460, Speaker A: You can see we've got some metadata at the top. And then we get a count by program of the transactions in the slot. So you can see that like the vote program is the one that gets run the most often, which is what you'd expect. And then, you know, we've got the rest of these, the rest of these programs that were all run just a handful of times during this slot. Cool, thanks. Right, yeah. So you can run it with no verbose and you'll get, I think, less.
00:38:01.460 - 00:38:54.412, Speaker A: I forget, I think it's maybe just the metadata that's at the top without the program count. And then here is the full verbose version of this. So this is actually all the transactions in a JSON human readable form. And you can look through this and go find transactions that you sent or find interesting transactions. This is fun for me and I'm hoping that this resonates with some other people because it makes the ledger directory something more than just like this big thing that takes up space. It's like I can actually see what's in it now and that makes it, I don't know, approachable and engageable. Yeah.
00:38:54.412 - 00:39:18.134, Speaker A: A question from book brux about whether there's an updated or, excuse me, whether there's an open question about snapshots. I've lost track. So chime in there. If there are unanswered snapshot questions. There we go.
00:39:22.874 - 00:40:06.092, Speaker E: So the question is, inside of the accounts directory or multiple accounts directory, there's two subfolders. One is run and one is snapshot. And these exist so that we can support what we call fastboot, which is now available in 117. And so what that effectively does is that when you start up you can reuse whatever account data is already on your system. You don't need to unpack a new snapshot archive in order to get started. This can greatly speed up restarts. In particular, what we have to do is we need to make sure that we have the accounts at a specific slot available.
00:40:06.092 - 00:40:29.186, Speaker E: And that's what this snapshot directory is. So we say slot x. We need to hard link and save off all the accounts at that slot. And that's get put in. The accounts in the account snapshot subdirectory. Those all eventually get cleaned up. We only keep the latest around and so those are cleaned up automatically.
00:40:29.186 - 00:40:39.094, Speaker E: And as part of startup we identify if there's any orphaned ones and clean those up as well, so there shouldn't be any manual cleanup necessary.
00:40:42.554 - 00:40:49.644, Speaker C: Just to confirm. Brooks. So there's a snapshot directory under the base ledger directory. And there's a snapshot directory under the accounts directory.
00:40:50.424 - 00:40:51.204, Speaker E: Correct.
00:40:52.024 - 00:40:52.464, Speaker C: Okay.
00:40:52.504 - 00:41:08.884, Speaker E: And under the snapshot directory, there's a slot. And then inside of there, there is an accounts hardlink directory, which points to the accounts snapshot directory. Now, we are known for our confusing terminology, and this is no exception.
00:41:16.564 - 00:42:07.316, Speaker A: Okay, cool. Thanks for all the clarification. Yeah, hopefully that answers all the questions there. I've got some commands here for looking at accounts. This is probably something that most of you are aware of, but if we get a few new people to understand this, that's a success. There is also a Solana ledger tool, accounts command, but I forget the flags to grab the data for one account. So this command is using the CLI and going through an RPC node, which is not great from the perspective of looking at local state.
00:42:07.316 - 00:42:37.478, Speaker A: I will update this just with Solana Ledger tool commands. But anyway, so here's an example. This is obviously serialized and or, excuse me, parsed in human readable. But if we. So that's a system account. If we run the same command for an account that is not a system account, like a vote account, we actually get the, the raw data back. The vote account command would parse this and make it human readable.
00:42:37.478 - 00:43:17.694, Speaker A: But anyway, so I mentioned earlier that accounts is basically a giant hash map. And so just for example, this key in the hash map points to this big blob of data, 3762 bytes worth of data, again, hoping that that pulls back the COVID a little bit on what is otherwise just a giant directory that takes up a bunch of space. Yeah. So that's all the material that I have. But I am hoping that there are more questions. So, yeah, what else can we explain?
00:43:20.654 - 00:43:58.244, Speaker C: I think one interesting thing to point out will. So when you use, like, Solana account, like Solana, anything like that, you're fetching from an RPC node. So assuming that RPC node that's servicing your request is up to date, you're getting like, the, you know, the state of your account. Right now, when you use ledger tool, the state is dependent on what your system has. So if, you know, if you're running a validator that's like up to date, like you, you'll have the latest, but you can also have like a snapshot from like a year ago. So in that case, Solana ledger tool account. You know, if you get an account, it would give you the state of that account from a year ago.
00:43:58.244 - 00:44:11.444, Speaker C: So like, you know, ledger tool allows you to like do historical sleuthing, I guess, you know, assuming you have the actual, you know, snapshots and or blocks to go dig those up.
00:44:12.884 - 00:44:18.224, Speaker A: Yeah, thanks. That's a great point. Let me see if I can get the.
00:44:21.124 - 00:44:35.104, Speaker C: That one unpacks snapshot. So if your validator is running, it may not go well. Also, you can't look up a single account right now. Funny enough, I have a pr open to add that, but right now it just scans the entire database.
00:44:35.524 - 00:44:36.132, Speaker A: Okay.
00:44:36.228 - 00:44:37.904, Speaker C: For the entire accounts database.
00:44:39.904 - 00:44:49.480, Speaker A: I know there's a flag on there about whether or not it prints the contents. Is that just going to print the entire contents of all accounts by default? Yep.
00:44:49.592 - 00:44:58.032, Speaker C: Yeah, I'll print all. Yeah, Testnet right now it's like 30gb because that's what I've been testing my thing against. But Mainnet, you're going to probably kill your terminal.
00:44:58.208 - 00:45:40.074, Speaker A: Yep. Okay, well, I'll not do that. Yeah, thanks for the call out on the, the RPC call being the current state, but the ledger tool having the ability to look back at any point in the past that you have data for. Question, what is the best way to use the ledger tool to track how my node is voting? Like, see forks, etcetera. So there is a graph command that will generate a graphviz. Graph for the forks. I don't know, hopefully somebody else has a more detailed answer there.
00:45:42.294 - 00:46:18.994, Speaker C: I guess you could use ledger tool. So you could like print out like a range of slots and you could grep your, like your vote account. That will tell you, I mean, you can see where your votes landed. That doesn't necessarily correspond, you know, if your bones are boats are not landing for some reason. You know, maybe that analysis wouldn't, you know, it wouldn't tell you that, but you can, you know, you could grep your vote pub key and then. Yeah, like see what slots you have a hit for and that, that would tell you. And then you could see, you know, what slot you voted for in those, in that vote.
00:46:18.994 - 00:46:41.514, Speaker C: And then, you know, you could kind of do some analysis there. The graph thing. Yeah, that one's powerful too. But that looks more at the entire cluster, the graph sub command. So if you're looking at your node individually, that probably won't be as useful for you.
00:46:48.414 - 00:47:00.330, Speaker A: Cool, thanks for that. Looking back through the questions here to see if we missed anything, I think.
00:47:00.362 - 00:47:02.174, Speaker B: Sam had a question about accounts.
00:47:03.834 - 00:47:13.174, Speaker A: Oh, yeah. Some of the metadata shown by the accounts command is not in the account data itself. Yeah.
00:47:19.054 - 00:47:37.634, Speaker D: What I was getting at is there like a file for every account where there's some header portion that includes those extra metadata fields like I just described, and then the remainder is the data that's dumped by that command you showed there. Is there like one file for every account or is it sort of scattered into separate files, the different parts of metadata?
00:47:39.454 - 00:47:44.314, Speaker A: I don't know. Steve or Brooks, do you happen to know where that metadata is stored?
00:47:45.894 - 00:48:26.322, Speaker E: It's all in the appendix. And so depending on the account, it'll be stored in one of the appendvacs that's under your accounts directory. So all the storage files in there, we have a appendvec structure in the codes. And so storage file or storage store Penvec are all interchangeable usually, but we store the metadata in the account data next to each other. There is a store tool that you can inspect the individual appendix as well. It's really more of just a development tool. So it's not particularly publicized or well known.
00:48:26.322 - 00:48:37.254, Speaker E: It's probably not even installed by default. But if you open up the repository, go to accounts, there's a directory called store tool that's a binary for inspecting appendix.
00:48:41.044 - 00:48:45.824, Speaker C: So if you switch, if you change directory to run and then do ls, it will show you all the appendix.
00:48:49.084 - 00:48:50.184, Speaker E: It'll be a lot.
00:48:59.364 - 00:49:05.992, Speaker A: I'm assuming if we look at these, it's going to just be not useful or not human readable. I mean.
00:49:06.048 - 00:49:07.656, Speaker E: Yeah, yeah, they're all binary.
00:49:07.800 - 00:49:50.714, Speaker A: Yeah. Cool. Okay, so if you want to read those, go check out that tool that Brooks just mentioned in the accounts directory on the Monterey. Let's see, we had a question about how does no snapshot fetch differ from the new flag introduced for Fastboot? Basically before fastboot your node was going to unpack a snapshot and use that. But with fastboot you can skip the unpacking the snapshot and just use the existing accounts data that's already on your disk without unpacking the snapshot.
00:49:53.934 - 00:50:27.124, Speaker E: And they're related because they're both about kind of snapshots. However, they different, they differ in terms of like whether you're starting up from scratch and need a snapshot or if you're restarting. Because if you've never started up, you have to have a snapshot to begin with. And so the snow snapshot fetch is just controlling whether you're downloading a new snapshot and then the fastboot is controlling what you're using to boot from, whether it's a Snapchat archive or whether you already have something on disk that you want to use.
00:50:42.104 - 00:51:40.704, Speaker A: Let's see more detail about full stop shot interval stops and incremental snapshot interval slots. Optimal values. Okay, so this is a trade off between how much resources you want to spend writing snapshots versus how up to date you want snapshots to be in the event that you need them. I think our default recommendations are 2020, 5000 for slots for full snapshot increment and 100 for or, sorry, the default values. If you don't use the flags at all, I think it's 25,000 for full and 100 slots for incremental. If I remember correctly, that will probably work pretty well on most nodes. If you.
00:51:40.704 - 00:52:19.454, Speaker A: I honestly, my recommendation would be start with the defaults and if you're on into problems, try changing the defaults to mitigate whatever problems you're seeing. But yeah, those values work pretty well. 25,000 slots works out to, I think it's a little less than 10,000 slots an hour is my recollection. So it's like two to 3 hours. You'll write a full snapshot and then incremental snapshots will be written with the defaults like once a minute, something like that.
00:52:25.514 - 00:52:35.974, Speaker B: We're at time for the hour. I want to make sure you're not late for any other meetings. So, yeah, I guess the first question is will, do you have more time or.
00:52:37.444 - 00:52:39.624, Speaker A: Yes, I can stick around.
00:52:40.924 - 00:52:44.024, Speaker B: And the second question would be, any other questions from the audience?
00:52:54.044 - 00:53:13.234, Speaker D: I have a question. Do you guys have an expectation of when sort of the snapshot mechanism will not be sustainable due to the sizes of things and how big they're getting and how big ledgers are? I mean, are we heading for a cliff or do we have a long, long roadway in front of us?
00:53:16.454 - 00:54:12.924, Speaker E: This is an active research topic that we're trying to figure out. We're also trying to coordinate among different clients teams. So fire dancer and sig as well to see, because if more clients are going to reuse snapshots and we want to design something that's going to be reusable by everyone versus are we designing something just for the salon labs client? So we have some private test clusters where we just have tons and tons of accounts. They're up to like 20 billion accounts. And at that size there, the snapshots are like, you know, 500gb. They're not really practical to download and then catch up to the network. And so we're also trying to figure out how do we package and distribute snapshots what do we put in snapshots? Do we have another way to identify accounts that we never need to send around? Do we bittorrent stuff.
00:54:12.924 - 00:54:17.544, Speaker E: So all of these things we're trying to figure out, but we don't have answers yet.
00:54:24.344 - 00:54:24.696, Speaker A: All right.
00:54:24.720 - 00:54:48.808, Speaker B: I think we're probably at a good spot to wrap it up. Thank you, Will. And all the question answers from labs, very helpful for all operators who are on this call. There is a community led validator call going on right now, so highly encourage you to join that as well. The info is in discord right underneath info for this call. So. Yeah.
00:54:48.808 - 00:54:49.880, Speaker B: Thanks again, Will.
00:54:50.072 - 00:54:51.184, Speaker A: Cool. Thanks, everybody.
