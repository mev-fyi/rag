00:00:01.200 - 00:00:42.784, Speaker A: All right, welcome, everyone, to the salon and validator community discussion, March 2, 2023. The agenda today is all about the outage. On February 25, we're first going to talk about a community led post mortem that happened just the other day. Chris and Max, who led that call, are here, and they'll give us an update on kind of the highlights from that call. We'll have an update on the root cause analysis from engineering. So they'll talk about kind of what they at least think might be the issue and how we're going to test it out. And then I'll talk a little bit about the plan for Testnet.
00:00:42.784 - 00:01:20.074, Speaker A: So we're going to do something that we maybe haven't really done before, which is downgrade the cluster, test out the issue, make sure that we can actually reproduce the issue on Testnet and then upgrade again. But I'll get into the specifics as we get to it. All right, so first things first. Just yesterday, the validator community had a post mortem led by Chris Remus from Chainflow and Max Sherwood from h two o nodes. There are notes here that I will put in the chat, but I wanted to let either Max or Chris talk about the highlights from yesterday. So go ahead.
00:01:20.154 - 00:02:00.074, Speaker B: Yeah, yeah, I'll do that. So, hey, everybody, I'm Max from h two o nodes, and also ultimate app, which is a wallet on Solana. So, yeah, I mean, Chris thought it was important to kind of host this call while the outage was still in our fresh memories. So it took place, I guess, on Tuesday, two days ago. And, yeah, there was a lot to discuss, obviously. I guess we started talking about, like, the discussion on Discord. A number of people said, yes, it was a bit frantic, perhaps, but overall, you know, discussion is very important in times like this.
00:02:00.074 - 00:02:52.044, Speaker B: There was a bit of debate around how to temper voices that are a bit too loud and also kind of whose voices are more important than others. I guess the second topic that we talked about was on the decision of whether to restart or not because we kind of spent a long time seeing if the bug would resolve itself. And generally, the consensus in the call, I think, was that that was time that was wasted, and that generally these kinds of situations always result in a restart. So we should be more willing as a group to kind of undertake that restart quickly, perhaps after 20 or 30 minutes. It was suggested, you know, with no kind of concrete thing that we can point to as having caused the issue, we should. We should restart. That was kind of the consensus from the call.
00:02:52.044 - 00:03:56.014, Speaker B: Then we also debated how to empower validators to contribute to the discussion, to the conversation. So it was not clear to a lot of validators, kind of, you know, what the forking looked like. At some point, someone posted a chart with a graphical description, which really helped. So perhaps if we had a way of like script to kind of assist with making those kinds of visualizations, then more validators would have been able to contribute to the conversation. We also talked about how in the discord that day, there was some polling going on via emojis, which is obviously far from ideal. And we discussed that we might need some kind of, excuse me, some kind of off chain voting system. Obviously, if the network is not running, then we still need some kind of way of voting and polling each other, if there's some kind of decision that needs to be made, like whether or not to restart and on what version, for example.
00:03:56.014 - 00:04:47.144, Speaker B: And yeah, again, we discussed whose votes would be more important than others, maybe stake weight, et cetera. Then we started discussing disseminating snapshots. So, you know, that was also quite painful for people. Some people like, couldn't reproduce the snapshots and we're just kind of taking other people at their word for, like that. It was valid. So it was said that instead of having to generate the entire snapshot, maybe there should be a way for people to like verify a snapshot using a bank hash or something like that. So, yeah, Chris made a note of like a lot of to dos in his notes in section six, which is call call notes and key takeaways.
00:04:47.144 - 00:05:47.044, Speaker B: So, yeah, identifying tooling stuff around validator spec, some kind of, you know, way to reach a quorum and what kind of criteria we should use to kind of put importance on different people's votes, potentially, yeah, and some stuff around disseminating snapshots as well. So that's kind of the summary. It was not our intention at all to kind of, you know, front run this call, per se. But yeah, I think it was important to just do something quickly and, you know, it was a community led thing, so everyone was kind of welcome to post their own thoughts on the agenda beforehand. And, you know, the call wasn't gated or anything like that. So hopefully it was a productive space. I think there was some feedback that it was a productive conversation, and obviously we can kind of further that conversation today.
00:05:47.044 - 00:05:48.604, Speaker B: So, yeah, that's it.
00:05:50.024 - 00:05:54.014, Speaker A: Cool. Thanks, Max. Any thoughts, questions from that call or the highlights?
00:05:56.834 - 00:06:29.554, Speaker C: The only thing I would add is that I spent some time consolidating the to dos and they, they fall into I think three buckets now, if not four. I don't remember exactly, but what I'm thinking is we should try and find some people to sign up to address, like form a small team to sign up and address each one of those buckets. So after this, I'll start reaching out through the Discord channel, the mainnet beta validators discord channel, and see if anybody wants to sign up and help follow through on some of these things.
00:06:32.014 - 00:06:57.744, Speaker D: I had a question. I noticed that Solana had published a medium article that kind of outlined what steps they were taking to help make restarts better or reduce the outages. And there's some overlap between the things that the community wants to do and Solana. Are you guys working together, or is it possible you'll end up, like, conflicting? Where they build a tool, then you've already spent time on this and just wondering if there's communication.
00:07:01.764 - 00:07:05.516, Speaker A: Yeah, I guess I could point. Chris, go ahead.
00:07:05.540 - 00:07:15.664, Speaker C: I was gonna say, yeah, I was gonna say, I mean, this was just the start of an idea, so, Tim, maybe we can make sure to be coordinating closely there so there's not a duplication of effort.
00:07:16.124 - 00:07:49.764, Speaker A: Yeah, I guess just a little background on what happened this past week. There was an engineering offsite that was scheduled for, you know, two days after the outage happened. So a lot of the discussion at the offsite has been around testing and how we're going to improve the release cycle. So I think there's a lot of similar thoughts happening internally. But the. The offsite just happened up until yesterday, as far as I'm aware. So there's definitely a lot of discussion that needs to be had, and I'm sure there's some duplicated effort that we can work together on.
00:07:49.764 - 00:08:36.064, Speaker A: Okay, any other thoughts or questions? All right, so thank you, Max. Yeah. As most of you mentioned, there is a outage report that is posted on solana.com dot. It's pretty dry. It just tells us what we know so far, that there was an outage, and we're working on a root cause analysis. The details on what engineers know so far is in the MB outage 2023-0225 channel.
00:08:36.064 - 00:08:42.694, Speaker A: But if Steve or someone else is on, if there are any updates you want to add to that, that would be helpful.
00:08:45.994 - 00:09:39.272, Speaker E: Yeah, I can comment there. So, yeah, I'd say, yeah, efforts are still ongoing. I think for those that have been following along, there was a large block that was issued by a leader who forked off from the main fork about 50k slots before their. Their leader slots, and they that that's what caused them to create a block that was quite large, like a, like 150 case shreds, whereas the typical black is about one k shreds. And that that caused trouble for gossip or not. Sorry, turbine. And just the network in general, you know, heavy, heavy traffic and repairing, repairing that slot.
00:09:39.272 - 00:09:51.524, Speaker E: So, yeah, we're still digging. We're kind of running experiments on, like, private spun, privately spun up clusters. So still. Still a working effort.
00:09:55.944 - 00:09:57.724, Speaker A: Thank you. Any questions for.
00:09:58.334 - 00:10:28.374, Speaker F: Can I ask a question? And I don't want to rehash any sore points in any way, but I am curious to know, and this may be way far out there, do you have any evidence that had we somehow managed to get back to 113, it would have fixed this? Because I know that there's some discussion I don't quite understand about 113 was better at rejecting these bad shreds than 114 or something like that. If we had somehow managed to get back there, would we. Would it have fixed itself? Do you have any sort of prediction about that?
00:10:30.234 - 00:10:58.624, Speaker E: We actually are. We're playing around with it. I don't think I have a conclusive answer for you, but we are playing around with, like I said, we're spinning up. We have some scripts in the Monorepo, and we're spinning up clusters on, like, 1.131.14. So we're playing around with that. No answer for you yet, but we will can make a line item in the final post mortem when we're done about whether we think that would have been the case or not.
00:10:58.964 - 00:11:27.044, Speaker F: Yeah, I ask only because we have had some discussion about should we ever wait for these things to resolve themselves. And I guess having some confidence that this would have resolved itself could maybe inform that discussion we had yesterday, which was done without knowing whether or not 113 somehow would have fixed it. It's, you know, we're not going to know a perfect answer at any time about what we should do, but it's really just about having the most accurate information so that we can judge the probabilities of a similar event later.
00:11:27.704 - 00:11:50.230, Speaker E: Yeah, agreed. Um, yeah, I. Yeah, I personally wasn't online. I, you know, I caught up on discord after, but, um, agreed it would be good to know, like, yeah, if downgrading to 1.13 would have been the prudent move. So, yeah, that's definitely something we're thinking about. And, you know, having in the background back of our mind is like a, you know, something for next time.
00:11:50.230 - 00:11:55.754, Speaker E: Well, hopefully never next time, but something that's, you know, on the slate.
00:11:59.574 - 00:12:59.704, Speaker A: All right, thanks. And Tetsu, for those who didn't see, I'll put this in the chat as well. Solana, written by Anatoly, released a plan to improve testing and the release process for the network. So it goes into some details, and I'll kind of go over it after this slide as well. But it lays out the current process, how we do upgrades, and then, excuse me, the planned future process. So there's still a lot of details to figure out in here. Some of these things may be changing over time, but the general idea is a lot of the resources right now in engineering are going to be shifted from writing code to focusing on sort of penetration testing and verifying that the new release that we have is more stable, more reliable before we actually ship to mainnet.
00:12:59.704 - 00:13:04.004, Speaker A: Any questions about this?
00:13:06.664 - 00:13:21.656, Speaker C: All right, just that I'll say, I think that sounds great. We're maturing as a blockchain and it's time for the practices to mature as well. So that's news. Well received. I'm really happy to see it.
00:13:21.840 - 00:13:56.112, Speaker A: Cool. Yeah, so one of the points made in there was what we're going to be doing with Testnet. And I want to lay out a little bit kind of the plan for the immediate changes that are going to be happening to testnet and how we're going to reproduce, or hopefully reproduce this, the bug that caused this outage, and then how we're going to test it moving forward. So the basic plan is Testnet is currently on 1.15. Pretty much all the stake is on 1.15. We're going to have Testnet downgrade to 1.14. And the idea is, once we're downgraded to 1.14,
00:13:56.112 - 00:14:26.864, Speaker A: we could try to reproduce the bug that we all experienced on Mainnet. Once we reproduce that and we verified, we're pretty confident that the same issue that happened on Mainnet also happened on Testnet. We can then restart the cluster. Well, sorry, excuse me. First we can come up with a fix, make sure that we have a fix that we're pretty confident resolves the issue. We can downgrade the cluster again to 113.6 or five, whatever Mainnet is on.
00:14:26.864 - 00:15:07.896, Speaker A: We can then do the upgrade process that we plan to do on main net. So first get 10% of stake on 1.14, then 25% of stake on 1.14, and then the entire cluster on 1.14. After we've done that, we can again test the same issue and verify that it is in fact, resolved. So I think this is sort of the blueprint for what we'll do in the future for future rollouts to make sure that the state of Testnet and how we are sort of operating on Testnet is the same as how we're operating on Mainnet. And there's a lot of changes that will go into that in addition to just version upgrades.
00:15:07.896 - 00:15:20.544, Speaker A: There will also be, I think, more efforts focused on testing, but that's the general rollout plan. Any questions there? Yeah, Zendetsu.
00:15:20.884 - 00:16:03.034, Speaker F: Yeah, one more question for Steve. I also saw a little bit of discussion in discord and I may always miscategorize these things because I don't always understand what I'm reading, but it suggested that there had been sort of these large blocks on occasion in the past and maybe they were the same kind of scenario, but it just wasn't as long of a fork or whatever. And whoever was offending produced not so large of a block, but still large. Is that, is my assessment accurate? And if so, is that the kind of thing that since it kind of happens sometimes apparently on Mainnet, are we expecting it's going to happen also on Testnet or has been happening on Testnet so that we feel that, you know, that is getting covered or maybe you guys are going to do it manually also? I don't know.
00:16:03.934 - 00:16:40.944, Speaker E: Yeah, I think, I think we're, for any test efforts, we're going to do it manually. Um, actually probably not on the call, but um, John and Bazaar already have code that does it manually. Um, and the private clusters and then we have, we have a stake node on testing that. So we can do it manually. Um, but yeah, we, we have seen large blocks before, but not as far as I'm aware, not that large. I think we've seen, you know, like 20k shreds or something, but not, you know, 150k like I said, which was the one that knocked over the main net.
00:16:42.044 - 00:16:49.684, Speaker F: Was there an assessment previously but the 150k was like almost all proof of hash threads or something like that. Isn't that true? That's what it was doing.
00:16:49.724 - 00:17:21.860, Speaker E: Like, yeah, so the, we actually, yeah, so we have, we have code. So yeah, that node forked off as I mentioned. So it, yeah, it forked off 50k slots before and so when its leader slot came up, it had the parent going back to wherever it last voted correctly. So all the data shreds were thrown out. But the data shreds were thrown out because we have a check that says if the parent is smaller than the latest route, we know it's invalid.
00:17:21.892 - 00:17:22.060, Speaker A: Right.
00:17:22.092 - 00:17:39.958, Speaker E: Because we can immediately say it's on the wrong fork. Coding shreds don't contain the parent slot in the headers, so we ended up retaining those and doing like extra processing on them. Sorry, I can't. I don't recall. Not sure I got off the whole tangent. I'm not sure if I answered your.
00:17:39.966 - 00:17:54.476, Speaker F: Question, but for the smaller ones that were still big, is it the belief that those were similar occurrences or that we may have some other weird thing going on that could produce large blocks for other reasons? That could also be a problem that we're not evaluating? Is that.
00:17:54.620 - 00:18:22.264, Speaker E: Yeah, I think it's. I think it's similar, yes. Like the, like in this case, specifically, I was able to go back and logs. Most, you know, our nodes have like a, like a weak retention on logs and we're, you know, we're adjusting that. But like, I, you know, I was able to go back and confirm that this node voted incorrectly. So that's something we're looking into as well. This was the only node that voted incorrectly on that slot.
00:18:22.264 - 00:18:39.404, Speaker E: So as far as we know is a solar flare. You know what? You know something. Something. You know, I don't know. Maybe there's a chance there could be a non deterministic bug somewhere. But, yeah, we're. That's still an evaluation as well.
00:18:44.584 - 00:19:06.528, Speaker A: There's some questions in chat. Maybe you can elaborate. There's a question about why we went with 114.16 is recommended before allowing 5% to upgrade and then 20%. That's exactly what we did. So there was a call for 10% to upgrade. That happened.
00:19:06.528 - 00:19:26.224, Speaker A: There was a call for 25% or maybe 20% to upgrade. That also happened. Things were going well. Then we called for general adoption. And the bug only happened after the cluster fully upgraded to 1.14. So it's unfortunate that wasn't caught sooner. But again, the normal roll out plan happened there.
00:19:29.644 - 00:19:50.694, Speaker E: I'll also comment too. So the. In terms of wanting to get a majority to 1.14. So when we were in that period where there's like more than 33% on 1.14, if there's a, like a runtime change or something, that would cause a 1.14 node to vote differently than a 1.13 node.
00:19:50.694 - 00:20:25.974, Speaker E: Any of those difference there, differences there would cause the network to go down because, you know, we wouldn't be able to hit 66 and two thirds on Mainnet. So that, that's like a vulnerable region to be in. I think you might remember Michael Vines made a comment. So I think we called for an upgrade on Tuesday. It was coming in and then it was before the weekend. So that was kind of the rationale for wanting to get to a majority on one version because, you know, like 5050 is not great if there's, if there ends up being a difference.
00:20:29.954 - 00:20:51.914, Speaker A: Yeah. One process improvement we've been talking about internally as well is always calling for general upgrades Monday morning early or Sunday night. That way the majority of teams have the whole week to ideally plan out a rollout process and it's a little easier for, you know, non bigger operators to do it.
00:20:54.254 - 00:21:39.354, Speaker E: Yeah, I mean, I was on the call yesterday as well. And I think at like a valid point, I think, you know, maybe assuming we keep cadence with the schedule, you know, we could typically, we'll say like, okay, let's go to five or 10%, you know, and hold that for a couple days. Like we could explicitly spell that out with like days on the initial post and, you know, like whatever that cadence ends up being. You know, we can make that known in advance so that everybody's on the same boat because I got, you know, that that's maybe some tunnel vision or like, you know, maybe all of you probably aren't spending as much time in discord on the Rowling Channel as some of the people internally. So I think that's something we can do better in the future.
00:21:41.854 - 00:21:42.914, Speaker A: And. Go ahead.
00:21:49.174 - 00:21:56.634, Speaker D: Tim. Was there any plans to start equalizing the testnet stake weight to more. Well, to more mirror main net?
00:21:57.894 - 00:22:08.190, Speaker A: I think there is a plan to do that. We haven't talked about it a lot yet, so I've heard mention of it. It just hasn't been formalized.
00:22:08.382 - 00:22:26.478, Speaker E: Yeah, we're doing that. We're also, we're gonna make a ton of stake accounts so that we have a similar issue around. Like we have a heavier computation around epic boundaries that was in progress yesterday. Not sure if it was finished, but yeah. Yeah, we're doing that.
00:22:26.646 - 00:22:27.062, Speaker A: Cool.
00:22:27.118 - 00:22:27.914, Speaker D: Thank you.
00:22:32.494 - 00:22:35.834, Speaker A: Any other questions, things people want to bring up?
00:22:42.014 - 00:22:50.072, Speaker D: I guess I do have another question because a lot of people thought that the upgrade was happening on a Friday night, but obviously it happened on, on Tuesday.
00:22:50.208 - 00:22:50.504, Speaker A: Right.
00:22:50.544 - 00:23:04.764, Speaker D: And the, it continued through the weekend. Even if you start Monday morning, isn't there still a chance that there could be a massive change on a Friday evening again, like how, how does, how does, how does that get solved?
00:23:08.744 - 00:23:42.756, Speaker A: Yeah, that's a good question. I think one thing that maybe could be improved is we called for a general upgrade, but we sort of nudge bigger operators later in the week. I started contacting teams Thursday, Friday, and maybe that process could be earlier as well. So that's something we can do to kind of nudge it along. But also, it's difficult, right. It's distributed and we can't force people to upgrade. So there could always be a case where we call for it Monday morning and someone doesn't upgrade till Friday night.
00:23:42.756 - 00:23:53.064, Speaker A: And then, you know, we have a similar issue, but ideally, with more testing and more simulation of the rollout process, we'll have fewer. Fewer issues in Mainnet.
00:23:55.684 - 00:23:56.984, Speaker D: Cool, thank you.
00:24:05.284 - 00:24:20.884, Speaker A: I'm just looking through chat now to see if there's any other questions. Max, can you mention a little bit more about the lido plans? How does that work? They tell all the other operators to upgrade.
00:24:22.904 - 00:24:54.904, Speaker B: Excuse me, just to clarify, they didn't tell everyone to upgrade at the same time. We were one of the last ones to upgrade from the lido set. So they were just coming around to us toward the end of the week and saying, hey, guys, if you could upgrade soon, that'd be good. And we said, okay, we can do it, probably this weekend. And then the outage happened. So I don't know exactly how many of the lido operators had upgraded by that point, but it just. I was just saying, like, it just so happened that, you know, we were about to upgrade, being encouraged to do that by.
00:24:54.904 - 00:24:58.524, Speaker B: By Lido because we were kind of one of the last ones from their set.
00:24:59.264 - 00:25:07.370, Speaker A: Okay, makes sense. Zetetsu, go ahead.
00:25:07.562 - 00:25:46.994, Speaker F: I mean, I know this may be difficult for a lot of people to achieve, and it was difficult for me to achieve for a very long time, but if you have a primary and a secondary, you can keep different versions on them, which makes swapping back kind of, you know, it may be good for the cluster because it means that, for example, this most recent incident, you can swap back to 113 very easily, which I did. And I don't know how many people have a setup like that, but it's the kind of thing which could make these rollouts potentially more robust is if validators can quickly switch back if they need to, instead of having to do a restart and maybe download a snapshot, which may be hard to get, or so on.
00:25:50.934 - 00:25:58.624, Speaker A: Yep. I added the link from a few calls ago about the transition demo, so if you haven't checked that out, be sure to check it out.
00:25:59.564 - 00:26:18.772, Speaker D: Maybe going back to Zantetsu, like, having this hot swap. This is probably like a month or two ago, but there's a lot of conversation on how to actually set it up, and it wasn't actually super clear how to do it yet. And I don't think that there is very good instructions on that yet. Is that something that maybe is going to be moved closer to the floor?
00:26:18.868 - 00:26:46.130, Speaker F: Like, I'll be honest, I think. I think Michael Vine's instructions are sufficient if you already have the kind of knowledge you should have to operate a validator. I mean, I think the problem is that everyone has a slightly different way. They're going to run two machines. Some people may run two machines on two different networks. Maybe they're on the same network, maybe, you know, there's a lot like, I have them behind a little bastion system, and I can't get to them except through one machine. So I have a script on that machine that kind of does the work for me since it can connect to them both.
00:26:46.130 - 00:27:08.924, Speaker F: Other people may not find a script like that useful, and most of my work was involved in making that script work for me. So I think Michael binds instructions and pumpkin pools addition to that. Personally, I think they're sufficient, but if anyone reads them and finds them too confusing, then that's questions should be asked so that we know what parts are confusing to people. That's my opinion.
00:27:12.704 - 00:27:19.016, Speaker D: I didn't see pumpkins pools transition. This looks pretty great. Even has a YouTube video.
00:27:19.120 - 00:27:19.764, Speaker E: Nice.
00:27:20.944 - 00:27:21.232, Speaker A: Yeah.
00:27:21.248 - 00:27:35.764, Speaker F: The harder part is having two systems. That's just an extra expense that most people aren't going to be able to have. I understand that, but we have, you know, probably at least the top 20 or 30 have, probably should have the resources to be able to do that in terms of stake wave.
00:27:40.224 - 00:27:47.852, Speaker D: Well, maybe everyone can afford it now with the new TD's program, which is very generous and paying a lot of Solana out.
00:27:47.908 - 00:28:17.824, Speaker A: So. Great. Any other thoughts, questions before we wrap it up? No? All right, well, thank you, everyone, for joining. Thank you to Steve for explaining the root cause, ongoing investigation, and I'll see everyone in two weeks. Reminder that in two weeks, I'm just going to keep with the same cadence. This was supposed to be the alternating time call. I don't want to confuse things.
00:28:17.824 - 00:28:30.404, Speaker A: So next week I'll just do the same time slot as today. Sorry, two weeks from now, we'll do the same time slot as today, and then after that, we'll go back to the normal cadence. So anyway, see you all in two weeks. Thanks, everybody.
