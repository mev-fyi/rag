00:00:04.480 - 00:00:42.104, Speaker A: Hi and thanks Colin, for that great introduction. I'm Kevin Bowers and I guess my title at jump trading is chief science officer, but we aren't very title centric. The title of this talk is hold me closer, Frank and dancer, an introduction to jump and fire dancer in five easy pieces. When I was asked to speak at this conference, I was asked to cover five topics in this talk. Talk a little bit about what fire dancer is, give a little bit of background about me, give a little bit of background about jump, describe our development approach, and show our progress thus far. So that's just what we'll do. This is tricky and the limited time available, but there's a longer talk with demos tomorrow which will cover many more details.
00:00:42.104 - 00:01:26.056, Speaker A: The slide button's not going ah great. So what is firedancer? Fire dancer is a new Solana validator under development. I'm not going to spend a whole lot of time here talking about it, at least in terms of what it is. This audience already knows better than me what a validator is, and the long term importance of the sauna ecosystem of having a second validator and a third and a fourth. It is worth noting, though, that this development is being done fully in the open, and we are having this development extensively audited and where possible, formally verified. We encourage the community to take a look at the repo above. Likewise, I'm not going to spend much time covering my background.
00:01:26.056 - 00:02:24.480, Speaker A: You can read my bio or look me up online for details. However, I'm very much an outlier here as my background is neither crypto nor finance. This makes the how did I get here? Question relevant and worth a brief summary. I did my undergrad at Purdue in electrical engineering, initially focused on chip design and manufacturing. After an extended internship at Intel, I concluded Moore's law would end before I retired, so I pivoted towards electromagnetics and signal processing to unleash my inner physicist and mathematician. At the end of undergrad, I decided it was now or never for grad school and my chip exposure and my pivot accidentally primed me for doing computational plasma physics research after an internship doing signal processing research at Los Alamos, did my PhD at Berkeley and EECs on computational simulation of plasmas for chip manufacturing, and won a number of awards for this work. At this point, it looked like I was on the standard academic research track to become a professor, so I went to Bell Labs next to research optoelectronic chip manufacturing.
00:02:24.480 - 00:03:16.200, Speaker A: I got exposed to a lot of advanced telecom tech as Bell Labs continuously reorganized in response to the.com bust after several false starts at Bell Labs, I went back to Los Alamos and worked on massively parallel plasma simulation. This work also won several awards. Then a unique opportunity came up. A hedge fund was interested in designing custom supercomputers for biochemistry simulation in my past with chip design, massively parallel supercomputers, and plasma simulation, which, oddly enough, has lots of theoretical overlap with mechrodynamics, accidentally primed me for this work. So I went to de Shaw Research in New York City and spent several years doing computational biochemistry, and I won several awards for that work, too. Being adjacent to finance work in New York City demystified it for me and gave me the hubris to say probably when jump asked, could you make our computers faster? So I was off the jump in Chicago.
00:03:16.200 - 00:04:02.246, Speaker A: At that time, I didn't think of my prior work to be directly useful, but I was wrong. Nearly everything I've done in my past has been applied both directly and indirectly at jump trading. For years, I thought I was just lucky to stumble into a role that exactly matched my rather atypical background. A few years ago, I realized that this wasn't entirely chance JMP adapts to its employees, more so than any organization I've been exposed to. And though all this was pre JMP getting into crypto, this adaptability probably is related to why JMP adapted so fast to crypto as well, too. There's been a recurrent theme in all of this, and what basically has happened is an organization approaches me and says, hey, we have some useful code, some useful models. It's not very well documented.
00:04:02.246 - 00:04:45.834, Speaker A: It doesn't run as robustly or as fast as we'd like. Can your team make a robust, fast, and documented implementation? And then I say, probably. From my point of view, all that happened was I was asked the same question about Solana about a year ago, and I gave the same answer. And since this work started a couple months ago, the main surprise for me is the level of attention this work is getting. It's kind of surreal to be standing here on this stage with our code snips tweeted about as we're checking them in, and Internet memes floating around about us. We are generally more comfortable putting our heads down, coding away in the background, and then maybe a much smaller community noticing much, much later. But we have been pleasantly surprised and grateful for the welcoming reception this community has given us.
00:04:45.834 - 00:06:08.644, Speaker A: It isn't obvious what JMP might want to do with a computational physicist, or what Solana might want to do with the trading firm, for that matter. So why jump? On? This slide is an abstract from a talk at an FPGA conference seven years ago given by my colleague Kaveh Asari. Kave will be joining me on stage tomorrow on the Dev workshop. Like me, he was on the academic track and eventually found his way to jump trading. His colleagues, still in academia, were asking him, what interest could a finance firm possibly have in an FPGA hardware research engineer? Isn't it all Wolf of Wall street in badly written Excel spreadsheets? It's not. And as an aside, like when interviewing somebody who spent his career in the extremes of scientific computing and was uncomfortable if he'd fit in at a finance firm, when I asked at the end of his interview, do you have any questions for me? He asked, what is the airspeed velocity of an unladen swallow? And I promptly said, african or european? And I guess that was the right answer because he did join jump, and afterwards he related to me that my answer dispelled any notion that he was joining a stereotypical finance firm when he came here. So anyway, Kavi's colleagues were asking him to make an abstract for this talk, and this presented us with the challenge of making an abstract that was sufficiently academic enough to make it through peer review, but also sufficiently anodyne to make it through legal review.
00:06:08.644 - 00:06:33.096, Speaker A: And this is what we came up with. I'm not going to read this abstract to you, I'll just tell you what it really says. Everybody hires smart people. Our job would be a lot easier if our competitors weren't so good and everybody has the same information. This is less obvious than a frequent worry to people outside this nation. This once included me. In practice, exchanges are under intense competitive regulatory pressure to ensure everybody has the same information.
00:06:33.096 - 00:07:04.722, Speaker A: Nobody wants to trade where the fix is in. The level of exclusivity required to get co located. Space and connectivity is little more than do you have a credit card? Overall, exchanges don't want to simultaneously make less money and irritate regulators by turning away customers. Broad trends in the events like this conference make it abundantly clear to us that we're likely to see even further reductions of barriers to entry to finance. And as we see, we view this as a good thing. And this explains a lot of jumps. Interest in this space next is everybody has the same laws of physics.
00:07:04.722 - 00:07:47.376, Speaker A: This almost seems too obvious to state, but it has deep implications for this industry. If you look in this competition for fairness between exchanges, in some venues it has trading firms fighting over single digit nanoseconds and if you really want to understand this, you think about the size of a packet and how long it is. You're actually talking about a couple bytes within a packet of competition there. When you're down there, nothing in the commodity space is relevant, nothing works. And this has forced firms like us to essentially create a parallel universe tech stack developed from the transistor and fiber optic up. Now, when you combine all these things together, what you find is that how you execute is decisive in this industry. And there are two big areas of execution.
00:07:47.376 - 00:08:24.818, Speaker A: One is taking an idea into a strategy, and then the second is implementing that strategy sufficiently fast to be competitive. And so, ideas to strategies is the quantitative research side of the equation. It's figuring out what the prices are. This involves a lot of supercomputer style number crunching on large data sets, machine learning. JMP is somewhat the hipsters in this space because we were into this niche before it was cool. But since everybody is smart and everybody has the same information, everybody eventually figures out the right prices. So you just want to figure them out first, because, as we'll see, exchanges are strongly incentivized to prioritize the people who bring this new info to market first.
00:08:24.818 - 00:09:03.068, Speaker A: So, strategy to implementation is where all the high performance networking and computing come into play. Now, this structure, and the entire structure of the electronic trading industry for that matter, is just a consequence of price time priority at an exchange. To understand this better, price priority is basically quantitative research. Time priority is all the high performance computing. And from my more computer science point of view, an exchange is just a concurrent data structure that runs a distributed protocol to do consensus price discovery. In old timey photos like you see here, you can even see people standing at a chalkboard, running the algorithm by hand. Nowadays, people at chalkboards are servers at an exchange.
00:09:03.068 - 00:09:39.364, Speaker A: People shouting at them are trading firm servers, and the shouts are network packets. I didn't know any of this before coming to JMP. My mental model of the world at that time was the price ferry, flittering about bestowing prices randomly on securities. But understanding how it really works is useful both in general and for this talk to understand why firms like JMP are so into trading and suitable for a project like fire dancer. Now, this figure is a cartoon over here, where the buyers are on the left, the sellers are on the right. The prices go vertically from low to high. And Bob and Carol have shown up to sell at different prices, but there are no buyers, so they wait.
00:09:39.364 - 00:10:04.574, Speaker A: Price priority is an algorithmic guarantee that if a trade happens, it was at the best possible price at that venue. At that time, Alice has shown up as a buyer. The price she is willing to buy at is lower than the price Bob and Carol are willing to sell at. There is no price that satisfies any the buyer seller pair. So Alice waits two Alice raised her price now. And now there's a range of prices that Alice and Bob agree are mutually fair. Nobody agrees with Carol.
00:10:04.574 - 00:10:30.286, Speaker A: Alice and Bob will trade. The specific prices will be in the agreeable range, but otherwise depends on the exchange's policies. If Alice wants more than Bob has to offer, Alice will end up rating for the west and vice versa. Now, in this example, Alice is impatient to buy and offers a very high price. Alice and Bob trade first. If Alice wants more than Bob is offering, Alice and Carol trade next. And if Alice wants more still, Alice will wait for the rest after the trades.
00:10:30.286 - 00:11:06.210, Speaker A: If the exchange gave the initial trade to Carol, Alice would conclude the exchange is a bad place run by bad people who should feel bad and take her business to another exchange where they have better policies. And if there was no such other exchange for Alice to take her business to, there would be big pressure and big opportunity to create one. And then the bad exchange would learn the hard way. The answer to the question what would happen if you made an exchange and nobody showed up? Now, in this example, Carol reduced her price to be competitive with Bob to break ties. In cases like this, a very common method is time priority. Like before, Alice and Bob will trade, followed by Alice and Carol. If at all.
00:11:06.210 - 00:11:28.008, Speaker A: Alice and Carol get the same price for the trade with Bob and a better price for any trade with Carol. Bob gets the lion's share of the trade. This is literally preschool logic. I was first in line teacher. It is well motivated. Theoretically, though, people want the best prices because nobody wants to pay more, and they want them as soon as possible because few are willing to wait. By statistical necessity, these are going to be the places with the most participants.
00:11:28.008 - 00:12:03.014, Speaker A: And you can just understand this, that if I take a small subset, random subset of sellers, the best price amongst them is probably not going to be the best price amongst all sellers because it's probably unlikely I picked the best one on my small sample. So, exchanges want lots of people to show up because they can only improve the price. Now here, Carol sped up a little bit. Now she gets the lion's share of the opportunity. And this means that all a firm light jump can do is offer better deals than everybody else, at least some of the time. Otherwise, nobody trades with us and we die a slow death. This is a necessary condition, though not a sufficient condition.
00:12:03.014 - 00:12:24.858, Speaker A: Our best prices need to be profitable on average for us too. If they aren't, everybody trades with us and we die a fast death. The process is known as adverse selection. Our counterparties neither know nor care whether a deal is good for us. In fact, they don't even know who we are because exchanges rarely give away that information, and for good reason. Shares are interchangeable. We don't need to know who is offering and why.
00:12:24.858 - 00:13:14.654, Speaker A: And conversely, we generally don't want to know our counterparties to know why we are offering or who we are. The second thing that we can do is we can take deals that are net profitable for us at least some of the time. And because those will probably be competitive for profitable for our competitors as well, we have to do that really, really fast too. So in this dynamic, there are other rules for breaking ties in time priority. The big kind of question here is what generates the best outcome for retail and institutional investors? And anything that incentivizes people to show up and get lots of traders and people and institutional investors and whatnot to be there is going to be a place with good prices. Empirically, price time priority works best, but it doesn't really make a difference to us. It just changes the flavor of the technical challenges that we have to deal with.
00:13:14.654 - 00:13:57.894, Speaker A: Now, the psychology of an electronic trading firm further creates a feedback loop which drives technology investment. To understand this, understand that traders live to trade. For them, profits are just a market signal that they're good at what they do, and they look at that as saying, great, I get to do more of what I love, and they reinvest their profits in trading. Now, on top of that, I stole this phrase from our Coo, who recently used it in a presentation of this competitive paranoia. And this price time dynamic that I talked about creates a winner take most dynamic. It's very Talladega nights if you're not first year last kind of mentality. And so you always have this justified worry that you might be falling behind.
00:13:57.894 - 00:14:40.126, Speaker A: Or to put it more colloquially, on good days, the firm thinks to itself, we have a deep and prophetic understanding of markets. And on bad days we think to ourselves, we have a deep and prophetic understanding of markets, but our competitors are killing us because our tech is awful. So when you're looking at where to reinvest your profits, what are you thinking about doing? Well, always at the top of your mind is thinking about, let's reinvest it in technology. Now, to us, commodity technology is table stakes. Anything that we can buy, our competitors can buy. So by itself, any commodity technology is not a source of competitive advantage. Finding new ways to leverage commodity tech can be.
00:14:40.126 - 00:15:22.322, Speaker A: We continually evaluate vendor offerings to see how they might improve our performance. So you see huge investments in commodity tech systems that are as large as anything I was playing with at the national labs. At the same time, commodity tech is often uncompetitive. In fact, it's worse than that because it's getting worse over time. There's an anti Moore's law thing going on for us. Our tech stack has been diverging from commodity tech stacks for over a decade now. And that's because vendors are increasingly prioritizing things like, you know, the average throughput, the latency, sorry, the trading away, their burst throughput, their latency, their predictability of their systems for average throughput, better adaptive power consumption and yield.
00:15:22.322 - 00:16:10.770, Speaker A: And that's because they're targeting loud cloud vendors. So these are the worst possible trade offs for a firm like us. So when we're looking at the things that we need to do to keep on surviving, excuse me, like, you know, sending the right order to an exchange in second place ten times is just a really expensive way to lose money. So while we are high margin for vendors, we're not big enough for them to really care about us. So you see huge investments in custom tech as well. Now, jump has a vc arm, and a vc once related to me, that he viewed electronic trading as the super bowl of technology. And that's because we deal with all the problems we need, systems that are this laundry list of, you know, accountable, transparent, fault tolerant, adaptable, scalable, predictable, high throughput.
00:16:10.770 - 00:16:38.044, Speaker A: And yes, the one everyone thinks of when they think of a firm like this, low latency. But this isn't one of those situations where they say, here's a laundry list of things you want your system to have. Pick two. We have to get them all right, otherwise we die in the markets. So you see, customized tech has gone everywhere within our stack. Prior to crypto taking off, though, though we had all this tech, we were mostly blind to use cases outside of trading and rarely talked about it. And we are still pretty freaked out talking about it right now.
00:16:38.044 - 00:17:28.130, Speaker A: Remember, traders lived a trade. So discussions that started with we could use our trading tech x to do big non trading thing y. We'd get puzzled looks and responses on bad days. Like, didn't you get the memo that our tech is really, really slow? Or, you know, on good days, what does that have to do with making our trading more competitive? When crypto started taking off though, and we saw the potential and the trends, we started waking up to the fact that we have a lot to offer outside of conventional trading, we realized that instead of being way behind in many areas, we were way ahead. Given these dynamics, from a networking point of view, traditional markets look like a distributed denial of service attack that never ends with multiple high bandwidth network links that routinely simultaneously saturate at line rate. And we need to keep up with that to stay alive. And much of this is beyond what current commodity tech can handle, especially in software.
00:17:28.130 - 00:18:18.934, Speaker A: So our bread and butter is redundant. Multi homed high bandwidth network links feeding a distributed heterogeneous mixture of CPU's, GPU's, FPGA's and Asics. Ultimately, jump production systems look like a distributed packet filtering and transform engine, along with some capture and replay infrastructure for various types of offline analysis. And so does a Solana validator. Thinking longer term, it isn't too surprising JMP would be deeply interested in an efficient, robust and trusted means for recording transactions and handling transaction loads like seen in traditional finance requires handling burst speeds in the millions. Even supporting just one transaction per day per person in the world requires about 80,000 transactions per second. We want to give Solana a lot of room to grow, so we're approaching fire dancer development layer by layer with the three high level components shown here.
00:18:18.934 - 00:18:47.796, Speaker A: This is in alignment with our development philosophy. We tend to run along the data path and optimize as we go. To understand why, I've grabbed some slides from a presentation I made 15 years ago about the first large scale simulations on the first supercomputer in the world to break a pedaflop. I actually presented these in Lisbon many, many moons ago, around that time. I view this slide as where Moore's law accidentally died. You can blame me if you want. This is a picture of a dimm and the high level specs of a high end compute core circa late two thousands.
00:18:47.796 - 00:19:22.680, Speaker A: Well, today, dimms are still the same size and commodity cores are still about the same speed. All the advances we've seen in the last decade in manufacturing technology have gone into making more cores and not faster cores. And there's good engineering and economic reasons for this. And given the way dram works under the hood, the critical path roughly scales at the perimeter of the dimm. And we know the speed of light, and we know the materials involved, and we know the distance here. So we know it takes a couple of nanoseconds for light to propagate around the dimm that's around ten clocks. We could have done 80 floating point operations per core at that time.
00:19:22.680 - 00:20:04.764, Speaker A: There were eight such cores per socket in the system. We could have done 640 flops in a socket in that time, and the equivalent nowadays would be in the mid to high thousands on a high end cpu socket. JMP's tech stack is the result of optimizing around the speed of light at a global scale. In essence, we do relativistic trading, as everything we do is speed of light limited, like the realtor expression location, location, location. Optimizing location is the key for high performance. Unfortunately, most vendors have gone the opposite way and keep trying to abstract away locality details from developers as though everything is still just a pc at from 1982. Unsurprisingly, most developers only get results that scratch the surface of what is possible.
00:20:04.764 - 00:20:30.370, Speaker A: Fortunately, it's even worse than that. When developers try to get into this, they are discouraged. I think about half the tech headlines I read today basically amount to write wasteful code. Buy more cloud encourages green cloud provider there's lots of expressions like premature optimization is the root of all evil, which abound within the area. No premature optimization. That's completely wrong. We view that as the ultimate NPC comment.
00:20:30.370 - 00:21:07.326, Speaker A: Like we wonder when somebody says that to us if they go home and stand in a closet in a t position at night. The big thing that we keep looking at in those areas is that, excuse me, the big thing that keeps on going on there is that if you don't get your data flow right up front, it's not a simple recompile to fix it. It's a complete rewrite. And so we start off by optimizing the data flow and making sure we get that right. So we have something that's fundamentally sound from the get go. The next is the tyranny of Amdahl's law. Something we didn't bother to optimize will pump up whack a mole style and become performance limiting.
00:21:07.326 - 00:21:46.216, Speaker A: We've seen too many projects fail because of uncompetitive performance caused by a computer science version of decision paralysis. Essentially, nothing gets optimized because there's no immediate payoff to any one optimization, because there's so many components of the system and none of them are particularly well optimized. So when you combine locality constraints with heterogeneous computing, the world is an even harsher place. It doesn't obey Amdahl's law. It obeys Amdahl street justice. Dedicated compute in the right location is compute that can't be used for other calculations because either it's in the wrong location or or it's too specialized for the other calculation. The parts you don't optimize actually get worse because they have less compute available to them for a fixed system cost.
00:21:46.216 - 00:22:30.036, Speaker A: As a result, in niches like high performance computing and finance, roughly speaking, rare niches. Unlike most of the technology industry, where machine time is more valuable than developer time, everything must be optimized eventually. We just accept this as a matter of fact and optimize as we go. I'm not getting an advance. Ah, there we go. Additionally, we are doing this incrementally to modularize the validator as part of the documentation and standardization goals, and to keep fire dancer development in sync with ongoing Solana development. This leads to the concept of a Franken dancer, where we stitch components we've developed into Solana's existing validator as we go and slowly make a new validator, and one that others can replace components of as desired for their own validators.
00:22:30.036 - 00:23:23.194, Speaker A: We've named our first Franken dancer. Frankly, while there might be some advantages to running Frank in the wild, we suspect this initial step will be akin to putting a sports car in the middle of a traffic jam. But we also expect more. As more components get improved and bottlenecks get removed, the robustness in performance will grow. This is a high level block diagram for Frank. We don't have much time to get into details, but we are planning for support for multi homed redundant high bandwidth nics, coherently randomized flow steering for load distribution, the ability to use hardware acceleration and the use of multiple processes process boundaries for additional security between components, and potentially support even more aggressively distributed implementations in the future. We are supporting relatively low end vanilla cloud hosts running vanilla Linux operating systems with minimal external dependencies, but we are optimizing for higher end high core account servers and longer term aiming to leverage hardware already available in the cloud.
00:23:23.194 - 00:23:39.118, Speaker A: Drilling down this is zoom with some of the tiles. We also don't have much time to get into details here. Suffice to say, it's all checked in. Take a look. Notably, there's no memory allocations or atomic operations or hardware fences on the critical path. Allocations are bounded and pneuma optimized. At initialization.
00:23:39.118 - 00:24:13.012, Speaker A: Objects are persistent and typically on a single gigantic page per NuMA node. For maximum TLB efficiency, we have good inter process hardware interfacing interface. We have the ability to asynchronously inspect these things remotely and snapshot the running state tiles could be asynchronously started, stopped, and restarted. They can be grouped into processes for that enhanced security and operational things. The tile stacks are typically back, so on and so forth. I can keep on going, but we can talk more about that tomorrow, along with talking more about development strategy. Then we plan to do a live demo of the code as it is checked in today.
00:24:13.012 - 00:25:08.180, Speaker A: Running on a very old x 86 architecture GCP host with a very vanilla operating system and a Solana like distribution of transactions in a two to one redundant high availability configuration, getting around 1.2 million raw transactions per second and around 0.6 million unique after deduplication on a recent high clock rate, high core count hosts. With all the fancy we can run substantially faster, but we thought it important to demonstrate that you can just run this on widely available hardware now and then Philip will present some of our results from our initial efforts applying quantitative techniques to improve block packing efficiency. We've already been able to improve the prediction quality dramatically while simplifying the algorithm, and this is applicable to the existing validator too. And that can give benefits right now to just efficiency of packing blocks and better utilization of the existing block rates. Then Kave will do a live demo of some of our cheap and widely available FPGA's to dramatically reduce the hardware footprint required for a validator while dramatically increasing throughput.
00:25:08.180 - 00:25:45.984, Speaker A: We're calling the hardware acceleration efforts related to fire dancer, particularly network ones. Wire dancer components we've studied for acceleration are shown on the right. Roughly speaking, hardware accelerated Sig verify a protocol and flow steering offload, hardware accelerator replay and hardware accelerator transaction processing. We've been targeting readily available FPGA hardware initially. Among other things, Kave will show SHA 512 at 100 gigabits line rate using only a tiny fraction of the area on an older, low end FPGA readily available at cloud providers. And this is the first step of doing hardware accelerated Sig verify, and I think with that, that's all I have to say, but see us tomorrow for additional details.
