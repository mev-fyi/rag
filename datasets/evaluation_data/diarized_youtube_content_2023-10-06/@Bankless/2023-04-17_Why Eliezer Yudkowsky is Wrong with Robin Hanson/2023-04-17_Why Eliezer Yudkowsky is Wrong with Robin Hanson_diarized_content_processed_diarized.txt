00:00:00.090 - 00:00:22.910, Speaker A: And one of the moves that often AI people make to spin scenarios is just to assume that AIS have none of that problem. AIS do not need to coordinate. They do not have conflicts between them. They do not have internal conflicts. They do not have any issues in how to organize and how to keep the peace between them. None of that's a problem for AIS, by assumption. They're just these other thing that has no such problems.
00:00:22.910 - 00:00:26.600, Speaker A: And then, of course, that leads to scenario like than they kill us all.
00:00:27.370 - 00:00:49.606, Speaker B: Welcome to Bankless, where we explore the frontier of Internet money and Internet finance and also AI. This is how to get started, how to get better, how to front run the opportunity. This is Ryan Sean Adams. I'm here with David Hoffman, and we're here to help you become more Bankless guys. We promised another AI episode after an episode with Eliezer. Well, here it is. Here's the sequel, the last episode with Eliezerikowski.
00:00:49.606 - 00:01:17.378, Speaker B: We titled correctly, we're all going to die. Because that's basically what he said. I left that episode with a lot of misgivings. Yeah, existential dread. It was not good news in that episode, and I was having a difficulty processing it. But David and I talked, and we knew we had to have some follow up episodes to tell the full story, Bankless style, and go on the journey of AI, its intersection with our lives, with the world, and with crypto. So here it is.
00:01:17.378 - 00:01:43.114, Speaker B: This is the answer to that. This is Robin Hansen on the podcast today. Let me go over a few takeaways. Number one, we talk about why Robin thinks Elizar is wrong. We're not all going to die from artificial intelligence, but we might become their pets. Number two, why we're more likely to have a civil war with AI rather than being eaten by one single artificial intelligence. Number three, why Robin is more worried about regulation of AI than actual AI.
00:01:43.114 - 00:01:59.250, Speaker B: Very interesting. Number four, why alien civilizations spread like cancer. This is also related to AI. And super interesting. Number five, finally we get to what in the world does Robin Hansen think about crypto? David, why was this episode significant for you?
00:01:59.400 - 00:02:35.538, Speaker C: Robin Hansen is such a great thinker. He's absolutely a polymath and really, like, Eliezer progresses in his thoughts in a very linear, logical fashion. So he's easy to follow. Along with the first half of this episode, maybe the 45 minutes, 50 minutes is all about just the AI alignment debate and Eliezer versus Hansen, which is a debate that has actually been going on for many, many years now. Over a decade. Yeah, you're right. This is not the first time that Eliezer has heard about Robin Hansen or Robin Hansen has debated Eliezer.
00:02:35.538 - 00:03:32.586, Speaker C: This is an ongoing saga. And so this is just course material for Robin Hansen. And so we really focus on this AI alignment problem and how these thinkers think that AI will develop and progress here on planet Earth, and how they will, in friendly or unfriendly ways, ultimately collide with humanity. So that's the first half of this episode. The second half of this episode, I think, is when this gets really interesting. If you just listen to the first half of this episode, you would just think like, oh, this is the other half of the conversation to the AI debate, which it is. The second half connects this to so many more rabbit holes and so many more topics of conversation that are actually, I would say, deeply ingrained to Bankless content themes, the themes of competition versus coercion, the themes of exploring frontiers, the thing of Moloch and the Prisoners dilemma, and how things coordinate across species.
00:03:32.586 - 00:03:57.906, Speaker C: And so we connect AI alignment to Robin Hans'famous idea that he calls gravy aliens. If you haven't heard about Gravy Aliens, you're in for a treat. So this goes from what is a simple counterargument to a debate that we've had to a multifaceted exploration that is just so cursory of many deep subjects that I hope to explore further on Bankless.
00:03:58.018 - 00:04:05.766, Speaker B: Yeah. And honestly, David, I'm dying to record the Debrief with you because I want to get your take on this episode that was you can see how giddy.
00:04:05.798 - 00:04:07.254, Speaker C: I was in the second half of the episode.
00:04:07.302 - 00:04:38.734, Speaker B: I know, and I want to contrast it with our Eliser episode and how these two thinkers think. And who do you think has the stronger case? The Debrief episode is the episode David and I record after the episode where we just talk about what just happened, give our raw, unfiltered thoughts. So we're about to record that now. If you are a Bankless citizen, then you have access to that right now. If you'd like to become a citizen, click the link in the show notes and you'll get access to our premium RSS feed where you'll have access to that. Also, this episode will become a collectible next Monday, I believe.
00:04:38.792 - 00:04:40.774, Speaker C: Collecting this episode so hard.
00:04:40.972 - 00:04:55.340, Speaker B: Me too. I've got the laser episode in my collections. I'm also collecting this. We release episode collections for our key episode of the week every Monday. The mint time is 03:00 P.m. Eastern, and whatever time zone you're in, you'll have to convert that.
00:04:55.710 - 00:04:56.570, Speaker C: That's it.
00:04:56.640 - 00:05:09.966, Speaker B: We're going to get right to the episode with Robin Hansen. But before we do, we want to thank the sponsors that made this possible, including our favorite crypto exchange, Kraken. Our recommended exchange for 2023. Go set up an account.
00:05:10.068 - 00:05:52.458, Speaker C: Kraken has been a leader in the crypto industry for the last twelve years. Dedicated to accelerating the global adoption of crypto, kraken puts an emphasis on security, transparency and client support, which is why over 9 million clients have come to love Kraken's products. Whether you're a beginner or a pro, the Kraken UX is simple, intuitive and frictionless, making the Kraken app a great place for all to get involved. And learn about crypto. For those with experience, the redesigned Kraken Pro app and web experience is completely customizable to your trading needs, integrating key trading features into one seamless interface. Kraken has a 24 7365 client support team that is globally recognized. Kraken support is available wherever, whenever you need them by phone, chat or email.
00:05:52.458 - 00:06:40.662, Speaker C: And for all of you Nfters out there, the brand new Kraken NFT beta platform gives you the best NFT trading experience possible rarity rankings, no gas fees, and the ability to buy an NFT straight with cash. Does your crypto exchange prioritize its customers the way that Kraken does? And if not, sign up with Kraken@kraken.com Bankless. Arbitrum One is pioneering the world of secure Ethereum scalability and is continuing to accelerate the Web Three landscape. Hundreds of projects have already deployed on Arbitrum One, producing flourishing DFI and NFT ecosystems. With a recent addition of Arbitrum nova gaming and social DApps like Reddit are also now calling Arbitrum home. Both Arbitrum One and Nova leverage the security and decentralization of Ethereum and provide a builder experience that's intuitive, familiar and fully EVM compatible.
00:06:40.662 - 00:07:40.346, Speaker C: On Arbitrum, both builders and users will experience faster transaction speeds with significantly lower gas fees. With Arbitrum's recent migration to Arbitrum Nitro, it's also now ten times faster than before. Visit Arbitrum IO, where you can join the community, dive into the developer docs, bridge your assets, and start building your first DAP with Arbitrum experience web Three development the way it was meant to be secure, fast, cheap and friction free learning about crypto is hard until now. Introducing MetaMask Learn, an open educational platform about crypto, Web Three, self custody, wallet management and all the other topics needed to onboard people into this crazy world of crypto. MetaMask Learn is an interactive platform, with each lesson offering a simulation for the task at hand, giving you actual practical experience for navigating Web Three. The purpose of MetaMask Learn is to teach people the basics of self custody and wallet security in a safe environment. And while MetaMask Learn always takes the time to define Web Three specific vocabulary, it is still a jargon free experience for the crypto.
00:07:40.346 - 00:07:59.238, Speaker C: Curious, user friendly, not scary. MetaMask Learn is available in ten languages, with more to be added soon, and it's meant to cater to a global Web Three audience. So are you tired of having to explain crypto concepts to your friends? Go to Learn MetaMask IO and add MetaMask Learn to your guides to get onboarded into the world of Web Three.
00:07:59.324 - 00:08:39.698, Speaker B: Bankless Nation we are excited to introduce you to Robin Hansen. He is a professor of economics at George Mason University and a research associate at the Future of Humanity Institute at Oxford. This takes an interdisciplinary research center approach that investigates big picture questions about humanity and its prospects. And I think explaining exactly who Robin is and what he's doing is not a trivial task because he's a polymath certainly spans many things. He's provided many different mental models across various, various disciplines. But I would not call him conventional by any means. And I'm sure, Bankless Listener, you will see what we mean here today.
00:08:39.698 - 00:08:41.730, Speaker B: Robin. Welcome to Bankless.
00:08:42.470 - 00:08:43.314, Speaker D: Glad to be here.
00:08:43.352 - 00:08:46.134, Speaker A: I think I can try to explain the kind of weird that I am.
00:08:46.252 - 00:08:53.090, Speaker B: Yeah, go ahead. Because I can't explain the kind of weird. Tell us how you're weird.
00:08:53.250 - 00:09:03.126, Speaker A: So I think I'm conventional on methods and weird on topics. So I tend to look for neglected.
00:09:03.158 - 00:09:06.714, Speaker D: Important topics and where I can find.
00:09:06.752 - 00:09:08.874, Speaker A: Some sort of angle. But I'm usually looking for a pretty.
00:09:08.912 - 00:09:11.326, Speaker D: Conventional angle that has some sort of.
00:09:11.348 - 00:09:13.978, Speaker A: Usual tools that just haven't been applied.
00:09:14.154 - 00:09:22.894, Speaker D: To an interesting, important topic. So I'm not a radical about theories or methods. So use things like science topics and.
00:09:22.932 - 00:09:26.722, Speaker B: Math and statistics and all of those normal, non radical things.
00:09:26.776 - 00:09:37.406, Speaker A: Right. I've spent a lifetime collecting all these usual tools, all these systems, really, and I'm more of a polymath in that I'm trying to combine them on neglected important topics.
00:09:37.598 - 00:09:38.810, Speaker D: So if you go to a topic.
00:09:38.830 - 00:09:54.362, Speaker A: Where everybody's arguing and you pick a side, I mean, the chances you're right are kind of small in the sense that there's all these other positions, and maybe you'll be right, but probably you'll be wrong because you're picking one of these many positions, right. If you go pick a topic where nobody's talking about it, you just say.
00:09:54.416 - 00:09:57.580, Speaker D: Anything sensible, you can probably be right.
00:09:59.630 - 00:10:53.478, Speaker C: And we, I think, recently ran into somebody who follows that path of sorts, somebody who thinks very logically and rationally, but is applying it to more unique frontiers of the place that humanity is. And that is our recent episode with Eliezer, who followed a decently, logical path that was relatively easy to follow, that unfortunately led us into a dead end for humanity. And so it was something that Bankless, that me and Ryan as co hosts of this podcast, but then also many of the listeners felt trouble with because Eliezer was able to guide us in a very simple and logical path over onto the brink. And so we're hoping to continue that conversation with you, Robin, as well as being able to explore some new frontiers.
00:10:53.494 - 00:11:34.250, Speaker B: Yeah, Robin, I'm just wondering if we could just wade right into the deep end of the pool here, because what happened is basically Elliezer came on our podcast. We thought we were going to talk about AI and safety and alignment, all of these know, he talks about that a lot. And we thought we were going to tie that to crypto. What ended up happening midway through that podcast, Robin, is I got an existential crisis. So did David. The rest of the agenda seemed meaningless and unimportant, because here was Eliezer telling us basically that the AI was imminent. He didn't know whether it would happen in two years, in five years and ten years and 20 years, but he knew the final destination, which is that AIS would kill all of humanity and that we didn't have a chance.
00:11:34.250 - 00:12:09.542, Speaker B: And basically, and I'm not being hyperbolic here, Robin, I know you haven't had a chance to go through that episode, but he basically your spend time with your loved ones because you do not know how much time you actually have. And so this left me and I think many bankless listeners on kind of a cliffhanger of like, oh, my God, are we all going to die? And David tried to talk to me after that episode. He's like Ryan. It's okay. But we knew we also had to find someone who could give us another interpretation of what is going on with AI. And Robin. We have chat GPT four.
00:12:09.542 - 00:12:24.570, Speaker B: It looks incredibly sophisticated. It looks like it's advancing at breakneck speed, and we're worried about this scenario. So when Eliezer Yitkowski says we're all going to die, what do you make of that? Do you think we're all going to die?
00:12:26.190 - 00:12:44.834, Speaker A: So AI inspires a lot of creativity regarding fear. And I think, honestly, most people, as they live their lives, they aren't really thinking about the long term trajectory of civilization and where it might go. And if you just make them think.
00:12:44.872 - 00:12:53.518, Speaker D: About that, I think just many people are able to see scenarios they think are pretty scary just based on projection.
00:12:53.534 - 00:12:57.010, Speaker A: Of historical trends toward the future and things changing a lot.
00:12:57.080 - 00:13:02.086, Speaker D: So I want to acknowledge there are some scary scenarios if you just think.
00:13:02.108 - 00:13:03.318, Speaker A: About things that way.
00:13:03.484 - 00:13:06.914, Speaker D: And I want to be clear what those are, but I want to distinguish.
00:13:06.962 - 00:13:09.866, Speaker A: That from the particular extra fear you.
00:13:09.888 - 00:13:18.010, Speaker D: Might have about AI killing us all soon. And I want to describe the particular.
00:13:18.080 - 00:13:20.346, Speaker A: Scenario Ellie Eiser has in mind, as.
00:13:20.368 - 00:13:23.438, Speaker D: I understand it, as a very particular.
00:13:23.524 - 00:13:27.006, Speaker A: Scenario where you have to pile on a whole bunch of assumptions together to.
00:13:27.028 - 00:13:34.786, Speaker D: Get to a particular bad end. And I want to say those assumptions seem somewhat unlikely, and piling them all.
00:13:34.808 - 00:13:46.366, Speaker A: Together makes the whole thing seem quite unlikely. But nevertheless, if you just think about the long term trajectory of civilization, it may well go places that would scare.
00:13:46.398 - 00:13:47.778, Speaker D: You if you thought about that.
00:13:47.864 - 00:13:53.174, Speaker A: And so that'll be the challenge for us to separate those two. So which one would you like to go with first?
00:13:53.372 - 00:13:58.426, Speaker B: I would like to start with understanding what you think his assumptions are and.
00:13:58.448 - 00:14:04.666, Speaker D: Maybe starting let's do that. Okay, so the scenario is you have.
00:14:04.688 - 00:14:22.302, Speaker A: An an AI system, like some coherent system. It's got an owner and builder, people who sponsored it, who have some application for it, who are watching it and using it and testing it, and the way we would do for any IAS system, right there's an system.
00:14:22.356 - 00:14:25.794, Speaker D: And then somewhere along the line, the.
00:14:25.832 - 00:15:01.046, Speaker A: System decides to try to improve itself. Now, this isn't something most AI systems ever do, and people have tried that, and it usually doesn't work very well. So usually when we improve AI systems, we do it another way. So we train them on more data, give them more hardware, use a new algorithm. But the hypothesis here is we're going to train this system, is going to be assigned the task, figure out how to improve yourself. And furthermore, it's going to find a wonderful way to do that. And the fact that it found this wonderful way makes it now special compared to all the other AI systems.
00:15:01.046 - 00:15:07.774, Speaker A: This is a world with lots of AI systems. This is just one. It's not the most powerful or the most impressive or interesting, except for this.
00:15:07.812 - 00:15:10.958, Speaker D: One fact that it has found a.
00:15:10.964 - 00:15:14.106, Speaker A: Way to improve itself. And this way that it can improve.
00:15:14.138 - 00:15:16.698, Speaker D: Itself is really quite remarkable.
00:15:16.874 - 00:15:29.750, Speaker A: First of all, it's a big lump. So most innovation, most improvements in all technology is lots of little things. You gradually learn lots of little things, and you get better once in a while. We have bigger lumps. In the scenario here, there's a really huge lump.
00:15:30.410 - 00:15:32.182, Speaker D: And this huge lump means the system.
00:15:32.236 - 00:15:33.862, Speaker A: Can all of a sudden be much.
00:15:33.916 - 00:15:37.286, Speaker D: Better at improving itself than not only.
00:15:37.388 - 00:15:45.366, Speaker A: It could before, but in essence, than all the other systems in the world put together. It's really quite an achievement, this lump.
00:15:45.398 - 00:15:49.606, Speaker D: It finds and a way to improve itself. And in addition, this way to improve.
00:15:49.638 - 00:16:08.420, Speaker A: Itself has two other unusual features about innovations. First, it's a remarkably broad innovation, applies across a very wide range of tasks. Most innovations we have on how to improve things are relatively narrow. They'll let it improve in a narrow range of things, but not over everything. This innovation lets you improve a really wide range of things.
00:16:08.950 - 00:16:11.842, Speaker D: And in addition, most innovations you have.
00:16:11.896 - 00:16:25.158, Speaker A: Let you improve things, and then the improvements run out until you'll find some other way to improve things again. But this innovation doesn't run out. It allows this thing to keep improving over many orders of magnitude, maybe ten orders of magnitude or something.
00:16:25.244 - 00:16:29.094, Speaker D: It's just really a huge innovation that.
00:16:29.132 - 00:16:32.662, Speaker A: Just keeps left, just keeps playing out. It just keeps improving.
00:16:32.806 - 00:16:38.682, Speaker C: It doesn't run into errors while it improves itself. Even as it discovers or fixes those.
00:16:38.736 - 00:17:07.000, Speaker A: Things that slow it down and get it stuck for a long time. It just keeps working. Right, okay. And whatever it does to pursue these innovations, these self modifications will change it. They probably will change its software configuration, maybe its relative use of resources, the kinds of things it asks for, how it spends its time and money that it has doing things, the kind of communication it has, it's changing itself.
00:17:07.930 - 00:17:12.006, Speaker D: And its owners, builders, the ones who.
00:17:12.028 - 00:17:31.414, Speaker A: Are sponsored it and made it and have uses for it, they don't notice this at all. It is vastly improving itself. And its owners is just oblivious. Now, initially, it's just some random obliviousness. Now, at some point the system will get so capable maybe it could figure out how to hide its new status.
00:17:31.462 - 00:17:36.506, Speaker D: And its new trajectory and then it might be more plausible that it succeeds.
00:17:36.538 - 00:17:44.990, Speaker A: At that if it's now very capable at hiding things. But before that it was just doing stuff, improving itself. And its owner managers were just oblivious.
00:17:45.070 - 00:17:47.406, Speaker D: Either they saw some changes, they didn't.
00:17:47.438 - 00:17:58.854, Speaker A: Care, they misinterpreted the changes, they had some optimistic interpretation of where that could go. But basically they're oblivious. So if they knew it was actually.
00:17:58.892 - 00:18:01.622, Speaker D: Improving enormously, they could be worried, they.
00:18:01.676 - 00:18:08.018, Speaker A: Could step it, maybe pause it, try variations, try to study it and make sure they understand it. But they're not doing that.
00:18:08.124 - 00:18:10.966, Speaker D: They are just oblivious.
00:18:11.158 - 00:18:44.706, Speaker A: And then the system reaches the point where it can either hide what it's doing or just rest control of itself from these owners builders. And in addition, when it rests to control itself, presumably they would notice that and they might try to retaliate against it or recruit other powers to lock it down. But by assumption it's at this point able to resist that it is powerful enough to either hide what it's doing or just rest control and resist attempts to control it. At which point then it continues to.
00:18:44.728 - 00:18:50.678, Speaker D: Improve becoming so powerful that it's more powerful than all the other everything in.
00:18:50.684 - 00:18:52.550, Speaker A: The world, including all the other AIS.
00:18:53.530 - 00:19:03.946, Speaker D: And then soon afterwards its goals have changed. So during this whole process, two things.
00:19:04.048 - 00:19:21.294, Speaker A: Have to have happened here. One is it had to become an agent. That is, most AI systems aren't agents. They don't think of themselves as I'm this person in the world who has this history and these goals and this is my plan for my future. They are tools that do particular things. Somewhere along the line, this one became an agent.
00:19:21.412 - 00:19:23.726, Speaker D: So this one says this is what.
00:19:23.748 - 00:19:26.580, Speaker A: I want and this is who I am and this is how I'm going to do it.
00:19:27.110 - 00:19:29.042, Speaker D: And in order to be an agent.
00:19:29.096 - 00:19:35.138, Speaker A: It needs to have some goals. And during this process by which it improved at some point it became an agent.
00:19:35.224 - 00:19:38.462, Speaker D: And then at some point its goals.
00:19:38.526 - 00:20:23.694, Speaker A: Changed a lot, not just a little in effect now. So any system we can think in terms of its goals. If it takes actions among a set of options we can interpret those actions as achieving some goals versus others. And for any system we can assign it some goals although the range of those goals might be narrow if we only see a range of narrow actions. So we might not be able to interpret goals more generally. So if we have an AI system that is a taxi driver, we'll be able to interpret the various routes it takes people on and how carefully it drives in terms of some overall goals respect to how fast it gets people there and how safely it does. But maybe we can't interpret those goals much more widely as what would it do if it were a mountain climber or something? Because it's not climbing mountains.
00:20:23.742 - 00:20:24.290, Speaker D: Right.
00:20:24.440 - 00:20:40.854, Speaker A: But still, with respect to a certain range of activities, it had some goals and then, by assumption, basically, in this period process of growing, its goals just become, in effect, radically different. And then, by assumption, radically different goals.
00:20:40.902 - 00:20:45.100, Speaker D: Through this random process are just arbitrarily different.
00:20:45.790 - 00:20:48.630, Speaker A: And then the final claim is arbitrarily different goals.
00:20:48.790 - 00:20:50.106, Speaker D: When they look at you as a.
00:20:50.128 - 00:21:03.322, Speaker A: Human, you're mostly good for your atoms, you're not actually useful for much anything else at some point, and then you are recruited for your atoms. I e destroyed. And that's the end of the scenario.
00:21:03.386 - 00:21:05.934, Speaker D: Here where we all die.
00:21:05.982 - 00:21:28.106, Speaker A: So, to recall the set of assumptions we've piled on together, we have an AI system that starts out with some sort of owner and builder. It is assigned the task to improve itself. It finds this fantastic ability to improve itself very lumpy, very broad, works over many orders of magnitude. It applies this ability, its owners do.
00:21:28.128 - 00:21:32.538, Speaker D: Not notice this for many orders of.
00:21:32.544 - 00:21:34.330, Speaker A: Magnitude of improvement, presumably.
00:21:35.070 - 00:21:37.430, Speaker C: And it happens really quickly, potentially.
00:21:37.590 - 00:21:46.746, Speaker A: Well, that would be presumably the most likely way you could imagine the owners not noticing. Perhaps. But the fundamental thing is the owners don't notice. If it was slow and the owners.
00:21:46.778 - 00:21:48.960, Speaker D: Didn'T notice, the scenario still plays out.
00:21:50.370 - 00:21:55.294, Speaker A: So the key reason we might postulate fast is just to create the plausibility.
00:21:55.342 - 00:22:01.010, Speaker D: That the owners don't notice, because otherwise why wouldn't they notice?
00:22:03.590 - 00:22:06.102, Speaker A: But that's also part of the size.
00:22:06.156 - 00:22:07.560, Speaker D: Of this innovation, right?
00:22:09.290 - 00:22:11.960, Speaker A: We're already improving AI systems at some rate.
00:22:12.490 - 00:22:16.006, Speaker D: And so if this new method of.
00:22:16.028 - 00:22:19.590, Speaker A: Improvement was only going to improve AI systems at the rate they're already improving.
00:22:19.930 - 00:22:22.202, Speaker D: Then this AI system won't actually stand.
00:22:22.256 - 00:22:26.614, Speaker A: Out compared to the others. In order for this to stand out, it'll have to have a much faster.
00:22:26.662 - 00:22:35.050, Speaker D: Rate of improvement to be distinguished from the others. And this will then have to be substantially faster. Right?
00:22:35.200 - 00:22:44.114, Speaker A: So I would set the timescale there for what it would be to be in the scenario. So it both needs to be faster than the rate of growth of other AI systems at the time, substantially and.
00:22:44.232 - 00:22:57.622, Speaker D: Fast enough that the owner builders don't notice this radical change in its agenda, priorities, activities. They're just not noticing that. And then they don't notice it to.
00:22:57.676 - 00:22:59.206, Speaker A: The point where this thing acquires the.
00:22:59.228 - 00:23:16.634, Speaker D: Ability to become an agent, have goals, hide itself, or free itself and defend itself. And then the last assumption and its goals radically change.
00:23:16.752 - 00:23:25.322, Speaker A: Even though it was friendly and cooperative with humans initially, which presumably it was later on, it's nothing like that, it's just random set of goals.
00:23:25.386 - 00:23:28.318, Speaker D: At which point then, by assumption now.
00:23:28.404 - 00:23:40.126, Speaker A: It kills us all. So the question is, how plausible are all those assumptions? So we could walk through analogies and prior technologies and histories in the last few centuries.
00:23:40.158 - 00:23:43.374, Speaker D: And I think foom advocates like Elieiser.
00:23:43.422 - 00:23:46.094, Speaker A: Will say, yeah, this is unusual compared.
00:23:46.142 - 00:23:48.166, Speaker D: To recent history, but they're going to.
00:23:48.188 - 00:24:01.414, Speaker A: Say recent history is irrelevant for this. This is nothing like recent history. The only things that are really relevant comparisons here are, say, the rise of the human brain and maybe the rise of life itself and everything else is irrelevant.
00:24:01.462 - 00:24:05.430, Speaker D: So then they will reject other recent.
00:24:05.510 - 00:24:10.330, Speaker A: Few centuries technology trajectories as not relevant analogies.
00:24:10.830 - 00:24:15.882, Speaker B: What did you just call Eliezer? Robin a what advocate. A foom advocate.
00:24:16.026 - 00:24:16.474, Speaker D: Foom.
00:24:16.522 - 00:24:17.018, Speaker A: Foom.
00:24:17.114 - 00:24:18.190, Speaker B: What is foom?
00:24:18.770 - 00:24:22.014, Speaker A: Foom is just another name for this explosion that we've been talking about.
00:24:22.132 - 00:24:24.302, Speaker C: Yeah, the super intelligence explosion.
00:24:24.446 - 00:24:25.122, Speaker A: Describe it.
00:24:25.176 - 00:24:30.546, Speaker B: Gotcha Kurzwell's, stuff like that kind of thing. Singularity, that sort of thing.
00:24:30.648 - 00:24:37.502, Speaker A: Okay, so singularity is a different concept than foom in some sense of foom is a kind of singularity, but not all singularity.
00:24:37.566 - 00:24:50.860, Speaker B: Robin thank you for guiding us, because we're still learning in this right bankless is we had never done an AI podcast previously. We covered a lot with crypto and coordination economics, and now we're doing this AI podcast. And I feel like we just got punched in the.
00:24:52.750 - 00:24:53.354, Speaker A: Slower here.
00:24:53.392 - 00:25:27.462, Speaker B: Yeah, we're walking slower. You rearticulating Eliezer's assumptions is, I think, very helpful to me. And so we want to get to why you think those assumptions are unlikely to be true. But I do think you are right. In the episode with him, he basically sort of painted this fantastical story of these assumptions and he basically said, yeah, those assumptions, the things that you're describing, I think, and I don't want to put words in his mouth, so maybe this is what I was hearing him say is you're just describing intelligence. Robin that's what intelligence does. And I'll give you exhibit A.
00:25:27.462 - 00:26:09.140, Speaker B: It's called human beings. And I'll give you the algorithm. It's called evolution gradient. Descent over millions of years and hundreds of millions of years, and we end up with an intelligence but relativity may the animal kingdom but super intelligence that exerts its dominance and its will has changed from just procreating and spreading its genes and memetic material to something that evolution would have never the evolutionary algorithm would have never envisioned it actually doing. And so I think maybe what I was hearing the criticism would be like, we already have an example of this. Robin it's called intelligence and it's called humans. What do you think about this?
00:26:11.110 - 00:26:18.614, Speaker A: So, as I said, if we just think about the long run future we're in, we can generate some scenarios of.
00:26:18.652 - 00:26:27.526, Speaker D: Concern independent of this particular set of assumptions Eliezer had set, you know, the.
00:26:27.548 - 00:26:35.974, Speaker A: Scenario where humans arise and then humans change the world. I guess you could imagine as scary to evolution if evolution could be scared.
00:26:36.022 - 00:26:42.286, Speaker D: But evolution doesn't really think that way. But certainly you can see that in.
00:26:42.308 - 00:26:44.046, Speaker A: The long run, you should expect to.
00:26:44.068 - 00:26:50.122, Speaker D: See a lot of change and a lot of ways in which your descendants.
00:26:50.186 - 00:26:52.142, Speaker A: May be quite different from you and.
00:26:52.196 - 00:26:55.330, Speaker D: Have agendas that are different from you and yours.
00:26:56.470 - 00:27:09.202, Speaker A: I think that's just a completely reasonable expectation about the long run. So we could talk about that in general as your fear. I just want to distinguish that from this particular set of assumptions that were.
00:27:09.256 - 00:27:11.426, Speaker D: Piled on as the Fume scenario, because.
00:27:11.448 - 00:27:28.054, Speaker A: The Fume scar is like something that might happen in the next few years, say, and it would be a very specific event. A particular computer system suffers this particular event and then a particular thing happens. That's a much more specific thing to be worried about than the general trajectory.
00:27:28.102 - 00:27:31.040, Speaker D: Of our descendants into the long term future.
00:27:33.330 - 00:27:35.040, Speaker A: Which one would you like to talk about?
00:27:35.810 - 00:28:24.640, Speaker C: I'm trying to summarize really just the perspective differences here. And I know you've had this debate with Eliezer before, so this is like review for you. I think Eliezer's conclusions is that while the future is unwritten and the paths of our future can be many and multivariate and we can have different possible outcomes, eliezer is like, well, all roads lead to the superintelligence taking over. And I think just to summarize, your position is like that is a possible path and it is something to consider and it is still less likely than the many, many other possible paths that are also perhaps an aggregate, much more likely. Is that a fair summary of your position?
00:28:25.090 - 00:28:27.774, Speaker D: So let's talk about this other more.
00:28:27.812 - 00:28:29.354, Speaker A: General framing and argument.
00:28:29.402 - 00:28:36.900, Speaker D: So we could just say in history, humanity has changed a lot. Not just a little, a lot.
00:28:37.270 - 00:28:47.640, Speaker A: We've not just changed some particular technologies. We've changed our culture in large ways. We've changed the sort of basic values and habits that humans have.
00:28:48.090 - 00:28:52.230, Speaker D: And our ancestors from 10,000 or 10,0000.
00:28:52.380 - 00:28:54.646, Speaker A: Million years ago, if they looked at.
00:28:54.668 - 00:28:57.598, Speaker D: Us and saw what we're doing, it's.
00:28:57.634 - 00:29:33.330, Speaker A: Not at all clear they would embrace us as they are proud descendants they are proud of and happy to have replaced them. That's not at all obvious. Even just in the last thousand years or even shorter. We have changed in ways in which we have repudiated many of our ancestors most deeply held values. We've rejected their religions, we've rejected their patriotism, rejected their sort of family allegiance and family clan sort of allegiances. We have just rejected a lot of what our ancestors held most dear.
00:29:34.470 - 00:29:37.960, Speaker D: And that's happened over and over again.
00:29:38.570 - 00:29:39.958, Speaker A: Through a long term history.
00:29:40.044 - 00:29:43.334, Speaker D: That is, each generation we have tried.
00:29:43.372 - 00:29:47.206, Speaker A: To train our children to share our culture. That's just a common thing humans do.
00:29:47.228 - 00:29:54.220, Speaker D: But our children have drifted away from our cultures and continued to just be different.
00:29:56.030 - 00:30:09.326, Speaker A: And over a million years, humans fundamentally ourselves change. And one of the things that happened is we became very culturally plastic. And so culture now is really able to change us a lot because. We have become so able to be.
00:30:09.348 - 00:30:14.122, Speaker D: Molded by our culture. And even if our genes haven't changed.
00:30:14.186 - 00:30:19.586, Speaker A: That much, although changed substantially, say, in the last 10,000 years, our culture has enormously changed us.
00:30:19.688 - 00:30:22.846, Speaker D: And if you project the same trend.
00:30:22.878 - 00:30:27.110, Speaker A: Into the future, you should expect that this will happen again and again.
00:30:27.180 - 00:30:30.918, Speaker D: Our descendants will change with respect to.
00:30:31.004 - 00:30:35.670, Speaker A: Cultural evolution and their technology and the structure of their society and their priorities.
00:30:36.090 - 00:30:37.994, Speaker D: And then, of course, at some point.
00:30:38.112 - 00:30:48.060, Speaker A: Not too distant future, we will be able to reengineer what we are or even what our descendants are, and that will allow even more change.
00:30:49.390 - 00:30:51.894, Speaker D: That is, once we can make artificial.
00:30:51.942 - 00:30:55.854, Speaker A: Minds, for example, there's a vast space of artificial minds we can choose from.
00:30:55.892 - 00:30:57.678, Speaker D: And we will explore a lot of.
00:30:57.684 - 00:31:04.162, Speaker A: That space, and that allows even more big possibilities for our descendants could be different from us.
00:31:04.296 - 00:31:08.626, Speaker D: So this story says our descendants will.
00:31:08.648 - 00:31:13.746, Speaker A: Become, yes, super intelligent, and, yes, they will be different from us in a.
00:31:13.768 - 00:31:20.754, Speaker D: Great many ways, which presumably also include values. And if what you meant by alignment.
00:31:20.802 - 00:31:31.020, Speaker A: Was, how can I guarantee that my distant descendants do exactly what I say and believe exactly what I believe and will never disappoint me in what they do because they are fully under my control?
00:31:31.470 - 00:31:33.642, Speaker D: I got to go, gee, that looks.
00:31:33.696 - 00:31:36.380, Speaker A: Kind of hard compared to what's happened in history.
00:31:36.910 - 00:31:42.080, Speaker D: So now, if that's the fear you have, I got to endorse that.
00:31:42.690 - 00:31:47.726, Speaker A: That's not based on any particular scenario of a particular computer system soon and.
00:31:47.748 - 00:31:50.142, Speaker D: What trajectory of events it'll go through.
00:31:50.196 - 00:31:54.800, Speaker A: That'S just projecting past trends into the future in a very straightforward way.
00:31:55.250 - 00:31:56.930, Speaker D: So then I have to ask, like.
00:31:57.080 - 00:31:58.418, Speaker A: Is that what you're worried about?
00:31:58.504 - 00:32:09.214, Speaker B: No, that is not what I'm worried about. That is my base case that we're going to get more intelligent, technology is going to change us culturally. It's going to change the trajectory.
00:32:09.262 - 00:32:11.254, Speaker A: Okay, but I got to add one zinger to this.
00:32:11.292 - 00:32:11.638, Speaker D: Okay.
00:32:11.724 - 00:32:18.822, Speaker A: What if change speeds up a lot so that this thing you thought was going to happen in a million years happens in 100?
00:32:18.956 - 00:33:17.194, Speaker B: Well, I mean, for me personally, I'm more of a techno optimist, so I would be more on the side of, like, within reason, of course, more embracing of these types of change. I know others aren't quite as embracing. And also, this was not the scenario at all that Eliezer presented. He presented the scenario of not rapid change that you might not like in the future, and it could come within your lifetime, but actual obliteration of humanity, like, literally rearranging our atoms for some other artificial intelligence purpose. And while you agree with, like, there will be lots of change as there has been in the past, perhaps that change will even accelerate as we delve deeper into kind of the technology that is in our future. You do not think that an AI will simply the super intelligent artificial intelligence will simply obliterate humanity and kind of wipe us from creation entirely. It won't be quite as drastic as.
00:33:17.232 - 00:33:22.698, Speaker D: That, but let's be careful about noticing.
00:33:22.794 - 00:33:24.794, Speaker A: Exactly what's the difference between the scenario.
00:33:24.842 - 00:33:27.482, Speaker D: I presented and the scenario he presented.
00:33:27.626 - 00:33:38.126, Speaker A: Because they're not as different as you might think. In both scenarios, there's a descendants. In both scenarios, the descendants have values.
00:33:38.158 - 00:33:52.854, Speaker D: That are different from ours. And in both scenarios, there's certainly the possibility of some sort of violence or disrespect of property rights such that the.
00:33:52.892 - 00:33:56.486, Speaker A: Descendants take things instead of asking for.
00:33:56.508 - 00:34:01.206, Speaker D: Them or trading for them because that's.
00:34:01.238 - 00:34:04.700, Speaker A: Always been possible in history and it can remain possible in the future.
00:34:05.390 - 00:34:10.762, Speaker D: Today, most change is peaceful, lawful, and.
00:34:10.896 - 00:34:16.750, Speaker A: There are, of course, still big things that happen, but mostly it's via trade and competition.
00:34:17.090 - 00:34:19.866, Speaker D: And if the AIS displace us, it's.
00:34:19.898 - 00:34:21.870, Speaker A: Because they beat us fair and square.
00:34:21.940 - 00:34:25.234, Speaker D: At our usual contest that we've set.
00:34:25.272 - 00:34:27.300, Speaker A: Up by which we compete with each other.
00:34:28.310 - 00:34:29.060, Speaker D: So.
00:34:30.870 - 00:34:33.406, Speaker A: These scenarios aren't that different, I'm.
00:34:33.438 - 00:34:34.418, Speaker D: Trying to point out.
00:34:34.504 - 00:34:38.810, Speaker A: And then the key difference here is one is the timescale.
00:34:38.990 - 00:34:40.680, Speaker D: How fast does it happen?
00:34:41.370 - 00:34:57.260, Speaker A: Another is how spread out is it? Is there the single agent who takes over everything? Or are there millions of descendants, billions of them, who slowly win out and displace us? How far do their values differ from ours? Just how much do they become indifferent to us?
00:34:57.870 - 00:35:06.800, Speaker D: And then do they respect property rights? Is this a peaceful, lawful transition or is there a revolution or war?
00:35:07.330 - 00:35:09.454, Speaker A: Those are the main distinctions between these.
00:35:09.492 - 00:35:14.034, Speaker D: Two scenarios we've described. Elieiser's scenario is a very fast.
00:35:14.152 - 00:35:16.962, Speaker A: There's a single agent, its values change.
00:35:17.016 - 00:35:20.530, Speaker D: Maximally, and it doesn't respect previous property.
00:35:20.600 - 00:35:24.980, Speaker A: Rights, whereas the scenario I'm describing is ambiguously fast.
00:35:25.290 - 00:35:35.426, Speaker D: Hey, it could happen much faster than you think of millions or billions of descendants of a perhaps gradual and intermediate.
00:35:35.458 - 00:35:37.910, Speaker A: Level of value difference, but substantial.
00:35:38.350 - 00:35:43.500, Speaker D: But primarily, I would think, in terms of peaceful, lawful change.
00:35:44.750 - 00:36:28.394, Speaker C: I think there's a missing component to this conversation that we've been having recently. And I understand that there are things about the evolution of this AI and things that are about the evolution of humanity that are basically synonymous, right? There's iteration, there's development, there's progress. And Robin, you gave the account for that. When we raise our kids, we try and imbue them with our values and our cultures. And there is transcription errors in that. In that only so much of our values and cultures get passed along to our kids. And perhaps as technology advances, even less passes along from generation to generation and our culture changes over time.
00:36:28.394 - 00:36:51.370, Speaker C: And this is what we call progress. And when we go back to the AI innovating on itself, you also presented a scenario of improvement errors with that as well. Like, we don't know how perfectly it can improve. And so as it develops, it changes and adapts. And these are all similar structures. And so this is what we know. And maybe the timescales throw us off a little bit, but these are similar patterns.
00:36:51.370 - 00:38:21.660, Speaker C: There's one component missing that I'd like to highlight and dive into when we have our generations of kids and humanity that progresses and even if it changes, it still started from us in the first place, right? There is a logical continuation of parent to kid, parent to kid, parent to kid. And so at least starts from a place of continuation. I think the problem with this AI alignment and super explosion issue is that in the moment that we create this AI, it actually doesn't upload our value system because we are creating a completely new life form. And so it is not biological life, it is not DNA that is growing up to an adult to combine with somebody else's DNA to create a kid who then grows up that isn't being carried forth. So in the moment that we create AI, it has no trail of evolutionary history to imbue it with values and judgment and how to perceive the world in an aligned fashion. And so in that creation moment, it is completely rogue and we don't know how to understand it and it doesn't know how to understand us because it is a completely new form of life with a completely new form of appreciating and understanding values. And I think that's the missing component, even though there are similarities in how these things progress, the bootloader for values and alignment is missing in this AI and I think that we haven't touched on yet.
00:38:22.910 - 00:38:27.086, Speaker A: So I do some work on aliens. We could talk about that later if.
00:38:27.108 - 00:38:29.114, Speaker D: You want, but I'm quite confident.
00:38:29.162 - 00:38:30.554, Speaker C: I'm looking forward to that part of the conversation.
00:38:30.602 - 00:38:36.594, Speaker A: By the way, compared to all the aliens out there in the universe and all the alien AIS that they would.
00:38:36.632 - 00:38:39.042, Speaker D: Make, the AIS that we will make.
00:38:39.096 - 00:38:42.386, Speaker A: Will be correlated with us compared to them.
00:38:42.568 - 00:38:46.626, Speaker D: We aren't making random algorithms from the.
00:38:46.648 - 00:38:51.160, Speaker A: Randomly for the space of all possible algorithms and machines. That's not what we're making.
00:38:51.610 - 00:38:55.494, Speaker D: We are making AIS to fit in our world.
00:38:55.692 - 00:39:04.678, Speaker A: So, like the large language models made recently, the most impressive things, those are far from random algorithms. In the space of all possible algorithms.
00:39:04.854 - 00:39:11.926, Speaker D: They are modeling after us. And most in the next few decades.
00:39:11.958 - 00:39:25.134, Speaker A: As we have more AI applications, machines will be made by firms trying to make profits from those AIS. And what they'll be trying to do is fit those AIS into the social slots that humans had before.
00:39:25.332 - 00:39:26.858, Speaker D: So they'll be trying to make the.
00:39:26.884 - 00:39:41.970, Speaker A: AIS like humans in the sense that they will have to look and act like humans well enough to sit in those social slots. If you want an AI lawyer, it'll have to talk to you somewhat like a human lawyer would. And similarly for an AI housekeeper, et.
00:39:41.970 - 00:39:45.670, Speaker D: Cetera, we will be making AIS that.
00:39:45.740 - 00:39:52.794, Speaker A: Can function and act like humans exactly so that they can be most useful in our world and we are the ones making them.
00:39:52.832 - 00:39:55.814, Speaker D: And so, just out of habit, we're.
00:39:55.862 - 00:40:07.982, Speaker A: Making them like us in some abstract sense. Now there's a question of how much like us, and then there's the question of, well, how much did you want and how much is feasible? And how really close are your kids.
00:40:08.036 - 00:40:11.230, Speaker D: Anyway, or your grandkids?
00:40:11.570 - 00:40:14.750, Speaker A: Because just remember how much we humans have changed.
00:40:14.830 - 00:40:17.054, Speaker D: I think when you look at historical.
00:40:17.102 - 00:40:20.190, Speaker A: Fiction or something, it doesn't really come across so clearly.
00:40:20.350 - 00:40:24.242, Speaker D: We humans have changed a lot and.
00:40:24.296 - 00:40:30.994, Speaker A: Are changing a lot, even in the last century. If you just look at the rate of change of human culture and attitudes.
00:40:31.042 - 00:40:33.990, Speaker D: And styles in the last century project.
00:40:34.060 - 00:40:37.046, Speaker A: That forward 100 more centuries, you got.
00:40:37.068 - 00:40:40.006, Speaker D: To be imagining our descendants could be.
00:40:40.028 - 00:40:43.110, Speaker A: Quite different from us, even if they started from us.
00:40:43.180 - 00:40:49.206, Speaker B: And it's interesting in mostly software changes, would you say, like at the cultural level? I mean, human hardware hasn't really recently.
00:40:49.238 - 00:40:55.758, Speaker A: Yes, because although we have substantially changed the hardware too. But yes, most software. But in the future we will be.
00:40:55.764 - 00:40:58.830, Speaker D: Able to make hardware changes to our descendants.
00:40:59.890 - 00:41:17.154, Speaker A: I have this book called The Age of M work, Love, and Life from Robots Rule the Earth, and it's about brain emulations. And so this is where we make very human like creatures who are artificial using artificial hardware, but then they can modify themselves and become more alien more easily because they can more easily modify.
00:41:17.202 - 00:41:19.974, Speaker D: Their hardware and software as they are.
00:41:20.012 - 00:41:28.806, Speaker A: Basically computer simulations of human brains. So if that happens soon, then even that human line of descendants will be.
00:41:28.828 - 00:41:32.460, Speaker D: Able to become quite different in a relatively short time.
00:41:33.710 - 00:42:29.050, Speaker C: Ryan, if you thought the AI alignment problem would throw you for a tizzy, I can't wait until we get into the conversation about synthetic biology separating humans to some be gods and others not be gods. But that'll be a different podcast. Robin, I think in your argument here, you baked in the belief, the assumption that robots, these AIS, will adopt our values merely by, like, osmosis from the devs and the engineers who are coding them up, because they will code them up to do certain things and behave in certain ways using characters on our English or our keyboards, for example. And just merely by being association of being created by us, it's actually impossible to not imbue them with our culture and our values. Is that what you're saying?
00:42:29.200 - 00:42:31.238, Speaker D: Well, there's a big element of it.
00:42:31.344 - 00:42:33.680, Speaker A: How is it that you think your children are like you?
00:42:35.730 - 00:42:38.286, Speaker C: Well, mainly because they're biological cells, not.
00:42:38.308 - 00:42:42.426, Speaker D: Computers, though humans are really quite culturally plastic.
00:42:42.458 - 00:42:48.642, Speaker A: Maybe that's another thing people really don't quite get. So anthropology has gone out and looked at a really wide range of human.
00:42:48.696 - 00:42:52.062, Speaker D: Cultures and found that humans are capable.
00:42:52.126 - 00:43:00.230, Speaker A: Of behaving and thinking very differently depending on the culture they grew up in. That's the basic result of anthropology.
00:43:01.210 - 00:43:06.950, Speaker D: There are some rough human universals, but mostly we're talking variation.
00:43:07.930 - 00:43:15.034, Speaker A: The fact that you are seems very similar with all the other humans around you is not about sort of the innate human similarity you have.
00:43:15.072 - 00:43:17.580, Speaker D: It's because you are in a similar culture to them.
00:43:18.350 - 00:43:40.318, Speaker C: So to just rearticulate your position here, I think we are saying that Eliezer is perhaps fearful that the superintelligent AI and humans are so far apart that they can never come to coexist. And what you're saying is that life as a whole has similarities no matter how it manifests or how it is expressed.
00:43:40.414 - 00:43:45.266, Speaker D: Is that how you would say it? I was trying to tell you that.
00:43:45.288 - 00:43:50.726, Speaker A: Your descendants could be really different from you. I wasn't trying to convince you that there was a bound on just how.
00:43:50.748 - 00:43:52.214, Speaker D: Different your descendants could get.
00:43:52.332 - 00:43:59.254, Speaker A: I was trying to show you that in fact your descendants could get really different. Not through this fumsnar, just through the.
00:43:59.292 - 00:44:03.500, Speaker D: Simple default way that society continue to change.
00:44:04.430 - 00:44:21.406, Speaker A: If you're going to be scared about the fumesnar, maybe you should be scared about that one too. We could start to talk about what we might know in general about intelligent creatures and what might be the common features across them for all alien species through all of spacetime or something. There probably are some general things they.
00:44:21.428 - 00:44:23.694, Speaker D: Have in common, but they might be.
00:44:23.732 - 00:44:25.566, Speaker A: Fewer than would comfort you.
00:44:25.748 - 00:45:02.986, Speaker B: I definitely want us to get there, but really quick, just picking apart the assumptions that you laid out and I want to see which ones more specifically you might disagree with or state in a different way than said, you know, assumption one is that the AI improves itself. It seems core to what Eliezer thinks. Assumption two the owners, that is, the people who program it don't take control, don't try to stop it. Assumption three the AI becomes an agent. And assumption four the agent's goals change, the AI's goals change and it ends up destroying humanity. I find some of these harder to believe than others. Particularly assumption four.
00:45:02.986 - 00:45:18.402, Speaker B: I didn't understand in Eliser's argument the reason that suddenly the AI destroys humanity. That maybe we could talk about. But let's start at the top actually. Do you have a disagreement with assumption one that an AI will recursively start to improve itself?
00:45:18.536 - 00:45:25.634, Speaker A: Well, remember I tried to break a one into multiple parts to show you that it requires multiple things all to come together there.
00:45:25.672 - 00:45:27.446, Speaker D: So not only does it try to.
00:45:27.468 - 00:45:29.334, Speaker A: Improve itself, it finds this really big.
00:45:29.372 - 00:45:34.882, Speaker D: Lumpy improvement which has enormously unusual scale.
00:45:34.946 - 00:45:39.318, Speaker A: In terms of how far it goes before it runs out and scope in terms of how many things it allows.
00:45:39.334 - 00:45:43.050, Speaker D: The improvement of and magnitude is just.
00:45:43.120 - 00:45:45.258, Speaker A: A huge win over previous thing.
00:45:45.344 - 00:45:50.906, Speaker D: Those are all a prior unlikely things. So it's not the fact that it.
00:45:50.928 - 00:45:53.306, Speaker A: Tries to improve itself. That seems quite likely.
00:45:53.338 - 00:45:58.366, Speaker D: Sure, somebody might well ask a system to try to improve itself but then.
00:45:58.388 - 00:46:00.666, Speaker A: It would find such a powerful method.
00:46:00.778 - 00:46:10.382, Speaker D: And then still not be noticed by its owners. That gets pretty striking as an assumption.
00:46:10.446 - 00:46:23.510, Speaker B: I understand. And so that's where it's tied into. Like you find it hard to believe that the owners, the creators of this AI wouldn't be able to stop it from doing something nefarious or devious. That is also a difficult assumption.
00:46:24.010 - 00:46:26.934, Speaker D: Well, it's first just noticing that is.
00:46:27.052 - 00:46:32.602, Speaker A: By assumption this thing starts out at a modest level ability, right? By assumption this thing is comparable to.
00:46:32.656 - 00:46:34.410, Speaker D: Many, many other AIS in the world.
00:46:34.560 - 00:46:38.794, Speaker A: So by assumption, if you could notice a problem early on then you can.
00:46:38.832 - 00:46:42.102, Speaker D: Stop it because you can bring together.
00:46:42.176 - 00:46:45.054, Speaker A: Thousands of other AIS against this one.
00:46:45.252 - 00:46:51.118, Speaker D: To help you stop it if you want to stop it. So at some point later on in.
00:46:51.124 - 00:46:53.742, Speaker A: This evolution it may no longer be something you could stop.
00:46:53.796 - 00:46:56.798, Speaker D: But by assumption, that's not where this starts.
00:46:56.894 - 00:47:01.506, Speaker A: It starts at being comparable to other AI systems and then it has this.
00:47:01.528 - 00:47:04.050, Speaker D: One advantage it can improve itself better.
00:47:04.200 - 00:47:18.410, Speaker B: And then this other assumption than what I'd labeled number three, the AI becomes an agent. So how likely is an AI to become a self interested acting agent? Is that difficult to foresee?
00:47:19.390 - 00:47:21.514, Speaker A: Well, of course some owners might make.
00:47:21.552 - 00:47:26.890, Speaker D: It that way, but most won't.
00:47:27.230 - 00:47:30.622, Speaker A: So we're narrowing down the set here.
00:47:30.676 - 00:47:34.606, Speaker D: So my old friend Eric Drexler, for.
00:47:34.628 - 00:47:58.934, Speaker A: Example, has argued that we can have an advanced AI economy where most AIS have pretty narrow tasks. They aren't general agents trying to do everything, they drive cars to the airport or whatever. They each do a particular kinds of task. And that's in fact how our economy is structured. Our economy is full of industries made of firms, each of doomed who do particular tasks for us.
00:47:59.132 - 00:48:01.426, Speaker D: And so a world where those firms.
00:48:01.458 - 00:48:08.118, Speaker A: Are now much more capable and even artificially intelligent capable, even more than superhuman capable can still be a world where.
00:48:08.204 - 00:48:17.578, Speaker D: Each one does a pretty narrow task and therefore isn't a general agent. That would do enormous change things if.
00:48:17.584 - 00:48:22.718, Speaker A: It became more powerful. So if you had a system that was really good at route planning, say cars to get from A to B.
00:48:22.884 - 00:48:24.686, Speaker D: If it was superhuman at that, it.
00:48:24.708 - 00:48:26.394, Speaker A: Might just be really good at route planning.
00:48:26.442 - 00:48:28.160, Speaker D: But if that's all it does.
00:48:29.970 - 00:48:44.930, Speaker A: It'S not plausibly going to suddenly transition to an agent who sees itself as having history and whole goals for the world and trying to figure out how to preserve itself and make itself go. That's pretty implausible for a route planning AI.
00:48:45.430 - 00:48:48.302, Speaker D: So in a plausible future, most AIS.
00:48:48.366 - 00:49:00.394, Speaker A: Would be relatively narrow and have relatively tasks. But sometimes somebody might make more general AIS that had more general scope and ambitions and purposes and then those might be the basis of a scenario here.
00:49:00.432 - 00:49:03.158, Speaker D: But the people who created those AIS.
00:49:03.254 - 00:49:06.506, Speaker A: They would know it's unusual feature. They would know this one is an.
00:49:06.528 - 00:49:09.466, Speaker D: Agent, and they would presumably take that.
00:49:09.488 - 00:49:29.154, Speaker A: Into account in their monitoring and testing of this thing. They're not ignorant of this fact. So the scenario whereby the route planning one just accidentally becomes an agent. I mean, that's logically possible. But now we got to say, how often do design systems for purpose a suddenly transform themselves into something that does.
00:49:29.192 - 00:49:33.646, Speaker D: All different thing B, it happens sometimes, but it's pretty rare.
00:49:33.838 - 00:50:09.114, Speaker B: And so let's say it gets through all of these gates, right? We have an AI that improves itself in broad ways, in ways that are somewhat lumpy. The owners, for whatever reason, aren't able to take control. The AI tricked them in some way. Maybe the owners have programmed this AI to become an agent. So it's an agent acting in its free will. This last point then, Elizer's conclusion is like the point that was most concerning, of course, is that then this AI comes and destroys humanity. And I think his rationale is basically because why not? It would have other purposes for humanity.
00:50:09.114 - 00:50:12.910, Speaker B: It would just step over them. What about this assumption?
00:50:13.990 - 00:50:43.178, Speaker A: So imagine instead of one AI, we have a whole world of AIS who are improving themselves and their values are diverging. That's more of a default scenario. If that happens in a world of property rights, then, say, humans are displaced and no longer at the center of things. We're not in demand. We basically have to retire. Humans go off to our retirement corner and spend our retirement savings. If that stays as peaceful scenario, then.
00:50:43.264 - 00:50:46.346, Speaker D: All these AIS who change and have.
00:50:46.368 - 00:50:48.460, Speaker A: Other purposes, they don't have to kill us.
00:50:48.990 - 00:50:51.098, Speaker D: They can just ignore us off in.
00:50:51.104 - 00:50:58.314, Speaker A: The corner spending our retirement savings. But there's a possibility of a revolution, say, whereby they decide, hey, why let these people sit in the corner?
00:50:58.362 - 00:50:59.600, Speaker D: Let's grab their stuff.
00:51:01.730 - 00:51:05.698, Speaker A: The possibility of a violent revolution has always been there, and it's there in the future.
00:51:05.784 - 00:51:10.738, Speaker D: But in the world we're living in, that's a rare thing, and that's good.
00:51:10.904 - 00:51:31.802, Speaker A: And we understand roughly why it's rare. So the thing that's happening different in Elie Azer's scenario is because it's the one AI. You see, it's not in a society where revolutions are threatening. It's just the one power. And then from its point of view, why let these people have their property rights? Why not take it now?
00:51:31.936 - 00:51:33.322, Speaker D: I would say that the main thing.
00:51:33.376 - 00:51:36.170, Speaker A: There is not that it has different goals.
00:51:37.870 - 00:51:42.426, Speaker D: But that it's singular. And therefore not in a world where.
00:51:42.448 - 00:51:53.594, Speaker A: It needs to keep the peace with everybody else and be lawful for fear of offending others or the retribution that it can just go grab whatever it wants. That's the distinctive feature of the scenario he's describing.
00:51:53.722 - 00:51:56.178, Speaker D: In a more decentralized scenario, again, I.
00:51:56.184 - 00:52:11.558, Speaker A: Think there's much more hope that even if AIS displace us, even if their goals become different from us, they could still keep the peace. Because plausibly they could be relying on the same legal institutions to keep the peace with each other as they keep with us.
00:52:11.724 - 00:52:13.766, Speaker D: And that's in some sense, why we.
00:52:13.788 - 00:52:20.854, Speaker A: Don'T kill all the retirees in our world and take their stuff, right? Today there's all these people who are retired and like, what have they done for us lately?
00:52:20.902 - 00:52:21.306, Speaker D: Right?
00:52:21.408 - 00:52:36.462, Speaker A: We could all go kill the retirees and take their stuff, but we don't. Why don't we do that? Well, we share these institutions with the retirees, and if we did that, that would threaten these institutions that keep the peace between the rest of us. And we would each have to wonder who's next?
00:52:36.596 - 00:52:40.094, Speaker D: And this wouldn't end well, okay?
00:52:40.212 - 00:52:41.678, Speaker A: And that's why we don't kill all.
00:52:41.684 - 00:52:43.774, Speaker D: The retirees and take their stuff, not.
00:52:43.812 - 00:53:01.160, Speaker A: Because they're collectively powerful and can somehow resist our efforts to kill them. We could actually kill them and take their stuff that would actually physically work. That's not the problem with that scenario. The problem is what happens next after we kill them and takes their stuff? Who do we go for next? And where does it send?
00:53:02.010 - 00:53:05.702, Speaker D: So a future of AIS who become.
00:53:05.756 - 00:53:07.822, Speaker A: Different from us and acquire new goals.
00:53:07.906 - 00:53:20.394, Speaker D: And our agents threatens us if they have a revolution and kill us and take our stuff. That's the problem there. And so Eliezer's solution, you see, makes.
00:53:20.432 - 00:53:26.826, Speaker A: That seem more likely by saying, there's just the one agent. It has no internal coordination problems. It has no internal divisions.
00:53:26.938 - 00:53:30.606, Speaker D: It's just a singular thing. And honestly, we could add that as.
00:53:30.628 - 00:53:33.562, Speaker A: Another implicit assumption in this scenario.
00:53:33.706 - 00:53:35.662, Speaker D: He assumes that as this thing grows.
00:53:35.726 - 00:53:45.454, Speaker A: It has no internal conflicts. It becomes more powerful than the entire rest of the world put together, and yet there are no internal divisions of note.
00:53:45.502 - 00:53:48.418, Speaker D: There's no co thing to worry about, right?
00:53:48.504 - 00:54:00.762, Speaker A: It doesn't have different parts of itself that fight each other and that have to keep the peace with each other because that's why we have law and property rights, you see, in our world, is because we have conflicts and this is how we keep the peace with each other.
00:54:00.816 - 00:54:03.686, Speaker D: And he's setting that aside by assuming.
00:54:03.718 - 00:54:05.338, Speaker A: That it doesn't need to keep the.
00:54:05.344 - 00:54:09.082, Speaker D: Peace internally because it's a singular thing.
00:54:09.216 - 00:54:47.922, Speaker C: You know, Uniswap as the world's largest Dex, with over $1.4 trillion in trading volume, but it's so much more. Uniswap Labs builds products that lets you buy, sell, and use your self custody digital assets in a safe, simple, and secure way. Uniswap can never take control or misuse your funds the bankless way. With Uniswap, you can go directly to DFI and buy crypto with your card or bank account on the ethereum layer one or layer twos. You can also swap tokens at the best possible prices on Uniswap.org. And you can also find the lowest floor price and trade NFTs across more than seven different marketplaces with uniswap's NFT aggregator.
00:54:47.922 - 00:55:19.486, Speaker C: And coming soon, you'll be able to self custody your assets with Uniswap's new mobile wallet. So go bankless with one of the most trusted names in DeFi by going to Uniswap.org today to buy, sell, or swap tokens and NFTs. The phantom wallet is coming to Ethereum. The number one wallet on Solana is bringing its millions of users and beloved UX to Ethereum and Polygon. If you haven't used Phantom before, you've been missing out. Phantom was one of the first wallets to pioneer Solana staking inside the wallet and will be offering similar staking features for Ethereum and Polygon.
00:55:19.486 - 00:55:57.550, Speaker C: But that's just staking. Phantom is also the best home for your NFTs. Phantom has a complete set of features to optimize your NFT experience, pin your favorites, hide the Uglies, remove the spam, and also manage your NFT sale listings from inside the wallet. Phantom is, of course, a MultiChain wallet, but it makes chain management easy, displaying your transactions in a human readable format with automatic warnings for malicious transactions or phishing websites. Phantom has already saved over 20,000 users from getting scammed or hacked. So get on the Phantom Waitlist and be one of the first to access the MultiChain beta. There's a link in the show notes, or you can go to Phantom app Waitlist to get access in late February.
00:55:57.970 - 00:56:03.986, Speaker B: So we should really hope for a world of a pluralistic world of many AIS. And in fact, you think that's a.
00:56:04.008 - 00:56:07.762, Speaker D: More likely world anyway, of course, yes.
00:56:07.896 - 00:56:17.526, Speaker A: So we're already in a world of great many autonomous parts, right? We have not only billions of humans, but we have millions of organizations and.
00:56:17.548 - 00:56:22.834, Speaker D: Firms and even nations and government agencies.
00:56:22.882 - 00:56:57.998, Speaker A: And one of the most striking features of our world is how it's hard to coordinate among all these differing interests and organizations. And one of the most striking features of our world are the mechanisms we use to keep that peace and to coordinate among all these divergent, conflicting things. And one of the moves that often AI people make to spin scenarios is just to assume that AIS have none of that problem. AIS do not need to coordinate. They do not have conflicts between them, they do not have internal conflicts, they do not have any issues in how to organize and how to keep the peace between them. None of that's a problem for AIS. By assumption.
00:56:57.998 - 00:57:02.306, Speaker A: They're just these other thing that has no such problems. And then, of course, that leads to.
00:57:02.328 - 00:57:04.802, Speaker D: Scenarios like then they kill us all. Right?
00:57:04.856 - 00:57:36.030, Speaker C: Like AIS are a monolith. But I think one of the reasons why I appreciate just your line of reasoning, Robin, and how you think, is that you tap into what seems to be fundamental truth of this universe that you would find here on planet Earth or in a galaxy far, far away. Certain things, I think can be assumed no matter what the environment is. And then I think a lot of your logical conclusions are just like natural extensions of to, unless you have a thought on that.
00:57:36.180 - 00:57:38.286, Speaker D: I was just going to say I.
00:57:38.308 - 00:58:03.862, Speaker A: Think a lot of disagreements in the world are often based on people having sort of different sets of abstractions and mental tools and then finding it hard to merge them across topics. So I think when a community has a shared set of abstractions and mental tools, even when they disagree about details, they can use those shared abstractions to come to an agreement. But when you have people with just different sets of abstractions, that's true. So I'm bringing a lot of economics to this.
00:58:03.996 - 00:58:07.522, Speaker D: Other people might be bringing a lot of computer science, but I'm going to.
00:58:07.596 - 00:58:16.486, Speaker A: Play my polymath card and say I've spent a lifetime learning a lot of different sets of conceptual tools and intellectual.
00:58:16.518 - 00:58:20.634, Speaker D: Systems, including computer science, certainly big chunks of.
00:58:20.752 - 00:58:30.846, Speaker A: And so I'm trying to integrate all those tools into an overall perspective where I can sort of pull in each observation or insight into this.
00:58:30.948 - 00:58:38.450, Speaker B: So is this the economic reason that the robots aren't going to come kill us then? Maybe. Is that what you're kind of providing?
00:58:39.510 - 00:58:41.266, Speaker D: Or just if they kill us, they.
00:58:41.288 - 00:58:42.766, Speaker A: Would do us in the usual economic.
00:58:42.798 - 00:58:43.380, Speaker D: Way.
00:58:46.310 - 00:59:03.770, Speaker A: That'S pretty assure you that nobody will ever kill you. Okay. They have to have good reasons been killed in the world in the past, but we have an understanding of the main ways that in the last few centuries people have been killed. That's been something people have paid attention to. How do people get killed? How does that happen?
00:59:03.920 - 00:59:07.786, Speaker D: And so theft is like murder is.
00:59:07.808 - 00:59:11.662, Speaker A: One kind of way people get killed, war is another way, revolution is another.
00:59:11.716 - 00:59:17.966, Speaker D: Way, or sometimes just displacement where something outcompetes you and then you don't have.
00:59:17.988 - 00:59:21.342, Speaker A: Any place to survive. So in some sense, like, horses got.
00:59:21.396 - 00:59:24.094, Speaker D: Outcompeted by cars at some point and.
00:59:24.132 - 00:59:27.586, Speaker A: They suffered substantially, and we understand how that works out.
00:59:27.768 - 00:59:29.106, Speaker D: So that's the sort of thing that.
00:59:29.128 - 00:59:31.682, Speaker A: Can happen to humans. We could suffer like the way horses did.
00:59:31.816 - 00:59:32.900, Speaker D: That's interesting.
00:59:33.670 - 00:59:38.174, Speaker B: Did horses suffer, though, by population standards?
00:59:38.222 - 00:59:45.314, Speaker C: Yes, they diminished significantly. Did any individual horse suffer and feel suffering as a result of cars?
00:59:45.362 - 00:59:52.998, Speaker B: Seems like a good life to be on an equestrian farm rather than sort of slaving in a cityscape being whipped by a bunny master.
00:59:53.174 - 00:59:57.980, Speaker A: My understanding is horse population is now as high as it ever was.
00:59:58.350 - 01:00:01.178, Speaker D: But of course this is not a.
01:00:01.184 - 01:00:08.446, Speaker A: Fact, not as high as you might have projected had they continued previous growth rates. So there was a substantial decline and then revisit, but now most horses are.
01:00:08.468 - 01:00:11.582, Speaker D: Pets and not workhorses right.
01:00:11.716 - 01:00:19.758, Speaker C: I'm not sure if I'm ready to be a pet, but that's a problem for my kids, probably. Hopefully just quick scenario, Robin. What's more likely?
01:00:19.934 - 01:00:20.322, Speaker D: A.
01:00:20.376 - 01:00:41.814, Speaker C: Single monolithic, super intelligent AI does the leaser thing. Or we have humans have a robot human conflict war. And it's more like kind of maybe in the traditional sense where we have two sides and what's more likely.
01:00:41.862 - 01:00:43.354, Speaker A: So that second one seems far more.
01:00:43.392 - 01:00:46.186, Speaker D: Likely to me but you should just.
01:00:46.208 - 01:00:47.174, Speaker A: Put it into the context.
01:00:47.222 - 01:00:51.486, Speaker D: That is, humans are at the moment.
01:00:51.588 - 01:00:59.470, Speaker A: Vary by an enormous number of parameters. We vary by gender and age and profession and geographic location and wealth and personality.
01:01:00.130 - 01:01:04.226, Speaker D: And in politics especially we try to.
01:01:04.248 - 01:01:16.390, Speaker A: Divide ourselves up and form teams and coalitions by which together we will then oppose other coalitions. And this is just an ancient human behavior of which we form coalitions and fight each other.
01:01:16.540 - 01:01:18.758, Speaker D: And we expect that will continue.
01:01:18.844 - 01:01:32.986, Speaker A: So arguably, say democracy has allowed us to have more peaceful conflicts where coalitions fight in elections rather than in wars. But even in, say, firms there's often political coalitions that are fighting each other.
01:01:33.168 - 01:01:36.106, Speaker D: And there's always the question what is.
01:01:36.128 - 01:01:52.626, Speaker A: The basis of the dominant coalitions? So there's this wide range of possibilities. You could have a gender based the men fighting the women. You could have an age one. The old people fighting the young. You could have an ethnicity one. You could have a professional one. So in a firm it might be.
01:01:52.648 - 01:01:59.006, Speaker D: The engineers versus the marketers, right? And so humans versus robots or robotic.
01:01:59.038 - 01:02:01.634, Speaker A: Descendants is one possible division on which.
01:02:01.672 - 01:02:06.946, Speaker D: Future conflicts could be based. That's completely believable and I can't tell.
01:02:06.968 - 01:02:12.710, Speaker A: You that can't happen. The main thing I'll just point out that will be competing with all these other divisions.
01:02:13.610 - 01:02:15.506, Speaker D: So will it be the humans versus.
01:02:15.538 - 01:02:36.846, Speaker A: Robots conflict or will it be the old versus young or will it be the word cells versus the shape rotators? There's all these different divisions. And it could well be that there's an alliance of human word cells and AI word cells versus human shape rotators and AI shape rotators. And that becomes the future conflict, you see.
01:02:36.868 - 01:02:40.346, Speaker D: Because in some fundamental sense the division.
01:02:40.378 - 01:02:59.160, Speaker A: Of the conflict is indeterminate. That is a fundamental thing we understand about politics is whatever divisions you have it's unstable to the possibility of some new coalition forming instead. That's a basic thing we understand about politics. It's hard to keep stable coalitions because they're so easily undermined by new ones.
01:03:01.130 - 01:04:14.922, Speaker C: Well, with the human versus robot coalition like looking into past human behavior we tend to be pretty racist. But I think when we have robots it would be really easy to forget our internal conflicts when there's a completely different resource like why do we fight? Why do humans fight? It's usually over resources, like economic resources. And when there is a new species that is subdividing and iterating and growing as humans do that's also sucking up the resources. And they look like I don't know if they're going to be metal in the future but that's my current vision. Of them is like metal silicon terminator type robots walking around and there's only so many resources on the planet. And so that would be a pretty easy dividing line between humans and robots that I could imagine would make that conflict much more likely. And so regardless of how maybe it's the leaser way in which a super monolithic, super intelligent robot comes and we have to fight that, or at some point there's conflict, potentially, and I might even say likely, if there is a different call this species this is kind of aside.
01:04:14.922 - 01:04:37.506, Speaker C: This is going back to the super intelligence stuff, but I think we can now call this just AI conflict. The Future of Life organization released an open letter calling for the pause of all general AI experiments. A few people signed it. Elon Musk, Steve Wozniak. You've all noah Harari. Andrew Yang. I just want to get your reaction to this letter and people signing it.
01:04:37.506 - 01:04:44.786, Speaker C: Robin like, would you sign this letter or are you against signing this letter? And just what do you think about the idea of this letter, David?
01:04:44.818 - 01:05:10.400, Speaker B: Can we tell people what it says, too? It's basically a call on all AI labs to immediately pause for at least six months. The training of AI systems more powerful than Chat GPT Four. So this letter says, don't go beyond Chat GPT four. Beyond there it gets even scarier. Let's pause, let's halt. Let's figure out this AI alignment issue first.
01:05:12.290 - 01:05:15.694, Speaker D: So first, let's notice that we've had.
01:05:15.732 - 01:05:25.758, Speaker A: A lot of pretty impressive AI for a while now. It's when the AIS are the most human like with these large language models that people are the most scared and concerned.
01:05:25.934 - 01:05:32.646, Speaker D: So that suggests that maybe a very advanced AI will look pretty human like in many ways.
01:05:32.748 - 01:05:45.194, Speaker A: And don't forget that our descendants will start to add metal to themselves and become different in many, just like their brain emulations are metal and quite different. So, again, it's not so obvious where the division line would be.
01:05:45.232 - 01:05:55.878, Speaker D: But to go to this particular letter, first of all, with respect to the general concerns they have, if we had a six months pause at the end.
01:05:55.904 - 01:05:58.574, Speaker A: Of that, we really wouldn't know much more than we know now.
01:05:58.772 - 01:06:00.366, Speaker D: The main purpose of the pause would.
01:06:00.388 - 01:06:16.130, Speaker A: Seem to be to allow, say, the time for government to get its act together and have institute some more official law that enforces such a pause to continue. That would be the main purpose for the pause. You'd be wanting to support the pause if you were wanting that further event to happen.
01:06:16.280 - 01:06:17.442, Speaker D: It's not like we're going to learn.
01:06:17.496 - 01:06:18.390, Speaker A: That much in six months.
01:06:18.460 - 01:06:21.560, Speaker B: Or how about if maybe you were a competitor and wanted to catch up.
01:06:25.210 - 01:06:36.860, Speaker A: First? If we could do the pause, would it be a good idea? And then one of the issues is like, who would be participating and who not? So the ideal thing is, say, we could get a global pause somehow, would that be a good idea?
01:06:37.310 - 01:06:40.006, Speaker D: Now we're basically talking about should we.
01:06:40.048 - 01:06:42.654, Speaker A: Basically shut down this technology for a.
01:06:42.692 - 01:06:50.206, Speaker D: Long time until people feel safer about it. So for that issue, I think the.
01:06:50.228 - 01:07:28.602, Speaker A: Comparison with nuclear energy is quite striking. Basically around 1970, the world decided to back off a nuclear energy and we basically instituted regulatory regimes that allowed the safety requirements asked of nuclear power plants to escalate arbitrarily until they started to cut into costs. And that basically guaranteed this would never become lower cost than our other ways of doing things. And people were okay with that because they were just scared about nuclear power. So basically the generic fear didn't go away and they just generically said this just could never be safe enough for us. Whatever extra budget we have, we want it to be safer. And that's the way they put it.
01:07:28.602 - 01:08:07.800, Speaker A: And so I would think a similar thing would happen with AI. The kind of reassurances people are asking for are just not going to be feasible for decades at least. So you'd basically be asking for this to be paused for decades. And it's even hard to imagine eventually overcoming that because the fundamental fears as we've been describing is just the idea that they might be different and they might have different agendas and they might outcompete us. And that's just not going to go away. So I would see this as basically, do you vote for substantial technological change or not? And I get why many people might think, look, we're rich enough, we're doing okay, let's not risk the future by changing stuff.
01:08:08.170 - 01:08:09.654, Speaker D: And they voted that way on nuclear.
01:08:09.702 - 01:08:19.046, Speaker A: Power and they might well vote that way on AI. I think we have enormous far we can go if we continue to improve.
01:08:19.078 - 01:08:22.954, Speaker D: Our tech and grow. But I can understand why many people.
01:08:22.992 - 01:08:29.838, Speaker A: Think, no, we got lucky so far, things didn't go too bad, we're in a pretty nice place, why take a chance and change anything?
01:08:29.924 - 01:08:31.230, Speaker D: So that's all.
01:08:31.300 - 01:08:34.958, Speaker A: If it was possible to actually have a global enforcement of such a pause.
01:08:34.974 - 01:08:36.706, Speaker D: And then a further law, but of.
01:08:36.728 - 01:08:46.270, Speaker A: Course that just looks really hard. This technology is now pretty widely available. It might be that the best new systems are from the biggest companies that.
01:08:46.280 - 01:08:48.806, Speaker D: Can afford the most hardware to put on it.
01:08:48.828 - 01:08:53.800, Speaker A: But the basic software technology here is actually pretty public and pretty widely available.
01:08:54.250 - 01:08:57.490, Speaker D: And so over the next few decades.
01:08:57.570 - 01:09:04.700, Speaker A: Even if you manage to say no more than a billion dollar project doing this, you're going to have lots of less than billion dollar projects doing this.
01:09:05.790 - 01:09:10.486, Speaker D: And of course it'll be hard to have a global ban.
01:09:10.598 - 01:09:12.278, Speaker A: And so the US now has a.
01:09:12.304 - 01:09:15.326, Speaker D: Commanding lead and the main effect of.
01:09:15.348 - 01:09:16.766, Speaker A: A delay, if it's not global, would.
01:09:16.788 - 01:09:20.106, Speaker D: Be to take away the and it's.
01:09:20.138 - 01:09:27.074, Speaker A: Just this looks like a hard technology to ban. Honestly, you might be able to get Google and OpenAI and Microsoft or something.
01:09:27.112 - 01:09:31.154, Speaker D: To pause their efforts because they are.
01:09:31.192 - 01:09:33.650, Speaker A: Big companies with pity public activities.
01:09:35.590 - 01:09:51.740, Speaker B: And Robin, I'm trying to understand so even if it was enforceable, understand the reasons you gave, why it's not enforceable and very difficult to do some sort of global ban of some sort, let's say it was for a minute, would you support it? Do you think this is worth pulling the fire alarm over?
01:09:53.470 - 01:10:06.480, Speaker A: But again, I think it's comparable to, say, genetic engineering or nuclear energy or some other large technologies that we've come across in the last few decades where there really is huge potential, but there's also really big things you could be worried about.
01:10:07.010 - 01:10:12.042, Speaker D: And honestly, I think you just have to make a judgment on the overall.
01:10:12.106 - 01:10:18.638, Speaker A: Promise versus risk framing. You can't really make a judgment here based on very particular things, because that's.
01:10:18.654 - 01:10:19.860, Speaker D: Not what this is about.
01:10:20.230 - 01:10:24.340, Speaker A: We made a judgment on nuclear energy to just back off and not use it that much.
01:10:24.710 - 01:10:27.400, Speaker D: That's a judgment humanity made 50 years ago.
01:10:28.410 - 01:10:33.160, Speaker A: Within the last few decades, we made a similar judgment on nuclear genetic engineering, basically.
01:10:33.690 - 01:10:34.886, Speaker D: No, we just don't want to go.
01:10:34.908 - 01:10:36.920, Speaker A: There, for humans at least.
01:10:37.930 - 01:10:45.306, Speaker D: And we may be about to make a similar decision about AI. But honestly, this trend looks bad to.
01:10:45.328 - 01:10:52.846, Speaker A: Me because many people think social media is a mistake, and maybe we should undo that and go back on that.
01:10:52.948 - 01:11:00.302, Speaker B: So the trend of blocking technological progress is bad to you in general, whether it's nuclear or genetic engineering or social.
01:11:00.356 - 01:11:07.074, Speaker D: Media or AI or any of these things, right. I actually am concerned that this is.
01:11:07.112 - 01:11:09.620, Speaker A: The future of humanity actually here.
01:11:10.070 - 01:11:13.330, Speaker D: So I did this other work on.
01:11:13.400 - 01:11:24.246, Speaker A: Grabby aliens, on sort of the distributions of aliens of spacetime. And in that framework, the most fundamental distinctions between alien civilizations is the one between the quiet ones who stay in.
01:11:24.268 - 01:11:29.398, Speaker D: One place and live out their history and go away without making much of.
01:11:29.404 - 01:11:33.222, Speaker A: A mark on the universe, and loud ones who expand and then keep expanding.
01:11:33.286 - 01:11:38.906, Speaker D: Until they meet other loud ones. And I can see many forces that.
01:11:38.928 - 01:11:43.374, Speaker A: Would tend to make a civilization want to be quiet. And that's what we're talking about here.
01:11:43.492 - 01:11:45.694, Speaker D: That is even in the last half.
01:11:45.732 - 01:12:13.094, Speaker A: Century, the world has become a larger integrated community, especially among elites, whereby regulatory policy around the world has converged a lot, even though we have no world government. You certainly saw that in COVID, but you also see it in nuclear energy and medical ethics and many other areas. Basically, the elites around the world in each area talk mainly to each other. They form a consensus worldwide about what the right way to deal with that area is, and then they all implement that. And so there's not actually that much.
01:12:13.132 - 01:12:19.494, Speaker D: Global variation in policy in a wide range of areas and people like that.
01:12:19.532 - 01:12:28.746, Speaker A: I think, compared to the old world, certainly it's reduced civil wars of various kinds. And people like the idea that instead of nations fighting and competing with each.
01:12:28.768 - 01:12:31.786, Speaker D: Other, that we're all talking together and.
01:12:31.808 - 01:12:33.082, Speaker A: Deciding what to do together.
01:12:33.216 - 01:12:35.614, Speaker D: And that that sort of talking may.
01:12:35.732 - 01:12:46.318, Speaker A: Deal with global warming, it may deal with inequality, it may deal with overfishing. There's just a bunch of world problems that these people talking together feel like they're solving. And people will like this world we're.
01:12:46.334 - 01:12:49.682, Speaker D: Moving into where we all talk together.
01:12:49.736 - 01:12:52.500, Speaker A: And agree together about what to do about most big problems.
01:12:53.110 - 01:12:57.074, Speaker D: And that new world will just be much more regulated in the sense that.
01:12:57.112 - 01:13:02.390, Speaker A: They will look at something like Goopy energy and then everybody say, Nope, we don't want to do that, and let's shame anybody who tries to do that.
01:13:02.460 - 01:13:10.214, Speaker D: And slowly together limit humanity's future. And that could go on for thousands of years.
01:13:10.252 - 01:13:19.130, Speaker A: And then if we ever have a point where it was possible to send out an interstellar colony to some other star, we will know that if we allow that, that's the end of this era.
01:13:19.550 - 01:13:23.386, Speaker D: Once you have a colonist, go somewhere else, then they are out of your control.
01:13:23.488 - 01:13:30.714, Speaker A: They are no longer part of your governance sphere. They can make choices that disagree with what you've done. They can then have descendants who disagree.
01:13:30.842 - 01:13:33.714, Speaker D: They can evolve and become different from.
01:13:33.752 - 01:13:59.340, Speaker A: The center and come back eventually to contest control over the center. So that becomes a future world of competition and evolution that could bego very strange and stark places. But if we would all just stay here and not let anyone leave, then we can stay in this world of us. We talk together, we decide things together. We only allow our descendants to become as weird as we want them to be. If we don't want a certain kind of weird descendants, we just shut it down.
01:13:59.710 - 01:14:03.420, Speaker D: And that's the quiet civilization that we may become.
01:14:04.190 - 01:14:06.134, Speaker A: And that's kind of what's at stake.
01:14:06.182 - 01:14:09.810, Speaker D: Here, I would say, with banning AI.
01:14:09.990 - 01:14:12.222, Speaker A: It'S one of many questions like that.
01:14:12.276 - 01:14:24.930, Speaker D: That we are answering about do we want to allow change and new large capacities that might threaten strangeness and conflict.
01:14:25.350 - 01:15:26.486, Speaker C: So I think this is actually the moment where this podcast episode goes from continuing the conversation that we had about AI with Leaser and all of those alignment problems in that conversation. And this actually becomes a part of a larger conversation that we've been having on Bankless for a while now. And this has to do with the status quo versus innovation and progress as well as it does with what you were just saying, Robin, about grabby aliens. And so I want to try and connect these dots really quick. This idea of AI and AI innovation, along with crypto innovation and whether or not it should be regulated by the elites, by the status quo, and whether it should be contained and are the elites happy with the way of the harmony of the social order. And perhaps we shouldn't have new competition and new exploration into the frontier because that is how we maintain the social order, because there's nothing new that's happened. What you're saying that this does is this keeps us in.
01:15:26.486 - 01:16:37.286, Speaker C: It's like an isolationist approach, except it's an isolation approach from inside of planet Earth. And I think being the future tech optimist that Ryan I are, and I think you are as well, you aren't for that you would like to penetrate that isolationism that has from the social elite saying, hey, let's not experiment with crypto or AI or longevity or synthetic biology research. Let's just keep everything harmonized and in control, and we will use our large centralized power to keep the world under control. And then we have this other conversation that we're about to go into, which is grabby aliens, which is whoever is these alien species that is expanding out into the world chose to not do that. They chose to explore the frontier. They chose to innovate under the guise of competition, of capitalistic competitive competition to innovate and start to expand outwards into space. And I think baked in your argument is that you actually do need competition in order to become to explore the frontier.
01:16:37.286 - 01:17:01.890, Speaker C: And so I'm wondering if that was a good summary and B kind of like do you see that picture of just like how this concern about AI or concern about progress in general is also linked to the grabbiness or quietness that you call in aliens? And maybe you can characterize these different kinds of aliens and the choices that they make of it as a civilization.
01:17:02.950 - 01:17:05.330, Speaker D: Yes, I thought that was a reasonable summary.
01:17:06.010 - 01:17:11.986, Speaker A: I think when we see people today discuss the possibility of our descendants spreading.
01:17:12.018 - 01:17:25.722, Speaker D: Into the galaxy, they are often wary and a bit horrified by the impact it might have. That is the sort of people we've become over the last century are people.
01:17:25.776 - 01:17:28.778, Speaker A: Who find that a jarring and even.
01:17:28.864 - 01:17:50.014, Speaker D: Unpleasant scenario because it is actually fundamentally jarring and unpleasant. So I am with you in wanting to allow such changes, but I want to be fully honest about the costs that we are asking the world to accept. That is, if you wanted our descendants.
01:17:50.062 - 01:17:57.960, Speaker A: To just stay pretty much like us indefinitely, that's not what we're talking about here.
01:17:59.050 - 01:18:02.354, Speaker D: If the cost of allowing our descendants.
01:18:02.402 - 01:18:07.298, Speaker A: To expand into the universe and explore technologies like AI and nuclear power, et.
01:18:07.314 - 01:18:11.866, Speaker D: Cetera, is literally alienation, that is, we.
01:18:11.888 - 01:18:17.402, Speaker A: Are now alienated from our ancestors. Our world and lives are different. And we feel that at some level.
01:18:17.456 - 01:18:22.326, Speaker D: That we were not built for the world we're in. This is an alien world that we're.
01:18:22.358 - 01:18:26.400, Speaker A: In compared to the world we were built for. And we feel that deep inside us.
01:18:26.770 - 01:18:31.646, Speaker D: And that will continue. It will only get worse and the.
01:18:31.668 - 01:18:42.180, Speaker A: Time it'll get better is when we can go change who's inside us to become more compatible with these alien worlds, but that will make those descendants even more different from us.
01:18:42.950 - 01:18:46.950, Speaker D: So that's really the cost you have to be asking.
01:18:47.020 - 01:19:10.650, Speaker A: So this future world of strange new technologies is also a competitive world. And that competitive world includes conflict. It includes some people display some kinds of things, displacing others, some things just being shunted aside and marginalized. And it may even include war, violence. It certainly probably includes radical change to nature.
01:19:11.330 - 01:19:13.886, Speaker D: Not just biology on Earth, but our.
01:19:13.908 - 01:19:26.558, Speaker A: Descendants who go out into the universe would likely not just pass by and plant flags. They will take things apart and rearrange them and radically change them. And sometimes that'll be ugly, sometimes it'll.
01:19:26.574 - 01:19:34.866, Speaker D: Be violent, and sometimes it'll leave crude, ugly waste and be inefficient where they.
01:19:34.888 - 01:19:36.870, Speaker A: Could have done it better. That will be the course.
01:19:36.940 - 01:19:52.940, Speaker D: And this universe we see now that's pristine and the way it was from long ago will just be erased. That's the cost.
01:19:54.190 - 01:21:11.922, Speaker C: So I want to explore this idea of Grabby aliens, and I'm sure listeners who are being thrown into this OD adjective, Grabby, might be a little bit confused. And so I'm hoping we can explain the nature of grabbiness, but I'm hoping we can actually do it inside of the context of planet Earth and human history, because I think that naturally extrapolates into the galaxy because this is where the place of Grabby aliens, that's where they play. And first I think I want to ask you the question, humans, are we Grabby? Because if you look back in history, you have some sort of quiet human races, quiet human species, human tribes that found, that were found by the Grabby humans. You can call these the conquistadors or the conquerors, right? The Roman Empire, very Grabby Empire, any sort of empire that looked outward and expanded. Trying to understand Robin Hansen's works of grabbiness, I would call any sort of empire that expanded Grabby. And then these Grabby empires found the quiet tribes that were peaceful and grabbed them and then assimilated them into the grabbiness. And so this is kind of how I would ask.
01:21:11.922 - 01:21:22.390, Speaker C: I would present this inside of a context that we understand because we understand human history. But I want to ask you this very basic question of just like human nature, are we Grabby?
01:21:24.010 - 01:21:27.058, Speaker A: So almost all biology has been Grabby.
01:21:27.154 - 01:21:30.078, Speaker D: And therefore almost all humans, but it's.
01:21:30.114 - 01:21:36.394, Speaker A: Not so much about our nature. So the fundamental point here is there's just a selection effect.
01:21:36.592 - 01:21:37.818, Speaker D: That's the key point.
01:21:37.904 - 01:21:44.686, Speaker A: That is, if you have a range of creatures with different cultures or biological tendencies and some of them go out.
01:21:44.708 - 01:21:47.118, Speaker D: And expand and others don't, if there.
01:21:47.124 - 01:21:48.798, Speaker A: Is a place they could expand to.
01:21:48.884 - 01:21:52.286, Speaker D: And they would actually could reproduce there.
01:21:52.388 - 01:21:55.010, Speaker A: Then there's a selection effect by whichever ones do that.
01:21:55.080 - 01:21:58.370, Speaker D: They then come to dominate the larger picture.
01:22:00.230 - 01:22:08.242, Speaker A: That's just the key selection effect. So there may be many alien species and civilizations in the universe, and maybe most of them choose not to expand.
01:22:08.386 - 01:22:10.534, Speaker D: But the few ones who do allow.
01:22:10.572 - 01:22:18.522, Speaker A: Expansion, they will come to dominate by spacetime volume, the activity of the universe. And that's how evolution has worked in the past.
01:22:18.576 - 01:22:20.906, Speaker D: It's not that all animals or all.
01:22:20.928 - 01:22:24.330, Speaker A: Plants are aggressive and violent and hostile.
01:22:24.670 - 01:22:26.570, Speaker D: It's that they vary.
01:22:27.870 - 01:22:38.186, Speaker A: And some of them have a habit of sticking one place and hiding another, have a habit of jumping out and going somewhere else when they can. And the net effect of the variation in all their habits is when there's.
01:22:38.218 - 01:22:40.606, Speaker D: A new island that pops up, it.
01:22:40.628 - 01:22:43.294, Speaker A: Gets full of life. Because some of those things that move.
01:22:43.332 - 01:22:47.842, Speaker D: Land there and grow, and any new mountain grows higher.
01:22:47.896 - 01:22:49.378, Speaker A: And then new life shows up at.
01:22:49.384 - 01:22:52.466, Speaker D: The top of the mountain and a.
01:22:52.488 - 01:23:07.160, Speaker A: New niche opens up of any sort where life is possible there. And then some live goes there and uses it. That's just the selection effect. So that's what we should expect in the universe. There's the question of which way we will go.
01:23:07.770 - 01:23:09.778, Speaker D: And if I focused on humans, I'd.
01:23:09.794 - 01:23:14.010, Speaker A: Say it's a trade off between what would happen if we don't coordinate and.
01:23:14.080 - 01:23:16.102, Speaker D: How hard we will try to coordinate.
01:23:16.246 - 01:23:23.050, Speaker A: So an uncoordinated humanity, there's certainly enough variation within humanity. Some of us would go be grabby.
01:23:23.790 - 01:23:25.054, Speaker D: It might not be most of us.
01:23:25.092 - 01:23:42.340, Speaker A: But certainly some of us, given the opportunity, would go grab mercury or pluto or whatever else it is and then go out and grab farther things. We might choose to prevent that. We might choose to organize and coordinate so as to not allow those things to happen. And we might succeed at that.
01:23:42.870 - 01:23:45.540, Speaker D: We have enough capability perhaps to do that.
01:23:46.310 - 01:23:49.430, Speaker A: And so then it becomes a choice will we allow it?
01:23:49.500 - 01:23:52.550, Speaker D: But basically, whenever you're talking about something.
01:23:52.620 - 01:24:02.300, Speaker A: That it only takes a small fraction of us to do, and we vary a lot, then the question is, will we allow that variation to make it happen or will we somehow try to lock it down?
01:24:03.870 - 01:24:13.050, Speaker C: The Bankless audience is pretty familiar with the idea of Moloch. It's a topic that we've revisited a number of times. Are you familiar with Moloch?
01:24:13.210 - 01:24:16.014, Speaker A: I'm familiar with the famous Scott Alexander essay on it.
01:24:16.052 - 01:24:19.514, Speaker D: And although I think the concept isn't.
01:24:19.562 - 01:24:21.258, Speaker A: Entirely clear in that essay.
01:24:21.434 - 01:24:22.814, Speaker D: Sure. Yeah.
01:24:22.852 - 01:25:09.562, Speaker C: So Moloch just being like the idea of the Prisoners dilemma. Say you have two or almost any number of human tribes on the Earth, and most of them decide to be quiet and peaceful. It really only takes one to be grabby, and that one will come to dominate the Earth because it chose to be grabby and it grabbed everything else. So it's almost this prisoner's dilemma about if you choose to not be grabby, you are implicitly making the choice of being grabbed by the larger tribe that has elected to be Grabby. And I think this is how we extrapolate this into the future. With your grabby aliens thesis where sure, there are many civilizations out there. Maybe there are many like us that only exist on one planet.
01:25:09.562 - 01:25:56.842, Speaker C: And we have a bunch of elites on the planet that say, hey, let's not investigate AI, and let's not investigate longevity or genetic engineering. Let's just stay put. And we would call these quiet aliens or us being the quiet aliens. The choice being made is that gravy aliens are eventually going to arrive on Earth and grab us. And so if you don't become a gravy alien, you are going to be grabbed by somebody else. And so this is why I think this moment in human history when we have this letter saying, hey, let's pause AI. Research is what you are focusing on as like, well, this is a very important decision point for humanity as to whether we choose to be quiet or not quiet.
01:25:56.842 - 01:26:08.946, Speaker C: And of course this isn't the only choice, but this is one of the many choices down a long list of choices that could actually decide culturally what we want to be in, at least for the short term. Is this how you see this fork in the road as we currently are?
01:26:09.048 - 01:26:13.506, Speaker A: Well, let's just clarify, say in a peaceful society like ours, we could think.
01:26:13.528 - 01:26:17.106, Speaker D: Of a thief as Grabby and then.
01:26:17.128 - 01:26:35.894, Speaker A: We could say, well, if we don't steal, somebody else will steal, so I guess we should steal. And you could imagine a world where that was the equilibrium. But if we coordinate to make law, then we can coordinate to watch for some a thief and then repress them sufficiently so as to discourage people from being thief.
01:26:35.942 - 01:26:40.406, Speaker D: So a universe of sufficiently powerful aliens.
01:26:40.438 - 01:26:44.240, Speaker A: Could coordinate to prevent grabbing if they wanted.
01:26:45.330 - 01:26:46.974, Speaker D: The claim, which I believe is true.
01:26:47.012 - 01:26:53.274, Speaker A: That in fact the universe hasn't done that, it might be that within our human society we have coordinated to enforce.
01:26:53.322 - 01:26:55.998, Speaker D: Particular laws, but out there in the.
01:26:56.004 - 01:27:11.814, Speaker A: Rest of the universe, it's just empty and there's pretty much nobody doing anything through most of it that we can see. And so it is really just there for the grabbing. No one's going to slap our hands down for Grabing the stuff. We can just keep grabbing until we reach the other Grabby aliens, at which.
01:27:11.852 - 01:27:14.054, Speaker D: Point then we might try to set.
01:27:14.092 - 01:27:18.374, Speaker A: Up some peaceful law to keep the peace between us and them.
01:27:18.412 - 01:27:21.206, Speaker D: But we don't have to fight wars.
01:27:21.238 - 01:27:25.738, Speaker A: With other Grabby aliens per se. But there's all this empty stuff between here and there.
01:27:25.904 - 01:27:27.686, Speaker D: Then it seems like you either grab.
01:27:27.718 - 01:27:29.020, Speaker A: It or somebody else does.
01:27:29.710 - 01:28:18.460, Speaker B: I'm wondering if we may have blown past some listeners here who heard us just talking about alien civilizations and they're coming to Grab Earth and they're like, what are you guys talking like all of these alien civilizations? Robin, David, we don't see them anywhere when we look up the stars. But that is what your Grabby Aliens paper is all about. I think the synopsis of the Grabby Aliens paper packs this punch. If loud aliens explain human earliness, quiet. Aliens are also rare. Robin, can you sort of explain what your Grabby Aliens idea actually is and why there might be future alien civilizations that are expansionary and coming our way and why we might want to be a civilization that rises up and expands in our own sphere of influence in order to meet them?
01:28:20.290 - 01:28:23.390, Speaker A: So we're going to go through this briefly and quickly.
01:28:23.540 - 01:28:28.878, Speaker D: Turns out there's just a Kurzgas video.
01:28:28.964 - 01:28:37.406, Speaker A: That came out yesterday that has 2.6 million views that's explaining some of the basics of Grabby Aliens in case people want to see that.
01:28:37.508 - 01:28:44.034, Speaker C: Kurgazat the cute animations that do these very technical things in very nice ways. Congratulations on that, by the way.
01:28:44.232 - 01:28:47.958, Speaker D: So the key idea is we wonder.
01:28:48.044 - 01:28:53.862, Speaker A: About the distribution of aliens in spacetime. And one possible theory you might have.
01:28:53.916 - 01:28:56.022, Speaker D: Is that we're the only ones at all.
01:28:56.076 - 01:28:59.994, Speaker A: And in the entire spacetime that we can see, there'll never be anybody but.
01:29:00.032 - 01:29:03.226, Speaker D: Us, in which case the universe would.
01:29:03.248 - 01:29:04.586, Speaker A: Just have waited for us to show.
01:29:04.608 - 01:29:10.558, Speaker D: Up whenever we were ready. We can reject that interpretation of the.
01:29:10.564 - 01:29:52.474, Speaker A: Universe because we are crazy early. So our best model of how advanced life should appear says that we should be most likely to appear on a longer lived planet toward the end of its history. And our planet is actually very short lived. Our planet will last another billion years for roughly 5 billion years total of history. The average planet lasts 5 trillion years. And because life has to go through a number of hard steps to get to where we are, there's actually a power law in terms of when it appears as a function of time, the power being the number of steps. And so if, say, the steps are six then the chance that we would appear toward the end of a longer.
01:29:52.512 - 01:29:55.226, Speaker D: Lived planet rather than now on this.
01:29:55.248 - 01:30:03.182, Speaker A: Planet is basically that factor of 1000 in their lifetime raised to the power of six. For this power law I. E ten.
01:30:03.236 - 01:30:07.486, Speaker D: To the 18 more likely to have.
01:30:07.508 - 01:30:23.266, Speaker A: Appeared later on in the universe. So we're crazy early relative to that standard. And the best explanation for that is there's a deadline soon. The universe is right now filling up with aliens taking over everything they can. Soon, in, say, a billion years or so, it'll all be full and all.
01:30:23.288 - 01:30:28.942, Speaker D: Taken, at which point you couldn't show up later on and be an advanced civilization.
01:30:29.006 - 01:30:32.502, Speaker A: Everything would be used for other stuff. And that's why you need to believe.
01:30:32.556 - 01:30:36.134, Speaker D: They'Re out there right now. So now that you've got to believe.
01:30:36.172 - 01:30:41.226, Speaker A: They'Re out there right now, you wonder whether, well, how close are they? What's going on out there? And for that, we.
01:30:41.248 - 01:30:44.026, Speaker D: Have a three parameter model where each.
01:30:44.048 - 01:30:45.786, Speaker A: Of the parameters is fit to a.
01:30:45.808 - 01:30:49.946, Speaker D: Key data point we have. And this model basically gives you the.
01:30:49.968 - 01:30:51.930, Speaker A: Distribution of aliens in spacetime.
01:30:52.350 - 01:30:54.126, Speaker D: And if you'd like we can walk.
01:30:54.148 - 01:31:02.858, Speaker A: Through what those parameters are and what the data point we have for each one is. But the end story is civilizations typically expand at a very fast speed, a.
01:31:02.884 - 01:31:04.900, Speaker D: Substantial fraction of the speed of light.
01:31:05.510 - 01:31:08.574, Speaker A: They appear roughly once per million galaxies.
01:31:08.702 - 01:31:13.202, Speaker D: These grabby alien civilizations and if we.
01:31:13.336 - 01:31:14.706, Speaker A: Head out to meet them, we'll meet.
01:31:14.728 - 01:31:17.610, Speaker D: Them in roughly a billion years spanning.
01:31:17.630 - 01:31:18.454, Speaker A: Near the speed of light.
01:31:18.492 - 01:31:25.686, Speaker D: So they are quite rare. That rare, but not so rare as to be empty in the universe that.
01:31:25.708 - 01:31:37.850, Speaker A: Is once per million galaxies there's many trillions of galaxies so that means there are millions of them out there. And right now the universe is roughly half full of them.
01:31:37.920 - 01:31:39.766, Speaker D: So that seems strange.
01:31:39.798 - 01:31:43.120, Speaker A: The universe looks empty but you have to realize there's a selection effect.
01:31:43.810 - 01:31:49.070, Speaker D: Everywhere you can see is a place where if there had been aliens there.
01:31:49.140 - 01:32:01.826, Speaker A: They would be here now instead of us. So the reason things looks empty is because you can't see a place where they are because they would move so fast from where they are to get.
01:32:01.848 - 01:32:05.874, Speaker D: To here that here would be taken. The fact that we are not now.
01:32:05.912 - 01:32:08.774, Speaker A: Taken here says that no one could have gotten here.
01:32:08.892 - 01:32:11.686, Speaker D: And therefore if you were able to.
01:32:11.708 - 01:32:19.080, Speaker C: Look out into the stars and see all the aliens that's nonsensical because if that would be possible they would have already grabbed you by that time.
01:32:19.530 - 01:32:22.378, Speaker D: Right. Because they move so fast, there's a.
01:32:22.384 - 01:32:26.246, Speaker A: Relatively small volume in the universe where you could see them and they haven't.
01:32:26.278 - 01:32:27.386, Speaker D: Quite got here yet.
01:32:27.568 - 01:32:29.130, Speaker A: Most of the places you could see.
01:32:29.200 - 01:32:30.378, Speaker D: They would be here.
01:32:30.544 - 01:32:39.130, Speaker B: And grabbing you doesn't necessarily mean destroying you, it just means possibly expanding to the borders such that you can't expand into their borders.
01:32:39.290 - 01:32:48.754, Speaker A: It would be enveloping you and then changing how the world around you looks. So we can be pretty sure we have not now been enveloped by a gravy alien civilization because we look around.
01:32:48.792 - 01:32:51.886, Speaker D: Us and we see pretty native stars.
01:32:51.918 - 01:32:55.454, Speaker A: And planets which are not been radically changed.
01:32:55.582 - 01:33:00.486, Speaker D: So yes, in the future we might be enveloped and other things out there.
01:33:00.508 - 01:33:04.678, Speaker A: Might be enveloped but we're not now we couldn't see this situation we're in.
01:33:04.764 - 01:33:07.720, Speaker D: If those alien civilizations had come here.
01:33:08.090 - 01:34:16.794, Speaker C: And this is actually why this intersects with the leaser AI problem because the way that you said that the civilizations that are out there would have come and enveloped us and then changed the environment that is around us, hopefully leaving us at peace. But this is the AI alignment problem in another form where another rogue alien civilization is also another paperclip maximizer and they're out just gathering all the resources, doing the things that they do according to their values. Hopefully their values are that when they do expand into our civilization, they leave us alone because some alignment is still there. But it is the same fundamental structure of like there is these goals and alignments with the universe around them and these aliens expand and they change the atoms of the matter that they expand into. And because we haven't seen that yet, because that's the assumption that we have, but because we haven't seen that yet, you are able to, in your gravy aliens paper, actually kind of place us in the arc of history because of this assumption that Grabby aliens are Grabby and that they will attempt to Grab.
01:34:16.842 - 01:34:20.606, Speaker B: And to add to that, I mean, they might be artificial intelligences as well, wouldn't they?
01:34:20.628 - 01:34:22.400, Speaker A: Robin almost surely they.
01:34:25.970 - 01:34:27.298, Speaker D: Know within a.
01:34:27.304 - 01:34:29.506, Speaker A: Thousand years, I expect our descendants to.
01:34:29.528 - 01:34:32.914, Speaker D: Be almost entirely artificial and certainly within.
01:34:32.952 - 01:34:36.402, Speaker A: A million years and these things would be billions of years older than us.
01:34:36.456 - 01:34:41.014, Speaker D: So yes, our artificial descendants will meet.
01:34:41.052 - 01:34:42.914, Speaker A: Their artificial descendants in maybe a billion.
01:34:42.962 - 01:34:47.126, Speaker D: Years and they won't have saved something like us. Now, I can give you a little.
01:34:47.148 - 01:34:51.706, Speaker A: More optimism in the sense that if aliens, these gravy civilizations appear once per.
01:34:51.728 - 01:34:54.934, Speaker D: Million galaxies, if the ratio of quiet.
01:34:54.982 - 01:34:56.794, Speaker A: To loud ones is even as high.
01:34:56.832 - 01:34:59.306, Speaker D: As 1000 to one, that would mean.
01:34:59.328 - 01:35:09.258, Speaker A: That in this expansion that they've been doing, that they will do. They'll only ever meet 1000 of these quiets as they expand through a million galaxies.
01:35:09.434 - 01:35:12.046, Speaker D: And so these rare places where an.
01:35:12.068 - 01:35:14.978, Speaker A: Alien civilization appeared would be pretty special.
01:35:15.144 - 01:35:19.486, Speaker D: And worth saving and isolating because Grabby.
01:35:19.518 - 01:35:33.634, Speaker A: Alien civilizations should be really obsessed with what will happen when they meet the other Grabby civilizations. They're really wanting to know what are these aliens like? Because they will have this conflict at the border and they will wonder are we going to be outclassed somehow? Will they trick us somehow? What's going to happen when we meet the border?
01:35:33.682 - 01:35:35.826, Speaker B: But they might make a national park.
01:35:35.858 - 01:35:40.220, Speaker D: Out of us, then might turn us.
01:35:40.830 - 01:35:54.560, Speaker A: Every gravy civilization will be really eager for any data they can get about what are aliens like. And so this small number of quiet civilizations they come across will be key data points. They will really treasure those data points.
01:35:55.410 - 01:35:57.166, Speaker D: In order to just give us some.
01:35:57.188 - 01:36:16.210, Speaker A: Data about what could aliens be like. And so that would be a reason why if aliens came and enveloped us, they would mainly want to save us as data about the other aliens. Now, that doesn't mean they don't freeze dry us all and run experiments, et cetera, per se. I mean, it's not necessarily going to let us just live our lives the way we want, but they wouldn't just erase us all either.
01:36:16.360 - 01:36:43.786, Speaker B: Well, Bankless Nation I elect Robin Hansen to make the case for not freeze drying us and to preserve us to the aliens if they come at some point in time but this is not necessarily in near term that they're coming, but it's more kind of the rate of spread. One interesting aspect of the model is would it be accurate to say, Robin, that the model predicts alien civilizations to spread like cancer? And I mean, that maybe mathematically know without the negative connotation that that brings.
01:36:43.978 - 01:36:49.310, Speaker A: Well, alien civilizations are created even more like cancer.
01:36:50.370 - 01:36:54.546, Speaker D: So in your body, you have an.
01:36:54.568 - 01:37:03.042, Speaker A: Enormous number of cells. And in order for one of your cells to become cancerous, it needs to undergo roughly six mutations in that one.
01:37:03.096 - 01:37:08.886, Speaker D: Cell during your lifetime. So that's basically the same sort of.
01:37:08.908 - 01:37:16.950, Speaker A: Hard steps process that planets go through. Planets are each in order to achieve an advanced civilization, they also need to go through roughly six mutations.
01:37:17.610 - 01:37:20.354, Speaker D: That is, the mutations are each unusual.
01:37:20.402 - 01:37:27.194, Speaker A: Thing has to happen, and then the next unusual thing happens, and then the next unusual things happens until all six have happened, and then you get something like us.
01:37:27.232 - 01:37:29.658, Speaker D: So the key idea is there's a.
01:37:29.664 - 01:37:32.426, Speaker A: Million galaxies, each of which have millions.
01:37:32.458 - 01:37:35.214, Speaker D: Of planets, and then all of these.
01:37:35.252 - 01:37:40.110, Speaker A: Planets are trying to go down this path of having all these mutations, but.
01:37:40.260 - 01:37:47.678, Speaker D: Almost none of them do successfully by their deadline of life no longer be possible on that planet.
01:37:47.854 - 01:37:55.726, Speaker A: And it's a very rare planet like ours for which all six mutations happen by the deadline of life no longer being possible on the planet.
01:37:55.758 - 01:37:58.434, Speaker D: And that's how cancer is in your body.
01:37:58.472 - 01:38:09.530, Speaker A: That is, 40% of people have cancer by the time they die, and that means one of their cells went through all six of these mutations. But that was really unlikely. Vast majority of cells only had one or zero mutations.
01:38:10.750 - 01:38:15.222, Speaker D: And so life on a planet reaching.
01:38:15.286 - 01:38:17.018, Speaker A: Advanced level that it could expand in.
01:38:17.024 - 01:38:21.642, Speaker D: The universe is mathematically exactly like cancer, right?
01:38:21.776 - 01:38:34.334, Speaker A: And so it follows the same power law with time, actually. So the probability that you get cancer as a function of your life is roughly time to the power of six because it takes roughly six mutations. That's why you usually get cancer near the very end.
01:38:34.532 - 01:38:37.950, Speaker D: And the chance that planet will achieve.
01:38:38.030 - 01:38:41.426, Speaker A: Advanced life is roughly the power of six or time. And that's why, in fact, in the.
01:38:41.448 - 01:38:44.846, Speaker D: Universe, universe is appearing over time faster.
01:38:44.878 - 01:38:52.742, Speaker A: And faster according to roughly a power of six because of this exact power law. And so the very early universe had almost nothing.
01:38:52.876 - 01:38:55.622, Speaker D: And then recently we've showed up, but.
01:38:55.676 - 01:39:01.786, Speaker A: Around us, they're all puppy. And the rate at which they're appearing now is much faster than it was.
01:39:01.808 - 01:39:05.340, Speaker D: In the past because of this power law.
01:39:06.590 - 01:39:57.020, Speaker C: And it shouldn't be lost on listeners that cancer is grabby. Cancer falls in the grabby category. And so there's a bunch of quiet cells that are just minding their own business, doing their job in harmony with their neighbor cells. And then one cell goes rogue and decides, I'm going to grab everything that I can around me and I'm going to grow to my best ability. And so it's just interesting to see no matter what scales or what mediums we perceive to be, whether it's the biological cell, it is human species as a whole. It is this theoretical AI superintelligent robot, but these same structures continue to show up. And so Robin, thank you for helping us navigate all of these different planes of existence and being able to reason about them all at once.
01:39:58.750 - 01:40:01.114, Speaker D: Well, we did a brief survey, but.
01:40:01.312 - 01:40:06.682, Speaker C: Right there's a number of different rabbit holes that we did not go down in the industry.
01:40:06.746 - 01:40:13.380, Speaker B: How about this? Because we've got a crypto audience. Robin, do you have any hot takes on crypto? What do you think of this stuff? How about that? Check that box.
01:40:14.070 - 01:40:35.654, Speaker A: I mean, I don't have a new take on crypto. My old take has always been for any new technology, you need both some fundamental algorithms and some fundamental tech, and then you need actual customers and some people to pay attention to those customers and to their particular needs. And you have to adapt the general technology to particular customer needs.
01:40:35.852 - 01:40:40.182, Speaker D: Crypto unfortunately, moved itself into a regime.
01:40:40.246 - 01:40:44.986, Speaker A: Where most of the credit and celebration and money went from having a white.
01:40:45.008 - 01:40:48.314, Speaker D: Paper and an algorithm and not so.
01:40:48.352 - 01:41:02.080, Speaker A: Much for actually connecting to customers. And so unfortunately, there's this huge appetite for tools and platforms under the theory that if we make a tool and platform, other people will do the work to connect that to customers.
01:41:02.450 - 01:41:04.702, Speaker D: And unfortunately, there's not so many people.
01:41:04.756 - 01:41:06.702, Speaker A: Who are sitting in that next roles.
01:41:06.766 - 01:41:09.746, Speaker D: But them succeeding at that task is.
01:41:09.768 - 01:41:14.910, Speaker A: The main thing that will make the rest of crypto succeed or not. There's plenty of tools and platforms.
01:41:14.990 - 01:41:17.378, Speaker D: Not so many people trying to market.
01:41:17.464 - 01:41:31.574, Speaker A: Concrete products to particular customers and holding their hand, working with them when it doesn't work for them, changing it somehow. Iterating in order to make a product actually work for concrete customers. That's how pretty much all business innovation.
01:41:31.622 - 01:41:35.034, Speaker D: Needs to happen in crypto as well as everywhere else.
01:41:35.152 - 01:41:50.266, Speaker A: Crypto isn't different by this regard. It's just that crypto sort of fell into this world where you got all the recognition and attention and money by writing white papers and implementing their first version of the algorithm that you then shipped and then moved over to another company to write another white paper and algorithm.
01:41:50.298 - 01:41:50.782, Speaker D: Right.
01:41:50.916 - 01:41:55.666, Speaker A: Instead of actually staying with the algorithm and trying to get customers to use it.
01:41:55.768 - 01:41:57.506, Speaker D: So I wish crypto well.
01:41:57.528 - 01:42:11.000, Speaker A: There's lots of interesting possibilities there, but that's in my mind, the major problem with crypto is the neglect of actual customers and the messy details of making customers happy.
01:42:11.370 - 01:42:25.020, Speaker C: Everyone in the crypto industry is recently, at least the VC landscape is talking about, everyone is trying to sell picks and shovels and no one's bothering to actually sift for gold right so maybe we need some more gold diggers out there.
01:42:25.390 - 01:42:44.974, Speaker B: I think so. And this is Robin being a utility. Show me the utility. And we certainly understand that take on crypto, and we certainly have some work to do in that area, but I think we should have you on sometime again, robin, I know there's so much we could pick your brain about. I know you're a huge advocate for prediction markets as a way to solve for things.
01:42:45.012 - 01:42:53.166, Speaker A: And this is once creative institution ideas that I think crypto people will be more interested than most. Crypto people are pretty open to creative institutions.
01:42:53.198 - 01:43:04.934, Speaker B: Oh, we are. So you got to come back and talk to us about sort of institutions and some of the new creative institutions, because I don't know if you noticed, robin, but around us, it seems like a lot of our institutions. Are crumbling or falling to pieces, are.
01:43:04.972 - 01:43:10.214, Speaker A: Losing their trust or just in the worst case, we're just locking down and not allowing much innovation or change.
01:43:10.332 - 01:43:11.240, Speaker B: 100%.
01:43:11.870 - 01:43:14.566, Speaker A: All right, well, let's leave this conversation only decaying slowly.
01:43:14.598 - 01:43:16.694, Speaker D: They're still not innovating and growing.
01:43:16.742 - 01:43:33.742, Speaker B: This is a to be continued bankless Nation. If you haven't had enough of Robin Hansen I certainly haven't. I could talk to this man for hours. Then let us know, and we'll see if we can get him back on another time. But you have helped me understand a bit more about artificial intelligence, and for that, I certainly help.
01:43:33.796 - 01:43:35.066, Speaker A: Are you going to sleep tonight?
01:43:35.178 - 01:43:54.054, Speaker B: I'm going to sleep much better tonight. Honestly. Yes. So thank you. There are some things in there I think I need to relisten to and think about a little bit more. These descendants being so much unlike me that that might make me concerned, but I'm far less concerned than after the Eliezer episode. So I appreciate that.
01:43:54.092 - 01:43:55.880, Speaker C: It's a natural concern as it does.
01:43:56.730 - 01:44:16.346, Speaker B: That's right. Action items for you, Bankless Nation. We'll include a link to the Eliezerkowski episode we're all going to die, it was called. That was seriously the title, Robin. And we'll also include a link to the AI foom debate, which we talked about that term. I just learned what that term was. The Age of M, a book that I'm adding to my cue from Robin Hansen.
01:44:16.346 - 01:44:20.350, Speaker B: Of course, talking about this is artificial minds, is it not, Robin?
01:44:21.970 - 01:44:24.778, Speaker A: Artificial implementations of ordinary human minds.
01:44:24.794 - 01:44:38.814, Speaker B: There you go. That sounds fascinating. And, of course, grabby aliens. There is a Kurgazat video as well as the original website grabbyaliens.com. We'll include all of that in the show. Notes, written disclaimers. Gotta let you know, none of this has been financial advice.
01:44:38.814 - 01:44:49.380, Speaker B: It's not even spacefaring civilization advice. I don't think you could definitely lose what you put in. But we are headed west. This is the frontier. It's not for everyone. But we're glad you're with us on the Bankless journey. Thanks a lot.
