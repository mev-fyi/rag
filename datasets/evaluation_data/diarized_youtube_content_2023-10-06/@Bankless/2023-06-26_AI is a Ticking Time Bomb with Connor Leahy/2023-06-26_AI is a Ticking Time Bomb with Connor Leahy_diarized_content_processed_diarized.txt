00:00:00.250 - 00:00:23.390, Speaker A: Even if you, dear listener, don't totally buy the existential risk thing. Maybe you don't buy it's, fine. But it is the case that the leaders of all the top labs anthropic DeepMind open air, have been on the record saying clearly that they do think that there is a realistic possibility that these technologies will kill literally everybody. And they're doing it anyways.
00:00:27.530 - 00:00:46.790, Speaker B: Welcome to Bankless, where we explore the frontier of Internet money and internet finance. This is how to get started, how to get better, and how to front run the opportunity. This is David Hoffman here without my co host Ryan Sean Adams. But regardless, we are here to help you become more Bankless. Today on the episode, we're talking AI. Alignment and AI. Safety.
00:00:46.790 - 00:01:10.514, Speaker B: Once again, we're talking to Connor Leahy, the CEO at Conjecture, a mission driven.org trying to make AI. Go well. We wanted to do one last episode on the AI. Alignment and AI. Safety conversation because we think Connor can really deliver a very compelling and easy articulation as to why AI. Safety is real and why it needs to be treated as such.
00:01:10.514 - 00:01:34.742, Speaker B: Some main benefits and takeaways that you're going to get from this episode. First, the intuitive arguments behind the AI. Safety debate, the things that you can take to your friends to convince them that AI. Safety is a real issue. Second, the two defining categories of ways AI. Could end humanity. And third, the major players that are playing in the race towards AGI and why they all seem to be ideologically motivated rather than financially motivated.
00:01:34.742 - 00:02:10.154, Speaker B: Fourth, why the progress of AI power is based on two exponential curves. And lastly, fifth, why Connor thinks government regulation is the easiest and most effective way of buying us time here on Bankless, we've had the AI. Alignment, AI. Safety conversation a handful of times with different players from the industry ever since we had that eliezer episode, which we were hoping would have been an AI. Crypto conversation, but turns out it was an AI. Is going to kill us all conversation. We're bringing on Connor Leahy in June of 2023, a number of months after we first went down this rabbit hole, because the world of AI.
00:02:10.154 - 00:02:43.530, Speaker B: Kind of feels like the world of crypto in 2021, it is moving so fast. And so Connor gives us the lay of the land, a snapshot in time of the AI. Alignment conversation as it stands here in late June of 2023, as well as also articulates in the easiest and most simple terms possible, why AI. Alignment is such a big deal. If you, Bankless listener, are not convinced of AI. Alignment going into this episode and you remain unconvinced after the end of this episode, I have nothing left for you. And so let's go ahead and get right into that conversation with Connor Leahy.
00:02:43.530 - 00:03:15.702, Speaker B: But first, a moment to talk about these fantastic sponsors that make this show possible, especially Kraken, our preferred crypto exchange for 2023. Assuming that we get past the AI alignment issue, we will still need to buy crypto assets using our fiat dollar. So perhaps use Kraken to get that done. There's a link in the show notes to get started. Kraken Pro has easily become the best crypto trading platform in the industry. The place I use to check the charts and the crypto prices even when I'm not looking to place a trade. On Kraken Pro, you'll have access to advanced charting tools, real time market data, and lightning fast trade execution, all inside their spiffy new modular interface.
00:03:15.702 - 00:03:35.362, Speaker B: Kraken's new customizable modular layout lets you tailor your trading experience to suit your needs. Pick and choose your favorite modules and place them anywhere you want in your screen. With Kraken Pro, you have that power whether you are a seasoned pro or just starting out. Join thousands of traders who trust Kraken Pro for their crypto trading needs. Visit pro. Kraken.com to get started.
00:03:35.416 - 00:03:35.874, Speaker A: Today.
00:03:35.992 - 00:04:33.582, Speaker B: Mantle, formerly known as Bitdao, is the first dowled web3 ecosystem, all built on top of Mantle's first core product, the Mantle Network, a brand new high performance ethereum layer two built using the Op stack but uses Eigen layer's data availability solution instead of the expensive ethereum layer one. Not only does this reduce Mantle network's gas fees by 80%, but it also reduces gas fee volatility, providing a more stable foundation for Mantle's applications. The Mantle Treasury is one of the biggest dow owned Treasuries, which is seeding an ecosystem of projects from all around the web3 space for Mantle. Mantle already has subcommunities from around web3 onboarded like Game Seven for web Three gaming, and Bybit for TVL and Liquidity and Onram. So if you want to build on the Mantle network, Mantle is offering a grants program that provides milestone based funding to promising projects that help expand, secure and decentralize mantle. If you want to get started working with the first dowled layer two ecosystem, check out Mantle at Mantle XYZ and follow them on Twitter at zero x. Mantle.
00:04:33.582 - 00:05:10.686, Speaker B: If you haven't experienced the superpowers that a smart contract wallet gives you, check out Ambire. Ambire works with all the EVM chains that are out there the layer twos like Arbitrum, Optimism and Polygon, but also the nonessetherium chains like Avalanche and Phantom. Because of the power of smart contract wallets, Ambire lets you pay for gas and stablecoins, meaning you'll never have to spend your precious ETH again. The web app has numerous fiat onramps to make it easy to dump your fiat for crypto. And if you like self custody but you still want training wheels, you can recover a lost Ambire wallet using an email and password, but without giving the Ambire team any control over your funds. Check it out@ambire.com for the web app experience.
00:05:10.686 - 00:05:53.930, Speaker B: But also the Ambire Mobile wallet is coming soon for both iOS and Android, and if you want to be a beta tester you could sign up@ambire.com slash app. And since you stayed to the very end of this ad read, you should know that Ambire is airdropping its wallet token to early users for simply just using the wallet. So if you want to get started with Ambire, all the links that you need are in the show notes bankless Nation I'm excited to introduce you to Connor Leahy, the CEO at Conjecture, a mission driven organization trying to make AI go well. He's also the co founder of Eluther AI, an open source AI research nonprofit laboratory which, interestingly, operates mostly inside of a discord server, much like our company and so many of us in the Bankless Nation. Connor.
00:05:54.010 - 00:05:56.762, Speaker A: Welcome to Bankless. Thank you so much for having me, Connor.
00:05:56.826 - 00:06:43.726, Speaker B: The crypto World has more or less collided with AI and Bankless. We had our introduction with that, surprisingly, when we had our Eliezer Yudkowski episode, which we had to pivot mid episode from a crypto AI intersection episode into an AI is going to kill us episode. Since then, we've continued to go down that AI alignment rabbit hole. And I think a decent number of people in the Bankless Nation in the broader crypto landscape accept the AI alignment problem, but others completely reject it. And it's interesting to see some people just have a spinal reflex rejection of the AI. Alignment problem. So in this conversation, I hope we can kind of just talk about the conversation of AI alignment in the outside world, the companies that are playing here, the game board that is laid out in front of us.
00:06:43.726 - 00:07:05.958, Speaker B: But first, I really just want to dive into the very basics of the AI alignment problem and see if we can once again articulate clearly why the AI alignment problem exists, what it is, and why it's important. And you've gotten a lot of practice at articulating this. I'm wondering if you could kind of handhold us through some of the very basic premises behind the AI alignment problem.
00:07:06.124 - 00:08:06.390, Speaker A: Yeah, absolutely. So to start things off, the AI alignment problem or existential risk is really what I care about this, is that something could be so dangerous that an accident or a misuse could occur of such magnitude that it could threaten the continued existence of all of humanity or curb our potential forever. In some sense, this would, of course, be extremely terrible. I think this is pretty uncontroversial that if such a thing were possible, and if it did happen, that would be pretty terrible. And so the way I like to think about this is first kind of from like an outside perspective is if we look at the history of technology, we see that technology has gotten better and better all throughout our history and it's given us way more power, way more Fordance. A lot of this is great, right? We have medicine and I have air conditioning in my office, thank God. All these great things that are wonderful and I love.
00:08:06.390 - 00:09:04.966, Speaker A: But also as technology increases, as your power increases your ability to control the environment, you have more capacity for things to go wrong or for destruction to occur. So back in the stone ages, the worst possible thing that I could do with stone age technology would kill maybe like ten people or something. If I'm like a pretty big guy or maybe if I'm super smart about it, I could kill a few hundred. If I have my whole war band with me, I could kill maybe more. But like an individual person just using stone age level tools not going to really be an existential risk or anything of that sort. As our technology gets better, we develop more sophisticated weaponry, we develop gunpowder, we develop stuff like this. The damage that can be caused both on purpose and accidentally increases sometimes from benign reasons.
00:09:04.966 - 00:09:33.490, Speaker A: If you have bigger ships, more people can drown. That's pretty benign. We don't think that that's a risk that we are willing to take. But in other know, there was no such thing as TNT factories blowing up before we had explosives. And now that we had TNT when an accident occurred, the collateral damage was suddenly of a type and a degree that didn't exist previously. And this is only continuing. So we went from, okay, you can kill like five guys to you can kill like 50 guys to kill 500.
00:09:33.490 - 00:10:23.438, Speaker A: And now you press the button and drop a nuke out of an airplane and you can kill 50,000 people or even more than that. And so if you would graph over time the increase in blast radius of technology, of a misuse of a technology or thing, you would see an exponential. You would see that our technology is growing extremely fast, extremely quickly towards larger and larger blast radius. Not all technologies, many technologies are very safe and they're very good and I think we should invest in them and we should build them. But there are technologies that have larger and larger blast radiuses and eventually this blast radius will encompass earth. If our technology keeps improving, at some point we will have technology that is so powerful that it can destroy earth, that it can destroy all humans and even accidentally. It would of course be easier if you do it on purpose.
00:10:23.438 - 00:11:12.398, Speaker A: But at some point if we have powerful enough technology, we should expect things where even accents are a problem. And so I would make the claim that AGI is in this category, it's in the category of trend in technology towards more and more powerful systems where even an accident during the development of the system has larger and larger brass radius. And sometimes this is what we accept as a society. We accept that sometimes a clinical trial might go wrong. This is something that sometimes we accept to some degree, not to arbitrary degrees. We don't let anyone do any clinical trial without any oversight of course not. We do generally as a society, we think very highly of human welfare and life and that it shouldn't be endangered recklessly.
00:11:12.398 - 00:11:57.940, Speaker A: But as this goes on, as we're dealing with these more and more powerful technologies, we have to be very like. What do you do when you have a technology where the blast radius is everybody, including you? How do you develop a technology where getting it wrong ends everything? Ends you, ends your experiment? This is not something humanity has experience dealing with. This is not something that we are generally set up to do. The way we usually do technology is we build something and we fail a bunch of times. We mess it up, a few lab assistants lose their hands, a bunch of stupid things happen, government gets angry at you, and 20 years later you maybe have something. This is fine. But predictably, at some point this stops working.
00:11:57.940 - 00:12:24.698, Speaker A: Maybe people could argue the time is not now. And I would agree. I don't expect GPT-3 or four to kill all people. I don't. But GPT 5678 combined with some modern RL agentic systems, that's much less clear to me and happy to go into a bit more about why I think this technology has this kind of blacks radius. But first, just pause there for a second.
00:12:24.784 - 00:12:51.410, Speaker B: One thing I really want to emphasize in this argument is the neutrality of it. It's not saying that AI is good or bad. It's just saying that AI is. And this is a continuation of the arc of technology. Technology is not good nor bad. You're just really putting it into very neutral terms. That technology has an arc of goodness but has turbulence with associated blast radiuses along the way.
00:12:51.410 - 00:13:18.138, Speaker B: And we've all accepted these because the blast radiuses has been sufficiently small that it doesn't end all humans. But what you're saying is that continue that arc, get it to AI. AI. It doesn't want to kill us. It doesn't want to make our lives better. Humans will make choices with this technology. But it just so happens that in the advent of a bad outcome, that bad outcome's bad blast radius contains all of us.
00:13:18.138 - 00:13:34.158, Speaker B: So it's not even like a political stance or an opinionated stance about the goodness or badness of AI. It's merely just a statement about the magnitude of what could go wrong if something does go wrong.
00:13:34.324 - 00:14:36.734, Speaker A: Yeah, exactly. I think a lot of discourse around this kind of stuff is very bad. I think a lot of it is completely politicized and psychologized. It's about is open source good or bad? Is this guy a good person or a bad person? And I think this is just a terrible way to think about this. We should think about this very neutrally. The question is not is AI good or bad? It's not Is open source good or bad? It's not, Is Sam Altman a good person or a bad person? The question is just what are the outcomes of the various choices we make? How can we do our best to predict what the consequences of taking various actions are? And how do we feel about those consequences? How do we feel about taking these risks? How risk tolerant are we who should be consulted? I was never consulted to have GPT Four released onto the public Internet, which I use as well, for it to be released on my family and my friends. I was never asked if this kind of experimental new technology should be unleashed into the commons that I also inhabit.
00:14:36.734 - 00:15:16.370, Speaker A: And maybe that's fine. We don't generally seek consent. When someone wants to write a book, someone wants to write a book I don't have to read, it not my problem. But if someone is releasing, I don't know, a new substance into the water supply, well, I sure hope that I would be consulted about whether I think that's a good idea or not because I'm drinking that water. I feel like I have a say in this. And so a lot of the problem with the current state of discourse around this stuff is that there is some good technical discourse, but then there's also a lot of discourse which focuses way too much on intentions. It focused way too much on ideologies.
00:15:16.370 - 00:15:37.234, Speaker A: We can't derive from an ideology what is true. If you believe open source is good, maybe you believe that, right? But that's not a statement about reality. It's a mood affiliation. Sometimes open source is great. I worked a lot in open source. I think in many cases open source is fantastic. I'm so glad that Linux exists.
00:15:37.234 - 00:16:07.060, Speaker A: I'm so glad that we have so much great open source software. I think this is really good. But should the genetic sequence of smallpox be like, I don't know, man. I love science. I love academia. I think it's great that people are getting funded to do all kinds of cutting edge research. A group of researchers in, I think, Canada used government funds to reconstruct an extinct form of horsepox or smallpox virus and published how to do it.
00:16:07.060 - 00:16:33.418, Speaker A: I think they should have done that. I think this is bad. I think people shouldn't do that. I think that as much as I love open access science, as much as I love science, we have to be practical here. I don't expect the upsides of this being public to be worth it. So this is kind of like how I like to think about these kind of things. I like to think about these things way more as just like, look, what do we want? What are the scenarios? What are the outcomes? And then let's work from there.
00:16:33.418 - 00:16:44.880, Speaker A: Let's work at this completely neutrally. Let's be realistic here. I like to talk about strategy. I don't like to talk about good or bad, if that makes any sense, right?
00:16:45.810 - 00:17:14.214, Speaker B: There's a bunch of conversations to be had here about the players in the world of AI. We have chat GPT and OpenAI and Sam Altman. We have stable diffusion with Emad. There's a bunch of people in this game. The regulators are also in this game. I still want to continue a little bit down the defining the alignment problem conversation because there's something I want to parse apart. You're talking about the sheer neutral AI and its blast radius, right.
00:17:14.214 - 00:18:18.950, Speaker B: What could go wrong if something were to go wrong? When we had our conversation with Eliezer, Eliezer gave us a very strict prescription of how this will go wrong. That your explanation of the AI alignment problem contains. But his is more narrow and I want to parse apart that because he calls this or we call this the AI alignment problem, as in how do we align the goals of AI with the human goals? And that's a more narrow conversation than I think, what you're presenting. You're just saying, hey, AI is powerful, it could go wrong. And it could go wrong for any subset of ways, some of which may be the AI alignment problem, but there are also other ways that it could go wrong. I'm wondering, how do you parse apart the ways that are there categories of AI doom that are worth parsing apart? One of them is the AI alignment. Others is like, maybe humans go rogue and then we've seen humans do this, right? Like, what would happen if Ted Kaczynski got their hands on a very powerful like, that's another conversation.
00:18:18.950 - 00:18:22.410, Speaker B: How do you parse apart the actual sources of destruction?
00:18:22.830 - 00:18:38.590, Speaker A: Like the I like to use two, maybe three or four categories is that the first one is misalignment. It's just you don't have control, just lack of control. A thing does something random, that's it. And doesn't even need a human to be involved.
00:18:38.750 - 00:18:42.818, Speaker B: And that random things. Blast radius contains us is what you're saying? Yeah.
00:18:42.904 - 00:19:13.226, Speaker A: Yes. The claim is that an accidents can happen and that accidents can have blast radius of this side. So there's like accidents. The second one is misuse. So misuse is you have a system which does what you say and someone tells it to do something bad or for there to be conflict of some kind. Maybe there's multiple actors who go to war using this kind of technology or like fight. And then I can barely imagine something more horrific than multiple super intelligent systems fighting.
00:19:13.226 - 00:19:20.302, Speaker A: Could you imagine it's unimaginable what horrors such systems could unleash in the terms of a war?
00:19:20.356 - 00:19:40.998, Speaker B: And in that scenario, that scenario is kind of interesting because it implies that the AI alignment problem is actually solved. As in we've been actually able to align humans and AIS together to achieve the same goals, but human alignment is not solved. And so using our AI superpowers, we now commit war. And our war blast radius once again contains everyone.
00:19:41.164 - 00:20:05.658, Speaker A: Exactly. So it's even worse than that. It's like there are so many ways in which this problem is very, very hard. This is not a super narrow, specific problem. There are specific, narrow aspects to problems, specific, narrow technical problems which are very, very important and we can talk about those. But importantly, this is a generalized problem of society. This is a generalized problem of the human condition.
00:20:05.658 - 00:20:52.614, Speaker A: This is a general problem of how do we responsibly deal with powerful technology. This is the meta problem that we have to actually have to solve. And this is a problem that people have been talking about at least since, like, World War I. There is a Polish nobleman named Alfred Krasinski who, after the horrors of World War I, noticed in this, like 1920s, he was like, wait, if technology keeps increasing, but our wisdom and our control as a society increase much slower, well, then all humanity will end. He figured out X Risk in the 20s. His solution was he had to figure out how to improve the art of human rationality. Sound familiar? There was an el Yezer in the 1920s, actually named Alfred Krasipski.
00:20:52.614 - 00:21:28.486, Speaker A: So this is not a new thought. This is not something that I'm sure people before Alfred has also come up with variations on this thought. And this is continuing this problem. And we have not yet solved this problem. We are already in the in the last time we had a new level of powerful technology, we did drop two nukes on purpose. And after that there were several really close calls where nukes almost did stop flying, at least two, where it was just one single person each time who stopped the nukes from actually getting fired. So our track record here is like decent, but it's not good.
00:21:28.486 - 00:21:57.554, Speaker A: It is not good. And it's going to be much worse when we're dealing with agis and AI, especially. Like, imagine if these things are open source. Imagine if every person in the world had a nuke during the Cold War. I expect we would have gotten nuked. I expect it wouldn't have gotten well if we didn't have chains of command, several people signing off on something. If we didn't have sensible people that took their responsibility extremely seriously, I think it just would have not gone well.
00:21:57.554 - 00:22:40.846, Speaker A: And the way things currently are, there is currently more regulation on selling a sandwich to the public than there is to building unprecedented AGI level technology and releasing it to the general public. There is no oversight. There's no general processes here or controls or stakeholders something. There's nothing. It's just these private companies. It's a small number of private companies, to be clear about this. It's basically currently mostly just DeepMind anthropic and OpenAI and a few others who are trying to catch up that are really pushing forward to these high end, AGI level, dangerous technologies and erasing completely out of control.
00:22:40.846 - 00:22:52.018, Speaker A: So how would you expect this to go? Well at this current pace? It's like we're in the worst possible scenario we could be in. Kind of.
00:22:52.104 - 00:23:14.886, Speaker B: I just want to pin down the categories that we were talking about. I interrupted you, and I just want to make sure that we clearly identify them. The categories of how AI progress would go wrong. One of them is misalignment. This is the leaser conversation. The paperclip maximizer conversation. We create this super intelligent AI, and then we can't figure out how to harness its goals and align them with humans.
00:23:14.886 - 00:23:31.914, Speaker B: And so it accidentally turns us into paperclips. That's one category. Another category that you defined was misuse. We somehow do figure out the AI alignment problem, but we just abuse AIS to kill us all. So one superpower fights with another superpower. One of them has AI. Maybe both of them have AI.
00:23:31.914 - 00:23:50.642, Speaker B: And then we all die because we're using AI to have misaligned human goals. So that's misuse. Another one is accidents. We have super powerful AI. It accidentally does something that we don't like, and we're inside of that blast radius. That's a third category. Are those all of them, or are there others that are worth unpacking?
00:23:50.706 - 00:24:29.218, Speaker A: Yeah, I mean, I think you can even fold accidents into misalignment, in a sense. Is that a truly aligned system? If you tell it to shoot you in the foot, it will say, no, that's not what you intended, and bring you flowers or something. If you have a truly maybe like three categories or maybe four is like a good hierarchy, maybe four categories. Technology that is so dangerous that no one should do it, just like there is nothing. Like you just turn it on, it blows up everything. Then there's, like, technology, which is controllable. If you're really careful, if you're very sensible, if you're very careful, it's fine.
00:24:29.218 - 00:24:45.750, Speaker A: Then there is technology that is safe for general use, except if you misuse it, except if you're specifically trying to do something bad. And then there is technology that is good no matter who uses it, so that you could give it to the most evil sociopath in the world, and it's fine. Okay.
00:24:45.820 - 00:24:50.854, Speaker B: And AI. I can see AI fitting into all those categories, including the most dangerous ones.
00:24:50.972 - 00:25:03.578, Speaker A: Exactly. I think we will start at one, and then we can develop technology to move to two. And then we can develop technology to move to three. And then we can develop technology to move to four. But by default, we get a category one AI.
00:25:03.754 - 00:25:13.022, Speaker B: By default, we elevate. AI as it progresses, finds itself inside of the category of technology. You almost never, ever want to even open up at all.
00:25:13.076 - 00:25:26.222, Speaker A: Yes. This is what I expect. The shortest path to an AGI gets you this type of AGI. A system which is misaligned. Paperclip maximizer has some random values. It's very intelligent, very capable. It's very deceptive.
00:25:26.222 - 00:25:39.638, Speaker A: And if you just turn it on, it doesn't matter who turns it on. It doesn't matter if this know, the USA or China. It doesn't matter if it's OpenAI or anthropic. It doesn't matter who does it. It just blows up everything. It doesn't matter.
00:25:39.804 - 00:25:50.330, Speaker B: You said the shortest path to get to a super intelligent AI. Can you unpack why you emphasize the word short there? What is implied under a longer path to an AI? What does that mean?
00:25:50.400 - 00:26:28.658, Speaker A: So importantly, this is not a rule of nature. There is no law of physics which states it is impossible to have a safe AI that does good things for you. This is completely allowed by the laws of physics and computer science. We just don't know how to do it. And this is a very narrow target. Each of these categories, one to four, are like a narrower and narrower and narrower target. You need to know more and more about control, about intelligence, about safe practices, about human psychology, about values and game theory and whatever to narrow down on these more and more complex systems.
00:26:28.658 - 00:26:56.174, Speaker A: So by default, if you just want a thing which is just smart, it's just powerful, it just succeeds at goals. Well, just big, larger models. Throw a compute at it, man. Just continue doing what we're doing right now. Basically, all the research currently being done at AI companies is of the kind which gets you to type one. There is very little research that goes into getting you to type two, three or four. This is a great quote.
00:26:56.174 - 00:27:32.650, Speaker A: It's from Wilbur Wright. So this is one of the brothers who built the first airplane. And what he said was that when once the machine is under proper control under all conditions, the motor problem will be quickly solved. A failure of a motor will then mean simply a slow descent and safe landing instead of a disastrous fall. So this is the man himself built an airplane who realized the first step to building a good airplane was to solve the safety problem. It was to build a safe glider that if something goes wrong, it carefully, slowly descends instead of killing the pilot. Because before this, there was a lot of attempts at flying machines and they always killed the pilot so they couldn't develop them.
00:27:32.650 - 00:27:56.094, Speaker A: He recognized that we had to first build a safe glider and then we can worry about the motor part. And he even though it's almost dismissive of the motor, he was like, we'll fear our motor. It's not that big of a deal. And so basically, all of AI currently works on engines. They work on motors. There's very few people working on gliders. And basically everyone is working on building bigger and bigger turbojet engines.
00:27:56.094 - 00:28:23.214, Speaker A: And just like as fast as possible, as quickly as possible. And if you get a bigger and bigger turbojet engine without improving your glider design, without improving your wing control design, without doing the necessary experience for this, by default you get a type one engine. You get an engine that. Just explodes. It just zooms off into space and just does something stupid. And if it's like an aircraft engine, the risk, the blast radius is contained as before. But AI is not just an airplane engine, right? Yeah.
00:28:23.252 - 00:29:31.374, Speaker B: The AI engine is focused on elevating humanity to as high of an elevation as possible. And then all of the AI safety people are like, hey, we also need to make sure that we have a spaceship that is containing us that can take us back down in the inevitable case that eventually that engine runs out. Connor, I'm hoping I can just get your lay of the land of the state of the AI alignment conversation because as we've been saying, as you've been talking about, AI progress seems to be moving really fast. Is now June 2023 and it kind of seems that we have to timestamp these podcasts because if we were doing this podcast in just May or April of the same year, it would be a slightly different conversation about what is the state of the AI alignment conversation. Last we checked, there was this letter signed by many world leaders and AI experts asking for a pause on AI progress beyond Chat two, BD four. I'm wondering if you can just update us on the last few months in the mainstreaming of the alignment problem. Is there more reasons to be optimistic? Are people burying their heads deeper in the sand? Where is the world with regards to AI alignment and AI safety?
00:29:31.502 - 00:29:59.702, Speaker A: Yeah, I mean, man, has the world changed. The world's changed insanely over the last six to twelve months. I mean to say it lightly, AI alignment has gone to a large degree, mainstream. I just earlier today was talking to a member of parliament who didn't even have a smartphone, didn't know what OpenAI was, but he heard about this AI thing and he wanted me to tell him about it. And he got quite furious when I explained to him some of the risks. And he's like, no one taking this seriously.
00:29:59.766 - 00:30:00.138, Speaker B: What?
00:30:00.224 - 00:30:23.138, Speaker A: This is outrageous. Like, of course we have to do something about this. So it's quite fun for me. It's also been a very enlightening experience. So when I first got into this field, I came in from a pretty classic, kind of like, less wrong, eliezer, adjacent kind of viewpoint. Very technical, very nerdy, very philosophical perspective on things. Not much.
00:30:23.138 - 00:30:46.602, Speaker A: And there's a lot of weird social memes in that sphere around politics, that politics is bad, you should never do it. Don't talk to the public, don't talk to the government. They're all crazy, can't talk to them. And I feel it's completely gaslit because that's just not true. Don't get me wrong, politics is hard, politicians have their incentives, blah, blah, blah. All these things are true. But this is things you can do.
00:30:46.602 - 00:31:25.174, Speaker A: You can talk to them. A thing that's just been incredibly, positively shocking to me is that when I talk to normal people who don't work in tech, they really get it. I'm so used to talking to people in tech and they just totally dismiss these things like, no, AI could never do anything bad, blah, blah, blah, I can't hear you. And I talk to normal people and explain to them, hey, these things are becoming more and more intelligent and they can do more and more things and we don't know how to control them. And they're like, Holy shit, well, that's terrible. Of course this is going to go wrong. What do you mean? And then I'll repeat an argument that Sam album makes or something that's not convincing at like, what do you mean? He doesn't understand how his system works.
00:31:25.174 - 00:31:59.890, Speaker A: It's all black boxes. This is madness. And this is the correct reaction, the correct maxim is this is madness. This is complete and utter ludicro. Let me be blunt here. Even if you, dear listener, don't totally buy the existential risk thing, maybe you don't buy it's, fine. But it is the case that the leaders of all the top labs, anthropic, DeepMind Open Air, have been on the record saying clearly that they do think that there is a realistic possibility that these technologies will kill literally everybody and they're doing it anyways.
00:31:59.890 - 00:32:43.946, Speaker A: Even if you disagree about the risk being real, I'm quite shocked that someone would admit that they believe this, state that they do think this, and then also that they are willing to do it without the necessary safeties and precautions. And there are arguments and man, are there arguments. And we can get into those arguments, the counterarguments about why actually this is fine. I don't find them convincing, obviously. And we live in this weird twilight world where on the one sand, I don't think these people are malicious, to be clear, or like they're lying, per se. I think they're being inconsistent. Sam Altman will often go on the record and say, oh, he thinks AI is the biggest risk to humanity.
00:32:43.946 - 00:33:21.342, Speaker A: Cool, great. Thank you, Sam. That's really great. I'm not being facetious. This is actually fantastic, that he has the honor and the bravery to say this publicly as someone who is ultimately a businessman, but still willing to go onto the record with this that is respectable and deserves credit. But then he keeps racing and then he keeps still building. There was this incredibly funny interaction on Twitter where Jan Leiche, the head of safety and alignment at OpenAI, tweeted something, know, maybe we should be careful, know, slow down a bit before we integrate all this AI technology into all facets of our life.
00:33:21.342 - 00:34:01.560, Speaker A: And then six days later, Sam Altman tweets about Chat GPT plugins, plug in Chat GPT into whatever you want. And I'm like, man, wow. If this was in a movie, I could imagine just like, the cut and then the laugh track playing. I'm like, this is truly shocking and this is a consistent feature is that this is something I've been pushing on a lot in the current discourse is that a lot of the discourse right now is people are starting to wake up to it and they're a bit confused. They're confused about like, well, is this risk real? Which is a good thing to be confused about. This is a fair thing to be confused about. And there's other things about.
00:34:01.560 - 00:34:40.822, Speaker A: So there's a very funny thing that I see a lot. For example, on Twitter, people see like, say, Sam Altman call for regulation and they'll be like, Wait, this is suss. If he wants to be regulated so bad and he thinks this risk is so big, why is he doing this? Just stop. Just don't race if you think this is an extra interest, just stop. And my honest opinion, yeah, that's pretty like, some people criticize me by proxy. They'll be like, Connor, you're one of these doomer people, but you doomer people if you take it so seriously, why don't you stop? Well, first of all, I don't race. And yeah, that's a really good question.
00:34:40.822 - 00:34:51.114, Speaker A: This is a question that I have for the head of all these labs. Why like, stop? I'm happy to go into the arguments that I expect their straw men version two, but in case you have any.
00:34:51.152 - 00:35:11.146, Speaker B: Comments, yeah, maybe we can actually just define race, the whole race condition side of things. A decent part of the bankless audience will be familiar with Moloch and Moloch traps, but it's been a while since we've had a Moloch episode, so maybe you can kind of talk about just like what this term race means and why we are in a race trap.
00:35:11.258 - 00:35:45.670, Speaker A: Yeah, so race conditions, it's kind of like in the sense of like a race to the bottom is that the idea is you and other people don't want to go somewhere. You don't want a certain technology to exist, but other people are heading towards it. You think you're better than these people, you're more responsible, nicer, whatever. And so you think, well, you have to get there first before they get to it. And so you start trying to get there as fast as possible. They notice that now you're trying to get there as fast as possible. And then they're like, well, shit, I don't want that guy to get it because I'm the good guy.
00:35:45.670 - 00:36:15.602, Speaker A: And then they start going as fast as possible. So you get this game theoretic problem where now it's a race to the bottom. This is not unique to AI. This happens, for example, with like, safety regulation. The reason we have regulations for safety standards on the government side, instead of letting the market regulate itself is by default. If you have a market, no regulation, there is an incentive to cut as much safety measures as you can possibly get away with. You want to do as little safety as possible.
00:36:15.602 - 00:36:44.502, Speaker A: If you don't get in trouble for your employees getting killed, well, then just let them die. It's, like, not that big of a deal. And this is exactly what happened, like, during the Industrial revolution, is that the pricing of risk and of dangers and these kind of things can be unlike an economic scale. It's very easy to be mispriced from what we as a culture might want, as a society might want. And this is not supposed to be a statement that regulation is good or bad. Again, it's not about good and bad. This is how we started this podcast.
00:36:44.502 - 00:37:18.290, Speaker A: Not saying regulation is good or regulation is bad. It's never that simple. It's the question of what do we want? What do we as a society want, and how can we get that? And if we don't like where we are currently, what can we do to move to something that we like? So with the race, what we're seeing here is companies undercutting themselves on safety and speed and timelines. Sometimes you call it burning our runway. So humanity has a runway until AGI arrives. It will arrive sooner or later. If before that time we don't have a safe glider, we die.
00:37:18.290 - 00:37:42.410, Speaker A: And they're burning the runway. They're making it shorter by pushing forward this technology, by investing more into it, by building bigger engines. There's some engine size that when you get to that size, it blows up everybody. Unless you have a safe spaceship built around it. And currently we're not building a spaceship, we're just building Billy engines because, well, I don't want the other guy to build engines. He's really unsafe. He doesn't take it seriously.
00:37:42.410 - 00:38:10.306, Speaker A: Or what if China gets it? What if some other country gets it? No, America number one. We have to get it first. And I understand these arguments wrong, and I'm happy to go into some of the details about why they're misleading, but it doesn't matter. There's nothing to win. So the real problem here is the simple counterargument, is there's nothing to win. You just lose, you don't get a type. This would work if we were talking about type two AIS.
00:38:10.306 - 00:38:37.502, Speaker A: So AIS that are safe if you're careful, if we get to this, and I think this is one of the things that Sam Alken would claim. He would claim, oh, no, we're going to build a type two AI. Don't worry, we're not going to do those type one ones. Don't worry about that. And I would strongly disagree. I don't think that that is supported by our level of scientific progress on the alignment problem whatsoever. But if we were super on track to get like a type two, three or four AI, then I'm like, okay, fine, fair enough.
00:38:37.502 - 00:39:02.906, Speaker A: That's a reasonable thing to believe. I would still be more careful than that because I would never be that certain. I wouldn't want to risk it. Even if I believe we're probably going to get a type two or three. I wouldn't want to risk all of humanity on it. I would take some time. But this is kind of the situation we're in now is that of course the people who are the most optimistic will be the ones who end up in the position to push this kind of race forward.
00:39:02.906 - 00:39:19.182, Speaker A: It's not a coincidence. And they're going to be the ones who get billions of dollars of investment. This is not a coincidence. It's not a coincidence know Sam Altman is the head of OpenAI and not Eliezer. Eliezer is not the kind of guy to lead an OpenAI. Of course not. That's not what he would do.
00:39:19.182 - 00:39:23.586, Speaker A: And so that's exactly the kind of scenario where we're in right now.
00:39:23.688 - 00:39:37.902, Speaker B: Who would you say are the main players in this race? Sam altman certainly of OpenAI. China as a techno country as a whole. Who's in the race?
00:39:38.046 - 00:40:12.486, Speaker A: So just to say a word on China I don't want to go too much into this, but there is a meme that exists which I would like to dispel of. Like China is this massive rival here. They're going to overtake us, they're going to build AGI or whatever. I think this is really ludicrous for anyone who actually knows about China. What the Chinese Communist Party wants is stability and to stay in power more of anything. Do you think they want a crazy uncontrollable technology that could topple governments? No. And China has been very clear multiple times that they're willing to take massive economic burdens to censor and stamp out their own tech industry.
00:40:12.486 - 00:40:42.890, Speaker A: They've done this multiple times. Of all the countries that I think is most likely to regulate AGI away, it's China. This is completely in their interest. It is not in the Chinese Communist Party's interest whatsoever for AGI to exist. It is completely counter to their so. And also they're very far behind technologically. But this is just like a common way to say I think a lot of people are perhaps perversely benefiting from holding up China as a boogeyman.
00:40:42.890 - 00:41:31.002, Speaker A: But the truth is basically 100% of the risk comes exclusively from the United States of America. There is no, in my opinion, appreciable risk from non Western countries whatsoever. Maybe this will change 20 years or 50 years or something. But at the current point in time all these models, all these things are being built on US soil basically exclusively. Even model like there's a recent model called Falcon which was created by the UAE, but it was trained in US data centers. This was done on US hardware, on US data centers, as far as I'm aware, US programmers that were working with the UAE on these kind of things. So this is very much an American and partially UK European issue.
00:41:31.002 - 00:42:06.530, Speaker A: But that being said so players, the main players geopolitically it's the USA, I mean to some degree the UK and the EU a little bit as well, but not deeply so. A little bit and non trivially so, but not really. It's the USA and within the USA it's generally a small number of private companies. This is DeepMind OpenAI slash Microsoft and Anthropic. These are the main ones. There are other people who are trying to catch up. There's a bunch of other startups trying to race forward to that kind of technology but they're all very very far behind.
00:42:06.530 - 00:42:43.822, Speaker A: And most of the other technologies groups in these field are not nearly as ideologically devoted to superintelligence as these three companies are. The founders of all of these companies have been very clear and publicly so that their interest is godlike super intelligence. They want to build systems that can reshape all of humanity, that can upload us all into the cloud, that can turn the whole world into nanobots. This is not me saying this. This is what actually these people have been quite publicly about. This is what they're trying to build. They're not trying to build a better chat bot, they're not trying to make the most shareholder return.
00:42:43.822 - 00:43:41.890, Speaker A: They're trying to do is to build godlike super intelligence and then unleash it upon the world which they believe will bring utopia. And so this know people like Sam Altman, people like Demis Asabis, Dario Amade from Anthropic, these are some of these CEOs of these various companies. And again I would like to state I'm not saying these are bad people, I really don't want to say this. I've talked to all of these people and they're for the most part really pretty like they're really pretty nice and very smart and hardworking and mostly trying to do what they think is right, some more than others. But it should be clear what they are and what they are as transhumanists. They have an ideological interest in doing this. They have strong incentives to be very optimistic and not think too hard about the dangers or to find excuses for why well, it's a race, my hands are tied, nothing I can do about it.
00:43:41.890 - 00:44:05.738, Speaker A: And I understand, I don't want to criticize and say these are like evil people, there are evil people involved in this system maybe. But for the most part these are people of certain beliefs, who have certain incentives, who are trying to make the world a better place. But I from my perspective find what they're doing unimaginably reckless and cannot continue.
00:44:05.904 - 00:44:51.250, Speaker B: Yeah, I'd like to unpack a little bit more around that just because I can understand where the disposition of all of these founders are. You said that the tech industry is more or less in denial about this, at least in comparison to the outside world. And you've also said that people like Sam Altman and maybe the other founders as well have said that yes, AI is perhaps the largest existential threat that we have and yet they continue. And so I'm wondering if you can just diagnose that is Sam Altman and all these other founders. They're just, oh, we're in a race, my hands are tied. I guess I'll just keep on racing until some sort of external force stops me. How would you actually define why the disposition of these people are the way that they are?
00:44:51.420 - 00:45:09.358, Speaker A: The truth is, of course, that I do not know. And this is basically psychoanalysis. I could do psychoanalysis. I know some of these people at least a little bit. I have studied pretty extensively what they do and why they do it. I've read everything they've written and so on. I have guesses about their internal lives.
00:45:09.358 - 00:45:50.438, Speaker A: The truth is I don't know their internal lives. Maybe some of them are total evil scheming bond villains. I don't think so. But maybe, I don't know, maybe some of them have really good reasons to believe that AI is super duper uber safe. I haven't gotten those reasons out of them even after talking to all of them many times and pressuring them on this. So I don't know what their emotional motivation is, but what I can describe is their stated opinions, their stated beliefs, and their stated actions. My usual thing I would say, and I recommend this to everyone, I think this is a really, really important skill that a lot of people neglect is watch the hands, not the mouth.
00:45:50.438 - 00:46:40.474, Speaker A: These people all say very, very nice things that make you feel very, very safe and then their hands do something very different. This is a very consistent feature I found across dealing with I mean, not just these generally powerful people, politicians, CEOs, billionaires. Watch the hands, not the mouth. If you're very smart, it's very easy to come up with explanations why it is actually good to do the thing that you wanted to do anyways. And truth of the matter is you can go back to archives sometimes from like the 90s, from some of these people talking openly on email lists or on their blogs about how they want to build AGI, how they want to bring in the transhumanist future. They want to create this beautiful immortal world of transhumanist cyborgs or uploads or whatever. And they want to do it as fast as possible.
00:46:40.474 - 00:47:13.410, Speaker A: They want to save all this. Even Eliezer was at least in this camp and was in the Was accelerationist. He thought that building AGI was the best thing we could do and we should do it as fast as possible. He changed his mind, which is fantastic, and speaks to his character and his ability to think about things rationally and reasonably. That Ellie Geyser did change his mind when he was quite young. I think he was like 21 or something. But for other people, I feel like I think a lot of them have absorbed part of the arguments, but like, watch the hands, not the mouth.
00:47:13.410 - 00:47:40.800, Speaker A: I think a truly damning example of this is the effective altruist movement, which is deeply ingrained with basically all three of these organizations to various degrees. And look again, effective altruists for the most part are really well meaning, good hearted, smart people trying to do the right thing. Are there bad apples? Of course there are. No question about it. Are there weird culti dynamics? Of course there are. That exist in any large movement of this kind. Are there weird untones? Absolutely.
00:47:40.800 - 00:48:29.034, Speaker A: But most individuals are good people. The truth of the matter is that effective altruists and people like Eliezer are speaking out against these corporations for racing. Eliezer was one of the people who helped Dario found DeepMind. He was one of the people who introduced Dario and Shane Leg to Peter Thiel for initial funding. I don't know how much it helped them, but it was something I was thinking about at the time, how know push on OpenAI was founded by people, many of which were effective altruist or effective altruist adjacent. And Open Philanthropy, the largest funder of effective altruist goals, gave OpenAI a very large early grant. It is, I think, the second largest grant they've ever made to an AI organization.
00:48:29.034 - 00:49:10.954, Speaker A: I think it was $30 million roughly was given to OpenAI in the early days to make it a safe AGI lab. In fact, from what I hear, it was Dario Amode, later CEO of Anthropic, who suggested to Elon Musk to create OpenAI as a counterlab to DeepMind to do safety. And as you probably know, early OpenAI was also very open source focused. They changed their tune about that. And then later, after Sam got involved, a lot of drama there about Elon and Sam and such that I'm not privy to. I don't know exactly what happened, but some drama occurred later on. Dario led the project for GPT-2 and then the project for GPT-3.
00:49:10.954 - 00:50:03.360, Speaker A: He was the one pushing scaling laws. He was the one making these systems stronger and more powerful and pushing forward on this axis. And then after GPT-3, citing security concerns and disagreements with Sam and the direction of OpenAI, which seems completely plausible to me, I don't have a reason to doubt this per se. He and a bunch of other people, top people, are the GPT-3 authors and people left to found Anthropic. And what did they do at Anthropic? They built larger models and now they're racing. In the recent pitch deck that was leaked from Anthropic, they are talking about how they are going to build a ten x more powerful model than GPT four. There is a and to this day you will have people in the AI safety community vigorously defend Anthropic or even Open AI or DeepMind as these safety oriented no, this is good actually.
00:50:03.360 - 00:50:54.166, Speaker A: There are, you know, there's all these weird incentives going on. Like the president of Anthropic, Daniela is Dario's sister, is married to Holden Karnosky who was the CEO of Open Philanthropy. This is a great example of watch the hands. What has happened is that these people have stated very clearly that they think AI risk is real, this is a huge problem, et cetera, et cetera. But what has happened, what has actually occurred is that they have accelerated the race like no one else. And this is I'm talking about this publicly for one of the first times, I think, because I think this is so important to make this clear. What is going on here? I've gotten quite shunned by the EA community for a lot of these angry comments, but this is a common feature.
00:50:54.166 - 00:51:15.186, Speaker A: And the same thing with politicians. I've talked to many politicians. They come to me and I'd say them, oh, this is dangerous. And they say, Well, OpenAI told me that they tested their model and it was safe. And I'm like, man, okay, where do we start with so, like, none of this is surprising. It's corporations doing corporation things. It's Microsoft lobbyists doing their Microsoft lobbying thing.
00:51:15.186 - 00:52:10.530, Speaker A: I'm not mad. I feel like this is what any, I think, reasonable political analyst would have predicted about how the discourse on AI safety would go before saying, if I talked to an old school environmental activist who was there for the oil lobbying stuff and I asked them, how do you predict the current state of AI discourse is? He would probably make the same predictions for where it currently is, where, yeah, people are not on board with oil pollution. They think it's bad. But of course, companies have lawyers, they have lobbyists, and they have a lot of great excuses and they have a lot of great comments about like, oh, we should do self regulation. Let's just have the oil companies regulate themselves. Let's have the oil companies set the safety standards for their oil extraction, which is unironically what people are suggesting. We have people at OpenAI pushing for eval, for safety, which is a great incentive.
00:52:10.530 - 00:52:28.966, Speaker A: But this obviously has to be nonpartisan. This obviously has to be done by people that are neutral parties that's currently not the case. A lot of the people doing the evaluations are people who either work at one of these organizations or used to work at one of these organizations. And this is just kind of silly.
00:52:29.078 - 00:53:06.610, Speaker B: I'm kind of just getting the intuition that if we rerolled the dice on this universe and maybe we plopped out Sam Altman and OpenAI, something else would be there instead. And so we could go down the rabbit hole of just like, hey, let's talk about the personality disposition of each of these founders. But I think it's really just about the nature of the problem itself that it doesn't really matter. Like take out Google, take out Anthropic, take out OpenAI, and you'll just find three other companies that will take their place in the race. The race conditions are race conditions that expand beyond the current set of players. Do you agree with this intuition?
00:53:06.690 - 00:53:33.386, Speaker A: I'm more skeptical about that than I think you are. I think it is true in the limit, like, eventually someone will figure out AGI. But I think you're underestimating how much ideology is actually at work here. GPD Four was not a coldly calculated business decision. It was an ideological decision. GPD Four costs like $100 million or a billion dollars to build. It's not meant to make that much revenue.
00:53:33.386 - 00:53:48.070, Speaker A: It's like extremely expensive, extremely risky. It could have blown up at any time. This is not what a Goldman Sachs does. No. Goldman Sachs is going to build a GBD three or GBD four. No coldly rational organization like this is going to take risks like this.
00:53:48.220 - 00:53:53.302, Speaker B: Right. You're saying the rational economic actor would not have chosen to produce chat GPT for yeah.
00:53:53.356 - 00:54:14.846, Speaker A: After they have chat GPT. Sure, they might have liked it, but no rational actor would have let at least not now. Maybe when the cost comes down, ten more years of progress, maybe then. So I do think eventually it would have happened. But we do have people who are accelerating. It's not just that this is the one time where it happens. No, it could have happened in ten years.
00:54:14.846 - 00:54:50.138, Speaker A: It probably could have happened a few years earlier, but not many years earlier. I think maybe other people would have arosen. It's possible. I'm not saying it's impossible. We can't reroll history, really, but yeah, I don't like these arguments too much about like, oh, it's just incentive, it's just a race. Also because one of my maybe controversial beliefs is that I think that people have way more control over reality than they think they do. I think that actually the world is way more plastic and way more controllable than people think it is.
00:54:50.138 - 00:55:15.922, Speaker A: I think that individuals with high agency can get a lot more done than people think they can. I think that individual actors matter a lot. Individual great people or politicians or activists or whatever can make a huge difference. Actually, there's a saying, forgot who was from never underestimate the ability of a small group of dedicated people to change history. In fact, it's the only thing that ever has.
00:55:16.056 - 00:55:54.398, Speaker B: So what you're saying is that what I hear you saying is that because of the commitment to building chat GBD Four, even though it was economically just non rational, you're saying therefore it is ideological, as in therefore these people are motivated by something else, an ideology, maybe some sort of glory? I don't know. Maybe the idea of just like, creating the AGI is so attractive that they want to be the so that is a little bit more of a parnitious problem because that is harder to tinker around with external incentives. Correct?
00:55:54.484 - 00:56:14.420, Speaker A: Yeah. And to be clear, it's not just that. It's also that they literally state this on their own personal blog. Right? I am not doing just pure psychoanalysis analysis here. This is literal, actual statements you can actually read written by these people. And that will be. Confirmed by their friends and so on.
00:56:14.420 - 00:57:17.814, Speaker A: Not all of it, but these are not wild speculations that I'm pulling out of nowhere. Maybe they've changed their minds, maybe there's some subtlety to it. I'm not dismissing these possibilities by any degrees here, but I'm saying ignoring that there is an ideological component I think is not correct. I think it is very reasonable there is an ideological or glory or an aesthetic preference in some sense. There is a, in retrospect, really rather chilling interview with Jeffrey Hinton, one of the Godfathers of AI from, I think, like 2015 or 20, I don't remember. And the article ends on him asking, well, if you think these things could be dangerous, why would you do this? And Hinton basically answers, well, sometimes the lure of discovery is just so sweet you can't resist. And this is very grimly prophetic in the sense that now Joffrey Hinton has disavowed his life's work and has now come out in favor of that actually.
00:57:17.814 - 00:57:56.338, Speaker A: This is an existential risk. This could kill everybody. I was in a lecture with him that he gave in Cambridge a few weeks ago, and he's a wonderful lecturer and it was a very lovely lecture, but it basically ended on this note of just like well, it turns out, yeah, probably, that this is yeah, this is going to kill everybody and I really don't know what to do about it. Yeah. Damn. It was really this weird note. Even Yashua Bengio, one of the other touring award winners, so one of the most senior, most respected people in the entire field of AI, has said that he feels lost after having come to realize just how it's regret his life's work.
00:57:56.338 - 00:58:31.054, Speaker A: Imagine that. Imagine one of the most senior professors, one of the most senior scientists to exist in this field, who has built this field, saying that he regrets his life work. Like, this is unprecedented. This is unprecedented basically in history. I mean, there's some cases, but this is a truly extraordinary scenario for something to be this clear. So I think a lot of it is just curiosity, beauty, fun, glory, et cetera. And you are correct, this makes purely rational incentives harder to control.
00:58:31.054 - 00:59:00.986, Speaker A: I think this is a part of it. If we try to model this as a purely capitalist problem, this is a purely money problem, I think we would not get the correct solution. I think this is again where we just have to be pragmatic. I'm not being judgmental, I'm just being like, okay, let's be pragmatic. What can we do? What do things need to? And like, the truth is just this is where government has to step in at this point. These people are not going to stop. They've had many opportunities too, and they will not do so.
00:59:00.986 - 00:59:02.570, Speaker A: Government just has to make them.
00:59:02.640 - 00:59:32.110, Speaker B: MetaMask has something new. Introducing MetaMask portfolio. MetaMask Portfolio is the best way to view your crypto portfolio from a holistic level, see everything across all the chains all at once. In your portfolio, MetaMask will report the aggregate value of all the assets in your MetaMask wallets and even the other wallets you import too. But MetaMask Portfolio isn't just a passive portfolio viewer. It is a place to do all of the money verbs that make DeFi so powerful. You can buy, swap, bridge and stake your crypto assets.
00:59:32.110 - 00:59:57.642, Speaker B: So not only is MetaMask the easiest place to see your wallets in aggregate, but it's also a powerful battlestation for all of your DeFi moves. So go check out your MetaMask portfolio because it's waiting for you to open it up. Check it out at portfolio MetaMask IO you know Uniswap, it's the world's largest decentralized exchange, with over $1.4 trillion in trading volume. You know this because we talk about it endlessly on Bakelist. It's uniswap. But Uniswap is becoming so much more.
00:59:57.642 - 01:00:44.102, Speaker B: Uniswap Labs just released the Uniswap Mobile wallet for iOS, the newest, easiest way to trade tokens on the go. With a Uniswap wallet, you can easily create or import a new wallet, buy crypto on any available exchange with your debit card with extremely low fiat onramp fees, and you can seamlessly swap on main net, polygon, Arbitrum and optimism. On the Uniswap Mobile wallet, you can store and display your beautiful NFTs, and you can also explore Web Three with the inapp search features, market leaderboards and price jars. Or use Wallet Connect to connect to any web. Three application. So you can now go directly to DFI with the Uniswap Mobile Wallet safe, simple custody from the most trusted team in DFI. Download the Uniswap wallet today on iOS, there is a link in the show notes arbitram One is pioneering the world of secure Ethereum scalability and is continuing to accelerate the Web Three landscape.
01:00:44.102 - 01:01:25.122, Speaker B: Hundreds of projects have already deployed on Arbitrum One, producing flourishing DFI and NFT ecosystems. With a recent addition of Arbitrum Nova. Gaming and social DApps like Reddit are also now calling Arbitrum home. Both Arbitrum One and Nova leverage the security and decentralization of Ethereum and provide a builder experience that's intuitive, familiar and fully EVM compatible. On Arbitrum, both builders and users will experience faster transaction speeds with significantly lower gas fees. With Arbitrum's recent migration to Arbitrum Nitro, it's also now ten times faster than before. Visit Arbitrum IO where you can join the community, dive into the developer docs, bridge your assets, and start building your first DAP with Arbitrum.
01:01:25.122 - 01:02:03.794, Speaker B: Experience Web Three development the way it was meant to be secure, fast, cheap and friction free. I was going to ask you, what are the next steps here? Because I resonated with the quote that you said where the man feels lost. Because at the end of any sort of AI podcast, and again, our AI podcasts are primarily alignment and safety podcasts I feel lost. I feel a little bit hopeless. And so I think what you just suggested is, like the easiest thing, the lowest hanging fruit that you see possible, which is that governments have to step in. What does that look like to you? How do we get that process started? What are your ideas here?
01:02:03.912 - 01:02:29.674, Speaker A: Yeah, and so again, I want to call back to the beginning of this podcast and just be like, this is not about government, good or bad. I don't want to talk about you're. From a crypto background, I assume many of your viewers are quite skeptical of the government. I think there's very good reasons to be very skeptical about general distaste. Yeah, totally understood. I run a business. Nothing makes you a libertarian faster than founding a business and noticing all the red tape you have to go through.
01:02:29.674 - 01:02:43.326, Speaker A: I understand. I understand. But this is not about good or bad. It's not about an ethics judgment. This is not about, oh, they've mess up housing policy spoiler. Yes. But it's way more about, okay, let's be practical here.
01:02:43.326 - 01:03:18.022, Speaker A: What are the options? What can we do? So this is the kind of thing the government exists for. If oil companies are polluting, they're just putting poison into the river or whatever, and you ask them nicely, they don't care, and they keep doing it. Maybe they cite some reasons about acceleration or races or whatever. Then you send police officers to make them stop. They give them a polite knock and a polite letter, and you tell them, look, knock this off or you're in deep trouble. So this is going to have to happen. This is not a solution.
01:03:18.022 - 01:03:40.674, Speaker A: To be clear, I want to be very clear. None of this is a solution. This buys you time. This doesn't solve the whole problem of technology and the future of mankind. No one would claim that. No one would claim that us shutting down some of these dangerous experiments is going to solve us, but it buys us time. We can talk about this in a second.
01:03:40.674 - 01:04:24.400, Speaker A: But the longer story, of course, is that if we really want to solve this, if you want a good future for humanity, we have to do a lot more than this. We have to do a lot more than this. We're going to have to solve a lot of technical problems and a lot of political problems and a lot of philosophical problems if we want to get there. And it's unfortunately not optional. But the first, most clearest step, from my perspective is that to be blunt, I don't currently see any good timeline in which there is not a pause. If we don't buy more time, if we just continue accelerating, if we're just pushing as hard as possible, as fast as possible with all the resources our economy can muster, we're not going to make it. Type one system is going to pop out, and that's just going to be it.
01:04:24.400 - 01:05:17.650, Speaker A: And we're not even going to make it to a type two system if we make it to a type two system, metas can immediately make it open source and then we die that way. But that aside, I don't even think we're going to get to type two systems. So the government needs to step in and slow these things down. Luckily, there are actually very good levers that the government does have access to in current legal frameworks that can be used with quite small impacts on the wider economy. There is luckily we are in a scenario where really 99% of AI is completely fine. Love medical research AI, or using particle physics or doing picture generation stuff, even though that has some copyright issues. Okay, let's not open that can of worms.
01:05:17.650 - 01:05:51.360, Speaker A: But 99% of AI is great. It's progress, it's technology, it will provide great benefits to society. Fantastic. There will be ups and downsides, but the blast radius is contained. 99% of AI things has a small blast radius. Not necessarily small, not zero, not saying zero, but the kinds of blast radiuses that our society knows how to deal with, like we can handle the Internet had a huge blast radius, we're still here, we can handle the internet. And so we can handle 99% of AI just fine.
01:05:51.360 - 01:06:33.414, Speaker A: There's only really a teeny tiny percentage of these hard, huge GPT, four GP, four plus large language models, general purpose reasoning agents, RL, like far end stuff where basically all of the risk comes from. All of the existential risk comes from. Not that there's no risk from other things. There are other risks, but it's not existential. The existential risk is really focused on this teeny, teeny tiny subset of three, maybe companies in the United States, that's kind of it. And the government totally can just stop that. We don't even need international treaties.
01:06:33.414 - 01:07:11.958, Speaker A: The United States government can just unilaterally tomorrow, put out an executive order and just put an end to this. It's that easy. And this exact suggestion I would give to the US government is to simply ban or require strict oversight and regulation of any individual training run which takes more than ten to the power of 24 floating point operations. This is a unit of measurement for the amount of computing power used. And luckily this is very easy to measure. We know GPUs have this and this much computing power run for this and this long. You can calculate how much computing power is going to run.
01:07:11.958 - 01:07:29.990, Speaker A: This is very easy to measure. It's very clear. There's very few companies that have access to computers this big. They are known to the US government. They are easily tracked down and they know settled in the US. These are large cloud providers, microsoft, Google, Amazon, et cetera. And they will comply.
01:07:29.990 - 01:07:47.022, Speaker A: Of course they will. They're not going to risk their entire multi billion dollar business for, you know, eccentric clients. So if the US government tells Amazon no more large training runs, no more things above ten to the power 24, I expect Amazon will just instantly comply.
01:07:47.166 - 01:08:13.340, Speaker B: And that seems actually to be kind of a long term solution with my limited understanding of AI, constraining how powerful our computation can be, constrains how powerful the AIS can get universally across the board. And so while that law would be in effect, so long as no one's violating that law, we have as much time as we need. Is that correct? That's my intuition, fortunately no.
01:08:13.710 - 01:08:23.370, Speaker A: I wish though. Yeah, indeed. The truth is that we are not on exponential. We're on a double exponential. There's two exponential.
01:08:23.450 - 01:08:24.842, Speaker B: Is it hardware plus software?
01:08:24.906 - 01:09:12.950, Speaker A: Yes, that's correct. We are both on exponential in hardware and on exponential in software. If we can cap the exponential on hardware again, if people want access to their small AIS at home, they want to do stable diffusion, completely fine. Maybe the government gets involved for copyright reasons or something, but from my extension risk perspective, I'm quite fine with that, but just not these frontier runs. So for calibration, ten to the 23 is roughly how much to estimate GBT three used, GPT four probably ten to the 24 or ten to the 25. And we expect that GPT five will probably take another factor of ten on top of that. And so we cap at ten to the 24.
01:09:12.950 - 01:09:42.406, Speaker A: I personally am confident that algorithms will improve, that you could still build existential risk in that regard. I actually expect it's possible. Ten to the power of 23, it's probably powerful, capable of even less than that. It's probably possible, I expect. To do that you need algorithms that we don't currently have. I don't think currently you can make GPT-3 existentially dangerous. I think it's currently possible, but I'm not confident it's going to be the true in ten years or in 20 or in 30.
01:09:42.406 - 01:10:04.106, Speaker A: We will develop better algorithms for bootstrapping, for more sample efficient learning, for better RL planning things for bootstrapping. And there'll be all these things that we will discover and that we will continue to improve. There's lots of low hanging fruit here still, but this does buy us time. This does buy us a lot of time. This buys us a lot of time where we can do it cuts down.
01:10:04.128 - 01:10:05.326, Speaker B: The acceleration by half.
01:10:05.428 - 01:10:36.454, Speaker A: Exactly. It makes a huge difference. It's not going to save us, but it makes a huge difference. It gives us the time to do safety research, to build gliders, to build rockets. It gives us time to actually study these systems, actually understand, to integrate them into society, to figure out what is the right regulation. How do we want to integrate these things into society? A thing that OpenAI likes to say is like oh, we think the safest thing is to do iterative deployment so they can integrate into society, which is code word for lull. We release it immediately and just throw it on the market.
01:10:36.454 - 01:10:59.422, Speaker A: If OpenAI had built GPT-3 and then completely stopped and done only state GPT-3 until they fully understood it. They had full safety control of it, and all society had totally integrated it culturally. And we had regulation that totally handles it and we're all comfortable with it. And then they had produced GP four. Yeah. You know what? Fine. That's fair.
01:10:59.422 - 01:11:16.850, Speaker A: Honestly. Totally fair. If this was the way our society handled AI, fantastic. I think this is great. I want technological progress too, of course. I just want it to go well. And this is not the shape of a technological path that goes well.
01:11:16.850 - 01:11:17.470, Speaker A: Yes.
01:11:17.560 - 01:11:37.530, Speaker B: What you're saying is that instead of testing chat GPT-3 or four inside of a contained environment in the lab so that we can fully unpack it and understand it, OpenAI is just yeeting it out into the world and using the world as the test bed, not the contained environment as the test bed.
01:11:37.600 - 01:11:46.670, Speaker A: Even worse than that, never mind the contained environment. They're yeeting out the one thing and then yeeting out the second one before we've even recovered from whiplash from the first one.
01:11:46.740 - 01:12:34.602, Speaker B: Yeah. And so just to really unpack these two exponential curves, we have the hardware curve and the software curve. The hardware curve is the thing that you're saying that we can easily and objectively place a cap on, a government enforced cap on that curve with that specific certain to the power of number that we can totally measure. And so that seems like a solved problem. If we can actually get the governments to do that, then there's the other equation, which is like, we are still in the early days of these AI models in the first place. And so even if we do cap the hardware, we still have at least a handful of orders of magnitudes efficiency in the algorithms that have access to that hardware. And so we have like, Nvidia, the GPU supplier on one side.
01:12:34.602 - 01:12:43.714, Speaker B: Maybe there's others in the game as well. I don't know how relevant AMD is in the world of AI, but Nvidia is the big player that I think most people know on one side, supplying GPUs into the market.
01:12:43.912 - 01:12:45.106, Speaker A: And then we have the consumers of.
01:12:45.128 - 01:13:13.498, Speaker B: Those GPUs, which are the people racing right, the open AIS. And so in order to have a complete containment of the problem, there needs to be control on both sectors. And you're saying the hardware side of things is relatively objective, isn't easy, and we can prescribe action steps for governments to take those steps. And I'm assuming on the software side of things, it's a little bit more gray and we don't know how to proceed that well. Is my intuition correct here?
01:13:13.584 - 01:13:36.706, Speaker A: Yeah, that's definitely the case. I mean, easy is a strong word on the hardware side. I have talked to many people in policy positions, and many of them are like, this is literally impossible. You are insane. Other people, though, have said, oh, no, this is totally something the government can do, especially using national defense arguments. We already have a chip band where we don't export H, China and stuff like this. We already track this.
01:13:36.706 - 01:14:04.662, Speaker A: The US government totally has the affordances and has the capabilities and the incentives to do this. This is totally within their wheelhouse. This is not some crazy new thing that has to be invented. But getting the government to do anything is hell. So this is a massive undertaking. That being said, so software is much more complicated. I don't think we can or should try to ban mathematics.
01:14:04.662 - 01:14:36.546, Speaker A: That seems insane. I don't think this is a kind of level of coordination that humanity is capable of. Maybe if we're some kind of like super smart enlightened aliens, we could maybe all handshake on it. But I still don't think humanity can really work that way. But there are things that can be done here. There are actual, straightforward, very easy and clear things that can be done here, actually, which is, again, doesn't fix everything, but it does buy time. So in particular, the reason I think there are such easy wins here is because things are so bad.
01:14:36.546 - 01:15:02.538, Speaker A: Because things are so bad, no one's even trying to control these things. There's a lot of low hanging fruit. So one of the very obvious things, which is not again, I'm not saying it's easy, but I think this would make a difference, is I think there should be strict liability for not just model deployers, but also model developers. If you develop an AI system and someone uses it to commit a crime, you should be charged as if you committed the crime yourself. You should be liable.
01:15:02.634 - 01:15:17.230, Speaker B: And strict liability let's take a moment to unpack that. Strict liability is when you the individual. You cannot give your liability to an LLC. Strict liability is like yo you the individual that falls back on you.
01:15:17.300 - 01:15:17.582, Speaker A: Yes.
01:15:17.636 - 01:15:19.842, Speaker B: Just want to define that term before we move on too far.
01:15:19.896 - 01:15:42.098, Speaker A: Yes, that's correct. I think this would be the ideal case. Again, I have had a common section I'm sure is yelling at me right now. I have heard from plenty of people of policy people telling me how this is impossible. But I have also heard from very senior people that this is possible. Basically, the number one step is removing Section 230 protections from AI systems. This is already being discussed by senators in the Senate.
01:15:42.098 - 01:16:31.930, Speaker A: So Section 230 is the Internet neutrality thing, where if you host content, which is illegal, but you didn't know about it, and you remove it, you're not liable for it. If your users were doing it and you had no hands in it. One of the obvious things just do not apply Section 230 to AI. Simply, if you develop a model or a system, an AI system which causes actual criminal harm, you are criminally liable for it personally. This is currently not the case. Currently there's a again, look, I love open source, I love my nerd friends, but it's insane is that some nerd somewhere in a university can develop a voice cloning system, take 15 2nd audio of anyone perfilt clone their voice and use it to scam their parents, to call in bomb threats, whatever, to totally ruin their lives. And the person who developed that model, who posted it to GitHub, has zero liability.
01:16:31.930 - 01:17:09.430, Speaker A: It's not only that they don't have any legal liability, they don't even feel responsible. The people who do this feel no shame. It's not part of open source culture to even feel very responsible for this kind of thing. It's like, oh, you developed a cool thing, throw it out there, it's fine, let the lawyers figure out how society thing works. Not your problem. I had an actual conversation, it's a real conversation I have with some tech people and I was talking about how I think there should be straight liability, these kind of things and then one of the people sincerely made the point well, no, that's impossible, we can't do that. How would they possibly ever determine who is at fault? This couldn't be done.
01:17:09.430 - 01:18:13.546, Speaker A: And then a friend of mine was like courts, that is what courts are for. How do you not know what courts are? So there's a huge disconnect between a lot of the tech sector and how civil society actually functions, the infrastructure of civil, actual functioning government and societies and courts and so on. We have dealt with things that enable criminal activity in the past and another great example is automobiles. So automobiles there was a huge fight, I think it was in the 70s, if I remember correctly I might get the dates wrong, I'm bad with remembering exact numbers. There is a huge fight, legal battle, where automakers were arguing that they were not responsible for any deaths that occurred due to cars and including if their cars malfunctioned. And a lot of people pushed back at that, they said well, no, if your car malfunctions, if the brakes don't work, if it catches fire or whatever, no, the maker of the car should be liable for that, not the driver. It's not his fault that the car caught fire.
01:18:13.546 - 01:18:46.686, Speaker A: And this was a massive legal battle. And nowadays we do hold car companies liable. If you build a car and it explodes and it kills somebody yeah, your responsibility. You build the car, you put the car out there, of course you're responsible for this. And of course there is a spectrum here, there's a large spectrum. I'm not saying I know where on the spectrum is the right thing. Should everyone who developed Linux be held personally responsible if someone uses Linux for something bad? I think no, but it's a spectrum.
01:18:46.686 - 01:19:26.762, Speaker A: It's not a yes or no. It's not everyone is held responsible or no one ever. It's a spectrum and this is how laws work. This is how courts work. We as a society have to find where on the spectrum are we comfortable? And I'm simply stating I think we're way on the one side and we should move more towards the middle where there is actual accountability. That same conversation I had with this person was very funny as a different person, actually, but same conversation when I talked about strict liability, someone called me out and they're like, okay, Connor, well, you developed open source technology. Do you want to be held accountable if someone uses your language models? You developed a Luther AI to cause harm.
01:19:26.762 - 01:19:40.726, Speaker A: And my answer is yes. Yes. I would like to live in a society where I, as a technologist, if I cause harm to civil society, if I cause harm to other people, that I am held accountable to this. I think this is good. I think this is the kind of society I would like to live in.
01:19:40.828 - 01:20:45.434, Speaker B: I can see a large number of the crypto people, which are 95% of this audience, recoiling at this. I'm sorry. Well, I want to give an anecdote from the crypto world, which I think is universally accepted as like we are in a legal battle with the United States Department of treasury because they deemed I don't know how familiar you are with this, but this smart contract on Ethereum called Tornado Cash, they deemed using that software uploaded to the Ethereum blockchain. Using that as an American citizen is illegal. And so I am personally actually suing Janet Yellen and the Department of treasury on that and saying somebody open source developer, they live in the Netherlands uploaded this piece of hardware to Ethereum, and I was able to use that as a tool, a neutral tool. I was achieving some sort of financial privacy because Ethereum is transparent and open and everyone can see my transactions. And I would like to have created some sort of private version of my wallet that no one used, that no one knew about.
01:20:45.434 - 01:21:34.378, Speaker B: So I used Tornado Cash in order to achieve that end simultaneously. So did North Korea. And so my money was right next to North Korea. And so the Department of treasury deemed this piece of software to be illegal to use by all Americans. And so a few of us in the crypto world banded together to sue the Department of treasury, saying, hey, you cannot make a neutral piece of software illegal just because some bad actors are doing bad things with it, including perhaps funding the development of nuclear weapons. And so I can see where but when you tell me this about AI, how else do we solve this AI alignment problem other than giving strict liability to the people that create the models that ultimately destroy the world? To me, I'm like, Well, I don't have a better solution. And so I don't know what else to do.
01:21:34.378 - 01:21:52.350, Speaker B: And so I'm torn at this. I see the crypto code libertarian side of me on one side of things and then the, hey, let's not die from AI on the other side of things. And I see the conflict here and I'm sure you have run into these conversations a number of think so.
01:21:52.420 - 01:22:08.262, Speaker A: Thank you for sharing that. That's a great illustration of, again, coming back to the beginning of the podcast. It's not about good and bad. It's about finding the right thing. I think you suing the US government is fantastic. I think you're doing a civil duty here. I think you're improving society for all of us by doing this.
01:22:08.262 - 01:22:25.398, Speaker A: Whatever comes out of this case, someone has to do it. This is how society progresses. This is the mechanisms by which our society finds consensus on what is the correct dial setting. It's not that the government magically knows the correct dial setting. No. This is why in the USA we do sue. And this is great.
01:22:25.398 - 01:22:31.886, Speaker A: This is good. It's a huge pain. I know this must be so much pain for you to do all this. Such a and that's why I'm like thanking you.
01:22:31.908 - 01:22:33.306, Speaker B: Shout out to the Coin Center lawyers.
01:22:33.418 - 01:22:48.354, Speaker A: Yeah, that's why I'm thanking you and these guys. I don't have a horse in this race. I don't know whether this is good or bad. I don't have a horse in this race. But I think it's good that this is going to court. I think this is good that someone is holding the position. This is good and someone is holding the position.
01:22:48.354 - 01:23:03.094, Speaker A: This is bad and this should be fought out. This should be debated. This shouldn't be taken for granted that things are obviously good or obviously bad. Again, beginning of this conversation, things aren't good or bad. It's about decisions. It's about consequences. I don't know, maybe the software being banned is net good, maybe.
01:23:03.094 - 01:23:28.400, Speaker A: I don't know. Maybe it's not like the way you describe it. I don't know. I really don't know. And I would love for people who understand this technology much better than me, people like you and people at the government who hopefully have a better understanding of national security or something to battle it out and actually hopefully find truth. And our courts are not perfect. I'm sure there's going to be a bunch of bullshit and whatever the verdict is, no one's going to be happy with it.
01:23:28.400 - 01:23:55.414, Speaker A: I understand. But this is how our society works. This is how we make progress. This is how we, as a democratic, liberal, judicial, rule of law country, make progress. And I'm saying that this is painful, but it just has to be done. It's a price of living in a complex society. It's a price of playing in a complex society with complex trade offs, with complex technologies, which I think crypto is.
01:23:55.414 - 01:24:15.310, Speaker A: I think crypto is an amazing technology that has incredibly complex benefits and downsides. There is blast radius. FTX was a huge blast radius. His massive blast radius, right? And. That doesn't mean crypto is bad. No, of course it doesn't. Just because they're scammers doesn't mean it's bad but it also doesn't mean it's good.
01:24:15.310 - 01:24:37.502, Speaker A: It's just a technology. It's neutral the same way AI is. It is not good or bad. It's about how do we as a society digest this technology? How do we get as much of the good stuff while preventing as much as the bad stuff. And this is simply not easy. This is all I'm saying. I'm not saying I have the perfect solution that solves all the problems forever.
01:24:37.502 - 01:25:02.474, Speaker A: Don't worry, just do this. I'm saying this is a hard process. This is like doing science. Like in a sense going to court is like doing science. You're doing law making science or engineering. You're doing the epistemological labor that needs to get done to actually get to the right kind of civilizational software that we want to be running because we don't know what the right software is. We don't know what the right laws are.
01:25:02.474 - 01:25:42.790, Speaker A: We don't know what the right liability or the right insurance schemes or I don't know is right. And we have to find out. Someone has to come up with them and they have to debate against people who come up with alternative things and we have to try things out and some of them blow up and then people sue and then we try another sage and so on. So I think this is a natural part of how a functioning society should work and sometimes this is a huge massive pain. I understand but I've talked before about, okay, humanity can't coordinate that well we couldn't coordinate but this is what coordination actually looks like. Coordination doesn't look like, oh we're all happy friends and we hold hands and we all sing songs. That's not what I mean when I use the word coordination.
01:25:42.790 - 01:26:29.926, Speaker A: Coordination I mean we sue each other. I mean we have debates, I mean we fight, we argue civilly, no violence. We're very civil about it. We have rules, we follow laws, we also follow laws that we don't agree with. There's a bunch of laws I don't agree with but I am willing to make compromises on this and I'm willing to compromise with the government and with my fellow citizens that I will like abide if I go to a country where maybe I don't agree with all their laws while I'm there not going to break them. And this is not just because of fear of retribution it's also because of contracts and just like making deals and being fair and changing processes. I think violence is extremely bad for this reasons because violence violates our contracts, our social contract is this is not how we solve disputes.
01:26:29.926 - 01:26:55.938, Speaker A: If you disagree with something you don't use violence, you sue. You have debates, you start a campaign, you talk about a podcast, whatever. We have mechanisms and we should use them and they're not great like in case you haven't seen Twitter lately. Our coordination mechanisms aren't great, they're not very good, but they're something and we can do better. We can coordinate. It is possible. It's not easy.
01:26:55.938 - 01:26:57.954, Speaker A: But again, at the beginning of this.
01:26:57.992 - 01:27:52.926, Speaker B: Conversation, when we were trying to unpack the different categories for how AI might come to destroy the world, you called this a deeply meta problem. You talked about how we need to solve philosophical problems, about what it means to be human. It's a problem that really goes to the heart of the human condition. And I've been talking to quite a large number of AI people in the last couple of months or so and it's been very interesting to me every time the topic of AI alignment and AI safety comes up, which is like almost all the time, how different approaches the conversation can take across the board. And it seems to be that there are so many different problems that need to be solved in order to solve the AI safety problem in the first place. I'm wondering if you could just unpack I'm pointing you in a direction here and seeing if I can just unleash you here. But it seems to be like the AI.
01:27:52.926 - 01:28:28.702, Speaker B: Alignment problem goes very deep as to what it means to be human and what it means to solve problems in the first place. And especially when we are approaching this technology that I think if we as humans just developed as a society over and over and over and over again, like we rerolled the dice of the human experiment, you arrive at AGI almost all the time, like 99.99% of the time. So it seems to be like, hey, humanity, we have arrived at what seems to be the largest problem that humanity has ever faced. Nuclear war, nuclear bombs. Big problem, not as big. Right.
01:28:28.702 - 01:28:51.590, Speaker B: Like disease big problem, not as big. This is like the big problem and it seems to be posing very big questions. And so, Connor, as we come to the end of this conversation, I'm wondering how to tie this off with just understanding the relationship between what it means to be human and how to solve AI. Alignment. What does this vector of conversation sound like to you?
01:28:51.740 - 01:29:26.398, Speaker A: Sounds like the real question, the ultimate question. AI. Alignment is an important field and sometimes its definitions get stretched to the point where it starts encompassing these larger and larger meta problems. I think this is not a coincidence. I think really a potential rephrasing of the alignment problem is a general question of how do you control a powerful system using a weaker system? This is something that is not unique to AI. This is a problem that exists in organizations, in multicellular organisms. Cancer is a form of misalignment.
01:29:26.398 - 01:30:24.178, Speaker A: Some of your cells start having different values from the rest of your cells and they start taking actions, maximizing actions, which are harmful to the rest of your organism. Our organism has alignment mechanisms. It has stuff like apoptosis or like cell suicide. That when there's mechanisms that if it detects that a cell is going down a path which might be dangerous, the cell kills itself. And sometimes these mechanisms don't work, and you get cancers. So this might be a bit of a torturous metaphor, but I do think there's actually a deeper truth here in that alignment and power and coordination are, in a sense, all facets of the same coin. They're all facets of the same problem of how do you organize chaos? How do you create stable systems? Equilibria? How do you resist entropy? How do you resist just the eternal cold noise of the universe? Because by default, that's all there is.
01:30:24.178 - 01:30:48.954, Speaker A: By default, there's only cold nothingness. That's what the universe is. It's its space. There's nothing else space and just random particles. But then things can cohere into complexity. They can cohere into they can form, whether by simple mechanisms such as gravity or chemical things to create planets and stars. And then we have higher and higher level things.
01:30:48.954 - 01:31:20.518, Speaker A: We start seeing complex molecules. Then we start seeing the first self replicators. We see RNA molecules and protocells. We see early bacteria and archaea. And then these start to coordinate among each other. A lot of theories is that the mitochondria in the cell actually was once a separate organism that got absorbed into our cells, into eukaryotic cells. And so there is a quote uncoordination happening there in a very abstract sense.
01:31:20.518 - 01:31:54.170, Speaker A: And then eventually these cells learn to organize at even larger scale, to organize into multicellular organisms. And then first very primitive, probably like sponges or like jellyfish or something, probably very simple organisms. But then stuff started to specialize. But to specialize, you have to coordinate better. You have to have better coordination mechanisms for a brain cell to live. It needs a lot of digestion cells to help it out and a lot of blood cells to help it out. And a lot of the amount of infrastructure that needs to be in place and coordinate it to support a human brain is ridiculous.
01:31:54.170 - 01:32:07.890, Speaker A: There's a massive, huge system, and everyone has to play their part. Some muscle cell can't just be like, screw it. I'm going to be a neuron. Now, if that happens, we call this cancer. This is a disease. This is not coordinated. It's not controlled.
01:32:07.890 - 01:33:05.090, Speaker A: Mike Levin has a lot of great work on this, on bioelectricity and cell identity and stuff like this. And then it goes further. Then we get to the meta level, to the you know, we go from the genes to the know, as Dawkins might say, where we go from purely genetic evolution or like multicellular evolution to memetic, evolution to cultural evolution. There becomes now it's about cultures and tribes and civilizations and companies and gods and religions and these kinds of things. And these can also coordinate and mutate and war and change. So there's this reoccurring motif of life, of these higher order patterns of complexity and coordination and stuff emerging. And the way for these things to emerge is through coordination.
01:33:05.090 - 01:34:14.334, Speaker A: It is through coherence of smaller patterns, cohering into larger emergent patterns on higher levels of scale. And this is also where we humans are. We're nestled here, somewhere in the hierarchy, somewhere above multicellular, somewhere below pure memetic religions or stories, somewhere in between. We're kind of like with 1ft we're in the realm of animals of the earth and 1ft we're in the realm of the gods. We're in the realm of mimetics, of information, of stories, of religions. We're kind of this interface between these two things. So what is the next step from here? Where do we go? What does it mean? What does it mean for us to want something? In a sense, if we build AI, this is pushing us further into the realm of memetics, of knowledge, of pure abstract software, of something that exists not as a brain, not as a specific piece of hardware, but as a purely as basically a spirit, just like an immaterial code.
01:34:14.334 - 01:34:53.290, Speaker A: It's the next step. It goes up, it goes from the transition from purely biological, purely physical, biological, then half memetic, half biological, to purely memetic. They can run on any substrate, whether there's a CPU that can run its code, the AI can copy itself and it can exist in this purely memetic realm. So what does this mean? Well, this means many things. It means could we mean fantastic things? Culture allows us to do fantastic thing. Our control over memetics, over culture, over technology, over science allows us to build this wonderful place we're in. Like, look at this.
01:34:53.290 - 01:35:20.750, Speaker A: We got internet and I got air conditioning. I can eat all the food I want. It's great. My animal is feeling my inner animal is feeling great about this whole civilization stuff. Seems pretty cool. But things can also go wrong. Blast radius, as we dabble in the powerful, as we dabble in magic, in the technology, as we dabble in this blast radius increases, power increases.
01:35:20.750 - 01:36:00.590, Speaker A: We need to get control over that thing. So this is all just a very nice poetic reframent of where we started this podcast. It's about how do we deal with the upper level. There's also the question of down level, the question of cancer control or like medicine is the question of how do we align the levels below us, or engineering or like physics are the questions of how do we align the levels below us with our values? Now we have the question, how do we line the thing above us with our values? Because we want some things. There's many things I like and there's many things I don't like. I don't like my friends to die. I don't like being in pain.
01:36:00.590 - 01:36:37.418, Speaker A: I would like a beautiful universe full of cosmopolitan art and adventure and all these nice things. And this is possible, but it's not something that an animal can do. It's not something me or you, as half animals could even do. It is something that God could do. It is something that a very, very powerful intelligence, a Memetic super creatures could do. It's the same way that the Memetic super creatures that we call civilization has built technology that you or me could never build by ourselves. And this is great, but what are our alignment mechanisms? Our cells have apoptosis, they have a bunch of stuff and even that's not perfect.
01:36:37.418 - 01:37:04.622, Speaker A: But now I'm saying, okay, medicine is great, physics is great. We're developing a lower level alignment text. We also need to develop the upper level alignment text. I think it is possible that we can understand Memetics and AI and optimization unless this is magic. That's the cool thing in a sense is that it is both magic and it's not magic. It is like information processing. It is things that can be understood.
01:37:04.622 - 01:37:33.446, Speaker A: It is coordination. Humans could solve many coordination problems. Me and you can coordinate me and all the other people down the street. We can all live together peacefully. This is coordination. This is controlling something, working as part of something bigger than myself. And if we want a really good outcome, if you really want the world to go well, if we really want our far descendants, whether they're human or not, to live a great life, but we have to not blow this all up.
01:37:33.446 - 01:38:22.138, Speaker A: We have to actually make sure that the next step we make is not just some random thing that someone cooked up on a server somewhere, but it's actually something we like, actually something we endorse, actually something we want. We don't want something to inherit the universe and just blow it up or use it for nothingness. We want the universe to be something that we like. And this is really the full problem of alignment. It's not just the question of, okay, how do we solve this narrow problem? How do we regulate this thing? It's how do we think about what does it mean for humans to want things? What do we want? A lot of our preferences are culturally constructed. A lot of people want things because their friends want it or because they saw it on TV. Is that a real preference or is that a fake preference? I don't know.
01:38:22.138 - 01:39:24.546, Speaker A: This is something we should be asking what does it mean to be good? Is that even a well coherent concept? If we can't agree in any of this, is there some way we can bargain? Is there some kind of harsani's veil type solution where we build some super aligned type four system, it goes to all the humans, figures out what they all want and makes some kind of like fair distribution? I don't know. What we really want is these type four system, type four aligned systems that are so aligned that they figure it out for us. They're like, I'm going to figure out all humans. I'm going to understand their psychology, their traumas, their life story. I'm going to deeply, deeply understand them in ways that no human could understand them. I will use all this incredible superhuman intelligence in order not to do something random, but to do what these poor half animals actually would like, what would actually make them happy, what would actually be good for them. And I have no idea how to build such a thing.
01:39:24.546 - 01:39:56.380, Speaker A: I mean, I have a few ideas, but I don't know. This system is not forbidden by physics. There's no reason that such a system cannot be constructed. But we are so far away from that. We're not even going to get to there. If we can even get to the point where we can just have systems that we can carefully use, we coordinate on, and we use them to make cool world, where people are happy, where people don't have to fear for the next day, where people aren't hungry, where people can just chill. That would be pretty nice.
01:39:56.380 - 01:40:19.150, Speaker A: We can get there. It's technically possible, but it requires not just technology. It's like technology is not free. Progress is not free. If you just take the shortest path on the tech tree, you don't get to the final tech. You blow yourself up on the way. There is a final tech somewhere deep, deep, deep down the tree.
01:40:19.150 - 01:41:12.754, Speaker A: And if we're very careful about how we traverse this, if we improve our coordination tech, if we work carefully about this and we avoid existential, the traps, the traps along the tree, we can get that's. You know, I think what Eliezer as such would mean is truly solving alignment. That final tech that we're, like, we have understood. What are values? What does humanity mean? How do we bargain between values? How do we think about reality? How do we think about meaning? How do we think about all these things? And not just that, but never mind the philosophy here. This is not just about talking. This is how do we turn this into code? How do we turn this? How do we build this? A thing that I like to think about philosophy, that I think is often lost in modern philosophy is that good philosophy should make you stronger. Philosophy should solve problems.
01:41:12.754 - 01:41:59.806, Speaker A: If your philosophy isn't ultimately going to solve problem, why are you doing it? If you really do philosophy, if you really go to the end of philosophy, you should come out on the other side with something that allows you to do these things, that allows you to build these systems, to create this great future, to make the universe good, to make it great. And I don't know what that means. I'm not claiming that I do. Oh, God, no. I barely have a few technical ideas about how we can get to type two systems a little bit of an idea, but even that, it's going to be tough. All of us, humanity, technology, our politics, everything. It's going to be heroic.
01:41:59.806 - 01:42:05.314, Speaker A: It's going to be a heroic task to get to that point. But hopefully we just have to be heroic one more time.
01:42:05.352 - 01:42:26.266, Speaker B: Connor I really appreciated all of that and that was a very cerebral and deep conversation. If Bankless listeners are out there and they want to continue this conversation, where should we send them? I know that your company, Conjecture has a fantastic blog. I can definitely put those links in the show notes. Is there any other place where people who want to continue this conversation might.
01:42:26.288 - 01:42:52.740, Speaker A: Want to end up? So our website is a bit not super up to date, but thank you for your kind words anyways. I'm on Twitter at MP collapse. I'm not super reactive, but I exist there. I sometimes hang around Aluthar AI's discord. Still not super common, but if you ping me, there's a decent chance I'll respond at some point. I go on a lot of podcasts. I do a lot of stuff like this.
01:42:52.740 - 01:43:21.050, Speaker A: If you're interested in learning more about some of the regulation things I'm talking about, there is a website we have made called Stop AI. There's an asterisk after the AI is that we're not trying to stop all AI, just a specific type of AI. There's a lot of content on there about fleshing out some of these things. Yeah. And in general, I'm around and this is not the last conversation I think that will be had on this topic.
01:43:21.390 - 01:43:40.042, Speaker B: I definitely think that's correct. Bankless nation, you know the deal. Crypto is risky, ETH is risky, AI alignment is risky, and apparently we need a heroic effort to solve that problem. But nonetheless, you can lose what you put in. We are headed west. This is the frontier. It's not for everyone, but we are glad you are with us on the Bankless journey.
01:43:40.042 - 01:44:01.090, Speaker B: Thanks a lot, Sam.
