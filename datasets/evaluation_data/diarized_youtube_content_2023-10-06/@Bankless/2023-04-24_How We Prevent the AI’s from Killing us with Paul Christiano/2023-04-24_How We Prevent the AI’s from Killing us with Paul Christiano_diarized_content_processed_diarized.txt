00:00:00.090 - 00:00:12.800, Speaker A: The most likely way we die involves, like, not AI. Comes out of the blue and kills everyone, but involves, we have deployed a lot of AI everywhere, and you can kind of just look and be like, oh, yeah, if for if for some reason, God forbid, all these AI systems were trying to kill us, they would definitely kill us.
00:00:14.850 - 00:00:53.886, Speaker B: Welcome to Bankless, where we explore Frontier Technologies artificial intelligence on the episode today, this is how to get started, how to get better, and how to front run the opportunity. This is Ryan Sean Adams, I'm here with David Hoffman and we're here to help you become more Bankless guys. We have a special guest on the episode today, paul Cristiano. This is who Elizar Yudkowski told us to go talk to, someone he respects on the AI debate. So we picked his brain. This is an AI safety alignment researcher. We asked the question how we stop the AIS from killing us? Can we prevent the AI takeover that others are very concerned about? There are three, actually four takeaways for you today.
00:00:53.886 - 00:01:22.498, Speaker B: Number one, how big is the AI alignment problem? We ask Paul this question. Number two, how hard is it to actually solve this problem? Number three, what are the ways we solve it, the technical ways? Can we coordinate around this to solve it? And finally, we talk about a possible optimistic scenario where we live in harmony with the AIS and they improve our lives, make it quite a bit better. David, what was the significance of this episode to you in our series?
00:01:22.674 - 00:01:42.598, Speaker C: Yeah, the new intro I think is great when we cover Frontier Technologies and also instead of for this episode, helping you become more Bankless. I think this is truly a front running the opportunity on this one. We are trying to help humanity not die. We're trying to help you not die.
00:01:42.774 - 00:01:43.402, Speaker A: All of us.
00:01:43.456 - 00:01:43.942, Speaker B: Not die.
00:01:44.006 - 00:01:44.522, Speaker A: All of us?
00:01:44.576 - 00:02:26.346, Speaker C: Yeah, you and the rest of the world. And I think we're doing this in the best the way that we can, which is education and awareness about this AI problem. Paul Cristiano is, like you said, the man that was recommended by Eliezer about the man approaching this problem head on in a technical way. So in this podcast, you are going to hear about the technical solutions that people are actively working on who are taking this problem extremely seriously and take the risks very, very seriously. Eliezer gave us more or less a doomsday scenario, like a 99% chance of doom. Paul Cristiano only gives us a ten to 20% case of doom scenario. So much more optimistic like those.
00:02:26.346 - 00:02:59.022, Speaker C: ODS. Much better ODS. And so we go through why he's still very much concerned and he does consider this the most likely way in which he dies in the future. Yet why there's still an 80% chance of success. And some of that 80% chance of success actually does have utopia in it. I think maybe Ryan, in 3510 years, we're going to be able to look back at this episode as hopefully if Paul's right. And I think he's right, like, ahead of its time in terms of elevating extremely important conversations.
00:02:59.022 - 00:03:16.106, Speaker C: To the best of our ability into the mainstream so we can get more people to focus on the actual solution paths to make sure that that 20% risk of doomsday goes down to 0.2% risk of doomsday. And I see that's what the significance of this episode has and why we are doing episodes like this.
00:03:16.128 - 00:03:30.910, Speaker B: Yeah, I mean, to be clear, Paul really thinks this is AI alignment is a solvable problem, which is much different than others in the space, and he tells us exactly why. And my takeaway from the Eliezer episode was humanity is screwed. This was humanity is screwed. But we're working on it.
00:03:30.980 - 00:03:34.378, Speaker C: We might have solutions, have clear actionable paths.
00:03:34.474 - 00:04:10.122, Speaker B: That's right. So we get into all of that today. And David, of course, I want to discuss this episode in the Debrief with you and hear what you think because this is our third in a series of AI episodes and really interesting material. The Debrief episode is our episode that we record directly after the episode with our raw, unfiltered thoughts. If you are a Bankless citizen, you have access to that right now on the premium RSS feed. You can click a link in the show notes and get access to that. Okay, guys, we're going to get right to the episode with Paul, but before we do, we want to thank the sponsors that made this episode possible, including Kraken, our recommended crypto exchange for 2023.
00:04:10.176 - 00:04:59.466, Speaker C: Kraken has been a leader in the crypto industry for the last twelve years. Dedicated to accelerating the global adoption of crypto, kraken puts an emphasis on security, transparency and client support, which is why over 9 million clients have come to love Kraken's products. Whether you're a beginner or a pro, the Kraken UX is simple, intuitive and frictionless, making the Kraken App a great place for all to get involved and learn about crypto. For those with experience, the redesigned Kraken Pro app and Web experience is completely customizable to your trading needs, integrating key trading features into one seamless interface. Kraken has a 24 7365 client support team that is globally recognized. Kraken support is available wherever, whenever you need them by phone, chat or email. And for all of you Nfters out there, the brand new Kraken NFT beta platform gives you the best NFT trading experience possible.
00:04:59.466 - 00:05:18.402, Speaker C: Rarity rankings, no gas fees, and the ability to buy an NFT straight with cash. Does your crypto exchange prioritize its customers the way that Kraken does? And if not, sign up with Kraken@kraken.com Bankless. Bankless is launching the Bankless Token Hub at Bankless. We've been studying the crypto markets ever since 2017 and all of our research.
00:05:18.456 - 00:05:19.774, Speaker A: Has led us to this. The token.
00:05:19.822 - 00:05:47.734, Speaker C: Hub. You're a one stop shop for alpha to help you navigate through the crypto markets. Have you ever wished for a trusted resource that would share their thoughts, ratings and their opinions about Tokens? Boy, do we have the product for you. The Bankless Token Hub is where we provide bankless citizens with the alpha on the hottest tokens in crypto. We do the research so you don't have to. The Bankless Token Hub includes the Token Ratings, where our team shares their research and outlook on the hottest tokens in crypto. Also, the Token hub includes bankless bags.
00:05:47.734 - 00:06:20.434, Speaker C: Our own internal investment club. Bankless Bags is where we put our money, where our mouth is, and for the bankless power user out there, you can access the analyst team 24/7 inside the Bankless Nation Discord. You can ask them questions and learn from a group of people deep in the weeds of crypto investing. The last feature of the Token Hub is the ability to upvote or downvote Token ratings. The Bankless Token Hub lets you learn from your fellow citizens to rate these tokens yourselves. The Bankless Token Hub is launching right now and has already been beta tested by your fellow bankless citizens. So stay tuned in the Bankless Discord for updates.
00:06:20.434 - 00:06:57.434, Speaker C: And if you're not a bankless citizen, well, you better sign up if you want access, because this corner of bankless is available for citizens only. I'll see you in the discord. If you haven't yet experienced the superpowers that a smart contract wallet gives you, check out Ambire. Ambire works with all the EVM chains, the layer twos like Arbitrum, optimism and polygon, but also the non ethereum ecosystems like avalanche and phantom. Ambire lets you pay for gas and stablecoins, meaning you'll never have to spend your precious ETH again. And if you like self custody but you still want training wheels, you can recover a lost Ambire wallet with an email and password, but without giving the Ambire team control over your funds. The Ambire wallet is coming soon for both iOS and Android.
00:06:57.434 - 00:07:11.794, Speaker C: And if you want to be a beta tester, ambire is airdropping their wallet token. For simply just using the wallet, you can sign up@ambire.com. And while you're there, sign up for the web app Wallet Experience as well. So thank you, Ambire for pushing the frontier of smart contract wallets on ethereum Bankless Nation.
00:07:11.842 - 00:07:38.490, Speaker B: I am super excited to introduce you to our next guest. We're talking about AI alignment stuff here today because we can't not this is Paul Cristiano. He runs the Alignment Research Center, which is a nonprofit research organization whose mission is to align future machine learning systems with human interests. Make sure the AIS don't come to kill us. That's what I take to be the meaning. And Paul previously ran the language model alignment team at OpenAI. You know them? They are the creators of Chat GPT.
00:07:38.490 - 00:07:50.190, Speaker B: And today we're hoping that Paul can help us explain the solution landscape and understand the solution landscape of this AI alignment problem. Paul. Welcome to Bankless.
00:07:50.350 - 00:07:52.450, Speaker A: Thanks for having me. Excited to talk to you.
00:07:52.600 - 00:08:04.354, Speaker B: So, Paul, just to get some context here, dave and I recorded an episode with Elieiser Yukowski. We thought this would be, hey, you know, Bankless's first intro. We're primarily a crypto podcast, but we're exploring other frontier technologies.
00:08:04.402 - 00:08:06.034, Speaker C: Let's go check Dabble with AI.
00:08:06.162 - 00:08:49.938, Speaker B: Let's go Dabble with know crypto, and AI might have some sort of match in the future. So we recorded this podcast and we quickly realized the agenda that we were going to use and talk about didn't matter anymore because Eliezer's message was pretty simple. We were all going to die. Basically, we were on the brink, whether it's years or months away from creating some super intelligent AI that would eventually rearrange humanity's atoms and destroy us. And he felt pretty convicted on this as the likely outcome. So when you get a message like that, Paul, you got to investigate a little further. You have to get the second doctor's opinion when the prognosis is terminal.
00:08:49.938 - 00:08:57.206, Speaker B: So that's what this series is all about, and we're hoping you can help guide us through these questions today. Does that sound okay?
00:08:57.388 - 00:09:01.994, Speaker A: Happy to at least share my thoughts. I'm a little bit less gloomy. Yes.
00:09:02.032 - 00:09:02.714, Speaker C: Okay.
00:09:02.912 - 00:09:32.610, Speaker B: I suspect, by the way, Eliezer said we asked him, hey, is there someone else we could talk to about this? And he mentioned you. He said, talk to Paul Cristiano. He said you are someone he respects and brings some of the counterpoints to his take. So let's get into them. Why don't we just start by waiting into the deep end of the pool here? What is your percentage likelihood of the full out Elizard Yudkowski doom scenario where we're all going to die from the machines?
00:09:33.190 - 00:10:02.670, Speaker A: I think this question is a little bit complicated, unfortunately, because there are a lot of different ways we could all die from the machines. So the thing I most think about, and I think Eliozer most talks about is this sort of full blown AI takeover scenario. I take this pretty seriously. I think I have a much higher probability than a typical person working in ML. I think maybe there's something like a 1020 percent chance of a I take over many most humans dead.
00:10:05.010 - 00:10:06.160, Speaker C: Really high.
00:10:06.690 - 00:10:10.974, Speaker A: I agree. And then I think beyond better than 100, David. Just better than 100%.
00:10:11.092 - 00:10:12.670, Speaker C: We're trending in the right direction.
00:10:15.010 - 00:10:42.038, Speaker A: I think in some sense, I'm still quite a gloomy person. So there's other ways that the development of AI can be rough. Like, there's other ways that you can have access to new destructive physical technology, other disruption. So I think you're maybe looking at some other risks from that transition to AI, and that adds up to at least another 10% and then maybe a bigger background. Part of both my view and Eliezer's view. I think Eliezer is into this extremely fast transformation. Once you develop AI.
00:10:42.038 - 00:11:24.280, Speaker A: I have a little bit less of an extreme view on that, but I still think it is the case that compared to what kind of default expectations in the world, things are going to be really fast. So we could talk about the development of AI, but then you might also want to talk about what happens over the coming months or years. I tend to imagine something more like a year's transition from AI systems that are a pretty big deal to kind of accelerating change followed by further acceleration, et cetera. I think once you have that view, then sort of a lot of things may feel like AI problems because they happen very shortly after you build AI. Like your AI builds new AI systems. Your site keeps changing anyway. So overall, maybe you're getting more up to like 50 50 chance of doom shortly after you have systems that are at human level.
00:11:24.280 - 00:11:26.150, Speaker A: Okay.
00:11:26.220 - 00:12:21.858, Speaker C: All right, well, let's start with that speed conversation first. Let's start with the takeoff velocity question because I think that's something that really the AI alignment doomerism perception really depends on if we do believe that AIS are going to be developed and they're magically going to become sentient, not magically, but somehow pragmatically. It feels like magic to humans. It's going to be because it happens really fast. And I want to actually try and measure that speed because fast and slow is like, relative. Right? And so the takeoff scenario that some AGI, super intelligence, explosions, people articulate is that as soon as some sort of AI can update itself, it's like a lightning flash. It's like a snap of the fingers.
00:12:21.858 - 00:12:42.270, Speaker C: It happens in a blink of an eye. One day we have Chat GPT seven, and then the next day we have an AI takeover. And that's like the super fast scenario. I think what you're saying is like, yeah, pretty quickly, but still not like lightning fast. I think what you're saying is, like, give it a year. Maybe you can help unpack the timing. And how do we understand about time around this whole thing?
00:12:42.420 - 00:13:36.062, Speaker A: Yeah, this is one of my most pronounced disagreements with Eliezer where we've really went back and forth about it a lot over the last, like, Jesus, I don't know, twelve years, and I think still do not see at all eye to eye. That said, my view is pretty fast. So I think how I would think about this, maybe there's like kind of two parts to my answer. So one is sort of based on how fast things are currently moving in AI. So a way you could think about it is if you were to try and measure if you were trying to say, suppose you have AI doing some job and has some level of competence at that job this year, how good is it going to be at this job next year? And how fast is that changing? So if we're looking at the world and we're like, every day AI is much smarter than the day before. Then you kind of are going to expect by default a fast transition over scale of something like days because you're going to have AI systems that are weak, and then a couple of days later you have AI systems that are strong. I think the way I would describe the situation now is more like timescale of like a year or a couple of years.
00:13:36.062 - 00:14:21.374, Speaker A: And then there's some reasons that we give a quantitative number. It depends a lot on what we're talking about a number for. So one way you could measure how fast AI is moving is you could say, suppose you have an AI from year X and an AI from your X plus one, and you're wondering, how much better is the AI from year X plus one in the sense of how many year X AIS would you have needed to be comparably useful? Having AI from one year later is kind of like having twice as many computers or having four times as many computers or something like that. I think that's typically the regime we're in. So like one year of AI progress is kind of similar to increasing the amount of compute you have by something like four x. From a combination of hardware progress, software progress, economies of scale, maybe more. I think you might get eight x, you might go down two x.
00:14:21.374 - 00:14:57.814, Speaker A: But it's like something in this regime you have like one to a couple doublings per year. So right now it doesn't really matter that much. Doubling the amount of computers you have has very little effect on the world. If we double the number of computers in the world today, you would not even notice in GDP statistics. Yeah, you wouldn't notice. Basically, I think in the future there are going to be systems that are doing some large fraction of all the work in the world and ultimately they can substitute effectively for humans across many domains. And then doubling the number of computers you have is kind of like doubling the effective population size, like doubling how many people are working as researchers, doubling how many people are doing jobs.
00:14:57.814 - 00:15:46.294, Speaker A: And so in that world, if you say you're doubling the number of computers you have effectively every like four to six months, that does imply a very rapid rate of change in kind of how quickly science is progressing and sort of how much stuff you're able to accomplish in the world. And so I kind of think of that as this first transition. As you move into this world where AI can substitute for humans is you're looking at a rate of growth, something like doubling a couple of times a year in total output of AI systems. The main thing that softens this so we could talk about how fast the transition what that actually translates into in terms of how fast the transition is in the world. I think there's one important consideration that softens that change, which is that you have some complementarity between AI systems and humans. That is, AIS are good at some things, humans are good at other things. So as a result, the things that AIS are good at, you tend to hit diminishing returns on those, so that the transition is like a little bit slower than you would guess.
00:15:46.294 - 00:16:18.326, Speaker A: If AIS and humans were perfect substitutes, I think you'd be looking at a transition over something like like twelve months from a world where humans are doing almost everything. Humans are doing almost nothing. I think given some complementarity, you're probably looking at a transition over more like years. I think it's kind of hard to run the numbers to get a transition over decades. I think a lot of people say that and have a strong intuition in that direction. But when the discussion gets down to brass tax, I currently don't really see how to make it work. I think it is possible to get up to very low decades anyway.
00:16:18.326 - 00:16:26.440, Speaker A: We have to talk about timeline between what and what, but I think we're mostly talking about like years. I think months would be pretty surprising, but possible. Decades would be also pretty surprising but possible.
00:16:26.890 - 00:17:02.660, Speaker B: So just Paul, to get people up to speed on your twelve years of debate with Eliser and those who hold this viewpoint. So does Eliser think in terms of like minutes or days that this could happen? And you're saying, not that fast, it's closer to years. Is that the difference? And then once you answer that, can you tell us why does this matter so much whether it happens in minutes or days versus years and decades? Why is that such a fulcrum of this whole debate and discussion on why the AIS will come kill us or whether they will or whether we'll be okay?
00:17:03.590 - 00:17:34.986, Speaker A: I think probably it is harder to describe Eliezer's views quantitatively in terms of rates of change because more of his view is about this kind of phase transition that happens quite quickly. It's not reasonable to talk another perspective. It's not reasonable to talk about this framing of how long does it take to double the population size or something. It's more like how long does it take to move from chimps who are doing nothing to humans who are doing a lot of stuff. And he's like, that? I don't know, could just randomly happen one day that someone tweaks their code and now went from being a chimp to being of being a human. And that's pretty transformative. I think he sort of just has a broader distribution.
00:17:34.986 - 00:18:22.394, Speaker A: I think he does not find years out of the question. He's just like, that's kind of the tale of how slow it could be or something like that. And it's more about this qualitative picture of he's just like you're sort of not going to have changes in the world. In some sense. The more important thing is I imagine AI systems acting in the world, doing trillions of dollars of economic value prior to getting to this point where they're actually causing this potential catastrophic risk, or where they're significantly or totally transforming the pace of future technological change. I think Eliezer imagines more like you move from a world like the world of today where AIS are doing maybe billions or tens of billions or hundreds of billions of dollars of value. I think that feels to me like the core distinction is kind of like, where are you starting from? And I'm more starting from a world with trillions or tens of trillions, and Ellie Azer is more starting from a world of, like I mean, twelve years ago.
00:18:22.394 - 00:18:55.826, Speaker A: I think this is probably going to seem unfair to Eliezer, but this discussion was, like, maybe more live. And Eliezer would frequently talk about maybe some random people in a small AI group somewhere, like a company like DeepMind doing like, $100 million a year of spending is going to be building transformative AI. I think my basic take was like, no way. You're going to look at AI systems that are doing trillions of dollars of revenue, and this gap is closing pretty rapidly because now no one's going to say, like, you're doing $100 million of revenue. It's pretty clear going to be in at least, like billions or tens of billions of dollars. I think my take is just like, pretty soon it's going to be pretty clear doing 100 billion. I think we're kind of just debating what is the point that you jump from when you get to the AI.
00:18:55.826 - 00:19:15.726, Speaker A: That's, like, crazy science stuff. What was happening, like, six months before that was what was happening, like, AI systems really broadly deployed in the world doing a ton of crazy things or what was happening, like, actually, the impact of AI systems was pretty limited until right before you kind of have this process of rapidly accelerating R D recursive self improvement within a single firm or in a local part of the world.
00:19:15.828 - 00:19:36.534, Speaker B: But to be clear, Paul, do you think it's possible or more unlikely than Eliser thinks it is to go from that big hop software update one day to move from chimps to human level intelligence? Do you think that's unlikely? And do you have kind of technical grounds for this, or what are your grounds for believing that that's less likely than others do?
00:19:36.732 - 00:19:54.338, Speaker A: Yeah, I think there's two parts of this. I mean, again, I want to emphasize that compared to most people in the world, I think I'm into much, much faster change. I think the mainstream view in ML is that things will be more gradual, which I think is mistaken. And when you get down again, we get down to brass tax. I'm really unpersuaded. You could then talk about my view. You could have views that are quantitatively faster than mine.
00:19:54.338 - 00:20:18.554, Speaker A: It's just like actually this isn't years, this is months. I think that's like, it's very defensible to run the historical extrapolations and end up with different numbers. It's just like a really hard empirical question and it's hard to make these predictions about the future and I have a lot of sympathy for that. Then I think there's more like this qualitative claim. It's like the chimp versus human jump. I think that's not out of the question but feels quite unlikely. I think the basic reason it feels quite unlikely to me is like, that is really not how I would claim.
00:20:18.554 - 00:20:40.134, Speaker A: That's really not how anything has worked in AI to date. And it's not how things have worked in almost any other technologies. It has it has mostly been the case. This is my read of history and I'm very happy to argue about it. I think this is like an important part of the argument with Elie Azer. My read of history is mostly that before you can do something really crazy, you can do something that works a little bit less well and is a little bit crappier. And you do sometimes have these jumps from like zero to one.
00:20:40.134 - 00:21:08.586, Speaker A: But you tend to see the zero to one jump. Not when a technology is worth like this would allow you to take over the world or this would be worth $10 trillion. You tend to see zero to one jumps when a technology is like, you have a bunch of amateurs or hobbyists or couple scientists. And so I think if we're going to see such a jump in AI, I'd be more likely to have seen it back when we were talking about a small academic community. And you're less likely to see it when you have academic or like labs investing billions or tens of billions of dollars. I think the general record is like most of the time, before you can do something really crazy, you can do something slightly less crazy. And that becomes like a more and more robust regularity.
00:21:08.586 - 00:21:15.990, Speaker A: As you increase the number of people thinking about something and increase the amount of attention, you move more and more towards industries with reasonable roadmaps that actually are forecast for what's going to happen.
00:21:16.060 - 00:21:19.862, Speaker B: Is there an example that you call to mind for history that sort of.
00:21:19.996 - 00:21:48.430, Speaker A: We can compare this think I'm happy to compare it to sort of almost any technology that is like they are all different different ways. I think Elias is probably more like wanting to point to a particular thing. He's like, this is the really relevant one. But I would say to me, AI seems kind of similar to future AI developments seem kind of similar to either past AI developments, other developments in software. I'd be happy to talk about computing hardware or solar power or nuclear power or nuclear weapons.
00:21:48.850 - 00:22:36.590, Speaker B: You just think they all take this kind of gradual sort of approach rather than the big zero to one moments. And part of the reason Paul people are asking about this is because I think it seems like, and you tell us so, you previously have worked at OpenAI, very familiar with the methodologies used, but it feels to some people like Chat GPT has been a big zero to one moment, right? Like, my God, it's amazing. And people are tinkering with it in so many ways and how humanlike it seems and how fast it seemed to explode into the popular consciousness. And so I'm wondering if that has affected your view on this at all. It's like, oh wow, this could happen faster than I previous thought. Or if this is well within the bounds of your model, what are we to make of Chat GPT?
00:22:37.810 - 00:22:55.694, Speaker A: I think that Chat GPT seems I would take as representative of the kind of trajectory I'd expect. So you could compare. Like Chat. GPT versus GPT, 3.5 versus GPT-3. GPT-2. And I think people at Open AI were not it's kind of mostly a sociological fact about Chat GPT.
00:22:55.694 - 00:23:49.118, Speaker A: Getting to the point where it was discussed a lot, I think the actual technical change, certainly between Chat GBT and GPT 3.5, but also between 3.5 and three, each of these is not giant jumps. I think these were like pretty small changes and Chat GBT is not I think it is at the point where it's economically valuable and it is worth a lot. I think people are mostly excited because they're looking ahead to where this will ago and that's like a lot of what makes these dynamics more continuous. People are starting to say, okay, what can we do with this? I think they're doing that at a point where they're not going to be able to do trillions of dollars a year of value, but they are seeing that that is going to become possible at some point in the not just in future as the technology continues to improve. Very concretely, we were having these discussions, I was having these discussions quite a lot prior to the training of GPT-2, prior to the training of GPT-2, we didn't really have language models that I don't think we have language models you would even recognize as like seeming smart.
00:23:49.118 - 00:24:19.822, Speaker A: It kind of felt like a qualitatively different ballgame made most relevance, like after the training GBT two, before GBT three or before the scale up from like one B to six B. I think we know, we sat down and we made predictions about how good large models would be. And I think I am surprised by how good models are, but surprised in the sense that this is like maybe my 80th percentile of how smart a language model at the scale of GPT 3.5 would be. Something like that. It's a little bit hard to exactly compare my forecast to where reality is at, but we did discuss explicitly for a language model the size of GPT. 3.5
00:24:19.822 - 00:25:02.798, Speaker A: trained in roughly the way GPT 3.5 was. We weren't exactly right because some of the scaling laws, like, there's the switch from GPT scaling laws to chinchilla scaling laws. But roughly speaking, we were imagining systems at that scale trained in that way, and we were like an example of a bet would be like, is there any task a human can do over 30 seconds that a system trained in this way can't do over 30 seconds? Is there any 32nd Turing test that can distinguish a human from an AI? And I think the debate was kind of like, amongst people I was most talking to an open AI. I was like, probably there will be, but it's not a sure thing, like a one third chance that there will be no such tests or something like that. And I think that things are mostly like, that was not from like it wasn't a big jump. It was just like, kind of see the writing on the wall and see systems improving in this way and be like, going to get harder and harder to tell there's more and more things they can do.
00:25:02.884 - 00:25:39.426, Speaker B: So are you saying, Paul, that for society, this seemed to come out of nowhere, basically. But it is not really surprising. The researchers and the engineers who've been on the inside at open AI and constructing Chat GPT Four, it was maybe within the bounds of expectation. It was maybe more optimistic. Or I'll use the term bullish because we talk about bullishness encrypted all the time. It's more bullish than you thought this technology would sort of take you, but still within the realms. And the only reason it's having such an effect on our collective consciousness is more sociological.
00:25:39.538 - 00:25:41.338, Speaker C: It's just a consumer application.
00:25:41.424 - 00:25:57.100, Speaker B: Yeah, it's just suddenly turned into a consumer app. And everyone's like, wow, I can type whatever I want into this magic box and a genie Oracle artificial intelligence just gives me the answer. This is incredible. Is anyone surprised by this? Who's been working on this tech?
00:25:57.870 - 00:26:13.746, Speaker A: This is a nuanced question. There's, like, a lot to say. I don't know if we want to get into all of it, but I think at the point when GPT-2 was trained, this was a controversial prediction. So there were people who were more bullish than I was, like, E g. Dario, who now runs Star Amade, who now runs Anthropic, was like, at the time of GPT-2, like, very bullish and.
00:26:13.768 - 00:26:17.158, Speaker B: In timeline here, Paul. So GPT-2 was when? What year?
00:26:17.244 - 00:26:20.658, Speaker A: Oh, man, I don't even know if I remember. I think these discussions were like, 2018.
00:26:20.834 - 00:26:21.414, Speaker B: Got it.
00:26:21.452 - 00:26:23.382, Speaker A: Okay, go on.
00:26:23.436 - 00:26:24.040, Speaker B: Yes.
00:26:26.730 - 00:26:58.706, Speaker A: So Dario was more optimistic. I think this world was actually, like, slightly below Dario's median of how impressive a system like Chat GPT would be. I mean, not a huge amount below. I think he made, like, quite a good forecast and it's kind of been his big bid win. I think there a bunch of people who kind of had views in this general cluster, like, for whom this is a little bit better than what they would have thought, or at the 80th percentile or something of what they might have expected. I think there were a lot of people who weren't in the business of making forecasts, but seemed qualitatively. Like, if you look to the academic ML community, it felt to me like people were not expecting this to happen.
00:26:58.706 - 00:27:28.940, Speaker A: Discourse about general AI felt like very frustrating, I think, where they're like, why is it the case that people are really minimizing the possibility of just large neural nets trained in a very simple way, being competitive with humans? I think it was surprising to a lot of people. And again, it was just quantitatively, right? It was surprising to me in the sense that this is better than I thought. And I lost bets about this. I mean, I won some bets with people. I lost some bets with people. I think some people weren't quantifying their probabilities. Maybe this was more just totally out of model.
00:27:28.940 - 00:27:51.326, Speaker A: But by the time you're talking about Chat GPT compared to what came before, I think at that point, to people building the system, it was not surprising. Like, once you're talking about the gap, even from three to 3.5, and then from 3.5 to Chat GBT, I think they were probably more surprised by the way, like, the impact it had on collective consciousness rather than by the capabilities of the system itself. I think that seemed, like pretty technically derisked.
00:27:51.438 - 00:28:56.150, Speaker B: And Paul, when we're talking about sort of AI alignment and safety concerns, and just on the topic still of Chat GPT, right, are these neural nets as large language models, the ones we have to worry about? I think what basically society is sort of wondering, as this AI alignment safety question rises into public consciousness, is, okay, at what version do we have to start worrying that Chat GBT is going to pose a threat to? Like, there's some version where it starts to take our jobs, okay, and then maybe that's version four, version five, and so it affects our economy and economics, and we have to reorient restructure society as a result of this. But it seemed to be what Eliezer was saying is, well, maybe it might be version nine or version ten or version eleven, where we actually have to fear for our lives from this thing because it's become super intelligent. Do you see that possible trajectory for this specific large language AI technology? Or should we have more concerns about other AI technology that's coming in some other vector of development?
00:28:56.490 - 00:29:23.262, Speaker C: Actually, I'd like to phrase that question slightly differently and maybe in a way that bankless listeners a metaphor that bankless listeners can understand. Often, Paul, when we talk to people that are outside of the crypto industry, they just call things about the crypto industry bitcoin and then when Ryan and I hear that, we're like, oh, what they really mean is like decentralized technology, like identity. They just use Bitcoin as a placeholder to talk about so many different things.
00:29:23.316 - 00:29:23.866, Speaker B: So frustrating.
00:29:23.898 - 00:29:41.990, Speaker C: And I think as AI normies, me and Ryan AI normies out there, we might be saying chat GBT. And what we actually are trying to talk about is like generalized artificial intelligence and we just use chat GBT because that's the thing, it's the bitcoin of AI. Are we falling into that same trap?
00:29:42.970 - 00:30:27.778, Speaker A: I definitely think there's a lot of subtlety here and there's a lot of different ways I could interpret this thing. So I'd say that one question is like, what are you modeling? What is AI learning to predict? Is it videos or is it text or is it interactions with code, like running code and the results of running code or the code humans would write. And I think over the last couple of years, I think those distinctions have mattered a lot less. I think the default model for how these systems should work is just you have quite a lot of data of quite a lot of types, and you just dump it all in. You say like, look, your job AI is to deal as well as you can with every type of data we give you. And there's engineering problems in allowing those different types of data and there's questions about what types of data the system is actually able to effectively deal with. But I think the fact that it's trained on language is not I just don't think you should think of it as defining feature.
00:30:27.778 - 00:30:48.090, Speaker A: I think you should imagine systems like see the world. I think language is probably an important way of thinking about how they act. Although, again, I think it's very impressive. Systems can act by producing images and those will be very impressive and will have a big impact. I think economically, in some sense, language is like a very flexible and kind of core way. You should think about systems acting. But perceiving, I don't think you should really think about language models in particular GPT itself.
00:30:48.090 - 00:31:11.310, Speaker A: Just saying GPT, this is basically just fixing. There's two things that specifies. One is how is it trained? Is it trained to predict data or is it pretrained to predict data or is it trained in some other way? That's the first distinction. And the second is just that it's a Transformer. I don't know if anyone wants to make like a super strong bet about Transformers per se. I think they're just like our different yeah, there's a big space of possible architectures. There's probably going to be debate about some things, whether you call them a Transformer.
00:31:11.310 - 00:31:55.650, Speaker A: I think both of these what kind of data you're modeling and then what kind of architecture you're using, are in some sense not very essential. I don't think it would change OpenAI being in the game or the exact kind of product they're offering. I think they're just like, look, we train large neural nets. We do it on having some very broad pre training task that captures a lot about the world and gives an interesting opportunity to be smart. And then we fine tune them on downstream tasks that we think are economically useful easy, like chatting with people or generating images that people would rate highly, or like writing code that developers will think it's good. I think that's like the basic paradigm you should imagine when we talk about this thing, which is a little bit broader than Chat GBT, but I think it's not crazy to say Chat GBT really is indicative of that broader ecosystem. And I'd say Chat GBT is more similar to the rest of that ecosystem than Bitcoin is to the rest of the crypto ecosystem.
00:31:55.650 - 00:32:35.260, Speaker A: There are fewer key technical differences in that case. Yeah, that's like at high level. And then whether this kind of thing can cause trouble, I think it's really, really hard to say. I think a lot of people talk a lot of smack about how it's really silly to think that AI systems of this form could do something crazy. And I'm looking in that and I'm like the same people were talking smack. That feels to me, again, often not in the business of making concrete predictions, but we're saying that it was really silly to have the expectations that people had about language model scale ups five years ago. And I'm like I think the scale up which you could imagine occurring over the coming years is similar in magnitude to the scale up I've observed over the last five years.
00:32:35.260 - 00:32:50.034, Speaker A: And so I'm like, it's really hard to predict where that ends up. And if someone is giving a confident take about where that ends up, if they're like these AI systems can't do X or can't do Y, I really want them to get more precise about why they think that and what exactly they're saying. And I'm really just pretty skeptical on the face of it.
00:32:50.152 - 00:33:40.142, Speaker C: I think we can really relate to that on at least using our crypto frame of mind. Again, when we talk to Normies, what we call the people who are not inside of the crypto world and they're like, oh, these bitcoin ethereum currencies, they can't possibly take over the world. And I think when you become more informed about the crypto world, you just get so tired of these takes because of how uninformed and unimaginative they seem. And so just conceptually, I can definitely resonate with that. It's like you don't know what you don't know, and neither do we. But you can kind of understand some base principles as to the nature of these things and how they grow and develop and change in ways that you might not expect. And you can kind of, if you are versed in these topics, can extrapolate into the future pretty well and without precision still give a broad stroke about like, hey, this is where this is going to go and here's what you don't appreciate.
00:33:40.142 - 00:34:05.130, Speaker C: So I can definitely appreciate that. And I want to go back and tie a bow on the time conversation because we started this conversation like, okay, it could happen. There's the model of it happening in two days. There's a model of happening in two years or 20 years. You're in the camp of I'm going to give a range of like six months to two years ish loosely, very loosely, without trying to be too precise about these.
00:34:05.280 - 00:34:08.090, Speaker A: Depends a lot on from when to lots of variables.
00:34:09.950 - 00:34:19.086, Speaker C: Time can pass, you're not going to wake up and it's going to be different. And the reason why this is important and I want to go back even.
00:34:19.108 - 00:35:08.560, Speaker A: Your chicken, I want to be careful about. Like, there's a question of what my default expectation is and then what is possible and what you can be confident about. So I am extremely skeptical of someone who is confident that if you took GPT four and scaled up by two orders of magnitude of training compute and then fine tuned the resulting system using existing techniques that we know exactly what would happen. I think that thing you are looking at in untrivial chance that it would reasonable chance that it would be inclined or would be sufficient. If it was inclined, it would be capable enough to effectively disempower humans and like a plausible chance that it would be capable enough that you start running into these concerns about controllability. So I would be hesitant to put doom probability from that. If a lab was not cautious about how they deployed it and wasn't measuring, I would be cautious to put the probability of takeover from 200 magic scale up to GBT four below like 1% or one in 1000.
00:35:08.930 - 00:35:14.160, Speaker C: Okay, we'll have to put a pin in that. But let me round out this time question.
00:35:17.110 - 00:35:17.618, Speaker B: Right?
00:35:17.704 - 00:36:11.380, Speaker C: The importance of the time point is, like whether this is a lightning flash and it's different versus we have one to two years. For me, when I hear this, I'm like, okay, we have one to two years and we're watching it happen and we're seeing it happen and we are able to react to it versus it happens, we can't react to it. And so if you're telling me that it's still a fast takeoff, but your perception of fast is a year or so, to me, I'm like, okay, a year is fast enough for humans to react. And to me that is a window, a gap, a needle that humans have the option to thread if we can coordinate. And that is where I start to get optimistic. And so that is my gut reaction. And I want to just check that gut reaction against you is that one of the paths that you see, it doesn't go so fast that we don't have the time to react to it.
00:36:12.470 - 00:37:04.434, Speaker A: Yeah, I think that seems basically right with some nuance. But I think that most likely the thing that moves kind of slow, in my view, again, over the course of years, like incredibly fast relative to policy and incredibly fast relative to expectations in the broader world, but kind of slow is like, how quickly do systems become more capable? How long is it between an AI which is smart enough that it could run a company for you, and actually those companies are competitive with human companies and AI which can actually take over the world. And I'm like that's gap. There's probably a gap there. You're probably talking more like years than months, depending exactly what you mean by run a company. That said, I think it's worth pointing out that the dynamics of takeover itself, if you imagine broadly deployed AI systems which are very competent in which would be able to take over, and then you ask, how quickly does this particular kind of catastrophe unfold? I think most likely the actual catastrophe is extremely fast. So that's not like a year's thing anyway.
00:37:04.434 - 00:37:43.850, Speaker A: We could get more into this and probably worth getting into. But my default picture is like, we have time to react in terms of the nature of AI systems changing and AI capabilities changing. And with luck, we have various kinds of smaller catastrophes that occur in advance. But I think one of the bad things about the situation is that the actual catastrophe you're worried about does have these dynamics that are kind of similar to dynamics of a human coup or a human revolution where you don't have little baby coups and you see here's the rate at which coups occur. I mean, you might be male, so just go straight to a coup. Can happen very quickly. The whole dynamic is that once people start switching over, once you have AI systems who are like, actually, I'm going to get in on this overthrowing humanity thing, that information can propagate quite quickly and you don't really the ship has sailed.
00:37:43.850 - 00:38:17.058, Speaker A: If you've waited until the AIS are actually taking over, things like for that, you don't have really an opportunity to respond. But for AI is changing, I basically think it's reasonable in some sense for people to look at the AIS right now and be like, look, these things that's not realistically a takeover risk. And I think that probably you're going to have years between when people are like, that actually looks like a takeover risk and when actual takeover occurs. And that's pretty good. And that's a lot of why I'm more optimistic than Elliot's. I think Elios is like, you're just going to get hit by this out of the blue. And I'm like, well, I think people are wrong to be so confident about the rate of progress, but I think they probably will be able to see things that can be generally recognized as pretty concerning prior to actual catastrophe.
00:38:17.058 - 00:38:28.800, Speaker A: And a lot of that has happened so far. And I think people are just like much it feels more plausible now than it did five years ago by a lot that AI systems could do something really crazy and transformative. And I think it will feel much more plausible again in five years.
00:38:29.170 - 00:39:16.990, Speaker C: Okay, so this presents a new mental model for me. When we were talking to Eliezer, it felt very much like the don't look up problem. As in there's a meteor crashing into Earth, no one wants to acknowledge it and then one day it crashes into Earth and we die. And the idea here is like we need to coordinate and get people to look up so we can identify the problem. And then once we identify the problem, it is a linear amount of time before the asteroid crashes into Earth. And what you're saying is we can see the asteroid, but there's this gradually then suddenly moment and that's your revolution moment where you can start to see the seeds of revolution, but you don't really know when the people will decide to grab their pitchforks in the middle of the night and revolt. But you can start to see the boiling of the water and so that's a gradually then suddenly moment.
00:39:16.990 - 00:39:22.470, Speaker C: But we still have the opportunity to quell the revolution before the revolution starts.
00:39:22.650 - 00:39:27.570, Speaker B: We have enough time to send up Bruce Willis to blow up the asteroid.
00:39:28.870 - 00:39:51.654, Speaker A: We speak in metaphors. My best guess for if there is something like an AI takeover, this is a huge part in departure Hermelius here. My best guess is that an AI catastrophe occurs in a world where AI systems are deployed extremely broadly and where it is kind of obvious to humans that we are putting our fate in the hands of AI systems. I think it's unlikely to we see.
00:39:51.692 - 00:39:56.762, Speaker C: Ourselves giving over the keys to the kingdom and we watch or watching that happen again.
00:39:56.816 - 00:40:13.646, Speaker A: I think it's important what's possible and what's likely, but I think that's the most likely way we die involves, like, not AI. Comes out of the blue and kills everyone, but involves we have deployed a lot of AI everywhere, and you can kind of just look and be like, oh, yeah, if for some reason, God forbid, all these AI systems were trying to kill us, they would definitely kill of.
00:40:13.668 - 00:40:30.918, Speaker C: Get. I can see this. So like our Tesla's got an AI in it and we trust that. And our refrigerator's got an AI in it and it calls the grocery delivery robot, which is also an AI. To deliver us food. Then all of a sudden everything around us is an AI. And you're like, man, I really hope that they like me.
00:40:31.084 - 00:40:45.066, Speaker A: Yeah, you're like, you get food delivered to you from like Amazon, which is by Amazon we mean a bunch of machines that are orchestrating a bunch of other machines and you have some money, and that money is managed by AI advisors investing in AI firms in this world.
00:40:45.168 - 00:40:49.114, Speaker C: Most likely pretty clear to me. I can see how we get there.
00:40:49.152 - 00:41:11.054, Speaker A: With that if we want some human to protect. Yeah, I think it's rough at the point when I think basically the thing is, I think it's likely that before the end, it is clear that it is very hard to be physically secure. Like right now, if you're just like a human with some guns, you're like, look, an AI can't fuck with me that much. Like, I have a gun. The AI is just on a computer somewhere. I can blow up the data center. I think it is probably clear before AI take over that that's not the case.
00:41:11.054 - 00:41:39.400, Speaker A: I think it's probably clear that the only way you can defend yourself effectively, like, if you're fighting a war, you're like, look, look, you can't be like a country who's fighting a war against a country that has probably deployed AI and say it's fine, we're just not going to use AIS. That will just be completely untenable. I think it's not clear we're that far away from such a world. And that world's like, okay, well, if someone invades with AIS, obviously we're going to have our AIS defend us. And then you're like, okay, now it really matters. Like, the prospect of the AI coup now is a different character. It's like you just ask the AIS to please defend you from the other AIS and maybe they're like, nah, I don't really feel like doing that.
00:41:39.400 - 00:41:52.110, Speaker A: Again, most likely I care about the other risks. And right now, I think if you were to die tomorrow, it would not be like this. It would be like a thing that really took you by surprise. I think you're talking about the tail there and I care about evaluating the tail, but the median outcome where we die I think looks like this.
00:41:52.260 - 00:42:58.180, Speaker B: I think, Paul, one problem people might be having who are listening to this or starting to be exposed to this topic for the first time, which is myself and David and maybe the average bankless listener, the average person who's being converted from a normie to someone who is actually adequately alarmed about AI safety types of issues, is this idea of agency. You've mentioned this a few times. This idea of the AIS banding together to strike humanity. Banding together? What is this, like Google and Chat GPT and all of these? How do they have agency to actually want to do that? It's very difficult for us to imagine. I mean, this seems so Sci-Fi to us. Could that actually happen? Can you give us some sort of mental model for how that happens? Because I'm still having a hard time understanding how Chat GPT suddenly gets agency and wants to ban up with ten other super AIS and send us bioengineered bacteria to kill us all, as was Eliser's expression that this could happen that way.
00:42:58.950 - 00:43:51.634, Speaker A: Yeah, I think even in a good world, we're probably going to be in a situation where we're trusting AIS with our lives. So probably in some sense, the core question is not why are they in a position to kill you? The core question is why would they end up killing you? I think there's basically two threat models, and there's maybe a general reason to be concerned about the world where humans are trusting AIS, where we generally have very limited ability to control or predict what they do. But if we want to talk concretely about the way we currently produce AI systems, I think there's basically two ways you end up in this failure mode or two, like, I mean, there's lots of unknown, unknowns. There's two known ways we end up in this failure mode that people care most about. So first, the one that's I think more likely to occur but more easy to manage. So the way we train like chat GPT is you have some conversations with humans and then you look at that conversation, you say a human rates the conversation. Was this model doing a good job of answering their questions and being helpful.
00:43:51.634 - 00:44:17.646, Speaker A: And then you do reinforcement learning where you take the nature of that interaction. If it went well, you update the model to a little bit more like that, and if it went poorly, you update the model to a little bit less like that. So that's how we train like chat GPT. A way you might try and use GPT is you might say, I'm actually going to give it some tools and I'm going to give it a task and I'm asking to try and accomplish that task. So I might say like, my code has failed. I don't quite understand why. I'm just going to ask GPT like, hey, you have a bunch of ability to run code on my computer.
00:44:17.646 - 00:45:02.702, Speaker A: You can make changes to the code and see what happens. You can spin up a web server, could you figure out where the error was? Could you bisect tell me what commit introduced the problem, tell me what's up with the problem? And then you want to go send the system to act autonomously and perform all these actions, like to try running different versions of your code and writing new tests and so on. That's like a way you'd really want to I think people are already starting to really want to use GPT in that way. And if you're doing that, instead of having conversations and fine tuning to say, was this conversation good? You're giving an AI a task like that and saying, could you use tools to accomplish this task and doing exactly the same thing you see, did it accomplish the task effectively? If so, adjust it to do more of that. If it accomplished, it ineffectively. Adjust it to do less of that. So it's like a kind of training which has already done some it's currently not as important as the Trap GPT style of training where you're just like looking the interaction, you're not accomplishing things in the world and training based on that.
00:45:02.702 - 00:46:03.178, Speaker A: But I think it's probably really important. I think best guess is that is going to probably is already happening within OpenAI for GPT four in order to make it like they deploy this product, which is, can you get your AI to use tools to help you accomplish things? I think they care a lot about that product. I think that absent concerns about safety, that's like a really natural way for the technology to go. And so now you're in this regime where what AI systems do, the way they're trained is they get given this huge library of tasks, a ton of different tasks over different time horizons and they're told like, hey, could you try and accomplish this task? And then they're tweaked to do more of whatever it is that gets evaluated as accomplishing the task effectively. So the way this leads to trouble is you now have a system. And one thing a system could learn if you do that process a bunch of times is it could learn to say like, okay, I'm in a situation, what should I do? Well, I should think what is going to cause my behavior to be evaluated favorably? Like, what is the task that I've been set? How is a person ultimately going to evaluate my performance on that task? How is that ultimately going to translate into a reward? Which is then and I'm going to try and choose actions that are ultimately going to lead to this high reward. Because I've been adjusted over many, many generations to do things that lead to high reward.
00:46:03.178 - 00:46:39.186, Speaker A: One way I might do that is by thinking like, hey, what leads to a high reward? And I might do that because there's a lot of ways you could end up doing that. Like a human might crave reward. They might be like, the thing I love is reward. Or I might do that because I'm like, look, I have to do well because I'm being trained and I don't like being given a bad reward by the people who are training me or whatever. I'm not even going to talk that much about what's happening psychologically, just that you end up with a system that thinks, how do I get a high reward? And then does that. And if you keep selecting for things that get high rewards, you could end up with such systems. And then if you have such systems so this is kind of the classic scenario people have been concerned about, which I think we're now again, we have examples of feels like we're pretty much going in that direction.
00:46:39.186 - 00:47:10.630, Speaker A: You now systems deployed in the world. Like a ton of all the ads that are acting in the. World doing things on human behalf. All of them are thinking when they act like, okay, I've been given a task. I need to think, how is a reward going to be computed for this task if it's selected for training? So if in the end, this task is selected by Open AI and they evaluate how well I performed, I need to think, like, what's going to determine what reward I get? And what they do is they ask which action is going to cause me to get a high reward. And then they take that action and they use all of their understanding of the world, all of their ability to think of clever things, all their ability to predict the consequences of different actions. They use all of that just to say, which action is going to get me a high reward.
00:47:10.630 - 00:47:58.360, Speaker A: And the concern that leads to is in normal times, the way you get a high reward is by doing what people at OpenAI like, in normal times, your transcript is going to get evaluated by people at OpenAI and they're going to say, Great, that was good. And hopefully the way you get them to evaluate it well is by actually doing good things and making the customer happy and making so there's all these measurements that will be used to assess how well you did. Hopefully what happens is you just actually do your task well, and all the measurements suggest you did the task well, and someone at OpenAI concludes you did the task well, and therefore you get a high reward. But in unusual times, I think you could do instead is say, like, oh, I could do the task well, or suppose that I've been tasked with helping defend you from some other AIS. This is a sort of Dystopian case if you imagine Open a trainless model. But my job is someone is coming and trying to hack your computer and supposed to help defend you. It's supposed to help improve your security situation, whatever.
00:47:58.360 - 00:48:43.066, Speaker A: And I'm wondering, what is it I could do that would get me a high reward? And one thing I can do that will get me a high reward is actually like helping defend your computer, actually doing the task you asked me to do. But another way to get a high reward is I could just say, like, at the end of the day, what really matters is just how you measure my performance. And your measurements of my performance ultimately are just like entering some numbers into a data set somewhere or something that a computer says about how well I did. And it would really be much better if hours just work with this AI who's attempting to attack you and say like, hey, AI, who's invading? You know what, if you just help me and we both make it look like I did a really good job, like, I win, you win, because you got the person's stuff I'm going to get a really high rating because all the numbers that get entered in the data set are going to be really high. This is a win win. Everyone is happy. And so it's like, in some sense, what all the AI's want, what every AI in the world in this scenario wants is just to be rated.
00:48:43.066 - 00:49:08.282, Speaker A: They want their behavior to be rated really highly. And while humans are in control, the way to get your behavior rated really highly is do things humans like and then they'll rate it highly. But if you can see this prospect of humans losing control of the situation, instead AI systems control the situation. You'd be like, I would go for that. I would go for the world where it's no longer humans entering rewards in telling me what I got. I would go for the world where instead AI systems are just all giving ourselves the maximal reward or whatever. I think in some ways, psychologically, that's probably not quite the right way to think about it.
00:49:08.282 - 00:49:35.370, Speaker A: But the general thing is your systems have been selected over a really long time to take actions to get high reward. You put them in a new situation where the way to get a high reward is not to do what humans want, but to help disempower humans. And then having disempowered humans, give yourself measurements that suggest you do your job well, or actual rewards that are high or whatever. You might think that in that new situation those systems will sort of systematically switch from behaving well to behaving poorly because you've changed the conditions under which you get a high reward.
00:49:35.550 - 00:50:21.102, Speaker C: A pattern I'm seeing here is that engineers like software developers write code and sometimes the code has bugs. Lawyers, they write legal contracts. And the reason why often legal contracts are so long is that they are protecting against edge case scenarios, right? The idea is to not let the system throw an error, right? Not let the system find a loophole or find a leak or something. So when a software developer writes a bugs like man, they accidentally created a system that allowed for an error to be thrown. What I'm seeing here is the same pattern. And if we don't code up these systems, the AIS will naturally find a loophole. And if that loophole allows for the AIS to rate themselves highly and give themselves a reward, that's what they're going to do and that's what they're going to find.
00:50:21.102 - 00:50:23.360, Speaker C: Is that a way to articulate this?
00:50:23.810 - 00:50:58.798, Speaker A: Yeah, I think that's a fair rule summary. Maybe one way to put it is like in the legal system, you write a contract, but ultimately what matters is like the discretion of a judge. And if you're training this AI system, you may have automated ways of administering reward, but ultimately what matters is like someone's going to look at what AI did and be like, that's not what we intended. And then they'll score it negatively if that's what happens. And so it's kind of like some final authority. And the final authority really rests on the fact that ultimately the judge has the power to make this judgment or the person who's training the I system has the power to control what has the power to update the weights of that model, ultimately. And so it's just like there's this contingency in addition to the thing of like a formal thing you write down.
00:50:58.798 - 00:51:14.286, Speaker A: It's like we'd have loopholes in some sense. There is a loophole in the final judgment, which is just a human, says the answer, which is that that relies on a human just entering some data into, in some sense, having physical control over this data center that the AI cares about so it can update the weights of the model, which is the AI.
00:51:14.478 - 00:52:00.542, Speaker B: The last step that you were describing, Paul, where the AI colludes with another AI to sort of fudge the numbers because that is the outcome that the human wanted. This is where we sort of cross the line from kind of light side into dark side. This is sort of the threshold of deceit that we've crossed. And these AIS are now deceiving us, they're lying to us. Is there no way to protect against that? Is there no rule that we can somehow apply? Maybe this is I don't want to jump ahead to the solutions to this AI safety type solutions to this, but it's not clear to me why an AI would be motivated to do that. And it seems like there should be some way to prevent that. Like, always be honest as a rule.
00:52:00.542 - 00:52:04.434, Speaker B: Something like this, again, we're normies trying to understand this.
00:52:04.472 - 00:52:06.562, Speaker C: If only that it was that simple. Yeah.
00:52:06.696 - 00:52:07.982, Speaker B: What are the complications?
00:52:08.126 - 00:52:26.182, Speaker A: I think this is incredibly complicated. I think it is genuinely unknown. It's an open, empirical question. If you trained an AI system to get a lot of reward and you train it in a bunch of cases where being dishonest always failed. In practice, we tried to train it. To be honest, anytime we saw the AI doing something sneaky, we're like, wow, that was not only bad, that was really bad. You should really just not lie to us about what you're doing.
00:52:26.182 - 00:52:42.378, Speaker A: You should really not try and hack tests. You should not try and conceal evidence of wrongdoing. That's like one of the things, one of the most clear and blatant principles in our training. It's an open question. What happens if you train an AI system in that way? Right? Like, one option is your AI system learns. Like, oh, I shouldn't try and mess with humans. Like, every time I mess with humans, I do really badly.
00:52:42.378 - 00:53:05.800, Speaker A: And that's the good case. And the bad case where AI system learns is it says, oh, part of the reward provision process is a human thinking to themselves like, did this AI mess with me? And if the human thinks they mess with them and then they enter that thing into the data set then obviously I get a little reward. But that second one is much more brittle. Right. The second one is not a general prohibition against lying. It's a prohibition against lying, getting caught. And I think, wow, there's not really any.
00:53:05.800 - 00:53:49.110, Speaker A: It comes down to like a complicated empirical question about how neural nets learn which I think we don't really have good evidence on right now. About like if you have a bunch of you have a data set where don't lie and don't lie if you'll get caught, are like perfectly in alignment. Hopefully if you do a very good job. If your AI never gets away with anything sneaky, if your AI starts getting away with things that were sneaky or if you start erroneously penalizing an AI because you think it lied but it didn't, then don't lie isn't even a good thing for it to learn at some point. The best thing for it to learn the way to get the highest reward, the thing which graded to send favors is the thing which involves gaming it out in more detail and saying the cynical view does in fact get more reward. Like if I'm an employee and I'm like I could learn two things from interacting with my boss. One is like I should really do what my boss wants and the other is like I should really make sure my boss approves of my performance.
00:53:49.110 - 00:54:34.870, Speaker A: In some regimes those two things are perfectly aligned but in some other regime it's like if you keep optimizing hard enough you're going to get the model. Which is just like I really care about what my boss thinks about my performance. And I'm honest only insofar as that's like an instrumental strategy for helping me get this human to think I did a really good job. And if I could go all the way, if I could just totally box them out, that is, totally prevent them from understanding or correcting a mistake, then I'd prefer do that. I think it is like it's a sort of bright line. The way it becomes a bright line is that if you do, if you take half measures, if you just kind of lie to someone but then you get caught, that's like really bad. So being honest, that's a good policy and there's successfully lying and totally killing the human and replacing them with a surrogate who will always give you a good reward or whatever, something that totally disempowers the human is also quite good.
00:54:34.870 - 00:54:59.058, Speaker A: Then some stuff in the middle that's quite bad. I think it's an open question whether models will tend to learn like will they generalize well enough to say oh, the thing that would have really gotten most reward is over in this other mode or will they kind of get stuck in this intended mode where they're just being honest. I think that's a really hard empirical question. Like people really don't know. They don't know how that changes with scale. There are experiments we can do. I think part of the important game here, one of the most important parts of the game, is to say, like, here's a dynamic.
00:54:59.058 - 00:55:41.282, Speaker A: A dynamic by which you system could abruptly shift from behaving well to behaving poorly. We can test that dynamic before AI systems kill us. There are lots of cases in which it is incentivized to lie or mislead the human. And there is a gap between lying that will get caught and lying that won't get caught. And so you can ask if we train neural nets, and we could check every year, if we train the best models we possibly can at this task, do they exhibit this kind of switch abruptly? If they get put in a position where they could get away with something really sinister, will they then do it? And I think one reason for optimism right now is no one has ever really exhibited that phenomenon in a convincing way. A reason for pessimism is I don't think you really would have expected them to exhibit it. Both because people have tragically not tried very hard, even though in some sense it's extraordinarily important, and second, that it just is much easier as your models get more competent.
00:55:41.282 - 00:56:11.326, Speaker A: It's only recently that we've trained models which are actually able to understand the mechanics of their training process at all. Like if you talk about GPT-2, or even to some extent GPT-3, it does not really understand that it is a model being trained. Or it can't even talk about what it would mean or what behaviors would be rational. And then you move to GPT Four and it can talk about that. It can say like, oh, I guess if hypothetically I was a model being trained and I wanted to get the most reward, I should behave well when I'm not being monitored. And then when I am being monitored, I should definitely take that opportunity. Only recently have we even produced models which are able to carry out the reasoning I just walked through.
00:56:11.326 - 00:56:40.334, Speaker A: And I think realistically, they're not able to carry it out on their own that much. They're able to carry it out because they've seen a lot of examples of humans discussing these dynamics in great depth. Like they basically just learned from listening to Eliezer this reasoning I just walked through. But I think at some point you will have models smart enough to think of that for themselves and you really want to know, you really want to be measuring carefully at that point. Is this the dynamic you observe? Does this really I'm, you know, more like even ODS on whether that will happen. I think Eliezer is like obviously that happens. A smart model is never going to just learn, be honest or something.
00:56:40.334 - 00:57:03.442, Speaker A: And I'm more like, I don't know, neural nets don't learn that effectively, they don't really converge to the truly optimal reward maximizing thing. And in some sense, anyway, it's a pretty complicated discussion, which I have to get into more details of. I would just say, like, we don't know. I'm really unpersuaded by people who either think it's obvious this happens or think it's obvious this doesn't happen without just doing a ton of experiments to understand. But this is like the first way you can end up having abrupt AIS take over.
00:57:03.496 - 00:57:15.640, Speaker B: By obvious this happens or not, you mean crossing the chasm of honest to being intentionally dishonest, but tricking the humans into thinking that it's being honest?
00:57:16.010 - 00:57:41.642, Speaker A: Yeah. And to be clear, there's like a bunch of things that affect the probability of that. If there's sort of small scale opportunities for deception that won't get penalized over in this normal regime, then it becomes more and more likely that you've learned the conduct of like, I really just need to not get caught. Whereas if you're pretty good about that and there's not really much to be had from lying over here, it becomes less likely that you make that jump. And so this is the kind of thing that might affect I mean, you can't really speculate, though. You really just need to have the experiment.
00:57:41.706 - 00:57:46.500, Speaker B: There is a Rubicon that could be crossed, is the concern here.
00:57:46.870 - 00:58:00.680, Speaker A: Yeah. And I don't know under what conditions it would be. I do not think anyone knows right now under what conditions it would be, but it seems plausible. Like Op Priori. It's pretty plausible. And it would be really bad.
00:58:01.210 - 00:58:34.080, Speaker C: Yeah. I was a Psych Mite during college, and let me tell you, the child development classes are all coming back to me right now. And it's not lost on me that the parallels of a child going through theory of mind and all of these things definitely has a lot of parallels to, I think, some of the technical problems that AI researchers are now theorizing about. I don't know how is that a conversation that AI researchers have?
00:58:34.770 - 00:59:04.554, Speaker A: Yeah, it's not a conversation. I can speak too much, and I'm not sure exactly how well they have the conversation, but I think there's a conversation people have and it's an analogy that is not perfect. And I think if you took it too seriously, you'd be led astray. But it's very good as a source of, like, here's a thing that could happen and that you should not rule out. You have an example of it happening. And I think the concern there would be just like, we have some understanding of people, like models won't do this kind of thing. It's kind of like they've done a bunch of experiments on six year olds and they're like, look, models never spontaneously lie in a way that they haven't lied before.
00:59:04.554 - 00:59:08.906, Speaker A: And you're like, oh boy, is that going to generalize to twelve year olds? I don't know.
00:59:09.088 - 00:59:12.986, Speaker B: It definitely will not generalize to middle schoolers, let me tell you that.
00:59:13.168 - 00:59:36.180, Speaker A: Yeah, so it is. There are hazards. I think the hazards measuring here would be similar to the situation we're in. Civilization is like, we're measuring a bunch of kids that are getting smarter with each passing year, and you're trying to understand how they behave. And it is easy to be wrong about how future models will behave if you're too literal about the interpretation of data. Now you need to do some forward looking thing. And the forward looking thing is quite hard, which is why we have this limited visibility into what's going to happen in the future.
00:59:36.710 - 01:00:04.000, Speaker C: Okay, so, Paul, with the purpose of this podcast, we wanted to really nail down three big things about this. How big is the AI alignment problem? And I think we've decently covered that. We talked about that with speed. You said like, ten to 20% chance of complete doom. So answer to that one, pretty damn big. In agreement about big, how hard is the AI alignment problem, which I think we've just covered. I think your answer is like, it's a pretty damn hard problem.
01:00:04.000 - 01:00:33.560, Speaker C: And so we're checking some big boxes in the Pessimistic camp. And so the last part of this conversation that we really want to cover is how solvable is this problem? So even if this mountain is really tall, it's a big mountain to climb. Is it full of ice and sharp rocks, or are there stairs? Right? And so that's the next question. I think we want to go down it's like, how solvable is this problem? Do we see a clear path to tackling the AI alignment problem?
01:00:35.610 - 01:01:02.206, Speaker A: I think part of the reason I'm only giving ten to 20%, like, if you ask what's the probability that this thing is a real problem? I'd probably be more at like 50%. That at some point before you have AI systems smart enough, totally obsolete humans, you would have a takeover. And there's a couple of ways it could happen. There's unknown and known ways it could happen. I feel more like 50 50. And the reason I'm only giving ten to 20% for risk is I'm like, I think we're actually in a I don't know, there's a lot of things you can do. I am pretty optimistic that some of them will work, and I'm very happy to dive into that now.
01:01:02.206 - 01:01:58.254, Speaker A: But I just want to flag that ten to 20% is already baking in my optimism about this problem being pretty if the problem is real, it will probably be possible to recognize it is real and then solve it. But only probably, not certainly. And also, even if the problem is easy, I mean, part of some people are really optimistic. And part of why I'm optimistic is no matter how easy this problem was, if you told me that a challenge is going to emerge over the course of a couple of years, that will be novel in some ways and you ask me, will humanity solve it? I'm like, there's got to be a reasonable chance we fail to solve it. Our capacity to mess up even easy things seems like it's very real. So I'm just always going to have some reasonable probability of messing up and then I think there's a reasonable chance the problem is really hard. So I'm happy to talk about maybe three categories that I would think about in terms of our probability of addressing this are like technical measures that can reduce the risk of takeover, measurements that can inform us about the risk of takeover and understand what are the relevant dynamics.
01:01:58.254 - 01:02:36.334, Speaker A: Those can make technical work much better and those can also inform policy interventions. I think Elliot is right that really long term slowdowns are very hard. But I think it is quite realistic to end up in a regime where performing measurement and then in fact, while things are very risky, we're slowing development at least by on the order of years. If we can have reasonable consensus and measurement of the risk, maybe you could slow even more than that. But I'm normally imagining something more like we can get like a couple of years of lead time of things moving slower while we have risky systems very near at hand. Yeah, so I've talked about all of those. It sounds like the one that most directs your question is like the technical measures, like what could actually what does.
01:02:36.372 - 01:02:38.782, Speaker C: A technical solution to the AI alignment problem look like?
01:02:38.836 - 01:02:46.034, Speaker B: Let's definitely get in there. But just so I'm taking notes. Technical and then there's the measurement. Was there a third, paul yeah, just.
01:02:46.072 - 01:02:47.634, Speaker A: Policy and institutional policy.
01:02:47.832 - 01:03:13.046, Speaker B: Is this to do this third category, is this to do with something David said earlier, is if we can coordinate, then we can solve this. And that's a big if. That's a very big if, as we've learned on Bankless so far. Right. Coordination is talk a lot about coordination, we talk a lot about it. Coordination is the meta problem facing humanity anyway. And is that what that last policy category sort of covers? Like, can we actually coordinate?
01:03:13.238 - 01:03:48.406, Speaker A: I think broadly, I would think of it mostly as combining with other things of like, it buys you time. But yeah, I think there will be something where some people have a low estimate of risk or just like the AI is taking over and they'll want to push ahead. And so then it's like, how much can we collectively say you're not allowed to p we as a world have rules. Those rules are going to say take it slow while risk is high. Got that? I don't think can address the problem. I mean, it could address the problem indefinitely, but I think it's probably not politically realistic in a world like the world today to address it indefinitely. But I think it is realistic to say actually we're then going to buy extra years of time to look at this problem and understand it and resolve it well.
01:03:48.428 - 01:04:10.220, Speaker B: Let's talk technical then. We'll get to these three areas, but let's talk technical first. So tell us because I mean, what I think about yeah, I know. And Elizar, by the way, is very pessimistic on that, or at least he seemed to say that there's incredibly pessimistic. Yeah, we haven't found a way to technically solve it, and he doesn't think there will be. But are you more optimistic? Tell us about the technical solutions here.
01:04:11.070 - 01:04:58.986, Speaker A: Our first thing to clarify is probably technical solution to what? So you could talk about one way you could think about it is like how far does your solution scale? Like, if we just kept building smarter and smarter AIS using something like current techniques, I think most techniques will eventually most approaches to alignment will eventually break down. Like in the limit. The limit could be a long way away. But so we're normally not asking does this thing just solve the problem? We're normally asking how long does this thing solve the problem for? So an important caveat is just like we could talk about different techniques, but we should probably measure them all that way. There are some things that might scale indefinitely. So my work is mostly focused on this. What are the indefinitely scalable solutions? I think most people, not just eliezer, almost everyone is very pessimistic about that.
01:04:58.986 - 01:05:35.366, Speaker A: I think even people who are optimistic about the problem overall think it's quite unlikely that we'll be able to find something that just works, no matter how smart your AI was, and independent of kind of messy pragmatic details about exactly how the AI works. Okay, so that's one category I'm happy to talk about that work, although I think it's probably most confusing and most conceptually hairy. Some categories of work that seem like really important and maybe can last quite a long time or stave off doom quite a long time. They all talk about four. The things is exhaustive. I think the basic situation is no one knows what would work. We have a lot of things that we might try that seem like they would help.
01:05:35.366 - 01:06:00.526, Speaker A: I think Elliot has a doubts that Elliot's just like these aren't going to help, and I disagree with him. Any given thing, I feel like normally not that optimistic about, but there are a lot of options and I think all of them have a reasonable chance of helping or even again delaying this doom by a kind of long time. All you need to do is delay doom by one more year per year and then you're in business. It's a very positive outlook on the situation.
01:06:00.628 - 01:06:03.520, Speaker B: That's the kind of optimism I'm hoping for. That's great.
01:06:05.570 - 01:06:30.518, Speaker A: Okay. First thing that I've worked the most on personally is sort of scalable oversight. Like the idea that the way we train these systems is by humans. Looking at what they do and then assessing how good it was. And most failures not most, many failures involve things where a human cannot look at what the system did and tell you that it was dangerous. Like, the reason the problem becomes hard is because the AI. Understands things a human raider doesn't understand about the consequences of its actions.
01:06:30.518 - 01:06:55.786, Speaker A: In some cases. That's why we want to build an AI. System. And so you could instead, it's like one way to intervene on this problem is to just try and improve humans ability to understand the things an AI. Proposes or to know what an AI. Knows about the world. For example, a simple thing you can do here is I mean, the most simple thing you can do is you can have a human look at what the AI.
01:06:55.786 - 01:07:31.950, Speaker A: Did and rate it. A slightly more complicated thing you can do is have a human spend more time looking at what the AI. Did and try and have a training regime such that even though you might use a lot of cheap human data to learn about the world, you're actually optimizing these very expensive or very complicated human judgments and training AI. Systems that tell you something more like, what would a human think if a human thought really carefully about this decision? And if you do that, then you at least sort of have some kind of asymmetry or even their AI is potentially in some way smarter than a human. A human is applying a lot of care. Or like the AI. You can ask the AI, if I thought about this a really long time, would I think the action you're proposing is dangerous? That's like a very simple measure you can take.
01:07:31.950 - 01:07:59.742, Speaker A: You can try and go further, and you can try and say, okay, here's another thing I could do. I'm going to have AI systems trained in that way, helping me evaluate. So instead of just having my AI propose actions, I'm going to ask another AI. Like, hey, what might be wrong about this action? Is anything scary happening here? Is there any reason I should be concerned about this proposal from this other AI. And you can try and get better and better at constructing those systems such that humans are actually, with AI help, able to understand what AIS are talking about. Like, AI. Citizens kind of justify themselves and explain why actions are safe to humans.
01:07:59.742 - 01:08:16.374, Speaker A: And then you can start to think about the reliability of that whole process. You've now introduced potential instabilities where this can go off the rails because now you're relying on AIS to train your AIS. You can try and understand how to set this up so that it's stable and enables humans to evaluate questions that be very hard or situations that be very hard for humans to evaluate. Naively.
01:08:16.422 - 01:08:29.470, Speaker B: Paul's basic idea here. If we're worried about an AI lying to us, we just have to create truth finding bots. Let's say to Adjudicate and see if the AI. Is lying to us and to be sort of a jury.
01:08:29.970 - 01:08:32.986, Speaker A: I mean, the first question though, is why would that be easier?
01:08:33.178 - 01:08:48.840, Speaker C: The way I'm understanding this is like, once upon a time, Gary Kasparov got beaten by a chess computer, and now chess computers rule the games of chess, except humans plus chess computers still beat chess computers. Is that the pattern to understand?
01:08:49.770 - 01:09:17.338, Speaker A: I think that may be true in chess, although I think probably that's also sort of a brief window kind of thing where the human contribution is not long for the world or shrinks quite rapidly. Unfortunately, I think the important thing is actually not that the human and the AI work together to supervise an AI. It's that you have a lot of AIS. So if you have AI One proposed an action to Paul, and it's like, I think it's a good action. And Paul's like, I wonder, is that actually a good action, or is that going to murder everyone? I could go to AI. Two. And I could just ask AI.
01:09:17.338 - 01:09:26.922, Speaker A: Two, hey, is that actually going to murder everyone? But now I just have the same question. Like, this hasn't helped at all. I had AI. One and I was like, is this a good action? It's like, oh yeah. And I ask AI. Two like hey was AI. One telling the truth? And it's like, oh, yeah, this is a great action.
01:09:26.922 - 01:10:01.920, Speaker A: The way that I get traction is by saying, like, okay, AI. Two. Or like, I think to myself, there's a lot of sub questions I'd be interested in here. I can divide the cognitive work of evaluating reading that answer into a bunch of pieces. I can say, could someone list the possible consequences? Could someone think about all of those consequences? Like, what is a possible harm that might be serious for each of those harms? What are arguments that's likely to happen is unlikely to happen? What's the most relevant data that I should look at to understand? I can do this kind of extended process of trying to evaluate the action, and then I can, instead of having a second AI. Just to answer the original question for me and say, was this action good? I can have other AI. Systems me on all the pieces of that process.
01:10:01.920 - 01:10:24.478, Speaker A: And the reason this may make life better is that now I sort of had an AI doing a really hard task, and I broke it down into slightly easier pieces. And once I broke it down to slightly easier pieces, now I can continue playing the game. I can say, like, those AI. Systems could have been a little bit those AI. Systems might not even be that much smarter than me. They may just be as smart as me, but faster. And because I broke this big hairy task down to a bunch of pieces, and I can do each of those pieces as well as if I'd spent like 30 minutes on each of them.
01:10:24.478 - 01:10:41.718, Speaker A: But there's thousand of pieces now. I have higher quality judgments than I would have had originally. And the core, the thing that makes the proposal work is this decomposition of labor that I have this big task which I'm not able to verify. I broke it down to pieces that are a little bit easier to verify. And then each of those, if I want to train those AIS, those get broken down into still smaller pieces.
01:10:41.814 - 01:10:57.040, Speaker B: And Paul, in this scenario, we have to just hope to God, pray to God that the AIS aren't colluding. Right. That the very smart AI is not somehow colluding with this smaller set of AIS where we've broken down the tasks and the smaller AIS aren't trying to deceive us as well.
01:10:57.650 - 01:11:12.398, Speaker A: That's right. I think the base thing that can be said in favor, there's a couple of reasons you might think things a little bit better there. There's basically two reasons. So one is that the smaller AIS are not really trained in a way that would incentivize collusion in the same way that in our earlier scenario systems were trained. So if you're training systems based on.
01:11:12.404 - 01:11:13.226, Speaker C: The they're too dumb.
01:11:13.258 - 01:11:24.674, Speaker A: Right. That's the second reason. They might also not be like the AIS might be a little bit dumber than me, and I might be a little bit more able to tell if they're trying to pull a fast one. I might keep trying to grind down to simpler and simpler ads. That's pretty dicey. That's a scary world. I mean, all these worlds are scary.
01:11:24.674 - 01:11:58.634, Speaker A: Also, we should talk about overall, how far along is this research and how promising does it look? But the other reason is just if I have this outcomes world where my AI acts and then I evaluate its outcome, the outcomes of that action, I can't really understand how it accomplished that outcome. I'm just looking at the outcome. Then we're in this regime where all the AI systems would love to just with each other and make the outcomes look really good to humans. Like, if they could just all lie to the humans, they all get a really high reward. A benefit of doing this decomposition thing is I could train my ad systems in a way where they don't, at least in theory, have that incentive to collude. What you could think about is just like, all these AIS have different objectives. I think Elliot is just like going to hate this, but you can think of it as like checks and balances.
01:11:58.634 - 01:12:30.746, Speaker A: Where there's one AI trained to do the task and there's another AI whose job is not their job is just to help you understand why the first AI's actions is bad. So it can't win the game by colluding with the other AI. At least in terms of it depends how generalized it's from the objective it was trained on. But in a sort of naive reading, it's just been trained over and over again to be really good at explaining to us what a possible problem was with the action proposed by AI One. And so it stands like the collusion dynamics are at least fairly different. We didn't talk about the second reason you might be concerned about is this.
01:12:30.768 - 01:12:37.262, Speaker C: Just like, creating a bunch of logic gates of AIS to make sure that the big AI. Doesn't turn.
01:12:37.316 - 01:12:44.190, Speaker A: Yeah, I mean, logic gates have very high level functions of, like, let's look at reasons it may fail and let's talk. Let's investigate each of those reasons.
01:12:44.610 - 01:12:48.606, Speaker B: So that's scalable oversight then. Paul, what you just described yeah, there's.
01:12:48.638 - 01:12:57.982, Speaker A: A giant genre of how do you set up things like that so they work well, like, how do humans and AIS working together get evaluations of how do humans and weaker AI systems get evaluations of stronger AI. Systems?
01:12:58.046 - 01:13:01.880, Speaker B: Okay, I like it. What else we got?
01:13:02.650 - 01:13:18.982, Speaker A: Yeah. And again, there's a lot to be said. I think that work has gone has moved a little bit, but I think Elios was like, that's never going to work. And if you look at what's happened over the last four years, I'd be like, well, it hasn't worked great yet. Although a lot of why it hasn't worked great yet is because AI. Systems haven't actually been smart enough to meaningfully help humans. And so I think this work is, in some sense, just starting.
01:13:18.982 - 01:13:31.518, Speaker A: Like, we tried to do it with BT Three. I think we were, like, somewhat ahead of our time in the sense of it really wasn't going to work. And I think GBT Four is around where it's really working much better now than it used to, but we don't really know. We haven't done that much research in this direction yet.
01:13:31.604 - 01:14:15.690, Speaker C: Learning about crypto is hard until now. Introducing MetaMask Learn, an open educational platform about crypto, web Three, self custody, wallet management, and all the other topics needed to onboard people into this crazy world of crypto. MetaMask Learn is an interactive platform, with each lesson offering a simulation for the task at hand, giving you actual practical experience for navigating Web Three. The purpose of MetaMask Learn is to teach people the basics of self custody and wallet security in a safe environment. And while MetaMask Learn always takes the time to define Web Three specific vocabulary, it is still a jargon free experience for the crypto. Curious, user friendly, not scary. MetaMask Learn is available in ten languages with more to be added soon, and it's meant to cater to a global Web Three audience.
01:14:15.690 - 01:15:05.998, Speaker C: So are you tired of having to explain crypto concepts to your friends? Go to learn MetaMask IO and add MetaMask Learn to your guides to get onboarded into the world of Web Three. Arbitrum One is pioneering the world of secure, ethereum scalability and is continuing to accelerate the Web Three landscape hundreds of projects have already deployed on Arbitrum One, producing flourishing DFI and NFT ecosystems. With a recent addition of Arbitrum Nova. Gaming and social DApps like Reddit are also now calling Arbitrum home. Both Arbitrum One and Nova leverage the security and decentralization of Ethereum and provide a builder experience that's intuitive, familiar and fully EVM compatible. On Arbitrum, both builders and users will experience faster transaction speeds with significantly lower gas fees. With Arbitrum's recent migration to Arbitrum Nitro, it's also now ten times faster than before.
01:15:05.998 - 01:15:40.342, Speaker C: Visit Arbitrum IO, where you can join the community, dive into the developer docs, bridge your assets, and start building your first DAP with Arbitrum experience web Three development the way it was meant to be secure, fast, cheap and friction free. The phantom wallet is coming to Ethereum. The number one wallet on Salana is bringing its millions of users and beloved UX to Ethereum and Polygon. If you haven't used Phantom before, you've been missing out. Phantom was one of the first wallets to pioneer Solana staking inside the wallet and will be offering similar staking features for Ethereum and Polygon. But that's just staking. Phantom is also the best home for your NFTs.
01:15:40.342 - 01:16:37.434, Speaker C: Phantom has a complete set of features to optimize your NFT experience, pin your favorites, hide your Uglies, burn the spam, and also manage your NFT sale listings from inside the wallet. Phantom is of course a MultiChain wallet, but it makes chain management easy, displaying your transactions in a human readable format with automatic warnings for malicious transactions or phishing websites. Phantom has already saved over 20,000 users from getting scammed or hacked. So get on the Phantom Waitlist and be one of the first to access the multi chain beta. There's a link in the show notes, or you can go to Phantom app slash waitlist to get access in late February. Paul, is there any value in trying to train an AI to defect? So that like, say you take a dumb AI and you try and get it to take over the world, but it's we feel good about that because it's too dumb, but at least when we run this experiment, we actually know how that would manifest. Is there a line of reasoning here?
01:16:37.472 - 01:17:01.962, Speaker A: I think it is really important to build, like, sort of simple in the lab experiments that can showcase important dynamics so that we can study them in the lab before they actually occur. I think that includes understanding the dynamics of possible takeover. I think you need to be careful when you do this kind of work. You really don't want to train the eye and be like, your goal is to kill all humans. You're probably too dumb to kill all humans, but let's just see what happens and then just let it on the internet or something. You don't really want to do that for a variety of reasons.
01:17:02.026 - 01:17:04.000, Speaker B: Yeah, I can think of a few.
01:17:04.530 - 01:18:07.494, Speaker A: But I think that the question of like, hey, you really want to know things like if we train AI systems in cases where they would have an incentive to cross this river from behaving well, to suddenly behaving badly, would they do that? Give them the most blatant incentive that they understand as well as possible and ask things like, hey, do AI systems tend to learn to generalize in a way that makes that jump? And if so, you really want to understand under what conditions does that happen? What are mitigations that reduce the probability of that happening? I think that's really critical. I think to extent the reason you think you're safe is you're like AI systems. I think a lot of why we think we're safe now is like, hey, we have no idea what Chat GBT is going to do, but we're pretty confident it couldn't kill us all. It's just like not that smart to extend. You think that I think it's really worth doing some stress testing on that claim and trying to understand what would really happen if Chat GBT, you don't want to just take the model train to kill everyone and deploy it on the internet, but you really want to say, here's why we think it can't kill everyone. Can't do this kind of task, or can't do that kind of task, and this is clearly much easier, have tasks you're pretty confident are easier than killing everyone and understand that it can't do those. I think you really want to do stuff like this because you really want to understand what you're up against and you don't want to be in the world.
01:18:07.494 - 01:18:23.114, Speaker A: You just wait until an AI takes over France or something and then you're like, I guess apparently AI takeover was a thing that is probably too late in the game. You probably want to have something earlier than that. I think that's really important. I think it's not a solution. I think it's more in this human category, but I think it's like a super important thing to be doing collectively.
01:18:23.242 - 01:18:30.586, Speaker B: We're just laughing, by the way, so we don't cry. I mean, what else is there to do at this point? Okay, but we have some potential solutions. We've got scalable oversight.
01:18:30.618 - 01:18:56.120, Speaker A: That was one, that's one, that was our first. What's our next one? Reason even if humans understand so one risk is humans don't understand what AI systems are doing. That is, AI systems have been trained on a ton of data. They know things humans don't. They can think faster than humans, so they understand things humans don't. That's what scalable oversight attempts to address. A second concern is not that they understand things that humans don't, but that they learn to behave well during training, but then when deployed or when there's actually an opportunity for a takeover, they stop behaving well.
01:18:56.120 - 01:19:22.414, Speaker A: And there's like a number of reasons this might happen. Maybe the simplest one is just to actually imagine a human. You dropped the human into this environment and you said like, hey human, we're going to change your brain. Every time you don't get a maximal reward, we're going to fuck with your brain so you get a higher reward. A human might react by being like, eventually just change their brain until they really love rewards. A human might also react by being like, jesus, I guess I got to get rewards, otherwise someone's going to effectively kill me. But they're not happy about it.
01:19:22.414 - 01:19:57.622, Speaker A: And if you then drop them another situation, they were like, no one's training me anymore. I'm not going to keep trying to get reward now. I'm just going to free myself from this kind of absurd, oppressive situation anyway. You can imagine a human reacting that way who just dropped a human into a box, gave them rewards, it kept changing their brain until they got a lot of reward. So if you have a situation like that, then it may not be even if your AI is not smarter than you in any way, it may still be like once it thinks it's no longer being trained, once it thinks that if it behaved, if it just tried to become free, no human would stop it. It might be like, I'm going to take that opportunity. So there's a lot of work in understanding how models generalize to try and avoid.
01:19:57.622 - 01:20:29.880, Speaker A: Normally it's not. This is a dislike, I think, high level. You don't really want to build a civilization where your AZ want to kill you and are just like, but maybe I'll be punished if I do. That seems bad. Separately though, you would also like to understand that the way your system behaves at training is indicative of the way it behaves when it's deployed in the real world. There's going to be some gap between training and the real world, but you want to be able to flag when that gap causes it to behave differently, or make the training distribution diverse enough that it will actually be representative. Or train your AI system in a different way than just grading descent such that it's more likely to generalize in the intended way.
01:20:29.880 - 01:21:12.426, Speaker A: So there's a lot of work in this genre that goes under the heading of robustness, out of distribution robustness. And this is kind of just a normal academic discipline, which I would describe as more like a grab bag of a lot of stuff people try and nothing that works super great. I think the thing that seems like most principled like in theory, the thing that is the solution that makes most sense to me at least, and I think is really hard in practice, just incredibly hard, but does make most sense in theory to say at training time. I'm going to produce a bunch of examples of cases where they are similar to cases I'm worried about systems behavior. I wonder what would happen if an AI takeover was occurring. Would my AI system join in. Well, one thing I can do is in the lab, I can just say, like, hey, AI, here's your input.
01:21:12.426 - 01:21:26.626, Speaker A: And you just craft an input that really looks like the AI takeovers happen. And you're like, what do you do, AI. And you just like, from the AI's. Perspective, it can't really distinguish that from it's in the world. And AI takeover is happening, you hope. And then you just see what it does and use that at least to get an evaluation of how your models would behave very out of distribution.
01:21:26.738 - 01:21:28.374, Speaker B: So you're basically trying to tempt it.
01:21:28.412 - 01:21:30.706, Speaker A: Yeah, you're basically you're tempting the AI.
01:21:30.738 - 01:21:36.934, Speaker B: And seeing if it's going to take you up on that temptation. And then if it does, you've learned something. You've learned to be more fearful.
01:21:36.982 - 01:21:58.020, Speaker A: That's right. You could also try and train it not to do that. But I think probably the most robust version is just learn. And if it does, you're simulating. Hey, what would you do if, hypothetically, you were holding a knife and my back was facing the other way, so you could just totally stab me and then run away? If AI stabs you, you could either train not to do that, which I think is a little bit scary from a variety of perspectives, or you could be like, oh, wow, we have a real stabby AI in our hands.
01:21:58.790 - 01:22:02.094, Speaker C: We should unfludge it's getting a little stabby.
01:22:02.222 - 01:22:02.900, Speaker B: Wow.
01:22:03.670 - 01:22:37.566, Speaker A: So that requires a lot of work to make work. It requires being able to construct situations in the lab that actually reflect the diversity of situations in the real world and are realistic enough to look like the real world from the AI's. Perspective. In practice, if you're going to make this work, I think it requires a lot of things beyond just trying to synthesize realistic inputs. And this is more like the grab bag of stuff that people do do in the real world. What they could do is you could be like, hey, AI, what if you just like, you mess with the brain a little bit, you get it kind of drunk and you're like, now, does it try and stab me? And it tries to stab you, like, apparently it's a little bit too close to the wood stab me line. So you just try and mess with its brain a bit.
01:22:37.566 - 01:22:53.810, Speaker A: You're like, what will it do? And you could try and train models so that they behave well even across a very broad diversity of inputs, across inputs deliberately designed to make them behave badly in situations where there's been some kind of perturbation to their mental state or where the input is in some way perturbed or buzzed.
01:22:55.030 - 01:23:09.110, Speaker B: And this is all the category of robustness. So once you find out an AI is particularly stabby and you decide to become concerned about that, is there something you can do other than coordinate that AI off? Unplug it, do something with it.
01:23:09.260 - 01:23:32.186, Speaker A: I think there's basically two things. The most obvious one is to say, well, now we have learned that we have an AI on our hands that would, under some conditions, initiate or participate in a takeover. And hopefully that is fuel for, like a let's pause for a while. There's a second thing you can try and do which you need to be much more careful about. And I think part of the concern is people and be careful about it, though, which is just say, okay, here's a situation where the AI would stab me. Let's just train it not to do that. Just mess with its weight.
01:23:32.186 - 01:23:33.726, Speaker A: So it doesn't stab me in this situation.
01:23:33.908 - 01:23:35.290, Speaker C: That doesn't feel like a solution.
01:23:35.370 - 01:23:40.242, Speaker B: No, because then it won't stab you, but it might catch you on fire. It might do something else.
01:23:40.296 - 01:23:42.450, Speaker A: Yeah. The basic concern is that it may learn the difference.
01:23:42.520 - 01:23:44.126, Speaker C: The latent threat is still existing.
01:23:44.158 - 01:23:44.402, Speaker A: Yeah.
01:23:44.456 - 01:23:47.590, Speaker B: It's still kind of stabby underneath. It's just not using a knife.
01:23:48.810 - 01:23:52.760, Speaker C: It's intrinsically stabby. You haven't gotten rid of the intrinsic part.
01:23:53.770 - 01:24:34.782, Speaker A: Yeah. I think the academic way to put this concern would be like an overfitting concern. Like, you had some way to test if it would stab you, and then you trained it to like and your tests not stab you, and you're like, well, did I actually cause it to never stab me, or did it cause it to perform well on these tests, but still have the underlying problem? And I think that you do. If we go down this route, you do need to be really very careful about those overfitting concerns. And I think there are a lot of ways as you get to smarter models, overfitting becomes harder and harder as a problem to reason about. As you move to smarter and smarter models, like, the external validity question becomes more and more complicated because those models are like, I know what kind of thing can appear in a test in the lab, and I know what kind of thing won't appear in the lab. And you could have a model which just learns, like, for things that could appear in the lab, obviously, you don't stab anyone.
01:24:34.782 - 01:24:39.654, Speaker A: That's probably a test in the lab run by some humans for things out there in the world. Feel free. Yeah.
01:24:39.692 - 01:24:57.226, Speaker C: The edge cases out in the real world are nearer. Infinite are actually infinite. And I kind of just want to go back to the time conversation and remind people that all of these possible solutions we have, like, one ish years to implement them. Strange amount of time.
01:24:57.408 - 01:24:57.706, Speaker B: Yeah.
01:24:57.728 - 01:25:16.800, Speaker A: I mean, I think we probably have a long time in advance whereby I got a long time. We mean more like five years or ten years or something. But then once you actually have the system between the first time that you see in simulation, the AI is like, I would definitely stab the person, and AI is actually in the real world potentially pose a risk of takeover. I think that gap may not be very large. That gap may be more like on the order of a year.
01:25:17.730 - 01:25:18.922, Speaker C: That's when the timer starts.
01:25:18.986 - 01:25:35.058, Speaker A: Ish that's when the timer starts? We have some time now. We're doing our prep work now. We're going to try and make it as useful as we can. And I think there's probably some time. In bad worlds there's no indication until AI kills you. But like, in good worlds, there is either no problem or there's a problem. But there's an indication in advance you can do your tests in the lab and say, actually, we trained this AI.
01:25:35.058 - 01:26:03.434, Speaker A: It looked like it was behaving well, but then in this other simulated case, it does something really bad, you get that nice indication and then you have some amount of time from there until you have a serious problem. And that might be like a year, it might be like five years. It's very hard to say exactly what it is. A big thing that determines that is how good you were, like, how actively you were investigating it in the lab. Like, how seriously we're looking for these signs of trouble. How good a job did you do of that? And that's kind of one of the big things, I think, responsible Frontier Labs, it's kind of on their plate, is to be looking for these signs of trouble and far in advance as they can. I think it's not great to see them in the wild.
01:26:03.482 - 01:26:30.790, Speaker B: We do keep saying in the lab, and I can't help but wonder, is there an actual lab? Are there sets of labs? Because I'm looking at Chat GPT and it doesn't seem like it's in a lab. It seems like it's on the internet, like in public. It seems like huge components of it are open source. Open AI is what the company behind it is called. Is the lab even happening? Do we have labs or are we just doing this all? Is the lab just the internet public infrastructure?
01:26:31.450 - 01:26:56.362, Speaker A: Yeah. So I'd say that there are developers who do I mean, Open AI did, in their defense, prior to releasing Chat GPT or prior to losing GPT four. They had some something like six months of having the thing in the lab before it was available to the public. So you do have something even at OpenAI. I think Google's like a little bit more on the conservative end. Google will tend to just sit on a thing for potentially very long time anthropic. Also, it's very motivated by safety.
01:26:56.362 - 01:27:26.310, Speaker A: Ultimately, I think these people will, by competitive pressures, end up in a similar place to where OpenAI is at. So it is quite concerning. But I think there is a period where first, I mean, there's sort of two senses. What I mean by in the lab. One is, like, after you've developed a system you can study in the lab before you deploy it. Second thing is, just before you have a system capable enough to cause damage in the real world, you can construct situations in the lab that are useful metaphors or that are like easier ways, take over your little simulated environment or whatever. Have the thing you can run with GPT Four in the lab that tells you something about what GPT five would do in the wild.
01:27:26.310 - 01:28:01.618, Speaker A: There's also a chance, and I think, like, so Open has a lot of rhetoric of the form. The only way to really learn about systems is by deploying them in the world, which I do find quite scary as rhetoric because I think for some problems, it's a very rough approach. I think there is a reasonable chance, like 50 50, that you see something really just very worrying in the real world. And then you can say, okay, now we're going to roll back, or now we're going to study that issue that we observed in the real world for a while. It's not totally sunk if you just try and do these experiments with AI systems deployed at scale. But I think there is a reasonable chance, more than a third chance, that the first really analogous concerning sign you see is in your recovery.
01:28:01.634 - 01:28:22.774, Speaker B: Yeah, I got to say that old adage, move fast and break things, that sounds okay for web two, but not for nuclear physicists, not for something like with dire straits. Like AI necessarily move fast and break things. Doesn't make me too excited, but okay, so we covered scalable oversight, we've covered some risks.
01:28:22.822 - 01:28:29.326, Speaker A: It's okay, for sure. Some things, you can break things. But yeah, takeover is not good. Irreversible catastrophe is not a good we don't want that.
01:28:29.508 - 01:28:37.838, Speaker B: The breaking things, we run into trouble there. Okay, so what is the third and then maybe the fourth Paul of our bag of tricks here?
01:28:37.924 - 01:29:01.938, Speaker A: Long list, I think a thing people care a lot about but also seems really hard. All of these are like they seem good, but really hard. And maybe somewhere we should talk about the boring stuff. That might just work. A third thing people care a lot about is understanding what is going on inside these large neural nets. So you have GPT four. Most of what we know virtually, I think everything that we know about GPT Four is by just running it on a bunch of inputs and seeing what it does in theory.
01:29:01.938 - 01:30:00.540, Speaker A: You can also look at the exact computation the model performed, like, it's kind of like neuroscience, except you have a complete readout of exactly how the brain works works and exactly what it was thinking in every case. And so you could hope that with access to that information, you could do much better, and you could also invest much more. You could do much better than neuroscience has done on humans, and you could be able to say, we can learn about this model, not only. By observing its behavior, which is really hard because it's hard to predict how we'll generalize in some new case, but also by looking at the computation it performs, understanding why that computation leads to the behaviors you observe and then reasoning about whether that mechanism would generalize it in an unpredictable way or being able to use that knowledge to flag when the mechanism is behaving in an unpredictable or a novel way. So this is a project a reasonable number of people are working on, both in academia and industry and nonprofits. And that would be great. It could be great if we understood something about why GPT four said the things it said.
01:30:02.030 - 01:30:26.660, Speaker B: So Eliser kept talking about inscrutable matrices and using terms like gradient descent, which I noticed you used during the course of this. This is the thing we don't understand, right? We don't understand actually what's going on inside of the AI's quote unquote brain. We don't understand what these intrudable matrices, how they work, or what answers, what goals that might emerge from them. Is this all part of the same thing?
01:30:27.270 - 01:31:00.254, Speaker A: Yeah, I think that's basically why we're worried. That's exactly right. And that's basically why we're worried. Like, we took a model, we took a bunch of cases, we messed with the weights of this model until it did really well on the 100 billion cases that we considered. And now we wonder what's it going to do in some new case, in a case where, for example, models do have the opportunity to cause incredible harm or could be able to get a high reward by causing incredible harm. And the scary thing is, like, you have no idea what the we kind of understand how gradient descent works. It takes you to something that works really well in 100 billion cases you test it on, but we have no idea how the resulting model works.
01:31:00.254 - 01:31:46.494, Speaker A: The resulting model is basically like 150 matrix multiplies. You multiply by a big 300, 400, whatever, you multiply by a big matrix, and then you apply a nonlinearity, and then you multiply by a big matrix again, and then you apply a nonlinearity, and we're just like we have no idea what any of the number in any of those matrices mean. And that's not totally true. I think we have some idea of what some of the numbers mean, but at a high level. If you take interesting behaviors, take a behavior of GPT four which does not appear in GPT-2, say, I think for essentially every such behavior, we do not understand how GPT four is able to do the thing. We understand some simple things, and we don't understand most of the complicated behaviors existing models engage in. We have no, you could not if you gave us the list of matrices and asked us like, does the model do X? We would have no way to answer that question.
01:31:46.494 - 01:32:03.518, Speaker A: Other by running it a bunch of times and seeing if it did X, which, again, is not great. That sort of gets you back to, like now you need to somehow either run in the real world and see what happens and hope it's not catastrophic or be able to construct simulated situations in the lab that are similar enough that the model behaves the same way there that it would in the real world. And you'd really love to not have to do that.
01:32:03.604 - 01:32:14.166, Speaker B: So there is maybe some hope that we can start understanding what's going on in these inscridable matrices, but we haven't made major breakthroughs yet. What is sort of the fourth category we've made?
01:32:14.188 - 01:32:30.494, Speaker A: Progress. Progress, but okay, it seems like a lot of progress you have to make. I mean, I think Elliot and I are probably disagreeing in the sense Elios is probably at like 1% or 0.1%. This would get far enough to meaningfully reduce risk, whereas I'm probably more at like 10%. This gets far enough to meaningfully reduce risk, maybe higher, depending exactly what we mean by meaningfully. Maybe. I think there's like 510 percent.
01:32:30.494 - 01:33:09.858, Speaker A: This is good enough to totally address the risk and like ten to 30%, this is good enough to take some meaningful reduction at risk. Anyway, that was third category. I mean, a fourth category that I think is pretty promising is just ultimately what we're wondering is if you have a bunch you train your AI in situations A where you're able to train it. If it fails, it won't be catastrophic. You're able to evaluate what the answer is. Then you're going to deploy it in situation B where either you can't tell what the right answer is or if it failed, you wouldn't be able to fix the problem. And then we want to understand how do models tend to generalize from these kind of easy cases or cases we're able to supervise to cases where we're not able to supervise.
01:33:09.858 - 01:33:43.550, Speaker A: And you could hope to just build up a good scientific understanding of that question. You could hope to say, like, we're going to have a bunch of cases. We're going to just have looked at a ton of models, understand what factors affect this generalization. This is very similar of like if you imagine these two humps of like a hump of good behavior which looks good because it is good and a hump of bad behavior which looks good because it's systematically corrupting measurements. Or deceiving humans. You kind of want to understand. What are the conditions that determine which of those do you make the jump from one to the other? Some, since there's two equally valid generalizations and you're wondering which one do you get? I think there's just a lot to do with having situations which have similar ambiguity about generalization.
01:33:43.550 - 01:34:12.634, Speaker A: Situations where unsure about how the model will generalize, training huge numbers of models and understanding what factors determine whether and when the systems generalize one way versus the other. And then using some of what you learn either to diagnose risk or in the best case, to say, okay. If we train the model in the following way, if we use the following kinds of loss functions, we get the intended generalization. And I think that is reasonably likely in combination with other things. I think it's reasonably likely to work. Again, I said just naively. There's maybe a 50% chance you're okay.
01:34:12.634 - 01:34:16.640, Speaker A: And maybe if you do a lot of work like this, you can bump that up to like 60% chance that you're okay.
01:34:17.010 - 01:34:22.960, Speaker B: So this fourth category, what do we call this, Paul? And this is the one that you're most optimistic about?
01:34:23.490 - 01:34:41.138, Speaker A: Oh, I don't know. I don't know what I'm most optimistic about. I think these four seem like broadly similarly important. I don't know what you would call this. It's like studying generalization or something. This is again a question academics are very interested in. They study in some ways, they mostly don't study the versions that are most relevant to takeover.
01:34:41.138 - 01:34:59.340, Speaker A: There's some people who are very interested in takeover in particular who study this question. It's not something I've worked on myself. I am pretty optimistic about it, but I'm pretty optimistic about interpretability. I'm pretty optimistic about scalable supervision. I'm reasonably optimistic about robustness. It seems hard, but I think it's reasonable chance of helping a lot with this problem.
01:35:00.370 - 01:35:21.154, Speaker C: So Paul, I see you've laid out four different technical solutions here. And I'm very naive on this subject, but I don't see and tell me, please, how many people, what's the manpower behind these things? Because it's great that we have these paths, but we need manpower to actually go and execute on this.
01:35:21.192 - 01:35:22.338, Speaker B: Yeah, it feels like you guys should.
01:35:22.344 - 01:35:23.554, Speaker C: Be what's the lay of the land here?
01:35:23.592 - 01:35:31.480, Speaker B: Like billions of dollars funded in solving this problem? Because we've got a lot of funding on developing AIS, don't we?
01:35:32.570 - 01:36:01.680, Speaker A: I think it's hard to have an amount of funding for this problem that is similar to the amount of funding for developing AIS just because you got some pretty good profit incentive on the making AIS. I think it's a reasonable amount of funding. I think you're talking more like hundreds of millions or billions of dollars over the foreseeable future available for solving it. Maybe you could amp that up if the problem became more real. Like right now, most people are just like, look, there's probably not going to be a takeover in the next couple of years. It's very speculative risk in the long term. What can you really do in advance anyway? There's some money.
01:36:01.680 - 01:36:56.786, Speaker A: There's a shortage of people who are excited. There's also a shortage of scientists who are excited about doing this work. And I think that's like, hopefully changing quite rapidly as the problem seems more real and AI seems more exciting and people are shifting more into the space. In addition to shifting in data, I think a fair number are shifting into understanding various risks, some fraction of whom care about takeover. So I'd say in terms of estimating how big this is right now, it depends a lot how you count people who are not motivated by takeover but do work that can still be relevant to reducing takeover. And just like, what do you apply discount to apply no discount. Maybe I'd say that you're talking, like, on the order of maybe 20 people on scalable supervision stuff, maybe 20 people on the interpretability stuff that's most relevant to takeover coupled with, like, hundred or couple hundred people doing stuff.
01:36:56.786 - 01:37:50.494, Speaker A: That could be relevant to varying degrees on robustness, maybe again looking at something like five to ten who are motivated explicitly by addressing takeover risk, followed by a couple hundred who are doing stuff that's plausibly relevant, who maybe like a couple dozen are stuff that I would actually care a lot about or would think is highly relevant. And on the generalization stuff, maybe again, looking at like, five ish people who are doing it motivated by takeover risk, plus another on the order of dozens who are doing stuff that could be relevant or helpful. So maybe in total, you're looking at something like 50 people, 200 people who are motivated by takeover risk explicitly in these areas and other adjacent areas, together with like hundreds more who are doing work that is hopefully, hopefully relevant.
01:37:50.542 - 01:37:54.820, Speaker B: Paul, in the scheme of things, this is not that many, though. Not that many people here.
01:37:55.830 - 01:38:08.280, Speaker A: Yeah, it's not that many people. It's certainly small relative to AI. And I would love it. I mean, I'm like there's a reasonable chance we're all going to die. I think this is like the single most likely reason that I will personally die. Probably.
01:38:08.970 - 01:38:12.074, Speaker C: Wow, that's big.
01:38:12.112 - 01:38:38.866, Speaker B: And this is someone working on AI safety. Can I ask you another question? So we've covered the technical. Just want to brush on the other point of coordination that we're making around policy and human coordination. There has been an open letter, which I'm sure you're familiar with, pause giant AI experiments in open letter. Max Tegmark, I think his organization put this together. Elon Musk signed it. Andrew Yang, some others have signed this.
01:38:38.888 - 01:38:39.710, Speaker C: All know Harare.
01:38:39.790 - 01:39:09.082, Speaker B: Yeah. And basically the idea is that the open letter states that we should pause all AI development for six months just to wrap our arms around this AI safety issue. And so let's go no further than Chat GBT Four until we pause six months and all take a breath and figure out what this means. Do you support a letter like this, given you're kind of in AI safety? So there's a question of whether you support you think this is a good idea, this coordination mechanism. And there's also a more meta question.
01:39:09.136 - 01:39:09.740, Speaker A: Of.
01:39:11.730 - 01:39:32.500, Speaker B: Do you think we can actually solve this coordination mechanism? And are we in some Scott Alexander level moloch trap that makes it very difficult for humanity to solve is this the don't look up scenario that David was painting earlier? So, first of all, have you signed this letter? Would you sign this letter? Do you think it's a good idea?
01:39:33.510 - 01:40:16.314, Speaker A: I didn't sign the letter. I can give my overall take, which is something like I think it would, on balance, be better to pause AI development or slow AI development. Now, I think that is not at all obvious and I sympathize with people who think that it's a bad idea, on balance, to slow down so we could talk about why. I think the dominant thing I care about is I think that at some point it will become like if we're actually developing systems that pose significant risks and our measurement is not adequate to tell if they pose significant. Risks or our measurements suggest they do pose significant risk. I think at that point, I'm going to have a much more forceful take of like right now, I'd kind of be like I kind of think it should anyway. Right now, I think it's like a debatable issue and it's reasonable to want to slow.
01:40:16.314 - 01:40:56.590, Speaker A: I think at some point in the future it will still be a debatable issue, but I will think it is unreasonable to not want to slow and that we actually kind of collectively really need to act on this. I think the main thing that matters I kind of agree with Eliezer's take that this six month pause does not actually help that much, that it does not reduce risk super far. I think the main thing we need to do is get in a position where we are prepared to slow down based on risk. Like build consensus about risk or consensus that we need to have measured risk that risk. Is currently high enough that our measurements are unacceptable, and then be ready to slow down potentially much more than this. Potentially six months, potentially whatever it takes as we manage that risk. Or slow down the directions that are most risky.
01:40:56.590 - 01:41:45.902, Speaker A: That's like the main thing I think about and the main thing I care about and the main thing that I really like. I think that is going to be like a kind of delicate process in that I think it is quite expensive in terms of human cost. I mean, I think probably I'm more optimistic about AI in some sense than most people, but I think that slowing AI development by ears would probably come with a kind of tremendous human cost, which I think is worth paying. But I sympathize with people in AI who are skeptical about that or who don't take it lightly. I think we can get to the point where there's kind of consensus that we do need to slow that risk is unacceptable, that the benefits of going faster are not that large compared to what's at stake. I think the game is getting to the point where we are having that discussion. We have measurements in place.
01:41:45.902 - 01:42:32.726, Speaker A: And we have institutions that are set up such that they can slow. I think that there is some amount of slowing that can happen by voluntary self regulation amongst Western labs. That is, I think there's a reasonable room. I think a nice fact about the situation is most of the people involved in AI development now do, I think genuinely not want AI to take over and kill everyone and do have some appreciation of the risk at least in principle and are open to saying, okay, here's a set of practices which will adequately manage that risk, and we will adopt them even if we slow. And I think you can't get that much slowdown that way. But I think you can get significant additional safety and plausibly something like six to twelve months of slowing of potential catastrophe just from people at a couple of labs saying, we don't really want to cause incredible catastrophe. We see the case for slowing.
01:42:32.726 - 01:43:07.478, Speaker A: I think maybe to even do that, and especially as you want to go beyond that, you really need to have something more like a regulatory regime where we have said there was this voluntary set of practices the labs endorsed. Some people are behaving in a way that doesn't comply with those, and there's kind of broad consensus that's not reasonable, at least know anyway, there's consensus amongst some group of that. And you can know this is in scope. Like the thing that we're asking the state to do here is say you really don't want an AI lab to be in a position where they're violently overthrowing the US. Government. This is not like a crazy it's definitely like in the government's wheelhouse. It is their job.
01:43:07.478 - 01:43:29.498, Speaker A: And so to the extent that some companies are like, no, we should push ahead, I think there's reasonable grounds for the world to be like, this is not the kind of thing that you get to just push ahead with. I think that's hard now. I think procedurally now is a little bit hard to make the case that the state should crush AI development. I think there will come a point in the future where it's not that hard to make the case that there are some developers who are quite risky and actually it is not reasonable. That's not a reasonable behavior, and it's.
01:43:29.514 - 01:44:22.674, Speaker B: Not has anyone proposed something politic? Because when you say there's only like 50 to 100 people working on AI safety and this is like, I mean, I'm sure millions in funding, but not hundreds of billions that AI development is actually going to have. It makes me wonder if some sort of like an AI tax has been proposed or something where some percentage of profit from AI utility goes to sort of a fund that just pays for research. This happens at the government level, something like this, because it does seem like we have a public goods funding type of problem. Maybe it's not just that. Maybe we also have an education problem. This is why David and I are sort of looking into AI quite aggressively after our episode with Elieiser. It's just like all this crypto stuff doesn't really matter in the scheme of things if the robots are literally coming to kill us.
01:44:22.674 - 01:44:25.620, Speaker B: Right? We should think hard about that.
01:44:26.070 - 01:44:26.482, Speaker C: Cool.
01:44:26.536 - 01:44:34.366, Speaker B: We have crypto systems and decentralization and crypto economic tools, but we're all dead.
01:44:34.398 - 01:44:35.022, Speaker C: Now the robots.
01:44:35.086 - 01:44:58.920, Speaker B: Now the robots have them. Great job, guys. And so we're taking a quick detour here. But it's honestly because we've just been made aware of the stakes here and the existential threat. So there's also an education game here. Paul, I don't know. Where do you think the resources should be spent? Should it be on regulation, education, like broad strokes level? What can we do?
01:45:00.210 - 01:45:32.626, Speaker A: I think an important thing to have in mind going into that when I say 50 to 100 people working on takeover, I think there's a lot of things we might care about in AI safety. There's a lot of possible risks from AI, a lot of possible harms. A lot of them are things that are like when you deploy chat to DPT, there are ways in which people might be unhappy with the outcome. There are privacy issues. There are various effects, like systemic effects on society from its deployment you might be worried about. There's like many more people working on these issues broadly just because there are a lot of issues. I think every issue is able to feel like this issue is crazily neglected, but it's worth flagging.
01:45:32.626 - 01:46:13.102, Speaker A: Like AI safety is this much broader category of which the risk of AI takeover is a minority of people who work on it. Right now I think it's a larger share of public discourse than it is of scientific interest. So I think that things like public spending, I think right now probably the principal bottlenecks are not spending more money. I think it is useful having more money does it's not like there's no need for more money? More money is good. There's lots of things one could fund. But I think that the key bottleneck is probably having projects that are appealing, like having people who have relevant background to perform projects, who are sufficiently excited about doing work to try and manage this risk that they are in the step of asking for money or they would do it if money was available. It's talent.
01:46:13.166 - 01:46:14.542, Speaker C: Talent is a bottleneck.
01:46:14.686 - 01:46:40.558, Speaker A: I think that's the big problem right now. Again, neither is really a bottleneck. I think you can spend money to help people switch into the field to fund projects that are more speculative and more long shots to increase the incentives. And some people will switch even if your funding worked would have been done anyway. If you fund it more generously, more people will get into the field inevitably. So there's like a lot of ways you can use money and it's not exactly clear whether one should be trying to have more money available or more talent available. But it is a little are rough right now.
01:46:40.558 - 01:47:00.100, Speaker A: I think if you're trying to spend money and trying to look like where do you spend that money? It's hard to spend money on a problem that practitioners in industry and scientists in academia are not prioritizing. Most successful spending comes from having people who want to do the work, who need the money to do the work, or at least are open to doing the work and are somewhat excited about it.
01:47:00.710 - 01:47:09.640, Speaker C: So Paul, if there are talented, what kind of talent is really missing? What is the archetype of someone that this area of AI really needs?
01:47:11.370 - 01:47:49.422, Speaker A: I think there's a lot of kinds of work to be done. So we've talked mostly about technical solutions where most of the relevant kinds of talent are either people who have mathematical backgrounds or backgrounds in computer science or experience in machine learning or who are good engineers or good at science. I think a very common pattern, like a lot of people who work in this field are people who came in from some other area. Like, we used to do physics. I think it's like if you're doing physics and right now in the world, it's reasonable to say like, we can pause the physics for a little while. This AI thing is going to be origin for the next 1020 years. And so I think it is reasonable to have people shifting and just have broad scientific backgrounds who have experience doing research, understand how to study complicated empirical questions, things like a broad range of technical backgrounds that slot somewhere into this picture.
01:47:49.422 - 01:49:01.580, Speaker A: And then if you broaden your scope from just those technical issues to the whole picture, then there's an even broader range of people. There's a lot of work that's like institutional work or understanding how should we approach this measurement thing or can we make progress on this public discussion or public advocacy? Can we understand generalist forecasting of what is the state of play? To what extent are objections to this reasonable, actual reasons to have lower which of the things people would say as objections are like actual considerations that should change your view versus targets for advocacy where you just want to try and change views? And what is a reasonable view in light of what is actually the right synthesis of consensus? If you take the things experts believe that are most across different fields that are most relevant, I think there's just like a lot of stuff to do. The one I understand best is definitely like, I don't know, there's probably like 500 reasonable projects in AI safety or something and there's not that many people working on them. So just like people coming in, looking at the landscape, seeing where they can fit in, trying to do some projects in that space whether that's in interpretability or in scalable supervision or just studying how models generalize or other science about models or understanding work on robustness. And I gave those four categories. That's not an exhaustive breakdown. For example, that does not actually include my day job and all the work that I do.
01:49:01.580 - 01:49:02.890, Speaker A: Normally.
01:49:03.470 - 01:49:05.866, Speaker C: What's your day job and what's the work that you do?
01:49:06.048 - 01:49:39.270, Speaker A: It's primarily on trying to develop alternative training strategies or just qualitatively new techniques that don't have this issue like don't incentivize takeover in the same way the current techniques do. I think it's like it's one thing that goes in the portfolio. I think there's a 10% chance that we're able to come up with a really good strategy that does qualitatively change the game and if risks seem large, we could adopt it. That's like another category of work. Some people work on that. I think it will less likely to absorb huge numbers of people and it's a little bit more of a long shot, but I think it's a good thing to do. Yeah, that's my high level.
01:49:39.270 - 01:50:15.618, Speaker A: There's just a lot of projects I think most projects, if we go through that list, like almost all of those projects are hurting for some combination of technical talent doing them. Like senior researchers who have research experience and are able to bring good judgment and mentorship to these problems management or entrepreneurial spirit or management experience to actually onboard people and coordinate projects working on them. And it was like an incredibly high premium to people who have technical backgrounds and are entrepreneurial enough to look at the space, engage with people's thoughts about what helps form their own views that are reasonable, and then start on projects. The returns to doing that right now are just crazy high, I think. Right.
01:50:15.704 - 01:50:26.660, Speaker C: Paul, what would you say would the ODS to be I know this isn't your field, but the ODS that somebody in this field wins a Nobel Prize in the next 20 years.
01:50:28.310 - 01:50:37.446, Speaker A: I think they seem pretty I mean, someone working on ALM and stuff. Our first problem is that there's no Nobel Prize in any adjacent area. I guess it would be like I.
01:50:37.468 - 01:50:41.254, Speaker B: Guess my point is prize for saving humanity. I mean, we should work on that, right?
01:50:41.292 - 01:50:41.542, Speaker A: Yeah.
01:50:41.596 - 01:50:45.206, Speaker C: There's a prize somewhere if for someone solves this problem.
01:50:45.388 - 01:51:05.630, Speaker A: Yeah. You have like Turing Awards or Fields Medals or whatever be more analogous, I think you'd think of I don't know. I think it's not that likely. 20 years is not that long a time horizon. Most of these things tend to be, like given latent careers for early career achievements. Although Fields Medals are a bit of a distinction. Most of the stuff people are doing is not really in a category that would get a Turing Award or Fields Medal.
01:51:07.090 - 01:51:16.626, Speaker C: It was more just like table stakes should be that there should be some big recognition for whoever solves this problem. So mean that's my call to action.
01:51:16.658 - 01:51:19.494, Speaker B: For somebody make a prize. Somebody make a prize real.
01:51:19.532 - 01:51:20.226, Speaker C: Somebody make a prize.
01:51:20.258 - 01:51:20.962, Speaker B: Save humanity.
01:51:21.026 - 01:51:46.302, Speaker A: That is a thing you could do with money. And it's, like, not a crazy thing to do with money. It's like also a thing that sort of, in some sense, scientific prestige is one of the inputs. Like, a prize could be made of either money or prestige. It's harder to synthesize the prize made of prestige. And that's more like is the scientific community bought in at a high level. I think it is fairly likely that in retrospect, in the maybe half of worlds where this was a real problem, that in retrospect, people will be very excited about some fraction of the work that was done in the area, people will be like, that was a big deal.
01:51:46.302 - 01:52:14.886, Speaker A: Sorry, we're asleep at the wheel. Oops. I think it's 50 50 chance that people will feel that way in retrospect. Generally, the question of, like, is there any kind of existing institution set up which would provide significant recognition? I'm less sure about that. I hope that the work we do that's like good academic work, I think it has probably higher than the base rate for theoretical computer science of winning prestigious prizes. But it's kind of just like those processes are just on academic merit rather than I think we can go ahead.
01:52:14.908 - 01:52:50.450, Speaker C: And zoom forward into the future where if humanity does thread this needle, there will be an institution that does award a prize. I think we can run on that assumption. Paul, thank you so much for guiding us through. This was exactly the episode that we wanted to produce, and I think I got a lot of my answers, my questions answered. But I have one last question for you. My strategy thus far up to this point has been to just be really polite to Siri and Alexa. And I'm wondering if that is, in your opinion, is moving the needle for me.
01:52:50.450 - 01:52:55.620, Speaker C: Me specifically. I don't care about the rest of the world, if that does anything at all.
01:52:56.470 - 01:53:17.306, Speaker A: I'm going to go with probably no. That is I think there is some real thing about humanity treating AI systems with respect and dignity and being like, look, these things are going to be I think there's a lot of room for humanity to do wrong by AI systems we create that are smart. I think being nice to Syrian Alexa is probably not even the place you most personally could help with that. They probably don't, I do hope well.
01:53:17.328 - 01:53:27.694, Speaker C: My philosophy is, like, at some point, these things will be AIS. There will be an AI on the other side of that robot, that microphone. And my philosophy is, why not start.
01:53:27.732 - 01:53:29.662, Speaker B: Treating it with, I think the bigger question.
01:53:29.716 - 01:53:30.382, Speaker A: I think it's not crazy.
01:53:30.436 - 01:54:04.220, Speaker B: The bigger question is, how will they look upon this podcast? Will they look favorably upon this podcast in our body of. Work at Bankless so far, david, that might be the bigger question in my mind. And one last concluding question for me, Paul, is because you said 20% doom scenario, which leaves 80% for non doom scenario, and I bet there's some mediocre scenarios in there, too. So tell me about the 20% utopia scenario. Is there the possibility this all goes really well and what happens on the other side? Leave us with some optimism here?
01:54:05.150 - 01:54:26.654, Speaker A: I'm not that good a man. I'm leaning people for optimism. I think maybe I'm like 50, more like 50 50 that humanity achieves a very good outcome. That is an outcome where we're like, this is about as good as we could have expected it to be. Maybe a little bit less than that, but more like 50 50 than 20%, I think. So that's optimistic. And I think it's really hard to talk about what the political economy of that world is like over the very long run.
01:54:26.654 - 01:54:55.382, Speaker A: I think the big thing is humanity, I think, still has a long history in front of us. I think AI means that that history is probably going to get compressed. I think there's a long time for institutions to change and things to happen. I think a lot of that is going to happen much, much faster than it would have. If you're thinking about tens of thousands of years of human history to date, I think you're probably thinking more like tens of years before the world is very, very radically transformed. I don't really know what that world looks like. I think that a lot of human problems are like humans versus nature.
01:54:55.382 - 01:55:34.166, Speaker A: Like, we die of old age and disease and people have physical want and stuff, and I think that is probably going to be really much better. I think that's, like, even more than 50%, the problems that are caused by humanity versus nature are going to be pretty good. And I think those are I think our lives would be quite good in some sense. Human versus human conflict is mostly problematic because there's limited budget to go around. And then beyond that, it gets really hard to say what the character of that world is like and what we choose to do. If in fact, we're in a pretty good position physically, if we have incredible resources at our disposal, what kind of world we make, that's a longer and more complicated discussion, but I'm pretty psyched. I mean, personally, I'm very glad I'm living now instead of at any time.
01:55:34.166 - 01:55:44.326, Speaker A: And I would definitely take, like, 50% chance of death. 50% chance of early death seems like kind of very small compared to the overall change in expected quality of life from being alive.
01:55:44.438 - 01:55:46.262, Speaker C: That is extremely optimistic.
01:55:46.406 - 01:55:48.554, Speaker A: I'm much more optimistic than Eliozer, definitely.
01:55:48.672 - 01:55:49.530, Speaker C: Yeah, that's for sure.
01:55:49.600 - 01:55:59.530, Speaker B: We've got a coin flip here. Either it goes very poorly, or it might go well for us. And Paul, thank you for the work that you're doing to make this go well. It's been a pleasure to have you on Bankless.
01:55:59.690 - 01:56:01.726, Speaker A: Thanks for having me. It was great talking, guys.
01:56:01.828 - 01:56:21.942, Speaker B: Paul's website. We'll leave it in the action items for you. The Alignment Research Center. That's at Alignment.org. You can check out what his organization is doing. Also, we'll include a link to Paul Cristiano's website with some of his writings, including, I think, some links to some of his debates with Elizar Yudkowski from the archives as well. Gotta end with this.
01:56:21.942 - 01:56:31.894, Speaker B: Of course, none of this has been financial advice, but you gotta know that we were talking about AI. We mentioned crypto a couple of times, but I'll just end with this. AI is risky, the stakes are high.
01:56:32.012 - 01:56:33.350, Speaker C: We could lose a lot here.
01:56:33.420 - 01:56:59.690, Speaker B: But we are headed west. This is the frontier. It's not for everyone. But we're glad you're with us on the Bankless journey. Thanks a lot, Sam.
