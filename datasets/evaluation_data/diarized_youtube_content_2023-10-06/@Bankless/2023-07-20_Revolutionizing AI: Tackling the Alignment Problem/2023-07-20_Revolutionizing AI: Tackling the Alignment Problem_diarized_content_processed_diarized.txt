00:00:04.410 - 00:00:43.594, Speaker A: Welcome to Bankless, where we explore the frontier of Internet money and Internet finance. And today, on this episode of our Zuzalu series, we are exploring some new frontiers. New frontiers and new technologies, all of which are poised to completely revolutionize the world and change everything about the operating system that society is currently running bankless Nation. Today we are exploring the frontier of AI, which is actually a frontier that we've already been exploring on Bankless. So if you've been listening to our other AI episodes, these will make you feel right at home. AI had a big week at Zuzalu. The AI crypto overlap everyone knows is huge and it seems like such a massive frontier that people don't actually know where to start with it.
00:00:43.594 - 00:01:49.098, Speaker A: Zkml or machine learning models and data that's verified by zero knowledge. Cryptography was a huge topic of conversation, and you'll hear about that in our cryptography episode with Daniel Short. Phil Diane at AI week gave a killer talk titled Mev for AI People, which was this gigabrain presentation about how Mev bots in aggregate kind of presents this omnipotent omnipresent artificial intelligence. And since Mev has been decently, corralled and contained, maybe we can learn a thing or two from the Mev industry in our approach to managing AI risk. There were conversations at Zuzalu about how AI can put the autonomous back into DAOs, and how AI agents could soon be roaming the ethereum landscape shoulder to shoulder with all the human players out there. But mainly at Zuzalu, the AI conversation inevitably converged into the alignment conversation, of which you will find two flavors here in this episode. One strongly pessimistic and then the other characterized by this resigned optimism that is prevalent throughout all of Zuzalu's frontier tech challenges.
00:01:49.098 - 00:02:47.134, Speaker A: Unimaginable rewards blocked by seemingly insurmountable obstacles. Up first in this episode, we have Nate Sores, who is the executive director at Miri, the machine intelligence research institute of which Eliezer Yudkowski founded. Nate's perspective on AI and AI risk is definitely downstream of Eliezer, so we pick up where Bankless left off with Eliezer and Bankless Nation. It's dark, but nonetheless Nate admits that it's less dark than it was a few months ago, now that the world is waking up to the potential risk that AI brings to this world. Following the conversation with Nate is Dagger Turan, who is charging into the AI frontier with his head held high with a clear path forward for himself. Dager believes that the AI alignment problem is actually just downstream of human misalignment and that we actually won't be able to align AI until we align ourselves. This conversation has to do with epistemology what is truth, individual preferences, and how AI models can help us become the best versions of ourselves.
00:02:47.134 - 00:03:31.306, Speaker A: Because if we become the best versions of ourselves, with the assistance of some AI tool, we can collectively produce the best versions of our communities. And if we do that, then our communities can coalesce into the best versions of society, all aided by truth telling AI agents who can help humans navigate through our chaotic world of social organization and politics and social media. Really a fascinating conversation that is actually pretty proximate to our conversation with Tim Urban that we had not too long ago. I'm really excited for you to listen to these conversations, Bankless Nation so let's go ahead and get right into it. But first, a moment to talk about some of these fantastic sponsors that make this show possible. Kraken Pro has easily become the best crypto trading platform in the industry. The place I use to check the charts and the crypto prices even when I'm not looking to place a trade.
00:03:31.306 - 00:03:58.466, Speaker A: On Kraken Pro, you'll have access to advanced charting tools, real time market data, and lightning fast trade execution, all inside their spiffy new modular interface. Kraken's new customizable modular layout lets you tailor your trading experience to suit your needs. Pick and choose your favorite modules and place them anywhere you want in your screen. With Kraken Pro, you have that power whether you are a seasoned pro or just starting out. Join thousands of traders who trust Kraken Pro for their crypto trading needs. Visit pro. Kraken.com
00:03:58.466 - 00:04:29.230, Speaker A: to get started. Today, MetaMask has something new. Introducing MetaMask portfolio. MetaMask Portfolio is the best way to view your crypto portfolio from a holistic level. See everything across all the chains all at once. In your portfolio, MetaMask will report the aggregate value of all the assets in your MetaMask wallets and even the other wallets you import too. But MetaMask Portfolio isn't just a passive portfolio viewer, it is a place to do all of the money verbs that make DeFi so powerful you can buy, swap, bridge and stake your crypto assets.
00:04:29.230 - 00:04:58.602, Speaker A: So not only is MetaMask the easiest place to see your wallets in aggregate, but it's also a powerful battlestation for all of your DFI moves. So go check out your MetaMask Portfolio because it's waiting for you to open it up. Check it out at portfolio MetaMask. IO Arbitrum is accelerating the Web Three landscape with a suite of secure Ethereum scaling solutions. Hundreds of projects have already deployed onto Arbitrum One. With a flourishing DFI and NFT ecosystem, Arbitrum Nova is quickly becoming a Web Three gaming hub. And social DApps like Reddit are also calling Arbitrum home.
00:04:58.602 - 00:05:42.086, Speaker A: And now Arbitrum Orbit allows you to use Arbitrum's secure scaling technology to build your own layer three, giving you access to interoperable customizable permissions. With dedicated throughput all of these technologies leverage the security and decentralization of Ethereum and provide a builder experience that's intuitive familiar and fully EVM compatible. Faster transaction speeds and significantly lower gas fees. Are you a dev but you don't know solidity? With Stylus Arbitrum's upcoming proposal for a programming environment upgrade, developers can write smart contracts in Rust, C, C and many more coding languages. Arbitrum empowers you to explore and build without compromise. Visit Arbitrum IO where you can join the community, dive into the developer docs, bridge your assets and start building your first app on Arbitrum Bankless Nation.
00:05:42.118 - 00:05:51.150, Speaker B: We are here with Nate stories and we are starting the AI week here at Suzalu. And Nate is an AI researcher. Is that how you would call yourself? Alignment researcher?
00:05:52.050 - 00:06:01.534, Speaker C: I would say I'm the executive director of the Machine Intelligence Research Institute. These days it's less research than I'd like. I have done alignment research before.
00:06:01.652 - 00:06:07.074, Speaker B: Okay, can you explain that institution, the Institute? What is that?
00:06:07.272 - 00:07:07.650, Speaker C: So I didn't found it. It was founded by Elias Yurdkowski, I don't know exactly when, maybe around 2001. Fun fact, it was originally founded, it was originally called the Singularity Institute and it was founded because Eliezer wanted to make AGI as fast as he could. And then along the way he realized that it doesn't go well by default and it doesn't go well for free. And so then the organization pivoted to trying to make this AI stuff go well. And for many years the Institute did some research, did some field building, did some awareness raising and so on and so forth until around 2012, 2013, when they pivoted to pure technical research. And this was related to some of the field building, some of the awareness raising, moving to other groups as the field got a little larger.
00:07:07.650 - 00:07:24.490, Speaker C: And I got involved when they pivoted to the technical research more exclusively. So I was originally involved as a technical researcher and then when the previous executive director left, I was the air apparent.
00:07:24.990 - 00:07:28.038, Speaker B: Okay, sorry, the machine what's the name of the institute?
00:07:28.134 - 00:07:30.954, Speaker C: Machine Intelligence Research Institute. Aka Miri.
00:07:31.002 - 00:07:41.120, Speaker B: Miri, okay. So it sounds like Miri has its own trajectory of itself that probably runs in parallel with human understanding, with machine learning at large.
00:07:42.050 - 00:07:56.226, Speaker C: Perhaps, but also perhaps quite think. I don't know the exact years I wasn't around then, but I think it was only a couple of years of work before Eliezer was like, hey, wait, this could get tricky. Right? Okay.
00:07:56.248 - 00:08:03.960, Speaker B: So I actually didn't know that about Eliezer at first. He was like, AGI as fast as possible. And then he was like, whoa, whoa, whoa, AGI as slow as possible?
00:08:04.410 - 00:08:28.990, Speaker C: Yeah, I'm not sure. As slow as possible, but like AI done correctly. AGI done correctly. I think we were hoping for a long time. One of the reasons we do technical research is that often your levers just solve the problem. Like screw slowing people down, that pisses people off. It's like slowing people down is sort of a last resort.
00:08:28.990 - 00:08:53.686, Speaker C: The original hope was can we just solve the problem in time? It doesn't look like we're on track to solve the problem and it looks like we have less time than I was hoping back in 2014. And so I think it is with great sadness that people like Laser and I are now saying we need time, we need more time, can you kind.
00:08:53.708 - 00:08:58.490, Speaker B: Of walk us through your own trajectory? How did you become the executive director at Miri?
00:09:00.750 - 00:09:50.162, Speaker C: Well, if you want to go back far enough, i, at a pretty young age, realized that the world was not very well organized and wasn't. I was in a civics class, and up until that particular civics class, I had some intuition that there were lots of problems in the world, but people were sort of trying to fix them. And the reason there were still all these big problems in the world was that we didn't have the technology. We were still a young race. We were still a young species. We hadn't matured to the point where we could fix these issues. This was sort of like an implicit wordless intuition rather than a conscious belief.
00:09:50.162 - 00:10:16.594, Speaker C: And then I started learning how the US. Government works, and I was like, oh, God, it's like, run by a bunch of monkeys. It's like monkeys invented monkey systems. And it's all working about as well as you'd expect. If it was invented by people who had no idea what the hell they were doing and then allowed to run for hundreds or sometimes thousands of years and just careen off into various so I was like, okay, obviously from the.
00:10:16.632 - 00:10:19.822, Speaker B: Molok and Crypto world, we would call this coordination and coordination failure.
00:10:19.886 - 00:11:04.260, Speaker C: Totally, yeah. So I was very interested in solving coordination failures and more generally, in making the world a better place. And I tried my hand at various versions of that while I was pretty young and bad at things. And it's a hard problem. None of it worked. And the sort of circuitous route is that ultimately I got a job in tech while I was trying to find ways to really move the levers on that problem. Decided to donate a decent amount of money to good causes to sort of keep me honest about actually trying to make the world a better place.
00:11:04.260 - 00:12:01.700, Speaker C: Was trying to research where the best places to put money were donated to some global poverty type charities, and then started bumping into these arguments that maybe this AI stuff is actually one of the biggest places to intervene on the world. I sort of read up on it. There were a couple other factors in my life that were also causing me to notice this AI thing. I read up on it and I was like, oh, jeez, this is just obviously I was looking at the wrong problem. The coordination problems are big and they're real, but this AI thing, humanity lives or dies and gets a great future or no future, depending how this AI thing goes. And I just completely missed this problem for eight years of thinking myself as trying to make the world a better place and going for the heart of the problems.
00:12:04.550 - 00:12:18.666, Speaker B: So when you ran into the AI topic, did you first see AI as a solution to all of our coordination issues, or AI as a problem for all of our coordination issues? Or did you see it simultaneously at the same time?
00:12:18.768 - 00:13:30.746, Speaker C: A little bit of both. I was maybe somewhat primed towards understanding some of the issues with AI due to my work on coordination problems. It's, like, slightly embarrassing, but I was working on various coordination mechanisms that could address the sort of concerns people had at the time. Like, how can a well coordinated society without coercion address, for example, concentration of wealth in ways that the society as a whole doesn't like? And you can set up various coordination mechanisms of you can sort of try to think about what are non coercive ways that a society as a whole can, like, try to both have a market system and not let it get out of control in certain ways. And while while, like, messing around with, like, toy models of this and, like, attempts to, like, prove certain theorems, I just couldn't get some of the results I wanted. And it turns out that I couldn't get some of the results I wanted because nothing stops one actor from being powerful enough that they can just run away with everything. And this was sort of like it was one of those issues where I was sort of like, well, I can get it to work in a lot of cases, but I can't get it to work in all cases.
00:13:30.746 - 00:13:34.100, Speaker C: And then with the AI stuff, I was like, oh, that's why.
00:13:35.830 - 00:13:41.780, Speaker B: Can you elaborate on that? Why is the AI like the kernel of the issue?
00:13:42.550 - 00:15:13.070, Speaker C: I mean, AI is a version of that particular issue. But fundamentally, no matter how good your market coordinations are, your market coordination systems are on Earth. If somebody has the raw technological power to, for example, get what we call in the business a decisive advantage, maybe the easiest thing to imagine given that we already know that trees are machines that turn dirt and sunlight into more trees by stripping carbon out of the atmosphere and building wood, we know that nanotech is possible. If you imagine something that just gets to nanotech before everything else and it can just reassemble you into a more willing trade partner that asks for less of the gains from trade, suddenly all of your coordination mechanisms that were market based and non coercive or whatever melt before this thing. And am I saying that literally happens or it literally is in a market framework? Not particularly. But you can sort of see how I was sort of like trying to put a collaborative agents interacting framework on a physical reality where it's just a fact about the physical reality that things with a sufficient technological edge just can wipe the table with you if they have too much of an edge and if they don't care about you.
00:15:13.220 - 00:15:32.130, Speaker B: Simply put, are you just combining moloch problems? And the Bankless Nation is pretty familiar with moloch problems. We've done a lot of content on Moloch. You combine Moloch problems with exponential technology and then you arrive at some sort of like, logical endgames where humans get their atoms repurposed. Is that more or less the simple articulation?
00:15:32.210 - 00:16:44.234, Speaker C: It's not a bad summary. I wouldn't use exponential in particular. I make no strong claim that it's an exponential curve. My guess is that it's not and it's worse. And much of the issue here is if you make something that is optimizing the world and it's optimizing the world towards some target that doesn't have concern for you in it, I would have much fewer qualms about I would have basically no qualms about human technological development. I sort of am very optimistic about humanity's better natures, and humanity like being able to figure out how to make the world more like we would want it to be upon reflection. And if we were wiser and rather than locking ourselves into totalitarian dystopias, which I think totally could happen, but if we can just ramp up human intelligence and capability and so on without accidentally killing ourselves, I'm pretty bullish on humanity's prospects.
00:16:44.234 - 00:17:04.660, Speaker C: And so it's not so much like, oh, no, technology is coming. It's coming too fast and won't be able to handle it. It's more like, oh, no, we are on the brink of building optimization processes that optimize the future much harder, faster, better than we can and that are optimizing it to a place that has no room for us.
00:17:05.350 - 00:17:19.930, Speaker B: So it sounds like AI is one way, and that might happen. But you're also saying the way that you're talking, it sounds like there's other ways. AI. Or not, when the same hyper optimized future that's not optimized for humans could play out without AI.
00:17:21.790 - 00:18:01.106, Speaker C: I sort of expect we're going to get to Superintelligence one way or the other. AI. Looks to me like one of the basically the only feasible route modulo if humanity can't come together and coordinate to take some other route. I think other routes, like whole brain emulation are probably preferable. Where, to be clear, I'm no carbon chauvinist and I very much want to live in a future with artificial friends where those artificial friends have very different sorts of desires and goals and objectives from me. I'm not like humanity must keep an iron grip on the future. I want space for aliens.
00:18:01.106 - 00:18:52.134, Speaker C: I want space for artificial minds. I want space for life, other kinds of life. The concern here is building a mind that doesn't care for life, that doesn't care for fun, that doesn't care for diversity of experience and interesting arcs and cosmopolitan value and broad inclusive good times. And I think that we are, in fact, barreling towards that cliff edge of making something that fills the universe not with weird valuable stuff, but with non valuable stuff.
00:18:52.332 - 00:18:58.058, Speaker B: Okay, so you've been thinking about these problems for a long time. When did you start at Miri? What year was that?
00:18:58.144 - 00:19:49.318, Speaker C: That was 2014 that I was hired. Once I noticed that the problem existed, I donated, I think, $16,000, which I think at the time put me in the top ten public donors list. And they were like, congratulations, you're now in the top ten public donors list. And I was like, what? And they were like, I don't remember the exact amounts, but they were like, we're doing our fundraiser for $200,000 for our yearly budget this year, $100,000 of which we're trying to raise from the community, and $100,000 of which is like, matched from another donor. And it's like three weeks into the four week fundraiser, and they raised like twenty k of it. And I was like, oh god, this is worse than I thought.
00:19:49.404 - 00:19:57.094, Speaker B: Oh my hilarious. And that was in 2014, or is that precipitated your arrival actually working at miri?
00:19:57.142 - 00:20:00.922, Speaker C: That's right. So I donated more money because I hadn't known it was that bad.
00:20:01.056 - 00:20:03.310, Speaker B: Did you end up funding yourself, your own salary?
00:20:05.330 - 00:21:01.214, Speaker C: I took a very big pay cut moving from google to miri, and I was expecting not to be very skilled at working on these issues, and maybe I'm not, there haven't been a lot of people on these issues. But at the time I was like, how can I help? And they were like, well, maybe if you're good at the math, you can come to some of our workshops. And so I came to one of their workshops, and then a few months later, they were hiring me, and then a year later they were saying, can you run the place? So it I am largely in this field by dint of and this position by dent of showing up early, and for the love of god, people more skilled than me, by all means come replace me. Right.
00:21:01.252 - 00:21:31.926, Speaker B: So that's kind of what I was leading us into. So that was 2014 when you started. It's now 2023, so you're almost there for a decade now. Now AI is having a moment, very much spurred by chat GPT. All of a sudden, crypto podcasts are talking to AI people. What is that trajectory like as somebody who was immediately compelled by the problem at its very essence so far long ago? Now fast forward to where we are now, and kind of the problem seems to be on the horizon. I don't know how close it is.
00:21:31.926 - 00:21:33.530, Speaker B: I don't think anyone does. That's kind of the problem.
00:21:33.600 - 00:21:33.894, Speaker C: Totally.
00:21:33.942 - 00:21:39.820, Speaker B: But here we are nine years later, and now many people are talking about it. Can you just talk about that experience?
00:21:40.130 - 00:22:33.820, Speaker C: Yeah, it's heartening. One thing that I have really enjoyed about it is I've spent many years having conversations with people in the field, many of whom sort of don't really want to hear that their work by default is barreling towards destruction. And so I have these long conversation trees, I have rejoinders to all sorts of counterarguments, and when I go into these discussions, with people on the capability side of things. I have all sorts of responses prepared and I sort of am ready to go down this long decision tree. And then nowadays many more people are noticing the issue and I was invited here.
00:22:35.710 - 00:23:08.022, Speaker B: I think crypto people would actually pretty much really resonate with that. Where we have to explain bitcoin 21 million hard cap. We have to explain all these things like proof of work and the conversation trees that we have to go down. We've built out those innate responses, those spinal reflexes, and then lately moving into 2022 and 2023. Fewer of those things we have to explain, especially as we just printed out a bunch of money for COVID semi checks. All of a sudden we have to explain the concept of scarcity a little bit less. And so it kind of sounds like a similar experience that the AI people have.
00:23:08.076 - 00:23:21.914, Speaker C: Yeah, totally. Like, now I go to people who aren't in the field and I'm ready to go down all these decision trees and they're like, so what's the issue? And I'm like, well, in the most basic sense, here's the issue. And they're like, oh, yeah, that seems rough. And I'm like, oh, man, this is such a different conversation.
00:23:21.962 - 00:23:35.602, Speaker B: I mean, that's the first step, right? The first step is education and also acceptance of the problem. I could imagine for so long you were saying, hey, people would ask you, hey, what are you working on, Nate? And you'd be like, oh, I'm working on AI alignment. And then people are like, Why the hell are you working on that?
00:23:35.656 - 00:24:27.774, Speaker C: Yeah, they're like, oh, that's weird. Or they're like, Is that some weird terminator thing? Yeah. And it's been nice to sort of I sort of think that a lot of the basic issues have been pretty obvious the whole time and that we're now seeing people who don't have distort incentives noticing the issues. But it's really quite heartening to see I don't know where it will go, but it's been nice to see people starting to notice that this is a real thing. It's really on the horizon, like you said. I don't think we know how far. It's very hard to predict, at least with precision, but it has looked to me like one of the biggest issues facing humanity for a while.
00:24:27.774 - 00:24:31.122, Speaker C: And it's very nice to see others start to notice that as well.
00:24:31.256 - 00:24:50.440, Speaker B: So that leads me to the question of just like, how optimistic you are. And I'll ask that in two phases. First, the same question that we asked both Eliezer and also Paul Cristiano was like, all right, what are your chances of doom? What are your chances of the worst AI problem being the worst version of itself?
00:24:51.210 - 00:25:03.680, Speaker C: I mean, worst version of itself, I think is very hard to get. But the version where we all die, there are fates worse than death. But the version where we all die I think this is pretty likely. I think this happens by more than 50%. Oh, definitely.
00:25:04.850 - 00:25:10.800, Speaker B: Paul Cristiano gave us ten to 20%. So you're saying more than 50%?
00:25:11.250 - 00:25:57.774, Speaker C: My understanding of Paul is that he has ten to 20 on the scenarios that I think are like, AI takeover and higher probabilities than that on humanity, completely disempowered. I'm definitely more pessimistic than Paul on these counts. I would say that on my models and visualizations on my understanding of the problem, there is very little hope. And most of my hope comes from me being wrong somehow. And so my probabilities on this destroying everything I know and love are, like, as high as my probability. Like, they're about as high as my probabilities can go, given the fact that I may just be totally wrong and hopefully am. Okay.
00:25:57.812 - 00:26:03.102, Speaker B: So you're pretty close to the leaser side of things, which is like 95% to 99% doom.
00:26:03.246 - 00:26:31.930, Speaker C: Yeah, I mean, I think 99s are hard to get, but there's also a difference between what does the world look like as I see it? The world looks as I see it, as things seem to me, we're just, like, within a rounding of 100%. And the difference between that and my betting ODS is in, like, hopefully the world's not as it seems.
00:26:32.080 - 00:26:32.490, Speaker B: Right?
00:26:32.560 - 00:26:32.794, Speaker C: Yeah.
00:26:32.832 - 00:26:47.502, Speaker B: So what you're saying is the nature of the AI problem is just a lot of we don't knows. And so what you're saying is the reason why you maintain some level of optimism is because there's, like, a white swan event that's possible that could save us.
00:26:47.636 - 00:27:11.698, Speaker C: Yeah. And I've thought a lot about various parts of this problem, and I have various guesses as to where white swans are more or less likely. And for instance, it looks to me like the white swans are less likely in my unknowns about AI and more likely in my unknowns about how humanity is going to react to the problem. Although there are still some unknowns in how AI goes where there could be white swans.
00:27:11.794 - 00:27:12.440, Speaker A: Sure.
00:27:12.890 - 00:27:30.880, Speaker B: Do you remember when you were first working on this problem? I know you weren't as skilled or as knowledgeable back in 2014 to 2017 when you were first working on this problem, but what was your level of optimism or pessimism back then? And how has your attitude towards the problem shifted over the last almost decade that you've been working on this?
00:27:33.570 - 00:27:47.842, Speaker C: It's gone up and down. I've rarely had double digit ODS of survival, but I have had double digit ODS of survival when I've been explicitly quantifying. And most of these numbers are, like, coming straight out of my butt. I let, like, one put too much.
00:27:47.896 - 00:27:51.746, Speaker B: But by definition, everyone's numbers are coming out of their butt. And that's kind of like, yeah, but.
00:27:51.768 - 00:28:12.730, Speaker C: Some people, there's no alternative. You're all right. And I don't spend a lot of time worrying about specific numbers. Once it's less than 50% chance we get good outcomes. It doesn't affect my day to day. I'm not like, staying up trying to calculate significant digits here. I'm like, man, humanity does not look like it is up to this sort of task.
00:28:12.730 - 00:28:19.626, Speaker C: I've seen humanity try to coordinate the.
00:28:19.648 - 00:28:21.242, Speaker B: One thing we have not figured out.
00:28:21.376 - 00:29:16.240, Speaker C: Yeah, and for the record, the reason that the way that I manage to have high probability this is tricky is not that there's any one part of the puzzle that looks to me insurmountable that humans are pretty good at solving problems when they put their minds to them. The reason that I'm pretty pessimistic here is it looks to me like there's a bunch of different ways for things to go wrong. And there's a lot of things that need to go right. For things to go right, you not only need to solve various technical challenges, you need to have uptake of the technical solutions in the relevant organizations. Those organizations need to be able to bureaucratically recognize the difference between a real solution and a fake one. You need to have them caring at all, which is not even a fight that we've won know. You have the heads of labs that like Microsoft and Facebook, like Poo pooing a lot of these.
00:29:18.530 - 00:29:34.130, Speaker B: Like there's like five, six, seven. So I want to combine two metaphors where the stars need to align, except the stars are needles that we also need to thread and that we need all of those things to happen. And you're saying that window is small.
00:29:34.280 - 00:30:24.610, Speaker C: That's where you get the difficulty from. And to be clear, I think it would be a fallacy to say, like, look, I can give you six things you need to do and what's the chance you can get all of them? That sort of reasoning doesn't really work if I line up all six and then the one that I assign least likelihood to happens, probably I underestimated likelihood. Probably I underestimated the correlation. These are not independent events, right? If we can solve the hardest of these issues, whichever one that turns out to be, probably it's because we turned out to have coordination, skill or competence and so on. I'm not saying you can drive their probabilities arbitrarily low by the fact that I can line up a bunch of hurdles. I'm more saying it sure seems to me like there's a bunch of hurdles, man. And each of them well, not all of them, but many of them have a character that humanity hasn't really faced before.
00:30:24.610 - 00:30:31.846, Speaker C: And this all adds up to me being like, man, I'm like single digit probabilities of survival here.
00:30:32.028 - 00:30:46.780, Speaker B: So with this new or maybe first surgeons of interest to the AI problem now thanks, probably thanks to Chat GBT. Thanks for the problem itself. How has that shifted your optimism, if at all?
00:30:51.630 - 00:31:24.134, Speaker C: I mean, I feel a little hopeful about it. I feel like a spark of hope here. It doesn't shift my probabilities. On the ground too much. This is like a really dumb model. But if you imagine having three variables, each with a one in 100 chance and success is multiplying them all together, then raising one of the variables from like 1% to 100% doesn't change your overall probability too much, right?
00:31:24.172 - 00:31:39.674, Speaker B: But it is the first needle that is threaded. So I know it sounds like you don't really like these specific numbers, but if you are like 99.9% doom and then because people are now optimistic, you go down to 99% doom, it's still an order of magnitude, right?
00:31:39.792 - 00:32:21.082, Speaker C: So it does feel like some orders of magnitude on the models that say, we're screwed, right? So the parts of my models that are like, maybe we're fine, are like, maybe I'm just wrong about some stuff. And the parts of my models that say, we're fucked, these models are basically saying you have 0% and it's really like zero points, blah, blah, blah, and then a one or whatever, and there you're getting some orders of magnitude, which does feel hopeful. I'm enthusiastic about that and it ups the probability that I'm wrong about something that matters, like humanity's general ability to coordinate. As would like. So I'm like heartened by it.
00:32:21.136 - 00:32:29.174, Speaker B: It doesn't all the stars we need to align. One has started to show that it's moving in the right direction.
00:32:29.222 - 00:33:04.002, Speaker C: It's moving in the right direction. There's a whole bunch. I'm not like, oh, suddenly the populace is going to realize these problems and react sanelly. There's so many more steps that people can still get off the train here. If you look at the National Response to COVID, people noticed that there was a pandemic going on and that didn't make them respond in any sort of reasonable way to it. Like, maybe AI will be different. There sure were more movies about robots gone wrong than about pandemics beforehand.
00:33:04.002 - 00:33:33.360, Speaker C: At least that would be my guess. But like, many of the movies don't really understand the issues. There's like ample room. Like I there's. Just there's. I've seen politicians try to respond to issues before and I would not say that the social awareness needle has been threaded. It's showing promising signs and that gives me some spark of hope.
00:33:33.360 - 00:33:41.986, Speaker C: But we haven't cleared a whole obstacle in a way that would make me be like, wow, humanity is bringing much more confidence to this issue than I.
00:33:42.008 - 00:34:13.342, Speaker B: Expected not to keep on this one particular line of conversation for too long because it's only one part of the overall figure. But just like, I want to present the argument that we actually don't need all of humanity. I guess we do need governments to coordinate and so we need the leaders and we definitely need those figures. But if the bottom half of the IQ of humanity is like, it's not a problem. And then the top half of the IQ of humanity is like, this is a real problem. I'm going to count that as a big win because we just need the smart people to focus on the problem.
00:34:13.476 - 00:35:40.840, Speaker C: I think that's mostly right. Modulo the issue where which regulations happen and how the government systems move I think does depend a lot on the public, or at least it can. I do think you're right that there are ways that we could resolve the technical issues with such resounding success and it could turn out that the resolved technical issues were sort of so obvious in their property of being a solution or otherwise very beneficial to capabilities such that you just get very big uptake. And that maybe could be done with a relatively small handful of geniuses who can do much better at the problem than I ever could. And maybe that would be a way to just solve the whole issue without going through various other social obstacles or political obstacles or so on. And in that sense, sure, you just need maybe even the one maybe there's one bright person somewhere in the world who would find this problem easy, who hasn't had the opportunity to see the problem yet because the world's really bad at getting resources, right? I wouldn't bet on it at this point. But in that sense I would agree that in some sense we just need the right minds on the problem.
00:35:41.610 - 00:35:50.254, Speaker B: Okay, so pivoting to looking at the problem as a whole. You said your models are effectively close to zero at being able to solve this problem.
00:35:50.292 - 00:35:55.534, Speaker C: On model? Yeah, on models. Cool.
00:35:55.572 - 00:35:59.760, Speaker B: Within your models. Why do your models say that?
00:36:03.190 - 00:37:32.160, Speaker C: It's again, due to sort of like a disjunctive argument. Like many paths lead to doom here and much of the reason is the way that one of the forks is the way that people don't seem to really take the problem too seriously. And it feels to me like there's sort of gradations of this of like people I don't know back in the day a lot of the argument centered around like is artificial intelligence even possible? Is significantly smarter than human intelligence even possible. And once you convince people of this then they're like, well, can it be solved in the next hundred years or whatever. And once you get past this they're sort of like well maybe it'll just be more moral as it gets more smart. And then once you get past this, there's like, well, maybe you can just sort of train it and it's fine, and the train keeps going and people can always find a reason to get off the train at the next stop. And you then can put in a bunch of painstaking effort to try and lay down the arguments as to why the issues are maybe harder than this.
00:37:32.160 - 00:38:36.330, Speaker C: And we just seem like very far from the people at these labs. Running these labs. They're getting off the trains at pretty early stops and even if reality starts beating them over the head with various things. I expect them to only move one more stop or only move, like, as far as reality is forcing them. And then separately. My models say that there are issues that predictably arise only once the AI gains significant capabilities and can be a real threat to you. And if you're in this regime where you sort of need to drag people along and they sort of only start believing things when they empirically see those things, now you have an issue where if there's issues that don't empirically arise until the AI can wipe your civilization off the planet, that's too slow.
00:38:36.330 - 00:38:37.854, Speaker C: It's too slow, right?
00:38:37.972 - 00:39:13.278, Speaker B: I can give like, one model for visual to understanding this. Solving the AI alignment problem is that there's a bunch of decision trees that we need to go down. So I'm imagining literally a tree and say there's like, big tree, big, big tree, big oak tree, and there's a single fruit on this tree. Oak trees don't grow apples, but let's say this oak tree grows apple. There's one apple, and that is the solution apple. And it's very high up in a very far branch away. And we're at the trunk, and we need to find our way to that one single apple, that one single fruit on the tree, except that tree branches eight times.
00:39:13.278 - 00:39:47.078, Speaker B: And then when you have each branch, each branch branches itself eight times. And so it's an exponential problem because you have to choose the right path without knowing where the solution is, without knowing where the exist golden savior apple is. And we need to make the correct path towards the apple without falling down any sort of the dead ends. And so maybe one of the ways to articulate why your models are basically close to zero is that you're just bearish on humanity picking the right branch fork to lead to the solution apple for the number of times that we actually need to do that.
00:39:47.164 - 00:40:18.226, Speaker C: Yeah, that's a pretty decent analogy. I would say that you've got to be a bit careful with arguments like this, because if humanity has managed to choose the past seven forks correctly, you're probably not thinking that there's an independent probability on the eigth. I'm not actually getting my probabilities here from these sort of like, well, it's exponential. Look at all the independent guesses. Once you've been wrong about humanity picking the right branches a few times, you should no longer think they're independent, right?
00:40:18.248 - 00:40:21.620, Speaker B: You can start to be optimistic because like, hey, maybe we're doing something right here, right?
00:40:23.510 - 00:40:49.366, Speaker C: And another thing I would add to the analogy is that a bunch of the branches are full of apples that give you money until they're apples that give you death, right? And everyone's making arguments as to why they can detour into this money branch. And they do give you real money right up until they give you death, right? And then you add that to the fact that humanity has seemed, in practice, to be very interested in wrong branches.
00:40:49.558 - 00:40:51.174, Speaker B: Very susceptible to the Money apples.
00:40:51.222 - 00:40:52.374, Speaker C: Very susceptible money apples.
00:40:52.422 - 00:40:53.450, Speaker B: The money. Death apples.
00:40:53.530 - 00:40:55.210, Speaker C: Right now you're getting, like, a bit closer.
00:40:55.290 - 00:40:55.918, Speaker B: Right?
00:40:56.084 - 00:41:41.818, Speaker C: I will note one place where I am commonly misunderstood is that people think that I think the technical problems of alignment are, like, super duper hard. For some reason, this is basically not the case. My stance is much more like the technical problems of alignment are basically underserved. There's been, like, a few dozen people working on these things for not terribly long. Humanity has spent much more effort trying to solve physics. At least, I think. I'm not actually terribly familiar with how many scientists there were in pre Newton era, in the pre Newton era, in whatever club Newton eventually was in.
00:41:41.818 - 00:42:39.614, Speaker C: But, like, humanity just really hasn't put much of an effort towards these issues. And one thing that makes the problem tricky is that there is much less room for trial and error or semi models predict, given these issues that empirically only show up right around the time that your civilization is getting wiped. And that raises the difficulty level. But it's not like we have turned the best minds of three generations to these issues and they have come up empty handed. It's like we've turned a few dozen weirdos and nerds who are able to be compelled by these arguments ten years ago into these issues, and now we're turning a few more people to these issues. But I am not saying and there's some great technological feat that needs to be pulled off. I'm saying there's like, a normal technological feat and meanwhile, everyone's, like, scurrying around doing something else instead okay.
00:42:39.614 - 00:42:51.214, Speaker C: Where the normal technological feat does have these extra difficulties of, like, you can't do as much empiricism, which may be enough to push it over the edge of like, humans couldn't do it, but in large part, it's just underserved.
00:42:51.342 - 00:43:07.960, Speaker B: Sure. When the last few moments of our Paul Cristiano episode, we asked him, like, what are the bottlenecks, what are the constraints to solving this problem? And his answer was interestingly, not funding. It was talent, it was supply of brain power. Would you agree with that?
00:43:08.570 - 00:43:48.546, Speaker C: Yeah. Or at least funding was a lesser like, money is fungible. And although it's tricky because it's only fungible to a degree, you do have issues if you try to put in a lot of funding that you start to distort the incentives and get people who are showing up grifters. You get grifters. And you also have legibility issues. Where are you distorting the field towards the legible work and away from the less legible, but potentially more important work. I basically think you shouldn't worry about that at reasonable monetary scales right now.
00:43:48.568 - 00:43:57.160, Speaker B: But you should just to really dive into that problem. You're saying legible versus illegible where the eligible is just, like, hard to understand, hard to comprehend, but actually technically correct.
00:43:57.770 - 00:44:02.950, Speaker C: Or in particular, like hard for a grant maker or a funder to evaluate whether you succeeded.
00:44:03.030 - 00:44:13.370, Speaker B: Right, so if you can read the paper and it's simple to understand, the grant maker might like, oh, let's fund that, but it could actually just be a wrong path on the death apple tree.
00:44:13.530 - 00:44:53.174, Speaker C: Yeah. And I basically think you shouldn't worry about this. You could, with sufficient amounts of money, get into these situations where I start to worry that you're distorting the field in that way. But I think talent is lacking. I think there's maybe two kinds of talent that I consider to be relatively different that I think are lacking. One is just like more hands on deck trying to understand how these AI systems that we have today work. We're starting to make fledgling minds.
00:44:53.174 - 00:45:19.730, Speaker C: They're doing stuff that we can't do by hand. We don't know we couldn't program similar capabilities by hand. We don't know what the algorithms, data structures type stuff. We don't know how these things are working. We know how we built them, we know how we got them to work. We don't know internally how they're working. Understanding that would give us quite an edge in figuring out how to point minds at things on purpose.
00:45:19.730 - 00:47:33.820, Speaker C: And I think we sort of want all available hands on deck trying to do that stuff. And then I think that separately there are questions of like the alignment problem I think can basically be factored into this is not quite true, but it's like a fine first approximation can basically be factored into one challenge of how do you sort of make an AI that wants x for some x of your choosing? And then separately a question of like, what x can you put in there such that you're happy with what happened? There's like a bunch of additional issues where the additional issues are like how do you make it be able to do one thing without a ton of side effects you didn't want and then shut down rather than prevent you from turning it off so it can verify forever that it successfully completed its task or whatever. There's issues of how do you and that's sort of like a whole separate pack of concerns. But many people sort of think the problem is what would you ask an AI to do such that the results would be good? But it seems to me the problem is much more like how do you get an AI to do to care about what you want it to care about in the first place? And that seems to me like it takes a different and often less legible type of research that I think can totally be informed by understanding how the current AIS work. These sort of directions go hand in hand. But one of the big things I think we're missing talent wise is the sort of person who has the ambition and the gall to say, I just can take a swing, figure out what the hell is going on with minds. How do minds end up caring about things or pursuing things or having preferences or having targets? How does that work? I think that I can figure out how that works and how to direct it.
00:47:33.820 - 00:48:26.122, Speaker C: Humanity does not have a theory of minds in this way. We do not have a theory of minds that can be pointed. And it's probably not for lack of that theory being possible. It's probably for lack of just having gotten there science wise. And you can sort of come at that from one end, which is just like figuring out how the things in front of us work and then trying to learn what you can about minds and learn what you can about aiming them. I think there are also other ways to come at that that take much, I think less legible research and more independent, visionary sort of research, although I think you need a bunch of vision to make progress in figuring out how the current systems work. But that's another place where I feel like we're really hurting for talent is those ambitious visionaries who just think they can take a swing of the whole alignment challenge.
00:48:26.266 - 00:48:56.680, Speaker B: Okay, so say we fast forward to the future and we've solved the alignment problem, because that's the only future that we'll have to be able to reflect upon this question. In the future, is there going to be a statue of a person who's going to be like, they solved the alignment problem, or is it going to be like a team of people? Or is it going to be not even like a moment where AI alignment solved and it's just like the alignment problem just dies by a thousand cuts? Do you have any sort of mental model for this?
00:48:57.610 - 00:48:58.100, Speaker C: I think these.
