00:00:00.170 - 00:00:45.946, Speaker A: We are creating financial systems that work without relying on any specific country. We are creating forms of privacy that work without relying on a central actor to hold everyone's information in custody for them. We are creating forms of accounts recovery that don't depend on Google or Twitter having everyone master keys. And that's happening with social recovery wallets and account of traction and erc. Four, three, seven. We are creating zero knowledge proof technologies that let people prove that they are trustworthy without revealing any more information about themselves beyond that.
00:00:46.048 - 00:00:46.410, Speaker B: Right?
00:00:46.480 - 00:00:53.120, Speaker A: So we're creating all of these really powerful tools that in a lot of cases are substitutes for more centralized forms of trust.
00:00:55.570 - 00:01:18.834, Speaker B: Welcome to Bankless, where we explore the frontier of Internet money and Internet finance. This is Ryan. Sean Adams. I'm here with David Hoffman, and we're here to help you become more bankless. Vitalik Buterin on the episode today, he's sharing his philosophy with us. He calls it Diac, and he explains what he means by it in this episode. I think there's really three reasons we wanted to have this conversation on bankless.
00:01:18.834 - 00:02:03.970, Speaker B: The first is this. There has been a society wide debate on what to do with technology, specifically AI technology. So we've got the tech accelerationists, that's the EAC community. We've got the tech deaccelerationists, that's the EA community. And the debate is whether we should continue on this path forward towards AI the way we've been doing it, or if we should stop because maybe the robots are going to come kill us. And I know David and I have been hoping to pick Vitalik's brain on this for quite a while, ever since we had our episode with Elieiser Yudkowski, who informed us very politely that we're all going to die. So we asked Vitalik what's his probability of AI Doom? The second reason for this episode is, I think the philosophy that Vitalik lays out is something everybody in crypto can align on.
00:02:03.970 - 00:02:27.770, Speaker B: It's a way to really unite the tribes. It can help us explain what we're doing here to the world and why it matters. And more important, I think coming out of 2022, when we seem to have lost our way in crypto, it reaffirms why we're here. It's a core part of reestablishing our soul and getting to the bottom of things. And the third reason we're having this conversation, it's Vitalikbuterin. Okay, that's enough. He's always got interesting things to say.
00:02:27.770 - 00:02:29.738, Speaker B: David, why was this episode significant to.
00:02:29.744 - 00:03:39.250, Speaker C: You, I think, even more broadly, outside of crypto, society at large is having a conversation with itself as to how fast it wants to go into the future. I think there's some parts of society which is concerned about the vanguard of Silicon Valley and tech innovation moving faster than what society can really, as a whole, keep up with. And then there's other parts of society who's like, guys, you solve problems via technology. The acceleration and the speed of technology will help others catch up as well. And this conversation of how fast we want to go collectively, as humanity, is causing tension in society at large and well as podcasters, what do we do? We try and define the landscape, define the contours of the conversation, help people learn the perspectives of other sides, discover what is signal versus noise, discover what is truth. And so I think when we have clarity on these conversations, we can all be in agreement in the direction that we want to go. And I think this is really what Vitalik did with his blog, and what we were hoping to do with his podcast is help define the conversation a little bit more so humanity can get on the same page.
00:03:39.250 - 00:03:41.730, Speaker C: And so that's why this episode is significant to me.
00:03:41.800 - 00:03:55.798, Speaker B: All right, guys, we're going to get right to the conversation with Vitalik on Diac and his philosophy. But before we do, we want to thank the sponsors that made this episode possible, including our number one recommended, crypto exchange. That's Kraken. Go create an account.
00:03:55.884 - 00:04:25.870, Speaker C: Kraken knows crypto. Kraken's been in the crypto game for over a decade, and as one of the largest and most trusted exchanges in the industry, Kraken is on the journey with all of us to see what crypto can be. Human history is a story of progress. It's part of us, hardwired. We're designed to seek change everywhere, to improve, to strive. And if anything can be improved, why not finance? Crypto is a financial system designed with the modern world in mind. Instant, permissionless, and 24/7 it's not perfect, and nothing ever will be perfect.
00:04:25.870 - 00:04:46.406, Speaker C: But crypto is a world changing technology at a time when the world needs it the most. That's the Kraken mission, to accelerate the global adoption of cryptocurrency so that you and the rest of the world can achieve financial freedom and inclusion. Head on over to kraken.com bankless to see what crypto can be. Not investment advice. Crypto trading involves risk of loss. Cryptocurrency services are provided to us and us territory customers by Payword Ventures, Inc.
00:04:46.406 - 00:05:36.038, Speaker C: Pvi, doing business as Kraken Metamask portfolio is your one stop shop to navigate the world of defi. And now bridging seamlessly across networks doesn't have to be so daunting anymore. With competitive rates and convenient routes, Metamask Portfolio's bridge feature lets you easily move your tokens from chain to chain using popular layer one and layer two networks. And all you have to do is select a network you want to bridge from and where you want your tokens to go from. There, metamask vets and curates the different bridging platforms to find the most decentralized, accessible, and reliable bridges for you to tap into the hottest opportunities in crypto, you need to be able to plug into a variety of networks, and nobody makes that easier than metamask portfolio. Instead of searching endlessly through the world of bridge options, click the bridge button on your metamask extension or head over to Metamask IO portfolio to get started. You know Uniswap as one of the largest decentralized protocols, with over $1.7
00:05:36.038 - 00:06:14.814, Speaker C: trillion of trading volume, but Uniswap is becoming so much more Uniswap X is the newest product from Uniswap labs, which aggregates liquidity across the ecosystem to give you the best defi trading experience. The best part? It's gas free and mev protected. The best prices, zero gas and mev protection all rolled into one app. So head over to app uniswap.org, click the gear icon on the swap page and make sure that Uniswap X is toggled on. And if zero gas trading on Uniswap wasn't enough for you, the Uniswap app is now available on both iOS and Android. Start swapping seamlessly with products from the most trusted team in defi.
00:06:14.814 - 00:06:58.274, Speaker C: Visit app uniswap.org to get started today. Bankless nation I'm extremely excited to introduce you to Vitalik Buterin. You know him as the creator of Ethereum and Ethereum researcher, but today he comes to us wearing the hat of philosopher, which is, I think, what we are increasingly need, at least in the sector of tech, as we are having conversations around the world of technology acceleration, as we are approaching new frontiers both inside of crypto and outside of crypto that are defining society at large. Vitalik recently wrote this post, my techno optimism, which has made the waves in the tech space about what to do about this increasing pace of technology. And that is the subject here on today's episode of Bankless Vitalik. Welcome back.
00:06:58.472 - 00:07:00.900, Speaker A: Thank you, guys. It's good to be here.
00:07:01.270 - 00:07:51.220, Speaker C: This post that you wrote, vitalik, which subtitled my own current perspective on the recent debates around techno optimism, AI risks and ways to avoid extreme centralization in the 21st century, made the rounds inside of crypto and just immediately outside of crypto as well. And this has been, I think, a continuation of a larger conversation that much of the world is having at large with its relationship to the globe's technology sector. So can we set up the conversation and kind of set the table, if we will, because it's happening society wide. Two camps are forming. There's what is known as now recently, the accelerationists, the pro tech, and then the decelerationists, the anti tech. I'm not sure if anti is fair. How would you set the table of this global conversation that's being had?
00:07:52.310 - 00:10:34.818, Speaker A: I'm not even sure if two camps is the right way to describe it because I think what I honestly see in the world and some of the discussions that have been happening in the world is like people being confronted with these completely new issues made by completely new people that they are not used to paying attention to, with completely new memes and weird vocabulary like Shaw Goth and P Doom and timelines and all of these things. And it's almost like an awakening in that for the first time, people are actually thinking, because their existing camps don't really tell them how to think, right? If you think about the even most recent EAC versus effective altruist debate, to kind of give very approximated, crude labels to the two camps, for example, this is not red versus blue. This is not us versus China. This is not Europe versus Russia. This is not woke versus anti woke, right? This is basically a debate that's happening between two groups of people who even two or three years ago, considered themselves to totally be part of the same tribe, basically the kind of San Francisco centered, milk, tech forward, AI leaning, gray tribe, people who went, and in many cases, continued to go to the same dinner parties or in the same social circles. And suddenly there's this issue that has basically pulled them in varied different directions. And if you're coming at it from the outside, then existing tribal mirrors don't really tell you much about it, right? Because if you're the type of person who, for example, thinks that tech people are bad and these are rich, white, male dominated fields that are totally out of touch with the rest of reality, then, well, guess what? Both of these camps have a very large number of people that fit both of those descriptions, right? And both of those camps have, I think, a large number that do not, that also gets underappreciated.
00:10:34.818 - 00:11:35.942, Speaker A: Right. Like, there's a huge international audience for a lot of these AI focused things because similarly to how within the crypto space, things like Zkevms and ERC four, three, seven are kind of resetting a playing field and giving opportunities to people from regions that haven't really been historically well represented in Ethereum. I think a lot of people around the world do also see AI as that kind of opportunity. Right. And then if you're a left leaning person who is very skeptical of corporations and who is pro governments, to put it crudely, but most skeptical of governments when that government is being influenced by corporations, then, like, well, guess know, the EIAC and EA camps are both doing a lot of influencing of governments and there's lots of money on both sides.
00:11:36.006 - 00:11:36.522, Speaker C: Right.
00:11:36.656 - 00:13:14.666, Speaker A: And so I think the way that this conflict really doesn't map to these existing tribal mirrors, it does create this interesting property. Right. In that people actually have to figure out for the first time what actually is their own perspective on the particular issue and how do they actually think about this totally new thing that was not even on most people's radars given two years ago. Right. And I think within crypto, it's the same thing in the sense that I feel like crypto has operated in a bit of this bubble where there hasn't really been too much dialogue between what's happening in the space and some of the discussions, like both technological and political, that are happening outside of it. Right. And I feel like there is to some degree, this kind of extents to which big parts of the space were born in this 2008 era context where we're talking about the chancellor being on the brink for the second bailout for banks as the bitcoin Genesis block, like literally as part of the block body, the discussions around ending the Fed creating an alternative to central banks and all of these things.
00:13:14.666 - 00:13:23.966, Speaker A: And a lot of the big discussions in 2023 are just totally not related to that at all.
00:13:23.988 - 00:13:24.174, Speaker C: Right?
00:13:24.212 - 00:14:21.082, Speaker A: It's like, I'm sorry, but the Israel Gaza situation is not going to be better if those lands ran on sound money. Right? Same with AI. Sorry. Sound money is not going to make P doom go down right. Now. It is possible to kind of overstate this, right? I mean, I think, for example, with the recent election of Argentina, where I feel so far totally unqualified to give a kind of like a, this is good, this is bad perspective on it. But what I have noticed from the sidelines that fascinates me is that Milay is actually talking about economics, and argentinian people do actually care about economics.
00:14:21.082 - 00:14:57.434, Speaker A: Right. And there definitely is an extent to which kind of the US and the rich countries in general, we have, I think, like asbology has pointed out, moved from caring about the economic access to caring about the cultural. Like the issue that emotionally arouses people in the US these days. It's not pensions and health care and savings. Right. Or at least that's not what the newspapers report about. But in places like Argentina, it still is.
00:14:57.434 - 00:15:49.290, Speaker A: And there's a refreshing, sort of grounded in reality aspects to it. So it's important to recognize the ongoing importance of that, but also at the same time, recognize this growing, rapid emergence of these conversations that have just nothing to do with any of those questions. And there's this big question of how does crypto actually relate to these topics. Right. And I think a lot of people who come into the space, come into the space because they have ideals and values and kind of goals and dreams that extend beyond fairly narrow details of this is what the structure of the money is going to look like. Yeah. So I think it's important for the space to try to kind of engage in some of those other topics as well.
00:15:49.290 - 00:16:32.342, Speaker A: Right? Yeah, I think for a lot of those reasons, I've been thinking about some of these other technological topics as well. And one of the things that I noticed, especially this past year in 2023, is that I have a lot of beliefs about blockchains and cryptocurrency and CK snarks. I also have a lot of beliefs about the importance of longevity research. I also have my beliefs about geopolitics. I also have my beliefs about AI. Also have my beliefs about effective altruism. But these sets of beliefs were not really talking to each other enough in a lot of cases.
00:16:32.406 - 00:16:32.826, Speaker C: Right.
00:16:32.928 - 00:17:03.714, Speaker A: And asking the question of what is your actual take on how crypto fits into this larger picture of the world? And do the different parts of that perspective really actually make sense in the context of the other parts of the perspective? Right. One of the questions that we totally should ask, for example, is if AI is so important, then why not drop everything and start working on it?
00:17:03.752 - 00:17:04.146, Speaker C: Right.
00:17:04.248 - 00:17:58.118, Speaker A: And I think there are good answers to that question, but it is a question that actually needs to be like, I was thinking about a lot of these topics. And then, of course, about a month and a half ago, Mark Andreson's techno optimist manifesto came out. And then, of course, an entire spectrum of replies to the techno optimist manifesto came out. And then I started at least thinking about how I would write this kind of piece, and then exube, connect and devconnect, deleted a bunch. And then finally the OpenAI situation just kind of like blasted basically the exact same topic into focus in a lot of people's minds again. And so I decided, like, I'd actually need to get this document out there. And here we are.
00:17:58.204 - 00:18:31.354, Speaker B: Yeah, that's what I very much saw in your post. Vitalik is kind of like it accomplished this goal of maybe creating a unified philosophy for crypto in the broader context of this societal conversation. And I want to set that up because this year is, I think, the year that the societal conversation between this acceleration view, which you called EAC, which stands for effective accelerationism for folks that are not familiar, and this more kind of anti tech type view, effective altruism, maybe to your point, Vitalik.
00:18:31.402 - 00:19:03.282, Speaker A: Well, I mean, just to kind of insert a ten second parenthetical, I think. Just remember that as little as two years ago, the main criticism of effective altruism is that these are tech people who believe that quantitative and technological solutionist stuff is the problem to everything and ignore the nontechnical and immeasurable side of life. Right. And so fast forward two years later, and now it's just interesting to note that they're basically being criticized from the exact opposite direction now.
00:19:03.356 - 00:19:45.554, Speaker B: Right? It is fascinating. And there does seem to be like, to your point, this is all the same tribe that has maybe like in crypto, we'd call this a fork, maybe a social fork, right? Where we've got now these accelerationists coming out and saying, whoa, whoa, whoa, we don't subscribe to kind of some of the anti tech philosophy of some in the AI safety movement. I would say that bankless listeners maybe, and from David and myself's perspective, were first exposed to this through the AI conversation, the AI safety debate. So we had an episode back in February of 2023 with Eliser Yudkowski. That is like what I would say imprinted on my soul. Vitalik. Okay, so it was basically the first time I was exposed in depth to someone who's very intelligent and had been thinking about a concept for decades.
00:19:45.554 - 00:20:46.666, Speaker B: And now here it was with chat, GPT that the AI posed actual existential threat to humanity. And so we went on a quick side quest from our regularly scheduled crypto channel to explore that a bit. And we uncovered that this is not just a question that bankless and people in crypto are facing, but it's a societal level question, and it feels like the rest of society. The reason David put this into two camps, I realized that there's a lot of subtlety and granularity between the two camps, but it's almost like society is being asked to choose. What do you think about technology? Do you want to pedal to the floor and fully accelerate, or do you want to just stop, slow it down and just be cautious? And so I'm wondering, Vitalik, if you could give a quick definition for folks that are unfamiliar. What is the accelerationist view? I think you have a meme of this that opens your article and it says, like, dangers behind utopia ahead. That's the accelerationist view with respect to technology.
00:20:46.666 - 00:21:04.450, Speaker B: And the anti tech view is there's safety behind and dystopia ahead. So we're journeying into this dangerous frontier, I guess I would say, is the anti tech view. Could you just illustrate the core viewpoints of both camps to ground us in this episode?
00:21:04.950 - 00:22:57.462, Speaker A: Sure. So the way that I would think about the effective accelerationist perspective is basically it's all about recognizing the invisible graveyard, right? The invisible graveyard is a phrase that I think either Alex Tabarak or someone else came up with in the context of talking about the harm that the FDA causes in the US just by delaying the extent to which it approves certain drugs, right? Basically that if a life saving medicine gets delayed even by a month because of regulatory hurdles, then that's something that can easily kill tens of thousands of people. And if you do the math, then the amount of people killed by these things potentially goes way higher. And if you kind of zoom out even a bit more broadly, like in the meme, there you have a utopia ahead. And then behind you, you have a bear. And the biggest bear of them all is probably aging, right? Which is a condition that kills about 60 million, like literally a world War II scale number of people every year. And if technology doesn't massively accelerate the base cases that literally all of us, including everyone listening to this podcast right now, is going to die, right? And the gains that come from technology are just massive, right? Like, if you just think about the difference between the kind of life that we have now and the life that we have 1000 years ago or even 50 years ago, there's just a whole number of both measurable and immeasurable things that have massively improved, and these improvements are incredibly large.
00:22:57.462 - 00:23:24.562, Speaker A: And these improvements can even overshadow some of the worst things that happen. And even if you can blame some of those things on technology, right? So if you look at the chart, for example, we can understand what some of those dips are, right? So, like, for example, if you look, there's like a whole bunch of correlated dips in the 1910s, and obviously that's World War I. In the case where there is a double dip, the second dip is the spanish flu.
00:23:24.646 - 00:23:30.446, Speaker B: We're listening, by the way, bankless listeners, at a life expectancy chart over the past 120 years or so.
00:23:30.628 - 00:24:32.734, Speaker A: Yeah, and it has, I think, about eight lines for various countries, US, Europe, Asia. So there's some dips in the 1910s where there's double dips. It's World War I and spanish flu in the 1940s, there's dips, which is obviously World War II. Then China's got a dip in 1960, which is the great leap forward. And so there's very visible disasters along here. But if you just zoom out even a bit, all of these dips ultimately are overshadowed by, just like, the incredibly large gains from medical technology. Right? And so growth in technology does a lot, and even growth in wealth does a lot, for example, right? Because if you imagine a world where everyone is wealthy, then that's a world where if you suddenly have to leave your home and pack up and go to another country, then you're not going to starve.
00:24:32.734 - 00:25:41.658, Speaker A: You're going to basically walk into a much more stable situation than you would otherwise. Right, and so there's just all of these big positives that come from technological gains, and there's like an unrelenting history of thousands of years of technology repeatedly doing good despite lots and lots of people excreeching and complaining that things could end up going in the opposite direction. Right, so that's the accelerationist case, the antitech case. I think it's important to separate the kind of old school anti tech case versus the AI doomer case, more specifically. Right, so the old school anti tech case, it's one that I admittedly am, on balance, very not sympathetic to. And I've written about this in the article, but I think if I had to illustrate kind of the really biggest and most important, the strongest parts of that case, obviously, the environment and climate change are a really big aspect of this. Right.
00:25:41.658 - 00:26:15.358, Speaker A: So there's this chart of how temperatures are suddenly rising in a way that's totally unprecedented in any historical natural situation, except possibly for asteroids that fell many tens of millions of years ago or something similar. Right. Yes. There's the graph. Right. Over the last century and a half, it's just gone vertical. There's graphs of species extinction that are pretty bad.
00:26:15.358 - 00:27:25.034, Speaker A: There's graphs of even populations of particular animals that are pretty bad. And then another kind of aspect of this that starts bleeding into the AI discussion is the possibility of technology getting misused by authoritarian governments. But I feel like that's not a risk of super intelligent AI. But that is a. I mean, super intelligent AI would make the risk worse, but that even is a risk of present day AI and present day surveillance technology. Then there is just the fact that easier communication creates greater economies of scale, and that creates greater centralization, which creates opportunities for political conflict of a scale that totally did not exist before. So you could try to make that argument, though I think, at the same time, Genghis Khan would be a pretty big counterargument to that.
00:27:25.034 - 00:28:11.994, Speaker A: Right? This is the guy who genocided about as many people as Hitler back in the 13th century. But then for some reason, it's, like, totally socially acceptable to sing songs about him today. But Jing Jing Jing is gone. Oh, my God. Yeah, but the. And then I think on the climate side, my counterarguments to that case is basically that there is a history of lots of specific environmental issues that once they became bad, we actually did get together and solve them. And improvements in air quality in cities are a big one.
00:28:11.994 - 00:28:41.102, Speaker A: Right? I remember seeing the tail end of this myself and the first time I visited in Beijing back in 2014. Just remember how incredibly smoggy it is. And that aspect of China improved massively and very visibly over the six years that I went to visit over and over again since then. And then there's, like, ozone and some reforestation in some areas and so forth.
00:28:41.166 - 00:28:41.394, Speaker C: Right?
00:28:41.432 - 00:29:55.194, Speaker A: So that would be the counter to the counter. But then the anti tech argument that is more compelling to me personally is this very specific one about super intelligent AI, right? And super intelligent AI, to me, is one way to think about it, is to think about it as being something that's in the category of technology. So think about it as being the same kind of thing as smartphones, the Internet, contraception, the printing press, the wheel, guns, the steam engine. And these are technologies that in many cases, really were socially disruptive and in many cases, definitely did harm people who depended on the incumbents. But at the same time, if you just look at it from the eye of long term history and you realize that there's massive good that came out of most of them, guns are, I think, more controversial. Right. Because military technology is the one branch of technology where it's, I think, much less clear that improvements are good, though actually, even there, right.
00:29:55.194 - 00:32:16.406, Speaker A: There's an interesting argument that some historians make that guns are sort of more democratizing than the previous wave of military technology, which was Bose, which required, like, ten years of training to be able to use well, and so kind of enabled more centralized forms of government. But even still, military technology is like the other big exception in general, right? But aside from military technology, on average, technology has been crazy good. And so if you think of AI as being technology, then your first instinct is going to be AI is going to be crazy good. And maybe you would worry about AI military applications, right? But if you instead think about AI as not creating a tool, but as creating a new type of mind and creating a new type of mind that is far more intelligent and powerful than the human mind, then this puts us in a totally different category, right? If you think about humans, right, humans have been able to take over and utterly dominate the world and even accidentally genocide all kinds of species of animals, in most cases milk, without even intending or even realizing that that's what we're doing. And humans got into this position of power entirely because of our minds, right? Like, our minds enabled us to create tools, technology, better work together collectively and cooperate and do all of these things. And then now imagine an AI that beats humans on that exact same metric by a factor of 10,000, right? Then the question is like, well, what's going to happen to humans? And the big risk here comes from this argument of that I call, well, two arguments, right? One argument is the difficulty of alignment, and the second argument is instrumental convergence. So the difficulty of alignment is basically the difficulty of just making a thing that has the same kinds of goals that we have.
00:32:16.406 - 00:33:06.614, Speaker A: And this is like a surprisingly hard problem that we just have no idea how to do, right? And there have been plenty of these myths and legends in history that kind of talk about the alignment problem. So there's King Midas, who famously got the touch that turned everything into gold. And then, of course, he ended up dying of starvation. Then there was, I forget who but the greek milk mythical figure who wished for immortality but forgot to wish for eternal youth, right? And so there's, like, the problem that if you don't perfectly specify what you want, then there's lots of ways for that to be just, like, slightly satisfied in a slightly different way.
00:33:06.732 - 00:33:40.260, Speaker B: Do you guys remember that? I think it was an Edgar Allan Poe short story called the monkey's paw, or something like this, where the character receives a monkey's paw that gives him three wishes, and so he would wish for things. One of his wishes, his. He wished for money. And what ended up happening is the monkey's paw granted the wish, but his son died later that day in a tragic factory accident, and so he received some proceeds from the workers compensation policy. That's how the wishes were fulfilled. It reminds me of that as well.
00:33:42.310 - 00:34:47.718, Speaker A: Yeah. I think one kind of rebuttal to this that lots of people make and that I've made is that if you just interact with existing ais, like even Chad GPT just a little bit, these are not hyper autistic robots that have no idea how to understand context and unexpressed intentions and subtext. Right? JGBT will totally be able to understand that. If a human says, I want anything I touch to turn into gold, he has an exception in mind for things like food and water and medicine. Right? This by itself doesn't save us, and it gets a little bit more tricky to sort of explain why. Right? But I can try. Basically, the best analogy, unfortunately, is kind of looking at some of the previous generation of AI that we've had.
00:34:47.718 - 00:35:50.810, Speaker A: Remember back in the mid 2010s when people were starting to work with deep learning ais for the first time and they were starting to get kind of good at making pretty pictures. And there was like a thing that you can do where you can run the model forwards. And if you run the more model forwards, you pass in an image and it tells you, is this thing a cat or is this thing a dog? But then you can also run the model backwards, right? And you can pass in the input that I want the ultimate essence of a dog. I want the thing that really is going to be 100% dog. It's going to maximize the extent to which this classifier is going to tell me that this thing is definitely a dog and it's not something else. And then you run it through and it generates the image that maximizes that. And it turns out that what you get is totally not a dog, right? What you get is some insanely crazy contraption that definitely has doggy aesthetics, but it's also got maybe twelve eyes, maybe 48 eyes, maybe there's like a whole bunch of dogs that have merged bodies.
00:35:50.810 - 00:35:54.846, Speaker A: It just looks like some totally crazy thing.
00:35:55.028 - 00:35:57.358, Speaker C: I'm getting visuals of a kaleidoscope of.
00:35:57.364 - 00:35:59.006, Speaker A: A dog, right, exactly.
00:35:59.108 - 00:35:59.422, Speaker B: Right?
00:35:59.476 - 00:38:22.138, Speaker A: And this is the thing that maximizes the dog parameter, right? And I think another example is us humans have sort of hacked evolution in the same way, right? So if you think about what evolution is, evolution is like this agent that has a goal, right? And its goal is to maximize the reproductive fitness. So survival times, how many children you have of whatever agents it's operating on in their environments, right? And in order to fulfill this objective, evolution gave humans a lot of desires, right? Like there is the desire to have food, the desire to have delicious food, right? And what is delicious and what isn't delicious? Well, those are things that were fine tuned based on what is nutritious in the natural environment, right? And then there's a lot of desires associated with reproduction. There's desires associated with survival and all kinds of things, right? But then look at how modern humans have dealt with these desires, right? One is we've created a lot of food that's, like, hyper optimized for deliciousness, that in a lot of cases doesn't do well at all on nutritional value, right? We've invented at least, like, five separate different types of technology that let people have sex without getting pregnant, right? We've invented all kinds of things that satisfy the proxies that evolution has created in our minds for evolution's goals, but that do not actually satisfy evolution's goals at all, right? And the results of this is that lots of people are eating unhealthy food, and there's increasingly a depopulation crisis with lots of countries having fertility rates that are below one right? Now, one thing you might ask is like, well, surely we as humans know that we are not following the goals of mother nature. And the answer is yes, we know very well that we're not following the goals of mother Nature. But guess what? We are humans. Our goal, maximizing reproductive fitness is the goal that Mother Nature had. It is not the goal that we have.
00:38:22.138 - 00:39:36.382, Speaker A: And we know that Mother Nature was not able to perfectly sort of copy its goal into us. But we don't care. We have our goals, and we follow what our goals are, right? And so in the case of AI, what might happen is we tell the AI, as Alex Friedman would say, to bring more love and peace into the world. And then the AI would discover that, okay, here's a bunch of things that look like love and peace, and these are things that we would all recognize as being love and peace. But then at some point, it would discover, like, wait, if I create this 47 dimensional squiggly that looks in this particular way, then it's going to be even better at satisfying its own internal conception of love and peace, which is going to be slightly off, because anything that is created by any finite process is going to be slightly off. And this 47 dimensional squiggly is going to look incredibly lovely and peaceful to the AI. But it's like nothing that any of us would recognize as being love and peace in any sense, right? And then we go and tell the AI, bring more love and peace into the world, and then the AI just kills all of us and replaces us all with 47 dimensional squigglies.
00:39:36.382 - 00:40:07.246, Speaker A: Right? So this is kind of the AI safety case, right? I mean, this is basically the same as what I remember Elijah Rudkowski told you guys the same thing as what a lot of people have said. So this is like one of my points of concern regarding AI. But in my post, I also talked about two other points of concern regarding AI, where one of them is just this question of like, well, even if everything goes well, is this actually a world that we would want to live in?
00:40:07.268 - 00:40:07.646, Speaker C: Right?
00:40:07.748 - 00:40:53.294, Speaker A: And it turns out that if you examine the Sci-FI worlds that people have tried to come up with that show humans and bots living in harmony, then either the world is insanely unrealistic and it's just unstable and it's just obviously going to collapse into ais dominating everything in another one to ten years, or it's like a world that actually really feels quite deeply unsatisfying from most people's perspectives today, right? We're basically talking about a world where we all become pets of the superbot, and a kind of human agency doesn't really play any part in determining which way the universe goes from there.
00:40:53.332 - 00:41:42.554, Speaker B: And by the way, Vitalik, for those that maybe doubt that a bunch of machines or a bunch of computers could actually rest control over humanity, when I was reading your article, I was in kind of a serious mood, and I was drinking some coffee. I literally spat out my coffee, laughing at this turn of phrase you used. You said this to see why the machines could rest control over humanity. Imagine that you are legally a literal slave of an eight year old child. If you could talk with that child for long enough time, do you think you could convince the child to sign a piece of paper setting you free? I have not run this experiment, but my instinctive answer is a strong yes. And so, all in all, humans becoming pets seems like an attractor that is very hard to escape. I was just visualizing in my mind trying to convince an eight year old child to set me free and what that process would look like.
00:41:42.554 - 00:41:58.820, Speaker B: And that's kind of what we're dealing with when we're dealing with a super intelligence. To them, we would be that eight year old child. It would have probably no trouble convincing us to do whatever it wants to fulfill whatever outcome and set of goals that it had.
00:41:59.990 - 00:43:27.710, Speaker A: Yeah. And then the other thing to keep in mind is that to the extent that there is any notion of competition in this world of the future, whoever really gives up control of the reins to the AI is going to outperform the people that don't, because that's what happens in chess, that's what happens in go. That's just what eventually happens anywhere. Right? And then the third risk that I outlined is basically centralization and surveillance, right? And this isn't even just a risk of super intelligent AI, it's a risk of basically AI of the type that exists already. And this is actually, in some cases, situations that happen already. Right? So one of the things that's been happening in Russia for the past while actually even quite a bit before the recent war started, is that you'd have protests, right? And unfortunately, the authoritarians discovered this one weird trick, which is you let the protest happen and you send the police out and you do the usual protest versus police thing a little bit, but you don't aim to crush it right then and there. But you have the cameras out, and then you identify who all of the key people in the protest are.
00:43:27.710 - 00:43:42.610, Speaker A: And then at some point later, at 02:00 a.m. They get a knock on the door and repeat and rinse about 100 times. And five years later, suddenly you have basically almost no one left to lead the protests. Right?
00:43:42.680 - 00:43:51.270, Speaker C: And meanwhile, every other single country is like, oh, this is a normal, functioning society who's expressing their desires and they are free to express their desires.
00:43:52.090 - 00:44:52.774, Speaker A: Exactly. Yeah. It's much less visible, which makes it much more difficult to coordinate against it, makes it much more difficult to even create outrage against it internationally. Yeah, it's a big problem. I think it's a big part of the reason why it feels like protests against authoritarian regimes have been getting less and less effective for the past while. And if you just extrapolate this trend even further, then the risk basically, is that there isn't a place to hide anymore. Right? There isn't a place where any kind of credible opposition movement to a government could even start, because as soon as it starts, the surveillance can detect it and you don't even need physical police.
00:44:52.774 - 00:45:32.130, Speaker A: At some point, the AI soldiers could just go and shut it down. And this gets even worse when we think about wars, because the need to get the population on your side has historically actually been a pretty significant break, at least slowing down people's desires to go to war. But then if your entire army is a bunch of robots then the dictator gets drunk at 10:00 p.m. They see someone being mean to them on Twitter at 11:00 p.m. And the drones start flying and raiding hellfire on other countries before midnight.
00:45:32.210 - 00:45:32.838, Speaker C: Right.
00:45:33.004 - 00:46:48.762, Speaker A: And so this kind of natural check and balance that comes from the fact that ultimately there are decentralized humans that have to be doing the executing. And if you try to do something really terrible, then those humans are going to be demoralized, and they're going to be much less willing to go along with your plans. And even if they do, lots of them are going to leak every single detail of your plans to the CAA or to whoever your opposition is, which is actually another thing that fortunately did happen in, like, you have AI armies, all of their checks and balances go away, right? And so this is my other big concern about AI. Like, basically is this sort of the ultimate centralization from which at some point there might not actually be an escape. So those three cases are my kind of big note of caution on artificial superintelligence in particular, and how it's pretty unique and pretty different from all the other technologies that we've dealt with over the past ten millennia.
00:46:48.826 - 00:47:19.394, Speaker B: Okay, Vitalik, so David's going to come and summarize this in a moment. So we're tracking our journey so far through this, and then we want to kind of introduce your philosophy here and what you think the counter is and how that applies to crypto. But I have one follow up question to you specifically that's been kind of burning in me since Eliezer podcast back in February. And that is, what do you think? So we talked about three different AI risk scenarios. The first is doom. Basically we all die. The second is we become pets.
00:47:19.394 - 00:47:42.814, Speaker B: And I'm like, at this stage, it's better than one that's not so bad. And then the third is totalitarianism. But I want to go back to the first because that's been giving me, like an existential crisis all year. What's your personal take on this? I know you've got a spectrum in your article on what you called earlier, the probability of doom, the p doom ratio. What's your p doom ratio, and why? I'm just curious where you weigh in.
00:47:43.012 - 00:48:39.778, Speaker A: Yeah. So the number I gave in my tweet thread is 0.1. So 10% chance super intelligent AI is going to kill us all. And I think the reason for this is I see both sides of the argument, right? Like, I see the sort of, quote, doomer argument, which basically is essentially what I've already outlined to you guys. And then I think if I had to give the counter doomer argument, I would basically say something like, look at the kinds of ais that we have now. Those kinds of ais are not even goal seekers. They basically are things that put on human costumes and sort of play out roles of the type that, whatever type, they sort of pattern match themselves into thinking that they're in at that particular moment.
00:48:39.864 - 00:48:40.258, Speaker C: Right?
00:48:40.344 - 00:50:27.670, Speaker A: Like Chet GPT does not act like something that maximizes any particular objective. If you tell it to make the world more doggy, it's not going to do anything that looks like maximize the essence of dog, right? It's going to just give you a five paragraph essay that's a pretty normal and human thing that expresses what might it mean to make the world a more doggy place. And it's the sort of thing that's pretty inoffensive and it looks pretty fine, right? So basically, if you just compare the specific scenarios that people worrying about AI feared would happen with AI at current level of capabilities, and then you compare that to the actual thing that AI at current levels of capabilities does, and it's always quite different, right? One thing I'm happy about is that I feel like the level of harm from deepfakes in particular so far has been much lower than what I think most people expected. And even what I expected at current levels of capabilities. Right? Like, if you explain to someone from back in 2015 what the current level of capabilities of AI making deepfakes is, they're probably going to tell you like, whoa, we can't trust anything that people say anymore. This is going to totally break elections and it's going to lead to all kinds of horrible consequences and some bad stuff has happened. But the reality is much less than that fear, right? Which is interesting and surprising.
00:50:27.670 - 00:51:34.714, Speaker A: To some extent it shows the adaptability of mankind. To some extent it shows that mankind is less evil than some people fear. And then, of course, the doomer would sort of counter by saying, well, guess what? With super intelligence, neither of those two things even matter because the AI is going to be doing all the work. But it does still feel true that the way that things keep progressing do sort of go in different directions than what people's existing worst fears have been, and in ways that sometimes feel like we're going further and further away from the kinds of hyper optimizers that people are afraid of. So that's the case against. And if I had to again counter the counter, I would say, well, that's llms. And it's looking very possible that llms basically tap out at some level of capability and GPT four, and a little bit better than GPT four is basically what we're going to get.
00:51:34.714 - 00:52:19.490, Speaker A: But then there's going to be some next technology which could be combining llms and Q learning, it could be something else, and we don't even know what properties that next level of technology is going to have. And so it's like a big washout, right? And so I think there is a big chance that the AI doom problem is just like, it turns out that it was never that big a problem to begin with. There is like a big chance that it is a really big problem. And within that chance, there is a really big chance that it is a really big problem. But with awareness and hard work, we will be able to deal with it and make sure that we don't actually get doomified.
00:52:19.570 - 00:52:19.958, Speaker C: Right?
00:52:20.044 - 00:52:49.890, Speaker A: So that's where sort of, I think the balance comes in, right? But it's important to keep in mind that a 10% probability of doom is still a big deal, right? So, for example, one analogy for this is like 10% is, I think, greater by somewhere between a factor of one and three, I forget, but it's only a little bit greater than the probability that any of us is going to die from a non biological cause. And so if you think about accident.
00:52:50.310 - 00:52:52.206, Speaker B: Or something like this, right, car accident.
00:52:52.318 - 00:53:33.760, Speaker A: Homicide, suicide, like any nasty thing that's not disease, then if you think about the amount of just care and effort and thought that you personally put into your physical safety and the amount of care and concern that you expect governments to put into your physical safety with things like police, then roughly that level of care is a reasonable level of care to have about the possibility that something really bad is going to happen out of AI, right? And that doesn't mean overturn the entire world to suddenly care about this problem, but it does definitely mean care about the problem more than we do today.
00:53:34.290 - 00:54:23.470, Speaker C: Arbitrum is the leading Ethereum scaling solution that is home to hundreds of decentralized applications. Arbitrum's technology allows you to interact with Ethereum at scale with low fees and faster transactions. Arbitrum has the leading DFI ecosystem, strong infrastructure options flourishing nfts, and is quickly becoming the web3 gaming hub. Explore the ecosystem at portal Arbitrum IO are you looking to permissionlessly launch your own arbitrum orbit chain? Arbitrum orbit allows anyone to utilize Arbitrum's secure scaling technology to build your own orbit chain, giving you access to interoperable, customizable permissions with dedicated throughput. Whether you are a developer, an enterprise, or a user, arbitrum orbit lets you take your project to new heights. All of these technologies leverage the security and decentralization of Ethereum experience. Web three development the way it was always meant to be secure, fast, cheap, and friction free.
00:54:23.470 - 00:54:47.042, Speaker C: Visit Arbitrum IO and get your journey started in one of the largest Ethereum communities. Cello is the mobilefirst EVM compatible carbon negative blockchain built for the real world. And now something big is happening. Introducing the cello layer two. It's a game changing proposal that's going to bring Cello's rapidly growing ecosystem home to Ethereum. Vitalik has shared its excitement for the cello layer two on the cello forum. So has Ben Jones from optimism.
00:54:47.042 - 00:55:18.210, Speaker C: But why? The cello layer two will bring huge advantages, like a decentralized sequencer, offchain, data availability, and one block finality. What does all that mean? Rock solid security, a trustless bridge to Ethereum, and more. Real world use cases for ethereum without compromise, and real world adoption is happening. Active addresses on sello have grown over 500% in the last six months. With the cello layer two, gas fees will stay low, and you can even pay for gas using ERC 20 tokens. But cello is a community governed protocol. This means that cello needs you to weigh in and make your voice heard.
00:55:18.210 - 00:56:12.850, Speaker C: Join the conversation in the Celo forum, follow@celo.org on Twitter and visit celo.org to shape the future of Ethereum introducing GMX, the deepest on chain futures market to trade bitcoin, Ethereum and leading altcoins. GMX is a permissionless, decentralized exchange that offers perpetual futures and spot trading, lightning fast trade, execution and competitive pricing with the security and self custody of a decentralized exchange. GMX is live now with v two, bringing new optimizations to onchain leverage trading and even more than an improved trading experience, GMX will reward you for just participating. All GMX users can easily set up a referral link, and with $12 million of arbitrage grant being distributed as incentives and over $150,000,000,000 in trading volume today, all settled on chain GMX is leading the charge in terms of opportunities for DfI liquidity providers. The future is on chain, with your wallets, with your trades, and with your money in your own hands.
00:56:12.850 - 00:57:15.014, Speaker C: Try it out now at app GMX IO. So Vitalik, I want to kick the stool and throw us all the way back to the start of this conversation, we just went down like this AI rabbit hole to talk about the potential risks of acceleration, which is one of these camps that has kind of emerged in thought as our society is having this conversation. The risks of going forward in time and accelerating our progress with technology, and you've labeled like, well, AI is this exceptional case that we need to consider. The circumstances of some other examples are like climate change. There seems to be a correlation with the increased risk of climate change and the quickening of technology. And this is the decelerationist camp, if you will. And then the accelerationist camp, which I think you have said that you are more resonant with, says that, well, as technology advances, we find these occurrences in history, these wars, these increased capabilities to cause harm, but by and large, they are completely drowned out by all the other focus, all the other developments of technology.
00:57:15.014 - 00:57:47.214, Speaker C: We've kind of like set the stage for these different constructs of thought. And like all extremes, they're all relatively blunt, if you will. If you just only focus yourself in one school of thought, it's a blunt tool for the job. And as we progress forward in your article and in your thought, we start to get a little bit more nuanced, a little bit more. We don't have to be so blunt in our thought about the direction of society. We can pick and choose components from different schools of thought and put them all together. We understand that AI has risks.
00:57:47.214 - 00:58:33.166, Speaker C: We understand that we have to solve climate change. We understand that technology presents new risks, yet nonetheless, technology also helps us navigate those risks all the same. So rather than having to pick an extreme, pick a tribe, if you will, a school of thought and be shoehorned in there, how would you propose we think about all of these different things that we've put together here? If we're talking about a unified idea of thought, how would we have a framework for understanding, if we are going to go forward with technology, what is the correct path, the more optimistic, more precise path that we can maximize the good? Because really, all of these conversations are all about how do we maximize human welfare and well being. At the end of the day, how would you proceed in this conversation from here?
00:58:33.348 - 00:58:40.834, Speaker A: Yeah, so I think this is where the idea of DAC that I came up with in the post really comes out of.
00:58:40.872 - 00:58:41.074, Speaker C: Right?
00:58:41.112 - 01:00:09.920, Speaker A: So AC is obviously acceleration and D is intentionally standing primarily for defense, but also for decentralization and democratization. And so the idea of this philosophy is to basically look at the offense versus defense balance of the world, right? So basically look at how easy is it to do things that harm or go against the goals of other people versus how easy it is to protect yourself or your community against that. And think of that as something that is and has always been shaped by technology, but also something that can be shaped by future choices in technology. And to really focus on building defensive technology and especially, and I think this is the angle that really naturally appeals to people in Ethereum and people listening to this, defensive technologies that work by improving defense in the abstract without kind of coming with a built in perspective or a built in enforcer that decides what is good and what is bad across the entire world or across an entire ecosystem, right?
01:00:10.690 - 01:00:19.730, Speaker C: Would you say, like, phrased differently, defensiveness as a platform or just a technology platform that allows defensive technologies to emerge?
01:00:20.470 - 01:02:00.242, Speaker A: The word platform is interesting, right? Because I feel like this is one of those sort of sometimes vc buzwords that means a lot of different things. The part of platform that appeals to me is like this idea that a lot of this is going to involve creating common infrastructure and both defensive infrastructure and even common infrastructure that enables building many kinds of defensive infrastructure. The part of platforms that I'm kind of like that cautions me against embracing that word is that a lot of things in the modern world that call themselves platforms are things that do contain this centralized actor that controls the thing and that plays this role of Minok deciding who's good and who's bad. We talk about Facebook being a platform, Minok, OpenAI being a platform, Twitter being a platform, and all of these things have centralized actors that run and control them. They have centralized forms of moderation. They have centralization on all kinds of levels. And this is one of those things that creates a lot of problems, right? So there's this book that was written recently that talks about this concept of weaponized interdependence, basically this idea that the type of technology that we've been building for the past 20 years, and when I say we, I do mean the centralized world.
01:02:00.242 - 01:03:55.558, Speaker A: Like, this is the place where the decentralized world gets to pat itself on the back and say like, yeah, no, we're actually better than that, is that the technology has been networks technology, and it's networks technology that creates centralized choke points where the creator of the technology has ongoing power over the users, right? Like, if you think about just going back to the year 1970 and let's say, let's pick a random country that has powerful technology that we don't trust, actually, let's just not say anything bad about anyone in the physical world. Let's say you have, like, Mordor, right? Let's say you have literal Mordor, and it just pops up in the middle of the Atlantic, and it turns out that that's what Atlantis was the whole time. And it's the technological superpower, right? And imagine you're in the 1970s and you're buying cars and forks and knives and all kinds of things that are made in Mordor. And we asked a thought experiment of, like, what is the worst that the Mordorians can do to you? And I think probably the worst that they could do is, one, they could do false advertising. They could create things that look like they satisfy certain properties but actually way underperform on durability or on safety, for example, right? And that is bad. Probably the most evil thing that they could do is what we call the razor blade model, right? Like, basically sell you devices, but then those devices end up kind of depending on these sort of attachment parts that have to constantly get renewed. And it turns out that Mordor is the only vendor of, like, that's the closest that Mordor could do to really getting power over you just by being a vendor, right.
01:03:55.558 - 01:05:16.786, Speaker A: Otherwise, it's like, even if you're very anti Mordor and you have a Sauron must die poster on your wall and you go around wearing free serif on gold t shirts, Mordor can't really do much to hurt you, right. But then fast forward to 2020, right? In 2020, we have networks, technology, and if Mordor builds smartphones and you use a smartphone from Mordor, well, the smartphone can spy on you. If you use Internet platforms from Mordor, those platforms can censor some political viewpoints and promote other political viewpoints. Right. And it's going to tell you that, hey, this free Syria found goal thing is actually a bunch of terrorists and nobody's allowed to support them anymore. It can affect the domestic politics of other countries. It can at any particular moment, just, like, flip a switch and take away the technology from any subset of its users, right? And so the amount of power that you have over users by being the producer, at least if you're building this kind of centralized network, technology has just gone way up now compared to what it was in 1970.
01:05:16.786 - 01:06:02.954, Speaker A: Right. That's the aspect of platforms that actually, it's one of the things that this is even reacting to. Right. It's even one of the things that the, I think crypto space is really reacting to. Right. And so the goal here is to build defensive technologies that are not like that, right. Defensive technologies that do not assume that they are being built in America, and they are going to be good because everyone in the world agrees that America is good, right? Because unfortunately, this kind of consensus does not exist in the world, right? And what we want to do is we want to build technologies where people can trust them, even if they have different opinions on what's good and what's bad.
01:06:02.954 - 01:07:10.534, Speaker A: And these are technologies where there exist a lot of really interesting examples of already, right? So I split defensive technology into four different parts. The first split is the split between the world of bits and the world of atoms, right? And in the world of atoms, you have defense against big things and defense against small things. And defense against small things is, of course, biodense. And then in the world of bits, we have what I call cyber defense, which is like defense against things where if you look at them hard enough, it's obvious that they're attackers. And then what I call info defense. And this is like a very specific distinction that I think other people haven't quite made in the same way that basically is about defending against things where there is much less consensus about who the attackers are, right? And the big example here is what we call misinformation, right? Like, people don't want to be misinformed. People want to know the truth and not know false things.
01:07:10.534 - 01:08:17.658, Speaker A: But a lot of the sort of, quote, anti misinformation ideas that have been proposed by the mainstream world or what we might call the centralized world, they all involve there being a particular actor who understands what's right and what's wrong and basically forces that perspective across an entire ecosystem, right? And so the question is like, well, can we build tools that actually avoid having that central point of deciding what's good and what's bad for everyone. And so in the case of the world of atoms, this is kind of somewhat easier for macro. For example, I talk about building resilient physical infrastructure. The fact that we have solar panels and the fact that we have such amazing batteries now is amazingly good, right? And if every household had those kinds of things, then the amount of disruption that would happen to people's lives, even as a result of cyber war or even regular war, would already be significantly lower.
01:08:17.744 - 01:08:18.090, Speaker C: Right?
01:08:18.160 - 01:08:23.150, Speaker A: If we had much more distributed agriculture, then that would improve things even more.
01:08:23.220 - 01:08:23.646, Speaker C: Right?
01:08:23.748 - 01:09:45.814, Speaker A: So there's things that are just obviously defensive without having to come with an opinion attached of who is the one that you're trusting to do the defending for you. And then in the biospace, there's vaccines, there's other kinds of prophylactics, there's things that boost your immune system. I basically talk about how there's this entire set of things that we can do that we totally are not putting enough resources into right now that could totally create a much more airborne, pandemic resistant world where we have much less Covid, much less long Covid, and even much less colds and flus, and where lots of diseases would basically stop before they even start because there are zero, would end up even being less than one in this kind of world. But we just need more funding and more effort to actually make this happen. And then in the world of bits, this is where crypto stuff really starts coming in somewhat. Right? So one of the things that I began this whole episode with is asking the question, well, crypto needs to also think about some of these issues that people are really thinking a lot about in 2023, and what is the way that cryptos plays into some of those concerns. And here it is.
01:09:45.814 - 01:11:02.126, Speaker A: Right. Basically, we want to create a world that has much more digital hardness baked into it by default, and where digital attacks become much harder and digital defense becomes much easier, right? And what's interesting about the cryptocurrency and blockchain space is that it's great at doing that without relying on a single centralized party, right? We are creating financial systems that work without relying on any specific country. We are creating forms of privacy that work without relying on a central actor to hold everyone's information in custody for them. We are creating forms of account recovery that don't depend on Google or Twitter having everyone's master keys. And that's happening with social recovery wallets and account abstraction and ERC. Four, three, seven. We are creating zero knowledge proof technologies that let people prove that they are trustworthy without revealing any more information about themselves beyond that.
01:11:02.126 - 01:12:38.458, Speaker A: So we're creating all of these really powerful tools that in a lot of cases are substitutes for more centralized forms of trust. And one of the arguments that I make in that section is basically that one of the reasons why the Internet has become more centralized and a less free place over the last 15 years is basically because there are threats. And the easiest and laziest responses to threats that you could implement are responses that involve centralization, right? Like require everyone to have a Google account to sign in. And that's your anti symbol mechanism, right? And the question is like, well, how can we actually bring privacy back? How can we actually bring the ability for anons to participate in the Internet back? How can we actually let people do all of the things that they need to do without creating these mechanisms where if you're in one of the, quote, good countries, then you're trusted, but if you're in one of the untrusted countries, then you're screwed. And these are things that actually happen. Right? I love community notes, for example, and I talked about community notes very positively, but I remember there was this one thing about it that at least when I checked a couple of months ago, in order to join community notes, you needed to have a phone number from a, quote, trusted carrier. And I remember seeing a tweet from someone in India basically saying, like, hey, guys, you just made one of the major carriers in India that serves hundreds of millions of people untrusted.
01:12:38.458 - 01:13:01.574, Speaker A: And there's this big population that's like 5% of the world that's locked out of being a community notes participant. And you can see how those kinds of problems just naturally come out of this centralized perspective on trust. And so if we can create better and decentralized alternatives, then this ends up really solving that kind of problem as well.
01:13:01.612 - 01:13:02.294, Speaker C: Right?
01:13:02.492 - 01:13:43.650, Speaker A: So that's cyber defense, right? Basically, all of the stuff that we've been working on in terms of creating more decentralized and more robust financial systems, in terms of creating these zero knowledge proof systems that let us prove that we're good guys without revealing any other information, that let us prove computations like all of these things, they make a much more defense favoring world. And so it's amazing that our space has been accelerating these technologies so much, right? And so that's kind of the core of the way in which crypto fits into the DAC vision.
01:13:44.070 - 01:15:02.750, Speaker C: I'm getting a notion, Vitalik, of Daniel Schwachtenberger and his metacrisis concepts, where we have the increasing capabilities, increasing capacities of technology to do stuff, good or bad, doesn't really matter, just stuff generally neutrally. And some of that stuff sometimes ends up as, like, bad outcomes or just problems that need to be solved. And the concern here is that when technology introduces new problems to society, that society just comes up with centralized solutions to that problem. Or corporations and entrepreneurs can just move quickly and solve problems before humanity can come together to coordinate on a mutually assured platform, a mutually assured defensive technology. To answer this, and I think what I'm hearing from you is that, well, certain elements of cryptography coordination via Ethereum allows for a solution space to emerge that isn't merely just some large company slapping on that patch onto society at large by saying, hey, here's our solution. It seems to be like what you are illustrating here is like there is a middle ground between the chaotic production of high capacity technology and just like, centralized companies solving that problem space. Is that a fair illustration?
01:15:04.530 - 01:15:37.240, Speaker A: Yeah, I think the really important piece of this is basically creating these technologies that kind of improve the baseline defensiveness of the world, while at the same time allowing the world to remain and be even more of a pluralistic place. Right. So avoiding the usual trap where you basically have danger all the way up until one group just takes over everything and imposes its will on everyone else.
01:15:38.110 - 01:16:33.210, Speaker B: So, Vitalik, what you're proposing here is maybe a philosophical framework for where crypto fits, right. And the reason I really like this is because it seems to be like kind of a big tent, and it's something that I personally resonate with. So you're basically saying you don't have to pick the effective accelerationists, the techno utopians version of the world, you don't have to pick the techno phobes vision of the world and kind of the Luddite doomerism picture. Either this is a third way, and you're calling this defensive or decentralized. The D can stand for all sorts of different things, or democracy, or differential accelerationism. So this is DAC, basically. And crypto fits under the defensive technology in kind of the cyber type of use case.
01:16:33.210 - 01:17:15.574, Speaker B: And it feels very much like what you're advocating for is technologies that increase and enhance human freedom. And so this can also be a bulwark against maybe your AI risk scenario of probability three, that the AI brings about totalitarian technologies, and now we have this defense against it. The other thing I would say is it seems like it's a very broad tent. Like, who can't get behind some good old fashioned defense, right? We're not talking about something that could destroy the world. We're just talking about regular individuals and societies being able to defend against something that can destroy the world. And I want you maybe to talk about this as a philosophical framework. Like, obviously people in crypto are hearing this.
01:17:15.574 - 01:17:59.222, Speaker B: I'm sure they resonate. Are you telling me that there's a way to express my beliefs about crypto and this defensive and freedom enhancing and decentralized technology, and you call it DiAC? Sign me up. There's also, I think, some other camps that could listen to this and be interested. So you've got this section in your essay saying, is Diac compatible with your existing philosophy? If you're an effective altruist, this is a rebranding of the idea. If you're a libertarian, there's something here for you, if you're a pluralist, if you're a public health advocate, I'm wondering if you'd talk to the specific camps here and the value that they might find in subscribing to the DiAC belief. Of course, I know you're pushing it. You're not trying to necessarily convert people.
01:17:59.222 - 01:18:04.710, Speaker B: But in order to explain it, maybe, can you talk about the wins for these various camps?
01:18:05.450 - 01:18:08.342, Speaker A: Sure. Any specific camp you want me to start with?
01:18:08.396 - 01:18:12.940, Speaker B: I would love you to start with the libertarians, actually, because I think we have more than a few listening, maybe.
01:18:14.110 - 01:18:31.322, Speaker A: Sure. I think the best way to think about this is it's a pathway to basically preserve manolic liberty going into a much more technologically advanced 21st century.
01:18:31.386 - 01:18:31.902, Speaker C: Right.
01:18:32.036 - 01:20:17.374, Speaker A: And I think the challenge that DAC is looking at is basically that there are lots of technologies, including technologies that are being developed by governments or by corporations that are increasingly working with governments. Right. There was that recent a 16 Z post that was talking pretty enthusiastically about american dynamism in defense, which of course convenes military tech. And basically looking at how do we create a world that through all of these changes and through all of these pressures, doesn't just kind of maneuver itself into being this incredibly centralized place where you've basically got these probably somewhere between one to four of these sort of big super states worldwide that are in full control of their tech ecosystems, and regular people basically have no option except for being stuck inside of one of these with no other real options for getting out of that equilibrium. Right. And so we can talk about the possibility that we'll have much more offensive AI in the hands of governments. You can also just look at existing trends in how the Internet is not going the way that a lot of us have hoped.
01:20:17.374 - 01:21:10.674, Speaker A: Right. The concept of Internet anonymity, for example, which was like a big hope of people, I think, like ten or 20 years ago. But then the Internet is obviously becoming an increasingly difficult place to actually be anonymous. And a lot of the reason why is definitely just all of these cybersecurity issues, and people just kind of naturally grappling for the centralized solutions for those problems because they're the easy ones. And Diac basically tries to ask the question of like, well, there are threats, and you can go after a threat either by hunting down all the wolves or by putting armor on the sheep. Right, and putting armor on the sheep is philosophically much better if you can do that.
01:21:10.712 - 01:21:10.962, Speaker C: Right.
01:21:11.016 - 01:21:23.462, Speaker A: Because the problem with hunting down all the wolves is like, well, the wolves are some of us, and we have to agree on who the wolves are. And there is a risk that the government's going to decide one day that you're one of the wolves. Right.
01:21:23.596 - 01:21:25.670, Speaker C: And also the wolves don't want to be hunted.
01:21:26.170 - 01:21:50.320, Speaker A: Exactly. And if we instead say, let's make the world a more defense favoring place by default, then that is something that is much harder to twist into a narrative for why governments should just go after all kinds of people that they don't like.
01:21:51.010 - 01:22:05.380, Speaker B: So Diac has some wins for libertarians here. How about the solar punks? How about the Kevin Owaki Regen type community, who are very oriented on collective action and human coordination? Are there wins in DAC for that.
01:22:08.070 - 01:23:31.226, Speaker A: Mean? I think solarpunk, again, is, I think, a school that values human flourishing. It values cooperation. It also values decentralization as well. I think ultimately there is a punk in solar punk. It's not like solar monolith, right. And I think a lot of people in that camp are concerned about the resilience of the world going forward, our ability to survive different kinds of risks, and are probably very cognizant of the fact that all kinds of centralized actors, including both corporations and governments, can be a big problem in making a lot of those risks larger. And what DAC basically says is, it says here is a set of tools that we can use to just cut down on a lot of those risks just across the board and make the world one that is much more friendly to human flourishing without having to construct any of those kinds of monoliths.
01:23:31.258 - 01:23:31.502, Speaker C: Right?
01:23:31.556 - 01:25:09.966, Speaker A: So if you think about the idea of, let's say, the biodefense side of this, right, we could basically make a world that is much more protected against diseases, natural and artificial pandemics, all kinds of things natively, just by having cleaner air. And it's a much more natural solution. And it's a solution to that problem that really avoids some of the downsides of things like lockdowns that we've seen, which can be justified if there's enough of a health risk in a particular situation, but which also are just kind of massive forced changes in the way that people just live their regular lives with their families and their regular relationships and their work. It's an approach that allows us to be in harmony with each other. I would also even say in harmony with nature, because I think defense includes protecting the environment. Absolutely. It's an approach that leverages local communities instead of trying to put power into these kind of big superstates that decide what is good and what is bad on behalf of the entire world or on behalf of much larger groups of people.
01:25:09.966 - 01:25:44.714, Speaker A: It's a world that really empowers local coordination much more. And so I think there's a lot of technologies within the Diac umbrella, especially if you look at the kind of info defense category that we didn't go too much into, that really talks about improving social technology that can really make society both more defended against attacks and much more of the kind of society that has the kinds of relationships that solarpunks would want us to have.
01:25:44.752 - 01:26:14.020, Speaker B: The D can also stand for democratic. And yeah, we didn't have a chance to delve into that, but, okay, so we got the libertarians, we got some wins in dact for libertarians. We got some wins for the solar punks. Let's talk about the original group that we started this entire episode with, which is on the one side is someone like Mark Andreessen, who is an EAC. He's effective accelerationism, full throttle pedal to the metal. Let's just do technology. On the other side is somebody who's maybe in the EA community, the effective altruist community.
01:26:14.020 - 01:26:29.720, Speaker B: Can you get Mark Andreessen on board with DIAC? And can you get Elizir Yudkowski on board with DIAC? Could they both agree about this one narrow subset of technology accelerationism, do you think?
01:26:30.250 - 01:27:31.638, Speaker A: The post got retweeted by Mark Andreessen and by AI not kill everyone is a meme? Yeah, it's pretty successful already. I think the really big piece of it here is that I think for the EAC side, the thing that it brings that a lot of the previous philosophies don't bring is one is there's just optimism about technology in general, but then there also is an alternate path forward. Right? So the message is not just pause. The message is like, we proceed, and here are some alternative routes for how to proceed differently. And so if you are a builder, then the perspective does not kind of frame you as being an incorrigible enemy, right. You can continue being a builder. And there's plenty of amazing roles for builders to do really great things within the DAC context.
01:27:31.638 - 01:28:38.002, Speaker A: And the final stage of DAC being successful, if we imagine going out to the year 3000, really does look like a post singularity Kardashian type two super advanced technological society of exactly the type that IAC and transhumanist people have been dreaming about. And for people who are in the AI safety camp, I mean, the concept of differential technology development is something that a lot of effective altruists have actually already been talking about. Right. So I included a link to one of those posts. But I think the thing that it adds is this kind of emphasis on a more democratic political approach. And this probably is one of the big areas that effective altruist do get criticized for.
01:28:38.056 - 01:28:38.466, Speaker B: Right.
01:28:38.568 - 01:29:07.514, Speaker A: And sometimes the criticism is unfounded, because if you put the governments in charge of distributed public health funding, then realistically it's going to be rich people, countries, governments and national governments have a huge track record of not even caring about what's going on in Africa. Whereas effective altruists actually already have put a lot of money in.
01:29:07.552 - 01:29:07.946, Speaker C: Right?
01:29:08.048 - 01:30:43.178, Speaker A: But at the same time, once effective altruism starts going away from putting money into obviously good, but we just might disagree on how good things and into kind of manipulating big political objects, then you start really needing to care much more about legitimacy. And to me, I feel like both of the big effective altruist related fails, if you can call them, of the last two years, where one is the OpenAI situation and the other is f two x ftx. To me, they both have to do with underrating legitimacy. Right? Like SPF was clearly, he had all kinds of massive problems, but one of them is definitely that he just overrated the extent to which he could become a massively negative value actor just by delegitimizing the ideas that he deeply cared about. Right? And then on the OpenAI side, basically what we saw was we saw a seemingly earnest and well intentioned effort to kind of create kind of clamps on the OpenAI effort that could try to kind of reduce its potential to become super harmful by creating this board that could push things in the other direction. But the problem is, it tried to do all of this through a completely undemocratic and unaccountable board of five people that saw no need to even try to explain its actions to the wider public.
01:30:43.264 - 01:30:43.658, Speaker C: Right?
01:30:43.744 - 01:32:17.430, Speaker A: And then what happened was like, well, it fired Sam Altman, and then basically within three days, well, it was in some ways a pretty unprecedented political fail, because what happened was the employees of the company, who are probably capitalist, delibertarian leaning, just like tech software types in a lot of cases, formed an impromptu union to side with a billionaire CEO against the board. Right? That's like a pretty big fail if you think about it that way, right? Like, congrats, you got software engineers to unionize, except they're standing behind the billionaire. I think the thing that DAC ideas can really bring here is basically bringing back some of these concerns about legitimacy and understanding that you're not just spending money points, but you're also spending social capital points. And you really need to take that seriously and bringing that in in a way that's not just sort of an adjunct, but that is a really core part of the philosophy, right? It's a core part of the DAC philosophy that we are trying to create a world that is more defense favoring from anyone's perspective, regardless of whether or not you agree with any specific actor that would enforce its own idea of who the wolves are and who the wolves are not.
01:32:17.580 - 01:32:56.660, Speaker B: Vitalik, the last group to ask you about here on kind of compatible philosophy that's near and dear to our hearts is how about the crypto tribes? So we have bitcoiners, we have ethereums, we have people who are into Solana and cosmos, and there's a lot of tribalism. Do you think something like Diac? We can all stack hands on something like Diac and say, hey, these are a common set of core values, defensive decentralization technology that we all agree on. Yeah, we have our differences with respect to implementation, but can we all unite behind something like Diac? Is it that wide of a tent to bring the crypto tribes together?
01:32:57.830 - 01:34:19.440, Speaker A: I think it absolutely could be. And I think this is one of those places where I think it's good to give a positive shout out to some of the positive aspects of the bitcoin community, which is that there definitely is a strong subcommunity in there that cares about non blockchain decentralization tech, right? Like, there's bitcoiners who really support things like Noster, there's bitcoiners who support Tor and things like Internet freedom tech. There's bitcoiners who have supported more secure operating systems. And then there's Ethereum people who have also supported all kinds of things in each of those categories as well. Right. I think the idea of viewing the blockchain world as being one part of this somewhat larger thing, which is a decentralization favoring vision of cybersecurity, and then seeing that itself as being one picture in a broader vision of decentralization friendly, pushing the offense defense balance strongly toward defense, is something that Ethereum people and bitcoin people, and Solana people as well, can absolutely get behind.
01:34:20.370 - 01:34:28.542, Speaker C: Batak. I'd like to throw a different candidate for what the D means for in DaC. Mine might be directional acceleration.
01:34:28.606 - 01:34:30.130, Speaker B: I thought were going to say, david.
01:34:30.870 - 01:34:32.660, Speaker A: I was going to say that too.
01:34:34.950 - 01:35:28.902, Speaker C: That can be a different topic with the. So to me, there's like the tribal debates between decelerationism and accelerationism, or effective altruism and accelerationism. And when there is these tribal debates, usually the weaknesses inside of one tribe never really get addressed by that tribe because they only ever really argue in relation to a different tribe. The way I hear this blog post and hear you speaking about is like, hey, we're going to move forward in scientific progress. We're going to have technology that has higher capacities. And now it's really just a matter of picking our direction, our priorities, and where we want to go and what technologies we want to prioritize first, because I think generally most people accept that technology has helped the world over the grand arc of time. And now it's really more about choosing which direction we go in.
01:35:28.902 - 01:35:41.290, Speaker C: Rather than blindly saying, yes, it's forward. It's more about saying, yes, forward. And over here, in this particular direction, how would you feel about directional accelerationism?
01:35:41.870 - 01:35:53.726, Speaker A: Yeah, I think with the caveat that it's totally possible to create a directional acceleration story that I totally disagree with. But as long as we still have.
01:35:53.748 - 01:35:54.990, Speaker C: To debate in which direction exactly.
01:35:55.060 - 01:35:58.880, Speaker A: As long as we understand which direction is I mean, you think? Yes, absolutely.
01:35:59.490 - 01:36:46.414, Speaker B: Vitalik as we maybe conclude this episode, you've given us a fantastic tour of this whole debate, the societal debate in this context, in crypto, and a great definition of DiAc, which is, I think, just a fantastic philosophy that I think bankless listeners will probably take some time to mull over. But your article concludes with this. It includes with some optimism for human potential here. You say human beings are deeply good. And I got to confess, in my darkest moments, of course, I sometimes doubt whether that is actually true. And if you look at even crypto in 2022, I feel like we all came out of that collectively as an industry. Pretty beat up, pretty doubtful that human beings are deeply good.
01:36:46.414 - 01:37:25.690, Speaker B: But you say this, I love technology because technology expands human potential. We are the brightest star. 10,000 years ago, we could build some hand tools, change which plants grow on a small patch of land and build basic houses. Today, we can build 800 meters tall towers, store the entirety of recorded human knowledge in a device that we can hold in our hands, communicate instantly across the globe, double our lifespan, and live happy and fulfilling lives without fear of our best friends regularly dropping dead of disease, zooming out. We have come quite a ways, haven't we? What grounds your belief that human beings are deeply good? Vitalik.
01:37:27.790 - 01:38:38.178, Speaker A: I think just like what other thing even remotely compares to us, right, is the question to ask, right? The universe is a very lifeless and unforgiving place. Right, where we've had first seven to 9 billion years of just stars and planets crashing into each other and randomly creating supernovas that would just completely wipe away everything within light years without thinking about it. And then we had 4 billion years of life. But life that was very nasty, very brutish, very short, and that basically involved milk predators constantly running around and eating prey, and everyone being on the brink of dropping dead of disease and starvation. And there is not a single example of a cat that modifies its eating behavior because of a principled stand that killing mice is wrong.
01:38:38.264 - 01:38:38.706, Speaker C: Right?
01:38:38.808 - 01:39:16.862, Speaker A: That is just not a thing that happens. Whereas with humans, there are plenty of humans that have written entire screeds on why this is the case and that have made huge personal sacrifices to protect the people or animals or plants that they care about. And I think to the extent that this happens, it's incredibly amazing and incredibly beautiful. And I think if humanity continues on a positive trajectory, then the amount of good that we can do just multiplies even further exponentially from there.
01:39:16.916 - 01:39:17.520, Speaker C: Right.
01:39:18.130 - 01:39:46.594, Speaker A: In the 21st century, I think there is a big chance that we're finally going to turn the corner on factory farmed animals. And probably the biggest moral catastrophe that you still can blame humans for is something that we will actually end up moving beyond. And then 1 billion years from now, the sun is scheduled to get so bright that life on Earth is not going to be possible anymore.
01:39:46.642 - 01:39:46.998, Speaker C: Right?
01:39:47.084 - 01:40:24.354, Speaker A: And does the sun think about the moral consequences of this act that it's going to make? Well, no, it does not. Right. But humans, well, what can we do? Well, we can sprinkle calcium, carbonite, or sulfur into the air and compensate and reduce the amount of light that reaches the surface. We can build giant mirrors in space to reflect the light. We can go and terraform Mars. We can do all kinds of things. And so if the beauty of earthly light, life is still going to shine 2 billion years from now, it will be because of us.
01:40:24.354 - 01:40:43.978, Speaker A: Right? And so I think we carry the torch of this just enormous potential that is unparalleled in any other thing in the universe that we currently have evidence of the existence of. And it's our job to do a good job of carrying that torch and carry the torch forward.
01:40:44.064 - 01:41:14.546, Speaker B: What a fantastic way to end it. I think that's what the DAC movement, it sounds like is all about. And I could say bankless is certainly part of that movement. And I'm hopeful with crypto, we can help carry that torch further. So I'll leave maybe bankless listeners with this line from your article. We are the brightest star. There's a lot of good that can come from ongoing human progress into the stars and beyond, but there are big forks in the road and we need to choose carefully, accelerate, but accelerate carefully and well.
01:41:14.546 - 01:41:31.650, Speaker B: Vitalik Buterin, thank you so much for joining us today. Some action items for you bankless nation link to the tweet thread that we discussed today. My techno optimism. That's Vitalik's article. We'll include a link to that and also the article itself. Risks and disclaimers, of course. Crypto is risky.
01:41:31.650 - 01:41:41.582, Speaker B: So is philosophy, so is technology. So many of the things we talked about. Yeah, I think so. You could definitely lose what you put in. But we are headed west. This is the frontier. It's not for everyone.
01:41:41.582 - 01:41:44.300, Speaker B: But we're glad you're with us on the bankless journey. Thanks a lot.
