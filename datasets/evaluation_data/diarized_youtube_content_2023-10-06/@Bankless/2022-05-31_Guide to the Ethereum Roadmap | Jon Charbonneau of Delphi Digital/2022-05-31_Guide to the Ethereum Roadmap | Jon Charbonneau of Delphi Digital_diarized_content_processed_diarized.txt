00:00:08.120 - 00:00:55.370, Speaker A: Welcome, bankless nation to a very special episode. I'm super excited to bring you on today's state of the nation. State of the nation is where it comes out every Tuesday on the bankless livestream and then every Wednesday morning on the RSS feed where we relate it to big picture action items. We drop some insights and action items, and I'm happy to bring you some awesome alpha coming out of a research analyst analysts out of Delphi Digital we're talking about the Ethereum roadmap beyond the merge. In some ways, the merge feels like the finish line for much of Ethereum's history, but there is still so much left to do. You might have heard of things like dank sharding or data availability sampling or proposer builder separation, and you might have thought, wow, that's really complex. I might not ever understand that, and you might be right about that.
00:00:55.370 - 00:02:09.948, Speaker A: And I'm sure that we are not going into the full math about some of these things, but we are going into some of the shared themes and shared strategies that each of these complex mechanisms have for the future of Ethereum beyond the merge. There is this common structure to all of these things, and it has to do with harnessing complexity and the separation of powers to make Ethereum scale computation while remaining decentralized. So today on the show, we're bringing John from Delphi Digital, who wrote a fantastic research report praised by some of the leading Ethereum researchers, Tim Baco, Poly Nya and even Vitalik himself, called the hitchhiker's guide to the Ethereum roadmap. And so we're bringing him on the show to talk about not the math, but the meaning behind all of these things and what it means for Ethereum, what it means for you as a potential Ethereum validator, and what it means for you as a potential eth holder. You guys might be aware that somebody is missing from this live stream. Ryan is in the middle of travel, got rugged by some travel plans, so I'm taking this one solo today, and our fearless leader will be back with us next week for next week's state of the nation. But in the meantime, we have to talk about a message from one of our sponsors, Alchemyx.
00:02:09.948 - 00:03:07.190, Speaker A: Alchemyx. You guys know Alchemyx? It's the crypto powered savings paradigm where you can save money either in ETH or dai or your other stable coins. You can deposit it into the Alchemyx DFI yield farming account. And for all of the money that you deposit into alchemyx you can withdraw up to 50% of your deposits while that deposit grows and grows and grows from your yield farming in DFI, which Alchemyx does for you, so it allows you to save your money and spend it too. It allows you to get your interest payments paid to you upfront through the Alchemyx web app and so you can check them out. There is a link in the show notes bankless cc slash Alchemy so the capital a you can see on screen all the assets that you can leverage, including staked ETh from lido and rocket pool, as well as your preferred stablecoin. And if you are an Alchemyx token holder, there is a coming tokenomics upgrade and there's a link in the show notes for you to explore that as well.
00:03:07.190 - 00:03:47.996, Speaker A: At this point in time, this is when Ryan would ask me what the state of the nation is and so I just have to ask myself what the state of the nation is. And today the state of the nation is checking and balancing. And that, I think is going to be a theme of the episode. All of these very complex things like I mentioned earlier, dank, sharding, proposer builder, separation, data availability, sampling all fits under the theme of checking and balancing. And we've heard the phrase checking and balancing before. This comes from your basic us history class, and why it's so powerful is it allows for not one part of the power structures around Ethereum to outsize the others. And so today the state of the nation is we are checking and balancing the state of Ethereum.
00:03:47.996 - 00:03:57.844, Speaker A: So we're going ahead and get right into this show with John from Delphi Digital to talk about all the different ways that Ethereum checks and balances itself. Right after we get to some of these fantastic sponsors that make the show.
00:03:57.882 - 00:04:00.384, Speaker B: Possible, the era of proof of stake.
00:04:00.432 - 00:04:29.880, Speaker A: Is upon us, and Lido is bringing proof of stake to everyone. Lido is a decentralized staking protocol that allows users to stake their proof of stake assets using Lido's distributed network of nodes. Don't choose between staking your assets or using them as collateral in Defi. With Lido, you can have both. Using Lido, you can stake any amount of your ETH to the Lido validating network and receive St ETH in return. ST ETH can be traded, used as collateral for lending and borrowing, or leveraged on your favorite defi protocols. All this without giving up your eth to centralized staking services or exchanges.
00:04:29.880 - 00:05:02.452, Speaker A: Lido now supports Terra Solana, Kusama and Polygon staking. Whatever your preferred proof of stake asset is Lido is here to take away the complexities of staking while enabling you to get liquidity on your stake. If you want to stake your etH, terra Sol or matic and get liquidity on your stake, go to Lido fi to get started. That's li d o fi to get started. The L2 era is upon us. Ethereum's L2 ecosystem is growing every day and we need bridges to be fast and efficient in order to live a L2 life. Across is the fastest, cheapest and most secure crosschain bridge.
00:05:02.452 - 00:05:51.928, Speaker A: With across. You don't have to worry about the long wait times or high fees to get your assets to the chain of your choice. Assets are bridged and available for use almost instantaneously. Across bridges are powered by UMA's optimistic oracle to securely transfer tokens from L2 back to Ethereum, a token proposal is being deliberated as we speak in the across forum, where community members will decide on the token distribution. You can have your part of across the story by joining the discord and becoming a cofounder and helping to design the fair fair launch of across. You want to bridge your assets quickly and securely? Go to across to bridge your assets between Ethereum, optimism, arbitram or Boba networks if you're trying to grow and preserve your crypto wealth, optimizing your taxes is just as lucrative as trying to find the next hidden gem. Also, IRA can help you invest in crypto in tax advantage ways to help you preserve your hard earned money.
00:05:51.928 - 00:06:21.428, Speaker A: Also, crypto IRA lets you invest in more than 150 coins and tokens with all the same tax advantages of an IRA. They make it easy to fund your alternative IRA or crypto IRA via your by contributing directly from your bank account. There is no setup or account fees, and it's all you need to do to invest in crypto tax free. Let me repeat that again. You can invest in crypto tax free. Diversify like the pros and trade without tax headaches. Open an alto cryptoira to invest in crypto tax free.
00:06:21.428 - 00:06:37.290, Speaker A: Just go to altoira.com bankless, that's altoira.com bankless and start investing in crypto today. All right, bankless nation, we are here with John Charboneau from Delphi Digital. John, welcome to the show, man.
00:06:37.740 - 00:06:40.712, Speaker B: What's up? Happy to be on first podcast for me.
00:06:40.766 - 00:07:24.870, Speaker A: Yeah, congratulations. And we were talking a little bit backstage. You have a very recent history with Ethereum and Crypto, which is pretty impressive that you wrote an article praised by Vitalik himself as being extremely accurate and extremely well researched. And so like I mentioned, I teased in the intro, some of these things are extremely complicated. There is like crazy math, like you got to go back to algebra and polynomials to understand some of these things. That's not here what I want to talk about today, because I kind of want to talk more about just the vibes of all of these things, how all of these things have a shared structure, a shared pattern. Before we get into some of the more complicated stuff, can you just summarize the vibe of the Ethereum roadmap post merge? What should we expect?
00:07:26.120 - 00:08:13.300, Speaker B: Yeah, so the big thing obviously is the merge is not going to be scaling Ethereum. So the priority after the merge is going to be all of these different steps that we need to scale all of the computation on Ethereum through the roll up centric roadmap. So basically trying to make Ethereum a really good scalable base layer for roll ups, while at the same time scaling that throughput, keeping it really decentralized and easy to validate, because that's what keeps everything in check. Ultimately true scaling isn't just what's your TPS, it's throughput relative to what is the cost to validate. And that second part is what a lot of other typical monolithic l ones that just try to jam through and put through on beefier hardware. They ignore that second part. So that's not true scalability by mind.
00:08:13.300 - 00:08:17.616, Speaker B: It's keeping that second part in mind at the exact same time that you need to do right.
00:08:17.658 - 00:08:43.804, Speaker A: One version of scalability is we just do away with the whole blockchain thing and we just go back to a database, and then we have just the most scalable system on the planet, but then we lose all that trustlessness. So to summarize what you're saying post merge posts, once we get to proof of stake, it's all about finding these different ways to scale computation to scale throughput without losing all of the cool properties that make a trustless blockchain.
00:08:43.852 - 00:08:45.810, Speaker B: A trustless blockchain is all right.
00:08:46.180 - 00:09:17.690, Speaker A: Yes, in the intro to your piece, and just to dive into this a little bit more, you wrote scaling computation without sacrificing decentralized validation. Let's dive into the validation aspect of this. What does it mean to scale computation without sacrificing validation from the user who might be thinking about staking Ethereum or ether in the future to become part of the network? That validation aspect I think is really important. Can you talk about elaborate on that part?
00:09:18.480 - 00:10:32.716, Speaker B: Yeah, so the big theme with a lot of what Ethereum is going to be doing going forward, both at the l one level and the roll up level, is that there's probably going to be some specialization in centralization and tasks, primarily block production. The realization of that is that you just need to really focus the most on decentralizing the validation of that, because that's what keeps everything in check. Like Vitalik's endgame post is what put it the best that most roads tend to lead to that end scenario, whether it's in roll ups, you see that you're probably going to have somewhat specialized block production if you want to have the highest throughput on them. What's important is to keep regular users behind essentially full node security without the resource requirements of what typical high throughput l ones would require you to need today. So you want to give people that level of security by fully validating the chain. So if you have a malicious block producer, you just reject the transaction. You could just see right away if you're just running a light client of a monolithic l one, because it just has too high a hardware requirement to run a full node, they can pretty much do whatever they want to you, and you're just not going to notice it because you're trusting the honest majority.
00:10:32.716 - 00:10:48.744, Speaker B: That's not the case with a full node. A full node won't be accepting invalid transactions, and if you're working on a roll up, then you're hiding behind the safety of fraud proofs or zk proofs. So keeping that validation for regular users is what really, really matters a lot.
00:10:48.862 - 00:11:12.930, Speaker A: And so I mentioned three components in the intro, data availability, sampling, proposer builder separation, and dank sharding. Proto dank sharding. These are all different strategies to get to the same end result, correct? And so different ways to scale Ethereum. That scale different parts of Ethereum, but ultimately produce that scaled ethereum with decentralized validation. Is that all correct?
00:11:13.300 - 00:11:46.248, Speaker B: Yeah, that's right, because the overall vision of Ethereum right now is obviously the roll up centric roadmap. The problem with Ethereum right now is it's not built to actually host roll ups. It's kind of just makeshift right now, and we're making do with what the l one can handle. So a lot of these are primarily geared towards scaling the data availability throughput, because that is a big bottleneck for the roll ups right now that they need to post their data to the l one. And the layer one is not optimized for that today. So it's being able to scale that, while at the same time making it really easy for regular people to validate that.
00:11:46.414 - 00:12:47.820, Speaker A: So people who were familiar with the older Ethereum roadmap? We talked about sharding as a way to scale computation on the Ethereum layer one, but we've shifted away from that, where computation scaling has been thought to now move into the roll ups and computation scaling, that's we're really talking about like transaction throughput, like how fast transactions can go. And so now that we have this roll up centric roadmap of Ethereum, a lot of that computation is happening on the roll ups. That's what roll ups do. But in order to allow those roll ups to really become unleashed and go up to their maximum throughput potential, we need to enable data to make data available to them, because the resource that roll ups need the most is data. Can you talk about this transition from this older version of the Ethereum roadmap, where we were going to actually scale the Ethereum layer one in terms of transactional throughput to what we've kind of pivoted to now, which we've actually focused on scaling the data availability?
00:12:50.180 - 00:13:32.620, Speaker B: Yeah, sure. So the old roadmap was just jammed through the execution on the L1 through execution sharding. We've since realized essentially that it's better in nearly all respects to probably put that on roll ups and then just optimize the base layer for data availability. So that is what the old sharding design was doing the previous one. After execution shards, it shifted to the previous data sharding design, where there was going to be these 64 different data shards, and you just post data to them as the roll ups. And there are committees that are checking all of them. And so as a validator, you're testing that all of the data was made available because the data availability is important for the security of those roll ups.
00:13:32.620 - 00:14:15.076, Speaker B: For example, in the case of an optimistic roll up, you need the data available to successfully be able to arbitrate a fraud proof. You also need it in the case of a ZK proof, for ZK roll ups to be able to recompute the state and be able to exit it safely. So the key thing that we've realized is, okay, we can shift that block production and execution off chain to roll ups while now just focusing really on making it a really scalable data layer, while also keeping the regular execution part for settlement of roll ups, like they post approves to the l one, they can bridge through it, keeping all of those trust assumptions together but focusing on now scaling. Okay, how do we put a bunch of data through the L1 without jacking up resource requirements?
00:14:15.268 - 00:14:42.790, Speaker A: And correct me if I'm wrong. And again, listeners, we are going to go into the three things in detail, data availability, sampling, proposal builder separation, and dank sharding. But John, before we get there, just again, correct me if I'm wrong, but it's data availability, sampling and dank sharding that do scale up the effective data of the Ethereum layer one, effectively increasing throughput on the roll ups. And proposer builder separation is something different. Is this all correct?
00:14:43.560 - 00:15:40.612, Speaker B: Yeah. So proposer builder separation is one of the really important, pretty much necessary things to enable dank sharding compared to the previous sharding design. So you could do data sharding without proposer builder separation, but proposer builder separation makes the new sharding design possible, which wasn't previously, because otherwise it would have been too high of a resource requirement for regular validators. Data availability sampling is part of that, making it really easy for validators, part that I can very securely check that all of the data was made available and know with 100% certainty, or near 100% certainty that it was all available. And that means that, okay, if it was all available and I'm working on a roll up, if the data was available and no fraud proof has been posted, I can safely assume if there was one person who would have posted that fraud proof that I'm good to go, or if a ZK proof was posted and I know all the data was available, then I know I'm good to go.
00:15:40.746 - 00:16:16.130, Speaker A: And what that does is that allows effectively more data to become useful to the Ethereum validators. While because we only have to verify a subset of that data, we get to prove that all of the data is available without having to check all of the data. And so we effectively get a data throughput increase because our validators only have to check a minority subset of the data. But as a whole entire ecosystem, we get to leverage the full expression of all of the data. And this is what we mean by scaling Ethereum without scaling computational requirements. Am I tracking here?
00:16:16.820 - 00:16:31.216, Speaker B: Yeah, exactly. Today is the opposite of that, where it's not built as a data availability layer. So when you post call data, the l one, it's just every node has to fully download it, and then you hold on to it. That's a very resource intensive way that you can't scale that up to massive throughput.
00:16:31.328 - 00:17:03.520, Speaker A: Okay. And then overall, I would say resource intensive, I would say that these three things, again, which we're going to go into, all fit into that vibe of how do we scale computation without scaling resource requirement as like literally using cool cryptography, using cool math tricks? How do we scale computation, I. E. Throughput, I. E. Transaction speed and cost, without increasing the resource requirements of these tricks? Because that is what preserves Ethereum and keeps it decentralized.
00:17:05.060 - 00:17:06.528, Speaker B: Yes, that's right.
00:17:06.614 - 00:17:36.170, Speaker A: Okay, and one last question as we stay high level, because again, super complicated subject. So I want to make sure all the listeners and all the viewers get the right vibe. What does this end state look like post inclusion of all of these different mechanisms? Again, data availability sampling, proposer builder separation and dank sharding. Proto dank sharding. What's the end state of Ethereum look like? Can you kind of just walk us through a holistic visualization or interpretation of Ethereum in this end state?
00:17:36.540 - 00:18:30.732, Speaker B: Yeah, the really high level of it will be that you basically introduce this new entity who's going to be a specialized builder. So they're going to be responsible for making this really big block together that has all the data, the beacon chain block, everything put together. And then you have this very decentralized, very low resource requirement set of validators who don't have to build the whole block, they just take it and they have to say, okay, is this block good to go? And was all the data made available? And you can pretty easily do that data check with data availability sampling, which is very different from today. Today you would need to download all of that data. So that's why we can't scale it up. When you introduce the PBS proposal builder separation as that new role, you get rid of the high resource requirement parts and then you add data availability sampling to make it really easy to do that proposer job of just checking the data was available. Okay.
00:18:30.786 - 00:19:26.030, Speaker A: And the last subject I want to get to is the interactions between all these components. And so I want to put my devil's advocate hat on, my Ethereum critiquer hat on and say, well, Ethereum is complicated and it's just like solving its problems with more complication, right? It's just adding complication on top of complication. And then eventually, once all these three things that David keeps on listing off by name, once those get included, then we're going to have eight more things to solve the complexity there, and then we're going to have like 18 more things to solve the complexity there. So that's the devil's advocate version of Ethereum, where it's already complicated and we're just making it more complicated to do all these things. And then there's the opposite interpretation, where actually these three things fit together really, really well and have this sort of like elegance between the interactions between these three components. What side of that take would you say that you're on?
00:19:26.480 - 00:20:12.732, Speaker B: It's all a mess. Everything does fit together really well, even beyond just these three components. When you zoom out and look at almost every major component of Ethereum's future roadmap, almost everything does boil down to those two points of scaling computation while making it really easy to validate. I mean, taking these as an example and how they are necessary and kind of interweb with each other. Dank sharding, which is what we're going for. Proposer builder separation is necessary to do that helps it scale easily, and it makes it easier for us to validate it because you make the proposer job really easy. All of these different pieces are kind of just running in parallel, and they're all chipping away at different things.
00:20:12.732 - 00:20:50.280, Speaker B: Whether you look, even looking at a bunch of stuff on the Ethereum roadmap, so much of it is just how do we lower the resource requirements to validate whether you look at things like history Xpry, that's okay. You don't need to have as big a hard drive to run a full node. If you look at statelessness, that helps reduce your SSD requirements, that you don't have to hold your state on hand. They're all chipping away at different pieces. I mean like data availability sampling, which is one that we'll go into more. It helps you lower your bandwidth requirements and that you don't have to download all this data. All of these things ultimately do really come into this very cohesive view when you zoom out and you start to understand how they all work with each other.
00:20:50.430 - 00:21:30.272, Speaker A: So I think with one of these components, like take data availability sampling, for example, we get a bandwidth reduction, which is nice because then blocks propagate faster. We can have more throughput that way. That might be like one single sized upgrade. I don't know how you want to size this thing, like maybe one order of magnitude, but then when we layer on the next thing, like proposer builder separation, we actually get another order of magnitude. But then these things interact, right? These orders of magnitude compound on each other. And then we throw in the third thing and we have three different ways. We are decreasing resources requirements, and they're not linear.
00:21:30.272 - 00:21:42.250, Speaker A: It's exponentially scaling. It's exponential reduction of resource requirement, which turns into an exponential scaling of the actual throughput of the holistic ethereum ecosystem. Is that a fair take?
00:21:42.700 - 00:22:11.648, Speaker B: Yeah, that's generally about right. And that's why you see, like people will have heard of, and we'll go into this in more depth, protodank sharding, which is a halfway step to dank Sharding. And that's exactly what you see where you have a certain amount of data availability throughput today. If you go to protodank sharding, which implements part of the steps of dank sharding, you get some orders of magnitude scaling and then dank sharding layers on even more of the exciting changes that are coming. And then you get even more orders of magnitude scaling for data availability on top of that.
00:22:11.814 - 00:22:21.270, Speaker A: So just to answer a question which I think might have popped up in some listeners, dank sharding, proto dank sharding. Can you talk about how these names came to be?
00:22:22.040 - 00:22:41.104, Speaker B: Yes. So Dank Sharding, which is a great name, playon took the old just sharding design, and Doncrad is the one who came up with that. And then the halfway one, proto dank Sharding is a play on taking that. And then also proto lambda pitched in to add that like halfway measure.
00:22:41.252 - 00:23:07.220, Speaker A: Right? Okay, so Proto Lambda, I can't remember where he was, but he was working maybe at the EF before this. Yes, he was working at the EF, then he joined the optimism team. He's called Proto Lambda. I'm sure it's not his actual name, but that's what we call him. And then there's Dank red feist, also at the EF. And so these things are fondly named after them. And so Ethereum, sticking true to its name of, kind of picking weird names for its upgrades.
00:23:07.220 - 00:23:33.100, Speaker A: Okay, so here's where I get a little bit confused. John and I kind of want you to pick the next path forward. We have three different things to talk about. Data availability, sampling, proposer builder separation, dank sharding, proto dank sharding. Since they're all interrelated, is there a starting place? Is there one that we should pick out first to talk about first? Or is it kind of a chicken and an egg problem? Since they're all interrelated, there is no starting point for this whole entire conversation.
00:23:33.920 - 00:23:50.672, Speaker B: Yeah, they're all pretty interweaved. Like you're going to see data availability, sampling, proposer builder separation, and dank sharding all come together. And then protodank sharding is the halfway step that implements some of the transaction formatting, so they all do very much interlock those three.
00:23:50.726 - 00:23:52.244, Speaker A: So we're just going to have to pick one.
00:23:52.442 - 00:23:52.852, Speaker B: Yes.
00:23:52.906 - 00:24:01.700, Speaker A: All right, let's go with data availability sampling. Can you explain data availability sampling and how it scales computation while keeping Ethereum decentralized?
00:24:02.040 - 00:24:48.550, Speaker B: Yeah, so back to that message of we're trying to scale the data availability throughput of the l one. And just to remember, there's a difference between data availability and data retrievability. What we need for the security guarantees of the consensus layer isn't that this data is retrievable forever. What we need to know is, was this data published? Did we attest that it was available such that it's available for some period of time that any interested party could have downloaded it and could have reasonably submitted a fraud proof or done anything like that? That's what we need for our safety. The history retrievability of it is a separate, much weaker assumption that is dealt with separately. So that's what the data availability is.
00:24:49.400 - 00:25:45.030, Speaker A: Well, let's actually dive into that a little bit more. So data availability, if I'm recalling my Ethereum knowledge, it makes it availability for an amount of time. And that amount of time is just assumed to be long enough to know that the whole world could have downloaded and had that data available to them. But the Ethereum protocol does not commit to embedding that into the blockchain perpetually, so it will not be saved inside of the Ethereum blockchain forever. But the Ethereum protocol generates assurances that there was enough time for the whole world to have downloaded that data at some point in time. Let's elaborate and unpack the difference between these two things and why we wouldn't just save it in the Ethereum protocol forever. And how do we know that it was actually available to the whole world for a sufficient amount of time? And why do we feel secure pruning it from the blockchain at all?
00:25:45.480 - 00:26:17.516, Speaker B: Exactly. Yeah. So the problem with, if you require it stay on chain forever, that every whole node has to hold onto this data forever. You just can't feasibly keep this thing decentralized. That's just way too high of a resource requirement to ask all of these nodes that you want to be validating to be able to hold onto this ever growing, massively growing thing that's going to have terabytes of data that are running through it when we implement sharding and these things. So right off the bat, we know that that just can't keep it like decentralized validation. If we do that, the reason source.
00:26:17.548 - 00:26:42.970, Speaker A: Bandwidth that we're talking about here is hard drive space, right? Like if we let it get out of hand, all ethereum nodes that all of us that plan on staking our eth and running our nodes, we would have a normal size computer except for this massive, massive, just like rack of hard drive space. And so that's the resource requirement that we are minimizing here with this data availability sampling, is that correct?
00:26:44.300 - 00:27:40.380, Speaker B: So that would be minimized by the fact that we are going to start pruning these data blobs when we introduce the new transaction format with protodank sharding. Okay, this history issue is going to be exacerbated by the fact that we start posting a bunch of data to the l one. But it's already an issue, which is why apart from all of this, there is separately another EIP, like called EIP forty four s that is going to start allowing nodes to prune all historical data after a year and stop serving that to the network. The difference today is that call data, which is what roll ups use today, it just persists on chain, it just stays there. But that's not the security guarantee that we really need for roll ups. So what protodank sharding is going to do, one of its big halfway steps to going to full dank sharding, is it introduces the new transaction format that roll ups will use. So today they post call data.
00:27:40.380 - 00:28:12.160, Speaker B: Once protodank sharding is enabled, they'll start posting data blobs. And the thing with data blobs is they can get pruned after about a month. So that's more than enough time that we guarantee that it's available, that people could have posted fraud proofs or done whatever, anyone could have downloaded it in a reasonable period of time. But then full nodes just don't have to hang on to, it's not their problem, because history is just a one event like trust assumption. As long as someone out there is holding the history, it's fine, I can just go ask them for it. You don't need everyone to be holding that on hand, it's just not necessary.
00:28:12.320 - 00:28:29.770, Speaker A: So one of n assumption, as in if there's one Ethereum node that's out there that has this data, or what is the actual process of why would I would want to go retrieve very old data? And then how do I actually do go about finding that data?
00:28:31.280 - 00:29:03.984, Speaker B: Yeah, it could be anyone. I mean, the easiest example is block explorers. If you have some kind of business that depends on, I need to be able to serve this history. Whenever anyone asks for it, they're a perfect example of as long as they're storing it, you're fine. There are other decentralized and more innovative options to make sure that we are better incentivizing history retrieval as it starts to become more of an issue. It's not like a big issue. Like another thing is, it's really not an issue for the Ethereum protocol.
00:29:03.984 - 00:29:29.928, Speaker B: It's only an issue for apps, basically, if they lose their old history and they're like, oh, I can't prove that this thing happened. So one thing that, for example, roll ups could do is they could just mandate that they have to hold onto their own relevant parts of their history. There's lots of innovative solutions where as long as someone is holding onto it, and I could just ask them, I can always prove that what they gave me is valid. So as long as someone still has it, it's not lost forever.
00:29:30.104 - 00:30:12.588, Speaker A: Okay, so if a roll up makes a layer one transaction with a specific amount of interactions with a part of the Ethereum, layer one, like it talked to these tokens and talked to those tokens, the roll up can make sure that the data for those relevant transactions are always stored by the roll up providers. Right, the roll up nodes. And so that would make that part of Ethereum persistently and perpetually available for that particular roll up. And so that part of Ethereum is secured. And then we could probably repeat this process like 5100, 1000 times, and then eventually we'll get to the full, entire, full archive state of Ethereum. But it's no one single node is responsible for storing all of it. Is this correct?
00:30:12.754 - 00:30:47.428, Speaker B: Yeah. It's never going to be mandated in protocol that Ethereum needs it. You could also try to do decentralized, different incentivization, where you can have different networks, where you are paying people to hold your data. But it's just fundamentally not the Ethereum l one consensus layers problem, because it's not like breaking anything if you can't resurrect your history, it's a problem for that app. Like, hey, I can't do this transaction or whatever. Now I can't prove this thing, but that fundamentally is just a different task that's much easier to outsource, and it makes sense to keep everything very decentralized.
00:30:47.524 - 00:31:34.090, Speaker A: Okay. It's not breaking anything if you can't. I can't remember how you exactly said it, but I think it led into my next question. Where if I put on my bitcoiner hat and they're like, you guys are pruning the data from your chain, I will get triggered. Right? Like, bitcoiners are extremely hyper focused on backwards compatibility and making sure that if somebody mined a block in 2010 and then they didn't move those coins from 2010 onwards into 2050, the emphasis from bitcoin is that you will always be able to retrieve your coins and the data, and you also get the self sovereignty from always being able to run a node and spin up a node from genesis and verify every single transaction. So we're not violating that principle by doing this.
00:31:35.580 - 00:32:03.280, Speaker B: Yeah, because the reality is you won't yourself have the data, but as long as you can get from one single person out there, it's really just not an issue. And it fundamentally isn't a big concern because there are always going to be these things and you can incentivize them in different ways. You can use their third party indexing things. Like, you could have the graph. There's work with all of these different things that as long as someone's holding on to it, it's not a major concern.
00:32:04.420 - 00:32:20.470, Speaker A: And the idea is that even if one person, again, putting on my bitcoiner hat, okay, but you're asking me to trust one single person to serve me the data. Like, what if they serve me incorrect data that serves their needs instead of mine? What's the response to that?
00:32:21.160 - 00:32:39.436, Speaker B: So you only need one out of all of these different candidates who should be storing it, whether it's roll ups, storing theirs, their own data, whether it's individuals, block explorers. If people start using the graph or all these different solutions, you just need one of them to provide it to you. But how do I know that they're.
00:32:39.458 - 00:32:40.924, Speaker A: Giving me the correct data?
00:32:41.122 - 00:33:03.924, Speaker B: Exactly like they can't give you fraudulent data. You could always still prove that, hey, this thing happened. They can't just serve you invalid data and then you'll take it. You can always prove on chain that you can go back and look that, hey, does this match? They can't trick you into doing something. The worst thing that could happen is just they don't give you the data and then you just can't retrieve what happened.
00:33:04.042 - 00:33:19.210, Speaker A: Okay, now this is where some of these interaction between these things get a little hairy. Is this what we're discussing now about the ultimate pruning of the blockchain? Is this data availability sampling? Or is this dank sharding or protodank sharding? Or is it both?
00:33:19.980 - 00:33:23.172, Speaker B: This will be implemented with protodank Sharding.
00:33:23.236 - 00:33:23.560, Speaker A: Okay?
00:33:23.630 - 00:33:56.324, Speaker B: And then the other steps will be implemented with full dank sharding. So this is the main reason that protodank sharding is able to scale, is because it introduces the new transaction format that are going to be carrying these data blobs. And these data blobs will be pruned after about a month or so, once we know that they've safely been available. Anyone could have downloaded them, anyone could have checked them. You can prune them. And then because of that, we can send through orders of magnitude more data throughput, because we're not requiring people to hold onto these things forever, so we don't have to worry about blowing up the history with them.
00:33:56.442 - 00:34:18.684, Speaker A: Okay, so with dank sharding and proto dank sharding, we can juice up the throughput of ethereum, because we are only requiring the nodes to really store the more recent parts of Ethereum and not the long term parts of Ethereum. Therefore we're more okay with juicing up the data requirements because it's all in the short term, not in the long term, correct?
00:34:18.882 - 00:34:49.604, Speaker B: Yeah. And the reason that we could then take another step up from protodank sharding to full dank sharding is because in protodank sharding, we're not going to have data availability sampling or any of that. We are still going to require the validating nodes to fully download the data. That's how they'll have to check that the data is available, is they will have to fully download everything and say, hey, was this data available when we go to full dank sharding, after that, you'll only be checking a subset of the data. So then the data will be sharded. So then that gives you the extra orders of magnitude scalability, okay?
00:34:49.642 - 00:35:23.810, Speaker A: And then going from proto dank sharding to full dank sharding, that's when that crazy polynomial meal math comes into play, where we can allow for more data to come in because the validators only have to check parts of it. And because of crazy polynomial math, we have the assurances that to the 99.99% likelihood that all of that data is there. And so we can, do you know the order of magnitude increase in data from going from proto ding sharding to dang sharding? Do you know what that is?
00:35:24.660 - 00:35:26.716, Speaker B: Sorry. From proto to full dang sharding?
00:35:26.748 - 00:35:29.920, Speaker A: Yeah. What's the scalability increase on that one?
00:35:30.070 - 00:35:33.260, Speaker B: I want to say it's about a 30 x, if I remember correctly.
00:35:33.340 - 00:35:44.920, Speaker A: Wow. Yeah. Okay, so we take proto dank sharding and then we add this polynomial mass to get to full dengue sharding, and that gives us a 30 x increase in data availability.
00:35:46.620 - 00:35:54.168, Speaker B: Yes. I want to say 16. I don't remember the multiple on it, but it's in the low orders of magnitude number.
00:35:54.254 - 00:35:54.552, Speaker A: Okay.
00:35:54.606 - 00:36:11.596, Speaker B: The target block size is going to be around a megabyte for data availability for prototynct sharding, and then it'll go up another couple of orders of magnitude or so from prototyl because we have data availability sampling, because you don't have to download everything anymore.
00:36:11.708 - 00:36:16.610, Speaker A: Okay, so data availability sampling, is that the polynomial mass thing that I was just talking about?
00:36:18.100 - 00:36:46.824, Speaker B: That is part of it. The KZG commitments are like how you commit to the data, and that includes. That's the polynomial part of it. That's where some of the trickier math comes in. The KZG commitments would be partially introduced by protodank sharding, but not the full 2d KZG scheme with data availability sampling, like some of the nitty gritty stuff.
00:36:46.942 - 00:37:07.810, Speaker A: Okay, so what components of data availability sampling have we not already discussed that we need to dive into? Or maybe we should start from scratch, because I think the listeners can figure this out by now. I'm literally learning as we go. Let's go back to data availability sampling. What is the resource requirement that is trying to mitigate, and how does that work?
00:37:08.180 - 00:37:40.110, Speaker B: Yeah, first I'll explain how it works. So the basic idea is, like I said, with predator dank charting, it's not super scalable to just download all the data. So what we want to do is make it such that you only have to download a piece of the data, a really small portion of it. But how do you. The naive way to do that is just to take this data that was posted. I check a bunch of little pieces, and if they're all there, I just say it's good. The problem with that is you could have missed the one transaction that prints a million eth or whatever, and then you're screwed and all your money is gone or whatever.
00:37:41.200 - 00:37:43.560, Speaker A: Verify every single transaction.
00:37:43.720 - 00:38:25.844, Speaker B: Exactly. So that would be the naive way to do it. The better way to do it is you do what's called erasure coding, so that initial data, the original actual data gets extended. So you have this parity data, and this is used in error correction. It's the same type of technology that's used in cds that if they get scratched, you can reconstruct it with only part of the data, it's the same concept as that, where now you have this original data and you have the extension data, and you only actually need 50% of that data, and then you could reconstruct the whole thing, and it could be any 50% of the data. It could be a little bit of this, a little bit of this. Like, it could be anywhere.
00:38:25.844 - 00:39:00.230, Speaker B: So then it makes it really easy to do that sampling math, because as long as I have 50% of the data, I could recreate the whole thing. So that makes it really easy now, because I could do the samples. And for you to trick me into attesting to this block, you have to be hiding more than 50% of the block, not just like one transaction. You have to be hiding more than 50% of this data that was given to me. So now let's say you were trying to do that. And I check once, and I was successful, I got the data. So I still have a 50 50 shot that the block isn't fully available.
00:39:00.230 - 00:39:44.612, Speaker B: But if I check it again now, I have a 75% chance, and then you have an 87.5% chance, like, you keep doing that. And if you do that, like, a fixed number of times where you have a 50% chance of being tricked each time, and you do that, like 30 times or so, your odds of being successfully tricked, like 30 times in a row at 50% ods, is essentially zero. So that's what we do, is we extend the data using erasure coding such that you only need 50% of it to be available, and then you could safely sample the different pieces. And then if those were successful, you can say with pretty much statistical certainty that the block is available to me.
00:39:44.746 - 00:40:22.172, Speaker A: Okay, so I'm going to repeat this back to you to hope that I got it. So there's this amount of data, there's this data that I got to download and verify, and I need to make sure that 100% of all transactions in this bit of data is completely valid. It's not enough to do all transactions, but one. We can't cut that shortcut, because if one single transaction gets in, if this is coming from an attacker, they might do something like print a billion ether and give it to themselves. And all of a sudden, we have a huge problem with Ethereum security. If somebody is able to print a billion ether, because now they probably control the network. They definitely do control the network.
00:40:22.172 - 00:40:29.616, Speaker A: So we need to make sure that 100% of all transactions are completely valid. But what we want to do, we.
00:40:29.638 - 00:40:53.032, Speaker B: Don'T have to make sure that they're valid. Sorry. That's one point that is really important is the layer one says nothing about the validity of those transactions that are posted to the l one from roll ups. It just says that they're available. And then you rely on proofs to tell you, was it valid or not? But the l one guy who's just checking the data doesn't say it's valid. It just says it was available and someone else will check it.
00:40:53.086 - 00:41:32.212, Speaker A: Okay, cool. Again, leaning into the whole separation of checks and balances, this one person says, this data is available, somebody else go check if it's valid. But right now we're talking about, okay, all transactions are available to be checked by somebody else. And so this one person's job is to say all of this data, every single transaction that exists, can be checked by someone. And again, we need to make sure that every single transaction can be checked. So we have this thing of data, and we do this thing, this weird thing, this complex math thing called extends the data. And my version of that is my interpretation of that.
00:41:32.212 - 00:42:06.384, Speaker A: It makes kind of like a square root of negative one version of it. I don't know. It takes this data and it makes this random manipulation to it to extend the data. And what that does, again with crazy math, is that it takes the same blob of data, but then it makes this clone of it that is out of a particular derivation, and then this person, as a result of that, I might be skipping a step here. As a result of that, I need to check that. This is when we get into the 50% odds thing. Right.
00:42:06.384 - 00:42:14.660, Speaker A: Because of this particular way of this expression, of this creation of extra data. And also with polynomial math.
00:42:15.000 - 00:42:47.100, Speaker B: Yes. So there's two parts of it. You basically need to know that. One, the data was made available. So that's what data availability sampling gives you, that you could check all these random pieces, say it was available, and I'm good to go. The other part of it is you need to make sure that the data was extended properly, because if that extra 50% they gave you was just like dummy data, then I can't actually reconstruct it because that was just like useless data. So that's where the polynomial math comes in, these things called KZG commitments.
00:42:47.100 - 00:43:00.208, Speaker B: Those will prove to you that the data was extended correctly. So then you have the KZG commitments telling you, proving that the data was extended correctly. And then I could data availability samples saying it was extended correctly and it was also available.
00:43:00.374 - 00:43:45.808, Speaker A: Okay, so once we get to that point, we have those assurances, and then we just start taking actual samples. Right? So the first step is to prove that all of the data is extended correctly. And that extending correctly, once we get to that point of extending the data correctly, that's when we get to sample it with the increased assurances that there's nothing hidden. So the extension allows us to have more effective sampling. And then once we get to the extension, then we start sampling this thing. And sampling is like flipping a coin, except once we get to 30 coin flips, you will never, ever, anyone in anyone's life flip a coin 30 times and have it be heads every single time. That will never ever happen in your life.
00:43:45.808 - 00:44:16.090, Speaker A: And so that's how we get to the assurances that we feel secure about, like, it's literally impossible to hide a malicious transaction in this fake bit of data. And so the first step is to extend the data to allow for more efficient and more effective sampling. And then we sample it about 30 times, because that's where the level that we deem safely safe to assume that all the rest of the data is also valid or not valid, but available. Is this all correct?
00:44:16.560 - 00:44:28.044, Speaker B: Yeah, in the real number, it's going to be 75 for some complicated 2d KZG scheme stuff, but it's probably not necessary to go into the details of that.
00:44:28.162 - 00:44:39.890, Speaker A: Okay, so as you can see, my brain is starting to break. John, can you just walk us through one more time? Now that we've gone through this, can you just walk us through the data availability process one more time from the high level?
00:44:40.500 - 00:45:06.500, Speaker B: Yeah. So in protodank sharding, it's going to be simple. You still just fully download the data, and then that's it. If you fully downloaded it and you got it, then you're good to go. You sign off on it. The reason that you could do more throughput for the data in full dang sharding is because now the validators will only be downloading parts of the data. They'll be doing data availability sampling.
00:45:06.500 - 00:45:27.250, Speaker B: They'll be downloading like certain rows and columns, and then they'll attest to if theirs were available. Then collectively you're good to go, as long as there's enough people who are sampling it. So that easier requirement allowed because you don't have to download the full thing, makes it easier for you to kind of juice up the data availability through. But another level.
00:45:27.940 - 00:46:10.732, Speaker A: Fantastic. Okay, so this is where we're about to get into a very fun part of this whole entire story, which is proposer builder separation. And this is where we start to talk about getting paid in ether. And so this is where the ether economics of being an ETH validator come into play, and overall where we'll actually start to be able to differentiate Ethereum scaling strategy from the rest of the alt layer ones. And so, John, I'm going to pick your brain on that because I know you've also written another article comparing the overall the different strategies of alt layer ones and the relations to their l one assets. So definitely a very fun part of the conversation, which I definitely understand a little bit more than the first part of the conversation. So we're going to get there right after we talk about some of these fantastic sponsors that make the show possible.
00:46:10.732 - 00:47:11.532, Speaker A: Ave is the leading decentralized liquidity protocol and now Ave V three is here. Ave V three has powerful new features to enable you to get the most out of DFI, including isolation mode, which allows for many more markets to be launched with more exotic collateral types, and also efficiency mode, which allows for higher loan to value ratios and of course, portals, allowing users to port their AAVE position across all of the networks that AAvE operates on like polygon, phantom, avalanche, arbitrum optimism and harmony the beautiful thing about Ave is that it's completely open source, decentralized, and governed by its community, enabling a truly bankless future for us all. To get your first cryptocuralized loan, get started@ave.com. That's aave.com. And also check out the Ave protocol governance forums to see what more than 100,000 Dow members are all robbing about at governance ave.com. Arbitrum is an ethereum L2 scaling solution that's going to completely change how we use DFI and NFTs. Over 300 projects have already deployed to arbitrum, and the DFI and NFT ecosystems are growing rapidly.
00:47:11.532 - 00:47:55.216, Speaker A: Some of the coolest and newest NFT collections have chosen arbitrum as their home. All the while, DeFI protocols continue to see increased usage and liquidity. Using arbitrum has never been easier, especially with the ability to deposit directly into arbitrum through all the exchanges, including binance, FTX, Huobi, and crypto.com. Once inside, you'll notice arbitrum increases Ethereum speed by orders of magnitude for a fraction of the cost of the average gas fee. If you're a developer who wants low gas fees and instant transactions for your users, visit Arbitrum IO slash developer to start building your DaP on Arbitrum. If you're a dgen, many of your favorite dapps on Ethereum are already on Arbitrum, with many moving over every day. Go to bridge Arbitrum IO now to start bridging over your eth and other tokens in order to experience defi nfts in the way it was always meant to be.
00:47:55.216 - 00:48:29.288, Speaker A: Fast, cheap, secure and friction free. Living a bankless life requires taking control over your own private keys, and that's why so many in the bankless nation already have their ledger hardware wallet. And brand new to the ledger lineup of hardware wallets is the Ledger Nanos Plus, a huge upgrade to the world's most popular hardware wallet. With more memory and a larger screen, the Nanos plus makes it easy to navigate and verify your transactions. And the paired Ledger live desktop app gives you increased transparency as to what is about to happen with your NFT. What you see is what you sign. The Nanos plus gives you the smoothest possible user experience while you're doing all of your crypto things.
00:48:29.288 - 00:48:53.520, Speaker A: So go to the Ledger website to check out the features of the new Ledger Nanos plus and join the waitlist to get yours. And don't forget about the crypto life card. Also powered by Ledger, the CL card is a crypto debit card that hooks right into the ledger live app, right next to all the defi apps and services that you're already used to doing, like swapping tokens and staking. So if you don't have a ledger hardware wallet, go to ledger.com, grab a ledger and take control over your crypto.
00:48:54.740 - 00:48:55.824, Speaker B: Alright, bank sensation.
00:48:55.872 - 00:49:16.520, Speaker A: We are back with John from Delphi. We're going to talk about the last of the three strategies to get Ethereum scaled computation without scaling trustlessness and permissionlessness. And that comes through proposer builder separation, which thankfully is actually baked into the title about what this is. But John, what's a proposer, what's a builder, and what are we separating?
00:49:17.020 - 00:50:10.284, Speaker B: Yeah, so this is one of the key things that makes dank sharding possible from what the old sharding design was. But it was actually initially designed as an MEv fighting strategy that fights the centralizing forces of it. So the way that it works generally is today, if you're like a validator after the merge or a minor today you have to do two things. You have to actually build the block, and you also have to say that this thing was valid and sign off on it. The realization is that those don't have to be the same person who's doing those. And it makes a lot of sense to split those out, because that person who builds the block today, it's like a mining pool operator. But the point is, it's a very difficult role to do that effectively because one of the biggest advantages of being the block producer is MeV, like maximum extractable value.
00:50:10.284 - 00:50:42.980, Speaker B: So ordering these transactions in a certain way that I can extract the most, like whether it's sandwich attacks, Arb, whatever. The problem is that I don't know how to do that on my computer, I'm not going to be able to effectively get Mev out of this block. So I'm just going to say, screw it, give my money to someone else, you validate for me, and then just like, it's fine. We don't want that to happen. We want it to be really easy, that a validator doesn't have to worry about Mev. So we need to just outsource that building task.
00:50:43.320 - 00:51:29.508, Speaker A: And that's what we need to put the mind of the listener in the right spot. I think there's a lot of listeners here, a lot of viewers on the YouTube who plan on staking their ETH, and hopefully they plan on staking their ETh from the comfort of their own home with their own computer. And then they're probably hearing about how lucrative MeV is and they're probably really excited to get some of that MeV when it comes to the merge. The problem is like, yo, listener, yo YouTube viewer, do you actually know how to get me? Are you technical? Because if you're not, then you're just going to leak Mev in favor of those who do know how to capture it. And so if you are thinking about like, oh, I can't wait to get my hands on some of those juicy Mev rewards, what you need is proposer builder separation. So, John, I'll throw it back to you.
00:51:29.674 - 00:52:22.150, Speaker B: Yeah, and exactly. So this won't be implemented until dank sharding, which is why at the merge, flashbots is going to come to the rescue again. And basically this thing, which is effectively out of protocol proposal builder separation, they're going to introduce something called MeV Boost, which is like a plug into your consensus client, which basically replaces your execution aspects of your client. And they will take the bids from searchers and builders who are like, they want to bid. It will build this optimal block and it'll just hand it to your consensus client. The problem with this is you now have trust in that relayer through mvp boost, who's sending you this thing, that all this is valid and all of it checks out. And obviously you don't want to have that trust and all these different things.
00:52:22.150 - 00:53:00.140, Speaker B: So PBS will then actually bring all of that building and auction fully in protocol. And that's what allows for dank sharding as opposed to the old design. Because in the old design, we had these 64 individual separate shard blocks. So it wasn't like that hard to produce a single shard block. The big advantage of dank sharding, the big difference between then and now, is that we just put everything in one big block. It's not sharded in the sense that we normally think of sharding, where it's like these different blockchains that all have different data. Everything is put into one big block.
00:53:00.140 - 00:53:35.960, Speaker B: The problem is, that's really resource intensive. You can't do that on a consumer laptop or anything. So we realized that, hey, we have this specialized role now that we designed because of mev. Let's just take advantage of that and make the blocks bigger, because they're the ones who are building the blocks anyway. We don't need it to be super easy to build the blocks. We just need to validate them. So that's what it takes advantage of, is now we have this high resource builder who's going to build the block, the whole thing with the beacon chain, block all of the shard data, put it all together, and there all the builders are going to send their block headers along with their bids.
00:53:35.960 - 00:54:02.144, Speaker B: And all the proposer has to do is just take the block that has the highest bid, propose it to the network, and people can take that block and they'll be able to do data availability sampling and say, hey, if my part was available and the execution on the execution chain was valid, to sign off on it. So it makes that proposer role super easy, where that builder role is going to be a very high resource, like relatively specialized role.
00:54:02.272 - 00:54:52.112, Speaker A: Okay, and so, going back to the theme of this particular episode, which is checks and balances, this is separating Ethereum block production into two separate groups of people. You have the consumer people, people that plan on staking their ETH, don't know how to capture meV. They probably have like a Mac laptop or some home built pc that they just want to stake their ETH on, but they want to capture Mev. And so the proposer builder separation is it separates this consumer group into the proposers, and then it separates the builder group into the builders. And the builders take all of the transactions and bundle it up into a block that's ready to go. They already did all that computation, and that block is ready to go. It just needs to be approved by the proposer who is staking their ETH.
00:54:52.112 - 00:55:02.840, Speaker A: And then the proposer says, like, okay, I will take this block and embed it into the blockchain. How does the proposer choose to select which builder's block to include?
00:55:03.980 - 00:55:47.316, Speaker B: So they should take the one with the highest bid, is what they should do. You obviously want to make sure that they are acting honestly. One of the issues with this is now you give the builder more power because they can censor transactions. Maybe they bid the highest, but they keep bidding the highest for every block, but they want to censor me because they don't like me or whatever. So the builders just never include my thing. That's where something called Cr list comes in, censorship resistance list. So the proposer will put out this list of, these are all of the transactions I see in the mempool.
00:55:47.316 - 00:56:09.804, Speaker B: And so when the builder is submitting their block and it goes through, they have to either prove that the block was full, so I just couldn't include everyone. So you obviously don't have a guarantee in that scenario or that I included everyone in there, so it helps to offset that power. But, yeah, all the proposer really has to do is just take the one with the highest bid.
00:56:09.932 - 00:56:33.480, Speaker A: And so it's outsourced all of the responsibility of capturing as much mev as possible to this other group of people who have to compete with each other to bid the highest. What's the incentive for the builders to be builders? Can you just walk us through at the basic level, what's the value in being a builder? Types of. For the builder role?
00:56:34.060 - 00:57:08.844, Speaker B: Yeah. So they're the ones who will directly get the MEV and the transaction fees out of this block. So that gets paid to them. Like, searchers will bid to the builders. Like, hey, I'll pay you this much to include my bundle, another search will bid to them, I'll pay you this much to include my bundle, and the builder will get the transaction fees out of that block. And then that's why we want to have, at minimum, enough builders such that there's an efficient market. Because if there's an efficient market, every builder should bid up to the maximum value of whatever they can get out of that block.
00:57:08.844 - 00:57:34.516, Speaker B: And then so indirectly, all of the value of that block then flows over to the validators. Because obviously, if you have an inefficient market and there's just like one super builder, well, then I could take all the money and then I could just bid whatever. But as long as you have an efficient market with enough builders, then they should be able to bid up to the full value of what that block is, so that all the value flows indirectly to the validators, which keeps super decentralized.
00:57:34.628 - 00:57:50.210, Speaker A: So once we get to this point in Ethereum's history where we have these builders who are building blocks, what group of people today does that correlate to? Who are the people in the Ethereum ecosystem that will be likely candidates for being builders later on?
00:57:51.460 - 00:58:31.416, Speaker B: So that is going to be actually a new role because the person who's playing that role today is the mining pool operators. Okay. The actual miners themselves don't actually build the blocks or do anything. The mining pool operator, basically they're the one who does it, and then they send it to the winning miner and they put their proof of work and prove that the block is good. What's going to happen with this mev boost? Which is what will happen initially after the merge, which is like this flashbots program. Flashbots is going to run the builder and the relayer to start, so they'll be the ones taking on that specialized role. And then the searchers are the ones who just submit bundles.
00:58:31.416 - 00:59:11.400, Speaker B: So they run a specific strategy and just say, include these transactions like a little piece. The building is a separate process because it's not just figuring out specific strategies, it's how to take all of the bids that you were given and optimize with some algorithm. What's the best way to put them together? So that's going to be actually like a pretty new role. Flashbots is going to be taking that initially and then looking to kind of progressively decentralize that and allow other builders in. And then when you get to full PBS, it'll be totally permissionless. Like, anyone can do it. And hopefully by then, especially because Mav boost will decentralize enough people know how to do.
00:59:11.400 - 00:59:13.368, Speaker B: Like, there should be parties who are doing it at that point.
00:59:13.454 - 00:59:19.310, Speaker A: Okay, so the people that are running mev bots today, are they the likely candidates to become searchers in the future?
00:59:20.320 - 01:00:01.204, Speaker B: The people who are running mev bots today? Yeah, pretty much nothing's going to change for them. If you're like an mev searcher today, right now you're just submitting to miners. After the merge, you'll be submitting through mev boost to their builders. And then after proposer builder separation, you'll just be submitting your bundles to the builders in that. But all MeV searchers, they for the most part, run a single or a couple optimized strategies of like I'm just doing arbitrage on this, or I'm just doing sandwich attacks here. They're not able to build what is all of the mev out there most efficiently. They're running like two or three strategies.
01:00:01.204 - 01:00:07.948, Speaker B: So the builder is going to be that new role of taking all of those things and putting them into a block, which the operator is doing today.
01:00:08.034 - 01:00:55.220, Speaker A: Cool. So it's like a meta searcher, right? And so we have like some searchers are making sure there's a balance between balancer, Uniswap and sushi swap, and so they're taking the best arbitrage opportunities there. And that is a highly competitive market that will be many, many searchers for. And then there's like the liquidation searchers, so people that liquidate people on compound and Ave and other borrowing money markets, and those people have a highly complex, highly refined strategy and they compete in that realm. And then there's like 17 other different sources of mev in the ecosystem. And every single source likely has its own searcher. And the searchers produces bundles, which bundles up all the MEV that they can capture out of the MEV opportunities in Ethereum.
01:00:55.220 - 01:01:36.520, Speaker A: And then the builders take all the searchers bundles and they bundle it up into one single block, and then they propose that to the proposers. And the proposers accept the highest bid, which likely comes from the most efficient collection of MeV, from all the searchers coming from the most competitive builder going to the people that are just staking ether. And so all of this work that goes on behind the scenes from all the searchers collecting all the mev, and all the block builders competing to build the best blocks, ultimately gets captured by the single solo staker who's just staking eth on their ethereum node, and they didn't have to worry about any of it. That's pretty cool.
01:01:36.670 - 01:01:52.028, Speaker B: That's pretty cool. Yeah. There's a lot of steps to all these specialized parties, but that's the beauty of all this, is it just completely abstracts all of that away. I, as a proposer, need to say that's the highest bid. Simple. That's pretty much it.
01:01:52.114 - 01:02:34.410, Speaker A: Simple. And something that Vitalik said in his end game, which you cited in your article, is some centralization is needed to scale, which is know centralization. That's like a bad word in this industry. And additionally, with this whole proposer builder separation, we're going to have people on their tiny little consumer laptops, and then we're going to have other people with their supercomputers that are competing against other people with supercomputers. And so we do have this stratification of like, we do kind of have this alt layer, one super node type setup in Ethereum, but it's okay in this version because of the checks and balances by this proposer builder separation. Would you say that's a fair take?
01:02:34.860 - 01:03:22.072, Speaker B: Yeah. You still want the builder to not be like literally one guy at Google who needs this ridiculous computer to do it. You want it to be enough that it's decentralized enough that you have an efficient market and that you don't have to worry about liveness concerns, that, oh, it's five builders in the same place, and some regulator is just going to go shut them down. And the hardware requirements that we're talking about for these builders, it's like running a GPU and having, I think it's like two and a half gigabits, like at least bandwidth. Those are high resource requirements for a regular person. But just on your consumer laptop, you can't do. But they're not like these crazy requirements that it's decentralized enough that we shouldn't have to worry about with all these checks in place, about censorship or liveness from them.
01:03:22.072 - 01:03:28.564, Speaker B: And then we're just relying on the decentralized validators at the end of the day, who are like, they're the ones who are ultimately still going to sign off on everything.
01:03:28.702 - 01:03:38.332, Speaker A: Would you say that it goes from a proposer you can do on your regular consumer laptop and then a builder, you kind of need a high powered gaming computer. Is that a fair take?
01:03:38.386 - 01:03:45.292, Speaker B: Yeah, you would need a GPU or a really high end cpu and a very good Internet connection.
01:03:45.356 - 01:03:45.728, Speaker A: Okay.
01:03:45.814 - 01:03:50.690, Speaker B: And to be a proposer, I could do it on my laptop. It won't be anything crazy.
01:03:51.140 - 01:04:16.330, Speaker A: Okay, so those are the three things that finishes off proposer builder separation, which I do think is like the most fun and interesting part of this conversation, because that's where we start to talk about who gets paid ether. But I do want to kind of compare and contrast this to other l one scaling strategies. And so, at the highest of levels, how does Ethereum's roadmap with all this stuff differ from the strategies of the generalized alternative layer one? How would you classify that?
01:04:16.700 - 01:04:53.152, Speaker B: Yeah, so there's a few different approaches. One approach is I would simplify as like the Salana approach, which is for the most part, you make some optimizations to the execution environment, like parallelism, et cetera. But mostly you get a lot of your scaling out of like it's higher hardware requirements. You need higher hardware requirements to run a full node and that lets you process more transactions. Downside of that, obviously I can't run a validating node or be a full node of Solana on my laptop. It's not a reasonable constraint for me. So that's trying to like, all right, let's jam everything through one chain.
01:04:53.152 - 01:05:53.620, Speaker B: The other vision would be more along the lines of like avalanche or cosmos which use subnets and zones respectively, saying, okay, one chain is not enough, we'll split it a boss across a bunch of chains. Problem with that obviously, is it fragments security and you probably end up with pretty small, not super secure validator sets for a lot of these things and you're still just trusting the honest majority of them. You're not going to have people running full nodes and validating all these different subnets and zones. You're trusting the honest majority of them and you're fragmenting security across all of them. And then there's like the modular vision approach, which is like Ethereum, Celestia, Polygon avail, where hey, we're going to be this base layer and then we're just going to let roll ups live on top of us and they can inherit our security and run it like really low resource requirements for users who just need to be checking proofs basically. And from that they will have the full security and the higher throughput all in this interconnected network.
01:05:53.780 - 01:06:21.760, Speaker A: And talking about a line from one of your other reports, and you alluded to this, but with a component that I want to lean into, you say many monolithic ecosystems such as avalanche and Cosmos can achieve meaningful scale. However, this fragmented security approach inherits far less value capture back to the base asset. It will instead disproportionately accrue to subnets and zones respectively. Can you talk about the value capture part of this thing and how that fits into this conversation?
01:06:22.200 - 01:07:09.164, Speaker B: Yeah, so that's a big difference of this. Effectively, when you look at either avalanche or Cosmos, they're not inheriting security from the primary network or the hub or anything. So there's no reason to pay that main network. And so you don't each zone and each subnet, they run their own validators and they get paid the fees for that chain like anything that gets paid or burnt or anything that goes to them. That's obviously not the case with roll ups. And that's where you get a real staking yield and all of these things out of them, not just a made up staking yield that it's inflationary and you're giving out money. Roll ups will continue to pay back to the l one like fees.
01:07:09.164 - 01:07:21.670, Speaker B: And at the same time, Ethereum will continue to have its native execution environment that also has. That is ultra secure and that you're getting settlement to the l one. Everything gets paid back to it still.
01:07:22.460 - 01:07:31.770, Speaker A: Okay, John, this is a lot, this is super complicated. How long did it take you to actually go through and understand all these things?
01:07:34.380 - 01:08:04.660, Speaker B: So I started working at Delphi two months prior. So these were my first two reports. Prior to that. I got interested in crypto last year, like beginning of last year. Basically just started buying bitcoin when it was going up, because I was like, this is going up. Got really interested in d five around April, May, just in time to get wrecked by the crash there. And then over summer, it's like end of the year was actually when I found you guys.
01:08:04.660 - 01:08:51.040, Speaker B: And that's when I started to get red told on the whole modular vision, the first time that I ever first understood, okay, what is the modular approach? How do these roll ups actually scale? Was the ultra scalable Ethereum article that you put out, like end of October. And that's when I had the oh shit. Moment. I remember when I got to the part of that where you start to realize how these modular things scale is. They actually scale by getting more decentralized, which is like a crazy realization because as you have more of these decentralized validators saying the data is available, you can safely increase throughput even more. And then you just keep adding more, and then you can add more data availability throughput, which means more roll ups, which means more transactions. And it's just like this virtuous cycle.
01:08:51.040 - 01:09:00.452, Speaker B: So, yeah, I mean, I first got into looking at modular, like six months ago and started full time like two months ago. So it's all out there on the Internet. Yeah.
01:09:00.506 - 01:09:45.350, Speaker A: Can you just explain for the listeners what it's like to some people go down the rabbit hole pretty damn fast? But I would say you've gone down the rabbit hole faster than I've ever seen anyone go down the rabbit hole. Can you just talk a little bit about what that's been like for the last few months for people who are perhaps sitting on the edge of the rabbit hole thinking about, do I dive in or not? Do I get a job in crypto? How much do I commit myself to this? Can you talk a little bit about just how you got versed in these things that are super niche like, polynomials are one thing, but then we can get into erasure coding and all that stuff. I don't even understand this. From what I've gathered from you, you didn't understand it either more than a couple of months ago. So can you explain to the listeners what that's been like?
01:09:46.440 - 01:10:26.656, Speaker B: Yeah, I would definitely encourage people to, if you are really interested, dive in. I was slow on that for a while. The first eight months or whatever longer that I was in crypto, I was spending a little bit of time on it on the side, but I wasn't thinking I was probably going to go do this as a job kind of thing. Once I got into the whole modular from you guys, I then found Polynaya. And that's when I went through all of his stuff over end of last year, like beginning of this year. And that's when I hit the point of like, okay, I'm going to go do this as a job full time. I enjoy doing this after my job in banking way too much.
01:10:26.656 - 01:10:56.104, Speaker B: I need to just spend a couple of months, really learn all this stuff, and then just immediately start go interviewing. So that's what I ended up doing was if you just bust your ass on it, literally everything is out there. And that's what I did. I basically read a ton of poly naya for a couple of months and then I put out a modular related post and then started interviewing from there. And it moves really quickly. That is one of the really cool things about crypto that is very different. No one really cares what your background is.
01:10:56.104 - 01:11:03.916, Speaker B: If you're smart and you know your shit, you'll get hired and all of it is out there on the Internet. It was a lot of fun going through that.
01:11:04.018 - 01:11:30.564, Speaker A: Amazing. Well, John, fantastic work on this report. I'm going to have to go listen back to this podcast again. I'm going to have to read your report again to fully understand it. But again, for the listeners who brains feel a little bit broken, I'm with you. But all you have to understand is that there is a separation of powers that puts power and keeps power in the hands of the individual, the people that hold and stake ether at home. And so the cool thing about Ethereum is if you are just doing that, that is enough.
01:11:30.564 - 01:11:38.760, Speaker A: John, thank you so much for diving into the very technical parts of the Ethereum roadmap and explaining that to us here on the bankless state of the nation.
01:11:39.820 - 01:11:40.890, Speaker B: That's fun.
01:11:41.260 - 01:12:02.312, Speaker A: Awesome. Bankless nation. You guys know what to do next. There is a link going to be links in the show notes linking to John's report, both the one that we were talking about the majority of this podcast, as well as his other report comparing Ethereum to other layer ones. As you all know, ETH is risky, crypto is risky, Defi is risky. You can lose what you put in, but we are headed west. We're on the frontier.
01:12:02.312 - 01:12:39.228, Speaker A: It's not for everyone, but we are glad you are with us on the bankless journey. Thanks a lot. Hey, we hope you enjoyed the video. If you did, head over to bankless hq right now to develop your crypto investing skills and learn how to free yourself from banks and gain your financial independence. We recommend joining our daily newsletter, podcast and community as a bankless premium subscriber to get the most out of your bankless experience. You'll get access to our market analysis, our alpha leaks and exclusive content, and even the bankless token for airdrops, raffles and unlock. If you're interested in crypto, the bankless community is where you want to be.
01:12:39.228 - 01:12:54.890, Speaker A: Click the link in the description to become a bankless premium subscriber today. Also, don't forget to subscribe to the channel for in depth interviews with industry leaders, ask me anythings and weekly roll ups where we summarize the week in crypto and other fantastic content. Thanks everyone for watching and being.
