00:00:04.250 - 00:00:25.682, Speaker A: Welcome to Bankless, where we explore the frontier of Internet money and Internet finance. This is how to get started, how to get better and how to front run the opportunity. This is Ryan Sean Adams, and I'm here with David Hoffman. And we're here to help you become more bankless. You know how we say blockchains sell blocks? Well, soon theorem's going to be selling more than just blocks. It's going to be selling blobs. Blobs.
00:00:25.682 - 00:00:46.250, Speaker A: That's right, blobs. So we're just a few months out from the biggest Ethereum release since the merge and I think no one has fully mapped out the implications of this. But it's going to be huge. So Ethereum is getting a new product to sell. It's called Blob space. That is in addition to block space. The cost of transactions on L2s is about to drop towards zero.
00:00:46.250 - 00:01:14.222, Speaker A: The economics of ETH gas and the burn are about to change forever. We're calling this upgrade the Blob Space upgrade. EIP 4844 protodank sharding that's what the geeks are calling this new Ethereum feature upgrade. And we want to cover today everything that you need to know about Blob space. And this coming Ethereum upgrade with Ethereum researcher domathy. And this is an absolute banger of an episode. I couldn't be more proud to present this to the Bankless nation.
00:01:14.222 - 00:01:32.810, Speaker A: A few takeaways here. Number one, we go through what is Blob Space? Number two, we go through the history, how we actually got here. This rollup centric roadmap. Number three, we go through the economics. What does this mean for Ethereum's economics? For ETH value accrual, for ETH the asset. David, why was this episode significant to you?
00:01:32.880 - 00:02:14.214, Speaker B: I think if there's any sector of conversation that you and I really just love, it is the intersection of cryptography and economics like numbers and economic manifestations on Love language protocols. That's our love language. This episode is that we've talked about EIP 4844, we've talked about Proto dank, sharding. Those are the same things. We've defined it a handful of times in a number of different capacities. We've never done the aggressive headfirst dive down the rabbit hole and come out the other side of economics. So we have technically scaled data availability at a technical level.
00:02:14.214 - 00:03:12.966, Speaker B: That is a protocol improvement. But how does that connect to the markets side of Ethereum? The two marketplaces, the one marketplace that are now being fractured into two block space and Blobspace are now two different independent markets that are being contained each inside of an Ethereum block. What does that mean for Ether? What does that mean for the marketplaces that arise around these things? How does the equilibrium of the supply and demand of each push and pull on each other? What does this do for L2 scalability? What does this do for economic use cases on top of L2s? How does demand for layer one change the demand for L2? There's so many good stuff at the bottom of this conversation, we're going to start with the basics, the ones that bankless listeners probably know all about. 4844 just to lay the groundwork. But then we're going to poke out the other end of the rabbit hole into the economic side of this conversation, which I don't know if many people have even had before, other than back channels and Harvard DMs and speculation.
00:03:13.078 - 00:03:32.974, Speaker A: Yeah, going through the implications is perhaps my favorite part. And this is one of my favorite types of episodes that we do because it's both frontier and imminent. I mean, we're just talking months away from this upgrade, so it's right around the corner. It's very relevant to our here and now lives. This is not Sci-fi, actually. This is not sci-fi ethereum stuff.
00:03:33.012 - 00:03:34.862, Speaker B: This is like the near term stuff.
00:03:34.996 - 00:03:44.722, Speaker A: Yeah, this is the stuff we get to look forward to. So, guys, we're going to get right to the episode with Domathy. But first, we disclose. Nothing big to disclose today. Both David and I hold ether. You already know that.
00:03:44.776 - 00:03:45.214, Speaker B: Shocker.
00:03:45.262 - 00:04:01.494, Speaker A: We are long term investors. We're not journalists. We don't do paid content. There's always a link to all bankless disclosures in the show notes. All right, we're going to get right to the episode with Domathy, but before we do, we want to thank our friend and sponsor Kraken, which is our number one recommended exchange for 2023. Go check them out.
00:04:01.532 - 00:04:34.818, Speaker B: Kraken Pro has easily become the best crypto trading platform in the industry. The place I use to check the charts and the crypto prices even when I'm not looking to place a trade. On Kraken Pro, you'll have access to advanced charting tools, real time market data, and lightning fast trade execution, all inside their spiffy new modular interface. Kraken's new customizable modular layout lets you tailor your trading experience to suit your needs. Pick and choose your favorite modules and place them anywhere you want in your screen. With Kraken Pro, you have that power whether you are a seasoned Pro or just starting out. Join thousands of traders who trust Kraken Pro for their crypto trading needs.
00:04:34.818 - 00:04:37.334, Speaker B: Visit pro. Kraken.com to get started.
00:04:37.372 - 00:04:37.862, Speaker A: Today.
00:04:37.996 - 00:05:43.434, Speaker B: Mantle, formerly known as Bitdao, is the first Dowled web3 ecosystem, all built on top of Mantle's first core product, the Mantle Network, a brand new high performance ethereum L2, built using the Op stack but uses Eigen layer's data availability solution instead of the expensive ethereum layer one. Not only does this reduce Mantle Network's gas fees by 80%, but it also reduces gas fee volatility, providing a more stable foundation for Mantle's applications. The Mantle Treasury is one of the biggest Dow owned Treasuries, which is seeding an ecosystem of projects from all around the web3 space for Mantle. Mantle already has subcommunities from around web3 onboarded, like Game Seven for web3 gaming and buybit for TVL and Liquidity and Onram. So if you want to build on the Mantle Network, mantle is offering a grants program that provides milestone based funding to promising projects that help expand secure and decentralize mantle. If you want to get started working with the first Dowled layer Two ecosystem, check out Mantle at Mantle XYZ and follow them on Twitter at zero Xmantle, Arbitrum is accelerating the Web three landscape with a suite of secure Ethereum scaling solutions. Hundreds of projects have already deployed on Arbitrum One.
00:05:43.434 - 00:06:33.146, Speaker B: With flourishing DFI and NFT ecosystems, Arbitrum Nova is quickly becoming a Web Three gaming hub, and social DApps like Reddit are also calling Arbitrum home. And now Arbitrum Orbit allows you to use Arbitrum Secure scaling technology to build your own layer three, giving you access to interoperable customizable permissions with dedicated throughput. Whether you are a developer, enterprise, or user, Arbitrum Orbit lets you take your project to new heights. All of these technologies leverage the security and decentralization of Ethereum and provide a builder experience that's intuitive familiar and fully EVM compatible faster transaction speeds and significantly lower gas fees. So visit Arbitrum IO, where you can join the community, dive into the developer docs, bridge your assets, and start building your first app with Arbitrum Experience web Three development the way it was always meant to be secure, fast, cheap, and friction free.
00:06:33.248 - 00:07:06.146, Speaker A: Bankless Nation we are excited to introduce you once again to Dom, also known as Domathy. He is a researcher at the Ethereum Foundation. He's working on research and development of some key Ethereum upgrades that are coming down the pipe, including EIP 4844 that's the subject of today. Also full dank sharding and also mev. Burn. We last had Dom on the podcast, in fact, to talk about Mev Burn, and this time we're bringing him on to describe this new property that Ethereum will get post EIP 4844. And that is Blob Space.
00:07:06.146 - 00:07:19.122, Speaker A: Yep, you heard that right. Not blob space. Blob space. If you've not heard about Blob space, you don't know what this means. This is the episode for you. We're going to take you through the 101 of Blob space and get all the way to the 400s level. Dom.
00:07:19.122 - 00:07:20.578, Speaker A: Welcome to Bankless.
00:07:20.754 - 00:07:23.194, Speaker C: Yeah, I'm happy to be here. Thanks for having me on.
00:07:23.312 - 00:07:54.814, Speaker A: All right, so we're going to get into what Blobs are, Blob space, how it's different from block space. And I think we want to do that in kind of three different sections here. The first is history. We want to talk about how we got here and why roll ups are really going first. And then we'll get into the technical of what Blob space actually is, and then we can conclude with the economics, what all of this means. I just want to give a quick TLDR at the very beginning of why we're doing this episode, why it's important. And I think there's probably three reasons.
00:07:54.814 - 00:08:28.126, Speaker A: Number one, Blobspace is a new resource on Ethereum, okay? And I think we think that this will be as important a resource as blockspace. That's key insight, number one. Number two, Blobspace makes rollups really, really cheap and scalable. And that's going to change everything we know about the Ethereum ecosystem. And number three, Blob space is coming, like possibly this year. Some estimates are possibly November. Of course, we don't have firm dates from the Ethereum core developers, but that might be a consensus bet.
00:08:28.126 - 00:08:41.534, Speaker A: And once it does, it's going to reshape just about everything economically, structurally, about Ethereum. So that's why we're doing this episode right now and why it's timely. Dom, did I say anything incorrect there? Is that a decent summary?
00:08:41.662 - 00:08:43.266, Speaker C: Yeah, it's pretty much all correct.
00:08:43.368 - 00:08:52.150, Speaker B: Before we get into the history, Dom, can I just ask a very simple question? To understand Blob space, is Blob space just block space for L2s?
00:08:52.650 - 00:09:10.410, Speaker C: Pretty much, yes. All the data that L2s need to commit on chain are going to go into these Blobs, which is the new resources Ryan said, into layer one. And the layer one doesn't know what's inside these Blobs. It's just there to prove that L2 committed it so that no one can cheat.
00:09:11.070 - 00:09:40.038, Speaker A: So that's all it is. Blob space is just block space for L2s. And right now, L2s are actually using ethereum block space instead of Blob space. And what we're doing with this new upgrade, EIP 4844, is we're partitioning off this new resource called Blob space and we're making that cheap and very available for roll ups. And so now they can start consuming more Blob space than they do block space. Is that about right?
00:09:40.204 - 00:09:40.920, Speaker C: Yes.
00:09:41.530 - 00:10:18.260, Speaker B: So, Dom, in order to fully understand how we got here, how we got to Blob space, I think it's worthwhile going back into memory lane to understand the fullness of the Ethereum roadmap, because it came to a very logical conclusion of Blobs and Blob space. So maybe we can go back in time, because at one point in time, ethereum's roll up Centric Roadmap was not a thing. We had this thing called execution charting, which we never actually got, but we kind of are now. Can you take us back to wherever in the history of Ethereum's roadmap is appropriate to really understand the full context of Blob space?
00:10:18.710 - 00:11:26.806, Speaker C: Yeah. So in the research space, it was always assumed that the solution to scaling a blockchain was going to be some form of sharding in one way or another. And back when we were at proof of work, it didn't really work to split the blockchain into 64 or 1024 or whatever power of two number of mini blockchains, because each blockchain would have a very small fraction of the overall security. So the plan was always proof of stake first, and then we do this magical split into mini blockchains that run in parallel, where each validator is randomly shuffled. That was basically the general idea for execution charting, to really flip the trilemma of scalability on its head, where instead of being limited by the lowest requirement node on the network, now you would have more nodes equal more scalability, which is what we're as we're going to see. This is what we're going to get with EAP 4844 and full dank charting. So that's pretty much the research goal was to get to charting and then we had more research discovery along the way.
00:11:26.806 - 00:11:31.734, Speaker C: But that was the earliest design to get sharding.
00:11:31.862 - 00:11:39.260, Speaker A: So Dom, when you say sharding, that's basically just like splitting ethereum into different pieces, right?
00:11:41.150 - 00:12:06.194, Speaker C: This is a concept that already exists in computer science or database management. Basically, it's sharded, as in a lot of smaller shards of the blockchain get much more scalability in parallel. So that was the goal, but it turns out it's pretty hard to do that safely with execution charting. And it was a very debated area of research until roll ups came.
00:12:06.312 - 00:12:48.546, Speaker A: Yeah, I was going to ask you, and this is by the way, we're going to memory lane. So this is somewhere around 2019, I think. Just about yeah, that was around the timeline where we sort of discovered that, oh, sharding in this way. Full execution layer. Sharding is going to be really hard and it might overly complicate the protocol and it might take us years to get there. And we might never be able to get there. And so there was a lot of like I felt anyway, as not an Ethereum researcher, but like an ethereum, I don't know, advocate, user, investor, it felt a little bit sad and hopeless back in 2019, like we were never going to make it.
00:12:48.546 - 00:13:01.000, Speaker A: This technology that we hoped would be possible scaling blockchains was never going to happen because there were all of these dead ends. Can you describe what the dead ends actually were? Why was this so difficult?
00:13:01.690 - 00:13:13.626, Speaker C: It was a very big upgrade that was planned, if you remember, around 2019, that was the E 2.0 slogan, where the goal was to have one big upgrade with proof of stake with sharding, with everything along.
00:13:13.808 - 00:13:24.560, Speaker B: We were going to go from the Stone Age of ethereum to Sci-Fi Ethereum in one single upgrade. We were going to get all the upgrades that we've gotten in the last like four years at once.
00:13:25.010 - 00:13:50.542, Speaker C: Yeah. So one big jump and it turns out that's hard to do. There's a lot of complexity involved and it was split up into different phases. And then we said, okay, first we're going to launch the beacon chain, then we're going to figure out how to actually merge it with the current execution layer. And then we're going to do phase one, which is just data. Sharding. So no execution, just all these smaller blockchains, they're just going to contain data and then we're going to figure out how to do execution.
00:13:50.542 - 00:14:02.620, Speaker C: Sharding. So it was a lot of let's figure out as we go, but also in a safe way, so we don't do something we regret later and then break the whole blockchain because there's a lot of economic activity going on on it.
00:14:03.070 - 00:14:49.386, Speaker B: I want to keep on going and defining execution sharding just because once you understand execution sharding, the current roadmap of Ethereum, the roll up centric roadmap of Ethereum is placed into even better context, I think. And so a quote from your article, Dom, that you wrote is that sharding, execution sharding, the original plan for sharding of ethereum. And I did like how you said that we were going to get to sharding one way or another, we just didn't know how we were going to do it. We knew sharding was the answer. We thought originally Ethereum the roadmap was like, okay, execution sharding. Turns out that was a dead end and we picked a different path, still ended up getting to sharding, a different flavor of sharding, which we'll get to. But in the beginning you wrote this article, this line in your article about execution sharding and the technical details about it.
00:14:49.386 - 00:15:43.142, Speaker B: And you said sharding is the shuffling of validators. The Ethereum beacon chain Layer One validators that we know today, shuffling validators randomly across distinct shards of the blockchain, each shard essentially being its own mini blockchain running in parallel to the beacon chain, which sounds a little bit like what we have today with roll ups. But the difference here is that the shards of Ethereum are actually a part of the Layer One protocol. It's like something that has been in my mind lately is like the relationship that a chain has with its own infrastructure. And so execution sharding to me is like state sponsored shards. As in the protocol, the Layer One protocol actually determines what the shards are and that stands in contrast to the role of centric roadmap that we have today. But really to me execution sharding is at first we started with 64 with a planned upgrade to 1048 shards.
00:15:43.142 - 00:15:56.814, Speaker B: But the Ethereum Layer One was going to be all 64 of these shards operated and managed and produced by the Ethereum Layer One protocol. And that's where we started. Am I articulating this correct?
00:15:57.012 - 00:16:46.560, Speaker C: Yeah, exactly. So I would say that the way we're getting execution sharding this way is more indirect with roll ups and data sharding. But it's kind of like a cheat code from a research perspective because Ethereum has much fewer things to do and worry about and then the rest is offloaded to roll ups, which is in my view better than the original plan. Like you said, state sponsored shards where everything is the same. It's the same blockchain, same EVM, same trade offs about everything which was imposed on users. And now instead of that you can have roll ups competing against each other to get the best environment, the better trade offs. If you personally prefer super speed over super security, then you can go on a different roll up and you have more choices and there's more innovation and competition at L2.
00:16:47.330 - 00:17:37.838, Speaker A: There's so much, I guess, to unlock here. And there's a lot of I know we're trying to explain Blob space at the 101 level, but there's so much kind of like, I guess, stacked knowledge that feels almost necessary to explain going to this episode. We have a ton of resources bankless to explain these various topics. But one I just want to touch upon is the three different layers of A blockchain and kind of this modular world that Ethereum is in. There's the consensus layer, and this will be a recap for some Bankless listeners. But there's the consensus layer, there's the data availability layer and then there's the execution layer. The consensus layer defines what's of with Ethereum Mainet, by the way, all of three of these layers are kind of combined in the same thing.
00:17:37.838 - 00:17:47.810, Speaker A: The consensus layer is arguably what A blockchain should really be focused on doing, figuring out what's true in the most decentralized, corruption resistant way possible.
00:17:47.960 - 00:17:50.770, Speaker B: What's true being the order of the blocks exactly.
00:17:50.920 - 00:18:08.594, Speaker A: The order of the blocks. Exactly. And the processing of those blocks. The next layer out is the data availability layer. And my rough approximation of what that is, is what's happened. So you've got what's true, consensus, what's happened. That's kind of like the data layer of A blockchain, like Ethereum.
00:18:08.594 - 00:18:35.380, Speaker A: And then you have the outside layer, which is kind of where all the activity is. That's where all the action is. That's what's happening right now. That's execution as we define it. And the original Ethereum 10, let's call it combined. All of those three things on main chain in just one environment couldn't be parallelized. And now what we're doing with rollups, the roll up centric roadmap with Ethereum is we are, I guess, sharding out.
00:18:35.380 - 00:19:43.094, Speaker A: I'm resistant to using that word, but maybe increasingly we will kind of use that word, sharding out execution from the main chain into these roll ups. But the roll ups, in order for them to be still secured fully with the similar security guarantees as Ethereum mainnet have to post their data, that is what's happened back to the Ethereum mainnet, right, in order to get kind of the consensus benefits and the data benefits. And when they do that, it cost money. It cost right now, block space, and it costs kind of a lot of money relative to what they're doing. So EIP 4844 and what we're calling proto dank sharding. The reason for this whole discussion is the economics change post this update in a very roll up favorable way. But anyway, important for us to know that when we're talking about consensus and data layer and execution, all three of those are kind of separate layers in this new Ethereum paradigm that we're moving towards.
00:19:43.094 - 00:19:46.294, Speaker A: Dom, David, do you guys have anything to add there?
00:19:46.492 - 00:20:26.930, Speaker C: I would say you got it pretty much correct. I'll just hints more on a certain point is that data availability right now is more implicit and it boils down to trustless verification. Right. We want everyone to be able to verify the chain by themselves and not have to have a trust Me bro third party in the middle, which right now this is the bottleneck, right. You need to be able to verify everything, which implicitly means you need to have the data available to you to check the state transitions and execute the transactions so that anyone who sends you money, then you can verify that it's legit. And they really did send you that money and it's not like on some other worthless fork or something.
00:20:27.000 - 00:20:50.460, Speaker A: So let's talk about that for a minute. So the roll ups post the Ethereum data availability layer so that it's not a trust Me Bro type of situation. Exactly. What does that mean for a user inside of a roll up? Does it mean they always have a way to withdraw their assets from the roll ups? Or what kind of certainty security does that give them?
00:20:50.830 - 00:21:30.954, Speaker C: Yeah. So at its core, we say that roll ups inherit the security of L1, which is a very quick tagline for roll ups. And the way it works is you have the layer one contract that bridges asset between Ethereum and roll ups. And ideally, this smart contract can and should in the future all be immutable and not upgradable. Which is why layer one is going to enforce some constraint on what these L2 blockchains can commit to layer one and so that they can't steal assets. And if they censor, then you can still rely on the layer one contract to enforce, like withdrawing your asset or forcing a transaction to go through on L2.
00:21:31.152 - 00:21:52.414, Speaker A: This is why, by the way, I think Dankrad said recently that his definition of an Ethereum roll up is a roll up that posts its data on Ethereum. And if it doesn't do that, then it's something else. It's not an Ethereum roll up. Maybe we use the term validium or something else for that. Do you subscribe to that way of defining an Ethereum roll up?
00:21:52.452 - 00:22:36.640, Speaker C: Dom he actually said this about L2, not roll ups. The roll up term is very specific. That is what he means. He says L2 are roll ups and there is no L2 thing that isn't a roll up, according to Dankrad. And he means that as in, like you mentioned, validiums that have data availability off chain not enforced by Ethereum. And once you leave this Ethereum bubble of security, then you have more trust assumptions regarding the data availability, which leads to a sort of semi trust Me bro situation, as in if you want to exit your assets, then you need to get the data from somewhere. And if it's not Ethereum itself, it's going to be someone else and you have to rely on them to give you that data.
00:22:37.250 - 00:23:25.550, Speaker B: Not only do you have to rely on them, but you also have to rely on the mechanism, right? So you could post your data on bitcoin if you wanted to, and bitcoin is super trustless, but then you need to trust whoever's facilitating that relationship. Typically we call that a bridge. And so just wanted to add that nuance there. There's another quote in your article, Dom, from Italic, that I'm hoping you can kind of put us into the shoes of tall order. I know the quote that you put in there is it seems very plausible to me that when full execution sharding finally comes, essentially no one will care about it. Everyone will have already adapted to a roll up centric world whether we like it or not. And by that point, it will be easier to continue down that path than try to bring everyone back to the base chain for no clear benefit and a 20 to 100 x reduction in scalability.
00:23:25.550 - 00:24:00.410, Speaker B: Basically, we could do the complex thing in scale layer one execution, but all that would really achieve is that roll up sequencers would be like, oh cool, more data and then barely touch the execution that we took so long to shard. This is Vitalik saying that ethereum layer one was always destined to only ever be for settlement. Even if we sharded layer one execution anyways, can you kind of just put us through the thought process of vitalik? Why did we know that even if we did do execution sharding of the layer one, that we would still ultimately end up at the roll up centric roadmap anyways?
00:24:01.630 - 00:24:56.618, Speaker C: The idea is that roll ups can afford to do sacrifices that layer one can't do because layer one enforces constraints on L2 with the bridge contract that I mentioned earlier. So it's something that we can't just achieve the scale in the order of magnitude that roll ups can, even if we shard execution at layer one. And the quote about vitalik is that we're going to spend so much time doing execution sharding and it's not going to matter because roll ups are still going to be much more scalable. And even if we do scale execution, that also means scaling data, because like I said earlier, to check execution, you need to have the data. So if scaling execution implies scaling data, but not the other way around. So to get back into context, scaling layer one just means it scales L2 exponentially. So whether we want it or not, roll ups are here and they're going to be much more scalable than layer one.
00:24:56.618 - 00:25:14.240, Speaker C: So the pragmatic thing to do is to just go down that path and work with roll ups rather than try to be some kind of layer one maximalist. That said, no, you stay on my modular, my monolithic chain that I spend so much time sharding. Everyone's going to say why it's so much faster on L2.
00:25:15.110 - 00:25:50.134, Speaker B: Isn't this related to the dynamic I was talking about earlier where execution sharding is like the state sponsored version of sharding, where every single shard is totally homogeneous. They're all the same size, they all go at the same speed. And then what you're saying is, and Vitalik is saying in this quote is like even if we have that world of execution charting, they're still too rigid. The shards are still too homogeneous. We want more pluralism of core value of ethereum. We want more pluralism in our roll ups. And so even if we do execution charting, what we get out of that is still too rigid.
00:25:50.134 - 00:26:01.102, Speaker B: We can still get more expressivity, more optionality for roll ups anyways on top of execution charting. So we might as well just lean into that. Am I connecting the right dots here?
00:26:01.236 - 00:26:09.886, Speaker C: Yeah, it's related. But the quote about Vitalik was really about scalability because even an EVM rollup is going to be much faster than a sharded layer.
00:26:09.918 - 00:26:49.550, Speaker A: One, one thing I find interesting about this rollup centric approach, which hasn't been mentioned, but I think kind of implied with what you were saying, David, is the original version of Ethereum was very top down central planning, right? It's kind of like the Ethereum protocol is expanding main chain in all of these different directions. And that of course, has some benefits. But we've discussed many of the trade offs versus the roll up. Centric roadmap is very bottom up, like free market developing, its vision going off in many different streams.
00:26:50.130 - 00:26:52.826, Speaker B: The Ethereum Foundation is not developing roll ups.
00:26:52.938 - 00:27:34.026, Speaker A: Yeah, and there's something interesting with the analog that we so often use of nations and how they're built. And the first model is how much should a government actually do? Right? It's kind of like an open question. And different societies, I feel like, have different answers to this question. But the extreme answer almost never works in any organized society. Which is like the extreme answer is the government should do freaking everything right, like your clothes and run all the companies and do all the banking and do everything okay. And I think the most well functioning societies are those that do just the right amount of goldilocks. Government, it's kind of like this goldilocks zone.
00:27:34.026 - 00:28:29.370, Speaker A: And of course, there's probably an overton window of what sort of works there. But this was Ethereum trying to figure out how much work kind of the central government should effectively do here. And where it's left off is we're going to focus on settlement, we're going to focus on this consensus layer and then the outer ring kind of giving a spot for settlement for cheap data availability and we're going to let the free market, the private market, all of these experiments run in parallel in the roll up world. And I feel like this is almost like a hidden genius that I didn't see at the time. 2019, the kind of the Pivot from E 20 felt to me like. A crushing blow. I almost felt like a little bit like Fred Wilson, the notable VC who said at that time, ethereum has failed to execute.
00:28:29.370 - 00:29:23.970, Speaker A: I felt a little bit of that like, oh, okay, so we're giving up on the dream because it's too hard. And all of this research about Sharding was for not. And now I look back at it in 2023, I'm like, holy shit, that was genius. I can't believe we pulled that off. And I don't even know if all of this was intentional or like there was foresight or we just kind of got lucky with how this experiment has played out. But the amount of innovation I see in the roll up world and all of these experiments being pursued in parallel and by the way, funded, funded adequately, I'm putting it mildly, but funded well by VCs and token incentives and all of these different things have really accelerated Ethereum development to warp speed anyway. I don't know if there's anything there, Dom, that you can kind of glom onto and respond to, but that's certainly an observation that I've had recently.
00:29:24.550 - 00:29:56.598, Speaker C: Yeah, it's a very astute observation. But in terms of being intentional or not, I would say it was kind of forced upon Ethereum researchers because many points we've already touched on is execution. Sharding is complex. And the way we're doing with Dank Sharding plus roll ups, it's kind of a cheat code, but a cheat code that's going to happen whether we want it or not. And then we just lean into that. And as you said, it turns out that it's one of the best way to scale a blockchain if we want to have everyone in the world to have access to a scalable, trustless environment.
00:29:56.694 - 00:30:33.906, Speaker A: Okay, so that's a little bit of the history and how we got here and I hope we explained, gave you a tour de force on how we got here to a roll up Centric Roadmap. Now take us to the current state, if you will, Dom. So right now we do have many roll ups. And right now these roll ups, these Ethereum roll ups, many of them do use Ethereum. I guess all of them, if you take Dankrad's explanation, do use Ethereum for data availability. That means they post kind of the fraud proof type data back on Ethereum mainnet and they consume block space in doing that. It was a few days ago, I checked so that these numbers will be somewhat inaccurate.
00:30:33.906 - 00:30:54.730, Speaker A: But I think L2 is, according to ultrasound money, consume about 200, 300, say 200 to 500 ETH worth of block space on the daily. All right. And this is them paying for data availability back on Ethereum. So tell us about that current state. What's wrong with that? That sounds pretty good. We've got roll up. Centric roadmap.
00:30:54.730 - 00:31:10.260, Speaker A: We've got L2 experiments. The fees are fairly cheap across L2, I think. I don't know what you guys have seen recently, but like on the order of cents for most of them. What's wrong with our current state?
00:31:10.630 - 00:31:43.440, Speaker C: What's wrong with it is that a few cents is still too much for scaling blockchains worldwide. Like Vitalik said years and years ago, the Internet of money should not cost $0.05 or $0.50 or I got the exact quote and it's something that people make fun of him because of layer one gas fees. So the current state of things is, like you said, we do have rollouts and they do use layer one's, data availability layer, which is like an implicit layer as we'll see later. And it's still very expensive because of two things. It's that one call.
00:31:43.440 - 00:32:29.094, Speaker C: The cheapest way to commit data on chain right now is call data which is about 16 units of gas for every byte that they commit on chain. So it goes into the gas fee market, which can be very expensive as we see, like when one NFT drops and then everyone's using the chain that makes it more expensive. For roll ups, gas goes up. The cost of putting the one byte is the 16 gas and that price goes up too. And it's also limited. The block sizes have to stay limited because of the problem of scaling a blockchain with the cheapest node have to be able to verify the chain. And that's why blocks have to be small and there's no cheaper way to put data on chain right now for rollout.
00:32:29.142 - 00:33:04.454, Speaker A: So it's too expensive, it's limited. And we've got this weird kind of resource issue where yeah, coupling. Resource coupling issue, that's a great word for it, David. And so what happens is 100,000 users on a roll up are competing with like 200 NFT bidders on main chain and they're all competing economically for the same block space and it drives the 100,000 users prices up. So there's this resource coupling problem as well. Those are the three problems with the current state.
00:33:04.652 - 00:33:36.886, Speaker C: Yes. So the resource coupling problem is kind of a weird thing economically, but there's a technical reason, historical reason why Ethereum went to couple everything into a single unit of gas. But it's weird when you think about it because these 200 NFT bidders are using execution mainly to send money and settle an NFT trade, whereas these roll ups are using data. So it's two completely different resources that shouldn't be linked together, they should have their own supply and demand market. But one goes up, the other goes up as well. So that's kind of the problem of the current state with roll ups.
00:33:37.018 - 00:34:00.018, Speaker B: So not only are these resources coupled, but what you're saying, Dom, is that these resources are inappropriately coupled, as in they don't have to be coupled. One is using data, the other is primarily using execution and these are two different resources that just because everything is contained inside of block space that these things are coupled and inappropriately.
00:34:00.114 - 00:34:03.002, Speaker C: So, yes, that's exactly it.
00:34:03.056 - 00:34:11.694, Speaker B: Okay, so, Dom, we're going to get to what is about to be my favorite question that is ever about to be asked on the bankless podcast. Are you ready for it?
00:34:11.812 - 00:34:12.480, Speaker C: Yeah.
00:34:12.850 - 00:34:13.690, Speaker B: Dom.
00:34:13.850 - 00:34:15.150, Speaker A: What's a blob.
00:34:17.010 - 00:34:25.518, Speaker C: So you want the technical answer because we already end waved it. Blob is just a piece of data that roll ups can put onto main chain.
00:34:25.694 - 00:34:30.690, Speaker B: Yeah, I think it's time to get into a technical. Okay, so again, technically, what's a Blob?
00:34:31.270 - 00:34:49.190, Speaker C: Technically, a Blob is 4096 field elements that are just under 32 bytes each. So that's the playground that roll ups have to put data and they have to abide by this format.
00:34:49.350 - 00:34:52.970, Speaker A: What is the difference between a block and a Blob?
00:34:53.630 - 00:35:20.862, Speaker C: The block contains basically everything that trade, everything at execution. And I don't know how to explain this. It contains the transactions and then you execute the transaction and you see that I sent you one ETH and then that changes the state of Ethereum layer. One, it's like I have one less ETH and you have one more because we executed that transaction on chain and everyone agrees that this transaction happened before and after some other transactions.
00:35:21.006 - 00:35:37.686, Speaker B: Wait, so we have blocks and then we have Blobs and then we have like, data of transactions. Transaction data goes into block space, blobs go into Blob space and everything contained is contained in a block, correct?
00:35:37.868 - 00:35:38.310, Speaker C: Yes.
00:35:38.380 - 00:36:14.622, Speaker B: And so the new thing is this new Blob space. So we have block space and I think people might get and I'm getting confused, but I'm trying to parse this apart, that we have blocks and we have block space and we have Blob space, but Blob space and block space are spiritually equivalent and both of them are going into blocks. And so there's this weird dynamic where block space and blocks have the same name, the same block. But Blob space and block space are shoulder to shoulder with each other, equally contained by a block.
00:36:14.766 - 00:36:39.578, Speaker C: Yes. Now we're getting into metaphor land on the consensus layer. What we say is that these Blobs are in some sort of sidecar alongside the block so the block can stay small. And then there's the big Blobs next to it that are not exactly contained in the block, but the block contains a reference to the data, which is what rollouts are going to use to do all sorts of zero knowledge, magic, that sort of stuff.
00:36:39.664 - 00:37:38.650, Speaker A: I think one of the first learnings here, the first takeaways in this section, because it's going to take us a few minutes to really understand what the heck a Blob is and what the heck Blob space is. But you know that thing that we say on bank lists so often is blockchains sell blocks. Well, I think, David, we have to amend that. Blockchains sell blocks and Blobs, this is a new resource, I think is the key insight that Ethereum is kind of going to market with, if you will, after EIP 4844, which again could happen this year, it's not only selling block space to the market, it's also selling Blob space. And Blob space is a product tailor made for roll ups. What does it do for roll ups? It gives them a space to park all of their beautiful fraud proof data, a data availability layer. It's so beautiful, incredibly cheaply.
00:37:38.650 - 00:37:54.690, Speaker A: So it's a much more efficient resource to get done what roll ups need to get done, which is post these proofs into Ethereum, I guess, block space. But it does it. Ethereum is now in the Blob business.
00:37:54.760 - 00:38:05.442, Speaker B: Baby Tom, you said that a Blob references a Blob, but that a Blob isn't in a block. Is that correct? And then can you elaborate?
00:38:05.586 - 00:38:45.246, Speaker C: Yeah, that's the metaphor. The Blob is the sidecar that's alongside the block. And then basically one key insight is that, like I said earlier, if I send one E to you on layer one, every single node has to compute that transaction to verify my signature, verify, access the state of your account and subtract a number from me, add a number for your account. And that's the very expensive thing. That's kind of the bottleneck that doesn't scale very well. And the way that roll ups scale this way is that I send you one ETH on a roll up, and that's cool. The roll up is going to batch that alongside a lot of other transaction onto layer one inside of a Blob.
00:38:45.246 - 00:39:18.538, Speaker C: And now layer one doesn't care about the data inside that Blob. That's the roll ups business. Right? So I send you one E that doesn't impact layer one nodes. They're just going to see the data, say, okay, the data is there, it's cool. The roll up is everything on the roll up is happening. It's legit. But there's no expensive computation at layer one happening for L2 transactions other than this very small transaction to verify like a proof or update optimism state route every time a sequencer commits a transaction.
00:39:18.538 - 00:39:24.134, Speaker C: So that's why the biggest resource they need is data. And it's how we scale the whole blockchain.
00:39:24.282 - 00:39:38.386, Speaker B: Right, okay. So the way that I understand, I give a metaphor to block space is that block space is like a container of data. And if I send Ryan some Ether, that's a very small bit of data that I'll throw into block space, start.
00:39:38.408 - 00:39:38.806, Speaker C: To fill it up.
00:39:38.828 - 00:40:02.990, Speaker B: If I send Ryan an NFT, that's an even bigger bit of data. If I mint 13 NFTs at once, that's an even bigger amount of data, and that fills up block space. Block space is like this bucket that you fill with different sizes of transactions and eventually you become full. Is Blob space like that? As in, like, there's a container that is filled? Or does it operate with different properties?
00:40:03.650 - 00:40:24.878, Speaker C: It's similar, but it's a decoupled market, too. So there's this other different bucket that only gets filled with data. The layer one doesn't care. It's very agnostic about who post what into these Blobs. All it cares about is getting these blobs out there for whoever needs them to commit to them and download the Blob content. This is what layer one enforces.
00:40:24.974 - 00:40:39.498, Speaker B: Okay, so post EIP 4844, when we introduce Blob space, aggregate ethereum blocks, the size, the data size of aggregate ethereum blocks will be Blob space plus block space, correct?
00:40:39.664 - 00:40:40.138, Speaker C: Yes.
00:40:40.224 - 00:41:09.410, Speaker A: Okay, let's look at this through the different lenses of kind of actors and stakeholders in the ethereum ecosystem. So I want to go through validators in a second, but let's first start with roll ups. So why is Blobspace a better product than block space for roll ups? Is it just because it's cheaper? Or what other properties does it have that makes it appealing and better for roll ups to consume?
00:41:09.910 - 00:41:27.894, Speaker C: From the point of view of a roll up sequencer, it really boils down to being much cheaper and much more plentiful. Layer one is going to do more technical stuff to scale this Blob space, but from the point of view of a roll up, it doesn't really matter. It's just data for them. That's all they need and that's all they care about.
00:41:27.932 - 00:41:56.030, Speaker A: And how do they sort of activate it? Is there anything that they need to do on their side? Is it basically like so all of these proofs that they were posting and buying block space in order to prove they just switch over. So it's just like rather than using, I don't know, electricity for your furnace, you're using now, natural gas, you just use a different resource because natural gas is so cheap, but the system works the same way. You still get your heat. That's what they're doing here. Yes.
00:41:56.180 - 00:42:07.910, Speaker C: Yeah, exactly. After 4844, they'll have to update to support these blobs and stop posting everything on layer one. In the expensive call data section of blocks.
00:42:08.410 - 00:42:24.006, Speaker A: Is there anything that they'll lose by posting, like doing this via Blob space versus block space? Or is it just basically kind of an equivalent substitutionary? Good. Like, it's just as good for roll ups.
00:42:24.118 - 00:42:29.158, Speaker C: It's just as good and even better for them because it's cheaper and more plentiful.
00:42:29.334 - 00:42:40.640, Speaker B: Can I put a transaction in Blobspace? Can I send Ryan some ether in a blob? Or what prevents me as a layer one user from consuming Blob space?
00:42:41.090 - 00:42:54.034, Speaker C: Nothing prevents you. It's a permissionless system. It's tailored for roll ups. But of course, you can put any data inside layer one Blobs as you please, but it's up to you if you want to pay for that.
00:42:54.232 - 00:42:57.038, Speaker A: I'd be more expensive. Maybe a bad idea.
00:42:57.224 - 00:43:04.950, Speaker C: Yeah. I'm predicting that there will be some NFT project where the JPEGs are inside Blobs at first because Blobs are going to be pretty cheap.
00:43:06.090 - 00:43:32.830, Speaker B: Wait, okay, so there's a phenomenon in the NFT world about NFTs that are on chain like crypto punks and MFers and a few other NFT projects. What's the famous one, the generative one that Autoglyphs are like the big on chain art? Does Blob space just simply allow for more? Perhaps as one use case of Blobspace, of which I'm sure there's infinity that we can just put more NFTs more JPEGs on chain?
00:43:33.250 - 00:43:38.642, Speaker C: Yeah. I'm pretty thinking there will be a lot of degen stuff happening with Blobspace because it's going to be very cheap at first.
00:43:38.696 - 00:43:48.050, Speaker A: Oh no, we don't want that though. We want the Blob space for our roll up transactions. We don't want to create another degen market. Isn't that the point of resource decoupling?
00:43:48.710 - 00:44:05.942, Speaker C: Yeah, it's decoupled because if they want to put data on chain, we can't stop them, right? They could just pretend to be a roll up. We can't be top down about who gets to use what from a credible neutrality at layer one. And I don't think there's a way to enforce that. To enforce that only roll up sequencers use Blobspace.
00:44:06.086 - 00:44:43.590, Speaker A: But probably the properties of Blobspace are such that they are most conducive to well, so one example dom, isn't there like an expiry on the Blob space? And this brings us to kind of the we'll talk about the expiry, but this brings us to the second lens I wanted to talk about, which is from a validator's perspective or Staker's perspective, like I'm running a node and I'm staking. Does this the introduction of Blob space change anything for me? Does it increase the requirements of the validating machine that I have to run? Do I now have to store, in addition to all of this block space, do I now have to store a Blob space? Maybe those questions are related.
00:44:43.930 - 00:45:06.542, Speaker C: Yes. So with 4844, it's going to go up a little bit. You have to download and serve these Blobs all the data that's inside the Blob, but you don't have to store it forever. So that's what you mentioned. Blobs will expire after. Right now, the specs say about 18 days, I believe. And after that it becomes like it was attested that it was available during those 18 days.
00:45:06.542 - 00:45:18.946, Speaker C: But if it expires, then you're not going to be able to get it from a node if the node pruned it. So you have to get it from some other source and then you'll be able to see that it was available on chain during that time.
00:45:19.048 - 00:45:34.246, Speaker A: So this might be a reason why it's not as conducive as block spaces to an on chain NFT, although there may be ways around this. But why is this 18 days not a problem? So why isn't that a problem for L2? If the data goes away after 18 days, aren't I? Yeah.
00:45:34.268 - 00:45:38.230, Speaker B: Wait, pruning data away from blockchains is Blasphemy. How dare you?
00:45:38.380 - 00:45:40.394, Speaker A: Yes. How could you?
00:45:40.512 - 00:46:37.654, Speaker C: So this is kind of the weird thing with the name data availability because it's not data storage and it's not continued data availability forever. So it's kind of the opposite. There's a big problem with storing everything by every node forever that doesn't really scale. If you were to pay once to get your NFT JPEG on chain and then it is stored by everyone forever, for all of eternity, with data just going up and up only forever. So instead we use Blob expiry to just ensure that the data was available once it was published and available to be downloaded by everyone. And this is also from the lens of roll ups, is that if you're 100 year from now and you want to sync the optimism state from scratch, you'll have to get the Blob data from somewhere. But then once you have that data, nobody can lie to you and say, oh, a hundred years ago I had a thousand eat that came from nowhere.
00:46:37.654 - 00:47:02.114, Speaker C: And then here's the data. And then you can check that data and you can just check the integrity of that data, but check that its availability was enforced by Ethereum long enough for anyone to snitch on an invalid transaction on optimism. So you'll have these properties of being able to verify the data yourself. It's just that Ethereum is not the one that's going to serve you that data forever for free. Okay? So.
00:47:02.152 - 00:48:30.522, Speaker B: Data availability. In contrast to data storage availability, as in Ethereum is making this commitment to the world around it that it is making data, it is enforcing that data is sufficiently available for the surrounding universe to be able to take and do what they need with that data in order to do whatever they want to do. And we've chosen 18 days as some time length that data is being made available to individuals, to node operators, to other L2 infrastructure providers, to other protocols like Filecoin or data availability solutions like Celestia anyone to take the data that Ethereum has made available to the world, hence data availability for 18 days. And then that data has gone from Ethereum to some more long term storage solution, which there are so many of, it's not even worthwhile for me to list them off. But we can start with data availability solutions like Celestia Eigen layer, data availability, something else, something else. Your hard drives like roll up operators themselves. And so we're just making this statement that so long as we can say that Ethereum enforces data availability of all Blob data for 18 days, then the universe is going to be able to do what they need to do with that data in order to have a fully trustless chain of data that goes back to Genesis.
00:48:30.522 - 00:48:31.680, Speaker B: Is this correct?
00:48:32.390 - 00:49:13.118, Speaker C: Yes. So the goal is really to have lightweight nodes be able to verify everything, including the availability of what's been published in a scalable way. And also another thing to point out is that this Blob data is not like part of the state where you need to read and write to it many times a second. You can just store it on a cheap hard drive. And if you're like a hobbyist or any stakeholder in a specific roll up, you can just download the data you need from the roll up you're using, and then you can just store it for pretty cheap on hard drives. And data storage is just a thing that's continuously getting cheaper and cheaper over time.
00:49:13.204 - 00:50:00.138, Speaker B: Right. Notably, like, terabytes of data are under $100 right now. So maybe if I'm like, maybe we can theorize that in the future there will be some software application that you run on your desktop computer and you link it to your main addresses, and then it follows your addresses around some preselected L2s, like optimism and Zksync and Arbitrum. And then it looks at your addresses and then automatically downloads all of the Blob data for your addresses that you've specified. And then it just stores that on your personal computer. And then you can be the one that verifies the history of Blob space in addition to the many other data storage solutions that may also be downloading and saving the same data. Is this perhaps a version of the future?
00:50:00.304 - 00:50:13.258, Speaker C: Yes. And also roll ups can have their own designs to incentivize storage of their own data if it's something that's very important to that particular roll up in its community. So the design space is so in.
00:50:13.284 - 00:50:24.626, Speaker B: Addition to pushing execution out to the free market and making L2 teams optimized for execution, we're also just pushing data storage out to the free market as well.
00:50:24.648 - 00:50:44.534, Speaker C: Yes. So one metaphor that I've used, vitalik uses that ethereum is not supposed to be more of, like a billboard where you can have information in real time about what's on the billboard. But once it's erased, all you can do is verify that this information wasn't on the billboard available to everyone to see back when it was published.
00:50:44.662 - 00:51:03.486, Speaker A: Yeah, that's kind of the image I get from Ethereum as well. It's kind of like a present moment of time type of consensus mechanism. It's not trying to store the full consensus truth of the universe inside of the universe. It's more like kind of like a.
00:51:03.508 - 00:51:05.342, Speaker B: Hurricane or just kind of like tip of the chain.
00:51:05.406 - 00:51:37.690, Speaker A: Yeah, it twirls around. And so that is this distinction between data availability and data storage. So Ethereum is trying to be a data availability computer, but not a data storage computer. And so this 18 days is also significant because there does have to be some reasonable time period here, right. It can't be like 15 minutes or like 2 hours or even 24 hours because there are things, like with roll ups, right. We have kind of like optimistic roll ups, kind of a seven day proof challenge type window. Right.
00:51:37.690 - 00:52:02.722, Speaker A: And that's why it has to be greater than, like, seven days or just 18 days feels like an okay time horizons. But it shouldn't be like six months because that gets into data storage and it shouldn't be like an hour because that's not enough time for all of the data storage and fraud proof type solutions to kind of react. Is that accurate as well?
00:52:02.856 - 00:52:43.102, Speaker C: Yes. So it basically just has to be long enough for anyone who needs that data to download it. So that's why I said earlier it's kind of a cheat code for execution charting because then you have these roll ups posting Blobs and if it's a roll up you don't care about then you just don't download the Blob. You're going to see that it's there as a layer one validator in layer one node. You're going to participate in securing that roll up. But even if you don't care about it, you don't download that Blob and that's it. It's like the original plan with these mini blockchains where you would only care about the blockchain that has your fund, like one of the shards that have your fund and you don't care about the others.
00:52:43.102 - 00:52:50.402, Speaker C: But this is even better because then the security is not split around these mini blockchains the same way it would have been with the original sharding plan.
00:52:50.456 - 00:52:56.546, Speaker A: Just to tie off this question, you said the requirements for a validator do increase a little bit.
00:52:56.648 - 00:53:23.302, Speaker C: Like what are we talking about with proto dank sharding? I did the math in my blog post. With the current size of a Blob and the target number of Blobs every block, we're looking about 50GB extra storage by every node. But this is for 4844. After full dink, sharding is going to go down somehow, blobs are going to be bigger and the requirements are going to be lower for every node. So the goal, which is the magical.
00:53:23.366 - 00:54:00.358, Speaker A: Sharding, the goal of Ethereum in general is to be able to run a validator on consumer grade hardware, right? And we meet that goal right now. Like you can run a validator on a Raspberry Pi. And what you're saying is this Blob space change increases the hardware requirements by about 50 gigs of just about hard drive space. So doable I think on most consumer hard drive rigs. So it's not nothing, but I mean 50 gigs, you could get thumb drives for $10 that are 50 gigs these days. Okay.
00:54:00.444 - 00:54:38.502, Speaker B: MetaMask Portfolio is your one stop shop to manage your crypto assets and to tap into DFI all in one place. And the most important part of that experience? Buying crypto. Obviously, MetaMask Portfolio's buy feature enables you to purchase crypto easily without going through centralized exchanges designed with you in mind. You can fund your wallet directly in just a few clicks with convenience and simplicity. What happens when you press the buy button? Rather than being limited to a single payment provider, MetaMask brings together a bunch of vetted trustworthy providers to present you with customized quotes for your crypto purchase. Once you've funded your wallet you'll be able to plug into DeFi with all the money verbs like swapping, bridging and staking. But first things first, you need skin in the game.
00:54:38.502 - 00:55:02.790, Speaker B: Head over to MetaMask IO portfolio to buy crypto the easy way. Cello is the MobileFirst EVM compatible carbon negative blockchain built for the real world. And now something big is happening. Introducing the Celo L2. It's a game changing proposal that's going to bring Cello's rapidly growing ecosystem home to Ethereum. Vitalik has shared its excitement for the Cello L2 on the Cello forum. So has Ben Jones from Optimism.
00:55:02.790 - 00:55:33.990, Speaker B: But why? The Cello layer Two will bring huge advantages, like a decentralized sequencer offchain data availability and one block finality. What does all that mean? Rock solid security, a trustless bridge to Ethereum and more real world use cases for Ethereum without compromise. And real world adoption is happening. Active addresses on Cello have grown over 500% in the last six months. With the Cello L2, gas fees will stay low and you can even pay for gas using ERC 20 tokens. But Cello is a community governed protocol. This means that Cello needs you to weigh in and make your voice heard.
00:55:33.990 - 00:55:50.214, Speaker B: Join the conversation in the Cello forum, follow@cello.org on Twitter and visit Sello.org to shape the future of Ethereum. You know, uniswap. It's the world's largest decentralized exchange, with over $1.4 trillion in trading volume. You know this because we talk about it endlessly on Bakelist.
00:55:50.214 - 00:56:19.866, Speaker B: It's uniswap. But Uniswap is becoming so much more. Uniswap Labs just released the Uniswap Mobile Wallet for iOS, the newest, easiest way to trade tokens on the go. With the Uniswap Wallet, you can easily create or import a new wallet, buy crypto on any available exchange with your debit card with extremely low fiat on ramp fees. And you can seamlessly swap on main net, polygon, Arbitrum and optimism. On the Uniswap Mobile wallet, you can store and display your beautiful NFTs. And you can also explore web3 with the inapp search features, market leaderboards and price charts.
00:56:19.866 - 00:56:32.422, Speaker B: Or use Wallet Connect to connect to any web. Three application. So you can now go directly to DFI with the Uniswap Mobile Wallet safe, simple custody from the most trusted team in DFI. Download the Uniswap wallet today on iOS. There is a link in the show notes.
00:56:32.486 - 00:57:03.570, Speaker A: Dom. There's two other lenses I just want to put this through. One is developers, which I think a lot of the development, a lot of the app building will kind of migrate to L2s. And probably the obvious answer, the good news for them is your application can support cheaper, more transactions per second, because they'll be a whole lot cheaper. So go have fun. Right, that's probably the answer for developers. How about users? Is there anything that they how will they experience this new Blob space world? This post 4844.
00:57:03.570 - 00:57:09.614, Speaker A: I know David's going to try to send me some ETH using Blobspace somehow. I don't know if he'll be able to do that, but maybe the typical.
00:57:09.662 - 00:57:10.730, Speaker C: User, not truly your wife.
00:57:10.750 - 00:57:12.806, Speaker A: Yeah, how will they be able to do it?
00:57:12.908 - 00:57:44.846, Speaker C: Yeah, the typical user, hopefully all the technical stuff is abstracted away from them. It's more the heavy users who really insist on having trustless verification of everything to secure their funds themselves with their own keys. And this is what Blobspace allows them to do. They just keep that data, only the data relevant to them about their funds, where it's stored on the state, how to move it, stuff like that. So the overall trustless and permissionless is not sacrificed from a blockchain point of view.
00:57:45.028 - 00:57:54.098, Speaker A: Yeah, I guess this will just accelerate the migration from mainnet to roll ups as well. I'm sure we could probably predict that this economic change will result in that.
00:57:54.184 - 00:58:24.102, Speaker C: You quickly mentioned developers. I would also like to add that right now, developing on layer one is kind of a weird thing because every block space is so limited that as a developer you have to do all sorts of fancy tricks to use less gas for the same transaction. But on L2 you're going to have so much more block space of L2 blockchains and roll ups that you're not going to have to worry about these fancy tricks and to use less gas because there's just going to be way more bandwidth for L2.
00:58:24.256 - 00:58:46.818, Speaker A: Dom, you just said as a developer block space and you use that term block space. And I think that's because for L2. For L2. So L2s basically are almost like a value added reseller of Blob space. So they take this Blob space, it's now a lot cheaper and subsidized by Ethereum, and they convert that to block space in their own L2 and then they sell that to devs and applications.
00:58:46.914 - 00:58:49.880, Speaker B: Oh, L2s turn Blobspace into block space.
00:58:52.330 - 00:59:06.102, Speaker C: That's the tagline of the roll up centric world is that it's much easier to scale data availability on layer one and roll ups are going to take that data and convert it to scalable execution. So scale one, you scale the other for much easier.
00:59:06.166 - 00:59:13.486, Speaker A: It's just like unrefined oil that you then process and turn into like a petrol or something like this.
00:59:13.588 - 00:59:17.418, Speaker B: Amazing. That's a great metaphor.
00:59:17.514 - 01:00:16.014, Speaker C: So as a developer on L2 you have much more freedom to do whatever you want, however you want. And it's something that kind of goes under the radar with L2 execution is that you can only batch the state transitions onto layer one. So if I do a very complex execution at L2 that sends one east to like 1000 people and then I don't know, loops around and buys a bunch of NFTs and trades them somewhere, buys collateral, you can think of this hyper complex transaction and then that batches onto layer one, only the state transition. So people don't need to verify the whole execution of the transaction. They only need to verify the output, the actual outcome of the transaction, and that's much more scalable. And it's pretty cool how more complex transactions at L2 become cheaper compared to the same transaction at layer one. It's not the same ratio of scalability for each transaction.
01:00:16.014 - 01:00:29.640, Speaker C: Like, a more complex transaction has way more savings. So as developers on L2, there's way more use cases and way more freedom that become available to do all sorts of things that we aren't even conceiving of right now.
01:00:30.090 - 01:00:52.010, Speaker B: God, computers are so cool. Okay, Dom, let's do our best to get into probably what's about to be the most technical part of this conversation, which is actually, how does a blob become a blob? There's some crazy math involved here. There's things like polynomial commitments that make me scared. How does a blob become a blob? How do we start this conversation?
01:00:52.170 - 01:01:02.490, Speaker C: It's all polynomial magic. Basically, you have these 4096 elements that contain your data of your blob.
01:01:02.570 - 01:01:04.026, Speaker B: Sorry, what's an element?
01:01:04.218 - 01:01:12.690, Speaker C: Field element. It's basically a number. But inside of modular arithmetic, I don't know how technical you want me to be about this.
01:01:12.840 - 01:01:16.770, Speaker A: I have a feeling we're getting to the firmware level of technicality.
01:01:17.110 - 01:01:56.130, Speaker C: Yeah, this is the stuff that everyone on L2 doesn't need to care about, including developers and roll up sequencers. All they do is send the data to layer one, and then layer one transforms it into this big polynomial equation that if you have a bunch of data, you put them inside these things and then you interpolate into a polynomial. So it's kind of like, you know how two points make a line? Then you can just treat your two point data set as a line equation, and then suddenly this line is like, extended to infinity on both sides. So that's what we're doing, but with 496 numbers instead of just two. So it's going to be a big degree polynomial.
01:01:56.470 - 01:02:44.778, Speaker B: I think at the end of this technical conversation, the Mic drop punchline is that this is what Sharding is. And then there's a bunch of middle ground question marks, math steps that I don't understand, but the end result is like, oh, the execution Sharding that we originally planned for in the roadmap. Now we have data Sharding, and it's done with these polynomial commitments. And ultimately you have more data that is actually contained by a much smaller amount of data. That like how you said with this extending the line. The smaller amount of data contains references to every other bit of data that is in the blob. And once you quote, unquote, extend the line, you can unfold the packet of data to create the full amount of data that is the blob.
01:02:44.778 - 01:02:47.860, Speaker B: That's my summary of this. Do you want to add any more of that?
01:02:49.350 - 01:03:26.286, Speaker C: There's more polynomial magic to reconstruct the whole data from just a portion of it, but also to check its availability, which is the crux of denk sharding. You'd want nodes to be able to check that data availability, like that data was actually published by block producers and a roll up sequencer, you can check that a roll up sequencer was legit and posted the data on chain, but without downloading that data yourself. So you're not suffering that burden from trying to enforce the availability of data. This is what the polynomial magic so.
01:03:26.308 - 01:03:47.250, Speaker A: Really quick here, Dom. So you just mentioned data availability sampling, I believe, and this is a future upgrade that is beyond EIP 4844. So EIP 4844, another synonym for that is proto dank sharding. And the proto means like, I guess pre it's first, right?
01:03:47.400 - 01:03:48.690, Speaker C: It comes from proto lambda.
01:03:48.770 - 01:03:49.506, Speaker A: Proto lambda.
01:03:49.538 - 01:03:49.894, Speaker C: That's right.
01:03:49.932 - 01:04:12.510, Speaker A: Never mind, I forgot about that. So proto lambda also implies maybe like it's first, we do that first and then the data availability sampling you were just discussing, which also uses some magic polynomial math that comes later with full dank sharding and the benefit there with full dank sharding later TBD, we don't have any dates on this. Like think years, not months.
01:04:12.580 - 01:04:13.200, Speaker B: Okay.
01:04:13.890 - 01:04:32.340, Speaker A: And the benefit that that gives us with full bank sharding, with data availability sampling, full bank sharding is what the Validator clients just hold less data. So that 50 gigs that you were talking about earlier, that drops down or what properties does full dank sharding give us?
01:04:32.950 - 01:05:22.258, Speaker C: Yeah, so 4844 sets the stage for full dank sharding with all the polynomial stuff. But also, like I said earlier, ethereum right now has this kind of a data availability layer in the form of block space and call data. But it's not scalable, right, because every node needs to download everything which makes the bottleneck. So to answer your question, the data availability sampling aspect of full dink sharding will enable this data availability layer to become much more explicit where even a full majority, like a supermajority of Validators can't fool you into believing that an unavailable block is available. And this is what your node is going to sample, just a tiny amount of data to be sure with like one in a trillion probability that the data is actually there.
01:05:22.424 - 01:06:15.858, Speaker A: Okay, so Dom, just again, high level, not in the weeds because there's so many things we could explain here, but high level, this branch of magic polynomial math which enables proto dank sharding and full dank sharding. Okay, this feels like a free lunch to me. It's like, wow, here you go with some math. We just put some math on it and we get scalability. Is this new stuff, is this branch related to cryptography at all? Because we're used to exploring on bankless, of course, Azik snarks and this whole new branch, this relatively new decade old branch of cryptography that's completely revolutionized everything about blockchains and how we scale them, what we do on them. But this you're talking polynomials. I mean, I learned some polynomial stuff in algebra, right, in high school.
01:06:15.858 - 01:06:27.400, Speaker A: Is this a new branch of math, or is this stuff that we've used all over the place and are just now applying to ethereum and blockchains just because we figured out a clever way to do it?
01:06:28.110 - 01:06:55.302, Speaker C: Yeah, I would say it's a combination of many things that we already knew inside cryptography. So like erasure coding and data reconstruction and polynomial commitments. And this is basically it's new in the way that Bitcoin was new. Right. Because Satoshi did not invent proof of work, it did not invent cryptographic signature. Like the novel thing was combining these things together and getting consensus. So it's kind of like that from a research perspective.
01:06:55.302 - 01:07:00.370, Speaker C: I'm not saying that data availability sampling is as novel as Bitcoin was in 2000.
01:07:00.440 - 01:07:16.406, Speaker A: Like this stuff, like data availability sampling and ratio coding, that sort of thing, I mean, that's existed for a while with just like, hard drives, right? Am I right? The old style of hard drives, when you put them in Raid arrays, that sort of thing, you use some of them. Yeah.
01:07:16.428 - 01:07:17.634, Speaker C: You're talking about redundancy.
01:07:17.682 - 01:07:18.982, Speaker A: Yeah, redundancy, exactly.
01:07:19.116 - 01:07:50.318, Speaker C: Yeah. So it's an old concept. I know Raid doesn't really do it with polynomial, but it's kind of the same idea where if you lose a portion of the data, you can reconstruct it. So, of course, this part is not new or novel, but applied to blockchains. We can combine all these elements together that we already knew into this data availability sampling to solve the problem of data availability in a scalable way. Because right now it's not scalable. To confirm that a block is available to the network, your node has to download the whole block.
01:07:50.318 - 01:08:14.330, Speaker C: And that's why it's kind of implicit where it's like, of course I need the block to verify the signatures and the transactions and everything. So that's like an implicit step. You need to download the block. But now we're really thinking about it. How do we solve the problem of checking if the block and the blobs are all available without having to download the whole blocks and all the blobs? And this is what full Dank Sharding will solve.
01:08:14.670 - 01:08:27.150, Speaker B: Dom, are you able to kind of put numbers on the scalability that proto Dank Sharding and then full Dank Sharding enhances? How much more scalability? Is that a valid question? It cannot be measured.
01:08:27.810 - 01:08:55.798, Speaker C: It's very speculative. I would say right now, people are throwing numbers around, like ten to 100 x with just 4844, and then with Dank Sharding we're talking, I know Vitalix throws like 100,000 TPS, but it's a bunch of speculative and weird metrics because a transaction can be anything, and the more complex one get more scalability, like I said earlier. So I don't really have any numbers, but with four and four, we're going to get those numbers and something I'm very excited for, right?
01:08:55.884 - 01:09:29.726, Speaker B: It's impossible to really measure these things like a transaction is a different thing depending on different contexts. But I think the point that I was trying to get out of you is that there's a number of zeros. It's a number of zeros of scalability increases depending on how you measure it. And there's this one little part about full dank Sharding that I think is actually just really emblematic about the balance that ethereum takes between hardline trustlessness and pragmatism. And that is the number that you said earlier. I don't know, maybe you just threw it out there, but it is spiritually aligned with this number. It's like 1,000,000,000,000th.
01:09:29.726 - 01:10:04.902, Speaker B: And in full Dank Sharding we do this data availability sampling mechanism where you sample a data and if you are somebody who's trying to produce a fraudulent block or hide data, there is at most after one sample, a 50% chance that is a fraudulent block. And then you take another sample of the data and you cut that in half. And then you cut it in half. So it goes down to 25%. Twelve and a half percent, 8.275%, whatever. And you do that like, I think, 30 times and you get to one 1,000,000,000th OD that this block is fraudulent.
01:10:04.902 - 01:11:01.710, Speaker B: And at some point we like, ethereum in dank sharding pick this number. It's like, okay, we'll do 30 samples and then that will give us a one in 1,000,000,000th chance that this block is fraudulent and we will accept that probability. We are not so crazy hardliners that we won't trade off multiple orders of magnitude of scale increases when we are giving up a 1,000,000,000th probability that this single block in the blockchain has fraudulent data. That is a trade off that Ethereum is making multiple orders of magnitude of scalability increase for a one 1,000,000,000th probability that somebody hid data inside of a block. I think that's just like such an elegant way of articulating ethereum's value and the way that cryptography allows us to do cool things by compressing the bad and magnifying the good. I just think it's so elegant.
01:11:02.530 - 01:11:54.526, Speaker C: It ties in with the concept of weak subjectivity, right? Because with Blob, aspiry we're kind of losing that aspect that a caveman can just go hibernate in a cave for a thousand years and then come back out and then verify the whole blockchain from Genesis. That's something that we already kind of sacrificed with proof of stake with the weak subjectivity. But it's such a small trade off compared to the scalability it enables in everything. And yeah, compared to what you said, I would say we're probably going to do more than 30 samples with full dank sharding. So it's going to be even lower probability that you believe an invalid block is valid and available. But also this is the probability that your node personally gets fooled. There's no way you can fool the entire network, right? With all these probabilities, even a supermajority can't convince the network that unavailable Blobs are available and then do nasty stuff.
01:11:54.548 - 01:12:29.146, Speaker A: At layer know, David gave a shout out to computers earlier. I just want to shout out math right now because yeah, this is great. The statistics, the math behind this is absolutely fantastic. I do want to just ask a general question here, dom and order of magnitude is fine, right? But this will lead us into the next section. So we talked about the history, we talked about the technical, and I think we have a sufficient definition of what Blob space is. At least I feel like I know Blob space now I know what it is. Now we want to talk about the economics of Blob space and this decoupling of the market.
01:12:29.146 - 01:13:44.606, Speaker A: But before we do, I do want to get some rough approximation of what David was saying based on kind of like the scaling factor, right? So if I go to l two beat scalingactivity, there is this chart here that shows me the current scaling factor of L2s, and it gives an approximation, and it says L2s are operating at their activity is about five x main net right now, and that's useful for me. And this is five x mainnet. And right now, this world, they're consuming a very small amount of block space, aren't they, per day of ethereum, right? And they could be consuming a lot more block space. And if we had the activities to support it, L2s could be much higher than a scaling factor of five. I don't know if that's like ten or 50 or 100, I'm not really sure. But then when they have this new Blob space resource available to them, then they have this whole other scaling factor. And as you and David were saying, it totally depends on what they do inside of that Blob space, right? So they're going to take the Blob space, they're going to resell it as block space, and the block space that they resell could be very complicated transactions, or it could be very simple transactions.
01:13:44.606 - 01:14:14.880, Speaker A: But rough order of magnitude here, right? If we have Blob space and it's almost like, let's say it's 80% used, this Blob space is now used post EIP 4844. How much of a scaling factor are we talking for this type of typical DFI type transactions? Are we going to be able to turn this like, five x into a 50? Are we talking about like a 500 x ethereum main net? And again, just rough order like approximations here.
01:14:15.730 - 01:15:31.350, Speaker C: Well, the amount of scalability really depends on the users. So roll ups could update to 4844 and then have the same amount of users and the same five point something x on l two beat would stay the same. The difference would be that it would be much cheaper for each L2 users to the point where it's practically free, right? So like you said, if the 80% of the Blob space is used, then the price of Blobs goes to zero, right, because it's anything below 100%, right? It's like EIP 1559. If we're above 100% of gas target usage, then the price goes up and down to manage congestion. So the prediction would be that at first, blobs are going to be mostly empty and only used scarcely by roll ups because they don't have the actual user base to fill them up. So we're going to see roll ups operate practically for free with just a tiny amount of execution gas at layer one and some other expenses for roll ups, whatever expenses they have. But then that L2 transaction can be subsidized and then you can have a world where L2 is basically free for a while until there's enough users to fill that Blob space and congest it and make it go up in price.
01:15:31.350 - 01:15:42.066, Speaker C: So that's kind of the economics of 4844 in a speculative prediction of mine. And that's one of the thing I'm really excited to see, is raw data and how it's actually used.
01:15:42.188 - 01:16:26.006, Speaker A: So, Dom, let's spend more time kind of defining the economics of this. And this is a quote from your article, and we've alluded to this earlier in the episode, but you said this. Another fun aspect of EIP 4844 is the introduction of the two dimensional fee market, meaning execution and Blobs will be priced separately according to the individual demand for each. The price of execution is simply the gas fees we know today with EIP 1559 and all that good stuff. So can you explain what you mean here? So this is a two dimensional fee market. There's like two types of gas. Is that one way to think about this? And then you mentioned the hallow EIP of 15 five nine.
01:16:26.006 - 01:16:31.020, Speaker A: So how is that related? What does that imply about the gas market for Blob space?
01:16:31.630 - 01:17:02.530, Speaker C: So it's a two dimensional market in that these two resources are going to be priced separately. So one concrete aspect is that the famous NFT drop at layer one example, if it happens, then the roll ups are shielded from that because the price of Blobs that they commit on chain is going to stay the same relative to the demand of other roll ups, whereas layer one, execution gas can be very expensive. So that's a cool aspect, that L2 users are shielded from whatever degen activity happening at layer one.
01:17:02.600 - 01:17:08.562, Speaker B: Right? We'll use the word decoupling now coupled markets. Now we are decoupling the markets.
01:17:08.626 - 01:18:00.018, Speaker C: Exactly. So the high level overview of 1559 that I'm sure listeners know very well is that there's a target of gas used per block and if it goes above that target, the price of gas goes up for the next block and then it goes down. If there's below the target, the goal is to manage congestion, right? If there's too much demand for gas, the price goes up and it makes a more efficient auction than we had before. And then that base fee gets burned because you don't want validators to be able to manipulate it to their own benefit, to have high gas fees and reap the rewards. And we get the same kind of market for Blobs, but it's a completely different market. So right now the specs say that we target three Blobs per block, but we can handle six. And if it's six, then the price is going to go up for the next block.
01:18:00.018 - 01:18:23.930, Speaker C: And it's that same idea. Like the price goes up exponentially if the blocks are always filled with six Blobs. And by the way, these numbers might change because it's still a discussion in progress. And that's the Blob market. We're going to have a separate EIP, one five, nine, and with another base fee for each Blob gas being used, which also gets burned for the same reasons.
01:18:24.090 - 01:18:37.666, Speaker B: Okay, so both gas fees to pay for block space, aka the status quo that's burned. We know this Blob space is also burned in the same mechanism. It's basically one to one parity with block space, correct?
01:18:37.848 - 01:18:43.234, Speaker C: Yes. Just the price function is a bit different for Blob space. But that's technical stuff.
01:18:43.352 - 01:18:53.282, Speaker B: Yeah, you gave the number three is the target. So we target three Blobs. Can Blobs, blobs are all uniform size, right? One Blob is always the same size as another Blob.
01:18:53.426 - 01:19:08.598, Speaker C: Yes, but if a roll up has more data, they can post two Blobs in the same transaction. If they want the data requirements for roll up, they can use as many Blobs as they want, they just have to pay for it like the rest of other roll ups.
01:19:08.694 - 01:19:24.634, Speaker B: Okay, so in post 4844 in Ethereum block space, a block will have space for three Blobs. So when we talk about Blob space, we're actually just talking about three slots for Blobs, but with tolerance to flex up and down.
01:19:24.772 - 01:19:35.134, Speaker C: Yeah. So that's where the 50 gigabyte number comes from. If there were six Blobs every single block, then that number would be 100GB. But that's completely unsustainable.
01:19:35.182 - 01:19:35.394, Speaker B: Right.
01:19:35.432 - 01:20:05.998, Speaker C: If you have six Blobs for every block for 18 days, then the price of Blobs just goes up exponentially. And at some point there's just not enough ETH in the world to pay for those Blobs with that high base fee. So the target is going to be reached at, on average, three Blobs per block. But that's to manage congestion. But if there is no congestion, then you have like one Blob or zero Blob in a block, then that price goes down to basically zero. So that's where at first when there's no congestion, blobs will be practically free.
01:20:06.164 - 01:20:54.862, Speaker B: So one perspective about this is, oh, we have a brand new resource called Blob space and it also burns ETH. There's another mechanism, a brand new mechanism to burn ETH. Yay, we're going to burn more ETH. But actually I don't think that's true because the existence of Blob space is likely going to pull away demand for block space because why are we doing this in the first place? We're trying to make roll ups cheaper. And so there will be a tension, a balance between block space and Blob space. They won't be formally coupled in the protocol, technically, because we are decoupling this block space. But they will be coupled because if it is so much cheaper to do whatever you're doing on layer one on a L2, then transaction demand is going to flow out of the layer one, towards the L2, and then rebalance the demand between blob space and block space.
01:20:54.862 - 01:21:20.002, Speaker B: So these are like informally coupled. If it is so much cheaper to buy Blob space than it is to buy block space, then these things will probably balance out a little bit. But in aggregate, I think total burn will come down because we are encouraging incentivizing L2 activity which is fundamentally cheaper than layer one activity. This is my intuition. Is this right, Dom?
01:21:20.146 - 01:22:11.240, Speaker C: Yeah. In aggregate it's going to go down at first, and then as L2 hopefully gains more adoption and more users, then it's going to go back up eventually with way more users doing like half a penny or thousands of a penny per transaction on L2. In aggregate, if that's enough to fill the Blobs. And then the roll ups have enough fees from these many thousands of a penny transactions, they can afford to pay for high gas prices for Blobs once that's congested. But right now I don't believe Blob space is going to be congested at first with that level of activity, which is why that's very alleviating. Just three Blobs per block is going to be very alleviating for roll ups needs. But we're going to lose that 200 or 300 ETH a day that Ryan said earlier that's probably going to go way down.
01:22:11.240 - 01:22:23.898, Speaker C: We're going to burn slightly fewer ETH at first, but it's not really a problem because if we're getting much more adoption from that and scalability, then that's a win.
01:22:24.064 - 01:22:54.260, Speaker B: Right? The line here was like we were just going to make it up in volume so where things are going to drop to zero because we're reducing fees to L2s, but then L2 fees also drop to zero. And then all of a sudden we're like, we can take off the brakes of L2 economic demand and pick it up in volume. And so this is Ethereum opening up its L2s to the long tail of economic use cases that can approach one pennies per transaction because we've enabled block space, Blob space.
01:22:54.950 - 01:23:29.102, Speaker C: And like I said earlier, roll ups can have their own incentives and pay for these fractions of a penny for each users. And then you have a L2 experience where it's completely free and you can just do whatever you want. And more complex DApps can now be viable because there's basically no fees and that sort of activity at first that's going to be incentivized by these effectively zero transactions. That's going to drive more users onto L2 and that's going to fill up the Blobs eventually and then at some point it's going to start burning more.
01:23:29.156 - 01:24:37.410, Speaker A: So this is the part that I think is like, at least for me, all of this has been a fascinating conversation, very interesting, but one of the least explored areas, right? So I think some people listening will just maybe early in the podcast or up to this point be like WTF, there's going to be a whole new resource market. How does that impact demand? There were these ideas earlier, and some people still think this, that L2s will compete against Ethereum and kind of the layer one, right? This is a whole new variable. I guess I'm saying that I'm not sure many have looked at when it comes to how do you try to predict the future price of ETH and the future demand for block space. What's effectively happening here though is we get a new furnace, right? So we've had the furnace one of just block space demand and that burns some ETH. Now we have the second furnace firing up. In the short run this will be all of the L2 block space consumption will drop down towards zero. In the longer run we may end up burning more ETH and we probably will, but that will take some time.
01:24:37.410 - 01:26:09.874, Speaker A: I don't know if that'll take months or that'll take years. In fact, it's probably impossible for us all to predict here. But one of the side effects I think is it depends kind of how you model out the value of ETH and where that sort of comes from. Right? Because there's certainly some value of ETH accrual that is related and correlated to kind of burn, how much block space is consumed, how much Blob space is consumed, how much ETH is burnt, that sort of level. But then you also have to think about in the L2 world, all of these new DFI applications or in general applications that open up based on cheap block space, how much ETH will they actually consume as what we've termed before economic bandwidth or like ETH based collateral? I mean, you look at an app like Friend Tech on base and it's purely denominated in Ether, right? And so that has been a net accretion point for Ether value. And so the question I think becomes with these new L2s opening up, how much more do they start to use Ethereum's monetary properties and ETH as money, right? And ETH as collateral and ETH as a store of value and all of these things and does that compensate for the short term reduction in burn? This is a very complicated kind of economic model, but those are the puts and takes of this. One question I have for you though, Dom, is you mentioned this.
01:26:09.874 - 01:26:50.014, Speaker A: Use case of L2s will no longer have to compete against the Dgen NFT drop on main net. What would a world look like where other L2s, though, are competing against other L2s for Blob space? So that would have some contention and resources. So can you imagine a world where we are not 80% of Blob space consumed, but we're doing the full three Blobs per block thing? We're getting into four and we're getting into five and we're getting into six. Does this mean like an Arbitrum? Layer two competes against an optimism L2, I'm not even sure how to imagine this world. What does that look like?
01:26:50.212 - 01:27:30.054, Speaker C: That's pretty much exactly what you said. They're going to compete each other. And you can think of roll ups taking small pennies from a lot of users for transaction fees on L2. That's enough to pay for layer one Blobs and whatever is left over is the roll ups profit. Right. So if the Blobs just go up, then L2 becomes like, I don't know the word, they're going to lose money. If the Blobs are too expensive for them, then the most efficient roll up that can have more activity, can batch, more transaction, can compress better, they're going to make more use of that more expensive Blob.
01:27:30.054 - 01:27:51.266, Speaker C: So that's sort of a world where we're heading to where if block space becomes congested, then the best roll up that uses this more efficiently is going to be able to provide cheaper transactions on their L2, which are going to drive them in, like you say, arbitram versus optimism. They're going to fight amongst each other for who can use that Blob space better.
01:27:51.368 - 01:28:14.474, Speaker A: So all of these roll ups are in this race to convert this raw material of Blob space from ethereum into the most valuable product it can in the form of L2 block space. And some roll up ecosystems will be more successful than others at doing that, whatever that means. Whether that's kind of the network effect of the roll up or it means.
01:28:14.512 - 01:28:42.674, Speaker B: The same thing as the ethereum layer one, we call the ethereum layer one block space as the most valuable block space in the world because people do NFT drops on there. People network effects do DeFi stuff on there. There's a lot of liquidity more for ethereum layer one block space. And L2 block space will be judged on its same merits, like how dense is L2 block space will be a function of how much demand there is, which is going to be a function of how much aggregate economic activity there is on every respective L2.
01:28:42.872 - 01:29:30.274, Speaker C: Yeah. And it's also on a spectrum where, like a roll up that aims to be ultra secure. They're going to want to commit to a Blob every single block too, because from the perspective of a L2 roll up, once a transaction is batched onto a layer one Blob, it becomes finalized. From the roll ups perspective, the security is now offloaded to Ethereum. So if you're a roll up whose community really values high security, then it might be worth it for them to post a Blob every single block, even if it's not completely full or stuff like that. So that would make the roll up more expensive. But a roll up that doesn't care too much about security, like a roll up that has like game assets or something where it's not too valuable and they can afford to just wait out a few blocks and have the price of Blobs go down.
01:29:30.274 - 01:29:41.640, Speaker C: And now they commit a lot of data. Compressed roll ups don't have to post a Blob every single block. So that's another aspect of individual trade offs that roll ups will make.
01:29:42.090 - 01:30:21.650, Speaker B: This is what I was going to something I wanted to unpack is there's this new variable in Blob space resources, which is exactly what you said. How frequently does a L2 choose to commit its date route to the layer one? And with protoding charting, you said that there is space for three roll ups per block to submit their Blobs for three Blobs. Excuse me, three Blobs per block on the layer one. So like three total roll ups can submit a total of three Blobs per block. That's a low number. That is a very low number. I mean, that goes up with full dank sharding, but only three Blobs per block every 12 seconds.
01:30:21.650 - 01:31:15.822, Speaker B: And so this is also going to be a vector a variable that L2s tinker with, because if you submit your Blob every single block to the ethereum layer one, you are maximally secure. But that might not be the optimum point, the optimum balance of security versus expensiveness or efficiency that your users on your particular L2 demand. Maybe like the typical optimistic roll up, which is already operating on fraud proofs, maybe they only need one Blob every, I don't know, ten minutes or something. And all of a sudden going from 12 seconds to ten minutes is massively more efficient. And so this isn't really anything that's part of the core ethereum protocol, but that is a variable to consider when we talk about L2 economics, which is how often do they commit their Blob to ethereum layer one block space Blob?
01:31:15.886 - 01:32:10.022, Speaker C: Yeah, I'll just add something for the listeners. If you're a L2 users and the L2 post every ten minutes, you don't have to wait that ten minutes. Your transaction is instantly confirmed by sequencer and then you can instantly do other stuff. But every ten minute would be the full commitment and batching on chain, where your transaction on L2 now becomes completely settled and finalized and the sequencer can't change it. So that's sort of the trade off like we have today or with Bitcoin, where you wait more blocks if you want more security, if you're on a roll up and you want ultimate security, then you're going to wait until the transaction is committed and batched on chain before considering it finalized. But if you don't really care, you're just doing small transactions and the sequencer is going to be the one in charge of the security in transit until it gets onto main layer one.
01:32:10.156 - 01:33:13.878, Speaker A: One word you used earlier in this conversation, Dom, I want to return to is subsidy. That's kind of an interesting word because we like analogies on bankless and this sort of feels like almost like a government subsidy of a particular sector that the government wants to grow, right? Like say solar. There is going to be a government subsidy in the form of tax credit or tax reduction for a particular industry or set of resources because why? Because let's say some government wants to subsidize green energy in the form of solar. You can agree or disagree with that policy. Let's not get caught up in the weeds here. But this is effectively the Ethereum protocol subsidizing in a way roll up the roll up roadmap. Would you use these terms? Maybe the net effect is going to be the propagation of roll ups because the resource is now much cheaper.
01:33:13.878 - 01:33:16.410, Speaker A: How do you think that analogy holds?
01:33:16.910 - 01:33:44.062, Speaker C: I would just argue against what a tiny thing you said about it's not the layer one subsidizing. The layer one just says here's the Blobs, do whatever you want with it. And if they're so cheap then it's up to the roll ups to decide if they want to subsidize and run at a loss just to attract more users. It's like we've seen this war with DApps, like subsidizing liquidity or giving token incentives. Roll up sequencers can do the same thing. And see here's few transactions. Here's the token incentives.
01:33:44.062 - 01:34:11.414, Speaker C: Come use our roll ups, come increase the network effects just so that in the long run they have these users and compete against each other for network effects. But this is in the end it's positive for Ethereum because it's all getting settled on Ethereum and it's really up to the roll ups to decide if running at a loss for a little couple months is worth it. But while the Blobs are very cheap and not congested, then that's probably what a lot of them are going to try to do.
01:34:11.472 - 01:34:42.920, Speaker A: But the broader point here, Dom, is that the Ethereum protocol in general and Ethereum researchers are choosing to implement this change in order to make roll ups far cheaper. And this is towards this goal of what Vitalik said is a blockchain transaction should be fractions of a penny, not even like pennies. Right. It's all towards that goal. But it is an intentional decision that the protocol is making. Right?
01:34:43.290 - 01:34:58.380, Speaker C: Yeah, it's the roll up centric roadmap. It basically puts roll ups as first class citizens on the layer one chain. But it's not really a subsidy as such. It's just decoupled market to have more efficient usage of resource that can then be scaled up.
01:34:58.830 - 01:35:47.190, Speaker B: I think you might be actually able to take the alternative perspective on this, guys, which is like, Ryan, the way that you articulated that is that the protocol designers of Ethereum want a specific outcome and I think that is actually totally true and exactly what happened. That's why we call it the roll of centric roadmap to Ethereum. Yet also what we are doing is merely decoupling data from execution into two different spaces in an eight block. And then what happens as a result of that decoupling is that it just so happens that roll ups are cheaper, but simply doing the act of decoupling is all that 4844 is doing. We are just separating data from execution. It just so happens that makes roll ups cheaper. And so I think there is an argument that is actually decently credibly neutral.
01:35:47.190 - 01:36:12.830, Speaker B: Like we are not picking winners and losers here. We are not saying that roll ups are the winners, even though, again, it is kind of like why we're doing this. We are just merely decoupling data from execution and letting the free market build on top of this permissionless protocol. And it just so happens that decoupling data from execution is more efficient and allows for more total net economic activity to be produced through crypto economics. Do you like that approach, Dom?
01:36:12.990 - 01:36:52.250, Speaker C: Yeah, it's fine. It's a good analogy. We don't really pick and choose winners and losers. It's really here's the resources available by layer one. Go make use of them as efficiently as you want and that's what the market is going to converge onto, right. The roll up that has the most efficient usage of Blob space will offer the cheapest fees and thus attracts the more users. And on the long run, I know something Justin Drake likes to ponder about is enshrining a roll up onto layer one, at which point we're using the full Dank, Sharding and 4844 infrastructure to scale up layer one execution.
01:36:52.250 - 01:37:02.260, Speaker C: But that's kind of in the far future where we enshrined a Zkbm and now we can have much higher gas limits and have a roll up at layer one. That is going to scale up.
01:37:04.070 - 01:37:11.960, Speaker A: That's 2019, isn't it? Are we full circle to 2019 and we have, like Sharded execution, we're finally going to be able to do that.
01:37:12.330 - 01:37:30.618, Speaker C: With an entrant roll up? Yes, that would be effectively be scaling up execution, but in a much better way than we initially planned. So we're going full circle with roll ups and then getting that into layer one in the end. But that's kind of in the far future in crypto time, we're just going.
01:37:30.624 - 01:37:45.858, Speaker B: To have roll ups, doing the full circle and returning to execution. Charting in the distant future of Ethereum's roadmap is like one of the weirdest, most fascinating things about this universe's timeline. Yeah. Books will be written about I agree.
01:37:45.944 - 01:37:50.020, Speaker A: This would make 2019 me so happy to hear that this is actually happening.
01:37:51.270 - 01:37:53.106, Speaker B: We found the more natural route to.
01:37:53.128 - 01:38:23.014, Speaker C: Execution charting, and it's a much better outcome. And it's a much better way to get there, too, because at layer one, we don't have to do as much upgrades and as much research, because we can just let the market innovate and find the best designs, the most secure zkvm and stuff like that. And then we just enshrine it. We just yoink it from L2 that developed it, innovated, competed for it, and then assuming it's open source, of course, and then we enshrine it into layer one and reap the benefits. And then we get back the coveted execution scaling.
01:38:23.142 - 01:38:27.738, Speaker A: You slide dogs, you. I don't know how you made this happen, but it's pretty clever.
01:38:27.914 - 01:38:50.742, Speaker B: This is just the beauty of the harmony of Ethereum that I think really exemplifies why and so many others are just attracted to the protocol. It's just like a nice, beautiful harmony between the free market and the protocol, right? Like top down rules mixed with bottom up markets, and then that is ethereum. And we got there in a roundabout way. Dom, this has been thank you for guiding us. Sorry.
01:38:50.876 - 01:38:59.240, Speaker C: So it's the philosophy of addition by subtraction, like whatever the community can do, we just delegate and there's no top down approach. Really beautiful.
01:38:59.930 - 01:39:32.580, Speaker B: So just to really drive home the roadmap on this, we have 4844 optimistically you may be coming November, pessimistically some coming in sometime 2024, full dang sharding sometime after that. And then you just kind of left us with what will definitely be future content, which is enshrining a roll up into layer one. What does come next? Say we're post full dank sharding. Is that it when it comes to data availability or what are the next steps after this?
01:39:33.030 - 01:39:48.402, Speaker C: It's not really a sequential roadmap, as you know, so it really depends where we'll be with the other items like enshrined PBS and stuff, by the time we have Full Dank Sharding. So it'll be definitely one more step towards the end game as described by Vitalik.
01:39:48.466 - 01:40:26.514, Speaker A: So let's just talk through that Blob space and how this kind of pans out with respect to the roll up. So we'll have EIP 4844, right? And at that point in time, Blob space and L2, data availability and transaction costs will drop towards zero. Okay? And then over the months, we know apps will take off. If you have a really cheap resource like a Blob space, guaranteed, it gets used. Okay? If we have cheap block space, it gets used on ethereum. That is the rule here. And so by the time that starts to ramp up, maybe we are consuming those three, four, five Blobs per block, we start to get congestion.
01:40:26.514 - 01:41:03.470, Speaker A: And so rather than the 2022 people complaining about expensive block space, everyone's complaining about this Blob space is too damn high dom. And then we have another ace up the sleeve, which is full dank sharding. And again, we don't know the exact timeline here, but once the Blob space fills up, full Dank sharding gives us even more Blob space availability. Right? And if things work out from a timing perspective, maybe we meet the market need at the right time when things are just starting to get congested again. Is that roughly the idea here?
01:41:03.620 - 01:41:30.262, Speaker C: Yeah, that sounds about right. Will be a crazy increase in Blob space with Full Dank charting, but we definitely have some things to figure out before we get there with regards to networking and proper sampling techniques and everything. So this is why 4844 is the perfect stepping stone for Rollouts, because they don't have to wait as long, they can just use the Blob space today and then not even have to upgrade. Once we have full name sharding, it'll just be done behind the scenes for them.
01:41:30.396 - 01:41:43.370, Speaker A: Wow. Well, this has been an epic episode in a tour de force. I think Blob Space 101 of everything we needed to know. And I was trying to keep track, David, of how many times we said the word Blob on this podcast.
01:41:43.870 - 01:41:45.994, Speaker B: I don't know why, but you definitely failed.
01:41:46.042 - 01:41:56.738, Speaker A: Well, you know what, I think some bankless listener will be able to tell us how many times the word Blob was said on this podcast. And if you DM us, maybe there'll be something special at the end of that.
01:41:56.824 - 01:41:59.614, Speaker B: Any number. I'll be like, wow, you can't believe you're actually counted.
01:41:59.742 - 01:42:01.566, Speaker C: Just control f the transcript.
01:42:01.678 - 01:42:04.658, Speaker A: Dom, thank you so much for walking us through this today.
01:42:04.824 - 01:42:05.762, Speaker C: Thanks for having me.
01:42:05.816 - 01:42:21.778, Speaker A: Fantastic. Thank you. There is an article as well, if you like written material that Domathy wrote about Blobspace. We'll include a link to that in the show notes. Gotta let you know, as always, crypto is risky. You could lose what you put in. But we are headed west towards Blobspace, the frontier.
01:42:21.778 - 01:42:45.690, Speaker A: It's not for everyone, but we're glad you're with us on the bankless journey. Thanks a lot, Sam.
