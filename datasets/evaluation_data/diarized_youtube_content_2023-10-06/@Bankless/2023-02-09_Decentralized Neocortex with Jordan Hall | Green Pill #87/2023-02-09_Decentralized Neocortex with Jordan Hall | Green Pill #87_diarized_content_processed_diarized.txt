00:00:00.250 - 00:00:05.326, Speaker A: You. What up?
00:00:05.348 - 00:00:33.142, Speaker B: Coordination on the pod. Today we have Jordan Hall, who is a serial entrepreneur, was at M P Three.com and founded Divix. And on this episode, we are it's actually the second episode that I've done with him. We did an episode about a year ago entitled Game B, which is all about creating an omni Win civilization. And, um man, a lot has happened since my first episode with Jordan Chat. GPT-3 has happened, 3 hours capital has happened, SBF has happened.
00:00:33.142 - 00:01:39.530, Speaker B: And we've also seen how Dows can evolve a little bit more. So in this episode, we talk about how do we create decentralized collective intelligence, how do we create a decentralized neocortex? Neocortex, of course, being part of the brain that's responsible for executive function and for coordinating resources, how do we solve coordination failures but without creating authoritarianism, which is one of the themes that we explore with Daniel Schmachtenberger, is this idea of a metacrisis and a third attractor where we solve coordination failures with decentralized governance. So just really appreciate Jordan's perspective as a serial entrepreneur, someone who has decentralized power in organizations that he's been involved in, but also as a philosopher and a priori thinker about how to create decentralized social movements that are intelligent, collectively intelligent. So I really just think that Jordan is this lucid and articulate thinker, and I enjoy the way he breaks down problems. And this is one of my favorite episodes that we've done over the last few months because it deals with decentralized collective intelligence. I think that's one of the big promises of the Web Three space. So coordination, I think that you're really going to enjoy this episode with Jordan Hall.
00:01:39.530 - 00:01:40.634, Speaker B: Enjoy.
00:01:40.752 - 00:02:19.394, Speaker C: Working in web3 is awesome. But working outside of the typical W Two employee structure is a deal breaker for so many. Opolis is helping the self sovereign worker focus on what they do best their work. Tax time is coming up. Opolis helps professionalize your business by helping you form an entity, generate proof of employment through Pay subs, and receive a W Two at the end of the tax year. Are you self employed and forced to spend money on expensive healthcare insurance with limited coverage? Opolis leverages group buying power through a community employment coop, helping you save 20% to 50% on high quality, affordable healthcare options through Cigna. And finally, Opelis's member owners share in Opelis's success and profits based on their work token holdings.
00:02:19.394 - 00:02:55.902, Speaker C: You must be authorized to work inside the United States in select Canadian provinces to receive Opelis's benefits. Book a 30 minutes free consultation with Opelis's experts and join Opelis by March 31, 2023, to get 1000 work and a thousand bank tokens. Go to Connect Opolis Co Bankless to get started. And if you're going to East Denver, make sure to stop by the Opelis booth or attend their Future of Work Summit hosted by Opolis. The Glow Dollar is a new stablecoin with a very special property. As the market cap of Glow goes up, extreme poverty goes down. Glow is a dollar backed nonprofit stablecoin that creates basic income for people living in extreme poverty.
00:02:55.902 - 00:03:32.042, Speaker C: Glow is basically the same business model as USDC, with yield generating treasuries on one side and a stablecoin on ethereum on the other. But instead of being a for profit company, glow is a nonprofit that donates 100% of all yields from the Glow Reserve to Give Directly's basic Income Program. Give Directly is a charity that gives people money, no strings attached, to people living in poverty, and is a charity that Vitalik has previously donated to and supported in the past. With Glow, you can reduce poverty just by holding a stablecoin. Glow is launching in early 2023, and you can join the waitlist@glowdollar.org greenpill. That's glodollar.org
00:03:32.042 - 00:03:33.150, Speaker C: greenpill.
00:03:34.850 - 00:03:36.740, Speaker B: All right. Hey, Jordan. What's up?
00:03:37.910 - 00:04:19.486, Speaker A: Well, we're talking about this notion of the new form of governance, or what I've begun to start calling the neo neocortex, which, by the way, is one of the reasons why we're talking about it, is that it's at least substantially distributed. So unlike the neocortex that happens to live inside our individual skull, the neonocortex is what happens when distributed cognition kind of finally gets its shit together. Maybe a fine point on it. And you and I had a brief back and forth on Twitter in the context of decentralized governance, which is a part of that story, and I think it'd be great to just start hammering on that question.
00:04:19.668 - 00:05:15.646, Speaker B: Yeah, sounds good. Let's go down the deep end. So I think that the Tweet thread that we engaged on and the reason why we decided to do another podcast together was that I think I was talking about decentralization of power, meaning decentralizing responsibility. And so basically, how do you teach people to be able to self govern? There's a lot of responsibility that comes with that power. So I think that that was the immediate trigger for the conversation. But one thing I'll drop before I hand the speaking mic back to you is know, in some of the work that I've been doing with talking to Daniel Schmachenberger and others about the metacrisis, there's this idea of coordination failure being one attractor for human activity and systems of human activity. You basically have uncoordinated action that leads to things like climate change and externalization of harm, biodiversity collapse.
00:05:15.646 - 00:06:14.740, Speaker B: And then the second attractor is basically regulating that coordination failure so that there is no more externalization of harm. But that attractor can lead to authoritarian sort of government. And once you give people the power to regulate you, how do you know they're not going to use it in a way that abuses that power? And then so looking for a third basin of attractor for human activity, which is decentralized governance, governance that allows you to solve coordination failure but not get the authoritarian outcomes that would come from handing over complete power to someone over you. And so my aperture for this conversation is not only practical trying to figure out how to decentralize responsibility at Gitcoin, but it's also trying to solve this game theoretic problem of how do you solve coordination failure but without creating authoritarianism, how do you create a decentralized way of coordinating humans? So that's my volleyback to you. What does that bring up for you?
00:06:15.430 - 00:06:28.226, Speaker A: Well, the first thing that I like to put out there is I really like it when I'm interacting with people who have both sort of the tactic and the strategic or the practice. Big picture simultaneously empirical and a priority.
00:06:28.258 - 00:06:29.666, Speaker B: Knowledge are really complementary.
00:06:29.778 - 00:07:23.018, Speaker A: You're dealing with the problem really in gitcoin, which is, say it grounds in. Does it actually work for something that you're putting a lot of skin in the game in and you're contemplating it from the point of view? Of the biggest picture concerns and that if you don't have both, you're probably going to get into some kind of trouble, because we're in the thick of it. Right. So things that kind of work on the ground practically but if they're not grounded in something that will continue to work as the system continues to evolve and the stakes continue to go up, we might actually have sort of bought the ticket on the wrong train. Contrary wise, of course, if all we're doing is sort of pie in the sky and we're not actually grounding it in what actually works in reality, then we're definitely going to kind of get it when it comes to the end of the game. So another triple I'd put to you and I think they may actually fit in an interesting geometry is the false dichotomy of market and state. So in some sense somebody might say, oh yeah, decentralized governance, we already had that.
00:07:23.018 - 00:08:20.670, Speaker A: It's called market and in some sense they're actually correct. So you have coordination failure up here, then you have pure centralization or totality down here. I'll actually call it totalitarianism even more than authoritarianism. And we're looking for something which is and I won't say actually the middle, but something that's not that like a third axis, but the same thing's going on with the false dichotomy of market and state, where I would say that both Market and State are the same in the same sense, that sort of Coke and PEPC are both a form of sugar water and they're in a de facto conspiracy against water itself. That market is something that has a role to play in supporting distributed or decentralized coordination but it has a whole series of failure conditions and bureaucracy or hierarchical formal constructs have a role to play. It has a whole series of failure conditions and we've been led to believe that the solution is in the alternative. I e.
00:08:20.670 - 00:09:28.770, Speaker A: If you really believe in markets, they're the right answer and hierarchies are the trouble. If you really believe in hierarchies, then they're the right answer and markets believe to trouble and maybe you think they'd trade but I'm saying there's actually a third attractor that is the actual proper response and I think that these two ways of thinking about it actually fit together quite nicely. However or and one of the most challenging elements is something that I've been calling the through the looking glass problem and the reason why I call it that is that there's something like a weirdly potent inversion or reorientation is actually even better. Remember the scene in Ender's Game where they realize that they're actually in a non gravity environment so it's not clear what the direction of down could be and how difficult it is to actually coordinate when you don't have an orienting basis in a shared direction. And of course Ender is like the enemy's gate is down done. We now are reorienting in a new basis. And it's something like that, like an inversion of the past 500 years or so have put us in a deeply psychological and cultural inertia of a particular form of how we go about even thinking about what coordination means.
00:09:28.770 - 00:10:27.926, Speaker A: And that one of the things that's happening right now is we're going through a looking glass. We're actually coming into something which is in many ways the inverse of that. I'll give examples of that in a moment. Actually, I'll give you an example of that right now. One of the things that I would propose for those who are thinking about things like Taoist, like quite practically or for that matter, any group is a very interesting distinction between the concept of politics and the actual root of that which is polyte and they're almost perfectly inverse to each other. So polyte has the sense that there is some being, some wholeness like let's go with like an ocean or lake or river or a forest that has its own, let's say, essence or its own spirit or its own wants and most particularly values. And the human beings that are called to give agency to that being are in service to that.
00:10:27.926 - 00:11:40.166, Speaker A: And your job is not to make choices yourself but rather to discern what that would want if you could perceive clearly and this is by the way palitea athens. Athens has a beingness and our jobs as citizens of Athens is not to engage in horse trading between each other to figure out who gets to win what piece of it but actually to collaboratively discern what is actually the essence of Athens expressing and how do we embody that. And then politics is the kind of crude inversion of that when that being goes away where we are actually no longer guided by a wholeness but rather are in fact reduced to horse trading between each other. These kind of limited, finite and relatively moloch captured beings that will fall into various kinds of multipolar traps. Multipolar is the exact inverse of wholeness. So one proposal I would make is to see if there's a way for us to collectively reawaken or return to what polyte really means and no longer endeavor to solve problems by means of politics which I would propose is in fact just sort of preordained to do in the moloch sense. I'm using terms that have some legibility in different subcommittees.
00:11:40.166 - 00:11:45.402, Speaker A: If these terms like moloch don't mean anything to you, just let know. I can expand it into a different frame.
00:11:45.546 - 00:11:54.250, Speaker B: Sure. Moloch being the sort of like visceral representation of coordination failure as the god of coordination failure is sort of the.
00:11:54.260 - 00:12:35.390, Speaker A: Shorthand being used here I think specifically multipolar traps. So coordination failure that is baked into the way that an system is actually architected. So try as you might, we all can kind of sit around the table and agree that X is the place we'd like to go. But because of the way that the communications and incentives landscape is constructed, we will always end up in a completely different location. Typically a really bad one. Sort of a great classic example is imagine that we were on an island that had a culture where everybody gave themselves a very intense, painful, electric shock every day. Obligate it was sort of culturally obligatory and anybody who didn't do that was immediately killed by everybody else.
00:12:35.390 - 00:13:43.394, Speaker A: And so we could all kind of like sit around going man, it should be a whole lot better if we didn't shock ourselves every day. We're all like, yeah, that'd be a whole lot better. But if you don't have the right kind of communications infrastructure the coordination failure leads everybody to default to the local optima of shocking yourself every day rather than getting torn apart, meaning killed. And the premise is that the notion the entire category of politics which is effectively how do we generate a synthetic group that has a strong enough capacity to win some kind of conflict? If you design it as a first pass the post winner takes all electoral system using each individual one man, one vote kind of mechanism that's a conflict that is a zero sum game and it has a certain kind of topology that defines what winning looks like. So such a system will always have two parties and they'll be maximally polarized in a finite time. That's that the idea is once you design the game in a certain way, the end state is just a matter of how long does it take for the system to get to its attractor yeah.
00:13:43.432 - 00:13:45.474, Speaker B: Becomes deterministic in a way.
00:13:45.592 - 00:14:31.806, Speaker A: Yeah, exactly. And so the premise is that this other thing, polyte at the big picture is a piece of what we're looking for. Let me throw some other pieces in. Another piece that's very important is to say, okay, if you're working like many of the people who I think might be listening to this in the subdomain of decentralized governance or dealing with governance challenges in the context of web, three crypto, even bitcoin, et cetera. Be mindful of the fact that you're dealing with something which is part of a larger story with this new form of governance, this neonocortex. And that's important. Okay, I may be solving a very particular problem that is right in front of me, but I'm actually dealing with a much bigger picture.
00:14:31.806 - 00:15:07.054, Speaker A: I'm dealing with governance writ large. And therefore, by the way, many of the problems that I'm dealing with are because I'm trying to force a hammer to also be a screwdriver. And maybe the better thing to do is to look outside of the toolbox that I think I've got say what are the kinds of tools are there in the sort of the total toolbox of human history that might be the proper tool. So, for example, a good king is a very effective form of governance. If you happen to actually be able to be lucky enough to get a good king, then you're probably going to have a very successful governance mechanism that's actually really good.
00:15:07.172 - 00:15:22.434, Speaker B: What's the meme there? It's a benevolent dictator. So you've got someone who's got total power, but they're totally benevolent in how they use it. And yeah, I think those two things are like countervailing metrics. Like you have total power, but you're totally benevolent in how you use.
00:15:22.632 - 00:16:03.566, Speaker A: Well, let's even define a little bit of what would that look like because there's a couple of boundary conditions. One is you actually have to have a coherent domain in the sense of the good king. You have to have a kingdom which is like an actual boundary. It has to be well, say it's a coherent domain. A nice example is to shift metaphors to the body. The heart the heart is a monarch, right? It has a single governance principle which happens to be the signaling mechanisms of the particular cells that enable coordinated no coordination failure at all, like trillion cells, all perfectly in sync and almost no failure. But it's bounded, right? It actually is very clearly a single thing.
00:16:03.566 - 00:16:26.034, Speaker A: It has a razor and detra. It has what does it mean to be a heart cell and not a lung cell? There's a lot of stuff that tells you what it is and what it's not. And so it's very clear. So you need to have a well defined domain. It actually has to be real, actual thing in the world that makes sense as an integrated whole. It has to actually be an integrated whole. So the first speaks to its potentiality, the second speaks to its actuality.
00:16:26.034 - 00:17:14.946, Speaker A: So if you accidentally have a cancer cell in the middle of your heart, you're going to have a lot of trouble if for some reason in the way that you developed embryonically, your heart has three valves instead of four valves, it's going to be challenging. I mean, I'd actually function. So it has to be not just in potential an actual hole, but it has to be an actual hole. So that's that. And then finally you have to have some kind of governance mechanism which is from a cybernetics perspective, has exactly the right control capacity to deal with the inputs and outputs of that domain. It can perceive all the signals that are relevant to the thing it's dealing with and it can issue control structures with the right kind of rate. Or say bandwidth is a good term with the right bandwidth to provide the feedback.
00:17:14.946 - 00:17:50.786, Speaker A: So the control operates and is intrinsically bound to the domain itself. So the heart's governance mechanism has no capacity, interest or desire to govern the lungs. That's the thing we're talking about. And notice that the body is a really interesting example of a quite nicely decentralized governance mechanism. It's not like you're a single homogeneous organism where everything is governed by the brain. The liver does a lot of stuff, a lot right and things that the brain has no awareness of at all. The lungs do their own thing and the heart does its own thing.
00:17:50.786 - 00:18:36.346, Speaker A: And got to help us the small intestine and the gut biome. This is massively decentralized governance and in fact the proper partitioning of functions to domains that are well bound, that are well integrated and have cybernetically proper governance mechanisms in the context of a larger protocol where each piece is actually held in proper dynamic relationship with all the other pieces. Okay, now we're starting to get somewhere. So those are like some more things to put into place. And I would mention, by the way, access to mechanism, different metaphors. Like I'm using the human body as a metaphor to describe governance so as to give perspectival insight to a different problem domain like a dow. You can do that like four to five different ways.
00:18:36.346 - 00:19:03.158, Speaker A: You can say an ecosystem, a forest, which is different than a body. Organisms aren't the same as organisms. All right, interesting. I can start thinking more clearly about this. The other one I would put we talked about this actually in our Twitter feed is actual lived experience. This isn't the kind of thing that can properly be dealt with by means of what I'm going to call call it ideology or theory. It's complex.
00:19:03.158 - 00:20:01.594, Speaker A: It's very complex. One of the most complex things in the universe is humans trying to work together in groups. And if you haven't actually done it, if you haven't actually lived it with real skin in the game, like real stake, and therefore, as a consequence have failed a lot meaningfully and felt that failure deeply, there's just going to be a whole bunch of stuff you don't grasp at all. And contrary wise, we probably should be, let's say, honoring or at least not throwing out the past when you say 100,000 years of acquired wisdom in how this sort of thing works. So there's a lot of jewels or a lot of diamonds that have been gathered by humanity many of which aren't proper, let's say the various forms of domination incentive constructs that were developed to govern the Bronze Age. Okay, we're going to probably throw away threat of torture as a primary mechanism of intimidating people into doing what you want. Fine.
00:20:01.594 - 00:20:10.910, Speaker A: Good. And also there's a lots of other stuff that was actually learned in that time frame that we want to conserve because it was hard earned and actually deployable in our current environment.
00:20:11.410 - 00:21:25.880, Speaker B: Yeah, I think one of the sort of memes that comes to mind for me there is the idea that any new system of governance should transcend and include the previous form of governance and that can mean different things in different contexts with Gitcoin going from being a company to a dow. Hopefully my context and awareness is transcended and included in the way Gitcoin governs itself. And now I'm a minority stakeholder and yeah, to your point, the heart doesn't govern the lungs and vice versa. By modularizing a system and giving each organ its own specific form of governance, then you sort of limit the scope of one entity being in charge of the whole on around it. And that's a way to sort of break up the way a distributed system can do governance. But I'm wondering if, from what you just said, if there's any other best practices that if we could come up with by the end of this episode, a list of six best practices for doing decentralized governance, for building that neo neocortex of how a system can govern itself. I'm wondering what else you would add to the list.
00:21:26.330 - 00:22:36.960, Speaker A: Yeah, well, let's dance with that. So the metaphor of neoneocortex is quite nice because you say the development of the human body, the cortex, does most of the stuff in terms of coordination of the various distinct organs of the body. The neocortex is actually a relatively thin control mechanism and that's important to think about. And if we're talking about a neoneocortex interesting. So if we have stuff like markets and we have stuff like hierarchical organizations, bureaucracies, of which all corporations are a variation on that theme, we may want to be thinking about this category of decentralized governance neonocortex in that same sense. Meaning what we're actually trying to do is properly modularize each of the particular well defined domains that are part of our ecology and then creating a protocol that allows them to operate together well. So the kinds of choices that are held at the highest level of the stack are many cases quite simple and much of the complexity is actually handled much lower in the stack and at modules that may in fact be corporations, for example.
00:22:36.960 - 00:23:36.338, Speaker A: Okay, now this is a little bit of a slice, but I think it needs to land quite nicely. One of the challenges that we're dealing with is that because we haven't had this kind of integrating protocol in the past, human activity has largely been almost exclusively been captured or concentrated in this say corporations or hierarchies. We've had a problem that our hierarchies are in fact not kingdoms. They're not well defined like Lockheed and Barton is a clip maximizer it intrinsically cannot help but want to expand to actually be everything. And I'm not pointing at them like every corporation google alphabet in other words unlike a heart that very much is just going to heart and not at all try to liver every corporation intrinsically must expand to fill the entire space. And the only reason it doesn't is because other corporations are pushing back against it. And a proper operating protocol would in fact prevent that from happening.
00:23:36.338 - 00:24:28.670, Speaker A: It would actually design organisms that are more like organs which is to say that they're well defined, well bounded and their governance doesn't have in its intrinsics a want need or desire to expand. So what that means is like the human shit. Let me actually add one more piece to this because it's critical part and parcel of the reason why the previous period of time has created so much trouble is that each individual human's diversity of interests and needs have been forced through a very narrow channel. Meaning you worked at a job and so all the different values that you have as a human had to be squeezed through the director of marketing at GM. And so this actually is way wrong. It's a mess. And what it does it creates an energy inside GM for it to become an empire, to expand and expand and expand.
00:24:28.670 - 00:25:11.982, Speaker A: But because in this concurrent environment that's not how it works or at least it will evolve more and more to not being the case. You will actually have many like dozens or hundreds of distinct ways that you can give and add value, many of which might be qualitatively distinct. So it's a much wider distributed field of ways of giving value. And therefore the fact that some particular sub node has kind of reached all you're doing is sort of providing maintenance and small trim tabing to make sure it's exactly right and not endeavoring to expand. It doesn't bother you because you're not limited by the fact that you've been wholly captured into that piece. You're not losing your ability to thrive by having this subpiece stay exactly what it is in fact quite the opposite. So keep that in mind.
00:25:11.982 - 00:25:35.510, Speaker A: That's another how would I say principle is every individual should be only contributing to a particular organization as a partial aspect of their whole self and to the degree to which their particular values are being highly if not perfectly realized in that contribution.
00:25:36.410 - 00:26:22.614, Speaker B: Let me just say back to what I think you said, which is the fiduciary duty that happens in for profit corporations causes them to be a paperclip maximizer for shareholder value. And therefore if you're, I don't know, the VP of marketing at that organization, the whole plurality of all of your attributes and your values gets compressed into how do I create fiduciary value for this corporation and a world that transcends and includes that with a neo neocortex? Maybe that person has a plurality of organizations in which they give larger apertures of themselves and there's a plurality of things that are being maximized instead of just like that one. Paperclip maximizer. Did I get that right?
00:26:22.732 - 00:26:52.986, Speaker A: Yeah. Let me set the other piece a little bit more robustly. So the first piece totally right. Like the fiduciary responsibility for shareholder maximization is paperclip maximizer. Done. The other side is something like if I'm working and all of my life energy as a productive person is being poured into a single tool, the only way for me to feel like I'm expanding my potency in the world is for that thing to expand and include more of the world in it. So it becomes an imperial mandate.
00:26:52.986 - 00:27:53.026, Speaker A: Even if I'm just the director of Marketing at GM, the only way for me to personally feel like I am growing is either to eat more of GM or for GM to eat more in the world, neither of which is ultimately healthy. But if instead of that I actually have, let's say, 50 things that I'm contributing to, there's another whole nother way for me to expand my capacity in the world, which is I can have 51st things. I could just add another dot on my graph, like my network just gets more robust. And the wholeness of that no longer has a built in mandate either from the central node, the Dow, or from the individuals who are part of it. To have that sort of imperial expansion, to just integrate everything into a single point of control, you get rid of the incentive for continual centralization. The Languaging is actually quite formal. And guys at I think I don't know what his organization is called right now, but Joe Adeleman and the team that's been working on that sort of stuff and I can give you a link, have been doing a lot of good work here.
00:27:53.026 - 00:28:23.702, Speaker A: But it has to do with the ability to actually operate from values instead of purposes or goals, or at least specifically to be able to actually have values, be very clear, and be kind of the orienting basis for how people operate. Again, I'll give you an example. So if you're going to work at GM, I don't know why I'm giving poor GM the thing. Let's go make it Google. Fuck Google. So you're going to work at Google. One of the consequences of going to work at Google is that you have to leave your values at the door.
00:28:23.702 - 00:29:06.090, Speaker A: You have a job now to some extent you're going to work at Google because you kind of like, maybe it fits my values, although for the most part these days it's because they're paying me the most money and maybe it has some interesting work to be done. So one value I have working on interesting things another value I have getting paid a lot of money those get satisfied and all the sort of like 38,000 values I have get kind of like shit canned. All right, so then what happens is your incentive structure is very oriented towards goal maximization. I have extrinsic things in the world that I want to accomplish. Like I want to get a Lamborghini, I want to get a Big Island. I want to get all the things. And by the way, you can articulate values as goals but we're not going to make that a little bit esoteric.
00:29:06.090 - 00:29:14.560, Speaker A: Natural, ordinary humans don't work that way. Natural, ordinary humans. Is that noise too much?
00:29:15.250 - 00:29:16.990, Speaker B: I think it's fine. It's bearable.
00:29:18.050 - 00:29:56.602, Speaker A: Natural, ordinary humans. The notion of values, the term value is the kind of being or the kind of being that you want to become in the world. The qualities of being, which is to say the set of values that you express into the world are the orienting basis that constrains your choices. And it's profound that's living a meaningful life if you're actually able to make your choices on the basis of your actual values, you're living a meaningful life. And I'm not meaning that to be like fuzzy. I'm trying to articulate what that word means. Like it's a concrete thing.
00:29:56.602 - 00:30:38.998, Speaker A: It's an actual object in the world. And in this new environment, what we're trying to do is we're trying to bring that back in. We're trying to actually say, okay, I need to be having some aspect of my values. First of all, everything that I'm doing, every way that I'm making choices and expressing my choices into the world must be fully harmonized in harmony with the values that I'm making. So my values are expressed deeply through this egenic channel. So the Tao is a way of extending my values or my values into the world. And because you can partition things quite nicely and break things into modular pieces I don't have to have anything carry more than it can actually carry.
00:30:38.998 - 00:31:23.266, Speaker A: This simplifies things tremendously at the level of governance and the notion of fluidity. Meaning if I'm part of a dow structure where there's not high values alignment then I should just probably go somewhere else or maybe a subset I should drop into a different aspect. Maybe there's a certain piece of the story that where the values of limit is very high. Or I should be adding value to a different construct over here where the values I want to express are actually going to be well held. It doesn't necessarily mean I shouldn't be here. It just needs to be there's nothing contrary. So it's a weirdly more intense holding of principle, a weirdly more intense holding of what you actually care about as a governing principle.
00:31:23.266 - 00:32:18.534, Speaker A: I'm actually saying that fundamentally that the group should strive continuously such that the values of the individuals are actually being very well expressed by means of participation in the group. And where that's not happening, that ends up being a primary problem that needs to be resolved. Let's figure that out. Maybe we're doing something wrong or we're the wrong group of people or we're not a well defined domain. And so it's actually just intrinsically noisy because if you can get there, if everybody's able to understand here are the values that are the polyte, here are the values of the essence of this thing and my values are deeply aligned and well expressed then the kinds of conflicts that we find in the domain of politics don't really arise. You have different problems but they're not those kinds of problems. So that's another one.
00:32:18.534 - 00:32:21.450, Speaker A: It's just a design principle. Cool.
00:32:21.520 - 00:33:12.918, Speaker B: So I'm just going to play that back to you. So say I am trying to decentralize some organization that I've been involved in. I think having a holonic perspective in which you can see the parts of that organization as holes in themselves but also parts of another whole allows you to sort of think at the right level of scope. And there's probably some modularity in there where each piece has its own scope. And then maybe at the level of the ecosystem, the highest hold on or maybe at individual hold ons, you've got values and values alignment. And as long as the values of the individuals are able to be expressed by their participation in the group then that Dow will remain on track or is more likely to remain on track. And where that's not happening, that's maybe a primary place to evolve.
00:33:12.918 - 00:34:17.674, Speaker B: The one thing I would take and I would sort of extend what you've given me so far and I'd love to hear your feedback on what I just said. But this idea of socialware versus trustwear I think is new in the Dao space. So Jordan, basically the idea here is socialware, is this idea that we can rely on agreements with each other. For example, you and I agree that you're going to give me an Apple and until we see each other that's just a social contract in between us. And Trustware is relying on cryptographic systems, machines in order to deliver that. So the smart contract example of this would be you create a one Apple token and then you promise that to be on chain through a smart contract, that's trustwear. And so I think that it's interesting to think that some of these Dows are going to have instead of typical companies which are just socialware up all the way down with the exception of maybe some legal trustwear in there in the form of the bylaws and the governance structure.
00:34:17.674 - 00:34:54.394, Speaker B: With Dows you've got trustwear and you can sort of start to segment out where the humans actually have power over each of these modules and scope what exact governance power they have over these modules which allows you to, I would think, of it as like, if everything is socialware, then there are no bumpers. But trustwear is almost like bumper bowling, where you can't go that far off course of whatever the programmatic set of values that the organization has. So that's how I would take and extend what you said about modularity and values, alignment and values of individuals in these organizations. How does that resonate with you?
00:34:54.512 - 00:35:36.226, Speaker A: Yeah, it resonates very highly. So let me just kind of give you an example of what that looks like in a different domain. What I mean is this. There's bones in my fingers that's trustwear, right? There's a whole bunch of shit that's going on at the causal level that I have no conscious awareness of or need to control in any way. It's taken care of at a different level than the Stack, with extremely high confidence. Like, effectively, I can assume now on occasion there may be something going on where that doesn't work and my finger breaks or something else happens, but more or less that's built in, and that's how it works. We actually consistently have these layers of different this notion of different layers of the Stack, and we always want to render unto Caesar with his Caesar.
00:35:36.226 - 00:36:15.830, Speaker A: So to the extent possible and appropriate, and that's a caveat I'll give in a second. This is a big piece of the new form of governance. It's actually being able to say, yeah, we can now render into code a whole bunch of functions that make them no longer needing to be governed by this weird tweener capacity of bureaucracy or yeah, it's actually very specifically bureaucracy or humans acting as machines. Right? We don't need humans to act as machines anymore. We can actually have machines act as machines, and they could do that job very well. Awesome. Okay, now there's essentially two primary failure conditions in that design modality.
00:36:15.830 - 00:37:02.030, Speaker A: One, we go back to that notion of an improperly partitioned domain. So if you're trying to build trustaware technology that causes the lungs to do heart stuff, or you've got heart and lungs mixed into a single thing, you're going to find yourself very confused and creating all kinds of weird errors. You have to be very clean to understand that your modules are in fact modules. Like, they're really well balanced. They actually make sense as a whole, and that the interfaces make sense. And this is tricky when you're decentralizing something that already exists, because history, the legacy that's gotten us here, has produced monstrosities. Every organization that exists isn't a well defined, well bounded thing.
00:37:02.030 - 00:37:54.946, Speaker A: In fact, it's kind of like a Frankenstein's monster of sort of partially produced objects. Each distinct function in, say, Google and Facebook are weirdly bound cancer cells. It doesn't make any sense necessarily for Google to have a particular set of organizational functions in a properly functioned ecology, but because we don't have that, the boundary of corporation is wrapped around it. You've got all these things that are inside it that actually maybe should never be there when you decentralize it a lot of that actually just needs to go away and big pieces of other stuff actually need to be reshaped until they are in fact properly modular. Okay, that's one piece. So that's the notion of if you don't actually have well defined domains, then all of your design efforts are going to be trouble. And what will end up happening is that the human stuff will end up having to Klude what the fuck's going on.
00:37:54.946 - 00:38:34.158, Speaker A: Like everything keeps breaking or getting broke. We call it wrong results are coming out of it, et cetera, et cetera. Or the complicatedness of the mechanical structure results in a machine that is too Kludy and therefore is error prone like crazy. The other side of it is when we endeavor largely because of ideological presuppositions, to do in trustwear what in fact must or should be done in socialware, this is kind of the ideological problem of decentralized all the things. Or when we say code is law, we mean code is the whole of the law. Something like that. And that doesn't make any sense.
00:38:34.158 - 00:39:22.480, Speaker A: Right? The reality is that choice is an aspect of reality. And more importantly, choice is part of the aspect of reality, which makes reality a thing that we care about at all. And so what we want to do is we actually want to be rendering to the machine all the stuff, but only the stuff that the machine is exactly the right solution for. I should mention, by the way, that as I contemplate this conversation, I noticed in myself that things like crypto and AI are part of the same domain, like they're part of the same issue. And the machining, how do we actually deal with the machinic, integrate the machinic, have it support and empower our choices well, as opposed to convert us into very poor machines or pull from us stuff that we actually care about.
00:39:23.190 - 00:40:21.742, Speaker B: Yeah, my sort of theory about, first off, the Wellscope domain, the point that you just made is very well put. But just to take your point about AI and blockchain fitting together and run with it, my sort of theory on that had been that blockchains are really great at creating Heuristics. So your token balance is X Jordan and mine is Y on, I don't know, token Z, but on token A, you have balance B and I have balance C. And those are like Heuristics. And AIS are really good at Heuristic maximization, which is just basically to say, climb this hill. If the Heuristic is what's your token balance, then an AI can try to maximize that. So what if we had, I don't know, a token that represented the fight against climate change? So basically you had an AI.
00:40:21.742 - 00:40:36.280, Speaker B: That's job was Heuristic was to maximize the value of that token and the amount that they had, and you set an AI. Loose on that heuristic. If Blockchains can set Heuristics for things that are actually good for humanity, then that's maybe how the two systems fit together.
00:40:38.490 - 00:40:57.598, Speaker A: Yes and no. That's a really good example because it's like the trickiest place. Let me flag because I don't want to forget about it. Let's go wisdom and good choices. Let's get back to that. But there's a profound and by the way, obligate. There's no other way of doing it that I'm aware of.
00:40:57.598 - 00:41:50.622, Speaker A: So there's a profound and obligate way for the construct that you were just imagining, AI. Blockchains, et cetera, to support humans in properly stewarding the complex world that we live in. And it's not going to be the kind of thing where we want to or could design a finite set of metrics that are going to actually result in the solutions that we want. So it's more like, let me think of a good example. Well, I'll give you a very simple example and then we'll expand from that. A thermostat is kind of what you're describing. It's one of the simplest possible constructs between a token system and an AI system.
00:41:50.622 - 00:42:10.998, Speaker A: In this case, token system is a very simple on off in relationship to a boundary condition, temperature, and an AI system that has the ability to actuate some kind of mechanism heating or cooling in relationship to the flagging of the metric. So that is actually really good. That's a very functional thing.
00:42:11.084 - 00:42:15.560, Speaker B: I think it's a very apt metaphor too, because I think I have an AI thermostat in my house right now.
00:42:16.410 - 00:43:11.426, Speaker A: So what you want is you want to say, okay, there's two things. There's the alignment problem, where if I'm in charge of telling it ultimately what I'd like it to do, the subtlety and nuance of what that exact means is kind of the hard problem. The challenge, I'll get to that in a second. But as long as it's not in charge that's it one. So thermostat doesn't tell me what temperature it wants to optimize for, it does what I tell it to do. Here's the other piece and in the context of the actual whole reality that I actually live in. So to make an example, if your AI thermostat could sort of like perfectly modulate the temperature your body's in, but at the expense of turning everything outside of your house into an unlivable hellhole, you'll be externalizing something that was wrong.
00:43:11.426 - 00:43:40.306, Speaker A: So you've got these two problems. This is the real thing. This is actually where we are. You have to actually be able to have a full accounting for literally the whole of reality that you're affecting. And the more powerful we get, the more that is actually a reality. And you have to actually be able to have a full how would I say this sovereignty. You have to have full empowerment of the center of sovereignty being in the singularity of the human.
00:43:40.306 - 00:44:20.846, Speaker A: And so those are the two points. In between those two points, we can actually have very, very high agency or lots of actuation happening in the machining that will sort of include all of the stuff that happens in technology. But those two points are subtle and tricky. But let me speak to a simpler version of that, which is the wisdom and good choices thing. If you look at the expanding frontier of what people back in the 80s were fucking talking about, but now we're feeling it, of AI getting better. So chat GPT, killing X number of jobs. Hypothetically, we don't know that it actually will, but let's just assume worst case scenario and it just sort of obliterates a whole bunch of jobs and then let's iterate it again.
00:44:20.846 - 00:44:45.880, Speaker A: Like there's just a sort of expanding sphere software eating the world. How much of the world gets eaten? Well, my proposal is that the boundary of that is wisdom, meaning effective choices. Actually making the actual choices of what to do is the thing that humans are here to do. That's our thing. We're here to make choices. And I mean free will, if you want to be theological about it. That's what I'm saying.
00:44:45.880 - 00:45:56.350, Speaker A: And we must not ever and there is no way to get this right if we tried, endeavor to what's that word? Not take that responsibility. Like to actually drop our responsibility for making choices and endeavor to actually have machines, AI. Or whatever make those choices for us or in our stead, which ultimately ends up being quite nice because what that means is that we're endeavoring to cultivate. And by the way, at the level of simple Taoist, for example, we're trying to cultivate virtue, cultivate wisdom, effective choice making in individuals. So unlike corporations, which have as their characteristic the need and desire to convert humans into machines, cogs I need you to not be a human. I need you to be an effective director of marketing, which means stripping out something like 85% of who you are and then building a bunch of stuff that isn't you so that you can fill this particular role in the extended machine of this organization. The endpoint, and probably not that far away, but the endpoint of this new thing we're dealing with is actually, no, I actually need you to be a whole human.
00:45:56.350 - 00:46:43.150, Speaker A: Like, I need you to be wise and I need you to be prudent and I need you to have courage. I need to actually upregulate virtue in human maximally, like really high quality human one, because more and more and more of that other stuff is going to be rendered into the machining domain. So the fact that you were very good at X function is actually rendered obsolete on an accelerating basis. And because the potency, the power that we're wielding is increasing as a consequence and therefore the consequence of the choices that we're making gets actually deeper and deeper and deeper. So profundity in governance becomes kind of the thing we're doing. We're actually steering these monstrous, tremendously powerful machines. We kind of do a really good job at it.
00:46:43.150 - 00:47:19.106, Speaker A: So keep that in mind. Next key principle is you're really invested. I mean really actually, the HR function of dows should be extending resources and cultivating the virtue of every individual who's participating. And one of the things we talked about, by the way, is that notion of boundary remembering. Meaning a sense that just because you happen to have been born a Homo sapiens, you therefore can and should have perfectly equal rights in governance of every single Tao that you want to be part of. Is not true. The way I put it is humans are not born sovereign.
00:47:19.106 - 00:47:51.858, Speaker A: Humans are born with the potential to become sovereign. All humans are born with the potential to become sovereign. It's hard work to actually be able to carry the responsibility of making effective choices in the world. So one, good boundaries an invitation only to people who are mature and capable of actually taking stewardship responsibility for something important and being serious about reality in life. And then two, a deep mutual commitment on the part of everybody in that group as individuals and as a group to cultivate that capacity further and further by virtue of what they're doing. Right.
00:47:51.944 - 00:48:04.134, Speaker B: So I'm going to add commitment to maintaining the organization or maintaining whatever the scope, the purpose for being of the organization from each individual is sort of one of these necessary criteria here.
00:48:04.332 - 00:49:06.330, Speaker A: No, not actually deconstruct that in two ways. So one is cultivating in themselves like a deep commitment to growth in their own capacities to make effective choices and to discern their own values and to express those values well into the world, to become that kind of a person. And then as a consequence, the expression of the values of the thing, the organization, but it's not quite the right term, not the purposes, but the values of the organization into the world with increasing precision and increasing the right word. It's like it's intensity, right? You've got precision, fidelity and intensity, but all three really happening. So really delivering what it's supposed to be doing really well and cleanly and nothing else into the world. That's what you're real committing to do. Be a really good heart.
00:49:06.330 - 00:49:14.510, Speaker A: Be a really good heart. Like heart like a motherfucker. Don't ever drop the ball on hearting because the whole organism is counting on you to not skip a beat.
00:49:14.850 - 00:50:16.910, Speaker B: Yeah, well, given that we're getting towards the back half of the hour and I just want to make sure that we're converging here. We started off by asking the question of, like, how do decentralization, how to turn company into a dow, how to organize a dow such that it can be a decentralized coordination mechanism that has a chance at creating a third attractor. And that's sort of my framing. And I've got sort of a list here of having well defined domains, modular pieces for each organism, defining values and creating values. Alignment, allowing values to people, values of individuals to be expressed by participation in a group and like a self correction mechanism if that's not happening a deep commitment to growth in people's own capacities, a deep commitment to maintain. To create your own values through the organization's values. But also, hopefully, that becomes a collective expression of values that amplifies the values of the organization.
00:50:16.910 - 00:50:38.226, Speaker B: So if I'm looking to turn a company into a dow, it feels like these are all of the things that should be invested in and then there's sort of like a reclusive infinite game in which they play out in such a way that creates a coordination mechanism or creates a force in the world towards decentralized coordination. How does that resonate?
00:50:38.338 - 00:51:25.650, Speaker A: Yeah, really good. Let me kind of take a piece that we hit on I want to bring out. So the notion of trustwear and socialware and again, both have an important role to play and it's critical to actually do the trustwear well, but only exactly where it's proper. Because the converse of that, and of course this has been sort of the massive autistic gap that has slaughtered crypto so far, is you have to actually take the human piece seriously. You can't just assume for the whole, what is it? FBF fiasco? People are people. We have ways of knowing that people are lying, that they're untrustworthy and it's okay. It's okay to actually use the wisdom of humanity to not give your money to somebody who's untrustworthy.
00:51:25.650 - 00:52:14.790, Speaker A: And it's okay to use the stuff we've developed for the past, whatever, 30,000 years to verify whether or not a person is in fact trustworthy or not carefully and judiciously over time. So we should take the human piece seriously. And then if you do that well and you combine that with taking the machinic piece seriously, now you've got something that will actually work and can carry a lot of weight. That's kind of the key message here. Yes, there's a lot of really cool, powerful requisite obligate like we can't do it stuff happening in the technical domain, AI crypto. Yes. And there's a bizarre naivete around probably mostly people who just don't like dealing with the human aspect and therefore really want to not have to build that capacity in themselves, trying to pretend that they don't have to by inventing we're going to put it all in the code.
00:52:14.790 - 00:52:47.650, Speaker A: No, sorry, that's not how it's going to actually work. You're going to have to actually learn how to deal with other people, how to negotiate conflict, how to grow maturity in yourself, how to communicate clearly, how to learn whether or not somebody is actually trustworthy the hard way. You're going to have to build groups. You're going to have to know how to create boundaries and not let people in. Like it's just basically what I would say ordinary human maturity, but we're several generations from actually having that. And so that's a cultivation. We have to consciously recultivate that and then extend it with the mechanical piece.
00:52:47.800 - 00:52:53.314, Speaker B: Feels like there's a certain amount of change of self and change of system that go hand in hand here.
00:52:53.512 - 00:52:57.666, Speaker A: No question, man. We're in a new world for sure. And so there has to be both.
00:52:57.848 - 00:53:42.722, Speaker B: Yeah. Well, I want to talk a little bit because I know that we've been sort of talking in this future realm of Dows and AIS and networked organizations, but I just want to ground this a little bit in some of your experience in the past as the founder of several successful companies in the past. And I think that you said that several of the people who worked for you went on to found their own companies, and I think you said there's always a nod of helping them, but until you've sat in the chair, you don't really know. I'm curious if you could take us back over the last 2030 years of founding companies and being an entrepreneur in some of the tangible examples of things that you've seen that might sort of ground the audience a little bit in what you're seeing for the future.
00:53:42.856 - 00:54:16.000, Speaker A: Yeah, I mean, obviously there's a lot more than we're going to be able to do in five to ten minutes. Let me just throw some things that pop up that seem call it relevant. So the first is just the one you mentioned, the nod. Being a leader, like actually taking on honestly, like, earnestly and honorably the responsibility of being a leader of an organization. It's no joke. There's a lot going on there, and it's not quite as intense as childbirth, but it's in that milieu, meaning you can't know it until you've actually lived it. You actually care.
00:54:16.000 - 00:54:40.486, Speaker A: I'm not a sociopath, so I don't know what the sociopaths do. But for the people who aren't, you actually care for the people that are part of the organization that you're working with. You care for their thriving and well being. You don't want them to be wasting their time. You don't want them to lose their jobs. You actually care for each and every one of them. And you're overwhelmed beyond comprehension by the fact that there's a lot more of that going on than you could possibly hold.
00:54:40.486 - 00:55:15.386, Speaker A: And you care about the stewardship of the purpose of the thing, your own values, and how the thing you're leading is expressing itself into the world. And you're dealing with the highest degree of awareness of the complexity of the whole deal. You don't have the advantage or the benefit of looking at a particular subpiece and focusing your energy. You actually are kind of feeling all of it, and you're aware of all the troubles and trade offs and challenges. And in many cases, you may be one of the few people in the organization who's actually taking a look at the world outside of the membrane, being aware of the complexity of what you're navigating. So this is a really serious burden. It's a hard challenge and for most it actually is a backbreaking challenge.
00:55:15.386 - 00:55:57.422, Speaker A: Like you get quite burned out doing that and you grow a lot of ways that you may not want to. You may have a lot because you have to. And if you're committed to the thing you're doing, you will. And that's a growth that again can't happen unless it's under that kind of pressure. We know this, humans know that we at the end of the day, will almost never actually develop new, deep, rich, powerful capacities just because we want to. It usually requires some kind of potency or some kind of forcing function to just make us have to get up at six in the morning or five in the morning and just get the job done continuously. Okay, so one, that's all real.
00:55:57.422 - 00:56:40.282, Speaker A: Two. There's something about that that you're not going to be able to just abstract away by building a consensus voting function. Because now you have a bunch of looky lose who aren't really skin in the game, who aren't living and dying and breathing and bleeding, who kind of vaguely think like they have lots of rights to do stuff but aren't carrying anywhere near the full responsibility of it. There's a reason why those kinds of things don't work very well and there's a lot of kind of entitlement and petulance that comes from not having participated in that but kind of looking at it. And I remember when I got myself stuck in being a CEO for the first time, literally speaking out loud, like, well, this doesn't look that hard. I'm sure we can do better. Whatever.
00:56:40.282 - 00:57:59.400, Speaker A: 26, right? Yeah, it actually is really hard and it doesn't look that hard because you're only seeing a tiny fraction of it. So what's the point? The point is something like anything that we are doing that is actually deserves to be named governance is actually a lot more important. A lot more, yeah, I'll say important potent. Right. Then we kind of have been given not we, but particularly, and I don't mean to be mean, but the millennials in particular and younger have been given a life experience to actually even recognize what that means. Unfortunately, we've built kind of like a Disneyland civilization that hasn't actually given people a lot of capacity to carry heavy weight and deal with real responsibility in a deep way where the choices you're making are in fact actually important and serious. So that seriousness and the soul that comes from making bad choices where people have gotten hurt and knowing that choices have consequences that you can't really just narrate away or bullshit away is an element of any real group that I would trust with governing anything.
00:57:59.400 - 00:58:17.260, Speaker A: Now, by the way, that's the choice making. That's not necessarily the perception. Lots of things. Can happen in the context of input. Everybody should have input. You have to know what's actually going on. And no given individual can vaguely perceive like a small fraction of the whole of what's happening.
00:58:17.260 - 00:59:00.642, Speaker A: So that's that piece. The other piece is just to say one of the elements that I learned the hard way is humans and human vice and how incentive landscapes can fuck things up bad. You had a well functioning team that all seemed to be like a crew that were really pulling in the same direction, had each other's backs, and then suddenly the stake of everybody's piece of the ship gets a lot more valuable and suddenly backstabbing starts happening in way that you would never have expected would happen. Or people suddenly start slowing down their collective pulling and thinking about their own individual piece of it. Yeah, man, that's real. And so more for that matter. People are good at talking a good story, but actually can't deliver.
00:59:00.642 - 00:59:29.978, Speaker A: There's all kinds of that stuff. Right? So the hard earned piece of the fact that we are really building things out of humans and we want to, we need to. And there's a difference between being kind of naive and ideological about how that works and being practical about how that works is what I would put into the middle of the equation. That there's something actually important and real there. And the more you honor that and the more you earn your spurs, the more effective the things you're participating will end up being.
00:59:30.164 - 00:59:57.450, Speaker B: Yeah, well, I think that there's certainly a lot there. I wish that we had a half hour more to get into it, but we are running out of time. Is there anything that I didn't ask you that you want to say? And I'll just ground that by refocusing the audience's attention on the original intention of this episode, which is how do we build a decentralized neocortex, a neonocortex? And how can Dows solve for the third attractor solve for coordination failures? Is there anything that I didn't ask that you want to say on that subject?
01:00:00.270 - 01:00:42.794, Speaker A: Yeah, two things. And I apologize because well, like, I apologize too much if you know anything about me. This is part of what I seem to be doing. So I'm going to throw out some shit that may actually not be valuable to anybody, but if it is valuable, you'll like it. So one is the degen thing. To the degree we know this, this is very well understood psychology to the degree to which people's actions are motivated by extrinsic metrics, meaning piles of stuff, they're going to become degen. This is the moloch thing at the individual level, to a degree that people are in fact motivated by their values well perceived and well expressed, you will have the regen thing.
01:00:42.794 - 01:01:39.162, Speaker A: It's kind of as simple as that. So keep that in mind. If you found yourself surrounded by degens, there's probably a very simple reason, and you're going to have a really hard time steering that ship. Two, this is two is completely different. So just like frame change, there is a project that I quite like, I've been following called Afropolitan that is doing something like a network state, although I think it's much richer than that. And I was watching a video by a woman who was talking about how much the consequences of the Napoleonic civil code form of law had wreaked havoc in the former French colonies in Africa and how the English common law system, which in many ways resonated with the indigenous system of governance or jurisprudence, almost always proved more effective or better. And I would point to that just as a really powerful thing.
01:01:39.162 - 01:02:50.020, Speaker A: So civic code, the Napoleonic model, comes directly out of the French Revolution and the Enlightenment style of thinking, which endeavors to perceive or imagine that you can kind of write down a schematic that can literally take into account every possible case and produce the appropriate rule for how to deal with that case. In other words, it's algorithmic. It takes an AI approach to how we do governance, and it's a terrible way of doing things. Common law works in exactly the opposite direction. What it says is we have a whole bunch of loose heuristics based upon collective experience, and we're ultimately going to ground things in wisdom. So we're going to try to have we're trying to cultivate wisdom in individuals, and we're going to try to find the wise people, and we're going to try to use our sort of hard earned collective experience, which is context relevant. It actually understands the context, all the deep context of what's happening, and use that to discern really effective choices that are going to be relating to the actual reality of what you're dealing with.
01:02:50.020 - 01:03:15.654, Speaker A: I keep hitting on the same theme, but in different directions. This is the same theme that's important. Like, it really needs to sink in that if you're running a network state, your jurisprudence should be common law. I'll simply put it like that. And this is the same thing. There's stuff that can be rendered into the machinic. Yes, do it, do it cleanly, do it well, understand where it works, what it can carry, and then take that other piece seriously.
01:03:15.654 - 01:03:29.760, Speaker A: And we can point to history of what that looks like and we can copy that. We don't have to reinvent it, but avoid the Enlightenment mistake. Avoid the mistake of thinking that humans acting like shitty machines are the right way to govern.
01:03:32.930 - 01:04:01.782, Speaker B: Well, it seems like a great place to end. Jordan, thank you so much for doing this episode with me. I really enjoyed our Twitter exchange. I enjoy how deeply you've thought about some of these subjects. And as an operator in the Web Three space, I think that some of these lessons are quite useful and tangible for people who are trying to create dows and try to create successful dows at that. And it just feels less lonely to hear someone like you talk about it. So thanks again for doing this episode with me.
01:04:01.916 - 01:04:04.486, Speaker A: You're super welcome. I'm glad to hear that.
01:04:04.668 - 01:04:11.262, Speaker B: Yeah. Any parting thoughts, or where can people find you online before we break?
01:04:11.396 - 01:04:13.200, Speaker A: Oh, I hope they don't find me online.
01:04:14.690 - 01:04:18.202, Speaker B: Okay. There we go. All right, well, thanks again, Jordan.
01:04:18.346 - 01:04:18.890, Speaker A: All right, bye.
