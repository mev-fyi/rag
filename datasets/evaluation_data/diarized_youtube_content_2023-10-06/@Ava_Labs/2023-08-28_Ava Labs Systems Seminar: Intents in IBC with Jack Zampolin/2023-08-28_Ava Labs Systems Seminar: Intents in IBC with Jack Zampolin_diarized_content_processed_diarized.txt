00:00:10.010 - 00:00:21.962, Speaker A: All right, thanks, everybody, for joining us. And thanks especially Jack, for coming to talk to us today about intense and IBC. For those that don't know him, Jack Zampalin is one of the leads at Strangelove, perhaps CEO and founder. Is that right, Jack?
00:00:22.106 - 00:00:23.102, Speaker B: Yes. Yes.
00:00:23.156 - 00:00:31.490, Speaker A: CEO and founder, or as like to call them, bringing IBC to all the friendly blockchains near you. So with that, let you take it away. Thanks so much for coming, Jack.
00:00:31.650 - 00:00:50.662, Speaker B: Awesome, Aaron. Really appreciate that. Excited to talk about intense in IBC today. As Aaron mentioned, my name is Jack Zamplin. I'm the co founder and CEO of Strangelove. Most folks know me from my work on Cosmos. I started working in blockchains back in 2017 at a company called Blockstack.
00:00:50.662 - 00:01:25.554, Speaker B: Helped them do their ieo and launch their developer platform back in 2017. When 2018 came, they pivoted away to do what became stacks 20 and kind of threw away their whole developer platform. And then that's when I went and joined Cosmos. When I joined Cosmos in early 2018, it was kind of chaotic. A collection of a bunch of people who believed that there were going to be a ton of different blockchains, and we needed to connect them. And I remember having a conversation with Zachy after we launched the Cosmos hub, and I said, okay, wait a second. So our go to market for IBC is we just launched one of these blockchains.
00:01:25.554 - 00:01:54.930, Speaker B: We got to go convince ten other people to do it. Then we got to build an interoperability protocol and connect all these blockchains. Zachy looked at me straight face and said, yes. So that's what I went and did for the next couple of years and helped launch IBC and build out the cosmos ecosystem. And I've taken a lot of that work that I do and built it into a company called Strangelove. So Strangelove is dedicated to building IBC and supporting its growth and then profiting off of that. And that's what we work on here at Strangelove.
00:01:54.930 - 00:02:43.470, Speaker B: So what are intents? There's a lot of talking heads talking about intents on Twitter, but it's fundamentally most basic. A user specifies an intent in a transaction on a single chain, and that intent is carried out across multiple different systems, and the user ends up where they want to be. And I think that the way that I always think about this is in Ethereum. If you want to go borrow some money on Makerdao, go trade it on uniswap and then enter a position in Aave, you can do that seamlessly because all of the data is in the same global environment. All of the contracts are in the same global environment. And you can do those calls synchronously. When you separate each of those systems into separate application specific blockchains, it requires you to do this asynchronously.
00:02:43.470 - 00:03:18.730, Speaker B: And that introduces complexity, intents kind of aim to capture that. So this diagram is still work in progress, but a historical example. A user sends a request to google.com. Google Com's server farms that out to a ton of different servers, and then they all fan back in and return to the user. This is kind of what intents look like in blockchains. So IBC currently requires users to sign many transactions for a user intent. So let's pick an example here.
00:03:18.730 - 00:04:02.870, Speaker B: And some of these app chains might be unfamiliar, so I'll just talk about what they each do. So maybe you have some atom and maybe other tokens in a liquidity position over on a Dex, and you want to end up on another chain in a lending position. So Kujira is a decentralized exchange, as is osmosis. UMI is a lending protocol, similar to Aave on Cosmos. So right now, if I want to go from Kujira to Umi and trade on osmosis in the middle, there's a ton of transactions. Each one of these blocks represents a user transaction. So the first thing I would need to do is send a transaction to get out of my lp.
00:04:02.870 - 00:04:28.606, Speaker B: Then I send a transaction to IBC Atom to the cosmos hub, then IBC that atom to osmosis. And the reason why we need to go through the cosmos hub and not directly to osmosis is because of token fungibility. Also. Happy to dig into that if anyone has questions. But I think folks on this call probably familiar with that problem. Once we are on osmosis, we trade that atom for USDC. On osmosis, we move the USDC back to noble again for fungibility reasons.
00:04:28.606 - 00:05:23.714, Speaker B: We IBC that USDC over to the lending protocol, and then we finally lend the assets on the lending protocol to start receiving our yield. So that is 123-4567 transactions to do something that you can do on Ethereum in a single transaction. And I think this shows kind of the scale of the problem that this asynchronous environment brings in and what we have to do to solve that. So let's talk about how we solve this using IBC middleware. So what does middleware do? It adds additional processing to IBC packets as they are sent and received. The simple JSOn structure that allows users to specify actions and then any next actions. And ics 20, which is the token transfer protocol, added a memo field that adds for arbitrary data to be passed in.
00:05:23.714 - 00:06:01.418, Speaker B: So that's like fundamentally how we do it. I think at this point I'm going to just pause real quick to talk about IBC architecture at a high level. In IBC, there's a few kind of fundamental layers. The first layer is the client, and this is where we do verification of data from other chains. Think about it as the light client. In avalanche, we're using your warp messaging verification as the light client verification. So it's kind of that layer you want to be able to externally verify from another chain what's happening over on your counterparty.
00:06:01.418 - 00:06:56.286, Speaker B: So that's where all the cryptography lives. It is the hardest part of the system to build. On top of that, there's a set of handshakes called the Tau, and that is transport, ordering and authentication, or transport, authentication and ordering, if we want to call it the tau, but not too technical there. And basically what that layer does is authenticates one chain to the other. If you're on avalanche, you know what your validators look like and you can go over to your counterparty chain and say, hey, counterparty chain, I expect us to have these validators at this have, does your light client say that the other chain will say yes, and then that happens, vice versa. This authenticates the chains to each other in a permissionless way. Then there's another handshake on top of that to determine application format and ordering.
00:06:56.286 - 00:07:37.886, Speaker B: So by application format, what do I mean? If we're sending tokens back and forth, you might have a packet that has a sender, a receiver, an amount, a denomination, and potentially a memo. And on one side you would lock or escrow the tokens that are moving over, and on another side you admit vouchers. So that is a simple application protocol on top. So these are kind of the fundamental layers of IBC. A middleware wraps an application protocol, so in this case we're wrapping ics 20. And when we receive. So there's two protocols to help make the example that we showed above happen.
00:07:37.886 - 00:08:16.534, Speaker B: One is packet forward middleware. If you get a packet containing tokens, forward it via another channel to somewhere else, and the other one is swap and forward. If you receive packets down this channel, take those resultant tokens and swap them in a dex and then send the value elsewhere. So here are examples. On the left we have a swap and forward example. And on the right we have a chained packet forward middleware example. So here in the swap example, we've got Dex arguments essentially and functionally.
00:08:16.534 - 00:09:17.150, Speaker B: What happens is when the packet comes in, instead of minting the vouchers to the user account, we mint the vouchers and then go trade them immediately on a Dex, take those resultant tokens and then forward those out via another IBC channel. And this allows for the same type of composability you get on Ethereum, where you can chain together multiple projects and protocols while only sending a single transaction on the right. This is an example of assets traveling over two intermediary chains on the way to a fourth chain. And what this highlights here is the use of this next. So in that memo field in the ICS 20, you can sort of chain together multiple of these interactions. So let's look at our example and sort of see how this would work. So if we're going to remove our atom from the Kujira LP and IBC, the atom to the Cosmos hub, that's a single transaction.
00:09:17.150 - 00:10:27.314, Speaker B: It would be received by the middleware on the Cosmos hub and then forwarded directly to osmosis. We would have a little next in our JSON blob that would say we're going to swap these assets on osmosis for USDC and then forward them to noble chain so that trade and then the forward to noble would happen on Noble. We would again hit the IBC middleware, again hit the packet forward, middleware, forward that USDC to UMI. And then the only piece that's not in production today would be a lending middleware on UMI to allow them to receive those tokens, put the user into a lending account and leave them on UMI in that lending account. This is a much nicer user experience and is what's going to make this multi chain future possible. Rebuilding a lot of the primitives that we have in the HTTP world in another asynchronous programming context into this cross chain bridging world. So I'll just pause there real quick.
00:10:27.314 - 00:10:40.714, Speaker B: I'm going to dig into happy path and sad path for packet forward middleware. But does anyone have any questions? It might not be clear on any of these points here. Cool, great.
00:10:40.832 - 00:10:50.074, Speaker C: Actually, hey Jack, I had one quick question on some of the Tau related stuff and how these zones kind of update the state of each other.
00:10:50.272 - 00:10:51.110, Speaker B: I was curious.
00:10:51.190 - 00:11:20.680, Speaker C: I had always been under the impression that there were quite a bit of updates that had to be sent between different zones to keep that in sync as new messages were created. I was wondering if you could just kind of clarify how often zone states need to be updated for this all to work. If a new message is emitted by a zone, do I have to go now? Update anyone that may read from that zone, or do I do it on some delayed cadence or periodic or something?
00:11:21.210 - 00:12:23.514, Speaker B: Well, so the light clients have a height associated with them, so you can prove anything about the state at that height. So as users send new transactions, commit new packets, in order to view that on your counterparty chains, you have to update the clients to the height that includes those packets so that you can process the proofs. So if I'm a user sending from, say, the Cosmos hub over to the landslide subnet over on Avalanche, when I send my message transfer over on the Cosmos hub, a packet gets created under the hood. The relayer sees that packet, and then over on the landslide side, we're going to need to send a transaction that updates the client that represents the cosmos hub to a height that includes the packet commitment and then a message with the proof for that packet commitment to receive the packet over on the avalanche side. Does that make sense? Patrick? Yeah?
00:12:23.552 - 00:12:30.638, Speaker C: Do you have to include that update for each height that includes a message or just some height greater than the message?
00:12:30.724 - 00:12:34.334, Speaker B: Some height greater than the message. Thanks. Yes.
00:12:34.532 - 00:12:35.630, Speaker C: That's all. I mean.
00:12:35.780 - 00:13:14.780, Speaker B: Yeah. And in practice, how this works is relayers will wait for a number of packets to kind of queue up or some time period, and then they'll process them all at the same time. Some connections require lower latency. People are willing to pay for that lower latency, and they can get that. And I think that this is one of the really nice things about the protocol, is that there's multiple ways to play that. And depending on how much money you want to pay or how your relayers are incentivized, that's really flexible and allows a lot of choice on behalf of users. Cool.
00:13:14.780 - 00:14:07.014, Speaker B: All right, so on the happy path for the packet forward middleware, the way this works is the user initiates a transfer on one chain. There's a receive packet action on the other chain, and then we yield into the packet forward middleware processing. That would then forward that asset to a received packet on another chain. So this initially had four chains up here, and we took out one of the chains, but we didn't take out the second forward. So we'll talk about the second forward here. Then we would forward those assets through another packet forward, and then there's an acknowledgement that would actually go all the way back to the original sending chain. One other piece of IBC, it is a call and response protocol.
00:14:07.014 - 00:14:49.338, Speaker B: Just like HTTP. You can send a request and you receive an acknowledgement. So the packet forward middleware will forward that acknowledgment all the way from the final destination chain all the way back to the originating chain so that you can properly do error handling in sort of a sad path case, I. E. When the intermediate chain potentially fails, user initiates a transfer packet is received over on the counterparty chain. We attempt a forward which times out, and users can specify the number of timeouts they want. So in this example, we're going to time out twice, and then we're going to retry the forward again.
00:14:49.338 - 00:15:41.370, Speaker B: And then if it times out, we send an error back to the originating chain. There are other ways to enable this type of composability between chains, and the other one is an accounts based model where you allow accounts to control accounts or smart contracts on other chains and then use that to perform the same type of interoperability. Interchange accounts is a piece of infrastructure that allows this. Interchange queries is the sort of partner to interchange accounts. So this would allow you to write and read data from another chain originating those transactions and queries on one chain and then Polytone. Those are two separate protocols that each travel over separate channels. Polytone is a single protocol that wraps interchange accounts and interchange queries in a single protocol.
00:15:41.370 - 00:16:04.834, Speaker B: Quasar and stride have been on the cosmos side, leading the way in deploying these types of systems. Quasar is a vault product that manages liquidity on different dexes, and they do that via interchange accounts. Stride is a liquid staking protocol that manages staked assets on other chains, and they do that through interchange accounts. Stefan. Yeah.
00:16:04.872 - 00:16:19.234, Speaker D: So going back to the failure case on the previous slide, does that mean that the packets that you're sending, so the requests, do those have like, a maximum height included in them? Yes.
00:16:19.432 - 00:16:39.780, Speaker B: Okay. And that maximum height is, I believe, a maximum height on the sending chain, so that once the sending chain advances enough, if there is not a proof of receipt packet that gets acknowledged, users can time it out.
00:16:41.110 - 00:16:57.160, Speaker D: I guess if that's the case, couldn't it both get delivered and have an error? Basically, does the protocol guarantee that there is.
00:16:58.990 - 00:17:20.980, Speaker B: An exact in order to. Yes, the protocol does guarantee exactly once delivery. The way that works is you have to get a proof from the chain that should receive it that the packet has not been received, and then you can submit it to the sending chain as long as the height is greater. I'm pretty sure that's how it works.
00:17:22.870 - 00:17:28.340, Speaker D: I think you probably need to make sure that the message can't be delivered. So there's probably something on the recipient height. But yeah.
00:17:30.390 - 00:18:20.002, Speaker B: I think it's not actually on the recipient height, it's the height of your client on the recipient chain. And if you can advance your client on the recipient chain past the timeout height, then you can prove that on the sending chain. That's a great question, Stefan. Awesome. So what's next for middlewares? The way that I like to explain middlewares to app developers is like, this is your interchange API. You can expose whatever API that allows your protocol to be used asynchronously you want, so it can be a superset or a subset of whatever functionality you offer on the chain. So stride middleware is an example of this, where they have this liquid staking protocol that requires multiple transactions.
00:18:20.002 - 00:19:00.130, Speaker B: You need to move your assets over, then you need to deposit them into a contract, and then there's a bit of a handshake back and forth. You can do all of this with a single IPC transfer, and that is the API that strive wants to offer staking middleware, lending middleware, these are other examples of that. I think that the standardization and expansion of swap and forward, there's now currently three chains that support swap and forward middleware. All of the arguments are the same, thank God. But we have to continue to do that and sort of expand this to other chains so that we can do more Dex aggregation via IBC, which is kind of a killer app. All right, really appreciate your time. Thank you guys very much.
00:19:00.130 - 00:19:02.600, Speaker B: And happy to answer questions at this point.
00:19:13.630 - 00:19:14.860, Speaker E: I have a question.
00:19:15.650 - 00:19:16.302, Speaker B: Yes?
00:19:16.436 - 00:19:27.550, Speaker E: Are you just limited this IBC middleware, is it just limited to forwarding or can you sequence multiple app interactions?
00:19:28.210 - 00:20:02.150, Speaker B: Yeah, so you can sequence multiple app interactions. And let me show you what that looks like right now. So you can see here in this nested call, the first action is going to be forward. And then once the forward happens, we're going to pull off that forward data. And then this nested object here is then going to become the memo that gets sent over to the next chain. And on that next chain, we're then going to forward instead of forward. That could also say swap.
00:20:02.150 - 00:20:10.320, Speaker B: So you might want to forward and then swap, but this composes arbitrarily. And there can also be another next down in here. Does that make sense?
00:20:11.810 - 00:20:19.758, Speaker E: Yeah. So this would need to be supported by all of the app chains for this to work, right?
00:20:19.924 - 00:20:38.326, Speaker B: Yeah, it would. And again, there's other approaches to this but this is one that's started to catch on. We've got some decent wallet support for it, and people are building stuff on top of it. The account space model that I mentioned is the other sort of like asynchronous composability primitive that we have, because I.
00:20:38.348 - 00:20:52.490, Speaker E: Think another model that I have seen with Mars protocol is they don't have the ability to do that, but what they're doing is they're building like, outposts on all of the cosmic chains.
00:20:54.350 - 00:21:42.250, Speaker B: I really like the outpost model, and in fact, I think in the wild, what we're likely to see is all of these models end up being used for various things. Developers really like that flexibility. A and some applications are better suited to some of these models than others. In the Mars protocol example, they want to lend assets across a number of different chains. So they might need to have a number of different outposts that are sort of sharing liquidity with each other. One really cool thing that's sort of a side quest in this discussion. A team from Stanford last year built out a non interactive Paxos implementation on top of IBC to hold data and consensus between smart contracts across multiple chains.
00:21:42.250 - 00:22:01.810, Speaker B: You can imagine a system like that managing liquidity, but it is a little bit heavyweight. I don't think that's likely to see production anytime soon. But I thought it was a cool, at least thought experiment. How would you implement consensus on top of consensus? Yeah.
00:22:01.880 - 00:22:18.630, Speaker E: And constructing this steps, what would the user experience be, I'm wondering, because it kind of sounds complicated, the user experience. And what of like, having all of this actions?
00:22:20.010 - 00:23:04.680, Speaker B: Oh, over here, what the user experience looks like is there's a WYSIWYg where you say, or in simple examples for swapping forward, you just present the user with a Dex interface. And all of this happens behind the scenes. I think overall that gets to a really important point, which is I don't think we should expose users to any of this shit. Users should have the same types of experiences that they want on Ethereum, where they see, I'm going to make this trade, I'm going to do this LP position, or I'm going to go lend these assets. And there's a number of different user experiences for building out those interactions. But we don't need to expose any of this to the user. It should all be inferred from the other.
00:23:06.330 - 00:23:10.280, Speaker E: Yeah, yeah, cool.
00:23:10.650 - 00:23:40.218, Speaker B: Yeah. And that's the way it works today. I think the best example of this is TFM. I think it's called TFM Cosmos. This is an NFT DeX aggregator that utilizes a lot of these features in production today. So if you want to go give that a try, you can go try it there. They support packet forward middleware and swapping forward on osmosis.
00:23:40.218 - 00:23:47.970, Speaker B: Once duality launches, that will be the second chain with swapping forward. And I believe neutron is developing their own version of this that conforms to the same API.
00:23:51.850 - 00:24:05.910, Speaker D: So because there are potentially many chains involved in all these different hops, how does the user actually end up specifying their fee and paying their fee for interacting with all of the different chain hops?
00:24:09.070 - 00:24:55.830, Speaker B: The fundamental question here is relayer incentivization, because at the end of the day, the relayers end up paying these fees. Right now, protocols themselves are basically incentivizing relayers. So osmosis pays for all of its relayer connections, and they do this because they don't want end users to have to take on a lot of extra fees or have additional friction in these steps. They want easy user onboarding and they want to pay for that. I believe that model will kind of continue to scale also. The other model that I think is going to continue to scale is apps paying for user interactions on things like this. I just think in order to continue to bring in users, keeping it as simple as possible and keeping fees low is relatively easy.
00:24:55.830 - 00:25:12.320, Speaker B: There's quite a bit of work on relayer incentivization and IBC that is in production today that exists, that is not used, because fundamentally, apps are just paying for this right now. So in the future, there's other ways that it could work and we could talk about that, but, yeah, does that answer the question?
00:25:14.050 - 00:25:35.270, Speaker A: So, Jack, does that mean that if I'm osmosis, for example, and we have a connection, so we have to decide which one of us is paying for it? Is it just like I pay for delivery on my chain, and then if Steven wants to connect to me and I decide I don't want to pay for those connections, does that mean Steven sort of has to set up the relayer infrastructure to pay for those transactions on my chain?
00:25:35.610 - 00:26:04.030, Speaker B: Practically, the way that it happens is one chain ends up paying for bi directional. And on a lot of these things, there's actually multiple relayers running, and each chain might be paying for their own relayer operators. That's the way it works on Noble right now. Noble, like strange love, runs relayers for everywhere Noble is connected. But there are also osmosis relayers running against Noble. There are stargaze relayers running against Noble. There are Kujiro relayers running against Noble.
00:26:04.930 - 00:26:05.486, Speaker A: I see.
00:26:05.508 - 00:26:05.694, Speaker B: Okay.
00:26:05.732 - 00:26:22.098, Speaker A: So that when you said at the beginning, I was sort of having a question about building IBC and then building a business model around that. That essentially comes down to all these different chains are paying for the operation of the relayers directly in order to improve the UX for all of their users. And that is sort of. You guys are operating relayers in order to do that?
00:26:22.264 - 00:27:12.142, Speaker B: That's one thing we do. I think that there's a number of ways to build business models around IBC, and I think it's kind of unclear exactly what those will be in the future. One thing that I've long thought is needed is in the Internet, we have DNS, and at any time where you have a network where untrusted participants can join, you have no way a priority to understand who are the participants in a network. So you need some kind of registry or something to help do service discovery. This is a classic computer science problem. I think that that's a really interesting piece of infrastructure that people can and will pay for, and that's one potential way to get revenues out of IBC. I think building some of our own protocols is something we've already done once with Noble.
00:27:12.142 - 00:27:14.740, Speaker B: We'll continue to. Yeah.
00:27:16.490 - 00:27:30.310, Speaker C: I had a question about the acknowledgment process of rolling back intent. Does that jam the channels that are touched on the way? So it's totally a separate.
00:27:31.630 - 00:28:00.210, Speaker B: This gets to the question of ordered versus unordered channels. So there are two different types of channels, ordered and unordered. ICS 20 is unordered, so there's no blockages possible. Ordered channels are used for interchange accounts so that you can guarantee ordering in your message delivery over on your counterparty chain. And in that one, yes. An acknowledgment could potentially. No, acknowledgments don't block.
00:28:00.210 - 00:28:11.846, Speaker B: They're two separate queues, one going and one coming back. So you could continue to receive packets, but acknowledgments might be blocked coming back. Got it? Yeah.
00:28:11.868 - 00:28:31.310, Speaker C: I was wondering if some complex kind of intent could cross, let's say a bunch of different channels, like high throughput channels between zones. And then the acknowledgments that we're talking about would prevent more intents from being processed in some way if the resulting zone just doesn't respond or waits for the max timeout to respond.
00:28:32.370 - 00:29:03.830, Speaker B: This is one of the problems with the current interchange accounts implementation that folks have flagged, and it's one of the reasons why there's other alternate implementations there was that polytone thing I mentioned. I believe that they run all of those queries or interchange accounts, calls across a single unordered channel to help avoid this problem. And there's other ways. It's basically up to the applications consuming the data on those channels to ensure message ordering.
00:29:04.730 - 00:29:39.586, Speaker C: The other thing is, with the intents, there's no forward look ahead, right? Like you basically have to execute at the next zone to know where the next forward could go based on. Yes, because when you start, you could say, let's say there's some pattern that eventually took a bunch of hops and then eventually routed back to your zone, and then on your zone, you just kind of held it there. It'd be an easy way to just have all these random strings all over the place that you control fully. So that's why I was curious, both the look ahead routing path as well as the blocking characteristics of different things.
00:29:39.608 - 00:30:27.250, Speaker B: I guess the way that I look about it is it's like client determined routing on the client side. You have to go pull data from a bunch of rpcs to kind of understand what's going on out within these multiple chains, and then you put together a transaction and send it out. There's obviously timing issues potentially with that. There's a lot of MeV stuff that's kind of unanswered, but I think that in the app chain model, you have much more power to externalize and control MeV as an app chain, as well as mitigate it. And the fact that there's not a single pipe like on Ethereum, where all the MEV comes out of, creates a very different ecosystem in cosmos. Also, validator identity and slash ability is another huge mitigating factor against Mev.
00:30:27.830 - 00:30:40.310, Speaker C: I guess some of the future work is going into proving that a particular step of an intent was done in the best way possible for that packet. Or is that not really a concern?
00:30:43.770 - 00:31:00.720, Speaker B: I'm not. Personally, I don't give a fuck about that. I think that with this on the swap, it's the exact same arguments that you get with the swap on the chain. So like the bad cases, your swap doesn't execute, you get your money back.
00:31:02.850 - 00:31:49.874, Speaker C: I guess the way I'm thinking about it is from the perspective more of kind of how the NBBO works, where as you route orders through the stock exchange, they have to prove that they routed it to one of the best national organizations of a particular time. Imagine a world where there's 15 osmosis app chains, or DYDX, and then you, for some reason have like a three step path that ends in a Dex. How do you know that the intermediate router didn't pick an exchange or not even that, but maybe they control one and they're willing to route you to one with lower. If you have maybe just a limit order and you'll accept anything above that range. But there's one that has and one that doesn't. And if you don't have control of the path, I guess that's why I was curious.
00:31:50.022 - 00:32:15.106, Speaker B: I think in your example, the next step of the work would be to build an intent router. And I think anoma is talking about that. There's other folks talking about that, but I think an intent router would be a really interesting thing and it would be built on top of this sort of swap and forward thing. But right now the routing happens client side. So like this is what squid does. They have a bunch of data in their back end infrastructure. User says, hey, I want to swap.
00:32:15.106 - 00:32:40.990, Speaker B: And they go, where's the best price? And then they put together a bundle that the user then signs that predetermines the path. In order to have the problem you're talking about, it would need to look a lot like that server example where user sends requests to google.com. Server fans out, server fans back in, returns to user. That pattern is not established yet, and it is interesting future work. Got it.
00:32:41.140 - 00:32:44.480, Speaker C: I asked the last question I had, and sorry to rapid fire here.
00:32:45.570 - 00:32:49.140, Speaker B: I love this style, by the way, so I really appreciate it. Thank you.
00:32:50.470 - 00:33:08.242, Speaker C: I guess one other question was in the order of overhead in this world. So I think one of the most complex parts of not sharding, but I guess in this case, heterogeneous sharding, where you have different applications kind of on different shards, these things being zones.
00:33:08.306 - 00:33:10.920, Speaker B: Maybe it's an asynchronous environment. Yes.
00:33:12.410 - 00:34:01.158, Speaker C: Do you guys track at all the overhead of transactions required to perform what otherwise would be a single transaction if the state was shared? So I saw this post. I've been kind of following along this particular line of thought, which was, hey, back in three years ago, when everyone was talking about sharding, it took five transactions to perform a swap, but that was more scalable because there were like 20 shards or something like that. I'm curious, within the entire framework of the app chain thesis, how much more efficient can it actually get? If every single transaction that would otherwise be performed requires 15 transactions, or like different hops to go all over the place, you could see the aggregate system being generally less efficient because of that?
00:34:01.324 - 00:34:47.960, Speaker B: Yeah, I think that in terms of the amount of compute, it's roughly the same. Because instead of that one transaction that does 15 things, all being computed on a single chain, that computation is broken up into steps. So we're not increasing the amount of compute required. What we are increasing is the amount of storage, because we end up storing that data across multiple chains. I think that we live in a world where block space is roughly infinite, and there isn't a problem with that. Is this less efficient? Strictly, yes. Is the way it is less efficient something that is a fundamental game changer or game stopper for the applications built on top of it? I don't think so.
00:34:47.960 - 00:35:18.750, Speaker B: And will this property of this inhibit the growth of the protocol? Again, I don't think so either, because it offers a ton of user benefits, cheaper fees, access to much more block space than on a single chain. And it offers protocol benefits in terms of scalability and composability. So I think we will move to this future, and I think that the cost is going to be a bunch of extra storage, historical archive nodes across many different chains.
00:35:19.110 - 00:35:33.206, Speaker C: So the theory on your side would say that we're going to add block space and compute capacity greater than the overhead will increase. To process a transaction across those regions would be the way that you would think.
00:35:33.228 - 00:35:38.790, Speaker B: So again, but just to come back, I don't think the compute increases drastically.
00:35:42.330 - 00:35:56.780, Speaker C: This is why I'm saying, I was wondering if you guys had any data on how many hops, like a particular message typically takes to understand one hop right now. Yeah, fair enough.
00:35:57.790 - 00:36:11.038, Speaker B: It'd be interesting to chat with the TFM folks to see what percentage transactions they process that are multi hop, because they make the best multi hop UI. Right now, strangelove is working on one. The skip protocol guys have one in production called IBC.
00:36:11.134 - 00:36:12.178, Speaker D: Fun, I believe.
00:36:12.264 - 00:36:42.022, Speaker C: Yeah. I think some of the results, for example, on sharding, were based on the number of shards. The scale increase you get is like the square root of the number of shards added in the system. It's like if you have 16 shards, your capacity only increases by four, not 16, because of the amount of over. I think if there are multiple, I think that's overhead transactions, it may be less. I think there's an upper.
00:36:42.166 - 00:37:21.250, Speaker B: I don't know. I think those types of calculations get to how many major chains are there going to be in the, you know. Yeah, we're going to live in a world in hundreds, thousands, tens of thousands of chains. But most of those chains are not going to be routed through most of those chains are going to be a single bank that has their own IBC firewall rules that enable them to perform compliance in whatever jurisdiction they're in. And then they're offering this interchange API, and users can move in and out of those things. And those banks will likely have a settlement relationship. I hear that researchers call bridges settlement.
00:37:21.250 - 00:37:38.878, Speaker B: Have a settlement relationship with a couple of chains, and then there'll be this denser core network that helps settle a lot of the exterior stuff. This is how the Internet ended up. This is how all of these kinds of systems end up. Likely this will end up that way. I think we're a ways away from that, though.
00:37:38.964 - 00:38:54.230, Speaker C: I could understand though, by instead of trying to almost share messages on a message by message basis, you almost send these batches of messages that have a proof associated with them. So you don't have to actually verify the signatures, or you can kind of verify that it came from a particular origin instead. And that may greatly reduce the load in a way that you couldn't necessarily do in some other context. Because if shards have minimized validation sets, you'd still have to try and verify, because you don't know. There's a lot more going on there that I think a zone or a subnet may benefit from by not having this kind of stricter architecture that sharding tries to provide. But it's been interesting because we've had a lot of conversations with people saying, oh, are subnets or zones or these things kind of a stepping ground to hopefully figuring out generalized sharding on the world of a near or Tom or things like that? And I'm not trying to make this conversation about whether or not that's the case, but it's interesting to think about it in the framework of what is the work that a multi zone or multi subnet doing, and how would it compare to something on trying to be like a homogeneous, sharding execution surface?
00:38:54.650 - 00:39:56.460, Speaker B: I think that the reason the Internet was designed the way it was is to avoid a single point of failure in the event of a soviet nuclear attack. That's like literally the shortest way to say that if we think about blockchains as aspiring to provide the same level of reliability and consistency as the Internet, we have to think that we need to avoid single points of failure in the system in the entirety. And yes, will there be some parts of the Internet that have a hierarchical organization like that? Yes, I believe the fundamental backbone of this blockchain thing that we're building does need to be peer to peer interaction. And those peers might be huge systems like Polkadot and Avalanche and Ethereum talking to each other, but there does need to be a fundamentally peer to peer layer that connects the whole thing, or else we will introduce single points of failure, even if those single points of failure are huge BFT systems. Yeah.
00:39:57.950 - 00:40:03.670, Speaker E: Do you have any privacy? For the intents, I'm just kind of concerned.
00:40:03.750 - 00:40:36.614, Speaker B: Great question. Yeah. So IBC doesn't necessarily have an opinion on privacy. You could easily have a system where there's an application framework where users chains would encrypt data that's about to go over the wire, and then it would be decrypted over on a counterparty chain. No one has built this. People will build it. As far as privacy within the IBC context, we've been working really closely with the Penumbra team for about a year and a half now.
00:40:36.614 - 00:41:23.220, Speaker B: They're pretty close to launch, and they have a working IBC implementation. They have a fully shielded state pool as well as a transparent state pool. The transparent state pool holds Dex trades and IBC data, essentially so that you can read the commitments from a counterparty chain, and then all of the data about accounts, who owns the coins, all that stuff is private. So there will be places. I mean, secret network today offers privacy, but there will be. And then Anoma has its own shielded pool architecture, which is fairly interesting, and if folks are interested, that's a cool project. But, yeah, I think that right now, no, privacy is a tough part of this story.
00:41:23.220 - 00:41:39.626, Speaker B: There are things coming in the near to immediate future that will help mitigate that, and in the far future, building that encrypted IBC, which is actually what anoma is proposing to do as well. And they've got some initial technical architecture docs on that. That's the solve there. Yeah.
00:41:39.648 - 00:41:45.340, Speaker E: Because it could go very wrong if I know four steps ahead of what you want to do. Right.
00:41:46.450 - 00:41:54.880, Speaker B: Yeah, I know. We've talked to the skip guys pretty extensively about this. They're the team working on MeV and Cosmos, so. Yeah.
00:41:59.250 - 00:42:00.430, Speaker F: Well, the relay.
00:42:02.850 - 00:42:12.360, Speaker B: Okay. Yeah, I had a question about how the packet delivery works, where you have multiple hops and you have to act it all the way in the reverse direction for the packet to be completely sent.
00:42:12.890 - 00:42:15.686, Speaker F: If there is a timeout, is it.
00:42:15.708 - 00:42:22.966, Speaker B: Expected that the packet is retried from the first part of, I guess, of that long chain of nodes, or is.
00:42:22.988 - 00:42:25.146, Speaker F: It kind of checkpointed? And you restart from.
00:42:25.168 - 00:42:30.380, Speaker B: Like, it's checkpointed, you restart from the last failure point. I see. Okay.
00:42:30.910 - 00:42:33.082, Speaker D: Matthew. Oh, sorry.
00:42:33.136 - 00:42:47.680, Speaker F: Yeah, Matt, first I was just going to ask for the relayer transactions, how are they checked so that we know that the transaction is successfully sent to the destination network. And then how are the relayers then paid?
00:42:49.430 - 00:43:09.478, Speaker B: Yeah, so relayer incentivization is a bit of a longer topic, but the way that it works is user commits on one side, relayer brings proof of commitment over to the other side, and that packet is received and then the acknowledgment goes back. Okay.
00:43:09.644 - 00:43:17.962, Speaker F: And are they all first come, first serve basis where they're all trying to relay the same transactions and whoever delivers it first?
00:43:18.016 - 00:43:30.880, Speaker B: Yeah. Okay, thanks. I will note in this model, users are free to relay their own transactions. There's Javascript code for this, users could easily do that.
00:43:31.250 - 00:43:33.310, Speaker D: There's Javascript code for this?
00:43:33.460 - 00:44:16.480, Speaker B: Yeah. No one does it because they don't want to pay the fees, they don't want the additional friction, but a combination of account abstraction and some better ux around using that. As a user, what you would do is you would sign one transaction that delegates permissions to another account and then that other account would then go pay fees on all the chains for you. And from a user perspective, you're again back in this. The desired user experience is I want to do a thing and I only want to say I want to do it once. I don't want to have to checkpoint at each step. And there's a number of ways to kind of get there.
00:44:18.770 - 00:44:36.446, Speaker F: You describe, sorry, just to quickly follow up, I just wanted to clarify so it makes sense that users can relay their own transactions, but the general trend that you've been seeing that's more scalable is that chains tend to pay for their own relayer operators.
00:44:36.638 - 00:45:08.334, Speaker B: I would get even more specific to that and say applications tend to pay for their own relayer operators as we see applications being more on smart contracts as well. And I think somebody mentioned the Mars example where they've got outposts, Mars is relaying all the Mars transactions. They're just doing that themselves because they have a lot of inner protocol traffic that they need to move. Who pays for that? Mars pays for that. So they run their own relayers. This self relaying thing takes care of a lot of the relayer incident problems, I think.
00:45:08.532 - 00:45:11.054, Speaker F: Yeah, that makes sense. Thanks.
00:45:11.252 - 00:45:20.080, Speaker B: Yeah, sam? Yeah, I had a pretty similar question to that answered my, thanks. Cool, great.
00:45:21.810 - 00:45:50.700, Speaker D: So like, I guess my question is more just like on what the baseline spec of IBC is. So I know that we were talking about signed JSON blobs that can have might have this format if you're sending tokens. It might have this other format if you're trying to do this relayed thing. Are these like standards that are agreed upon across Cosmos, or are these more standards that are agreed upon on a chain to chain point?
00:45:52.670 - 00:46:38.330, Speaker B: These are standards that are agreed upon across Cosmos generally and Cosmos IBC. You can see our spec repo here. Go check it out. And there is a section in the spec folder under app ICS 30 is middleware, and it shows which are the current standards around middleware application development and design. I don't think it includes the swap and forward example yet, because we haven't pushed that upstream. But packet forward middleware is covered by this. And basically what I'm showing you today is kind of this in process evolving standard that we're in the middle of standardizing.
00:46:39.390 - 00:47:08.340, Speaker D: So I guess for that obviously can't really be enforced from because you have all these blockchain stuff. But as far as the actual code goes, is that something that I guess, how do people actually generally build their blockchains in IBC? Is there like a base fork repo that they fork off of and then build? Or are they importing a lot from an SDK? Or.
00:47:10.150 - 00:47:36.694, Speaker B: Like most of the initial development was like, we've got this standard SDK app, it comes with all the wires in, a bunch of people deploy it. We're moving into this world with composable IBC being enabled. You guys are going to be the second one. I'm working on a couple of roll up projects. There's anoma and penumbra and a bunch of these non standard IBC things. Basically, we're trying to standardize IBC around two libraries. One is IBCGo and IBCRs.
00:47:36.694 - 00:48:19.494, Speaker B: We believe that this covers the vast majority of blockchain application use cases, and if we have full implementations of the transport layer in both of those, it gets us to where we need to go. I would say the third one of those is a solidity implementation. There are two teams collaborating on an implementation and a user experience for solidity that will be available soon. So all three of those implementations follow the same course, spec repo. And we believe that that's going to be sufficient to cover most of the use cases. But even the Go repo, we're working with the optimism team to spec out what an optimism IBC would look like. We would use the go bindings and expose them as pre compiles to the solidity environment.
00:48:19.494 - 00:48:56.660, Speaker B: So in that way there's even less dependency. I think the only chain that would need the solidity bindings would be eth mainnet. Thanks. Yeah. And yes, open standards, it's a process, and in a PVP environment like blockchains, it's not easy. But one thing that I found is that once you get enough weight behind a particular open standard, it starts to get a life of its own and people just accept it. And that's kind of where IPC has happened.
00:48:56.660 - 00:49:35.640, Speaker B: People are like, okay, what's the standard? And then you're either going to follow the standard, which a bunch of projects have done, or you're going to go build Axelr, and that's kind of where we're at. So what I've seen over the history of computing is that the projects that end up building on open standards have more robust security, have better user experiences over time, and end up paying a lot less cost than software maintenance. And those advantages end up accruing over and over again to sort of encourage coalescence around these open standards. And yeah, there's a lot of work to get there. It's not easy. Yeah, it's not straightforward.
00:49:39.180 - 00:49:46.940, Speaker E: Talking about XLR, how does USDC going to work on Cosmos ecosystem?
00:49:47.360 - 00:49:50.270, Speaker B: We built it. What do you want to know?
00:49:52.240 - 00:49:57.360, Speaker E: All I know is that there's this XLR USDC, but somehow.
00:49:59.380 - 00:50:27.364, Speaker B: There'S a native blockchain in Cosmos called Noble that has signed an agreement with Circle. Circle has already issued the first few hundred dollars of USDC. They're waiting for an official launch for something which I honestly can't fathom. My team built this and I think there's a lot of exciting IBC things we can do with the USDC issuance zone that I'm personally just excited about this packet forward middleware thing and some of the swap stuff we've already integrated. Yeah. How does USDC work in Cosmos?
00:50:27.412 - 00:50:31.850, Speaker E: There's a native issuance unknowable on one of the.
00:50:32.380 - 00:50:33.012, Speaker B: Yep.
00:50:33.076 - 00:50:36.612, Speaker E: And then you IBC to different chains.
00:50:36.756 - 00:50:53.970, Speaker B: Yep. Okay, makes sense. Asset issuance is an application and it either lives in a set of smart contracts on a platform chain or it lives on its own application specific blockchain. Makes sense.
00:51:00.520 - 00:51:19.100, Speaker D: Have you, have you ever seen, so I know you briefly touched on asset fungibility. Have you ever seen agreements across different chains to basically treat assets equally.
00:51:21.040 - 00:51:21.356, Speaker B: As.
00:51:21.378 - 00:51:23.900, Speaker D: A way to get around that additional hop?
00:51:25.940 - 00:51:29.200, Speaker B: I've seen attempts to do this, but they've all fallen apart.
00:51:31.060 - 00:51:34.530, Speaker A: Where do they fall apart exactly? Human problem.
00:51:35.140 - 00:52:26.770, Speaker B: There's different bridge risk in the different denominations, so how can you treat them the same? You're trusting different counterparties. They're fundamentally different. Is this is the sticking point that always shows up. And I think that there is the best effort at treating them this way, Stefan, is the osmosis curve pools where they would have multiple different denominations that came from different places and they would put them all in a single pool and they would be traded one to one, basically. And this is the Dex interoperability solution for this problem. It's been tried out in the wild and honestly no one uses it. People want to do this instead.
00:52:26.770 - 00:52:28.930, Speaker B: I don't know.
00:52:30.180 - 00:52:43.610, Speaker C: Yeah. At least the way I've done it when experimenting has always been to prefix the asset with its origin, I guess in the same way, and there's no other way to avoid the correlated risk of it, otherwise it just turns into a runaway train, I think.
00:52:43.980 - 00:53:02.030, Speaker B: Exactly. And Patrick, that's exactly what we do in IBC. When we move an asset over a channel, we prefix it with the channel information so you immediately know where the asset came from. It's programmatic and it's easy. So I guess the difference move it over additional hops, you just append additional prefixes on it.
00:53:02.560 - 00:53:13.456, Speaker C: I guess the difference is in IBC is that you're prefixing it with the channel identifier rather than the subnet identifier, because the channels are what bring information.
00:53:13.558 - 00:53:16.960, Speaker B: Over channel implies subnet.
00:53:18.600 - 00:53:22.116, Speaker C: But you could have multiple channels from a single zone, right?
00:53:22.218 - 00:53:41.896, Speaker B: Yes you can. Yeah. And technically those assets are all not fungible. I have been through this argument like 15 times. I can't reproduce it right now. But what I can tell you is there's a strong reason why we didn't prefix it with the client, I. E.
00:53:41.896 - 00:53:48.090, Speaker B: The subnet name, as opposed to the channel name. And I don't know exactly what that is.
00:53:50.140 - 00:53:51.384, Speaker C: Hold it up right now.
00:53:51.502 - 00:54:05.520, Speaker D: I'm sure that it's just because you have balance is that are actually tied to the channel. And so if you tried, I think that's it. That only is a problem with lock and mint.
00:54:05.940 - 00:54:06.412, Speaker B: Yes.
00:54:06.486 - 00:54:20.676, Speaker D: If you actually gave the channel the ability to mint the asset completely, then you could probably get away with that. Which is one thing that I think we've been talking about.
00:54:20.698 - 00:54:47.250, Speaker B: One of the biggest issues with that is it fucks with the total supply of the asset on the originating chain. So that when you query the originating chain for the total supply of the asset, you don't get the right number. And to actually get the total supply of a given asset, you have to go to anywhere where it's traveled. Query those balances and bring them back. Whereas if you just escrow and mint, then you can easily query the originating chain for the full total supply. And that was why we went with that design instead.
00:54:48.740 - 00:55:29.470, Speaker D: There's a lot of reasons that I think you need to do a lock and mint if you have two different trust models across different chains, because otherwise you're going to take the minimum. But there were some interesting people that we talked to that were interested in running multiple chains or multiple subnets for sharding purposes, like strictly performance, where they are actually different chains, but the people running them are actually the same and in a permissioned network. And in those cases, having a kind of like whitelisted burn and mint design to get around fungibility is.
00:55:31.280 - 00:56:03.590, Speaker B: Not. You know, I think that this is the beauty of IBC. You can reuse all the same transport pieces and experiment with all these different application protocols fairly easily, and especially when you have control of smart contracts on both sides. You can decide what application protocol goes over the wire. You can invent an arbitrary packet data format and the underlying transport layer doesn't care. It's like, oh, there's a packet, it's got some stuff in it. I'm going to commit it.
00:56:03.590 - 00:56:37.728, Speaker B: And then over on the other side you say, I'm going to receive the packet and it goes to this module for processing. I don't really care what's in it. Relayer software all works with these arbitrary data blobs. All of the IBC software does. So if you want to build that application protocol, all you got to do is build it and make sure one of your counterparties can understand it. And that's how Polytone got developed. Basically the Dowdow team is a bunch of cosmosm devs and they have this DaO software that they have deployed on a bunch of chains.
00:56:37.728 - 00:57:11.804, Speaker B: They need to move data back and forth and they tried doing it using the existing protocols. They were like, this doesn't really fit our use case and we control contracts on both sides, so why don't we just make up our own data formats? And that's what they did. Well gents, as we wrap up at the top of the hour, this has been a blast. So I really appreciate it. Thanks for all the wonderful questions. And if anyone has any feedback for me about the presentation, happy to receive it. Please feel free to reach out to me in dms or other, or just send it to Aaron and Aaron can send it on.
00:57:11.804 - 00:57:15.628, Speaker B: But really appreciate you guys, that was a great audience. Thanks a lot.
00:57:15.794 - 00:57:16.970, Speaker A: Thanks so much for coming. Jack, really.
