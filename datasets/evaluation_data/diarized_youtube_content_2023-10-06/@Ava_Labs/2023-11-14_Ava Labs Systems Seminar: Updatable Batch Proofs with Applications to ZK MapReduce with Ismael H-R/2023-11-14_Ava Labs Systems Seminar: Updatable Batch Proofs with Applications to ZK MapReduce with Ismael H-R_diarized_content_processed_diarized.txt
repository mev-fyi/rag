00:00:09.690 - 00:00:14.714, Speaker A: Today we're joined by Miss Mal, the CEO and co founder of Lagrange Labs.
00:00:14.762 - 00:01:37.030, Speaker B: Which specializes in scaling zero knowledge proofs. Thanks for having me. At Lagrange, we focus on building proving systems that allow us to verify expressive data parallel computation in a Mapreduce format when run on top of commitments to on chain data in the form of block headers. So today we're going to be talking about a recent publication that our team has put forward called rec proofs. So what are rec proofs? Well, rec proofs are a vector commitment scheme that is both updatable in logarithmic time when the set or the subset that vector commitment is run on top of changes. So to start with, we're going to discuss a Merkel tree, which I'm assuming everyone here is familiar with, a commitment to data, or commitment to an ordered sequence of values created through recursive concatenation, hashing of adjacent leafs and then adjacent branch nodes until you get a single root. And so what is the value of a Merkel tree? Well, you can assert commitment or insert inclusion of some element within this ordered set of values by simply proving its inclusion within a root by recursively concatenating and hashing it with its adjacent elements.
00:01:37.030 - 00:02:48.302, Speaker B: So in this situation you can prove the inclusion of a five by recursively concatenating and hashing with everything adjacent to it until you get to the root. And so what about updates? So if you were to update an element in the set that this Merkel tree was a commitment to, you could update any of the inclusion proofs by simply updating the component of their path that intersects with the updated subtree. So in this situation you can see that the component of the path corresponding to a five updates or intersects with the updated element at CR and CR, then becomes CR prime. And so you can update the inclusion proof to include the portion of the path that changes. And so why is this useful? Well, this gives you the opportunity to update this proof whenever there is an alteration to the set. So now let's think of a batch proof. So a batch proof is a single proof that opens K arbitrary elements simultaneously, which has the advantage that it's much smaller than running K individual inclusion proofs.
00:02:48.302 - 00:04:04.758, Speaker B: But the current approaches to this are not updatable efficiently. So you in essence trade the computation time that you save for the lack of updates that you're then able to render. And so now, looking at how you would construct an inefficient batch proof that doesn't have updates, well, very simply, let me skip forward too far, but very simply you would just include all of the overlapping portions of a path such that you would prove that the relevant elements in this batch were included with respect to the root, without having to recompute overlapping aspects of that path. But obviously as some subsets of the Leafs change, then the overall batch proof would have to be recomputed from scratch. So once again, we can talk about this batch proof when done in the snark as something that can't be updated, but the proof size is much smaller. But obviously you can update this if you were to do that outside of a snark, as you could simply just update the portion of the path that changed. So this is where we talk about the work of recproofs.
00:04:04.758 - 00:05:23.698, Speaker B: So what recproofs allows you to do is to construct a batch proof using recursive snarks that can be updated in logarithmic time. That allows you to verify a Mapreduce style computation on top of the dynamic data set, concurrently with proving the inclusion of that data set within a commitment to that data within the Merck word. So what this allows you to do is to both prove inclusion and computation over some subset of elements that can then be updated in logarithmic times. And this gives you all the benefits we traditionally get from being able to run a batch proof in a stark including small batch size and small proof size, but also adds the concept of updatability there. And so we'll also discuss a few applications of this, including things like dynamic BLS aggregations and dynamic digest translations, which are very applicable in the blockchain context. And so now as an overview of rec proofs, we can think of defining a subtree at every branch as the set of leafs that are rooted at that point. In this situation, we can define, for example, Clrl as the subtree containing a four and a five.
00:05:23.698 - 00:07:07.422, Speaker B: We can define Cl as the proof of the subtree containing a two, a four and a five. And by defining this abstraction that we call canonical hashing, which is a subtree rooted at each branch or each branch proof, we're able to then in essence update the portions of those subtrees that change, while keeping the batch proofs over the batch proofs of the subtrees that don't change consistent. And so what this enables us is to have a very large snark that can support both expressive computation and inclusion of some set of leaf elements, without having to recompute all components of that batch proof from scratch, as most of the subproofs will be reused as most of the subtrees have not changed. And so to further this abstraction or further this embodiment, you're able to, in this binary composition of proofs, this PCD composition, you're able to allow map and reduce operations to be superimposed on top of that data that you are showing inclusion of. So you can in essence prove both the inclusion of a subset of the leaves concurrently with proving a computation over that subset by just simply proving the computation subtree as well as the inclusion of the subtree rooted at each branch. And so in this example, you can think of the map proofs as on the leaf level, and you can think of the reduced steps as on the branch levels. And so let's think of the application of this in a blockchain context.
00:07:07.422 - 00:08:11.820, Speaker B: So if you think about the state structure of a blockchain, these are inherently low entropy structures when looking at successive blocks of storage state. So between block b and b plus one, the majority of the storage tree is going to be reused. So there's some very low entropy that results in every given block state based on the execution of transactions contained in that block. But the majority of contract state is the same from block to block. So basically you have a very large set of leaf values with very low entropy, and on a block by block basis the set changes, but the majority of the subsets you'll be computing over will be consistent. And so when we think of an inclusion proof in the blockchain context, you're typically opening a leaf proof of some storage slot within a contract's value. So for example, the NFT balance or the ERC 20 balance is some arbitrary user of some protocol that you can prove was of some certain value at some certain time point.
00:08:11.820 - 00:09:38.126, Speaker B: So we can think of the basic parameters here as the state updates every 12 seconds. In the context of Ethereum, a naive batch proof would force you to update from scratch as every new block comes in. But with rec proofs, you can have very large computations that span over very large windows of blocks that don't have to be recomputed from scratch, where you can use the fact that these are very low entropy state structures and your computation is over a very low entropy data structure as an expediency factor in how you are constructing these proofs. So think about a simple moving average. You want to construct this across a series of blocks from block b to b plus k, where you want to sum up the contract values of, for example, some decks, and you want to divide that by the number of intervals so this would also be thought of as a time weighted moving average. And so in this example you can think of the batch proof as summing up all of the values of the dex price during the reduced steps and in the final step just dividing up by the number of intervals and as something changes. For example, you want this to be a ring buffer, so a sliding average and so a new block is added.
00:09:38.126 - 00:10:48.070, Speaker B: You no longer want to include the data from p zero, you want the new interval to be from p one to p eight, for example. You can simply just update the portion of the path that has changed in that computation and you don't have to recompute from scratch. Standard deviation is another example of this where you would want to execute the same type of computation by summing up across all of the leaf and the branch proofs and then finally doing the requisite division in the final reduce operation. And now we can think of some more fun examples where you can start using SQL as ways to define the computation that runs at each of the map and reduce steps. So think of a gamefi application that wants to compute some predicate over on chain storage. So in the situation you say there's a large amount of contract state that is encoded in a given contract that I can't efficiently query or execute over in the execution environment. So I want to for example run select query over a pudgy penguins contract on Ethereum or some contract in avalanche.
00:10:48.070 - 00:11:38.262, Speaker B: But in the execution environment I can't effectively iterate through every single NFT in that contract whenever a user wants to figure out which penguin has some property. Since this is typically encoded as a mapping, you can think of index then some relevant metadata properties. If I wanted to search by the metadata properties, I would not be able to do that unless I was to iterate through every single memory slot. So in this situation you can think of the ability of proving that final computation as something that can be defined in a map and a reduce operation. And so now to get to another use case that we think has a lot of implications. Aggregated BLS keys that are updatable. So say you have the public keys of n validators that are stored in memory of some smart contract.
00:11:38.262 - 00:13:11.000, Speaker B: This is a very common primitive in things like Eigen layer where you want to encode some avs sets, participants in the contract corresponding to that active validator set, and you then want to compute the aggregate public key of some subset of the validators on demand. In the event that you want to show that they've completed some task with the requisite quorum. So in this situation you want to compute the aggregate public key of some subset s within the set of s validators, and you want to do that subset. The expectation is that subset changes across blocks and the set may change across blocks as well. Well, in this situation you effectively can compute the aggregation in a rec proof where you show both inclusion and then the group operations required to construct the aggregate public key. And whenever there is some alteration of whoever the signer set is, you simply have to update the aggregate public key that you've then computed from storage without having to recompute the entire aggregation from scratch. And you can think about this as applicable for things that are leveraging on chain key stores, which would be anything using Eigen layer ABS, or really anything that's trying to use Ethereum validators in the beacon chain where you want to commit, you want to use a commitment to some set of validators that you've previously computed a public key over, or an aggregate public key over, and then you want to update that with respect to a new signer set.
00:13:11.000 - 00:13:23.790, Speaker B: Now this is digress translation is another application of this that we're very excited about as well. So if you are familiar with the concept of ETL or track transform load in the web two context.
00:13:26.850 - 00:13:27.514, Speaker C: You typically.
00:13:27.562 - 00:14:52.918, Speaker B: Do some computation, some very large computation, over some structured or unstructured set of data. You extract some feature set from that, and you load that into a now secondary data store that you've done some preprocessing or some transformation of the underlying data over. And in the blockchain context, we can think of the storage state, the receipt tree, or the transaction tree of a given chain across, let's say some window of blocks as a very large unstructured data store. And you may in many situations want to compute some predicate on top of that, or extract some feature set from that to further use as input for a secondary zero knowledge system or for some type of function in your execution environment. In this situation, you don't want to have to compute snark unfriendly hash functions like catch act, and you don't want to have to compute large numbers of RLP encodings over this window of data. What you functionally want to do is to be able to take that very large window of data once and prove that it is equivalent to some optimized structure. For example, you want to take the RLP encoded and ketchack hash functions that are being used over some window of blocks, and you want to prove that all of those leafs are equivalent and are contained in the correct order in some different structure.
00:14:52.918 - 00:15:39.510, Speaker B: That is, for example, a binary Merkel tree using Poseidon or the Peterson hash. That is more friendly to do in starks. And so in this situation, you can effectively prove that there is some subset of data contained in some contract that is contained within the state tree. And then as that set changes in a new block, for example, where let's say you want to show ERC 20 balances and you want to construct that into a new data structure, you can simply just update that as the path changes. All right? And so that's the summary of rec proofs in a lot of our applications that we target there. So, any questions? I know I moved.
00:15:41.070 - 00:15:54.960, Speaker D: So if we go back to the BLS aggregation. So I'm interested on at what point that's actually practically more efficient, right? Because it's kind of assuming that.
00:15:56.850 - 00:15:57.214, Speaker B: There.
00:15:57.252 - 00:16:25.842, Speaker D: Is sufficient overlap between the two sets, right? Because otherwise you're essentially providing an entire new proof, right? And also it's assuming that the set is sufficiently large. So I'm just kind of interested on at what point it's actually more efficient to use rec proofs or something like that compared to just computing like an aggregate key on demand.
00:16:25.986 - 00:17:35.374, Speaker B: Yeah, the point is entropy, and I think that's kind of the most important idea with recpruse. So the value of updatability is really important when you have sets of data or large amounts of data that have low entropy, because you can reuse portions of that computation both in terms of inclusion and then in terms of the group operations across incremental runs. If you have a data structure that's very high entropy, so you have an entirely different set of signers every time, an entirely different set, entirely different subset of signers, then there's not an incredibly strong impetus behind using rec proofs. So you can recompute from scratch. In most blockchain applications, the amount of entropy you have in a validator set is quite low, where the number of signers who change per block for Ethereum is less than the number of people who are signing per block. On Beacon, for example, with the number of people who have stopped staking or altering, their stake is less than the number of people who are participating in signing. So compared to the overall data structure, the amount of entropy is low.
00:17:35.374 - 00:18:12.106, Speaker B: But for example, another application you can think of, transaction trees. That's something where rec proofs are not incredibly well suited in their current form, because you don't have a reason really to update your proof if you have new transactions every block. So in the case of an eigen, Larry, AVs, think of like eigenda, you have 150 signers, roughly, I believe, 175, who are mostly consistent from task to task to task. In that situation, the Niagara key is very efficient when you update it. Cool.
00:18:12.288 - 00:18:48.626, Speaker A: Yeah, we use BLS multi signatures on avalanche as well. So that's where some of our interest comes from on that front. Yeah, I think our signing sets right now are low, like much lower on average for what we need. But some of the stuff that we want to do relies on basically doing more stuff on the primary network, which has like 1500 validators. And those BLS keys aren't in the same way, probably with Ethereum not changing too often. So having some better mechanism for maintaining a form of an aggregate public key is certainly interesting research. But that's, I think, some of the context with Stephen's question.
00:18:48.728 - 00:19:41.750, Speaker B: It's a good question. I think it also depends on what you want to do with the aggregate public key. So typically, for the context that we're talking about here, you need to prove that key somewhere that is a computationally constrained environment. So you want to be able to prove that some subset of signers, if you were to aggregate their keys, results in some aggregate keys, so you can check, since they can turn something. If you have, for example, full agency over altering consensus mechanism, this may not be the most useful primitive to you. But if you need to, for example, take avalanche's consensus mechanism, and you want to prove that on a subnet, or you want to prove that elsewhere, this could be useful if you don't have the ability to directly read from consensus from that environment. So, for example, if you wanted to prove avalanche's consensus on a block by block basis on Ethereum, this would be a situation where this type of updatability would be highly useful.
00:19:43.210 - 00:19:51.818, Speaker A: Do you have any benchmarks on how fast you've gotten some of the stuff so far? Like with some of the proving and ZK stuff, I know it's probably something you guys are actively working on.
00:19:51.904 - 00:20:30.280, Speaker B: Yeah. So in simple binary trees, we update about 140 times faster than recomputation for large proofs. We expect, depending on the data structure, this will increase in terms of the amount faster it is, because in essence, the larger the amount of computation you have to expend on recomputation, the more of a benefit you get from rec proofs. So we expect MPT trees to be somewhere in the range of thousands of times faster for large proofs, for updates, for three computation, and I believe avalanche, and correct me, if I'm wrong, you guys are based.
00:20:32.010 - 00:20:34.306, Speaker A: So go ahead, Aaron.
00:20:34.498 - 00:21:01.086, Speaker C: So we have a multi chain architecture. So we have the c chain is our EVM based chain. We also have people with the ability to launch their own subnets or their own blockchains. Basically the most popular VM that they use right now is subnet EVM, which is another EVM chain. And those are all MPT based. Our XP chains are not currently using MPT or any tree structure, but we're planning to switch to using one soon.
00:21:01.268 - 00:21:21.350, Speaker B: Makes sense. Makes sense. So yeah, in that context, if you were trying to run something that's like very large and expressive, because I imagine execution environments, you still have constraints over gas. I know you probably have fewer constraints because of the validator count, but you likely still have high constraints. I'd assume then that's where this type of thing is very useful.
00:21:24.090 - 00:21:46.462, Speaker C: I'm very curious, in the context of ethereum, what type of entropy you see and what type of environment you're actually using. This, if you're talking about are you using this only when things are finalized and then you have the entire validator set that you assume most of it has signed. Are you doing this on a committee by committee basis for things that are not fully finalized yet, but the live tip and where you see the most entropy there.
00:21:46.596 - 00:22:51.550, Speaker B: Yeah, so that's a very good question. So we don't typically use this for ZK bridging. It's a bit outside of our focus area. As a company, what we focus on mostly with this is on computation or storage state, which are very very low entry structures, you can think of the storage tree and I might be off, but I think it's several billion leaves and per block. You're changing about 1000 2000 from our benchmarks. So the amount of alterations you have in the structure compared to what the structure actually contains in terms of breadth is very very low. So if you want to in essence prove that, if you want to have an optimal structure for computing over, let's say transaction treatments like USB 20 balances, so you want to take the USDC contract and you want to take the entire chunk of memory corresponding to where all asset balances are stored, and then you want to be able to compute over that efficiently such that you can have a secondary application that can better enable, let's say, coprocessor based trading.
00:22:51.550 - 00:23:24.990, Speaker B: You can take that entire chunk of memory, prove that it's equivalent to an optimized structure that's a binary tree with Poseidon, and then anyone can compute over that much faster in some secondary system. And then you can just very quickly use an equivalence proof to show that with respect to the most recent block that you want to prove that you've computed the correct data over, you'll just verify the rec proof there. And so you can get around the complexity of having to recompute over and over and over again of the non stark friendly structures, while still being able to use this non stark friendly structures for consensus, which are much faster out of circuit.
00:23:25.410 - 00:23:26.400, Speaker C: Very cool.
00:23:27.090 - 00:24:04.978, Speaker D: Going back a bit to the. So there was a point where you were talking about the equivalence between two different trees, right? Where you have like one is Poseidon, the other one's like say, shaw. In order to actually produce the rec proof of including all those Shaw computations, doesn't that require running Shaw inside of that kind of more expensive Zk prover? So is that slow? Isn't that typically something that people try to avoid?
00:24:05.074 - 00:24:26.000, Speaker B: It is, and that's why the updates are useful, because you can then avoid having to redo it. So you can think of it basically as a setup, and then you have to update based on entropy. So if you have a very large chunk of memory, you have to compute once with ketchak over the entire thing and prove the equivalent Poseidon's. And whenever something changes, you have to update the subtrees that change. But you can do that without having to.
00:24:27.010 - 00:24:41.190, Speaker D: Right, okay, so basically you have this constant factor, and you're basically saying, well, with the shop thing, you have a really high constant factor previously, it's going to be like n log n times that constant, whereas this is like log n essentially.
00:24:42.970 - 00:24:57.686, Speaker B: Yeah, exactly. And you'd have to expense n to recompute from scratch. So another benchmark would be. Sorry. So the runtime would be. You'd have to. The recipe from scratch.
00:24:57.686 - 00:25:10.830, Speaker B: Yeah, there's a cost and the runtime. Right. So the cost of things would be just n for the recomputation. The runtime would just still be logged in with height if you're doing everything recursively.
00:25:14.910 - 00:25:54.246, Speaker C: So another sort of very specific benchmarking question I'm just curious about. I think that the best numbers that I've heard for proving a full Ethereum block is something on the order of like two minutes. I'm sure that's a little bit out of date. It's gotten a little bit faster. But I'm really curious, since if you were talking about this sort of co processor like architecture, where you want to prove the state of only, let's say, the USDC contract and then do some sort of computation based off of that. I'm wondering what is the entropy within those contracts and what is the amount that you can sort of prove out about some subset of the entire block and what is the proving time and verification time for a system that you're trying to use? Maybe the USDC contract for example?
00:25:54.348 - 00:27:06.270, Speaker B: Yeah, that's a very good question. So I think the way we like to think about it is that you have hot and cold portions of the state tree where you have aspects of the state tree that are rarely touched with computation in the execution environment, and then you have portions that are frequently touched. So for example, trader Joe is likely a very hot contract in terms of the amount of volume of going through that, your USDC contract, et cetera. Then you have portions which are test contracts someone deployed three years ago that just aren't going to be very frequently touched. So if you look at for example, the amount of the chain that's actually being frequently touched for applications that people are wanting to build on it, it's a lot smaller than the overall storage tree. And that's why being able to prove the equivalence of that chunk with respect to some optimized structure that you can then further compute over in geo knowledge is a very powerful primitive there because you can start running SQL or the Mapreduce computation that we discussed on top of that. And so I don't have exact benchmarks for the whole USDT tree, for example, but we know for a fact that we're several thousand times faster than if you were to have to recompute that from scratch.
00:27:06.270 - 00:27:25.240, Speaker B: And so when you think of things, for example, like you want a SQL query, select the top five users in terms of USDC balance, you can't effectively actually do that from a contract today because the amount of data you have to iterate through to check something that's not the primary key in the mapping is just too large.
00:27:28.620 - 00:27:29.944, Speaker C: I see, that makes sense.
00:27:30.062 - 00:28:05.410, Speaker B: And the storage proof architectures that people have been talking about as well, they can't do that either. Because you're effectively just looking at a single cell in an excel sheet, for example, you're not really able to do anything that spans a material amount of that data. And then if you do that proof for material amount of the data for one block, you're going to have to redo the whole thing next time. And if you're talking about 100,000 or several million wallet balances in the USDC contract, it's far too large. And we don't have fast enough proven because nobody does right now to be able to block. Cool.
00:28:06.180 - 00:28:46.480, Speaker A: Yeah, I was going to say some of the ways that I've heard other people do this, I think like space and time as well, is they have basically a group of validators that attest the data ingestion. They put that in to a large SQL database and then they create a ZK proof over a query on top of that database. But the way that you described it, with a mapreduce over the tree itself of state, it's like a very different approach, I guess, than that. And I think it's just a very different approach. I think if you have that proof, it seems better to just use the state directly. But I think there's probably some nuance I'm missing on the pros and cons of maybe some of that.
00:28:46.550 - 00:29:41.168, Speaker B: It's technically complex. I think one of the things we frequently see in the space, a lot of teams will create very complicated architectures that I call like decentralization washing, where they'll have the hardest part is more or less insecure, and then a bunch of other parts that are easier are secure. So this is, for example, where you can say, I'm going to build a mechanism that proves ethereum consensus. And by proving ethereum consensus, I'm going to have five nodes that attest to Ethereum consensus with NPC, and I'm just going to check the NPC signature. So I'm not actually proving Ethereum consensus. I'm generating a zero knowledge proof that five NPC nodes have signed something. And so what you effectively do with a lot of these things is people try to add step after step after step after step to the flow of it, to detract from the fact that the hardest part of it has really not been solved yet.
00:29:41.168 - 00:30:04.600, Speaker B: This is not a comic about space and time. I have not looked at their architecture in very much detail and I have heard great things about their team, but this is more of a comic about what we're seeing in space. So effectively, if you have an economic backed mechanism and the ZK backed mechanism, the same product, then the economic backed mechanism, the minimum security, which oftentimes makes the CK mechanism redundant because the entire flow is predicated on the security of the economic mechanism.
00:30:05.920 - 00:30:42.964, Speaker A: Yeah, makes sense, I guess. Based on your learnings of analysis of some of this tri e structure, and know we have our own tri e structure we maintain for our own merkle structure, for the stuff that's not in the EVM. I'm curious if you have any particular insights on what sort of things to do and not to do that make proving better or easier. Because if we want to have a tree structure that we hopefully use everywhere, and then don't have to migrate from later to provide some better proof performance, I'm wondering if there's anything in particular you'd recommend doing or not doing.
00:30:43.082 - 00:31:28.736, Speaker B: Yeah, I wish one of our cryptographers was giving this talk. They have a much better answer than me here. But yeah, I think broadly, there are certain hash functions that are a lot faster to compute out of circuit. Things like shot two, five, six, ketchack. And then there are some that are much faster to compute in circuit beside in Peterson, but typically have more time to compute out of circuit. And so this is kind of the trade off we typically see where people opt for the hash function that makes their consensus faster, rather than the hash function that makes the applications approve over their consensus approve things using their consensus faster. I think there's great work that's being done with new hash functions that are faster out of circuit and faster in circuit.
00:31:28.736 - 00:31:49.630, Speaker B: I think reinforced concrete was one that people have been talking about for a while. And I think there's some new hash functions that are a lot of space that people are working on that will be better there. I don't have a very strong opinion on the soundness of them either. I think when working with different hash functions that are more emergent, it's very important to have a proper assessment of their soundness as well.
00:31:51.600 - 00:31:58.988, Speaker A: So, not much about the tree structure itself, more so just about the hashing function. That seems to be the biggest point.
00:31:59.074 - 00:32:52.800, Speaker B: That's a big point. And then the tristructure. I think you want to make sure that the serialization you use don't have a high overhead to compute in a circuit. I believe SSZ serializations that they use in the beacon chain are a lot faster in circuit than the rlPs. But I imagine there's other ways that one could design that that would be faster as well. And then there's other works you can do by using polynomial equipment instead of just trees, which I think there's some good work to be done there as well. So, in fact, like hyperproofs, some work that Travon Trinvoss and our team did during his phd is an example of a vector commitment that could be designed, for example, as part of a consensus.
00:32:55.860 - 00:33:04.640, Speaker A: Also, take a look at that paper. Thanks for sharing it. Hyperproofs.
00:33:05.560 - 00:33:14.710, Speaker C: I was going to say the same thing. What's the name of that full paper I just googled? Hyperproof and there's a decent number of ads. It's I guess a common reused name.
00:33:15.420 - 00:33:25.130, Speaker B: I will link the paper. Thanks. Cool.
00:33:26.540 - 00:33:38.124, Speaker A: Anyone else have any other questions here? Thank you for answering all the ones we've already asked. It's pretty cool work. I think I saw some of the stuff at SPC. What do you guys presented there, right?
00:33:38.162 - 00:33:42.228, Speaker B: Yeah, we did. That was when the paper is Mel.
00:33:42.264 - 00:33:44.288, Speaker A: Do you have any implementations that we.
00:33:44.294 - 00:34:47.760, Speaker B: Should keep a lookout for? Yeah, we're going to have some things that are coming out later this year, open source some of our circuits, and then we're also going to be having a test event very soon as well. There's some interesting things you can do with very large proofs over transaction trees that are going to unlock a lot of functionality to EVM, we believe, as well as over storage and receipt trees as well. And so we think that this is going to be a very powerful primitive. One of the things we like to say is that we're all in blockchain. So we all fundamentally believe that there will have to be more consumption of block space, there'll have to be more block space, otherwise we're all in the wrong industry if we see less. And the corollary of that is that there will be more data, there'll be more on chain state, there'll be more transactions, there'll be more event logs, and there will need to be mechanisms that as a corollary can actually derive meaningful analysis on top of that. And so a lot of the tools we build are designed to kind of look at when we have that scale.
00:34:47.760 - 00:35:13.740, Speaker B: I think building towards that with the proliferation of l two architectures, proliferation of modular architectures, of the kind of the role of centric future a lot of people in theorem are focusing on. And then I start talking about l three s as well. The question is, we have this now numeraca data that functionally updates itself every 1 second. How do we have computation that spans that can be efficiently verified back in the execution environments?
00:35:16.560 - 00:35:38.852, Speaker A: Yeah, only wish list is on the verifier side for the open source stuff that you also have a rust implementation of it out. I'm assuming that that's the easy one if you already have a solidity verifier out. But a lot of the Wazm stuff we're doing is super compatible with the rust verifier. So if you have that out, it'd be really cool. Not that you need a wish list, but I felt like it'd be fun to ask.
00:35:38.986 - 00:35:47.770, Speaker B: I bet it's I expect that because C chain is MPT based, it shouldn't be.
00:35:48.460 - 00:36:38.504, Speaker A: The funny thing I found is people. Maybe it's just my misunderstanding of what the goal is for. A lot of these sort of really cool proving text is that they don't actually have a rust prover or some sort of native proverb built out. They usually just implement the solidity prover because that's where they think it will be most widely used, even though I would imagine the rust prover to be multiple times faster maybe than the solidity prover. And so when I've always asked, hey, super cool, I know that all the generations in rust do you have maybe just something from your CI you use to verify it that's in rust? Can we just pull that out and hook that up? We don't want the solidity version of it, but I'll keep my eyes tuned on the reboot, see if I can just rip out some stuff.
00:36:38.622 - 00:37:04.336, Speaker B: I'm fairly certain we have a rust prover. I believe the use of solidity provers is solidity verifier. We have a rust prover and we have a rust verifier. The use of solidity verifiers is, I believe a lot of them are just transpiles in circum. And so I think a lot of teams aren't building those. I think they're just getting them spit out by. Got it.
00:37:04.358 - 00:37:08.770, Speaker A: So they implement the verifier in circum and then it just plops out what's needed.
00:37:09.860 - 00:37:47.448, Speaker B: They implement the circuit in circum and then the circum just picked up the verifier. I don't think Ganark does that right now, but I'm not certain. Actually, no, I do believe Ganark does that, and I think Halo two does it, too. A lot of the recursive and folding schemes are not doing that. So I think there's some folks working on Ganark verifiers, sorry, on plonky, two verifiers in circum and Ganark. And then I think that there are some teams that are working on Nova verifiers or for to have the final proof of Nova to be something that can be verified on chain. So I'm like moving from Spartan to a graph.
00:37:47.448 - 00:37:48.540, Speaker B: 16 proof.
00:37:52.260 - 00:37:56.324, Speaker A: Yeah. I was going to spend the time to dig into it, but, yeah, very cool stuff.
00:37:56.442 - 00:37:58.980, Speaker B: Do you guys have a ZK on the roadmap anywhere?
00:37:59.480 - 00:38:39.200, Speaker A: So our approach. Great question. Our opinion, at least maybe my opinion on this is if you don't have one of the best ten people in the world in ZK. It's better to work with one of the best ten people in the world rather than try and hire someone and like, hey, let's learn ZK together and go hook it up. So we don't have one of the TED best people in the world. We focus more on our consensus and engineering research teams, and that's where we spend our resources. And so we're very open to finding the right group to work with in the ZK space and then finding ways to deploy their technology somewhere.
00:38:39.200 - 00:39:27.056, Speaker A: And from what we've seen so far, that seems like a pretty good meeting point because a lot of the ZK teams don't have anything to do with consensus, nor do they want to have their own network. They just want to find a way to get their technology more widely deployed. Most of them are starting on Ethereum, which makes sense, but I think we offer a pretty cool alternative outside of Ethereum, where the techniques themselves can maybe be more widely deployed or widely utilized as part of the core chain mechanism. So we've had a few conversations with different folks about that and we're setting up some stuff right now that would be good to just take ZK stuff off the shelf and then just hopefully just put it up on chain outside of the EVM. But no, we don't have our own ZK effort. We are instead trying to find the right group of people to work with on stuff.
00:39:27.078 - 00:39:32.784, Speaker B: Like, makes sense. That makes definitely, definitely something we'd love to engage with you guys with.
00:39:32.982 - 00:39:44.040, Speaker A: Yeah, maybe I'm a pessimist, but I've talked to some people in this UK space and I'm like, this is going to take a while to build this context, including doing like an eight year PhD or some shit.
00:39:48.220 - 00:40:01.310, Speaker B: They're very complicated engineering problems that I think a lot of teams are working on. And I think it's very difficult always to make sure that the teams that are doing it are the ones who have the capacity to do it themselves as well.
00:40:02.000 - 00:40:49.996, Speaker A: Yeah, I think the fear is like the flaw space of security faults here is not, in my opinion, it's still pretty nascent in its exploration pattern. And I think that my fear of just plugging stuff up that's like all the libraries that people use is like you're just going to have a gaping hole somewhere in the middle of whatever you're doing. And even if you get it to work, it's not going to be robust even if you make it fast. I think working with folks like you or other groups is something that we're super interested in doing and what we've told people. It's basically just like, do your thing and whatever you're going to do, and then as soon as you have open source stuff out there, hopefully some sort of verifier, then we can hook it up to our stuff and see what.
00:40:50.018 - 00:41:34.970, Speaker B: We can do with it. Yeah, there's, I think, a lot of design decisions that ZK teams are taking now that I think are trying to create barriers against issues with circuits approving systems. And I think that is a very early design space, like multiproover architectures or the things people are doing with whitelisted provers, whether or not you believe in that or not, I think those are all kind of indications that there's sustained concern among many of the teams of whether or not there's in fact sufficient soundness with the circuits and then the underlying proven system they're using. So I think it's something where people should be very cognizant and really only use the stuff that they feel they have a firm grasp on. Yeah.
00:41:35.760 - 00:41:41.310, Speaker A: Which brings us back to let us know when you got something fun to play with. We're happy to hook it.
00:41:45.040 - 00:41:45.790, Speaker B: Awesome.
00:41:48.080 - 00:41:48.828, Speaker A: Cool.
00:41:48.994 - 00:41:52.224, Speaker B: Well, thanks for coming us mail. I just sent the Uber Eats code.
00:41:52.262 - 00:41:53.596, Speaker A: As well, so feel free to grab.
00:41:53.628 - 00:42:00.050, Speaker B: The lunch on us. Oh, very nice. I didn't even know I was going to get an Uber Eats card from this. Thank you, guys.
00:42:03.860 - 00:42:04.930, Speaker A: This is awesome.
00:42:05.540 - 00:42:07.810, Speaker B: Awesome. Well, thanks, everyone.
00:42:08.820 - 00:42:11.910, Speaker A: Yeah, thank you so much. Appreciate you taking the time. Thanks. It.
