00:00:10.490 - 00:00:39.180, Speaker A: Hey, everybody, thank you so much for joining us. And thanks, Frederick, for coming to join us today. So Frederick has been an ambassador of the open source monitoring for almost a decade. He served as the Kubernetes tech lead for SIG instrumentation. He's currently a maintainer for Prometheus, Thanos, Kube state metrics, Prometheus operator, and Kube Prometheus, just to name a few, and most recently started a new company called Polaris Signals, where he's the architect of the open source project Parsa. So really looking forward to having you today, and thanks for joining us.
00:00:39.870 - 00:01:47.230, Speaker B: Yeah, again, thanks for having me. So, yeah, before we kind of dive into it, just to make sure that we're all on the same page, let's quickly talk about profiling, just in case there's a single person on the call who hasn't dealt with it. So profiling is kind of as old as programming itself, and profiling is really about understanding where resources are being spent down to the line number. So this can be where is cpu time being spent, where is memory being spent, where are allocations happening, where does file I o originate from? All of these things we can measure with profiling. And like I said, this is something that we kind of always needed to do. Maybe in the beginning of computing history, maybe even more than now. Now we have very, very powerful machines, and for decades, we kind of didn't really have to think about performance a whole lot.
00:01:47.230 - 00:02:34.880, Speaker B: That's kind of changing now, right? Like, we're kind of starting to get to the physical limits of how small we can make a transistor. That's a whole different story. But basically, it's always been important that when we needed to optimize code, we needed to understand where are these resources being spent so that we can do something about that. So, like we've already heard in the intro, this is kind of me in 2018 2019. I basically was working on all things observability in the open source world. I kind of still am. But then I was very much focused on the Prometheus world and the Kubernetes world and everything that combines the two.
00:02:34.880 - 00:03:26.014, Speaker B: Among other things, I was also working on the Prometheus storage. And so basically my whole life was profiling software, right? Profiling kubernetes, profiling Prometheus, and kind of trying to squeeze as much performance out of it as you all probably are very well aware. Everybody has very unrealistic expectations on infrastructure, that it has zero latency and uses zero resources. Right. Of course, completely unrealistic. But that's kind of what users expect. And so my life was basically, and I understand you have a lot of go code, so maybe you've interacted with these go profilers before.
00:03:26.014 - 00:04:32.514, Speaker B: I was using these profilers on an ad hoc basis basically every day to try and understand where are our resources spent so that we can improve this over. You know, I was working on Prometheus and a logging stack and distributed tracing, among other things. I was actually architect for all things observability at Red Hat at the time. That's actually where Sam and, and, you know, we had these basically state of the art observability tooling. And here was manually profiling the software that we are in charge of every single day. Right? And then I read this paper that Google had published a couple of years before that, where they were describing how they at Google were continuously profiling their entire production infrastructure all the time, all of their data centers, and all of the amazing kind of benefits that they were getting out of this. And that's kind of when it dawned on me that profiling is observability just like everything else.
00:04:32.514 - 00:05:30.502, Speaker B: Just like metrics, just like logging, just like distributed tracing. And that's kind of when I realized, in order for that to be reality, we actually need to treat it the same way. We need to systematically collect profiling data, and we need to systematically make it possible to kind of real time query this data. Just like we've kind of gone from metrics, right? Where in the 2000s we had nagios that did like individual checks to see if our infrastructure was working. We kind of went to metrics where we measured individual things and then did alerting based on the time series data. So I felt like continuous profiling was kind of that same evolution for profiling, right? And so I read this paper and I was very excited about this. And I was building observability software for several years at this point.
00:05:30.502 - 00:06:15.960, Speaker B: So I felt like, if anybody, I can build this for myself. And so the first kind of open source project that came out of this was very creatively named Con. Prof. Continuous profiling, right? And this I kind of published, and I actually got incredibly lucky and got to present this as part of an invited keynote at Kubecon. But as you can see, this was just like a terrible piece of software, right. From an engineering perspective, this just barely compiled very bad user experience and so on. But the thing was, this was providing value very quickly to the teams at redhead that I was working within.
00:06:15.960 - 00:07:04.270, Speaker B: And so I kind of figured eventually there's something here, and we figured out this is not only for improving performance, but we can understand where memory leaks are happening basically at glance, right, we already have this data. We don't need to search for the needle in the haystack. We can understand incidents better. We always have the profiling data available whenever we want to improve anything. And we can get all the typical things that we already know we can get out of profiling, but so many things more that fundamentally weren't possible before. But everything kind of boils down to, we can actually understand how code behaves down to the line number. And this is incredibly powerful.
00:07:04.270 - 00:08:25.940, Speaker B: And so once I started kind of playing around with these things and realized how much value there was here, I felt like this was the time to pursue this more seriously. And I kind of went out and asked a bunch of companies that potentially could have used a product like this and asked, what do you want from a product like this? Right? Like kind of typical market research and this kind of informed what kind of technology we ended up needing to build. And the first one was, and there's certainly an element of bias here, right, like given my background, but basically what we heard over and over was that people didn't want to learn a new tool. They want to have Prometheus, but for profiling. So that whenever they introduce this to someone new, they kind of already naturally understand how to use this. So before we kind of go into the nitty gritty details of how to store this data, let's first have a quick look at how profiling data actually looks like. So let's take this very simple go program.
00:08:25.940 - 00:09:47.870, Speaker B: On the left here, we have a main function that just calls up to two functions, one that does a billion iterations and another one that does 10 billion iterations. This is just to produce some amount of cpu time, right? And in its simplest form, what we have on the right hand side is what profiling data looks like, a stack trace and a number. So in this case, because we iterated 10 billion times in iterate long and 1 billion times, in the short iteration, we get 20. This is obviously made up numbers, but the ratio is roughly the same. Right? So that's basically the crux of what profiling data looks like. So, putting this into a table, roughly what we have are stack traces and values, right? I've only shown you cpu profiling data here, but again, this could be anything, any resource that we could measure and associate to a stack trace. It can be memory allocations, it can be amount of memory that is held by functions and were originally allocated by some stack anything, right? Like anything that we can think of.
00:09:47.870 - 00:10:47.170, Speaker B: So if we think of this as kind of time series data, and if we, let's say, just kind of threw this into prometheus, this is vaguely what it would look like, right? The problem with stack traces and representing them as time series is very contrary to time series. Time series tend to be long lived, right. Stack traces, however, are kind of almost randomly appearing or not, right. It depends on whether a user is doing the interaction that causes this stack trace to be executed. Profilers are sampling profilers, so we only look at the stack traces, the current stack trace, let's say, 100 times per second. So there's a chance that in this profiling iteration, we may not have caught it. So there are a whole lot of reasons why stack traces may appear and disappear over time.
00:10:47.170 - 00:12:20.610, Speaker B: So the classic kind of Prometheus storage just doesn't fundamentally can't work out here. And just in case there's anybody who's not familiar with how Prometheus works and how Prometheus querying works vaguely, this is what it looks like, right? You say, what type of data is it that you're looking for in this case? I've adapted it to profiling data already, so I'm requesting cpu profiling data to be shown, and then you can have arbitrary label sets that you can use to slice and dice. The data, however, is useful to you, right? And the important part here in the Prometheus world is also, it is up to you what labels you add to this, so you can make this tool your own, right? The tool doesn't dictate the way the dimensions that are available. It's whatever is useful to you. And however your organizations happen to organize yourself, right? That can be data centers, that can be regions, that can be, again, whatever is useful to you, the tool shouldn't be putting this opinion on you. You configure it to be however your processes are. So just because I think it's a little bit easier to understand this kind of multidimensional query model in an interactive demo than just me telling you, let's quickly pull this up.
00:12:20.610 - 00:13:23.020, Speaker B: So what I'm showing you here is a continuous profiling environment where I'm continuously profiling the whole infrastructure. And what we can do is we can kind of filter this down to individual nodes, for example. So now we see everything that's happening on this node. And what's interesting about that is we could see, for example, the interactions between the different processes, maybe because what we're also getting down here are like kernel stacks. Now we don't see any, but fundamentally we do collect kernel stacks. So we could see if one process keeps trashing the l, one cache of another process or something like that, right? So if this is something that's useful to look at, you can do that, but you could also say, okay, I'm only interested in everything that's in the API namespace or something like that. And you can pull that up.
00:13:23.020 - 00:15:35.628, Speaker B: The point is, whatever is useful to you is something that you can map into this tool and you can slice and dice it, however, is useful. So going back to the slides, additionally, apart from just creating that query model in the first place, what's also hard about all of this is continuous profiling is a lot of data. So how can we very efficiently store this data so that it's cheap in storage as well as very efficient to be retrieved? So I'm not going to tell you about all the experiments that we ran, but basically where we ended up is that whatever we're going to create or whatever we're going to end up with has to be at least look like a columnar database because it is the only thing that kind of gives us the ability to query data at the low latency that we want, as well as give us the compression that we need to store it in an efficient way. Because logically, the way that continuous profiling data now looks like is we have these individual columns that represent our metadata, these labels that we can slice and dice data by our stack traces like we've seen before, even in individual profiling roundups. And then of course timestamps and the values. Now the ideal thing that we were looking for is to be able to kind of exploit this locality and this locality of data within rows, so that we could essentially just say, okay, we know all these rows are going to contain this same value, essentially run length and code this. And this is something that plenty of databases out there can do.
00:15:35.628 - 00:16:46.340, Speaker B: But the problem with existing databases that we had found was that the label model that we were looking to implement is not something that out of the box existing Kotlin or databases could do. The best that we could find were things like were map like data types where we could store the label set in itself. And this is very efficient to store. But the problem is how can we efficiently query this? We don't have this highly consecutively laid out in memory or on disk data so that we could quickly search through all the possible values of a column. For example, we would actually have to decode the whole map every time that we are searching for something. So while this would be very efficient to store, it wouldn't give us the characteristics to efficiently query it. So unfortunately, and we were trying very hard not to do this, we ended up building a database.
00:16:46.340 - 00:17:45.850, Speaker B: So basically we kind of went back and tried to figure out what are all the properties that we wanted, right. And what is the thing that makes this special. And ultimately it's something that some other databases call white columns, where partially there is a static schema in the database. So that's our stack trace, our timestamp, our value column. But the label columns, they essentially dynamically get created as we see a label key for the first time, basically. And this feature we call dynamic columns, they essentially dynamically expand as we see new keys. So now we can actually lay this data out logically the way that we want, and we can optimize it in the way that we want.
00:17:45.850 - 00:19:03.904, Speaker B: However, to maximally exploit this kind of continuity of data, to compress it very very well, we needed to sort this data additionally globally, right? So that we can do the run length encoding as much as possible. There are other encodings involved here as well. I think just run length encoding is one that's very visually well representable. But how do we do this, right, like how do we actually maintain global sorting? And on a high level, this is how it works. The detail, of course, is a little bit more complicated, but in essence, the way it works is we say we're only maintaining a sparse index, so we kind of don't have a full index of where all the values are. But because everything is globally sorted, we just need to know where within the globally sorted data we need to jump to. And then we decompress a whole block, and then we can scan that whole block, right? And the sparse index essentially tells us the first place where we need to jump to.
00:19:03.904 - 00:19:43.948, Speaker B: However, in order for all of this to work, we need to have the sparse index basically says we are jumping every couple of rows, right, or whatever the scheme is that's being defined. In this case, I've made it pretty simple to understand. We have what we call a granule size. This is essentially the chunk of continuous data. And when we insert a new piece of data in this case, let's go back one more time to see what happened here. Before. In this granule we had the numbers five, seven, eight and nine, and now we're trying to insert the number six.
00:19:43.948 - 00:21:14.168, Speaker B: And while I'm just using a single column here with numbers, this can theoretically work with any data type that's sortable and any amount of columns that are sortable. In this case, we're inserting the number six, but we've exceeded the kind of sparse set that we previously defined of the granule size can at most be four large. And so at this point, we need to split the granule, insert a new item into our sparse index so that we can still kind of maintain these kind of quick jumps from the sparse index into the globally sorted table. But now we're kind of maintaining that this is both maintaining the sparse index as well as maintaining the global sorting. In reality, these granules are actually not necessarily eagerly sorted. This may happen at compaction time in the background, but for the purpose of going through this presentation, that's kind of where I kept it. And the last thing, because fundamentally, in observability, the thing that writes data isn't really the same entity as the one that's reading data.
00:21:14.168 - 00:22:25.090, Speaker B: So what I mean by that is servers generate data, right? Servers generate profiling data. Or in the case of Prometheus, servers generate metrics data, and it's humans or alerting mechanisms that ultimately read the data. So it's not the same entity that's writing and reading the data. So this allowed us to kind of be a little bit more loose about our consistency requirements, more specifically about not necessarily having to have read after write consistency. So that means that if you've written something and written that successfully and you do a read after that, you may not immediately get that data returned. However, we essentially release transactions in batches, so that we can insert data in very, very high volumes and then in batches, make them available to be read. And in reality, this means hundreds of milliseconds of latency, so that for humans, this still feels real time.
00:22:25.090 - 00:23:52.812, Speaker B: All right, so now we're at the point where, all right, we have created a Prometheus style experience where we can have these arbitrary label dimensions that we can slice and dice data by. We can search data very quickly because of the column nature, and we can aggregate and do all the calculations that we want at very low latency. Now, the next thing that we heard a lot is, doesn't profiling cause a lot of overhead on my applications and sort of in the same realm, and I say this in the same realm because the solution ended up being the same one. Was, is this going to be as expensive and as time consuming as implementing distributed tracing at the time, in 2020, distributed tracing was still relatively new and quite the hype. And those that implemented it at the time were a little bit hurt from the months of engineering time that they had put into it. And the products were also pretty expensive to purchase. So we kind of went and asked ourselves, how far down can we drive? The implementation overhead and ultimately the runtime overhead as well.
00:23:52.812 - 00:24:45.390, Speaker B: Those were the things that people were concerned about. So this is where we kind of accidentally stumbled into the EBPF world. If you're not familiar with EBPF yet, no worries, I'm going to explain everything. But if you are, you probably already know where this is headed. But originally, because we came from the go world much like you are, we felt like the collection of profiling data wasn't something that we needed to solve. Go has incredibly good profiling support built into the runtime, but it turned out this is not really universally true. So we felt like, okay, we need to go back to the drawing board, we actually need to solve collection as well.
00:24:45.390 - 00:25:37.260, Speaker B: So this is how we got to EBPF. EBPF is the way that you can think about it is you can dynamically load new code into the Linux kernel to be executed by some triggers. So let's take a very simple example here. So one thing that you could do is that you write an EBPF program that gets executed every time the exec ve syscall is called by some program on the host. Then you can do something with that information. You can generate metrics from your EBPF program that this syscall was called or whatever is available to you within that context from the kernel you can use. Right.
00:25:37.260 - 00:26:53.092, Speaker B: And then the Linux kernel, after calling out to your EBPF program, continues as usual. Again, you can kind of attach these EBPF programs to all sorts of predefined hooks that can be existing syscalls, kernel functions, trace points, network events. Or you can define some custom events as well using k probes or upropes, where upres are. Essentially you're saying you want to have your EPPF program called every time this function from my user space program is being called, right. That way you could, for example, create metrics without doing any instrumentation of your go code using EVPF. And ultimately the thing that we happen to use are perf events. And the way that these work in the kernel is that you say, okay, call my EBPF program every x amount of cpu cycles and then continue, right? And what the EBPF program does is it essentially records the stack trace that we're currently seeing.
00:26:53.092 - 00:27:44.100, Speaker B: But we'll dive into that a little bit deeper in a bit. So let's first cover the rest of EBPF programs. The way that EVPF programs communicate with user space is using something called EBPF maps or BPF maps, like we just saw. We have these hooks that end up calling our EBPF program. And the EBPF program can write into these maps and then you can also make these maps accessible from your user space program that can then read the content from these maps. And that's ultimately how these two can communicate with each other. Now, something that I think because EBPF, if you're not very immediately working with it, it sometimes feels a little bit like magic.
00:27:44.100 - 00:28:19.724, Speaker B: I just want to very quickly clarify that there's nothing magic about EBPF. Right. EBPF is written typically, EBPF programs are typically written in c. And this is how you would compile c program, right. And an EBPF program is basically just the same. You're just saying you're targeting BPF instead of some specific Linux or architecture. What this does is it compiles to EBPF bytecode.
00:28:19.724 - 00:29:47.596, Speaker B: And this EBPF bytecode is what then a user space program can ask the kernel to load and then ultimately attach to these hooks that we were talking about before. And at this point, if you're not already familiar with EBPF, you maybe are asking yourself, and you're rightfully asking yourself, this, is this safe to do? Right, like we're basically loading arbitrary code into our Linux kernel, not just running as root, but actually running within the kernel, right? Like this is more than root. And the good news here is this is obviously something that the Linux kernel engineers have thought very long and hard about. And something that they've basically done is the EBPF is basically sandboxed, so it can only do certain operations. And most importantly, the code when you're loading it is verified using the EBPF verifier. And what this does is it essentially determines whether your EBPF program is going to halt. If you kind of think back to computer science classes, this is essentially solving a subset of the halting problem.
00:29:47.596 - 00:30:44.400, Speaker B: Because the way that they solve this is they say you cannot have any unbounded loops, you cannot have arbitrary jumps. So like go to statements or something. Everything has to kind of is basically unrolled and is verifiably going to terminate. And this obviously makes writing EVPF programs pretty difficult. And there have definitely been fights with the EVPF verifier to make it happy and accept EVPF code that you're writing. But this is ultimately what makes it safe to do this, it's sandboxed and it's guaranteed to halt. And it's not just guaranteed to halt, but guaranteed to halt in a certain amount of instructions.
00:30:44.400 - 00:32:01.732, Speaker B: So this is essentially then the architecture of the EVPF program that we happen to use for profiling. So like I said, we create a perf event, or we register a perf event where we say per cpu wake up, or call our EVPF program after x amount of cycles. And the kernel basically makes sure that it calculates the correct amount of cycles. We just say at most, call our program 100 times per second. If truly 100% of our cpu is saturated, the number is actually not 100 times per second, but 100 is just something that's very easy to calculate with, because ten samples at 100 times per second would mean 100 milliseconds of cpu time, right? So just kind of trying to simplify a little bit here, but the EBPF program then gets executed. What we do is we look at what is the current function, call Stack. Then we write that function, call Stack, into our EVPF map.
00:32:01.732 - 00:32:58.888, Speaker B: And if we've never seen this stack before, we insert it also into a counts map and then say, okay, we've seen this for the first time, but if we see it for a second time, we just count these counters up, right? And that's how we ultimately determine how often have we seen the same stack trace over a ten second period of time. Because every 10 seconds, we then take all the content in these EVPF maps, then delete everything in these maps. And that's essentially one profile over a ten second period of time on our host. And then we have profiling data. But let's take an example of calculating Fibonacci using Go. The data that we would get from a program like this looks something like this. So we have the program counters.
00:32:58.888 - 00:33:51.624, Speaker B: These are basically the offsets to our code within our go binary, because it's just Fibonacci. Calling Fibonacci, we see the same memory address multiple times, right, calling itself. And then ultimately at the very bottom of the stack, we have our main function, because that's what we started with. This is just an example stack, right. The thing though is when our EBPF program actually gets started, all we have is this very top address, what we call the RSP register, the instruction pointer. Sorry, the stacked pointer. So all we have is the stack pointer.
00:33:51.624 - 00:34:45.100, Speaker B: From there, we actually need to walk this stack in order to understand what is actually the current function, call stack. And if we're lucky, and this is the case in go. We have something called frame pointers. So in that case there's a special cpu register that's reserved for always telling us what is the next lower frame. Right? Where does the next lower frame begin? And this is basically we can then use to make one jump, and then we can use the RBP register again to do the next jump and so on. And that basically just means that using frame pointers for unwinding purposes is just walking a linked list, right? Like we have the frame pointer that points to the next one, that points to the next one and so on. So we're just walking a linked list.
00:34:45.100 - 00:36:08.168, Speaker B: Very simple to do, very cheap to do, because remember, stacks are basically continuous amounts of memory on our hardware, so it's very fast to access and to walk. So that's good, right? For profiling purposes we want the lowest amount of overhead that we could possibly achieve. Now the problem is, like I said, go is basically the only compiler that guarantees us that frame pointers are present. All other common compilers out there have the option to turn them off and as a matter of fact also default to having frame pointers off. In my opinion, this is kind of a shame. And at the hyperscalers at Google at meta, we most of the time hear that frame pointers are on by default, because it's just a thousand times better to have debuggable and cheaply debuggable binaries than squeezing out a single register that can be used as a general purpose register instead of four frame pointers. However, this is the world we live in, so we have to deal with it.
00:36:08.168 - 00:36:53.968, Speaker B: So how do we support languages or binaries that don't have frame pointers present? And this is something that we worked on for a very long time. Basically there are these tables in the X 86 ABI. So if these tables are not present, you're basically, or what the X 86 ABI specifies is that the execution of your program is undefined. So I highly recommend having these tables. These are the exception handling tables. That's where they originally came from. Exception handling tables from c, but rust has them as well.
00:36:53.968 - 00:37:39.540, Speaker B: Basically everything has these tables. These tables basically tell us how can we unwind a stack without frame pointers. And long story short is when we are somewhere on the stack, we can do a lookup in this table that tells us what is the calculation that we need to do to do the next jump. And we do that n times until we've fully unwinded the whole stack. And again we have the full function call stack. This was very simplified version of this. Basically, in order to make this work with EBPF, to also make the verifier happy, we needed to do a lot of work.
00:37:39.540 - 00:39:04.450, Speaker B: We basically needed to take these unwind tables, optimize the hell out of them, and then also write a very performant unwinder using this table in EBPF so that the verifier is actually happy with executing this. If you're interested in even more detail about all this, I highly recommend you watching this talk by some of my colleagues that they presented at the Linux plumber conference in 2022, where they described in very much detail how this dwarf unwinder works. So that's the kind of name for these exception handling tables. So now, kind of going back, we have created the experience, the query and storage experience that people actually wanted from a continuous profiling project. And we've delivered an EBPF based profiler that has less than 1% of overhead after we've done all this work. So now we can truly always on profile production infrastructure at negligible overhead. As a matter of fact, I say less than 1% most of the time, it's way less than 1%.
00:39:04.450 - 00:39:56.092, Speaker B: And our customers and open source users, like we mentioned in the very beginning, everything that I talked about today is not just obviously we are a company, we're selling a product around this, but it's also available as part of the Parka open source project. Parca. But yeah, we've yet to encounter any customer that can even measure this overhead. This overhead. We've only basically come up with synthetic benchmarks, so most people can't even measure the overhead, which is great, right? This kind of creates that kind of trust that we need in order for people to run this in production. Always on. So yeah, this is how we do profiling in modern computing.
00:39:56.092 - 00:40:10.980, Speaker B: And thank you for listening. If you have any questions, I'll be around. So, any questions?
00:40:14.150 - 00:40:48.430, Speaker C: I have a question. I'm curious, like in the case of avalanche go, as far as I understand, we do enable profiling for nodes in the network and it's Golang level. So I'm curious, how would it compare continuous profiling with Parka versus just using bog standard Golang profiling? Is it more efficient or is it just that you can actually scale it to multiple processes and have a cohesive view of all of your nodes?
00:40:50.150 - 00:41:42.450, Speaker B: Yeah, definitely makes sense. And one thing that I want to say is the storage backend that we've created, actually you can write anything that's in the PPRF format to it. So even if there was something like the go routine profiler, for example. Of course that's not a concept that's generally applicable to any program out there, so we can't really map it to EVPF. So it would still make sense to write go routine profiling data, for example, to this storage. But in terms of cpu profiling data, you definitely want to use something like this EVPF based profiler, not just because it's lower overhead, it certainly is lower overhead. But we can show you the kernel stack trace as well.
00:41:42.450 - 00:42:29.762, Speaker B: So if you're doing a syscall, you can actually understand why this syscall is taking more time than maybe you expect we can. Additionally, I don't know if this is the case for you, but if you have sego extensions, for example, if you use the Pprof profilers that are built into the runtime, they end at the CGO boundary, right? You can't actually see what's happening in the C code, which makes sense because the go runtime fundamentally requires frame pointers for unwinding itself, so it can't make any assumptions about something that's Sigo. But long story short is you can actually also see whatever sigo you're calling into. Okay.
00:42:29.816 - 00:42:39.140, Speaker C: So the summary gives you greater visibility and potentially just means for aggregating all that data in a useful form.
00:42:39.510 - 00:43:14.270, Speaker B: Okay, exactly. Cool. Hi Frederick. I'm someone who also came from Red Hat and did a lot of work on operators. I'm interested in the sort of overhead you mentioned with continuous profiling. Like, let's say you want to profile something in a Kubernetes cluster, and you want to use this EBPF based solution. Do you distribute it as an operator, or do you do the configuration on the node? And so in general, I appreciate your thoughts on operators and the overhead involved in that type of solution.
00:43:14.270 - 00:43:53.020, Speaker B: Yeah, great question. So EBPF fundamentally works on a host level. So the way that this is deployed is you deploy it as a daemon set into your cluster. So I think there's a time and place for Kubernetes operators. Actually, fun fact, I was part of the team at Coreos that created, literally the very first operator and created that work. But I think in this case it's actually not necessary because all we need is to deploy an agent to every host in the cluster. So it's just a plain old daemon set in this case.
00:43:53.020 - 00:44:37.590, Speaker B: Okay, cool. Thanks, Frederick. I think we had A-Q-A question about other language support. Like, you'd likely mentioned rust. Maybe you want to dive into that a little bit, yeah, definitely. So basically what I described here today is anything that compiles to a native binary, right? So c, c, rust, go, Haskell, all of these can basically be covered with all of the techniques that I've gone over today. There are a couple of special cases that we need to look at for some other languages.
00:44:37.590 - 00:45:35.622, Speaker B: Let's take node js as an example. So node is just in time compiled, right? So we have the runtime, the v eight, this is written in c plus plus, so we can do the unwinding the way that I've described here. But what about this code that is basically generated on the fly, right, just in time compiled from kind of compiling this Javascript code to native code. The good news is, most just in time compilers generate code with frame pointers. So that means that we can, let's say, let's make it a simple example. We have our v eight runtime at the bottom of the stack, and then we have our node js functions. We can start by unwinding with frame pointers and then we jump into the section of the v eight runtime.
00:45:35.622 - 00:46:18.890, Speaker B: We notice, okay, we're now in that section. So now we need to continue with unwinding using dwarf. This is actually something that is unique to this profiler. Profilers like perf, like regular old perf on Linux can't actually do this. It can't jump from just in time compiled code to dwarf based unwinding. So it's actually kind of unique in that sense. So that kind of covers all just in time compiled languages, right? So node js, java, Erlang, Julia, these kinds of languages, the ones that need very specific integrations are the truly interpreted ones.
00:46:18.890 - 00:47:26.240, Speaker B: So Python or ruby for example. So these, if we're profiling them today, all we're seeing is the C code of the actual interpreter. That sometimes is useful, but probably most of the time only useful to people actually working on the interpreters themselves. Right? So there we're actually right in this moment working on deeper integrations where what we're doing is as we're unwinding the stack, we're noticing, okay, now we're in an interpreted function, so we actually end up reading memory from the interpreter to reconstruct what these frames actually represent in this moment. This sounds brittle, but actually this is how practically every profiler in those ecosystems work. So there's plenty of prior art. As a matter of fact, we actually ended up hiring someone from meta who has literally implemented profilers in EBPF for Ruby before.
00:47:26.240 - 00:48:13.710, Speaker B: And so it's only a matter of time until we get there. We already know how this works. We already have some proof of concepts working, but reality is native binaries work just in time. Compilers work, interpreters currently work in progress. I have a question regarding on the application side, I understand this profiling is looking more at low level calls on the system. So how would you then tie that back to the actual code that's causing those calls? Is there like an obvious path that I'm just not seeing or what's the connection between the code if it's complicated? Because I feel like once you compile it, it's not the same anymore. So how would you know where to look? Yeah, great question.
00:48:13.710 - 00:49:06.174, Speaker B: So compilers are very nice basically in this regard. They output something called debug infos and they essentially help us understand how we can translate these memory addresses back to human understandable function names. Right? And so this is basically that translation layer. I didn't really dive into that because that would have doubled the whole presentation because we basically built yet another database just for these pieces of metadata. But the short story is the compilers generate metadata for us that help us make that connection. Okay, cool, that makes sense. Thank you.
00:49:06.174 - 00:50:38.974, Speaker B: And since we're already talking about this and you all use a lot of go, what's actually really nice about go is that the runtime makes it mandatory for these debug infos to be available. If you've ever looked at the low level bits of a go binary, like if you use the read elf tool, for example, to see what are the sections in this binary. ELF is the common binary format that we use on Linux. You can see that there is a specific section called Go PC line tab and this is what's used by the go runtime when you have a panic, for example, to symbolize that panic because the go runtime also only technically has memory addresses, but it uses this information stored in the go PC line tab section to translate that back for us humans to understand. And I mentioned this because for other languages like rust or c or c plus plus, these are in optional sections. And so what happens often is that people, for reasons of binary size, kind of throw away this debug info and then you end up with binaries that are not debuggable. We do have a solution for this, actually.
00:50:38.974 - 00:51:14.570, Speaker B: What you can do is, let's say in your CI CD pipeline you want to get rid of these debug info. What you can do is you can split them off from your binary and upload them in your CI CD pipeline so that they are available when you need them for throwing on a debugger or profiling. But your binary size doesn't increase in production. So with go we always have these available, no matter what happens with other languages. It can be a little bit more complicated, but solvable.
00:51:18.300 - 00:51:25.400, Speaker A: Frederick, one question that I had is what is the state of the art for profiling in wasm? I'm very curious.
00:51:26.560 - 00:52:24.190, Speaker B: Great question. So basically wasm runtimes are just in time compilers, and so it's a little bit of a fragmented ecosystem at the moment. But for example, wasmtime actually generates debug info on the fly and writes them in this standardized format. So anything that you would run using the wasm time runtime you can actually profile, which I think is pretty awesome just because they followed standards, this whole ecosystem magically works. This is not universally true for every wasm runtime out there, but the state of the. It's definitely getting better. And wasmtime is setting a good example there.
00:52:24.190 - 00:52:36.486, Speaker B: Awesome, thanks. I think we have one more question.
00:52:36.588 - 00:52:40.680, Speaker C: On EBPF and Mac support.
00:52:43.230 - 00:54:01.700, Speaker B: Yeah, so EBPF is very fundamentally a Linux feature. We kind of primarily focused on where does production software run, and that overwhelmingly is Linux environments. However, like I said, as long as you can generate profiling data in the PPRA format, you can write it to the storage and do the same kind of interesting analysis over time that you can do with the data generated by say, and I happen to do my development on a Mac as well. And what we do is we just produce profiling data using let's say the go runtime profilers on our local machine and use that to analyze because we're not really that concerned with overhead on our local machine. So there it's kind of okay. But yeah, ideally you're running this on production and you're actually looking at the real world. It's kind of the same, again, kind of tying back to the beginning as any other observability data.
00:54:01.700 - 00:54:09.170, Speaker B: Nothing truly behaves the same as production, and so you really need to measure it in production.
00:54:10.810 - 00:54:16.230, Speaker C: I'm curious, is there any equivalent functionality that could be achieved on Mac with like DTrace?
00:54:19.130 - 00:54:35.980, Speaker B: It's certainly possible. It just hasn't been something that we looked at very extensively. But yeah, EVPF is heavily inspired by DTrace, so I wouldn't be surprised if that's possible. It's not something that we've tried yet, but.
00:54:39.570 - 00:54:53.106, Speaker C: Production first, that's where the important data is being generated. But as a developer who mainly works on Mac it's nice to be able to run experiments without having to rely on a VM. So I don't mean it's an open.
00:54:53.128 - 00:54:54.034, Speaker B: Source project, I guess.
00:54:54.072 - 00:55:01.682, Speaker C: Is that something that maybe could be, maybe extended to support DTrace hooks on Mac?
00:55:01.826 - 00:56:13.926, Speaker B: Yeah, I don't see why it couldn't. So Parker actually is kind of split into two components, the agent that does the EBPF collection and the server that accepts any data written in the PPRF format. It would be very feasible to write another agent for Mac that does the same profiling just then also writes it to the storage. What's actually really nice about the kind of modern Macs, the m one hardware or any of the Apple silicon, is they actually require everything to have frame pointers. And so profiling becomes very simple. It's part of the whole operating system, it's very tightly required, and you can get a bunch of Sec faults and everything if you don't have framecoiters on that hardware. So actually it should be something that's fairly easily achievable.
00:56:13.926 - 00:56:45.502, Speaker B: Everything that I talked about with frame pointers, that's the EBPF demo that everyone does. The hard part is the dwarf based unwinding. Awesome. Thank you. Just for completeness sake though, something that we do a lot, especially with like micro benchmarks or something. Let me show you this real quick. Something that we've created is this website called Pcroft me.
00:56:45.502 - 00:57:30.610, Speaker B: So what you can do, and we have a CLI for this as well. You can run your go micro benchmarks and output data in the pprof format. And then you can use this CLI called PPrFMe to upload this. And then you have this shareable link to profiling data that you can share with your team or share on an open source project if you're reporting an issue or something like that. And this is a free service because it's very cheap to run. Yeah, just thought I'd share that because I think for local development, this is most of the time what we do. I'm glad I joined at the very end, at least.
00:57:30.610 - 00:57:37.472, Speaker B: Awesome.
00:57:37.526 - 00:57:43.584, Speaker A: If there aren't any more questions, then. Frederick, thank you so much for joining us today and for giving the presentation. I really appreciate you taking the time.
00:57:43.702 - 00:57:45.570, Speaker B: My pleasure. Thanks for having me.
00:57:46.540 - 00:57:49.050, Speaker A: Thanks so much. All right, see everybody.
00:57:49.820 - 00:57:51.060, Speaker B: Thanks everyone. Goodbye.
