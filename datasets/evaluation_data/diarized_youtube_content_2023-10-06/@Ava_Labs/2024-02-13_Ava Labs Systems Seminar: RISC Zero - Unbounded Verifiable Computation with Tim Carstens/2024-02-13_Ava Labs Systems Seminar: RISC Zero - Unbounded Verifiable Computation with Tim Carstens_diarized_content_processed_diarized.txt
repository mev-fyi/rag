00:00:09.530 - 00:00:33.960, Speaker A: So, hey, everyone, welcome to the system seminar. This is a series created by Avalab for developers, engineers, product builders, and anyone interested in learning about the latest research and technology advancements in blockchain and the technical space. And I'm super excited because today we are joined by Tim Carson, who is a principal engineer at RISC Zero. And Tim, thanks for joining us today, and we're excited for your talk.
00:00:35.610 - 00:01:09.218, Speaker B: Thank you so much for having me today. I'm super excited to tell you a bit about some of the stuff that we've been working on here at RISC Zero. Might be a shorter talk. I wasn't sure how long speakers usually go for in here, and so I'll just give a heads up. I have a deck here that's kind of an overview. We're doing a lot of different things, and so what I wanted to do is make sure to save time at the end for questions. Why don't we go ahead and dive right in? So first I want to kind of talk about sort of the establishing problem here.
00:01:09.218 - 00:02:08.150, Speaker B: It's probably something that I think everyone here has probably thought about on some level, but here we go. So, a long time ago, 15 years actually, for anyone who's keeping count, Satoshi showed us this remarkable trick. He showed us how to solve the verifiable compute problem in a trustless regime. And the primary innovation there was the use of economic incentives to create this network, where basically the best strategy is, in fact, to be honest, or alternatively, the best strategy is to, of course, own all the computers in the world. But the simplest best strategy is just be honest. Absolutely phenomenal system introduced the concept of having a vm that ran scripts that a bunch of people would re execute. An idea that Ethereum took and got a lot of credit for anyway.
00:02:08.150 - 00:03:29.274, Speaker B: Amazing system, but lots of issues in Satoshi solution, right? For example, does not support non trivial computations. In fact, it used to support more computations than it supports today. So they've kind of been going the other direction, I guess. Other issues include famously like kind of a slow block frequency, right? So it's not very good for high frequency and low latency interactions. And then, of course, the other major thing about Satoshi solution is that in order for it to work, all of the data has to be public, which plenty of interesting trade offs to be had in that space, right? Very much antithetical to how the rest of the Internet actually operates with sovereign private data. So fast forward something like eight years and we start to see the emergence of usable zero knowledge circuit systems, right? And so if you've heard about things like, say, the circum platform in general, the language in general, Halo two, plonky, all sorts of things started happening in 2016 that basically took things a step forward and said, hey, what if there's ways to do verifiable compute without just having everyone rerun the computation? Which, if you think about it, as kind of a hilarious notion of what it means to verify something. And this work was, on the one hand, very exciting.
00:03:29.274 - 00:04:32.740, Speaker B: It's the culmination of a line of research that started before I was born, and one of these great examples of some extremely theoretical computer science potentially having some applications several decades after the Turing award had been given out. But lots of challenges in working with ZK circuits. They're extremely complex. Your programmers are going to go from writing in languages that they're familiar with to writing constraint systems over finite fields, which technically means that they are now algebraic geometers, and therefore get to know Grot and Deke and Sarah and all the rest, just to write some on chain application. Very complicated. And some of the things that come along with that, of course, is that this circuit approach to ZK exhibits some extremely tight coupling to your particular application. And then lastly, of course, sort of the fundamental nature of circuits in the complexity theoretic sense, is that circuits are functionally complete, but they are not Turing complete.
00:04:32.740 - 00:05:26.760, Speaker B: And for most software engineers, getting to a point where you even know the difference between those two things is usually not a direction people even want their career to go. So a lot of issues with the ZK circuit kind of paradigm, although I will say it's kind of an awesome nerd snipe. If you really are just looking for a way to take a simple task and turn it into months of worrying about exponents, this is a pretty good way to do that. A couple years later, a guy named Ed Felton was done work with his stint in the Obama administration and returns to Princeton. And him and his grad students decide to ship some work that they had been doing on these things that we now call fraud proofs. So this is the sort of like arbitram, for example, and then all of the follow on projects that came from that research here. This is kind of the like.
00:05:26.760 - 00:06:08.260, Speaker B: It's actually a step closer to Satoshi's vision than ZK. And here, of course, the idea is that we're going to go ahead and create a slightly more elaborate economic incentive mechanism that, again, allows for this notion of fewer people validating all of the computations, or perhaps different parties focusing on different aspects of the computation. Very exciting stuff. Obviously, it's done really well in the l two space, but it does have some things that the protocol people have generally been concerned about. It has some trade offs, actually. Operationalizing a fraud proof mechanism is extremely complex. You can ask optimism how that's going.
00:06:08.260 - 00:06:49.870, Speaker B: There's a very high barrier to entry in terms of designing protocols that use these types of dispute mechanisms. You sort of have to answer this unstable equilibria whereby people looking for fraud are incentivized to find it, because they will be rewarded if they find it. But because of that incentive, other parties are disincentivized to commit fraud, which means that the people who are looking for it will never actually realize any rewards. So maybe they stop looking for it. So it's kind of an unstable solution. Very hard to work around that. And of course, still no support for private data, so not a very good solution in essentially any of the regulated industries on the Internet.
00:06:49.870 - 00:07:18.850, Speaker B: The solution. Here's the most shocking thing. No one would expect me to say this is solution the risk zero ZkVM. So what is it the risk zero offers? The risk zero ZKVM is basically a risk five emulator. You can call it a virtual machine, that's fine. It's based on the RISC five architecture, Rv 32 IM core. And the idea is that you can run your programs inside this VM.
00:07:18.850 - 00:08:04.626, Speaker B: They produce some kind of output, and the VM will generate this string of cryptographic magic. And if that cryptographic magic is accepted by some other function, then it tells you in extremely high probability that the output actually did come from running the program inside the VM. In essence, you can think of it as when you run a program in the VM, the program essentially digitally signs its output by virtue of simply running. There's no private keys involved, but there is still this non repudiation property that is so critical for digital signatures. The key thing about our system is it's designed well. First of all, it's a general purpose VM. It's not a circuit language.
00:08:04.626 - 00:09:09.366, Speaker B: So far as I know, we were the first to release an open source VM of this type based on a standard architecture. Other vms do exist and predated us. The Cairo VM is a great example of a VM that predated us, but a novel architecture that necessitates the development of its own stack of languages and tools and such. We provide, in fact, fully compatible RV 32 Im core that pass the test and all the good stuff and it does so without sacrificing performance. At the time when we were getting ready to release this, a lot of the cryptographic royalty and blockchain raised an eyebrow and said, what on earth? Everyone in circuit land is focused on circuit performance and you guys are just going to move up the abstraction stack? How's that going to work out? The answer is that it works out quite well so far as I know. We are not only the only general purpose VM that's been audited and all these good things, but we're also the fastest. We do proof generation in seconds on commodity hardware.
00:09:09.366 - 00:10:03.982, Speaker B: When I say commodity hardware, I mean standard kind of laptops you might encounter in the real world. Macbooks like M one, two and three consumer pcs with Nvidia gamer graphics cards, not scientific AI cards. You don't need a Xeon, you don't need 100 gigs of ram. And the vision here was to be able to produce zero knowledge proofs on exactly the kind of hardware that decentralized protocols typically run on. Our primary focus is to be easy to build on, and our strategy for that is to embrace standard technologies, right? So today if you wanted to write some ZK programs in RISC Zero, you would grab the dev kit, make sure that you had the rust tool chain installed, and you would just write your software in rust plain as day. Other languages are supported to various levels. C Plus plus works just fine.
00:10:03.982 - 00:10:44.838, Speaker B: I'm not a huge fan of doing C and C Plus plus in ZK for interesting reasons related to exploitability, but it does work. And in the next major version of the circuit, I anticipate that it's going to switch to an rv 64 core, which will then bring in support for go. The basic idea here is like, who wants to maintain a compiler tool chain? Compiler teams already do that. Let's just support a standard architecture that people can already target. But for now, we're starting off on rust. The beautiful combination of cycle efficiency together with memory safety is quite attractive. So mainstream programming languages.
00:10:44.838 - 00:11:45.386, Speaker B: We also have support for mainstream cryptographic primitives. The virtual machine is not just an RV 32 Im core, but it also contains what you can think of as like some ZK peripherals that provide accelerated arithmetic for crypto. So a great example of that is we have the fastest shaw two implementation in ZK. We also have a high speed big integer coprocessor, which is useful if you want to do operations like ECDSA, signature checks, or other kind of things over large finite fields. And then lastly, of course, recursive proof verification is or recursive proof verification, which we call proof composition, is also supported in the VM. So this is the essential thing that you would need if you were going to build like a ZK roll up or anything of that sort. The tooling itself is built in rust, which means it runs pretty much anywhere that rust runs reliably.
00:11:45.386 - 00:13:00.390, Speaker B: Right? And so certainly you can use it as a library in your native application, command line tools, mobile apps, desktop apps, no problem. But through the Rustwasm support, you can also just absolutely run it in the browser. You can bundle this up, put it into your typescript application, verify proofs in the browser, no problem. And then through a slightly longer chain that cleverly sidesteps rust, you can also verify the proofs in your Ethereum smart contracts, which I hear that Ethereum is getting popular, so probably time to think about how to support that. Anyway, all of this is to say, this enables builders to focus on building we've done the developer community a disservice by removing the classic nerd snipe of writing circuits, but we've done the product managers an enormous service by making it easier to actually ship applications. How it works this would be several talks, and so I'm going to invite folks who are curious about specific parts of this to drag me back to this topic at the end. The main heart of the matter is that we have several ZK stark circuits.
00:13:00.390 - 00:13:48.150, Speaker B: I listed three of them here. We actually have a couple more that are used for gluing things together. But the basic idea is, like I mentioned, we have this guest core that implements the RV 32 IM instruction set architecture together with SHA two accelerator, the beginning accelerator, and this very fancy mercurized paged memory system that allows you to have this gigantic memory space over for long running applications in a way that's still actually efficient to work with. We have a few other circuits that are used in service to that. We have this thing called the continuations joiner and the proof composer. Talk about those things in a little bit here. But like I said, the heart of it is in this ZK stark circuit formalism.
00:13:48.150 - 00:14:21.386, Speaker B: We actually write those circuits in a custom language that we've developed in house. It's pseudo functional. I don't know. I can say a lot about it. It'll be released soon enough, so I'll save it for then. What else do we got? The other magic component is that we also have a gross 16 variant of our proof verifier. Right? So you can generate a gross 16 proof that witnesses that you have verified a proof that came from one of these other circuits.
00:14:21.386 - 00:15:34.790, Speaker B: And that's our trick for how we do on chain verification. And so gross 16 proof, you're looking at like 900 bytes, maybe 270,000 gas to verify on chain. Really quite favorable. And then the other fancy major component that can be its own talk is orchestration for parallel proving. And so one of the critical features in our platform is if you have a large computation and you would like to prove it in ZK, we actually have a trick that allows you to spread the work for that proof across several machines and actually execute on it in an embarrassingly parallel manner, which allows us to achieve the fastest proof times of any of the general purpose vms rather considerably due to just the unfairness of I can throw 200 machines at it. So that's the super high level of what kind of things we're doing at risk zero. So I want to talk a little bit about some of the use cases and applications, because to me this is actually the interesting stuff here is what can you do with it besides hello world or verifying your Merkel proof, or any of the other kind of basic examples you see in ZK.
00:15:34.790 - 00:16:17.990, Speaker B: So the first of these projects that I want to talk a little bit about is a demo that we did towards the end of last year. We called it bonsai pay, which makes it sound like a product, but really it's just a demo. What's the deal with Bonsai pay? So here's the setup. Let's say that you have a token and you would like to airdrop it to a bunch of people. You would love to just make it show up in their wallets, but maybe you don't have all of their addresses. Maybe what you have instead is like a marketing list of email addresses. So bonsai pay shows a particular flow for being able to send tokens to any Gmail user, specifically Gmail, but any Gmail user.
00:16:17.990 - 00:17:13.640, Speaker B: The neat thing is that there's then a smart contract that indicates hey, user with email, hash, whatever has this balance waiting to claim, and then waiter, those users can show up, produce a zero knowledge proof in their browser that they are that Gmail user, and basically then have the smart contract withdraw their tokens over to their actual wallet address. So this is an example of openID integration taking place within our VM, and ultimately then resulting in a proof on chain about your identity. Another interesting demo that we put together last year is this governance demo. So I'm not much of a Dow person myself, but pretty massive use case like what are you going to do with your tokens. Here's an idea. Have people vote with them. Okay, that's fine.
00:17:13.640 - 00:18:30.554, Speaker B: But now, of course, you have the complicated thing of you're trying to do something that for blockchain is actually very expensive, namely collect a bunch of boolean values from people and then calculate a sum. So we did an experiment. We said, what happens if we take that, compute as small as it is, and just move it into the VM instead? And in this particular example, baselined against the open zeppelin kind of reference contract, we found that even going through the cost of generating and then verifying a ZKP, even if you do all those steps, you can still have pretty massive savings. What does it say here? 66% less gas per vote than sort of the obvious already optimized solidity code from open zeppelin. So even for simple tasks, you can get that valuable gas savings out of CK next project is the one that actually, I spend most of my time on the Zeth project. So in the ZK space, there's been this holy grail for a while to build a so called Zke EVM. These days, if you don't have a ZKe EVM, what are you, even the like? It's an essential service for every blockchain company to have a ZkevM.
00:18:30.554 - 00:19:10.054, Speaker B: So we wanted to help lower the barrier to entry for folks who wanted to be in that party. So we just sat down and wrote one and just shipped it. So Zeth is, at least according to the EF, the first type one Zkevm. Fully ethereum compatible Zkevm. No compromises, catch hack hashing, standard gas schedule, standard opcodes, all the good things. The fun thing about this project is that it took me and two other engineers about three weeks to go from an empty repo to having this work. And this is earlier.
00:19:10.054 - 00:19:41.858, Speaker B: Remember when I said the ZkVM, it allows you to focus on building? This is really the main example I had in mind. The Zeth project wound up being hilariously simple. Fun fact, actually. I'm an old bitcoin guy, right? I'd never written any solidity. I'd never had an Ethereum hot wallet. But I was a part of the team that shipped the first type one Zkevm. And the reason that that was possible is because in the rust ecosystem, there's already libraries that do all the things that you need in order to build a ZKe EVM.
00:19:41.858 - 00:20:25.490, Speaker B: The only thing missing was the ZK component. So this is how I learned Ethereum. We used to joke, and it really trolled some people that we actually had built the first type zero ZKe EVM because it's not just type one compatible, but it's in fact literally the same source code, the same libraries used in ref and other full node clients. So it's like it's even more compatible than the standard sort of thing. Related to that, we've also been engaged in a project to extend Zeth for the optimism ecosystem. Optimism, of course, is an EVM based chain. It's an l two architecture, which means it's actually two chains, a canonical transaction chain together with an actual state chain.
00:20:25.490 - 00:21:41.370, Speaker B: So we kicked off a project with them to basically show what it would look like to do full ZK validation for their system, and in doing so offering a pathway to deprecating the old canon fraud proof system and achieving faster finality. My graphic on here has some example proof times for various real world optimism blocks, and you can see that they're typically anywhere from like one and a half to seven minutes or something like that. I don't know why the data isn't there for the second block in there, but something like ten minutes or something and many many more. Other applications that we've done in house, or which other people are currently building include an off chain central limit order book, multiplayer games in principle, one day you'll be able to do some software supply chain stuff in here. You can run your compiler inside the VM and get a proof that the binary really did come from the compiler in the source. Opaque formal methods is another application area we've talked to a few companies about. This allows you to prove that a piece of software conforms to some public spec without revealing the source code of the software or the formal proof.
00:21:41.370 - 00:22:10.280, Speaker B: And then we also have some examples showing how to do machine learning and inference inside the VM as well, which can be very large computations. Although as you can imagine, the models here are not Chat GPT scale. So this brings us then to the main question, what do you want to build? And is there already a rust crate for it? And if so, why don't we go ahead and put something together? There we are. Awesome.
00:22:11.050 - 00:22:16.600, Speaker A: Yeah, super interesting. So if you don't mind, we can open up the floor to some questions.
00:22:18.590 - 00:22:19.500, Speaker B: I see.
00:22:21.150 - 00:22:26.060, Speaker A: So anyone who has a question, feel free to go ahead. I think Aaron will start.
00:22:26.590 - 00:22:41.790, Speaker C: Yeah, one question I have, Tim, is what workload do you guys typically use for benchmarking your system? Just because I know there could have very different performance on different types of workloads, and you've obviously created specialized coprocessors for a few things. So I'm very curious what you use to actually benchmark the system.
00:22:41.940 - 00:23:16.810, Speaker B: Awesome question. ZK benchmarks is actually a topic that's pretty dear to my heart. It was something I was working on in collaboration with Dylendom research a little over a year ago. We got in with Bob in from polygonmiden and a few other teams, and we discussed actually, like, what is the best way to do comparative benchmarking for different ZK systems. The answer is, it's actually really hard right now. So the TLDR. My answer is when people ask me about the risk zero performance, I just talk about the performance of Zeth.
00:23:16.810 - 00:24:04.650, Speaker B: How much work does it take to apply 200 ethereum transactions and build a new state route and verify the signatures and do everything else? And I picked that example because, one, I have data on it. Two, it's a super real world problem, and a lot of engineers in blockchain already have some sense of the work that goes into it. But it's kind of terrible for a benchmark because other systems don't have their own zeth type thing to compare against. Right. There are teams that went down that rabbit hole trying to build ZKe vms in circum or what have you, years of effort, still not finished. So it gives useful performance data, but it's not comparative. On the other extreme scale.
00:24:04.650 - 00:24:43.250, Speaker B: One of the kind of common sort of answers here for how to benchmark ZK systems is that you come up with these toy problems and you just compare them on the toy problems. Right. Anything that's fast and easy to implement. And so Fibonacci is like every now and again I'll hop on a call with a partner and they want to know how well we perform calculating Fibonacci numbers. And I usually tease them a little bit and ask how many fibonacci numbers they need to calculate in their use case. Actually, because fib is so popular, we went ahead and just produced an implementation that's order one time. Because there's tricks you can use for Fibonacci, all these theorems and everything, it grows exponentially.
00:24:43.250 - 00:25:31.046, Speaker B: Right. So the benchmarking thing is hard because you want to pick benchmarks that lots of ZK systems are capable of doing. But many ZK systems are in fact very high friction to build in, and so you wind up with the toys. We've also done benching on hash performance because this is actually super critical for a lot of use cases. And for that, I think that we're able to do one round of Shaw two compresses in like 72 stark cycles or something, which is crazy fast, but of course doesn't help Seth or other things like that that actually want Shaw three. The benchmarking question, I think is still a scientific challenge generally for the industry. Yeah.
00:25:31.046 - 00:25:43.260, Speaker B: In case you're curious though, we can do a full ethereum block, fully proven in about ten minutes when we're doing proofs using in parallel regime, in the sequential regime. It might be like two days or something.
00:25:44.430 - 00:25:51.562, Speaker C: Very cool. Yeah, I can understand that being a very difficult problem. There's a lot of funny games that people will play with benchmarks when it comes to.
00:25:51.616 - 00:25:52.206, Speaker B: For sure.
00:25:52.308 - 00:26:28.230, Speaker C: Yeah. Especially when there's some speculation on it as well. We definitely have some consensus. Will always talk about their TBS numbers. And I think it's sort of another question I had for the coprocessors that you chose to develop. I'm very curious what type of performance improvement on what workloads you saw from those, and then also how do you integrate those coprocessors in, and to what extent that might be open source and modular so that people can build their own coprocessors, or if that's sort of a part of the dev experience you want to abstract away, but what that process looks like, I'd be very curious to learn more.
00:26:28.380 - 00:26:51.054, Speaker B: Awesome question. Yeah. So we have two coprocessors in production right now. One of them is the sha two core. I think that using that core is something like 1000 times more performant than just running sort of a naive implementation of Sha two and software. And then we also have a big integer accelerator. The main thing it actually does for us is fast.
00:26:51.054 - 00:27:26.460, Speaker B: Big integer multiplies. And this is critical for if you're doing anything with elliptic curves, which would be like mostly signature checks, or if you're doing pairing checks. And for that, I don't remember, it might be like a 100 x kind of improvement. It could be massive. And part of why it's massive, especially in the big integer case, is that there's fancy ZK specific algorithms for doing certain integer operations that involve outside oracles making guesses and probabilistic arguments. There's like all these things you can do. You're working in a totally different complexity class than if you're just writing code.
00:27:26.460 - 00:28:25.306, Speaker B: So the performance gains have been pretty massive for those things as to extensibility and then also reusability. It's not yet released, but we do actually have a circuit language in house that we use for these things that's been spending the last year maturing it pretty massively. And part of the vision is to enable teams to write their own accelerators that are compatible with our vm so that basically, they can stay current as, like, dan Bonet invents a new signature scheme. There's a BLS 2.0. You want to be ready to rock and roll with that catch act. Hashing is another great example. Actually, the neat thing about our approach to ZK, the actual way that we've implemented our stack, is that we already have this ensemble of circuits, and we already have a notion of one circuit verifying the work of another circuit.
00:28:25.306 - 00:29:24.500, Speaker B: And basically, everything that you need in order to create this notion of what we call peripheral extensibility. What you really want is a basic ZK processor core, together with the ability to attach accelerators to it as needed for your application. You want to do this in a way that works smoothly in the ecosystem. You want to be able to say that I have verified on this ZK hardware that this program ran and produced this output. So that's the direction it's going. And I expect that that will be a big theme this year. And then, lastly, as for reusability, because our proof system is somewhat different than the other proof systems, it's not really possible, unfortunately, to take our accelerators and use them like in a circum project or something else like that, just owing to particulars of how the stark math works out.
00:29:24.500 - 00:29:37.000, Speaker B: But the whole stack is, in fact, open source, permissively under Apache two, and so things can go in a different direction. Yeah.
00:29:38.570 - 00:29:52.140, Speaker C: Cool. One other question. I have to follow up on that. If people are compiling their programs, like from rust to RISC five, how does RISC zero actually know when to invoke the coprocessor for something?
00:29:52.510 - 00:30:13.802, Speaker B: Right. Yes. Sorry, I forgot to answer that. Yeah. So, under the hood, all of the coprocessors are implemented by taking advantage of the syscall RISC five instruction. Right. But, of course, if you're just writing some rust code, you don't want to think about our crazy syscall ABI.
00:30:13.802 - 00:31:13.314, Speaker B: What you really want to do is just grab a rust crate that provides Shaw two and use it and hope that behind the scenes, something magic is happening so that it's actually accelerated. And indeed, that's the paradigm that we support. And so, for the popular cryptographic libraries, here's how we achieve this. For things like the Shaw two crate, simple example, risk zero maintains a fork of this crate that, when compiled for our vm actually dispatches out to this syscall interface. And using this patched version of the crate is actually very simple because cargo, the rust package manager, allows you to specify in your project or called patches, which basically means, hey, when you need the sha two crate, here's the URL for the risk zero version. So this allows you to just literally patch out for your binary and includes deep into the dependencies. It just takes care of the whole thing.
00:31:13.314 - 00:31:16.610, Speaker B: And so it's actually surprisingly smooth.
00:31:17.990 - 00:31:31.450, Speaker C: That sounds like a pretty easy developer experience. So I guess that means you have like a slightly modified compiler to produce a different risk five bytecode that just makes the syscalls into your system instead. And then that invokes the coprocessor, is that right?
00:31:31.600 - 00:32:00.046, Speaker B: That's approximately right, yeah. The compiler that we're using is actually the standard RISC five compiler maintained by the rust community. And there they're taking advantage of the LLVM support for the risk five target, of course. But yeah, it really is that we advise setting this cargo patch directive to use our patched crypto libraries and then they link against the syscall API.
00:32:00.158 - 00:32:26.810, Speaker C: Cool. Yeah, makes sense. The one final question I have, and then if anybody else has one, I'll let others. I don't know if Steven has one, maybe I see him smiling a couple of times. But the benchmark that I'd be very curious about is how do you guys compare when you have sort of the risk five intermediate, or I don't know if you call it an intermediate representation, but the risk five intermediate to sort of handwritten circuits or things that are compiling directly to ZK or to some sort of ZK circuit.
00:32:27.150 - 00:33:21.280, Speaker B: Yeah. So that's actually one of the hardest things to directly compare for the following boring reason. Right. So we can pick some toy problems where it's easy to write benchmark code for it. And unsurprisingly in the toy case, yeah, if you don't need, for instance a random access memory model, if you don't need general purpose control flow or any of the other things that you get with a general purpose processor, then yes, you're going to be able to produce something that is faster to prove if it's just like a raw dog circuit. But then the next question is how many real world use cases are actually covered by. And so Fibonacci is one of those simple examples where it's like, yes, you can actually implement this in circum very efficiently, but what does that tell you about whether or not you should use circum for your project.
00:33:21.280 - 00:34:12.186, Speaker B: And so as we move up the stack to more complex computations, it becomes harder to do these comparative benchmarks precisely because it's hard to build nontrivial things in a circuit language. Right. My personal intuition about this, though, looking ahead at the long term, is I think that we can learn a lot from the history of the pc revolution. I remember when I was a young kid, there were these things called sound cards that you can get for your tower. And I remember at some point, MP3 started becoming popular, but the general purpose cpus were still pretty miserable. And so if you had real money, you could get a creative lab sound blaster that had a built in MP3 coprocessor. Boy, oh boy, did I want one of those.
00:34:12.186 - 00:35:03.150, Speaker B: Of course, by the time I had the money for one of those, it was no longer a good use of money, because the processors, the cpus, had gotten fast enough that you could just play MP3 s without having this coprocessor. So what we saw in the pc revolution is that things actually started out with a lot more specialized coprocessors. They eventually got to a place where they were less necessary. Today, that kind of thing, like fpgas, cplds, and other kinds of programmable logic. They still have their use in industry, but they tend to be in very specific industries, right? Like, if you're doing a backbone switch, you'll do an FPGA just because it makes no sense to make an ASIC at that low scale. Or if you're doing a super low power device, like for the Mars rover. Great example of a situation where it might make sense, although even JPL has been moving away from customized chips.
00:35:03.150 - 00:35:16.180, Speaker B: So I think that right now we're in sort of the awkward teenage phase for the ZK tech, where it's starting to become capable of doing so much, but there's still a lot more development to take place before we're in the end.
00:35:19.670 - 00:35:20.130, Speaker C: Cool.
00:35:20.200 - 00:35:21.140, Speaker B: Very cool.
00:35:25.030 - 00:35:43.020, Speaker D: Aaron actually already asked most of my questions, but one question I do have is, this is all based on top of risk, right? And risk is nice because it has a generally much more simple architecture than something like AMD or whatever. But I would have imagined that.
00:35:44.750 - 00:35:45.018, Speaker B: At.
00:35:45.024 - 00:36:07.060, Speaker D: The end of the day, reducing the number of instructions as much as possible would end up optimizing both proving and verification times, because you can actually implement the more optimized circuits for those ops. Why did you guys end up choosing risk? Was it more of to make the development more reasonable? Because AMD is crazy.
00:36:09.430 - 00:36:50.862, Speaker B: What was the rationale? Yeah, so this is a brilliancy that no one remembers whose idea it was to implement RISC five. I know for sure it wasn't my idea, but it might have been any of the three original founders. The motivation is kind of many folds. So one RISC five has the benefit of being a completely open source architecture. So unlike arm, for example, where you would need licensing from the Arm consortium with RISC five, you can just go ahead and just do it. And so this was attractive. I think MIPS is probably in a similar place these days.
00:36:50.862 - 00:37:20.600, Speaker B: I don't remember exactly, but of course, risk five is essentially MIPS 2.0. So if you like MIPS, you probably like risk five as well. They're extremely similar. As to why not doing like an X 86 type thing. It's exactly what you identified. It's actually hard to even count how many instructions are supported in X 86. You have to really get into the weeds about what counts as an instruction, which is a nightmarish debate, especially due to proprietary vendor extensions and what have you.
00:37:20.600 - 00:38:03.830, Speaker B: So I think that something of that scale was just going to be out of the question just due to how do you even test it, right? But also, if you look under the hood of a modern intel or AMD processor, what you'll actually find is a risk core that is interpreting your CISC instructions. And so even they have gone to kind of a risk place with basically this X 86 compatibility layer wrapping around it. But kind of the simplest and fastest answer is that Jeremy Brussels, our chief scientist, was on the hook for implementing this. And so he wanted something that had like 32 instructions, like risk five. He's like, I got to get this built so we can raise.
00:38:05.050 - 00:38:20.218, Speaker D: Makes sense. I've heard some people fuzzing AMD chips to try to find undocumented instructions in the past and see if they work programs faster. Not always faster, but I did a.
00:38:20.224 - 00:38:57.800, Speaker B: Project like that several years ago. This is unrelated to ZK, but my buddy Patrick and I were fuzing x 86 processors not for speed gains. We did want to find undocumented instructions, but we were actually looking for is ways to. We were basically writing exploit payloads, and we wanted to know, is my payload running inside a VM or is it actually running on the target? So you wanted to find ways to make your code branch and do different stuff. If some malware reverser was like, on your tail, x 86, man, you can make a whole career out of doing evil things to it.
00:38:58.650 - 00:39:12.682, Speaker C: I think one of the coolest things about this is just to see risk just open source instruction sets winning. And you guys leveraging that is really cool. As well as just like, the fact you went through this decision making process and it was just a clear winner because it was open source.
00:39:12.826 - 00:39:35.898, Speaker B: Super clear. Yeah. I think that our one regret is, like I said, we're very specifically on the rv 32 IM core, which is like an embedded risk five core. Works great with rust. Here's a bummer. Doesn't support go? That's fine. Depends on what audience you're talking to, how finer.
00:39:35.934 - 00:39:37.000, Speaker C: I'm fine with that.
00:39:39.130 - 00:39:53.514, Speaker B: Yeah. So we're looking at switching over to an rv 64 core, which many variants of rv 64 are supported by the official go compiler, but yeah, very cool.
00:39:53.632 - 00:39:54.922, Speaker C: Does anybody else have any questions?
00:39:54.976 - 00:39:55.146, Speaker B: Or.
00:39:55.168 - 00:39:56.540, Speaker C: Steven, did you have another one?
00:39:58.190 - 00:40:50.810, Speaker B: I have a question. So the crypto space, it's super noisy, and it's probably an argument to be made that the, you know, it's even more abstract and even more noisy. What storylines are you most looking forward to? And like something that we could follow on more of the outside for 2024? Hot question. Yeah, I'm not sure that I have the cleanest, simplest answer for it. I'm just going to speak freely, and I'll speak for Tim and not necessarily for risk zero. 2024 is going to be a crazy, loud year in crypto for a bunch of the same silly reasons that we've seen for the last 15 years. Blockchain is a solution looking for a problem, and the list of applications that we're going to see are going to be more DFI things for the near term.
00:40:50.810 - 00:41:29.654, Speaker B: I do routinely talk to folks who want to build non DFI applications in a decentralized manner. Hope spring is eternal for that. But the current limitations of the technology across the industry make it such that's essentially a pipe dream. The hope is that ZK technology will address some of the issues, primarily around the scalability of verifiable compute. But as a primitive for trustless computing, it's only part of what you really want. What do I mean by that? In trad computing, you want to do a startup, you have an idea. You're like, hey, I'm going to invent notion so, or whatever.
00:41:29.654 - 00:41:50.880, Speaker B: I'm going to make an in browser text editor, and somehow that's going to be a successful business, even though there's a million of them. You could never do something like that on chain, right? Because if it's on chain, this data is available to everyone. It's no longer a good place to store your business notes. Right. You can never do that for healthcare. There's a regulation that says you can't. There's like all these issues.
00:41:50.880 - 00:42:22.394, Speaker B: So I do think that the ZK thing is going to be interesting as far as scaling. That'll be useful for defi exchanges. It might be useful for some on chain applications we've seen like guilty geosentopology, GG exploring those kinds of things. But I do think it's going to be another DGEn summer, though. I think we're still going to be looking at token trading as the primary use case for blockchain, essentially in the near term. Nice.
00:42:22.512 - 00:42:28.140, Speaker A: Do we have one more question or is that it for now?
00:42:30.850 - 00:42:31.600, Speaker B: Yeah.
00:42:33.410 - 00:42:52.942, Speaker A: Awesome. Thanks again, Tim, for joining us today. Super interesting talk. We've seen a lively discussion at the end, so that's great. We will upload this talk to the able apps YouTube channel. So if you want to watch it again or check out any other episodes, you can do that there. And yeah, thanks a lot, Tim, for joining us.
00:42:52.996 - 00:42:54.800, Speaker B: Awesome. Thank you so much.
00:42:55.290 - 00:42:56.166, Speaker C: Thanks, Tim.
00:42:56.268 - 00:42:56.740, Speaker B: Thanks, Tim.
