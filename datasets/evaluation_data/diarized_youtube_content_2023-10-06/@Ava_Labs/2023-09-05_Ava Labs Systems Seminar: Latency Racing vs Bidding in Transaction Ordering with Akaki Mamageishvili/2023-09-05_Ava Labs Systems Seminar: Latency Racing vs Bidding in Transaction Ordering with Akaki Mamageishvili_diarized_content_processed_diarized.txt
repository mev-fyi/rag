00:00:11.130 - 00:00:25.798, Speaker A: Hey everybody, thanks for joining us today. Today we have Akaki from off chain Labs research coming to talk about his recent research, buying time, latency versus bidding and transaction ordering. So thanks so much for joining us. Akaki, we're really happy to have you today. Yeah, take it.
00:00:25.964 - 00:01:22.934, Speaker B: So thank you for the invitation. So welcome to this presentation. So this is a joint work together with Mahimna Kelkar, who is PhD student at Cornell University Jan Christoph Schlegel, who is a senior lecturer at City University of London, and he recently joined flashbots company and Edward Felton, who is co founder and chief scientist at Offchain Labs, where I'm working as a research scientist. So motivation is one of the biggest questions in my opinion. In blockchains this is transaction ordering policy. And we know that in the traditional finance applications they keep order books of buy and sell orders. And all of them by default or even by law, are obliged to have first come, first serve policy.
00:01:22.934 - 00:02:12.520, Speaker B: So they are executing transactions as they come, or if it's limit order transaction, then they need to wait. But they will always execute the transaction that came the first, if it is possible to execute. In case of roll up protocols. So roll up protocols for those who don't know, try to scale the base layer. In case of Ethereum, I'm talking about arbitrum, that's the main product of chain labs. This optimistic roll up we are also using first come first transaction ordering policy. So there is a sequencer that orders transactions as they come by their time and executes them or passes it for the execution stage in this order.
00:02:12.520 - 00:03:20.250, Speaker B: We know that in base layer situation is different and they use blocks, block approaches and that's why they are called blockchains. So the transactions contain bits and there is some designated block creator in every round, in every instance of block creation. And there is also some mempool of transactions, of publicly visible transactions that is called mempool. And then the next block writer just chooses a set of transactions for the next block and then they somehow reach a consensus about the block. So there are different ways how to reach consensus, but main idea is that either miners or proposers, or block writers or block creators, there are many different names. They decide how to choose the transactions in the block and they also choose how to order them. And especially because there are now financial applications deployed on the blockchains, or at least the blockchains that support smart contracts.
00:03:20.250 - 00:04:07.046, Speaker B: There is a lot of incentive to have your transaction early or maybe late, depending on what is the intent. And this transaction ordering policy gives a lot of power to the block creators. But in case of roll ups, we know that it's first come, first server, or at least that's the promise by the sequencer. And in case of arbitrum it is kept. And in case of other roll ups, I also believe, or at least it's also easy to detect if there is some, that the sequencer tries to violate this policy. So there are a lot of advantages to first come, first serve. It's simplest ordering policy, and it's also easy to explain to users.
00:04:07.046 - 00:04:58.726, Speaker B: And it also seems intuitively fair. Whoever is faster gets transaction executed faster. And there is nothing that has lower latency, because it just needs to reach the sequencer and then sequencer can tell about this to the transaction sender. So any other ordering policy has some delayed. And we know that in blockchains there are many seconds of delay for the block creation, but there is a disadvantage also, otherwise we would not be trying to design different policies. And it is very well known in the traditional finance field, and there is a huge industry around it. It's called high frequency trading.
00:04:58.726 - 00:05:53.530, Speaker B: And that policy creates the latency competition. And a lot of institutions and independent players invest to have low latency, because for the arbitrage searchers, it matters that their transaction is fast enough. So this high frequency trading is so big that it even constitutes to more than half of the exchange volume. So there are references to this. In case of blockchains, it was in my opinion acknowledged too late. And there is this paper by Diane and quarters called Flashboys 2.0, which looks to this well known minor extractable value and documents the front running and back running, and all kinds of attacks by miners and other players.
00:05:53.530 - 00:06:58.194, Speaker B: And the problem for the consensus layer, also because in case of Ethereum, it was a lot of these priority order auctions happening, and that was creating congestion. So unlike the front running, we see the back running. So after transaction is scheduled, some arbitrage seekers are trying to get their transaction executed as fast as possible to adjust the price or take advantage of the previous transaction. This we see as a positive activity, and because it optimizes the market, for example. So we want to support that. So in case of roll ups, we also observed in the context of arbitrum that there is latency raise. In particular, the players try to come closer to the sequencer, because arbitrum or roll ups have one sequencer at the moment at least.
00:06:58.194 - 00:08:05.778, Speaker B: And the parties that have this advantage to be closer to sequencer, they have advantage to be scheduled as fast as possible. And we see that this to be unfair, but also it is inefficient. And by inefficient I mean that not always the user that has the highest value for the arbitrage value gets its transaction the fastest. And all this latency investment and improving the latency is a total waste from the protocol perspective because the players pay to some outside services to have better latency. Okay, so there is one particular example that also slowed down sequencer. So the parties were creating a lot of nodes because sequencer was applying the fairest feeding policy. So sequencer has feeding of transactions to all the nodes that subscribe to it, and it feeds at random, uniformly at random.
00:08:05.778 - 00:09:11.882, Speaker B: And if you just create more and more nodes, the chances that your node gets feed faster is increasing. But this also slows down the sequencer, because if there are already hundreds of thousands of nodes, it is too slow to feed all of them. Now, how to fix this problem of latency competition? So first of all, we should acknowledge that it would be much better if the protocol could capture these resources because this could be then used. So this additional revenue could be used for maintenance and improvement of the infrastructure, or even can be used for subsidizing regular transaction fees. So if there is some pool created from this additional revenue. So one way or another, this helps the protocol and is not a complete waste if we can monetize this. But we need to come up with a transaction ordering policy that has some properties.
00:09:11.882 - 00:10:33.240, Speaker B: So first of all, we want to ideally make the latency racing disappear, but if it's not possible to make it disappear and it's not, we want to reduce the effect of it. So we also want to keep the low latency. So particularly we want the property that any transaction that arrives to the sequencer is executed or scheduled to execute in the sequence within some time bound, and this time bound should be as low as possible. So one more maybe exotic properties, independence of irrelevant transactions, but let me explain what it is. So we want that different latency races don't interact with each other because this complicates the strategy of the participants of the latency races. And more formally it means that two transactions will be scheduled in relative order only depending on the properties of these transactions, and other transactions don't affect the relative order of two transactions. Ideally, we also want will decentralize the sequencer in the future, and the policy should be such that decentralization is still possible.
00:10:33.240 - 00:11:24.226, Speaker B: And last property is dark man pool. So right now sequencer sees all the contents of transactions and ideally nobody sees contents of transactions until they are actually scheduled. But once they are scheduled, then everyone should be able to see and then the race can start. So this is this back running race. And the last two properties actually are almost the same, decentralization and dark mempool. So with one sequencer you cannot achieve dark mempool, but with multiple sequencers and threshold encryption schemes it is possible. So for that we need to have a committee of sequencers and threshold encryption.
00:11:24.226 - 00:12:09.602, Speaker B: So let's say we have some low number of sequencers, because as soon as we increase number of sequencers, the latency increases. So that's why it needs to be low, but not too low, let's say in the range of seven to 16. And we have some threshold trust in them. So we trust that, let's say, third of them are not more than a third of them are malicious. So we trust that two thirds of them are honestly doing their job. And we assume the byzantine fault tolerance. So we require that from the mechanism and we assume that network is asynchronous.
00:12:09.602 - 00:12:55.694, Speaker B: So this is the hardest assumptions on the problem at hand. That's why the speed is very low when you increase and it grows at least quadratically. And also it depends how geographically distributed the sequencers are. So let me jump to the algorithm or the policy itself. So, informal description is that it is simple and we think that it is fair and mixes the beats of the transaction with the timestamp. So it takes in some sense better of the both vaults. So if it's only timestamp, then it's first come, first serve.
00:12:55.694 - 00:13:48.710, Speaker B: But we want to mix the timestamp with the bead and beat is per gas or per resource unit. In case of ethereum it's called gas or in case of arbitrum. But maybe there are some names in other chains and that is important, that will become later clear why it is important. Okay, so now instead of investing all the resources into reducing the latency and improving the timestamp, players try to increase their bid and invest at least fraction of their resources in the bidding. We analyze this proposal from the economic perspective, and it is almost equivalent to the first price. All pay auction. And all pay means that even if transaction sender does not like the position of its transaction, it has to pay for it.
00:13:48.710 - 00:15:04.510, Speaker B: So there is no approach of block building here, where you are only paying if your transaction or set of transactions are in particular order, especially against other transactions. So the so called sandwich attack, but you pay always you try to get as early as possible, because we are only looking into this kind of arbitrage searches. So only back running opportunities, opportunities where it's only about to be as fast as possible. Okay, so then the proposal satisfies the economic properties of these first price all pay auctions. And it's clearly incentive compatible because more you bid earlier, your transaction is scheduled, so you will bid more. Okay, so in these lines, if you want your transaction to be scheduled earlier, you may try to have lower latency. But if the price of bidding is not too high, you prefer to trade the money for time.
00:15:04.510 - 00:16:07.218, Speaker B: So pay for the time advantage. So this gives a chance to those players that have lower budget to have lower latency to pay high for a particular arbitrage opportunity and still win the race, which they would never be able to win any race against large players that have low latency. Okay, so this is our economic efficiency. And also it goes against the centralization because if you have the lowest latency at, let's say one particular chain, then you are winning. So one party is winning all the races. Okay, so here, one more property that we would like to have is that there is never a transaction that has bid low amount and it's executed before another transaction that comes right after it. So after a few milliseconds and bids much higher value.
00:16:07.218 - 00:17:08.580, Speaker B: And this problem is a fundamental problem of block based approaches. So if you have block based approach, so you have some time interval where you are collecting all transactions and sort them in some way. There are transactions that come right after block is created and they may beat high, just be unlucky not to be included in the previous block and they lose the race. Okay, so then last properties, we want to have fair treatment of unsophisticated players that bid zero. So they are just regular bidders who want to pay their regular gas fees and they are not contesting or participating into any race. 95% of transactions, let's say, are like this and they get at most g delay. So you can never outbid someone who came earlier than g time unit before you.
00:17:08.580 - 00:17:56.450, Speaker B: Then our more formal distribution of the description of the algorithm. There is some stream of transactions labeled by large t's. And each transaction has two characteristics. One is timestamp or the arrival time to a sequencer, which we denote by ti and bid per resource unit. So in case of arbitrum, that would be gas that we denote by bi. So the next transaction that is posted to be executed or included in the feed for the execution is the one that has the highest score. And let me give you description, what is the score function so we translate beats into the time advantage.
00:17:56.450 - 00:18:34.734, Speaker B: So if a player beats amount bi in some token, then the priority time is given by this formula. This is upper bounded by g. So if you take bi goes to infinity, the limit is g. So you can never get more than g priority time. So c some constant that for the analysis we set, it equals to one. So you think that this is, let's say one ethereum. In case of arbitrum, it would be one ethereum.
00:18:34.734 - 00:19:28.900, Speaker B: So, to buy g half time, you need to pay one ethereum in total. So that would, if we, for example, consider swap transactions, we would divide by the amount of cash that swap transactions take. Okay, so then the score is calculated by priority time minus the timestamp. And we sort the transactions within site, so we don't know future transactions. So far we see, let's say window of lengths g time units with the decreasing score. And the transaction that has the lowest. So highest score is executed next.
00:19:28.900 - 00:20:30.230, Speaker B: Okay, so first property that we obtain is maybe very mathematical. We obtain the result that if we want the property of independence of irrelevant transactions, then the only algorithm or the class of algorithms that satisfy this property is that are based on scores. So we post transactions to be executed by their score only. And we don't compare any other global property. Okay, so this is mathematical result. Now, about the functional form of PI, why did we take this particular function? So, first of all, if transaction sender pays zero, it gets zero priority. So that's normalization.
00:20:30.230 - 00:21:11.618, Speaker B: We wanted that maximum time priority or priority time one could buy is g. So if we take x goes to infinity, so bit goes to infinity, then we get to g. So you can come arbitrary close to g, but you can never get to g. Also, of course we want that if more bids are paid, the priority time is higher. So that's the first derivative is positive. And this is maybe more technical. We want the priority time to be concave in bid, which would mean that producing some priority time.
00:21:11.618 - 00:22:00.020, Speaker B: So cost of producing would be convex and that is needed to have internal solution to the equilibrium. So this something more technical. But yeah, this is also quite needed property. And then with these four properties, in my opinion, the function that is described here is the easiest. But if there is anything else, I really curious to see. So now, the algorithm, the transaction that has the highest score, so has the priority time minus timestamp or the lowest update timestamp. So if you negate that score, then you would get timestamp minus the priority time is posted the first.
00:22:00.020 - 00:22:46.514, Speaker B: And if the transaction that algorithm posts is the earliest arrived in real time. Then we move the time to the right, so the time window moves to the right. If it's not the earliest, then we still keep the time window in the same position and we just continue to posting other transactions that have lower score. Okay, so, the complexity of this algorithm is as simple as it gets. The space complexity, it's linear. We just need to keep the transactions so we don't construct any additional data structure around them. And the runtime is not the lowest possible, but it's quite low.
00:22:46.514 - 00:23:32.634, Speaker B: It's n times login. We just need to sort the transaction so we can have some priority keep structure that keeps the highest core transaction in the root. And this allows us to handle tens of thousands of transactions per second. So we don't get any disadvantage from this, because if you come up with any other algorithm, complicated algorithm, for example, graph based algorithm about transactions, then this will easily scale to n square. And then you already have much slower algorithm. So, let me jump to the economic analysis of this proposal. We have very simple stylized model.
00:23:32.634 - 00:24:12.590, Speaker B: Suppose there is some arbitrage. And now players need to take a decision how fast they need to respond to this arbitrage opportunity. And there is some cost to have, let's say average time or exact delivery time of your transaction. After t time, the arbitrage opportunity was there, and the users have some valuation to be the first. So there is some value to derive. And different users may have different valuation. It depends how many assets or how much assets they have on the chain.
00:24:12.590 - 00:24:55.850, Speaker B: So, higher liquidity means that valuation will be higher, let's say. And we have very simple model with two players that have these random valuations. So they don't know each other's valuations, but they know how the valuations are distributed. So there are these distribution functions. So now the players take a decision how fast they want to react or what should be their latency. And in case of bidding, what should be the bid. So, we assume that latency cost function is as simple as it can get.
00:24:55.850 - 00:25:51.114, Speaker B: So, in order to be after t time your transaction to be delivered to the sequencer, you pay price CI, so it costs you cit, which is one over t, which means that if you want to be right after arbitrage opportunity is there, it costs you infinity. But if you don't care and you take e equal to infinity, then you don't pay anything. And distributions are as simple as it can get. Let's say they are just uniformly distributed on zero one. So everything is normalized. So there are two ways to look at this. In the first, it's much simpler and much more natural assumption that players invest in the latency in advance.
00:25:51.114 - 00:26:49.994, Speaker B: So before they see the arbitrage opportunity and they know what their valuation is for this opportunity, they invest in the latency in advance. And so there are different examples how you can do that. But we are abstract away from this. And in the other one, we assume that sender can invest even in the latency after arbitrage opportunity is seen. Or let's say there is some arbitrage opportunity, potential arbitrage opportunity, and you can condition on that with some service provider. So if that arises, then I want you to deliver, the service provider delivers the transaction with some latency, and you pay for lower latency. Of course you pay more, but in both cases, the bidding is assumed to be exposed decision or interim decision, meaning after you know what is your valuation, you decide how much to bid.
00:26:49.994 - 00:27:31.850, Speaker B: And that is very realistic assumption. Okay, so the only latency investment we are getting much easier game. So it's a static game. Let's say that Xi is the amount that is invested in the latency by player I. And payoffs are the following. So if the player wins the game, then the payoff is expected value of arbitrage opportunity, because it's not realized yet. We haven't seen the arbitrage opportunity minus the amount that was already invested.
00:27:31.850 - 00:28:09.334, Speaker B: If there is some pie, so they equally invest. Let's say there is one half probability that one of them wins. So this is the payoff in that case. And if you don't win, then it's just minus Xi. So the investment. And let's assume that there is one investment, so one arbitrage opportunity, or you can think about it as average. So here it's already actually averaged, but you need to outbid in the latency competition to your competitor.
00:28:09.334 - 00:28:56.242, Speaker B: So your XI should be larger than XJ of the player J. So in this game, there is no pure strategy in nature equilibrium. That's why we need to focus on the mixed strategies. And mixed strategy equilibrium is very simple. They just choose the effort level uniformly at random, and we can show that this is the equilibrium. I will not go in the proof because of time, but the idea is that both of them in this game have expected pay of zero, which is kind of fair and expected result. It's a perfect competition, so no party should have any advantage.
00:28:56.242 - 00:29:46.902, Speaker B: And they are completely exhausting their resources on average. So their payoffs are zero. But if there is some budget constraints or you cannot invest any amount you like, because you have some budget for the investment in the latency. So that actually refers to the case that there are weaker players that have lower budget and stronger players that have higher budget. Again, there is no pure Nash equilibrium here, but there is again mixed strategy equilibrium in which. So there are of course some mixed strategies here, but the main idea is that the weak agent always obtains the average payoff zero and strong agent obtains some positive payoff. And that gives advantage to the strong agent.
00:29:46.902 - 00:30:31.106, Speaker B: And this result is quite robust, actually. So this may be equilibrium is not unique, but payoffs are unique. So in any mixed equilibrium, you will get these payoffs. Okay, so now we study the sequential game where we have some interim bidding. So, once you know the value of the arbitrage opportunity, you decide how much to bid, but you know your opponent's latency. And this is a reasonable assumption if the game is played repeatedly. And this is the case, actually you observe what your opponent's latency level is, or how often it is winning.
00:30:31.106 - 00:31:28.878, Speaker B: And here the game is much more complicated. So they have already invested in the latency technology. And in order to produce a score sigma, you need to pay this amount. So I will not go into the formula, but it uses the formula of our score function. So, in the equilibrium, we need to determine, depending on the valuation and on the investment levels of the latency, how much score we need to produce or equivalently how much we should beat. And this is an optimization problem for both players where they need to decide on the signal or on the score. And this is solved by first order condition.
00:31:28.878 - 00:32:04.322, Speaker B: And in the end we get system of differential equations. So here it becomes a bit complicated. We are not able to solve this analytically, but we still managed to obtain some observations. And let me go into the observations. So if in the latency, both of them invested the same amount. So delta denotes the time. So, latency difference between these players and this delta is equal to zero.
00:32:04.322 - 00:32:52.110, Speaker B: So we have symmetric case, we have a completely separating equilibrium, which means that depending on the valuation, the bid is increasing. So bid is increasing in the valuation, and you start bidding as soon as valuation is positive. So we know exactly what is the value depending on the bid. And this is very efficient in case of asymmetric latencies. So now one party has advantage over the other. We have partially separating equilibrium means that bidders don't bid for some low valuations, but they start bidding for high enough valuations. And actually we can identify what is this threshold.
00:32:52.110 - 00:33:22.878, Speaker B: So, picture looks like this. So up to some point bidders don't bid and the party that has latency advantage so has lower latency, is always winning. But once the valuation is higher than this threshold, both of them start bidding the advantage. So lower latency player bids less. So that's blue line. And the one that is disadvantaged beats slightly more. So this is the formal result.
00:33:22.878 - 00:34:02.150, Speaker B: We identify what is threshold, it depends on delta and g. So this is the square root of delta over g minus delta. And yeah, so we know how this looks like. So now we know that in the case of same latency we have the standard all pay auction. This is the efficient outcome. And if there is some asymmetry in the latency, then we have kind of inefficient outcome that for low valuations they don't beat. So let's say the roll up also doesn't get any beats.
00:34:02.150 - 00:34:50.060, Speaker B: But if you take g so the valuation is large enough, then approximately equal signals are produced in asymptotic terms. Everything now depends on the parameter g and the latency difference delta. So if g is large enough, then auction becomes approximately efficient. So if you can buy more and more time. But of course this goes against low latency. We want g to be actually quite low. And we will see that if g is low, then the bidding is low because the threshold is large enough.
00:34:50.060 - 00:35:40.698, Speaker B: So if the parameters are chosen optimally, then we can get a balance between the fairness low latency and efficiency. So low latency, we want g to be low enough, fairness, we want that the disadvantaged each party to have some chances to win. And this comes with the efficiency that we want. Efficiency comes with the motivation that we want roll up to collect bits more often. Of course we can extend this analysis to more players and more general functions. And we assume that valuations are independent, but in reality they are actually quite dependent. But the qualitative results probably will not change.
00:35:40.698 - 00:36:38.970, Speaker B: Of course this is theoretical analysis so far, but we can get these estimates of these distribution functions from the data. Another interesting thing is how to update parameters. CNG it would be nice to have some rule that helps us when to increase c. Especially let's say g is fixed to half second we are thinking about it would be also interested to see how this proposal actually works in real life. So one other alternative is block based approach that we call frequent ordering auction, where we take all the transactions that come in g time interval. So we take them as a batch and sort them by decreasing bits and schedule them by decreasing bits. So this is simpler.
00:36:38.970 - 00:37:10.642, Speaker B: And to compare them to both satisfy the independence of irrelevant alternatives. Because we are still looking at the score based function. We just look at the transactions. So the now score is just transaction beat. Okay. Both have low latency. Actually, frequent order auction has, on average, half latency of the time boost proposal.
00:37:10.642 - 00:38:03.670, Speaker B: So this proposal that I discussed so far, the only disadvantage that frequent ordering auction has is that it allows low bid to make in the early bid, early block, and then the high bid transaction that didn't make it to early block just loses the race. But as I said, any block based or only time based approach would have this problem. And this also has. But there is also another aspect of this alternative proposal. It's much easier to decentralize. So there are trade offs between them. And I think it's very interesting to see which one is performing better in the practice.
00:38:03.670 - 00:38:11.080, Speaker B: So that's all from my side. Thank you for the attention. If you have any questions, happy to answer.
00:38:12.650 - 00:38:35.282, Speaker A: Thanks so much for joining. Cocky. Really appreciate it. Does anybody have any questions? I think if you have any, you can just unmute and go ahead and shout out. Well, I guess it looks like we don't have any questions. In that case, thank you so much for joining us, kaki, I really appreciate. Thank you so much.
00:38:35.282 - 00:38:36.546, Speaker A: All right, see everybody.
00:38:36.648 - 00:38:37.330, Speaker B: Bye.
00:38:37.670 - 00:38:50.800, Speaker A: Thanks. Bye. Anybody have a question? I saw one raised hand. Okay. All right, thanks, everybody. See you guys.
