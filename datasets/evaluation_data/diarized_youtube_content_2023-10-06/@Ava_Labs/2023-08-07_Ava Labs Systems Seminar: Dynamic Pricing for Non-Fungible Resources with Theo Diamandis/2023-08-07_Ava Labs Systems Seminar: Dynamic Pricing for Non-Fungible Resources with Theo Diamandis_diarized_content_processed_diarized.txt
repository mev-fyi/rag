00:00:11.850 - 00:00:27.240, Speaker A: Again, thank you so much for joining us, Theo. This is our first system seminar that Darish and I are as we're taking it over and we're hoping to record, publish, and throw these on our socials. So we're really excited to have you and learn more about dynamic pricing for our multi dimensional markets. Yeah, thank you for being here.
00:00:28.010 - 00:00:48.294, Speaker B: Let me share my screen real quick. Cool. So, hi everyone. My name is Theo. I'm currently a, I guess, fifth year or fourth year, depending on where the cutoff is. Phd student, MIT. And I also do research with Vein Capital Crypto, and this is work on designing multidimensional blockchain fee markets.
00:00:48.294 - 00:01:45.680, Speaker B: That was joint work with Alex, Evan, and Guillermo, and Jarris from Bain and then Tarun Chitra from Gauntlet. And so kind of the big high level picture that I want to convince you of is that fee markets with fixed relative prices are inefficient. So this is denominating, say, everything in some joint unit of account like gas. And what we introduced here, which I was talking about a little bit before when we were chatting, but just this is kind of a framework to optimally set multidimensional fees. So a framework for a network designer to figure out how to set fees for their network. And first, I want to convince you of that. First claim that I made is like, why we don't want to do fixed relative prices, and then we'll kind of get into defining the model, figuring out kind of an optimization problem that we want to solve, and then coming up with a way to update prices so that we're implicitly solving said problem.
00:01:45.680 - 00:02:40.320, Speaker B: And the first thing is that fixed relative prices in the past have led to denial of service type attacks on the network. Because, say, like in the EDM system, all opcodes have fixed relative prices to each other, measured in gaps or just some unit of account. And potential mismatches in the past between these relative prices and the resource usage has led to what's been termed resource exhaustion attacks in the literature, or which are simply just denial of service attacks. And so there's notable one in 2016, which exploited a disk read mispronouncing, which is why syncing a node can take so long once it kind of hits that period. And then, of course, opcodes had to be manually adjusted. I can hear the chat, but I can't see it the way my screen is right now. So if there are questions, just feel free to butt in and stop me.
00:02:40.320 - 00:03:10.330, Speaker B: More importantly, or perhaps for this talk, is we want to see how fixed relative prices limit throughput and solve this. So let's consider like a really simple toy example where we have a mem pool, a bunch of transactions. Some transactions cost only cpu. So they only cost compute. They don't take any other resource. Of course, this is a toy example. Some transactions only cost kind of bandwidth.
00:03:10.330 - 00:03:50.374, Speaker B: These both have a gas cost of one, and there is a utility difference. So the cpu transactions have a utility of four. The bandwidth transactions have utility of two. If we have a one dimensional market where say the gas price is three, and we have a block that has four slots of compute and four slots of bandwidth, we're of course going to fill up all of the compute. But we're not going to use any of the bandwidth because we would have negative utility by paying three for these transactions, which have a utility of two. However, if we had a two dimension, these two things could float independently. Maybe the cost of CPU would end up equilibrating at three.
00:03:50.374 - 00:04:36.374, Speaker B: The cost of bandwidth would end up being one, and we would be able to fully fill this block. So you can see that kind of when resources are totally orthogonal, they should be priced separately. And this naturally motivates kind of trying to figure out how to design a fee market, how to design some mechanism to price these resources. So I'm going to define kind of these terms a little bit. And give a model or a mathematical model for the rest of this talk. And first I want to kind of talk about when we say a resource like what I mean by this and the definition we're going to use is it's going to be anything that could be metered. So this could be, for instance, blob data.
00:04:36.374 - 00:05:18.414, Speaker B: So kind of the EIP 48 44 effectively introduces kind of a two gas market with blob data and kind of the rest of execution, et cetera. So it separates out blob data from call data. We could consider broad categories like compute, memory storage. However, we could also go granular down to sequence or to opcodes, where each opcode is its own resource and priced separately. We could even go more granular than that and talk about sequences of opcodes. So, for instance, let's say we use a memory slot repeatedly. The first time we hit that memory slot, it's going to be cold.
00:05:18.414 - 00:06:20.162, Speaker B: But then after that it's going to be hot. So maybe calling some sequence of opcodes is cheaper than calling one opcode and multiplying that by the gas price instead. We might want to reward users for using memory in an effective way. And you could see how this could get quite a bit, quite complicated, depending on how we could also say compute on a specific core is its own resource, and so on. So this is something that's really up to the network designer as to what makes sense. And of course, more granular resources are going to require more overhead for the market, more coarse resources, kind of the courses of which being the one dimensional gas markets are going to be easier from a compute perspective, but may give a trade off in terms of performance or throughput. So, to formalize our model, we're going to consider transaction j's or transactions j, which are going to consume some vector of resources aj.
00:06:20.162 - 00:06:57.170, Speaker B: There's going to be m resources. So this is going to be a vector in rm, and we're going to assume that this is non negative, although it doesn't necessarily have to be. So then that means that the ith entry of this vector aj is going to denote the amount of resource I consumed by transaction j. Then we're going to introduce another vector x, which is this binary vector. So each element is either zero or one, which is going to record which of n possible transactions. So that might be transactions sitting in the mempool are included in a particular block. So just xj is one if transaction j is included, and zero otherwise.
00:06:57.170 - 00:07:51.790, Speaker B: This allows us to write out the quantity of resources consumed by this block as just the sum of the xjs times the ajs, which we can put in matrix vector notation as ax. And we're going to call this resource consumption y. So the columns of a here are each resource vector and then x is this binary vector. Usually as the network designer, we kind of want to do things like constraining and charging for each resource used. So for example, we might have some resource consumption target, which we're going to call b star. And then of course the deviation from the target is the amount of resource that the blocks use, which is y or ax minus b star. In Ethereum, this is 15 million gas, and we might also have something like a resource consumption limit that transactions included must satisfy.
00:07:51.790 - 00:08:25.722, Speaker B: Ax is less than or equal to b. So in Ethereum this is 30 million gas. Where something is a block is invalid if we don't satisfy this constraint. Finally, we want to introduce a price for each resource where we charge some price PI for the cost of resource I. And that means that for a transaction j, it's going to cost the dot product of p and its resource vector aj. And when I talk about cost here, this isn't cost. That's going from the user to the block builder or the user to the validator.
00:08:25.722 - 00:09:03.734, Speaker B: This is what's burned by the network. So this is cost from kind of the group of people that's interacting with the network and the network itself. So these are kind of the two in this talk. These are the two groups to think about. So like kind of Allah EIP 1559. This would be like what's burned? And the natural question then is, well, how do we determine these prices? There's a few really natural properties that we would want. For instance, if the quantity of resource I that's used by a block, which is the it entry of ax, is equal to our target bi star, then probably we don't want to update.
00:09:03.734 - 00:10:15.906, Speaker B: That means we chose a good price. On the other hand, if the block is using too much of a resource or more than our target, we probably want to increase that price so that fewer people use this resource in the future. And similarly, if we're using under our target, maybe you want to decrease the price because we're charging too much for it. And there's actually like many, many possible update rules that have these three properties. So one thing that was proposed, this is back, kind of in the Ethereum forums back in January 2022, was this update rule, a variance of which is being used for 48, 44 right now. And you could check that this actually does satisfy all these properties where kind of, if things are equal, then we're multiplying by x of zero, which doesn't do anything, and then we kind of have the increase and decrease properties as well. Then the question is, well, is this a good update rule? And how do we even talk about what it means to be a good update rule? Like, what does good mean in this context? So what the rest of this talk is going to be about is showing that update rules are actually implicitly solving some optimization problem.
00:10:15.906 - 00:11:18.710, Speaker B: And the specific choice of an objective function by the network designer implies a specific update rule. So after this talk, I hope to convince you that instead of looking necessarily directly at the update rules when we're thinking about designing fee markets, we should be looking at the optimization problem instead. And that naturally leads us to the optimization problem, which we call the resource allocation problem. So the setting to set up this problem is that the network designer is going to be totally omniscient and is going to determine the transactions that are in each block. Of course, this is entirely unrealistic, and we'll see that we do not need this assumption whatsoever, but it helps us as a mental model to actually construct this problem. And then when we go to solve it, we'll be able to not have any of this. Okay, so the first ingredient that we need to introduce is a loss function, which is going to characterize the network or the network designer's unhappiness with the current resource usage.
00:11:18.710 - 00:12:18.790, Speaker B: So this is going to be a function of the resource usage, which is y. And for instance, one example might be that our loss is zero if we're exactly equal to our target, and otherwise if there's any deviation in any resource usage from the target, we incur a loss of infinity. This is kind of silly, but just for the sake of example, we could have things like this, or perhaps we actually don't care if we're under the target and we only want to penalize if resource consumption goes over the target. So this would essentially mean that we don't care if there are fewer transactions being submitted to the network than we would like or for less usage than our target usage. But we do care if we go over that. The next ingredient that we need is some way to talk about what transactions are allowable. And so I'm going to denote the set of allowable transactions by s.
00:12:18.790 - 00:13:14.940, Speaker B: And so this is going to be a subset of zero, one to the n because it's going to be a set of binary vectors. Recall that our variable x is the transactions that are included in a block. And so this is going to have to be in s. This set is going to include things like network constraints, so it'll include things like ax or the resource consumption is less than or equal to b, but it could also include very complicated things like interactions among transactions. So, for example, if there are a lot of people bidding for a particular liquidation opportunity for an underwater loan, and there's a lot of searchers trying to get that only one person can actually liquidate that loan in a block. And so s can include these types of interactions and these types of constraints as well. So you can think of this as a very general way to define the constraint set.
00:13:18.270 - 00:13:27.790, Speaker C: I think there's a question posted in the chat, which is, is the network target b star fixed independent of network size, nodes, power, et cetera?
00:13:29.650 - 00:14:31.300, Speaker B: It doesn't have to be, I guess. So the b star I introduced earlier is going to end up just kind of getting into the set s and or going into the loss function. So here. So I suppose it is fixed insofar as once you create the mechanism for the fee market, and then it's kind of set after that. And of course you could change this in the same way that you would do any other type of change to some part of the consensus mechanism for the network or some part of the, sorry, not the node client for the network, but it's not necessarily fixed insofar as like, I guess it's totally able to be decided by the network designer and then it can be updated at some point in the future. Does that answer the question? Yes. Thank you.
00:14:31.300 - 00:15:38.226, Speaker B: Cool. So we're going to do this slight mathematical sleight of hand, where instead of considering s, we're going to consider its convex hall, which I'm going to denote by convex s. And this essentially means you take all the points that are in s. So s here is going to be the subset of the n dimensional hypercube, and you're going to kind of fill in everything in between those points. So as a result, this means that some transaction j can be partially included in a block. And the mental model for thinking about that is that if xj is between zero and one, so let's say it's like one half, that roughly means that we can think of transaction j being included in the next two blocks, or after roughly the next like one over xj blocks. And we'll see that this actually allows us to apply the theoretical tools that we want in a very sound way, but then it doesn't end up being necessary in kind of our final algorithm.
00:15:38.226 - 00:16:30.780, Speaker B: So this is again the type of thing that we're going to use right now to get to where we want to go, but then we'll see that it's not important. So the final ingredient is we need some way to talk about utility of transactions to the transaction producers. And so this is going to be clump together all the people that interact with the network. So like I said before, we're kind of considering now the people that interact with the network. And the network is like the two players in this. And if transaction j is included, this group of people interacting with the network is going to get some joint utility QJ. And of course we almost never know q in practice, but we'll see that the network doesn't need to know q as long as people are behaving not even rationally, but kind of in a utility maximizing way.
00:16:30.780 - 00:17:36.960, Speaker B: So then this allows us to write down the resource allocation problem, and that is just to maximize the utility of included transactions, which is just the sum of the qjs that are included minus the loss incurred by the network. One thing to note is that these two things have to be in the same units and then constrained to that. Y is the resource usage of the included transactions, and x is in the set of allowable transactions s, which again can be this very complex set that's hard to solve. So as I mentioned earlier, the network designer of course cannot solve this in practice to build blocks for a whole number of reasons. First, as the network designer, we don't decide which transactions are actually included in the block. We have no way to necessarily know the utilities of the transactions to the transaction producers, and we also have no way to include fractional transactions as well. That doesn't necessarily even make sense.
00:17:36.960 - 00:18:47.628, Speaker B: So to actually solve this problem, we're going to turn to convex duality theory, which is going to give us a way to, as the network, implement a fee mechanism such that the people interacting with the network are implicitly solving this problem. And before I go on to that, are there any questions about the optimization problem itself? Cool. So at a very high level, duality theory gives us tools to relax constraints and problems to penalties. And then if we correctly set the penalties by solving this relaxed problem, we actually solve the original problem. And one thing to notice here is that the network designer cares about the resource utilization, which is y, which is based on the transactions x. But the network designer doesn't necessarily care about the transactions x directly. On the other hand, block builders don't really care about the network resource utilization or people interacting with the network.
00:18:47.628 - 00:20:04.916, Speaker B: They only care about what transactions they can include. So this kind of motivates trying to decouple utilization of the network y, and kind of what transaction producers can produce x, which is given by this constraint here, y is equal to ax. And again, if we correctly set the penalty on this relaxed constraint, that means that the dual problem which I'm going to define momentarily, which is this kind of relaxed problem, is going to, solving the dual problem is going to be equivalent to solving the original problem, and the resource utilizations are going to be equal. So the dual problem is going to be find the prices p that minimize the dual function, which I'm going to call g of p from before. These prices p are the prices for violating this constraint. And because this vector y is actually the resource usage vector, these prices are going to be exactly the prices that we want users to pay for their resource utilization. So the ghoul interpretation is that we are paying per unit violation of this constraint.
00:20:04.916 - 00:20:44.100, Speaker B: The mechanism interpretation is that we're paying per unit of consumption of this resource. And essentially the way to think about it is that the prices p are going to try to force the transaction submitted to be that of what the network wants so force kind of y and ax to be closer and closer together. The problem is separable. So GFP, this dual function, which is below, decomposes into two kind of nicely interpretable terms. The first one is this l conjugate. Evaluate the prices p. And this is kind of like the network term.
00:20:44.100 - 00:21:50.936, Speaker B: The only really thing to know about the conjugate function here is it's like this very standard object in convex optimization. It's very often known in closed form, but you can think of it as being something that's easy to evaluate. So this can be done on chain. This is something where the Computation isn't some huge overhead that you're imposing on people running full nodes. Then the second term isn't just look at it a little bit more. The second term is just to maximize the net utility, which is q, which is the utility minus a transpose p is going to give the cost in terms of fees to the network for each transaction and include the transactions that maximize utility, subject to these being allowable transactions, which of course is exactly the block building problem. One thing to note back to what I said earlier, we actually don't have to use the convex hall of s here.
00:21:50.936 - 00:22:36.420, Speaker B: Some results from linear programming tell us that if we replace the convex hall of s with s, this problem has the exact same optimal value. And as I said, this ends up being the exact problem solved by block builders. The important implication of this is that the network doesn't have to solve this problem. The network can just observe the optimal value, which I'll denote by x star by say, previous block. Before I talk about how to solve this dual problem, let's convince ourselves that this is actually the thing that we want to be solving. And there's a question. Well, what do we get at optimality? Well, first, let's assume that we've set the prices optimally.
00:22:36.420 - 00:23:08.032, Speaker B: So p star is a minimizer of g of p. And then we'll assume the block building problem has an optimal solution, x star, given these prices, p star. So recall that p appears in the block building problem. So this x star is going to be a function of p star. The optimality conditions for the dual problem are just that, the gradient is equal to zero. This is an unconstrained convex optimization problem. And that's exactly that.
00:23:08.032 - 00:24:00.768, Speaker B: Y star minus ax star is equal to zero, where y star comes, which comes from this conjugate function. Satisfy. The first thing to note here is that this is exactly our original constraint. So this tells us that at optimality, the dual problem satisfies the constraint of the primal problem, then let's look at this condition real quick here. So this says that the gradient of the loss function evaluated at y star is equal to the optimal prices, p. If we substitute back in, we know that y star is equal to ax star. And so the interpretation of this then, is that the prices that minimize g, so that minimize the dual function, charge the transaction producers exactly the marginal costs faced by the network.
00:24:00.768 - 00:25:22.910, Speaker B: So the marginal costs faced by the network are just the first derivative or the gradient of the loss function, and that's exactly equal to p star, the optimal prices. Furthermore, because of duality theory, because this dual problem gives us the solution to the original problem, these prices are those that incentivize the transaction producers to include the transactions that maximize the welfare generated minus the network loss. So without knowing q, without actually being able to control anything other than the prices, if we set prices optimally, then we solve that original problem, that kind of impossible problem that the omniscient net designer wants to solve, which is all great. But then the question is, how do we actually minimize GFP? One thing to note here is that we can compute the gradient of it, which is just this y star, which can be computed pretty easily, minus a times x star. Again, the network can determine y star from that conjugate function. And the mental model for this is, it's like computationally easy. The mental model for x is that it's computationally very difficult, actually.
00:25:22.910 - 00:25:58.376, Speaker B: But the network doesn't have to solve it. The network can observe it by looking at the previous block, because it is exactly that solution to the block building problem. So then we have the gradient, and we can just apply our favorite gradient based optimization method. Here's an example of gradient descent, but you could imagine momentum terms. You can imagine scaling different resources differently. So that say maybe there's particular resources where you want to update the prices very quickly, and other resources where maybe you update the price more slowly. This is all kind of things that the network designer could do.
00:25:58.376 - 00:27:05.468, Speaker B: And this is something that we actually talk about a little bit more in the appendix to, sorry, not the appendix, I think the extension section, which is the second to last section of our paper. So some simple examples of, like a loss function and the update rule implied the loss functions we talked about earlier. This one that's just, we're infinitely unhappy if we have any deviation from our target and zero unhappiness if we're at our target, gives this rule that looks like very similar to kind of a very standard gradient descent, where we're just updating p with a step in the direction of the residual. Interestingly, if we take y is less than or equal to b star, and that's our zero loss. So we don't care if we're underutilizing resources, we get the same thing, except these prices have to be non negative. So this operator here just means that we're taking the non negative part of this vector. So if anything's negative, we're zeroing it out.
00:27:05.468 - 00:27:53.928, Speaker B: And this makes sense because in the first case we're unhappy if we're underutilizing, not just if we're overutilizing. So maybe in the first case we could have negative prices, which can be thought of as subsidies. So for instance, if we're trying to bootstrap the network and try to get to some level of resource utilization, perhaps we want to incentivize people to use the network, so we would actually charge negative prices. Whereas in the second case, in this loss function, we're saying that we actually don't care if, say, resource utilization is zero. We don't care if no one's submitting transactions at all. So we're only going to charge non negative prices. And finally, the one that I talked about earlier, this exponential update rule, unfortunately, it's a little bit more involved.
00:27:53.928 - 00:28:44.524, Speaker B: So I'm not going to go into in this presentation, but if you're interested, this is in appendix C of our archive paper. It follows essentially from a log transform of the variables, and this is also going to constrain the variables to be non negative as well, or the prices to be non negative as well. Okay, so that's most of the theory. One thing to note is actually instead of charging from a per resource lens, you could translate this entire thing to charge from like a per contract call as well, which has been discussed in various communities. And we talk about that in the extension section of our paper as well. But some very brief kind of silly numerical experiments. This is something that definitely future work would do.
00:28:44.524 - 00:30:00.500, Speaker B: This on a testnet kind of have much more robust numerical examples, but for illustrative purposes, we ran some really simple simulations and kind of even this is like a steady state type distribution where you have one transaction that's being submitted, and we can see that kind of having this multidimensional prices. So in this case, it's a two dimensional price instead of a one dimensional price does increase throughput. You could see kind of, we start and then we run gradient descent to let the prices kind of converge to their optimal values on the right. What's really interesting though is what happens when we change the distribution of the transactions that are submitted. So, for instance, here we add a bunch of different types of transactions, which we call like type two transactions at block ten. And you can think of these as, say, something like an NFT mint, anything, where the distribution of, kind of the steady distribution of what people do on the network is going to change for a very short period of time. And the two kind of bluish lines are the throughput under multidimensional prices.
00:30:00.500 - 00:30:54.820, Speaker B: In terms of number of transactions. The two orange and red line are under uniform prices, so the single dimensional prices. And you can see that we're getting better throughput actually on both types of transactions by using multidimensional prices. And on the right here, you can kind of see what the network is doing is essentially what happens is it really cranks up the price of resource two and drops the price of resource one a little bit until kind of it's gone through the backlog of this type two transactions. Then it lets the price of resource one spike and then kind of allows these things to settle back to their, kind of their steady state level. It's also interesting to look directly at what the resource consumption here looks like. So on the left you have multidimensional fees.
00:30:54.820 - 00:31:36.020, Speaker B: The dashed lines are b star, which are targets for each type of resource. The dotted lines are b, which is the limit. So this is kind of our burst capacity for each resource. And you can see that essentially what happens is for resource two, we crank it all the way up to the limit for some short amount of time, and then we go right back to target. For resource one, we allow it to go down for a little while, and then we're going to allow some burst capacity to clear out the backlog. And then it's going to track the target back to track the target as well. When we have one dimensional fees, we see this kind of oscillatory behavior.
00:31:36.020 - 00:32:34.760, Speaker B: This has been observed by others. There was a recent paper at financial cryptography this year, I think, that observed some types of oscillations in fee markets as well, which this might suggest. It's kind of due to say, only having one knob on your controller. Instead of with multidimensional fees, you have multiple knobs on your controller. But you see this kind of oscillatory behavior and it takes a while to start to settle back down to the target. So hopefully the main takeaway that you've gotten from this talk is that it's really valuable to kind of choose your objective function and think about designing the fee market in terms of this resource allocation problem, and then use that to design some optimal price update rule by looking at this objective function instead of starting from the price update rule directly. A few pretty interesting, I think, additional results.
00:32:34.760 - 00:33:55.904, Speaker B: One thing that we kind of were thinking about a lot is like, to what extent is this optimal? To what extent can you give certain types of guarantees on this framework? And one interesting thing is by applying kind of online convex optimization results, you can show something to the effective that this framework has zero regret on average. And what that means is that if you were totally omniscient and you knew all the transactions that people were going to submit forever, kind of as t, as the number of blocks goes to infinity and you set all the relative prices from the get go up front, you would not do any better, kind of on average as time goes to infinity than by using our framework. So in some ways, like this adaptive framework is always going to be better than having these fixed relative prices. And this is somewhat stronger than some of the traditional game theoretic results in that we don't require the adversary that's trying to screw with the pricing mechanism to be rational. We only require them to have a bounded budget. And you also don't need to do anything like playington equilibrium. This is likely relevant for many similar mechanisms as well.
00:33:55.904 - 00:35:01.044, Speaker B: And one really interesting thing about this mechanism is because you have this kind of pricing mechanism, in some ways you are charging the adversary to interact with your mechanism. And there's some indication that actually if you set the prices in a certain way, roughly proportional to the square of the gradient, you can actually get better than zero regret on average. Which essentially means that doing some type of dynamic mechanism like this, or actually doing this dynamic mechanism to set fees is better than being omniscient and setting the fees kind of from the get go, knowing all the transactions forever. So yeah, for more info, really encourage you to check out the paper. And yeah, thanks for the opportunity to present. I'm reachable on Twitter, but much more responsive via email. So the email that was on the calendar invite is probably the best.
00:35:01.044 - 00:35:04.920, Speaker B: Thank you. Awesome.
00:35:04.990 - 00:35:15.770, Speaker A: Thank you so much for, thank you so much for joining us. It was a great presentation. Does anybody have any questions? I know Steven's been pretty interested in a lot of this stuff for a.
00:35:17.180 - 00:36:31.170, Speaker C: Sure, I can definitely for. Thanks for doing this. I think it all made a lot of sense, and I think it definitely makes sense to model these kind of auction systems as an lp for benefit analysis. But one thing that I'm kind of interested on, your thoughts are they're tangentially related to this, but it's pretty topical in ethereum, I think, right now, which is around the ability of the network to actually charge the fees. So if you look in, I think, whatever, yesterday, some random person made some publicity stunt around basically being able to bribe the miners to choose to ignore the network optimization problem and basically optimize for their own benefit. And that just seems like kind of an interesting aside here, where this is focusing, kind of maybe assuming that the only way that the block builders can build is with the network's best interest in mind. So I don't know if you have thoughts on that.
00:36:33.700 - 00:36:46.890, Speaker B: Yeah, I'm not familiar with exactly what you're referring to. I can give an example in some ways. Sure.
00:36:48.540 - 00:37:35.510, Speaker C: Right now with EF 1559, you have burned fees for the base fee, and then you have the tip. Right. And so the miners are incentivized to basically stuff the block with whatever is going to give them the most money. So they don't necessarily care about the base d, they just care about the tip. So if you can bribe a miner and basically just pay them to say, you can either include my transaction, which doesn't actually use any gas, so there's like no base fee that's actually paid out to the network. But I am taking up that full block as my transaction, then the block builder still gets the same reward that they would have had for the sum of all the tips of the actual transactions by just doing nothing. And so they kind of get around this.
00:37:35.510 - 00:37:52.030, Speaker C: In the normal mempool world, it probably isn't too big of a problem, because the block builder can be selfish and include both. But in a proposer builder world, the block builder might not actually know what they're proposing. They just know the reward that they get.
00:37:53.680 - 00:39:27.924, Speaker B: Yeah, that's a great question. I think we're really only looking at kind of the burned fees, and we're more or less kind of sweeping under the rug the interaction between, say, the proposers and the users submitting transactions. So there's kind of a question as to when does this make sense? There's a paper from, I think, Matthias Ferreria and David parks at Harvard, along with some of their other collaborators. It's like dynamic posted price mechanisms for blockchain markets. And essentially, it does show to a certain extent that depending on kind of the demand for block space versus the amount that you have available in a certain regime, a mechanism like this is exactly what you want to do. But in another regime, it's going just to end up reducing to this first price auction, where the only thing that people care about is the tips, not what's burned. So then there's, like, two questions that come out from that is like, well, is there any way to get around? I think the first question is, is there any way to get around? Is there any way to make sure that you're kind of always in this regime, maybe by setting prices in a particular way or setting the loss function in a particular way, where you kind of always have these nicer game theoretic properties? I'm not a game theorist.
00:39:27.924 - 00:40:38.192, Speaker B: As a caveat, I work on the optimization stuff, but can we design things in such a way where we make sure that we stay in this regime and don't kind of get into the case where we're just running this first price auction and at that point, a lot of it breaks down? And I think that's where you kind of introduce some of these, say, attack vectors. But again, not a game theorist, so don't hold me to every word on that. So I think that's kind of the question there. But then if you could do that, then this type of thing makes sense as this kind of posted price auction mechanism. I think that's basically, it's like, if it is such that kind of the prices are high enough so the demand is such that people are like, the base fee is always going to be what's important. Like, the tip isn't going to be how people choose one transaction over the other. I think very roughly, it's been a little while since I've looked at this paper, but it's like, roughly, if you're not filling blocks, then you're probably fine.
00:40:38.192 - 00:41:24.640, Speaker B: But then once you start filling blocks and then people are just competing by upping their tip, then you get into kind of this first price option. And then I think that's when some of these kind of nasty situations to reason about appear. Yeah, I would check out that paper, and that's one of the things that we kind of have to think more carefully, I think, about with this framework. The fortunate thing is that it looks like a lot of the analysis for EIP 1559. These one dimensional markets kind of generalizes pretty naturally to these multidimensional cases. But if there's something that we haven't really solved yet in the one dimensional case, this framework doesn't fix it is what I would or I will not claim. This framework fixes it.
00:41:24.640 - 00:41:42.810, Speaker B: This is more of like that kind of throughput and solving these problems of throughput and then kind of these denial of service attacks that are, because you have these fixed relative prices and you can kind of exploit something that's cheaper than it should be.
00:41:43.900 - 00:41:59.340, Speaker C: Yeah, that makes sense. I was hypothesizing that you might be able to force the block builder to basically pay for the resources that weren't allocated. But. Yeah, I haven't read up in any of these papers.
00:42:03.280 - 00:42:10.210, Speaker B: Yeah, unfortunately I would have to look more at the details, but off the top of my head, I'm not sure.
00:42:14.900 - 00:42:27.190, Speaker A: We had one other question. In the Q A, how is the parameter in ETA said? Does it control how fast price is updated and how fast it reaches the optimal value? I think this was.
00:42:29.560 - 00:43:03.952, Speaker B: Yes, it's probably ether. It was a greeklet. Yeah, exactly. This is equivalent to the step size or the learning rate in gradient descent. You could also generalize this to be a diagonal matrix so you can a different step size or learning rate for every single resource. So maybe there are certain things that you want to adjust very quickly based on demand and then others that you want to adjust kind of more slowly. But yes, exactly.
00:43:03.952 - 00:43:42.430, Speaker B: To answer the question, it controls how fast you're updating the prices, and to a certain extent that's going to control how quickly you get to the optimal as well. But of course this problem is going to change for every single block because this x is kind of a vector per block. The prices probably will change as the distribution shifts. Thank you.
00:43:43.200 - 00:43:59.990, Speaker A: Does anybody else have any other questions? Awesome. Theo, thank you so much again for coming. It was great to have you on this talk and definitely looking forward to talking to you again. If you come to New York, then we'd love to have you.
00:44:01.400 - 00:44:05.988, Speaker B: Yeah, thanks so much for organizing. Awesome.
00:44:06.074 - 00:44:07.430, Speaker A: Thank you guys so much.
00:44:08.040 - 00:44:09.030, Speaker C: Thank you.
00:44:09.640 - 00:44:10.630, Speaker A: See everybody?
00:44:11.240 - 00:44:30.920, Speaker B: Thank you. It's.
