00:00:10.090 - 00:00:29.560, Speaker A: Everybody, welcome back to the Avalab system seminar. Today we're really excited to have Guillon ballet from the Go Ethereum team, who's done and led a huge amount of the vertical tree research over the years. So we'll be talking about verkel trees as well as a lot of the migration path to get there. So really excited, and thank you so much for coming, Guillaume, and I'll turn it over to you.
00:00:30.170 - 00:01:34.730, Speaker B: Hey, thanks for having me. Yes, indeed. So unfortunately, I need to start correcting you. My name is Guillaume, so don't worry, I'm not offended. I'm so used to people mispronouncing it that it's not a problem. Yeah, I've been working indeed at the EF for six years now, and recently I'd say in the last two years I've been working on something called what we refer now as the verge, which is the conversion of the current etherm state, which is stored in an MPT or Merkel Patricia tree into vertical trees. So, yeah, I don't expect to teach you a lot of things when it comes to client development, but yeah, I assume you're already aware that the state of Ethereum is growing, and we expect that in a year there will be about a billion leaves in the tree.
00:01:34.730 - 00:02:15.766, Speaker B: So this has consequences. One of those consequences is that databases, as they get bigger and bigger, they get slower, but it's also, in general, inducing a lot of maintenance issues with nodes. Well, first you need to upgrade your disks, because a lot of people bought a 1 tb disk and we're starting to hit that limit. It means that if you're trying to sync, it's going to take a long time. So we have made a lot of progress on the sync algorithms, even recently. But, yeah, it's still not satisfying. It's still quite difficult to think.
00:02:15.766 - 00:03:07.270, Speaker B: So, yeah, there's a lot of things that feels like the system is bursting at the seams, and the consequence is that more and more people are relying on centralized providers just because they don't want to maintain the node. And as a result, basically, Ethereum would no longer be the decentralized ecosystem that it strives for. It tries to be. So, of course, we tried to mitigate a few things. We repriced some opcodes. One of them we also can do the behavior. For example, I'm talking about the gas refund.
00:03:07.270 - 00:04:30.514, Speaker B: When you used to write some data that was nonzero, you would pay some gas to write it in the state. But as soon as you wrote zero instead, the assumption was that this would reduce the size of the state, so people would be incentivized to write zeros to delete what they create if they can. And that backfired really badly because someone created a contract called gas token, where people could just reserve space and hope to get the refund at a time when gas was more expensive. So very quickly that turned out to be the biggest contract, maybe not the biggest, but one of the big contracts in the Ethereum state. So clearly that was not a good idea. So we stopped doing that so that people don't keep adding more states. Well, that was not the initial goal of EIP 1559, but because it had some pressure on the gas price, making it like if you want to pay a lot of gas to create a block, to make a very big block, the price would go up.
00:04:30.514 - 00:05:26.806, Speaker B: So the hope was also that this would sort of reduce the gas price. There's clearly not a significant result with this on this front, but that was an interesting aspect of it as well. And then there's more technical things that really are gas specific. So we started taking stuff out of the database and putting it in an ancient store. So somewhere where if it's not going to be accessed, it's very unlikely to be accessed. In the day to day business of client execution, this will be stored out of the way and make the DB less, a bit smaller. We invented this thing called snapsync that comes with a snapshot, so it's basically a key value store.
00:05:26.806 - 00:06:23.490, Speaker B: So instead of jumping from node to node in the tree, you go straight to the data that added more data to the state, but it's improving the performance. So it's quite useful. And very recently, Gary from DF, especially from the guest team, has worked on the path based storage. So instead of storing nodes by hash, we store tree nodes by their path in the tree. So you can go straight to the node you're looking for, and that's improving the reads. That's also helping not store several versions of the tree in the disk. There's always one version of the tree in a disk, and all the updates are kept in memory in RAm until they can be collapsed and written to disk.
00:06:23.490 - 00:06:58.670, Speaker B: So that's a very useful change. And vertical trees are going to build on top of that. But yeah, so I would say there's still a lot of room for long term improvement. And we proposed ideas. So vertical trees are stateless, ethereum, something historic expiry. So it's all about deleting old data, old blocks, old logs, a lot of things that right now clients are keeping around, but maybe they should not. There's steady expiry scheme, there's been a few proposals.
00:06:58.670 - 00:07:34.320, Speaker B: None of it has been really worked on for the last three years at least, but it keeps coming back as a topic. And then there's also ideas where you would shard the state and distribute it all over the network. So that's what a project like the portal network is doing. So I'm going to focus on vertical trees because. Yeah, that's what I've been working on the last two years. Yeah, it's a new tree structure. So I don't know how much of the structure of Ethereum, you know, I assume quite a lot.
00:07:34.320 - 00:08:27.594, Speaker B: But just as a reminder, currently Ethereum has one single account tree, and then when you get to the account, you have a field in that account that is called the state route, and that is another tree, that's the root of another tree where you get all the data for that contract for that account. So all the storage that pertains to this account will be stored in that second tree. Well, with vertical trees we change that structure completely. We only use a single tree. So all account, all storage, all code, everything goes inside the same tree and we use a new key format to find everything. I will explain that in a minute. And we also replace hashes with polynomial commitments.
00:08:27.594 - 00:09:23.486, Speaker B: I will also talk a bit about this, it's a pretty big topic, so I will just give a very high level overview. But it's a very interesting topic. So yeah, I was talking about the new key formats. So what we do is each account has each item in an account. So for example, the balance, sorry, the balance, the nons, any slot, storage slot, any piece of code like chunk of code. So we chunk the code, we break it into chunks of 31 bytes plus some indicator bytes at the beginning to say if it's been pushed data, what we call push data, and every one of this item has an offset associated to it. So for example, the balance is at offset one, the nonsense is offset two, slot number 200.
00:09:23.486 - 00:10:19.438, Speaker B: Well we have this main, what we call the main storage offset, which is a huge number, and we add, well there's a bit of a complication I'm not going to get into, but we have a calculation so we can find the offset from this formula, the number -64 plus the main storage offset. And then the code chunk have a start offset of 128 plus the plus the trunk number. And what we do is to calculate the key. Where do we find each item in the tree? We calculate the key, sorry. By taking the address. So the offset is taken as a big NDN 32 byte number, and we only take the 31st bytes out of this and we just compute that hash. So it's not a ketchack, it's a Pedersen hash.
00:10:19.438 - 00:11:20.278, Speaker B: It's a hash that is based on elliptic curves, but has some nice properties, including ZK friendliness. And out of this hash, we take the first 31 bytes, and then we take the last byte of the offset that we did not include in the hash calculation, and we take it and we add it as the last byte of the key. So how does that work in practice? Well, if you take balance and nonce, we have the offset here. We take the first 31 bytes, so it will be all zeros, because those are numbers that fit in the byte. So it will fit in the least significant byte. So basically the 31st byte. And so you get the hash, which is the same, and then you take the 31st bytes of that hash and you just put the last byte as the last byte of the key.
00:11:20.278 - 00:12:18.918, Speaker B: So that has the effect that even though the data is spread all over the tree, it's actually grouped by groups of 256, which has some interesting properties and is going to imply a lot of changes, especially for solidity. The way solidity stores its data, which is just hashing a lot of its data in its map, which is just hashing the key and storing it in the middle of nowhere. That's going to be a bit of a problem because you prefer grouping things. So I would say array are going to be the preferred future data structure compared to maps. Yeah. So what I wanted to say is, if you take the slot, the offset of the slot, the number is no longer zero. So you will get the first 31 bytes of the offset are no longer zero.
00:12:18.918 - 00:13:26.186, Speaker B: So you will not get the same hash as you did for the previous two items, but you will take that number, and you will take that number modulo 256, and you will find it at the end of the key. And same thing for the code. So, yeah, I might be saying something very basic, but I think it's important to understand that the problem with the current structure, why do we switch to polynomial commitments? It's simply that if you want to prove that item B in a pre image is present first of all, and at the exact location that you want to prove it, to prove it is, then you have to pass all the other siblings. So if you're thinking of a tree node, you have to pass all the siblings. Basically, you have to pass the entire data. If you want to prove that one item is where you claim it as. So if you want to make a proof of the location of something in the tree, for every level, you have to pass all the siblings.
00:13:26.186 - 00:14:15.206, Speaker B: And that makes for very big proofs if you go for polynomial commitments. So, yeah, I'm only going to skim over the surface here because it's a very complex and deep topic. But assume you have a vector and you want to prove that B is at position one in a vector, you create a polynomial that is in Lagrange form. And, okay, I don't want to get too much into the math, but the idea is those polynomials, those L polynomials are known as the Lagrange basis. And l one, for example, will be one if x equals one, and all the others will be zero if x equals one. And, for example, l two will be one if x equals one. Sorry.
00:14:15.206 - 00:14:59.210, Speaker B: If x equals two equals two, and all the others will be zero if x equals two. So, yeah, don't want to get too much into the polynomial stuff at the moment. We can always discuss it afterwards if you're interested. But the idea is that when you pass a commitment, you just evaluate this polynomial at a random number that the prover does not control. So it can be done interactively, or it can be done through what's called the fiat shamir. Forgot the word. But like technique, I'm going to say technique, the prover cannot predict beforehand what this s is going to be, so they can't really cheat.
00:14:59.210 - 00:16:19.426, Speaker B: And that's what's interesting. The proof that your vector equals b at its first position is equivalent to saying there exists a polynomial that q that the prover will send to you and convince you it's a polynomial. Actually, the prover will send to you the polynomial evaluated at S and prove to you that it comes from a polynomial. And the way it proves it to you depends on the technique you choose. You can go with KCG, you can do that with what we use, which is IPM, and yourself, you are building a polynomial, which is actually the division of the polynomial value that you divide by. Basically, you factor out the root that you want to prove, and this division represents the evaluation of that second polynomial at the point you're interested in. And if they both agree, there's a lemma, it's called the schwarzleepilemma, that say that if two polynomials agree on one points, they are probably with very high probability they are the same polynomial.
00:16:19.426 - 00:17:07.030, Speaker B: Okay, so that was a lot of math. That was very simplified math. So if you want more details, you can always ask me later. But yeah, hopefully that was enough for you to get the point that the advantage is that if you want to prove the position of one value in a vector and what that value is, you only need some constant data. So that means that you can make your tree much wider. Like each of your branching nodes can become much wider because you don't have to pass the siblings. And because those trees become much wider, they also get more shallower.
00:17:07.030 - 00:17:56.280, Speaker B: And as a result, your proof, like there's some kind of virtual cycle, that your proofs becomes smaller and smaller. And just to finish with the tree structure, we also have a structure. I'm not going to get too much into the details, but the structure is basically divided in three parts. So forget about this part. It's only used to compute the commitment, and I'm not going to cover that. But the idea is that you have a first part, which is only branching nodes. So you can't have, like you do in the MPT, at least in theory, right now, an extension node in the middle, stuffed in the middle of all those branching nodes that is no longer possible.
00:17:56.280 - 00:18:28.174, Speaker B: And then you have some kind of extension node, and that corresponds to the stem. So the stem, I forgot to say, is that part here. The hash of the address and the 31st bytes of the offset. Actually, the first 31 bytes of the hash of the address and the first 31 bytes of the offset. So it's a 31 byte value. And so the last byte represents the group. So that's what I was saying.
00:18:28.174 - 00:19:57.850, Speaker B: The values are grouped by groups of 256 values. And let's say that this group, because the tree is fairly shallow, it will not share more than maybe four to six prefixed bytes with any other group in the tree. So in order to not have branching nodes with only one value in the tree, all the way down, all the way along those 31 bytes that we call the stem, what we do is we create this extension node that just says, yeah, that just contains the stem. And that's quite interesting, because unlike the MPT that we use in Ethereum, if for whatever reason, you find yourself inserting in the tree and you end up in having to insert into the extension node, so you will have to create intermediate branching nodes, of course, but the commitment or the equivalent of the hash to this tree doesn't change. So because the extension node in the MPT, you would have to break the extension into two pieces. And so the node itself that used to be there will have its commitment updated. Not so here, which is quite interesting because you don't have to recompute everything, or at least the commitments to that node that gets a new sibling.
00:19:57.850 - 00:20:48.086, Speaker B: Yeah, so trying to not spend too much time on that either. But why do we have vertical trees, or why do we want to use vertical trees? Well, basically because the tree gets smaller, the proofs that are connected to its use gets smaller. So it makes stateless clients, light clients, more usable or more practical. At least. You can also enable other features, like I was talking about the portal network. So you could imagine getting the data as you need it off the network in real time, and verifying the proofs. And another reason we also designed this whole very complicated structure has several intents in mind, but one of them is to be ZK friendly.
00:20:48.086 - 00:22:10.134, Speaker B: So everything is, all the hashes are really, all the values are considered to be field elements, and that plays along nicely, or more nicely than the MPT currently does. So now let's talk about the conversion and the fork itself. So yeah, there's this item on the roadmap that's called the verge, after the merge we had, trying to remember, what was the name of 4844, the surge, I think the next one is called the verge, and it's about translating an MPT to a vertical tree. And the idea is also to make the proofs of all the data that gets accessed in the block widely available. So either we were either going to put them inside the block, we could just spread them on a different peer to peer network, we could just dump them on some website. Either way, the idea is, when you get the block, you can download the proof and its witness, and this is all you need to execute the block. You no longer need to sync the network to have the right state so that you can verify if the block is correct or not.
00:22:10.134 - 00:23:09.770, Speaker B: And that will lead to other interesting properties, namely that you could validate the block without being synced. And if you need to produce a block, we have this other thing called the PBS proposal builder separation. So when it's your turn to produce a block, what you do is you just ask a builder to do it for you, a builder that would have the whole state. And what you do is you validate the block yourself, because the builder will be incentivized to give you the proof anyway, otherwise you cannot propagate their block. So you verify the proof, and you just propagate the block around the network along with the proof so that people can attest to it. So yeah, the real difficulty here is in conversion, because like I said earlier, in one year there will be about 1 billion leaves in tree. There's no going back.
00:23:09.770 - 00:24:10.460, Speaker B: Given the amount of computation of data that is involved, the amount of Ram, disk, everything, you have to get it right the first time. It's not like you can make an emergency fork to solve everything. When we tried to see what it was like, how much data and how much resources, how many resources we would need to produce this, a very fast machine from actually a couple of years ago, but still axion took roughly 18 hours. 16gb was really the minimal requirement. Honestly, you need more like 32. And it dumped roughly 150 extra gigabytes. So yeah, all of that has a cost and is more importantly a pain for the node operator to deal with.
00:24:10.460 - 00:25:12.142, Speaker B: So we've been looking at some design ideas, and what we saw was what we looked at are the following items. So is it easy to sync? How much of a burden is it on the node operator? Risks of bugs, of course. How long does the conversion will take, et cetera, et cetera. So we came up with those five ideas. So the first one is called the roll up appreciation month. And it's terrible idea, but it has a lot of good properties nonetheless, which is why I mentioned it. The name itself is a joke, but the idea is that for a month you only produce empty blocks, so no transactions get included on main net.
00:25:12.142 - 00:25:50.542, Speaker B: That means if people want to transact, to interact with the network, they have to use roll ups. Clearly that's not really acceptable. But on the plus side, the execution is safe because you don't execute anything. Synchronization. You don't have to synchronize anything. So that's nice, at least during the conversion. Yeah, and the risk really, not only is the Ux bad, but on top of that, it's more likely that a lot of people say, I'm not going to bother doing that conversion, I'm just going to wait for people to do it for me.
00:25:50.542 - 00:26:43.646, Speaker B: And when they do, I will download the image, which is exactly what the next idea is. And the idea is that some very powerful nodes are going to do that conversion and then share it with everybody else. So the idea is that, for example, here you have two nodes with their own view of the chain, and one of them that is very powerful at some height that has been predetermined and it needs to be the same. It's a consensus value. Everybody needs to agree on that. At some block height that is agreed by consensus. Some of those nodes stop following the chain and they just start converting like we did when we wanted to estimate how long it would take.
00:26:43.646 - 00:27:29.610, Speaker B: To do the conversion. And when they do, they produce a database that is the same view as the states here, but it's in the vertical world. And then they distribute this around to the network. And so everybody has to download this state and then replay the blocks as they have them in kind of revisionist history where they just pretend. Yeah, no, actually we were in vertical mode since that time. And we just replay. And hopefully they manage to catch up fast enough that they are on par with the chain before the fork.
00:27:29.610 - 00:28:01.480, Speaker B: Sorry. They are on par with the MPT chain before the fork height. And then at this point, the revisionist history becomes canonical. So there's a few problems with that. I mean, there's a disconnect. Clearly there's replaying past history in vertical mode. But because the gas schedule, the gas model changes, that means that you could potentially create an attack or nodes are going to pay.
00:28:01.480 - 00:28:48.418, Speaker B: Maybe something that used to be cheaper in MPT becomes very expensive in vertical tree. So you could create some attacks. And in general, I've tried this model, it's extremely brittle, so it's really not practical. There's also the question of the sync. How do you make sure that, what happens if you join late? What happens if you have a problem converting, et cetera, et cetera? It's not very nice. And on top of that, it uses twice the disk space because, yes, you need to have basically two clients running side by side. Okay, I need to go a bit faster on the alternative method.
00:28:48.418 - 00:29:16.666, Speaker B: Yeah, we have the bulk conversion, which is basically the same thing as what I just described. But everybody does their own conversion. So it's really the worst of both worlds. Even though you don't have to trust anybody with the conversion. But here comes the method that we want to really cover, which is the overlay method. And the idea is that you have two mpts. You have the old frozen mpt tree.
00:29:16.666 - 00:30:05.454, Speaker B: So at some fork height, you say, okay, I will not write into that mpt tree anymore. You start with a fresh overlay tree. And every write goes into that tree. And so what you have to do, and I will show it, but I have a little demo. But what happens is that when you read from the tree, when you read from the state first you read from the MPT, from the vertical tree, which is the overlay tree. And if you don't find the data in there, you have to go to the base tree to see if the data was present there. And then every n blocks, sorry, every block, you take n values from the mpts, actually.
00:30:05.454 - 00:30:46.060, Speaker B: And you move them into the verocal tree. And yes, it's a process that starts after the finalization of the four blocks that you can assume, for example, that the snapshot is there. You can assume that it will not reorg. So it simplifies things a bit. Okay, I'll get over that. I'll get to this topic in a minute. And what you can do is when you know that you will not need to reorg the MPT, at least through the MPT you can delete internal nodes so you can save some space.
00:30:46.060 - 00:31:31.158, Speaker B: Because it's in consensus. Everybody can find out if they have a bug, the chain will split so you can make an emergency release. So it's really nice for that, just to show how it plays out, you have the blockchain that is going to expand in that direction. And the blue squares represent the values that were written to the state before the fork. The purple ones represent the values written after the fork. So currently there isn't any. The red squiggles here represent the internal nodes of the merkle Patricia tree.
00:31:31.158 - 00:32:19.638, Speaker B: And the green arrow represents the head of the iterator. So I was saying, each block you have n values being moved, or at least copied from the MPT to the vertical tree. This is the head of the iterator that is going to sweep over the state, basically. So when the fork starts, at the first fork block, you start with an empty vertical root, which is represented by a green triangle. And then because you execute the block, you execute your transactions. Values get written to the tree. They are values that are written after the fork.
00:32:19.638 - 00:33:06.774, Speaker B: So it's purple. And then the iterator starts sweeping the state and moving n values. So in this little demo, n is two, right, and they are copied into the vertical tree. Now, what you have to understand something that is inaccurate on this little diagram is that the hashing function is actually different. So those values are actually not in order. But for simplicity, I keep the same order, even though it's not going to be the same order. And so imagine you want to access some data that is here, you don't know where it is first.
00:33:06.774 - 00:33:37.090, Speaker B: So you have to go for the tree, the vertical tree first. You don't find it there. So then you go to the MPT and you have it and you find it. If you don't find it, it's simply not present in the state. Next block, you come up with a new overlay tree. I mean, you keep increasing, the size of the overlay tree keeps increasing. And you move like the iterator moves by another two values, so they get copied.
00:33:37.090 - 00:34:33.362, Speaker B: But while you were executing the block, some values got overwritten, so they get clobbered, really more than overwritten. So next time if you want to read the value here, you will find it in the Veracle tree. You will never hit the merkle tree. And when the MPT nodes, sorry, when the conversion block, the transition block, the fork block is finalized, you know you will not reorg through the MPT, so you can delete the data and eventually, sorry. You can delete all the internal nodes, and eventually all the data is moved is copied from the MPT to the vertical tree. So you can delete what's left of the MPT. Right.
00:34:33.362 - 00:35:40.498, Speaker B: So I didn't go too much into details, but for example, because you rehash every key in the tree, you need to download the pre images because geth, in fact, I can't think of any except Aragon, I don't think anyone else stored the pre images. So yeah, you will need to download the pre images. And yes, we have several options, but it's probably just going to be dumped on the CDN or maybe some people will want to propagate them through a peer to peer network. But yeah, that seems overkill for this thing. One thing that this little diagram that was simplified did not go over is that actually we won't start the sweep before the vertical fork has finalized because, yeah, we don't want to reorg through the transition basically, or we don't want to reorg through the copy of the MPT. That's a simplification. And during the transition, another thing that is not set.
00:35:40.498 - 00:36:30.346, Speaker B: So we will make proofs available right off the bat right at the transition, but proofs and witness. But we will not prove what used to be in the MPT. We'll only prove what got written into the vertical tree. So that conversion. Yeah, it is heavy. The question is always how many nodes will be able to follow the network? How many nodes will we lose? So it depends how many leaves we translate in one go, but we seem to be okay with ten k. So if we look at the logs, we see that just inserting the values in the trees takes about half a second.
00:36:30.346 - 00:37:25.686, Speaker B: Preparing all the values also take maybe 300 milliseconds. So let's say it's one extra second of work that can actually be done in preparation. So the way the ethereum consensus algorithm works, you have more or less 4 seconds to produce a block, then you have 4 seconds to attest to it for the rest of the network to attest to it. And then you have those 4 seconds where the node is not really doing anything. I mean, of course it's doing something, but yeah, it's not as critical. And so in that four second time frame, you could expect nodes to prepare the conversion for the next block. Now clearly not every machine, it's still quite heavy for some machines.
00:37:25.686 - 00:38:15.818, Speaker B: Raspberry pis are not going to make it, but maybe their beefier friends like the rock. Five b can do it. We know it can do five k. We tested it, we wanted to try ten k, but it died and they're extremely hard to get. So we have more testing to do, but we haven't been able to do it yet. Yeah, so the idea, okay, maybe I'm going to skip over that because time is passing, but the idea is that if we follow ten k, if we move ten k per block, the conversion, and if everything goes well and none of the nodes, the validators drop off, we can achieve the conversion in like 15 days, which is acceptable. Right.
00:38:15.818 - 00:39:01.382, Speaker B: So I wanted to say something about the gas costs. The idea, I was saying we have the snapshot, which is a key value database, and we have the tree, which correspond. Normally the snapshot and the tree are in agreement, but what happens is that, okay, the question, from what I've said until now, you could get the impression that you potentially need to read two trees. Like every time you need to access the state, you will need to read two trees. And so the gas cost, the write and read gas cost should be double. But that's not the case. And the reason for that is because you have the snapshot.
00:39:01.382 - 00:39:51.306, Speaker B: And the snapshot is, like I said, a key value store, and on top of that, the MPT will be frozen. So you can actually delete the MPT and just keep the snapshot. And every time you read, in normal guest execution, you go straight to the snapshot. So what you could do is actually modify the structure of the snapshot so that it contains both the MpT values and the vertical tree values, so that you can read everything in one go. If you have a reorgan, the snapshot gets invalidated. Then you would still have to read the vertical tree, but that's more or less what you're expected to do. That's what the gas costs correspond to.
00:39:51.306 - 00:40:30.680, Speaker B: It's a tree read, not a snapshot read. So that's fine. Basically what I'm trying to say is if you read, actually it's just one database read most of the time. So you do not need to change the gas cost, really? And the write you need to do, but you only write to the veracle tree. So you're only writing to one tree. So actually the gas cost need not evolve, need not change just for the. I mean, the gas cost will change actually, when you switch to vertical trees, but you do not need to design a special gas cost for the conversion process.
00:40:30.680 - 00:41:24.230, Speaker B: Yeah. So just as a summary, the overlay method is a bit complex, but it's not as complex as, for example, the bulk conversion method. You still have to handle those two trees, especially in a very tricky case where, for example, I was talking about gas token or cryptokitties or all those very large contracts. You can't translate the whole contract in one go. So you have to move some leaves of the contract at a time. And you might find yourself accessing the contract that has some of its leaves in the Veraco tree and some of its leaves in the MPT. That's a bit difficult to handle.
00:41:24.230 - 00:41:52.830, Speaker B: A lot of bugs can come out of that, but so far we've been fixing them and it's been quite stable since we fixed a lot of bugs, so we should be fine. But, yeah, caveat. That's where a lot of bugs could come from. It's not so bad for the computation. The ram, the sync is pretty simple. I'm going to get into the sync if I still have some time. I don't.
00:41:52.830 - 00:42:25.594, Speaker B: Okay. So I'm going to skim over it. But yeah, the idea is that you can reuse a lot of the snapsync and because you've got all those values. Sorry, all those proofs, the healing phase of snapsync is quite nice. So, yeah, that's the overlay method in a nutshell. I'm going to talk. Can I still take five minutes? Because I realize it's almost 45.
00:42:25.632 - 00:42:27.180, Speaker A: Yeah, 100%. Go ahead.
00:42:29.310 - 00:43:20.290, Speaker B: So I'm almost done anyway. So, yeah, it's about the synchronization. So unfortunately we'll have to get two different algorithms, one for the conversion and one for after the conversion. But I would say it's not so bad if we don't have time to create one for the conversion because the conversion is going to be two weeks to one month. So if we have to full sync, it's going to be a bit of a pain, but it's not the end of the world. And so the idea is during the conversion, what we could do, we have to download the MPT, at least a snapshot. And we have also to be up to sync with the vertical tree.
00:43:20.290 - 00:44:05.974, Speaker B: But that's quite the fact. You have proofs and you have the witness as well. You can start reconstructing the states as you download blocks and as you download the witnesses that correspond to them. So you can start accumulating the overlay layer and then in the background all you have to do is download the snapshot. The snapshot doesn't get updated, so you don't really need healing. Now of course if an attacker gives you bad data, that's a problem. But very quickly I assume people will start putting the MPT database directly on some website or via torrent.
00:44:05.974 - 00:44:50.954, Speaker B: So there will be a lot of sources to get it from. So yeah, that's the first step. And then when you have downloaded the entire MPT snapshot, all you have to do is merge down and get back to the view you had before. So we have an idea on how to make it work during the conversion. Either you do this or you could also just basically implement snapsync for vertical. And in fact, what I'm saying here is theoretical. We have just designed the thing, but we haven't implemented it yet.
00:44:50.954 - 00:46:10.254, Speaker B: But there's an implementation by Tanish from nethermind of the vertical snapsync that we haven't tested on the testnet yet. But it should happen next week. So yeah, next week I'll be able to give more information about this. And so after the conversion, well, either we do what I just said, we use the snapsync that Nezromine has worked on, or you could do, and I apologize, I realize my picture is actually very hard to understand, but what you could do is just accumulate the data from the proof and then backfill the data you're missing, either from getting historical proofs or just doing something like Snapsync, like downloading the historical data in the snapsync way. But the advantage is that you do not, don't really need to heal because every time you get the proof, you get an update to the value of the upper nodes. So you don't have to go back and forth with the healing. You can find what is out of date and in fact you can even update the values from the proof.
00:46:10.254 - 00:46:40.818, Speaker B: So it really simplifies the healing phase. All right, just trying to wrap up. So there was another method, it's called state expiry. The idea is that we don't copy anything from the MPT to the vertical tree and then in the end people will come up with a state expiry scheme. But like I said, no one has worked on it for a while. So it's kind of kicking the can down the road. And in the meantime people have two databases on their disks.
00:46:40.818 - 00:47:14.546, Speaker B: So it's not really nice. And yeah, we don't get all the nice features of Veracle tree because we still have the MPT. So we don't get the ZK friendliness, we don't get the proofs unless we want to do MPT proofs, which are huge. So that's why this is like the backup solution. But we don't want to do that. And to finish. Yes, where are we at? So we have acceptable performance, at least executing blocks is on par with what we get for MPT on Mainnet.
00:47:14.546 - 00:47:49.598, Speaker B: So that's really nice. We have a working testnet that we haven't really discussed announced yet because we're still doing some tests, but it's been running for almost a week now. We have a shadow fork that we started working on, hopefully before the end of the year. And yeah, we have already two execution clients and two consensus clients supporting us. And those things we're going to work on in the future. We are deciding if we want to put the proofs in blocks. We want to check if it's going to work with Cancun.
00:47:49.598 - 00:48:17.820, Speaker B: We want to see if l two s are interested in adopting vertical trees. Probably not, but yeah, we need to have this conversation. And currently our proving speed, our verification speed is not practical. It's like one to 2 seconds. So we need to improve that. So that's going to be what we work on next. And with this, that's pretty much it.
00:48:17.820 - 00:48:35.534, Speaker B: I encourage you to look at vertical info and you can visit the testnet landing page where you can get all the information you want to join the testnet if you're interested. Super cool.
00:48:35.652 - 00:48:54.040, Speaker A: Yeah, super cool. Thank you so much. I know Steven already had a question like five minutes for actually before questions. Geome, is that right? Okay, cool. All right, Stephen, you're out.
00:48:55.850 - 00:49:32.718, Speaker C: So first question, I think I just want to talk a little bit about the pre image dissemination because the pre images are still being obviously updated all the way up to that fork point. And the translation of the vertical tree doesn't start until that fork point is finalized. Is there any concern that block production might stall because you don't actually have the pre images of those first things that you need to translate in those coming blocks? How is that kind of thought around and handled?
00:49:32.814 - 00:50:05.194, Speaker B: Yeah, that is a concern indeed. So something, yeah, I kind of skimmed over that because I could not really, I mean, there was a lot to cram in those 45 minutes. But what we do is we do more than wait for the MPT to be finalized. Sorry, for the fork block to be finalized before we start the conversion. We're going to give it maybe a week. So it will be a week with two trees and no copy happening. And the reason for that is because we want those nodes.
00:50:05.194 - 00:50:50.918, Speaker B: So what I was describing about the offline conversion method, we don't want to do that for the state, but we can do that for the pre images because you can check that. Well first, there are two things that are really interested. First, the current pre image size database is like 78gb. And by the time we get there it's probably going to be 100gb. But out of those 78gb there's maybe 4gb that are really useful. So what we would do is there will be some processes that will stop at some nodes that will stop at the finalization block. And they will actually at the finalization block of the fork block.
00:50:50.918 - 00:51:29.010, Speaker B: And they will go over the snapshot and resolve every pre images in their database and produce a file that is in the exact order. And the reason why the order matters is because currently the pre images they're stored in, at least in guess they're stored in hash order. Right. So that means you're basically jumping all over the database looking for your pre images. So you're thrashing. So the goal is to get a very powerful node that does this thrashing for everyone else and then distributes the file. It's a six gigabyte file.
00:51:29.010 - 00:51:50.410, Speaker B: Okay. Make it ten. Right. People can download this and then iterating the snapshot takes about 2 hours. So very quickly you can find if the file you've been given is correct. And the reason why we have this one week is in case we find out that a lot of nodes are reporting. I don't have the right pre images, I don't know where to download it, blah blah blah.
00:51:50.410 - 00:52:09.910, Speaker B: We make an emergency release to delay it by a week. We try to figure it out. So it's a bit risky. But I would say compared to doing the conversion of the state like distributed 150gb of data, asking everyone to replay blocks on top of that, it's much safer.
00:52:11.770 - 00:53:02.982, Speaker C: Yeah, that makes a lot of sense. Thanks. And then the second question I had and then I'll let other people talk, is actually not even necessarily on vertical per se, but I know early on path based storage scheme was talked about a little bit along with snapshots. And I'm very interested on if the path based storage scheme which I think probably still somehow incorporates in with vertical, but is actually going to deprecate these snapshots because you can do that one lookup directly into the MPT with path based. And so when path based kind of first dropped, I was really looking into how that was set up, and it didn't seem like it was directly deprecating snapshots. But yeah, I don't know if you have any insight into that. I know it's a little deviation from this.
00:53:03.116 - 00:53:42.020, Speaker B: No, it's actually a very good question. I'm transitioning myself as guest is transitioning from the hash based to pass based. I'm also transitioning the whole vertical design from hash based to pass based. And indeed, a lot of my thinking has been made on the snapshot, but indeed the goal. So it's not going to happen immediately. But the goal is to replace, according to Gary, at least, the goal is to replace the snapshot with the pass based approach, because it's the same thing. I mean, honestly, a lot of the code is similar.
00:53:42.020 - 00:54:17.028, Speaker B: Okay. A lot of the storage code, the layer, the layering, there's a lot of similarities. Right. So it's true. I've been mentioning the snapshot a lot, but typically at the time the fork happens in a year or so, we will probably be looking at a past based version of what I just presented. Sorry. To answer your question more directly, there was also a direct question.
00:54:17.028 - 00:54:22.340, Speaker B: Are we planning to get rid of the snapshot? Yes, but it's not going to happen immediately.
00:54:28.840 - 00:54:59.024, Speaker A: I was going to ask you mentioned at the beginning that you're making this change with offsets, where you have one byte that stays the same and is not part of the hash. So that you'll actually be doing having a bit more locality. When you're looking up locks or keys that are in a similar sort of storage area, is that going to coincide with a change in gas accounting as well, to charge less for accessing, let's say, all 256 slots if you're doing that. And if so, I assume that'd probably be after completing a conversion, no?
00:54:59.062 - 00:55:36.060, Speaker B: Yeah. Okay, so indeed, that's also a good question. I didn't cover that in the slide. There's indeed a gas model. And what happens is, because you have this group, you pay. Opening the group implies paying some gas, and then accessing an individual leaf implies paying more gas, but it's cheaper to then once the group has been opened, each individual leaf, accessing them is much cheaper than opening a new group. So, yeah, there's a gas model change that corresponds to this new reality.
00:55:36.060 - 00:56:14.624, Speaker B: It's going to implement it right off the bat at the conversion because we will have a vertical tree. And yes, the MPT is accessing. The MPT means accessing. Well, I was going to say the snapshot again, but let's say this flat database. So the discrepancy between the two gas model is not going to be too much of a problem. But yeah, I was thinking of something else. Okay, sorry, I forgot.
00:56:14.624 - 00:56:18.710, Speaker B: But I think. Yeah, if you have more questions about it, let me.
00:56:28.800 - 00:57:19.116, Speaker D: Patrick here. My questions related to fee pricing, related to the type of modification to the tree. With these tree structures, you can modify arbitrary values into smart contracts and whatever. As the tree gets bigger, certain modifications can be more expensive than other modifications based on the density process. I'm wondering if you guys, through any of this entire revisioning of how you're going to manage the trees, have discussed it all, trying to couple gas costs with the actual time that that modification is made. Especially if you have these smaller smart contract trees like me making a change to one part of it may be very different.
00:57:19.298 - 00:57:21.630, Speaker A: Yeah, dude, I've never heard anything like that.
00:57:25.620 - 00:57:34.732, Speaker B: Yeah, sorry, I got a bit confused because indeed there was a slowdown. I'm happy to answer it, but. Yeah. Could someone repeat.
00:57:34.796 - 00:57:38.930, Speaker D: Yeah, I'll try again. Someone else go.
00:57:44.570 - 00:58:29.654, Speaker B: I have a quick question. Part of the benefit of the vertical version is that you can increase the branch factor without increasing the proof size. Why can't you increase it to be completely flat? What ends up being your bottleneck? That's a good question. Well, I mean, if it's completely flat, then you find yourself having like you need to update it's polynomials, right? Yeah. Wait, let me think. I thought about it some time ago, I just don't remember what conclusion I came to, but basically. Okay, there are two things for sure.
00:58:29.654 - 00:59:06.514, Speaker B: The problem is that if your polynomials get very. If your polynomial gets very large, that means that you need. Okay, sorry, let me take over. Restart. When you evaluate your polynomial, the size of your database, the size of your branch is also the degree of your polynomial, right? So that means you need to have a lot of. This polynomial is encoded on an elliptic curve. So you have a lot of elliptic curve points.
00:59:06.514 - 00:59:52.880, Speaker B: Right now you have 256 that you encode your polynomial against. If you start having 1 billion degrees, like if the degree of your polynomial is 1 billion, that means you need to have 1 billion elliptic curve points either in ram or in your database. So that's already a lot for something that you could optimize by having a tree, and otherwise, there was something else. Okay, if I don't remember before the end of the presentation, I will write to Aaron about it. But there was another reason. I just can't remember it right now.
00:59:57.840 - 01:00:36.040, Speaker D: I can try again here. Is this any better? Am I still much better? Cool. Thanks again for stopping by to present. I'm kind of excited to watch the recording and see what I sound like. But my question is related to whether or not any thought as you've been working on new structures or tree structures, has gone into thinking about pricing lookups or modifications of state based on the complexity of the tree. So based on where you're inserting how many intermediate nodes, or what the depth of it is in the tree, or if that's just not really a concern anymore with some of the stuff you're doing with vertical.
01:00:36.400 - 01:01:12.040, Speaker B: So because indeed the branching factor is much larger, the tree is deeper. So I'm not the one who worked on the gas model. That would be Vitalik and Dankrad, I assume. But my understanding is, yeah, they were considering an average depth of four, maybe six, because it would even out. We still have the hashing, we still have everything. So it would even out. So it makes little sense to design a very specific guest model.
01:01:12.040 - 01:01:28.190, Speaker B: And also from a user perspective, it's quite difficult to. You can't expect the user to know beforehand what the tree is going to look like. So I would say no. We haven't really given too much thought about that.
01:01:30.240 - 01:01:30.940, Speaker D: Makes sense.
01:01:31.010 - 01:01:31.630, Speaker B: Thanks.
01:01:33.680 - 01:01:48.160, Speaker A: I was going to ask one more, which I assume the answer is probably fairly simple, but for the overlay strategy, when you're having data that's within the Merkel tree, and then also potentially falling back to the Merkel tree, how do you handle deletes in that case? Do you have some sort of tombstone or use the SAP?
01:01:50.280 - 01:01:52.020, Speaker B: So we don't delete anymore.
01:01:54.440 - 01:01:56.228, Speaker A: Even for storage keys as well.
01:01:56.314 - 01:01:58.980, Speaker B: So storage keys, we overwrite with zeros.
01:01:59.560 - 01:02:00.310, Speaker A: Okay.
01:02:01.560 - 01:02:33.564, Speaker B: And we no longer delete. I mean, that's not even a vertical tree thing. It's going to happen next fork, Denkun. We're going to simply refuse to. I mean, self destruct is removed mostly because it hasn't served its purpose either. No one is really self destructing contracts. They are more like doing a lot of weird shenanigans where they create delete, create delete million times, but they don't really delete permanently.
01:02:33.564 - 01:03:00.276, Speaker B: And we also have a lot of, maybe not a lot, but fairly painful history with self destruct. That's the Dow hack. That's a lot of things. So we'd rather get rid of it. It hasn't delivered on its promise, basically. Cool. Yeah.
01:03:00.276 - 01:03:12.436, Speaker B: I'm still bothered because I know the answer to the question by who asked it. Daniel. Sorry. I'm trying to. David. Sorry. But yeah, so I insert.
01:03:12.436 - 01:03:38.492, Speaker B: There's the size of the evaluation thing. There's also, well, computing a polynomial, because every time you insert, like, at least the first time, you need to compute the entire polynomial. So if it's a million leaves, that's a lot. But now we insert differentially, so it's less of a problem. You don't have to recompute the entire polynomial every time. But, yeah, it's basically the same thing as Merkel. Oh, yeah.
01:03:38.492 - 01:03:50.688, Speaker B: No, wait. That doesn't work. But it's basically the same thing as Merkel tree. So it's separating. It's kind of a divide and conquer. But there was another very good reason. I'm sorry, I'm really annoyed.
01:03:50.688 - 01:03:56.230, Speaker B: I can't remember it right now, but it's going to bother me all night. I will get there.
01:03:57.900 - 01:04:05.704, Speaker C: I thought that generating the commitment of the node was o of the branching factor. Is that not the case?
01:04:05.902 - 01:04:33.744, Speaker B: No, because if it's a new node and you're filling everything in one go. Yes, but because it's polynomials, you can just do a diff. Like, it's basically just adding two numbers. So you can just add, like, the polynomial is just adding a number compared to. At the right degree. I mean, actually, it's not the way we do it. We don't do coefficient form, we do evaluation form.
01:04:33.744 - 01:05:03.420, Speaker B: But the idea is the same. It's just about adding the difference. So you can just update the polynomial without update the commitment, without opening every single branch, every single item in the polynomial. So we call that diff update. You can update it basically in one time. That's the other advantage of polynomial commitments over hashes.
01:05:06.910 - 01:05:10.090, Speaker A: What is used as the source of randomness for the Peterson commitments?
01:05:12.350 - 01:05:53.980, Speaker B: The randomness. Well, actually, do we use. Oh, yeah, of course. No, we basically have a string, and we hash it and we use it. I'm trying to remember the exact details in the code, but the idea is that you have the string, you hash it, and then you take that number, and you say plus one, and you apply that to the. What do you apply it? Because if you do that, then you would be able to. Well, okay, sorry.
01:05:55.070 - 01:05:55.914, Speaker A: Never mind.
01:05:56.032 - 01:06:19.940, Speaker B: I don't remember the exact details, but the idea is that it's a hash, basically. So it's a string. It's a hash. And if you can find. The idea is that because you still have the. What is that called? Discrete log problem. Because you can't solve that.
01:06:19.940 - 01:06:35.560, Speaker B: You can't calculate the difference. Sorry. The common factor between the two. Yeah, I'm talking. Sorry. I haven't touched that in a while. So I need to remember how that was done.
01:06:35.560 - 01:06:45.980, Speaker B: But, yeah, it's basically a hash. Like, we have a tree that says vertical tree October 21 or something that we hash, and this hash is our source of randomness. Cool.
01:06:47.150 - 01:06:58.218, Speaker A: Does anybody else have any questions? All right, I'll take that as a note. I'm going to try one more time. Guillaume.
01:06:58.314 - 01:06:59.230, Speaker B: Oh, yeah. Very good.
01:06:59.300 - 01:07:00.320, Speaker D: Okay, great.
01:07:01.330 - 01:07:10.340, Speaker A: Thank you again so much for coming. It was really fantastic. We really appreciate. Thank you again so much. And feel free to use the Uber eats code too, if you haven't already.
01:07:10.870 - 01:07:21.570, Speaker B: Cool. Thank you. Those two questions, it's going to bother me. I'll get back to you on Twitter.
01:07:21.730 - 01:07:24.694, Speaker A: Awesome. Thank you guys so much. We'll see you guys.
01:07:24.892 - 01:07:26.130, Speaker B: Take care. Bye.
