00:00:00.810 - 00:01:03.294, Speaker A: Have to hold on to things. Okay, I'm Edge guinea, Dex guru, CTO and founder. And that's going to be like a technical architecture kind of talk about how we deliver in web two user experience for our web3 application. So index guru, we index in 70 dexes across multiple chains and like enormous amount of trading pairs, tokens, whatever we have. And obviously it's kind of impossible to do it solely on web3 because as everyone know, the node itself under the hood, it's a key value storage, and you're very limited to what you can do with it, how you can access the data, how you can select and search the data. Of course it's decentralized, but it's slow to write and kind of slow to read. It's very costly to do boss things.
00:01:03.294 - 00:01:47.562, Speaker A: So to power up something like that, you need some caching layer in between web two and web3. And that's the only way everyone proceeding right now. So solutions wise, we need indexing, obviously. So to get web3 into web two, then to get our web two, web3 application work on top of it, let's say Azer scan, right? I just stole that picture from some explainer from YouTube. But basically what they do, they index in blockchain into centralized DB, and then all of us can explore all those things in Azer scan. That's how it works. Or the graph.
00:01:47.562 - 00:02:34.800, Speaker A: The graph is cool. It's kind of the same approach like indexing stuff, but then the data layer itself is decentralized and you have pretty complicated way of contributing to it as well as it's paid to consumers and curators. So the graph is really cool. But for applications like ours, it's not zay yet. In case of like DB tooling, analytics toolings around it. So traditional DB is offering much more tooling to analyze the data so you can execute actual analytics jobs on your notes and so on and so forth. It doesn't exist in the graph yet.
00:02:34.800 - 00:03:35.854, Speaker A: And here is how we're doing it. And that's pretty much what Dexguru uses to get all that information. So we're actually currently using three different dbs and bunch of indexation workers, whoever network we support, as well as we parse in hell of a ton of different events from different dexes and put them in all of those dbs simultaneously. So we have radius, which is like a cache for our real time data. So users, whenever they hit on our website, can see those real time things, really real time. Then we have elasticsearch, where we have like search, select and aggregation possibilities, as well as we're executing some data analytics scripting on top of it. And that code is actually executed on elasticsearch node.
00:03:35.854 - 00:04:27.810, Speaker A: So you kind of utilizing sharded computer power there. When they're indexing millions of transactions and trying to calculate prices of them, like trending, categorize traders and figure out if whales are selling or buying. Right now, that's a crucial thing to do. Also we use on Clickhouse and click house, it's mostly for the thing called materialized views. And Clickhouse, that's like a precalculated view of your data based on your ingestions. So Clickhouse is used on our website to calculate balances of every trader. So you can see when he was selling or buying and what's the top token holders and so on and so forth.
00:04:27.810 - 00:05:10.734, Speaker A: So we have like a zoo of different databases in here, and we indexing in all of them right now because all of them providing different tooling. And whenever we are indexing different chains, we are actually hitting different performance issues. For example, index in Ethereum is not the same as index in BSc. Much more transactions blocks coming much faster. So you're figuring out next approach to it, maybe you're switching DB and so on and so forth. So that's where you are at. So what's actual pros and cons? Of course, I already talked about it.
00:05:10.734 - 00:06:08.660, Speaker A: It's like traditional DBS been there for many years, and they really good at things they already good at. You can get as much speed as you want. You have in memory DBS, you have sharded DBS, you can actually parallelize your queries across multiple nodes if you have replicas. So like squeezing up the speed is actually what makes traditional DB solutions much faster than basically grabbing it against web3. Even if we could request the data from web3. So there is all kinds of database as a service solutions from clouds. So you can actually, instead of managing it yourself, you're basically outsourcing it to AWS or something and then basically just getting everything out of it.
00:06:08.660 - 00:07:12.680, Speaker A: The next thing is about business scalability, actually. If you grow in your product, you need to scale up your team really fast. And whenever you're trying to find web3 developers or anything related to smart contract or anything like it, it's really hard to find people on that market as well as prices are really high. But when you're trying to solve this problem, you figure out, okay, I have indexation, I have whole web two tooling, so I can actually hire web two developers for everything. What's not related to web3. So you can scale up your teams which solely focus on organizing back end, organizing data analytics and maybe doing some new APIs and so on and so forth with web two developers. Yeah, and all those toolings which is available like executing actual queries and aggregations on database nodes themselves.
00:07:12.680 - 00:08:28.974, Speaker A: Really complicated stuff sometimes. Like you have scripts running on the nodes, calculating some averages, means of figuring out if data is biased and so on and so forth. Of course, conservators, you centralize on everything, right? And what we're actually going to do is that because we all know that we want to go in direction of decentralization instead of centralization. So how are we going to decentralize solutions which are needed for analytics? And in blockchain, everyone need to understand that we have tons of data, like thousands of transactions happening every block and every minute. And even for traditional kind of indexation and database problematics, it's a huge piles of data. But if we're trying to do decentralization at the same time, it's even more challenging and so on and so forth. So what we do to decentralize things and how we are planning to approach it.
00:08:28.974 - 00:09:20.030, Speaker A: So currently we are like a year in development, and during that year we try different approaches and we see how to build some of the analytics, let's say on radius. Radius is key value storage as well. And whenever we try things on radius, obviously if those things work on radius, we can migrate them to something like layer two plugin for the nodes, which would work with level DB, same way it's working with redis. And then we have something like plugin for some particular feature, some particular analytic feature, or maybe set of them. And then we can actually decentralize that plugin which we're hooking up to every node. That's pretty much how the graph approaches it. But maybe we would need some more customization that the graph provides.
00:09:20.030 - 00:10:16.270, Speaker A: And that actually comes to a search thing. We have in mind that we need something like l two, but for data layer. So nodes would be working themselves as are right now, but we would have like plugins which are working directly with nodesdb without any web3 API in the middle and organizing themselves into layer two. But for the data with additional features on how you can. So let's say it would be layer two for defy data or something like that. So you would have actual swaps, burns means from every dex possible already there, ready for querying in different kind of ways and so on and so forth. So actually my talk was labeled as workshop.
00:10:16.270 - 00:11:06.078, Speaker A: I'm not sure how many developers I have in here. Any Python developers really. Okay, so to get to the point to actual developers problem here. So this is like the cycle we do for every block. So to actually get all the transactions from all the transfers as you see them in other scan, let's say for some transaction from the block, those are all operations you need to do against like EVM node. So you need to get last block numbers and get all transaction indexes from the blocks and request all transaction transactions by their indexes. So it's like a loop there.
00:11:06.078 - 00:12:02.814, Speaker A: Then request recepts, because you parse in recepts to get transfers. And then basically in recepts you're looking for transfers event and that's how you parse in the same. So if you're looking on something like this is a transaction Azer scan, and you see here, Azer scan already did a nice job. And actually parsed transfers in here and you see tokens and everything. Okay, it's too small, sorry. How the hell you would get it parsed out of your pure Webster node? So from Webster nodes you get in something like this and it's like unparsed recept logs. And because we have so a little amount of developers here, we can go through the code actually, but probably not useful.
00:12:02.814 - 00:12:55.940, Speaker A: But you guys can go and check actual GitHub in here and just cloning it. You basically have that portion sorted out. So you can actually start indexing transfers into DB of your choice and start analyzing them. And those are years to 20 transfers. And by swapping APIs there, you can actually start indexing NFT transfers if you want. So that's the idea, and that's why we need to have indexation, because this is a way to get transactions for one block. And we're converting it in DB, we're converting it to the view where it's easily searchable or requestable by any key we want to request it by.
00:12:55.940 - 00:13:58.130, Speaker A: So that's the complexity and index guru. We have sets of those workers which are actually working with dozens of nodes on different chains. And that's how we're getting like real time data. And in summary, it's like 1500 docker containers right now, which are running that kind of code all the time. So yeah, that's a question section. Any questions on. So this work was something like with what Celeste is doing, kind of trying to do so like a data lake.
00:13:58.130 - 00:15:12.704, Speaker A: I hadn't checked them, but what we do. So all data we index in, we actually have as an API available, like as a public API. So after we already index that normalized it, you can use our public API to go through transactions which are happening on all dexes across different chains and see like a pricing feed which we calculated for you already. So we're providing some access to our data already. It's not like to all the data, and we're planning on extending our API so we will have more access to raw data and maybe just kind of giving it to the world as a database interface makes sense in some way. Yeah, that's definitely what we may say. Any other questions? Cool.
00:15:12.704 - 00:15:32.950, Speaker A: So here is my telegram, Twitter Dex guru. We are hiring as everyone here, if anyone interested. We're sitting on pile of data and looking for someone to start analyzing it. We're doing some categorization right now, but looking for more things. Okay, that's it then. Thank you.
