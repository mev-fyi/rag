00:00:00.250 - 00:00:18.000, Speaker A: Notes East Denver 2024 is your hashtag. We have lots of people watching live on twitch at the moment. Hey twitch friends. YouTube videos are being posted almost immediately after these sessions wrap. So plenty of ways to engage with this content after it takes place right here, right now on the stage. Take away Margo. Thanks so much.
00:00:20.050 - 00:00:36.566, Speaker B: Can you hear me? Can you hear me? Is this on? Oh, there we go. Okay. Hi everyone. Thanks for coming here today. Very briefly, we're going to talk about DA, and we have some of the biggest names in the Da space right now. But why don't you guys introduce yourself and your project? Start with you.
00:00:36.668 - 00:00:51.550, Speaker C: Sure. So, hi everyone, my name is Dan. I'm a head of product at Avail, which is an infrastructure layer designed to help modular execution layers scale and interoperate. And part of that is at the a layer.
00:00:52.690 - 00:01:14.610, Speaker D: Hi everybody, I'm Sriram. We are building a project called Eigen Layer. Eigen layer is a mechanism to allow stakers in Ethereum to support arbitrary new decentralized protocols. One of the first protocols we're building on top of it is Eigenda, which is an Ethereum centric data availability system. We want to scale Ethereum or scale roll ups that are building on Ethereum.
00:01:15.750 - 00:01:25.830, Speaker E: Hey, everyone, I'm Brendan. I work on ZK tech and R D at Polygon. Not DA directly, but as Dan said earlier, we're kind of the customer for DA.
00:01:26.970 - 00:01:40.234, Speaker F: Hey, everybody. Hey, everyone. I'm Nick White. I'm the COO at Celestia Labs. And we have built Celestia, which is the first modular blockchain network. So it's an l one that specifically focuses on DA and execution. Sorry.
00:01:40.234 - 00:02:04.050, Speaker F: And consensus, and does not do any execution. And so it's built specifically for a world where we want to have lots and lots of different roll ups and execution layers built on top of one sort of unified, scalable security layer. And we implemented a new technology called data availability sampling, which is like a step function change in the verifiability and scalability of DA systems.
00:02:04.470 - 00:02:17.000, Speaker B: Great. Well, we've been using a lot of jargon, and I just want to make sure that everyone is on the same page. Let's start with what data availability is. What is it that these layers are trying to solve? Any one of you take it away.
00:02:17.310 - 00:03:10.650, Speaker F: I'll take that. Data availability is probably a confusing term because it sounds similar to data storage, but in fact, data availability is not data storage. It's about the publication of data. It's about publishing data publicly and making it verifiable for anyone to check that that data was published. And this is important because if you want to verify some kind of blockchain system, any kind of decentralized application, you need to know what the inputs to that system were. If you don't know what the inputs are, or you don't know what the state of the chain is, you're not able to recreate the state, then there's no way to either verify that that state is valid or progress the state if the sort of block producer goes offline. So data availability is really important for the safety and liveness of blockchain systems.
00:03:11.070 - 00:04:20.302, Speaker D: I'll just add a little bit to that really clear explanation. Why do we need data availability as a separate system as opposed to actually having a common blockchain? If you look at the scaling roadmap of Ethereum, one of the really cool things is you can off outsource computation. So you have a node, a single node, even, that actually does the computation and submits approved that the computation was done correctly. So this is a mechanism to scale computation, because now parallel thousands or hundreds of thousands of systems can be making computation claims to Ethereum or any other chain or users who can then verify it. But the problem is, when computation is being outsourced, what about the data relative to that computation? What if the data inputs like Nick just pointed out, or the data outputs were not made available? And so by building a separate mechanism to scale the production and verification of data, publication of data, as well as the verification, you can actually have blockchain systems that scale massively. So that's the reason why data availability kind of abstracted from the rest of the blockchain makes sense.
00:04:20.436 - 00:05:27.314, Speaker C: Yeah, totally agree with everything here. But one thing that I will add is that if you think about blockchains in general, like l ones, they inherently do data availability by having every participant in the system download entire blocks. And so one thing that makes data availability focused solutions really interesting is that they allow the ability to be sure that data was published correctly, but without actually downloading the data in question. And so, for example, avail does that by doing data availability sampling as well, with validity proofs, which is how an avail like client, that is a client that doesn't actually download the body of a block, only the headers, is still able to be certain that the data was published because you don't necessarily care about what the data is, you just care that the chain is proceeding correctly and that all of the data has been available for the chain to verify.
00:05:27.362 - 00:06:13.166, Speaker E: The next block, and maybe to just add one thing. Not to get too far in the weeds, but data availability has like a subtle but very important difference in how it functions in optimistic and Zk systems. So in an optimistic roll up, safety is provided by the fact that anyone can generate a fraud proof. So if the operator of a chain decides to steal everyone's funds, there's some challenge, period. And anyone can provide a proof saying, hey, the transaction where you minted a million ETH was invalid. And so we're going to change the state of the chain. And so data availability for optimistic systems is really, really important, because if you don't have the input data, then you can't generate a fraud proof.
00:06:13.166 - 00:07:23.814, Speaker E: And so data withholding is equivalent to a safety failure, because if data is withheld, then the operators of an optimistic roll up can steal everyone's funds. And so in the ZK case, it's slightly different. It's still really important to have data. But in a ZK roll up, the operator of a chain is providing a proof, and the proof guarantees that the operator of the chain can't mint a million eth out of thin air. The risk is one of liveness, where if the operator of the chain withholds data, then they can make it impossible for a user to withdraw their funds from the chain, and they can halt the progression of the chain. And so this can lead to ransom attacks and is an attack vector for zoologic proofs for ZK rollups. But it's like a really important distinction to make that there's a very substantive difference between being able to steal everyone's funds in the event of a data withholding attack and having to navigate sort of like the expected value of a ransom attack and having to process forced transactions.
00:07:23.814 - 00:07:30.982, Speaker E: And so I do think it's important to note that this guarantee provides different properties in different systems.
00:07:31.046 - 00:08:01.078, Speaker B: I want to throw it back to you, Brendan, because you mentioned in your introduction that you're sort of representing the customer. Right. And Ethereum has sort of pivoted its roadmap to this roll up centric age. And I wanted to have your take on what data availability solutions do for an l two. What problems it is they're specifically trying to solve. And also, what are these trade offs of having an external DA solution versus data from an l one?
00:08:01.244 - 00:09:04.870, Speaker E: Yeah. So I think the core thing to note is data availability is actually very expensive on Ethereum. And so if you look at what drives transaction fees on l two s, it's overwhelmingly data availability, the cost of posting data to Ethereum, and so data availability layers or systems provide a really compelling alternative to posting data directly to Ethereum for applications that don't require complete Ethereum security. I do think it's really important to note that if you use a different settlement layer from the layer that's providing data availability, you're getting a slightly different guarantee because you can't verify that data was made available on Ethereum. There's no way to prove like, okay, actually, data in fact was made available on this other system. And so you rely on the consensus of the data availability layer. So you verify that validators on celestia in fact attested that data was made available on Celestia or avail or eigendi.
00:09:04.870 - 00:09:36.350, Speaker E: And so I think that this can provide a very sufficient level of security for a ton of use cases. But it's important to distinguish the guarantee that's being provided by Ethereum data availability from the guarantee that's being provided by external data availability systems. I think it's fair to say that we need as much data availability as possible even after dank sharding. Our total capacity for data is, I think, fairly low, but it's important to distinguish the security guarantees.
00:09:36.430 - 00:10:20.770, Speaker B: Well, you bring up protodank sharding, which is great, because I wanted to get into this Duncan upgrade that's happening next week, in ten days. I can't do the math right now, but I guess what everyone is most excited about is these lesser fees for l two s. And I wanted to hear from you guys what this specifically does for DA solutions. And then on top of it, there's been an argument that the DA solutions are needed because of this data problem. But on the other hand, it's supposed to sort of solve sharding for now, since sharding is so far away, what happens when full sharding will eventually be implemented? What does that mean for a DA solution?
00:10:21.590 - 00:11:01.614, Speaker C: Yeah, happy to lead here. So I think that protodank sharding is a great step. It's the next step in the Ethereum roadmap towards increasing its block size. But my sense is that there's a lot of latent demand out there. Essentially, since the pivot to a roll up centric roadmap back in 2020, there's been a ton of work in creating new roll ups and making easier to develop new roll ups. And we're now at the point where we have dozens, maybe hundreds of roll ups just already in existence. And it's super easy to, it's getting very easy to spin up new ones.
00:11:01.614 - 00:11:51.774, Speaker C: There are roll up as a service providers, for example, that make it extremely easy to develop new ones. There's a lot of interest from developers to encapsulate their app in a roll up. And so we think that what we have now is just the tip of the iceberg and there's a ton more coming. And so EIP 48 44 is a great step, but I think it will quickly be filled up. And the larger Ethereum roadmap is great. The avail technology stack has a bunch of the same elements that the Ethereum roadmap implements. Like we use KCG commitments with data availability sampling, for example.
00:11:51.774 - 00:12:42.400, Speaker C: And so we believe in the technology that is being proposed for Ethereum. We already have it implemented for chains that are built on avail and for chains that want to settle on Ethereum, like a validium, for example, or optimium. We have a ZK bridge that allows verification on the Ethereum side that the avail chain has come to consensus, that the validator side of avail has come to consensus. So we think that that's a great way to expand, like drastically expand the throughput of the system. And of course, every solution has to be evaluated. Like as Brendan was saying, you have to look carefully at what the specific trade offs that you're making when you deploy, when you're in one construction versus another. But yeah, that's my take.
00:12:43.350 - 00:13:16.326, Speaker D: Yeah, I'll just add something to this. So there are two ways to look at a roll up. We are offloading data from Ethereum to a roll up, or we are onboarding cloud to crypto. We are in the second camp. We are onboarding cloud to crypto. What do I mean by that? You look at the cloud, you have services that are built on top of the cloud. These services do not give rigid guarantees to their users, whether it's verifiability, whether it's immutable APIs, whether it's antirent seeking, whether it's user governance.
00:13:16.326 - 00:14:07.050, Speaker D: We need all these properties for end users, and what is the best way for us to provide that? And I think crypto and blockchain is the right answer for that. And so if we want to bring all these applications into crypto, one of the most important things we need is a scalable substrate. And Ethereum already has a scalable substrate in terms of liquidity, social consensus that lots of people are interested in watching, and developer involvement and so on. So can we actually take this system and add like a scaling layer to it just for data availability, for example? Because that is the key bottleneck in the roll up centric roadmap. So Ethereum's like four, eight, four. Data bandwidth is tens of kilobytes per second. Eigenda's launch throughput is ten megabytes per second.
00:14:07.050 - 00:14:47.170, Speaker D: So you're seeing several orders of magnitude there, but we will not stop there. We want to keep scaling this till we actually hit cloud scale. So when we're thinking about cloud, one of the other things we need to think about is economics. Today, most of the roll up or like blockchain economics are all spot prices. Like I have to go and bid for block space on a spot basis. And if you're sharing a common block space or data availability or any other system across multiple different applications, one application can crowd out the entire block space. Suddenly this leads to bursts in prices and inability to access block space for other applications.
00:14:47.170 - 00:15:31.670, Speaker D: One of the things we are trying to do is to resolve this problem by actually having similar economics as the cloud. For example, reserved bandwidth. Most of the Amazon web services is reserved instances. You go and have this instance just for yourself. Can we have data bandwidth reserved for your own roll up or your application so that you're not contending with anybody else, you're not having variable prices, you have a fixed price for this entire year of horizon. So this is some of the elements from the cloud, some of the elements from crypto is can you pay in your own token for your rollup's data availability service? Can you use your own native token of your rollup to actually also additionally secure the data availability layer? These are the features that I NDA brings that are unique and differentiated from Ethereum.
00:15:32.410 - 00:16:49.854, Speaker F: I want to touch on the original question about Ethereum's DA roadmap. And one of the core things that we realized when we were building Celestia was that there was a massive need, or like latent demand, essentially for data availability, that was not being sort of supplied by the market because the Ethereum block space was just too constrained. And since we launched at the end of last year, around ten different roll ups have either migrated or deployed on Celestia that were formerly, some of them were formerly ethereum L two s, and they've seen a hundred x reduction in their costs. And so teams like Manta, for example, have already saved around $2 million in fees, and they've passed those savings on to users, and they've also been able to capture some of that savings for their sequencer, and they're using those funds to fund more development. And I think this is like a really powerful catalyst for the acceleration of more rollouts be launched. And we're actually seeing now a new wave of demand. So there's this concept in economics, which is called induced demand, which is when the price of some kind of service or good is high, the demand for it is actually lower.
00:16:49.854 - 00:17:33.638, Speaker F: And when you decrease the price, there's actually more people come out of the woodwork basically saying, I want more of that thing. Now that it's cheaper, it's more useful to me. Or like, it makes sense now for me to build a new roll up, for example. And so we're seeing, I think in the next few months, we're going to see an acceleration of more roll ups. And specifically, there's a lot of people interested in deploying l three s or basically taking an application that exists on ethereum or some other monolithic l one or some l two, and deploying their own EVM or app specific chain. I think we're really just at the tip of the iceberg, kind of like what my colleague was saying over there. And I think it's an exciting time.
00:17:33.638 - 00:17:40.670, Speaker F: I think Eip four four four is going to be good because it'll reduce the costs further, but it still won't be enough, at least in the near term.
00:17:42.050 - 00:18:05.986, Speaker B: There's been a lot of hype around da solutions, right. Avala has had a bunch of announcements. Celestia had this main net event that happened in October, November, October. Airdrop was like sort of heard around the world. And Eigen layer, there's been a lot going on restaking. There's $7 billion locked into TBL. And I think a lot of folks also have a hype around this potential airdrop that might happen for Eigen layer.
00:18:05.986 - 00:18:15.770, Speaker B: So are you worried that once this airdrop does happen, that activity around Eigen layer might dissipate, users might go to other protocols?
00:18:16.910 - 00:19:03.686, Speaker D: We are not focused on stake and TVL. Our main focus is on open innovation. Open innovation is what are we unleashing for developers to build on top of us? That is the primary focus. TVL is a short term metric where users who think that there is enough rewards come to the system. But the real long term value of the system comes from the ability of open innovation. The ability that now you can have hundreds or hundreds of thousands of roll ups actually building on eigenda. Or a lot of these other services that are coming up on Eigen layer, like oracles, AI inference, fully homomorphic encryption, oracles that actually check what your HTTPs information is, which is locked inside a web server.
00:19:03.686 - 00:19:24.830, Speaker D: These kinds of services unleash a whole new category of products that can be built on top of crypto. That is what we are excited about. We don't focus on the TVL metric. It is a short term metric which can fluctuate massively. And I also want to encourage people who are staking and so on to also adopt this long term mindset.
00:19:25.490 - 00:19:57.686, Speaker B: That's fair. I wanted to ask sort of, this is a new emerging space in the Ethereum ecosystem, and you're solving a whole bunch of issues, but there must be shortcomings. So what are these shortcomings that maybe, Brendan, you can take this, because why would you plug into an avail or into a celestia, or into an Eigen dA? What's still missing? What's missing on the DA side? What are the shortcomings mean?
00:19:57.788 - 00:21:18.242, Speaker E: I referenced it earlier, that external DA, if you're settling on Ethereum, which is what we're doing at Polygon, external DA provides a different security guarantee than using Ethereum for DA. But we are already planning to integrate, or at least offer roll ups, the option to integrate with Celestia and eigenda and avail. And so I think that if you look at proto dank sharding, I think that there's a lot of optimism that it will reduce transaction costs, and I think that it will for two reasons, because call data was never meant to sort of function as a data availability service, and because we're increasing the capacity for Ethereum. But I would echo what Sriram was saying earlier, that I think that a real barrier to unlocking latent demand is obviously the fact that fees are too high and it's pricing users out, but it's also like fee volatility and the variance, and not having the ability to offer a consistent user experience, because applications can get crowded out by other sort of unrelated applications that might be on other chains or completely separate use cases. And so I think that increasing the total bandwidth and capacity for DA is something that will only improve user experience.
00:21:18.376 - 00:21:21.830, Speaker B: And unlock new applications. Sorry, go for it.
00:21:21.900 - 00:22:54.402, Speaker C: I just wanted to add that I think if I take a step back and think about downsides of the overall roadmap that we're following, this very roll up centric roadmap brings a lot of extra computation and increased bandwidth, but it really increases fragmentation in the overall experience, both for liquidity, for the developer experience, and the user experience. And I think anybody who has tried to transfer some assets from one l two to another l two, or God forbid, to another l one, you know, that is a world of pain, basically. And as a developer, if you're trying to develop an application that uses multiple chains at the same time for different purposes is a total nightmare to do right now. And it's only going to get worse, right? Because right now we have a few dozens or hundreds of roll ups. If you think of a world where we have thousands or hundreds of thousands, obviously the current situation can't scale. And that's why the avail team has been working on our Nexus layer, which is a unification layer, that it leverages our DA, so it brings blockchain data to the DA layer and then uses ZK aggregation to prove the correctness of multiple roll ups on top of that. And so it's the combination of the ZK aggregation with the ordering guarantees of DA that allow us to provide a unified experience across all of web three in a permissionless and trust minimized way.
00:22:54.536 - 00:22:57.190, Speaker D: That's Brendan's prompt to talk about the aG layer.
00:22:57.530 - 00:24:31.234, Speaker E: Yeah, maybe there are elements of that that may have been inspired by Polygon's AG layer, but I think it's really important to distinguish between the interoperability guarantees that are provided by sharing a DA layer and the interoperability guarantees that are provided by aggregating proofs, and the interoperability guarantees that are provided by something like the AG layer, which I think a lot of people misunderstand that it's about aggregating proofs, it's not about aggregating proofs. Proof aggregation is a mechanism that allows us to ensure safety and consistency between chains that are interoperating. And so I don't want to hijack this panel to talk about the design of the AG layer, but it's really important to sort of emphasize that if you want interoperability, you need to provide safety for chains that are interoperating at lower latency than Ethereum, block finality time and further to provide safety for chains that share assets and provide fungibility of assets via a unified bridge. And so I think if you look at what the AG layer provides, it's both of these things. It's safety for fungibility of assets across chains. It's also safety that guarantees that chains can't be settled in a way that's inconsistent or that atomic bundles can't be unbundled when settlement on Ethereum happens. And so I think that DA is a really important security mechanism for individual chains, but it doesn't solve interoperability on its own.
00:24:31.234 - 00:24:34.050, Speaker E: I think you need additional infrastructure.
00:24:35.290 - 00:25:11.738, Speaker D: Just a couple of comments on this fragmentation problem. The first one is, I think people overemphasize the problem of fragmentation. And the reason is we are starting from a defi oriented locus, because in financial services, liquidity is the main product and liquidity gets fragmented, then it's a problem. But if you've ever developed any kind of like a cloud scale thing, like imagine a game. Most of the games have scenes. Each scene is run on a separate server. You cannot run the entire game on a common synchronous composition layer.
00:25:11.738 - 00:25:44.118, Speaker D: So synchronous composability is actually one of the most expensive things possible. It is neither necessary nor sufficient for large scale adoption because we are starting from a very defi oriented lens. People think and overemphasize this problem. Lots of other places where absolutely no need for super fast composition or fragmentation is not a problem. Fragmentation is a solution. You can have highly scalable systems. The second point is something like aglayer.
00:25:44.118 - 00:26:29.098, Speaker D: And I haven't looked into the design of the avail aggregation system, but something like Aglair has this really powerful primitive where you can use synchronous composition on demand. So we are running on asynchronous zones. And when I need to create synchrony, I lock the state of these two systems and then I can do synchronous composition and then release the lock and then go back to asynchronous composition. So you pay for synchronous composition on demand. So I just want to point out that these solutions also are having emerging these problems also having emerging solutions. And there are other services built on eigen layer for super fast composition using crypto economic guarantees. So lots of interesting stuff coming up here.
00:26:29.264 - 00:26:48.480, Speaker C: Yeah, Blake. Plus one for asynchronous composition being a new thing that I think will be pervasive throughout web three. So far the focus has been on synchronous composability. And I think that's overly restrictive in terms of use cases. Fully agree.
00:26:49.490 - 00:27:16.514, Speaker B: We have like three and a half minutes. I'm looking at the clock here. We have three and a half minutes left. And so I want to get like a big picture take, sort of combine my two last questions, but I wanted to ask. We now have these three major projects right around Da sitting on stage here, and Polygon, I mean, I don't need to speak for Polygon. Everyone knows what polygon is. But I want to ask sort of what this market is going to look like, because especially on the layer two landscape.
00:27:16.514 - 00:27:42.790, Speaker B: We've talked about this, Brendan, that there's just been an explosion of l two s. I can't keep up with how many l two s are just like popping up left and right. What will the market eventually look like for this Da ecosystem? How are things going to evolve? And what can we expect from each of your solutions moving forward? And Brendan, what do you sort of see from the l two side? Let's start with Nikko.
00:27:42.890 - 00:28:16.822, Speaker F: Yeah, I think a lot of people have this idea that DA will become a commodity, and I strongly disagree with that. I think it will be cheap, and that's actually a feature, not a bug. It's really good to have very cheap, scalable da. And this is what Celestia, for example, is all about. I think that's also what eigenda and avail are about. And at least at Celestia, what we believe in is economic sustainability by economy of scale. So it's about providing cheap, scalable da, but at massive scale.
00:28:16.822 - 00:28:41.700, Speaker F: Because if you believe that web three is going to be globally adopted, and we're going to be running very critical things on chain, the demand for block space is going to be absolutely massive. And so it's about being a layer that can actually support that level of scale. And even if the margin on DA is very, very low, at that level of scale, it becomes economically sustainable. So that's what we believe, at least.
00:28:42.710 - 00:28:44.420, Speaker C: Yeah, sorry.
00:28:46.230 - 00:29:21.418, Speaker D: Okay, I'll go for it. One of the things I think roll ups unleash is this is a little bit of a counterpoint, is the single centralized sequencer is a superpower of a roll up. What it means is I can do instant confirmations. Not only I can do instant confirmations, I think the thing that I'm much more excited about is we can do subjective admission control. What do I mean by that? Right now, to access a chain, you need to pay a fee. Fee is very nondiscriminative. A real user cannot be discriminated from like an MEV bot, which just upbids a real user.
00:29:21.418 - 00:29:53.626, Speaker D: And one of the really powerful things that the Internet did is to offer cheap or free usage to lots of people around the world. We talk a lot about decentralization in the space. Having gatekeeping our systems with fees is a really bad thing. What a single central sequencer can do is to do admission control, admit users. Some people pay fees, but other people who authenticate themselves through other options can be admitted into the system without a fee. This is going to unlock a massive amount of innovation in crypto apps, which I'm very excited about.
00:29:53.808 - 00:30:28.770, Speaker C: Yeah. Plus one thing from my perspective is that I do think that DA, in some sense will be table stakes, and that a lot of value will be elsewhere in the stack and for us, this idea of being able to unify all of web three, both on the user experience, developer experience, liquidity, as well as with our fusion security product that brings liquidity to the availda, I think, is our take on this topic.
00:30:28.850 - 00:30:30.680, Speaker B: Brendan, I'll give you the final few.
00:30:32.810 - 00:30:57.966, Speaker E: I mean, I think that from our perspective, there will be network effects that accrue around roll up ecosystems that come from the ability to interoperate. But I think that having extreme flexibility and choice and giving users the ability and chains the ability to exploit trade offs in how they select DA is something that's really, really important. And I think everyone else on this stage is kind of working toward that future.
00:30:58.068 - 00:31:03.358, Speaker B: All right, well, thank you all for this discussion, and thank you to the audience and enjoy the rest of your day.
00:31:03.444 - 00:31:04.720, Speaker C: Thank you. Thank you.
00:31:12.630 - 00:31:14.018, Speaker F: Hello. Hello.
00:31:14.184 - 00:31:15.390, Speaker D: My name is Miko.
