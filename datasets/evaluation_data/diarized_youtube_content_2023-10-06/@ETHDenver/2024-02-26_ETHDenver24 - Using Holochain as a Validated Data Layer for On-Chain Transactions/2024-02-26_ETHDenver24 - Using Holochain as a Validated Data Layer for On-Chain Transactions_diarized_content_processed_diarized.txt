00:00:00.170 - 00:00:38.694, Speaker A: Investors, average users, smart contract developers, creators, basically anyone who's building a product in web3. There's something in this talk for everyone, but it's mostly geared towards developers building the next web3 products. And that's you guys, the builders. Although I am from consensus and metamass, this talk is not specifically or just about us. This talk is about web3, making it better. The concepts, tools and technologies you'll need to know about to make web3 a great onboarding experience for the average users, not just the seasoned web3 folks. So I am HTTP junkie on Twitter again.
00:00:38.694 - 00:01:19.794, Speaker A: My name is Eric Bashard and I'm HTTP junkie on Twitter Telegram warpcast. I currently work at Consensus as a Devrel supporting the metamask, SDK and API, but we also work with all the various consensus products. You will see us out of the booth, you'll see us at the main event. The Devrels will have these nice pink shirts on. If not, I'll also have some pictures of some of our devrels in the slides. Yeah, so in 2021, I was a node JS Devrel at Couchbase, and when given the opportunity to host an ethereum conference in Dubai, my boss would not let me do so. So that's why I left Couchbase.
00:01:19.794 - 00:02:07.506, Speaker A: That was part of the great purge when there was a mass exodus of, I guess, web two over to web3. So I picked up the telephone, dialed the seven digits and said, hey Couchbase, what's up? I quit because they wouldn't let me explore blockchain. Yeah. So before that, before consensus and Couchbase, I was working with SolarCity, Tesla and progress software and web two. So I've been an applications engineer for Tesla and SolarCity, and then I've also been a react developer advocate and a node JS developer advocate for past eight years. Or, no, sorry, six years. Yeah.
00:02:07.506 - 00:02:57.154, Speaker A: So from 2015 to 2019, kind of finishing up my web two career and then getting into web3 with consensus. And I've been here ever since. So what do I think? Well, I think web3 will continue to grow, reach more and more people all over the world, and that's why I'm passionate about iterating developer experience and striving for a better UX. In web3, some of the things that we'll talk about today. What is user experience? What's developer experience? How might developer experience influence user experience? In web3? We'll talk about some of the eips that help to iterate developer experience in UX and web3 in 2024. And also some of the products and then some closing thoughts. So I'm old, so when I think of user experience, I don't think just about a UX designer.
00:02:57.154 - 00:03:58.566, Speaker A: I think about the overall experience that a user has with a product, be it a tangible or a digital good. In my honest opinion, user experience is everything that touches upon your user's experience within your product, period. So the holistic experience, everything from how the developer relations team talks about it, how your documentation works, tutorials, everything is part of, well that's part of developer experience. But kind of everything that happens on your product is kind of the user experience. So you need to be kind of clued into everything, start small, iterate and really create a great experience overall. Another great resource that I wanted to talk about that has some overlap with this talk, but it's one of the most important talks that I've seen and it digs a little bit deeper into product design. So I'm going to periodically have some of these screens where there is a tiny URL at the bottom and I'll try to leave the screen up for a minute so people have the opportunity to take a picture of those.
00:03:58.566 - 00:04:30.494, Speaker A: But also remember that this is streaming out on YouTube and X. So if you want to go check the talk out and see some of these URLs later, there should be no problem. The talk is recorded. So on the right, right here, we're looking at Uniswap's connect wallet experience. And the thing that's great about this and one of the eips that we're going to talk about is EIP 6963. So it noticed that I had metamask flask installed. That's our developer version of Metamask.
00:04:30.494 - 00:05:06.926, Speaker A: But if I were to also have another version of Metamask and Flask both installed, it seems that Uniswap is using EIP 6963 already. So it would have both of those wallets listed as well as some of the wallets that they might also put into their menu. But let's say that I also had Rabbi wallet. I had Rabbi Wallet and Metamask flask both installed before EIP 6963. You might click on Metamask and actually get taken to another wallet that identifies as metamask. So it's a great thing, kind of EIP 6963. We'll talk more about that.
00:05:06.926 - 00:05:56.240, Speaker A: But user experience is everything from usability, how easy it is to use, or onboard new users, to your web or mobile product performance. What's the time to first render and loading states? How quickly data is fetched or how long transactions take and how to keep your user engaged during those periods of load. It's also how you explain your product. Part of the user experience is the nomenclature that you use in your website. Try to make it to where people that aren't super experienced in web3 don't feel alienated when they visit your product. It's also about how users interact with your product and accessibility. Like we're in web3, some of the experiences are kind of bad, but we have to remember all the accessibility tools that we use in web two and we shouldn't forget those.
00:05:56.240 - 00:06:41.200, Speaker A: Uniswap, it just works. It's great. Here you can see one of the other screens in Uniswap. This is like the settings and they've gone the extra mile to ensure people from all over the globe can use this interface in their language of choice and their currency. But more importantly, Uniswap is great because everyone from backend, front end, smart, contract UX designers, everyone on their team, PMS, everyone was in lockstep creating this amazing product. And one thing I'd like to know if you guys want to stop by and see me is if you have another web3 experience that you think is a great user experience. I use Uniswap because it's been around for a long time and it's always been kind of, I think an example of kind of like a shining web3, good user experience.
00:06:41.200 - 00:08:38.100, Speaker A: So another point I want to make about UX is it's not practical to outsource your UX design and not have those UX designers know what tools and technologies and frameworks that the web developers are using, what UI frameworks, what CSS frameworks, all of that. Your product designers and your UX designers should kind of be working with the developers to ensure that the product that ends up being built is similar to what that designer had in mind. If your devs use react, tailwind or certain UI libraries, the UX designer needs to know about that and needs to design for those tools. This is obvious, but it suggests that if you don't know your target audience, if you don't know your users and what they want, and you don't do the testing, research, studies, interviews and receive feedback from your users, you won't be able to build a product that is great for your users. So yeah, product teams and engineers need to build things that people want and you can only do that by having a close relationship with those users and getting that feedback constantly and iterating your product. So some final thoughts on UX as you start to build your project here at East Denver. As you start to hack, some final thoughts about kind of UX is just to start small, iterate, figure out what you want to kind of create as a user experience for your users and build that stuff first, make sure it's functional and again, only move on once you've kind of really nailed down that user experience and that flow and those small amount of components first, and then only kind of continue and get information from your users as what kind of features you should add next.
00:08:38.100 - 00:09:31.160, Speaker A: So what is developer experience? It's a little bit different than user experience. So it's from your docs, your tutorials, your articles, your workshops and presentations, even the presentations that I give about our products at Metamask. That's part of the developer experience. It's the actual experience a developer has when building with your tools. Metamask comes to hackathons to put new products in front of nice and understanding hackers. Knowing that our products are kind of new, they might have some bugs in them, but we get that feedback and it helps to shape the product and helps us to iterate our developer experience. One of the first things that our developers did, our developer relations team did at Metamask was work directly with the Metamask team to ensure that we could recreate all of our docs and put a new version of the docs out there that continued to iterate that developer experience.
00:09:31.160 - 00:10:21.926, Speaker A: So here is what a front and center. So we're going to go through some of the different this is DX on the right here we have Wagme. The point I'm trying to make here is that this is a good, Wagme has good developer experience in their docs. They have a get started, they have a why Wagme? They have a GitHub link, they have what developers need. What are the react hooks that they provide? What are the typescript support that they provide? What wallets do they connect to information about other products that's used to create Wagme, like VM. And then a long list over there of kind of things that I think are just very super important that you should kind of have nailed down as part of developer experience. And there's a lot of it, it takes a lot of time to do that.
00:10:21.926 - 00:11:19.894, Speaker A: So if you're building a product or a service, you should keep these things in mind. So how does DX influence UX in web3? So the onus is on us as the developers of a product to know how to use these developer tools and when to use out of the box features versus customized and more specific technology to enhance the user experience. If you have a good developer experience, the developers who use your products should in turn be able to create a user experience for their users. You need subject matter experts and highly qualified software engineers to build your product. A lot of web3 sites that we see out there look a lot like mvps and they should take this into consideration. And using products, providing good developer experience makes it easier to create good ux. If you kind of follow that.
00:11:19.894 - 00:12:11.942, Speaker A: If you create a good developer experience around your products, those developers should then in turn be able to go and create awesome web3 experiences, some products and features. So this is kind of what I wanted to get to, and we're going to go through these pretty quickly, but we're going to talk about some of the eips and some of the different products out there. We'll talk about a few of Metamask products, but we're also going to talk, I'm going to kind of step away from that a little bit too, and also look at the ecosystem and talk about some eips and products that I think are really good and have good DX and Ux out there. So first, EIP 6963. There's a tiny URL at the bottom here. When I was learning EIP 6963 and how it helps developer experience and how I can help to connect to wallets, I didn't have a good resource out there. So I wrote the resource that I would have liked to have down here.
00:12:11.942 - 00:13:07.138, Speaker A: So EIP 6963 is the multi injected provider discovery proposal. It introduces a different approach for discovering and interacting with providers. So in contrast to the existing window Ethereum object, the best way to think of it is if I have several different wallets installed in my browser, then whatever product that I'm using, whether it's Wagme or web3, modal or web3 onboard, these products should be able to detect those different wallets and be able to connect to them correctly. And that's kind of what EIP 6963 does. And I'm super excited about developers being able to have this technology and build it into their applications and credit where credit is due. Wallet connect and web3 modal were the first ones as a library to implement EIP 6963. Since then we have WagMe.
00:13:07.138 - 00:13:49.950, Speaker A: The metamask SDK kind of integrates with WagMe, but you can also do it as yourself as developer. I have a demo down here in this article that can show you how to do that from scratch on your own. Next one EIP four three seven account abstraction so account abstraction is set to revolutionize the way we perceive and interact with wallets, making gasless transactions the standard. And secure social logins makes social secure logins the new norm, fundamentally reshaping the Ethereum user experience again. Tinyurl.com down here, I can't spend a lot of time talking about this. I only got 6 minutes left, but I think this is one of the better talks out there about account abstraction.
00:13:49.950 - 00:14:38.870, Speaker A: It's a little bit old, but it's a good primer on what account abstraction is and what you need to know as a developer to kind of get started using it and how to think about it. Cookbook dev so as hackers here at East Denver, you might have interacted with a guy named Tyler out there. He's one of the mentors and cookbook Dev is doing a lot for developer relations and developer experience in the web3 space. Cookbook lets you find any smart contract protocol and library, and you can access chain quick starts and fine tuned AI chat bots to explain unfamiliar concepts. In web3, developer resources are often scattered. Builders need to learn many different technologies and many docs to build a good dap. So cookbook aggregates all this information and these resources and makes the learning process a lot easier.
00:14:38.870 - 00:15:47.690, Speaker A: Also, he built a chat bot for the it's kind of an unofficial chat bot for the East Denver hackathon, but it's great and it answers the questions for hackers so that he can kind of offload some of the work that he's doing and help more hackers and give them access to that chat bot. And I just think that Cookbook Dev is doing a lot for developer experience and developer relations in 2024. So check them out. Cookbook Dev this is a little bit of a I can't fit too many screenshots on here, but go check out cookbook dev and I think it's a really great resource to start to learn the ecosystem and all the different tools. Gelato networks, I'm just pointing out really good user experiences out there and they have a great developer experience also. So as a developer, you want to be able to kind of rely on automation and reliable infrastructure so developers can focus more on building innovative features rather than managing repetitive tasks or their own custom infrastructure. Enhanced user interaction by automating smart contract executions, it provides users with a smoother and more seamless experience.
00:15:47.690 - 00:16:15.982, Speaker A: Go check out gelato network if you haven't used it before. It's like the back end for web3. I don't know AWS for web3, horrible probably analogy, but here's some of the different services that they offer, and I think they're doing a really great job as well. Metamass snaps. So we're going to get into a few of the things that we have that we're trying to do to make a better developer experience in 2024. And one is metamask snaps. We launched this back in September through interoperability.
00:16:15.982 - 00:16:55.118, Speaker A: Metamask snaps can expand the wallet's functionality. Think of snaps kind of as plugins for metamask, kind of as a plugin for chrome or extension for chrome is to chrome. Snaps kind of are for metamask. It allows you to extend the metamask API, catering to users engaging with non EVM chains. You can manage your bitcoin in a metamask wallet in a secure manner through security. Snaps implements measures such as checking receipt addresses against known phishing databases and simulating transactions to determine their potential outcomes, and basically just ensuring that users have a clear picture before confirming any transaction. That's just a few of the things that metamask snaps does.
00:16:55.118 - 00:17:23.180, Speaker A: And here's some of the team that we'll have here at East Denver. Ziad Sab is our, Ziad saab is our developer relations guy for metamask snaps. Christian Montoya also, and then Francesco, you'll see all these guys either at the booth down here or at the main event at our metamask in linear booth also. Ziad's going to be, he's going to be djing, I think, in the chill room. They turned me down. But whatever, he's going to be there, it's going to be great. So go check him out there, too.
00:17:23.180 - 00:18:15.500, Speaker A: Finally, the Metamask SDK. So the Metamask SDK is a library that provides reliable, secure, and seamless connection from your DAP to not only the Metamask browser extension, but also gives your user the option of connecting to Metamask mobile. Also, with mobile and Unity games, you can connect directly to or through a QR code to Metamask mobile. And by integrating the SDK into your DAP, millions of Metamask mobile users can connect to their preferred metamask client, and again on web, they can connect to the extension. So this is just a little bit of a screenshot here. When someone clicks on the metamask button, if you have the SDK installed, they have the option of clicking on desktop or mobile, and if they click on mobile from the web, you can scan this QR code with your phone. It'll connect seamlessly and you'll be able to approve all the transactions right through your phone.
00:18:15.500 - 00:18:40.466, Speaker A: Also, we're working with the Metamask portfolio team to make SDK better. So they are testing it out. They haven't launched it yet, but kind of they're putting it through all the paces and they have a huge laundry list of all the things that we need to do to make it a better user experience. So working great with them. But if you haven't checked out metamask portfolio, check that out. You can do swaps, you can do bridging, staking, all that. That's it.
00:18:40.466 - 00:20:09.410, Speaker A: Thank you. My name is Eric Bashard, HTB, junkie on Twitter, Telegram, Warpcast, everything. And good luck building here at ETH Denver. Thanks a lot everyone. It won't appear until the timer. I'll make sure that they have it though. You should probably test that once that gets down.
00:20:09.410 - 00:25:30.768, Speaker A: Test, test plasma Plasma Plasma exit game exit game exit game hi everyone, I'm Leona from Plasma next. I've been researching the scalability of plasma and scalability of ethereum for many years, especially since 2018. I participate in the Plasma research group, Plasma research community. Yeah, it was really good day for me. While doing this research lifestyle. I went to many countries, including El Salvador, Nigeria, Turkey and Argentina. The first talk I had in Argentina was about crypto.
00:25:30.768 - 00:26:15.940, Speaker A: It was with taxi driver from the airport to my hotel. She says she her crypto to her family for the savings. Another taxi driver was saying, same thing. In Nigeria, people want to earn and save money in crypto, not in fiat. Basically nigerian people are english native, so they can get a lot of job online, unlike in their country. And yeah, they can earn and save money with crypto. So in some countries there's significant adoption of crypto, unlike in developed countries.
00:26:15.940 - 00:27:10.260, Speaker A: Unfortunately, this is their story about crypto, not about ethereum. People are not using ethereum, even layer twos for these use cases. People using more centralized blockchains like drone or something like that. And why is that? Simply because of one reason. Yeah, lolaps are great. Lollaps are totally great for scaring defi, but it's not cut out for these use cases for million billion level of mass adoption. So we need plasma for that today.
00:27:10.260 - 00:28:14.960, Speaker A: I want to introduce you guys and announce the plasma next, which is the complete version of plasma. And actually this is the first completely stateless asset system in crypto history. So do you guys remember plasma? Plasma was the biggest and hottest topic in 2018 in server community. Literally almost all people are talking about plasma, how to scale reserve and how to complete plasma. But after that, plasma seemed to be fail in 2019, their early 2019, and people moved to roll up the reason why plasma failed once in early 2019. There are three reasons. The biggest issue was online requirement.
00:28:14.960 - 00:29:03.090, Speaker A: People need to be online to watch over the malicious activities of the others for seven days on chain, that was their most terrible part of plasma. And the second worst is that on chain verification cost was quite high. And yeah, each coin had each on chain verification cost, so it was about in plasma cash. And it was not so realistic way to scare servers as layer two in that perspective. The third one is a long waiting period to exit money to layer one. You need to wait seven days to exit money to layer one. So it's not nice us, of course.
00:29:03.090 - 00:30:04.192, Speaker A: So basically we can summarize that in plasma we had problems simply because of online requirements, not these problems deliver from online requirements. So if we fix their online requirement problem, we can renew the plasma and actually the plasma next can avoid all of these problems. Plasma next doesn't harbor any problem of online requirement. So that's why I named this project plasma next. I was thinking about the plasma final as well, but yeah, I chose plasma next. Okay, what is plasma next? In technical discussion? It's a payment channel upgraded by ZKP. So the famous HTLC is replaced by ZkptLC payment channel itself.
00:30:04.192 - 00:30:47.280, Speaker A: It's replaced by one way payment channel to eliminate their online requirement. So it's one way. So the senders balance will keep decreasing the payment channel. So the last commitment is always the best for their hub and operator so that the operator does not have any incentive to roll back their commitment. And on chain channel opening is replaced by off chain channel opening. Actually, this is the biggest thing in this architecture. For example, in lining network, you need to funder payment channel or open the payment channel.
00:30:47.280 - 00:31:33.020, Speaker A: On chain, the procedure is on chain, but in plasma next, everything is off chain. So basically almost all things, yeah, all things are off chain in plasma next system. So that's what we call stateless. So yeah, let's deeper dive to their architecture. So in plasma next, let me start from the payment talking about the payment from Alice to Carlo via Bob. Bob is an operator or server or hub. No, it's kind of middleman as a server and Alice is a sender, Carlos is the recipient.
00:31:33.020 - 00:32:25.100, Speaker A: And you can imagine that there are a lot of Alice's and carols in this world, right? Okay, so the first things we need to talk about is Bob to Carol's payment. So there are many carols. Bob need to send a lot of money to many carols to perform this one too many payments. The most efficient way to do this is our DKP airdrop with market tree. So we can utilize Margot tree and include a roll up transaction from Bob to many carols and integrate that to one tree. And bob posts only the loot of the tree to the blockchain. And this data is only 32 bytes, so it's kind of super limited.
00:32:25.100 - 00:32:57.576, Speaker A: And Bob can perform a lollip lollipop transaction. Too many carols are in this idea. And Carol gets a proof in that tree, and it means that she gets their money from that payment. Okay, it's one too many. So how to make it to many to many? There are Lola carrots want to sell money to many carols. Right? There's a lot of pair of rs and carols in this world. So let's combine the payment channel to that market tree.
00:32:57.576 - 00:33:41.828, Speaker A: Airdrop. Yeah, airdrop is a kind of word, a bit misleading. So yeah, I will use this word carefully, but no, yeah, it's kind of airdrop. Right. And Alice will set the payment channel here, and she sets the condition of the completion of the payment. The condition is that proof of the payment from Bob to Carol in that tree. So if the bob makes a payment, market tree payment and maker proof of payment, the balance of aris in that payment channel will shift automatically with this proof.
00:33:41.828 - 00:34:14.852, Speaker A: So yeah, then Alice will decrease the balance in the channel. Bob gets their balance in the channel and decrease the balance in the tree. And Carol can get their money in that tree. So that's how it works. And surprisingly, Carol can reuse that payment, given money to fund or open the channel or off chain. This is quite important part. So Carol can use the money received in the tree as a payment channel.
00:34:14.852 - 00:34:49.330, Speaker A: And so it makes this system kind of bi directional. Right? Aris can send money to Carol and Carlo can send money to anybody. It's completely off chain. So I said that it's a stateless idea. So you might remember that impossibility of statelessness. It was proposed and proven by a 16 the notable researchers. And basically their paper is right, correct.
00:34:49.330 - 00:35:33.720, Speaker A: It's basically impossible to make a stateless blockchain. But we can avoid this problem in theoretically and practically. So actually, we don't solve this problem directly, we can delay this problem as long as we want. That's how to avoid this problem. So, yeah, as I said before, channel openings is off chain. Their payment in payment channel is off chain, stateless, without on chain data cost. And channel funding itself is also stateless.
00:35:33.720 - 00:36:00.016, Speaker A: Right. And only the channel closing is stateful. It uses state and it makes the state growth. But you don't have to do this simply because you have to do this when you have a conflict with the operator. So if you don't have the conflict with the operator, you don't have to do this. So basically we can avoid this problem. Impossibility of statelessness.
00:36:00.016 - 00:36:58.436, Speaker A: Totally and perfectly. Okay. The salary is perfect, right? This is our first stateless asset system in crypto history. Then after this, what should we do? So basically this is an open source project and our intention is making Ethereum the best finance system, most scalable finance system, the most scalable payment system in this world. So you can feel free to forget cronit and deploy and launch your own network. You can feel free to do that on your favorite chain. We have a roadmap to integrate these separated network to one network in the later stage of Loloma.
00:36:58.436 - 00:37:46.720, Speaker A: So yeah, you can really feel free to do that. And if you care about the programmability, don't worry about that. No, we can inherit the programmability solidity, programmability from the same layer one the settlement layer. Unlike lightning network, we have programmability and there's no Watchtower. You might ask where's the demo? Right? Yeah, basically we already launched the maintenance offer, so you can test this concept under. You can experience fast stateless payment. Fast stateless programmability with this link.
00:37:46.720 - 00:38:40.270, Speaker A: Yeah, we can make a great payment network together. And please participate this. And after joining this, please participate in the discussion we have. I just posted their paper in their research. So yeah, you can join the discussion and you can find their documents and you can join their core dev and development around this and you can literally do anything. Thanks for your time and let's participate in this kind of stateless movement to make Ethereum great. And let's bring the plasma to the main stage of Ethereum.
00:38:40.270 - 00:50:12.610, Speaker A: Thanks very much. Yeah, feeling great. Hi everyone, my name is Evan and today I wanted to talk about stop building the dystopia. A little bit about me first to get started. I'm the CTO and co founder of Dmox Labs. I'm also a zprize architect. I work exclusively with programmable privacy through ZK.
00:50:12.610 - 00:51:10.062, Speaker A: We work with Alio, Polygon, MIDN and Aztec. Our largest product is called the Leo Wallet. We have more than 250k weekly active users, but we do everything from building full products to acceleration on web GPU building infrastructure. Our kind of whole goal is to make programmable privacy a reality. So the first thing I kind of want to talk about is what is really like. If we imagine that we stop using credit cards and we stop using chase and bank of America, what does the world actually look like? Well, the first thing is once you're doxed, once everything you do is public forever. The only acceptable way right now to get privacy is through centralized exchanges and custodians.
00:51:10.062 - 00:52:00.190, Speaker A: Basically, you can use Coinbase as a mixer. This is bad. If you put open source privacy preserving code on chain like tornado cache, you go to jail. And something I really want to kind of emphasize is people who tout web3 as a solution for the unbanked. Well, most of those unbanked people do not live in the most democratic government with the best representation. How long is it before some government who doesn't treat their people super well pays chainalysis to help give a list of who the users are. So what I really want to do with this talk is reframe privacy.
00:52:00.190 - 00:52:39.766, Speaker A: Privacy is not just about it's the right thing to do. It's not just protecting fundamental rights. Fundamentally, it's inevitable. It's economically productive. It's going to make things cheaper, faster and better. So to kind of understand the history of privacy, we have to look at the Internet. Before privacy, when we had HTTP, basically the Internet before the introduction of SSL by Netscape in 1995 was not a system that could be used for anything meaningful.
00:52:39.766 - 00:53:39.742, Speaker A: You couldn't have ecommerce, you couldn't have things like social media. All sorts of financial online activities did not exist on the Internet before we had widespread adoption of encryption network systems. Without privacy, you have all sorts of man in the middle attacks, just pure data theft, right? These sorts of things like the Internet only really became useful once we started integrating privacy. And it's kind of crazy now that we don't really even think of SSL or TLS or now quick as privacy technologies. It's just how the Internet works. And we really can't imagine going back by basically, encryption is expensive, and before encryption was widespread, it was actually expensive to encrypt the whole Internet. And this was a big barrier.
00:53:39.742 - 00:54:39.522, Speaker A: But what we ended up seeing is that by reducing the amount of trust on the infrastructure that powers the Internet, like you don't have to trust every computer in this decentralized system, you actually get way more economic productivity from the Internet. So encryption is never free. It's always going to be more expensive than plain text, but we have to weigh it against the economic benefits. So that's why I'm really excited about enhancing blockchains with zero knowledge proofs. Kind of my view on tees. I'm happy to give that if you want to talk to me later, but there is a reason I focus exclusively on zero knowledge proofs, especially with the introduction of different ZK vms, especially those that preserve privacy. Things like SnarkVM, things like noir or Berettenberg polygonmiden.
00:54:39.522 - 00:55:46.426, Speaker A: There's a lot of different approaches to privacy that I'm really excited about. And most importantly, zkps are really just a fundamental advancement in encryption, and we can kind of see that as the costs come down, we will be able to realize the economic benefits of using zkps more and more, especially around things like identity. The way that we manage identity right now on the Internet, like forgetting web3, is insane. Age verification is an absolute joke. Your identity is probably worth about two or $3 on the dark web. This is your Social Security number, every address you've ever lived with, your family's addresses. Identity on the Internet is really kind of a joke, and it's insane how much manual processes we have to build around identity and how expensive this is and frustrating this is as a user to be able to kind of deal with these problems.
00:55:46.426 - 00:56:44.400, Speaker A: So I'm really excited about zkps in terms of identity, but I don't want to totally discount public blockchains. This is ethnver. We work with l two s on ethereum, but public blockchains are all about decentralizing financial power away from nation states and giving it back to individuals. But right now, I do really think we have to accept that it is a toy technology. Banks, large institutions, governments, they don't take blockchain seriously. And having everyone take, like, we're still kind of pre Internet bubble, I'd say, with the technology, because people are excited about the future of blockchain. No one is super excited about the present.
00:56:44.400 - 00:57:33.120, Speaker A: So another thing, I kind of want to reframe privacy. Not only is it economically productive, but it's not going to start with individuals. I think a really big misconception is that, and sometimes I say we're building tools like private Venmo. Businesses and governments are going to be the primary drivers of privacy technology the same way that they were the primary drivers of Internet technology. I just want to give you a quick example. Who here actually uses signal more than telegram? Just quick show of hands. All right, we got like three out of 40, 50, right? That's not great.
00:57:33.120 - 00:58:27.150, Speaker A: It's going to be really tricky to force individuals to new technologies because individuals, they want convenience, they want speed, they want a nice ux. There are all these things that we have to do for individuals. So I can't get my mom to use signal no matter what I do, no matter how many times I tell her about the benefits, I promise her it's not related to crypto, that it's just a privacy technology. My mom isn't going to use signal, she's going to use iMessage because it's the thing that works really conveniently and it's integrated across her iPad, her MacBook, her phone. It's going to be really difficult to get individuals to use privacy technology. Everyone at my company uses signal. Why? Because I don't talk to them if they don't use signal.
00:58:27.150 - 00:59:42.962, Speaker A: This is the difference between businesses and governments, where if a business sees a benefit in using a certain technology, businesses can mandate adoption, and businesses are going to be much more rational about the economic benefits of a technology and much faster to adopt it. And governments as well. The leader in cryptography is some government, probably the US government, I hope, but it's definitely a government. So in terms, why are businesses going to use privacy and blockchains? Well, I think a really kind of important understanding is that right now every service that a business uses exposes all of their data to that service, which is not great for the business. Right. If you use, for example, payroll system, you have three or four different companies who have all of the data about how much you pay employees and who knows if they're selling that data. There was actually a case with Carta, who manages cap tables for startups.
00:59:42.962 - 01:00:39.706, Speaker A: So investors and employees would manage their equity with a startup through this platform. And Carta then started reaching out to investors, saying, hey, do you want to buy secondaries in this other company that's like the one you invested in. So businesses obviously kind of revolted against that. So you're now going to be able to build services where businesses and individuals using, you don't have to trust you, you simply provide like there's this trust assumption built into all software right now that we can start removing. And I do want to clarify too, I don't think zkps are the end all. I'm really excited about fhe and encryption pretty much keeps going until we get program indistinguishability. But that's a talk for another time.
01:00:39.706 - 01:01:31.850, Speaker A: But zkps and fhe aren't the only technologies, but they're ones that are really useful for decentralized systems. So that's why we talk about them a lot. Governments, too, are really weird. I think a lot of people in crypto, especially those who work with privacy technologies, are worried about regulation. They're worried about kind of regulation by enforcement, because rules are not well articulated of what you can and can't do. So I think there's a little paradox there where governments are also the first movers to adopt new privacy technology. The government, like I think it was the navy, invented Tor.
01:01:31.850 - 01:02:19.100, Speaker A: The NIST handles all of the standards for encryption. There was a recent competition to handle, like Falcon twelve signatures, which are quantum proof signatures. They're the ones kind of leading the way for the path of general cryptography. And governments want privacy. Something that's kind of ridiculous to me and something that no one really seems to be focused on is privacy for how governments pay contractors. Right? So it's sort of insane that the whole banking system sits between the government and how much we pay, like Boeing, to make secret fighter jets for us. Right.
01:02:19.100 - 01:03:20.826, Speaker A: I very much believe that if a government had a peer to peer way to pay contractors privately and without any additional information being leaked to any external parties, governments would really want to use this sort of technology. I think that there is a giant misconception that governments are against cryptography. Governments just want it for themselves. Right. But that's why we build open source software, so that as we develop these technologies, governments who will be kind of the first to test and battle, harden them are also. We're not kind of closing individuals off, because ultimately, cryptography is applied math, and it's really hard to restrict the usage of applied math to any particular group. So I really kind of like to refocus the talk.
01:03:20.826 - 01:04:02.882, Speaker A: This is kind of nearing the end. Public blockchains right now, at best, are going to be a toy. They're going to be fun to play with. Maybe we can gamble and speculate, maybe we can experiment with new forms of building organizations and incentives, but they're going to be a toy. And at worst, if you think about actually adopting this technology, where every purchase you make is public, every time you receive a paycheck, it's public. Every time a parent gives their child their allowance, it's public. It's literally the most dystopian technology imaginable.
01:04:02.882 - 01:04:57.030, Speaker A: And building privacy, I just don't even think it'll happen. Right. I don't think the adoption of this technology, which is fundamentally about removing governments and nation states, from controlling monetary supply. It's just not going to happen. And privacy gets us there because it's not just morally right. I think intuitively, if you're at this sort of event, you probably agree that privacy is important to protect. But do you use a VPN every single time you use chrome, or do you use the Tor browser? Probably not, right? But the economic efficiency, reducing the trust requirements because trust is really expensive is going to drive business and government adoption.
01:04:57.030 - 01:05:54.666, Speaker A: And the business and government adoption is really important because I think a lot of us, we try to build experiences for individuals. We're not focused enough on building private payroll for businesses or private cap table management, these sorts of solutions that are economically productive and valuable to corporations. This is where we should really be focusing our effort to get back to the main event. We are sponsoring a $4,500 prize that is going to the best use case of privacy in DFI. We're calling it ZFI, but it's not in my job description to name things. So we're building it on alio using the Leo wallet. If you have any questions.
01:05:54.666 - 01:06:43.980, Speaker A: If you want to learn more about everything from where proof generation acceleration is at to how do we actually build consumer grade products that hundreds of thousands of people use that integrate these privacy technologies, feel free to reach out. To me, this is a QR code to the bounty. I got a few extra minutes, so if anyone has questions, there are no mics, so you have to scream really loud. But thank you for joining me. Really appreciate it. All right, any questions? If not, I think we're good. Well, thanks again, everyone.
01:06:43.980 - 01:13:25.170, Speaker A: I hope you have a wonderful time in Denver. Well, you're the moderator, right? Like, you're the. Oh, you want to switch. You want to moderate from over here? I thought you'd want to see us. Oh, there you go. Pretty easy. And you're gonna ask a lot of questions amongst your test.
01:13:25.170 - 01:14:07.370, Speaker A: I'm the. But how do I know if it's working? All right, cool. Okay, thank you. Do you have a soundboard with, like, the clapping effect and maybe, like, the bass line from Seinfeld, like, emailed us ahead of time? Fiends. Come on. Okay, clap. Checking.
01:14:07.370 - 01:14:27.378, Speaker A: One, two, three. Doesn't sound like it. One, one, check, one, check. Two, three. Can you hear us? Okay, 1231 to free. 1231-1231 to free. One to three.
01:14:27.378 - 01:15:31.210, Speaker A: Two, three. This probably isn't the last time we'll be on a panel together. We should learn how to harmonize something like one, two, three. Okay, so we are going to begin our chat today, my name is seams and I will be the moderator for this conversation. Today we'll be talking about decentralization, how it is described in theory and how it's really applied in practice. I'll be to keep it more grounded using the dimensions of decentralization described by Vitalik, the dimensions including architectural, logical and the political aspects. And we will then explore how it works in practice and how these theories manifest into the real world.
01:15:31.210 - 01:15:58.914, Speaker A: So before we begin our discussion, I would like our panel to introduce themselves. So I would go from my right, starting with Jose. Hey everyone, I'm Jose, head of marketing at Medis and I'm very excited to be here. Hey, my name is Chris. I'm from l to bit. If you don't know l to bit, we are the ones who are like watching over all the l two s. What exactly are they doing and how safe and secure they really are.
01:15:58.914 - 01:16:46.994, Speaker A: And I hope to share some insights from that in this panel. Hi, I'm disruption Joe on the twitters and I am very interested in grant frameworks and doing a lot of that work with arbitrum and thrive protocol. Awesome. And so, just that we understand your kind of experience and the topic of decentralization, how does decentralization play into the work that you currently do? Well, into the work that, into the work that medicine is doing right now. It's going to have a huge role. We are soon to decentralize or attempt to decentralize the sequencer, which hasn't done previously by other layer twos. We have been building up and exploring this for a long time and preparing ourselves for this.
01:16:46.994 - 01:17:45.506, Speaker A: So plays a huge role in our day to day. It's one of the goals that we are looking for, and it's just going to be the first step into achieving full decentralization. So for us, decentralization is something that we are looking for in the projects that we are overseeing. So why do we even care about decentralization? We care because decentralization is essential for the security of the user's funds. So if you go to l two beat, you can check actually how safe and how secure all those different l two systems are. And also you can see some aspects of decentralization included there. For example, what can you do if the sequencer is down? What can you do if there is no data? Like is the data available, how data availability is addressed in any given project? So stuff like that, these are the metrics, our risk framework.
01:17:45.506 - 01:18:57.600, Speaker A: Basically, we don't say decentralization per se. We look into the details we dig into the details, but that's exactly what we are looking for when assessing projects. Yeah. In my work with Gitcoin for a few years, we were looking at how do we decentralize the review? When somebody makes a decision around eligibility to participate in a grant program and or the review of did they hit a milestone? Did they do the work? And we talk about decentralization a lot in the decision making, and a lot of times we don't think about the accountability processes that accompany it so directly. I'm looking for frameworks that we can apply decentralization to the accountability side of the equation. So just adding on to the accountability in terms of decentralization, sometimes it can be seen as like an exit to community. So a lack of accountability really, in terms of projects or in terms of dows, where do you see decentralization aligning with a sense of accountability on the social or human layer? I could jump in on that one.
01:18:57.600 - 01:19:51.614, Speaker A: What we're trying to do in decentralization isn't so new. We say decentralization like it's this new word, but you might remember a thing from politics called liberalism. And we were like, Marna Keys, one person making the decision isn't always the best way to do it. Right, and we want more people to be able to do this. There's a buzword going around of, like, pluralism. All we're saying is that there should be more than one person who has agency to affect the system. And in the human layer, I think there's a fundamental belief that there's a lot of potential in the world, that the gatekeep systems that we have, like hiring systems that just close out anybody without a degree or close out whatever it is, those exclusionary frameworks don't allow a lot of people to get in and take shots at being the best person they can be.
01:19:51.614 - 01:21:00.114, Speaker A: And whether it's like third world countries or some kid on the south side of Chicago who isn't being given an opportunity, if they can access the opportunity without having to go through all those gates that we built just to save ourselves time, we don't have the time to go out and reach out to those people and help them. If they can just show up and they can get recognized for the value they're able to create, that's going to create the human condition where I think we can all thrive, we can flourish. Absolutely. I agree with that. Although my attempt into social decentralization is in an oversight, I think that while decision making could be centralized and could be more efficient, if it is centralized, then the oversight is best, if it's decentralized. And if many people are looking into what other people are doing and going to the technical perspective as well, that's something what we are trying to do in Altobeat, because decentralization is hard. This is something that we tend to undervalue how hard it really is.
01:21:00.114 - 01:22:19.694, Speaker A: This is one of the most challenging aspects of it, real decentralization and distributed systems. So it is hard, and it's hard on the theoretical level, but it's as well hard in the practical level of how to actually implement those ideas. And the topic of our talk is decentralization, how to talk about it versus how to do it. And that's exactly what we are assessing in Altobeat, because we never, for example, rely on projects documentation. We don't trust those projects, that their documentation is accurate and it's honest, for example. So we look into the code, because the code is the final judge in how decentralized are those systems. So that's the reason why for every claim that we make on our website, whether something is like, whether there is an exit window for a project, or whether there is some mechanism that allows for everybody to propose blocks from l two, every claim as such, we try to back with the link to the source code of the mechanism that we looked into to be sure that it is exactly how it works.
01:22:19.694 - 01:23:18.014, Speaker A: Because the issue is that quite often even the teams themselves, the people in the team themselves, they don't know what exactly has been implemented. Polygon had this issue for quite a long time when marketing department said something and then the technical team had to explain the details on the Twitter because they didn't get it correctly or they were thinking that they implemented plasma. But if you look into the code, then the challenge mechanism is actually empty with a comment TBD. So this is the code that is deployed on chain. Everybody can look into that. So I think that this accountability, especially in decentralized systems that we have relies on us actually checking the claims of the projects, not just l to b, but everybody else. So I encourage everybody here listening to me to fact check us as well.
01:23:18.014 - 01:24:14.498, Speaker A: If we claim that some system is solid, that it's green on our risk assessment, go and check it. And if you find out that we are wrong, we would be happy to give you appraisal and shout out that you actually corrected us. To top it off, I would just say that piggyback from what Christophe said, the collective or decentralized overview makes a lot of sense. And this is something that blockchain itself is enabling for everyone transparency in this aspect. And if we take it to day to day concepts. For example, I'm personally from Latin America, and Latin America, a lot of countries and governments manage state funds in very shady ways. And there will be contracts or payments exercised that no one can be aware of, and not because we don't have the right to be aware of, but because they make it so that the public and the citizens can be aware of.
01:24:14.498 - 01:25:30.602, Speaker A: So I believe the idea or what blockchain is creating can contribute a lot to the idea that you described about that collective overview. And I think it's a very straightforward way to implement it at a global scale, if I can relate to that, because, yes, blockchain allows for it. The transparency that blockchain gives allows us to actually fact check whether all the procedures go smoothly, whether all the mechanism works. But it only works assuming that somebody actually looks into it. Something that we were discussing with Joe just before this panel, that in the daos, we quite usually have this clobberg mechanism that we provide some funds, for example, and we allow the DAO a clobber mechanism that if anything goes wrong, if those funds are being misused, then the DAO has the right to cloback those funds and stop the funding. But this works only if somebody is looking into that, if somebody checks, that's why, and I'm not just shilling out to beat right now, but that's right, we should make sure that we do have those checks and balances mechanism actually working, because without that, everything else is just our wishful thinking. Great.
01:25:30.602 - 01:26:50.658, Speaker A: So moving on now to the political, which is the idea that decentralized systems should not be controlled by a single entity or a small group of entities. So as both of you are really engaged in terms of governance and delegates yourselves, we're starting to also see a growth in terms of apathy, but also the same players kind of playing within governance. So if DaO governance is to be different than our current governance systems, how can it foster greater transparency, accountability and participatory decision making? It goes right to what you were saying. You got to pay somebody probably, to do that review. What bitcoin learned was that decentralization didn't come from the goodness of people's hearts. It came from a system where they took a little bit of inflation to pay for the people to run enough nodes that they'd hit minimum viable decentralization. So what we're thinking about with thrive protocol is that how do we enable that same system where you're paying for minimum viable decentralization of the people who participate in reviewing things consistently so we can move away from delegated authority to either individuals, companies, councils, and actually have a random selection of people in the community that's minimally viable, decentralized that are being paid to review it.
01:26:50.658 - 01:27:33.790, Speaker A: Because I agree with you, if nobody's getting paid, nobody's actually looking at that every day. So what's the schedule that we have some people on that they're mobilized to review that in a decentralized way? I think the name of the game is proper incentive alignment. That's the name of the game every time. Yes, but also I believe that this is a cultural thing, so decentralization does not guarantee anything. It's a possibility that we need to learn how to use. And coming to politics, I'm from Poland, and in Poland we've been given democracy just like 30 years ago. So we've been in a communist state and now we are in a democracy.
01:27:33.790 - 01:28:26.466, Speaker A: But even 30 years later, I'm old enough to be seeing this conversion from the front seat. And I would say that we have to learn how to use this mechanism. The mechanism itself does not guarantee that you will have all the benefits of democracy unless you learn how to use it, unless you take the responsibility of actually using your rights to vote, to fact check your decision makers and to keep them accountable. And we are learning this after 30 years in Poland. And I think we need to learn this also in decentralized systems, because with this power of decentralization, with this voting power comes responsibility. Unless we learn how to use it, it's meaningless. Yeah.
01:28:26.466 - 01:29:26.386, Speaker A: So I think bringing on to that though, is that in the real world, a lot of people don't have this education on how they could actually be better citizens. We have a social contract, but we understand that those with money can have much greater influence. And it's also seen in the Dow world with token weighted voting. So in terms of using onboarding as an example, what is a pathway for onboarding? For people to actually receive the agency to then understand that they can acquire the resources and tools to be better Dow citizens, to participate in governance so that we don't end up in this current stage of apathetic individuals. So I'm just going to make a quick remark. I think going back to the same point, it's incentive alignment. If we create a mechanism in which all the actors that should be playing a vital role for the system to be working are being incentivized in a proper way, then the system will work correctly.
01:29:26.386 - 01:30:21.078, Speaker A: And will also bring more people from the outside. One thing that, or a way in which we're looking at sequencer decentralization is also, we never think about decentralization being a factor for fueling adoption of blockchain or layer twos. But by decentralizing sequencer, we're also creating this incentive alignment in which users or token holders can get part of these incentives. Builders can also access these incentives through different initiatives of the ecosystem and can be rewarded accordingly. And node operators are also being rewarded accordingly. So node operators, key for security, we need the builders to keep the momentum going and to have the projects alive to create a healthy ecosystem and users. By creating these incentives for users, we could actually see a bigger inflow that we've seen in the past.
01:30:21.078 - 01:31:13.378, Speaker A: All of these inflows of users that we've seen have been because of monetary reasons, even games, they're mostly play to earn. And this has brought a lot of users. So I think that incentive alignment, even tying it back to decentralization, can actually fuel a very big influx of new participants in the market. I call it the big O in the dows, right? We talk about the decentralization in the technical aspect and autonomy is your ability to have agency. So that's really the political aspect. And what we lose is that we actually want centralization in the O, the organization, right? And dows, what they've done so far is maybe it comes bear market and they're running out funds and they start thinking, oh shit, we need to be organized because we haven't figured that out. You need to be organized to onboard somebody and say, here's where you go to onboard.
01:31:13.378 - 01:32:15.500, Speaker A: And what we hear from almost everybody, delegates, builders, service providers, every Persona is that they don't know where to go to understand what's going on in the dow to keep context, right? They go to the forum and they just need a nap and they know less. So that's not a good answer. And then they don't know what their civic duty is. So if they knew every week, what are the votes they should do? And here's five. Like you were saying, if there's transparency only gives us transparency, it doesn't give us accountability, right? So if you ask every citizen every day, here's the ten transparent documents I need to review you to review one of them a week, that's your duty, right? And I'm going to reward you for that. And if you get streaks of a few months of always doing that, you're going to get a special hat or whatever it is. And people are going to know that you do your duty to that ecosystem and you're going to want to level up and you're going to want to flex how good you are for that ecosystem, because that ecosystem is good for you.
01:32:15.500 - 01:33:02.646, Speaker A: Yeah, I absolutely agree with what you said. What I would like to add to it that those incentives that I absolutely agree that it's all about incentivization and providing resources so that people can actually do something. But I would like to know that financial incentives are not the only incentives to have. Being able to have an impact, being able to do some meaningful work is also an autonomy. What you mentioned is also an incentive in itself. We need to make sure that we provide people the possibility to do an impactful contribution to the ecosystem. That's when the kid from the suburbs of Chicago or from the suburbs in some small city in Poland can actually get involved in those systems.
01:33:02.646 - 01:34:24.440, Speaker A: This is the beauty of permissionless systems, that everybody that can contribute something meaningful is able to, but we need to provide them a path for a meaningful contribution. And of course, this has to be linked with some financial incentives as well. But the most important thing is that after a month of contributing to this system, this person can be proud of themselves and can tell their friends, hey, this is what I did, this is me, this is what I contributed to this system. There's something super intrinsically meaningful there too. People think about the gamification, and a lot of times the rituals in gamification aren't about what the rituals say they're about, right? So when you level up and you can show your neighbors, hey, I'm a good citizen in this ecosystem, that is something intrinsically more meaningful than the payment that you got, right? If you actually want to show that off, and you take that badge and you put it on your coat that you wear every day, you want to show that off, right? You want to find those things that are intrinsically more meaningful than the actual price of the asset. There is an absolute reputational value in all those ecosystems, and being able to build your reputational value is something that our systems provide. That's one of the most important things that we have in this web3.
01:34:24.440 - 01:34:55.226, Speaker A: And I fully agree with that. And one of the things that excites me the most about web3 attacking this specific problem is the lack of portability and composability that there is with reputation in the current world. If I'm an Uber driver, an Amazon seller, and I've been working for two years, I have five stars and I get in a fight with the organization. My platform is lost. All of my history is lost in web3. This doesn't have to be the case. And so just really quickly, just to add on to that, is that there needs to be a culture shift.
01:34:55.226 - 01:35:55.774, Speaker A: So when I'm talking to people about crypto, they're still always thinking about the monetary gain. So what do you think? Could we shift in terms of the narrative of onboarding people into crypto outside of the money kind of narrative, you say bring impact and value. Impact and value doesn't pay the bills. Majority of the gigs in crypto are not ones in regards to impact or value. So if that is to be a secondary or another type of incentive to keep people on onboard them and to the continual journey in crypto, what is a culture shift then, or a narrative shift then that we should apply in order for that to be true? I'm not sure if we need a shift like the people I'm talking to, they do care about this non monetary incentives out there. And I'm coming from the free software and open culture ecosystem. And 20 years ago, we didn't have crypto.
01:35:55.774 - 01:36:36.458, Speaker A: We couldn't make money on creating free software. Like money is actually something that happened by accident, if ever, in creative commons, in free software, in Linux, in FreeBSD, and stuff like that. And still we created all those huge ecosystems. Why? Because we had an impact. Because we were running the Internet. Because if I contributed to the source code of some platform, I knew that I could later tell my friends, hey, all those systems, all those servers are running my code, and that is my input to the society. And this is hugely valuable.
01:36:36.458 - 01:37:24.026, Speaker A: This is something that you can't pay in money for. This is the shift that I would be looking for, not just the monetary value, but the value of the brick that you are putting into the future society. That's a great point. And I want to kind of highlight that there will never be such a thing as no monetary value involved. I think there's always going to be at some extent. But by being able to develop a solid reputation, like you were saying, being proud of what you do, you are not looking to extract that monetary value per se, but those achievements and that reputation can take you to obtain that monetary value in the end. Yeah, I'm really getting excited about this idea of minimizing friction.
01:37:24.026 - 01:37:57.470, Speaker A: Actually, it was something you said that kind of inspired me around. It was like, why don't we have 100% of people being accepted for grants? And at first I was like, well, that's stupid. How would we accept 100% of people? And then I sat there for like a day. Usually I do with comments like that. It was like, oh, if we had basic requirements and a starting level and some pathway to something at the end. I was talking about it yesterday with the example of a leprechaun. Everybody understands the idea of the leprechaun.
01:37:57.470 - 01:38:26.254, Speaker A: He wants the pot of gold at the end, and there's a rainbow. It's the pathway he takes to get there. Usually our grants and everything that we do, it's like this very transactional thing where we're going, here's some money, don't fuck it up. And we're not saying, here's some money. Let us help you grow and let us coach you and build you to be better. And the pot of gold is whatever's meaningful to that person. Right? It's not necessarily the money, it's the autonomy.
01:38:26.254 - 01:39:04.574, Speaker A: It's the ability to travel and live the way you want to. It's the ability for that kid to go and participate without being gated out of the system for reasons that are just like, reversion to the mean. Like, I'm going to take this stupid criteria and cut out 80 of the applications because I need to cut out 80 of them because I don't have the human labor hours to look at them. So I'm going to require a degree, not because I think the best person is in there, because I don't have the time to look through this shit. Right. So we have all these gating mechanisms. That's the culture change we need to change is that you can step in and get the value that you put in.
01:39:04.574 - 01:50:27.950, Speaker A: And we are the only ecosystem where some monkey or penguin could be more influential than some celebrity or a blueberry. So our time is up. Thank you so much to our panel for this lively discussion. Thank you all to have joined to listen in. And that's it. Check, check. Test, test.
01:50:27.950 - 01:51:08.370, Speaker A: Can we stand over there? And you over here? Hang over. All right. Good morning. Good afternoon, everyone. We are here to introduce streaming quadratic funding to the world. My name is Graven. I am one of the co founders at the Geoweb.
01:51:08.370 - 01:51:40.746, Speaker A: With me here is Vijay from superfluid. Yep. Head of product of superfluid. And want to give a shout out to our friends and collaborators at Gitcoin, aloe and Gitcoin passport. They're not up on stage, but they were integral to making this project come to life. So before we get into streaming quadratic funding, we're just going to do a quick refresher on what quadratic funding is. If you're not familiar, it is a mechanism that is used to democratically and effectively allocate a matching pool.
01:51:40.746 - 01:52:55.554, Speaker A: And so in quadratic funding, your dollars are votes. If you care about something a lot, you can donate a lot of money to it. But instead of just having a direct linear relationship between the amount of funding that you donate and your power of the matching pool, quadratic funding introduces a really cool concept in which is that how many people are supporting that grant with you, that's a super important signal for how much of the matching pool you should get. And so it's a dynamic matching mechanism. And so in this simple example we have up here, the same amount of dollars are being donated to a grant, but the grant that is getting smaller donations, but with ten x, the number of people supporting it are going to end up with ten x, the number of votes or matching. And so why would we want to do that? It seems took me a while to even explain what was going on there. Why do we go through all that trouble? Well, quadratic funding was introduced by Vitalik Buterin, Zoe Hitzig and Glenn Weil, and they did a bunch of fancy math and basically proved that this is a near optimal way to fund public goods.
01:52:55.554 - 01:54:04.560, Speaker A: And we won't get into the math of how that works, but we can actually just rely on our intuition on why this mechanism works. Gitcoin grants, if you're familiar, if you've been around web3 for a while, they use quadratic funding for not all of their grants programs, but their main grant program, and they've raised over $56 million for public goods with that for both crypto and non crypto public goods. And if you've been on gitcoin or not, you can visit a grantee page and you can end up seeing, if I donate one dollars, I'm going to get 100 x or more in matching, if it's a popular grant. And when you see that it's like a superpower, it draws you in and you're going to donate, right? You feel that pull. And you don't just give out the 100 x matching to just any grant, it's the grants that have been supported by the most people. And so it ends up with this feedback loop where you can overcome one of the most tricky things in public goods, and that's the free rider problem. And so all that math, all that thing, when you see a big number, you're going to donate and you end up getting more efficient decision making.
01:54:04.560 - 01:54:42.426, Speaker A: But how quadratic funding rounds have been run to date come with some downsides. If we think about that mechanism, what's actually happening? You have to aggregate all the votes together, do a computation, and then you're going to release the funds. So it happens in these batches, in these rounds, and typically in crypto. So far they're say, two weeks long. And this means that as a donor, you have two weeks to go, log in, sift through potentially hundreds of grants, and make decisions about which ones are worthy of your support. There's lots of them. It's really hard.
01:54:42.426 - 01:55:21.158, Speaker A: It's asking a lot of donors to make that many decisions. On the grantee side, as a public good, this might be your whole budget for the quarter. And so in this two week period, basically you're going to drop everything and you're going to focus on marketing, but so is everyone else. So it creates this dynamic where everyone's competing for attention that's inherently limited, and you end up losing track of what you're actually building. You lose momentum. And the other part is just structurally it creates pressure to centralize discovery of these grants. You have limited time.
01:55:21.158 - 01:56:18.150, Speaker A: You're going to log into a single website, it's going to present you the grants, and the order in which you see them could affect who you're going to donate to. Are you really going to make it to grant 500 and give them as much attention as you did the first one? It really becomes a really challenging dynamic. And so this batch processing and this setup just creates a lot of rigidity, and we start to lose some of that theoretical efficiency that quadratic funding offers and what gets people so excited about it. But it doesn't have to be this way. Quadratic funding is a very versatile idea, and so this is where streaming quadratic funding comes in. We're going to do away with that batch processing, and we're going to embrace the full programmable nature of money that we have on Ethereum, and we're going to implement quadratic funding with streams. It's a little blurry up there, but you can see some money flowing on a UI.
01:56:18.150 - 01:57:14.550, Speaker A: We'll get into what is actually happening there. So what is streaming quadratic funding? It is a implementation of quadratic funding that is funded and distributed in real time. Instead of collecting all those votes, batching them together and sending out big piles of money after some period, we're going to shift the model to having a matching pool that is streamed out every single second. And the allocation of that matching stream is going to be determined by who is voting at this exact second. And so those votes can change over time and adjust, and so will the matching streams. Effectively, what this ends up creating is a series of micro quadratic funding rounds run each second, funded and distributed each second. And it really changes the whole dynamic of quadratic funding rounds.
01:57:14.550 - 01:57:50.434, Speaker A: It's not just faster, it is faster. But the continuous nature just really changes the dynamics across the board and it becomes something totally different. And so there's a whole bunch of benefits. We'll cover a few here. The first one is really about embracing informational advantages and changes over time. And so with streaming quadratic funding, with a quadratic funding round, every single second new information comes in. You can adjust and make a better decision.
01:57:50.434 - 01:58:42.866, Speaker A: You can add grantees, you can remove grantees, you can increase the matching pool, you can change your mind about what you are supporting. You don't have to wait until next quarter or next year or next half year to change your decision about what public goods you want to support. Up next is decentralization. I talked about that pressure to consolidate all the activities that are required for a successful quadratic funding round. Because streaming quadratic funding can be ongoing, it can be open ended, or at least way longer than two weeks, we can start to push the discovery and the decision making to the edges. Instead of a centralized UI, we could have something like this right next to the follow button on GitHub or Twitter or wherever. There could be a button to donate to your favorite public good through a streaming quadratic funding round.
01:58:42.866 - 01:59:33.190, Speaker A: But instead of just a simple donate buy me coffee button, you're still going to get that extra jolt of support through the matching pool. Next, as we relieve this pressure on decision making, we're going to end up with higher conviction support systems between donors and grantees. It's like a supercharged patreon. You're going to open your streams, they can be open ended. And the builders are going to be able to focus on delivering their vision rather than marketing and worrying about where their next paycheck is going to come from. They'll have instant access and be able to just focus on making the maximum impact. And last but not least, what I'm most excited about and what we see a lot in web3 is composability.
01:59:33.190 - 02:00:12.222, Speaker A: Money is information, and streaming quadratic funding processes that money and information in real time. So it makes a great thing to plug into other mechanisms. You can even start stacking streaming quadratic funding rounds to embrace localized decision making and make it more a bite sized problem in your allocation challenges. So how does this thing actually work? What did we actually build here? We'll just go through a quick little overview. We'll start with. Just say this round is already running. There's a matching pool.
02:00:12.222 - 02:01:24.090, Speaker A: We are streaming it to three grantees here through a tool that superfluid built that VJ is going to get to in a little bit here called a streaming distribution pool. A new donor comes, they decide they want to support the developer. Up top here, they're going to call what's referred to as a strategy and allo protocol. Allo Protocol is a new tool that Gitcoin developed to help do exactly this, allow people to come up with new and interesting ways to allocate funds towards the things that they care about. And so we built a streaming quadratic funding strategy, you're going to call it, your money is going to go through to the donor, to the Grantee, just like you would expect, but it's going to do a secondary thing and it's going to check to see your on chain Gitcoin passport score. It's basically just a trust score. Are you a Sybil? Are you trying to subvert the mechanism? And if you have a high enough score, we're going to go ahead and update the allocation with that quadratic matching formula in that streaming distribution pool that superfluid has built.
02:01:24.090 - 02:02:04.642, Speaker A: So this is all in one transaction. And what happens is magically, all three of the grantees in this round are going to have updated matching streams. It's the coolest thing I've seen in blockchain right now. It feels like magic. And I'm just going to pass it over to Vijay here for a little bit to talk more about what these streaming distribution pools are. Thanks, Graven. So you might be asking yourself, how is it possible that we can achieve this UX benefit of basically the UX of donations, monthly subscriptions, yet all of the power of quadratic funding in the back end? And the answer is soupfluid distribution pools.
02:02:04.642 - 02:02:58.614, Speaker A: Soupfluid has been built in a modular way where we've been able to add functionality over time. This is the latest and greatest piece of our puzzle, something we've been working on for a while. But you can think of it fundamentally as a scalable way of doing index based streaming payments. And streaming really here you can think of as recurring, if there is not already an idea of that in your mind. The most powerful part of this is that you can update all of the streams that are coming out of one pool with one single transaction, which basically means that it is hugely scalable to as many inputs and outputs as you want. All of the outgoing streams will update from a single transaction, and there are a number of other benefits you can tap into besides that. So actually under the hood, all of the units, the sort of individual percentages of allocation are actually transferable.
02:02:58.614 - 02:03:48.426, Speaker A: ERC 20 units. So if you're a builder, you can imagine some of the things that that would give you in terms of capabilities and optionally you can make it, and this is something we take use of here, is that you can make it that any sender can actually pay into a superfluid distribution pool and have the proceeds split equally to all senders and update by the same mechanism. So this is actually live and available today. If you're a builder interested in sort of finding out more about how you can use this, please go and check out our docs. There's a QR code below if you can scan it, but otherwise you should find distribution pools there, really encourage you to go and check it out. It is hopefully a kind of, well, of creativity. And I think what Graven was alluding to earlier is also really important.
02:03:48.426 - 02:04:50.190, Speaker A: This isn't just an isolated thing. It isn't that streaming quadratic funding sits by itself in this kind of ecosystem of kind of composable money as one kind of app that you can use end to end. This actually fits into the bigger superfluid model of basically composability of money. And the idea here is that actually almost any type of kind of on chain recurring payment can be modeled as a superfluid stream. And when you do so, then you can essentially chain them together and you can kind of have salaries vesting rewards, paying outer contributors, and those contributors paying into various types of superfluid enabled apps, like the kind of matching pool of the kind of stream of quadratic funding, or even things like DCA investments or subscriptions for content. And these revenue streams loop back around to the dows and web. Three businesses that provide them sort of create this flowing economy where the velocity of money is almost infinity, which is incredibly powerful.
02:04:50.190 - 02:06:07.078, Speaker A: And we have a number of other sort of benefits besides. We have basically the concept of hooks at the superfluid protocol level, which basically enable these kind of apps like streaming quadratic funding or streaming exchanges, and a lot of other stuff that I'm excited to announce and talk about very soon. But in this model, you can imagine in a very practical sense, optimism guarantees for RPGF three today are receiving their streams of rewards in op tokens, and they could, for example, stream those rewards directly to the contributors of the projects, and those contributors could stream them into a matching pool, and you could basically have this full circle of streams composing infinite number of levels deep in real time, which is something that we think is incredibly powerful. And we're actually going to see some live examples of this popping up over the coming weeks and months, which I'm excited to share more about. But if you want to learn anything more about superfluid and building on superfluid, particularly if you're hacking this week, feel free to reach out to me or check out our docs. They really explain all of this in more detail. Okay.
02:06:07.078 - 02:06:55.806, Speaker A: And so why I'm up here, I work on a project called the Geoweb. I mentioned that earlier. But we are the first users of streaming quadratic funding. We launched our pilot just last week, on Wednesday, the 21st, I believe, and couldn't be more excited to share it and see what happens with this experiment. Need to give a shout out to public nouns, one of the forks of Nounsdao, but focused on supporting public goods experiments like this. They gave us five eth for the matching pool, but also this matching pool is really the genesis of why we wanted to build this is the Geoweb is using superfluid on the revenue generation side of our project. I gave a talk last year about harburger taxes and partial common ownership.
02:06:55.806 - 02:07:38.290, Speaker A: We use that to manage a digital land market, and it produces revenue, and it's been producing revenue for about a year. But until streaming quadratic funding, we would get those funds, they would pool up, and then we'd, like, release them in batch, and that just didn't feel right. We always wanted to keep pushing towards this idea of composable mechanisms and getting this funding out into the world so it can make an impact. We're a small public goods project. The only way we're going to be able to compete is by being more nimble and making better decisions. So that's what streaming quadratic funding does for us. It's basically our governance protocol now on the geoweb.
02:07:38.290 - 02:08:19.226, Speaker A: So you can log on to geoweb land governance and help us make these decisions about where the funding goes. We're going to run this experiment for about two months. It could have been open ended, but this is the first one we did a small, trusted group of grantees, just six, to see how it goes. But we are very excited about where to go next. It's not all rainbows and unicorns. There are challenges, and a lot of them are similar to what you see in traditional quadratic funding rounds. Cybl attacks and collusion can really undermine the mechanism.
02:08:19.226 - 02:09:19.090, Speaker A: Instead of donating $10 from one wallet, wouldn't it be better if I just split that across ten wallets? Same thing happens with streaming quadratic funding, but there are a few differences. The payout function on streaming quadratic funding is going to be incremental rather than these big batches. So that kind of changes the game a little bit. In the civil attack space, we also have the challenge of these donation streams are potentially open ended. We can't just have one check of your civil score or what your trust score is up front, because things will keep evolving. And then finally, vijay entered the near mentioned the near infinite power of streams and the capital efficiency. An attacker could open up a donation stream, quote unquote, to a grantee and loop it back to themselves and really inflate what they're actually donating and keep like a net actual outflow of zero.
02:09:19.090 - 02:10:01.678, Speaker A: They'd have to do some hops in between. But these are really interesting problems if you are into this sort of thing. This is just constant red team, blue team sort of thing. Gitcoin has been leading the way on that front for a long time, but streaming quadratic funding is going to put out some new challenges to nerd, snipe, you, and hopefully you can join us on the offensive side. It's not all defense. We do have some exciting things on the roadmap that we're looking forward to next with streaming quadratic funding. We started with streaming quadratic funding, but there is another version of this that is pure votes based, and it might actually be less complex in a lot of ways.
02:10:01.678 - 02:10:42.350, Speaker A: You don't have to deal with those loops. We'd keep the same streaming payout mechanism, but the mechanism for voting would be like a commitment that isn't like an active flow of money, and so you wouldn't have those same loops. Obviously, we're looking forward to scaling this to more donations, more grantees, see how all those things go. And then, like everything else in crypto, yeah, Ux has a ways to go. We're at the bleeding edge of this tech. We didn't implement account abstraction or batching transactions. We haven't gotten to those decentralized donation mechanisms that I talked about earlier.
02:10:42.350 - 02:11:20.726, Speaker A: We did build a centralized UI for now. But yeah, lots of exciting stuff there. And yeah, the water is warm, so hopefully come and join us. And so we'll wrap up with a call to action for this. This big QR code is that link that I gave earlier Geoweb, landgovernance we need you to participate in this. We need to see what happens as we stress test this, more donations, more data. We got to figure out what happens in the pilot.
02:11:20.726 - 02:11:56.680, Speaker A: Also, I legitimately think streaming quadratic funding is the coolest thing you can do on a blockchain right now. Like full stop, but not that many people know about it. Not that many people are here to learn about it. So we need you to go out and tell people about this and participate in it. This is only going to work if the meme replicates and helps get this out into the world. We've gotten great feedback initially as we did our Twitter launch this week. A lot of people reaching out to ask to run their own rounds, hopefully to help us build.
02:11:56.680 - 02:12:31.742, Speaker A: We are definitely open for that. This is all open source. We want to see this across the industry. So definitely, I guess first go, donate, interact, learn, help us get some more eyeballs on this and then hit us up. And we would love to build with you and see where streaming quadratic funding can go. I guess we have a few minutes for Q A if we want. I don't know, there's no mics out there or might be kind of awkward, but if anyone wants to shoot up a hand has questions, we'll do that.
02:12:31.742 - 02:30:07.100, Speaker A: If not. All right, y'all, thank you for being here. Good luck with the rest of Biddle week and hope to see you on the Internet. Thanks everyone. Don't forget to go donate. Yo, how's everyone doing? You all right? Great to meet everyone. I am Liam.
02:30:07.100 - 02:31:33.640, Speaker A: I'm the head of growth here at Fairblock, and I'm going to be talking today about encryption tools that can be used for bad mev prevention in Defi. Just some things to preface first, maybe I just found out about this talk yesterday. The founder who was supposed to give this talk, his flight got canceled very abruptly due to some septic issues on the plane. Unfortunately, it's a pretty disturbing way to get your flight canceled, but nonetheless, we are here and I'm really excited to be sharing what we've been working on with you guys. So the second bit of preface to say is, while bad MeV prevention is one of the benefits of the solutions that Fairblock offers, it is only a small subset of what we do. We're working on cryptographic solutions to programmable privacy, and the bad mev prevention side of this is, like I said, just a small bit of the benefits that come from the tool set that we offer. The main point of this presentation is the extent of what's now possible on chain with programmable privacy.
02:31:33.640 - 02:32:42.844, Speaker A: And when we talk about programmable privacy, we're not talking about just zero knowledge proofs. We're talking about fhe, we're talking about a number of different cryptographic schemes that can be brought on chain with now what's available today with WASM and other rust based cryptography libraries. So, yeah, with that preface, we are not a competitor to flashbots. We are very much working in alignment with what they're doing, but more on the cryptographic side of bad MeV prevention. So we respect them, we love them, they do great work. But we also understand, too, that the design space for bad MeV prevention is much larger, and what they've worked on so far is just kind of a temporary solution with their centralized server. So, with that as the preface, let's get into it a bit of a itinerary.
02:32:42.844 - 02:33:55.430, Speaker A: First, we're going to be starting off with just reviewing the state of MeV, going into what is fair block, an introduction to our architecture, threshold Ibe, applications of Ibe in DeFi, such as encrypted intents, sealed bid auctions, and more generalized programmable privacy. Some cool things that can be built from this, and what to expect from us in the next year. So, let's get into it. So, the current state of MeV so MeV, as we all know, is the value that can be extracted by privileged actors in any system. This is kind of the famous Hasu quote. Some traditional forms of MeV include arbitrage, sandwich attacks, liquidations, just in time attacks. Some newer forms of MeV include cross chain arbitrage, sex to Dex arbitrage, and opportunities at the bridge and oracle levels as well.
02:33:55.430 - 02:35:04.488, Speaker A: And then, in terms of the mitigation techniques that are now available, we have the encryption side of things, which is what we're working on at Fairblock. To also preface, too, we're not working on solutions that support good MeV. The cryptography tools that we use are really just to prevent some of the opportunities for Bad MeV. There's flashbots with MeV Boost and Mev share, and then there's also mitigation techniques at the app design layer, such as frequent batch auctions and Protorev. So with that, that leads us to Fairblock. What is Fairblock? We are a modular ecosystem of privacy enabled infrastructure and applications. So we deliver programmable privacy to protocols and their applications to protect users from malicious actors and the downsides of onchain transparency.
02:35:04.488 - 02:36:54.880, Speaker A: And just to kind of give a bit of color, first on why we're building Fairblock we think that a lack of privacy on chain is one of the bigger deterrence to blockchain adoption, and that's a major reason for why we're not maybe where we want to be in terms of the whole blockchain adoption scheme right now, because most on chain applications are limited in their designs to a global user base because of a lack of privacy. And up until now, most on chain privacy has been materialized into isolated l ones that position privacy more or less as a luxury instead of as a standard. Meaning that if we were to make privacy a standard, we should be delivering it to protocols and their applications so that this privacy can be accessible inside of users favorite ecosystems and applications without having to sacrifice UX. So this is how, in my opinion, we create the foundation to build stickier apps that attract the next billion users. So with that, I'd like to go over fairy ring at first. Ferry Ring is the foundation of all things that we do at Fairblock. It is essentially a key generation chain that acts as a decryption key oracle that sends encryption and decryption keys to any blockchain or any application across any ecosystem.
02:36:54.880 - 02:38:19.528, Speaker A: This can be done via IBC or a network of relayers that we use called ferryport. And like I mentioned or sorry, as we are connected to IBC, we are also a cosmos SDK chain as well. So just to kind of go over a bit of the architecture to lay this out and how this works, it starts with users encrypting a transaction with a certain on chain condition. This condition could be a certain block height, an asset price, a zero knowledge proof really whatever data point on chain that you want to program your transaction to be decrypted at, this is possible with IBE. And so once the condition for decryption is selected, the encrypted transaction is then committed on chain where it sits in the mempool waiting for those conditions to be met, to then be decrypted and executed. Once that condition is met, the ferryring validators are notified via ferryport and work together to construct the private key. They do this by deriving their share of the private key first, which is derived locally.
02:38:19.528 - 02:39:24.800, Speaker A: They will then submit it to ferryring. Ferryring once a certain threshold of validators private key shares are met, ferryring then performs a function to construct the derived private key, which is then sent to the destination chain, where the transaction is then decrypted and executed. So, as you guessed it, if you're in tune with the cryptography world, this process is what we call threshold Ibe. Ibe, like I said earlier, stands for identity based encryption. This is what enables us to choose certain conditions on chain that will be used for the decryption of transactions. So in the threshold Ibe that ferry ring runs, we generate a master secret key. We call this the MSK, from an API.
02:39:24.800 - 02:41:19.056, Speaker A: This API, just as a preface to, will soon be converted into a DKG based approach where all the validators in the chain are working together to derive the master secret key in the public keys that are used. And then once that key is generated, it's then split up amongst the validators in the network and stored locally. And then when every condition for decryption on chain is met, the validator will then generate their private key share and then submit it to ferryring for the derived private key to then be constructed and then sent to the destination chain. So there are a number of applications that threshold Ibe unlocks on chain, but I'm going to focus most of the rest of this presentation on the applications in DeFi, because that's where we want to kind of show the value here is that with bringing Tibe to DeFi applications, we do help these apps and their users mitigate some of the opportunities for Bad MeV. And the first one that I'd like to go over is encrypted intents. So these could be limit orders, stop loss orders, really any kind of programmable privacy or programmable, sorry, yeah, programmable trading available on chain. And so you might ask first, why would you want to encrypt transactions? This could be to keep the size of your order hidden.
02:41:19.056 - 02:42:10.420, Speaker A: This could be to retain information asymmetry. This could be to protect any of your trading strategies. The point is that you can really program the decryption of these transactions to meet whatever need you're looking for. So a general flow of what this looks like is a trader will submit an intent. Say the intent, sorry, say the intent is set to decrypt at ether price, 4k, for example. That transaction will then be sent to the app front end. The app front end will then submit that transaction to the chain where it lives in the mem pool, where it'll wait for the conditions to be met on chain.
02:42:10.420 - 02:43:23.988, Speaker A: Once that condition is met on chain, ether suddenly reaches 4k, which we're all patiently waiting for. The relayers of ferryport will then request the decryption key from ferry ring. Ferry ring will then generate the decryption keys, send it to the app chain, and then the intent is decrypted and executed. This design is also really important for sealed bit auctions too. This is something that we've all probably been seeing recently with Paradigm's recent post on leaderless auctions. So the design here would be very similar where the bidder, for example, would submit a bid to decrypt at block height, whatever that block height is generally the end of the auction or the end of the round in the auction. That encrypted bid will then live inside of the chain in the mempool, where it's gathered alongside all of the other encrypted bids.
02:43:23.988 - 02:44:29.420, Speaker A: Once that block height is then reached, ferryport requests the decryption keys from ferryring. Ferryring sends those decryption keys to the chain. The chain then performs a function to decrypt all of the encrypted transactions in the mempool with logic that is then set to select the winning bid. That winning bid is then decrypted, or, sorry, executed. And you might ask what are some advantages of this over traditional auction systems? One of them is that there's no collateral needed to guarantee bids are revealed, like in a typical commit reveal scheme. The other one is that there's no risk of censorship based on bid amount. So each of these things are kind of part of what we considered highly in thinking about how Tibe can be used on chain to secure sealed bid auctions.
02:44:29.420 - 02:45:50.596, Speaker A: And then just maybe to wrap our heads around the more like meta approach here. This conditional or sorry, this programmable privacy can be brought to any application on chain, and the flow inside of these applications will be similar. It's not just useful in DeFi, it could be useful in private governance. Gaming, the list goes on and on. So in terms of maybe the one nuance here that I hadn't mentioned prior is that there is a initial communication between ferryring and the app chain itself, where ferryring sends a master public key to the application chain. That master public key is then used inside of the process where the user chooses their decryption condition. That decryption condition is basically matched with the MPK in order to become kind of like the public key for that transaction, and then kind of the flow goes on from there, as we talked about prior.
02:45:50.596 - 02:46:52.430, Speaker A: So there are a lot of really cool things to build with programmable privacy on chain. One of these things could be private governance, which we've already built for the cosmos hub. Like I've said, this could also be encrypted on chain intents as well. This could be randomness generation for onchain gaming, gambling casinos, whatnot. There's also censorship resistant shared sequencing where decryption is enabled at the sequencer layer. There's encrypted mempools, privacy preserving roll ups, private on chain games, and so much more that even we haven't fully thought about just yet, but are actively researching. And like I've kind of alluded to earlier, the design space for applications on chain with programmable privacy is huge.
02:46:52.430 - 02:48:01.280, Speaker A: This is what we think is the next big on chain integration that will unlock the design space for the next million apps that will attract the next billion users. So, yeah, we are in the business of pushing the boundaries of what we can do here on chain, I think is always important to remember. That's all for now. If you guys want to scan this QR code, it's our link tree that takes you to our GitHub, our Twitter, all of our docs. You can find us throughout the week at basically every event. One that we'd like to get a lot of developers at is our workshop with Dorahacks at app chain day, where we can test out these Mev resistant auctions with some of our cryptography libraries. We've got our testnet launch scheduled for March 6 right after e Denver.
02:48:01.280 - 02:51:26.216, Speaker A: We've got a hackathon with Dora hacks that's going to be announced soon and many exciting partnerships across the infra and l two landscape that will be announced soon, as well as opportunities for builders to contribute early. So that's all I've got today. Thanks, everyone. Thank you for your attention and time. And we're really excited to see what we'll build with programmable privacy on chain. Thank you. The only thing is we should stay together.
02:51:26.216 - 02:53:35.680, Speaker A: Okay, but cameras, are you also capturing the. All right, what's up? How great is fan? Did you get good pictures? All right, awesome. We brought paparazzi in the audience. We got a minute till we start. How's Eve Denver doing? How you guys feeling? Good. Good so far. Awesome.
02:53:35.680 - 02:54:51.954, Speaker A: Yeah, I like that guy. I like that guy in the front left. Awesome. Very excited for you all to be definitely do this. Would find us very funny. About a ten that we're just 10 seconds. All right, you ready? All right.
02:54:51.954 - 02:55:05.990, Speaker A: What is up, ETh Denver 2024. How's everybody feeling today? Incredible. Wow. We did not expect this amazing turnout. Hundreds of people in the audience. My name is Sean Conrad. I do Devrel for Secret Network.
02:55:05.990 - 02:55:41.402, Speaker A: And I'm Alex. I'm also doing Devrel at the Secret Network foundation. Welcome, welcome, everyone. And we're very excited to be presenting today our talk on trustless confidential bridging with secret network. So what is secret network? If you've never heard of secret network before, we're the premier layer one privacy chain of the cosmos. We've been around since the beginning of the cosmos, and we now have an EVM developer toolkit where our tools can be used on the EVM seamlessly. And we're extremely excited to be presenting that technology for anyone who's here today.
02:55:41.402 - 02:56:31.478, Speaker A: You're going to be able to learn how to drop our code into your code with proxy contracts with a single contract address. So now that you know about CQ network, the question is, how do we bring in the privacy tech from SQ network and confidentiality tech into the EVM space and into other chains? The way we do it is something that we call Snake path. Snake Path is a cross chain, trustless bridging solution that leverages secret network's privacy preserving contracts. The idea behind it is we can use this to basically build any DAP that requires confidential computation on the EVM. Hold on, we're not done yet. We're definitely not done yet. With Snakepath.
02:56:31.478 - 02:57:24.714, Speaker A: It allows you to do a broad range of things. It was originally developed by community members, and it was such an amazing solution that it's now being used to do a whole bunch of things in secret to bridge privacy preserving computations into the EVM. Idea behind it is we will have encrypted, and we have encrypted payloads and call data, basically that we can bridge into EVM. So for you, this is the most easy way to use encrypted payloads and call data and to leverage this to build your privacy preserving DAP or just to leverage confidentiality as you want. Yes. There's two main things you can do with Snake path, which is, as Alex just stated, any encrypted payload you can imagine. So you can kind of just think of secret as like a side chain where you can have composable privacy on the EVM or two.
02:57:24.714 - 02:58:00.406, Speaker A: We've developed something called Secret VRF, where you can have on chain verifiable randomness that is faster and cheaper than Chainlink VRF. And we're super excited to do a demo today to show you exactly that. And if anyone here is in the link army, please come for us afterwards, because we want the heat. We're bringing the spice. Exactly. What we wanted to do is to essentially have something that's super easy to use for you. Chainlink VRF has a lot of complicated stuff for SQL, VF, we want to have something that's super easy to use for you.
02:58:00.406 - 02:58:34.026, Speaker A: We are deployed on a lot of testnets. For example on Ethereum, Sipolia, Polygon, Mumbai, optimism, sepolia, a bunch of testnets. We also deployed on a lot of mainets, Ethereum, Mainet, arbitrum, optimism, linear base, you can name it. If you have chains that you want to have support for and it's not supported yet, you can check out a docs to see if your chain is supported yet. If not, please come to us. We're really happy to extend our support for these chains as well. The thing that you need to take a look at is basically one thing is just the gateway address, that's all.
02:58:34.026 - 02:59:01.798, Speaker A: Basically that you need. The gateway address is all that you need to call to leverage the full privacy preserving features of seeker network. Snakepath will do the rest for you. And we have extensive tutorials for you to make the most out of this. Yeah, so if anyone here has worked with Axilar GMP before, you might be familiar with pointing to gateway contracts. Same exact idea. We have these gateway contracts that abstract away all the complexities of working with Snakepath.
02:59:01.798 - 03:00:08.318, Speaker A: All you need to do is point your smart contract to these addresses and you'll have access to these private computations as well as secret VRF. So now the only question now is, since we want to make VRF very simple for you using Snakepath, nice question is, since Snakepath uses a lot of broad bunch of cryptography to pull this feed off, actually what we wanted to do, NCQ network is the following. We want to make everything complicated, just go away. Abstract it away from you so that you as a dev can only concentrate and focus on something that you just drop into your contract less than ten lines, you drop in secret graph and less than ten lines into your code. And we abstracted everything else that's complicated away from you. So this is what we're going to claim. We're claiming essentially that we have secret VRF in less, basically just ten lines of code for you, nothing else that you need to do.
03:00:08.318 - 03:00:38.834, Speaker A: Drop in the code and it will all work. Of course, if we support you on your chain. If not, please come to us afterwards, shoot us an email, shoot us, whatever, or come to us when you see us. Yes. So now for this, Sean, we're going to do a brief demo where what we do, we're just going to show how easy this is to actually work with. So if you've ever worked with Chainlink VRF before, you might have gone through their docs, they have a contract that's like it's already open. And remix.
03:00:38.834 - 03:01:08.398, Speaker A: We've already uploaded their contract. We're going through Chainlink's exact docs to request randomness. You receive two random numbers, and then we're going to do the exact same thing with secret's docs and show what it's like to receive two random numbers and show that you can do it for a fraction of the cost and speed of Chainlink VRF. And what we're so excited about this is secret is a cosmos chain. And one of the big promises of the cosmos is interoperable programmability with these blockchains. Right. But right now, we're at a state where we haven't seen a lot of interoperability.
03:01:08.398 - 03:01:45.944, Speaker A: Everyone's kind of their own chain. Everyone's kind of siloed. We found a way with snake path to bring this not only to ibc, to other chains on the cosmos, but also to ethereum in a way that's extremely accessible and also familiar for EVM devs. You've worked with something like Chainlink before, so you can do this for encrypted payloads, but you can also do this for VRf, which is what we're going to show right now with our on chain verifiable random number generator. So what we have is we have two instances of remix. Open. One is Chainlink VRF's code from their docs that requests two random numbers.
03:01:45.944 - 03:02:01.244, Speaker A: Can you make it a bigger? Yes, make it a little bigger. Whatever the people want. If there's any requests from the audience, please shout them. You can read it now. It should be readable. All right, perfect. So we have two instances of remix.
03:02:01.244 - 03:02:41.710, Speaker A: One is for Chainlink VRF's, their docs, and then we also have one for in secrets docs. And both our resources are at the end of these slides. You can pull this up and you can go right to this documentation if you want to do this on your own and experience how simple this is. So we just pulled this straight from Chainlink stocks. We took their contract, we compiled and uploaded it to Sepolia Testnet, and now we're going to do that with secret VRF as well. So first, let's just request randomness with the Chainlink VRf contract. Let's confirm and let's see how fast this is.
03:02:41.710 - 03:03:15.700, Speaker A: And then while we wait for that, what we can do is go into the secret contract. Do you want to explain this code really quickly, Alex? Absolutely. So the important things that you need to look at here is the following. You can go up. Yes, exactly. So at first, as we claimed, we are claiming to get secret vf in just ten lines of code for you. First, what we need to do is just define an interface.
03:03:15.700 - 03:03:34.040, Speaker A: In this case we're defining interface for you to request randomness. So that's the interface that you basically implement and that's what you're going to call on the gateway contract. You're requesting randomness. That's it. Basically three lines. That's it. Then we're going into our sample contract.
03:03:34.040 - 03:04:06.848, Speaker A: Now the next line comes in, which is just you defining the gateway address. In this case we use a storage variable. Then on the next stage we go a bit lower here, down here to set the gateway address. In that case, what we do is you basically do it as a contract owner. You set the gateway address that's pointing towards the VRF gateway. I can show that right now too. Yes, of course, if you go into secrets docs, we have all these gateway addresses for both Testnet and Mainnet.
03:04:06.848 - 03:05:00.020, Speaker A: And as Alex stated, if you have a chain that you want to work with to request secret randomness and you don't see it listed here, just DM us and we will get it up and running so that you can have it for your chain on the EVM. We're doing this for Sepolia though, just to match and mirror what Chainlink stocks do. So all you have to do to work with this on sepolia with secret VRF is copy the gateway contract address, go back into our instance of remix, and then just set the gateway address for our contract. So I'm just setting the gateway, calling this function and confirming the transaction, and then from there we are able to request randomness. Let's just make sure that our chainlink transaction went through already. Okay, great. So we're going to look at this chainlink randomness request in 1 second here we're going to look at this event, but first let's just request randomness with secret VRF.
03:05:00.020 - 03:05:43.590, Speaker A: So if we go back into our remix instance, let me make sure the gateway transaction went through. Okay, great. So we have this transaction showing that our gateway was set successfully for Sepolia testnet and now we can just request randomness. Of course you need to make sure to pay enough gas for the callback. That's what you just basically send in a little bit of extra gas to pay for the gas on the way back. That's what we define with the callback gas limit. That's everything of this is explained in the docs thoroughly so you can make sure to not miss this.
03:05:43.590 - 03:06:08.056, Speaker A: All right, so we just requested randomness. Let's see if it's gone through. With metamask. It's pending. Right. And now at this point, the amazing thing about this is this works as quickly as the chain on which it's deployed. So we're just waiting for this to go through with sepolia.
03:06:08.056 - 03:06:42.812, Speaker A: But whatever the block time, in this case, we actually didn't pay enough gas. Make sure to pay enough gas, guys. The contract doesn't like it if you don't, so now we're being a bit more generous. By the way, the exact formula on how to calculate the gas fee that you need to pay for the way back is also mentioned in the docs with an extensive tutorial on basically on how to automatically calculate it. It's going to speed this up. Hope it went through. All right.
03:06:42.812 - 03:07:15.972, Speaker A: Perfect. Now, while we're waiting on this, I can actually explain the other part of the code, the last bit that we just did. Can we just switch to the screen again? Yes. So now basically what we just did is execute the last part, which was actually requesting this randomness test. And as you can see, this basically are the last lines that we need to do, which is first, just define our VF contract. Next, we are requesting our randomness. We're just basically calling VF contract.
03:07:15.972 - 03:07:46.990, Speaker A: Make sure to also give over the value so the contract actually gets to have its callback gas. And lastly, we just define how many words we're going to have, how many random numbers you're going to request, and how much the callback gas limit is. And next, for this test, we're just emitting a request. A lock. Exactly. That's it. All right, 1 second here.
03:07:46.990 - 03:08:45.910, Speaker A: Oops. Are these the same instances? Give me 1 second. See, the transaction was so fast, we even lost track of how fast it was to make sure I have the correct contract address. Up. Here we go. All, all right, our core dev is checking on the relayer to make sure this went through. So one of the amazing things about secret VRF with chainlink or with Snakepath is that the relayer is trustless.
03:08:45.910 - 03:09:09.250, Speaker A: Everything that's trusted, that's happening. It's all just happening with the smart contracts. So you can design a relayer to actually put the transaction through. You could do it manually. We just have one relayer spun up right now for this demo. But basically the only way this could fail is if the relayer itself fails to pick up the transaction. But the smart contracts themselves are trusted and verified.
03:09:09.250 - 03:09:37.420, Speaker A: So that means that essentially you don't need to trust the relayer. In the future, we'll have multiple relayers relaying this. So as this demo basically just runs on one relayer, and we're improving relay performance every time. We want to make sure basically that in the future we will have multiple relayers relaying this. But you don't need to trust the relayers. That means you only trust the gateway contract that lives on SQL network. That's the most important thing that you need to know.
03:09:37.420 - 03:10:38.380, Speaker A: All right. Okay, to request. Give us 1 second. Does anyone have any questions on what you've seen so far? Always spin up this relayer. Yes, it goes as fast as the latency of the blockchain on which it's deployed. So what we've been seeing on Sepolia testnet is about, it takes about two blocks for the randomness to be returned. Okay, so for example, if we look at the chain link randomness request, we can see that it took about five blocks from the initial request for randomness to when it was returned, we see it was block 74, it was returned block 79.
03:10:38.380 - 03:11:46.376, Speaker A: And then we can go in here, see the transaction fee and the gas price, and then if we go into secret VRF, we have this request for randomness going through. Yes. Let's see. All right, we're just going to try this one more time, see if it's a relay error. It's going to compile and deploy a fresh instance of the contract, as you can see. And that was one of the most important things for us is essentially this stuff is so fresh out of the printer press for you that we specifically wanted this to show you at Eve Denver so you can try it out in the hackathon. That was one of the most important things to us.
03:11:46.376 - 03:13:33.750, Speaker A: So as you can see, you're basically right at the forefront of what's going on and what we're doing. And that's why we wanted to get this into the hands of you as fast as we can. All right, deploying an instance of the contract, and then we're setting the gateway once more. All right, set Gateway was confirmed. And now let's try requesting randomness. Let me see if it pops up on the relayer. All right, we got the relayer open live cross.
03:13:33.750 - 03:13:58.836, Speaker A: You're connected to the Wifi. Yeah. All right, just going to make sure we pay enough gas. You didn't pay enough back. It was, it was. No, you can't. You can't do it.
03:13:58.836 - 03:14:38.106, Speaker A: Like definitely go down function. Yes, exactly. And how much here? Thousand. Otherwise the transaction. All right, I'm going to try this one more time, but as you can see, this is the most important thing actually. You need to actually also pay enough gas for the callback and you need to define this one. But this is all mentioned in the docs for you.
03:14:38.106 - 03:15:10.322, Speaker A: So we're basically showing you on what not to do right now. And for the demos, we're actually really happy to just help you out because this is so extremely easy to do. And we got more info for you for the EVM toolkit docs for the demo that we have here, and a QR code if you want to do four more for the docs. All right, thank you so much. We've been secret network. Stay tuned for our evm toolkit to learn more how to use this. It's still just being worked on right now as we speak, but it's really incredible and we've seen how it's faster and cheaper than Chainlink, VRF and other VRF solutions on chain currently.
03:15:10.322 - 03:17:16.840, Speaker A: Thank you so much. Had to happen, had to happen. Cool. I it I'm trying to figure out. Thanks, Chris. That works. Point at the computer, don't have to point anything.
03:17:16.840 - 03:20:12.400, Speaker A: And there's your timer when it gets down to zero. Mostly like this. Hello. Okay, you don't need to podium the whole time or you crush it. You got 2 minutes. So hello, my name is Atos and we're from CK Pass. So Zkpass is a private data verification protocol that is built upon the foundations of multiparty computation and zero knowledge proofs, and that provides the apps with the ability to use private data from off chain data sources as a dependency for their code.
03:20:12.400 - 03:21:22.386, Speaker A: So, a little introduction about me my name is Atos, born, lived and studied in Australia. I've done some entrepreneurial stuff back in university, and fresh off my education, I actually started my career at traditional banking sector and then I turned into Tencent and now in Zkpass. So the main purpose of this talk really is to just have an in person explanation of how SDK works as we've posted our bounty for ETH Denver. So this workshop would be less thought provoking compared to other talks here, but it's more straightforward. So what we'll do is to mainly talk about a few parts. First of all, it's what Zkpas does, and then we get into the technical nitty gritties of the entire product flow from a builder's perspective. And then finally, if we have time, I'll try to get into a live demo, but if we don't have time, there are docs that we have and that are still pretty self explanatory.
03:21:22.386 - 03:21:58.206, Speaker A: And we have a telegram channel for builders that are interested to build on us where all the questions can be directly answered. So there's a QR code here. Feel free to scan it to get into the Telegram channel. Yeah, so most of our reputation came from us winning the binance launch pad. So let me give you guys a really quick update of how we're doing. So we closed around last year with a 2.5 million seed round.
03:21:58.206 - 03:22:56.630, Speaker A: We are proudly backed by Sequoia, OKX Finance and a few other prominent vcs in the industry. And we have fairly nice traction because we launched our pre alpha at the end of last year and had some pretty significant number of users. We also collaborated with linear and binance attestation service for their proof of humanity campaign. But we also have a lot of other big use cases down the line. For example, some of the ones are telecom companies from traditional web two industries. So you guys might be wondering, why are we focused on private data? So let's look at a report from 2018 and you can see the overall data sphere is increasing and the percentage of private data is increasing. So the reason for this is pretty obvious as the private data is mostly customer data.
03:22:56.630 - 03:23:49.780, Speaker A: So with the increase of haptic devices that you have, every interaction that you have on the Internet could mean some sort of private data generated. So since these sort of data is very easily generated. So there's much more private data than public data. And the private data sphere projects to be seven times larger than the public one by 2025. So now let's look at the competitive landscape in terms of projects that are playing with data. As you can see, there are prominent names in their respective places. For example, the chain link, super oracles, stripe, fractal id, click id, binance, attestation, ethereum, attestation service, Gitcoin passport and all these.
03:23:49.780 - 03:24:58.230, Speaker A: But with off chain private data, there's only a handful of projects that you might be able to name, and we're definitely the pioneers in this realm. So what we propose is a solution that is able to verify any data from any account in any website. We're compatible with all existing infrast in web two, so we're able to break oracles free from the limitations of API. Oracle still relies on APIs to grab and feed information from different applications. So just look at how many projects and how many products that were built using Chainlink and super oracles now imagine that what they can do, we can do what they can do permissionlessly. And that really leaves a lot of room of imagination for applications that can be built with our solutions. So to better visualize this, let's try to look at the video for the linear voyage campaign where we used Uber as a standard to prove your humanity.
03:24:58.230 - 03:25:31.990, Speaker A: I'll see if this can be so. It's not. Okay, fine. So the demo is not available right now, but you can always look into it in YouTube. We have a channel there. But yes. So Uber as a standard to prove your humanity is really just the tip of the iceberg.
03:25:31.990 - 03:26:44.340, Speaker A: Zkpass is able to verify identities from a vast amount of data sources, such as government issues, ids, online credentials, healthcare data, gaming profiles, or online assets. So we're able to work with RWAs as. So now let's figure out how everything works in a backend. So in builder's perspective, Zkpas works in three simple steps. So it's define, retrieve, and verify. So going from define, you need to define the scope of data that you want from the user. So what you got to do is first you got to register on the dev center that we have, and then there is a schema validator that allows you to, which is sort of acting as like a plugin to validate that the schema does not have bugs, so it doesn't tamper or look into any of the work you're doing.
03:26:44.340 - 03:27:53.690, Speaker A: And then you can also, after installing it, you would create a customized schema, or you could use schemas that we've already written. When the user comes in and starts the minting process, the user will be redirected to the data source where the user logs in and starts the attestation process. Then the protocol starts to run and then returns a proof that can be submitted on chain. This submitting on chain process is really just a storage way to store this proof, so it can be also directly used to verify for your own project. So within this step is really what. So within this step is what the magic really happens. So the retrieval of private data and the verification process is conducted between the three parties, the zpass node, the user, and the data source.
03:27:53.690 - 03:29:25.894, Speaker A: So although the technicalities of our system is going to take a little too long to discuss in this workshop, our SDK carefully abstracts them away from the builder. So if you're interested to know more on how our protocol works from behind the scenes, feel free to read our white paper on our website. But essentially what our SDK does is after you've defined the schema for the data you're retrieving, you would provide the schema to the protocol and it would return a node addresses which contains an allocator node and a validator node, and also hashes of these node addresses. And then also a nullifier to prevent redundant duplicates that was generated. And also a task id that was allocated that is used for the allocator to tell the user which validator that they have to interact with. Then through the SDK, you can feed the hashes into the SDK verification function, and then it returns the address. So if the prior and the latter one matches, the SBT is valid, or else it's not valid.
03:29:25.894 - 03:30:48.938, Speaker A: So yeah, that's how you prove it. So we do have time. I'm going to put up a live demo. So starting from a dev center, 1 second lagging a little bit. Ah. All right, so this is our dev center. All right, this is our dev center.
03:30:48.938 - 03:32:05.120, Speaker A: And first of all, you have to connect your wallet, and then you can create a project here. So that project would, creating a project would give you a app id, which is, you can view it as an API key, and that. So you input the name and the domain. The domain has to be the one that you're hosting with. For example, let's put demo here, and then we're locally hosting it. And then these two are just. Could be anything.
03:32:05.120 - 03:34:30.150, Speaker A: So just leave it pen. Okay, so let's just assume we've created, because I think there's some issues with the network, but let's just assume you've created the project. You've registered a project in here. You can select add schema for the pre built schemas, and then you can also create your own schemas. And this schema is essentially. So you take the JSON response, for example. If you want to prove that your finance balance has over a certain amount of dollars, what you do is you would log to your binance account, and then there would be a.
03:34:30.150 - 03:35:40.580, Speaker A: Come on. There would be a QR code. And then after logging into it, you would go to the developer console, and then you would look into the network. So you would see that when it loads, there is a API that feeds the. This one. So there's my holdings. And then you would use this to create schema that you would insert in here.
03:35:40.580 - 03:36:59.440, Speaker A: So for the time being, I've already made changes to the schema. So what this does is it takes the data that was asserted, six of them in total, and then calculates if the actual value is over $100. And then also there's a code that proves that the person is you. Wait. Name, finance balance. So there's supposed to be a category in here that allows you to choose different categories that your schema belongs to. And then after you've done this, you would check the schema and then the schema would run for the first time.
03:36:59.440 - 03:38:32.080, Speaker A: Yeah, so after you've checked the loads. Yeah, sorry, guys, there's definitely like some technical difficulties here. Maybe we'll try to resolve this after. But the live demo, I think, doesn't work now. Yeah, so add the telegram channel and try. If you guys are building on this, definitely ask our devs on how this is going to be resolved. But I think this is just this computer's problem.
03:38:32.080 - 03:42:18.910, Speaker A: I don't think it's the problem of the protocol itself. Thank you guys. Thank you for your time. Yeah, we definitely have videos on YouTube, but I was going to show the whole live demo on how to use the SDK, but we also have a sample code on GitHub that shows you the whole verification process and the minting process. So feel free to check that out. If you have any questions, pop that question into the telegram channel and definitely we can help. Video.
03:42:18.910 - 03:45:07.096, Speaker A: Chris is the man. And if you start getting down, I'll like slash it. And you're just going to be at the podium, you think? And this is your mic. Okay. Good afternoon. All right, so, yeah, today I'm very excited to share with you guys our h two protocol. So forgive me, I just recently got a pretty bad code, so my voice is a bit like a bit shaky, but hope everybody can hear me.
03:45:07.096 - 03:46:05.320, Speaker A: Is it good? Okay, cool. All right, so first a quick introduction. So, my name is Jalin Lee. So I'm currently an assistant professor at the National University of Singapore, and also I'm representing at the vietna labs to tell you more about our very recent heel protocol, where we try to generalize this generalized notion of distributed consistency for building large scale, decentralized applications. All right, so let me first start with, I guess all of you here are developers in the web3 space. So you guys use or have been developing infrastructure, or use large scale peer to peer infrastructures. But so far there are very limited number of options in terms of a kind of consistency you get or kind of security you get from this infrastructure.
03:46:05.320 - 03:47:24.016, Speaker A: So here I gave you sort of like a graphical illustration, the kind of a spectrum of consistency levels, or this kind of trust you get from these infrastructures. So on the very left, far end of the spectrum, you get things like traditional tcp, IP stack or lip p, two P or things like that where you can build very fast, very fast networking communication between different peers in a peer to peer network. But the problem is that you don't really get a lot of security or consistency from these infrastructures. So you kind of get the bare minimum kind of security if you do end to end encryption. So that gives you things like authentication, integrity of your messages, but you don't get much more than that. So if you build distributed applications or decentralized applications where require a lot more security or consistency, instead what you do is you choose the far right end of the spectrum where you use probably a blockchain, use a permissioned BFT protocol, or you use any other form of infrastructure like layer two roll ups, restaking and other forms of infrastructure that gives you very strong security. So you have a totally ordered set of blocks.
03:47:24.016 - 03:47:56.690, Speaker A: So everybody agrees on the order. So there's global consensus. So security wise is really good. It's a very strong consistency, but the problem is it's very slow, it doesn't really scale. So for many kind of applications, it's not, probably not ideal to use this infrastructure as well. But we can see that this is a spectrum, right? So you don't only have two endpoints, there is a lot of ground in the middle. So all this middle for security options and consistency options, they're currently not a lot of for your infrastructure wise for it to use.
03:47:56.690 - 03:48:49.212, Speaker A: But actually we see that there are a lot of applications which can actually benefit from a more slightly more relaxed level, consistencies or trust models to scale them. So here I just gave you a sense of example applications. But here definitely there are not an exhaustive list. So here things like social applications, things like communication between different AI agents like games, social games and more recent like decentralized physical infrastructure and so on and so forth. So all these applications, one of the main properties that very interactive. So there are a lot of interactions between different parties in the network, a lot of communications going on. So putting them on chain is probably not the best option because of the scalability bottleneck.
03:48:49.212 - 03:49:52.704, Speaker A: So even though you have more scalability options these days, but eventually, because the cost and then there are still limited throughput and scalability, it's still not ideal for you to build these applications on top. On the other hand, these applications do require some level of security and trust and also consistency. So for example, if you're building social applications, you want to have some kind of consistency so that your followers, your tweets your followers. So these are your metadata changes. There are some kind of consistency guarantees at this moment. You either choose infrastructure that don't give you anything or you choose like blockchains, which are fairly limited in terms of scalability. So in this talk, so we are going to introduce hay two, which does provide this kind of middle ground where it offers a lot more kind of consistency and security options for you to build distributed or decentralized applications.
03:49:52.704 - 03:50:46.640, Speaker A: So hay two is a peer to peer communication layer. So compared to existing peer to peer infrastructures or communication infrastructure, it additionally provides this, what we call the trusted event ordering. Naturally from this communication layer and then underlying it, we are leveraging a novel form of verifiable logical clock, which I'm going to elaborate in a few minutes. So the nice thing about hay tool is that it is very low cost and then very highly scalable. But also it provides the opportunity for distributed or decentralized application to build various consistency levels on top of it. So it has really wide kind of applicability. So all the applications I just showed you will benefit from our communication stack.
03:50:46.640 - 03:51:40.596, Speaker A: So let me just elaborate by starting from a little bit background. So when you build any kind of applications, distributed systems, right? So the order of events is a really important aspect. So no matter what you do, you care about some kind of ordering, either it's like between different blogs or between a social media post and then some other activities. So these are all events and you care about the ordering. So how do you order events in a system? So the most traditional way. So for example, if you come to this east Denver event where you look at all the events on the schedule, so you clearly mark what time is each event happening. So that's the most traditional way of ordering events, right? So my talk starts at 145 and then the previous talk starts at like 01:00 p.m.
03:51:40.596 - 03:52:28.900, Speaker A: Or 100 and 20:00 p.m. Or something, right? So by looking at the different time spent timestamp, you can know exactly which event happened first. But the problem is that in a truly distributed or decentralized system, this kind of physical clock or a commonly agreed physical clock is not present. So there are multiple reasons why you don't have that in a decentralized system. So first, from a theoretical perspective, if you guys have heard or learned about relativity, so in relativity, so different objects which have different relative motions, they actually don't have the same common clock. So this phenomenon is called time dilation. So for example, here I show on the left is the International Space Station.
03:52:28.900 - 03:53:38.216, Speaker A: And then, so because it's moving with the relative motion, with, for example, this is the location of East Denver, because if there is relative motion, so the time actually disagree with each other. So they don't have the common time. So the other problem is that even if you don't have relative motion among different objects, because your clocks devices, they are not perfect, so they cannot track your physical time very accurately. So for example, depending on what kind of clock technology you use, if you use small atomic clocks versus very sophisticated atomic clocks versus a simple clock, you can find your computer like a crystal oscillator. So they have very different clock drift. So it's very hard for you how to have perfectly synchronized clocks among different parties. So if we can't have a common agree upon physical clock, what can we do to order events in a distributed system? So here we look back in the literature in distributed systems.
03:53:38.216 - 03:54:29.340, Speaker A: So, back in 1978. So, a famous distributed system researcher, his name is Leslie Lamport, who later got the training award for some of his work, including this. So he published a seminal work called Time Clocks and the ordering of events in a distributed system, where he exactly talked about this problem. So, if you don't have physical clock in the system, how can you order events? So he argued that the only trusted way of ordering events in a distributed system is to look at the logical ordering of different events. Or another way of saying it is called the causal ordering of events in a system. So what is causal event ordering in a system? So here I show if you have multiple processes in a distributed system, so they talk to each other through messaging. So you don't have a clock, so you don't know which event happened first.
03:54:29.340 - 03:55:30.140, Speaker A: But you know for sure that if I send a message from process p to process q, the sending of the event must happen before the receiving of the message. Now I establish a causal dependency between these events, and then you can use this. If I receive a message and later send another event, sorry, I send another message, I know the sending of the next message causally, depending on I received the previous message, because the sending may causally depend on actually receive the previous message, right? And then you can do a transitive closure of these events. Then you get this, what they called a happened before relationship, which captured the causal or logical dependency between different events, so that you don't require any physical clock. And then that is the true ordering of the system that nobody can disagree with. So that's a pretty cool and a pretty strong kind of event ordering property. And then Leslie Lampo argued that now we can develop a new kind of clock that doesn't depend on physical clock, but instead on a logical clock.
03:55:30.140 - 03:56:30.080, Speaker A: This logical clock essentially track this causal dependency between different events. By looking at this causal clock, sorry, logical clause, you can infer what's the causal dependency between different events. This paper and then the notion of a logical clock on the causal dependency have a lot of implication, a huge implication, on the development of a lot of distributed and decentralized protocols to this date. So we are taking this protocol and this notion a step forward. So we are looking at how do we build logical clock in a decentralized environment. So in a peer to peer environment, one of the biggest challenges in this peer to peer environment is the presence of potentially malicious or adversaries users, right? So Byzantine knows, or another way of saying it. So, because traditional logical clock depends on all the participants in the system, follow exactly the protocol of updating the logical clock.
03:56:30.080 - 03:57:48.084, Speaker A: So if there are any byzantine node in the system, it will break the entire notion of logical clock and the causal dependency guarantees. So what we do is we have done some recent research and then we found we develop a new way, a new logical clock called verifiable logical clock, that ensures that the logical clause passed in the system can be verified by any third party. So that means nobody can create a false logical clock that breaks causal dependencies. I'm not going to have the time to give you all the technical background, but a high level gist is that we leverage this new and novel application of zero knowledge proof. So this is a very recent development in the space. So this new technique is called incrementally verifiable computation, or IVC, that allows us to have multiple parties in the system by passing these messages, build a trusted logical clock with verifiable proofs, and by applying this novel kind of verifiable logical clock, we're able to build a lot of interesting applications and infrastructure on top of it here. I just give you some quick examples that we recently develop.
03:57:48.084 - 03:59:10.678, Speaker A: But you can certainly think of using this verifiable logical clock for a lot of more interesting applications, right? So for example, we can use this verifiable logical clock to build decentralized mutual exclusion, or the other way of saying is like a lock, a synchronization primitive between different participants in the system without centralized trust or centralized server for lock services. And you can build, as I hinted in the beginning of the talk. So there are a lot of applications that want something, consistency levels that are not strictly consistent, not linearizable in a more technical sense. So here we are able to use these verifiable logical clocks to build causally and eventually consistent data stores and then things like virtual machines, right? So your virtual machine state doesn't need to be always synchronized across each other, but we guarantee either eventually synchronize or if there's a causal dependency between the different updates, we make sure everybody observe them in a causally consistent order. And then we can use this verifiable logical clock to build a shared sequencer. For example, here we are able to use the partial ordering property of this verifiable logical clock to easily scale a shared sequencing layer. We have built a.
03:59:10.678 - 03:59:55.030, Speaker A: So from this verifiable logical clock, we built a hay to stack. So this hay to stack, as I mentioned before, is a peer to peer communication layer. Sorry that the picture might be a little bit blurry and small. So currently we structure as three kind of layers. So the first layer is underlying the lowest layer, we have this messaging layer. The messaging layer creates the proof and then generate proofs and then verify for these logical clocks and verify this logical clock while generating messaging. And then this layer also build the traditional peer to peer communication layer, things like kadamlia, DHT.
03:59:55.030 - 04:01:07.450, Speaker A: And then on top of it we have a middle over called the shim layer, where we build a sort of like a relay network where there is a large scale peer to peer network with a lot of participants messaging each other with the underlying shared, sorry, the underlying verifiable logical clock in each messages and on top of it expose a simple to use API to the application layer, where the application can use the logical clocks from the underlying layers to build their desired consistent levels and their applications. So that's the current hay to protocol hay two stack. And then we are a very community driven approach. We develop a fairly interesting and pretty noble protocol called the hay to improvement proposals. This is like the EIP system, where everybody, not just our core development team, can propose different improvement on top of hay two protocol, or components of the hay two protocol. So here we gave you some. We roughly coarsely developed into three different layers.
04:01:07.450 - 04:02:06.450, Speaker A: So the first layer is called HipA layer. So this is the fundamental layers of the hay two protocol. So here, so we build the fundamental technology behind hay, two things like logical clocks, the peer to peer network, and then the incrementally verifiable computation. And then on top of it, there is the hip b layer, where there is the middleware that people can build things like the relay network I mentioned about. And also things like sequencers and then random number generators. And then there are many developers who care more about the upper level applications and then they can propose things like messengers for social applications, decentralized games or things like other kind of social applications on top of it. So we open up this hip system to all the developers in the community, everybody can propose and then once it's approved they can be merged into the hay two stack.
04:02:06.450 - 04:02:46.070, Speaker A: So currently we have a proof of concept. So if you're interested, feel free to check out our GitHub page. So currently we have a few of these components currently already built. Things like the verifiable logical clock so the underlying peer to peer communication library. And then we also develop some, a few applications on top of it already as a demo. Things like a social application called z social and then a random number generation. So this is our proposed timeline for our hay to stack and the hay two protocol.
04:02:46.070 - 04:03:30.010, Speaker A: So I don't know if you guys can read it, but we are currently roughly here. So where we develop our current proof of concept POC and then our goal is to have the testnet sometime in the middle of this year, which is here. And then later this year, by the end of this year or early next year, our plan is to roll the main net of Haytu. So we welcome everybody to contribute, to talk to us, give us feedback. Many of our team members are here at East Denver, so if you are interested in our protocol or our stack, feel free to talk. So Akasha is here, I think somewhere here. That's me.
04:03:30.010 - 04:30:51.590, Speaker A: And then I think Sean is also here. And then we have many other people here from the hay two team and then also follow us on Twitter and then we welcome all your feedback and then, yeah, I think that's it. And thanks everybody. Wireless one, can everyone hear me? All great, great. So how's everyone doing? I'm going to be talking about the future of tokenization and the demand for cryptocurrency with that, just wanted to introduce myself. Hi, my name is Quincy, I'm a solidity developer for the XTC network and one thing I kind of wanted to bring up that's so interesting is I get approached all the time, probably like yourselves, a lot of other developers and people ask me what makes your network special or even better, people outside of crypto ask what makes crypto special? What makes crypto amazing, what makes it this great technology? And usually the go to answer is talking about the different tech specs, talking about the TPS, talking about different projects happening on that chain. One thing that I found really interesting, especially now in sort of the age of tokenization, NFTs real world assets is one thing that brings a lot of interest is the type of things that are being tokenized.
04:30:51.590 - 04:31:28.406, Speaker A: But even then, that doesn't necessarily make these networks special, because all these networks are focusing on tokenizing something. But then I realized something quite interesting. What ends up happening is different networks facilitate a certain level of activity that's unique to that network. Usually people in the retail space may look at this as community. People in the industry space or the corporate space may look at that as trade partners. But what ends up happening is you have a pool of people that are engaging in some level of activity that is unique to that network. And what ends up happening is in order for you to be able to engage with people like that, you have to go to networks where people are engaging in that way.
04:31:28.406 - 04:32:14.946, Speaker A: And what ends up being really interesting is different networks have different communities, different networks have different trade partners. Different networks facilitates different means of engagement. So when you start talking about what makes these networks different, it's not really the technology. There's like 70, 80, 90% overlap for a lot of these, especially evms. But it's the communities, it's the trade partners, it's the type of engagements that are being facilitated on these networks. Now this is really interesting, especially when you start getting into tokenization, especially when we start talking about real world asset tokenization or just different types of nfts, because we start seeing this, we start seeing people that are in gamefly start interacting with gamefly communities and networks that are focusing around gamefly, we start seeing people that are focusing on trade finance, engaging in trade finance communities. People that are engaging in retail speculative goods are focusing in these communities.
04:32:14.946 - 04:32:51.858, Speaker A: And what ends up happening is the fundamental demand for these networks are entirely built up off of the demand for people to engage in these communities. So when someone asks you or when someone's talking about what makes these cryptos special, well, the biggest thing is that people are able to engage in a way that they weren't able to before. They're able to utilize the technology to facilitate some level of communication or engagement in a manner that would otherwise be typically difficult or just simply wasn't possible. You see this with so many dows. You see this with so many communities, you see this with so many businesses starting to operate on chain and having access to many parties that they otherwise would have difficulty accessing. This is not a technical feature. This is almost like a social one.
04:32:51.858 - 04:33:39.970, Speaker A: It's one of these things where as we start encouraging more people to not only adopt the technology, but also sort of embrace it, it's not just about them embracing the technology, it's about them having the engagements that they already have and being able to utilize them a lot more efficiency with the technology. Now, what network they choose is depending on who they want to engage with. They want to go into games, find a network that's focused on gamefi, want to get into any number of other things. Those networks that facilitate those engagements are the defining factors of those networks. Now, one thing that I find really interesting, especially in the crypto space, is, what's the point of cryptocurrency? Right? We all probably hear this all the time, and it seems as if the cryptocurrency is almost just a medium of exchange for these type of engagements between people. The biggest thing that people like to say is, oh, there's the dollar, and crypto is replacing the dollar. But I think crypto is doing even more than that.
04:33:39.970 - 04:34:53.922, Speaker A: I think crypto is replacing a uniform means of how people would traditionally exchange, which would be all these different parties trying to come together to try to facilitate these services and being able to operate almost autonomously and operate almost decentrally, where anybody can now engage with anyone else and be able to facilitate these communities where you don't necessarily have to go to a central entity to be able to engage in these communities. I make jokes all the time where people are like, oh, I want to form a Dao, a community of decentralized people. And I'm like, do you want a Dao or do you want a Facebook group? Because there are tools like that. But the thing is, there aren't tools like that that allow people to engage in the exchange of value, the exchange in assets, the exchange in money, and the ones that there are incredibly expensive, incredibly slow. Now, we all talk about payments, and that's a simple way to look at it. But the exact same tools that are used to facilitate the next securities markets or even NFT marketplaces are the exact same tools that are going to facilitate marketplaces for video games, facilitate marketplaces for different type of brokers all over the world, and bankers, different marketplaces for carbon credits, whatever it may be. These are all fundamentally the same thing, but the engagements around them are almost entirely social, where people go to these different networks solely based off of that engagement.
04:34:53.922 - 04:35:32.450, Speaker A: So the biggest thing I really wanted to sort of emphasize is the future. Tokenization, at least in 2024, is going to be the year of tokenization. We're not only going to see more assets tokenized, more nfts represented. We're going to see more engagements between people utilizing these tokens, and we're going to start seeing tokens that we may not even recognize as, oh, that could be a token, and that could be a level of engagement that we may have. And that could be a way that we can recognize each other online and have an interconnected web of people being able to communicate and engage. And I think that's going to be all centered around tokenization. And with that comes the demand from the networks to be able to facilitate that engagement and facilitate that tokenization.
04:35:32.450 - 04:54:51.310, Speaker A: But that's kind of just my little spiel on the future tokenization. I see this a lot in my everyday life. I thought I'd be able to talk about a little bit, and I thought you guys enjoyed it. So I hope you have a good one. Thank you. Felt like a little short. Okay.
04:54:51.310 - 04:55:27.752, Speaker A: Good afternoon everybody. This is William from Psycho Network. Yeah, I'm pretty sure it's maybe a little bit like too green or something, but I guess everybody in crypto love Green. It's rising up. So I'm really excited and happy to introduce the trustless network of all blockchain cycle network. So how many people here today have ever built or trying to build any kinds of d applications? Please raise your hand. 12345.
04:55:27.752 - 04:55:53.024, Speaker A: Nice. It's not the wrong place. It's build a week. Yes. And how many other people have ever tried to interact with any kinds of d applications before? Yeah. So what happened to the rest? You guys confirmed the future or something? You know it's the future, right? Okay, so I guess everybody knows the user dilemma problem. Thanks.
04:55:53.024 - 04:56:33.710, Speaker A: Yeah, it's sometimes unsafe expensive, not even about the latency problem. And it's even complicated not just for developers, but also for the general users. Imagine this kind of situation. Like for ETh user, he or she trying to find some maybe interesting on arbitrarim, there was kind of like a DF over there. And after he or she interrupted with the application, he found out maybe there are kind of some profitable BnB dfs over there. So here is what he's going to do. Like this.
04:56:33.710 - 04:57:36.610, Speaker A: As you can see, the cost fee, like take like two times eth gas fee plus four times arbitrarium gas fee and two times Bnb gas fee expensive. No matter about the latency problem because you might miss a chance about the profitable things, right? And even the most apart unsafe because bridging after bridge after bridge, we all know the bridge problem right now. It's not safe. And for the guys who are the developers, you guys are facing different smart contract with different blockchains and of course the different liquidity because you are trying to deploy the liquidity on each different kind of chains which will separate you and now recycle. The user we mentioned, he only need to pay yes, master fee. We call it master fee two times. One is to the option one, application one and the other for the number two.
04:57:36.610 - 04:58:34.660, Speaker A: And for developers, you can only need to deploy all links at one time. And cycle will help you to reach all of the web3 users. So what exactly is cycle network? Here is the cycle network overview cycle network. Use the omnistate channeled indexer and trustless crosschain protocols to build the omnidistributed ledger which help us to focus on the Dapps and to accelerate to acquire more users from both web3 and web two errors. As you can see, there are three layers here. The bottom layer here we call security layer actually because we originally inheriting the Eth secret because we extend the DA here to the extended layers, to op, Polygon, Solana, whatever, you name it. And we use the bridge here.
04:58:34.660 - 04:59:25.872, Speaker A: But we put the bridge here, but it's not really bridge because we use the original l one to l two Zk roll up to deal with that. And we put ledgers on each chains so that we put the omnistate channel indexer, which is the decentralized indexer on ETH. And after that on the cycle layer, we put the decentralized sequencer to make this happen. And to link the extended layer, we build up the trustless cross chain protocol here. While the details you can check on the white paper, but it's probably the overview of everything. So one word cyclenet is the trustless omnidistributed ledger with the global state view of all blockchains. The core benefits are four sides.
04:59:25.872 - 05:00:53.760, Speaker A: The most important thing, security, because we maximumly inheriting the data security from ETH. And then because the omni Zk route for real time extension communications, we make it really low latency. And it's of course low cost because we leverage that by using the ro up to extend DA and ODLT. And this help all of us can do the cross ecosystem things, including the EVM, non EVM, l two, et cetera. So when we try to compare with the similar products like Zeta gen layer zero protocol, omni network, cycle Network is a trustless network of all blockchains with original l one to l two bridge roll up and the extended DA which bring us the only trustless crosschain with the highest security here. So what exactly kind of like products and service cycle network bring to us? We actually have two kinds of service here. One we call the standard Omnichan Interoperability network which will help you to provide the assets management supercons, assets liquidity channels and one click deployment of the EVM compatible Dapps and one click assets launch.
05:00:53.760 - 05:02:10.468, Speaker A: Of course we offering the customized ones, we call it omnichan at the service. Should we pronounce that Oist or owls or whatever? Yeah, it's a customized feature and even pricing models of the Omnichan network and we link by the ROE and Royal SDK to the basic network. The whole picture support the Dapp in games, finance, socials and tech accessories. And for the service we are offering for everyone here you can enjoy the self custody aggregation, omni asset trading and you can enjoy just one click login to operate the Omnikichen D apps by using our super hunt. And for developers, one click Omnichan assets launch and a customized version and application for any kinds of omnid apps. And also of course for the Defi assets with GM withdraw the system for ecosystem. This will help all kinds of ecosystem acquire more like Omnichan users and to help them with the assets liquidity pool.
05:02:10.468 - 05:03:00.344, Speaker A: And also we can also try to partner with different kind of chains opportunity to set up even the Omnichan application Incubation fund built together. So here is the case study. The case one we call it Omnichan Piggy bank which is focusing on the retail assets management. Like cycle's microfunding aggregation protocol enables users to merge the small funds from all kinds of layers. Like layer one, layer two. Before just like coins in sofas we got all kinds of shit coins, all coins on different blockchains. And sometimes we want to transfer it or maybe try to swap it into maybe some mainstream token or something.
05:03:00.344 - 05:03:39.860, Speaker A: But the gas fees like too high, too high, like higher than the cost. So we just leave it alone or something. But right now the user just one click to extend that to the certain mainstream token and the sweeper bot system will try to find the best price and maybe in the future. I think it's really falling, right? Yeah, in the falling step we are going to upgrade that into a silver cons. And the next case is about omnichan stablecoin solution. AUSD. Yeah, stablecoin always exists in the crypto areas.
05:03:39.860 - 05:04:28.470, Speaker A: Well it's actually for the stablecoin is nothing special at first or something, but the really unique part is because we help them to deal with the liquidity part. We help them to no need to worry about the liquidity. Put on maybe the emerging l two s or new ecosystems, and even also friendly to those kind of l two s or emerging ecosystems. So that they don't need to set up new liquidity pools. Everything on each change. They only need to focus on their own products to build up their scheme, make it stability or things like that. And the third case here today is LTP, one of our partner here.
05:04:28.470 - 05:05:24.200, Speaker A: It's about cross ecosystem omnichan liquidity solution for LTP. It's a gateway to CFI and DeFi, which is aiming to maximize the capital efficiency for institutions. So before the LTP, of course, they are already linked to the sacs already, and they try to get connections to different know. They have to connect one by one. So which means they have the separate liquidity, which is okay, but it's not efficiency even for the instilled in refund. So now we cycle, they have the integrated liquidity, so we help them to link all the decks to make it happen. Last but not least, we don't forget the biggest blockchain here, BTC.
05:05:24.200 - 05:05:56.060, Speaker A: Here we got BTC bot. The BTC bot is now focusing on BRC 20. And they do the auto cyber like a flashmade. They do really well and they can find the smart money and even do the copy trading. But I think that's a limitation or something. Just focus on the BRC 20 or something. So with cycle in the near future, it will expand the scenarios to the omnichan.
05:05:56.060 - 05:06:32.750, Speaker A: Like the BTC bot will tend to be like Omnichan bot. Yeah, I'm sure there will be more scenarios like omni trading about a prediction market. And even there was a guy from maybe some other protocol. They're doing AI parts. It's also like a big chance to emerge those new tags and to lower the battery. Okay, I think that's pretty much about everything. And here's the contacts of me.
05:06:32.750 - 05:07:13.620, Speaker A: We also have our like during the main event. Our booth is like 308. Please feel free to come to us and just say hi and try to see what happened. Right now we also have our online marketing campaign because we already launched our testnet and we are about to launch out the news about the first round foundation as well. Okay, thank you everybody. Have a nice day. It.
05:07:13.620 - 05:20:06.254, Speaker A: Hello. Hello. Yes, we are on time. Welcome everyone. I'm here to talk with you about CCIP cross chain applications. And this is me. My name is Solangeigerus.
05:20:06.254 - 05:20:39.594, Speaker A: You can call me Sol. These are my contacts. And you can find me in the other side, in the other building as well because I am leading the camp build. And let's start about talking with our bounties. I know that I have another mic, the hangs on mic. Where is this the other mic that I cannot walk around? Yes, now I can walk a bit. I know that I must be over there, but let's walk.
05:20:39.594 - 05:21:12.094, Speaker A: So this is our bounties today. Here it's the inverter hackathon. So we have two kind of prices. First is using CCIP and I'm teaching you how to use the CCIP. And we have 2000 for the best for projects and 2000 more for someone that uses CCIP and USDC. I'm teaching you how to use USDC as well. And also we have more for the five best projects.
05:21:12.094 - 05:21:56.900, Speaker A: We have more 2000 for each of these five best projects using any of the other chaining services. And if you use all of them, you can win in both tracks, maybe. Let's see. And this is the presentation. So this is the most important for you. You can take a picture, you can read this QR code and I will try to do a demonstration here using CCIP. And if you'd like to do my demonstration in a hang zone session later, you can go to the KP build because we have another session starting like 05:00 p.m.
05:21:56.900 - 05:22:29.420, Speaker A: Using this in a hands on part. Okay. And this is first of all, okay, I'm from chain link. Chain link, it's a platform related to Oracles, related to services that you can use. So Oracles are any kind of data source that are outside of a blockchain or even in another blockchain, and you'd like to bring it to your content. How you can bring information to your is much content. Oracle is the key to do this.
05:22:29.420 - 05:23:16.490, Speaker A: And when you talk about Oracles, it's so important that you have a decentralized Oracle network, not only one source to bring the information. So chainlink is a decentralized Oracle network platform with different services for you. So we have the data feeds to get prices to get information and the data streams as well. We have the CCAP to have cross chain transactions between blockchains. We are doing this in a few minutes, but also we have functions to get information. Imagine that. We would like to get an API, external API using a Javascript code and bring this information to your smatch content.
05:23:16.490 - 05:23:56.870, Speaker A: You can do it and also you can generate your own numbers. You can automate some execution in your smatch content. You can do everything using the services from Chainlink platform. And by the way, Chainlink is deployed in different blockchains so you can see the services in different blockchains as well. This is another important QR code for you. If you have a presentation, you already have this because all that you need to learn and it's not me, but you can learn more in our documentation as well. And this is, let's talk about CCIP.
05:23:56.870 - 05:25:01.358, Speaker A: So CCIP in a high level we have like you are the end user and you have your application, your application. You communicate with the CCIP howuter and the CCIP how touter is much content. In deciding the Cersei blockchain, you must say what is the message that you'd like to send to the other transaction, to the other blockchain? And what is the other blockchain? What is the destination chain? And we have an off chain part that is a decentralized Oracle network that take care of this. And after this the outer. In the other network, in the destination blockchain, you receive your message, you receive your transaction and then maybe you are sending some application over there. Or in my case now I'm sending directly to my wallet because I miss sending USDC. So I will do this first in a practical way.
05:25:01.358 - 05:25:28.346, Speaker A: And I have some minutes in the middle that I can explain more for you. So let's go. I need both. Yes. So here we are. I am on remix now and I have where is my smart content? This, I have this basic smart context. Don't worry, I will share this with you later.
05:25:28.346 - 05:25:57.290, Speaker A: But this is my content where I am on the avalanche Fuji network. So I'm starting on avalanche Fuji testnet and I miss sending my tokens to Sepolia. This is my goal. Some things that we must have. We must have the network set up in our metamask. We must have tokens and also you must have the USDC token you can get in the faucet. All the links are in my presentation.
05:25:57.290 - 05:26:20.766, Speaker A: So here we are. This is my contract and I would do my transaction first and then explain what I have here. I will deploy my contract first of all. So let's deploy it. This is my context to transfer USDC between chains. Let's see if I am on the right chain. Yes, I am a mavalash.
05:26:20.766 - 05:26:43.590, Speaker A: Deploy it. Go metamask. Yes, now. Yes. Okay. I think in a minute you have one more content here. This was other contest that I was playing here.
05:26:43.590 - 05:27:08.122, Speaker A: Probably you saw me playing here. This was other contents related to tokens? Like because I click it twice. I will not do this. Okay, one is enough. This is my contract. So now how can I transfer this? I can transfer this. In order to transfer iu pay fees only in the Cersei blockchain.
05:27:08.122 - 05:27:29.410, Speaker A: So I am in avalanche Fuji. Now I must pay the fees in the destination blockchain. But I do this here. Now on avalanche Fuji. The fees will be calculated by my contract. And when I'm paying fees, I can choose if I'd like to pay fees in the nativity token. In this case in the avalanche Fuji or using the link token.
05:27:29.410 - 05:28:06.426, Speaker A: It's like easy to get the link token than the nativity token. So I'm using link. Now I'm sending link to my contract to pay the fees. Let's do this. This is my contract so I have the address. I came into metamask tokens link and let's send to my contract five links. Here we are almost done.
05:28:06.426 - 05:28:37.670, Speaker A: Send. After this I'd like to transfer USDC. So I must approve the USDC that I have in my wallet. I must approve my concept to use my USDC to transfer my USDC. So how can I do this again? I will copy the address of my contract but I must me to the USDC token. Supposed to be here but it's not. So let's get in my presentation.
05:28:37.670 - 05:29:14.482, Speaker A: It's in some placed transfer. Okay. This is something to you but probably I have it here. Yes. I need this. And I have this in my presentation in another place. So what is this? I'm going to the OSDC content on Fujinaw and I must go to write as proxy to get the approve function.
05:29:14.482 - 05:29:48.490, Speaker A: And my account is already connected. So who is the spender? That I'd like to have is my contact here and how much I'd like to spend. Imagine that it's one and the USDC has six decimal places. So the connection with my metamask. It's a bit slow. But in a few minutes my metamask will open the pop up to do this. Yes, done.
05:29:48.490 - 05:30:20.712, Speaker A: Yes. You can see that I'm approving to send one USDC. Okay. If I come back to remix probably here I can see the balance of my account. And I can see that the balance must be five links. But not in USDC yet. If I see the hours.
05:30:20.712 - 05:30:39.984, Speaker A: Yes, it's already done. So now I'm ready to transfer USDC to another chain. Who will be the receiver? The receiver will be my wallet. So get the address of my wallet. Is this. And the amount is one in six zeros. It was that.
05:30:39.984 - 05:31:08.880, Speaker A: Let's get from here. Six zeros. Transact again. Wait, the connection. Not too much now. Okay, now when my transaction is confirmed, we can check the transaction on the CCIP explorer. So we have an explorer between the chains.
05:31:08.880 - 05:31:41.602, Speaker A: Let's see this. Let's get the transaction id. Then I'm going to the CCAP explorer. And you can see that I started on avalanche Fuj. I'm going to sepolia, but it's not done yet. So this is the moment that I can come back to my content and talk a bit more with you. So let's talk because we must wait a what? No, I don't like to share.
05:31:41.602 - 05:32:11.382, Speaker A: No, it's not this. No, here. Yes. So what am I doing? I have a cross chain message. The crosschain message can be to transfer tokens that are supported or to send any kind of data. I can send a message to another content. For example, I have another example over there where I'm minting nfgs from one chain to another chain and I can even send the token.
05:32:11.382 - 05:32:47.430, Speaker A: Imagine that I send the USDC and then I like to borrow to do a link, to do a trade in the other chain. I can do this. The CCIP sender can be you external account or any smart contract. The receiver can be an external account. Or if you have messages, information, data, you must have the CCAP receiver inside your contact. Also I present to you the how. We must have the how the communication is made between the howuters in both chains.
05:32:47.430 - 05:33:17.534, Speaker A: And we already talk that we can token transfer messages and transfer this in different ways. This is some examples that you can do. Like we talk about the token transfer. But imagine that you can have the collateral in one chain and do something with that collateral. Lock the collateral in one chain and do borrow something, another chain. You can do this. Imagine that you can do this with nfts.
05:33:17.534 - 05:33:59.280, Speaker A: Like I told you before. I show you an NFT that I have that the backing ground changed according with the surface chain. And I can use this with account abstraction to have some wallet included that is working different chains. Or in gaming in data storage or computation, you can have the computation in a cheaper chain and they result in another chain. And this is, we did the CCIP. This is the information that is important to you. If you'd like to do this alone, you have this gitbook, you have your video as well.
05:33:59.280 - 05:34:36.746, Speaker A: And we did this. So all the requirements are there. So to have the avalanche Fuji, to have the token to add link, to have the USDC, you can have all the information on this presentation. And this is the explorer that we are using now. And this is interesting, this is the workflow that is happening this moment. So because of the USDC is a token supported by CCIP. We have a token pool managed here.
05:34:36.746 - 05:35:26.022, Speaker A: So I create my contact, the sender, it went to the outer, and the outer is talking with the token pool, depends of the token. You can lock the token in one side or you can burn the token in one side. Then we don't have only one network, we have three networks to make this secure. We have one Hiski management network that take care of everything. To not have a double spend, take care of the finality of the transactions. We have another network that has a consensus to commit the transaction to the other chain. And we have a third network that's executing the transaction.
05:35:26.022 - 05:35:57.370, Speaker A: So we have different parts exactly. To be sure that this is really secure. When this arriving, the other chain is the same. Imagine that you send the token, what you did over there, you lock, maybe you are unlocking here or you are minting the token here. Depends of the token, depends of the strategy. And this is the same, it's going to the outer. And if you have a message, you must have a contact here, a receiver.
05:35:57.370 - 05:36:30.212, Speaker A: But if it's only token, you can go directly to your wallet like I did. Now I'm good. And like I said before, this is the token handling mechanism. Lock and mint, or burning and locking the other side, or burning mint. I already talked about the fees. So you are paying fees only in the search chain. And when you are paying the fees, it's calculated in the smart contract.
05:36:30.212 - 05:37:03.750, Speaker A: And also you pay a premium, that is for the node rewards. I'd like to show my content to you as well. Let's go here. So this is my content, I'm importing all the outer and client everything related to the CCIP. And then you can see here that I'm using the outer address, the link address, the USDC. This is the chain selector. I can get all of this information in the documentation of CCIP like this.
05:37:03.750 - 05:37:44.170, Speaker A: So it's loading. And then this is my message that I'd like to show you the token amount. I can send the token, the address of the token, the amount that I'd like to send. Then I'm creating a message. This message has the receiver, my wallet, no data. Now the amount of tokens, and if I'm using something like when I'm minting nfts, I must increase the gas limit. And I'm paying the fees using the link token.
05:37:44.170 - 05:38:06.594, Speaker A: Here we are. I'm calculating here the fees. And I must have more link than I must paying fees. I must have more USDC that I'd like to send. And here I'm sending the message is this. Let's see what happened with your messaging. It's here.
05:38:06.594 - 05:38:27.260, Speaker A: It should be like not this, but totally finalized it. Let's refresh my page. Yes. Success. We did it. How can I be sure? Here, one USDC. It was sent from Fuji to Sepolia 7 minutes ago.
05:38:27.260 - 05:38:50.834, Speaker A: So I'm going in my metamask now to Sepolia. I should show you before that I have some. I remember that I have, I think, 22 sdc. Yes. And now I have it. 23 is this. So this was just sent now from the other chain that I did.
05:38:50.834 - 05:39:15.190, Speaker A: This is really amazing. Every time that I do this, I love it. Yes. I need to tell this. And before we are almost finishing, I like to show you that I can do this with other things as well. For example, NFTs that are the arbitrary messaging. And this is the Nft that I can mint from one chain to another chain.
05:39:15.190 - 05:39:39.874, Speaker A: This is my NfT that I have it. If you'd like to see, my collection is over there. Depends of the Cersei chain. This came from Polygon Mumbai. This came from Avalanche, Fuji, that came from ethereum, Sepolia. And this is based on the bitcoin price. So I'm using data feeds to get the bitcoin price.
05:39:39.874 - 05:40:05.382, Speaker A: And if the price is going up. Since the last time that I minted the NFT, I will be happy or I will be sad or I'll be neutral. And I did you nft 1 minute ago. Let's see this live because my time is over. I know that I did this. Now we have something. Now something of this.
05:40:05.382 - 05:40:20.058, Speaker A: Like this is the zero. I think this is the last one I finished. Okay. I know. So is this. I just did this. Now in the details you can see that it was 18 minutes ago I was here.
05:40:20.058 - 05:44:44.080, Speaker A: So is this. Thank you, everyone. If you'd like to do this with me now, live encoding with your computer, we can go to the KP build in the other side and we can do together. Thanks so much. Was trying to let you know. Can you mute that test? September is the saddest month. September is the saddest month.
05:44:44.080 - 05:45:02.382, Speaker A: How's everyone doing? Good. Day three, right? Cool. Are we good to go? We're live. Fantastic. Number one. Thank you everyone for attending this is fantastic. Eth Denver is just kicking off, but already has great energy.
05:45:02.382 - 05:45:24.906, Speaker A: A little introduction about myself. My name is Robbie Krasinski. I'm part of the Chronicle labs team. I'm the lead of Devrel there. Admittingly, I joined about a month ago, but it's been really fantastic. And I'm going to give, I mean, in 15 minutes, kind of a quick overview of what Chronicle is in terms of what we're providing, why we're separate, and why. I think we're really interesting for you to start exploring.
05:45:24.906 - 05:46:01.686, Speaker A: So why don't we start out with, may I see a show of hands? Who is familiar with Chronicle? Okay, fair. So let's talk about the table of contents. We're going to be talking about the introduction to Chronicle, the current landscape of oracles. Why Chronicle deeper? Dive into what the Chronicle architecture is, and we'll ideally have some time for a remix demo. So my next question is, who here is familiar with maker? Yes. So you might be thinking, oh, like you've never heard of Chronicle. It must be some sort of new protocol.
05:46:01.686 - 05:46:36.626, Speaker A: Chronicle protocol is actually an oracle solution that's been exclusively securing the assets for makerdao in its ecosystem since 2017. So actually, Chronicle was the first oracle on Ethereum, and it was made for makerdao. And so when it comes to us exclusively securing those assets, since 2017, we've secured about $10 billion. Right. So Chronicle has been around for a very long time. Very long time. And only recently has it started to expand outside of maker in terms of what we're trying to do.
05:46:36.626 - 05:47:25.490, Speaker A: So I pulled this, actually today. If we look at DFI, llama, and we look at the scope, we can see that Chronicle is actually in the number two position when it comes to the total value secured. And we're floating at, I want to say, 22%. So, yes, it is very real. So let's talk about oracles today. Right? As we could see on that left hand side, we're just looking at price feeds, right? So with current price feeds being provided, a quick question. I would ask all of you who are like developers or are curious in this, how can you ensure the accuracy? Right? Number one, how can you ensure it? Who are you trusting? What vision do you have when it comes to data availability? What sort of data can you actually look at? If you wanted to dive deeper with the asset pairings, how would you go about doing this? Right.
05:47:25.490 - 05:48:03.610, Speaker A: That's a question that I think a lot of us have maybe overlooked in terms of, oh, what's the data that I actually have available to me. And if we go ahead and we look at that top, right, we've all seen those transaction fees. We've seen the ebbs and flows of how expensive that can be. Right? Oracles cost. There's no denying a considerable amount to operate gas costs to run oracles is certainly something that can't be ignored. Gas prices, as we've seen in the past, can have massive fluctuations. So what does this mean? This means at a certain point, it can be very difficult to maintain your project based on the current gas prices.
05:48:03.610 - 05:48:49.370, Speaker A: So this boxes a lot of protocols out. This boxes a lot of projects out. Unless you are able to eat those fees, this becomes very difficult, especially during tumultuous times in the space which we know will come and go as it normally does. So why chronicle? Right. So our goal is to build infrastructure that allows the founding ethos of decentralization and transparency to actually flourish, right? We ensure the convenience, but no longer at the cost of true decentralization or verification. We are a mission to develop frictionless, decentralized and verifiable infrastructure for all onchain data transmissions. Right? So what do we focus on? Cost efficiency? We focus on scalability.
05:48:49.370 - 05:49:18.950, Speaker A: We focus on verifiable data. We focus on the ability to plug and play, because we know this is important for developers. We focus on being decentralized, and we focus on reputation and security. This is something that we really take to heart. This is our ethos, and we really have a lot of pride in terms of what we're trying to accomplish. So let's go ahead and let's get under the hood. Again, this is a 15 minutes talk, so we're going to stay a little high level, but I will be very available and happy to dig into this.
05:49:18.950 - 05:49:56.500, Speaker A: So when we talk about our validators, we have decentralized validators. You'll hear me use the term validator and feeds. We could transfer these terms. So a validator is a feed when it comes to our nomenclature, right? So chronicle operates as a validator, consensus model. Chronicle is powered by a decentralized group of validators known as feeds. Right? So each validator is run by a renowned project, and you might think, oh, who are your validators? Here are some examples. So as you can see, very reputable companies that most of us are using day to day.
05:49:56.500 - 05:50:43.410, Speaker A: What's interesting about this, though, is because we are using these people, or rather, these companies, the validators are also staking the reputation. Obviously, we know in this space, reputation is quite important. If you lose it, it can be very difficult to regain this. So it is in everyone's best interest to avoid acting maliciously and to ensure all of the data that is provided is as accurate as possible. And Oracle has, our oracles have a canonical value which is obtained by a majority consensus of this consortium of validators. This essentially means that to trust the chronicle oracle, it boils down to how much does one trust the ecosystem of the web? Three applications that we use every day. And of course, as the number of validators feeds grows, the stronger this guarantee becomes.
05:50:43.410 - 05:51:38.686, Speaker A: So hopefully you can see this, the architecture at a very high level. So the Chronicle oracle protocol comprises of a distributed architecture spanning both on chain and off chain components, right? So the diagram highlights both of these components. The off chain components depicted are going to be on the left side and those are going to be green, and then the on chain components are going to be purple closer to my side. So we're highlighting both the on chain and off chain components. And so essentially what's happening is from the dashboard which we can see on the top left, that's going to be feeding in and hopefully is this legible? Can everyone read this? Fantastic, great. So what we're doing is we're pushing down to the archiver that's going to be going to the validator that's fetching information from the origin. Quick note that the origins can be on chain or off chain, such as a primary data source, so they can exist on both sides.
05:51:38.686 - 05:52:24.594, Speaker A: And so we're fetching that raw data. That validator is taking that data and it's pushing it down to the relayer. The relay is going to be collecting this data from the validators and creating a transaction that's going to occur on the EVM. We also have a challenger which is listening and verifying the optimistic updates to see if it needs to be challenged. If there's a malicious occurrence to ensure that all of the information is correct, the relay and the challenger are going to be then pushing onto the oracle, and then the validator is going to be giving a list of all of the allowed validators to the feed registry, and then it's also going to be giving a list of the appropriate models for the quorum and the validators into the Watt registry. So hopefully this makes sense. Again, it is quite high level, but I am trying to compress quite a bit in a short amount of time.
05:52:24.594 - 05:53:23.030, Speaker A: Again, I'm happy to continue this conversation, but we are going to be discussing this workflow a little bit later on in the presentation. So let's talk about scribe. What is scribe scribe is the name of a new type of oracle architecture developed by the team at Chronicle Labs. The design is built around five pillars built around scalability, transparency, accessibility, resilience, and security. Scribe is an incredibly efficient, gas efficient oracle based on aggregated schnorr signatures, right? And so this is where it becomes really interesting, is because we're using schnor when it comes to our architecture. So providing nearly consistent gas costs on layer twos and layer ones ascribe both on fixed prices, essentially leading to substantial savings over 60% on l ones. When it comes to competing oracles, we are cheaper by 60%.
05:53:23.030 - 05:54:21.724, Speaker A: When it comes to layer twos, about 68% cheaper. When it comes to some of our biggest competitors, we are about 80% cheaper than some other oracles that may have dominance in the space. So this approach to gas management significantly boosts the scalability of our oracle. So let's first take a look at the landscape prescribe for just a moment. So prior to scribe, architecturally, all blockchain oracles used a standalone implementation of an elliptic curve digital signature algorithm, ECDSA, to generate public keys to cryptographically authenticate the identity of the signer or the validator, and therefore the integrity of the reported data. The more signers or the validators that an oracle protocol has, the more it would cost the oracle builder or the operator to update to the latest report data of that oracle. Right.
05:54:21.724 - 05:55:00.330, Speaker A: Makes sense. This is because every signer or validator must individually sign the message to attest its integrity. But this ultimately means that those looking to utilize oracles have to consider the pros and cons when it comes to the importance of cost, speed and decentralization. Generally, one of those things is going to fall back a bit if you focus on two other ones. If you really care about decentralization and speed, it will probably be more expensive. If you really want it to be cheap and fast, probably won't be that decentralized, right? So there's a bit of a conundrum. One is always going to fall a little shorter if you are focusing on the two.
05:55:00.330 - 05:55:39.896, Speaker A: But now we can talk about scribe. So scribe has kept accessibility in mind while being designed. This was something that was very top of mind while our incredible engineers were working on it, right? The ichronicle interface is a plug and play for protocols that are already using, let's say chainlink or our oracles to simply swap out the contract addresses and you're already off to the races, right? So we make it very easy for developers to start to experiment with what we're providing at Chronicle. Chronicle's end to end verifiability ensures, let's say, defi. I bring this up because DeFi, of course, is right now a big driver when it comes to adoption. It's top of mind for everybody. So this is why I bring this example up.
05:55:39.896 - 05:56:28.452, Speaker A: It enables Defi dapps with liquidation functionality to offer unrivaled transparency. This is really attractive for the builder and for the user who doesn't want transparency. Right? But there are many examples of poorly designed blockchain oracles reporting inaccurate prices, price data rather, which then has led to illegitimate liquidation of users in DeFi projects. And this is very difficult, this is really difficult to swallow, but it is a reality. So there's never a question of whether the Chronicle oracle was operating as intended. Every aspect of any reported data is verifiable on our on chain dashboard and is legible to all blockchain users, regardless of their expertise. And we're going to see an example of this on the next slide and hopefully we'll have some time and we'll be able to see this in real time.
05:56:28.452 - 05:57:10.964, Speaker A: So what you're seeing is a gif that I've made, but what we're doing is we're looking at our dashboard and we're looking at a collection of ETh USD, that data feed, and we're looking at our validators. And so what we've done is I've picked one of the validators and I'm looking at the ability to look at the raw data. So everyone has the ability to look at the raw data and you can verify that the data is accurate. So you can see that data and verify it in browser, literally with a click of a button. You can see the clip that I've done. I've changed the timestamp upon trying to verify right here, you can see on that second line and it errors out. So this is accessibility for users being able to look at the raw data themselves.
05:57:10.964 - 05:57:38.940, Speaker A: It's something that we really pride ourselves on to ensure full transparency. So now let's talk about scalability with the help of schnor, right? So scalability is very important, right. It's really everything when it comes to our space, not just with oracles, but in general, we really have to think about scalability. So optimistic schnorr signature aggregation unlocks 66% of gas improvements. As I mentioned earlier. This is tremendous, right? This can't be ignored. This is a substantial amount of savings.
05:57:38.940 - 05:58:24.236, Speaker A: So what does that mean? So now we're looking at our optimistic schnorr. Right. So a snore signature is an algorithmic scheme similar to, let's say, BLS, right? Which snore you can verify on chain in more effective manner as opposed to BLS, at least right now. So Snor is actually quite attractive to build with, and that's why Chronicle has chosen this. So validators sign and create an aggregated signature. You have one signature and one validator and the signing of the correctiveness of that signature. Then we have a relay, which is a permissionless function, right? So it pushes the oracle update payload, which includes the snore signature, the ECDSA sign, the validator performed, as mentioned, one click above the data, which is going to be the payload, I.
05:58:24.236 - 05:59:03.636, Speaker A: E. The price and the timestamp. The oracle smart contract verifies the relayer passing the signature that corresponds to the payload, the ECDSA signature, and the signer is the validator and the timestamp is recent. So what's really interesting is what we've done here is we verify the relayers passing that signature that corresponds to the payload, the signer of the signature is the validator and the timestamp is recent. And then what happens is we now have a price and that price is going to be the optimistic price. What's interesting is all we do is we verify one ECDSA signature. We do not prove the snore signature.
05:59:03.636 - 05:59:47.888, Speaker A: We don't do this because again, we are trying to keep the gas prices low. So what we do have is we have an optimistic model that has a challenge period, which is a function that could be called to verify the schnorr signature. So if you run the challenge successfully, I. E. You catch something that is being malicious, that validator has proven to be faulty, so they're immediately booted and they're no longer a validator. So now what happens is we drop the optimistic price and the challenger, due to proving this negative act by the corrupt validator, then claims the eth that was inside of the oracle contract as a reward. So this creates an incentive to help ensure the validity of all of the data that's happening.
05:59:47.888 - 06:00:23.568, Speaker A: So what does this do? This architecture ultimately provides, again, on l one, gas, improvement by 60 plus percent. On l two, improvement by 68 plus percent, while providing transparency, decentralization, accuracy and security. So if anyone has a computer and they're curious, we're going to try to run this. It's a very simple example, it's a remix example of essentially an oracle reader. I'll give a second for you guys. To scan it. What it's going to do is it's going to take you to our docs, you just go to the tutorial, go to remix, and then you could see the contract there.
06:00:23.568 - 06:00:38.064, Speaker A: But what we're going to try to do is we're going to try to do this. Oh, man, this is all backwards. Bear with me. I'm sorry. Okay, so here's our contract. And so I can't see it. Shoot.
06:00:38.064 - 06:01:07.926, Speaker A: We'll figure it out. So here's our contract. And so now what we're going to do is we are going to go ahead and we're going to compile our contract. Oh, jeez, I really can't see. Let me, sorry, 1 second. We are going to compile our contract and now that we've compiled our contract, we are going to go ahead and we are going to head over to deploy. And what we have to do is we have to ensure that we're on the right network.
06:01:07.926 - 06:01:37.570, Speaker A: This has been hard coded to test sepolia. And so now we are just going to wait for this account to pop up. And so what we're doing is essentially we're going to be able to fetch the current price of Ethereum. Oh, forgive me. So what I know what I didn't do is I did not set the correct contract. So we are going to go to our oracle reader. There we go.
06:01:37.570 - 06:02:03.754, Speaker A: And now we're going to go ahead and deploy it. And so really, as we could see, there's not very much code in order to do this. In fact, there are more lines of comments than there are codes. But what we're doing is we have our kisser, we have our I chronicle contract. We have hard coded this to the testnet just for the sake of the example. And we are going to be able to read once this deploys. And it looks like it is working on deployment right now.
06:02:03.754 - 06:02:23.648, Speaker A: Yeah, we're pending. So once this deploys, we're going to be able to see a couple of things. We're going to see the Chronicle contract pop up. So we should be able to see this address. And then we're going to see our kisser address, which is going to be here. And then we're going to be able to read. So if we go down here, there we go.
06:02:23.648 - 06:02:49.864, Speaker A: So we can go ahead and we can click on Chronicle. And this of course, is going to give us that address as we were expecting. And we can go ahead and we can click on our kisser. That's going to give us that address as expected. There we go. And so now we are doing a read, and so we can see right here that we've already fetched the data and we can already see the time as well. Because if we look at this function and forgive me, I can't see so well.
06:02:49.864 - 06:03:45.832, Speaker A: I believe it's right here we can see we have an unsigned integer giving us the value and an unsigned integer giving us the age. And the team at Chronicle has done an excellent job when it comes to documenting and creating really fantastic comments when it comes to all the functions, to walk everybody through everything, if there is any curiosity. So I do encourage you to look at our documentation and especially look at our GitHub, look at our repos, because the documentation in there is also very robust. So again, this is a very simple contract, but it does show you how easily you could just start to spin up. Also, when it comes to our tutorials, we have kind of cooked in something quite similar to this into scaffold e two. So if you were curious to play around with this, we've done the env example, we've put a couple of things in there, and then we also have a contract cooked in under hard hat. So all you have to do is switch networks and you should be able to deploy, create the front and you're off to go, you're off to the races.
06:03:45.832 - 06:04:24.084, Speaker A: So if we head back to, oh, and here's an example of the dashboard. So let's go ahead and let's just click on, oh, I don't know. Well, actually we could click on validators. And so now if we look at our validators, we can look at their last update and we can essentially look at any information we want to. And so we can look at the pairings in which anyone is verifying any information. But if we wanted to look at, let's say, eth to USD, so we can go ahead and we can see that we have the ability to verify. Let's go ahead and let's verify with infura.
06:04:24.084 - 06:05:04.356, Speaker A: And so if we go ahead and click this, we can see that it's taking the medium price across all of our validators and it's showing us the raw data right here. And so now if we go ahead and we just verify, we can go ahead and we could see that's correct. If we make any adjustments to this, which you can in browser in the raw, then it would error out, as was demonstrated earlier. But I think that's really powerful that you could do that in browser and very quickly see and experiment with yourself. So I do want to be mindful of time because I think I'm just about done. If you want to get in touch with me specifically, there is a QR to my twitter, otherwise it's just Robbie K. But more importantly, get in touch with us, the team.
06:05:04.356 - 06:05:29.292, Speaker A: The team's going to be coming later this week. There are going to be two codes down here. On the left hand side, you are going to see our hackathon details. So if you're curious about this, I suggest you give that a scan. We're looking for innovation. We're looking for really interesting projects and we've got a lot of things documented there. And then on the right hand side, it's just going to be leading to our website, which very easily you can go into the documentation or you can go to the community element of it.
06:05:29.292 - 06:06:06.600, Speaker A: And I would suggest everyone join our discord. We're very active in there and we obviously want to nurture curiosity. So we want to continue to develop the product that we're building and with the assistance of everyone that's curious, obviously we want to ensure that we're listening to the people that are using our product and we're implementing that into our future development. So I just want to say thank you everyone for attending. Thank you everyone at ETH Denver for putting this together. I know it's a huge endeavor, especially with the amount of people. And specifically, thank you so much for Chronicle, for giving me the opportunity to represent the team and for giving me a try.
06:06:06.600 - 06:11:10.430, Speaker A: And with that, thank you everybody. Yeah, I got it. Good. Only that heavy demand. Go to your display settings and then you should be able to. Can you pull up your notes? Make that one big. Hello everyone.
06:11:10.430 - 06:11:55.720, Speaker A: Hello everyone. Welcome to Risk Zero Workshop. And today we're going to learn about general purpose ZKVM, then about Bonsai ZK proven service. And then we'll finish with ZK cop processor. This is our proven service that allow easy off chain ZK proof generation with on chain verification. So let's get started. So what we'll cover today, we'll go over first with the ZKVM architecture, then terminology.
06:11:55.720 - 06:12:50.850, Speaker A: Terminology. Then we'll go about features what make risk Zero stand out across other ZK products. We quickly go over example to show you how easy it is to get started and run and generate ZK proofs on risk zero. And then we go over ZK coprocessor. We'll see how seamlessly you can integrate Bonsai within your Ethereum application. So ZK co processor is very important because ZKVM and Bonsai together connect as a co processor to the smart contract application. And this unlocks quite a lot of powerful new applications on Ethereum.
06:12:50.850 - 06:13:59.034, Speaker A: So you can upload computationally intensive functions to ZkVM and then get back very small ZK proof that you can verify on chain. So Risk Zero was started in 2021 and is focused on revolutionizing the Internet by creating the infrastructure and tooling necessary for web3 developers around the globe. To build zero knowledge software. We are bringing general purpose computing to the zero knowledge ecosystem, enabling users to trust programs run anywhere, while allowing developers to use the tools they already know and love. So we made a circuit that usually developers have to construct to generate ZK proofs. An actual VM circuit doesn't change from program to program, it stays the same, but you have two inputs that you have to send to your socket. It's program that you want to generate the ZK proof on and input data.
06:13:59.034 - 06:14:39.878, Speaker A: And then circuit will take these two inputs and generate ZK proof for you. Circuits is within a ZkVM. So it's part of VM. Let's look at ZkVM. Very very high level overview. And let's start with the normal vms virtual machines that we all know how the architecture of them is very similar. You run VM on some type of host and you send a program that you want to run on ZkVM and the input data that you want to run this program on.
06:14:39.878 - 06:15:50.240, Speaker A: And virtual machine takes these two inputs, generates it, and provides you with some output of your program. With the ZkVM we have very similar architecture. You run ZkVM on the host and then it takes two inputs, a program that you want to generate a zero knowledge proof of and input data for this program and the ZkVM. Run this, run this, generate, run this program generates output of this program, and on top of it it also generates a proof that corresponds to this program and to this input data. And all of this data is part of the receipt. And we'll learn just in a minute what the receipt is. But this is a definition of verifiable computation where you just show someone a proof, and this proof verifies that you run some program with some private data.
06:15:50.240 - 06:16:55.742, Speaker A: And that's it. So RISC Zero ZkVM implements RISC five standard open source microarchitecture. And why we implemented a circuit that implements RISC five microprocessor. So advantages of that, that instead of making our own domain specific language that's difficult to learn, usually difficult to learn for the users or writing our own compiler. Our users can write their programs that they want to generate a proof of on any languages that LLVM or GCC compilers support. First we start focus on rust to make sure that we provide the best geomatic developer experience for our users. And then we extend to other languages and there is also ZK snarks.
06:16:55.742 - 06:17:48.610, Speaker A: So this part is a bit advanced. So if you are new to ZKV to zero knowledge, it's okay if you don't understand. So we use stark proof to generate proofs in parallel. So our risk five circuit generates stark ZK proofs and then we use recursive circuits to recursively aggregate many stark proofs together into one stark. And then we use growth 16 circuit to convert stark proof into snark. What it does, it provides you with very tiny proof snark proof that you can verify on chain. So risk zero ZK proof is growth 16 snark which is quite exciting.
06:17:48.610 - 06:18:58.640, Speaker A: So let's go a little bit deeper inside the KVM architecture, but still have high level architecture. So rust compiler takes rust code and generates object for LLVM. LLVM generates a bytecode and it also generates this risk five binary. And this risk five binary is the input for ZKVM. Executor takes this binary and generates execution trace or session and then proven takes these execution traits, checks its validity and then generates approve which is inside the receipt that we'll go in a minute. So right now you can have 256 megabytes of data inside ZkVM. So why it's important and what can you do? So developers can run ras code inside ZkVM and prove execution was done correctly.
06:18:58.640 - 06:19:48.608, Speaker A: And the second ZKVM makes verifiable computation easy to get started with. So let's go quickly about terminology. Terminology of ZkVM is very similar to almost identical to virtual machine terminology. So you have a guest which is a program that you want to generate a proof of this program that you send as an input to ZKVM. Then you run the KVM on the host and host is in charge of the KVM and portion of the KVM that actually generates the proofs the circuits. We call it proven. So we have guest program that you want to generate proof on.
06:19:48.608 - 06:20:54.868, Speaker A: You have host where you run your ZKVM on and approver is a part of the ZKVM that actually generates growth system snacks. So receipt that object that you get from ZKVM, it provides you with a valid execution of a guest program, and it also contains two objects inside. It contains journal public output of a guest and a seal. Seal is basically Goat 16 snark, basically ZK proof of correct execution of the guest program that you pass to ZKVM. So now let's look again at ZKVM architecture. So now you can see that we pass the rust code that we pass to ZKVM is a guest code, and then ZKVM runs inside the host. And this is the prover, this is that part that generates our zksnarks and receipt.
06:20:54.868 - 06:21:42.430, Speaker A: Receipt is the object that you get that contains ZK proof and public output from your guest program. So now we go to exciting part continuations. And this is what differentiates risk zero from other SDK solutions available on the market. We have continuations. So continuations enables you to generate proofs on parallel clusters. On other solutions you can generate on one cpu with risk zero ZKVM, you can run your proofs on parallel clusters. This is very exciting.
06:21:42.430 - 06:22:41.822, Speaker A: So before continuations came around, there was a limit how large your program was, so that you sent to the KVM, how large was your guest program? And it was around 16 million cycles. And after the continuations, the largest proof program that we send was 10 billion cycles, but theoretically run to trillions. So it's very large and it's very exciting. So this means that RisC zero ZKVM allows you to generate ZK proofs of very very large programs, and also it runs very fast. So another part proof composition. Proof composition make it possible to verify proofs inside the proofs. So using proof composition, you can create a proof that attests anything to anything that has previously been proven.
06:22:41.822 - 06:24:08.558, Speaker A: So here you see that you have output your ZKVM that generates receipt, and this ZK Snark growth 16 snark can be program inputs for next ZkVM. So it allows you to generate proofs of approves, which is again super exciting. So let's go through a very basic example to show you how easy it is to get started with ZKVM. So I want to go through example that, I want to prove you that a number is a composite number. So composite number is opposite of prime number. So I basically means that composite number have at least one divisor that is not one, and it's not itself. So from a user perspective, on the left side you have the guest code that actually checks that numbers that we pass are not one, otherwise it's prime number, it multiplies these two numbers and it sends product back to CKVM.
06:24:08.558 - 06:24:49.090, Speaker A: And this is host on the right. So host just run the ZKVM and it's also responsible for sending data to the guests. So let's look a bit closely what we need to know. First we need to make sure that we import functionality that interacts with the host environment over here. Then we read the data that was sent to us from the host. Then we do our multiplication, then we do our very intensive computations. And then we commit the public output to the journal.
06:24:49.090 - 06:25:51.610, Speaker A: So this output is the product that we want to our composite number and this is part of the journal. So on the host side, host is responsible for getting the data. So we just pick two numbers. Then host is responsible for preparing this data and sending it to the guest program before we actually start executing the guest program. So it put this data into guest environment, then it takes approver and it takes the prover to validate guest binary and it also provides the input data. Proven generates a receipt and here you can see how you can extract output from the journal. And this is how you can verify your receipt.
06:25:51.610 - 06:26:50.830, Speaker A: You can send this receipt to a third party and third party can verify it easily. So this is QR code for quick start guide. I highly recommend you to scan it to get started. So if I don't want to run in ZKVM locally, there is a remote option and this is Bonsai. Bonsai enables you boundless computation with bonsai, instead of generating proof of your own hardware, you request Bonsai to generate it for you. So Bonsai is highly parallelized, highly performant and horizontally scalable solution and is based on ZKVM proof engine. And this is a very high level overview of bonsai where you can have a guest which is part of your trustless software or part of smart contract.
06:26:50.830 - 06:27:44.010, Speaker A: You request Bonsai to generate approved. It sends you back a receipt and receipt contains ZK proof and the journal. So this is the most exciting part. ZK cooperates, so we want to make sure that zero knowledge proof integration into Ethereum is possible with minimal developer efforts. So this is high level overview of risk zero application on Ethereum using Bonsai. So first the off chain component of your app, whether it's the app client or a server, sends a proof request to Bonsai and then Bonsai generates a receipt and the app publishes the receipt on chain to your app. Smart contract and your smart contract called the Risk Zero verifier contract to prove the validity of this receipt.
06:27:44.010 - 06:28:59.638, Speaker A: If it's successful, the journal is trustworthy. So let's look closer around this risk zero verifier the verifier contract has irisc Zero verifier interface which provides with two objects, provides with a receipt, data structure and verify method. So Iris Zero verifier is implemented by risk zero growth verifier contract. And this contract is stateless and immutable verifier. So please scan this QR code for the Docs for the ZK coprocessor. And if you want to learn more about risk zero growth verifier, this contract is deployed on CFolia and you can find address at this link and you can choose this contract or you can deploy your own. So lastly, this is a link for risk zero information about hackathons bounties links to documentation documentation to ZKVM Bonsai ZK coprocessor links to discord Telegram channels where to find us during the actual event.
06:28:59.638 - 06:35:32.232, Speaker A: All this information is here on this link. Thank you very much and I'm looking forward to see you all. Thanks. Hey guys, good afternoon, how are we all? And I'm Kevin, I'm the co founder of Artala Network and you can always mark me with this hamburger hat. And I'm really glad to be here to share with you about Artella's work. So first of all, what is Artella? So Artella is an extensible blockchain enabling module and customized DApps through EVM plus aspect programming. So the key information here will be number one, we are a new l one network and we are focusing on the issue of the extensibility.
06:35:32.232 - 06:36:38.716, Speaker A: And number two, we are trying to make DApps more modular, more feature rich by promoting new solutions called EVM plus asback. And what is aspect and why we on earth need such a new paradigm will be the basic topic I want to cover in today's talk. So the reason why we need aspect will lead us to why we need EVM enhancement. And I think there's no doubt that EVM has already became the benchmark for data building patterns for most of the blockchain execution layers. But you do get thorough constraints regarding the custom, visibility, the extensibility, and also the realizations for complex scenarios. Basically, that's the reason why we got some alternative solutions like precompile, appchain, nonvm or other programming languages to try to solve all these challenges. But they all get their own trade offs like pre compile apption heavy for most of the developers.
06:36:38.716 - 06:37:57.268, Speaker A: And as for the non EVM, they still need time to grow to become the mainstream. So I think it's safe to see that the EVM is now the mainstream and the replacement cost for the EVM is extremely high and even invisible. So what Artetala is actually trying to solve is to find new solutions to promote and enhance the capability of the EVM while stay composable with it. So that's where we promote our new solutions like EVM plus aspect. So in Artella we will have EVM equivalence execution layer, but besides that we will allow developers to use the Wasam runtime to build a native extension, that is aspect in Artella as a copacessor to co work with the smart contract and enhance its capability for some specific scenarios and more functionalities. And the WASM runtime will also be a part of the artillery network, which means you will share the same level of the security as the EVM and you will be run by the arrow nodes of the artillery network. And you may also wondering that we already got several DUVm solutions.
06:37:57.268 - 06:39:44.712, Speaker A: So what exactly set aspect apart? So we're going to need to take a closer look on how technically aspect can separate different things. So like from a technical perspective, we are utilizing the join point model for aspects, which means the aspect can be injected by the developers in the whole blockchain transaction flows, which means the entire transaction cycle is now open for developers to customize. In another word, the programmable scope for developers right now on Artilla is not just limited to the EVM sandbox, but extend to the whole execution process of the blockchain transactions. And the second thing is aspect going to have more access to the base layer resources and underlying APIs of the blockchain base layer like call stack and call data. And the combination of these two features will make it possible for the aspect developers to realize in depth customizations will stay composable in the l one level or some more functionalities that cannot be realized elsewhere. The third thing is that the developers, they will have their own full discretion to decide whether they want to build, operate or even monetize an aspect. So let's say if you try to promote some modification specialized for your dapps in the visitor level, you don't really have to rely on the really heavy solutions like EIP to promote the global changes for your network.
06:39:44.712 - 06:40:47.436, Speaker A: Instead where the aspect you can realize the composable and the lightweight miners of the partial upgrades specialized for your applications and you can even monetize it by making it as a public services and be reused by other developers. As I mentioned, the four point for having ASPA is to have a better execution layer supporting for more on chain functionalities. So now let's shed some lights on some real interesting use cases now possible on Artala. The first case is about the onchain even driving programming. I know you may not be familiar with this specific words, but it's kind of like a very vital and well used programming patterns both in web two and web3. Like in web3, the liquidation process for most of the DeFi protocols, the timely task system and the back end systems. They are all the practice for the even driving programming.
06:40:47.436 - 06:42:11.288, Speaker A: But in traditional EVM architectures, it is nearly impossible to realize all this natively on chain. So we have to rely on the third party, the keeper network, to monitor the execution result of the EVM and trigger the additional actions to promote such actions. This will introduce several trade offs like basically because we are relying on a third party and asynchronous realizations, so we'll definitely lose the most part of the composability with the onchain smart contract and will have to suffer minis level of lattices and also by introducing a third party. So basically we are talking about more risk assumptions, but this will not be a problem with the help of the aspect. As a cop assessor for the smart contract, let's say you can put some aspect before the block is finalized and the aspect can monitor and analyze the results of the EVM execution and produce a corresponding just in time actions for the smart contract. So basically we are talking about like CaperNetwork or automation services, but number one with zero latency. Number two have the native level of compatibility and the transaction level of custom visibility.
06:42:11.288 - 06:43:13.676, Speaker A: You can really do a lot of cool stuff based on that. So these are the projects that are working on artillas for gamings. We have blade Dow and cellular, which are the two full engine game, or you can call autonomous world now running Artilla and they are utilizing the aspect to do the on chain automake NPC services that can combat with real players. And the movement of the NPC is backed by the really complex paste finding and combating algorithm that has been integrated into the aspect. So whenever user make a specific move, the MPC will conduct a corresponding move based on the calculation result of the EVM plus the logic that have been integrated into the aspect. And this kind of realization has perfectly proved like aspects work as an on chain gpu. If we take EVM as a cpu of our, let's say decentralized computers.
06:43:13.676 - 06:44:13.890, Speaker A: So because aspect can have a next level of computation power and can work as a perfect supplementary, a modular supplementary of the EVM in some specific scenarios and specific cases. And for Defi we also get some amazing developers working on some really cool stuff. Like they are utilizing the aspect to eliminate the bad MEV for their DAX and recapture it for their protocol itself. And also they are using AsPAK to product the just in time lp transactions to maximize the capital efficiency for their, you know, DaX. And also AspAk can work as on chain copaccessors which can realize the intern based transaction natively. And another real case is about the risk come true. The real time risk come true.
06:44:13.890 - 06:45:20.372, Speaker A: I think I don't have to emphasize how security means a lot for web3 wars, basically because everything about web3 is everything about the asset. But right now, basically we have to fully counting on the code auditing to prevent our fund from hacking. But the past experience has proved that the code auditing is not that 100% reliable. Well, I'm not trying to say that the code auditing companies, they are not reliable, just from the technical perspective, the code outing they are actually implementation of precaution and off chain white box solutions. And you cannot simply rely on the precaution to detect and prevent all of the kind of the risk from happening. But what about the real time control for EVM? It has been proven invasible because we are lack of the information about the transactions. And second, you cannot just simply run the real complex risk control logic through the EVM.
06:45:20.372 - 06:46:27.020, Speaker A: But with the aspect this will not be the problem. For the first time we can realize the web two scale real time risk control on web3. Like as a developer, you can integrate the risk control logic into the aspect and put it before the block is committed. And once you combine your smart contract with such aspect, they will help you to analyze every of your transactions and prevent your fund from being hacked. And this kind of implementation is really a modular and plugin and plug out solutions, which means as a developer, as a DAP owner, or even as a single users, you have your fully discretion to decide whether you try to step into such a relationship. That is like having a third party to analyze your transactions. And that's how we try to solve the dilemmas between something like privacy and securities, by giving back the decision making rights right back to the stakeholders.
06:46:27.020 - 06:48:04.776, Speaker A: Now our talent is cooperating with several leading security companies in this industry to really promote the security of the web3 into next level. Here is a real use case we conduct with several of our partners. We just deploy and replicated the curve contract on our tele network and use the aspect to successfully prevent the reintered attack from happening again at the runtime level. Yeah, this is just the tip of the Asperg about the real use cases of the capability of the aspect and artillery. We are just on our testnet and we just ran our first version of our hexons and we really get Artilla developer communities like transforming some of these interesting ideas into real codes like regarding the security, the on chain, KyC kyt stuff, the privacy, the AI and even the gaming and the DeFi part. So yeah, like I mentioned, we are currently on our testnet and like everybody here, we warmly welcome you to join our tela, to join us together to explore the boundary of the web3 communities and we'll have a booth here in Denver. And if you get any questions or if you want to share something with our team, feel free to reach out to me or anyone in artilla hoodies.
06:48:04.776 - 07:00:19.448, Speaker A: And yeah, so yeah, I wish you enjoying your stay in Denver. That is all for today. Thank you all. My name is Jerome. We are pleased to present ZKM is here in Denver. We are pleased to present our implementation of a modular zero knowledge virtual machine using lookups. My name is Jerome, Stefan is on my left.
07:00:19.448 - 07:01:25.520, Speaker A: Lucas also helps prepare the presentation, but he isn't here. Let's start with the concept of proof of knowledge. For some computational problem. A proof of knowledge is a protocol that allows approver to convince some verifier that it knows a valid solution for some specific problem instance. So an example is if you have the hamiltonian cycle problem, you have a graph, and you want to know if there exists a cycle, if there exists a path that will visit each vertex of the graph exactly once. So the context that we have is we have a prover, we have a verifier, and the prover says, well, I know a solution for this instance of a hamiltonian cycle, and it provides the proof. Now, for every proof of knowledge, there exists the trivial proof, which is just the answer, the full answer.
07:01:25.520 - 07:02:24.610, Speaker A: But for some context, some applications, this is not a good idea, because there are two contexts. The prover might prefer to keep the solution a secret. So maybe the prover, if we have a larger example, the prover might prefer to not reveal exactly what the secret is. And another context is that the resulting proof may be too long for the verifier to check. So these are two different extensions of the proof of knowledge. The first one are really called zero knowledge proofs. So the zero knowledge proof is the property that you have a proof that does not reveal information about the solution for a given problem.
07:02:24.610 - 07:03:37.880, Speaker A: And this is often implemented by performing randomization, a verifiable randomization, over the problem instance, and then show that you know a solution for it. So, in the example here, we have permuted the original instance, and we show in the permuted instance that a cycle exists. So this would be an example of a zero knowledge proof. On the other hand, we can have a different criteria. We may want that the proof is short, because in some context we have that the proofer is much more powerful than the verifier. And so the verifier doesn't have the computational capacity to process the proof and to verify it. So what we want is we want what are called succinct proofs, in which the verifier only has to flip random coins, perform some checks on the proof that the verifier sent, and then accept or reject.
07:03:37.880 - 07:04:50.000, Speaker A: So what we are doing in the case of a succinct proof is we are simplifying the work of the verifier at the cost of putting the burden on the prover. It's a setting in which we have, maybe the prover is a strong cluster and the verifier can be a cell phone, or we can have a context in which the prover is off chain and the verifier is layer one. Now, the context that we are applying this proof of knowledge is the context of verifiable computation. So in verifiable computation, there is one step before the proof of knowledge, because it's the verifier who is now the weak user and is asking to the proverb, I have this function x. I have this function f. I have this entry x. Please compute f of x for me, resulting in y.
07:04:50.000 - 07:05:59.600, Speaker A: So the idea in verifiable computation is that the proverb will compute y, which is the f of x. And in addition to giving the result, it will also produce a proof w, which shows that the value of y is indeed correct. And so important is that we want this proof to be short, because we want the verifier should have very little work to do. Now, in the context of blockchain, this proof of knowledge, they should be succinct, as I said, they should be not interactive. We want to avoid interaction between the proof and the verifier for a couple of reasons. And they can be zero knowledge, so they can provide privacy, but they don't have to provide privacy. And so the problems to which we apply these proofs of knowledge.
07:05:59.600 - 07:07:17.884, Speaker A: They can be the transition function for layer two, which are sort of Ziker roll ups, the execution of smart contracts, which is the ZKe EVM. So the smart contracts are then written in solidity, and we have the EVM that verifies the execution of the contract. And we can have other programs which are the ZKVM. So we have a program, in our case, it's a MIPS program, and we want to verify the execution of the MIPS program. And the technology that's being used for this are snarks, which stands for succinct, non interactive arguments of knowledge, which optionally can be zero knowledge or not. So how do snarks work? Well, this is a little bit high level, but it gives a little bit of the flavor a snark does something very surprising. A snark will translate everything, every computation into a polynomial.
07:07:17.884 - 07:08:34.372, Speaker A: And this polynomial is expressed modulo some large prime number, maybe a prime number with 64 bits, because, I don't know, there's a lot of mathematics behind this. But if you have a prime number, you can add and you can multiply, and that means that you can define, have a meaningful definition of what a polynomial is. So, having a finite field, well, it's easy to add, to multiply, or to exponentiate modulo this prime number. For bit operations, things are a little bit more difficult. So the first line that you see there, the bitwise operation, is a test to see that the element in modulo, the prime number is zero or one. And then we have expressions for the or operation, the and operation and exclusive or operations. And we also have a way to combine to batch a lot of polynomials together without increasing the degree.
07:08:34.372 - 07:09:32.840, Speaker A: This is important aspect, because what we do, if we want to verify that p of x is equal to a and p of y is equal to b, then we make a linear combination of this. Well, so the linear combination is of px minus a times alpha, and then p y minus b times beta. And so this value should be zero if this condition is satisfied. So that means that using this encoding, this discoding is also called arithmetization. It guarantees that any knowledge can be proven. We can do the succinctness and the verifiable randomness. We can make the proof non interactive using fiat JMA transform.
07:09:32.840 - 07:10:20.730, Speaker A: Nevertheless, some properties are difficult to verify just using polynomials, and this is why we use lookup tables. So, how does a snark work? We have the original problem. We transform it into a MIPS execution trace, the execution trace, we have the register, we put it into polynomial. And the opcodes, we put this into polynomial, we combine the two and this will give us polynomial commitments and lead to random polynomial commitment evaluation. And now Stephen is going to give all the details. Yes, thank you. Let me go through all the details with an example.
07:10:20.730 - 07:11:37.680, Speaker A: Okay. Given that the high level language can compare it into mips, and we got a sequence of the MIPS instructions, like the as instruction would be add register, fab register six and write the value back to the register seven. Okay, and for the write table, when we execute each instruction, we got the value of each register, and we go through all the instructions. We finally got a trace table including all the values we go through. Okay, after we got this trace table, then we need to apply some kind of technology like the continuation. That means we split the whole instructions or the whole table into different segments. Okay? So each segment may have fixed instructions.
07:11:37.680 - 07:13:04.552, Speaker A: And then here, actually we need to, because we have a trace table, we need to apply the constraints on each rule or cross the different rule. First, we convert each rule into a constraint polynomial. For the row I, the CI six r seven equals to r seven minus r six minus r five, we create constraint polynomial. So now with the instructions sequence, we have different constraint nomia. And on the other hand, we also have a trace polynomial. That means the r one x, r two x that we want to commit for the trace, for the execution trace. Okay, as I mentioned, we need to apply the constraints on the execution trace because the different instructions have different constraints.
07:13:04.552 - 07:14:22.216, Speaker A: So we split the instruction, as put to the tab, into different module. Like we have arithmetic module, CPU module, logic module, and memory module. So different module has different constraints. And of course, we also need to make sure that the values shared, like the different columns, are identical. Okay, so after we got the models and we have the trace table, we begin to prove each module, okay, so for the proving, we just use a stack. That means we commit the trace by Floyd and then we actually got different stack proof. Okay? Because finally we need to verify the final proof on l one.
07:14:22.216 - 07:15:34.120, Speaker A: So we need to join the different modules proof by kind of proof composition. Okay, so, yeah, okay, I think I would go this quickly because it's just saying that we have different modules and we produce the proof for each module by the stack. So for the stack, I think I will not go through the details. It's kind of the mocha proof. Mocha commitments. We mapping the trace polynomial into the mocha tree leaf and then calculates the mocha roots and of course we use the mat query to sample to open some points. So that's what we join the different proof into one that we use the proof composition.
07:15:34.120 - 07:16:33.932, Speaker A: Yeah, for the proof composition actually we have different segments. First we generate this dark proof for each segment, I think. So why we say the loot proof here because we have different table, different trace table, so we actually have different multiple stock proof for even one instructions. So first we need to combine the different stacks into one proof. Okay, so that's why I say the loot proof is the proof of root circuits of all the stacks. And then we aggregate the root proof. So the active proof would be the aggregation circuit over the two different root proof.
07:16:33.932 - 07:17:24.000, Speaker A: And finally, as all we know, the stack proof is quite large. It may be reach up to four megabytes. So we need use some other snack algorithm to compress the proof. So that's here we use a group 16 and we can achieve a constant proof size, about 200 bytes. And that's demo for all proof composition. It's quite easy if you can write a go noun program, then use the go companion to compare it into the mips. It's quite simple.
07:17:24.000 - 07:24:58.958, Speaker A: And then you can use our examples in our GitHub wrapper to split the mips into different segments and then generates the final proof for all the segments. Okay, so that's all we'll take any questions offline. Okay, thank you very much for your attention. Thank you very much. All right, I think we are good to go. Perfect. Okay, so this is actually my second talk on the stage over the past few days, and this is now the third day of Eth Denver.
07:24:58.958 - 07:25:57.018, Speaker A: And I'll say the same thing that I did during the first talk, which is that it's great to see already so many people here before the main event starts. So I'm actually particularly excited for this second talk of mine because it's a little bit closer to kind of what we're building here at particle network, which is a lot of account abstraction related products. And specifically it's going to be a very interesting kind of topic and angle that I'm going to be going through on account abstraction today, which is kind of understanding why account abstraction has maybe been adopted slower than you could have expected given its impact on user experience. So we're going to go through this kind of with this specific agenda. So we're going to start by kind of first defining what ERC four, three, seven account abstraction is, what does it mean for user experience, how does it actually impact applications and the possibilities within it from there we're going to go into the state of account abstraction through ERC 457. So to what degree has it been adopted, how many user operations have been executed and so on. From there we're going to go through kind of what's next for account abstraction.
07:25:57.018 - 07:26:38.770, Speaker A: So as you'll see within this second part, adoption hasn't been maybe as good as we have expected so far, or there could be some room for improvement. So how are we going to get to that point? How we're going to solve the remaining points of friction for account abstraction to make it truly a mainstream product that actually drives this idea of the next billion users. From there we're going to go through kind of two key components of the general adoption of account abstraction, or of kind of ux centric technologies in general. That a particle, we feel are very fundamental to the kind of improvement of user experience throughout the ecosystem. So first, what I want to do is go through kind of the timeline of account abstraction. I'm sure you guys have heard of this term plenty. What is account abstraction? Its impacts on user experience.
07:26:38.770 - 07:27:41.686, Speaker A: This probably isn't kind of a new term for many of you, but account abstraction has actually been kind of an initial idea for a long time now since 2016, with the original kind of proposal for ERC four three seven being done in 2021. Now, ERC four through seven, in terms of an actual implementation, wasn't live until March of last year. So whenever I go through some of these numbers in a moment, keep that in mind. In terms of account abstraction through the standard is still very, very new to the Ethereum ecosystem. And of course the numbers reflect this, but they can definitely be better as well. So as a quick overview, if you didn't understand what I just said regarding account abstraction and what it means, the easiest way to understand it is basically a transition from externally owned accounts, which are kind of the standard account structure within the EVM ecosystem, used for things like setting transactions and receiving tokens and interacting with contracts and so on, transitioning from that to smart contract wallets, which essentially allow for the same type of interaction, but through the means of a smart contract. So there's kind of a few key differences here that are important to understand.
07:27:41.686 - 07:28:27.754, Speaker A: The first is that externally owned accounts, which is probably what most of you use when it comes to metamask or any other typical wallet, these are secured by single points of failure. So private keys, as soon as your private key or seed phrase gets leaked, the control over your account is also revoked. It's given to whoever took your private key. This is a pretty major issue for a lot of reasons, and as a result, we see a lot of breaches and loss of funds, specifically through the loss of private keys today. Additionally, beyond just the kind of security implications of using an account with a single point of failure, external accounts also just have a very limited range of functions. They're made for very specific use cases of setting transactions and interacting with contracts, and that's about it. They aren't programmable as a result of this limited range of functionality.
07:28:27.754 - 07:29:01.310, Speaker A: And if you lose your private key, if you lose a single point of failure, there's no chance of recovery. Whoever has that private key now, it's their account. Essentially, the good thing about eoas is that creating an account is free. And whenever you create an eoa or you start using it, that's the same address, the same kind of interface on every other EVM chain that you use, which is great. This kind of creates a unified standard for users. And of course, it's kind of easy to understand whenever you're interacting across different EVM blockchains. So why do we want to move to smart contract wallets? Well, it's actually primarily to solve kind of those three points of red right there, which is number one.
07:29:01.310 - 07:29:32.410, Speaker A: Smart contract wallets are intrinsically programmable. So these are interfaces that you can program to basically have any arbitrary logic that you want. Do you want two fa, do you want guardians, do you want gasless transactions, et cetera. This is possible through smart contract wallets and isn't necessarily possible through eoas. Additionally, kind of like I alluded to just now, these have very flexible security. So while eoas are basically just bound to one private key and how you can secure that, smart contract wallets extend this pretty far into things. Like I mentioned, two fa, social recovery, et cetera.
07:29:32.410 - 07:30:25.802, Speaker A: And again, this goes in tandem with the fact that they aren't usually bound to a single private key. You could have multiple eoas that own parts of a smart contract wallet, or you can have one smart contract wallet that has different security features that, again, make it beyond this limitation of one point of failure via a private key. Finally, the kind of one downside to smart contract wallets, it's not a silver bullet by any means, is that they aren't free to deploy, because these are smart contracts at the end of the day. So if I want to use a smart contract wallet on a chain that needs to be deployed, and that costs money. So this is one point of friction, but ultimately you're going to have more benefits moving to a smart contract wallet via ERC 457 than you would by sticking with an eoa. So I want to kind of shift gears just for a moment here to contextualize some of the information I'm going to go through over the next 14 minutes or so. So first of all, I want to kind of go through the three pillars of friction that exist within web3 today.
07:30:25.802 - 07:31:16.438, Speaker A: So why don't we have a billion users within the ecosystem? Why isn't web3 as mainstream as it possibly could be? A large part of it, beyond just potential application limitations is actual infrastructure limitations that can be kind of put down to these three verticals. The first is onboarding and account flexibility. So the process of actually getting a user into an account, and ideally that account being as flexible as possible. So you want users to be able to onboard in two or three clicks. The account that they onboard into should of course be as flexible, as I mentioned, with smart contract wallets. So flexible security, gases, transactions, batch transactions, et cetera. Beyond this, you have transactional complexity, which is arguably a more important issue because today interacting with web3, especially when you're doing it across chains or across multiple applications, is incredibly complicated.
07:31:16.438 - 07:31:54.978, Speaker A: Users are basically expected to manually discover and execute every transaction in a sequence of transactions to achieve a goal. So if I want to do a on chain a, then I want to do b on chain b. Doing so could take five or four transactions and be pretty difficult for me as an end user to actually figure out and then execute. This is a major limitation. It's just as a result of kind of the rawness of interacting with blockchain today, and it's a pretty big barrier to entry for newer users or those with less technical acumen. For the third issue, which I'll talk about kind of for a good chunk of this, is multichain complexity. So some of you might have heard as of recently, this term of chain abstraction.
07:31:54.978 - 07:32:36.686, Speaker A: So this is kind of what that would aim to solve in terms of multichain complexity within web3 as a whole is a major, again, barrier to entry in terms of if I want to go from chain a to chain b, sometimes I'll do different bridges or it's non uniform standards, et cetera. It's quite difficult to interact across chains. And again, it isn't uniform. So for the end user, it's quite difficult and complicated. They need to be constantly aware of what chain they're on, what chain they want to be on, and the infrastructure that might get them between each chain, which often is difficult to work with. So these are the three main issues, onboarding, account flexibility, transactional complexity and multichain complexity. So this, as you might notice, kind of corresponds very closely with the user journey.
07:32:36.686 - 07:33:14.746, Speaker A: So number one, onboarding, account flexibility. This is the first step in a user journey. They start with the onboarding process and they of course onboard into an account. From there they start doing transactions and of course eventually they'll do transactions across chain. So these three problems tie in very closely with just what users, what you would expect users to do on a day to day basis. So of course these are all things that need to be solved and the fact that they aren't is probably a large contributor to maybe why we've seen less adoption throughout web3 as a whole, not just ERC 437. So given this context of these tying very closely with the user journey, what I do want to make sure is made clear is that the first problem is actually largely solved.
07:33:14.746 - 07:33:50.518, Speaker A: Maybe not to 100%, but we can consider it solved for the sake of this presentation. This is solved through number one, account abstraction. Number two, things like NPC wallets and social logins. The infrastructure exists today where an end user can go to an application and essentially onboard, and start using that application in two clicks, and they can do that into a smart account. So this is a big component of why the onboarding problem is effectively solved today, although of course it still needs some widespread implementation beyond this. The kind of latter two problems are things that are less solved. So these are kind of more emerging, cutting edge parts of the space that we have yet to really address en masse.
07:33:50.518 - 07:34:22.054, Speaker A: So that's again transactional complexity and multichain complexity. So I want to kind of go back to ERC four through seven here. So how does that relate to the three problems I just mentioned beyond the onboarding component? So first of all, ERC four three seven applies to a lot of different things. And you can see clearly this has been identified with an ecosystem through this large suite of infrastructure providers. You have a ton of smart account providers, bundle or paymaster providers, key management solutions. We fit into all four of these categories. So we do NPC wallets, we do smart accounts and so on.
07:34:22.054 - 07:35:19.254, Speaker A: But you have a huge list of organizations that are dedicated to building this infrastructure. But why? How does it actually translate into adoption, all these individuals building for ERC 47, what does adoption look like so far, since it was originally deployed in March of 2023? Here are some numbers. This is actually from Bundle Bear, which is a great website, and they give some amazing stats on account abstraction so some kind of key metrics here is, number one, between five core chains. So ethereum, polygon, optimism, arbitram base, and avalanche, there's been about 12 million total user operations. So that's by user operations that can either be one or multiple transactions. Beyond that, there's been about $2 million in sponsored gas across these five chains, 3 million total smart accounts, and around 200 to 300,000 weekly active smart accounts. So in kind of the grand scheme of web3 and what you usually expect out of kind of account adoption, and of course, comparing this to eoas, it's quite small at the moment.
07:35:19.254 - 07:36:06.694, Speaker A: Now, if you even look into account retention, which I believe John rising from stack up posted about this a few months ago, where the account retention with smart accounts specifically is pretty low, depending on kind of what month you're looking at, as you can see here, it kind of drops off pretty considerably from month one to month two. And eventually, starting at month six early in 2023, about 1% retention. So there's definitely some, I guess about 2%. There's definitely some kind of work to be done in this sense. So this is something to note as well regarding the actual state of adoption with ERC 457. So with 457, I don't have much time left, so I'm going to kind of go through three key points of friction contrasting with the three key points of friction I just mentioned for kind of the larger web3 space. So ERC four three seven.
07:36:06.694 - 07:36:50.182, Speaker A: The reason why that adoption, those numbers that we just went over a moment ago haven't been maybe as much as you might expect beyond just the fact that it's a relatively new technology that needs implementation, can kind of be broken down into three key things. The first is that for a long time, complexity as it relates to integrating ERC four through seven was quite high. It was pretty difficult to actually build an application that leverages account abstraction. Today, that's not necessarily the case. There's a lot of sdks, including sdks that we do and stack up and zero dev, et cetera, that abstract. Most of this intrinsic complexity it removes, eliminates it, essentially. So it makes it where you can basically now interact with the count abstraction to the same degree that you would with any other normal transaction structure, which is, of course a big step forward.
07:36:50.182 - 07:37:31.134, Speaker A: But even though the simplicity is there, even though it's easy to interact with account abstraction within an actual application, the issue beyond the actual complexity is incentives. So developers don't have large incentives to integrate account abstraction. This is because maybe they just don't have a large reason. They don't see the benefit ux wise that their application might see as a result of the integration. Now there's some exceptions to this, like gaming for example, has been one vertical where kind of traction has been integrated to a pretty large degree. But still in a lot of verticals you see a very low amount of adoption with this technology because again, a lot of developers don't see a reason to switch to ERC four three seven over the standard account structures that they're already used to. This ties very closely in with wallet participation.
07:37:31.134 - 07:37:59.734, Speaker A: So most wallets that users use are EoA based. They aren't based on smart contract wallets and therefore most users will naturally just automatically be in EOAS. Again, wallets don't have a major incentive to integrate smart contract wallets as an alternative or even migrate completely. And finally, again, multichain complexity. So smart accounts are especially difficult to work with multichain. We'll get into the reasons why in a moment. But this is a major point of contention with account abstraction, is the difficulty that comes up when you're trying to use them across different chains.
07:37:59.734 - 07:38:28.760, Speaker A: This can kind of be broken down into two core problems here. Number one is that it's expensive to use smart accounts across different chains. So you have to deploy a smart account on every chain that you want to use it on. This incurs gas, which is expensive. Beyond this, there's not enough feature incentives for wallets and for applications to integrate ERC four three seven. So as a result, you see fragmented integration, not a lot of adoption. And of course for multichain usage, there's some points of friction there as well.
07:38:28.760 - 07:39:08.898, Speaker A: Okay, so tying these back in with kind of the original issues that we just talked about with the larger web3 ecosystem. Again, we kind of identified three core issues, one of which was solved. The last two that are remaining for the entire ecosystem, not just account abstraction, is transactional complexity and multichain complexity. So this kind of ties very closely in with UX improvements. So incentives for wallets and dapps. Again, if we can solve this using account abstraction as kind of a layer, as a foundation, then account abstraction will intrinsically be adopted quite significantly. Multichain complexity, again, this can also be solved with account abstraction or be improved with account abstraction, which contributes to that problem that we identified with ERC 47 a moment ago.
07:39:08.898 - 07:39:37.030, Speaker A: So these two groups of issues, the fundamental issues of web3 and the issues of account abstraction specifically tie in very close together. So solving these within account abstraction will therefore contribute significantly to the solution of these problems within the larger web3 ecosystem. So this is very important to understand as we kind of go into the latter half of this presentation. Okay, so real quick, I want to go through basically two halves of the problem here. Number one, transactional complexity. Number two, multichain complexity. First of all, transactional complexity.
07:39:37.030 - 07:40:13.814, Speaker A: So this is solved primarily by what we call intents. I won't get into too deep exactly how this works within this presentation because I only have about 5 minutes left. But essentially intents, what these operate to do is basically take or remove the cognitive load away from a user when it comes to expressing and executing transactions. So as a user typically would have to go through every step in a process to achieve a certain goal on chain. So they have to do different transactions, signatures, approvals, et cetera, whatever they need to do to go from point a to point b, this is up to the user. They need to not only determine what these transactions are, then actually go ahead and execute those transactions. This is a problem.
07:40:13.814 - 07:41:01.990, Speaker A: This is quite complex. So what we can do alternatively is use intents via what we call the intent fusion protocol, to basically have a user just express what they want to do. So user says, I want to go from point a to point b, I want to spend this amount of x, right? And by doing so, by expressing that intent, that intention, the intent fusion protocol or any intent framework can essentially say, okay, well, we know what the user wants and we know what the transactions could be. Or the protocol should ideally figure out what the transaction should be to kind of get from point a to point b and then execute that for the user. So this graphic kind of visualizes that, rather than the user defining and executing every transaction that they do. Typically the intent fusion protocol, or intents in general, they just take an expression, a goal, and then they find the transactions that need to happen and then execute that for the end user. This is a major evolution and basically removes all transactional complexity.
07:41:01.990 - 07:42:01.078, Speaker A: I won't go into this, but if you want to take a picture of something, this is kind of the architecture of what you might see within most intent centric systems. Again, it just involves expression of an intent, then translation into actual transactions or tasks, and then execution of those tasks, of those transactions on behalf of the user. So intents can kind of be seen as an evolution of account abstraction to some degree. For example, this graphic was inspired mostly by biconomy, where they made the argument that intents are basically the next step from account abstraction when it comes to improving user experience. So you can see intents as leveraging account abstraction in a lot of cases and therefore pushing the adoption of account abstraction alongside, of course, solving this transactional complexity. So these two are again tied very close together. Okay, so at this point, going through that very quick overview of how we can solve transactional complexity, we now have both of these kind of initial two problems solved onboarding account flexibility through account abstraction and NPC wallets.
07:42:01.078 - 07:42:41.938, Speaker A: And number two, transactional complexity through intents, which are an extension or kind of a layer above account abstraction, which they typically use account abstraction underneath kind of that interaction layer. So now we have multi chain complexity. So how does this operate? What is multi chain complexity and how do we solve it? First of all, again, kind of a complicated graphic, but again, this is kind of serving to visualize this idea of whenever you're using multiple different chains, especially with a smart contract wallet, this very quickly becomes quite laborous and quite cognitively intense. So you have to deploy the smart contract wallet on every different chain. So if I have chain a, chain b chain c chain d, I have to go and deploy that wallet on every chain. So this is expensive. This is difficult, right? This is for the end user.
07:42:41.938 - 07:43:08.346, Speaker A: This is just kind of not very user friendly or kind of mass consumer ready, if that's a way to say it. So beyond this, of course, you have to manage balances on each chain, and actually moving between chains is quite difficult as well. So I think I went back. There we go. So this is kind of one solution to half of that problem, which is actually the deployment problem. Right. So because you have to deploy across every different chain, there's a solution to this, which is omnichan account abstraction.
07:43:08.346 - 07:44:03.226, Speaker A: This again gets a little bit complicated, but essentially the goal of omnichan account abstraction and kind of what we're building towards and what a few other organizations are building towards is to allow smart accounts to be defined and deployed in a single location and to be managed across every other chain that they're deployed on. Again, intent fusion protocol intents serve to contribute to this idea of reducing multi chain complexity through the cross chain execution of intents. This gets a little bit complicated, but if you want, that is kind of an architectural overview of how that would work. So how do intents kind of go from its expression to ultimately doing transactions across multiple different chains for the end user? Finally, I want to very quickly highlight this as well. BTC connect, which is another kind of thing that we're building at particle, which addresses kind of this idea of multi chain complexity with the bitcoin ecosystem. So we basically allow smart accounts to be interfaced through one native bitcoin wallet, Unisat OKX x versus et cetera. And this wallet can be used across every bitcoin layer two, including native bitcoin.
07:44:03.226 - 07:44:45.966, Speaker A: And that kind of results in a smart account that we're deploying on chain. So that is kind of unifying the interface across multiple different chains with the kind of abstraction, another example of solving multichain complexity via aa. Okay, so we've now solved kind of through these two very quick introductions, we've now solved transactional complexity and multichain complexity, ultimately leading to a complete solution that ideally will set the stage for actual mass adoption within web3. So that was that, about 20 minutes. I kind of went through it as quick as I could, but that's kind of the current landscape of ERC four, three, seven as it relates to user experience and potentially the evolution of that as kind of. We continue over the next few years, tying in again the solution to transactional complexity and multichain complexity. And that's about it.
07:44:45.966 - 07:49:55.400, Speaker A: Thanks, guys. There's our QR codes, and I appreciate you guys having me on. Good. Hey. All right, everyone. Thank you all for attending. Hi, team.
07:49:55.400 - 07:50:43.746, Speaker A: So today I'm going to talk about union, the sovereign interoperability layer, and, to be honest, about secure interoperability in general. I'm Corell Zero x Kaiser Carol on Twitter. I'm a developer and systems engineer by background and founder of Union Labs, where we do light client based interop, and basically, I think some of the best tech in the space. Our belief really is a massive horizontal scalability. We're building towards a unified, prolif centric future. I kind of believe in the 10,000 app specific l two s and everyone having full control of their entire stack. A state machine for every single product out there, all asynchronously composed, but in the end, achieving the same UX as monolithic stacks, so similar to Solana or Ethereum.
07:50:43.746 - 07:51:29.142, Speaker A: That's really the goal. What we're building towards, what we're seeing right now happening with Celestia and such, perhaps it's going to be kind of the next 30 years of crypto, perhaps it's only this cycle, but we're believers in horizontal scalability. First, I want to dive into a little bit why we do kind of web3 composability and what we're really building in the end. Of course, the objective of crypto is to build the world computer. For those that read, obviously, the Ethereum yellow paper, that's what we're building initially a single global state machine where code is law, ultrustless, verifiable, without intermediaries. This is obviously a great use case for DeFi because finance in the end is kind of the forefront of where we right now compete with each other. And where we do want code is law.
07:51:29.142 - 07:52:20.242, Speaker A: But in general, this is what really can enable the future of, I think, almost any application and kind of set us free from, not necessarily regulations, but free from unfair regulations or inequal standards. What's really great about the world computer is composability, which allows for a cambrian explosion of products. Really with smart contracts, we build tiny money legos, which we can post into further DeFi products. Right? You build a simple dex, you build perhaps a restaking product, and this allows a third party team to come in and just build an entirely new product. Which is why Ethereum is so powerful, right? There's no need to whitelist new products. The Ethereum foundation does not need to interact with these smart contracts being deployed. And you might build a product that in the end forms the backbone.
07:52:20.242 - 07:52:47.258, Speaker A: These individual protocol interactions are of course extremely secure. When one smart contract calls another smart contract, that's no issue whatsoever. You remain within this secure world computer state where no one can mess with your API calls. They don't disappear, ever. For me and for union, we believe the one global world computer, in the end, is dystopian. Single world computer isn't censorship resistant. It's basically a single sequencer queue.
07:52:47.258 - 07:53:15.030, Speaker A: And most of all, it also cannot scale. It cannot actually bank the unbanked. It cannot handle global finance. In the end, we're still stuck with at most like 400 tps. Not something that can handle world finance. Modular architecture solved this in a few cases, and on a security perspective, it's extremely competitive. We've got shared security, which across different l two s and app chains leads to massive economic security.
07:53:15.030 - 07:54:08.290, Speaker A: For any app, we obviously do execution proofs in ZK rollups, which, to be honest, are even better than shared security because they provide mathematical, indisputable correctness. And finally, right now with Celestia and Avil and such, we finally have data availability. And with Austria and conduit, we separate out sequencing into a different stack, which helps against sensor attacks in the end. Right now with the modular stack, I think it's already better than kind of the monolithic l one stack. And modular is on the rise. The amount of chains connected to each other through kind of standardized protocols, mainly IBC, has grown tremendously, and it's only more growing however, as it stands right now, kind of this whole modular thesis completely falls apart. It just cannot succeed yet, because what we gain with all this fancy technology in security, we're losing composability.
07:54:08.290 - 07:54:49.138, Speaker A: At the moment, we like to refer to asynchronous composability, which really is sending a bridge message from one chain to the other between two smart contracts and handling that. And right now it's simply not as good as synchronous composability. We built all these fancy bridging standards, right? Perhaps a single signature relayer, perhaps a multi Sig bridge, which I think everyone is aware gets hacked all the time, maybe MPC based like XLR. And finally you can go into consensus verification, state verification. But we really don't see many of those bridges yet out there. We're really in the kind of leftmost quadrant at the moment. And without proper asynchronous composability, really modular fails.
07:54:49.138 - 07:55:31.454, Speaker A: The current interrupt that we have can get completely attacked by many different parties out there. Most common, what we see right now is kind of hacker groups like Lazarus attacking bridges and emptying them out. But what we've really forgotten as well is that in the end, we build crypto to create a system that's resilient against nation state attackers, right? And with current interpr protocols, we have oracles, validators, multi sick holders situated in the US. So obviously that's a target for the SEC. We have bridges being completely in control of their team, to be honest. Why do we do smart contracts? Because we don't want the team that has built the product to control the product. We want something that's pure, that's abstract, that's mathematical.
07:55:31.454 - 07:56:10.170, Speaker A: So right now, if I want to do composability between an app chain and some app specific l two for DFI, this composability can be completely attacked in the Ethereum space. An interaction between uniswap and a vault contract, obviously completely secure. And truly, we cannot ditch interrupt security. As I said, right now we basically enshrined oracles as intermediaries. Deviating from code is law. We completely lack atomic execution right now for those building on top of bridges, so many different broken states can happen inside of your protocol. Funds are kind of in limbo.
07:56:10.170 - 07:56:51.610, Speaker A: They might reappear on ethereum, they might stand in an intermediate chain, really synchronous composability at the moment is web3. And to be honest, asynchronous composability. And web two is really web two. Bridging, to be honest, has been a bit of a disappointment really, with union. What we ask ourselves is how do we make asynchronous composability as good as synchronous? What we are truly obsessed with is bringing this trustless security level that you see in the EVM to the full modular stack. And this is probably one of the hardest problems to solve in crypto at the moment. Touching probably every single point in the stack, really.
07:56:51.610 - 07:57:23.522, Speaker A: It comes down to a few key points, and one of them is a very philosophical one. What we need to do is truly invert control. If we look at how Interop is being done right now, basically a bridging protocol, feds different chains and might choose to support them. For those that have tried to get support from perhaps layer zero or wormhole. This is only really possible if your chain has a massive community, raised 100 million from a 16 z and can be next in line. Otherwise you might be waiting for twelve to 16 months to get support. To me, interoperability is not a privilege.
07:57:23.522 - 07:58:17.862, Speaker A: Dydx was capable of building like a great app chain specific product because they had the community and the following to get bridging support. Where we have to ask ourselves, how do we enable the next 10,000 startups to also build their app specific chain or their app specific l two and gain full control of their product? If you think about it, doing this like app specific model is far better for the product. In the end, who really wants a product where your users pay 300 million in gas a year to the underlying l two, right. You want that value to flow back into your token, or you simply want your user interactions to be cheaper and for them, you use your product way more. What we need on top of that is unstoppable systems. So I've already alluding to this, but if we're going to get this 10,000 roll up future, these specific app chains, not all of them will be aligned with certain jurisdictions. There's differences in regulation between the US and Europe, for example.
07:58:17.862 - 07:58:52.862, Speaker A: Does that mean that if Europe outlaws privacy chains, but the US allows them, interrogators need to completely outlaw them as well? Is that a fair solution? I don't think so. And finally, we need to hyperscale. What we see right now is bridges kind of support 80 to 100 chains. Some of them struggle to scale up, and really no protocol out there right now can handle this massively scaled future. And in the end, we need to abide by a few things. Two of them obviously has to be permissionless ties into the inversion of control and it has to be fully trustless. We cannot have a system with oracles.
07:58:52.862 - 07:59:35.230, Speaker A: We need to have the same quality as Ethereum has. To me, security really is not pick and choose. Just like every website should have ssltls, interoperability should be fully secure. Right now we see some bridging protocols kind of go into the pick your security model, choose what security you want for what message. But really what this leads to is fragmentation. Again, this makes it completely impossible for builders to reason about their product, what they're integrating with, and just adds vulnerabilities. Tornado cache was a fundamentally sound protocol, worked perfectly fine, except people didn't have a proper look at the DAO itself and how that could be used to take control of the protocol.
07:59:35.230 - 08:00:29.038, Speaker A: Seeing that as kind of a lower security standard than you need for the actual mixer, and that was used by an attacker to gain full control. Imagine perhaps a protocol saying that they want the highest security for asset transfers because those are most often stolen, but then doing some of their governance methods with a lower security standard, allowing for exactly the same attack to happen. Now really, if you want to get this highest security level across the stack, you need to do consensus verification. Consensus verification is what's used by succinct labs telepathy, which is what all of this like ZK lite client hype is about, and inherents the underlying security of the chain. Instead of relying on oracles, relayers, or perhaps an Eigen layer abs, instead you inherit the security of the two chains connecting to each other. And when you think of consensus verification, you really have to go towards zkps. Zkps make sure that you can do consensus verification across this full stack.
08:00:29.038 - 08:01:29.014, Speaker A: Because in the end, we don't want to be spending about 50% of block space on interrupt, right? If we have this future, while all of these different l two s are communicating with each other, we can't be filling up the blocks and wasting millions of gads on just doing these bridging transfers. So we need something that basically compresses all of this and that massively scales it if it's going to be as good as smart contracts communicating with each other. And in the end, this permissionless nature leads to hyperscaling. Interoperability is a right. And like I alluded to, if the Ethereum foundation had needed to whitelist, every single contract would have about 100 contracts on Ethereum right now and no traction at all on DFI. Ethereum in the end would have failed in the end for modular to succeed, if the intro player itself need to vet all of these different roll ups appearing, it will fail and won't be able to outscale monolithic stacks. To us, we bet big on IBC, the interval protocol that originates in the cosmos stack, but right now is scaling outside of it.
08:01:29.014 - 08:02:04.370, Speaker A: All of this IBC traction that we've seen right now, it supports more chains than any other interop protocol is completely organic. It's because chains can integrate themselves to the interop layer instead of the interop layer needing to integrate chains. And that's what, to me at least, composability in crypto is all about. You can choose to integrate with other parties without needing their permission to integrate with you. So what are we doing with union about this? We're working on basically the first CK IBC solution. Initially we focused a lot on the cosmos ecosystem. We support any cosmos chain that's out there.
08:02:04.370 - 08:03:03.966, Speaker A: And more recently we're diving into Celestia and avil specific rollups supporting any sovereign rollup that's out there. More recently, we've started working together with scroll on their lightline technology to connect as well to IBC chains. And so this is kind of the architecture dev we're releasing this year that's going to enable us to scale to 500 to 1000 chains. This to me is still kind of the suboptimal architecture, because in reality, what we're doing is generating many different CK proofs and sending them to many different chains. So in the end, the more ecosystems we have, the more proofs we are generating and the more proofs need to be verified on chain. The truly cool technology is the next step to this which relies on proof aggregation, where we take all of these light client updates generated on all of these different chains and create one proof to rule them all. This is where you can do the really cool stuff like coincidence of once just in time liquidity.
08:03:03.966 - 08:03:34.622, Speaker A: And basically you end up in a state where every single blockchain just needs to allocate about 230k gas a block to have full interrupt with whatever other chain they want, be connected to 10,000 chains at the same cost as being connected to 100 chains. This is kind of what we're working towards in our five year plan at the moment. Proof of proof technology does work. It's simply too slow. You cannot generate this for every block. Yet for this we need GPU acceleration and Zksex. But this is coming.
08:03:34.622 - 08:03:57.766, Speaker A: And this is kind of the end state of interop, the God protocol. We can do better than this. So my take home message for you guys and what we believe in horizontal scalability is the future. Even though we scale ethereum with rollups, we are sharding it. We are creating more rollups, more app chains are created every day. Different l two s with different architectures. Move l two s.
08:03:57.766 - 08:04:18.182, Speaker A: Eclipse l two s. We're seeing this cambrian explosion of products happening right now to enable this future to actually work and compose and be attractive to end users. We need live clients. We need ckps. And finally, the take home message. The future is modular. If you guys want to follow more on us, check out our progress.
08:04:18.182 - 08:09:42.800, Speaker A: Go to Union build. Check us out on Twitter at Union build and give me a follow at zero xkys. Carol I tweet about this regularly and I think do most of the mind share on this. Thank you guys. Yeah, right in there. Hey there, it does work. Write down your name.
08:09:42.800 - 08:14:56.192, Speaker A: Hopeful Alex would be enough. Or do you need the full name? We got all. I guess that would be better than my full name. Is it fine now? Hello everybody, welcome to East Denver 24. Great to see you. Great to be here. It's always a good time.
08:14:56.192 - 08:15:36.300, Speaker A: February starting out, east Denver ritual obviously. And I'm Bernard and this is Alex A. We're both with fluence and we're here to introduce or discuss fluence. Cloudless cloud desk compute with fluence in general and of course how it pertains to unlocking 25,000 usd through the hackathon. So we at Fluence believe that the cloud as we know it is broken. It's controlled by oligopolists that seek excessive rents that want your trust but at the same time don't give it. They have high exit barriers, technology lock in, deplatforming approaches, and more.
08:15:36.300 - 08:17:38.256, Speaker A: The way fluence attempts to solve this problem is through a set of protocols where we have decentralized serverless compute, we have decentralized physical infrastructure and we have a decentralized compute marketplace that matches compute capacity from independent data center operator participating in the deepen structure with developers requests and desire for capacity to run the serverless compute and influence everything being decentralized, we verify we don't trust. So from a use case perspective, obviously it's serverless, so you can do whatever you want with any other serverless. However, due to the proof based cryptographic system underlying the fluence cloud is compute, verifiability at the runtime and execution layer is actually paramount and lends itself to some really interesting use cases, particularly around verifiable data preparation networks, real time data pipelines, decentralized RPC solutions and whatever you can imagine in your trustless mind. On the verifiable data side, it's interesting because as AI is getting increasingly regulated and you want provenance from data through data preparation to data lakes to AI models, fluent serverless can really help you significantly more than any other of the traditional cloud providers can. From an implementation perspective, we don't have time to dig into all the protocol aspects, but the behavior of the protocol, which is off chain compute, is actually carried out by what we call a reference peer. It's a rust implementation called NOx, and it's built on lip P to P. And it has a marine pool which is generalized webassembly runtime developed by Fluence, and an Aqua VM pool which actually handles your distributed choreography of your functions.
08:17:38.256 - 08:19:18.420, Speaker A: You can think of it as distributed workflows, and on top of it you have a scheduler basically grind jobs that allows you to manage your event triggers. And from the network perspective, it's truly deep in bottom up. We have large data centers, mostly filecoin miners right now, that provide significant capacity to the network for which they get paid if they're just providing capacity through a proof of capacity, or executing your functions for payment. From the developer side, how do you create these functions? How do you go about it? You basically have your business logic, you code them in rust, you end up with marine services, which is webassembly, and then you package your availability requirements. Do I want one invocation? Do I want one instance of my function? Do I want three? This is how you can build through our distributed choreography engine, your own failover and high availability solution provider specifications. Do you want to only operate with or work with green data centers, DFGO preferences, and of course the willingness to pay for your services. On the other side you have the providers, which they have their servers, they have peers which we just saw these noxes and then spelling, we sliced capacity into what we call compute units, which actually are cores for the most part, two threaded cores, and I think right now for 4gb of ram per cu allocation, which is what will run your functions.
08:19:18.420 - 08:20:26.580, Speaker A: Providers give these capacity commitments to the chain, and the chain is actually fluent's own blockchain built on IPC interplanetary consensus blockchain from protocol labs. And they also provide their offers where they say, hey, this is what we offer. This is the kind of cpus, this is the kind of ram, this is our location, this is our green status, our carbon footprint, what developer attributes these are, these services we can provide or we want to provide, and how much we actually need to pay or accept and that gives an offer. And then the marketplace, which is on chain, matches those offers from developers and providers, and then you create a deal which then leads to deployment and the execution or the availability of your functions to be executed. Okay, I only got 5 minutes left to get to the bounty, so let me go through this quickly because Alexey is going to dive into some of the aspects a little bit deeper. We have five cloudless serverless compute tracks for a total of 25,000. We have 5000 for each track.
08:20:26.580 - 08:21:29.688, Speaker A: And hackers should be cognizant that a you can provide or submit to more than one track and you can actually integrate our challenges, if you will, our bounty tracks into any of your other hackathon projects over the week. So the first one, and it goes sort of from easy to a little bit harder along the way, the first one is basically just hit the documentation, hit some of the videos, and create and deploy and execute a cloud as function. And that's basically it. And for that we have five times USDC 1000 improve your DAP with FrPC. So as most of you are aware, RPC is a sticky subject in the community. On the one hand, it's almost impossible for most DAP operators to run their own failover nodes. So you end up with RPC as a service, which is very available and very cheap.
08:21:29.688 - 08:22:41.248, Speaker A: However, it has problems. It has single points of failure issues, it has privacy issues, data trust issues, and multiple solutions have emerged. And what we've done is we built on Fluent's cloudless compute. We built a substrate for you to basically orchestrate multiple RPC endpoints, service endpoints, infura, alchemy many of them into a decentralized solution to eliminate the major sticking points, which I just mentioned, including a single point of failure. All this good stuff you work on, that, we have that substrate ready, and the substrate consists of multiple components, including some fluent cloudless functions, but also some pretty interesting distributed algorithms on how you manage the workflow, the choreography of these services. Anyway, you manage to do this, you implement it and use it in your DAP, and you're in the running for a five times 1000 USDC. Fluent cloudless is stateless fundamentally.
08:22:41.248 - 08:23:26.100, Speaker A: So if you want persistence, particular data, you need to add that on your own. And one of the more interesting projects is ceramic. And if you integrate ceramic stream or compose DB into your influence cloud list, you're in the running for again. 50 00 25 00 15 00 thousand each. Now this is my pet project and I hope we're going to get a lot of participation in that one with EIP 48 44. It's interesting that you can use this blob as a sort of intermittent or intermediate if you will state persistent option. And anybody who wants to indulge me and use that in their hackathon project, go ahead.
08:23:26.100 - 08:24:22.276, Speaker A: Again, you're in the running for 5000 USDC 2000 501,000 501,000. The last one is a little bit more involved and is more on the library enabling side. We give you two options, an MPC options and the CK option on the MPC option. Basically what I want you to do is part a multiparty TSS crate ECDSA and demonstrate its use through orchestration with our orchestration language, which is aqua on the CK side, port the halo two crate, which already is close to webassembly, to marine webassembly, and demonstrate it's used by proving hamming distances. The CK option is actually fairly not as complicated as it seems, since the circuit is. I give you examples for that in the repo has been written. So you literally can copy that circuit and demonstrate this.
08:24:22.276 - 08:24:49.704, Speaker A: So it's all about how to use it from fluence as opposed to how to write the circuit. Again, 5000 2515 o 1000 and that's it for me. So Alex, Abel, take over. Thanks. Thank you, Bernard. Hello all, I'm Alex. We're going to go through the experience of developing apps for fluence, and today we're going to focus on cloudless functions and their architecture.
08:24:49.704 - 08:25:27.300, Speaker A: You may already kind of know it because it resembles a lot of serverless architectures out there. For example, AWS Lambda. So it's pretty simple. You have compute functions on the right, they're usually written in rust, they're accessible from the peer to peer network. And we have HTTP gateway, which is complex a little bit, and it can call those compute functions and can serve HTTP API. So HTP client can call it if we look a little bit deeper than. What happened? My laptop hanged.
08:25:27.300 - 08:26:45.250, Speaker A: No, sorry, I have no idea. Yeah, thanks. No, it's my laptop, it's not yours anyway, so I guess they're going to be no demo. So the cloudless functions are usually written, the compute functions are usually written in rust and are accessible through the peer to peer network. So if we look inside the HTP gateway, we will see that it actually consists of several components. Obviously it consists of simple HTP server, and it has a cloudless function inside which you could also call distributed workflow. It's written in Aqua language, and we have a fluence js client that lets you send those cloudless functions to the peer to peer network.
08:26:45.250 - 08:27:31.570, Speaker A: And the cloudless function itself specifies the peer discovery mechanism to discover peers that host certain compute functions. So in order to start developing fluence project, you will need to set up fluency CLI project. And there are two ways to do that. One easy way to do just fluence in it here on the right, on the left in your case on the down. Interactive wizard will guide you through all the steps. You will have to select quick start or whatever and just follow the instructions. Or if you're proficient enough with fluence, you could do fluence in it with all the things specified.
08:27:31.570 - 08:28:29.022, Speaker A: So it will create a quick start project with the following structure. The most important things it contains are cloudless functions in the Aqua directory, the HTTP gateway, which calls those cloudless functions. It contains compute functions, definitions written in rust, and it contains configuration files which configure your deployment. And if you're using local development environment, which we recommend you to do, then you will also configure it in provider Yaml. So let's see it in action. But I guess not today. So yeah, so what it does under the hood of what happens in fluence, you will find peer discovery.
08:28:29.022 - 08:29:37.578, Speaker A: It works based on the fact that deal participants are stored on chain. So whenever you deploy something to the network, the process looks like that. So whenever at first you have deployment in your fluence yaml, you configure different stuff like what's the replication factor, that's the target workers. That's how many peers will be chosen to deploy this function to how much you would like to pay for a certain period of time for this function to be hosted and available. What's the initial balance of the deal? That means how long it will live until you have to put more money on it, and what services, what kind of compute functions it includes. Once you have that, you run fluence deploy and you get cloudless distributive uploaded to ipfs or to IPC. Effectively you get CID back, which is sent to the chain, which creates a developer offer which is then matched on chain through the market.
08:29:37.578 - 08:30:56.850, Speaker A: Deals are matched with providers, so providers specify their prices, available APIs and stuff. Developers specify its desired price, desired APIs and stuff, and market matches it all. And as a result you receive an active deal. Deal is active once it has enough balance to go on to burn through the money to pay to providers, and once providers signal to the chain that they are ready to serve that function. So based on that information on chain, the peer discovery mechanism works by querying that information from chain and returning that information back to aqua, which is your distributed workflow, iterates through the peers in the subnet in the deal and is able to call the function on each of them, collect the answers and present it to user, or let it do whatever it wants to do, modify data, process and stuff. And the last thing of the puzzle is how Aqua knows the functions API, because Aqua is a typesafe language, so it doesn't let you do silly mistakes. It does though, let you do clever mistakes.
08:30:56.850 - 08:32:36.606, Speaker A: What we do, we take webassembly file that we generate by compiling rust code. We look at its interface types that we insert there and we generate aqua bindings for it, so that whenever the cloudless function, the distributed workflow, finds the right peers and goes to them, it actually knows the types it should pass and can validate that stuff so there are no errors. Or if some data is wrong, it will alert the user and that's, unfortunately it a little bit shorter. What resources in your presentation? Sure. All right, thanks. Okay, so a lot of things Alex H has covered are actually things you as a developer don't have to take care of. This is all taken care of for you by both the CLI, which is our, it's not magic, but it's your way to scaffold and manage your project, and by the various components, including marine in particular, aqua.
08:32:36.606 - 08:33:36.246, Speaker A: So Aqua manages the topology so you don't have to program at the Lib P to p level at all. And one of the interesting part that comes from those deals Alexey was talking about, basically those deals when they're done, where you match providers with developers, their requirements, as well as the resource to peers, you actually get something, what we call a subnet, which actually is an overlay network on the lip p to p network that is managed through the configuration of your aqua. So again, you don't have to chase down any of that. The discovery, it's not inherent, it's not intrinsic, but it's taken care of for you. So you can really focus on building your function and choreographying them into these cloudless applications you probably want to end up with. Now, I know this was like really short, 20 minutes. It wasn't much time.
08:33:36.246 - 08:34:21.262, Speaker A: So we have a much longer workshop on the 27th. 60 to 90 minutes starts at one. And you'll just have to check our channels, including Discord, which is fluence chat, or our Twitter, which is Twitter comfluenceproject for more details. But if you're interested, definitely come to that workshop. If you have questions in between now and then hit us up on Discord or track us down in the building and we're happy to help. We obviously want you to take home that money, those USDC. That's our entire purpose of being here for you to succeed.
08:34:21.262 - 08:34:43.722, Speaker A: So if you want to take advantage of it, we're certainly up for it. Let me show a little bit. Okay, Alex has one more thing. Can you switch me please? I hope it won't crash again. Yeah, so just quick, I have like 30 seconds or something. So this is how compute functions look. It's just rust code, right? And it can do whatever it wants.
08:34:43.722 - 08:35:13.414, Speaker A: It can access external resources for HTP or whatever, but here it just returns a string. Here is what your fluencyaml looks, where you configure the development. And here you can see that it specifies that. It uses the service that contains this compute function and specifies the price. All that, and we can deploy it to the chain and it will work. That's how the project looks. That's how config looks.
08:35:13.414 - 08:40:14.778, Speaker A: This is it. Thank you everybody. Good afternoon. I'm here to talk to you today about Holochain, and in particular a certain architecture we can do with Holochain focused on integrating it in with other blockchain technologies. All right, hands up if you know about Holochain. There's some people, there's some not people. All right, I'm going to spend maybe 5 minutes on just a very high level overview of holochain.
08:40:14.778 - 08:40:48.998, Speaker A: And again, this talk is a non technical one. It's to give you a sense of the technology and what we can do with it. So it is an open source framework for building peer to peer applications. And one of the things I love most about this technology is it took deep inspiration on how nature coordinates and how nature is organized. And it's a very refreshing and different approach to decentralized computing and peer to peer coordination. And lots of it comes down into the inspiration taken from the biological world. Another thing that is really important to know about this is it runs on consumer hardware.
08:40:48.998 - 08:41:26.390, Speaker A: So it's not like you have to download a terabyte of data to run a local node or you have to buy expensive gear for it. You can run it on small laptops and people when they're running it in a peer to peer way that builds the network up itself. So consumer hardware is something that is easy to think is not possible. But there's a lot of stuff you can do with just individual laptops, individual devices, and it's the people who are using the applications who are hosting this network. So it's truly peer to peer, it's truly decentralized, and there's a lot of social and technical. Yeah. Technological implications that come out of these choices.
08:41:26.390 - 08:42:04.290, Speaker A: This is a whirlwind 5 minutes, by the way, so wave your hands if you want me to go slower and I can go down a bit. The security of the network is by peers countersigning and signing stuff with each other. And there's a lot of implications around not having to manage global state. But the way of doing security is a bit different than you're used to in the regular blockchain world. The applications that are built, they can be public or private. You can control who joins them and who participates and the roles within them in a very arbitrary and fine grained way. And as a developer, this gives me a ton of choices about the sorts of things I can do with this applications.
08:42:04.290 - 08:42:48.594, Speaker A: So each application is a standalone, peer to peer encrypted network. And that means that that application lives in its own right, it's its own network, and you can join it that way runs on the devices, the participants. And the thing I want to focus on today is the enforcing a shared set of rules that when you can start to write an arbitrary rule set which everyone publishing something agrees to and validates as true, then it starts to have quite profound implications on what you can trust in your systems. All right, that was backwards. So no servers, no blockchains, no tokens, no gas fees. It's a way of building stuff which is truly peer to peer. And I want to talk you a bit.
08:42:48.594 - 08:43:41.774, Speaker A: So that was. Oh, that was a faster whirlwind than I thought of holochain. So the thing I'd say next is that one of the most important things to think about in the Holochain world is that it made a very strong decision to not try and make global consensus easy. So instead of saying everyone in the world needs to agree with the state of our network, whether it's Ethereum or Solana or whatever it is, that everyone who's running a node agrees on that global state. By not doing that, and by making it so that apps can be independent of each other, each one has its state which is isolated from other ones, and you can connect them if you want to, but you don't have to, you can run it fully self sovereign. This is the decision which made all the efficiencies, the data efficiencies, the latency efficiencies, all of that is a consequence of that decision within an application. You also don't need to do global state for the nodes.
08:43:41.774 - 08:44:27.778, Speaker A: And this means that again, you can build applications which can scale, I don't want to say infinitely, but like nearly infinitely. Like as the application gets bigger, as the user base grows, the people joining it don't have a heavier compute load because you're sharding it and decentralizing it amongst the network. So that means if you join a really popular holochain application and millions of people are using it, you don't have to still download a terabyte to sync up like you do syncing up to the ethereum node. You can just have your local piece of it and sync up in that way so that not having global state has lots of big applications. And also, I realize this is a non technical talk, so like just wave at me if you want me to not go into that level of detail. All right, that was holochain in 5 minutes. Now I want to think about a particular application or technology.
08:44:27.778 - 08:45:12.640, Speaker A: So in Australia there's this thing called renewable energy credits. So the government has set targets of renewable energy production and it's essentially a subsidy they give out to energy producers. If you produce energy that's renewable for 1 can earn a credit, which is something like 30, $40, but the wholesale price of electricity is something like 20 or $30. So you can nearly double the value of the electricity you produce if you are eligible for this credit in Australia. And it's an incentive more people making renewable energy and to get the credits, regulation, bureaucracy, audits. It is really useful if your solar farm looks like this. If you can have running utility scale energy production, you can get access to this system and you can get a subsidy from the government and it helps you out in that way.
08:45:12.640 - 08:45:52.650, Speaker A: Not as useful if your energy system, if your solar production looks like this one. The regulation doesn't make sense for like, the work to do to get accredited is not friendly to small producers. And also the amount you produce is quite small. It takes these solar panels a long time to make a megawatt hour. But if they're selling energy back to the grid and they're getting double the price, that can make a big difference on how quickly they repay their solar installation. So this is the domain I want you to imagine of like, okay, this is the world and you want to build an application to help small producers, small generators get access to the subsidy from the government using a blockchain application. So who's a developer in the room.
08:45:52.650 - 08:46:10.286, Speaker A: Anyone here build stuff? A couple of people. All right. And so just think in your head, like how would you go about building that system? Like if we're going to use a blockchain, it's like, okay, tokenization. Well, that's easy. We can easily create tokens, fractionalize ownership of the rex. Okay. Or the renewable energy credits.
08:46:10.286 - 08:47:33.722, Speaker A: Yes, that's pretty known. You can start to do that. But the thing for me is like how would you get it over the line where regulators and enterprise customers who are used to going to these wholesale exchanges to buy their wrecks that are audited by XYZ, how would they see legitimacy in the work you're doing? How would they trust that when you say 100 households went and generated a megawatt hour of electricity together, that they actually did it? And that's the bit which is like, oh, layer twos, cheap gas fees, it's still a really hard thing to do because there's just so much data. Because if you're trying to track, to get data to prove that this has happened, you're going to be talking about, okay, sensors measuring the electricity, people looking at the, claiming that, yes, this person has the solar panels on their roof, energy bills coming back to them from the utilities, auditors, and all this energy you would need to track, and all the data you would need to track gets quite big. And that just large data sets don't really work well in blockchain worlds. And so you can build a centralized application. But what I've been focusing on for the demo for holochain, so I helped build the stall that you'll see at the main event where we're showcasing this technology is how can you start to build a validated data layer where you can take large amounts of data and you can have arbitrary data sets, you can put them into a blockchain friendly and amenable system, which is peer to peer encrypted, decentralized, and a lot of the things we value in the blockchain world.
08:47:33.722 - 08:48:29.726, Speaker A: And then how can you start to aggregate arbitrary data sets? So when I've been building this demo, it's focusing on like, okay, let's just measure electricity production and send data logs of that into the Holochain system. And if you're logging data every 10 seconds or 30 seconds, those logs get big. How can you start to have customers say, well, here's my utility bill, and here's the energy company giving me a subsidy for my energy generated and so on. You can look at the weather data on that day you can look at people who said, yeah, they really do have that amount of solar panels on their roof. And the numbers stack up. So you can start to look at this data set and you can start to say, yeah, you can audit it and have faith that the credits that were issued on chain have some validity behind them. And the thing I want to point out about this is it's about if you can make it cheap to audit so that anyone can go in and say, well, this issuer says they're issuing 100 of these credits and they're pointing to this application to say, here's the raw data to back it up, and it is cheap for someone to audit it.
08:48:29.726 - 08:49:18.058, Speaker A: Then yesterday I was hearing someone talk about that if you increase the penalty for breaking the law, it doesn't change how many people break the law or not. If you change the probability of getting caught, then you reduce the chances of people breaking it. Much, a lot, because if you think you're never going to get caught, you don't really care what the consequences are. But if it's very likely you will be caught, it's much less likely people will cheat. And so, cheap to audit, cheap to validate is what I think this system gives us, which is what I'm most excited about and interested in in this particular use case. Another point before I move on is that this is an arbitrary system. What I think Holochain is exceptional at is building auditable large validated data stores and linking them up to blockchains with simple links.
08:49:18.058 - 08:49:53.302, Speaker A: Because if you're putting lots of validated large data sets into these networks, and you've got a lot of control over who can see them and how can see them, et cetera, then you can do whatever you want in the blockchain world, where it's really good at the tokens, at markets, at trading and so on. So if you can get the validated tokens into there, you can do lots of stuff. And to link them up, you don't need to do much at all. Like in your on chain world, you're just storing references to. This is a unique reference to the Holochain app, which stores the data. And here's a unique reference within that app which points to a set of data that you can audit yourself. So it's really easy on the on chain implementation.
08:49:53.302 - 08:50:41.242, Speaker A: And on the Holochain world, all you're really doing is storing like transaction hashes of saying, oh, here's a reference to some tokens on chain. So the linking between the two systems is trivial technically, and I think combined they will really increase the scope of what you can do with blockchain and on chain stuff. All right, 9 minutes left. So I started this slide and I was like, two Holochain features I really want to talk to you about. And I was like, nah, three features. I was like, okay, I'm stopping at four before I do a list, but I wanted to talk about what I think is for people who don't know the technology that much, these are the things which allow me to build stuff that I can't imagine with other systems, or I can't do easily. Anyone see the spanish inquisition from Monty Python? How old is this lot? Okay, some people.
08:50:41.242 - 08:51:52.062, Speaker A: Oh, it's still exciting, isn't it? Okay, here's the first feature which I just really love in Holochain and all systems talk about validation, like how do you make sure your data is what you want, but the way Holochain does it in the way it means, the simple version, I would say, is that you've got lots of peers gossiping to each other and they're sharing signed transactions and signed information. If someone in that network starts to issue something which is factually incorrect, which people can say how I've got a signed transaction log which makes sure this person's lying, they can share that, they issue a warrant for that node and everyone can just start to ignore them. They get ejected from the network quite simply because you can prove that they broke the validation rules. And so by having the validation baked into the network one, it kicks bad actors out. So it keeps your nodes, your peer to peer network integrity high. And also it means that when you're writing applications, you can think about, okay, what must always be true for this to be valid. And having this set where I'm just thinking purely in validation mode, and then I can step away from that world knowing that the whole data set inside the application is sound, and then spending all my time thinking about usability and features and so on.
08:51:52.062 - 08:52:55.838, Speaker A: So the validation in Holochain takes a little bit to learn, but I think it's an exceptional feature and it's done quite differently than I've seen in other systems signals. This is the other thing in peer to peer systems, and often I find when you're building blockchain applications and other sort of novel technology, it's really hard to create a great user experience. And having systems where you can start to have just live updating and websockets type things, you have to do a lot of work to get the responsiveness in there. When you're dealing with on chain stuff where it's like 10 minutes for a new block to come through or whatnot. And even when you're going into the fast layer twos, you're still constrained by that environment. So signals one, the thing about the signals is if you imagine a peer to peer, so everyone in this room, we're running a node, and if you know the address of someone else, you can build applications where those two people can start to have a direct non intermediated conversation. They can start to talk directly to each other, they can sign things with each other, which they can store privately, or they can share with the network.
08:52:55.838 - 08:53:28.190, Speaker A: And that gives you incredible efficiencies and really low latency when you need it. Because often when you care about low latency, you don't care about low latency with like me, to everyone, it's like me and you doing things together where you can have direct low latency things. Which means that as I'm building an application, I can start to get these features in there which people just feel like, oh, it just works. And they kind of forget about the technology. Couldn't hear you, but I heard a. Okay, another thing about holochain, so feature number three that I love. Oh, wow, I'm talking faster than the time allowment.
08:53:28.190 - 08:54:18.826, Speaker A: So countersigning. And this is the idea of when you are doing transactions in a blockchain world and I want to buy something from you and you want to sell something to me, then we basically have to wait for the global state, for everyone in the known universe to agree that has happened, and that takes a long time, and that's where a lot of the cost and that's where a lot of the delay comes into blockchains. Whereas if you and I can just both sign transactions together and say, you sign this and I sign this, and then we start to publish it, we can start to do that in a way with witnesses when you need to. And you can do it very quickly and easily, which is a little bit less secure. You can do it a little bit more complex, which is more secure, but you can scale up the complexity and the security based off your use case, because sometimes you're doing a transaction which is like, yeah, I'm selling you a billion dollars of insurance. I want the whole world to agree. I want everyone to witness before I'm going to act on this transaction.
08:54:18.826 - 08:54:49.586, Speaker A: And other times I'm buying a carton of milk off you. And I really don't care about that much security, I'd like a finer grain of security. And having the ability to go up and down within an application is an incredible amount of flexibility. And so countersigning is one of the things you probably wouldn't have seen. And if you're diving into Holochain world, have a look at that feature. And finally, this is the one that I've used the most for the demonstration that we built for the Holochain, for the stall at the main event. Holochain's got this idea of capability tokens.
08:54:49.586 - 08:56:00.926, Speaker A: And the thing which I get from this feature is I can now build applications which people who know nothing about wallets, who don't have keys and so on, can use this application. And essentially it is a way for a node who is fully empowered in the network. So someone who is running a full agent, they have keys, they're set up, they know a lot about Holochain, so a more technical sort of person, that someone can interact with that and the node can delegate actions to that person. That's like a sort of complex mouthful in a way, but by giving them keys which I can generate and they can store in their machine, they're fully signing transactions. You can see who has signed stuff, you can apply validation rules to their actions, but you can do it in a way where they don't need to know anything about Holochain, they don't need to know anything about Ethereum or blockchains, and you can start to give them the ability to proxy into the network in that way. So it's incredibly useful for building like a mobile app, which someone can just go to a website, start doing stuff, have no knowledge of our ecosystem, but it's starting to being recorded in these decentralized auditable systems. And it can be a stepping stone for people where you can take them on a journey of, oh, now that you've used this app a little bit, if you want to use the full feature set, well here, go download something locally and you can go in that way.
08:56:00.926 - 08:56:58.030, Speaker A: So that's one thing I get from capability tokens. Another thing, and this is probably a combination of capability tokens and validation, is that if you think about these agents who are all signing messages to each other and doing transactions and recording stuff in the system, you can start to build the validation logic where it won't pass validation unless it's signed by a user token. So if you think about logging into this account with an Ethereum wallet. So as part of the demo, I built a login with token proof, so you can log in, it proves you have the address, and then you can start to have transactions which are only valid in the Holochain world if it's signed by that Ethereum address. So every agent will look at it and go, hey, is this a valid transaction? And unless it's got a signature of that address, they'll say no, it's not valid, which means no agent can act on that behalf. So these agents don't act like centralized server admins who can go and tweak the database on you. You can build validation logic which is only valid based off secrets that are held by users who know nothing about secrets.
08:56:58.030 - 08:57:30.394, Speaker A: So the Ethereum one isn't easy, like, okay, that's someone who's sophisticated and knows and set up a wallet and whatnot. But you can start to think about arbitrary secrets that your users have easily accessible. And that could just be like username password that they make up on the spot, or an email or whatnot, something which only they know. They can use that to sign transactions that the whole network will only validate if that secret is in the possession of that user. And so this flexibility with capability tokens to give new people access and also to make the agents a neutral. I think I've gone a bit technical. Again, sorry for pitching this as a non technical talk.
08:57:30.394 - 08:58:08.294, Speaker A: Hopefully it's useful for you. But that's the thing which has blown my mind most. Building this application in the last month is just like, that's opened up a world of stuff which I can't do in Ethereum, I can't do that in centralized land. There's this set of problems and domains and affordances which Holochain gives me, which is just really exciting. So quick introduction of Holochain, a quick conversation about using it as an auditable data validation layer, which I think is a really strong use case for it. And so what we've built for the demonstration at the stall on the main event, it sort of rhymes with renewable energy credits. We're calling them human energy credits.
08:58:08.294 - 08:58:46.946, Speaker A: And essentially we've got a couple of exercise bikes, we've hooked up generators to them so they could generate electricity and we're measuring how much energy they are generating. A little bit like how a household would generate electricity from their solar panels. We're aggregating multiple generation claims into a single issuance and then issuing that on chain with a link back to Holochain. So you can say, hey, this random person on Ethereum has issued some tokens. They say they are valid human energy credits. Prove it. Follow the link, go back to the application and see who cycled when to actually say, do you trust that credit or not? And there's lots of other things going in that space, but that's the store we've built up, so it's been a lot of fun.
08:58:46.946 - 08:59:47.570, Speaker A: Also, integrating the thing with holochain as well was because it's a lightweight client, you can run it on really small devices, and I've got a bunch of raspberry PI's acting as sensors, which are feeding into the application, which actually signs the things and integrates with the network. And I think that's probably my final message for you, would be that often when you're going into Ethereum or blockchain land, you're going into an ecosystem which has made some very conscious trade offs, like, we're going to prioritize security over everything else, so it's very expensive and it's very slow, or you're going into one, which is like, maybe we don't care so much about security, it's going to be fast and quick and whatnot. Whereas in Holochain you can start to have a broader set of options and you can decide yourself of, do you want strong security? Do you want easy usability? Do you want lightweight devices? Who are your users? How do you bring them? And it just gives me this breadth of expression as a developer, which I found really valuable. So, anyway, I'll be over here if you want to have a chat after this. Lovely to see you all and come see us all at the main event. Take care, bye.
