00:00:04.540 - 00:00:15.410, Speaker A: All right, guys, it is late afternoon, and you've learned so much. So I'm very excited to announce our next speaker. Let's give Jay a huge round of applause. Yeah.
00:00:23.850 - 00:00:47.810, Speaker B: All right. GM. Gm, everyone. Let's go ahead and get started. So, my name is Jay. I'm a co founder over at, at say, labs, and we are building the first paralyzed EVM. And today I wanted to take some time to talk about both what we're building and also go over some of the hot takes that our team has that might be a little bit controversial, especially at eat Denver.
00:00:47.810 - 00:01:26.106, Speaker B: So let's go ahead and get started. So, over the past couple of years, there's one thesis that we have started to develop, which is that the EVM is here to stay. 87% of crypto developers currently work on the EVM. So essentially everyone that's crypto native is an EVM developer. And if you basically ask them about moving to a new execution environment, let's say cosmosm, or any of these other execution environments, there's typically a lot of hesitation. And this comes from two major components. The first is, it's a pretty big security risk.
00:01:26.106 - 00:02:04.978, Speaker B: If you're comfortable with solidity and you know the internals for how the EVM work, it's pretty risky to go to a new execution environment. Let's say write a smart contract in rust, one small bug can result in your entire project getting drained. So it's super intimidating to go to a new execution environment. On top of that, there's also this idea of ethereum alignment, and a lot of devs feel kind of icky. It kind of goes against their values if they move away from the EVM. And this is fundamentally because the EVM is not just a tech stack. At its core, the EVM is an ecosystem.
00:02:04.978 - 00:03:01.402, Speaker B: It's not just the technology, but it's also all of the builders that are there, all the tooling that's been created, all the mindshare that's around it, all the research that's there. And because of that, we fundamentally believe that the EVM is not going anywhere. Definitely not in the near future, most likely not in the medium or long term future as well. The question then becomes, what is the current limitation of the EVM? Like, what is the biggest issue with the EVM? I'll make the argument that the biggest issue with the EVM right now is throughput, or more specifically, lack of throughput. So throughput is this general idea for how many transactions or how much work you're able to process per unit of time. And if you look at Ethereum l one, if you look at roll ups that are built on top of Ethereum, they're not really able to get more than 50 transactions per second that they're able to process. So what does this mean from the user experience and the developer experience? Well, from the user experience this means that you have to pay ridiculous gas fees.
00:03:01.402 - 00:03:30.814, Speaker B: For example, I'm sure a lot of you have been doing stuff on Ethereum recently. If you're trying to do stuff on the l one, if it's 100 gray for gas fees, you're going to be spending a shitload of money on gas. And look, no one wants to pay that kind of gas. That ends up resulting in most of the human population honestly just being excluded from participating on chain. And that's definitely not the world that we anticipate having happen on chain in the future. The other issue is it also becomes much more restrictive for developers. As a developer.
00:03:30.814 - 00:04:10.514, Speaker B: If you're constrained to a 50 tPS environment, then it becomes much more difficult for you to build the kind of applications that you would see in web two or in traditional finance. And it forces you to make use of anti patterns to be able to get stuff just working on the EVM. One example of this would be automated market makers. Like automated market makers, they do not exist in traditional finance. And the reason that they've started to become so popular, the biggest reason that they're popular on chain, is because they fit the limitations of what the EVM really allows you to do. So there's a ton of limitations that come with this low throughput. So then the question is, how do you actually solve for this? And that's when parallelization comes in.
00:04:10.514 - 00:04:56.670, Speaker B: So if you look at the landscape for EVM based chains right now, they're essentially all single threaded. So if you have 100 transactions that come in, you'll need to process every single one of them one by one. And this is super simple from an engineering standpoint to implement, but it's not performant, it's extremely inefficient, and it does not take advantage of modern hardware. Half of you are on your phones right now. Those of you that are on your phones, these machines have multiple cores, they're able to process multiple work streams simultaneously. And if you have single threaded execution environments, you're not really making the best use of the hardware that you have at your disposal. So when you paralyze the EVM, it unlocks greater throughput, and that helps enable lower gas fees in a much bigger design space for developers.
00:04:56.670 - 00:05:24.582, Speaker B: And that's kind of the inspiration for savvy two. There's a ton of different things that we've gone that we've ended up doing as part of save E. Two, we support the EVM, we've added in optimistic parallelization. We've created a new database layer, and we also have twin turbo consensus that I'll go over briefly as well. So if you look at how parallelization can be supported on chain right now, there's basically two ways the teams have done it. Approach number one is by having developers define dependencies. This is kind of the approach that you see with Solana.
00:05:24.582 - 00:06:03.240, Speaker B: When a transaction submitted, all of the dependencies need to be passed along. The second approach is by having the chain itself figure out what the dependencies are. So this ends up leading to a much better developer experience, but then it adds complexity to the chain because the chain needs to figure out how to actually do this. But if you are able to have the chain figure out what the dependencies are, that is a better end state to get to, because it makes the developer experience much, much better. And that's exactly what we've decided to do with, say we make use of optimistic parallelization. And a super high level idea here is you try to run things all in parallel initially. Then there will be conflicts for transactions that are touching the same state, and then you rerun those.
00:06:03.240 - 00:06:47.458, Speaker B: The nice thing though is when you start rerunning transactions for a second time onward, all of the state that they're touching has largely already been pulled into memory. So it ends up being much more performant because you don't really need to worry about needing to read from disk. So with this approach, we've been able to start paralyzing the EVM. There is one downside though, to paralyzation, and that is state load. So state is the data that needs to persist to be able to process any new transactions that are coming in and to also be able to generate state routes. And at a super high level, what state consists of would be two kind of broader ideas. What would be account balances? Like, I have ten say you have 20 say it also has smart contract state.
00:06:47.458 - 00:07:22.122, Speaker B: So let's say that there's like a uniswap smart contract. What are the actual liquidity pools that are there? What are the tokens that have been deposited? All that stuff needs to be persisted on full nodes, and it needs to be data that is stored on chain. The more throughput you have, the more transactions that are processed, the more state that actually gets created. And this is something you then need to account for. And when you have more state, the result of that is it leads to, first of all, more state storage that you need to manage. Like if there's ten terabytes of state, then most consumer hardware is not going to be able to just automatically support that. You need to start upgrading your hardware.
00:07:22.122 - 00:07:49.814, Speaker B: Then that leads to more centralizing forces because then you have people that are forced to run custom hardware to be able to run a full node, and then that leads to issues. So you need to be mindful about how much state gets created. And then the second thing is state sync. So when you start to run a new node, let's say that you want to start running like a validator for the network. You need to import all the states to be able to start processing new transactions that are coming in. There's ten terabytes of data that you need to import. That takes a lot of time, that could take on the order of hours, days, potentially even longer.
00:07:49.814 - 00:08:28.078, Speaker B: And that has pretty significant impacts on the network as well. You definitely want to be mindful about how to account for state storage, how to account for state sync. So to help with that, we introduced the idea of SADB. So SADB essentially has two major components. The first is a memory mapped IAVL tree, and the second is async writes to disk. The way that the memory mapped IVL tree works is it has three files that are there on disk, and you take the entire tree, you split it up, and then you put it into these three files. This is different than the approach that save you one had where the entire tree itself was just stored on disk.
00:08:28.078 - 00:09:26.562, Speaker B: And when you store the entire tree, there's a lot of extraneous metadata that needs to get stored. One example of this would be if you have an intermediate node that doesn't actually have a key value pair, guess what? There's data that needs to be allocated for storing these key value pairs. If you have a child node which is at the bottom of the tree that doesn't have a left child or right hash, guess what? You still need to store something for the left hash and right hash, and that ends up leading to a ton of unnecessary metadata that you're just tracking. When you have these three files that are stored on disk as part of this memory map file system, you're able to avoid all that extraneous metadata, which helps you get a 60% reduction in the state size, which is massive. That is a huge improvement on the amount of data that you actually need to persist. And when you have less data, that also improves state sync times, because suddenly if you have only 40% of the amount of data that needs to be imported, then it makes it easier to import that data. Additionally, if you have three separate files that you're writing to, then you're able to paralyze writes to these three separate files.
00:09:26.562 - 00:09:55.066, Speaker B: So that allows you to get roughly a twelve x improvement in state sync times as well. So that's massive for helping prevent state float. The other benefit of SADB comes from async writes to disk. So what happens with SADB is you have all this data that's stored in this memory mapped IAVL tree, which is these three files on disk. Then you pull as much data as you can into memory. Ideally the entire IAVL tree will be able to live in memory. If not, there's subsets of data that you can pull into memory.
00:09:55.066 - 00:10:51.250, Speaker B: And once you pull this data into memory, then you're able to generate the state route in memory itself. So typically in order to finish block execution, you need to import this state route, or you need to have some kind of state route that'll be used as part of the block header. And in the case of say, if you're able to have state route generation happen in memory, that makes it much, much easier for you to just kind of terminate block processing at that time. And then you can asynchronously write to a write ahead log and then also asynchronously actually write the data to disk. So this approach helps lead to a 287 x increase improvement in commit times, because you no longer need to actually wait for commit to happen to disk. You can just have it be in memory and then afterwards, once it's on the write ahead log, then you have disaster recovery. So it makes it much more efficient from that side, and that helps really improve the overall performance of the entire chain.
00:10:51.250 - 00:11:30.314, Speaker B: The other thing that we're continuing to use, which is there from Save E one already, is twin turbo consensus. So twin turbo consensus is say's novel consensus mechanism. And this allows us to get 390 millisecond time to finality, which makes it, I mean currently it is the fastest chain that is out there on mainet right now. And this will continue to be the case for Savy two. And this has single slot finality with an internationally distributed validator set. And the two big unlocks that we had over here to get to this is optimistic block processing and intelligent block propagation. So the way that optimistic block processing works is kind of highlighted in this diagram.
00:11:30.314 - 00:12:24.078, Speaker B: Over here, where normally what you would have is there's a block proposer who proposes a block. Other people in the network, they receive the block, and then from there they have two rounds of voting, a pre vote step, a pre commit step, and then afterwards they actually start to process the block. That is very inefficient though, because as a validator you receive the block, and then afterwards you don't do anything with the block for these two voting steps, which is completely unnecessary. So the idea behind optimistic block processing is that as soon as you receive the block, you can start concurrently voting on the block while also starting to process the block. And if you process the block, then you can update a candidate state instead of updating whatever the canonical state should be. If the block ends up getting approved by the network, then you can commit whatever that candidate state is. If the block is rejected by the network, then afterwards you can just discard the candidate state and there's not really any harm that's done.
00:12:24.078 - 00:13:15.066, Speaker B: So with this approach, you're able to get significantly improved. Time to finality. The other idea that we introduced was intelligent block propagation. The way that things normally worked before is that there would be some block producer who needs to send both a block proposal message along with the entire block of like, let's say 1000 different transactions across a network. For a validator to start voting, they need to receive all thousand transactions, even if all thousand of them are already in their local mempool, because there's already a PTP layer where these transactions are gossiped. So the current unlock with intelligent block propagation is what if you let validators try to construct a block locally before they start to wait to receive the block across the network. And what ends up happening is the block producer will just give a block proposal message, which will be an ordered list of transaction hashes.
00:13:15.066 - 00:13:48.586, Speaker B: Then validators will receive this ordered list of transaction hashes. Then they can start looking at their own mempool, try to construct the block locally. If they have everything, which actually does end up being the case a lot of the time, then they're just able to process a block immediately. Otherwise, if they don't have it, then they're able to wait to receive all the contents across the network. So from both of these approaches, we've been able to observe 390 millisecond finality, which is, I mean, that is the fastest that you can have that I guess any network has in terms of finality at the moment. Cool. So that's kind of where we're at right now.
00:13:48.586 - 00:14:38.682, Speaker B: I would say, I also wanted to go over some of the more controversial ideas that we have just based off debates that we've been having or like ideas that folks have been kind of sharing about, say. So the first idea that we wanted to go over is we don't think it's a good idea to blindly focus on the number of validators. And I think especially at eat Denver, this might be something that goes against what a lot of people believe in. But if you think about validators, how do you actually incentivize people to run validators? Well, there needs to be some kind of mechanism in place that whatever the validator costs are. So this would be the cost of the hardware, operational cost, opportunity cost, all of that is made up through some mechanism. This could be from foundation grants, as you see in some ecosystems. It could be from whatever the transaction fees are, plus potential mev redistribution, plus whatever inflation for the network is.
00:14:38.682 - 00:15:27.174, Speaker B: But it's expensive to run validators, and if you have 100,000 validators, then the network basically is paying the cost for these 100,000 validators to remain operational, otherwise people will stop running them. So it ends up being very expensive. And most of the time it's just like you're just redoing the same work. Like if you have 100,000 validators, there's 1000 transactions in a block. All 100,000 of these validators are literally rerunning those same transactions. And that is, I mean, it works, but it's not necessarily the most efficient thing to be doing. So if what you care about is decentralization, trustlessness and verifiability, I'd make the argument that it's better to have a smaller, sufficiently decentralized validator set where people are then able to consume zero knowledge proofs of whatever state transitions happen, to be able to verify whatever state of the chain they want using some kind of light client.
00:15:27.174 - 00:16:06.534, Speaker B: So what this could look like one approach would be block producers or other participants in the network. They're asynchronously creating zero knowledge proofs, and then light clients like you might just be running a light client locally. This could be something that is just part of metamask. You would receive a ZK proof, and then from there you're able to just verify the state of the network without needing to run a validator, without needing to run a full node. So it allows you to have a smaller validator set size, which makes the network more sustainable. It allows these validators to be potentially more performant machines, and it also allows people to have verifiability and trustlessness of the network. So I think that's a model that a lot of chains are going to move to towards in the future.
00:16:06.534 - 00:16:59.622, Speaker B: It's also interesting because a lot of the more recent launches that have happened for l ones, like within the past few years, almost all of them have had smaller validator sets, and I think a lot of them are going to move in this direction as well. The second idea that I want to go over is that validiums aren't full l two s. I think that there's a lot of folks that kind of view validiums as like the best of everything, where you're able to get external DA while also kind of being Ethereum aligned in a way. But if you're using a validium, then you're getting the minimum of the security between whatever the settlement layer is and the DA layer. So if your settlement layer is Ethereum, if your DA layer is celestia, then you're essentially getting the economic, or you're getting security of the DA layer and not getting full Ethereum security. So you're not really a full ethereum l two. At that point you're closer to being like a sovereign roll up or closer to being an l one than you are a true roll up.
00:16:59.622 - 00:17:58.470, Speaker B: And I mean, we kind of created this diagram to go over that, but roll ups are the only things that are getting settlement and DA from Ethereum. If you're not really getting DA from Ethereum, then you're not going to be fully ethereum secured and it's not really a true l two in that case. One of the other things that we've started seeing is there's a lot of focus on trying to improve discretes, maybe from things such as like asynchronous I o. And I would make the argument that instead of, I mean, you can definitely try to improve discretes, but I think there's going to be more gains that happen from trying to encourage in memory access as much as possible versus trying to focus on discretes. Because at the end of the day it's going to be impossible to get around discretes entirely and there is going to be an overhead that comes to reading from disk, and it's better to set up your mechanisms in a way to encourage in memory access as much as possible. So LRU caches do this implicitly. If you have data that some transaction is touching, and then this data results.
00:17:58.470 - 00:18:43.898, Speaker B: Like if this data gets cached, then future transactions that touch that same state, they can make use of that data so that is like one example of a very easy way to encourage in memory reads. Another example that I think is going to start becoming more common with multidimensional fee markets in the future is different gas pricing based off of whether you're doing in memory reads or if you're doing disk reads. So if you're doing in memory reads, then you will ideally have some lower gas fee that you need to pay, and you'll get penalized if you do disk reads. And I think there's very deterministic ways of doing this as well. For example, you could have like priority state where people have some kind of auction to bid on priority state. And then from there there's this priority state that is going to most likely get cash. And then from there you're able to just ensure that you're getting as much in memory access as possible.
00:18:43.898 - 00:19:30.834, Speaker B: So I definitely think this is going to be something that you'll see start happening in the next year or two from different chains that are considering multidimensional fee markets. The last idea that I'm going to bring up is that async writes right now are underexplored. We observed a 287 x speed up with SADB for commit times. And the reason that happened is because state root generation happens in memory and then you're able to move on and kind of have the rest of the writes happen asynchronously. And I think this is an idea that overall has been underexplored and it can definitely result in much, much better performance. And there's ways to ensure, like if you use a write ahead log and you have things getting written to disk afterwards, there's ways to ensure that there's disaster recovery in place as well. So it's very easy for the network to be reliable.
00:19:30.834 - 00:19:46.810, Speaker B: Overall, I think there's a ton of cool stuff that is going to be explored from safe side and. Yeah, that's it from my side. Thanks y'all. It's.
