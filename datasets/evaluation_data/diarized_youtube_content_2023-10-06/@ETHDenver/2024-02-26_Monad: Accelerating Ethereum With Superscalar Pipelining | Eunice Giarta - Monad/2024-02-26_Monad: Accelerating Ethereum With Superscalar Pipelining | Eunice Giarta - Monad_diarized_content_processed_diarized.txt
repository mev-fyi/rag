00:00:09.050 - 00:00:32.790, Speaker A: Hey, everyone. Oh, I can hear myself. Hey, everyone. Hope everyone is having a great Monday afternoon. My name is Eunice Guillarda, and I am co founder and COO of Monad Labs. And today we're here to talk about accelerating the EVM with super scale pipelining. Firstly, maybe we should quickly define what is superscaler pipelining.
00:00:32.790 - 00:01:03.806, Speaker A: So to do so, I have an example here. This example is a series of tasks associated with doing laundry. We're all familiar with that. This is doing laundry without pipelining. Let's say that for whatever reason, you have accumulated just a ton of laundry. You're doing your linens, you're doing colors, you're doing whites. The naive way of doing laundry would be to do them all, to do the full task for each load before starting the next.
00:01:03.806 - 00:01:29.702, Speaker A: So that would obviously take a really long time. Like if I did colors first and then moved it from the washer to the dryer, then folded, then put it away before then starting on the whites. This little example here shows that I would spend most of my night just doing laundry. We don't want to do that. We want to think smarter. So this is pipelining in the context of laundry. So here, I'm going to start on my colors at 06:00 p.m.
00:01:29.702 - 00:02:06.074, Speaker A: It's going to take about an hour. At seven, I move my colors to the washer, sorry, to the dryer. Now, the washer is free, so I have a free resource to use, and I can then start loading my whites. And that's what I do. And so, suddenly, a task that would otherwise have taken several hours and my entire evening is done in a much more efficient manner. Superscalable pipelining effectively is efficient scheduling of your available resources. All right, so what does this mean in the context of Monad? I'm going to talk about two optimizations that Monad employs.
00:02:06.074 - 00:02:37.994, Speaker A: The first one is asynchronous execution, and the second is parallel execution. Yeah. All right, so first one, asynchronous execution. So a little bit of background. Today, most blockchains have interleaved execution. What does that mean? It means that for every block, both execution and consensus need to happen before a block can be finalized and before the next block can begin. So consensus, however, is really expensive.
00:02:37.994 - 00:03:05.110, Speaker A: Like, you have nodes distributed across the world and you've got messages that need to be sent from one node to the next. It just takes really friggin long time. Which means that execution, since it needs to fit within the bounds of a given block, only has a fraction of the block time to complete. So let's take an example. Ethereum. Ethereum has a twelve second block time. This little block here will be our 12 seconds.
00:03:05.110 - 00:03:36.100, Speaker A: Within that 12 seconds, only 100 milliseconds is actually dedicated to execution. Today, that is a tiny budget, just barely 1%, which means everything else is dedicated to consensus. What we're trying to do, I'm sorry. More examples of interleaved execution here. So here are our series of blocks with both execution and consensus. Execution, the darker color, consensus, the lighter color. And here is how blocks are completed over time.
00:03:36.100 - 00:04:23.710, Speaker A: However, with asynchronous execution, which is what Monad does, we separate out consensus from execution so that blocks are no longer. Excuse me. Consensus is no longer a limit for the ability to execute transactions. How we do asynchronous execution is by deferring the execution. So a block is finalized with consensus, and execution happens independently. So here's the series of blocks with just consensus and then execution that happens in a deferred manner. The reason why this can exist is because the ordering of transactions is canonical as soon as blocks are produced and consensus is confirmed.
00:04:23.710 - 00:05:14.174, Speaker A: Here's our pipeline. Look where now we can allocate consensus whenever consensus is available, not taking in consideration execution and vice versa. Now, it may not really look like that much of a difference here, but what we now have is a greater opportunity to fill block space with execution in its entirety. So that basically means much more execution bandwidth per block, so more room for more transactions per block. All right, so, in summary, nodes come to consensus prior to execution, and execution will run in a deferred fashion. So execution for block one happens while consensus happens on block two. A couple of notes here.
00:05:14.174 - 00:05:52.094, Speaker A: Since it is a little bit different than the way other blocks, sorry, other blockchains create blocks. But we have thought about ways in which there is a delayed Merkel route to confirm the state executed across all nodes, as well as something called a. And now I'm blinking on it. We have a form to prevent spam transactions as well. Carriage costs, I believe, is what it's called. All right, now, part two. Oh, I'm already at part two.
00:05:52.094 - 00:06:40.010, Speaker A: I think I'm going really fast here. I'll slow down a little. All right, so the second one is parallel execution. So today, as we had already discussed, transactions are linearly ordered, and they are also serially executed. So there is a canonical and prescribed ordering for all the transactions within a block. One comes before two, which comes before three, so on and so forth, which is fine, but as we had noted, it's a little bit inefficient. So the goal is, can we do this faster? What is it? Or what is this? Can we get to the end state, the eventual end state of execution, as if we were executing serially faster by doing work smarter and not harder.
00:06:40.010 - 00:07:32.480, Speaker A: The way that Monad does this is through parallel execution. Which is what? It's a simple algorithm that has a lot of fancy terms. One example is OCC optimistic concurrency control. Another one you may have heard is STM software, transactional memory. Complicated terms, but it all boils down to just doing the intuitive thing, just realizing my cursors over there, doing the intuitive thing, which is to do things naively and optimistically. So essentially we're going to try to execute as many transactions as we can at the very same time. And what we do is we check the assumed inputs and the outputs of these transactions prior to confirming them and updating state.
00:07:32.480 - 00:08:26.520, Speaker A: By doing this, we can check to make sure that the transactions and the results are valid, and also reschedule transactions in the case of conflicts. And then, yes, eventually committing back to state. Okay, so this here is a very basic visual of transactions we're going to zoom into. And for the sake of illustration, these lines right here will denote some dependencies. So we'll see here that the first transaction in this array of transactions is a dependency for the third and a few others down the stack here. Basically this happens when what? When the same smart contract is called across different transactions or the same account balances it. Sorry.
00:08:26.520 - 00:09:17.670, Speaker A: Okay, so we're just going to zoom in, working backward. We're just going to zoom in to the first five here. So we have transactions 12345. The way that serial execution handles this is by scheduling them all sequentially. Right? Like the first one needs to be fully executed before the second one, before the third, fourth, and fifth. So it's not the most efficient, as we know, because this is exactly the same as doing a full load of laundry before going on to the next laundry pile that you have in parallel execution. We'll discuss the intuitive algo here.
00:09:17.670 - 00:10:04.790, Speaker A: It really depends on the number of resources that you have available to execute transactions. For this example, we're just going to assume we have three different resources available to transact. And again, the goal is to execute and shove in as many transactions as we can at the same time. So here we're going to do so in order, because again, we're maintaining the linearity of transactions and executing transactions, one, two, and three, all at the same time, across these three resources that we have and as each of these transactions complete right now we have transaction three. They're completed. We will add the pending results to a queue to then be committed to eventual state. All right, so in this example we have transaction three that completed first.
00:10:04.790 - 00:10:39.520, Speaker A: So we have that pending result. And you can see here is our pointer. Oh, you can't see it. You can see then that the next available transaction is slotted in the resource that is available. So t four is then starting to be executed on that open resource. Now we have pending results for each of these as they come available, and scheduling that happens for outstanding transactions for when these resources are available. Okay, great.
00:10:39.520 - 00:11:25.322, Speaker A: So we're going to zoom in now on the pending results that we were talking about, because again, we're doing things very naively. So we can't just blindly start committing transactions in the case that there are conflicts. So here's an illustration. You recall in our example that transaction three is dependent on the results of transaction one. So what we did previously is schedule all of the work at the same time. And so we're recording the presumed inputs of all of these transactions along with the expected outputs of these transactions. All right, so we don't really care about two, four, and five because they have no conflict.
00:11:25.322 - 00:12:07.082, Speaker A: They'll be able to be merged and committed to state appropriately. But anyway, let's say that I am the commit queue. So I'm taking a look first at the pending results of number one, confirming that the inputs match what I have in committed state. So right here I start with 1000 USDC in my account, and I plan to send 100 USDC to Alice, who starts with zero. So what I do is make sure that my assumed inputs match what exists. Great, they do. I can go ahead and commit this and update committed state the second transaction.
00:12:07.082 - 00:12:45.450, Speaker A: Fine, because again, there are no collisions here. Now we get to the pending result of the third one. Again, recall that we optimistically executed it. And so our inputs, our assumed inputs are now actually out of date, because our committed state has been updated by the output of transaction one. So I can't commit this because then it would throw the real execution state out of whack. So what I can do is send this wrong way is send this for rescheduling. So we'll go back to our queue here.
00:12:45.450 - 00:13:33.646, Speaker A: And now I am going to be rescheduling transaction three to the next available resource, which is the middle one right there. You'll notice though, that the time that it takes to reexecute transaction three is much smaller than the original one. Why well, it's because we already have all of the memory required cached. We've already been working with it recently, and therefore it's very easily accessible. And what I mean by this is memory that you have to fetch from SSD or a hard drive is multiples more than fetching something that's immediately in memory and in RAM. So anyway, quick reexecution. Great, so now we have a revised input and a revised output.
00:13:33.646 - 00:14:25.420, Speaker A: Does it match? Yes, and now we have updated the state, and that is my example. Okay, so I'm not sure what this slide was for, but anyway, some takeaways here. Again, parallel execution is to optimistically attempt to execute as many transactions as possible across all the open resources. It produces a lot of pending results as a result of inputs and outputs. And what we're trying to do is confirm the data that the input data for a transaction that is up for commitment matches what is available or what is already committed to state before finalizing. If there is a conflict, easy, we just reschedule immediately. And reschedules are generally cheap because data is cached and available.
00:14:25.420 - 00:15:01.494, Speaker A: This is a simple and a compatible way of boosting the number of transactions that can be executed at any given time. No access lists are required, which means completely backward compatible. And I think that pretty much wraps it up. So, superscaler pipelining. The two things that we highlighted today within execution are asynchronous execution and parallel execution. I hope you guys enjoyed and let me know if you have any questions. More doesn't happen.
00:15:01.494 - 00:15:22.020, Speaker A: I don't know if this is like there's an opportunity for a Q A session, but happy to do so as well. Give me a follow on X or Twitter if you'd like, along with our main account, and a quick community artwork featuring Keoni's PFP there for accelerating the UVM. Thanks so much everyone.
