00:00:00.090 - 00:00:16.842, Speaker A: Fabian. Hello, everyone. Great to see some familiar faces here. Great to see some new faces here. My name is Fabian. I am the co founder and CEO of Kive. We started the journey, I would say, a year ago from now, roughly.
00:00:16.842 - 00:00:38.140, Speaker A: And yeah, this talks about how to fish new store and validate data streams using kive. So one of the. Oh, sorry. The talk is going to be divided into three parts. First, we're going to do a quick overview about what is Kaive, how can we use. Next up is a quick demo, 10 minutes, and after that we jump into a small q and a session. If you guys have any questions around what we do.
00:00:38.140 - 00:01:56.666, Speaker A: So the fundamental problem in the blockchain ecosystem right now is there is no good way right now to store and access data because all the nodes are storing the, sorry, are storing the data very condensed at their own thing. And this makes indexing and retrieving large amounts of data very, very tricky because actually we don't know where the data is you are looking for as a developer. It could be some state, it could be some transaction, it could be some value transfer. And of course this leads to the rise of indexing services, which then of course need to take the data from a node which has it stored decentralized, move it over to a more centralized solution, and then is able to kind of retrieve it from that front. Same problem goes in with backups or snapshots, is if you're a foundation and you're running a protocol, it's kind of tricky for you to take a snapshot of the current state of your blockchain, storing it, as I said, somewhere locally, somewhere centralized, and then bring it out to your node operators to be able to bootstrap the nodes from there? Because why should the node operators trust you? Why should they know that you might not have, I'm sorry, modified some state or something like that. So it's really kind of a trust problem you're creating there by having a decentralized solution, but then putting the data access over to a centralized solution again. There's also no nice way right now to cache state transitions.
00:01:56.666 - 00:02:52.702, Speaker A: We see this happening with all those big blockchains really kind of growing up to infinity. I think Solana is a great example there, where there are only actually a couple of endpoints user developer can use to actually get access to the old kind of block zero data. And it's really kind of a threat that we will start losing the data if it's happening. We also see this on EIP four four and ethereum, right where we need to start this enter of the purge just because basically the nodes have to deal with all this data and they really don't know where to store them. And then it's also to economic problems, because if the mining flow rises and basically don't know then if my node is going to be economic anymore, if I need to maintain this big data center just for all the data, then what's going to happen is I need to shut down my node right in the end, leading from a big variety of decentralization down to just a few centralized nodes from node operators which are able to basically work with data. And this also goes in well with the fourth one here. The cost of distribution is expensive, right? We do want this data storage of redundancy.
00:02:52.702 - 00:03:24.294, Speaker A: We do want to see the data being stored on as many nodes as possible. Also on the other side, it's quite expensive storing a redundancy. So this is the reason, basically, why it's becoming a growing challenge for indexers and for blockchains in general, right, to scale, to be able to hand out the data to developers. And basically, in the end just leads to scaling issues, not only on the web three node side of things, but also on the web three data access side of things. I'm sorry. There we go. And this is the reason why we have developed Kaive.
00:03:24.294 - 00:04:32.946, Speaker A: And what Kaive does, it takes blockchain data, routes it through a decentralized network, and then stores it on decentralized storage. In our use case, we're using ARI for that. And basically the way it works is if you're a developer, you're going to set up one of those storage pools and you can connect it to any blockchain, any data source doesn't need to be really web three data source could also be an API you want to archive, for example. And then the way it works is you have those storage pools which connect to the API, which have a set of uploaders and validators where the uploader basically relays the data onto permanent storage. And then the validators, they take down the data, run some logic against it, check that the actual data you want to store is actually correct, and then is also able to retrieve the data out of it. So it's kind of this full blown cycle where you, as a protocol, as a developer, really don't need to worry anymore of like how do I need to worry about the storage for all those nodes? Because basically you get it out of the box from here. Kaive started, as I said, a year ago, John and I my co founder sitting all the way behind there working on a bounty for bridging the data from Polkadot over to Arweave, because they set up a bounty for that.
00:04:32.946 - 00:05:12.878, Speaker A: And we really thought that's a cool thing to get started. And then we showed it to other protocols, got some great feedback to it, and then suddenly the script turned into more scripts and we had a project, we had first integrations out, then we did a pre seed round, did a small test and release. I would say in like Marchish time, grew the community, improved the UI and UX around it. We are right now rolling out a big update where the demo will be actually about where we introduced the protocol governance. Because right now what we've been doing is we as a team, we needed to maintain some debugging purposes and other things going on. So now with the new governance we're introducing, things are going to be highly decentralized. Then we're rolling out the incentivized testnet later down this year.
00:05:12.878 - 00:05:47.420, Speaker A: And then we also, of course, focus around the main net launch. By the end of this year. We have integrated with, I would say any major l one and l two out there, kind of giving them the offer to store their data, making it possible for them to also retrieve it without to worry about which indexing service they're going to introduce. They don't need to build any in house indexing service or any in house data solutions. All right, let's jump into the demo. I can show you an overview about what it looks like working with Kive. There we go.
00:05:47.420 - 00:06:23.612, Speaker A: So we set up a demo environment. If anyone wants to check it along on their local device can go to Kiveinterface demo, herokuap.com. I'm logged in here with one of our addresses and you can see it starts off with a very nice dashboard where you can see the activity user user would have done. You can see all the nodes, you can see your delegation. Kive is a delegated proof of stake model to make sure that basically the data which uploaded are also economically secure. And then those are the storage pools you can see here, just for the demo purposes, we rolled out the EVM runtime. So we have Moonbeam here, Avalanche, Moonriver, Aurora, and Evemos.
00:06:23.612 - 00:06:56.880, Speaker A: The way it works is if you would be a developer, you would write one of those runtimes which basically just specify how do I connect to an API, how do I get the data? We will look into this later as well. And then you would just deploy the pool here through the governance, and then the indexing process would start. You see it here on the moonbeam pool. Very well. You can see how many percent of moonbeam has been indexed right now we had 45.1%. We are usually takes region around one to two days to archive a whole chain. You can see how much the storage size on Avis we're storing there.
00:06:56.880 - 00:07:30.808, Speaker A: You see the costs, of course, here it's set to zero because we are in our demo environment. You see the pool funders and the pool validators. The pool funders are very interesting concept we have introduced. So the way why a pool requires funding is, let's say you are, for example, the Moonbeam foundation would be actually interested in storing your data. Then you would for example, fund this pool to make sure and start the archiving process. Then what would happen over time is this funding would get paid out to all the nodes as a reward for uploading the data. If a pool would ever run out of funding, you would have the problem that it pauses.
00:07:30.808 - 00:07:54.656, Speaker A: And so everyone who builds indexing solutions and is kind of dependent on the data we are storing have then trouble accessing it. So everyone is highly incentivized to keep the pool funding up and running. Here in a node section, you can see all the nodes. Right now we have three active nodes running. You see their self stake, you see their voting power. I think that's pretty self explanatory coming from a proof of stake model over here. We could now delegate to a node.
00:07:54.656 - 00:08:37.260, Speaker A: You get some rewards for it as an end user if you would not run your own node. And basically very overview, things like that in the activity, you then actually see the stored data. So you would see the RWEF transaction id here, where it has been stored, you see which node was the uploader, you would see the votes, you would see the bundle sizes, you would see the time, and if it's valid or invalid. And actually the bundle size is quite an interesting thing. So what happens on our side, to make the data storage more efficient, we take the data we are storing, bundling all the blocks up into one big data bundle, encoding it and compressing it to make sure it's stored very efficiently. And then you can see the bundle size as well. And you can see actually how it starts to shrink the more you get towards the live tail of the chain.
00:08:37.260 - 00:09:04.556, Speaker A: So what happens if the nodes would be running here? You could see that we would basically always be up to like 99.9% of the chain. And then we would store the latest data routed through the system, and then it would be indexable for our indexing service. And then of course, in our current app, if you would look in here, you see, looks exactly the same. There are just some different runtimes out here. So we have a cosmos runtime as well. We have the Solana snapshot runtimes and some other exciting stuff happening here as well.
00:09:04.556 - 00:09:36.512, Speaker A: But it's exactly the same concept. If you would now want to access the data, we rolled out a side product called Warp which allows you to access the data Kyber storing through GraphQL for developers. So here I'm connected to the Moonbeam interface for that. And as you can see here, it's rather straightforward if you're a developer, to understand what's happening here. So we are querying a moonbeam block where we filter for the field where the number is one. And then if you would execute the query here, the result is already there. You can see you now get the first block of Moonbeam.
00:09:36.512 - 00:10:12.380, Speaker A: And this all comes basically out of the box. By using Kive, there's no need to use other indexing services. Another cool feature we have introduced with Warp is that it's actually possible to query multiple data source endpoint from a single query. So you can see here, this query would query the first blocks on avalanche, Moonriver and Moonbeam where the number from the block is less than ten and greater than or greater than five. I'm sorry. And if you execute it here, you can see, of course, we get 12345 and so on up to nine from avalanche. It goes on with Moon river and then it goes on with Moonbeam at latest.
00:10:12.380 - 00:11:11.876, Speaker A: So you can see here that this is very revolutionary because you can finally now use one API to query multiple sources. And this really comes in handy in the crosschain world. If you want to build, for example, a crosschain token bridge can now have two smart contracts which then just request both APIs from the two endpoints to really make sure that's fully decentralized. There's no custody involved anymore, there's no centralized server, sorry, there's no centralized server required anymore to make this happening. This is kind of what we try to do with kive is really being able to connect the whole web three space through this one API, making sure that all the traffic is routed through us and stored so anyone can really recover in case there are any problems or in case there's anything else going on. If you would now be a developer and you would want to know, well, how can I build my own kive implementation? It's actually very straightforward. So what's happened is we as a team have built just a core package here where you would just use the Kaive class form and then you would just extend it with the functionality you would need.
00:11:11.876 - 00:11:51.170, Speaker A: So in this case we're looking at the EVM integration and there are only really three functions you would need to think of. The first function would be how do I request my API? So in this case what we are doing here is we are requesting the API by the height, right? So we just say, well, from this side to this side we want to do the request, store them in the nodes local database. Next function is creating a bundle. So how are you going to bundle all the data together? How are you going to do the encoding of it? And we have a bunch of helper functions to also make that possible. And then what happens is the node automatically submits it to the KaF network, deals with all the voting, submits the proposal and so on. So there's really nothing from a developer perspective to worry about. It all comes out of the box.
00:11:51.170 - 00:12:36.700, Speaker A: And then the last function you would need to worry about here is the load bundle function. It's like how does my node interpret the bundle? How does it work with it, how does it deal with it? It's also pretty straightforward. Happy to share some more details with it. And this was basically the main overview about what kive is, how you can use it in your application, how you can use it to store data. It works with any web, two, web, three data source. And yeah, we have a bunch of use cases starting from, as I said, like multichain block explorers is a cool thing you can build with kive going over to the cross chain, going over to data storage things, we really try to cover the whole kind of web three space. I would now like to hop over into a quick open q and a, q and a session.
