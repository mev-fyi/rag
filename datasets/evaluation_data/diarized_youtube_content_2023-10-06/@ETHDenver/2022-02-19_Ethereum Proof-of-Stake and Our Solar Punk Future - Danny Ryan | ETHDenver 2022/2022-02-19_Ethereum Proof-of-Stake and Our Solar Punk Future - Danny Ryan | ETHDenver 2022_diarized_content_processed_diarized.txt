00:00:03.690 - 00:00:29.682, Speaker A: Okay, cool. I'm Danny Ryan. I work at the Ethereum foundation on Research and specification writing and a lot of coordination around major consensus upgrades for Ethereum to make it more sustainable, scalable and secure. So today I want to talk about the transition of Ethereum to proof of stake. This is this. The subtext is a nod to Kevin. Great conference yesterday.
00:00:29.682 - 00:01:10.980, Speaker A: Really appreciate it. Our solar punk future e Denver 2022. Thank you for having me. This is how a lot of people conceive of Ethereum today. This isn't like totally 100% accurate, but it's not that great. The consensus mechanism of Ethereum proof of work is archaic at this point in time and needs to go. So after the merge, which is the transition from proof of work to proof of stake, we're going to be in an environment much more like this, a blockchain, but in a cooler, more sustainable, more secure home.
00:01:10.980 - 00:01:53.946, Speaker A: Is Ubuntu telling? Okay, it's done. There was an error up top, but I think we're good. So the first thing I want to talk to you about is to give you an intuition for what does it actually mean? What is the merge? What is this transition to proof of stake? How can I think about it as an Ethereum user, as an Ethereum community member? And what I want you to get from this is really from your perspective, other than having a more secure, more sustainable home, nothing changes. That's excellent. So this is what Ethereum looks like. After the merge, we have the consensus layer. We have this thing giving Ethereum a home and giving it security, and we have the execution layer.
00:01:53.946 - 00:02:52.900, Speaker A: That's what we call the environment that most users and most community members interact with. And the execution layer is where state is, where the transactions are and where all the fun stuff happens. So Ethereum, after the merge, this is the model we're working on today, so we can drill into this. After the merge. It's going to look like the execution layer, the things that you know and love, got the state route, got the base fee for 1559, got transactions and NFTs and all the mad stuff you all do in that layer. And then in this outer shell, which most of the time you don't even have to think about, how often are you thinking about the proof of work difficulty? Probably very minimally, but we have this outer shell that's not proof of work anymore, but is the proof of stake consensus layer what we call the beacon chain? And this has all sorts of fun stuff like attestations and deposits and exits, sometimes some slashings when people do bad stuff. And really this is what kind of is the outer shell of the execution layer that people know and love and people interact with.
00:02:52.900 - 00:03:26.762, Speaker A: So after the merge, this execution layer, really from the perspective of a user, it's very unchanged. It looks exactly like we know and love. This is what the chain looks like today. It looks very similar to what the chain is going to look like after proof of work shell, this kind of outer security element, and then the execution layer embedded. And this is what the consensus layer looks like today, which the proof of stake consensus layer is live today. It's been live for 14 months. In 2020, December.
00:03:26.762 - 00:03:44.050, Speaker A: How many years has it been? The pandemic's really screwed up time in my head. But the beacon chain launched and it exists, and this is what it is. It's essentially this consensus mechanism. It's coming to consensus on itself and is primed. You notice that there's some extra space in here. That's what we're going to be leveraging in the merge. We're going to be leveraging that extra space inside the consensus mechanism.
00:03:44.050 - 00:04:21.886, Speaker A: And so really what Ethereum is today is the unification of these two systems running in parallel. So when you're hanging out with Uniswap or when you're trading some crazy NFT, like you're in this proof of work chain today, and you're being secured by the traditional mining proof of work mechanism, and in parallel, all these validators are hanging out. They're hanging out on the beacon chain, they're building the beacon chain, they're securing the beacon chain and they are priming it. They've been working their asses off to get it ready for you. So I want to give you an intuition for the merge. The merge isn't that scary. The merge isn't that crazy.
00:04:21.886 - 00:04:57.542, Speaker A: The merge is continuous. The merge is for and by users and gives them continuous landscape to ethereum, the beautiful thing that we have. So we have these two chains right before we have the proof of work chain, we have this beacon chain, and they're running in parallel, but at some terminal condition, at some difficulty that the validators coordinate upon, they're watching the beacon chain. The validators are watching the beacon. Sorry. The validators are watching the proof of work chain. And at this terminal condition, they all point to the execution layer inside of that proof of work chain.
00:04:57.542 - 00:05:49.610, Speaker A: And the next time something's built, instead of being built by the miners, it's built by the proof of stake validators. And so this dotted line, this subchain inside of the chain continues forward and so all of the user APIs except like get difficulty because that becomes meaningless at that point. All of the user APIs continue to exist. All of the state continues to exist at point a when you're in the proof of work chain, and at point B when you're in the proof of stake chain, that just looks like the building of another block, that just looks like the continuation of your NFTs. That just feels like, did my transaction get in over here or transaction get over here? So that is what the merge is. It's on some level an oversimplification, but on another level this is literally exactly what's happening. And this continuous heart of Ethereum, the execution layer, continues forward just in this new secure and sustainable environment.
00:05:49.610 - 00:06:15.486, Speaker A: So this is actually what Ethereum looks after the merge. It actually is what it looks like before the merge. And actually half of those blocks could be proof of work blocks and then the further half could be proof of stake blocks because it feels, it looks continuous and then shrouding. But we can talk about that later. So that's a mental model for the merge. That is what's happening. That is the final we've been talking about, proof of stake.
00:06:15.486 - 00:06:40.140, Speaker A: It's so close. It's so close, it's so close. It literally is so close. And that's what we're going to talk about now. So over the summer we had this really fun event, a development workshop called Amphora where we got ten client teams and a bunch of developers in a room to take the merge from specifications and prototypes to something that begins to look like production. This was like a very exciting moment. We were all in the room together.
00:06:40.140 - 00:07:25.866, Speaker A: We had like t -15 minutes to this final dinner. And the transition happened on this testnet and everyone was very excited, even Vitalik's clapping there in the middle. Very good time that we called amphora, although vetted and began to put all the machinery in place for all these know there were things we learned there. So we had a next iteration of specifications and the next iteration of productionizing this. And that's what we called Kinsugi. Kinsugi is the art of taking, well, maybe smashing pottery wear and then making it more beautiful with gold. I don't know if you're actually supposed to break it.
00:07:25.866 - 00:08:00.450, Speaker A: You're probably supposed to find broken pottery and then glue it together with gold. But it's pretty nice. So the idea is all the pieces coming together for this thing. So at the end of mid December, the Kinsugi testnet launched. This has been like a major excellent effort. Lots of community members engaged with testing and began to really put their hands on this new iteration of the software to look, see and feel how this thing works. Interjection this is what an Ethereum client looks like post merge.
00:08:00.450 - 00:08:54.626, Speaker A: So what I call an execution engine, you might think of as an ETH one client, or you might think of as a proof of work client. Call it guess, call it Nethermind, call it Aragon, call it Besu, it's probably another one. There's many of them. That becomes an execution engine that becomes like its purview becomes state, it becomes transactions, it becomes that layer, it drives the thing. Whereas the things you think about as an e two client or a beacon node or a proof of state client that becomes this, like the brain of the operation. So instead of proof of work driving, the beacon node drives the execution engine via this engine API. I won't get into too many details around it, but in the future it's really the unification of these two pieces of software that do very complex things in isolation, but together they come together to be a proof of stake.
00:08:54.626 - 00:09:41.746, Speaker A: Ethereum I mentioned earlier, like the user APIs, they don't change. That's because if you're running Geth, you're still running Geth and you're still having access to those ETH APIs, the interfaces are still able to use these things exactly as they did before. I bring this up because this engine API, there's something the, although it is simple, it's one of the most important and critical parts of this whole thing. So I just wanted to give you a little problem we ran into in Kintsugi that birthed the current sprint called kiln. So in Kintsugi we had this bit of a semantic mismatch between what we thought was correct and reality. So there's two key methods in the engine API. One is execute payload.
00:09:41.746 - 00:10:27.890, Speaker A: So I'm the beacon chain and I have this bundle of execution goods. So like transactions and other things related to the execution layer. And I pass it to the execution layer client to the execution engine, and it will execute it like it does today when it gets a block from the network and it will get the post date and it will pretty much say, is this valid? And am I willing to incorporate it locally? And then there's this other method that's very important called fork choice updated. And we think of a blockchain. But what is a blockchain? A blockchain is the canonical route through a block tree. A block tree can be any number of blocks in this hash chain structure. But one thing that we have to do is we have to use the fork choice to find the tip.
00:10:27.890 - 00:11:17.140, Speaker A: And the tip then defines what is canonical and defines the canonical history. And in proof of work, this fork choice is very native to the difficulty into mining, whereas in proof of stake the consensus layer dictates via its own algorithm to the execution engine what its fork choice is. So the interplay between these two methods was subtly off in Kintsugi and led to some interesting scenarios. So this is the normal blockchain, there's no forking, we have a head. It's very easy. Sometimes we get alternative branches and this will build up into the issue with the semantics here. But sometimes we have an alternative branch, it hasn't overtaken the head, clients will incorporate it in the block tree and do certain things.
00:11:17.140 - 00:12:21.270, Speaker A: That alternative branch could continue to be grown. Maybe it doesn't overtake the head yet, but then finally all of a sudden that alternative branch for some reason took over the head. The interesting thing about the semantics of execute payload before fork choice updated is sometimes I have this forking and I have this alternative branch and it looks like a block that probably could be good, but execute payload was saying to the execution engine, execute now. And so that was actually causing sometimes in weird scenarios like undue burden and undue load on the execution engine. Because the execution engine often are designed today with proof of work to not actually execute this alternative branch until it actually overtakes it with difficulty. Then it finally executes all those blocks, sees if the thing is in fact valid, and then sets that as the new head. Point being is only when we had chaos in a testnet did we realize we had these semantics slightly off and we're inducing in certain forking scenarios undue burden on the execution engines.
00:12:21.270 - 00:13:00.294, Speaker A: Thus not quite back to the drawing board, but modified here. So the semantics, more purely on the execution engine are not execute payload every time there's a new payload, but instead insert it into the block tree. It's new if it overtakes the previous head by a fork choice updated, or if it builds on the canonical chain, almost certainly execute. So Kiln was born out of these slight, we're like so near production, but slight modification of the semantics to handle these things more optimally. And by saying more optimally, I mean very importantly more optimally. Thus Kiln. We're in kiln right now.
00:13:00.294 - 00:13:55.730, Speaker A: This is the final hardening. This is where we're taking these specs. We've run through multiple sprints, we've been battle testing them. And in this final sprint, we're building up devnets, and in a few weeks, we'll be inviting the community to a long standing testnet to really test their shit before we fork gorely, before we fork Sepolia, and whatever testnets still exist before we finally kill proof of work. So when this is a fun screen grab from amphora entered proof of stake stage. Remember that picture where we're all like, happy and clapping? That's because of this tiny little piece of the screen. So when I've been working on this for quite a while, I honestly, when I jumped into this thing towards the end of 2017, I thought it was right around the corner.
00:13:55.730 - 00:14:24.960, Speaker A: I thought it was right around the corner. I believed it and I really believed it for a long time. But it's really actually very close. The difficulty bomb, which has been our tried and true friend for many, many years, we've diffused him, we've added him back. We've diffused him, we've added him back. I think he's going to be impeccably timed. The difficulty bomb is set to go off to begun to be felt sometime in June, sometime in July, somewhere around there, and then to ramp up from there.
00:14:24.960 - 00:14:49.320, Speaker A: That is the target. That is what all of these client engineers are working on. And I do believe that that is very realistic. And someone asked me to say this, I think I will say it if it does not happen this year. Something insanely catastrophic that I did not expect showed up. Something fundamental, something crazy, broken, which that's not where we're at in this cycle. I do not expect that.
00:14:49.320 - 00:15:28.050, Speaker A: So you can quote me on that. I'm going to get a bit into the why. I think I made way too many slides for the amount of time I have proof of stakes, more secure, more sustainable, more scalable. Proof of stake actually isn't more scalable. That's why we're kind of like, moving in this direction, because proof of stake allows us to design more scalable mechanisms a bit easier. So you don't get it for free, but you get the other two immediately. Lack of economies of scale, highly liquid and available pure function return on capital, essentially the ability to take ETH and stake, it is much more pure and easy, or the ability to even acquire that capital.
00:15:28.050 - 00:15:57.210, Speaker A: To participate in the crypto economic mechanism, compared to proof of work, is very much simpler function. Whereas to really participate in proof of work, I need economies of scale. I need to be entrenched in hardware cycles. I need to get the hardware before everyone else. I need to have special energy. There's all sorts of things that the more capital I have, likely the more return I can get in proof of work, where that is very much a diminishing thing. In proof of stake security, higher security margin.
00:15:57.210 - 00:16:39.340, Speaker A: So we have all this crypto economic capital in proof of stake and proof of work that secures, essentially, people show up with their capital, put it in, convert it into the crypto economic asset of that consensus mechanism, call it mining harbor, call it the native asset itself, ETH, and you post it essentially collateral to participate in this game and potentially be rewarded or potentially have some losses. So on top of that, there's some sort of issuance, some sort of return, and also transaction fees. And in proof of work, the security margin and what you can kind of dictate is essentially just this return. So you have a carrot, which is great. People show up for the carrot. I think it's tried and true. Been doing this for quite a while now.
00:16:39.340 - 00:17:11.682, Speaker A: Whereas in proof of stake, it's not just the return. The crypto economic capital itself is at risk. The protocol cannot go burn your mining farm down. Somebody probably could, but the protocol cannot. And thus, on the left side with proof of work, the only thing you really have is the carrot. Whereas in proof of stake, there are attributable faults where if you try to construct alternate realities or you try to contradict yourself, your capital can't actually be at loss. It's actually at stake.
00:17:11.682 - 00:17:44.820, Speaker A: And thus you have the carrot and you have the stick. That's an intuition for why these things are more secure, why proof of stake is more secure. Better emergency recovery. In proof of work, once you're at 51%, it's all over. Like, they literally own the chain, they own you. And whereas in proof of stake, because of the stick we have here, you can conduct an attack, but then your capital is literally burned, it's literally lost. You have a one time attempt at that, and then everyone loves burned ether, so people might celebrate after.
00:17:44.820 - 00:18:18.990, Speaker A: Stop spawn killing us, please stop spawning. Better emergency recovery. This is kind of the same thing. The crypto economic capital is either burned or in the event that you have, like, some 70% censuring majority socially, you can intervene, whereas again, in proof of work, the only thing you can do is really change the fundamental algorithm which bricks the good guys and the bad guys. There's not as much emergency recovery there. Sustainability, that's a big one. These are smokestacks.
00:18:18.990 - 00:18:43.218, Speaker A: We don't want the future to look like that. There's a much better and interesting and fun future. So proof of work I honestly believe will become, after the merge, seen as kind of an archaic technology. And I don't think it's going to be socially tenable for much longer. This is what we're going for. And I have some napping math to show you. So this is from Kyle McDonald.
00:18:43.218 - 00:19:01.742, Speaker A: There's a number of these estimations on how much energy proof of work in Ethereum uses. This one I've been told good sources is pretty good. So call it 25, 26 terawatt hash hours per year. That's a lot. That ranks up in at least definitely like top 100 countries. It might be like top 30. I need to look.
00:19:01.742 - 00:19:24.034, Speaker A: But then we have computers. Computers use like 400 kilowatt hours per year. It varies. That's an estimate that old computer might use a little less. It might use more actually. But then what is Ethereum? Ethereum is a network of nodes and network of computers, essentially. And how many are there? I don't know, 10,000, 5000, 20,000.
00:19:24.034 - 00:19:53.562, Speaker A: Lots of different estimates there. But call it four gigawatt hours for this whole network. And really what we go from is 25,000 gigawatt hours to just a network of computers, just a network of laptops, a network of servers, call it four gigawatt hours. So really that's actually a 99.98% energy reduction, which is fucking insane. But very importantly, very importantly, there's no sure, add a node, double the amount of nodes. That's great.
00:19:53.562 - 00:20:32.762, Speaker A: We want more people to run nodes, but those are just computers, they're just this, like you plug it into the wall and there's nothing that with the value of the asset, maybe we're not there today with Ethereum proof of work. But if the asset ten X in value or 100 x in value, then you actually get there because the amount of energy you want to burn scales with the value of the asset. We no longer have that feedback loop with the real world energy consumption. So definitely some good stuff right there. And then we end up there. Kevin Milwaukee told me that's where we're going to go, scalability. There are 10% faster block times in proof of stake compared to proof of work today.
00:20:32.762 - 00:21:18.350, Speaker A: So a little bit of scalability gain, but that's not actually what I mean. What proof of stake actually opens up is the ability to architect more sophisticated consensus mechanisms, usually called sharding, sometimes called scalable data data availability sampling, essentially like very advanced cryptographic techniques that allow us to bring in more scale without compromising on decentralization and the security. But those are very hard techniques to construct in a proof of work environment. So really, proof of stake becomes requisite to move into this more scalable zone and get more out of your consensus mechanism. Dank sharding, that's actually paying homage to Doncrod. I don't know if he's in the audience. Hello, Doncrod.
00:21:18.350 - 00:21:46.910, Speaker A: You're probably upstairs hacking. But the current sharding construction, which brings in a lot of simplifications, we call dank sharding, which, I mean, we're in Colorado, so good place to be if you're interested in this stuff. I'm not going to have time to talk about sharding. Today there was a really good dank sharding workshop. The video is online. You can check it out if you want to know what the situation is. I don't know if I have any.
00:21:46.910 - 00:22:06.902, Speaker A: Okay, this is my last slide. Huge shout out to all engineers and researchers. I'm just like one guy. There's literally hundreds of people working on this thing. I can see some of you all in the audience. I'm honored to be able to sit here and talk about the iceberg of work going on here. And if you want to contribute, come find me.
00:22:06.902 - 00:22:26.142, Speaker A: We're definitely looking for p to p experts. We got some fun, fun problems to solve with the sharding stuff coming up. But really, if you're into layer one and you want to talk, come talk to me. Thank you. Do you want to do a few questions? Is that cool? Sure. Yeah. So we have time for a few questions.
00:22:26.142 - 00:22:59.974, Speaker A: If you guys have questions for Danny, line up at the mic right here. I'll give you like a two minute warning or something. And I can ask myself questions if you want. I've never seen that man before. What's the current state of light client incentives from a research perspective? Light client incentives, there's probably two big categories. One would be like a pay for data model, which people are bearish on. But I would say that it hasn't fully been built out.
00:22:59.974 - 00:23:38.778, Speaker A: And so I would say that it's definitely worthwhile to see if we can get the light client server payment model done. There's a little bit of a bootstrapping problem there. And then the other model is. The other model has the assumption no one's ever going to pay for light client data. And so you need to build a more distributed web to architect it. And so Piper's exploring the portal network, which essentially takes, if you assume that there's tons and tons of light clients that want to participate and have a little bit of resources, then you can construct like a DHT to serve and kind of hang out with each other. I think that both are very promising.
00:23:38.778 - 00:24:18.844, Speaker A: Light clients are always a little bit disappointing because we wish they existed five years ago and they barely exist today. But I'm glad your company is working on them. Talk about Vedfs. Oh, yeah, can you talk about BDFs and how they fit? So randomness is pretty important in proof of stake protocols. You kind of get that in proof of work for free. For free? Well, not for free, it costs a lot of energy. But the random process of competing with hashing gives you kind of the randomness to decide who the leader is, who gets to make a block.
00:24:18.844 - 00:25:19.536, Speaker A: Whereas in proof of stake, you need some in protocol randomness to dictate, shuffling to dictate who's the proposer, dictate these committees and different things. So there's this construction called randao, where each time a block proposer makes a block, they reveal some private information that they cannot bias, other than the fact that they'revealing it or not. So they can either mix in into the randomness and help dictate the protocol, or if they really wanted to bias it, they could not. And so I say that because randao exists, randao is what the protocol uses. We have a lot of models showing that randao is not that biasable, and it's very difficult in practice to bias, but it is biasable to some capacity. So what a VDF does is it takes that with a crazy hardware project, assumes that an attacker cannot build a piece of hardware faster than some maximum speed up, than commodity hardware, and uses that to harden the randomness. There's all sorts of things you can do with VDFs.
00:25:19.536 - 00:26:12.328, Speaker A: Theoretically, this is the one that we're kind of interested in. Pursuing that problem is hard because it's a hardware problem. So the VDF has like an honesty assumption of one. So there only needs to be one person in the world, honest actor, actually running a piece of hardware like this. So it's not like some crazy proof of work thing that you attach to it, but there is like a whole R and d project to investigate these hardwares, to investigate which actual mathematical and cryptographic construction you embed in the hardware, and then to prove very rigorously how even in a no expense spared attacker take all the money and resources in the world, what the theoretical speed up is, and if you can bound that. This is a very exciting project. I think VDFs open up lots of really interesting crypto economic constructions in protocols like Ethereum and protocols like Filecoin and others.
00:26:12.328 - 00:26:58.324, Speaker A: But I think we're at a minimum a few years out from having anything worthwhile, truly secure and ready there and maybe many more years out than that. But some people are much more deep into the VDF project than I am. Is the merge going to affect gas prices? Is the merge going to affect gas prices? Great question. So I talked about that a little bit. Does the merge affect gas prices? So scalability affects gas prices. The merge does not inherently bring more scale. You're still bottlenecked in proof of stake by the same things.
00:26:58.324 - 00:27:42.830, Speaker A: In proof of work. The merge does open up sharding and versions of sharding, I believe, which could give us some more scale, could be released really within the next year. But one thing that does help with scale is sharding combined with these L2 constructions. So teams like optimism, teams like Arbitrum, teams like ZK sync, they are deploying more scalable environments into Ethereum today. And what we plan to do with upgrades after the merge is supercharge those scalable environments with more data. But, yeah, we're working on it. Cool.
00:27:42.830 - 00:28:09.520, Speaker A: Yeah, thanks, sharding. This is the thing. Oh, no, it's not up there. Mice don't put it up there. There's a number of people working on a, quote, mini dank sharding this weekend. So hopefully we'll have a fun prototype and be able to share more with you at the end of the weekend. Look out for it on the Internet.
00:28:09.520 - 00:28:20.490, Speaker A: I. All right, cool. I'm going to call it. Unless anybody else wants to talk to me, I'm happy to chat. All right, thank you. Appreciate it.
