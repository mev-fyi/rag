00:00:00.410 - 00:00:14.878, Speaker A: Hello everyone. Happy 420. Thank you for coming to my talk. I am Terrence from Arbitrum. I work on prism. It's a consensus layer, client and written. Go.
00:00:14.878 - 00:00:42.426, Speaker A: So today I want to talk to you guys about the net scalability upgrade EIP four. Four. I'm sure most of you guys have heard at this point. So let's get started. So outline. So you can't really talk about scalability without talking about the rob century roadmap, right? And then after we talk about the rob century roadmap, we're going to talk about EIP 44. Like a quick overview, very high level.
00:00:42.426 - 00:01:34.758, Speaker A: What is EIP 44? Four. Then at three and four I'm going to be a little bit more technical and hopefully it doesn't become too boring. So I hope to spend more time on one and two and on three and four I can quickly just be through it. And yeah, if you have any questions, feel free to come find me after. So refresher, what is this rob central roadmap? So, triangle, you guys have all seen this triangle before. Essentially the triangle basically states that we have three points, we have scalability, we have decentralization, we have security, and it's impossible to hit all three points, right? So you have to make trade off. And the trade off that Ethereum makes today is that we are decentralized and we're secure, but we are quite slow, right? And this is part of our DNA.
00:01:34.758 - 00:02:21.838, Speaker A: We don't want to sacrifice security and decentralization with speed. So this is where we operate at the bottom. And this is Ethereum. Today we process about 25 ERC transactions per second, 1550 transfers per second, and then you can do a few very fancy transactions here and there. So average about ten transactions per second. So now the question is, how do we increase the TPS, how do we increase the block space? How do we compete with this global Internet of value? Right? And if you look at Visa today, visa processed $3.9 billion of cards worldwide.
00:02:21.838 - 00:03:10.226, Speaker A: He processed about 80 male transactions, and then like $255,000,000,000 of total transactions, blah, blah, blah, right? So this is visa. And then how do we essentially compete with this Internet global of value? And unfortunately. So the hard truth is that you can't, right, because we are not willing to sacrifice decentralization with scalability. So what do we do here? Right? So we cannot increase the block space, we cannot increase the gas limit. And then given the decentralization is in our. So here comes the rob central roadmap. So, good analogy that Ben Edgerton was said the other day, I think it's really relevant.
00:03:10.226 - 00:04:02.486, Speaker A: If you look at Manhattan, right, it's some of the most expensive real estate out there, but because of economic force, right, you make people start making taller and taller buildings, you're packing more stuff on top of it, and then the land is still very expensive, but you're afford to pack more stuff in there. And I think the similar analogy could be also translate into our roadmap. So today with L2, you have your ten transactions per second on layer one. And then you have L2, which is using layer one as code data. And then L2 are able to process more transactions because of. With different security trade offs. And then it's optimized for throughput and ux, right? And then by L2, and layer one still shares the same security.
00:04:02.486 - 00:04:47.650, Speaker A: Layer two cannot rock you, for example. And then on the other side of spectrum, you still have apps that are willing to settle on layer one because they still want the security of layer one, and then they're just paying a little bit more. So this is what the rob central roadmap is. And then with EIP four four four. Instead of using code data previously, we are giving a new type of data that's called blob data. And blob data can enable people to process more transactions because it's cheaper and which I will get into more. Why? But then, if there's one thing to remember is EIP four four uses blob data.
00:04:47.650 - 00:05:29.482, Speaker A: And then blob data enables L2s to process more transactions per second. So how do we go with code data to blob data without sacrificing decentralization? Right. So that goes back to my earlier point that we do not want to sacrifice decentralization, but here we're actually processing more transactions. So what do we do here? Right, so this is where I'm going to go a little bit overview on EIP four four. So what is EIP 44? How does it enable you to process more transactions without sacrificing decentralization? So this is the image. This only thing to remember. Just remember this image.
00:05:29.482 - 00:05:57.866, Speaker A: The EIP four four boats blobs onto the block. Right? So blobs are not part of the block. Blobs are not inside the block. So blobs are bolted in the block. Remember this mental image. How does this work? Right, so this requires you to be computationally binding to a block, meaning that if you commit something from the blob into the block, you cannot change it later. Right.
00:05:57.866 - 00:06:43.570, Speaker A: So you have this computational binding scheme for example, like shy 256 Merkel root polynomial commitment. So it uses some binding scheme to ensure that once you bolt the blob onto the block, that cannot be changed later. So what are commitments? Right, so commitments like shy 256 mercury KCG. Before Roy, we're using KZG commitment. So we're using KZG commitment. So we take the Blob, we run through some KZG hashing functions. We get a commitment, we put it in the block, and then we run another hashing function and put it in the payload, which is used on the execution layer.
00:06:43.570 - 00:07:17.410, Speaker A: And then we want to use the hash because it's 32 byte. We want to be EVM friendly. And then also, one nice property of this is that it basically enable us to verify the blobs integrity later through EVM very easily. We basically code that just to open a polynomial at any point. So, here are some comparison of blob versus blob, right? So blob are seen by older nodes. So is Blob. Blob are seen by older nodes.
00:07:17.410 - 00:07:45.670, Speaker A: They all basically have to pay this network bandwidth to get both objects. So the block are 1 mb theoretical limit today. And then blob are 128 target, 256 kb max. So EIP 1509 style. The blob are stored on the execution client. So like GIF nettermind. And then the blob are actually stored on the transistors client.
00:07:45.670 - 00:08:12.870, Speaker A: For example, like Lighthouse light, taku nimbus. The last three are the main differences. And the differences are the reason why the blob is so much cheaper. Right? Because the block has EVM access. So that means that you have to pay compute power to access through EVM. And then it's also stored forever. Right? So, for example, if you put a code data out there, all the nodes have to store it.
00:08:12.870 - 00:08:38.506, Speaker A: So you have this longevity cost, basically for all the nodes. And then right now, it's sitting guests per byte for Blob. So, versus the Blob, you don't have EVM access. So blob actually cannot access the EVM. But Blob can actually access the proof of the blob's integrity. And then you only need to store for 18 days. So this is mainly used for the Rob challenge period.
00:08:38.506 - 00:09:15.594, Speaker A: So, given that it's 18 days and then you can be DD later, that's why we're able to price the relatively cheap. Right? So on the block, you have this EVM and gas. And on the blob side, you have this cheap data gas. So what is a blob? Transaction. So the Blob transaction is essentially a transaction with all the data or opaque data, it's submitted to the execution layer Manpool, and then it's gossip among all the execution layer clients. And then he has a new transaction type, zero x five. He has all the usual data field, like nons value code data.
00:09:15.594 - 00:10:09.414, Speaker A: And then it has this new field, it's called Blob version hashes, which allow you to verify the commitment scheme, and then it's encoded as SSZ instead of RLP. This is the new hashing algorithm that consensus layer client has been using. So this is how end to end will look like with L2. So EiP, four four four. I feel like that's something that you and I probably will not use this type of thing because the data will be DDT later and the EVM cannot access it. But turns out this is very useful for L2 to basically post data on chain, right? So as a L2, you have your sequencer, you have your batch poster, essentially you gather all the transactions data, and then you compress it, you batch it, you submit it to the layer one execution layer client mempool, and then execution layer client gossip those data together. And then on the right hand side, you have your consensus layer, you have your validator.
00:10:09.414 - 00:10:51.794, Speaker A: So validator wants to propose a blob, it gets a blob bundle, it signs it, and then it probably gets it among all the consensus layer peers, right? And then consensus layer peers also verify the blob is integrity with their own acquisition layer client. And then on the L2 side, you have also your L2 validators, and then your L2 validators are actually the ones that's reading the blobs verifying. So therefore you have L2 coming to consensus with each other. Verify that no one's cheating. And on the right hand side, you have your layer one coming to consensus with each other. So yeah, it is quite beautiful. So I have about 7 minutes, so I'm going to run through some of the consensus layer notes.
00:10:51.794 - 00:11:29.474, Speaker A: So this might get more like nitty gritty and stuff like that. So just bear with me. So what are the changes here? Right. The changes here are the blobs are stored for 18 days, okay? And there's not that many economic incentive for you to store data for 18 days, right? For example, like today, I can just be like, okay, I'm supposed to store for 18 days, I'm going to delete it one day later, I don't care. So there's no way to really enforce that to slash you to make you lose money, right. The only thing we can do is as a peer I can ask whether you have a blob and you say, no, I don't have it. Then I was like, okay, well you are a shady peer, I'm going to drop you from the network.
00:11:29.474 - 00:12:06.670, Speaker A: Right? So something to think about. Is there a better incentive alignment there? Can we do something better there? Versus just like this honesty assumption that we trust those will store 18 days of blobs all the time. Maybe we can have some data retention producing nodes, something like that. Also right now for the consistency client, we're syncing from which subject tb checkpoint. So if we do that, we also have the backfilling of the blob. So those are some of the storage nodes I have. And then you have your new fortress rule, right? So for now, if you import a block, the block can be valid or not.
00:12:06.670 - 00:12:42.358, Speaker A: But if the block is valid, you're safe. Then the block should be safe in the DB. But in order to do that, you have to make sure that block must have blobs on the side as well. So that means that you should not be importing the block unless you have the blobs. So there's implementation complexity here where you have to queue the block until the blob arrive or you have to queue the blob until the block arrived. So you have to kind of wait for each other. So you have these state machines, basically you have to play and then for blob syncing as well to sync to the head of the chain.
00:12:42.358 - 00:13:21.670, Speaker A: So the blobs are synced alongside the blobs. So there's two ways to sync. You can do syncing by range, meaning this is more like long range syncing. Or you can do syncing by root, which is more resonant, meaning that I'm just syncing each blob individually, versus syncing by range means that I'm syncing the blob through 512 times at once. And then the peer will also reply resource unavailable if they don't have the blob. But doing that they would get discord and then you would just drop the peer. So it's important to care for basically what you request because you don't want to request data that's outside the 18 days window.
00:13:21.670 - 00:14:06.706, Speaker A: And then, so you also gossip the blob as well because say if you're the head of the chain, you propose a blob, you propose a blobs, then you have the gossip among your peers and then all the blobs are propagated in parallel, so you have different matches of blobs. And then once the blobs gets verified, you basically rebroadcast them. So there are very fun engineering traits that you can do to optimize bandwidth and yeah, I'm really looking forward to working on that. And then blob storage. So each blob is stored for 18 days. So the output bond is that if you're running a node today, you're going to have an astronaut 67 gigabyte. Assuming a blob is always filled, that is very unlikely.
00:14:06.706 - 00:14:45.458, Speaker A: That means that ethereum gas will be threw off the roof and it will be burning tons of Eve. So 67gb is the worst case scenario and we likely will not get there. And then the block and blob separation also enable better persistent storage design, meaning that you can store your block in SSD because that's something that needs to be fast. And then you can store your blobs in hard drive because that's something that you probably will not read very often. So lots of client implementation details here and then blob retrieval. So you want to get a blob, how do you get it? So there's an API for that. So you can get a blob whenever you want.
00:14:45.458 - 00:15:14.374, Speaker A: And then there's utilities written by Mophie on blob utils, and then there's a blob scanning as well. So you can actually view the blobs online. So blob signing. So validator signs the blobs, the validator will get a blob from the API. You will sign with this new domain, domain blob sidecar. So one thing that's interesting is there's no slashing for equivocation. So if you sign double blobs, you will not get slashed.
00:15:14.374 - 00:16:01.062, Speaker A: But you are at risk of missing the block because say today someone requests your double blobs because you can only have one blob per block, right? So you cannot have duplicated blobs per block. So if today you double sign, then wherever, if the node gets the faulty one first, then the node will not process your blog. So it's more like an economic enforcement to make sure people don't double sign versus like a slashing condition and blob publishing. So as a validator, you publish the blob, you go through the Beacon API and then stop. You submit it to the consistency client, and then consistency client will broadcast it for you. And then with me boost. Yes, about 95% of the validators today uses Mev boost.
00:16:01.062 - 00:16:24.834, Speaker A: So that's something that we're still trying to figure out. So definitely watch for the pr on that one. Open question is that should we have blind Blob sign or should we have full blob sign? I guess it depends whether you think the blob can be front run. And then also latency matters here. So the earlier we can design it so test is the better. But this is still like an open question right now. Okay.
00:16:24.834 - 00:16:49.386, Speaker A: Eip four four one now. So where are we today? So we have the spec, the spec is getting ready to be finalized. We have the execution EIP spec, we have the APIs, we have five consensus layer client implementation. We have four execution client implementation. So I would say most of the clients are ready for the testing. We have launched four devnets. So Devnet five is coming in two to three weeks.
00:16:49.386 - 00:17:05.646, Speaker A: We have some tuning, we have Blob skin and then we have KZG ceremony which is ongoing. So you haven't participated in the KZG ceremony, please. Yeah, please do it. And then some deafnet history. So here are the deaf net. So we started from definite one which is prism get only. And then we started definite two.
00:17:05.646 - 00:17:30.098, Speaker A: We added this data gas and then we started defnet three which we, based on top of Shanghai, withdraw. And then Devnet four was the latest one which we have this blob syncing. So all the clients were able to sync blobs and devnet five where you essentially it's a new approach. We decouple blobs and blobs and the TS Mempos hardening. So Blob V four will come in. Blob V five, sorry, will come in a few weeks. Okay, so last slide.
00:17:30.098 - 00:17:59.390, Speaker A: So how can you help? Right? So if you want to help, you're interested, you can follow client progresses, there's weekly implementation code, EIP 4844, weekly implementation code. And then we have followed the R and D Discord channels. You can run client test software, find bugs, open issues, or just write blogs, just write education materials. People are usually very appreciative when you do that and then look out for bunties, grants and everything. So yeah, that's it from me. This is the last slide. So this is blobfish.
00:17:59.390 - 00:18:10.580, Speaker A: Blobfish is the official. Blobfish is the official animal for, because of blob space. So yeah, thank you for listening to me talk.
