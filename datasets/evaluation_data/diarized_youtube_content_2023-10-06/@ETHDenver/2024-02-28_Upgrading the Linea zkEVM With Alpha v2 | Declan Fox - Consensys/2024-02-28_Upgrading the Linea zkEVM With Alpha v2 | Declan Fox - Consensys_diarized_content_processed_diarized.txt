00:00:07.770 - 00:00:30.600, Speaker A: Okay, let's get started. Hi everyone, my name is Declan Fox. I'm the product lead for linear. Linear is a Zk EvM layer two on Ethereum. And today I'm going to give a talk about Alpha V two. Alpha V two is a recent major upgrade on the linear roll up itself. So we're going to go into a little bit of detail on alpha V two.
00:00:30.600 - 00:01:22.918, Speaker A: Okay, so high level, what we'll cover, we'll start a little bit with roll up economics 101. Alpha V two is mostly about reducing costs and prices, and therefore understanding how the sort of value flows through a roll up is quite important. Then we'll see specifically the impact of Alpha V two upgrade on the linear ecosystem. We'll then take a look at the roll up landscape today. Like, how does this fit in with all the other different Dk l two s and optimistic l two s out there? And then lastly, I'll just cover some of the future challenges and optimizations for roll up costs. Okay, so just want to quickly go over how money moves through the roll up system. So if you look here at the top, you can see the users, users submit transactions at a certain gas price to the l two.
00:01:22.918 - 00:01:59.854, Speaker A: This then goes into the mempool, and the sequencer picks up the transaction from the mempool and builds a block that revenue, those gas fees, that equals revenue for the sequencer. And the sequencer basically builds a block on layer two, so on linear, and it does this over and over and over again. And then what it does is it actually batches all of these blocks and these transactions, it rolls them up, hence the name roll up. And then it publishes them in a compressed format to the layer one, to Ethereum. Okay, so we submit the data batch to Ethereum. Meanwhile, we're also sending this batch off chain to approver. This is specifically for Zk rollups like linear.
00:01:59.854 - 00:02:38.782, Speaker A: The prover generates a Zk proof of the batch that has some off chain computation costs. And then we submit the zero knowledge proof to the layer one. And essentially the zero knowledge proof then takes the data batch we previously submitted as a public input to the verifier and confirms that the transactions are valid. And you can see here that there are revenue in form of sequence of gas fees, but there's also costs. So l one costs for submitting data batches, verifying proofs, and then generating the Zk proofs as well. So what's the problem here? Well, ethereum l one is extremely expensive, as we all know. Firstly, to submit the data.
00:02:38.782 - 00:03:05.782, Speaker A: So we're supposed to a lot of data to ethereum for our security and liveness properties. For linear, that costs a lot of money. We also have to verify zero knowledge proof. Each zero knowledge proof costs about 450,000 gas. And that just turns out to be quite expensive. Roll ups also take on a timing risk. So the time at which you submit a transaction to linear and the time that you charge a user transactions on linear is different to the costs that you end up paying on l one.
00:03:05.782 - 00:03:40.770, Speaker A: There's a time delay. And so you take on risk mostly because the l one gas price is volatile. It's not predictable, it's not stable, but also because it's difficult ahead of time to really sort of estimate the resources that will be used in the transaction later down the line when it hits the l one. And so ultimately these problems just lead to higher l two prices for end users. This is the kind of previous problem that we had, and that's what brought us to linear alpha v two. So with linear alpha v two, we had two key features. First feature is data compression.
00:03:40.770 - 00:04:20.990, Speaker A: So data compression, what we're doing is we're essentially encoding repeated bytes. So if we that data batch that we submit to the l one, we're encoding it, compressing it, making it very small. You can kind of think of it as when you take a photo on your phone, comes in a big file format, and then you have to compress it, maybe to send it over the Internet. It's very similar to how we're now submitting data batches. We're compressing them and then we're decompressing them in the snark circuit to make sure that the proof is valid. It's lossless. So we need to make sure the data that's being published, even when it's compressed, is still not losing integrity, which is akin to a sort of a g sip and deflate, if people are familiar with that format.
00:04:20.990 - 00:04:57.910, Speaker A: And we're seeing some amazing results. So we're seeing up to 15 x compression ratio calls for, say, a call to a uniswap contract. So we're getting a lot of cost savings from this data compression. And then the second key feature for alpha V two is proof aggregation. Okay? So with proof aggregation, what we're now doing is those batches that I shared in the previous slide, we're now recursively verifying them off chain. So we're actually recursively proving them over and over again inside a single circuit. And what that means is that we only have to verify sort of many batches with a single final proof.
00:04:57.910 - 00:05:49.702, Speaker A: So we're reducing our fixed proof verification costs and sort of with the scheme that we've implemented, we could theoretically verify up to 250k proofs in a BW six circuit, so we can get lots of verification and aggregation with this feature. Okay, so concretely then, how is alpha V two different to alpha V one? So here we have on the slide at the top, alpha V one. What you can see is that with each data submission, each batch approximately, we were including about 150 linear l two transactions. And when we submitted each batch to l one, we then verified each batch of a single proof. So every time there was one batch, there was one proof and it cost about 450,000 gas, very expensive. With alpha V two. Here you can see at the bottom how things have changed.
00:05:49.702 - 00:06:21.074, Speaker A: So a few key things. Firstly, we're fitting way more transactions into a single batch. Not only are we using more of the sort of transaction size limit of ethereum, we've gone from using 75. We're actually, as I mentioned, compressing the transactions as well. So we've gone from fitting 150 linear transactions in a batch of alpha V one to 1500. So that's a ten x improvement there. But also with the proof aggregation, we're now able to include many, many batches into one final proof.
00:06:21.074 - 00:06:55.600, Speaker A: So you can see here we're getting up to 30 of these larger proofs, sorry, 30 of these larger batches, and only then has to be verified of a single ZK snark proof. So this is great. Like the sort of TLDRs, we've gone from roughly having about 150 l two transactions of Alpha V one verified in a single ZK snark to about 45,000. So that's sort of 300 x increase. And the cost savings are significant. We've actually reduced the l one costs on linear by four x approximately. So there's some big cost savings there.
00:06:55.600 - 00:07:35.946, Speaker A: And this sort of results in cheaper prices as well. So the whole purpose of this is we can therefore lower prices on linear. And we already immediately lowered prices by about 66% following the upgrade a couple of weeks ago. Okay, so practically, how does this work out then across the roll up landscape? So you can see here, I took this, this is the data from the last week. So on a per transaction basis now, the sort of two main costs, and these are on chain costs. L one costs, the data availability cost, as I mentioned, and then the actual settlement, which is the sort of verification of the ZK snark cost. And you can see here that specifically you might notice that linear is the cheapest per transaction.
00:07:35.946 - 00:08:20.054, Speaker A: And again, that's because of the data compression and the proof aggregation. So the settlement cost, which is only really for the sort of Zk roll ups. In this comparison table you can see that sort of per transaction now we have less than a cent $0.1 per transaction, and that's because we're getting up to 45,000 transactions in a single proof. And that's 37 times cheaper than Zk sync, 150 times cheaper than scroll, just for the settlement costs. But the interesting part is that the data availability costs, which is a more significant part of the overall picture, they're also much lower due to the data compression. And so interestingly, with the compression that we implemented on linear for alpha V two, we're even getting compression ratios much better than optimistic roll ups.
00:08:20.054 - 00:08:47.030, Speaker A: So you can see here, for even arbitrary and optimism, some of the optimistic roll ups. We're paying less per transaction on Ethereum costs now with this upgrade. So the sort of TLDR here you can see at the bottom is the l one. Total cost of linear over the last week was something around seventeen cents a transaction. Ck sync around about the same with transaction. And then you can see there's a big jump to the other sort of ZKe evms. And even the optimistic roll ups are still much more expensive on a cost basis.
00:08:47.030 - 00:09:21.630, Speaker A: Okay, so you can see on the left hand side. So for alpha V two, what does the sort of cost profile on chain look like? 99.5% of that cost is now data availability only a smidge. So half a percent of the overall cost is for verifying the ZK snark. So the whole idea of ZK roll up, ZK evms being more expensive is now completely over. Like that sort of paradigm has completely changed. We can have much better security with zero knowledge roll ups, but also the same or cheaper prices.
00:09:21.630 - 00:09:54.810, Speaker A: Now, because the data availability cost is 99.5%, the real focus comes in. How do we reduce that further? And there's a big upgrade coming soon on Ethereum, which is EIP 4844. That's probably going to reduce the sort of cost of the data availability by approximately 90%. I put maybe we'll see, because no one really knows. It's a new fee market, but it kind of promises to lower costs by around 90%. And you can see here that that's going to further reduce costs for all roll ups.
00:09:54.810 - 00:10:41.506, Speaker A: And so how does that sort of, theoretically, if we apply that 90% to the DA portion of the previous table, you can see here sort of post 4844, the l one cost per transaction for linear will go down to around two cent. Obviously it depends on the transaction, but that's sort of like the median. And you can see for some of the other roll ups as well, we're getting much, much cheaper costs, therefore much cheaper fees. But interestingly, linear, with its alpha V two upgrade, still has the best or most cost efficient l two. Now, for a ZK roll up, this is all on chain costs, but there is an off chain portion as well. So we have to actually generate the zero knowledge proof. And this is where sort of linear, the technology, the value proposition has come in very strongly.
00:10:41.506 - 00:11:15.854, Speaker A: And that's because we spent a lot of time in r d to design vortex. Vortex is our sort of unique prover technology, sort of. Essentially with vortex, when we generate the proof, we have a two step process. So we have the inner proof. It leverages lattice based cryptography, which is much faster than elliptic curve cryptography, and it's optimized for what we call self recursion. Okay, so with vortex, we're able to take transactions, we're able to take lots of transactions, and then iteratively recursively prove them over and over again. So we get them very small, and then what we do is we wrap that sort of, we compress it and wrap it into an outer proof.
00:11:15.854 - 00:12:09.006, Speaker A: And the reason we do that is that the outer proof is a snark, which is much cheaper to verify on l one. So we're sort of optimizing for speed and cost in the computation side, but also optimizing for l one verification costs when we finally settle to Ethereum as well. And so the purpose of vortex really is to make sure the costs are cheap for linear. So when we think about having to include the off chain sort of computation cost around the prover into the l two price, it will still be very cheap to use linear. And we haven't had to sacrifice any sort of EVM compatibility here as well, just to sort of give people a framing for how the different sort of large ck l two s stand. In a sort of developer experience and compatibility perspective. We sort of have the specialized virtual machine l two s on the left hand side there you can see starklane, which uses Cairo.
00:12:09.006 - 00:13:08.950, Speaker A: It's not EVM compatible, sort of zk sync in the middle, where you can use a transpiler polygon Zk EVM, which is a translation for the bytecode into their own assembly language, and then sort of linear and scroll on the far side as the most sort of EVM equivalent, bytecode compatible l two s. And so using linear really much feels like Ethereum. And the whole point there is that we've been able to deliver a Zk EVM with linear that is not only cheaper and more secure, but has a great developer experience. It really does feel exactly the same as Ethereum, and it still is the only l two that you can go and sync an Ethereum node to like. If you want to go and take your vanilla geth or vanilla bezu node and you could sync it on linear and it will just sort of run out the box. Okay, so, challenges and future optimizations, what's coming down the line? I said with 4844, it's a new fee market. We don't know necessarily how it's going to respond to all the different roll ups, trying to post blobs and competing for blob space.
00:13:08.950 - 00:13:49.790, Speaker A: So again, we need to really think about the timing and optimization of how we're using these blobs to make sure the costs stay low. We need to get better at trading l one block space. Roll ups are effectively commodity traders. You're sort of selling and buying of block space, and there's ways that you can sort of optimize that to reduce your cost further and improve the user experience as well, and then having accurate three dimensional pricing as well for users. So when people are actually using linear, it's good to be able to price in three dimensions. So the three dimensions are the first dimension is the data availability layer, which is ethereum. The second dimension is your execution and your storage costs.
00:13:49.790 - 00:14:27.970, Speaker A: If I tap this on here, yeah, execution and storage costs on layer two, and even storage today is not well priced, in my opinion. You have to store this data forever, and it's probably underpriced. So getting better at accurately pricing that on l two proof generation. So when you submit a transaction to a Zk roll up, usually the sort of cost to execute it and the cost to prove it is roughly one to one. But there are definitely some operations like Ketchak, notoriously, which is way more expensive to prove. And so really what the ZK L two s need to start doing is actually saying, if you want to use this opcode, you can, but it's going to cost you more. And then being able to accurately price that at the time of transaction to the user.
00:14:27.970 - 00:15:01.350, Speaker A: As I mentioned, l one, data availability and getting better with compression as well. For linear, with the compression, we're sort of getting around 90 bytes per transaction on l one. And with the sort of specs of 4844. Initially, that turns out to be about a throughput of 415 transactions per second before we start to see congestion. So that's where we can get to with compression. For roll ups that are not using compression like linear, it'll be even less so. Again, it's better that more roll ups start adopting these tech stacks with compression.
00:15:01.350 - 00:15:14.250, Speaker A: And then for us, one way to solve that is we're going to be rolling out a new gas API on linear very soon, which is going to help solve a lot of these problems. And that's it. Thank you for listening.
