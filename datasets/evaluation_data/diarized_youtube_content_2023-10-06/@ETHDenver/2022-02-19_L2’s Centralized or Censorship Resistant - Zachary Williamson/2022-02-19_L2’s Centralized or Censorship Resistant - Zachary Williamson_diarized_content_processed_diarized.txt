00:00:00.970 - 00:00:40.486, Speaker A: Cool. Hi everyone. Yeah, I'm Zach. I am the CEO of Aztec. For those who don't know, Aztec is a privacy preserving layer two built on top of Ethereum. So our goal is to both scale Ethereum and make it private by default, enabling the awesome innovation that we're seeing in the current space, but where we don't have to sacrifice privacy, where we can all keep control of our identities. And I want to talk a little bit about the state of layer two, specifically zero knowledge layer twos, ones that are secured by ZK snark or ZK stark proofs.
00:00:40.486 - 00:02:05.438, Speaker A: And where we are currently with regards to are these things decentralized or centralized? And how do we decentralize this rather critical infrastructure as we evolve the technology? And basically, are we sleepwalking into somewhat severe centralization issues that could threaten the web free ecosystem and what to do about it? So I think a question that's on a lot of people's minds who are kind of aware of the history of web two and the kind of the cipher punk movement that created web one is how do we avoid repeating the failures of web two? Because web two started out with a very, very similar narrative to web three. The idea is you have this permissionless, decentralized network that anybody can access, anybody can use, and it was going to be an incredible liberating force. It was going to be the information super high. It was going to educate and emancipate the human race. But the reality of it is, whilst obviously it's been a transformational technology, we live in a world where services are hyper centralized. And the monetization model, I would argue, is predatory against individuals, because instead of users paying for products, instead of users being the customers, users are the products themselves. They're commoditized, their data is being harvested by these centralized entities like Facebook, like Google, and being sold to the highest bidder.
00:02:05.438 - 00:03:30.910, Speaker A: And it's causing some, rather, I would argue, some rather severe problems in modern societies. And so web three is very much like a reaction against that. But how do we avoid falling to the same mistakes as web two, as web one? And also how does this involve layer twos? And how are layer twos both a solution and potentially a cause of all of life's problems? But before we're talking about what could go wrong, I think maybe it's worth talking about, well, what do we mean by things going right, like what are the wind conditions of web three? Obviously, these are going to massively, wildly vary from person to person, but I've put up some, I think quite modest goals here. I'm not talking about stripping nation states of their ability to control their own currencies at a very low level. I think that the web three really should facilitate alternative business models than hoarding information, basically allowing users creating business models for the web that allows individuals to actually take back control of their own data and become the customer, not the product. And as part of this, effectively what we're talking about is massively, massively lowering the barriers, the barriers to entry for providing services based around web three to individuals. Like the typical stereotypical example is how DFI is much more decentralized than tradfi and it cuts out loads of middlemen and then it reduces costs.
00:03:30.910 - 00:04:41.254, Speaker A: And I think this is just sent across the entire web three stack. Not just the applications running on top of a network like smart contracts like DFI, but also the actual protocol itself. This is one of the reasons why Ethereum became so successful, because of its permissionless nature, because of its censorship resistance. And ultimately we should be, I think, creating networks, creating architectures that are extraordinarily hostile to entities that are attempting to carve out data monopolies and effectively become rent synchros, not really contributing value, but just utilizing the fact that they have control over a large amount of user data, which means switching services becomes very prohibitive. So taking a bit of a segue, I would argue that if we live in that world, I'm happy I don't know about you. I think it will certainly be an improvement on where we are today. And zero knowledge layer twos are kind of a critical lynchprint technology, I would argue into achieving this because effectively it kind of solves, or I would rather say bypasses the blockchain trilemma, that you can either for a distributed network, you can have throughput, scalability, or you can have security, or you can have decentralization, you can pick two of those three.
00:04:41.254 - 00:05:27.670, Speaker A: Basically it's a trade off space, but you have to sacrifice one of those. Ethereum sacrifices scalability, as we're all very aware with its extremely high gas costs. And so instead of the core architecture of Ethereum resolving this, I would argue the solution is for layer twos to solve the problem effectively. Layer twos will inherit the consensus of the layer one network like Ethereum. But every individual transaction that's processed by the layer one network represents the aggregated batch of thousands, tens of thousands, even millions of transactions built on these layer twos, where you can verify the correctness of all these transactions through Xeronon's proofs. You check a proof of that. Somebody else has performed all the computations required to verify the block.
00:05:27.670 - 00:06:03.274, Speaker A: It's incredible technology. I feel very lucky to be a part of developing it, and it almost seems too good to be true. So what's the catch? And the catch is, well, let's just talk about the currency of layer two, right? Like we're in a world where last year a bunch of layer two is launched. We have the asset network, private layer two. There's eksync, there's Polygon and Starknet. There's an absolute ton of them coming out, almost like a cambrian explosion. And the dream is that these networks will be completely decentralized and permissionless and censorship resistant.
00:06:03.274 - 00:06:42.598, Speaker A: But the reality right now is that they're all quite centralized and they're all highly permissioned and they're all vulnerable to censorship. And part of this is just the fact that this technology is still in a very early stage. Most of these networks, the layer one smart contracts that verify the correctness of the networks, they're controlled by multi signature contracts that can modify state, to be fair. Hands up. This is the case for aztec because we want the ability to rapidly fix cryptography bugs, given the early stage of the technology. And all these companies have plans to decentralize in the very near future. But I think we're going to see over the next year or two how hard this is going to be in practice.
00:06:42.598 - 00:07:33.374, Speaker A: And why is this? Well, I'm going to talk specifically about zero knowledge layer two, ZK snarks, because this is really what my core competence is, is what I've been focusing on. Nothing against optimistic roll ups, optimistic layer twos. One could argue that what I'm about to say is really actually to their credit and to the detriment of ZK layer twos, because I said before, you can just bundle tons of transactions together and then verify their correctness by constructing a zero knowledge proof that somebody else has performed the effectively performed the work of a validator node. But the problem is, constructing these zero knowledge proofs is monstrously expensive in terms of computation. The difference between doing a computation and creating a zero knowledge proof of that computation, it's about a factor of a million. Honestly, depending on what you're doing, it could scale to a 100 million times slower. We're talking insane numbers.
00:07:33.374 - 00:08:59.266, Speaker A: And so we're not going to really live in a world it's very hard to see, a world where we're going to have a massive decentralized network of these provers that are essential for the functioning of these networks, because you're talking about, depending on the complexity of the protocol, if you're doing a ZKVM, you may need to enter an entire data center of compute to construct these proofs in a timely manner. And so it's difficult to see a world where you're going to have a massive number of, effectively, these nodes when the infrastructure required to run them is so high. So how do we avoid compute sensorialization? How do we avoid, like a very, very small, maybe even one entity kind of monopolizing proof construction and basically being able to sense the transactions, hold the network hostage, all these bad actor problems. And so one of the solutions that's come out, which I think is quite elegant and solves a lot of the problems here, is the idea of splitting up this concept of like a validator node, approver node into two different actors. You have a sequencer and approver, where the sequencer kind of acts a bit like a validator in a proof of stake layer one, where the sequencer is the entity, which doesn't need a lot of computational horsepower. They're probably using proof of stake. And what they're doing is effectively the sequencer is determining what the next block is in the network.
00:08:59.266 - 00:09:33.874, Speaker A: So they're not constructing the zero knowledge proof that proves its correctness, they're just stating what the block needs to contain. And then it's up to some other entity, the prover, to actually construct the proof of knowledge that proves its correctness. And this is quite powerful because it means that the prover has no agency in selecting transactions. The prover can't do front running, the prover can't censor transactions. Basically, the prover is instructed what they have to compute, and they have no alternative but to compute it or to do nothing. And so obviously, this doesn't remove things like front running and mev and blah, blah, blah. It just means that those problems are part of the sequences problems.
00:09:33.874 - 00:10:15.150, Speaker A: They're not the provers problems. And that's a better world to live in because you can have a very large distributed set of sequences, because they don't require very much resources, and you can still have like a very small number of provers. So this is the model that pretty much all these ZK layer two s are going to route off. And it solves most of the problems I've just mentioned in the previous slide, but it doesn't solve all of them. And I want to talk about that because I don't think a lot of thought has been put into this, because everyone working in this industry, in ZK snarks and ZK starks has been focusing so much on building out the underlying tech. I think when it comes to the network architectures decentralization, there's not a lot of thought being put into it. And we should talk about it, because there are some problems that we need to resolve.
00:10:15.150 - 00:11:23.906, Speaker A: So what are some of the long term risks that are somewhat unique to zero knowledge layer twos, and one of them here is sequencer griefing attacks. I said in the previous slide that the sequencer basically fixes what the block is for the next block, and that all the provider has to do is construct a proof of knowledge of its correctness. Well, this requires that your cost model is extremely accurate, like the equivalent of gas before layer two. Basically, how much computation power is the proverb going to require to make the prover knowledge? Because that's going to define how expensive it is for the prover, and therefore the fees that users have to pay in order to get this block onto mainnet. And so if your cost model is off, say, there are opcodes or instructions that cost more cpu power than the gas metering would imply, then you got a briefing attack, because basically you're forcing the prover to construct a block which is not economically viable for them to do so. And so that kind of activity can hold the network. It can mean that nobody actually takes on the role of approver because it's not economically profitable.
00:11:23.906 - 00:12:17.042, Speaker A: And you can get to kind of worst case scenarios where a malicious sequencer could in some circumstances produce demand approver to make blocks which take arbitrarily long amounts of time to compute. And that would hold the network. You also have problems of proverb liveness, because this kind of model implies that you're going to end up with a very, very small number of quite centralized provers with a lot of computational resources. But when it comes to selecting provers and rewarding them, it can't be a zero sum game. The idea, like for example, fastest person to construct the proof for knowledge, they get to put it on chain, they get the fees, because in that case, the fastest prover will always win and the second fastest prover won't bother competing. And then you end up with one prover, and then you have a huge liveness problem, because if that prover goes down, then your network goes down. And for critical base layer infrastructure, that's really not acceptable.
00:12:17.042 - 00:13:17.510, Speaker A: If you have trillions of dollars flowing through these networks, you can't just stop them for an hour. You also have an infrastructure centralization risk, which isn't present for proof of work blockchains and their mining networks, because the resources required to construct these proofs, you need enormous amounts of compute and you need enormous amounts of data storage, you need enormous amounts of ram. We're talking like terabytes of ram. We're talking server farms of cpus or gpus or even Essex. And so right now, I believe every single layer two, ZK layer two, their proven nodes are run on AWS or a similar cloud provider. We do this as well because it's convenient. But when you centralize infrastructure like that, you have a huge problem because, well, the incentives aren't aligned because if, let's say we're living a world where all the layer twos, all the provers are running on AWS, they're running on digitalocean, et cetera.
00:13:17.510 - 00:14:13.440, Speaker A: Well these are supposed to be permissionless networks where people can get up to all sorts of junk and nonsense. You can have people trying to evade sanctions in Russia, you can have people mixing on tornado cash and all this stuff, which is going to be controversial and contentious and flat out borderline and flat out legal in the places where these cloud providers are located. And so what if these cloud providers say, well, you're violating our terms of service, we're going to shut down these proven nodes, what do you do? So I think this kind of infrastructure centralization isn't, we need to address this and deal with it and make sure that whatever architectures we end up building, they can be decentralized across the entire tech stack. And the last one I want to talk about is prover data modes. I'll get onto this later on. But effectively you can end up in a world where certain privileged provers have enormous amounts of access to user data that isn't present on chain. And then there are some issues that come with that, which I'll go into later.
00:14:13.440 - 00:14:32.190, Speaker A: So I might have actually spoiled a few of the slides, future slides, by spending so long on that one. But here's an example, pardon? 5 minutes. Cool. So, sequencer griefing attacks. This is basically what I was talking about earlier. Your sequencer can create basically impossible blocks to compute. It very much depends.
00:14:32.190 - 00:15:37.986, Speaker A: Also the architecture of your network affects us massively. For example, if you take each program that you're running on your network, each smart contract, and then you compile it into one of these zero nods proofs, then that compiler cannot apply optimizations like a traditional compiler, because then the compilation time is non deterministic and you can end up with this concept of a compiler bomb, where it takes an infinite amount of time not to construct the proof of knowledge, but merely to compile the program into a ZK snux circuit. This is what killed ewasm, for the record. And so we can't use models that expose this problem in ZK snark layer two s either. And so the solution is this universal simulator circuit where basically you have one Vm, where you emulate, like for example, a ZKe Vm, where you have one ZK snark circuit that will execute any program that you run through it. But the problem with these is they're extremely slow, which exacerbates all the problems I was talking about before. Similarly, prover liveness.
00:15:37.986 - 00:16:12.126, Speaker A: I've talked about the problems. Some of the solutions are, well, you need to be able to have redundant provers per block. Effectively. One solution is kind of sort of like a federated prover network. For example, instead of having one monolithic prover, you have thousands of provers where you split your roll up block into tiny, tiny subcomponents. So, for example, that you have one actor that rolls up two transactions, and then you have another actor that rolls up two of those previous roll ups, and you keep going and going and going in a tree like structure. And this allows you to massively parallelize proof construction and you can actually decentralize the proven network.
00:16:12.126 - 00:17:13.110, Speaker A: The problem with this though, is that it's really slow, because you now have this kind of decentralized federated network where you got to transfer tons of data around and you got to coordinate people, and you just don't have the kind of the economies of scale of a centralized cloud provider, which means that you can't use this kind of architecture for something like a ZKVM or something which has a huge amount of complexity in the circuits. You need to have something which is snark or stark friendly, for example, like what Aztec's doing. I think Starkwest Cairo would fall into this kind of could use this kind of model. And this solves the centralization of the proverb issue, because you've no longer centralized it, but it does limit the functionality of your layer two. And similarly, when I said about infrastructure centralization, the centers aren't aligned if everybody's using cloud service providers. And so that is a problem for this centralized prover model, which we need to address. It can be solved through a few means if you can have provers baked into hardware, bit like you have bitcoin miners baked into hardware.
00:17:13.110 - 00:17:52.210, Speaker A: The prover algorithm is an ASIC chip then the act of making a proof is a lot more plug and play. You can just buy some fancy hardware and then plug it in and away you go. And you don't need the cloud infrastructure anymore. But that's still got some problems because you still need vast amounts of ram and storage. And this zero proof tech that we're working with obsolete so quickly that in my opinion it would be a rather foolish thing to today like to start creating burning ASIC chips that perform zero proofs because in two years they'll be completely useless. And so another solution is this federation of decentralized provers, but then that's slow, so that's got problems with that too. And the final thing I want to talk about is privacy.
00:17:52.210 - 00:19:06.106, Speaker A: This is aztec's thing, we do privacy, so I feel like I should talk about it. And one of the reasons why we care about privacy, other than the very basic thing of well we want to protect user data, is that if you don't have strong privacy at the protocol level, then you can end up with a rather dangerous situation where you can get pseudo privacy by giving your secret information to a trusted prover. The idea is imagine you have some smart contract which wants to work on hidden data, then well, you make the data all public and available to the prover. The prover will then construct this like a stark or a snark of your smart contract transaction and send the proof to mainnet. And in doing so they can basically remove that secret information that you gave them. And so from the perspective of the network, it looks like it's private, but it's not private for the prover. And this effectively enables produces well, but you have a centralized server approver where there's very few of them harvesting user data as part of a service they're building, and having the ability to both monetize that data and it provides a kind of a data moat which prevents users from using alternative services.
00:19:06.106 - 00:19:50.450, Speaker A: And this is exactly the kind of web two model that we want to avoid. The solution to this, in my opinion, is by baking in privacy at the actual protocol level, by having your smart contracts capable of using advanced zero proofs themselves to obscure and hide data such that effectively for all private transactions you store the encrypted data on some data availability solution and it can be decrypted by having, by giving, and the user has the ability to decrypt them that data with their public private keys. That way it's very easy to create competing services because that data can be accessed by anybody anyway, that's basically everything I had to say. Thanks for listening, and. Yeah, it's been a pleasure. Cheers. Thank you, Zach.
