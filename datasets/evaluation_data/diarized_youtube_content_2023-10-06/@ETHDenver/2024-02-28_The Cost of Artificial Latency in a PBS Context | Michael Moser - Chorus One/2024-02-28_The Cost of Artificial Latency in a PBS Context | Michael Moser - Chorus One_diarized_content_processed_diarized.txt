00:00:09.130 - 00:00:29.330, Speaker A: Hi, everybody. I'm Michael. I'm head of research at Chorus one. We are a staking provider with now more than $3 billion in stake. And I want to talk about timing games. Today I'm presenting a paper which we wrote earlier in the year. It's called the cost of artificial latency in the PBS context.
00:00:29.330 - 00:01:05.602, Speaker A: And it's basically about what happens when validators delay bids. Let's take a quick look at the agenda. So, three major points. I want to start by defining what timing games are, why validators would play timing games, and what upside. So what financial upside validators could capture by delaying bit selection in a timing games context. I'll then talk about negative externalities that might arise from timing games. I think there's three major categories here, and we'll take some time to walk through all of them.
00:01:05.602 - 00:01:59.918, Speaker A: And then lastly, I'll talk a little bit about our timing games pilot. So we take an accelerationist point of view here, and we've been kind of testing out timing games since the summer. But for us, it was important to balance rational competition with ecosystem alignment. And for that reason, the study that I'll present today basically open sources all of our parameters and kind of makes it open for other people to look into and to potentially imitate, if that's what somebody is looking to do. Yeah. So by playtime and games, there is a certain dynamic, which is as the slot progresses, more transactions hit builders. So they have a larger pool of transactions which they can use to build an optimal block.
00:01:59.918 - 00:03:17.110, Speaker A: And secondly, statistical arbitrage are able to quote wider and more aggressively. So if you were to arbitrage, let's say, uniswap against binance, and you were to do that at the beginning of the block, you would have to be reasonably more confident about what you think the price is going to be by the time your arbitrage settles than you would have to be if you would execute this arbitrage closer to the end of the slot. And what that means, the upshot of that is that blocks become more valuable in expectation due to basically transactions and by proxy MeV accruing at the builders. And the graph that you can see on the right side here is when validators usually select the next block. And in the next slides, we'll look at what that would look like in a timing game context. So let's set the stage here by putting a number on the upside. If a validator were to delay its bit selection to 950 milliseconds in the slot, and we will assume that execution layer rewards are approximately 30% of all rewards.
00:03:17.110 - 00:04:09.062, Speaker A: That basically means an APR upside of 1.67% in expectation. And what that means is it's starting to get quite significant, and people would be incentivized to do something like that. This could reflect in different ways, depending on how big you are as a node operator. If you're a large node operator and you kind of pull your rewards across different clients, or let's say you have 13% of the voting power, which really would make you a very large node operator, then kind of the volatility that is inherent to MeV, it would smooth out a little bit, and you would have a pretty good idea of what you could earn by playing timing games. Whereas if you're a small validator, MeV Rewards mentioned are very volatile. You would be situated at a different point of the risk reward curves.
00:04:09.062 - 00:04:59.590, Speaker A: What would happen is that your rewards would vary a lot more. This is both a way of kind of reflecting the law of large numbers, which means if you run either a lot of validators or you run them for a long time, you would be approximately looking at the same result in expectation. Right. One thing we argue in the paper is that this theoretical assumption is actually not reflected in practice. And the reason for that is that timing games impose several negative externalities which disproportionately hit small validators. And with that said, let's go to the next section and talk about these three big categories of negative externalities. The first point here is excess burned eth.
00:04:59.590 - 00:05:52.414, Speaker A: Basically, the dynamic that underpins. That is, as the auction progresses, the gas price in expectation rises. This reflects on the next slot, which means that the ETH burned in the slots that are subsequent to a timing game slot would be higher than you would expect it to be without the timing game type dynamic in play. And the upshot of that, again, is that it's a negative externality which disproportionately hits small validators because it puts them on a worse point on the risk reward curve. These things don't really smooth out as quickly. So there's two consequences of that. It basically cuts into the profits of the next proposal, and the upshot of that is a zero sum game.
00:05:52.414 - 00:06:57.698, Speaker A: So if everybody plays Kyman games, what happens is that the slot basically just shifts backwards. In a way, everybody would be cutting into everybody else's profit, and at the end of the day, you would just be redistributing the pie. But if you're really small and you can't compete because of resource constraints, that we'll go into later, or because you are very aligned and you don't want to play timing games, you are basically subsidizing people that feel a bit more liberal about that. And for excessive burn, the median increase for the slot subsequent to a time and game slot. And again here in the theoretical part of things, we assume that somebody playing timing games means that the get header request gets delayed to 950 milliseconds in the slot. Just to recap that, the median increase of burned Eve works out at 0.4% in expectation, which is reasonably large.
00:06:57.698 - 00:07:59.894, Speaker A: And that's the reason why we wanted to address that as a negative externality, which is in expectation quite significant. Let's go to the next negative externality, which is basically based on the same dynamic as you delay your get header request as you play time in games as a validator, the amount of transactions that might accrue to your slot, they increase in expectation. And another way of saying that is transactions that may have been included in subsequent slots actually land in the current one, which is the slot that would be affected by timing games. So again, this is another way of describing the zero sum dynamic. And it's just to say that if you don't engage in timing games, it's going to decrease the rewards that you can expect as a validator. So let's recap that by specifically naming what the upshot of that would be. Right.
00:07:59.894 - 00:08:56.934, Speaker A: It's an increase in centralization pressure. Large operators in a timing node operators in a timing gain context benefit from comparatively higher APR at lower variance, and smaller operators on the other side of things are disproportionately increased to variance from the long tail of bad externalities. Let me just recap the Eveburn example here. And secondly, small node operators might not even be able to engage in timing games at the same amount of expected utility in the first place. And there's a couple of reasons for that. One is lack of internal data. If you're a big node operator and you have a ton of validators, you have a way of getting just accruing more data and forming a stronger opinion of what an appropriate latency parameter would be.
00:08:56.934 - 00:10:09.040, Speaker A: Whereas if you're a small validator, it just takes you a really long time to kind of find out how you can play timing games efficiently and if something changes. If a relay were to start behaving differently, for example, it would take you a disproportionately longer time to change your opinion than a large nod operator would have, right? So there is a big centralization factor here, and then something which kind of comes in tandem with that is just the fact that large noddle operators are typically better capitalized. So you can have people spend time on this stuff. Right? And the risk here is basically centralization on the one hand, and then on the second hand on the secondly, this risk manifests as a self reinforcing cycle, because if you don't play timing games at all, you're again, disproportionately exposed. So nod operators are compelled to employ latency optimization, play time and games as a matter of strategic necessity. Okay. With that said, let's look at what our approach has been here.
00:10:09.040 - 00:11:00.926, Speaker A: In the summer we started, summer of 23, we started to play around with a timing game setup. We named that Adajo. My co founder on the paper is Italian. And in Italian, if you were to play a musical piece Adajo, it would mean that it's performed slowly and with great expression. So we kind of named it Adajo as a nod to just going about calling get header slowly in a very thoughtful. So this has not been a client facing product for us, but more of an internal research initiative that ran on our own validators. Now, as soon as we kind of came to conclusions there, which seemed statistically significant for us, we did two things.
00:11:00.926 - 00:11:46.062, Speaker A: On the one hand, we outsourced our data and we outsourced our initial parameters. So again, this is the study I'm presenting today, and if you're interested in this, find the study. It's on our website, chorus one. And then secondly, we kind of acknowledge that the timing games were something that would just happen with the way PBS is currently set up. So if you're competing rationally, you probably would have to engage in this type of stuff. And so we went very public with it and we started implementing it in client side infrastructure. But at the same time, for us, it's important to emphasize three things, right.
00:11:46.062 - 00:12:22.982, Speaker A: Firstly, we don't run parameters which would expose anybody to excess risk, either the network or clients. Secondly, we frequently publish our research and we try to go about this whole thing in a transparent manner. And then thirdly, liquid staking services like Lido would be the first that comes to mind. To us is neutral infrastructure. So this is not a place where we would ever engage in this type of latency optimization. Let's set the stage here. Our pilot basically employed different setups for different relays, just in practice.
00:12:22.982 - 00:13:01.794, Speaker A: I think I alluded to it earlier. The way you play time and games is you call the get header request a little bit later, right? So in your slot as a validator, you would have to tell the relay that you want to receive a block. Now that's a so called get header request. You would call it a bit later. Our dadro setup speaks to several relays and we put them into different buckets. There's the benchmark setup, which is a relay for which we don't run a latency parameter at all. There's the aggressive setup, which is a relay for which we delay the get header request as much as reasonable possible on a risk adjusted basis.
00:13:01.794 - 00:13:47.794, Speaker A: So not as high as the theoretical expectation of 950 milliseconds, but slightly below that with a risk margin, still rather aggressive. And then there is what we. Christ, in the normal setup, and that's basically a latency parameter which is a bit below the aggressive setup and the moderate setup. That's just then again, another step below that and super conservative. In the box plot here you can see what that looks like. If you look at the right side of the box plot, you kind of see what adager started to do after we implemented our parameters. And let's consolidate that into one graph.
00:13:47.794 - 00:14:14.734, Speaker A: And what you see here is how the network usually would solicit get headers in expectations. So that's a box plot with a rather large margin around it. And then what Adajo would do. And if I had to put that into words, I would say that Adacho calls get header a little bit later. But it does it in a manner which is very risk aware. So the spread of that is rather small. Right.
00:14:14.734 - 00:15:04.202, Speaker A: So if you play diamond games, I think it's very important to go about it in a manner which is very consistent and which is risk adjusted in quantitative terms. You would say that you want to reduce your variance there. And I think we did that quite successfully. Let's look at how this practical type setup compared to what we would have expected in the theoretical part of our study, which again is a latency parameter of 950 milliseconds. So these graphs are quite closely clustered together. You might not be able to see it from the audience or on the stream. But basically what happens is that Adajo actually did slightly better than what we would have expected in theory.
00:15:04.202 - 00:15:51.710, Speaker A: And so let's conclude by putting a number on it. The network distribution of payloads basically is a mean of zero point 46 seven e per slot. You can see the quantiles below here. I'm not going to read it out, but of course there's a fair amount of variance and the Adajo payloads have a median which is quite significantly above that. Right? Again, I mentioned variance. So this is a data set which is of a reasonable size, but it's not something that we've been running on for years. I think the upshot here is just that it is something which has affected our bottom line meaningfully.
00:15:51.710 - 00:17:01.870, Speaker A: Despite remaining below the 950 milliseconds which we quantified the theoretical expectation with, Adacho outperformed the amount of revenue which we would have expected. So let's conclude here by providing a quick outlook. There's a couple of reasons why Adacho did better than what we would have expected in theory, and we do have a lot of data. So we are looking at rolling out Adacho 2.0 now, and for Adacho 2.0 we would be making some of these dynamics more explicit, which kind of helped us outperform the theoretical expectation there. I think the first insight is that the usual way timing games are described is you basically have a latency parameter, and this is what kind of, in our context there, taking a step further, is described as naive parameter based optimization.
00:17:01.870 - 00:17:57.150, Speaker A: What, what we acknowledge there is that the mev boost auction is not consistently timed. So Adacho 2.0 dynamically adjusts for divergences, right. And the fact that it's not consistently timed is definitely a reason why we outperformed the theoretical expectation. And we kind of found an approach which we think allows us to adjust for this on the fly and do even better in this regard going forward. Secondly, as I mentioned, have a lot of data, and we noticed a small but significant performance difference by geolocation and by data center. Right? So what you want to do here, of course, is you want to run in spots which look more attractive relative to other spots.
00:17:57.150 - 00:18:54.610, Speaker A: And specifically in the context of Adaji 2.0 and timing games, that means that it now optimizes for network latency in a few ways. And what I mentioned at the beginning of the talk is that we are committed to being very transparent about that, to kind of balance alignment with competition. So expect news, expect us to publish more specific takeaways once we feel the statistics are in a good place. That makes us reasonable, confident in what we are theorizing here. And then the last point I want to make is some of the learnings that we have had on Ethereum. So for example, I would highlight the last point here in terms of network latency optimization, they might also carry over to other networks.
00:18:54.610 - 00:19:28.830, Speaker A: So again, it's quite early days, and I don't want to say anything that doesn't seem statistically certain, but we've looked into some other networks. And we think there are definitely takeaways that would not only work in an ethereum context, but maybe on other chains as well. Yeah, and that's it. The outlook kind of concludes the presentation. We've published all of this on. We've research. It was really important for us to make sure that people are aware of these externalities and kind of to trigger a discussion there.
00:19:28.830 - 00:19:54.900, Speaker A: So if you're interested in that, feel free to scan the QR code. This presentation is also on our website, and I think we are tweeting it as well. So if you don't want to scan it now, feel free to catch up with that later. Join the discussion. If anything here sounded interesting or relevant, feel free to find me after the presentation. And that's it. So have a good conference, everybody.
