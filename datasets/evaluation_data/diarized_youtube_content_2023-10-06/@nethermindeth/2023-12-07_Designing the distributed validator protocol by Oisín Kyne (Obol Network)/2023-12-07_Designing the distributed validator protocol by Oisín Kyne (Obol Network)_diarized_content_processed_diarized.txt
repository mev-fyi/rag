00:00:06.010 - 00:00:36.162, Speaker A: Okay, thank you very much guys. I've been introduced so I'll get right to it. Today we're going to talk about some of the design constraints of building a distributed validator protocol. So firstly, for those unfamiliar, I'm going to briefly introduce distributed validators. Then I'm going to talk about some of the different pieces of the puzzles. I'm going to talk about the engineering challenges and some of the design decisions we've made there. Then I'm going to talk about some of the data aspects and how they can be both subjective and also costly to get kind of on chain and to do in a kind of a low trust but cheap manner.
00:00:36.162 - 00:01:11.438, Speaker A: I'm going to then talk about some of the more research focuses, particularly with consensus and some of the other challenges in ethereum like MeV. And then lastly I'm going to talk about the kind of coordination problems of getting everybody to share validators in a similar manner. So firstly, I wanted to talk about what are the different pieces of a blockchain that you need to scale. And I generally break it down into three different kind of core resources. You have your execution, which can be scaled with layer twos like optimistic rollups and zk rollups. Most people are familiar with excaling the execution layer. You can then also look at the data layer.
00:01:11.438 - 00:01:57.134, Speaker A: Data layer is preventing every single node, keeping every single piece of state and every piece of history for all eternity. Things like 4844 and data availability sampling are ways to scale the data side of blockchains. And lastly, on the consensus side of things, consensus is how many nodes can take part in coming to agreement on the state of the system that you can scale with distributed validator technology. So what is a distributed validator? A distributed validator is an ethereum validator that runs on more than one machine, and it has fault tolerance. You can think of it like a multisig for a validator. One of the most important things about a distributed validator is the fact that not one entity needs to run all of the pieces. You can actually have different groups running the validator.
00:01:57.134 - 00:02:41.866, Speaker A: This is a step up in trust minimization, particularly in a world where a lot of people are delegating to others to run their nodes. And there's lots of different advantages to distributed validators, and they are kind of different advantages depending on what type of user group you are in. So for the very first part, having fault tolerance allows you to have high availability. This means even if one machine dies, your validator stays online. This is useful for more or less all validator types for the liquid staking protocols. These liquid staking protocols currently are delegating to trusted node operators to run validators for them. This is a bit of a risk because you're giving them private keys that they can get compromised, they can lose.
00:02:41.866 - 00:03:42.770, Speaker A: We still don't have forced withdrawals, so there's quite a lot of trust in giving somebody keys to run on your behalf. So distributed validators helps to reduce that trust. You pick a group of them, and so long as more than threshold or the number of faulty nodes don't mess up, your validator is safe. For the at home and solo staker end of the spectrum, distributed validators allows you to do squat staking. This is where you can team up together. And even if you run on retail, Internet or hardware that you can't struggle to keep online all the time, maybe you don't have the full 32 e. All of these things can be ameliorated with squad staking and just generally for Ethereum more broadly, you can have these validators that run with a mix of clients, you can have them that run in a mix of geos, you can have ones that run with a mix of operators, and the odds of major catastrophes for the Ethereum set go down if everyone is running these nice blended distributed validators that don't have too much of an exposure to any one point of failure.
00:03:42.770 - 00:04:29.060, Speaker A: Where are we at with distributed validators and their adoption today? We just this weekend, or before the weekend, announced our permissionless beta. Up until now, we've had about 40 entities run as part of our alpha, and we've had 40 solo stakers run on main net with Etherfi's operation solo staker. We're currently going live on Holski for Lido's simple DBT module. This will be the last kind of dress rehearsal before we put it in Mainnet, hopefully early in the new year. We have been doing scaling tests with enterprises and mega Labs to test how high we can scale distributed validators on same fixed amount of hardware. And we've also done two audits. We've started our second audit of Karen, and we've audited our solidity contracts, which I'll talk about maybe at the end.
00:04:29.060 - 00:05:04.810, Speaker A: So that's a distributed validator. But I also briefly want to touch on what's a distributed validator protocol? You might be able to guess, but a protocol is simply a standard spec and set of rules that many different implementations all speak, and that allows you to have interoperability between these clients. One of the things that's important about our design is because BLS signatures are aggregatable. We have built Caron and the protocol as a middleware. This is good for optionality. If a better middleware client comes along, you can swap one out, swap in the other. And generally, if there ever was to be compromised, it doesn't have runtime access to the keys.
00:05:04.810 - 00:06:02.862, Speaker A: It shows something slashable to your downstream validator, and it says, no, I've already signed something for that block. So you have this kind of two sets of fallbacks or two sets of security, so you've more risk reduced. Okay, so that was a really quick kind of run through on what a distributed validator is, what it's useful for. Now, the purpose of this is to talk about some of the research challenges. So on the engineering front, the first one I want to talk about is this idea of private keys and private key test. If you have a distributed validator that has access to these private keys, especially if you've just one implementation, if anything goes wrong, they can exfiltrate all your keys, and particularly if they're kind of encrypted or put on chain or anything of the sort, this is something that nobody really wants and is a real kind of point of failure. So, with the distributed validator protocol, we do distributed key generations, meaning no one entity has the full private key at any point.
00:06:02.862 - 00:07:05.074, Speaker A: And the Nethermind research team, as of yesterday, published a report with the work of like Mikhail and Ahmed over there, on how to do publicly verifiable distributed key generations, which will be a big piece of the protocol where you can verify that this DKG was done fairly on chain with kind of eventually one DK proof or one crop zk proof at the end. So private keys, super important. The other piece of the protocol design that I want to raise to you, particularly on the engineering front, is networking. So in a lot of blockchain use cases, we heavily favor these kind of gossip networks. These one to many type mechanisms, these make sense for kind of lots of things, but in a distributed validator, you only have known counterparties with the other piece of your keys. It really isn't necessarily all that important for the whole world to see your partial signature. And having one shared gossip network can be a bit of a point of centralization or kind of a correlation risk at the very minimum where you have to kind of serialize your messages onto this gossip bus.
00:07:05.074 - 00:07:37.626, Speaker A: And if anything were ever to happen to it, all of the distributed validators go offline. So instead, with the distributed validator protocol. We've favored direct connections between your nodes. This is done with tcp connections using the lib PTP protocol, and this minimizes the amount of hops you need to get your partial signature to your other counterparties. You're not kind of wandering around a network trying to find one another. This is important for latency. Latency, as you might guess, is super important to running a profitable validator.
00:07:37.626 - 00:08:00.434, Speaker A: And it's also better for making them harder to kill. Because you can have end to end encrypted comms. It can't be easily introspected. There's not some open network you can abuse. You can't kind of connect to nodes that aren't your counterparty. They will only establish connections with people they expect to. And even if you're running in the enterprise, you can run stuff all in, like private networks and VLans.
00:08:00.434 - 00:08:22.442, Speaker A: You don't actually have to go across public networks at all. So we think networking is an important part of this. And one of the other aspects of that is version upgrades. You can upgrade one cluster to a new version, you can wait, you can upgrade a few more. You can have different clusters running different variants. We're looking at like bringing in quick instead of TCP. And you don't need to have all of these all running the same thing.
00:08:22.442 - 00:09:05.020, Speaker A: And versions don't, only to work in lockstep. So if there ever is an issue in future, it can be contained. It's not like an all or nothing thing. So there's some of the engineering sides. Now, I wanted to talk a bit about the data sides of this problem, because one of the things we did within oball is we divided out the distributed validator effort to kind of two phases. It would be great to have a distributed validator protocol that is purely trustless, purely crypto economic, and you run this with a counterparty that you don't know, can't find, but if they're offline, you need to kind of penalize them. And trying to figure out how to do that objectively with kind of an algorithm that doesn't have unintended consequences is very difficult, to be perfectly honest.
00:09:05.020 - 00:09:48.566, Speaker A: So back in 2019, the Ethereum consensus spec went from Ed two 5119 to BLS signature schemes. And this brought in homomorphic additiveity, which is one of the key pieces of distributed validators, particularly as a middleware, and not having access to the private keys. These two gentlemen, Carol and Doncrad, who many of you here probably know, gave a talk at Defcon five about what we now know, as distributed validators, I refuse to call them trussless. And most of that talk was focused on how to punish the lazy validator. And the short answer is that it's very difficult, frankly. One of the main reasons is it's subjective. Different nodes see different things.
00:09:48.566 - 00:10:15.246, Speaker A: You can rely on consensus to come to some sort of truth about who's doing what. But the moment you lose consensus is the moment where it's most important to figure out who to punish. And you have two different maybe nodes and two different groups pointing fingers at one another. And if you can't come to consensus, you can't punish people. So it's kind of a tricky problem. So we're doing a lot of work in that. Some of that will be published, hopefully in the near term, again through the Nethermind research effort.
00:10:15.246 - 00:10:58.426, Speaker A: And a lot of it involves using actually 4844 and data availability to post proofs, and posting hashes of these proofs on l two, which allow you to kind of continue on in the happy path, and then some piece of blower can report you. This actual cost side of things is also very important. So the subjectivity is one side, but trying to do this in a low cost manner is also very important. You don't want distributed validators to be prohibitively expensive or way more expensive than a normal validator. So you can't probably publish data to L one all that often. 4844 helps a lot. You also don't necessarily want to do everything on L two, or you have a bridge.
00:10:58.426 - 00:11:46.282, Speaker A: You have these kind of withdrawal delays. Potentially these can be kind of difficult. State channels are something we've also been looked at as part of this research, but it's difficult to prove to someone outside of the state channel anything about what's going on in the state channel. Dovadalica has some cool posts out just kind of yesterday about DK state channels or zk plasma, and generally zero knowledge proofs and BLS signatures are both very expensive to do on chain, so it's still not the easiest thing to do. But we've had quite a lot of cool work on that regard in terms of lowering the cost. So it's down to a world that I think will be acceptable we'll see when we kind of build it. So there's some of the data side, including kind of the subjectivity of the data, and just the outright cost of verifying the data somewhere.
00:11:46.282 - 00:12:25.066, Speaker A: The next piece I wanted to talk about is some of the research challenges, because there's quite a number of them that keep solving. So, so far. In a version one, we decided to kind of play it a little safer on the consensus side and we went with QBFT. It's kind of a well known, well understood protocol, but one of the downsides of it is when an operator is offline, you have to wait for them to time out. Or when they're like slow, you have to wait for them to time out. Some of the more modern asynchronous or partially synchronous consensus mechanisms can move faster and can kind of skip offline nodes. This is important because it doesn't give you a penalty for having a faraway node right now.
00:12:25.066 - 00:13:13.990, Speaker A: If you have to wait for the faraway node to kind of time out, that is kind of suboptimal. In a better world you have these kind of leader election mechanisms that take into account who was online the previous round and that can allow you to kind of lose less money. When you have someone like really far away, you don't really want a situation where it's like everybody's in the same country and we don't want to bring somebody from far away because it will have too much of a negative impact. One of the other pieces of the puzzle that would be very important to have is figuring out attributability within consensus. Being able to blame certain nodes for forking is important if you want to kind of punish them for being malicious. Some of the Eigen layer team have put out good research on kind of the limits of this. Can you find every malicious operator or like when there is a fork? Not necessarily.
00:13:13.990 - 00:13:56.006, Speaker A: There's hopefully some more research to come on. That's a very large, I think, 26 page report put together by Mikhail and some of the others. And one of the other pieces of the puzzle is MeV is a bit of an external influence on this. And you're like rationality versus honesty, assumptions. Honesty is one thing, you hope people to run the protocol, but if it's rational that they can make a bit more money with slight misbehavior, that can impact the stability of the consensus. They can all start to kind of defect and say, no, I don't agree with your block. I'm going to propose a block because I'm going to sweep a bit of MeV that may end up in a world where you just kind of go around in circles and never progress because they're all trying to kind of defect and steal money.
00:13:56.006 - 00:15:00.282, Speaker A: So things like Enshrine, PBS is one of the big unlocks. There some other things that are kind of in the pipeline that we kind of worry about is how do distributed validators deal with data availability in the first form of protodank sharding is kind of a best effort thing. It's not like a strong guarantee, but as we move to full dank sharding, we need to figure out, does every node within the cluster all sample the exact same pieces? Do they all sample different pieces? Is there a way we can kind of get caught or get in trouble that way where we kind of trust that somebody sampled something they didn't do we have to kind of be overly cautious? There's probably better ways for us to sample data availability within a distributed validator. The next one is, can we verify pve, ss, DKG and l one? I think the answer to that is not necessarily, but we can verify that a VSS was evaluated correctly with the ZK proof. I would direct you to the forum to give a read of that. It's pretty sweet. Does this pattern make sense for distributed sequences at l two is something we look at.
00:15:00.282 - 00:15:55.850, Speaker A: And another thing is, can we do key refresh schemes? Can we do them kind of on a recurring basis? Can we use something like single shot signatures that are kind of single use keys? These are all stuff in the research pipeline that hopefully we'll be kind of rolling out in the next few weeks onto our forum for people to go read. And lastly, I wanted to talk about some of the kind of coordination challenges here. As I alluded to, it's very difficult to do something really permissionless. So far, it makes more sense to kind of coordinate with other counterparties to run distributed validators. But the dream is to kind of get everybody using like one shared spec and stuff. Most of you have probably seen this XKCD meme, but with the middleware distributed validator, it doesn't have access to the private keys. It's helping you spread your risk across multiple machines, and it can really take away a lot of the safety risks in a validator setup.
00:15:55.850 - 00:16:56.080, Speaker A: But if there's still just one single implementation, you're still stuck with the liveness risk. The dream is instead of to have many different independent distributed validator implementations, you have interoperable ones. So that's kind of what we're doing with the distributed validator protocol, with ourselves and the Nethermind research team is to come up with the credibly neutral protocol for DB middleware clients that allows them to all interoperate. And one of the kind of key goals is to have attributability, so you can kind of see who's online who's offline in a manner that doesn't involve like oracles or some sort of kind of God mode. And you can actually use this to reward and punish people at the Altula or within crypto economics side of things. And one of the last pieces of it is I see distributed validators as very synonymous with mining pools. Back in the early days of proof of work mining, anybody could produce a block, and then very quickly it became prohibitive and they were like, let's all team up and share proof of work.
00:16:56.080 - 00:17:25.894, Speaker A: I see validators going more or less the same direction. It's already $50,000 and up to run one single validator, which most of you know, gets like a block once kind of a year. And we already have this kind of pooling happening, but without distributed validators. And I think distributed validators will be like a big piece of the puzzle. Mining pool software has extracted a bit of rewards for more than a decade now. So we think that distributed validators can and should take a bit of rewards as well and pump that straight back into client development. Effectively.
00:17:25.894 - 00:18:01.814, Speaker A: Our hope is to have everyone that implements a distributed validator protocol take a percentage of rewards and distribute said rewards to self sustain the ecosystem for more client development and keep things going. Yeah, that's more or less most of my talk. If you are interested in what I've had to talk about so far, you can join our discord. You can now run a validator on Mainet. Well, December 1 is what we're kind of communicating, but generally speaking, it's ready to go. We've captured up one validator for sanity reasons, and we're also hiring roles on engineering research on the marketing side. So please chat if you want to.
00:18:01.814 - 00:18:04.660, Speaker A: And yeah, feel free to ask me questions other than my research team.
