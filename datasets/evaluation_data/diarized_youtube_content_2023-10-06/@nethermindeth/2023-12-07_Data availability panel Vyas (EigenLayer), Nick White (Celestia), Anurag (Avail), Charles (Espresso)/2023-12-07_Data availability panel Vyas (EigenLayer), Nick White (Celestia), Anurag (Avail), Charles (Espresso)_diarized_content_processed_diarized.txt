00:00:02.170 - 00:00:03.466, Speaker A: All nice and cozy.
00:00:03.658 - 00:00:20.160, Speaker B: Cool. Let me take a question, actually, before we even talk about data availability. What is data availability and how is it different from the current state of the art or currently Ethereum has and this term that I keep seeing, data publishing. What's this all about? Could you explain it to me please?
00:00:21.890 - 00:01:09.554, Speaker A: So data availability is not exactly a new concept. Been there since blockchains have been there. All blockchains have data availability. Inherent in monothe blockchains. Data availability is one attribute. When you run a full node, you're downloading all the blocks and you're getting a guarantee of data availability. What this new wave of systems is basically at least like avail and Celestia, for example, and Ethereum, at least they have a proposal on that is that of data already sampling, which is this technique of sampling the data in such a way that with a few random samples we're able to achieve probability guarantee of 99.99
00:01:09.554 - 00:01:45.486, Speaker A: percentage. So essentially it's this kind of paradigm where we create a client where more users with less requirements are able to get a database guarantee without downloading all blocks. I mean, that's the long and very short version. But databird is also very nuanced in the sense that the word nonclature itself is very difficult. It's very misleading, it's very different from data storage. Data publishing matches that, but it's also a consensus and ordering service in general. Right.
00:01:45.486 - 00:02:08.680, Speaker A: Like when roll ups write data to Ethereum, they are basically betting on the guarantee that once you put the finalized order of transactions, it's not going to revert. So in that sense, it's the consensus and ordering service because executions are being moved to roll ups. It's a nuanced topic, but I'll let the others give their own.
00:02:10.730 - 00:03:10.090, Speaker C: Yeah, I'll just build on top of there and go straight to the proof of data publishing part. Right. I think proof of data publishing is trying to be a little bit more accurate in stating we're focused on whether availability for fraud proofs or were nodes storing the data. And was it available when someone needed to challenge have sampling as well as email? Specifically, they have coupled mechanisms for consensus, which comprises of settlement ordering and data availability, whereas IBMDA, we actually decouple consensus from data availability. So there is a slight separation in the system design, but that's what enables us to kind of have some of our more high performance design systems, and that was an active design choice.
00:03:11.550 - 00:03:16.346, Speaker B: I'll come back to that in a bit, but define what data availability, publishing and magic is.
00:03:16.448 - 00:04:11.530, Speaker D: Yeah, so I think, building off of what Anrug said, data availability is something that is critical to any blockchain system. It's critical to the security, because blockchains are supposed to be verifiable. So in order to verify the chain, you have to know what transactions were actually sent and processed. And even if you don't need to know exactly what those transactions were, you at least have to know that they were published, meaning that they are public information for someone else to go and see what they are. And so it's an inherent property that all blockchains have to fulfill. Now, for the most part, people didn't think about data availability until recently, because when you're running a standard sort of legacy blockchain, the security model, or the model is that you run a full node that just downloads all the data. So you have data availability built in, because if the block data is not there, you ignore the block.
00:04:11.530 - 00:05:09.594, Speaker D: But the problem is that becomes a scaling bottleneck, because now, if you want to process more transactions and the amount of data in that block grows, you have to download more and more data. So it ends up making it more and more expensive to verify the chain as the chain gets bigger. So that's why data availability is a problem. It's a problem when you try to scale. And so, specifically, at first, people thought, in fact, the bitcoin community way back in the day was talking about this problem and kind of was like, maybe it's not solvable. And there was a breakthrough in 2019 called data availability sampling, which was the first, like, as a new cryptographic primitive that lets you verify that data is available without actually having to download all of it. And what's beautiful about it is that it has this sublinear sort of like scaling property, where the amount of work you have to do, the amount of data you download, scales sublinearly with the amount of data that you're verifying.
00:05:09.594 - 00:05:22.210, Speaker D: And so it makes it so that we can actually finally have big blocks and have massive chains that actually have lots and lots of throughput for the first time. And so that's what's so exciting about data availability.
00:05:23.830 - 00:05:48.906, Speaker E: Cool. Let me just preface by saying so, I'm Charles. I'm one of the co founders of Espresso. We don't view ourselves as a data availability solution. We actually are a decentralized sequencer for rollups, so we handle the ordering for roll ups. But I will explain why data availability is essential for us as well. So everyone here on the panel gave great explanations, and I think Nick made a really good point.
00:05:48.906 - 00:06:38.618, Speaker E: I think Anirag actually made the first point, which is that data availability has always been a big issue for blockchains. It's just that it's really come to the forefront recently because of this modular design that Nick, can you get in closer, please? Yeah, this modular design that Nick hinted at. So at the typical layer one blockchain, like a state machine replication system. So what that means is every node in the network agrees on the state and they process transactions and they come to agreement on that. So there's three parts to that, right? So one is consensus. So that is like ordering everyone in the system in the network agreeing on the ordering of transactions. And then there's the data dissemination or the data broadcast part, which is kind of like data availability, making sure that every node in the system has the data, so broadcasting that to every node in the system.
00:06:38.618 - 00:07:19.334, Speaker E: And then lastly, there's the execution part, where every node, or every honest node in the system, executes the transactions and updates the same. So every l one blockchain really is a sequencer like espresso. It's also a data availability system like Celestia, like I Canda, like avail, and it also executes. Now, as Nick hinted, we are trying to improve the efficiency of these systems and kind of modularize it so that the system as a whole is not bobblenecked on the slowest of these three. Instead, we're splitting it up. So for espresso, we are actually taking out execution. So espresso does not execute at all.
00:07:19.334 - 00:07:59.460, Speaker E: Really what it's only doing is sequencing and deciding the ordering of transactions. But we do have a weakened version of the data availability problem that we need to solve, which is that though we don't need to make sure that every node in the sequencer network has the data, we only come to ordering on a consensus on ordering of a short digest of those transactions. But we do need to make sure that the relevant users have the data. So specifically the roll ups that we sequence for must have the necessary data. So anyways, that's a quick summary. I'm definitely not a data availability expert like everyone else on the panel, but I'll try.
00:08:00.870 - 00:08:01.486, Speaker A: Cool.
00:08:01.608 - 00:08:14.746, Speaker B: I hear that for some reason we need to do consensus, and then some nodes store the data, and some nodes store less than the full amount of data. And I also heard mention that maybe we don't need consensus. What's that about?
00:08:14.848 - 00:08:15.878, Speaker D: Who's lying?
00:08:16.054 - 00:08:17.740, Speaker B: I'm only kidding, by the way.
00:08:19.950 - 00:09:28.002, Speaker A: We asked you the answering, but I didn't think his point was that we don't need consensus, it's just that they kind of separated it out. So there are two parts to this, right? So one is the more what is the right order of transactions and how do you execute to arrive at the reduced state? These are two different functions. Now, there have been, now zk roll ups have been on the rise. So there have been cryptographic means of, for example, not doing the reexecution, just relying on a cryptographic proof to do the execution. But the question still remains is who decides the right order? And that essentially boils down to a crypto economic guarantee in the sense that there could be two folks, for example. How do you resolve those? So that's the real meat of the question, why you need a consensus mechanism. Now, in current implementations, I mean, most production grade implementations have like a single sequencer, or maybe espresso comes out with a production system and switches to that.
00:09:28.002 - 00:09:56.250, Speaker A: But still, the final order goes to a layer that provides eventual finality. So a roll of finality is only reached when you put, let's say, the final order of transaction on ethereum or any other base layer, for example. And so the idea is that anyone can go over there and just reexcute, or either do it in a pessimistic, optimistic, or Faziki sort of manner.
00:09:58.350 - 00:11:08.770, Speaker C: Yeah, I mean, that makes perfect sense for eigenda. I can kind of preface why consensus doesn't necessarily matter to us and why we made that active design choice. And honestly, the best place to start is actually prefacing by saying eigenda leverages the shared security that Eigen layer enables from Ethereum restakers, but also Ethereum validators and their decentralized set. So Eigenda will, by decoupling consensus from that, we are primarily focused on serving roll ups within the Ethereum ecosystem, and these roll ups are going to rely on Ethereum for the settlement and execution. So in that sense, and I don't want to foreshadow one of your later questions here, but maybe that might enable us to write with much faster commitments and not necessarily have to be privy to certain mechanisms that blockchains have to, where we can just make DA assertions just as fast as someone is capable of writing to the DA network.
00:11:11.540 - 00:11:54.030, Speaker D: I don't have that much to add to this. But one interesting thing is, I do agree that there are kind of slightly independent things. And specifically one thing, for example, in Celestia is basically that consensus and DA are kind of separate networks, in a sense, the way that's implemented. And so they are kind of isolated. And in the future there's an idea of running the consensus side of the network faster than the DA side. So you can actually generate consensus blocks in parallel to data availability blocks. So I think there's value in thinking about it in that way.
00:11:55.680 - 00:12:00.450, Speaker E: You made a great point. We're actually kind of doing that, I guess, but yeah, I don't have any.
00:12:01.540 - 00:12:01.904, Speaker A: Cool.
00:12:01.942 - 00:12:21.300, Speaker B: So I hear that Celestia and the reasons for ordering blocks and they drive value from that. I also see that the shared sequence is also ordering blocks and has a DA platform. So what's the real difference? Is there something in the latency of it or the security of how to draw a DA or ordering? What's the real difference here between a DA layer and a shared sequencer?
00:12:25.180 - 00:13:19.130, Speaker E: Yeah, well, I think as I mentioned, shared sequencing does need to have data availability. There needs to be some guarantee of data availability because the last thing you would want is for users to look at what a shared sequencer outputs and see that their transaction has been finalized. So, okay, I received the money from my house, I'm going to sign the house over or something like that, and then realize that the data is not available and that no one can pull transactions on that state. So that is like the worst case for roll up. So definitely a shared sequencer does need to have some type of data availability. Now for us we have a system called tiramisu, so it's not like a separate DA, it's just something that's integrated with hotshot, which is our consensus protocol running as a sequencer. And I can talk about that more, but I don't want to take up too much time on that.
00:13:19.130 - 00:14:05.850, Speaker E: That does provide data availability to the extent that we need it for sequencing, but at the same time data availability, I view it as very additive. So really any roll up can not only depend on espresso for sequencing and the basic DA guarantees that espresso gives, but also any one of these solutions here, as well as maybe even like Ethereum. And all of these have obviously different trade offs around security, around the cost, around the ease of getting the data, really. There's a huge design space and yeah, I think it's all very additive and I think there's a lot of things you can plug and mix and match here.
00:14:09.900 - 00:15:07.996, Speaker A: Yeah, no, I just wanted to put a quick point between what we asked was saying. So I just want to be clear. The crypto economic security guarantee matters. I have a little issue with iron layer taking the phrase of borrowing security from Ethereum, risk taking ETH is not the same as borrowing the security of the entire Ethereum validator set. So, I mean, also in response to that question, in the sense that why do you need this beige layer for data validity? I mean, it's all about crypto economic guarantees. When you put a roller, puts data on Ethereum, the ordering is guaranteed by the Ethereum manager set. Just to make this distinction, Eigen Layer has the resticked ETH, and the crypto economic guarantee is equivalent to the restick ETH within that specific committee that is doing the TA.
00:15:07.996 - 00:15:16.770, Speaker A: I mean, correct me if I'm wrong, but we just should be careful in our wording is what I'm trying to say.
00:15:17.380 - 00:15:53.676, Speaker C: Yeah, I can definitely clarify that here a little bit. Right. And it touches on Charles's point about the crypto economic guarantees that you're looking. Yeah, eigenda specifically will be only a secure, or rather, let me rephrase it this way. Right. Eigen layer has a certain amount of free staked ETH onto the smart contract, and that determines the security of Eigen layer. Now, that comprises of all of the ABS's actively validated services that are on I can layer, and each service will only be as secure as the amount opted into each service.
00:15:53.676 - 00:16:56.508, Speaker C: So now we're talking the subset of the initial subset. So it's definitely not going to be as secure as an in protocol VA, meaning something like writing to Ethereum. And that goes back to the crypto economic guarantee that you're looking for. Now, if you're perhaps an exchange that has a high value transaction, meaning you have, let's say throughput on the x axis and value per bit on the y axis, right? Meaning every transaction you have, you have a certain amount of data, and you have a certain dollar value associated to that data. Right. Now, there's certain projects like DeFi or NFTs or order book dexes that have really high value per transaction, value per bit, but don't necessarily have a lot of transactions or throughput, but something like social fi or gamefi, which has really high transaction count but not a lot of value per bit. So what's the crypto economic security guarantee that you're looking for based on the amount of value that your transaction has, value that your data has, that's going to be completely different.
00:16:56.508 - 00:17:04.608, Speaker C: And we anticipate you're going to make different choices of writing to in protocol or out of protocol DA, based on the amount of value that transaction has.
00:17:04.694 - 00:17:05.730, Speaker E: That makes sense.
00:17:07.720 - 00:18:03.140, Speaker D: I just want to add just a word on crypto economic security. So to me, crypto economic security is not actually the goal of blockchain. Crypto economic security is something that you have to rely on for something like consensus, because there's no way around it. You have to trust that the majority of validators or miners are not going to fork. And so that's the only way to secure the ordering, is crypto economic security. But the security of the validity of the blockchain, that the transactions are all valid and not breaking the rules is ensured actually by end users verifying the chain, not just like saying, oh, well, there's enough stake behind this that I'm going to trust it, because if that were the model, then the amount of value that was at risk in such a system could never exceed the amount of value staked. So you'd have this capital efficiency problem.
00:18:03.140 - 00:18:39.280, Speaker D: And so, for example, Ethereum, you can never have more assets on Ethereum than the underlying ethereum that is staked. And so I also think that people need to be more aware of the fact that staking is very limited in the kind of security that it can provide to you. That's why it's verification. That's actually the root of security in blockchain. So that's why there's such an emphasis, at least the celestia makes, on running light nodes and actually participating in verifying the chain directly, rather than just like, oh, there's enough money staked there that I think it's safe.
00:18:46.260 - 00:19:08.760, Speaker B: Taking a slight detour to a tweet that Luca from LGBT posted today where he mentioned that celestia is more Ethereum aligned than Eigenda. And I'm going to amend to avail because I understand they have similar designs, are more Ethereum aligned than Eigenda. What do you have to say about that? And what does EF alignment mean for Da Layer?
00:19:10.300 - 00:20:09.868, Speaker A: I think basically this was. So my fellow co founder was at this panel yesterday with Mustafa and Dankard, and I think this is something that I think Mustafa said because there is a question regarding, I think he said it in jest, basically, Ethereum alignment is a complex thing. In general, people try to get Ethereum aligned just because they're just this market community now. It's like a measurement contest, right? Like who's more Ethereum aligned in that sense? Right. So I don't really have any concrete opinion to say. Just, I would say there's an element to it. Some people, there are a lot of projects who want to be more Ethereum aligned just to.
00:20:09.868 - 00:20:33.140, Speaker A: Because the reality is that if you go against the grain you also bear the brunt of the maximalist culture or whatever, right? That's the reality. And so that's why people like to toe that line. And it's just a function of how crypto works, unfortunately.
00:20:36.510 - 00:20:55.902, Speaker C: Yeah, I'll take this from the Inda side for a mean for us. Again, we're interfacing directly with the Ethereum community. We have Ethereum restaked onto our smart contracts and we're using their operator network and progressively decentralizing using their operator network to run our abs.
00:20:56.036 - 00:20:56.430, Speaker E: Right?
00:20:56.500 - 00:22:02.902, Speaker C: So Ethereum alignment is one thing, but this also leads to things like, hey, let's make sure we're not overloading the execution layer, overloading social consensus. These are all like downstream problems or downstream concerns that the Ethereum foundation has had as a result of some of the initiatives that we have planned. And a lot of that goes to protecting home stakers, home validators, making sure there's incentive mechanisms in place for them. But I also think it's alignment for, as Anurag mentioned, maximizing your community. Ethereum has the largest developer community and ecosystem, and everybody wants to tap into that, which is why interoperability is a huge thing right now, which is why crosschain is a huge thing. Everybody wants to eliminate these fragmented liquidity problems or all these slight issues that we have in diverse ecosystems, and we want to be able to interoperate with each other. So that's kind of where I think the end game for all this alignment stuff is.
00:22:02.902 - 00:22:08.950, Speaker C: We all just want to be able to interoperate to some extent.
00:22:09.610 - 00:23:17.662, Speaker D: I think part of what Musafa was getting at is, well, first of all, like Celestia, the vision for it, a lot of the original ideas were discussed on the ETH research forum. John Adler, one of the co founders of Celestia, was researcher researching l two s and e two roadmap back in consensus. And I actually know that if there were a way to have built something like Celestia from within Ethereum, that's the path that would have been taken. But essentially in order to build this new DA layer, I can explain, maybe I'll go into this in a little bit. You have to actually start a new network. And at that time there was no and you have to actually, a new DA solution has to have its own native token to secure. So I think what Mistov was saying is Celestia is imbued, I think, completely aligned with Ethereum values and the values of the Ethereum community.
00:23:17.662 - 00:24:10.174, Speaker D: And that's really what matters. Something that I said tweeted recently is the only alignment that matters. Are you aligned with crypto values like open source, permissionless protocols, verifiable, decentralized protocols? And I think in that way, Celeste and Ethereum, I think a lot of projects across the modular ecosystem are all very aligned now, just on the point of like. And then I think Eigenvear is amazing in that it's helping to extend and grow the Ethereum ecosystem. But just like there's similarities with Lido, right, where Lido is controversial, I think they've done a great thing for theorem community, right? They've enabled more staking, they provide a very valuable service. But there's also concerns from within Ethereum. Like, oh, Lido is like almost too powerful right now.
00:24:10.174 - 00:24:50.534, Speaker D: They threaten sort of the fabric of security of Ethereum. And so I think there's a similar concern with Eigen layer, and that's not coming from me. That's just like, I think the reality of the situation. So maybe that's kind of what he was sort of like getting at, I guess, when he made that comment. And just a real quick point of why you need your own, why a data availability solution that's external to Ethereum have to have its own token, is that data withholding is an unattributable fault. So you actually can't slash. The only way to slash for data withholding is to socially slash.
00:24:50.534 - 00:25:25.510, Speaker D: Meaning all the participants in the network have to get together and say, I'm going to write down your stake to zero. You can't have a smart contract, read some kind of proof that someone withheld data and automatically slash. And so for that reason, I actually have concerns about the claims that you can even use Ethereum to secure DA, because there's no verifiable way to prove on chain that data was withheld. And so that's also why Celestia went that route. It's like that's the only option we knew to make DA secure.
00:25:27.650 - 00:26:20.750, Speaker E: Cool. Yeah, I have a few thoughts here. So first, I think it's not just about Ethereum alignment, but more generally about credible neutrality. And I think credible neutrality in a way, is really achieved through decentralization, because from the perspective of me working on a shared sequencer, and recently we announced a partnership with off Chain Labs, which developed arbitram, and Steven, their off chain CEO, actually had a great tweet, which was know, off chain certainly has the expertise to build a great decentralized and shared sequencer. But the problem is, if they were to build something, it would be very difficult to convince optimism and all these other roll ups to use it. And I think credible neutrality comes from having a third party build it. Not just that, but also being able to achieve sufficient decentralization and scale.
00:26:20.750 - 00:27:33.174, Speaker E: So part of Hotshot, which is the consensus protocol that's underlying Espresso, one of the design goals is to be able to scale to tens of thousands of nodes. Right now, Ethereum is operated by around 10,000 physical nodes. So in fact, we are trying to scale to that size while also providing super, super fast finality to get the user experience that users are getting with centralized sequencers. Right now, part of that Eigen layer is actually helping because we are trying to scale to not just the size of the Ethereum validator set, but actually involve Ethereum layer ones, sorry, layer one validators in operating the sequencer network such that espresso becomes a fast finality layer for Ethereum down the road. And to that end, on Ethereum alignment. It's actually very important from the perspective of a sequencer to also have to kind of be aligned with the l one validators on Ethereum, especially if the roll ups that are using the sequencer are settling on Ethereum, because the last thing you would want is for all the transaction activity and all the value to move to roll ups. And then l one validators may not be getting a cut of that if they're not being compensated in some way.
00:27:33.174 - 00:27:51.610, Speaker E: So they might have a perverse incentive to actually fork or mess around with the settlement or the state updates of the roll ups on Ethereum rather than behave. Also, you know, resaking and being able to be Ethereum aligned in that way is really important from our perspective.
00:27:53.890 - 00:29:33.058, Speaker C: Yeah, I just wanted to add to that because you touched on credible neutrality and Nick brought up Lido, right, which is a really good example of kind of the Ethereum community having some expressed concerns around the lack of decentralization to some extent. But as Nick mentioned, Lido has also improved the staking capabilities of the community. And I think one of the most important things that goes overlooked outside of just the liquidity portion, is just the barrier to entry, to stake, right? Like if we want to expand the overall crypto ecosystem, bring more adoption, we can't have every validator needing 32 E. But people can buy fractions of steep, which is far more accessible. And similarly, I think there's a certain value proposition there that goes beyond just liquidity. And I think that you mentioned data availability networks or data availability chains, meaning essentially like some form of ability to verify that the data was stored and when the node says it was stored, but maybe it wasn't, or maybe they were doing a grieving attack or something of the sort, right? And I think what you're getting at is something like token forking being required to be able to achieve consensus whether it was withheld or not. And I think we're still in the process of identifying some other potential design mechanisms in addition to assessing whether sampling kind of works for our system or not, and even gives that security guarantee that sampling is supposed to.
00:29:33.058 - 00:29:39.490, Speaker C: There's some open questions that we might have whether that gives us a security guarantee.
00:29:42.150 - 00:30:35.890, Speaker A: Just last point. I just also want to cut some slack to the eigenve folks, because they receive a lot of these issues. See, the thing is, it's very easy to blame Lido or eigenve for doing what they do, but it's also a function that there is no in protocol delegation system in the Ethereum protocol itself. There's a market requirement for someone to stake any amount of tokens, and so there's a market need. And if there's such a big market, there will be players who will target that. Right? I don't blame eigenve for taking the rest, taking ETH opportunity and applying it because it's there. Right? It's such a big market, why would you not attack it? Right? Certainly we should not be blaming these players.
00:30:35.890 - 00:31:08.000, Speaker A: In a sense. If you think these systems are permissionless, then they should be inherently permissionless for these services to operate. The only issue is sometimes, as I said earlier, that these services do not borrow the entire security of Ethereum, and services should be upfront. And like we have said, that was clear. But sometimes we kind of blame someone for doing something that is an open market.
00:31:10.210 - 00:31:22.174, Speaker B: I think this last point on Ethereum alignment is a really good point to end. So I'd like to thank you all for coming and sharing light on DA means for Ethereum and how we might see it roll out in the next few years.
00:31:22.292 - 00:31:22.860, Speaker D: So thank you.
