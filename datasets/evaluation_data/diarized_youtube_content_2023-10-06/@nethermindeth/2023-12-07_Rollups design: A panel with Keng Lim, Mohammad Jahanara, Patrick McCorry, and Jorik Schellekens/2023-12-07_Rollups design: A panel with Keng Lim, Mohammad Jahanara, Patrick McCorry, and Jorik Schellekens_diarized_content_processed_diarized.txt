00:00:05.530 - 00:00:19.230, Speaker A: Welcome to the panel on roller design from off chain to on chain. In the panel we have people from Arbitrum. Scroll, then the mine sash, Lachman and Tyco. But I'll let them introduce themselves. So let's have a quick introduction.
00:00:20.610 - 00:00:26.630, Speaker B: Hi, everyone. My name is Kane from Tyco. So Tyco is EPM.
00:00:27.130 - 00:00:27.880, Speaker C: Yeah.
00:00:29.130 - 00:00:34.520, Speaker D: My name is Yurik. I'm co CTO at Nethermint, and I mostly lead the net related developments there.
00:00:35.610 - 00:00:42.710, Speaker C: Hello, everyone, this is Mohammed. I'm from Scrooge. I'm a protocol researcher at Scrooge, and you guys probably know Scrub.
00:00:43.790 - 00:00:44.442, Speaker A: Hi, guys.
00:00:44.496 - 00:00:48.010, Speaker E: My name is Patrick McCrory. I'm a researcher at the arbitrary foundation.
00:00:51.470 - 00:00:51.882, Speaker C: Right.
00:00:51.936 - 00:01:00.270, Speaker A: So in the description now in the panel topic, we call something roll ups. Can we have each one of you define roll ups in two sentences?
00:01:02.610 - 00:01:26.520, Speaker B: Okay, so I think a very eli explanation of what a roll up is, is you put transactions onto a transaction, you put multiple transactions into one, transactions on L1. And this allows your transactions on the l two to be basically.
00:01:28.410 - 00:01:34.570, Speaker D: Maybe a very succeed version would be Ethereum security, inherited computation scalers.
00:01:35.950 - 00:01:48.160, Speaker C: You guys are very cryptic. I define roll ups as pushing execution off chain while keeping security. Same as l one.
00:01:49.170 - 00:02:11.414, Speaker E: Awesome. Actually, I think defining a roll up is very controversial. If you don't remember all those blog posts about what is a roll up? What is actually a roll up? What is actually a roll up? I think strictly a roll up. This is where you take data and you post into Ethereum. I think that's the strictest definition, but I think there's a much more easier way to explain it. So who lost money on FTX? They'll start off with that.
00:02:11.612 - 00:02:13.880, Speaker D: There you go. I'm so sorry for that, by the way.
00:02:14.330 - 00:02:15.238, Speaker E: Oh, sorry, sorry.
00:02:15.324 - 00:02:16.342, Speaker A: I'm sorry for that.
00:02:16.476 - 00:02:31.978, Speaker E: Great house in Bahamas. But the point is, the whole point of a roll up is, could you have an experience like FTX without having the trusted system operator, can I lock my money in? Can I transact, bring my money back out, but never trust the operator like us, but still have that same user experience?
00:02:32.064 - 00:02:33.680, Speaker D: That's the goal of a roll up.
00:02:35.970 - 00:02:36.382, Speaker C: Cool.
00:02:36.436 - 00:02:55.490, Speaker A: So we understand that roll ups are something that push data onto L1. Now, moving on to someone must be responsible for posting this data. And there's a few designs out there in the space called proof of governance, proof of stake. Could I get all of your takes on the difference between them and what is actually proof of governance?
00:02:57.510 - 00:02:58.862, Speaker B: Can I pass them?
00:02:58.936 - 00:03:49.590, Speaker D: Yeah, sure. Proof of governance is kind of a more recent idea here, where you're saying that the operator set that's actually in the network doesn't necessarily need to be governed by a proof of stake or may not necessarily be permissionless in this case. Instead, it's run by the governance of the L1 contract. And to explain that a little more, the reason is like a special case here is because all of the roll ups rely on a smart contract which is posted to Ethereum. And if you want to update the way that the rollup works, you need to update this contract. And this causes a lot of the issues in rollup design, because it's not just a case that the entire network can come to consensus and change the operation, they also need to change the contract. So they should also be able to sign a multisig or have some other way of mutating the contents of this contract.
00:03:49.590 - 00:04:15.820, Speaker D: And then it becomes a question of is it the operator set that has the control over this contract or is it some broader social governance that has control over the smart contract? And yeah, proof of governance says if you have broader control over this contract, then maybe that broader control should also have control over who the validators are. I think it's quite controversial actually. It depends on whether or not you come to consensus or if you're a base roll up, or if you have proof of stake and so on.
00:04:18.210 - 00:04:19.706, Speaker A: Very good explanation.
00:04:19.898 - 00:05:08.574, Speaker C: So I think proof of governance is basically adopting the very government's work, like the way governments work. We have elected officials who are elected somehow by stakeholders and they govern the thing. And if we adopt that for controlling updates as a role of a smart contract, we are susceptible to the risks that already threaten us in our real life, that prompted us to build this web trading. So that's quite bad. But still people are thinking about layered proof of governance. Like we have a set of governors who make most of decisions, but if they want to go rogue and do something bad, probably we need a mechanism to stop them. And people have proposed mechanism as crazy as this one.
00:05:08.574 - 00:05:30.740, Speaker C: Like maybe we have a decentralized ID and people with decentralized ID can vote. So one person, one vote and stop this governance if it's going to do something controversial, suspicious or bad. So that could work. But I mean it relies on a very strong primitive which is decentralized ID. And God knows when we get that.
00:05:31.510 - 00:05:32.486, Speaker E: I can jump in.
00:05:32.508 - 00:05:33.574, Speaker A: I think for that type of question.
00:05:33.612 - 00:06:08.574, Speaker E: It'S always worth dropping back a bit to wonder like why do we even need proof of stake or proof of governance? So when you think of a roll up system, a roll up is basically just a database, has account balances, program, state, smart contracts, everything. Then the question is, who is the actor that can order transactions, and who is the actor who can execute those transactions and actually update the database? So we call the actor who can order the transactions a sequencer. I'm sure everyone here has probably heard of the sequencer before. Then the executor could generally be one honest party who will pick up those transactions. They're already ordered, they just executed. Then they convince the smart contract. Smart contract.
00:06:08.574 - 00:06:41.818, Speaker E: This is what the database looks like. Maybe that's a zero knowledge proof, maybe that they're going to attest to the state, then it could trigger a fraud proof, et cetera. So now that you have these sequencers and these executors, you need to basically decide how do they get appointed? Why are they the privileged actors that can actually order transactions and update the system? And that's not actually a new problem. We've already had that for like 14 years with proof of work. The whole point of mining is to elect someone that gets the authority to create a block. It could be proof of stake. I put my own money in the game, I self appoint myself, I give myself the authority to actually do this work.
00:06:41.904 - 00:06:43.466, Speaker A: And then proof of governance is this.
00:06:43.488 - 00:07:14.678, Speaker E: Idea where one of you could have the token holders of the system elect and vote for someone to take on that role. So they're appointed, and that's basically the difference. And obviously they all come with their own trade offs. Proof of works bad for the environment, as you mentioned, allowing voters to vote for or appoint people, that's a bit like democracy and governance, and that can be a messy world. And of course, proof of stake. You like the thing that could be slashed depending on how the system is designed. A lot of proof of stake systems have to be slashed as well.
00:07:14.678 - 00:07:16.790, Speaker E: Probably not also a great solution.
00:07:18.190 - 00:08:20.700, Speaker B: Yeah, I think in terms of Tyco's context, proof of governance could be something like a part of our multi proof system. For example, our token holders could veto a particular proof that we might think that is wrong, a particular validity proof that that might be buggy, because we don't know. In the current state of zk proofs, we can't be for certain that they are 100% bug fee free. So if we need to have a kill switch, then we need this proof of governance. We need an alternative proof system to prove blocks, I would say. And for proof of stake, we actually tested this out in our previous testnet, Tyco Testnet, where we used a proof of stake mechanism to select provers, to prove blocks on Tyco L two.
00:08:21.630 - 00:08:22.090, Speaker A: Yeah.
00:08:22.160 - 00:08:41.890, Speaker B: And this could also probably be applied for proposals, decentralized proposals on l two s, where they have to stake something. They have to stake tokens in order to propose a block. And perhaps if they do not propose a valid block, their stake will get slashed.
00:08:44.630 - 00:09:28.302, Speaker C: I think everyone said very good things. Patrick made a very good summary. I think one thing that's important to say for people who were not following the discussion around this, is that the problem with proof of stake and why you are not just doing proof of stake is mean. Initially it sounds interesting because you stake, and if you do something bad, you get a slash. So it sounds perfect. But the problem is we have these derivatives, like liquid staking, and however you do your staking, these derivatives will show up and you end up with a situation that the capital is abstracted. Like the people who have the money and stake are distinct, I mean, separated from the ones who are doing the operation.
00:09:28.302 - 00:09:44.630, Speaker C: And as operator, the only thing that you have to lose is your legitimacy. And when the only thing that you have to lose is legitimacy, why not do proof of governance like it's the same? So that's the narrative that prompted proof of governance.
00:09:47.450 - 00:10:11.920, Speaker A: Thank you for summarizing the difference between proof of governance and proof of stake for us. Moving on to a bit more different topic, but similar in spirit, is there's a sequencer whose job is to sequence. Does a roll up need a sequencer of its own? Can it borrow sequences from L1? Can it borrow make its own sequences? Can the users of this roll up directly go to L1 itself? How does that look like in different designs in the rollup space?
00:10:13.650 - 00:10:38.918, Speaker B: So I actually don't know what a sequencer is because Tyco doesn't use a centralized sequencer, so we have like a decentralized sequencer. So anyone can actually propose a block on Tyco? Yeah, but I guess in the context of other rollups, you probably need a sequencer to propose a block. Maybe you guys can elaborate on that.
00:10:39.084 - 00:11:22.754, Speaker D: As far as understand, Tyco is based, right. So to explain this to people means anyone can propose what the next block is and they just need to submit a valid transaction on Ethereum to include those set of transactions. This makes it completely permissionless. It's a very interesting design, I think, because it does solve a lot of the complexities around the other roll ups. I've also been recently thinking about something called dank roll ups, which is the same thing just to keep it cheaper. But yeah, in Stark, then we've got a centralized sequencer, because the whole intention here is to go as quickly as possible. So they make this trade off saying that why does the roll up provide scalability? Because it's not as decentralized as Ethereum.
00:11:22.754 - 00:11:47.200, Speaker D: It doesn't need to be constrained by the same thing as Ethereum. So it can be bigger servers, smaller set of people, and then therefore big server goes fast. Right. I think this makes a lot of sense as a design, but you still need to trust this one user. And so you tend to need something like a soft finality that's already, I think, swapping some questions about that later.
00:11:49.250 - 00:12:23.738, Speaker C: Yeah, I have a lot to say about this topic, but I try to be brief. So the approach that Tyco is taking, the paste roll up approach, is very satisfying, like it's theoretically very satisfying. It makes everything clean. If L1 reorgs, it doesn't matter if you want to do cross communication between different roll ups. It's easy, everything is easy, it's very clean. The problem is you lose a lot of things that a centralized sequencer can offer. As was mentioned, we already have security.
00:12:23.738 - 00:13:09.770, Speaker C: The only worry is liveness and users want are safe, especially with secret roll up. So we don't need to have that level of decentralization. And if we do with the base roll up approach, like you have to wait for the proof at least and you don't get confirmation. And maybe you have to wait for a big block to be created, then submit it on chain. And a user has to wait like for five, six minutes to make sure that even his transaction is included in unchained batch of transaction. So not too fast confirmation, which you can get from centralized sequencers very easily. Besides that, there are a few problems, especially when you want to optimize the cost of your data availability.
00:13:09.770 - 00:14:00.102, Speaker C: There are a number of ideas for compressing transactions before sending them on chain, like removing signatures, removing nons mapping addresses to indices and stuff like that, that are already done by ZK sync, or even just submitting a state diff. Basically if you do that and you are the sequencer and the prover at the same time, it's okay because you already have the witness data, you can provide those things inside the circuit, inside the proof, and there gets validated. But if we do the base roll up approach, or any approach that separates sequencer from prover, then you can't use those optimizations. So it makes a lot of optimizations around DA compression more complex. But yeah, that is one of, yeah.
00:14:00.236 - 00:14:07.754, Speaker E: I give a little bit of historical context to this as well. So arbitrary was originally technically a BS roll up back in 2020. That was before I joined the team.
00:14:07.792 - 00:14:08.826, Speaker A: And I remember when I saw their.
00:14:08.848 - 00:14:52.570, Speaker E: Design, I pinged edit on. I'm like guys, you're crazy, why are you doing this for? Why would you remove the one thing that makes the roll up so excited and easy to do? But what you get out of that, like when you see the base designs, decentralized sequencers, TVs, committees, what it really highlights is that the sequencer is an optional role. It doesn't actually have to be there and it's very different. So when people look at L2, they always think the layer one mindset and so the kitchen sink in L2, think L2 has to play layer one, but they're very different setups. In a layer one you have to assume an honest majority of the stakers. So in ethereum you want to maximize, you can participate because you need an honest majority. In a L2, you need one honest party, one honest party to produce the proof, one honest party to trigger the fraud proof.
00:14:52.570 - 00:15:46.220, Speaker E: And that's like the safety of the system and the liveness is you just need a way to order the transactions. And so the goal isn't really about who your sequencer is or who your sequencer is not, it's really about what is your transaction ordering policy and how do you go about trying to enforce that. Do you want first come, first serve? Do you want the has to be first? Maybe the most MEB extracted transaction should be ordered first. What's your ordering policy and how does your protocol allow you to enable that? And so maybe there's only, so maybe for first come, first serve, maybe it's easy to have a centralized sequencer because if they break the rule you can just kick them out. Maybe you need a committee and then you have this special protocol where they can't observe the transaction until after it's ordered, which I get first confirmative ordering. But that's what you should really care about, not really whether there's one or two or many different sequences. Just what is your ordering policy and how do you enforce that? Because that's what users will care about in the end.
00:15:48.190 - 00:16:04.770, Speaker A: So jumping off Patrick's last question, the role of sequencing policy and someone should enforce it. Proof of stake and proof of governance team or decentralization seems one design to enforce this policy. Are there any other current designs in the space to enforce this kind of policy apart from several sequences?
00:16:07.590 - 00:16:07.954, Speaker C: Yeah.
00:16:07.992 - 00:16:16.610, Speaker B: So maybe going back to the base roll up we have like a decentralized proposer.
00:16:18.810 - 00:16:19.810, Speaker E: Mechanism.
00:16:19.970 - 00:16:52.640, Speaker B: And this actually allows for actually decentralized MEV extraction as well, because since anyone can propose a block, it means that anyone can order transactions on L two and submit them to L1. And this is pretty powerful because we are able to tap on existing protocols out there, like existing block builder protocols up there to extract meb on L two.
00:16:53.830 - 00:16:54.642, Speaker E: Yeah, I think.
00:16:54.696 - 00:17:22.650, Speaker C: Can I add one? The problem with that is you're flowing all that value to L1. Proposers l two don't get any of that value. Like the proposal. The L1 proposal, in which this transaction of base roll up is included, is going to get all the value from the builders for sending that transaction. Not that this is something bad. It's very it aligned. I like that.
00:17:22.650 - 00:17:32.080, Speaker C: But you don't get any money, or provers don't get any money, so your network doesn't get any money. But yeah, it's an interesting design.
00:17:33.010 - 00:17:55.634, Speaker B: Maybe I can just defend this statement so the prover actually can receive part of the MeV share, because in our latest testnet design, we implemented this prover market where the proposer can actually pay e fee to approver, so that payment can actually be part of the MD share.
00:17:55.672 - 00:17:56.740, Speaker C: That was okay.
00:18:03.670 - 00:18:04.774, Speaker E: What was the question again?
00:18:04.812 - 00:18:13.898, Speaker A: Sorry, I just got caught in the spice. Are there any ways apart from decentralizing my sequence of set to enforce the policy that I've chosen for my order?
00:18:13.984 - 00:18:37.086, Speaker E: Yeah, it really just comes down to your policy. So let's just say it's first come, first serve. That's the ordering policy you want to enforce. The simplest way today is the sequencers are black box. And if you suspect they're doing MeV, because you start getting on chain evidence, you use proof of governance, and you kick them out. That's how a lot of these roll ups work today, especially arbitrum. I know for that one, and one of the offers is sitting over there.
00:18:37.086 - 00:19:18.078, Speaker E: Ikaki worked on a proposal called Time Boost, where you basically set up a committee of sequencers, and then they'll get like its blob, and then they'll order it, and then it pops up the other end. But there you need a committee to enforce first come, first serve. And that comes with a lot of very interesting nuances around that as well, because actually trying to order a first come, first thing, guaranteed first come, first serve is very difficult. If you have an ordering policy where it's maximally enable PBS to its maximum extent, then maybe what you'd want is maybe we want like a BS roll up where you have the builders and basically the proposals. Maybe you'd want something like that set up. So it just depends on what your goal is. And then you'd have to construct a protocol to achieve that.
00:19:18.078 - 00:19:20.750, Speaker E: But a huge space to be explored.
00:19:22.370 - 00:19:34.030, Speaker D: I think a quite simple answer here as well is you can try select. Sorry? You try asking how do you select the set that is going to be the decentralized server?
00:19:34.190 - 00:19:56.250, Speaker A: I'm asking you a more abstract. Is a decentralized sequencer set the only way to enforce policy? So there's a centralized sequence set which says you're doing it right. Are there other ways? Some cryptographic protocols, some committee scheme? I'm just asking the questions, the fundamentals.
00:19:57.150 - 00:20:08.510, Speaker D: I actually not aware of any of this. I want to plug some traces here like you only to rely on your old state. Like you could use Eigen layers set in. But that's not related.
00:20:11.570 - 00:20:37.400, Speaker A: Kind of alluded to my next question. So we have this idea of that we want to have an ordering policy and someone enforcing it. What is my incentive to enforce this policy? Why would I want to do this for you? And also tag on question should the users get a choice in expressing this policy? Or does the role of designer, only the role of designer that gets choose this policy? Can the user send their transaction and their ordering policy? And then the role of has to play a massive order book.
00:20:41.980 - 00:20:42.730, Speaker C: Yeah.
00:20:45.740 - 00:21:13.264, Speaker E: There'S two excited questions here. One is how do you guarantee this ordering policy is actually enforced? And I think it comes down to your assumption of how do they get evicted if they don't do it. If you assume that it's a totally rational protocol and there's no way to evict a party, it's very difficult to not appoint them. Then you really need a cryptographic protocol to enforce it. You really need to tie their hands. So that's the only thing they can do. Today, none of those protocols are implemented.
00:21:13.264 - 00:21:42.968, Speaker E: So we have the weaker assumption that we assume they're honest, but we can get evidence that they're misbehaving. And then you vote them out and use a victim. It's a much more cozier, nice assumption that we have. Allowing users to decide their ordering policy that the sequencer would use. I'm not giving that too much thought, to be honest. I think that's a tricky one, isn't it? Because really the ordering policy is up to the sequencer. It's not really up to the user.
00:21:42.968 - 00:22:46.416, Speaker E: If I send it to the sequencer to say, well, maybe one thing you could do is we're not really about first come break. What you could do is say, should I allow this transaction to be entered into an MVP competition, for example, you could have optionality around that. And that brings us to another point where the sequence, what's unique about roll ups is that, well, not unique necessarily, but what's nice is because the sequencer can take these transactions and hold them for a long period of time, they do have a great opportunity for MEV extraction for per se, or also as they inspect these transactions. So what they can also do is enforce ordering policies where they only allow good MeV, like back running or maybe just in time transaction, just in time liquidity sandwiching, but they don't allow bad sandwiches or front running or et cetera. So there's a whole space there of can we be subjective over how those transactions are included to the betterment of the user? So maybe that's like a tie golf, but I've not really considered can the user decide ordering policy? That could be interesting to explore.
00:22:46.448 - 00:22:47.270, Speaker D: I don't know.
00:22:49.560 - 00:23:24.444, Speaker C: I want to add some more about ordering policy. I'm not really a huge fan of ordering policies. I don't think if it's going to maximize social welfare and it's economically efficient, if we do that and we could extract that value and give it back to users, why just throw that away? While I truly believe that arbitrum wants to be credibility neutral with first serve kind of approach, but people can find arbitram servers, colocate, play these latency games, and that's not fun either.
00:23:24.562 - 00:23:25.776, Speaker E: I can help them that, by the way.
00:23:25.798 - 00:24:36.484, Speaker C: Yeah, absolutely. I give the mic back. So I believe in a market approach, either we have to eliminate these opportunities totally altogether, like with mempool encryption or so on, or we should have a way for users to express their intent, or we should have some ways for market to kick in, like maybe some action to extract this value and then decide to give it back to users or protocol or whatever. So I want to inject one more thing. There was a recent paper in stock 2023 estac is a very theoretical computer science conference, and it was about an impossibility result to do credible sequencing so that you can prove that there is no extraction in the Dex transactions. That was a quite interesting paper for me, because it kind of sets the stage for this paradigm that it's even impossible to do mev resistance or credible sequencing for certain type of transaction. How do you want to stop it when you have all this craziness? So for that reason, I think it's better to go with market here and just decide how do you want to distribute that value?
00:24:36.682 - 00:25:15.824, Speaker E: Yeah, let me comment on that, because it's a really interesting story, actually. So when you consider a centralized, trusted sequencer, that should be the ideal thing. If you consider like an ideal functionality. No, Mev, the ideal world, we have a fully trusted system and a truly trusted party that will enforce the first come, first serve ordering policy. You would think, oh, that's amazing, it's always going to work. Now, what's interesting arbitrum, is that you have this set up, but what they also have is something called the sequencer feed. So every 250 milliseconds the sequencer will form a little block and post it to the world.
00:25:15.824 - 00:26:19.636, Speaker E: And the reason for that is if you're a user, you send a transaction to the sequencer, you look at Etherscan and you can say, oh, my transaction is confirmed, pre confirmation, in a sense. Now what happened with the sequencer feed? Is infuria connected to it, etherscan connected to it, and then Mev baths were like, oh, there's a sequencer feed, I'm going to connect to that. And they were like, oh, I can see the transactions, the most recent transactions that are confirmed. Oh, what if I look at this and backrun them? And so they started back running these transactions, another NBD boss discovered, and next thing you know, there's 150,000 websocket connections of everyone trying to backrun these transactions. So even in this ideal setup, you still have this stupid latency game that emerges because there's still a back burning opportunity. So as part of that acknowledgment, again, this sort of like the time boost work was, I hope it's being presented. I don't know if it is, but what they talk about there is instead of this latency game, could you have an MEV auction? Like a very rapid, short auction? And in their case they want to facilitate just back running.
00:26:19.636 - 00:26:43.452, Speaker E: So you provide a bid for these back running opportunities, then those funds could obviously be fed back to wherever they could go to. And I think they'll use the tip in the transaction to do that as well. That's sort of the goal. But that's cool though, because then you're like, well, either you have a latency game or you have an auction, and now maybe you can start plugging these in in certain parts where latency games pop up. So that's pretty cool. It's sort of like an acknowledgment that latency games are always good. We're going to be around.
00:26:43.452 - 00:26:52.050, Speaker E: So you may as well use an auction. But now the question is like what is the minimal auction that you need? And I think that's a cool question to explore as well.
00:26:53.860 - 00:27:34.344, Speaker D: Also a sort of fun story on the starknet side related to this. And it kind of brings on the point that it's really, really hard to prevent MeV or transaction orders, even if you have a scheme to do so. So on Starknet, for a long time we didn't have transactions. And this is because you couldn't prove that those transactions haven't been included in a valid way. And a few weeks before they fixed this, some parties got this idea, hold on a second. That means I could trigger a transaction consistently as fast as possible trying to extract MeV while doing the computation on chain because it's so cheap. So now we had Mev instead of off chain ordering, they just decided ordering is.
00:27:34.344 - 00:27:55.670, Speaker D: Yeah, I will be between every transaction by spamming it. And. Pretty smart. It almost took down the chain. Luckily we had the solution a week or two later. But yeah, it just gives you idea that the game can always be played somewhere, and if we try to prevent it one place, it pops up somewhere else.
00:27:59.970 - 00:28:25.610, Speaker A: Just adding my point of view in this, I think the way to prevent Mev is every 2 hours we go to every decks and we make every token value zero. So we talk a lot about Mev ordering all the fancy stuff, not with real stuff. Users care about rollers because they give them cheap transaction fees. How do rollers do that? And what factors are affecting the fees that the roll up is offering to their users?
00:28:27.710 - 00:28:58.834, Speaker B: Yeah, I think the fees would probably be affected by how much the L1 transaction cost is when you're proposing a block on L1. So in this case it will be on the proposal side. It will be like how well you can compress the transactions that are on l two onto L1 proposed block. And for zk roll up, when you're approving a block, it's basically how small you can get your approved size to.
00:28:58.952 - 00:28:59.620, Speaker C: Yeah.
00:29:01.350 - 00:29:32.430, Speaker D: I'm going to challenge this because I think that's the current narrative for rollup. And you're saying the real cost is how much it has for L1 inclusion. But the reality is that given any supercomputer, I can find you somebody who will perform that supercomputer to do something useful. Right. So it means that at some point all of the roll ups will also be running at maximum capacity. Right. And then we'll switch from trying to get l one inclusion to having a subset of the fee for L1 security, and then the rest is going to be priority fee for trying to enter into this set.
00:29:32.430 - 00:29:48.900, Speaker D: And I think a broad ecosystem statement. We're a bit naive to think that it's going to stay this way for a short amount of time and not actually very quickly change, especially when you're ordering these things.
00:29:50.390 - 00:30:33.774, Speaker C: Yeah, I think King and Jerry, both of them are right. Like today, the most of cost is the cost of data availability on L1. For us it's 80%, 85%. The numbers are very similar for other ZK roll ups, and we hope that data availability becomes cheaper on L1, but it's not the case today. And maybe it takes two, three years for it to become cheaper. Yeah, the way roll ups make transaction fee go lower is that they make execution cost small, because the execution happens off chain. And instead of everybody executing everything, one supercomputer executes everything and makes a proof and others just verify.
00:30:33.774 - 00:31:11.600, Speaker C: That's the gist of it for regular tools. But I do agree with Jorik that in the long term the cost of generating proofs will be very low. It will be the same as execution. It would be essentially free. Like if research goes like this, and also data availability technology will be matured. We have data availability sampling, and with data availability sampling, the more users you have, the cheaper is to expand the capacity of your data availability solution, which is quite amazing. That's what Celestia is proud of too.
00:31:11.600 - 00:31:30.610, Speaker C: So data availability also becomes cheaper. And then if we have built something useful, it's going to be used by people, by robots, by everything. And then we only have to pay for priority fee, essentially like priority of inclusion.
00:31:32.710 - 00:32:04.074, Speaker E: Yeah, I'm going to try to take a different way to explain this. I was trying to summarize in my mind the best way to explain this answer. So hopefully I get a good attempt at this. I think before we look at the cost of transactions on the L2, we should look at Ethereum first, work out what is the bottleneck, the scalability on Ethereum, and there's three resources that you care about. One is execution, stake, growth and bandwidth. Now we know execution is just expensive on Ethereum anyway. And the reason for that is when I create a block as a staker, if there's a lot of execution there, I pass them, I peer.
00:32:04.074 - 00:32:35.782, Speaker E: He has to replicate his execution before he passes it off. If there's a lot of execution as it slows down block propagation, and if you slow down block propagation, you won't get blocked every 12 seconds, and then some nodes will fall off the network. So execution is like the bottleneck. You're getting blocked across the network, and that's limited, really, by how big you want your set of stakers to be. You can participate in that process. The next one is stakeroof. Stakeroof is the biggest hurdle, that scalability in Ethereum, because it never goes away.
00:32:35.782 - 00:33:02.094, Speaker E: You just keep adding to the state. The database gets bigger and bigger and bigger and it just grows. And that's actually why we basically have the gas limit today. The gas limit is like a proxy to restrict the ability to grow the state. That's like one of the reasons why the gas limit sort of exists today. And really what you're trying to do is you want to maximize this set of parties who can hold this database and check that the data. Ethereum is correct, and so we need a small database for that.
00:33:02.094 - 00:33:52.846, Speaker E: And so the acknowledgment is that while the third resource, bandwidth, is actually the most abundant resource on the network, how fast can you load data across this network and then discard it afterwards? That's basically the idea behind bandwidth. And so roll apps take advantage of the bandwidth of Ethereum. We post our data to the bandwidth, it ends up on the blockchain, but that can be proven later on. It doesn't have to be kept around. So that's why we have roll ups, because we've acknowledged that bandwidth is the cheapest resource on Ethereum that we can leverage to extend Ethereum security to this off chain system. But now we're in the rollout world, and right now, today, data is our biggest expense. So even though it's sort of abundant on Ethereum, hopefully long term of all these DA layers and bank sharding and Celestia, but whatever is going to be popular there.
00:33:52.846 - 00:33:57.966, Speaker E: But that is still our biggest cost today for a lot of these roll ups, even for the ZK roll ups.
00:33:57.998 - 00:33:59.586, Speaker A: No, I think data would be, my.
00:33:59.608 - 00:34:43.230, Speaker E: Gut feeling would be data is still more expensive than proving costs, but maybe these guys can talk more about that. Yeah, that would be my gut feeling. Long term, we still have to deal with stake growth and we still have to deal with execution. In that case, the question is, what sort of nodes would you expect, will you expect in the future? Who can actually check the validity of this database? Because ultimately that's what matters. If you're, let's say, arbitram. Because I'm arbitram today, I want to make sure people in this room can check that the arbitrum database is correct at any time, and you have to basically decide that population size, and that will be what ultimately constrains your scalability. So that's sort of a long answer, but I hope it sort of gives an idea of why the transactions are expensive.
00:34:43.230 - 00:35:00.680, Speaker E: And there will eventually be a bottleneck of until there will be eventually congestion on these l two s as well. And then I guess the question becomes, how many roll ups can you glue to ethereum? Then how many roll ups can you glue to these l two s? And hopefully keep passing the bucket. This scalability problem.
00:35:02.810 - 00:35:25.870, Speaker D: There is an area of the Ck that I don't think anyone's looking at yet that I'd love to see is, can I efficiently get a proof that just a transaction or a subset of the database is correct according to the state transition? And that could help reduce the size of the nodes of the check. I hope that comes along. And then if we have one more minute on this, I would love to get people's take on how to reduce the data that's required.
00:35:31.970 - 00:35:45.480, Speaker A: Say the same question, how do you use the more data, maybe putting more color on it? What is Celeste doing? What is Dangshang supposed to be doing? What is eigenv innovation? What are they trying to do? And can they actually do it? Or is it just.
00:35:48.490 - 00:35:49.446, Speaker E: Actually, I would.
00:35:49.468 - 00:36:09.830, Speaker B: Like to go back to the state growth things. I think for zk roll ups, as long as we submit the last proof for the last block, I feel like that's the only proof of the only state that we have to keep in a node to prove the entire blockchain, essentially. So it's sort of like a constant.
00:36:09.910 - 00:36:10.540, Speaker C: State.
00:36:12.290 - 00:36:16.960, Speaker B: But in terms of DA or celestia and stuff, maybe I can park that.
00:36:17.650 - 00:37:11.082, Speaker D: Well, the thing in this case is then who is incentivized to give you the data that's in the blockchain, right? You may have the proof that the transition is correct, but then you run to the risk that the sequencer posted doesn't give you the actual data. And then the proposed solution to this, I know in the stocknet world, is to post the data before it before. And they say that only a valid state transition, a state transition is only valid if the data of the state dish is in four eight before. And that's clean solution. But it still doesn't mean that you get to request the entire state because you only get the diffs at an extended point in time. So I don't know if it's good enough to trust one person. Like to have a one party assumption that you might be able to get the entire state from somebody, especially if you don't have decentralized sequencing.
00:37:11.082 - 00:37:23.650, Speaker D: It means that this single sequencer doesn't actually send advice to give you the data, because they don't need to. And they can also protect the MEV if no one else is able to do the sequencing. So, yeah, I think becomes very stinky.
00:37:24.650 - 00:38:14.440, Speaker C: Yeah, I think Patrick can speak for himself, but I guess when he mentioned state growth, he meant this, not any of this. He meant that when you're running a node, you have to keep the state in the ramp, the whole estate in the ramp, because you need random access to a state, and that estate grows. Maybe I make a new account today and I throw it away tomorrow. I make another account and another account, and I make these states here and there, and everybody else is doing that. And after 100 year, 200 year, we have like terabytes and terabytes of estate that has to be kept in RAM, like at least the way today's full nodes operate. So you can't fit that all in RAM. Obviously, there are software solutions to deal with it, but fundamentally it has to be solved in other ways.
00:38:14.440 - 00:38:28.280, Speaker C: The way you mentioned is like transactions with proof could be one way. I could think, theoretically that would work, but we don't have any working proposals yet, as far as I know.
00:38:29.050 - 00:38:29.962, Speaker A: Yeah, that's exactly.
00:38:30.016 - 00:38:55.694, Speaker E: And I give a fun example of this. So when I used to work in infuria, I mean, I was a researcher, I wasn't very good at SRE. I respect people who do SRE. They're very special people, a lot of patients. But one of the problems that popped up. So when you look at a co ethereum node today, one thing people don't realize is if you consider Ethereum as a database, you actually have to keep around like 128 different copies of this database. Every block has a new database.
00:38:55.694 - 00:39:29.150, Speaker E: You have to keep that around for reorg safety, so you can quickly reorg, deal with it, and then move on. And relapse will probably have to give or take what won't be the same extreme of that, but you may need to keep around of some of them, or hopefully just. Well, ideally. But what's interesting, the polygon case is polygon also does use go Ethereum in the most part, and they all have the same issue. At one point, their memory was growing two megabytes every second. And I don't know if that sounds like a lot, but at one point it was getting like 15 terabytes. And the SRE engineers are like this isn't going to fit on Amazon anymore.
00:39:29.150 - 00:39:50.002, Speaker E: What do we do with. This is like a real legit problem. But just going back to the original question, there's three parts to it. So the one is how does Ethereum keep the data around for a while? That's like the DA layers and generally maybe just to sprinkle magic dust around it, because they're very hard to explain.
00:39:50.136 - 00:39:51.426, Speaker C: It's a bit like saying, can you.
00:39:51.448 - 00:40:29.034, Speaker E: Take the database, split up in a little fragments, throw the fragments out there and then randomly sample them and retrieve them all? If I need to. So if you're like a node in the network, you hold one fragment, you hold another fragment, you hold maybe the same fragment over there. We just randomly sample and say, okay, I can probably get the whole database, it's probably out there. The next one is the L2, or the rolled up has to send data to Ethereum as a way to broadcast and update to the database so anyone can get a copy of the latest database. That could be the list of transactions. It might be a state def. Those are two approaches right now, and what we're really missing is like a comparison of how well they work in practice.
00:40:29.034 - 00:40:39.614, Speaker E: Are you better off posting list of transactions? Is the state def more efficient or do you want a combination of both? That's interesting. Then the third one is how do these l two s reduce the size of their database?
00:40:39.742 - 00:40:40.420, Speaker D: Because.
00:40:42.470 - 00:41:10.350, Speaker E: You don't want to keep around an account balance from ten years ago. Maybe you could expire that somehow. And in 2017, Ethereum had a whole range of research on state expiry, state rental. A lot of those schemes can I pop up or maybe I could expire data after a year. But if you really want that data, you could prove to me that wasn't the database a year ago and it should be reinstated. And you could do cool stuff like that now. But I think that part is probably one or two years away from people really exploring because it is going to be a problem.
00:41:10.350 - 00:41:13.280, Speaker E: But I don't think it's the most immediate problem in everyone's mind.
00:41:15.250 - 00:41:47.946, Speaker C: I can add something about that. There is a real world example of the scenario that Patrick is describing. Finance smart chain. They took gas, they just reduced the limits. Like instead of 30 million gas per block, they had higher gas limits. And they were running roughly a centralized operation, like maybe six nodes in total, so they could deal with the blood propagation delay as well. And the first thing they encountered was this state gross problem.
00:41:47.946 - 00:42:39.638, Speaker C: And we have a real world example that this happens. So we have to be careful for that. But estate expiry, estate rentals, all those things are nice, but create a new ux that maybe is not appealing about data availability. So I had a talk yesterday in l two days. If anybody is interested, I encourage you to go see that, because I put a lot of force into L two. But yeah, the short answer is, the way data availability layers work is that using this data availability test, because we can't make data availability proofs or data unavailability fraud proofs, fundamentally that's impossible. So the way it works is that at the consensus level, the consensus clients have to make data availability tests.
00:42:39.638 - 00:43:01.066, Speaker C: This could be a knife test, meaning they download the whole data, or this could be an efficient test, they just sample the data. But regardless, that's what's happening in the data availability solutions. So we need to make this test more efficient. That's the goal. And full dunk. Sharding will try to do that. And Celestia and others, they have different ways to make this test more efficient.
00:43:01.066 - 00:44:04.930, Speaker C: But other than that, what rollup can do is compress their data before submitting to L1. As Patrick mentioned, you could just submit a stateive or, I don't know. Arbitrum is using broadly a compression algorithm, a generate compression algorithm to compress the data and then submit it. It gives you like three x four x compression gain. Optimism is using something else, Zstd. We are looking into doing one of these generic algorithms in combination to specialized tricks like mapping addresses to indices and removing signatures and so on. But the problem for Zika rollups is that you have to do the decompression in circuit, and that's going to be a tricky piece because compression algorithms usually operate on these like Hoffman trees, and they jump around in the state, and that's quite complex to do in circuits, but we are trying to do it with a lookup based approach.
00:44:04.930 - 00:44:08.774, Speaker C: And yeah, I know we're on time.
00:44:08.812 - 00:44:30.320, Speaker D: But data risk compression is not going to work that well, because at some point node operators are going to realize that they're getting compressed data and they're trying to put it in already compressed databases, and tends to be that compression of compression doesn't actually give me that much, so they're going to bump the price. And fun fact, we're working on the compression in starks at the moment with Claro. So.
00:44:34.850 - 00:44:35.214, Speaker C: Cool.
00:44:35.252 - 00:44:43.570, Speaker A: I think that brings us to the end of our panel. But I'll give you guys like last closing sentences, and you can only have 26 words in your sentences.
00:44:47.830 - 00:44:48.610, Speaker C: Mmm.
00:44:48.950 - 00:44:53.060, Speaker B: What do I say base boosted roll ups. Go check it out.
00:44:54.470 - 00:45:02.866, Speaker D: I'm going to say something similar. John, have some fun. Come design in the roll up space. There's so much. Please join us in the moment.
00:45:03.048 - 00:45:05.530, Speaker C: Thanks for listening. Think peace.
00:45:08.910 - 00:45:15.850, Speaker E: I think the most important thing to remember for roll ups is that we're building a technology that protects users and customers from system operators.
00:45:20.190 - 00:45:20.602, Speaker A: Cool.
00:45:20.656 - 00:45:21.354, Speaker B: Thank you.
00:45:21.472 - 00:45:22.120, Speaker A: See you guys around.
