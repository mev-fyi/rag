00:00:02.970 - 00:00:41.100, Speaker A: So now I guess we'll get started with the talk. Let me introduce from Axiom, he's the founder of Axiom. And just briefly on what Axiom does, they are building Zha Coprocessor and they're essentially trying to build a Verifiable data, have developer access Verifiable data and be able to perform arbitrary complex compute on top of it as well. So ISU do you want to take it off from here?
00:00:44.590 - 00:01:40.054, Speaker B: Awesome. Yeah. Thanks for the introduction Yuki, and thanks for organizing this wonderful event. So today I wanted to talk about slightly off the beaten track topic which is what ZK can do for developer experience for on chain applications and our perspective on that at Axiom. So to kick I wanted to talk about some of the motivation we had for even starting Axiom, which is when we looked at smart contract applications, we found that they're really data starved. What that means is that smart contracts really can access a limited fraction of all data that's even on chain today. So if you look at a listing page on OpenSea and you try to identify which pieces of on chain data are actually accessible to smart contracts from that page, you'll quickly realize that it's actually just the present owner.
00:01:40.054 - 00:02:30.570, Speaker B: So for this Puzzi penguin, the fact that it's owned by Zac Efron is usable on chain, but the rich transaction history and previous prices are not, although they are trustlessly available to any observer of the chain. And the reason for this is to preserve the decentralization of the chain and allow full nodes to validate transactions easily. So this isn't just a failing of ethereum but more of a fundamental trade off that any blockchain must make. So as a consequence, developers have a pretty painful trade off to make today. If they want to access more data trustlessly, then what they need to do is pay more and store the data in state. So this has two problems. The first is that it obviously costs more.
00:02:30.570 - 00:03:29.520, Speaker B: What that means is that every user has a bit of a negative externality but actually more concerningly. It introduces a lot of complexity on the part of the developer. You have to actually couple your business critical systems, for example, maybe your core mapping algorithm on chain with things that are less critical, for example, how you maybe reward users or your loyalty system. And this violates some basic principles of software modularity and also introduces some security risk. Now your other choice as a developer is basically to reduce the security of your on prem application. You can use a trusted Oracle to introduce data to your application which will definitely work, but will introduce additional trust assumptions for your users. And those trust assumptions might not scale as you compose your application into others.
00:03:29.520 - 00:04:41.350, Speaker B: And so as a consequence, today developers basically have to choose between increased cost and complexity or reduced security. And so we asked ourselves how we can simplify the design of Smart contract applications so that developers don't have to think about all future uses of their onchain data when writing their initial applications. And our answer to this is that we need to dramatically scale data access for Smart contracts. So why do we even think this is possible? Well, blockchains actually offer a different way to access data outside of the consensus that's provided by the EBM. So blockchains all have a property that the current block commits to the entire history of chain. So for Ethereum, this means that the current block commits to all past blocks, and each block commits to the state transactions and receipts inside that block. By decommitting all these commitments, we can access the chain history using cryptography instead of consensus.
00:04:41.350 - 00:05:50.190, Speaker B: Now, how does this work? Well, to actually show that you possess a valid transaction from a block that's 1 million blocks ago, what you have to do is demonstrate first that the block header of that block is valid. To do that, you have to verify a catch act chain of block headers from that previous block header to the current block. Namely, you need to show that the path block has a header whose hash appears in the next block and that block. Has a header whose hash appears in the next block and so on. After you've proven that the path block header is valid, you actually have to give a merkle pushes to try proof for the inclusion of your transaction. In the previous transaction trial and so you might notice this involves a chain of hashes of 1 million blockheaders which is prohibitively expensive in the EVM. So instead, by using the power of ZK, what we do at Astiom is to essentially check this proof in ZK.
00:05:50.190 - 00:06:59.490, Speaker B: So we take this large computation and make a ZK snark that the Merkel Patricia Tri proof was valid and also that we possess a valid chain of block headers from a past block all the way to the current block because DK allows us to compress the verification of this to a succinct computation. This allows us to do this large scale computation in a way which is verifiable on chain. Furthermore, it allows us to compose such computations for downstream usage so we patched this up at Axium into something we're calling a ZK coprocessor for Ethereum. What that means is that smart contracts can query into Axiom to perform two operations. First, to read historic on chain data, and second, perform verified compute on that data. Once we come to a result, we post it on chain and verify a zero knowledge proof that it's valid. After that, a downstream application can use that result with the same security guarantee as data that it reads from Ethereum itself.
00:06:59.490 - 00:08:29.450, Speaker B: And our philosophy at Axiom is that we should always have security that's cryptographically equivalent to Ethereum without imposing any additional trust assumptions on our users. So what's the advantage of this type of design? So first, by reading from Historic on chain data, we allow Astium to actually trustlessly interoperate with existing applications. What that means is that when you're designing a smart contract application, you actually don't need to think about what are the later usages of the data that you're catching in your application. So today applications have to really think carefully about what aspects and side effects of their transactions they might need to reference later. If you don't store that in state, then it's sort of gone forever. For the on chain usage with Axiom, you can exactly read that data and later make use of it even if you didn't anticipate that usage at the time of deployment. After reading that Historic data, we allow you to compute without being limited by the blockchain VM because we're verifying compute in ZK, this doesn't need to respect gas limits or the specific computations that are cheap in the EVM.
00:08:29.450 - 00:09:44.800, Speaker B: So let me just give a rundown of how ZK co processing with Axiom works today and how we think it's actually improving the developer experience. So what we're offering is trustless reads to Historic block headers accounts and contract storage variables on mainnet today and transactions and receipts on Testnet. And on top of that we're allowing computes via custom decay circuits. So I'll go through a few examples of how this might change your actual experience as a developer. So today if you're only using Solidity, then to track the user's participation in governance, what you have to do is that on every single time a user votes, you have to actually track the cumulative number of times they participated in Governance. Then later if you want to reward users for the number of times they voted, you have to read that cumulative participation count from your local contract storage. The disadvantage of this is that you have to pay additional gas to actually update the participation count every single time your users vote on chain.
00:09:44.800 - 00:10:54.686, Speaker B: This obviously increases cost to voting and this can decrease voter participation in on chain governance, which is already challenged due to the gas cost. By using ZK to read the history, we can change the design of this application. Instead of actually tracking the cumulative vote counts, all we have to do is trustlessly read the Historic vote events for each user. Then, when a user wants to claim a reward for historic voting, all they have to do is read these events and compute the number of times they actually voted. After proving that to a smart contract, the contract can reward the user or give them special rights, depending on whatever the developer wants to do after the fact. In a second scenario, suppose you wanted to make uniswap swap fees dependent on the volume that a user has traded in the last week. Today, to make that possible, you'd have to track for each user their cumulative volume on every trade.
00:10:54.686 - 00:12:01.720, Speaker B: What that means is that every uniswap pool would have to maintain a list for every user of the total volume and keep that updated every single time it makes a swap. What that means is that every swap is going to be more expensive for each user essentially because the volume updates might be comparably expensive to the actual swapping itself. Obviously that's unacceptable. But as a result so if you were to make such a system, you'd be able to read the cumulative user volume and then compute fees dynamically. If we use ZK to implement such a system, we can invert the architecture. This would allow us to make no changes to the current uniswap system, but instead trustlessly read historic trade events for each user. By processing those events we can compute the total volume that a user has traded and add up that volume to determine the fee rebate that a user is entitled to.
00:12:01.720 - 00:13:35.350, Speaker B: Once every user does this, they can submit a proof of their historic activity to a contract and trustlessly obtain whatever volume rebate that the system decides they are entitled to. So as you can see in this case, typically in smart contracts today, you actually have to bake into your core application logic of swapping the idea that you might care about volumes in the future. But by using ZK we can remove the need for developers to actually do that instead. Later on, once we decide to develop this volume based fee rebate system, we can go back and retrofit it onto our existing application without making security critical code changes to the core swapping logic. And just to give one more example that I'm pretty excited about, if you look at Airdrops today, if you want to make Airdrops fully trustless, namely not dependent on the actions of a centralized team or token distributor, what you actually need to do is tally all potentially useful user actions on chain. What that means is that when you deploy your smart contract you need to know the space of actions that you might later want to reward or incentivize and track for each user the extent to which that user actually took those actions. That might be pretty expensive and it might also be hard as a developer for you to anticipate.
00:13:35.350 - 00:14:56.590, Speaker B: Typically, when you're shipping the first versions of your application, you're more concerned with your core business logic and with just making sure everything is secure and probably less thinking about the space of future incentivization you might want to enable. As a consequence, if you later want to compute rewards on chain, the only things you're able to use are the information that's being tracked in the state of your contact. You see this today in the form that most rewards are simply done by staking some type of fungible ERC, 20 token. Often that doesn't actually match the incentives that application developers want to set, but it's frankly the only information that they have available to make these choices. Using ZK, we offer smart contract developers much more flexibility. Instead of specifying the space of data that they want to take into account, the developers can trustlessly read all historic user activity on their application. And after the fact, when they want to design an incentive or rewards program, they can simply compute rewards based on arbitrary custom criteria over the entire history of activity in their application.
00:14:56.590 - 00:15:55.630, Speaker B: So we think this is a much more flexible approach and also enables developers to have a much more modular design to their application architecture. So let me close by just talking about where we think DK Coprocessing is going. So we're already offering trustless reads to all on chain historic data. And we think the next step is to offer view function simulation by running a ZKE EVM proof on top of the storage reads that we're already providing together that will create a ZK archive node or indexer, depending on your terminology, on top of the data returned by this system. We think it makes sense to offer users arbitrary computation via ZK native VM. So we'll have more to say in this direction in the next few months. So watch out for our upcoming releases.
00:15:55.630 - 00:16:30.520, Speaker B: So that's all I have for today. We're Axiom. We have a ZK Co processor for Ethereum live on mainnet and you can check out our docs and demos. I wanted to mention at the end that we're running a new program called the Axiom ZK Intensive, which is targeted at helping smart contract developers learn to develop ZK enabled applications. So applications just opened for this program and are closing September 18 and you can apply at the link below. Thanks for the time and happy to take any questions.
00:16:32.270 - 00:17:11.080, Speaker A: Thank you. Thank you very much. So I guess to kind of just kick it off and on the questions. One thing I was kind of curious was like, for the arbitrary compute right now you have specific circuits for maybe specific functions, maybe some hashing functions, some arithmetic operations and whatnot, what is the sort of like the thinking behind shifting towards more like a ZK native VM as an approach to enable those compute? Instead of, let's say what you're doing right now is just like building more specific circuit, what is the consideration there?
00:17:12.170 - 00:17:47.550, Speaker B: Yeah, we're always actually driven by the developer experience. I actually don't necessarily think that SDK native VM is always easier for developers, but we do think in some cases it's going to be a little easier. So our point of view is that actually we think about what are the developer frictions in deploying actual functions that they might want to use on chain today? I actually think it's probably not a VM, it's more just having all the primitives that they're used to being implemented in ZK and also being able to easily express those primitives.
00:17:48.390 - 00:18:04.790, Speaker A: I see. So in that case you could see a scenario where with enough specialized circuits that perhaps the ZK native VM wouldn't be as useful in that kind of scenario.
00:18:05.690 - 00:18:32.110, Speaker B: Yeah, my feeling is the main criterion is actually not necessarily whether you have a VM or not, but more how fast you can implement and also how fast you can have your circuit audited and be really sure that what's being proven on chain actually matches the application logic you're interested in. So VM definitely helps for that, but I think having a very clear sort of circuit language and framework can also achieve very similar goals.
00:18:33.010 - 00:18:53.330, Speaker A: Gotcha and also one more thing that I also wanted to ask about was the Zkevm proof that you mentioned towards, I guess, the second to last slides. Could you elaborate a little bit more on what do you exactly mean by Zkvm proofs?
00:18:54.170 - 00:19:51.800, Speaker B: Yeah, so right now, when you use ZK to read data from your application, you kind of are introspecting about your application state, but what you have to do is operate on the raw data underlying your application. So in some cases, this is very straightforward. If you have a mapping, for example, for balances, that's a mapping from addresses to numerical values, and that can be read very directly. But actually, for some contracts in the wild, the balance computation or other computations are implemented via function view functions that do some pretty simple computation on top of the storage access. And we think the ultimate developer experience will be to exactly run the solidity or EBM computation that's deployed on chain on a historic state. So that, unfortunately, is much more complex. It requires you to actually simulate the full Zkbm on historic state.
00:19:52.670 - 00:20:25.730, Speaker A: Gotcha and then I guess one question that I saw was asking, will this benefit stateless Ethereum? So I think the question is probably more so asking about whether this is applied to a stateless chains, or if it is just specifically Ethereum itself, or if you have any plan around working outside of Ethereum, or if it's just constrained within Ethereum frameworks.
00:20:26.310 - 00:21:26.070, Speaker B: Yeah, we actually spoke a bit with the Ethereum stateless client team, and I think what we're doing is somehow parallel to the goal of statelessness, but some of the constraints are a little different. So you can view the data we provide as we're using ZK to give a cryptographic proof of the validity of that data. And in a stateless client, you would use probably not VK to give a proof of every data access. I think the differences in having a stateless client versus what we're doing in ASEAN is the stateless client would probably access a lot less data per block, but the latency requirements are much harsher. So in a stateless client, you have to be able to generate the witnesses in under one block, so you actually have a time budget of maybe seconds. Whereas at Axiom, we're enabling contracts to do much larger scale computations and okay. Does not take just seconds.
00:21:27.850 - 00:22:18.470, Speaker A: Got you. And then also, I guess one thing that I was kind of curious because I think when I go through the decks you also talk about arbitrarily complex compute on top of verifiable data. Now obviously that is a pretty broad statement but at the same time I believe that's also what it's intended for. But when I was thinking about arbitrarily large compute, what's sort of the bound are we looking at when we say arbitrary large? Are we constrained by, let's say, the amount of provers that we have to support this kind of compute or what is the constraints here that we're talking about that sort of access the bound for those kind of like arbitrary large compute?
00:22:19.130 - 00:23:03.346, Speaker B: Yeah, generally we find actually the bound is just how much latency people are willing to tolerate. Obviously if you're doing more computes, proving its validity will take longer. I actually think that typically the number of provers isn't necessarily a bottleneck just because the function of proving isn't fully parallel. So you can definitely put some good number of provers to use but before you hit your parallelism bound you're probably going to hit your latency bound. Got you. There's not really a conceptual bottleneck as we expect proof systems to keep getting faster. We think one of the main advantages of this type of architecture is that the compute isn't going to be gas limited.
00:23:03.346 - 00:23:21.690, Speaker B: Like fundamentally if you run compute on a roll up or any blockchain VM you're limited by things like the Sequencer and DA and this type of thing. Whereas in this sort of architecture those aren't really factors because you're not operating in this shared compute context.
00:23:22.830 - 00:24:16.720, Speaker A: Got you. And then one more thing that I wanted to also discuss briefly was about the I guess as you mentioned, a lot of the latency aspect. As you have said, latency is a major concern for users who are using Axiom and essentially kind of trying to see what kind of latency is acceptable for certain applications and what kind of latency may not be the case because I presume that some application might be more latency sensitive while others maybe not so much. So I guess in that spectrum do you foresee that Axioms to be initially catering for more, let's say, latency sensitive ones, lesser latency sensitive ones? And how do you think that the latency game will kind of change over time?
00:24:17.970 - 00:25:17.630, Speaker B: Yeah, in general our feeling is that as proof systems get better, latencies will go down across the board. I think that one important distinction is that any sort of ZK based data access or computation will always be asynchronous. So the advantage you get from running something on chain is that you really get access to synchronous changes in the state in the previous transaction. And that's something that no ZK based system will be able to do simply because you have to generate a proof relative to some change state. And obviously you don't know the change state of an included transaction when you're submitting your transaction. Now, in terms of applications today we actually found that things aren't necessarily so latency sensitive. And I think the reason for that is that if you think about non blockchain computation, the main synchronous latency sensitive computations which exist in the world are actually in financial exchanges like at Nasdaq or CME.
00:25:17.630 - 00:25:55.840, Speaker B: Whereas if you think about almost all other computation, it's actually asynchronous. Like if you hit a web server in the microservice model, it's going to make a ton of async calls to other services and sort of react on callbacks to them. And we think that Axiom is fundamentally building that sort of system. But on chain now, of course users don't want to wait, let's say an hour for results and so we're tackling more use cases where the user has already accomplished something and they're getting a reward or requesting some sort of message that already takes a significant amount of time in alternatives as well.
00:25:56.930 - 00:26:32.620, Speaker A: Got you. That makes sense. Another aspect that I wanted to also touch upon was that the proof scheme. So I think today you might have not discussed too much about a lot of the proof scheme aspects, but I think it could be would love for you to share a little bit more about what kind of proof scheme that you are using right now that you will be using in the future. Or perhaps different ones or the same ones or whatnot if you could elaborate on that as well.
00:26:33.230 - 00:27:05.554, Speaker B: Yeah, so today we're using halo two with a KZP back end so that's the general clonkish arithmetization on the front end and KZD commitments on the back end for the polynomial commitment scheme. Yeah. So we're looking into a variety of more modern proof systems including things like folding large lookup schemes like CQ or Lasso and also Fry with small fields. So hope to have more to share about that in the next few months.
00:27:05.752 - 00:27:32.630, Speaker A: Gotcha looking forward to that as well. So I guess on my end that's it on the questions and stuff. But again, thank you E for coming to the talk and giving been sharing about the Axiom and at the same time for all the attendees here, please feel free to follow e on Twitter and also check Axiom.
