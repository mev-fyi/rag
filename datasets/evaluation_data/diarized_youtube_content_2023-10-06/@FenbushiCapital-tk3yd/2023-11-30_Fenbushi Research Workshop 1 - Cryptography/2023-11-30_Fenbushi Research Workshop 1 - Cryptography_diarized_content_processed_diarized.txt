00:00:09.820 - 00:01:19.092, Speaker A: This is Yuki. I'm currently doing some research work at and I wanted to kind of have this research workshop as an opportunity for some researchers from different backgrounds like him, from academics and other engineering heavy backgrounds and basically a bunch of other different researchers from different places to share ideas and discuss about things. So I am hoping that we can make a lot of the conversations be more flowing discussions and whatever. So again, feel free if any of you guys have questions to the speakers researchers, and yes, just raise hands, can ask whatever you want. We'll also make sure to have any question times later on as well so that we can clear up any confusions if there was from the content. But yeah, just don't hesitate if you have any problems that you want to discuss. So that's the intention here.
00:01:19.092 - 00:02:24.040, Speaker A: Now, I guess just to kind of go over the schedule for today. As Chloe mentioned, for the first part we'll have I think what, three, four guests who will share their work and then later on we'll have a dinner for like probably 30 minutes and then come back and we'll continue onwards. And I think for probably the later half of the conversation it will be quite a bit of like maybe some prover coordination related stuff to some projects that will also share some of their ideas around. So yeah, basically a lot of stuff popping up. So please enjoy, feel free to just ask questions, whatever you want. And yeah, let's get this going. Now first speaker will be Isavan who will be sharing about, I believe it's the nature proof, is that right? Yes.
00:02:24.040 - 00:02:54.308, Speaker A: So yes, it will be nature proof. Now give it a round of applause. Okay, so in the next 30 minutes, and as Yuki said, feel free to interrupt me anytime. So if you don't understand something, it's most likely not you but me. So yeah, I don't want to left anyone behind. So yeah, just feel free to raise your hand and then Yuki or Cleo will give you the Chloe, sorry, we'll give you the mic and then you can ask your question. So I'm going to talk about nacare proofs which is already online.
00:02:54.308 - 00:03:54.020, Speaker A: So if you go to this URL, you can read the paper or just scan this QR code. This is a joint work with me from University of Maryland and Max Bank Institute in Bohom and with Joseph Bono who is a professor at NYU and a research partner at Httmz. And this work was done at http z during the summer when we did our internship with Amy there. So the motivation is that this is the takeaway message, is that you don't need to verify zero knowledge proofs or proofs in general on chain. This might sound crazy and stupid, I don't know if you see it in the background, but hopefully I will convince you that you don't need to verify them and the reason we don't want to verify proofs on chain is that because they are costly. So this is just an example. It's a smart contract taken from Starkware.
00:03:54.020 - 00:04:39.792, Speaker A: I don't have any beef with Stark. This is just an example. This also applies to ZK sync or it also applies to Aztec or whatever LTO you want to mention. So as you can see, hopefully in the first rows, you can see this smart contract in the last three months burned more than almost $200,000. All this smart contract does is just verifying proofs. And so the bottom line is that verifying proofs on chain is costly. And the other observation is that most of the time these proofs are correct anyways due to financial reasons or reputation.
00:04:39.792 - 00:05:23.492, Speaker A: You don't want to harm your reputation, so you will not post invalid proofs anyway. And the idea is that 99% of the time these proofs are correct. So we only want to interact with the blockchain. If there's an incorrect proof, and our observation is in this paper, what we make is that it's way more efficient to prove that the proof is incorrect rather than checking the proof directly. So I will give you some examples later, and then it might be clearer. Okay? So for this audience, this table might be trivial. This is just to give you an idea.
00:05:23.492 - 00:06:09.096, Speaker A: Like if you want to verify a growth, 16 proof or plonk proof on chain, it might cost you like $8 or something. These are gas prices taken a few days ago. The exact numbers doesn't really matter. The idea is that if you want to verify these proofs on a daily basis, like if you are Aztec or, I don't know, starquare or something, then you need to verify these proofs, like dozens of times per day on a daily basis, and you will burn lots of dollars. So what is our system model? So we have a proverb. Prover, pablo Picasso. And this prover wants to prove some statement to the Verifier Vincent.
00:06:09.096 - 00:07:11.460, Speaker A: But Vincent, he's a great painter, and he's busy painting good stuff like this Scary Night, which you can see in. But so Vincent is busy with his painting. So Vincent doesn't want to check the validity of this proof because he has better things to do. So what Vincent does is says that outsources the verification of this proof and says to this naysayer, it's kind of like a bird joke. I don't know if most of us English is not our first language, but in English, the horses say nay. In Hungarian, they say Nihala anyways, but the horses in English say nay. So if the proof is incorrect, then this horse will send a proof showing that this proof is incorrect.
00:07:11.460 - 00:08:00.790, Speaker A: But we define some delay some time parameter, and we only accept such naysayer proofs within this time parameter. Let's say one day or something, the naysayer will only come if and only if the proof is incorrect. Okay? So what do we gain? What we gained is that 99% of the time we don't do anything. If the proof is incorrect, we don't do anything. We don't touch the blockchain at all because we don't need to do anything. If the proof is incorrect, then there will come a fast and furious horse and it will send us and save us with this Naysay approved. Okay, so now we cut the costs of verification by 99.9.
00:08:00.790 - 00:08:52.020, Speaker A: But we have some added assumptions. What are the added trust assumptions? Is that now we need to assume a bounded delay model. So if there's no bounded delay, let's say the adversary, the NSA or KGB or CIA, whatever, can delay arbitrarily the messages, then we are screwed because this Naysayer proof will not arrive to the blockchain in a timely manner. And if it would arrive after the time bound, then we are screwed. So we need to assume like a synchronous communication model, obviously we need to assume that the underlying blockchain is sensorship resistant. If the miners doesn't want to include these Naysayer approves, then again, we are screwed. And we also assume that there's at least one honest horse who watches the blockchain and will send this Naysayer proof.
00:08:52.020 - 00:09:42.600, Speaker A: Okay, so this is some really simple formalization for you. Do I have a laser? So an ACR proof only makes sense in relation to Ni ZK, a non interactive zero H proof. So given an ni ZK pi, we define a corresponding naysayer proof. And this Naysayer proof will have three algorithms the Naysay and the Verifier Nay. Most likely you don't really see it. One important thing that you should notice is that the Naysay algorithm sees the instance and the pie. So we assume that the Naysayer somehow already have access to the proof instance, the proof itself, and the instance.
00:09:42.600 - 00:10:50.412, Speaker A: And the Naysay algorithm outputs this approved Pi. And the verify Nay algorithm will take as input x Pi and the Naysayer proof and will output zero, meaning that the original proof is incorrect or will output bottom, meaning that we cannot decide whether the original proof was correct or incorrect. Okay, I have some correctness and soundness definitions, but it seems that I lost some of you, so maybe I will just skip it informally. Correctness means that if there's an incorrect proof pie, then an honest Naysayer should be always able to convince the blockchain that the proof was indeed incorrect. And the soundness in this case means that if there is a correct proof, then no adversarial Naysayer should be able to convince the blockchain otherwise. Only if it's a negligible probability. Okay, so this was just super simple stuff.
00:10:50.412 - 00:12:02.820, Speaker A: It just looks complex, but maybe you wanted some formalization after, I don't know, 05:00 p.m. So some frequently asked questions like what's the difference between NASA? I mean, feel free to interrupt me anytime again. So usually the questions I got is that what's the difference between nacer proofs and fraud? Proofs. So fraud proofs, if you know this notion, what they do is that there's a computation and there's this bisection game between approver and the verifier, and they try to locate in the proverbs computation the point where they disagree. So again, they work over the proverbs computation trace, and instead naysayers try to locate a disagreement in the verifier circuit. And as you can imagine, in most zero lodge proof systems or proof systems, the verifier circuit is way smaller than the prover circuit. So generally what we expect is that the Naysayer proofs will be much faster than fraud proofs because we don't work over the proverbs circuit, but the verifier circuit.
00:12:02.820 - 00:13:04.920, Speaker A: Okay, so why storing proofs at all? So you could say like, if we are optimistic, then let's be super optimistic or optimistic squared, and then we wouldn't have any proofs at all. Well, that wouldn't really work. So we still need some proofs in place because if there was no proof on the blockchain, then we cannot really catch the liar because the Naysayers should re execute the computation and everything and reexecuting the statement. And the proof is way more costly than just checking the proof off chain. Okay, so can you give me some examples of Naysayer proofs? Yeah, wait, two more slides. Is there Naysayer proofs for so is this a meaningful notion? Is this a meaningful notion? And I will show you that every MP language has an AC approved. I mean, the precise statement is the following.
00:13:04.920 - 00:14:04.940, Speaker A: There is a really classic result from the 1990 or 1991 by Faigalapido and Shamir, and they prove that for every MP language there is an ni ZK that uses graph three coloring. And so given this specific ni ZK, there exists a Naysayer proof which is constant time verifiable, and also the proof is constant size. So basically, the Naysayer would just point to the edge in the graph where the graph coloring fails. So it's really easy to show that the graph three coloring is incorrect. The Naysayer prover would just, hey, this edge is not colored correctly. So now we basically prove that for every empty language there's a constant time and constant size naysayer proof. Okay, so here are some examples.
00:14:04.940 - 00:14:46.676, Speaker A: I have three examples today, like one with shuffles, the other one with Fries, and the third is with Boss quantum signature. Sorry. So you don't need to know what is Fry? Fry is a polynomial commitment scheme and it's used extensively today on the ethereum blockchain. So if you know what is a merkel tree, you are fine. So basically, if you just zoom out what the verifier does in Fry, it just does a bunch of hashing. I'm not going to do the details, but it doesn't matter now. So the verifier just does log squared.
00:14:46.676 - 00:15:58.224, Speaker A: So if there are n leaves in this merkel tree, the verifier needs to do log squared n roughly hashing. So this would be the cost of verifying a single fry polynomial commitment scheme opening proof. But instead, if this proof is incorrect, it's really easy to show that this proof is incorrect. The Naysayer prover can just point to one of the nodes, intermediate nodes in this merkle tree and say like hey, this is where the hashing fails. Or if you hash these two nodes then you will not get the parent node. So again, the original proof size is lock squared sized, but the Naysayer proof size is constant time, constant size and also concretely, much more efficient to verify on chain. Okay, so a shuffle proof is like if you have N commitments or Algamar ciphertacks and you want to shuffle them and you can create a proof which says that these two lists are basically the re randomization and the shuffling of each other.
00:15:58.224 - 00:17:00.036, Speaker A: And the current state of the art gives you a square root N size proof. It's a paper from 2012 by Buyer and Growth and Ethereum would like to do something similar that will use Verifiable shuffles. So it's a language, it's a proof system that we really care about because in who knows, ten or 510 years it will be used on chain. And if we have N participants, then the proof size is fed them. So really large and kind of this is the state of the art, it's not really true, but I'm not going into the details. So the proof is really large and also the Verifier time is like on chain is unbearable. So if you have like hundreds or thousands of validators, you'd need to do like four times this number of exponentiations in a smart contract, which is not going to happen.
00:17:00.036 - 00:18:22.328, Speaker A: So instead of verifying that this proof is correct, this proof of Verifiable shuffle, it's way easier to show that this specific shuffle is incorrect. Basically the Naysayer could just point to two elements in this list and say that hey, these two list elements are not correctly shuffled because and then the Naysayer will tell us why it's not correct. Know that there's a nuance here because we assume in this case that the Naysayer approver knows the discrete logarithm of one of the shuffled elements in the list. We can talk about it maybe during dinner. So another application that we came up with is post quantum signatures. Like recently in the last few years, NIST, the National Institution for Standardization, they standardized a bunch of post quantum secured digital signatures and they are also interesting for us Ethereans as we march towards account abstraction because in the future users could define their own signature scheme, whatever they want to use on chain. And it's likely that some of us want to use post quantum secure digital signatures.
00:18:22.328 - 00:19:10.120, Speaker A: And Swings is one of these signatures that were standardized by NIST. And the downside of all these post quantum signatures is that they have large signature sizes and or public key. So again, these are the numbers you can read them on the slide. And also in sphinx. Sphinx is really similar to Fry in the sense that if you want to verify a Sphinx digital signature, what you need to do is you just need to do a bunch of hashing. Like in this case, you just need to do 67 hashing. But if you don't want to verify this digital signature, what you could do is if there's an incorrect pink signature, you could just point to the index where this hashing failed.
00:19:10.120 - 00:20:08.140, Speaker A: Again, the Naysayer proof will be just two elements and a single hashing operation, which is on chain is really efficient. Okay, so what are the storage considerations? If you remember, I told you here like a few slides ago, maybe it's not worthwhile to go back, but we assume that the Verifier in our applications, the Naysayer Verifier would be a smart contract. Most likely the Verifier has input the instance X and also the proof Pi and the Naysayer proof. And this is a little bit problematic because today. So this is not the right audience. You all know this, that in Ethereum we have kind of three types of storage. We have persistent storage, call data, and data willipid service dos.
00:20:08.140 - 00:20:59.000, Speaker A: And usually we don't use the persistent storage to store proofs because it's too expensive. So rather, what projects like Aztec and Stark and all these nice projects do they store all these proofs in the call data. But the problem with the call data is that it's not persistent across transaction calls. So let's say that the prover puts down onto the blockchain the proof in the call data. Then this proof will not exist by the time the Naysayer would come along. So that's problematic for us. But in the future, the vision is that there will be some so called data availability services where we could store the data, the proofs, and on chain.
00:20:59.000 - 00:22:18.328, Speaker A: Instead, we would have like, polynomial commitment or vector commitment to the proof. And then the Naysayer could basically open this commitment to certain positions and prove some statements about the proof itself. So kind of that's the vision. How we envision that this paradigm could be used in the future of Ethereum because now it cannot be really used because the call data is not persistent across different transaction calls. Okay? So if you are interested in a practical implementation of Naysayer proofs, or you would be interested in deploying this on Ethereum today somehow, then hit us up some future work and food for thought for you developers and researchers, I think Naysayer proofs might be interesting also on their own as a notion in complexity theory. So it would be nice to approve statements about complexity of the NASA approvers efficiency or Verifier's efficiency. If we can come up with other interesting applications for NASA approves on Ethereum today, let me know.
00:22:18.328 - 00:23:10.570, Speaker A: I'm interested to hear your ideas. For instance, a stable matching. Like, it's easy to show that the matching is unstable, or in more general, any language in CONP might be interesting for this Naysayer paradigm. There might be many extensions naysayer proofs, like interactive Naysayer proofs, probabilistic naysayer proofs, naysayer proofs with non zero sound necessary, and so on. Sky is the limit. And obviously, if you want to deploy naysayer proofs in practice, then it would be necessary to have a proper game theoretic analysis, because for an incentive compatible deployment, there will be like many parties, there will be provers naysayers and so on. And without incentives, there will be no proper deployment of this proof system.
00:23:10.570 - 00:24:07.230, Speaker A: So if you're a game theoretician and want to think about it, I'm super excited to talk about it and think together. This is our email, or we can just chat during dinner, and I will be also here the whole week. Thanks for your attention. All right, thank you very much for the native proof. Oh, I guess I was going to ask question, but I think mara? Yes, I have a question. I'm not sure if you already thought about this. It but in the case that someone submits let's say we do a roll up with this and someone can you detect a fake proof and you submit that fake proof, how will you handle the state? Like, you just have to reverse everything.
00:24:07.230 - 00:24:51.560, Speaker A: Can you do something more, maybe, say, okay, this is wrong in particular, so don't go around trying to restart everything. Honestly, I haven't thought about it much. Like, it's really application specific. So your question is, in case of a rollout, you should just roll back the whole batch of transactions or just a single transaction? That's the question. Yeah, if you have to do how to handle those cases. We are academic people, so we can just say we leave this for future words. But this is really I mean, it's a good question, don't get me wrong, but it's really application specific consideration.
00:24:51.560 - 00:25:37.768, Speaker A: It's not something that you could generalize. We didn't think about it. It's a good question. And I guess what I would do, I would just roll back that specific transaction if you can locate that specific faulty transaction. But the thing is that there might be multiple. So we didn't go into the weeds of all these practical considerations, but Eigen Layer suggested this to work on during the week. So if anyone is in the Eigen Layer house or programmable trust house, and if you're interested in working on this, you can think about all these practical considerations that Mauro brought up.
00:25:37.768 - 00:26:21.596, Speaker A: I don't have a good answer, but I think maybe there's no good answer. It feels like more of an engineering problem to me. Okay, perfect. Thank you. By ZK. And let's say let's call it IP. Is your result kind of like specifying is your result basically? Can I interpret as specifying some properties of co IP? If so, how can we formalize? If not, what is? Why not? So you are saying that naysayer proves give some kind of give me the impression because you're proving something opposite, right? Actually, by the end you mentioned the co nt.
00:26:21.596 - 00:27:13.028, Speaker A: So I'm just thinking about this co complexity class, like how we formalize this thought, how you can generalize this result. Good question. I don't know. Yeah, that would be the handbag answer. Yeah, it feels super related to anything when being who IP but we didn't I mean I'm not a complexity theoretician, but I would feel that all those results that characterize co IP or coamp could be related to the complexity of naysayer proverb verifier complexity. Yeah, if you want to think about it. I don't know.
00:27:13.028 - 00:28:23.244, Speaker A: Well, actually, let me throw you an easier question that you could maybe answer, but I think it might be a bit more context specific here. But have you kind of delved into what's the kind of time bound for formalizing this natan or jump into nay? I think this is also super application specific, so I don't think there's a universal bound you can give for every application. Like this should be the time parameter that is used by rollouts or I guess maybe some experimental evaluation of let's say if we do apply this type of thing on some Dkvm or whatever robots out there might be, or maybe even just some DK application. Currently there's no implementation of this as far as I know. And so yeah, there's no I don't think there will be a universal balance in the first place. In the second place, it will be super application specific. I mean, there will be many factors to consider.
00:28:23.244 - 00:29:52.296, Speaker A: So it again seems like another roadblock to deployment of the system. That how to choose this parameter in a way that we can ensure the soundness of the proof system. Because you don't want to make it too large. Because then you can roll the state quite slowly but you also don't want to make this time parameter really small because then maybe the network will be congested and then the Naysayers cannot submit the proofs. I guess something interesting that I also kind of want to point out is that because currently a lot of the Ziggy rods kind of spend some time window to bash the proofs and then roll down to the proofs as of right now anyway. So to that point, having some time window as a way to kind of reduce the re execution cost could be an interesting exploration for those who already in terms of action submitting to an L1. In contrast, you could also explore with the likes of data you approved to essentially execute that with the same latency or even less potentially, but any other questions here? Yeah, this might be a little bit of a silver question, but essentially what's going on here is you're just posting the proof and then potentially executing it later.
00:29:52.296 - 00:30:48.510, Speaker A: So it's essentially optimistic you're not verifying sorry, you are not executing or verifying it. Is there like a granular difference between just are they just re executing the proof or is the naysayer proof a completely different computation? So most likely what the naysayer will do is just try to verify the proof of chain, and if it accepts, then does nothing like sit back, relax, and enjoy life. If the proof doesn't verify off chain, then you try to locate this verification circuit. Let's say, like in case of Plonk, you have, I don't know, three pairing checks. Like, which pairing checks failed, for instance. And then you could tell the smart contract that, hey, try to execute the second pairing check because it will fail. And then the smart contract will re execute only that specific check.
00:30:48.510 - 00:31:24.600, Speaker A: I'm not sure if this answers. Yeah, that definitely answers. And is that just an efficiency thing to only check that one part? Yes. And you only check it when the proof doesn't verify because currently, like today, right now, on Ethereum layer one, you check all the proofs even though they will verify. So you could save like 99, 99% of the work. Plus, even in the malicious case, you would do only a percentage of the Verifiers work because you just check a single step or a few steps. Very cool.
00:31:24.600 - 00:32:04.070, Speaker A: Thank you. Is it possible to do an ATM proof inside ZTP to prove that a smart failed? What do you mean by Sinai inside a Plunk proof here, which is failing. I want to recursively prove that that failed inside another proof. You don't need a VM. Sounds cool. Sounds cool. I don't if know it's possible.
00:32:04.070 - 00:32:23.736, Speaker A: So you mean like recursively. You want to prove that I'm going to try and recursively verify to proof A if it fails, I want to prove it failed? Normally, if you do, like, an aggregation scheme, the circuit just fail. If you try and do that, I.
00:32:23.758 - 00:32:24.890, Speaker B: Think it's possible.
00:32:31.330 - 00:33:24.610, Speaker A: But then the whole batch would fail. We don't want the batch to fail. We want to prove that one transaction has an active we'll talk to you later. I hand it over to all right, one more question. Maybe one more question. All right, there you go. I know you touched on this briefly, but what are some of the applications that you guys are imagining? I think one cool application could be Ethereum wants to use, like, Verifiable shuffles in a single secret leader in action protocol.
00:33:24.610 - 00:33:54.380, Speaker A: And all these Verifiable shuffle protocols are really beefy. So that could be like, a cool application or roll ups, digital signatures. These are the applications that are in the paper and also what I was talking about during the presentation. But if you have any other idea, let me know. I'm interested to hear. But these are the three main applications we were thinking about. But yeah, if you have any idea, hit me up.
00:33:54.380 - 00:34:43.534, Speaker A: Actually, maybe one more question we can have if anyone has a question. Good. All right, again, thank you very much for the discussion. We have Ismail from Lagaranjalad to share about the work that they're working on at the garage on DK Mac reduce full house. I really like this. I feel like a lot of the people don't go to the talks at the day conferences so it's nice to have so many people here. So how do I vent slides just with this? Awesome.
00:34:43.534 - 00:36:12.198, Speaker A: So, for those of you who don't know, my name is Ismail. I'm the founder of LeBron's Labs. Our team builds infrastructure to make it efficient to verify data parallel computations so things like SQL, MapReduce, RDB in zero knowledge when run on top of on chain commitments data. So broadly, the way I like to articulate the application of zero knowledge proofs in the blockchain context is as a mechanism, when we're not talking about zero knowledge, probably, we're talking about solely succinctness to shift the safety of computation away from a low trust verifier onto a high trust sorry, a low trust prover onto a high trust verification environment. So an example of that is when you, for example, want to have a large amount of transactions that are executed by some off chain actor to update some state, and then you want to verify the proof of the correct update of that state back on chain for example, the context of a ZK roll up we are shifting the assumptions onto a high trust verification environment away from a lower trust proven environment. So when we look at the applications of zero knowledge in the blockchain context, typically we are today looking at things that increase the abundance and the privacy of block space. And we're seeing a shift just in terms of how people are applying it towards things that I would say, increase the safety of data access as well as the expressivity of what we can encode in a given transaction.
00:36:12.198 - 00:37:17.938, Speaker A: And this is sort of the shift that we're seeing in terms of the newer applications being developed being things like Coprocessors VM extensions, non EVM execution environments, DK interoperabit more and what do we mean by, for example, the expressivity of a transaction? So we look at what we typically encode in the state transition today it is often just a simple exchange of value on an execution environment or a simple state transition. We look at, for example, the future state of what people are looking to encode within a specific transaction. There are things that are a lot more expressive. They're asynchronous computations that can be run using on chain state. They are verifiability of things that are too large to put within the memory of the chain and they are computation that spans both single chain and MultiChain data that would otherwise be infeasible because of gas constraints in the execution environments. So this is how we can take the concept of transaction and have this be extended to encode more robust sets of computations. And so this is what kind of we get into what LaGrange does.
00:37:17.938 - 00:38:20.534, Speaker A: So LaGrange focuses on what we call ZK MapReduce, which is a proving system that our team develops that allows us to verify large scale data parallel computation that is run on top of data encoded within a blockhead or commitment. So storage, receipt, transaction data. So in the lingua Franco that we usually talk about nowadays we something like big data co processing. And the impetus behind this is if we look at for example, the amount of data that's being created nowadays in the MultiChain, the modular and the roll up centric context, we have more state data than ever before encoded on chain. But typically the majority of execution environments can't access that data at the point of transaction. So really we can't verifiably search, compute or query across really any meaningful amount of onchain state that's not immediately accessible on the execution environment by function calls or just direct memory reads. So this is where we get to ZK MapReduce and some of the work that our team does.
00:38:20.534 - 00:39:16.630, Speaker A: We take a lot of principles involving either recursion or attractions of recursion like folding, and we develop the proving systems on top of that that allow us to in more performance ways than are currently possible, construct very large proofs that have some very interesting properties that make them well suited for large computation over large subsets of commitments. And so this lets us what we like to describe as data lake style computation that's verifiable over on chain state. And so what is required to generate a proof? You basically need to define a data frame the same way you would in web two. And then you need to specify a computation you want to run over that data frame. And the requirement set that we put forward is that this is data parallel computation. So things like SQL, MapReduce, Spark, RDB. And once we do that, we can verify it back on chain within execution environment.
00:39:16.630 - 00:40:34.910, Speaker A: So how do we do this and what is the underpinnings from a cryptography standpoint of what we do? And so our team at SPC this summer put forward a new paper called Rectroofs, which is a vector commitment scheme that's designed to prove batch set inclusion over any arbitrary data structure and dynamic MapReduce computation over the subset of data within that structure being proven. And it has a very interesting property that we have not seen in previous vector equipment schemes and that property is updatability. So why do you want a vector equipment that's updatable and what does that actually unlock in the blockchain context? So let's think about the idea of a batch proof with computation over the data that's being proven. So in this example we're going to talk about volatility of an asset. So you want to, let's say, derive the standard deviation of the price of an asset over some ring buffer k of on chain state such that you can do black scholes and compute and compute the price of an option. So you're going to start from block N to N minus k. You're going to prove the inclusion of the relevant leafs within each block, and then you're going to prove that you can verify the oldest block by the chronology of sequential hashing that gets you to the most recent block.
00:40:34.910 - 00:41:44.520, Speaker A: All right, well, a new block comes in 12 seconds later or faster if you're talking about a roll up and you want to reuse this proof, you want to update it to account for the new change in price. So in this situation, if you are using a naive batch proof or a snark of a naive batch proof, you would have to recompute from scratch that entire ring buffer. K all the inclusion and all the computation over that data. This is obviously not performant as the amount of computation you have to do is linear with respect to the amount of data the size of the data frame you're computing over, which is a pretty terrible paradigm if you think about it, because it doesn't suit itself well as you as you compute more just grows linearly in terms of proven time. And so this is where our team has brought forward the rec proofs, which is a batch proof that's updatable in logarithmic time. So in essence, the amount of recomputation that's required is proportionate to the change rather than the actual size of the data frame. So in this situation, you would just remove the portions of the proof from the oldest block and you'd reinsert something for block N plus one.
00:41:44.520 - 00:42:29.054, Speaker A: And so this works very well in blockchain since they're by design append only data structures and computation over storage is a very low entropy structure. The majority of blockchain storage across successive blocks does not change. So updatable proofs are highly viable in this context. And so we're going to now discuss a little bit of how we do this and sort of some of the innovations our team has brought forward in this paper. So think about a simple merkel proof, a commitment or a merkel tree commitment to an ordered sequence of values where you can create the route through recursive concatenation hashing of adjacent values. I think most people here are aware of merkel tree likely, and if you're not, this is a nice summary of it. And now we think about membership proof.
00:42:29.054 - 00:43:26.470, Speaker A: You take the leaf element and you take all the adjacent values from the leaf up to the root adjacent branches and you just recursively concatenate and hash them to prove inclusion. And so let's say you update something in the set or the subset such that the merkel proof you've had for the previous commitment to data is no longer valid with respect to the most recent commitment. Well, you can update your merkel inclusion proof simply in logarithmic time by just changing the portions of the path that have intersected with the inclusion proof that you previously had. So that's straightforward. And now we think of a naive batch proof and you can think of a batch proof as an inclusion proof that demarcates the minimum path required for the inclusion of all the leaf elements that you're opening. If you were to do that out of circuit, it could be updated. But if you said in a circuit, then you of course can't update that without recomputing the circuit.
00:43:26.470 - 00:44:24.010, Speaker A: And you can't just by design do this in an execution environment because you're just going to run out of gas. So this naive batch proof really can't be updated. It's done in the stock. And so that creates a limitation when you're trying to open a large number of elements within a commitment. And so this is a concept our team brought forward called Canonical Hashing, which defines every branch proof as the subtree of nodes rooted at that position in the tree. And by defining the node subset within the witness, this gives us the ability, when something in the set that we're computing, or the subset that we're computing over, or the set that we're computing within changes to solely update the subtrees that have changed without having to update the entire batch proof. And so what this allows you to do is to have logarithmic time updates of a batch proof done in zero knowledge.
00:44:24.010 - 00:45:34.586, Speaker A: And it does not just include the inclusion, it includes a MapReduce computation on top of that inclusion. So as you open those elements and you progress up the tree, you can prove both map steps at the leaves and reduced steps at the intermediary branches that can also be updated in logarithmic time as the data that you're computing over changes. And so this is very, very good for computation over very large frames of data that have low entropy, which is what blockchains by design are. And so this left us scale proving not through recomputation, but through recomposition of proofs. And this let us have a very low overhead of supporting new chains as we in essence solely need to update and verify historical proofs and we don't need to cache intermediary, for example, historical blocks on every chain that you're supporting. And so in terms of the performance of this in our paper, we're 135 times faster than naive approaches involving like Morkle snarks more traditional batch proofs. But we expect it to be a lot higher when we're just talking about data structures that are a lot longer and computational like NPT trees.
00:45:34.586 - 00:46:29.130, Speaker A: So for production MPT trees, I expect them to range of several thousand times faster and they're fully parallelizable since everything is data parallel, which means you can just distribute these for up to ten x or more improvement in efficiency and verification time is also substantially faster as proof size is smaller as well. And so we're now going to talk about some of the use cases of what this along because I think it's intuitive. Here how this is useful for like ring buffer based applications, which is where you want to compute up with some frame and just slide that frame along the blockchain as a blockchain optics like volatility, standard deviation, price, et cetera. But there's some interesting things you can do here that I would call nontrivial that are exciting. So one of these is a concept of digest transformation. So one of the things our team believes is that zero knowledge systems are fundamentally going to become modular. So you're not going to have an end user application that uses zero knowledge.
00:46:29.130 - 00:47:50.620, Speaker A: Likely it will have a downstream dependency on another application that uses your knowledge where there are different parts of a full proving stack that your team or your company might have internal to it and might have external. And so one of the things that has historically bottlenecked a lot of proving over EVM based data structures is the RLP encodings and the techjack hash functions. And so what our structure allows you to do is to in essence take a portion of a Snark unfriendly structure and continually keep it provably equivalent to a Snark friendly structure. Which means that a downstream application that doesn't want to have the dependencies of computing over a Snark unfriendly structure can simply just take a proof from us and a Snark friendly structure that has the same commitment to leaves and compute over that. So for example, you can take the MPT tree built with Ketchap and RLP and you can prove its equivalents to, for example, a binary poseidon merkel tree with some ordering of leaf values and then all downstream applications can compute over the binary poseidon merkel tree instead of computing over the MPT tree. So you can in essence amortize the computational work to prove inclusion over the MPT tree to the Updatable system and have everyone else downstream building Zkvms or Coprocessors or roll ups or whatever they want. Just computers aren't friendly one.
00:47:50.620 - 00:48:50.474, Speaker A: So another one more application level. You can do things like Verifiable rewards just by taking a bunch of transaction data, doing a full index on that, and doing some filter operation to determine users viability or eligibility for some reward structure. And you can structure the SQL computation. Another thing that we're very excited about that's a little bit more in the weeds is what we call an Updatable BLS aggregation. So as many people here are likely aware, if you're doing a BLS key verification, the aggregation time is typically linear, the verification time constant. So if you're constructing a very large BLS aggregation for a large number of keys that are signing successive things, you should generally ask yourself how many of these keys have overlapped from the last signer set. If there's a high overlap of signing keys from, let's say, signer set x to x plus one, then you really don't need to recompute all of the public key aggregation.
00:48:50.474 - 00:49:47.790, Speaker A: You can basically prove both the inclusion within the commitments assigner set and the aggregation of the public keys in a rec proof and then simply update that rec proof whenever you want to change a signer set or a signer subset or the signer set changes, since these can be updated as a set changes as well. So another thing you can do with this is what we call full storage quartz. This is one that we find very exciting as well. Basically, if you think about an NFD contract or you think about an ERC 20 contract, it's very difficult to have any type of computation that spans the majority of data in the tree. So if I said I'm going to encode a mapping on chain of address, sorry, it would be index of the NFT, for example, owner address and a bunch of metadata. And I wanted to search by a downstream metadata. The only way to do that would just be iterate in the execution environment through all of them and check that position and then filter.
00:49:47.790 - 00:50:53.666, Speaker A: You can't search by a secondary or tertiary value in a mapping. So this is a pretty big limitation if you're a contract developer because it means you can't say something like how many people in my NFT contract have a blue elephant or how many users have an asset balance of greater than X in an ERC 20 token contract. And these are very simple queries you could do in a database. But just how we structure data on the blockchain doesn't work very well. So this is something that we can do very effectively by in essence just keeping the portion of memory corresponding to the entire contract just hot in a data structure that is performed to compute over and then just structuring computation on top of that when we need to ad hoc. And then you can also build some very cool D Five primitives of this where you don't rely off chain price feeds or off chain oracles and you just do everything on chain where you're just computing a proof and therefore we have proof back on chain of some synthesis of historical data. And so for those of you who are interested, we have two resources that I'd suggest people reading who find this stuff cool.
00:50:53.666 - 00:51:32.740, Speaker A: First one direct proofs paper that our team did at SBC this summer. And second one is some work by Nickel on our team that combines Ckmr and Recproofs with folding by design. Since we're a company that believes that you should fundamentally be proving system agnostic, we build our constructions to support anything that has recursion or the abstraction of recursion like folding. So our rec proofs and our Zkamr system supports both. It's just a question of performance for either depending on use case. So that's the talk. Any questions? I think I'm a little short on this.
00:51:32.740 - 00:51:48.214, Speaker A: Questions? Yeah. So any questions? I think Maru was looking at it very closely, but seems like are you good? Okay. No questions. All right. So I think Yin has a question. Yeah.
00:51:48.412 - 00:51:49.798, Speaker B: Thank you for the talk.
00:51:49.884 - 00:51:50.680, Speaker A: Thank you.
00:51:50.990 - 00:52:05.690, Speaker B: Probably a naive question. Yeah. Most of your illustrations are like binary trees. I know. That's like, simplified illustration. So how do you think rec proofs would interact with vertical trees?
00:52:07.150 - 00:52:42.294, Speaker A: Yeah, we can support any structure, really. Rec proof is a vector. Clement right. So if you're building a vertical tree, you can just build your vertical tree to have vector clipments that are more amenable to the properties you want. For example, you can build a vertical tree using recproofs, theoretically. So what recproofs is very good at is taking a data structure, irrespective of design and having a way to assert a batch inclusion on it. That's very performant as an additional point.
00:52:42.294 - 00:52:52.380, Speaker A: So I mostly illustrate binary trees because it looked better on slides, but you can support any area of tree you want here. So MVP trees are obviously area 16, so you can do that as well.
00:52:55.570 - 00:52:56.320, Speaker B: Yeah.
00:52:58.690 - 00:54:06.434, Speaker A: All right, great questions. Any other questions that you guys have at the back? Anyone that have questions? So I guess a lot of the benefit of this comes from the fact that a lot of the state is staying the same all the time. So have you thought about just instead proving the state differences or looking at it from that perspective? Yeah, it would be a lot less efficient, because if you're proving state diffs between blocks, that's a lot of state diffs you're proving. So it's like you have a block, several billion leaves, and you're proving or sorry, you have a state truth is like several billion leaves if you're saying multiple memory values meaning more. And you have really, like, an entropy of a couple of thousand that are changed per block. So being able to just update the structure with a logarithmic to the amount of updates is a lot more performant than just trying to state it. I mean, I find actually, I have a question.
00:54:06.434 - 00:54:22.550, Speaker A: What would it look like to prove the state diff with the batchroom? Well, I guess if you were doing a state diff, you just use a batch as well. Right. So you just prove that between two blocks that the main difference was whatever was including the batch. I think you could actually just use this for batch proof.
00:54:23.930 - 00:54:48.990, Speaker B: So I should really read where it proofs. But I was wondering about prior art. So I'm aware of a few efficiently updatable accumulators, like, for example, by Alan Tomanschko. So how does recpres compare with these prior constructions that also have efficiently updatable accumulators?
00:54:49.990 - 00:54:56.980, Speaker A: So which construction is this? Hyperproofs. Because Alan was on hyperproofs, I think, right.
00:54:57.990 - 00:55:01.110, Speaker B: It was like some kind of dictionary bank.
00:55:01.930 - 00:55:51.582, Speaker A: I'm not sure the paper. So Alan's a collaborator of a few folks on our team on prior paper. So Alan collaborated, I think, on hyperproofs with Travon and babbas on our team, and I don't think hyperproofs is updatable. Actually, I know for a fact hyperproofs isn't, and I don't think balance proofs or point proofs are either. Actually, I know for fact I don't know if there's anything that's coming out concurrently with repprofs in the last couple of months that might also be updatable, but I don't think anything prior to all this is okay, great. You raise your hand. Okay.
00:55:51.582 - 00:57:23.682, Speaker A: Any other questions you guys may have just rev out for bits. Okay, you guys seem hungry. Maybe? I mean, the dinner is right after, so maybe that's why. Okay, all right, in that case then, thanks again, Ismail, for the we're hiring for cryptography engineers, so if anyone here is a cryptography engineer looking for a job, we'll get started with the second half of the session, and as you guys expect, it will be also packed with stuff to go through. So yeah, let's get started now. Next up is Sean from Polytope Labs. So please give a round of hello everyone.
00:57:23.682 - 00:58:08.958, Speaker A: It's great to be here. My name is Sean, and I have a background working on core ethereum ecore back in the day, also on the Polkadot team. So it's been a while. I'm going to be talking a bit about some of the research that I've been thinking about and kind of the motivation as to why I joined the blockchain, which is really scalability. Right. We've all been to the scalability problem, but I think that more recently is that the scalability problem is also the interoperability problem. Why is that exactly? If you wanted to scale a blockchain, you'll find that the only way to scale a blockchain is to sort of have multiple blockchains right.
00:58:08.958 - 00:58:31.826, Speaker A: Has a logical process or logical compute unit. And so you want to basically horizontally scale. So you have multiple compute units. But if all these compute units are kind of siloed, there's no secure way for these compute units to communicate. Then kind of we just got back to the same problem we started with. Right. Because now we no longer have interoperable money, which was kind of original vision.
00:58:31.826 - 00:58:52.446, Speaker A: We wanted to have interoperable money on a global scale. Okay. So, yeah, if we can solve scalability, then we solve it through state solving interoperability. Yeah, kind of how I think about it. You search for the problem long enough, you are this conclusion. So we need to verify consensus groups. Right.
00:58:52.446 - 00:59:24.250, Speaker A: Like, this is standard stuff. I shouldn't have to go through this. It is expensive because you have probably very large values or sets, capital G. I'm looking at you, and you probably have unsupported cryptographic primitives in there, some hash functions that are missing or maybe pre compiled DLS that we've been all waiting for and haven't gotten yet. So we should then figure out a way to verify these consensus proofs off chain and then present proofs for this verification on chain. Right. That seems logical.
00:59:24.250 - 01:00:07.074, Speaker A: But yeah, there's a few problems with this and I'll get to that, but I just want to define what this means. This ability to do computation off chain and proofs for it on chain. You probably know them as a verifiable computation scheme, but we can also think of them as a coprocessor because they process something and then they push it on off chain. So yeah, a naive VC scheme can be seen as a snark. I use naive because there are more complicated VC schemes. So the reason why I say snark isn't that great is because take for instance, our Casper FFG proofs. You have over 850K bowsers who are producing all these signatures.
01:00:07.074 - 01:01:08.358, Speaker A: And if you want to take like instant crosschain messaging okay, sorry about that. You would have to be able to compute them as fast as the Casper FFG consensus protocol is producing these signatures. We all know that it's possible, but it's all just consensus groups, right? Like if you had to bridge optimistic state machines, then you also have to verify the correctness of those transitions, which also seems impossible to do. So for optimistic machines, they produce plots like every 2 seconds. And yeah, you guys already know the numbers on all this, so it's clear that Snarks alone, alone insufficient for instant interoperability. So we need something else actually faster, something that's cheaper and comparable level of security. Right? Introducing blockchains as a coprocessor.
01:01:08.358 - 01:01:49.746, Speaker A: Now this actually is not my idea. This is from the Bitcoin white paper and it says here we can verify payments without running a full network node. A user only needs to keep a copy of the block headers and the longest crypto work chain. I'm going to rewrite this equation and make it much more abstract. We can verify work done by the network, right? Because if you have some virtual machine on the blockchain, it's doing some arbitrary computation and a verifier just needs the headers of the blockchain and the consensus proofs, right? Maybe signatures in the case of proof of stake could also be proof of work. The longer terrorist. So let's call this the light client lemma.
01:01:49.746 - 01:02:31.462, Speaker A: We move on. So a blockchain coprocessor immediately we see that if everything is predicated on the consensus proofs, then our consensus groups have to have high enough crypto economic security. If your consensus mechanism is backed by a few million, then it's not really secure because anyone can basically take over the network with basically the amount of money. So low consensus or low economic security means it's easy to compromise. Example is the Sync Committee, which only has like a few million dollars backing it. I expect to see that we'll see a Sync committee attack eventually. And putting that into CK doesn't make it more secure, right? Like it's still the same.
01:02:31.462 - 01:03:19.620, Speaker A: Still the same thing. Okay, this is where it gets tricky. If we're trying to use a blockchain as a coprocessor, then how fast is this blockchain. What is the computation throughput? Right? Remember, scalable blockchains have to have multiple sort of state machines and all these state machines have to be enshrined because that way the consensus proof attests to the validity of all of those state machines. You can think of the beacon chain as a consensus system and the ethereum execution layer as kind of being enshrined on the beacon chain. You can think of also these ZK roll ups as sort of being indirectly enshrined because you can't post invalid ZK roll up books. So yeah, a single state machine forms this bottleneck and we need multiple of them.
01:03:19.620 - 01:04:04.226, Speaker A: And also we should have cheap enough consensus and state proofs, right? Because if we want to verify the work that was done on the counterparty network, we should be able to verify the consensus of that network and of course state proofs of some of the work that was done. Actually, you could just do BLS and aggregate them. Roll up DLS aggregable. This APK proof is actually a new thing. It was worked on by the Web Three Foundation researchers which allows you to basically be unaware of the individual DNS public keys. But you can still receive the Snark proof that attests the validity of the aggregate I'll get into. And if you wanted to do cheap state proofs you could just do a forklift tree with really you could do up to 1024.
01:04:04.226 - 01:04:44.458, Speaker A: It doesn't really matter. So we get into what hyperbridge is, so we leverage Polkadot. And polka dot is this instance of a consensus system that has multiple enshrined state machines. Right now it has like 50 power chains, but this number will grow into the future and it also has very high propitmissory right now. So I already discussed this and I can kind of get into how polkadot achieves this multiple and trying state machine. So actually the bounding of set is sharded assigned to each individual parachain. And what happens is as these parachutes produce blocks first the sort of committee is assigned these values.
01:04:44.458 - 01:05:26.090, Speaker A: This parachute will check it and then they will shard the data across the entire set. And it's kind of optimistic in the sense that the entire set will not check it until it is reported to be fraudulent. But of course, the entire set has the data to reconstruct the block and assert that it's fraudulent. But at the same time the finality of the parachain blocks is sort of predicated on its validity. And if there was some consensus fault, then the committees and whoever was involved in that block basically get slashed. And polka also has very cheap consensus groups through this new rich protocol called DV. I won't really get into that.
01:05:26.090 - 01:06:12.454, Speaker A: But we can leverage as many parachain chords for basically our computation. We can just basically assign work to all these chords and let them do work and then we just verify the consensus proofs and some stake proofs for the output of that computation, right? And yeah, we can do some very interesting stuff by overlaying a vertical try on top of the classic base 16 tree. But of course, long time we want to probably try and add to the relay tree. So this is kind of the drawing that I had of hyperbridge back when we committed to building it. And I can basically show you that this is a black box. But Arbitrum is basically trying to send messages to optimism. For instance, Arbitrum has consensus to Ethereum.
01:06:12.454 - 01:06:32.900, Speaker A: It doesn't have any consensus of its own. Same thing with optimism. So you see, consensus here between hyperbridge and Ethereum is kind of two way. Hyperbridge has its own consensus, ethereum reads that consensus. And of course, Ethereum has its own consensus and hybrid. And then it can basically facilitate message passing across all these different chains. And you can also add as many chains as possible.
01:06:32.900 - 01:07:08.042, Speaker A: And this is all secured by Polkadot's own crypto security. But even better is that we can also run a full node of all of the connected blockchains. So this is where it starts to get different is that we first do consensus proof verification. Now we know what this is canonical chain. Then we can basically re execute all those blocks in the canonical chain just to assure that they're all valid. And then we can then start to do state proofs of the messages that are intended to be passed around. And for this, we're doing this fancy protocol.
01:07:08.042 - 01:08:04.880, Speaker A: We have ZK Casper, or the full Casper consensus said it's the case that we don't produce ZK proofs of these signatures. We just verified the naive BLS signatures. But we have a ZK proof of the aggregate public key of the valid. And I'll get into the math of that later on. But we can, like I said, re execute these blocks because Polkadot is going to have this on demand core time where you can buy blocks from the relay chain on demand and it can basically shard the work of doing all those block checks across multiple platforms. Now, I believe that this is of course critical for secure prosthetic messaging for any kind of optimistic state machine, right? Because there's nothing stopping from a sequencer from posting an incorrect assertion. And if you're just looking at Ethereum state and reading the L two state from that assertion, you're most likely going to get lied to and there's not even flashing for that.
01:08:04.880 - 01:08:29.298, Speaker A: Hyperbridge allows for arbitrary message passing. So you can pass arbitrary bytes around. We don't really care as long as you pay for them. It could also be used to read on chain state as well, also historical state because we don't delete any blocks that are stored forever on our chain. You can always query them if you want to and slap to be on testnet. We're connected to Sepolia and all of the L two owners on Sepolia. So if you want to.
01:08:29.298 - 01:09:32.884, Speaker A: Try it out later. You can talk to me and I'll give you a small down. So I think I didn't so I wanted to get into APK proofs for a bit. So just more formally, you have a set of signers. They all have public keys. The verify knows the commitment to the full set of public keys. And the proverb just wants to prove that some aggregated public key forms a subset and they will give the Verifier a bit list which represents the public keys in the subset and there's going to be a proof that this APK is correct.
01:09:32.884 - 01:10:13.164, Speaker A: Properly done, you can think of this as a sort of summation with the bid list over the full set, but then the bid list is of course turned off whereabouts there's no participation in there. And this is kind of what that looks like. You have the approver that just takes the business and it produces a Snark proof and the Verifier will have the commitment APK and the proof for the APK as well as the business. And if the Snark is correct, then it just outputs one or zero. So we get into CK Castle and kind of how it works. The expensive part in Castle FG is actually in handling the public keys, right? You have what, 850K validators. Each public key is like 48 bytes.
01:10:13.164 - 01:10:58.544, Speaker A: That's around 30 megabytes that can never fit into a block. So no point even trying. What if instead we don't need to know the individual public keys, we only know the aggregates of the attestation committee. So I'm kind of glossing over Castle FFG protocol here, but the protocol basically splits up the set into Attestation committees and there's like 2000 max in each. So we only need to know the aggregate of each attestation committee. And we do this Snark proof check to assert that whatever aggregates you're giving to us because we don't know what's in there, but you give us a proof that says that these aggregates are in the commitment to the entire asset. And yeah, the verified basically now has zero knowledge and the amount of data complexity or communication cost is now less.
01:10:58.544 - 01:11:34.472, Speaker A: So they only handle the commitment, the aggregate of the assessation committees Snark proof. And we just performed naive BLS signature verification because polkadot can do this. There's host functions for BLS signature checks and blocks are cheap. So we can just throw as many signature verification checks that we need in there, which is really faster than trying to do ZK proofs of validity. We can basically beat ZK proof to do this. And yeah, that's kind of the core of the ZK Casper protocol. And there's a little bit of something clever in there that I'll get into later.
01:11:34.472 - 01:12:39.680, Speaker A: But basically the proof is you have the attitude messages, the aggregate signature from each committee and the aggregate public key of the committee. And this is kind of a set of tuples and the Verifier will do this APK proof check, right? It will take the sum of all the aggregates because we have a commitment to the entire set, right? So we'll take a sum of all these aggregate public keys and check that this Verifies. And then finally we just do the check for signature verification. Now, if you're a cryptographer in the house, you will be asking, what about broke key attacks? If we're summing elliptic curve points, then aren't we kind of vulnerable to brokey attacks? Actually, no, we're not, because we have to do this check. This check has to pass, right? So say I'm an attacker and I wanted to basically do the inverse of the entire validator set, right? And I'm assuming we understand the rookie attack. Or I can just explain it because it's supposed to be a discussion. All right, so in a Roki attack, there's two signers, Alice and Bob.
01:12:39.680 - 01:13:10.488, Speaker A: They both have public keys. Bob is going to try to convince someone who actually wants signatures from both of them. They're hard to damn. Okay, I can make it bigger. So we'll have a Verifier here on this side. They are interested in a signature from both these signers. And how that kind of works is you will take a sum of the public keys of Alice and know.
01:13:10.488 - 01:13:57.284, Speaker A: And then you do a pairing check with the message and the signature. But the problem is in this summation is that Bob could present their public key as the inverse of Alice's public key. So they will take the inverse of Alice's public key. So what happens is when they present that to the Verifier and the Verifier does this addition, it just works out to be Bob's public key. And they produce, of course, a signature with their public key. And the Verifier successfully pulled that both of them have produced a signature that's valid, which is kind of how that works. But we don't do any sort of summation here, right? Because whatever public key that's provided has to have a valid signature.
01:13:57.284 - 01:14:39.136, Speaker A: So if you're trying to provide an inverse public key, you would also have to produce a valid signature for that inverse public key, which is hard. As long as you're not Peter Shore with the quantum computer, you shouldn't be able to get the public key for the inverse, which is the discrete lock problem. I can move on if there's any questions. Okay, go ahead. Is it right without the microphone? Yes. So you talk about the aggregate public key things like per community and how you're updating it between each epoch. But my understanding of the shuffling is that uncorrelated between ecommunic.
01:14:39.136 - 01:15:39.336, Speaker A: Yes, there is. So we don't actually care about who's assigned to what, because all we care about are that these public keys are in our step, the commitment. So we don't care. Maybe the parts that were perhaps just to add on that is because in this context, the power chain itself would perform the pls signature aggregations and verifications natively. So I guess maybe different from traditional ZK like glance in that they don't actually generate proofs for every single signature that attests to the consensus, but rather they already kind of have that verified natively within the power chain who basically performed the Vs aggregation. And then so they kind of like generate the proof around that whole aggregated BS signature. That makes sense, I think.
01:15:39.336 - 01:16:03.710, Speaker A: I understand. Yeah. So I'll say the key takeaways, we don't actually care about the attestation committees as long as they verify. Right. And they are in our commitment because we initialize the process with like a trusted commitment the entire set. And we can also update this commitment because it's a KDG holy morphet commitment. So you can share the same commitment between all different committees? Yeah.
01:16:03.710 - 01:17:32.226, Speaker A: So that was the Kasper and I will just basically run through a vertical tree and we can go into more in depth questions. So a vertical tree, basically you take the basic scene vertical tree we all know and love, you extend the RSC, you replace the hash function, use a KC commitment, and you can now produce sort of constant sized proofs for members in the branch nodes. And you can even take this one step further and basically combine all the proofs from all the different branch nodes because in a classic hash function, you kind of have to reveal the pre image of the hash function to get back the same hash, but in this case we don't. So we can combine these proofs into a single proof using a batch PCs theme and we can achieve like basic constant size state proofs for any items in our state. And now that we've sort of amortized the cost of verification, then we're basically cost competitive with MPC bridges that don't do any kind of consensus or state proof verification. Yeah, that's actually the end of my talk. If you have any questions and take questions yeah, I guess I think it's a lot of stuff that you went through, but any questions, please feel free to raise hands and throw any naive questions.
01:17:32.226 - 01:18:44.602, Speaker A: Okay? Too perfect. Thanks. Great talk. I wanted to bring back on the ZK Casper and the thing that we just discussed about the commitment. So as we established that the commitment to the validators can be used for any committees, but is that supposed to be to verify membership of the validators? Right? And is it supposed to be verified in circuit? I think in your paper, in your article you mentioned that it will be a custom gate that will in every step check that the validator is a part of this biasat in the committee. And is it possible to do this as a custom gate? And maybe you can provide some data on the performance. So, I mean, the performance of this in the original paper and I've run this myself on my MacBook and it takes like four minutes for a million.
01:18:44.602 - 01:19:02.010, Speaker A: So that's tune to the ten, I think. Yeah. For a million dollars. That's proof generation. Right. Is that creating the commitment or checking that the validator is a part of creating the proof? Oh, verification is like a few milliseconds because it's a single pairing check. It's a plunk verification.
01:19:02.010 - 01:19:13.620, Speaker A: But if you do that in circuit, are you supposed to we're not doing in circuit. This is going to be run, of course, natively on the power chain. On chain. That makes sense. Okay. Yeah, I see. Yeah.
01:19:13.620 - 01:19:57.458, Speaker A: Might have missed some stuff, but I think it might have been related to what Willem was asking. But how do you get away without verifying the committee? Yeah, I guess we have to take a step back and kind of understand that at the end of the day, we just have a set that's producing signatures. Right. And as long as their signatures verify and we can see that they are a member of our set, then we don't really care what I get it. I see that now. Yeah. Okay.
01:19:57.458 - 01:20:14.250, Speaker A: Thank you. Try out our stuff on testnet, please. By the way, we want to get after our discord. Great question. By the way.
01:20:16.540 - 01:20:37.170, Speaker B: I was doing the talk I during the talk in the middle. So Mike needs something. But I'm curious that your Michigan Casper or can actually prove the historical ones as well, because I saw your mention about historical data.
01:20:37.940 - 01:20:53.460, Speaker A: Yeah. So because we do all this verification on chain and they're kind of stored on chain, they live basically on architecture forever. So you can always query information about historical blocks. That makes sense.
01:20:53.530 - 01:20:57.540, Speaker B: You have an aggregator about the blocks.
01:20:59.400 - 01:21:17.390, Speaker A: Not necessarily an aggregator. You can think of it as we do a verification of the consensus in abstract terms and the output of that is a verified header. We process that header on chain and kind of live there forever. So anyone can always query state proofs associated with that header. Yeah.
01:21:18.880 - 01:21:20.744, Speaker B: Does it come from the ZK Casper.
01:21:20.792 - 01:21:48.904, Speaker A: Or like other yes, it was an output from the ZK Castle protocol. Thank you. Yeah. Sorry. Hi, sorry. Question actually kind of extending from what Bio was saying, but are you kind of like performing? She was asking about being able to check the historical state. So like historical state.
01:21:48.904 - 01:22:44.844, Speaker A: So in the case of what you would be working with polytopes, doesn't that mean that you have to also accumulate all the block header that you have attested to via polytope? And then with that history of block header, then you can say that okay. Some historical data that you can access via your state proof. Not like just arbitrarily old, I guess, historical data. Is that correct? So the thing is that every header actually is linked to all of its own historical headers and this is an artifact of the Beacon Chain protocol itself. Yeah. So yeah, I forgot. Yes.
01:22:44.844 - 01:23:28.216, Speaker A: So you were talking about this as kind of like an interoperability thing, right. So that means you're still going to have to do some proof of consensus between the hyperbridge chain and all of the other chains as well, right? Yeah. I think that was covered in here, where essentially, yes, we stream our consensus messages of proofs to all our connected chains. So in this case, and I can talk about the economic model of this, is that we're subsidizing the cost of consensus messages, right. So that these chains are always aware of our latest state, but then users, of course have to pay for the state proofs that are associated with these verified headers. Okay, yeah, I see what you're saying. Yeah.
01:23:28.216 - 01:24:09.072, Speaker A: Because in our state, there's the routes to Ethereum. So you verify the proof to Ethereum and you verify parts of Ethereum state that you're concerned about. But let's say optimism wants to talk to Arbitrarium, right? And then hyperbridge is, I guess, does the optimism chain have to verify the consensus of the hyperbridge chain in order to verify all the information of the other chains? Yeah, so I think of it also as like proof aggregation. So you can imagine all these different chains are connected to us and maybe their own sort of consensus systems. Right. Hyperbridge becomes this meta consensus layer that is doing all this verification. But then any connected chains only has to verify our consensus system and we kind of proxy using the light client lemma the verification of all these other consensus systems.
01:24:09.072 - 01:24:41.068, Speaker A: Right. The reason why you kind of built your own parachain to do all of this is because all of this is pretty expensive to do. Is it cheaper for an ethereum chain to do the proof of consensus of, like, a polka dot chain? Is that why? Yes, polkadot in the middle. It is. Okay, that makes sense. Thank you. I think the argument here is that the native BLS would be quite expensive to the own chain on any kind of EVM.
01:24:41.068 - 01:25:16.778, Speaker A: Yeah. Any EVM? And also there's also the speed latency aspect of it all as well. Any other questions? Oh, well, one more. Okay, let's go. How does this relate to a bunch of other efforts on shared sequencers and stuff like that? Because it seems like this is basically kind of like an opt in shared sequencer type kind of thing, potentially. Right. I actually really love that you brought up shared sequences because I feel like that's something else that people are kind of championing as a solution to.
01:25:16.778 - 01:25:58.200, Speaker A: L two interoperability. It's a case that, no, they're not, because, sure, you now sort of aggregated two state transitions and you posted that to the main Ethereum chain, but we still don't know if they're valid. Right. Like we still kind of have to wait the challenge period. Yeah, well, they haven't really solved the problem yet, but yeah, they're just putting all the data together. But I don't think they've actually solved the cross chain transaction problem. Yeah, okay, and I guess I will also add that with shared sequencers most likely you also have to begin with a single entity that's also performing that shared sequencing role and upon which you have a very different trust assumptions than let's say you have a different parachute that is running on top of Pluto for example.
01:25:58.200 - 01:26:45.926, Speaker A: All right, so I guess that's the end of the sean's talk. Thank you very much and please give a round of applause. Come talk to me after and I can also show you a demo if you're interested. Yeah, next up it's going to be Diego from Lambda class. Hi. Hello. My name is Diego and I am head of research at Lambda class.
01:26:45.926 - 01:27:52.150, Speaker A: And today we are going to be talking a little bit about the work we are doing at improving current start provers and of course then Mauro is going to give a talk on how we can improve these by making some changes to the current VMs. So this is an outside of my talk. First I am going to give a brief overview of STARx in case you are not familiarized and also to know about the things I will be talking because I can say well, we are going to optimize this, this and that, but we don't know where they fit in the protocol. So it's important to have an idea of the main steps. Then a short introduction to Star Platinum which is our implementation of a Star proverb which is trying to be compatible with Starnet's Tome Proverb. And then we will talk a little bit about the areas of field arithmetic, hash functions, of course something related to proof size and then lookup arguments. So Starks in a nutshell, starks are scalable transparent arguments of knowledge.
01:27:52.150 - 01:29:01.970, Speaker A: This means that Starks don't need a trusted setup that's opposed to different snarks which of course have their special setups and you have to trust them. Well, Starks, since their security depends only on the security of Hash functions, are conjectured to be post quantum secure. So what are the ingredients of STARx? First, an execution trace which is just a table containing the values of each register at each execution step. Next, Mauro's talk will go deeper into the VM and how it works. But let's think of this as just a table of values and of course maybe we have also the values of the memory and all that. Then we have a set of polynomial constraints over the trace which is the algebraic intermediate representation. There again, the virtual machine will play an important role because depending on the set of instructions which we choose, we will have a different set of constraints.
01:29:01.970 - 01:30:22.010, Speaker A: Then what we have to do in order to prove is transform these execution trace and these polynomial constraints into some relationship between polynomials. And so what we are going to do is encode the execution trace as a set of polynomials that is done as interpolation by FFT and then we will have to commit to this execution trace and that's why we'll have to build market trees and there appear the hash functions. And then what we have to do to show that the constraints are really forced over the trace is compose all the constraints and calculate some quotients because that way we can get the succinct argument. And then what we have to show is that effectively the resulting quotient is indeed a polynomial. And then we use the Fry protocol which is going to recursively hold the resulting function and try to show that it is close to some low degree polynomial. Okay, so these are like the main steps in our proverb. We have like five separate rounds where we kind of encode all these computations.
01:30:22.010 - 01:31:08.330, Speaker A: So what stack platinum? As I was telling you, it is our implementation of a general Stark prover. There you have the link if you want to look at it. Telegram group, it's written in Grass. We wanted to focus on security and performance. As I told you, our objective is to make this prover compatible with Starnet's prover and also as a drop in replacement for other popular choices such as Winterfell. It can be used to prove code written in Cairo because we have the Cairo VM and this proverb also has an implementation of the Cairo Air. And right now, as I was telling you, we're working towards full compatibility.
01:31:08.330 - 01:32:36.434, Speaker A: So what are the things we have to attack if we want to get to an efficient Stark prover? Well, the first thing is everything depends on field operations. So having really fast field operations is crucial because we are going to have to interpolate evaluate polynomials, sometimes use even arithmetic based hash functions. So the original proverb worked over a 252 bit field using basic mongomery arithmetic and we need that the field has certain characteristics if we want to interpolate and evaluate polynomials efficiently. So in general, what we want is these fields to contain a large subgroup that is a power of two. Of course maybe we can do better and right now the focus is shifting on smaller fields. This is an advantage of stars over elliptic curves since we can go over smaller fields in the case of elliptic curves to achieve the necessary security or everything, we need to work over a certain field. So in this case, for example, if we use the first prime, which is sometimes called mini goldilocks, we have special reduction formulas that are more performant than basic Montgomery arithmetic.
01:32:36.434 - 01:33:38.860, Speaker A: And also using smaller fields results in an advantage when you work with the VM. Because right now sometimes the VMs have to represent certain variables with a field element that takes 256 bits and here maybe with 64 we can just represent all the variables we like. Of course, another option is, for example, Marcine primes. And so in these cases we get smaller base traces so the memory use reduces a bit and especially we get faster operations. Of course one drawback that we have is that we need to achieve the necessary security and so the field has to be of a certain size and so in general we would have to work over extension fields. So for example to achieve 128 bits of security we will need to work with an extension three. With minigoldilocks though some projects work with extension two and they are targeting more or less at 100 or 90 bits of security in general.
01:33:38.860 - 01:35:02.600, Speaker A: So of course here we have a trade off between using smaller fields but then working over extensions or working over a base field. Maybe we can find something better than the 252 start field. Then of course one of the other things is we have to build lots of marital trees because we have to commit to several polynomials. For example, when we to Fry at every folding step we have first to commit to the polynomial. And so the problem that we have is that maybe generating just start proofs is really fast but we in general want to patch several proofs into a single one and so we have to use recursion. So what we can do is create a binary tree where we take for example each program we compute the proof that we calculated it correctly and then we take two proofs and use a verifier to generate a proof that we verify those two proofs. That way what we can do is apply of course recursion at each step and reduce and get to a single proof that attests to the validity of several hundred thousand proofs for example.
01:35:02.600 - 01:36:12.854, Speaker A: But of course when we have to prove that we verify the proof, what we have to do is verify mercury inclusion proofs. And so the problem is that for starts traditional hashes such as Blake or Chateri are not arithmetic friendly and so they add a significant overhead in probing time. So of course what we can do is instead of hashing with those we can use some other hashes, for example Poseidon or Rescue Prime which are like faster to prove when we want to verify. But the problem is that they are much slower than the others. So okay, we get faster recursion but still merkel trees are quite slow. So this adds for example significant overhead to the spray protocol since we have to commit to intermediate polynomials. But maybe what we can do is for example use some tricks like skipping Fry layers.
01:36:12.854 - 01:37:16.080, Speaker A: So instead of committing to every layer in Fry, we commit every second or third layer. Of course that reduces a little bit the soundness error, but what we can do is increases the soundness error. But what we can do then is of course do a little bit more queries and increase proof size. But that way we don't have to do that many hashes of course, something that is crucial is maybe to have some kind of hardware acceleration for these primitives. And we believe that with that we can get a lot better than before. Well, regarding proof size, of course there were several optimizations that we saw across different implementations, and so we tried to get them more or less together. The first thing is, of course, avoided sending to the verifier redundant information.
01:37:16.080 - 01:38:18.020, Speaker A: So there are lots of things that the verifier can compute directly from the information the prover is already sending. For example, if we send two evaluations from the upper layer of pre protocol, then we can compute one element of the next layer, so of course we don't pass the element of the next layer and we can then check everything when we do the batch inclusion proof. Another optimization is that in general, we have to pass two values for every query in every mercury tree. And these values correspond to the evaluations of the polynomial at x zero and minus x zero. So what we can do is instead of having them in different branches of the tree, we can put them together. And so that reduces the number of authentication paths we have to pass. Of course, then we can also reduce the redundant path and send less information.
01:38:18.020 - 01:39:30.620, Speaker A: Some other optimizations, of course have to do with keeping pre layers, that is to say, committing to every second or third layer, for example. And the basic idea in the Fry protocol was to randomly fold the polynomial until we got to a polynomial of degree zero. But what we can do is instead of doing that is finish before and just send the coefficients of a degree n polynomial until the verifier okay, check with this polynomial. Because really what we want to do with Price not having to send all the coefficients of a very large degree polynomial, so what we do is we fold. And so we have to pass like less coefficients. Of course, degree zero is convenient because it's only one value. But then, since we have to do lots of queries, maybe it's best to cut before reaching degree zero and sending all those coefficients, which will be much less than the information we have to pass from the authentication pass?
01:39:31.470 - 01:39:52.366, Speaker B: Yeah, by skip layers. By skip layers. Thank you. So skip layers just means like finishing with a higher degree. So it does mean like only committing to every other layer.
01:39:52.478 - 01:40:40.740, Speaker A: No, what you do when we say one thing is finishing before reaching degree zero and the other is skipping, which means that, for example, you started with a degree n polynomial and then when you go to degree n over two, you don't commit to that one, and then you jump directly to degree n over four. Of course you have to pass more elements from the upper layer, because when you skip, for example, if you don't skip, you have to send two elements from the upper layer. If you skip one layer, you have to pass four. But the advantage is you don't have to commit to that polynomial and maybe the total number of elements or hashes you have to calculate is reduced. So that's a common trade off. I don't know if that answers your question.
01:40:42.790 - 01:40:47.410, Speaker B: It does. Thank you. And how does that affect the security analysis?
01:40:48.390 - 01:41:32.686, Speaker A: In that case, there are some proofs it increases a bit the sumness error. But in Fry, the final security depends on the total number of queries. So you can either increase the number of queries or maybe increase it by using, for example, proof of work. So maybe to get back those bits of security, we ask the proverb to solve some sort of proof of work with certain number of bits of security. And then we recover that. Of course, then we have proof recursion. That is especially useful because what we can do is always batch more proofs.
01:41:32.686 - 01:42:25.170, Speaker A: There is like no security loss when doing recursion. That was a paper that appeared recently by, I think, Alessandro Chiesan collaborators. And then of course, if you like shorter proofs, you can always do like one final step of recursion. But instead of using Starks, maybe you can use a Snark, for example, Blanc or Grot 16. Of course there you have maybe some extra cost for the prover, but then the proof size is greatly reduced. And so, for example, I think the polygon ckevm is using this trick. Of course there is one caveat that you are sacrificing your post quantum security because these narcs are not post quantum secure.
01:42:25.170 - 01:43:15.390, Speaker A: And then of course there is another huge cost which is related, for example, to memory consistency and memory checks, where you have to do some sort of permutation argument. And so in general, that involves a grand product check. You have to sort a large vector and then you have to compute like several inverses. So really those are like expensive calculations. Even besides, when you want to do these arguments, you have to sample randomness. And if you want to work with smaller fields, then of course you have to sample your randomness from the extension fields. And then of course there are some operations which are really expensive to prove in Starks.
01:43:15.390 - 01:43:58.670, Speaker A: So you can use, for example, some specialized built ins. Of course. Right now, for example, in the Stone proverb you have static built ins. That means that you have certain boxes of a given size and you get to pay for the built ins whether you use them or not. Or if you use them one time, maybe you have to pay for using it like ten or 20 times. So that adds a lot of overhead. So there is a lot of study on how to work better with built ins, either by having dynamic built ins or dynamic layouts for built ins, or maybe using some other type of construction.
01:43:58.670 - 01:44:44.126, Speaker A: For example. Like chiplets or any other use. And of course everything related to this is tied to the kind of lookup arguments we are using. And so right now there is like a push for the logarithmic derivative lookup and there were some improvements using the GKR protocol. So we hope that in a quick time we will have an implementation of this and be able to measure the performance concretely and well. Finally, just a note on security, the security of Starks depends on four parameters. First, the field size.
01:44:44.126 - 01:45:52.422, Speaker A: So as I was mentioning you, the field size should be much larger than the low degree extension domain, that is to say the domain over which we evaluate the polynomials. And typically it has to be like the log two of the LDE domain of the size of the LD domain plus the security level. We want the hash function, but in general, using ordinary hash functions with sufficient digest size is more than enough. And of course then we have the number of queries which increases of course, proof size and of course then we have proof of work. Then there is some discussion on which are the optimal parameters for Starks, depending on whether you work with provable security, which may be like super conservative or maybe some conjecture security. Right now, more or less the different projects are targeting more or less around 100 or 128 different security. Well, I think that's pretty much it.
01:45:52.422 - 01:47:00.554, Speaker A: So thanks a lot. Thank you. Diego, any questions? Oh, perfect. Hi, I'm having a question regarding the computation of this size. You mentioned that you have to pick up on the large coefficient rather than the small coefficient. Any reason behind that? And the subsequent question after that is how can one be or one protocol power be selective about the coefficient size while deciding to the polynomial? You mean on the number of queries and all that? To achieve the rationale behind you have to pick the large coefficient rather than the small coefficient while assigning to the polynomial. Yeah, no, it depends on what you want to do.
01:47:00.554 - 01:48:01.534, Speaker A: If you want to be super conservative, you just stick with the provable security. But of course what we have seen so far is that using conjecture security has not led to any noticeable attacks. So maybe you are just covering yourself too much and maybe you can just work with smaller proofs. But in any case, if you want to be like super conservative and super paranoid and want to have your 128 bits of provo and security, which you don't know really, because there might be better bounds, there are still like several security analysis in Fry. Then what you can do is continue recursing and then batch more proofs. And then in the end you end like amortizing the proof size. Or if you want the last step, just go for a snark and then you get your constant size proof which is very short.
01:48:01.534 - 01:48:51.294, Speaker A: And then you're done. Got it. Regarding the recursion function so do I have to put on the protocol itself or is it something that's being auto embedded on that? No, in that case what you can do is just code, for example, the verification circuit in the caravm. Then you execute and then you get the execution trace to verify the proof or the two proofs. And then you can use the ordinary start prover. The only thing is you will need certainly to use some built ins for the hashing, for example, to make it efficient. But the good thing is the verification circuit is always the same.
01:48:51.294 - 01:50:06.358, Speaker A: So once you have it or two, then you can continue using it as long as you need to reach the final step where you can use your snark or you just obtain the final proof using Stark about the technique of skipping every other level in the market tree. Could you just give one practical example of how that could be attacked? If I'm a manager sector and I know you're skipping every two level. No, the thing is it works more or less like ordinary fry, maybe because the best attacks you have on fry are interpolation attacks. You know, you are free to choose like n different values in the mercury, but then of course, you have the blowout factor. So the probability of you getting a favorable query is like one divided by the blowout factor. Maybe if you skip layers there is the chance that you might get luckier when jumping below. But I don't know, I should check maybe the security proof or we can discuss it later.
01:50:06.358 - 01:50:58.214, Speaker A: But there is like this analysis by several papers where some techniques reduce soundness. For example. Another, another problem is when you use this communication reduced version of pride, where instead of sampling n different random numbers to perform linear combinations, you just sample one and then you calculate powers. Then that reduces soundness a bit. But really it's what most of the protocols use right now and everybody more or less lives with it. So yeah, thank you. Any other questions? And by the way, I think it was a great question.
01:50:58.214 - 01:51:44.920, Speaker A: It's on the security front. Any other security questions? Anyone who thinks that this skit just doesn't work? We're trying to be nice, all right? Trying to be nice. Okay. No, don't be nice. Be nice anyone? Okay, so I guess everyone thinks that this thing works. All right, so next up we are going to have Moro and then we'll have Eton. I think we originally had E and then but I think just continuing from this talk, I think makes more sense that Maro is going to go first.
01:51:44.920 - 01:52:51.894, Speaker A: Okay? So this is working perfectly. Okay, so Dio has just showed to everyone a lot of optimization regarding the starts and how to make them fast. So now the idea is to build a virtual machine on top of the Stark system and see how we make it parcel and efficient. But first I will try to introduce to everyone what is a VM and how it gets proven, and then we can start doing some simplifications to see how it works and hopefully move on to optimizations. For the record, I will be trying to explain tricks from many companies, including the software rig zero and polygon that we also use in our same system. So we have two approaches for serial Knowledge Group. One is the ASIC approach where each program is compiled into one circuit.
01:52:51.894 - 01:53:28.162, Speaker A: That's the usual case for Snarks. Or we have the Vum approach where we have one circuit that can prove any program. That's what we usually call a VM. Examples of each of them is for basic approach things like SynCom, noir, leo. From VMs we have kaido, mind zero, all sick, EVMs, that kind of stuff. And we can further split VM depending on the arithmetic they use. Some VMs are using as their basic element.
01:53:28.162 - 01:54:29.180, Speaker A: For example, Kaido and Maiden and some other VMs decide like risk zero, that maybe it's better to just prove any program, but just proving any program, traditional program needs a binary arithmetic that is a little bit slower. Okay, another thing I wanted to mention is how many constraints are we talking about when we talk about the VM? Because we have this idea from ASIC programs that maybe we have millions of constraints or thousands of constraints. When we are talking about VM made for finite fields, we are not talking about a lot of constraints. For example, the kind of VM has around 40 constraints when we don't have business, but I will talk about them later. And my VM with Chiplet has around 200 constraints and the degrees are really low. Kyle VM has only degree two constraints. It has one degree three that gets reduced and mine has at most degree nine.
01:54:29.180 - 01:55:42.450, Speaker A: So how do we measure the complexity of a program? For CK programs we will talk about constraints, but for VMs it usually makes more sense to speak about steps where each step is the execution of one instruction. Okay, so now that we have a general idea of both approaches, I will continue with VMs. So I wanted to show some generalization, some core parts that we usually have. So we usually have a lot of constraints regarding the CPU, like how each instruction should be executed and what are the valid next steps. When we see an instruction, usually in all this system, we have a core part that is a range checker that is going to check that values are between a range. For example, we can check that the memory is between value ranges that might be smaller than the size of the native field. We might have some constraints relating to the public memory because if we are proving the correct execution of the VM, it's good, but then I can differentiate between a fibonacci or a proof of recursive proof or whatever.
01:55:42.450 - 01:56:25.200, Speaker A: So we need to make sure not only that the VM was executed correctly, but that it executed the program that we want. And they usually have a mechanisms to upload computation because they are not optimal for everything. So that's usually done with additional constraint. They might be really efficient to prove hashes or Bit operations or whatever. But in that case, then it's like having different machines that needs to be communicated, so the communication has to be done. And there are many schemes for doing that. For example, Sharon memory, that is Star Wars approach with Kaido with built ins or maybe a bus.
01:56:25.200 - 01:57:12.090, Speaker A: That is my approach with chiplets. So this is already what is and now I'm going to show you a simplified version of how a trace that is going to be proven looks. And then we will go to the real one. So in the AC case, what we will have is each row of the trace is going to be one step of the execution, right? And then I'm going to see, okay, this is the state at step zero, state at step one, step two. And this works more or less, but in reality it's going to be a bit more complicated. In this case, one row is one step. In reality, this is Stompar or Kaio Pruber from Star Wars Trade.
01:57:12.090 - 01:57:53.430, Speaker A: And we will see that one step is actually a lot of rows. A lot of rows. So the analysis is a bit more complicated. Sometimes people stop and say, okay, we have this many columns, but it's kind of irrelevant because the layout might have multiple rows per step or whatever. In general, this has the evolution of the virtual machine, but it has additional data. There is embedded data that might be different. For example, you can see here that we have the sorted positions of all the memories that we are going to use to enforce some constraints.
01:57:53.430 - 01:58:28.838, Speaker A: And this is totally not linked to the step, it's just the whole axis to the memory in an order. Man okay. And we will have another embedded data. For example, I told you that we need to prove the public memory and the public memory. Let's think that if we are executing a program and it has branches, we won't take all the paths of the program, so it won't have all the data. You don't constrain the steps because I don't see the step. No, this is actually a state step.
01:58:28.838 - 01:58:56.170, Speaker A: The transition yeah. Okay. The transition constraint will be like two of this. This is state zero, for example. And then you will have another table that is going to be state one and you will have transition constraints. And for example, now let's go to the program of the memory. The problem of the memory is a bit troublesome because if you didn't take a path in your program, you will never see the actual access to the memory.
01:58:56.170 - 02:00:11.960, Speaker A: Maybe some instruction is never reached but you still need to make sure that that instruction was in the program. So since that is not going to appear in the trace, we are going to have to embed it and that is what is not in this column that is called the mempool. We will have some spare cells that we will use to place the public memory and this is actually not the full trace and I'm going to span it now because I'm telling you here we have a mempool and here we have a sorted Mpool. So we actually need to use a protocol that is called rack random arbitrary to prove that one is a perputation of another one that say that they have the same elements. So the trace is going to be extended with this and this can have some caveats. Particularly we're working with small fields that IGO mentioned before because there is some randomness involved on this and if the field is too small, just one random field won't be enough to generate enough randomness for this protocol. That's it.
02:00:11.960 - 02:01:13.558, Speaker A: Now let's move on because we are now going to merge many computers that do different things, make one that hopefully goes faster. So we are going to start writing coprocessors and the idea here with builtin is to write many coprocessors or machines that will share memory with the CPU. So in one way there will be memory positions that will have additional constraints. For example, we will place some data in some memory positions, and we will say, okay, this position memory, they have all the constraints related to the memory. And additionally, it will have constraints that make sure that it's in a given range, for example, whatever. Or we could go with a chiplet approach that works a little bit different. In this case, we won't share memory but the machines need to communicate anyways.
02:01:13.558 - 02:02:19.680, Speaker A: So what we are going to do is try to use what is called a bus. So if you see each chiplet has its own trace which is really nice, it's going to prove something but it may not be related to the main CPU. So we need to make sure that the values that we hope to be constrained are actually used by the chiplet. So this is going through the VAS and the VAS is a lookup argument, that is what Diego mentioned before. So this is from but I'm going to show you a quick approximation of how this works. So we are going to do a cumulative product or a gram product where each time that VM needs to communicate with the bus it's going to multiplicate do a product of all the data that should be in the bus and when a chiplet process, the data is going to divide for the same values. So at the end of time when we do the gram product, we should reach the initial state.
02:02:19.680 - 02:03:02.186, Speaker A: This of course doesn't always work like that because we are going to need some randomness that is going to be inserted. Phamil. Maybe we can use a smart construction like logup to make sure that this goes faster. Another nice trick that I like, that I really like is continuation. There are two variations, one by Riccio and another one by Alan Shepeniek. So the idea will be the following. I told you that we want to prove an execution of a program so we can actually split a program in part and prove them independently.
02:03:02.186 - 02:03:51.920, Speaker A: And this is going to be really useful because we can parallelize the proving of one program in many parts. Right? And one of the really good things about this is that we are no longer constrained by memory because if we can split the program, we can always make a smaller talk that proving pitching in the memory. So this is going to be really useful. One part is really easy. You just prove each part of the program and then you have to have a protocol to prove that the output that the state at the end of a part is the same state as the beginning of the other part. You are proving there are many protocols. There are these two protocols to do that.
02:03:51.920 - 02:05:09.830, Speaker A: And of course this is going to look great with recursion because if we can do an efficient recursion and now we can split one step of the recursion in many machines, on machines with not so much memory or not so much power can be really useful. Another trick that we have that might be useful for drawing is bootloaders. It's a trick done by Star Wars. So we know that the cost of verification in Star is logarithmic. So proving many small programs independently is more costly than showing them all and proving them together. And we are going to show them not by recursion, but by using a program that is called a bootloader. The purpose of the bootloader is going to load all the programs in the memory of the VM, prove them all together, generate hashes that say okay, I have loaded the following programs and these are the results and these are the caches of the program I have loaded and executed.
02:05:09.830 - 02:05:50.110, Speaker A: And then we prove that and by proving that, we greatly reduce the cost of the proofs. Okay, so that was a quick overview of many tricks that we have to push this analysis. I want to thank everyone that is doing building open source code for this which we are always able to learn. In particular the three companies I have mentioned at the beginning and of course Fembushi Capital and Lambda Class. That is our company. Thank you very much. And if anybody wants to ask questions, feel free to do so.
02:05:50.110 - 02:06:46.630, Speaker A: Perhaps ask Is if you could maybe elaborate a little bit on the so at the beginning you presented how some DMs were based on field, some VMs were based on binary arithmetic. If you could maybe elaborate a little bit on some of the trade offs that exist between that space because as you mentioned, maybe risk reserve is not that performant because it's like binary arithmetic based. But then why did they choose that options, et cetera. Could you elaborate on that? Yeah, I'm sure people actually viewing this may have a better answer, but I will give you my take on this. When you do a VM for finite fields, you are free to optimize it for proving it's something you have done just for this. So you will have little constraints, a few amount of constraints. It's designed for this.
02:06:46.630 - 02:07:32.870, Speaker A: The basic operations are going to be on fields, not on bits. Working with bits is always more expensive because the proving system works with fields, it doesn't know all bits. That's a higher level abstraction a bit. So when you are able to design everything from scratch, you can of course make it fast, but then you have to be the whole stack. You need a compiler, you need a language, you need absolutely everything. If you take another approach and say okay, let's just try to prove risk time or prove something that already exists, you can leverage everything that you have. Now your VM is not designer for this, but on your side you have everything.
02:07:32.870 - 02:08:52.030, Speaker A: You don't need to build nothing. It's everything widely used and you just have to prove it. And I guess following up on that, do you think that we will ever see a scenario where that Dkvm based on binary arithmetic would provide, I guess, sufficiently fast performance such that the field based Zkbm wouldn't have that much of an advantage? In that perspective, I think they will have the niche. Probably this is pure speculation, but I think if they can be good enough as to maybe make the others life little bit harder so probably you will see. Yes, at the beginning it's only a bit like an impossible task or like a really task, but we are seeing that it's not an impossible task and it's been done and they work really well, so they have an advantage. We will see how this ends. Do you see a world where we get VMs on client devices anytime soon? Yes, like phones.
02:08:52.030 - 02:09:22.946, Speaker A: I think you can already do that. We run our proverb in an ESP device that is much less powerful than your cell phone. That depends on what you are trying to prove. Probably you won't prove a recursion on your phone, but execution of the smart program. I think you should be able to do it now. All good. All right, I guess the round of applause tomorrow.
02:09:23.058 - 02:10:29.986, Speaker B: Thank you very much. Yeah, I think my talk is probably one of the only privacy talks today. I think Aftech will probably talk about privacy as well. And this is really reflective of a lot of the work in the ZK Snark space. A lot of it has been focused on succinctness and scaling and efficiency and I think so my background is I worked in the Zcash project for a while and for me, one of the motivators for studying ZK Snarks has always been the privacy properties. So today we're going to explore programmable privacy on blockchains, not just limited to zkSNARKs, but exploring basically the whole design space of computations we can do over private data. So this is part of a project that I'm working on with Andrew Miller, Brian Gillespie and Daniel Bannerok.
02:10:29.986 - 02:11:27.702, Speaker B: And we're going to write an Sok on programmable privacy. So I first want to motivate this problem with a very well known situation we see in D Five, which is a sandwich attack. I assume everyone's already familiar, so I will speed through it. Basically, a sandwich attack is caused by a privileged adversary having visibility into the orders coming into a certain block. And this adversary is privileged because they're able to reorder, insert or remove transactions and therefore cause the transaction to execute at a worse price. So one naive solution to this is, I think, done by Osmosis. It's called the encrypted mempool.
02:11:27.702 - 02:12:26.180, Speaker B: So instead of posting your transactions in the clear, you encrypt them to some threshold. So the block producer is only working with ciphertext that looks random and the block producer has no strategy to reorder them. Now, the problem with this naive solution is that basically it's less efficient. Economically speaking, there is no way to order your trades for a better execution price, firstly. And secondly, it actually also advantages those transactions that are commutative. In other words, transactions that don't depend on order and will execute successfully regardless of how they're ordered. And conversely, it puts at the disadvantage those transactions that need to come one after the other.
02:12:26.180 - 02:13:40.000, Speaker B: Of course, this can be solved to varying extent by revealing some parts of your transaction. But yeah, in general, this sort of brings up the question of how we preserve trade privacy, but also still retain the ability to do meaningful computation over them. By the way, interrupt me at any time because I'm not a DeFi expert at all. Yeah, and I've already had good conversations with people here who are like actual degen and I'm always looking for you guys'opinions. So yeah, or a more general way to phrase this question is how we preserve the privacy of the user's intent while still ensuring that they're executed faithfully. I motivated this topic with the sandwich attacks and I'm going to just stick to this example. I'm going to basically explore this topic using just one application, which is the limit order auction.
02:13:40.000 - 02:14:59.810, Speaker B: And yeah, we think that this application is general enough that it gives us an overview of all the permittives needed for a truly general application. So once again, I'm pretty sure you guys know what limit order options are. So I will also speed through this. But yeah, basically a limit order is an order for it's an order with a fixed input of a certain asset that you intend to sell. So we fix the input asset and we fix the amount that we're willing to sell. And then we also specify the asset that we're trying to buy as well as the amount that we're trying to buy. So in the case of an order book, we can specify all of these fields and yeah, basically we are willing to execute this order if we can get at least this amount or more.
02:14:59.810 - 02:16:40.970, Speaker B: So the buy value here is specifying the price, the limit price of our order so that's in the context of an order book and in the context of an automated market maker, we actually cannot specify our price, we cannot specify the value that we want to buy. Instead, this value is publicly determined by the amount of liquidity available. But we can specify slippage so that beyond a certain delta from the public price, we refuse to execute the order. So Cowswap is a very good example to look at here because they make use of both the order book and the AMM. So what Cowswap does is it takes in a bunch of individual traits and it batches them and then it can share liquidity for all the traits in one batch and it looks both in its order book and in the onchain PMM. So now that we are familiar with our Motivating application, the other sort of introductory piece that we have to go through is cryptographic primitives. So I'm pretty sure everyone already knows all of this.
02:16:40.970 - 02:18:03.760, Speaker B: And what's new here is basically a framework to compare these primitives. So I think of them as split into two classes. The first is private verification and the next is private computation. So anything you do with zero knowledge proofs falls into private computation in the sense that the computation all happens prover sight directly on the private witness and the zero knowledge proof can only be verified and shown to be valid or not. So no further computation is done on the zero knowledge proof. Whereas private computation requires some sort of encryption that preserves some structure of the original data such that a third party can perform meaningful computation on the ciphertext blindly. So yeah, to get a little finer grain for private computation, the simple case I described just now directly maps onto the tee.
02:18:03.760 - 02:19:03.350, Speaker B: So the tee, you literally just encrypt it to the key inside the secure hardware and they decrypt it inside and perform a computation in the clear and then re encrypt the results. Whereas threshold private computation is the same shape, but you're replacing the trust model of secure hardware with a threshold guarantee. So basically a quorum of these parties, if they collude, can decrypt your cyber attacks. But if everyone's honest, then what they do is they work together to compute some joint program on your secret share plain text. Okay, any questions? Yeah, this is all really high level so far.
02:19:07.660 - 02:19:56.660, Speaker A: This one, what the threshold compute and PE, right? Yeah, most of the things which you mentioned here is kind of a trade off. The solutions, the coastal app and intend and everything, they have a centralized RFQ and then some sort of solver kind of thing especially like is great to a certain extent but you still don't have a privacy or something like that there. But mostly interest is like you mentioned about Threshold Compute. If you take it from an NPC perspective, it never worked. It's kind of an alibi. That plausible deniability that we don't see, but you always see it. Sorry, that's my bias.
02:19:56.660 - 02:20:42.390, Speaker A: But MPC solution is like is it possible that could you have a credible commitment can be done for this kind of limit order book? Because even if you have privacy there are a lot of other things you can do. It like the options that you mentioned, you can escalate the price you still have the participants are mostly centralized or preferred and things like that. So you have a lot of analysis you can do and you can do a lot of attacks. But what I'm most interested is like the critical commitment devices combined with the virtual computation you can have some sort of a temporal rule that if then attack is coming, if the privacy is leaked then the actor can be kicked out from the game.
02:20:46.120 - 02:20:48.490, Speaker B: Sorry, if the privacy is leaked then.
02:20:49.260 - 02:21:34.928, Speaker A: Let me tell you a different way. This one. So you pay in the trades, in the game theory you basically pay for something to get a trade, right? So let's say you pay $100 to get an E, something like that, but the game will shift over the course of time. Then you will start to pay not to get excluded from right because you have more data. Most of this trade will actually going to get posted in a blockchain ledger. So how do you get this kind of the game theory going on parallel with the currency? Because we're talking about there's always leakage afterwards.
02:21:35.024 - 02:22:21.300, Speaker B: Yeah, so that's a really good question. I think oftentimes it is a trade off between privacy and efficiency of your trade execution. So we're going to go into a few implementations of limit order options later on. But one that I can think of off the top of my head is a protocol called Renegade Phi. So Renegade Phi is an onchain dark pool and by default all your trades are dark. Like they're completely concealed. But the problem with that is that if your trade is completely dark all you can do really is to sit and wait for it to be organically fulfilled.
02:22:21.300 - 02:23:24.484, Speaker B: So Renegade Buy provides this option for you to light up certain parts of your trade like the asset pair or maybe the price or the amount. And this can motivate a counterparty to fill. Your order. So, yeah, I think the design space is huge, but I do think that the question of what kinds of users will be attracted to what kinds of privacy is a very interesting one. So I was talking with someone yesterday who made a good case that the users who care most about privacy are the most toxic. Users like the whales who are trying to dump. Yeah, and that could very well be the case as all it's more ephemeral.
02:23:24.484 - 02:23:27.316, Speaker B: That's what you're saying, it's more ephemeral.
02:23:27.428 - 02:23:41.448, Speaker A: Like if you want to do an oversized trade or something, I would say tossed users, they want to move sizes and they want some privacy. Or if you want to do some sort of an air drop.
02:23:41.624 - 02:23:42.850, Speaker B: Yeah, exactly.
02:23:44.340 - 02:23:47.628, Speaker A: I think a better term would be more informed.
02:23:47.804 - 02:23:49.100, Speaker B: More sophisticated.
02:23:49.180 - 02:23:51.570, Speaker A: Yeah, that's great. Thank you.
02:23:53.140 - 02:24:14.090, Speaker B: More informed. Okay. Not more toxic. Okay. Um, cool. Yeah. But that was a good preview into the next part of our talk because, yeah, we're going to go into some implementations of limit order options.
02:24:14.090 - 02:25:24.194, Speaker B: But a side note I wanted to mention about primitives is that, yeah, oftentimes they're stacked. Oftentimes, for example, their knowledge proofs are necessary in anything that depends on Fhe to prove that your ciphertypes are a meaningful encryption of some real data, not just trash. Also, these primitives could be stacked for defense and depth. So I know Tees are suspicious, but in a threshold encryption scheme, you could put each key share in a Tee, and that would be better than having it on a Word document. So, yeah, that was just a side note. And now we'll move into, I think, the meat of this talk, which is the actual implementation. So, yeah, we're going to look at two order books and two AMMS, and this is the framework that we're going to use.
02:25:24.194 - 02:26:01.234, Speaker B: So we're going to progress through three phases of computation, and these phases could take place off chain or on chain. So let's just go and do the first example and you'll see. So the first example oh, now I'm going to skip this one. The first example is Zaxi. So how many people here have heard of Zaxi? Yeah. Cool. So Zaxi is basically a blockchain that allows users to verify arbitrary programs.
02:26:01.234 - 02:27:39.100, Speaker B: You can make a program, make a verification key, upload it to the Zaxi directory, and then just create and destroy records according to your own verification key. So given this, how we would implement an order book or swap is that we would enforce some validity predicate on the notes created and destroyed or created and consumed in that transaction. So some important invariants include that the value is preserved, and it includes that the asset IDs are slocked, and it also includes that the owners of the output notes are slocked. So I think I went over this in a Hen Waili way, but this is the least interesting example. And you'll see one in phase one of Z, both your maker and taker, they independently create a proof for a spend of their notes. So this is the note that they're intending to trade and they prove that they have spent this note. So the point of phase one is that you can do it independently without help from anyone else.
02:27:39.100 - 02:29:14.890, Speaker B: Now in phase two is when they actually need to discover the counterparty. So they need to be made aware that there's actually a fulfilling order on the other side. And how Zaxi implements this in their paper, they call this a closed order book. And yeah, it's not very interesting because all it involves is both parties revealing their traits to a centralized off chain order book operator. And this order book operator matches these traits and creates the output proofs. So why this is better than just a normal centralized operator is that because the order book operator has to create valid output proofs that fulfill these validity predicates they're not able to mismanage your funds and they can spend them only if you find a truly matching counterparty. And so this order book operator creates a completed transaction that balances out and submits it on chain along with all the spend and output proof.
02:29:14.890 - 02:30:20.490, Speaker B: Okay, this example is easy, right? Yeah. So like we said, there is no privacy from the order book operator. And the question here is, is this okay? Yeah. And the answer, it depends on your use case. I think a lot of trading, whether on chain or off chain already, now goes through trusted intermediary. And if I'm, for example, a whale and I want to make an iceberg order that is like, I have a huge cell and I want to cut it up into little cells and I don't want anyone to know that they were all made by me. So if I trust an order book operator to not reveal this, then I can still execute all my little cells at a good price because this order book operator doesn't reveal them on chain.
02:30:20.490 - 02:30:51.080, Speaker B: So, yeah, from the perspective of the whale, they still got a good execution. So this was totally okay, but awkward's. Order book operator click front run. So I have it in text what I just did, but I won't bore you with this. I'll put the slides somewhere.
02:30:52.540 - 02:30:53.720, Speaker A: Just a minute.
02:30:54.320 - 02:31:42.472, Speaker B: The next example of order books is pretty similar. So this is implemented in something called Tiger, which is part of the anoma protocol. So it is pretty similar, but it's just phrased differently. It's phrased in terms of intent. So the maker and taker, once again, they each spend their note and they encode their desired outcome of their trait in an intent note. And this intent note is sent to an off chain solver. So if the solver sees a match and intent, they can consume.
02:31:42.472 - 02:32:16.500, Speaker B: Oh, this is a typo here. This should say tape. Okay. Oh, and I can't edit it right chris, this is not my computer. But yeah. So the solver matches and consumes intents to create once again the corresponding output. And now the transaction, the balance transaction consists of these three partial transactions.
02:32:16.500 - 02:33:07.314, Speaker B: Yeah. So it's similar to Zexi, but I think anoma or tyga, really, what they've done is make the intent of first class citizen in their protocol. And so, yeah, they're expressing their applications in an intent centric way. But if you wanted to implement this in Zaxi, you could. So those were the two oh, yeah, here's what I said in words. And those were the two order book implementations. Are there any questions? If not, we'll move on to AMM.
02:33:07.314 - 02:33:08.070, Speaker B: Adam.
02:33:10.110 - 02:33:45.570, Speaker A: I had a quick question. So I think the order book model that you have looked at so far was well, besides the anoma one, maybe a bit more exception, I guess, but with the zkxc one, let's say that order book was is it managed off chain? Yes, it is managed off chain. So I was wondering, how would it look like, let's say, if the order book was managed on chain in some sort of binary tree type of fashion?
02:33:48.810 - 02:35:02.410, Speaker B: Okay, so I guess we would put commitments to our orders on chain and then arrange them in a binary tree structure. I don't see a way to do that that doesn't just reveal the order because well, unless we assume that we commit it to some threshold who is able to compare greater than on the orders, this introduces the threshold committee, which Zaxi doesn't have. So I think the point here is that in Zexi, because they only use verifiable computation sorry, they only use zero knowledge proofs that don't allow further computation on the proofs. Yeah.
02:35:02.560 - 02:35:07.806, Speaker A: Could you do it peer to peer and avoid the one in the middle?
02:35:07.988 - 02:35:46.838, Speaker B: So instead of going through a centralized operator, could we maybe send gossip these orders, for example? So sort of the desired privacy guarantee here is that we only leak our order to the person who successfully matches it. And so if I gossip my order to everyone, then I might as well send it to the public mantle.
02:35:46.934 - 02:35:48.860, Speaker A: Yeah, it makes sense.
02:35:50.110 - 02:36:28.980, Speaker B: But if you wanted to decentralize the operator, you could replace it with a threshold. And I think that's what renegade dye does. Yeah. So by the way, this off chain solver is like, this is suave right now, except they don't need to make validity proof and they can do whatever they want. Yeah, but they wanted to put it in a tee, but I don't think they have done it yet.
02:36:42.450 - 02:36:55.154, Speaker A: Because it seems to me you mentioned intense are first class citizens and that was the only real thing that I could hold on to that seemed different from the two. So can you just make that clear for me and it's a solver instead.
02:36:55.192 - 02:36:58.520, Speaker B: Of like a centralized I don't think there's a difference.
02:37:04.490 - 02:37:14.890, Speaker A: Are partial orders right? Unlike the AMMS, even in Tiger, there would is like constraint partial orders in terms of a normal language?
02:37:16.110 - 02:37:16.860, Speaker B: Yes.
02:37:17.310 - 02:37:24.810, Speaker A: Do you find this kind of balance between privacy and counterpart discovery on a fair market principle?
02:37:25.470 - 02:37:27.162, Speaker B: Can you repeat the last part?
02:37:27.216 - 02:37:41.506, Speaker A: How do you get the counterpart discovery if you want to have privacy? If the intent is a partial partial order, the counterparty should add some sort of incentive to satisfy that. So it's not an AMM, it's just on an open state or something.
02:37:41.688 - 02:38:17.930, Speaker B: So, yeah, if you know the intent, you know the order. So the intents here are submitted to a trusted solver. Yeah. And so it's private to the blockchain, but it's not private from the solver. Thank you. So now we'll move on to some AMM constructions. So the most straightforward one is Secret Network.
02:38:17.930 - 02:39:56.710, Speaker B: So how Secret Network works is that each of its consensus notes is a tee, and each of the notes unfortunately holds this consensus feed inside it. So there was that attack SGX fail that proved that you could exfiltrate the consensus speed of the whole secret network from the tee. But anyway, this is not like a fundamental flaw in tee based blockchains. This was secret network's flaw. So how you would yeah, there's an app secret swap on Secret Network, and how it works is that it's an AMM, which means that the liquidity positions are all public, so the user can publicly pick an optimal route for their trade. So the user then encrypts their trade and the route to the consensus feed and submits it to one of the consensus nodes. And then yeah, this is decrypted inside the tee and executed and the updated state is reencrypted.
02:39:56.710 - 02:40:18.240, Speaker B: That's super easy. And by the way, how the Secret network consensus works is that every other validator also is like in the EVM, they also run the same transaction and convince themselves that they end up with the same state.
02:40:24.810 - 02:40:38.560, Speaker A: Could you run like could you do what Swabi does on Secret? You don't just run an arbitrary compute if it's been deployed on the TV so that the inputs are kept secret. But you run jobs like block building, for example.
02:40:38.930 - 02:42:17.740, Speaker B: Yeah, like Swabe, you could run a solver. Yeah, that's but what the difference between Deeper Network and Swab is that Deeper Network has a whole network of these te nodes that are state. So sort of a more interesting AMM is Penumbra, and yeah, Penumbra is more interesting because it uses batch swaps now for Secret Network, by the way, by just observing. Okay, I tried out secret swap before this talk, and then I went to look for my trade on the Block Explorer, and then I realized that every block in Secret Network had only one transaction. So if you look at the public prices before this block and after this block, you can reverse engineer exactly what happened in that block since there was only one transaction. Yeah. So really what this says is secret network only works if there's many transactions in the block.
02:42:17.740 - 02:43:20.600, Speaker B: Or another way of saying this is that it only works if the swaps could plausibly be part of a batch. So, yeah, basically, in an AMM, in order to preserve the privacy of individual traits, we need to batch the traits. Otherwise, sequentially executing each individual trait would precisely allow us to reverse engineer them. So what Penumbra does is batch swap. Yeah. And so how it works is, first of all, each user, once again, they privately spend the note that they intend to trade. But here there's an additional step where they encrypt the spend to Penumbra's threshold.
02:43:20.600 - 02:44:33.330, Speaker B: And so the guarantee of the threshold is that only a quorum of them working together could decrypt any of these ciphertext. And so our assumption is that a quorum of the Panambra threshold will not collude, that we cannot get this quorum. So now, individually, these validators can add up the encrypted sense because they were homomorphically additively, homomorphically encrypted. So individually, each validator can blindly add them up. And by the way, it doesn't matter what order they add them in commutative. And so the validators reach some encrypted batch spend, and they work together to decrypt that batch spend. And this batch spend is now publicly decrypted and publicly traded on the AMM against the liquidity pool.
02:44:33.330 - 02:45:27.710, Speaker B: And then later on, what the users need to do is they need to claim their share of the output. And how they do this is they prove the knowledge of the pre image of some input that was involved in the batch slot. I will let this sink in. It's a cool design. They call this primitive flow encryption. So, yeah. So, once again, your anonymity set is the number of slots in the batch.
02:45:27.710 - 02:46:33.086, Speaker B: And if your batch has one slot, you get no anonymity. Yeah. And yeah, your goal as a trader is to preserve your long term strategy. So, since each of these spends and outputs is like a node commitment on Zcash, so a third party doesn't know the individual values committed to inside these spent and outputs. A third party cannot link spent outputs by the same person across different batches. Any questions?
02:46:33.188 - 02:46:39.010, Speaker A: Is there any time constraints, like, happening, like, every 30 seconds or minutes or batches?
02:46:39.670 - 02:47:18.190, Speaker B: Yeah, that's a good question. Yeah, I think Penumbra has a block time. And so the point of the batch swap is that it happens at the same granularity as the block production. And this is as opposed to an ethereum, where individual swaps are at a finer granularity than the block. And that's the problem. That's what causes mev, is that you can reorder trades inside a block. So, yes, the batch executes as frequently as a block is proposed.
02:47:18.690 - 02:47:22.010, Speaker A: Actually, the bigger the batch, the better the privacy.
02:47:22.170 - 02:47:36.190, Speaker B: That's true. If we waited longer, we would get more privacy. That is true. And the trade off is like for example, if I like the price now, I would rather trade now with less privacy.
02:47:36.270 - 02:47:57.770, Speaker A: Maybe the bulk time in this case wouldn't matter too much. Right, because I could arbitrarily set a parameter to say that Penumbra want to accumulate, let's say, thousand transactions, and if it doesn't reach that batch skill, then it just simply doesn't go on chain.
02:47:58.270 - 02:48:18.014, Speaker B: Maybe. Yeah, maybe. But once again, that's a trade off because as a trader, I don't have this guarantee that my trade will execute within certain time, and it could have slipped a lot in the interim.
02:48:18.142 - 02:48:47.820, Speaker A: You could also look at the solar secondary markets, the price of one goal. Like if you're trading sizes on same highly demand assets prices, it's still a really good design. But if a group of actors start spamming the blockchain with the transaction, they can reduce an empty set of everyone else right, and try to recover the information.
02:48:48.430 - 02:49:14.958, Speaker B: Yeah, definitely. You could fill up every trading pair with swaps that are your own and are totally meaningless. And then you go and observe the public price change and, you know, like, okay, none of that was caused by your garbage trades. Right. So it must have been caused by the only trade that wasn't yours.
02:49:15.134 - 02:49:25.480, Speaker A: Yeah, okay, thanks. Isn't the only thing you're getting here Anonymity? You're not getting, like, privacy of the trade price?
02:49:26.970 - 02:49:55.490, Speaker B: Okay, yeah. There is no privacy of the trade price because the price is public in an AMM. But what is private is the amount sold in each of these orders since they're executed as a batch. You don't know how much of the batch was contained in the first slot or the second slot. That's why the larger the batch, the better your privacy.
02:49:56.630 - 02:50:11.830, Speaker A: Is there any difference between someone doing a Penumbra style thing here and let's say, what Railgun is doing on L One or Aztec connected? Just having an anonymous shielded tool that's interacting with a public arm.
02:50:12.330 - 02:51:20.980, Speaker B: Yeah. So Railgun nocturn is also doing something similar, I think. And what they do is they allow arbitrary transactions from an anonymity set and so it's I think so look, if I spent from uni, if I made a trade on uniswap and I spent it from the nocturne shielded pool, this trade is made in the clear. Right. And so this trade is completely non confidential, but it is fully anonymous because anyone in the Nocturne pool could have done it. So, yeah, in that case, you only get Anonymity and you don't get the privacy from this batch. The batch swap, it allows you to conceal your long term strategy.
02:51:20.980 - 02:51:55.332, Speaker B: Like, people can't identify multiple orders that look similar, for example. Am I out of time? Okay, so that was all my slides and I wanted to do a little discussion, but we can do that after, like, at the end of the night. Thank you.
02:51:55.466 - 02:52:43.876, Speaker A: Thank you very much. Today we're going to be talking about proven decentralization and specifically how we approach it and certain design considerations that need to be taken into account. So I'm not going to bore with the whole roll up is how do you define a roll up? Because I think most of if you want to Joe, if you're interested right now yeah, I'm not going to pour with that. So let's assume we know what a roll up is. There are two types of roll up optimistic rolls, ek roll ups. We're talking about ZK roll ups now. And in case there's anyone from startware here, when I'm saying ZK roll up, it's what you think of when you say validity roll up.
02:52:43.876 - 02:53:42.032, Speaker A: It's the same thing just in case because startup people get confused by the word Tk roll up. And by default tk rollups have two roles, explicit roles. One is the prover who's responsible for computing the validity proofs and then the sequencer which is responsible for ordering the transactions and basically proposing preliminary sequences of transactions that are then finalized by the base player. And of course, the roll up communicates with the L1, which means that we have a bridge on the L1 that ensures that the validity proof is verified and also the data is correct. Viso is going to explain a bit about the building blocks of scroll, how it is right now, and then we're going to go into the details about the design considerations. So basically the current scroll system is completely centralized as it is right now. And it basically has two fundamental components.
02:53:42.032 - 02:54:48.120, Speaker A: So basically the sequencer, the sequencer which takes input from users and research transactions and is responsible for basically producing an ordered block of transactions that are executable. And the output is blocks which may be aggregated into chance if there are not enough transactions to fit circuit capacity within a block. So the next part is the proverb which basically takes us input a block and computes decay proof that proves the validity of the execution. And it does this for every block or chunk until we have enough of those to fill a path, which is basically the final unit in the scroll system, which is basically aggregates all the proofs for the individual components, the blocks or channels, if you like. And this again, zero knowledge proof that aggregates all the other proofs that are completed. And this is what gets submitted, like twelve one for finalization, which verifies the proof and basically updates the state puts, et cetera. And basically this is the completion of execution.
02:54:48.120 - 02:55:14.560, Speaker A: So is this finish? Yeah. Let's move down a little bit. So the really big component, the important component here, what we're dealing here, this is the most fundamental component we have. Approver, we can see your chain. Okay. Yeah. Without approver, you can also see the system.
02:55:14.560 - 02:56:07.220, Speaker A: But the proverb hits sometimes range right. It's really expensive hardware. You need like a VP, GPU and the proof kind of takes quite a bit of time practically takes like 1520 minutes. There is active research on basically reducing the proof time and also increasing the sailboat capacity but still a little bit ways off. So given that the provers have obviously the scaling factor, the scaling limit, so you can only scale the system with the number of provers that you have. Because even if you could cram like thousands of transactions solana style thinner block, you still need to be able to prove this block on time. And basically as you produce more and more blocks we need more and more provers so that basically you can keep your pipeline full.
02:56:07.220 - 02:57:01.216, Speaker A: So if we do back of the envelope calculation, this is my favorite types of calculation. So let's think about this. So if you have a three second lock time, which is the current lock time in scroll, that means within a 20 minutes period, which is how long it takes to compute a single proof, you're going to need at least 400 live probing hardware units. And if you want to add some fault tolerance because sometimes these units fail and you need a fallback and timeouts and everything. So you need to double that. And if you start thinking about okay, we have low transaction volume now we have like 3 seconds. But then what happens if we have high retraction volume as we hope we will have today and basically we make a block time 1 second then that means you need thousands approvers what's throughput size in the 3 seconds.
02:57:01.216 - 02:57:53.110, Speaker A: So currently blocks are pretty small but we're going to increase that. So if you go down to with high volume and you have 1 second block it's feasible to have eventually 1000 actions per block. It's a pretty decent DPS. But the thing is that you need hardware to drive it, right? Okay. So this is where the impetus is coming for decentralized approvers, right? It's pretty obvious there are a few reasons why we would like to decentralize approvers. So one, it just makes sense to decrease the probability of something just completely failing and therefore resulting in a liveness failure. Because if you have a decentralized proverb network then let's say if one program fails it's okay, you fail over to another one and it's fine.
02:57:53.110 - 02:59:00.412, Speaker A: And then also another thing to consider are operating costs. So you have for example a bunch of former ETH miners shout out to them that have their hardware just lying around doing nothing and that hardware is pretty useful for us. For example, you could hook up a rig and compute proofs. Whereas if we were doing it ourselves we would either have to either have to rent all the hardware or buy the hardware and build the rig ourselves. Which means that there's a lot of upfront cost even assuming that we somehow solve the problem with decentralization. And obviously we are building blockchains here and one of the primary, I don't want to say selling points but more like one of the benefits of blockchains is that they can be not trivially, but they can be easily decentralized. And I think a centralized blockchain just doesn't make sense.
02:59:00.412 - 03:00:01.788, Speaker A: Just use what's the point of a blockchain? So it's kind of part of the core ethos of what blockchains are about and therefore it just doesn't make sense to build a system that is centralized even if you ignore everything else. But of course, there are technical reasons why decentralization makes sense. So given what the rules basically we can do, why design? So basically we want to create a certain network where the costs are shared and the rewards are shared. And basically this creates alignment with the values of the ethereum community. It builds mindset and basically it's a must to move. We cannot afford to do it because otherwise what's differentiate us from solana? Why are we even the first? I'm not signing off on this. All right, so basically the pillars of decentralization is all the three things that need to align.
03:00:01.788 - 03:00:48.030, Speaker A: There is protocol, there is economics and there is governance. So here right now, we're in the phase where we are designing the protocol with input on how the economics are going to save the protocol because you cannot decide land one without the other. And governance is something that the system gets decentralized. It must come in because you're not the owner of the network anymore. So there has to be some governance spectrum. But this is something that can come in the form of a Dow or specifics of the L One contract, but this is something that can come slightly later when eventually we have a token. So with that, how do you decentralize the progress? Yeah.
03:00:48.030 - 03:01:28.872, Speaker A: Is it working? Yeah, it is working. Yeah. I think one of the main goals that we're going for, even initially when we were thinking of how to design reverse, is to make it permissionless. And also one taking here doesn't mean that we assume that there is some way to have civil resistance. In this case, assume there is ETH token, but you can also do it with POA, et cetera. There are multiple ways you can do it. So that was the core pillar of what we're trying to do is to have a permissionless prover network.
03:01:28.872 - 03:02:32.480, Speaker A: And on top of that, we also understood that we needed quite a lot of proverbs. So we understood that the prover set has to be quite big. As Diesel explained before, even like in the very optimistic scenario, you would need hundreds of provers and if you crank the proof it up, you might even need thousands of provers, which is not such an easy task. So, currently, the design that we were going for initially, we had a sequencer that was computing blocks, proposing them, and then basically assigning provers or like, deterministically getting provers assigned to each block. And then those provers computing the proofs and submitting them asynchronously let's say if the prover failed to submit the proof within a certain period of time, then we could have just elect a fallback prover and that prover would submit the proof instead. Vizo. You're going to go into the details? Yes.
03:02:32.480 - 03:03:30.268, Speaker A: So basically the main idea here is that you start with the VRF. It doesn't have to be an internal network or something particularly complicated, just needs to be something that provides consistent deterministic randomness about. Okay, this is syximum basically consistent randomness that can be deterministically verified by everyone and this you can do with prevent all. You don't need anything particularly complicated. Now, given a random seed and the set of participants, which brings us back to this has to be known at L One then everyone can compute the same order basically by using a PRNZ you don't need anything particularly complicated, just a permutative congressional generator will do. But what you need is good statistical properties so that the output is uniform. Given that then all participants compute the same schedule and you know who the next brewberries for each particular slot for this particular block and who is the aggregator for these bats.
03:03:30.268 - 03:04:14.930, Speaker A: So this is a nice and simple setting that have some missus. However, the first thing is that you have to fix the rewards to happen at l One. So you fix the rewards and everybody gets the same reward. Okay? That's a government issue and it's also not particularly efficient because you don't know whether these rewards have anything to do with the actual market cost of what we're computing. On the other hand, this is a very fair system. There is no way to have a centralizing force that can monopolize the product, the proofs and the fact that the rewards are known. It means that you can have a stable income and you can do your planning on that.
03:04:14.930 - 03:05:34.532, Speaker A: So of course that's not the only approach and it also has one of those abundance because basically once you've settled on your proof, what's the incentive to improve my proof? Why should I make 30 proof times priority or anything like that? So it's clicking leak stagnation of technology which is not a very good property to get. So given that there's also an alternative approach how you can do it. So you can do it with an action mechanism. So again, given the set of proofers and you have a block produced or bat, then you forward the block into the network and the provers beat how much they want to be rewarded in order to produce proof. So this in a sense a tender because it's not highest bidder, it's lowest bidder, so it's a tender that they're bidding for and it has the advantage that it's completely market driven, it's market forces, it's capitalism versus communism if you like, but it has some problems, right? So there is auction protocol complexity because you need to design an auction protocol that is fair. So that. Basically you can be front running, you can't just have a central trust identity resubmit the bids because then the central trust identity would front run you and you can track where to submit a lower bid.
03:05:34.532 - 03:06:26.454, Speaker A: And the other problem is that there are serious concerns about basically centralizing forces. So if you got the past as proven technology, then you can potentially monopolize the broker market. So that's disadvantaged. Yes, but we believe that there are solutions there. We're actively working on that. You have one. I think before we go there, there are also a few things to add that also should be part of the consideration about how to design provers is a lot of people, when they think about provers, they think, oh, yeah, you can just MLA's proof of work, like do something like what Bitcoin does and just have a bunch of provers compete with one another.
03:06:26.454 - 03:07:24.918, Speaker A: But the difference between using this approach in proof of work and in proofs there are a few differences. So one, it's not a difference, it still applies to proof of work, it's inefficient, it leads to a lot of waste of resources. It's not really necessary in this case, but the main concern is it's very centralizing because in Bitcoin there is randomness involved, inherent randomness involved in the process of how you select the leader. And so essentially you can think of your probability of winning at a given round, assuming all the other conditions are fair. So the network is synchronous, et cetera, et cetera, is roughly equivalent to your percentage of the hash rate in the network. Whereas in here it's likely that the most efficient prover is always going to win. Which means that long term you end up with a scenario where the most efficient prover always wins.
03:07:24.918 - 03:08:29.642, Speaker A: And is that a good thing? Because you end up essentially with the centralized prover and instead of having value, might as well just run a centralized prover yourselves. There's no difference here. And thirdly is no, that's it actually. Sorry. And then another thing that we have been actively considering lately, and we think that it's the right approach and I actually gave a talk about it at another event today, is we actually don't really need sequencers as an explicit role. So think of it this way, if you have a protocol that has a decentralized sequencer and a decentralized proverb, so you have a bunch of sequencers, let's say a few hundred, and then a few hundred provers as well, the sequencers are likely to outsource the block construction to some specialized builders the same way ethereum works. So what do you end up with? You end up with a system where essentially sequencers are the middleman between the provers and the builders.
03:08:29.642 - 03:09:43.640, Speaker A: So why not just remove the sequencers, give the pre confirmation rights essentially to the provers and just have them run the whole thing and just outsource block building. Because at the end of the day, it makes no difference from the protocol perspective, from the security perspective, but at the same time you leak less value and incentive balancing. It becomes much easier because assume that you want to distribute the incentives correctly between the provers and the sequencers by default. Think of it this way because sequencers have the privilege of ordering the transactions, they get to keep all the mev profits by default if you don't enforce any low incentive balancing. And it's likely that mev is going to be a significant percentage of the value accrued by the network. Which means that there are less incentives of being a prover than being a sequencer. And we actually need more provers than we need sequencers because even if you have like 2030 sequencers, that's perfectly fine, we don't actually need that many.
03:09:43.640 - 03:10:51.114, Speaker A: And you could solve it by adopting PBS or like a variation of PBS. But then the problem is if you assume a scenario in which the sequencers outsource the sequencing rights to the builders, which is likely to happen in a decentralized scenario, then you essentially end up with a model that has PBS inside PBS, which means that first, you introduce additional complexity that isn't necessary. And two, as I said before, it leaks value. And so we have been recently for a few months working in the direction of completely deprecating the sequencers and just basically giving the sequencing rights to the builders and building the auctioning model between the builders and the provers. So the provers run the consensus in the network and are responsible for pre confirmations for the users? Yes, and this is pretty much where we are. We've been churning through designs internally and we're now ready to start talking about them, hence this presentation. Just give me 1 second.
03:10:51.114 - 03:11:45.760, Speaker A: But basically we would like the community input here because we want this to become more of community project and would like to hear critique about both approaches so that we can move forward. And the nice thing is that we don't have to build a fully decentralized system in one go, right? We don't have to start with both decentralized builders and decentralized progress. What really matters right now is that we get to decentralized progress and we can do this initially with a centralized builder, that's not a problem. And later we can even decide to adopt, for example, base prolapse. So base sequencers, which is something that the ethereum I was going to say that, but just to know this is probably five years away, so maybe in five years we'd say to adopt it. So there's nothing stopping it from doing that. But the important part is going to have a satellite market for provers and a market that's efficient and fair.
03:11:45.760 - 03:12:55.460, Speaker A: I think. One last thing that I also wanted to address because we get a lot of questions about it and I think you guys also probably get it quite a few times is what about prover markets? Do you want to support proverbs or not? And at least we thought about it a bit and we think we're going to build a system that is agnostic to how you compute the proof. So essentially think of how Ethereum works. Ethereum is agnostic to how the staking happens, whether you stake and run the full node yourself in the validator or you give it to LIGO. And we think we're going to take a similar approach where you can either run the program yourself or force it to somebody else, et cetera, et cetera, and just let the market forces figure it out. Because at the end of the day, in our case, having, let's say, a lido like entity on scroll doesn't really impact the security that much. The worst thing that can happen, your groups get delayed by like a few minutes, which is not that big of a deal, whereas an Ethereum lido can actually cause a lot more problems.
03:12:55.460 - 03:14:20.298, Speaker A: We don't think something like lido emerging on scroll is a worry and therefore we're just building the system that is completely agnostic to how the proofer is, how the provers actually get to produce the proofs. And one more point is, hopefully by the time we get to release this stuff, visa and I commit to the fact that our blog post, et cetera, are going to look a bit nicer than absolutely the known sledge, I'm going to make sure that anything that comes out from scroll is a bit nicer than this. Would you have a question? Yes, actually, I have a background in mev, so I'm quite interested in improving markets. I think you're going to hear more about proverbs extractable value. And so one interesting thing, and I agree with the sequencer logic, is that there could be vertical integration between the block builder and the provers, because kind of the receipt to become a good builder is to be quite efficient in your compute is the same on the proverb side. And the other thing is, if you're a builder, you can kind of front load the proving job, so it gives you an inherent advantage. But I also agree that the sequencer could be a blocker, but it's kind of an inefficient blocker.
03:14:20.298 - 03:15:18.242, Speaker A: But I don't know what's your take on that. Funnily enough, when I joined scroll in late 2021, that was the first thing that I brought up, like this similar point. So we basically went a full circle and just came back there. Yeah, to some degree, builders, the roles are not similar in a sense that they perform similar roles, but the requirements are similar that you want to maximize the performance and extract as much as possible from your hardware. So I think it's possible that builders would be running provers. The only thing is that with provers we actually need a lot of provers, whereas with builders it's likely that it's quite decentralizing for zero. So you're going to end up with a handful of dominant provers sorry, builders that are participating in the network so I'm not sure if there are enough incentives for them to run a large number of provers but just running one proverb won't make a difference for a builder so I don't see why they would do that.
03:15:18.242 - 03:16:14.820, Speaker A: Another point I'd like to make is that running a builder is like running a service, right? It's a user facing service that has certain difficulties running it's not something you can run at your home. So that's yet another reason to basically make sure that we have professionally run builders that basically are highly efficient and providing you good user service which is not something everyone can use. And on the same time, the builders are shielding the rest of the network from all the user traffic. So they play a very important role. And it's absolutely fine if we end up with just a handful of them running hand, for example. But the only thing is, with that, we need to somehow try to enforce real time censorship resistance, because otherwise you'll have to rely on forced inclusion, which can take hours. So it's not nice and we actually have working ideas on that and we're actively working on that as well.
03:16:14.820 - 03:18:10.378, Speaker A: Hi, sorry, I have a quick question could you quickly go back to the VR part? Can you can see that someone is a Linux or user straight away? Yes yeah so as far as I understand you have these pseudo random selections based on VRC and provers are all like just permission instead of prover pools and you kind of like set it out there, right? Yes so basically think about it, you get from L One the list of approvers they're known this is why I know there was a mention about staking so somehow they need to appear on L One. So basically given from L One pre brando and there is possibly a transformation of each so that it applies for the next NF box. So you come up with a seed so basically you sort the progress, you generate random number given this random seed and then basically you just create a CDF and then you select the one that falls in there and are they staking or are they just it doesn't have to be staking. You can take different approaches here. Well approach for example, you can make you basically compute a certain amount of proofs that are required to be submitted to bill one, to be eligible to be an L. Two proof there are a few ways you can have civil resistance. It doesn't have to be staking, but I think using something like eat as staking makes sense here.
03:18:10.378 - 03:18:33.806, Speaker A: Right. It's also very simple. Yeah. Not restaking though. Different topic. Yeah. I think one thing I was trying to understand better was, like, what would be the cost of bribing in terms of if you have, let's say, I don't know, for whatever reason it might be, perhaps that at.
03:18:33.806 - 03:19:16.170, Speaker A: A certain slot in the future time. You know that the river is XYZ consecutively and that you know that perhaps some tokens are launching XYZ consecutively. But maybe, perhaps you are the sequencers for the Y slot XYZ Y. So you may have some incentive to censor all of the proverbs XYZ such that you can accumulate all the med all the way back but censor the proofer in terms of not letting them to submit. To submit proofs, yes, submit proofs. Basically that doesn't make a difference because your blocks are already so your data availability is already on L1. So the only thing that you do is you churn through the provers is a latency.
03:19:16.170 - 03:19:52.666, Speaker A: Yeah, it's a latency. So the only thing you achieve is you delay on Jquenality. That's the only thing that could there be a time in which you can actually achieve some sort of reorbs within the L two? So the current line of thinking that we're going to adopt hot stuff too and run hot stuff too. So basically the difference is that we want to use broadcast instead of point point. So you're going to have hot stuff too to achieve some sort of pre confirmation. Yeah, you're going to have strong economic confirmation and then you and then you get actual penalty. Right.
03:19:52.666 - 03:20:49.500, Speaker A: Because without it then you could say that whatever sequencers at Y that is blocking out all of the route. And this is also one of another disadvantage that you have with the L1 generated schedule is that basically you have to protect the IP address of the provers because it's also a problem that exists in the ethereum. Core network in the Valdez network. Because if someone knew the proverbs so, you could basically launch a DDoS attack to log it offline if you knew the IP address. So this is one extra difficulty which is harder to pull off with an oxygen mechanism because then you don't know in advance who the program is going to be. So if you're going to keep the ones independent against entire network so you can tell that Visa is a networking guy based on how much concern he has about I don't joe, I think you had a question. I forgot it.
03:20:49.500 - 03:21:29.914, Speaker A: Maybe. So it's my understanding that you guys break up like chunks and blocks are the same thing, right. And then a batch is a bunch of chunks. So basically the tank is comprised of multiple blocks to fill the separate capacity. So if you have enough transactions inside a block, then the block becomes a tank. So it's kind of an intermediate like plastic construct that's going to go away once you have enough volume and then the chunks are proven by different provers in the proverb network. Absolutely.
03:21:29.914 - 03:22:10.374, Speaker A: Okay, and then last part of the understanding here is is there an aggregation of the yes, yes. Once we have enough tanks, enough logs, basically there is an aggregator on gates that picks up all the ZK groups from the individual components and creates an aggregate group. And this is what gets submitted and verified on L One. Okay, so that's the question. Is this a centralized person? No, you just pick it from the same driver set, basically. Okay, so for each batch, so set of chunks, you allocate an aggregator similarly to how you aggregate. And you can even go further and potentially in the future.
03:22:10.374 - 03:23:01.638, Speaker A: And not aggregate one batch chunks for one batch, but aggregate for three or four, even 100 batches. It's just that at the moment, the most optimal approach is to aggregate for one batch, but you can see how you can expand it and do it for multiple batches. Got you. Oh, you remember. Are you not concerned about the complexity of implementing a competitive algorithm on the L2 to make this all work? We discussed that basically in terms of how we launch this to the market. Basically, if we start with having the full decentralized network with the validators and consumption and everything, then it's going to take us like a year to go. If we start with what we have now with the centralized, sequencer, effectives but decentralized the proverb network, then we can launch in six months, which is like a big difference.
03:23:01.638 - 03:23:40.302, Speaker A: And not only that is that no matter what approach we take into decentralizing the builders, the proverb network is more valuable because that's where the value is. We create value in a valuable network, we can easily transition to whatever sequencer technology we adopt in the end. Right? Not even sequencer. We don't like this term anymore. Builder technology. One thing to also consider is that we discussed it already and I think if we go with the consensus route that's the way we're going to go about it is we'll just verify the data station for each round in circuit rather than publishing and verifying it on an L One. That will make it a lot cheaper.
03:23:40.302 - 03:24:11.146, Speaker A: You'll still have a bit of overhead in terms of tracking the sets and everything, but it's going to be nothing compared to verifying an actual certificate on their own. I just think the complexity complexity is there more on the cost. It's not just the communication complexity. Communication complexity, but it's going to be worse. Is it fair to say that you just said that the value is in the proving network. Is it fair to say that you think more value is going to be.
03:24:11.168 - 03:24:12.454, Speaker B: Captured by the proving network?
03:24:12.502 - 03:25:01.654, Speaker A: Because from our perspective, it seems likely that most of the value is going to be captured in the block building from the builders. You're talking about the economic value we're thinking about in terms of the social value in a sense because this is what allows you if you have a proven network, this means that you can achieve higher throughput higher robustness and have mindset and build the community, but also value as well. Though if you implement PBS, like for example, if you look at Ethereum now the majority of the profits that are generated by the builders go to the Proposers. So you can see how the same can be replicated on the L two. Similarly, obviously it's not going to be the same. Yeah, it works on Ethereum because the proposer has the right to sell the block. The proven network doesn't have the right to sell the block if you're giving it to the builder.
03:25:01.654 - 03:25:50.166, Speaker A: I'm not sure what you mean by that. So on Ethereum I'm a proposer and I've got me view boots running over here. You come along as a builder, you say hey Joe, do you want to do your backup block or I'll give you ten E to put the block on chain. You can only do that because I'm the current proposer. But if you remove that role but we do have that role. The same proverb that is selected to compute the validity proof is also the one that acts as a leader, so essentially as a proposer. But I guess maybe to kind of reiterate this point, in a sense you have a strong monopoly by the sequencers to basically perform whatever sequencing of that specific block versus if it is just a prover.
03:25:50.166 - 03:26:36.166, Speaker A: I mean they don't have much control over the sequencing of the blocks and that they don't really have that kind of monopoly over the block as well. So to that point I do think that the sequencer has more monopoly. But does that really matter? Does the proposer have a monopoly on Ethereum at each slot? Well, only for 12 seconds. But you can emulate the same here where the prover that has assigned the leader role for a given block has the monopoly so they can choose whatever block they want to include. So it's the same thing here. I don't see how it's much different. Don't you post the core data before? So it's kind of committed? The data is committed after the block is proposed.
03:26:36.166 - 03:27:28.310, Speaker A: So you propose the block but then you verify. I mean you went the proof right after, right? Absolutely. We have the concept in the fully decentralized design, we have the concept of block reveal. So basically, initially the block builders submit bids. Basically the proposer calls for beats from the builders and once a builder is selected with an appropriate right, if you like, then the builder is committed to revealing the block so no one can sell your block. Also as you mentioned, because you will implement Hot Stuff for consensus, doesn't that also mean that whether or not prover actually does the job or not, it's more of a latency delay. Hence why I still believe that the sequence latency delay for what? For confirmation or for proof? For the confirmations of the proofs.
03:27:28.310 - 03:28:19.942, Speaker A: But the latency is a few seconds at most, especially in Hot Stuff since it's no, because as you mentioned, if someone is censoring the provers and that they're bribing the proofs not to submit the blocks sorry. Not to submit the proofs. Let's say in that case, because you have a hot stop, you still have proof confirmations at each checkpoint, right? Yeah. As such, proverb doesn't really have that much leverage to say that, okay, this brocket cannot be produced. It's just that if they are censored that proverbs doesn't do the work, blocks just gets finalized a bit later in the time. So in that sense, I still think that sequencer would have more monopolizing power over the bumper, but I just don't see how it's different to Ethereum. So let's say in Ethereum right now, you have a million validators.
03:28:19.942 - 03:29:24.800, Speaker A: Each validator has a monopoly for ordering essentially for 12 seconds, and you have the same thing. So Ethereum you can think of Ethereum's consensus in a similar way where it has the phenology, but it's essentially tendermen, but the voting is stretched out across two epochs rather than within one swap. And so I don't see how you get different properties if you use provers here rather than oh, no, I was just, like, responding to the point that you mentioned about how prover is kind of like more of a monopoly over compared to proposers on the TBS. Oh, no, I wasn't saying that. I was saying you can think of provers playing a similar role to what the proverb the other way around. So you can think of the proverb playing a similar role to what the proposer plays on that one in this specific part of the yeah, and which is something that I was disagreeing on. Okay.
03:29:24.800 - 03:30:08.330, Speaker A: I don't see a difference, to be honest. Yeah. You give one prover the right to pick a block based on the first one. This game of removing the sequencer altogether from the picture, it's clear that it works with L1 skate even for picking up a proverb, does it also work? If you have proven bidding like the other approach that you presented? I think we can make it work. Yes. Great. Thank you very much for the thanks for having us.
03:30:08.330 - 03:30:35.400, Speaker A: Yeah, thank you very much. All right, so this is Joel and then this is Copper. Enjoy. We also have Santiago over here. Yeah, no worries. Yeah. We're going to talk about decentralizing Aztec proving that we're we'll wait for Elvis to leave the room.
03:30:35.400 - 03:31:00.480, Speaker A: That was a very good talk, though. And actually, it helps me speedrun my talk to get to the interesting bit. So shout out to the scroll folks for setting us up nicely. I'm Cooper, I'm a product manager at Aztec. You have Joe, our co founder and president, and Tom Thiago, who's one of our engineers. And yeah, for those that don't know, Aztec is a privacy first L2 on Ethereum. That's our new tagline and we're sticking to it.
03:31:00.480 - 03:31:42.822, Speaker A: If we change it in a few weeks, don't ask me about it. Aztec also the privacy first, L2 for Ethereum supports smart contracts written in Noir, you may have heard of Noir and it enables both public and private on chain smart contract composability. You can try it today the sandbox is out. You can write your contracts and start building stuff. The agenda today we'll talk about why decentralized proving, we'll talk about Aztec's proofer coordination, request for proposals, and we'll talk about what the future kind of looks like. So similar to Tokroll and scroll, we see three kind of major reasons for decentralizing proving. The primary one is censorship resistance.
03:31:42.822 - 03:32:43.950, Speaker A: If you're building a L2 with an enshrined prover, they can censor the whole network. And if it is whitelisted and or the only one who can actually submit those proofs on chain, they can censor the entire L2 and prevent it from producing blocks. Similarly, that affects liveness in the event that provers drop off the face of the earth, that halts block production and takes time to deal with. And so that results in worse user experience, among other things. And then it's also really important for credible neutrality and I guess the vibes of crypto and what we're all trying to achieve, the idea that any centralized proving network who has a whitelisted prover can make arbitrary decisions that might be in their best interest but not in the best interests of their community. And those people building applications on top of that network is something that actually distributing and decentralizing your node infrastructures can support. It supports credible neutrality, pumps the vibes, gets people involved in your protocol.
03:32:43.950 - 03:33:18.774, Speaker A: I just have one thing on censorship resistance. As a privacy network, we can't take the view of progressive launching centralized sequences. The censorship resistance kind of portion is the thing we try and prioritize above all else, just based on kind of how we've seen the world react to privacy protocols is all. Yeah, and so you see scroll, you see other projects talk about we can decentralize in phases. For Aztec, these are day one main net requirements. And so why aren't we there yet? Honestly, it's hard. It's harder than other challenges.
03:33:18.774 - 03:33:51.490, Speaker A: A lot of people are talking about decentralizing their sequencing now, which gives better censorship resistance guarantees in the L2. You don't have to use inclusions or forced transactions. You can expect an honest proposer to come through and put your transaction in a block. So decentralizing sequencing maps really nicely to L One, PBS and all the work that has gone into decentralizing L1s for years. This is much more analogous to decentralizing compute networks or other types of things. And so there's less historical work for decentralizing proof networks as well. We can take lessons and topics from other things, but it's fundamentally kind of a new pickup challenge.
03:33:51.490 - 03:34:43.400, Speaker A: And then I would say the other thing is lack of social pressure. Like a lot of people are using L two S right now that are fundamentally centralized, that aren't censorship resistant, and they're putting hundreds of millions or billions of dollars into them. If that's the case, and people are adopting them, what incentives do projects actually have to decentralize? It takes customers, it takes people in the community. It takes people actually advocating for those rights and those requirements and their protocols for them to get implemented and have research and put money into adopting them. And so how do we get there? What can we do about it? We can do open research like what we're doing right now. We get here, we talk about it, we get people aware of the fact that you actually need someone from the community advocating for these things, for them to get adopted into protocols, open source code. Don't want to name any projects, but a lot of proving systems aren't open source or are actually open source after they've been in use for a while, which is interesting.
03:34:43.400 - 03:35:23.282, Speaker A: And I think we do need to be very proactive in applying social pressure. There's two ways where we get censorship, resistance and decentralized L two S. One's proactive with people like us advocating for it, encouraging teams to take these things seriously. The other one is something really bad happens and you either become decentralized or you die. And so we really think that the people in this room, specifically, and those at Desconnect need to be more proactive in actually asking for these things. Otherwise we may never actually get yeah. How is Aztec helping build this future? All of our major network design decisions are gone through a public RFP process.
03:35:23.282 - 03:35:46.010, Speaker A: We have an outstanding question. We put it up in an RFP, we make a decision. Consulting with industry leading experts. Hi, Yuki. Yuki has been super helpful in all of these RFPs that we've done, among others. Eventually we'll ship an open source reference implementation, and at the end of the day, as heck, really believes in social consensus. And so it's up to the community of node operators to actually agree on these changes and think that they're better for the protocol.
03:35:46.010 - 03:36:19.860, Speaker A: We can't force them into it, among other things. I'm also speed running this just because Togral talked about a lot of these things, so we can focus on the interesting design aspects. Yeah, so we did a sequencer selection protocol, which was actually written by Santiago, very similar to what Togroll and scroll was just talking about with a random VRF based leader election. We currently have a RFP for upgrade mechanisms. So if you're interested in designing blockchain governance upgrades, decentralized L two S, submit a design by Friday. If you have a good idea, let me know. Maybe we'll extend it just for you.
03:36:19.860 - 03:36:50.862, Speaker A: And then we currently are in the decision making process of our approving network, and so this is what we're here to talk about for the rest of the time. Highly encourage you to go to. Forum aztec Network. If you're interested in the upgrade RFP, there's about nine designs out there right now. Yeah. Joe, you want to speak to the requirements or paula yeah, I can maybe start. I think one of the key differences to Public l Two is thinking through what a transaction is.
03:36:50.862 - 03:37:13.718, Speaker A: So before I dive into these, maybe just kind of explain how Aztec transactions work. So ANZTEC transaction is an onworld proof. It's about 25 to 60 KB. We're not quite sure yet, but quite big. So it's not just a signature and a request to perform an execution. It is the execution, then a proof of that execution. So there's a big difference there.
03:37:13.718 - 03:37:57.026, Speaker A: And that affects how we do sequencing and proving, because if you have a private mempool and no one gives you those proofs, you have a very bad data availability solution. And it's a lot worse than the one on Public L2 S. So a lot of our designs are kind of done with that in mind. Basically, I wouldn't go through the elements that we've done. Yeah. So just basically focusing that this proverb network design is compatible with whatever sequencing we have picked before, which goes in line with the requirement that Georgia just mentioned. We'll focus for sure on permissionless, given that we want an open network where everyone can participate.
03:37:57.026 - 03:38:45.502, Speaker A: And I'm going to skip a couple, but mostly focus on graceful recovery, real resilience, and basically with the ability to upgrade and incorporate future improvements in cryptography, because we know this is a field that's changing constantly, and we want to be able to leverage new improvements in cryptography so that we can speed up the network. Yeah. And so these are just kind of things that we keep in mind as we walk through all of the designs that we've had today. These are the things that we prioritize. And if your proposal didn't match any of these requirements, it probably isn't up here today. We ultimately saw four categories of submissions. You can see there's an X axis, which is you can have a cooperative or competitive network.
03:38:45.502 - 03:39:28.574, Speaker A: Cooperative is leftover rule. And then we're talking about where you divvy up the work among nodes. Competitive nodes are either working against each other in auctions and then once you choose this, actually, maybe a first question is if you want to enshrine it or not enshrine it, which goes back to the and yeah, so we received eight plus submissions, potentially. We got a few shout out to Ellie and the Espresso Systems team for their submission. And then we actually got one about Staking reputation and proof races from a few people at the Ethereum Foundation. If you're in this room and you contributed, I appreciate you. And then, yeah, we'll talk about some of these today and their tradeoffs and where we landed on these design spectrums.
03:39:28.574 - 03:40:00.086, Speaker A: But all of these are published proposals at Forum Aztec Network. You can read nine different ideas for how you can decentralize proving networks. As far as I know, it's one of the biggest bodies of literature on the topic. So, Joe, we'll kick it to you to talk about cooperative designs. Yeah, so maybe to start, so an antec block has a two by two binary tree structure. So I'll go through that first and I'll talk about how a cooperative network works. So you can see down here the base level is transactions.
03:40:00.086 - 03:41:03.338, Speaker A: And we kind of have what we call base roll up circuit, which takes two of those transactions and puts them into a mini roll up. And then we kind of aggravate up the tree till we get to one proof. And the tree is designed like that just so you can have mass powerization across lots of different machines and kind of really speed up the time it takes to generate these proofs. And so the cooperative design was kind of like looking to take advantage of that and say, okay, how can we look at a staking set? And how can we divvy up this work based on kind of a VRF similar to what the scroll guys were saying and assign parts of this tree to individual provers so everyone could participate in block production? And so we had about three designs in this category. I think the one that was written by me was a Staking proving network. It basically took the opposite end of the VRF to sequencer such an algorithm. So sequencer is high VRF and the bottom end VRF outputs provers.
03:41:03.338 - 03:41:25.480, Speaker A: And then you kind of go through this and you just assign parts of the tree to them with some redundancy to produce a block. And so based on that, we were able to produce a block in a cooperative fashion, divvy up rewards based on that. It was quite an elegant design, but I think we get onto one of some of the races afterwards, maybe. Yeah, great.
03:41:26.890 - 03:41:27.302, Speaker B: Nice.
03:41:27.356 - 03:42:11.738, Speaker A: And so, yeah, this is one category of ways that you can decentralize your proving network. You can build kind of a BFT style network where you can have M layers of redundancy giving you 33 or majority assumptions. And then competitive designs, we'll get to Paula. So competitive designs is clearly the opposite. Basically, we want to set up some sort of auction where the different rulers will compete against each other for the rights to prove that block and basically get the reward back from that. We do not plan to go with something like proof of work here or proof racing. In the default case, we agree with Pool and the Astronom scroll that that's mostly wasted effort.
03:42:11.738 - 03:43:25.034, Speaker A: That said, competitive designs work, okay, work either by themselves or also a very useful fallback mechanism if something is going south, like if for some period of time we don't see yeah, I will definitely bring an article. So for some period of time we don't see any blocks we can fall back to a competitive design event, something like a base design, where basically it's free for all and anyone can submit a block. The two main differences that we have explored across competitive designs were regarding whether we enshrined the auction or not. So basically we have one option where we enshrine the auction, basically the category, the conditions in which the provers are going to submit their bids and how they are selected based on that versus it's completely out of protocol. The Frenet sequencing protocol elects a single leader per block. So in an enshrined mechanism, what we have is the sequencer being completely free to choose whichever approver they want to go with. And we expect in a competitive design just for provers to submit bid of chain of protocol.
03:43:25.034 - 03:43:54.614, Speaker A: Sorry, so that sequence are having complete freedom to decide which one they want to go with? Yeah. And on that note, you can see in this diagram we have like an enshrined version of an auction. You have the l two nodes. They're submitting bids which could be positive or negative based on how much they expect their block rewards to be. Could be inflationary or deflationary. And at least in this design, it's all you can see on l one. But what does the future look like for abstec? Paul, you want to take this one? Sure.
03:43:54.614 - 03:44:50.418, Speaker A: I think we can talk a bit about pros and cons of competitive. Competitive has the advantage, and I hate myself for saying this because I submitted a cooperative one and the less than me loves the cooperative one. But how basically competitive pushes participants into improving the services they are providing. And right now, with so much room for improvement that we have for getting better proving times, proving readability, lowering costs, it makes sense to push for this competition. Also, having a competitive design, in particular an unenshined one, allows us not to set the parameters by which proverb needs to be selected. What I mean by this, if we enshrine an auction in a competitive design, we run the risk of overlooking certain parameters. For instance, if it's an auction just based on cost, we could run to proverbs sacrificing speed or sacrificing hologram reliability.
03:44:50.418 - 03:45:55.242, Speaker A: We could hurt the chain in certain situations. So for now, what we are considering is leaving the criteria for this competitive auction out of protocol so we can integrate it faster. So the idea is not having this role of proverb as enshrined within the Aztec network, but rather having them participate and approach sequencers for the other way around with their bids for creating each of the blocks. Do you want to introduce cyber? Yeah. So this is one of the designs that was submitted by myself, actually, that facilitates an out of protocol RFQ or option, kind of depends what standards emerge and what economic guarantees a sequencer actually wants. But you can see on the far left you get a proposer with couple BRFS that's the Frenet sequencer selection algorithm. At some point you have a Kruger commitment phase which requires the sequencer to choose an ethereum address.
03:45:55.242 - 03:46:37.290, Speaker A: And it also requires that ethereum address to put up some stake that could be slashed. And you can use that acknowledgement of them putting up stake as a form of what I would call a minimum viable Payload Timeliness Committee. Payload Timeliness Committees are something that's part of the ethereum PBS research, and Mike Newter has a few papers on it. But it's a way that we can actually prove that this proving network, or this individual actually believes that they have all of the data required to actually prove this block. If they do not have all the data required to prove this block, they are unlikely to put up stake that will be slashed if it doesn't get proven. And so in that way, we can actually not build this massive, massive consensus network that's attesting to data availability. We can just use a very simple commitment slashing scheme.
03:46:37.290 - 03:47:20.070, Speaker A: And so we do imagine that this is facilitated via an out of protocol version. Kind of like Mazboost, maybe you call it proverboost. I'm open to really good suggestions for names if anyone has one. But we do see this to be kind of a nice new open source software that help have a relay ecosystem, et cetera, emerge. Kind of like mev. I was going to say the reason we need this is because if you don't have a system where you can just keep running ahead on the L2, and I guarantee that the L One validating bridge will eventually catch up, you just end up in a world where you can kind of have huge mev attacks. So a block on Aztec has a finite proving window.
03:47:20.070 - 03:48:26.686, Speaker A: In that time, it must be proven on L One. So this kind of prove a commitment and prove a deposit is that kind of bond that says the proving entity that's picked it could be nailed, it could be a third party marketplace is confident that in that window they can actually improve the block. And it's kind of key to making sure the network has liveness. I think that the elephant in the room regarding going with a competitive design is monopoly, the risk of monopoly, especially if we enshrine the selection. We have some proposals that enshrine the auction process, which by relying heavily on cyber resistant mechanisms, modify the rewards to make sure that different participants were selected even within a competitive section. Still, we prefer to keep that out of protocol so we could iterate and simplify. And the reason for that is that though we think that there will be forces for centralization, the barrier to entry should not be that hard in the sense that it should be feasible to spin up the necessary hardware in order to participate in a proving market.
03:48:26.686 - 03:49:34.242, Speaker A: So that should at least mitigate these risks. We're also relatively fine if we have not a massive amount of different approvers. And we also see a situation, as the scroll guys mentioned, where we start to see vertical integrated sequencers approvers, basically sequencers that are actually backed by a builder and a builder also taking care of the proven so that could naturally generate some diversity among the proven sets. And it also allows us to slightly increase the requirements for the provers. If we go with a fully comparative design, we want people to be able to participate, which means we need to place relatively low hardware requirements on provers, which is what led to the binary roller topology that John just presented. But if we go with a competitive design, maybe we can crank things up a bit. We can go with the higher RT tree and we can actually start compressing and flattening the tree, which even though it lowers paralyzation, it also lowers total latency and reduces total coordination between the nodes, which to the end user means less times, less cost, which is probably what the end user is about.
03:49:34.242 - 03:50:09.710, Speaker A: And they don't really care that much about their request being served by five provers or 500. This is also competitive purely on price. It's not an latency game. Like if you make this fastest proverb wins, you get in a whole world of pain where you have very large hardware and people are competing to kind of win. That right, based on pure latency. In this case, its cheapest block wins. And because the network is specifying the time limit under which that must be proven, you actually Settis proven costs to go to zero, which is kind of what we all want because it should be the smallest part of that transaction.
03:50:09.710 - 03:51:12.190, Speaker A: Yeah. And so you can see here, I'd say we may actually be taking an opposite approach to roll and scroll in the sense that we really prioritize fully enshrining the decentralization of our sequencers through the Frenet random leader election algorithm. And at this point it's totally up to the sequencers to choose what makes the most sense for them. For certain sequencers, let's say your coinbase cloud your figment, you have a significant amount of hardware, it does not make sense economically or even for the protocol for you to force that being outsourced and share all of those transactions with N numbers of provers. In a worst case scenario, you could have a proposer who's building a block that's entirely from a private transaction mem pool. And if that is figment or coinbase or someone who has enough access to compute, there's no reason that they should be forced to share all of those private transactions and private transaction proofs with everyone before the proof actually gets generated. And so this is nice where it doesn't enforce high networking or high data requirements on, let's say, thousands and thousands of machines at most or at minimum you have two, you have whoever's doing the proving.
03:51:12.190 - 03:51:50.174, Speaker A: You could actually just have one but you could just have whoever's doing the proposing and then they can be a vertically integrated prover as well. And so it's really nicely giving you that flexibility and not enforcing the whole network to kind of share these networking costs which in our case could be very huge. You could have blocks of hundreds or thousands of private transactions that could be 32 KB each. And so it also nicely addresses the proof withholding problem that non privacy networks don't have. So yeah, we actually are just going to open this up. I'm going to ask my colleagues a few quick questions here. And so we've kind of mapped out all this space.
03:51:50.174 - 03:52:55.940, Speaker A: What are kind of the biggest challenges when deciding on an architecture? Maybe we ask Paula first from an engineering perspective, what is the biggest motivating deciding factor? If you're watching this or listening to this and you're like I want to decentralize my provers, which of these options should I pick? The first deciding factor is how easy this code basically from an engineering perspective, as much as I would love to be coding this for the next 510 years have all the time in the world doesn't fly. And besides, I want to get something out there as fast as possible to start actually battle testing. It effort of implementation is one of the main drivers both in terms of time and simplicity. I firmly believe that design, the more simple it is, the easier to understand and the more secure and reliable it's going to be. And also another important point is the ability to iterate it. We want to make sure that if we enshrine something and enshrining them it's maybe a bit difficult. So we want to start slow keeping things out of the protocol if they don't really need to be shout out to the minimum viable enshrinement by vitalik going a couple of months ago.
03:52:55.940 - 03:53:57.010, Speaker A: So we can iterate faster of protocol and only get it inside the of start the protocol when we're sure about it and when it actually makes sense. Yeah, you can always add more formalization later. It's actually really nasty to remove formalization from some of these things. Joe from like a business founder project based consideration, what was the biggest challenge on deciding? Yeah, I think some of the points that the scroll team made about kind of people wanting to own the network is one that resonated when we're thinking about these cooperative networks. If you kind of don't have a decentralized sequencer and you don't have a decentralized brewing network, it's very hard to kind of let people feel like they're a part of the network. So we were kind of considering that making sure that we could grow to a large enough sequencer set when we were making these trade offs. I think also just making sure that the core values of the network came through these designs and ultimately we picked a design here that really favored censorship, resistance and liveness of blocks.
03:53:57.010 - 03:55:00.298, Speaker A: We want there to always be an Aztec block and not have delays. So the design, it really favors those properties. In fact, there's even a backup mode to make sure we always get a block to enforce that. What are the tradeoffs here between you talked about enshrining and iteration, but maybe what are the tradeoffs between cooperative and competitive designs? I think that what Joe mentioned just mentioned regarding involvement of the community and how diverse you ensure that your set of participants is, and that relates both to community engagement, social consensus, as what scroll guys were talking about, social value, and also addresses and social resistance, which is also an important point. I think those are some important points to consider. And also latency and tax requirements. If you have maybe high enough transaction volume so that you can amortize total proving cost, then you could sacrifice a little bit the efficiency of a competitive design.
03:55:00.298 - 03:55:49.322, Speaker A: And also if you have other forms of finality that don't depend on a proof being submitted to all one, you could also delay the time of the proof, not indefinitely, but long enough so that improving latency is actually not a requirement. Yeah, that makes sense. Like the batching and chunking that scroll was talking about earlier. Joe, we've been working on this for a while. I saw a talk from Zach in 2019 talking about this two x two roll up structure. What do you wish existed back then? Or what resources do you think that there'll be for decentralized improving in the next twelve months or so? Maybe I'll answer in a different way. I think the knowing benchmarks about your proofs ahead of time is very helpful when designing this, which is hard when you're kind of building the train tracks as you go.
03:55:49.322 - 03:56:39.018, Speaker A: So if you don't know how long a proof is going to take, it makes it very hard to design a proving network. So we've had to iterate along the way based on getting that information. But in terms of resources, I think it's always useful to start with a framework of the design space because otherwise you kind of end up with lots of the same designs and you really want to explore the design space, not get four of the same designs. So knowing that kind of cooperative, competitive, enshrined and enshrined design space ahead of time lets you fully explore it rather than kind of just focusing on one segment that aligns with your own lefty interests. Yeah, I'm excited for free product idea for anyone hacking this weekend. If anyone wants to fork tindermint and implement something that distributes like proof work to proving marketplaces, that's probably a good product idea. But yeah, that is all from us.
03:56:39.018 - 03:57:47.114, Speaker A: I guess we will do questions from the audience for five to ten minutes, but if you want to join the Aztec community discord really good documentation that outlines the entirety of our private execution environment and our Dkdk roll up, you can download the sandbox and start writing Aztec noir contracts. One of the first times ever in the world you can write public and private composable smart contracts and follow us on X Twitter. I don't know what we're saying these days, but follow us on X. Thank you. It I'm just curious on more Holistic view, it would be interesting to know who's the client of the sequencer is. It the user straight and then the sequencer, how long does he have to actually sequence the transaction before these are before he has to choose the best bit? Basically, yeah. Maybe we can help for network.
03:57:47.114 - 03:58:50.414, Speaker A: Okay, sure. We have an initial phase, we call it the proposal phase where sequencers with high VRF rankings submit a hash of the block that they want to propose. Out of that set, the one with the highest VRF wins. And then we go into what we still need to exactly figure out, which is a mix between prover commitment deposit and reveal where we expect the sequencer to once the block has been committed to pick, approver, have the prover place a bond on L One to say, hey, commit to prove this. And also the sequencer to eventually reveal the contents of that block to L One. So other clients can we ensure that other clients have access to it and can have some form of pre confirmation assuming that they have individual blocks to verify. So yeah, basically sequencers the first thing they do in this block building phase is commit to a particular ordering of transactions and so the proverb has no liberty in adding anything to the sequence transactions.
03:58:50.414 - 03:59:30.338, Speaker A: Right. I guess one thing to add as well different to kind of the scroll guys, all of this happens on L One. So there's no l two consensus. The whole out very base. Yeah, it's based with a few rules to make sure things happen correctly. But it's the only way we can get same censorship resistance properties as a theory. So many large roll up currently they have centralized sequencers and they defend them by saying that, well, you can always have the forced inclusion on the layer one to avoid all these problems.
03:59:30.338 - 04:00:13.070, Speaker A: So from your perspective we want to defeat that argument. I want to see what is the most strongest argument you would have or maybe arguments that you will have. Ultimately they want to run an ad tech sequencer. I would say that right now a lot of forced upgrades actually aren't enforceable giving all talk at all two days about this, but taking a popular network that doesn't have fraud proofs yet, they can actually just ignore any transactions in the forced delayed inbox. So you actually don't get censorship resistance properties by having a forced exit feature by itself. You also need fraud proofs and optimistic cases or valid zero knowledge proofs. And you also need non quick upgradability.
04:00:13.070 - 04:01:13.650, Speaker A: You can still get censored in L2 networks that have instant upgradability because if they see a transaction they don't like in the delayed message inbox, they can just update the implementation of the network and ignore it. And so I would say that you are not getting very good censorship resistant guarantees in almost any L2 right now, besides maybe like Fuel V One, among others. And so I would look more critically about what you want censorship from. And if it is possible that your applications that need to be censorship resistant can wait 24 hours every time that they want to get a transaction in and then wait seven more days to submit a fraud proof and guarantee that their transaction was actually forced included in this network. And so for most use cases, waiting 24 plus seven days 24 hours plus seven days to get a single transaction in is something that would fundamentally not be viable for a majority of use cases. It would work if you need to get your money out, but then you're just never using that network again. Force inclusion is a second order citizen.
04:01:13.650 - 04:01:40.700, Speaker A: It can be a first order citizen if you have a decentralized network that enforces it. Well, not on L two right now. Yes, not right now. Any other questions? All good. All right, so I guess that's it for today. Thanks again for the aspect team. What a great talk.
04:01:40.700 - 04:02:52.020, Speaker A: I guess thank you very much for the coordination problems and I guess I also want to thank everyone for coming in today, sticking around until like 10:00 p.m. For this research talks that great speakers have given out so far. So again, I really appreciate that people came over and participated. And also thanks again to a lot of the crews that supported and built this event to this day. Now, we do have three more workshops coming up, so if you guys anyone who well, not today, obviously, but for tomorrow and the day after oh, sorry, no, for Tuesday and Thursday and Friday and just to kind of show that a little bit. So for tomorrow it's going to be PBS MAV ORderflow type of stuff. So maybe a little bit related to crew coordination issues around MAV, but it's going to be a lot more PBS type of things.
04:02:52.020 - 04:03:34.660, Speaker A: On Thursday we'll have a bit more heavy focus on privacy related stuff and we'll have a lot of people working on privacy space also come and join on the conversation. And then on the last day, Friday, we'll have data storage, which I think is a very underrated topics, frankly very important on the blockchain space. So, yeah, please join, check out the Fembushi, Twitter and we'll have all the links there, so you can just find the links and join there as well. Again, thank you very much for coming and that's it for today. Oh, I guess one last thing, no party, just research. Thank you very much.
