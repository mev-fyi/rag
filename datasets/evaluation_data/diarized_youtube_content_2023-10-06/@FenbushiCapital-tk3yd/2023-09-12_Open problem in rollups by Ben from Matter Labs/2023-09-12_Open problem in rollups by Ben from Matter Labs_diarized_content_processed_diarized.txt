00:00:00.410 - 00:00:32.166, Speaker A: You, Ben, from MetaLabs to share more about roll up problems and just a little bit about MetaLabs. I'm sure a lot of people are familiar with the team behind Zksync. So they built the Zksync, the biggest Zkvm L two that we have in the space and been do a lot of research and development. Ben, you can take it over the.
00:00:32.188 - 00:01:05.634, Speaker B: Roll up space and as we're thinking about where we go forward in the next year or so. So, next slide, please. Just a few words about myself. In addition to being the VP of Research at MetaLabs and running the research team there, I'm also a professor at Imperial College London. Just a little bit about my research background. I come from a background in programming languages as well as system security. So some of the stuff that was covered by the previous speaker naked from ADSTEC Labs is actually near and dear to my heart.
00:01:05.634 - 00:01:26.760, Speaker B: But the last four or five years, I've been focusing squarely on blockchain related topics. So things related to DeFi, fee mechanisms, audit. So we have a few papers coming out in the next several months alone on these kinds of topics. But the slides disappeared. Looks like the curse continues. Okay, there it is. The slides are back.
00:01:26.760 - 00:02:06.070, Speaker B: Okay, next slide, please. And so just the TLDR for this presentation. So I think we're still quite early in the design journey when it comes to roll ups. I think there are still more design questions than there are answers. And I'll talk about some of the answers that a variety of people in this space are proposing. And I'll kind of pose probably more questions than I'll give you answers for the next 20 minutes or so. So my sense is that we as a community have not arrived at too many points of consensus when it comes to even the simpler aspects of roll up design.
00:02:06.070 - 00:03:02.600, Speaker B: Next slide, please. So I think it's no secret there is a significant consolidation of strength behind Ethereum as a layer one platform. And that's due to a variety of reasons. One of them is the fact that Ethereum has a quite an open design and development process. I think there is just a lot of ingenuity that goes into whole variety. And I'll mention some of these examples around, I don't know, account obstruction, 50, 59, some of the things that are coming out with 48, 44, a variety of ERTs that really have a lot of mind share and a lot of people sort of putting their sort of thoughts as well as their implementation efforts into that. And part of the thing, of course, is that given the extent of their market, oftentimes this is what happens in their markets as well.
00:03:02.600 - 00:04:13.562, Speaker B: So as part of that, EVM compatibility has become virtually the norm around, I'd say, the majority of the blockchain space, despite any number of shortcomings when it comes to the EVM. So Solidity is a language as well as the EVM as a virtual machine, far from perfect. And in fact, many people have complained about these kinds of things vocally and there are definite ways to improve upon the EVM. But at the same time there's a generation of developers who've been traded on solidity and as such, you basically have a supply of people who want to build a VM platform if you are EVM compatible. And so this is where we are with Cksync and part of it is because that's where developers are as well. So when it comes to scaling up execution, when it comes to making it more affordable as well, ethereum's plans are actually quite explicit. Their plan is to basically rely on roll ups as a scale up facility, as a way to basically outsource execution, to be largely off chain with some elements of it that are on chain still.
00:04:13.562 - 00:04:46.086, Speaker B: And that's the roll up story in a nutshell. And that's why we're here, that's why we are focusing on this. Next slide, please. So now this is probably going to be, I'm sure pretty much everybody in this audience has seen this somewhere. So people distinguish between optimistic broadproof based roll ups and zero knowledge roll ups. We are seeing on the validity proof side, on the zero knowledge side of this divide, so to speak. And there are probably more companies essentially trying to bring solutions to that side of this space as well.
00:04:46.086 - 00:05:38.946, Speaker B: And on top of these sort of end to end platforms, if you will, there is any number of companies trying to add solutions and kind of playing up to the modularity thesis. So for example, there are DA solutions, data availability solutions like Celestia and Eigen layer DA. And so I'll mention those in passing in subsequent slides. Next slide please. So here's a few interesting data points and this is something I got from L to Bit site that basically keeps track of roll ups progress in the past several days. This is activity or growth, I should say, in the last year or so, and a few things to point out. So the activity as measured by the number of settled transactions, that's what you see in the top chart.
00:05:38.946 - 00:06:52.782, Speaker B: And there's a scaling factor which is like how much are roll ups settling? More than the main net in aggregate, right? All the roll ups that they consider, what's the scaling factor like and seems like there's a chance that that number will keep growing. That is to say, the amount of activity that's settled on the collection of roll ups we have will keep growing compared to where a main net is. So some transaction will probably still be settled in mainnet, but a variety of things will actually go to roll ups. Here's another sort of set of data points at the bottom here, which is to say the total value locked, which is not a perfect measure, by the way, but it's an interesting one anyway to consider, keeps going up, right? So you can probably identify individual points like if you go back to March, given that a number of zke EVMs, including ours, became publicly available, you sort of see significant growth as a result of that. Next slide please. So I wanted to touch at least briefly on a variety of aspects of roll up design and this is what I have listed here on the slide. So I'll basically take these issues, these aspects in order and try to spend a bit of time on each of them in the next 1015 minutes, I would say.
00:06:52.782 - 00:07:38.522, Speaker B: So first decentralization. Next slide please. So at the moment the not so big secret is that just about everything about roll ups is mostly centralized. We have seen I mean, firstly, there is nothing wrong with the path of progressive decentralization. That is to say, you get things to work, you build up your user base, you build out your developer community, you make sure that things are working colorbust and so on and so forth. And then over time you basically decentralize various aspects of your system and this is a path that others have followed and I think the roll up space will basically follow the same mantra for the foreseeable future. At the same time though, we're sort of running things with some degree of efficiency.
00:07:38.522 - 00:08:47.938, Speaker B: One could argue things are a bit different though on the zero knowledge space where there is a lot of proof or work that needs to happen as opposed to the fraud proof space, the optimistic space where the amount of work is actually relatively less. But at the same time there are advantages of throughput and latency when it comes to centralized system deploying decentralized system by and large. And so how to attain these kinds of advantages in decentralized setting is actually relatively unclear. Right? And the other thing is that decentralization comes in different shapes. This is kind of a multidimensional kind of property ultimately. So we can talk about node ownership, we can talk about data center diversity, we can talk about GeoDistribution and geodiversity and then the question that begs itself is like well, how much decentralization is good enough? Next slide, please. So, if you stare at this chart which covers the last six months of Ethereum validator growth, you see that at the moment Ethereum has like a gigantic number, over 800,000, like close to 900,000, give or take, in terms of number of validators.
00:08:47.938 - 00:10:14.062, Speaker B: And this is like almost a twofold increase in a relatively short period of time as well. So, well, the question is what's the big enough number? Is that too many? Is that the right number? Should we ask for more? Should we ask for fewer? Is it like over provisioning things? And I think answers are kind of elusive and actually people have looked at the Ethereum validator ecosystem in quite a lot of detail, quite frankly. Right, and so they find some interesting sort of measurements. Like for example, this website Esunshine.com provides a variety of metrics and in fact they give you sort of a health score, so to speak. I think for that time it was like 58%, right? And so they basically rate the theorem ecosystem on a bunch of pails and so they look at things like consensus client diversity, right? So like other things other than guests that people are running and how often is that happening? Things related to, let's say geolocation diversity here on the right. Staking pool diversity, right? So basically it's like a single pool dominating the ecosystem, non pool based, validators, right? So like what is it? Solo staking, that sort of thing? I mean is that present, is that prevalent, that sort of thing? So you can even for something that we genuinely believe to be quite decentralized, you can sort of go and ask a whole bunch of interesting questions when it comes to decentralization.
00:10:14.062 - 00:10:45.050, Speaker B: And so next slide please. A question that I would ask about this. Next slide please. Yeah, thank you. Well, so what to do? Do we want to replicate the same thing for all ops? I mean is it sensible to expect every validator to basically become what, a sequencer or something like that? And that's to my mind not entirely clear at all. I think some aspects of this perhaps could be sensible. Like for example, maybe not everybody should be running the exact same piece of code.
00:10:45.050 - 00:11:50.958, Speaker B: So we are not basically just reducing our potential availability because we are sharing the same exact bugs everywhere, right? That's just an availability argument. But the idea that you need to have, I don't know, hundreds of thousands of nodes doing your sequencing I'm not sure that I'm sold on that at all. Let's say firstly secondly, even with that really significant number of nodes we are not quite where we are or where we would have been in the web two space. Like if you go back and think about the cloud for just a second, people talk about high availability systems. Metrics like five nines of availability for example come up quite often and there is very little of anything even in the ethereum ecosystem. But in blockchain in general that even starts to sort of resemble these numbers related to availability. And so one could pose a question like well, if you're thinking about sequencing designs, if you're comparing different designs to each other, should we basically be aiming at these kinds of high availability outcomes and what's the right way to get them? And maybe it's actually not trying to increase the number of nodes to gigantic amount.
00:11:50.958 - 00:12:37.914, Speaker B: Maybe there should be focus on perhaps something else entirely. And so here's another question that is often conflated but frankly I think is causing more confusion than it should. So does decentralization imply permissionless participation? Can you basically have a system that's relatively centralized but where things are permissioned in some ways that the community believes is appropriate. Right? Because if things are entirely permissionless, then we basically have the civil problem. You have potentially more dos related problems as well. And so is that what we want as well? Not entirely obvious, I would argue. At the bottom you see a reference to a paper that looks at measurements of bitcoin ethereum in a variety of dimensions of decentralization.
00:12:37.914 - 00:13:37.582, Speaker B: If you're interested in these topics and reading about these topics. Next slide please. So let's talk about sequences just a bit more so sequencers and this is where people spend quite a lot of time and sort of mental energy, if you will. I think the majority of work, not all of it, but the majority of work has been sort of lifted, if you will, from permissionless consensus literature. So basically people look at a bunch of variants of DfT based consensus and they basically reason about some of the latest approaches. There things like hot stuff, hot stuff, too fast, hot stuff and they look at essentially taking that and moving this into a POS setting and basically getting performance numbers that those kinds of algorithms genuinely give you, which is to say maybe latency of, let's say one to 3 seconds for most distributions. The throughput though might be a little bit more limited.
00:13:37.582 - 00:14:55.078, Speaker B: Some people have suggested DAC based solutions, but we have not seen those deployed or experimented with in practice. Let's say it's not obvious though that we should basically replicate a design that's designed that was there for state machine replication to the problem of sequencing. The problem of sequencing is its own specialized thing, right? I mean, nobody said that we should just basically take the latest and greatest in BST consensus and just wholesale move it to this space. I think that's not entirely obvious at all. Another thing that's also not obvious to me as well is what's the right way to connect, if you will, or to align sequencing and data availability provisioning, right? So there's one argument for modularity where these things are provided by different vendors and so on and so forth, but the challenge is that modularity is ultimately somewhat more difficult to make more reliable, right? If you worry about five nines of availability, like how do you actually take a modular design and make it work with five nines? I would say that that's a highly non obvious kind of question to answer. Next slide please. Another sort of thing is that comes up a lot is sequencers and med and sequences as mev extractors.
00:14:55.078 - 00:16:22.274, Speaker B: And so basically there are sort of two extreme points of view that are often taken here. So one is that prevent, like we should just stop or try to stop NAV at all costs and there's a lot of work that's focused on various committee schemes, private mempools, basically blinding transactions, things of that sort, where the sequencer is not able to really interpret what is being sequenced, right? And so such it's got a hard time basically profitably doing MEV extraction, right? As well as fitness focused consensus algorithms, things like Temis from Tech and a variety of others. But at the same time, there is a lot of doubt in terms of like, well, can we really suppress all dangers or all perditious forms of mev? And should we even try? Right? And then you sort of go to the other side of this spectrum that we will oftentimes right, so, well, mev should be embraced so mev can play a positive role and it should just encourage node runners. And this is like, in some sense, the better way to compensate sequences than it is to pay them money or to pay them sequencer fees, right? And so we should just basically take the latest and greatest and PBS and EPBs from Mainnet, from Ethereum's Mainnet and just basically repeat the whole thing and then basically have flashbacks as part of the picture as well. So my sense is that these are fairly extreme points of view. I'm not entirely sure what's in the middle. I wonder if there's something that's a little bit less extreme.
00:16:22.274 - 00:17:12.022, Speaker B: Let's say that is a middle ground here. And I think this question is especially relevant when you're thinking about not only L two S but L three S hyperchains that sit on top of L two S. That's what we call them hyperchains for, for example, enterprise settings, right? There's a company and the company doesn't really want its mev to be extracted by somebody else. I think that's just inappropriate for the company setting, right? Like, how do we stop that without becoming too rigid and sort of too crazy about it, if you will. Next slide, please. I think the part that's actually receiving perhaps a little bit less attention than Fuk, although it's certainly getting some these days, is Provers, right? And basically there is ongoing and frankly non stop progress in proof systems. I think the progress is not going to be like reaching a fixed point, reaching its limit for the foreseeable future.
00:17:12.022 - 00:18:03.862, Speaker B: I think we can expect to see progress on the proof system side. We recently released Boojum as our proof system, but I think, again, that's not the end of that story. And people will experiment with any number of things related to folding. There are exciting updates on Paralyzing provers. There was a paper that was accepted at Oakland Security and Privacy from Berkeley, from Don Song's Group on some of these things, showing really exciting scale ups from proof of parallelization. So I think all of these things will be the subject of a lot of focus for the next months and years, which is to say speed of proof production still single core, reducing hardware requirements and then figuring out how to ultimately increase parallelism as well as to connect custom hardware to the task of proof generation, proof production. And I think we actually don't know what the ultimate sort of steady state will look like at this point.
00:18:03.862 - 00:18:41.780, Speaker B: We are experimenting and there's no shortage of interest from hardware makers to basically make this process a great deal more efficient as well. So, next slide, please. And that's kind of what I already said. Right, so lots of players interested in producing new proof systems. We just saw updates from Fault Basics on Jolt and Lasso, as well as some interesting updates from people trying to do aggressive folding. And on the right hand side you see some names of companies that are trying to basically build advanced hardware for this. And next slide, please.
00:18:41.780 - 00:19:25.058, Speaker B: Yeah, so in a sense, Prover decentralization and opening the Prover game up for others I think is super interesting. The moment it sort of becomes even mildly available, I think there'll be a great deal of interest. And we already are speaking to people who are keen to get into this space. But yeah, should it be permissionless? How diverse should the hardware set up be? And what happens if provers decide to do something that is slightly unpredictable, perhaps malicious and so on and so forth. And this is where you basically have next slide, please. Any number of open issues. Next slide, please.
00:19:25.058 - 00:20:12.938, Speaker B: In terms of how to before that, please. Okay, yeah, that's it. How to basically make it so that there's no, like, we don't end up with a winner take all possibility where somebody with a really advanced hardware set up that they don't share with anybody else just basically wins every block. Right? That's one open question to my mind. Also another question is how to have variable pricing if I'm willing to pay more to have my proofs achieve finality faster, can I do that? And so what are the means by which I can achieve that? It's actually not entirely clear how to sort of create an alignment between incentives and prove our work. Next slide, please. And so on to economics.
00:20:12.938 - 00:20:50.218, Speaker B: Let me just say that we've seen a number of tokenomic formulas that have emerged over the last several years and some of them have come from DeFi, let's say. So stake based governance, stake based protocol, participation, things of that sort. I think it's tempting to just lift them and move them into the space. I'm not sure that this is a great match, quite frankly. And even then we have seen any number of problems with take based governance when it comes to DeFi protocols as well. I don't think it's necessarily perfect system. We've seen any number of governance activities that have not been quite as trustworthy as perhaps they should have been, that sort of thing.
00:20:50.218 - 00:22:03.858, Speaker B: And so maybe we should take a different approach here. Maybe we should sort of stop focusing on POS and staking and take more of a mining based approach where participation improving is actually more of a focus area, right? And we should actually reward provers a bit more than we reward people holding stake in the protocol. Just a thought. I don't think that we really have consensus, no pun intended on how this should be done, but next slide please. I think it's fair to ask what kind of outcomes do we want? And I think one of the outcomes we do want in roll ups is we want to have speed, we want to have low latency when it comes to both soft finality, which is genuinely what sequences give you as well as finality that you get from the proverb as well as high throughput and then other properties like sensory resistance and maybe reduction in mev but not clear to what degree as well as high network availability and how to get these things from tokenomics I think is actually a tricky question. I'm not sure that there are immediate answers that sort of jump to my mind at least. Another thing is that if we are sort of perhaps in the race to the bottom I'm reluctant to use that term, but I think you know what I mean.
00:22:03.858 - 00:23:08.694, Speaker B: We are in the process of reducing fees for the end user, right? We are in the process of making the blockchain more affordable. Well, how do we keep node runners interested? Like how do we keep validators interested? And if they're in the process of doing highly specialized proving, how do we essentially keep them interested given that there is probably some capital expenditure that needs to go into that as well. Next slide please. Just wanted to really briefly touch on the topic that was actually covered in the previous presentation, which is the thing is that roll ups is not just math, right? I mean, roll ups at the moment are fairly complex distributed systems and there are many ways in which things can go wrong and we need know reason about least availability, implementation bugs, which is what Kenzie talked about, as well as infrastructure vulnerabilities and there's a variety of interesting projects that kind of focus on sort of all of these aspects of things. So bugs, next slide please. Bugs improve production is something that's been the focus of a bunch of projects in the last three to six months. I think this will just keep expanding.
00:23:08.694 - 00:24:23.518, Speaker B: Some interesting references for you at the bottom. Next slide please. So one question that doesn't maybe get addressed quite as much is that how do we bring the right software engineering discipline? I mean, circuit generation is a pretty difficult activity, let's say, and this is where a bugs can be found. But at the same time we want to be in an environment where developers are relatively free to basically change things with quite a lot of code velocity. And so how do we basically marry this robustness with the fact that software engineering needs to happen fairly quickly? Is quite an interesting open question and then next slide please. The last point to make here is that we see a lot of efforts on the main net, on Ethereum's main net things like Cape from Espresso Systems Railgon as well as a bunch of other projects as well as proposals when it comes to stealth addresses and things of that sort, as well as more aggressive proposals from, for example, Ad Stack three on hybrid privacy where execution can be private as well. And so the question is, well, should any of that be provided out of the box by the L2 or should the L2 just act as an exploration facility? And I think there is something there that's not entirely obvious as well.
00:24:23.518 - 00:24:38.434, Speaker B: I'm not sure that anybody but Aztec has taken a strong position when it comes to these things. Next slide, please. And in fact, last slide. So this is what I just covered fairly briefly, given the time constraints. Happy to take whatever questions you might.
00:24:38.552 - 00:25:08.350, Speaker A: You thank you, Ben. So I guess one thing I kind of also wanted to dive a little bit more into was the prover decentralization aspect. And you kind of inferred a little bit on the proof of work aspect of prover decentralization. And again, sort of like bringing back the POW era. So in terms of proverbial decentralization.
00:25:10.450 - 00:25:10.814, Speaker B: So.
00:25:10.852 - 00:25:30.550, Speaker A: Far, from what I see, we're trying to decentralize it for better liveness assumptions. But are there other properties that you are specifically looking for from those proverb decentralization work to write, let's say, meet certain criteria that you want for those decentralized proverb network?
00:25:31.930 - 00:26:16.660, Speaker B: Yeah, I think it's not just liveness. I think ultimately, frankly, we approach it from the economic standpoint. It's ultimately the cost of improved production, right? Like, how much does integrity cost? Where are we as a system, as a roll up compared to just draw execution on Ethereum, right? And essentially, how do we reduce it while having sufficient levels of decentralization? And that's the question that I posed early on, which is like, well, how many is enough? Like, if we have basically five provers, well, is that good enough? And in sometimes it's a hard question to answer. I'd rather not answer it at the moment. I think there's quite a lot of subtlety here. So I'd say that it's the latency as well as ultimate cost of proof production that we are after in the system in a steady state.
00:26:17.830 - 00:27:01.150, Speaker A: Got you. And also regarding the, I guess, the sequencer side of things, I think you made a very interesting point about how it's not obvious to just bring in the existing BFT consensus and kind of just slap it onto whatever you call it decentralized sequencers. But then, in that case, how do you foresee, sort of in a near future the way that sequencer would exist for, let's say, Zksync, especially now that maybe it's not that obvious that we should rely on some form of POS? What do you foresee as a way to sort of governance around the sequences.
00:27:01.230 - 00:28:09.750, Speaker B: For those big L, two S. Look, it's the same path of progressive decentralization, right? And the problem is that the path is not entirely linear. So my suspicion is that it's quite clear that decentralization is now future. Right? And how exactly what are the steps? I think that's the part that's a little bit more mysterious at the moment. I think we'll probably go with something that's perhaps following some of these designs but I think we are also working on something that's a lot more adventurous, let's say. And it's a better sort of match in terms of the nature of sequencing as well as nature of what we are providing there and also with an eye towards like well, how many of these things do you need and are you ultimately after high availability or do you need to basically have replication? Right? If you basically look at deployment in some of these systems, be that tendermint or two is contentless, let's say people look at 100 nodes but again the same question sort of comes out which is like well, what's special about 100 nodes? And again, maybe not too much is special about 100 nodes.
00:28:10.670 - 00:28:54.150, Speaker A: Got you. And also given that around, especially around big roll ups, there are a lot of projects that are recently trying to, I guess, push this modularity thesis and people are in some way maybe taking parts of the roll up stack and trying to build something very specific about it. From the perspective of you building ZK Singh, do you have any takes on those aspects of modularities that is brought in by individual projects and how you see whether it's a good aspect of it or not so good aspect of it from your perspective.
00:28:55.050 - 00:29:56.290, Speaker B: I think modularity is a great way to push a certain sort of angle, if you will. I mean, be that data availability of shared sequencing, we appreciate where those themes are coming from. I think it's exciting exploration. I think, again, piecing these things together in a way that gives you a reliable and robust system that kind of I keep mentioning five lines of availability, but you know what I mean, right? So something that really kind of just works day in and day out, right, is a challenge and that's true in a variety of ways, right? I mean, if you have basically, let's say, diverse, non overlapping, validator set that don't really connect to each other, everything is independent in a certain sense. Everything has to work for the system to work end to end, right? So that's the one aspect. The other one is that if you have multiple tokens at play then things are potentially greatly complicated by that as well. So my sense is that I really enjoy some of these explorations.
00:29:56.290 - 00:30:11.194, Speaker B: I think they are there for a very good reason. But at the same time, if our goal is to get to the end result faster, they can be somewhat challenging to build with. Let's say gotcha.
00:30:11.242 - 00:30:19.982, Speaker A: Gotcha. Okay. Thank you, Ben, for today's time and a great talk about all the roll up questions that you have.
00:30:20.036 - 00:30:20.842, Speaker B: My pleasure.
00:30:20.986 - 00:30:22.620, Speaker A: Yeah. Thank you very much.
