00:00:03.050 - 00:00:41.286, Speaker A: Togru from scroll team and to kind of just briefly go over. So scroll is a ZKE EVM that is also very much up and coming. And then Toguru has been one of the passionate advocates for scroll as well as passionate advocate for the definition of roll ups in general. And I'm not sure if you're going to talk about it today. I believe not so much, but he's been very active on that front. Togu, well, stage is yours.
00:00:41.318 - 00:00:57.270, Speaker B: Feel free to take it over. Hello. Yeah. Thank you. Let me just share the screen before hello.
00:00:58.360 - 00:00:58.992, Speaker A: Long time.
00:00:59.066 - 00:01:09.432, Speaker B: See. Hello. Yeah. Can you see my yes, yes. Perfect. Hello, everyone. My name is Togrel.
00:01:09.432 - 00:01:51.540, Speaker B: I do research at scroll. I mostly focus on the protocol side of things. So I work on the bridge design, protocol, decentralization, et cetera, et cetera. And today I'm going to be introducing and overviewing roll ups, provers and zero knowledge roll ups. So you probably have heard about roll ups, there's been a lot of talk about them in the last few years. And there are two types of roll ups, optimistic roll ups and zero knowledge roll ups. Or I'm kind of forced to say it because the stockware folk don't really like it when we refer to them as zero knowledge roll ups, validity roll ups.
00:01:51.540 - 00:02:32.336, Speaker B: So the main difference between the two is in how the validities of the protocol is being enforced. So optimistic roll ups utilize fraud proofs. So you have a sequencer. It proposes ordering an ordering, then the ordering is published on the base layer. Then the executor fetches the ordering data, it published the state route. And then if the state route is invalid, so the challenger fetches an ordering and the state route. And then in case the state route that was committed by the executor is invalid, it just challenges the state route.
00:02:32.336 - 00:03:42.488, Speaker B: And there are two ways to challenge the state route. One is an interactive single route challenge. So essentially, you just replay the entire transaction or a subsection of the transaction from the start to end on the base layer, which is not ideal if you are not using P, two P proofs because essentially you're limited in how much gas your transaction can consume. This was originally used by optimism, but then they realized at some point that actually that's not really viable for complex transactions. And so they switched it out for Canon, which is another type of fraud proof, which is called an interactive multiround fraud proof. In this case, instead of just the challenger re executing the transaction, the challenger and the assertor, which in most cases is the executor. But anybody can take the position of the executor, go back and forth, dissecting the state to the point where they get to just one opcode that they have to execute on chain.
00:03:42.488 - 00:04:47.040, Speaker B: This is not ideal from the perspective of efficiency because you have to go back and forth. So the time consumed in the worst case scenario with their censorship is longer than just re executing the entire transaction. But the benefit of it is it's much cheaper to execute on chain. And on top of that, it's also possible that your transaction cumulative gas consumption of your transaction can be drastically higher than the block gas limit of the L one, which means that you can essentially challenge anything. Trevorly on the L one and zero knowledge roll ups utilize surprise, surprise zero knowledge proofs. So you can see there's some proposed preliminary ordering. The ordering is published on the base layer, then the prover fetches the ordering data, it computes the proof and publishes the validity proof and voila, a certain batch is finalized.
00:04:47.040 - 00:05:23.336, Speaker B: So typically there are two protocol participants within an ezk roll up. There are possibilities that you can have one, but usually we just use two protocol participants. So one role is a sequencer and the other is approver. And sequencer are responsible for defining the preliminary ordering. So essentially they are in direct contact with the users. The users propagate their transactions to sequencer. Sequencer either provides some receipt that guarantees that they'll include that transaction or doesn't, depending on the implementation.
00:05:23.336 - 00:06:09.260, Speaker B: They combine those transactions in a batch and then publish the batch. And the provers are responsible for forcing the validity. And they do that through zero knowledge proofs or validity proof proofs. Because in reality, in this case, zero knowledge proofs. We use zero knowledge proofs for succinctness, not for the privacy aspects of zero knowledge proofs. And therefore we actually usually refer to them as validity proofs. And basically the validity proofs allow the L one to succinctly verify that a certain state was committed without redoing or reexecuting all the transactions.
00:06:09.260 - 00:07:19.708, Speaker B: And proving can be competitive or it can be collaborative. So what I mean by competitive proving, it essentially emulates noncommodic consensus. So you have multiple different entities that compete to have their block included in case of bitcoin. And in this case you have multiple entities competing to have their proof submitted and finalizing the block that way. There's a drastic disadvantage with this approach is in the context of provers, competitive proving is centralizing. And the reason for that is, unless you insert some form of randomness, unlike Nakamoto, the most efficient prover is always going to win. So, in Nakamoto, your probability of having your block selected for a given height is proportional to your hash rate percentage in the network, whereas in here your probability is essentially 100% if you have the most efficient proverb, unless there's some networking issues for you, et cetera, et cetera.
00:07:19.708 - 00:08:19.092, Speaker B: Which means the long term everybody else is disincentivized to compute proofs and you'll end up having one or two provers maximum competing with one another to have their proofs submitted, which is not very resilient. It doesn't really affect the security per se, but it's still not an ideal situation. Because let's say if that prover fails for one reason or another, you're stuck with a liveness failure and you need some fallback or whatever. And collaborative proving assigns approver per batch. So think of how gasper, in Ethereum works you have slots, and for each slot you deterministically select a leader through some form of election mechanism that involves randomness. And that leader is the sole entity responsible for proposing a block for that slot. And the same is true here.
00:08:19.092 - 00:09:22.324, Speaker B: So you elect approver and that prover is responsible for producing a validity proof for that batch. And in collaborative proving you need a fallback scheme. Because let's say if the primary prover fails for a given batch, then your protocol will just stall and therefore you have to have a fallback which elects a second prover or makes it permissionless. There are different ways that you can implement it and basically still retain your liveness guarantees. And proving can be sequential or parallel. The problem with sequential proving is that if the proving time is longer than the block time, your finality time increases linear. You should think of it this way say you have 100 blocks and the proving time is two minutes and the block time is 10 seconds.
00:09:22.324 - 00:10:47.504, Speaker B: You can see if you wait until the previous block is proved before you start proving the next block. You can see how as the number of blocks grows, your finality time grows superlinearly to the point where at some point you'll have finality guarantees that are slower than the finality guarantees of optimistic roll ups, rendering zero knowledge proof and zero knowledge roll ups pointless. And therefore it's not a good way of implementing things. It can only work if your proof time is shorter than the block time, which is unlikely, at least for the foreseeable future. And the lower bound on finality for sequential proving of N blocks is N times proof time, as I explained previously. And the lower bound finality for parallel proving of N blocks is proof time plus N minus one times block time. Which means that essentially we can decrease the proving the finality time drastically by assigning as many blocks to different provers in parallel as possible and then either verifying those proofs separately, which might be costly, or taking some additional time and aggregating those proofs and basically verifying one single proof at the end attesting to the validity of end blocks.
00:10:47.504 - 00:11:52.804, Speaker B: And the more blocks you add to this, the higher the throughput and the lower the cost, assuming no other bottlenecks. And you can think of parallelization in this case similar to how Celestia works with light clients. So in Celestia, the more light clients you have in the network sampling the data availability, the greater the block size can be. And the same is true here again, assuming no other bottlenecks, the more provers you have in the network that you can assign different batches to in parallel, the higher your throughput can be. And now an interesting question. How do we incentivize provers so we can just allocate a fixed percentage of the protocol revenue to approver of a specific batch and call it quits. But the problem is that doesn't really account for Mev and it's likely that mev is going to be a large percentage of the protocol revenue.
00:11:52.804 - 00:13:22.710, Speaker B: So protocol fees may be depending on how the specific roll up is implemented, you can have token emissions, et cetera, et cetera. But long term it's likely that mev is going to be the predominant source of profitability for the protocol. And therefore you end up with a system that is completely imbalanced in terms of incentives because the sequencers control all the ordering and therefore they get to order the transactions the way they want to maximize their extractable value potential and just keep a large chunk of that profit to themselves. Which means that if people have a choice later between becoming improver and a sequencer, they're likely to choose to become a sequencer because it's just more profitable. So what is the solution? Well, I refer to it as prover sequencer separation and it's based on Proposer Builder separation that is proposed for Ethereum. Ethereum already has Proposer Builder separation that is out of. So essentially, in case of Ethereum, you have specialized entities called builders which are good at maximizing building blocks that maximize the profitability of that block through extracting as much value as possible.
00:13:22.710 - 00:14:46.560, Speaker B: And then we use a first price auction for those builders to compete with one another to have their blocks selected by the proposer for a given slot and then added to the block. And a similar thing is true here. You have sequencers who specialize in maximizing extractable value, competing with one another to have their block selected for a given batch and basically share the revenue with the prover through the process of bidding. And PSS can be enshrined, which means that it's part of the protocol, similar to how Ethereum is planning to enshrine PBS into the protocol. Or it can be external. And there are downsides to both approaches and upsides to both approaches. So in case of enshrine, it more clearly defines the role within the protocol, meaning that it's easier for the protocol participants to know who's performing what role and how many sequencers you have at a given time, producing blocks, et cetera, et cetera.
00:14:46.560 - 00:16:05.492, Speaker B: But the downside is that it's relatively complex and it requires two separate roles. An external is simpler because you just have one role internally and then the builders, or in this case the sequencers, are external to the protocol. So from the perspective of a full node operator or a light node operator, the sequencers don't even exist, they're just somewhere there. So approver can just be a sequencer themselves, for example. But the downside of this approach is, as I mentioned previously in The Benefits of Entryment, is that you just don't know whether there are enough builders, et cetera, et cetera, participating in the network, which creates a situation where you can end up with, let's say one builder doing everything and it will be difficult to tell which can result in bad resilience for the protocol. And before I end, I'll shortly touch on a few other things that have emerged recently in this space. One is prover markets.
00:16:05.492 - 00:17:14.912, Speaker B: So you can think of prover markets as general purpose marketplaces for provers to purchase computational power to compute proofs, similar to how nice cash works in case of proof of work. So an example of a prover market would be Nil protocol. They're working on a system where you can essentially generalize approver for any protocol and just have one market where anyone can just purchase computational power from this pool of provers. And then there are also prover networks which are application specific marketplaces similar to how Lido works in case of ethereum. So you have a prover market that specializes only on one chain or a specific use case and it just provides the computational power for that specific use case. So thank you for your attention and feel free to ask any questions. All right.
00:17:14.966 - 00:17:49.308, Speaker A: Thank you, Taguru. I guess it had a lot of touch point with PBS with the PSS. And one thing I kind of also wanted to discuss briefly with you about was the fact that you did mention like the builders of the block could well, the sequencer of the block could also centralize in a similar way that the builder on L One can also centralize. So how do you foresee, I guess, the centralization factors around builder centralization or the sequence of centralization in this case.
00:17:49.474 - 00:18:52.876, Speaker B: For the L two S? So in the case where you have enshrinement, centralization is more difficult because you can, let's say, set a certain minimum amount of sequences that the protocol should have. So let's say 20 or 30 or whatever. With external it's a bit more difficult because the protocol is agnostic to how the blocks are produced. So there might be only one sequencer producing all the blocks in case of L two S. That's not as big of a problem as, let's say, in Ethereum because you still have inclusion lists through the L one. So the force inclusion, but that's not ideal because that will cause a lot of delay and it will still be costly. So in case you do external PSS, I think the best approach is to still have some form of inclusion list similar to how Ethereum is planning to have them and just enforce certain real time censorship guarantees.
00:18:52.876 - 00:19:08.420, Speaker B: Obviously, you can never guarantee 100% that a certain transaction will be included within a certain amount of time, but increase the resilience of real time censorship resistance through inclusion lists.
00:19:08.760 - 00:19:51.120, Speaker A: I see. I'm trying to see from the sequencerbuilder's perspective if it is possible to sort of replay the same sort of competitive dynamics that is happening within the Builder space in L One and then kind of have the same things happening L two. Meaning that if the sequencer would be possible to do things like private order flow type of things that the builders on L ones are already doing, or is that something that can be, I guess, controlled by the protocol to some extent?
00:19:54.360 - 00:20:53.460, Speaker B: I feel like depending on how you do it, you could obviously, let's say, require a sequencer or let's say, provers to commit to a certain set of transactions on the L one by posting a hash of their cumulative string or somewhat like that. But I feel like in general, unless you want to go that route, which I don't think would be very efficient, you would still have a possibility of given sequencers having private order flows. It's especially true with external sequencers because it's likely that you'll end up having five to ten builders max who control the majority of the blocks, which means that users are essentially incentivized to send the transactions directly with them versus the PTP network.
00:20:54.280 - 00:21:12.072, Speaker A: Got you. I see. And I guess in that case then yeah, you will probably see some sort of like a builder competitions and potentially some sort of centralizing effect to one builder controlling like 40 50% of the order flow that is going through the network.
00:21:12.216 - 00:22:19.810, Speaker B: In that case, yeah, you can potentially end up with a single builder controlling the large percentage of the order flow. But again, there are certain mitigations here. So let's say if you have inclusion lists, then there's not a lot a builder or a sequencer can do in this case because they're essentially forced to include certain transactions even if they don't want to, or they'll be just slashed. There are certain mitigating factors you can implement, but ideally, I would like to have a protocol set up that doesn't really need to do that. Because one of the goals with L two S is to minimize the overhead. Because if you're doing the same thing that an L one is doing, you're just adding so much overhead on top of what you need to do, and therefore, it just doesn't really make sense to emulate L ones completely.
00:22:21.220 - 00:22:23.270, Speaker A: And do you foresee that.
00:22:25.880 - 00:22:26.276, Speaker B: A lot.
00:22:26.298 - 00:22:45.080, Speaker A: Of the PSS stuff will be sort of like how should I built entry in the scroll protocol? Or do you foresee that it may kind of exist as out protocol implementations of that kind of marketplace and then maybe one day enshrined?
00:22:47.120 - 00:23:49.340, Speaker B: I'm not sure. It depends on other factors within the protocol. So, for example, whether we decide to use a BFD consensus or something else. So the problem with having external PSS with a BFD consensus is that ideally we would need quite a significant number of provers, let's say a few hundred to 1000. Which is not really great for a BFT consensus because usually a classic BFT protocol can support up to a few hundred nodes, which means that we have to limit the number of proofers that we can have in the network at any given time. We can obviously do like random comedy sampling, et cetera, but then you degrade the loveness guarantees of the protocol and therefore, at least at the moment, I'm not really sure if external will work with a classic consensus mechanism.
00:23:50.080 - 00:24:45.600, Speaker A: Got you. Also, another thing that I'm curious to kind of hear your thoughts on was, as you mentioned, I think the prover incentive game is something to be kind of a little bit more explored. And I think Ben from matterlabs previously, earlier in the day have also kind of alluded to the possibilities of kind of like bringing back the POW like system back for the proverbial networks that may or may not have under the roll ups. That prospect seems like kind of like hovering around for a lot of the proverbial networks that may sit under those DKA roll ups. What's your general takes on those POW types of incentive mechanisms that may emerge.
00:24:48.500 - 00:26:19.580, Speaker B: So let's assume that you would have some form of randomness embedded into this POW like mechanism because otherwise you end up with a system that the most efficient prover always wins. So let's assume that even in that case, I feel like especially if you want to paralyze and you kind of have to because your prover times are longer than your block times, at least for the foreseeable future, it's going to be like that. And therefore, if you have a POW like mechanism, you would have to prove sequentially, which, one results in a lot of prover energy wasted, computational energy wasted, because only one proof will be used at the end and two will result in finality times growing super linearly with the number of blocks that are being proved. So I'm not sure if that's the right path, at least for now. Long term, I'm not really sure. But again, I feel like collaborative proving where essentially provers depend on another prover to compute the proof and they collaborate together and then aggregate the proofs and post them on chain just makes more sense because it's much more energy efficient and it maximizes the computational power that you have to compute the proofs.
00:26:20.100 - 00:26:54.010, Speaker A: Got you. I think it's very interesting because I've also noticed a similar kind of discussions around well, here we call it proverb sequencer separations. But I think in the lands of Aztec, I believe they also have some sort of like a similar implementation of B 52 which also kind of have this same dynamics. Do you have any sort of common sort of thoughts around what the Aztec lands are happening with that the proverbs sequences separation work there?
00:26:55.340 - 00:27:26.310, Speaker B: I actually, funnily enough, I had a call with a couple of guys from Aztec a few weeks ago about this specific thing because we were just catching up about their approach and how it differs from what we're thinking, et cetera, et cetera. Yeah, I feel like a lot of LTS, especially ZK roll ups are going to converge on the same problems at the end of the day. And I feel like we'll have one or two winning solutions that everyone's going to adopt at the end.
00:27:28.520 - 00:27:39.610, Speaker A: I see. So basically you're saying someone is going to come up with the proposals, everyone will look at it and they're like, oh, maybe this will be cool to actually use, and then people start to kind of standardize across.
00:27:40.780 - 00:27:53.724, Speaker B: Yeah, I feel like maybe two max, but I don't think every single roll up is going to have a drastically different decentralization approach to decentralization. Got you.
00:27:53.922 - 00:28:27.770, Speaker A: Okay, well, I guess that's probably also the community driven research part that's going to hopefully come in handy for everyone in this research space. Yeah, I guess that's going to be exciting. Well, I guess in the interest of time, I will have to wrap it up and toguru thank you very much for coming today. Very honored to have you here for the talk and sharing about the PSS. And again, looking forward to any future research that may come out of scroll as well.
00:28:28.860 - 00:28:29.830, Speaker B: Thanks for having me.
