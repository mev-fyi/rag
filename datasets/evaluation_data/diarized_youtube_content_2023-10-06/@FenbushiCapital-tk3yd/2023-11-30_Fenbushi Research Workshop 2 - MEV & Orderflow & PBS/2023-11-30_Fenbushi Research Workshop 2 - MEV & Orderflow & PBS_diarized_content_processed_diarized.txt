00:00:11.110 - 00:00:57.000, Speaker A: I will quickly introduce myself. I'm Yuki. I'm currently doing a bunch of research stuff at so we are hosting research house this time. And this is part of the workshops of the research house where we're going to go through a lot of research discussions about some of the, I guess open problems that exist in the space. Yesterday we finished the topics on cryptography, which was amazing, and today we're going to talk about MVB, PBS and ORderflow who is not excited? Very good. Oh yes, everyone's excited. All right, great.
00:00:57.000 - 00:02:25.382, Speaker A: Now I'm going to kick off the first discussion talk and we'll present something about here. Okay, great. All right, cool. So today I wanted to kick off the topic with this picture, the pursuit of relay incentivization, right? So I think recently I've been doing a lot of work around figuring out a lot of the relay incentive, why there's a problem around relays for PBS and how can we tackle that as a community with all of you guys here. So that's the conversation that I want to kick off here. So just to recap, I'm sure every one of you guys already kind of know this, but in a visually we have this PBS landscape and I kind of marked builders and relays red because these two entities are so far sort of like the more centralizing effect, where a lot of the centralizing effects are accruing. Right now, if you look at the top builders, all of the familiar names, if you look at the relays, the familiar names going away.
00:02:25.382 - 00:03:04.270, Speaker A: So there's interesting dynamics that are playing around in these two roles within the PBS. So one of the relay incentive problems, well, I mean, I kind of split into two parts, but first let me kind of go through some of the relay incentive problems that I see so far. And as I kind of stated here, most relays aren't running profitably. Is that correct, Alex? Yeah, that's fair. Yeah, that's right. Exactly. So most relays aren't running profitably and they're losing at around like one hundred k to five hundred k per year in just operating that relay.
00:03:04.270 - 00:03:49.280, Speaker A: A lot of which may be like human costs, like hiring engineers, expensive. If you optimize the machines, maybe you can amortize the machine costs, like maybe ten k or whatever, reasonably, but still it's a lot of cost. And now with the appearance of ultrasound, we also have a latency game that is appearing as well. So optimistic relay came in. So the latency assumptions become even more strict around the operational relay. So all the, I guess the more OG relays would now have to figure out, okay, how can we be even more competitive in this kind of like latency optimizer? Yeah, this is a great meme from Matt from Block native. So I kind of look at that.
00:03:49.280 - 00:05:23.550, Speaker A: And another instant problem here is that relays are doubly trusted entities. Validators builders both have to trust relays, but when stuff goes bad, they also blame relays, right? So relays are kind of in this like a middle spots where like, I'm doing good job for both of you guys, but when I fuck up, both of you guys also fucks me over, right? So that's like a very weird spot to be in and yet they don't have any incentives to gain from doing all of these relay jobs. Now obviously back in the day we all perhaps thought that, okay, relay might disappear one day. That with EPBs that may no longer be that relevant, or may no longer be that important, or perhaps may no longer be that trusted, or that need to be trusted. But I think based on some of the discussions across the researchers in the space, it seems to be more and more uncertain that whether EPBs will be a thing in the future anytime soon. Hence why I kind of have it here that perhaps relay we probably will exist, right? So relay matters. I think the point I kind of wanted to make here is that relay is a topic that we shouldn't discuss about.
00:05:23.550 - 00:06:42.902, Speaker A: It's one of the very important actors that is still withholding and kind of pulling together the PBS landscape. So why do people still run relay anyway? And I think one of the advantage that used to have was the colocation with the builders where you as a relay running closely located with builders enables you some latency advantage compared to other builders. So builder relay was basically a latency optimization plane, essentially. But obviously that may no longer be the case when there is a lot of searcher builders that are now also dominating the market with their own ORderflow. Even though the guys like Titan, well, I think probably mainly Titan are trying to hyper optimize their latency game, which is good for them, but at the same time there's still searcher builders that are fairly dominating the market with their own order flow. Hence the latency game would not matter as much as if you were just like aggregating those flow from everywhere else, like how Python is doing. And then the second 3rd reason I kind of put it here is like public good.
00:06:42.902 - 00:07:23.780, Speaker A: Public good. A couple of people made a lot of east from the back of the day they're like, all right, I want to give back to the community, all right, I'm starting relay. I heard a couple of people like that already. So I know for a fact that there's a lot of public good relay operators out there as well. Maybe perhaps some of you guys are here as well. So some obviously are also speculating that perhaps we may have a future for monetizing this relay. And I think that's what I'm going to sort of allude a little bit, probably won't have time to dive too much deeper into this, but just so that we can get this conversation started.
00:07:23.780 - 00:08:30.970, Speaker A: So why builders need relays. I kind of just have some bullet points here. But I do want to allude you guys to this diagram where you kind of see how many validators are connected to each relays. So you can think of it like ultrasound connected to 730K validators, flashbust connected to 875K validators. The relays act as a role of aggregating those connections with all of the validators such that the builders themselves don't have to do that. So that's why I said here that relay essentially provides a broader validator reach and the relay can handle the work of actually connecting and registering those validators to themselves. Obviously, if the relays have a good latency infrastructure, they can also provide a low latency in payload delivery for an impalo propagation of the block across the network.
00:08:30.970 - 00:09:41.550, Speaker A: Now, I guess I also wanted to mention here that you could also alternatively have a builder relay. But builder relay in general does require a lot of heavy bootstrapping in terms of one, builder trying to gain validator trust such that the validators are willing to register with them, and two, essentially devise a way such that their builder relay will somehow be more attractive options versus the existing relay, which oftentimes means that it may incur more cost to the builder themselves by perhaps sending lower bit to other relays and higher bit to their own builder relay, which may be less well connected when compared to ultrasound and flashboards and those OG relays. Well, okay, ultrasound is maybe a bit newer. So that's that. And then why validators need relays. Again, I kind of put a graph here. Same thing, broader builder reach.
00:09:41.550 - 00:10:44.270, Speaker A: They would essentially help aggregate again to aggregate all of the bids through different builders and then send it over to the VAT, in this case the proposers, so that they would be able to receive as many bids and possibly highest bids possible for that specific block. It also ensures the correctness of the bid such that they're not like, blocking anything. But okay, this can be a bit dependent, some requiring bit collateral, some may not. So there's some variance there. But the point here I want to make is that relay is sort of like a very key component so far on the PBS building block. And we may have dismissed them previously for the fact that, yes, we may have in PBS, but for the foreseeable future, that's what I see. And for the foreseeable future, this guy is not getting paid.
00:10:44.270 - 00:11:56.620, Speaker A: So let's kind of figure out how this guy can be paid right now. Some of the questions maybe we can ask ourselves who should pay for the relay cost and how should the relay be paid? And this question obviously kind of dive into like, okay, who sets the fee and how, et cetera, et cetera. But these two are probably like the overarching problems that is still kind of in a bit of a debate, like how should we do this, including later on, Alex will dive into more about this, but they're also doing some experiments about how we can better figure out these problems. So this is a very short opening intro to this problem statement about this relay synchronization. And again, I have done a piece on this relaying centralization problems and have dove into a bit more details about how exactly I see the relaying centralization landscape could span out. So for those anyone interested, please check out this QR code. She would have a bit more details about that.
00:11:56.620 - 00:12:50.090, Speaker A: Yeah, thank you very much. And yeah, so now I'm going to hand over the mic to Alex from ultrasound relay to dive more into the competitive relay dynamics for our relay landscape. So please give a round of applause to Alex. Yeah, competitive relays. So, to start, I'm Alex, I'm from the ultrasound team. About two years ago I started working on Ultrasound Money, the website that you may have seen screenshots of. You may have seen the little emojis on your Twitter or Xfeed.
00:12:50.090 - 00:13:44.590, Speaker A: And what I want to talk about today a little bit is how relays can be competitive and what that might look like the relay landscape in the near future. So I'd say that relays are a smaller component that sits within a larger topic, which is PBS, which sits within an even larger phenomenon, which is mev. These two I won't be talking about at all. I also think there's like many great researchers in the room that can tell you a lot more about it a lot better than I can. I'm mostly active as a builder, but what I might be able to offer some insight on is what I think is going to happen with relay specifically. Also good to note that this would then all be pre PBS changes as well, or sorry, pre EPBs changes as soon as the protocol enshrines, something that would of course change a whole lot more. So let's talk about relays.
00:13:44.590 - 00:14:31.010, Speaker A: So, very quick history starting point that I picked is like when flashbox deployed the Mefloost software, which was when the merge happened back in September of 22. A lot happened before this, a lot of post stuff during the proof of work era. But I think this is a good starting point as this is where the first relay was born. Now, this relay did very, very well. You can see how very quickly, like, the amount of blocks that gets filled with a MEP boost just shoots up. I think today we sit at about 93% and then pretty quickly, maybe not as quickly as some people, I hope, but reasonably quickly, a whole bunch of extra relays pops up. And I think they pop up for various reasons.
00:14:31.010 - 00:15:29.982, Speaker A: I think some of it is decentralization, like very ethereum aligned values. Some of them showed up for censorship, resistance, right? At some point the Flashbust relay decided that they felt like they had to do OFAC censorship for very reasonable reasons. And some relays, ultrasound being one of them, popped up very much so because they wanted to do something about how many blocks were now being censored, and some relays popped up because they wanted to make a profit. And of course, and this is also true for the ultrasound relay, it's a mix of all three for the different relays. Then notice when I did my run through that I don't have that much time for this. So I'll just say currently there's some things that we're very happy with with relays, right? How they connect all builders to proposers and how proposers now have very equal access to average mev that they earn per block. There's also some things that we're not so happy with, like there's maybe six, seven relays that's still not that many.
00:15:29.982 - 00:16:08.300, Speaker A: You could still say that that's very centralized. With that might come some power if relays try to use that position for something. You can also just say, as already happened, that if you want to pressure relays and make them sensor, for example, you might be able to do so because there's not that many. So you just need to have the right jurisdictions to do something about that. I'll skip over the other ones. What I will say is there's a great post why Ensuring Proposer Builder Separation by Mike Neuter and Justin Drake on ETH research. It has like a very elaborate overview of the current weaknesses that relays have and why we might want to get rid of them and move EPBs forward.
00:16:08.300 - 00:17:00.506, Speaker A: Also interesting to notice that some of these may be improved just out of protocol, like how many relays there are that could just become more relays. Some of them are impossible to do something about, like the trust based on reputations that we have with relays. That is the point. If you want to get rid of that, you need to move in protocol somehow to change that. So then going back to these three, it seems that as far as decentralization goes or this providing of mev value to proposers, relays are actually doing their job. It's still, you could say not that many, but they are at least working as envisioned and giving access to mev censorship resistance. I think we've also seen that relays will come to try and do something about it and relay censorship at least is a lot better now builder is not looking so great, but relays have shown up to do something about that.
00:17:00.506 - 00:17:34.898, Speaker A: So it seems that at least there, there's also some pushback happening and a natural ecosystem. For me, the profit one, as Yuki was already mentioning, is the one that seems to be where relays struggle the most. And you could also say that this is of course if this one doesn't work, the current equilibrium that we have might be very unstable. We've already seen that the block native relay stock. So let's zoom in a little bit on that one. I call it here relay sustainability. You could also fairly call it relay profitability.
00:17:34.898 - 00:18:11.342, Speaker A: Right? But this is like how relays can establish their existence. So this is like a very quick cut through that I did of what is the current value of relays. So I think relays are valuable to builders and proposers. And then again, just really simplifying some things. Some big aspects that count for builders and proposers are trust, of course. Relays are trusted by builders to not look into the blocks that they want to publish and steal mev. Relays are trusted by proposers so that whenever a proposer signs a block, they pick a bid.
00:18:11.342 - 00:18:50.286, Speaker A: Basically they trust the relay to make sure that that bid actually gets published and makes it on chain. Both of them, of course, are interested in having profitable blocks. And then finally you also see that censorship is a big dimension. You even see that some cases you have proposers which are part of companies, companies that might be US entities and those entities might have to comply with OFAC. So you even see some proposers demanding that the relay do some filtering, otherwise they won't connect again. The one I'm going to zoom in on is what we see around profit and what builders and proposers want there. So to approach it on a very high level, profitable blocks win.
00:18:50.286 - 00:19:20.866, Speaker A: And why do profitable blocks win? Because proposers are selecting the most valuable block. Bid value increases over time and faster they seem to win. I'll very slowly step through these three to explain this current state. I think this one, there's not that much to explain. Proposers will pick the more valuable block. We can maybe rely a little bit on the altruism of proposers, but in general, that's of course not what we want to do. And we just see that proposers are picking the valuable block.
00:19:20.866 - 00:19:42.878, Speaker A: We see some self building maybe. But like I have shown before, the numbers are pretty high. Something like 93% now is going through methods because they are far more valuable blocks. So then the second step, bid value increases over time. Let's look at that. So here is like an active overview that you can see. This is from the website.
00:19:42.878 - 00:20:13.994, Speaker A: Payload de. And you can see how bids are coming in as time moves forward. And you see on the right here like an overview of the builder bids and which one is winning. And then sometimes you see like a yellow line fly through the graph, which is when a bid gets selected by a proposer. So this is like the dynamic version. If you were to try and plot this on a graph. This is from the Time is Money strategic timing games in proof of stake protocols paper.
00:20:13.994 - 00:20:33.914, Speaker A: It's a great paper, very interesting. I think it's done by some EF researchers, someone from Jump Crypto and a university that I can't quite recall. But you can look up the paper. So this is a plot of this, where we see here. This is the time before a slot starts. And a proposer picks what they're going to propose on chain. Then we see here a really big wave.
00:20:33.914 - 00:21:07.660, Speaker A: These are all the bids that come in from builders. And then the relevant section is like this greenish area with the line flying through, which is the value of the bid. Right so the only takeaway that I want to bring across here is like up and to the right is what this line does. Bit value increases over time. Then maybe to simplify, you see that some bits come in after D is zero. For my next examples, I'll assume that it is as if the proposer always asks exactly at the start of their slot, what is the best bid that I can find. And we go from there.
00:21:07.660 - 00:21:39.366, Speaker A: You'll see why I make the simplification in a moment, because it seems like faster relays win. Why. So, to take an example, let's say there's some builders over there, there's a relay in the middle. And there's a proposer here on the right. And let's suppose that for the builders to get their bid to a relay, takes them 200 milliseconds. And for a relay to get their bid to a proposal, it also takes 200 milliseconds. Now, if we from this previous graph, suppose that the proposer at T zero is going to ask what is the best bid that I can get.
00:21:39.366 - 00:21:59.110, Speaker A: That means that the last possible moment, that the builder can still get a bid all the way to the proposer. Is t -400 milliseconds. Does that make sense generally. Okay, cool. So then let's suppose that there's another relay that's slightly faster. This relay only needs 100 milliseconds to go from builder to relay. And 100 milliseconds to go to the proposer.
00:21:59.110 - 00:22:28.622, Speaker A: So that means that the last possible bid, that we can still get to a proposer is at t -200 milliseconds now that would mean that there is. And I tried to use the length of the lines here. I'm sure there's a better visualization possible. But I ran out of time maybe if you have ideas, you can tell me after. But let's suppose that this is a faster relay. It has slightly shorter lanes through time. You can imagine that means that for this relay the builder has 200 extra milliseconds to come up with a bid.
00:22:28.622 - 00:23:01.962, Speaker A: And like we just saw, bid value increases over time. So there's 200 milliseconds, in which there could be a slightly more valuable bid that goes from that builder all the way to the proposer. And now, the next insight. And for this hat tip to Justin Drake. He's often the ideas guy for things that ultrasound ends up working on. And as he also often is way ahead of me, at least he pointed out that this second relay may be overbidding. If we go to this scenario where we're in the last milliseconds right before a proposer is going to ask for a block.
00:23:01.962 - 00:23:52.880, Speaker A: And there's still 200 milliseconds in which this builder is coming up with a more valuable bid to this relay. But we already know that this relay cannot reach that proposer anymore because there's less than 400 milliseconds remaining then this bid going up over here. If it's already higher than this one, it will never get out competed anymore. So this really could just outbid this one by one way when there's only these last 200 milliseconds left. And it knows that everything else that it gets is value that it doesn't have to pass on to the proposer, because this would be, in a way overpaying. You're already the only one that's still going to be able to talk to the proposer. Why would you be biding even more than anybody else and nobody else is able to bid anymore? Instead of doing this over bidding, we'll try to smartly look at what the other bids are that are available and try not to do that.
00:23:52.880 - 00:24:19.740, Speaker A: It's not simple to do this. You get built blocks from builders and you're supposed to pass them on to proposers. It really needs to a little bit become a builder to be able to do this. Like the payment to the proposer is in the block that we're getting from the builder. So it's a bit complicated to do it. But we've been experimented with it for a while now, or at least we've been researching it for a while, and it looks like it's very possible. So this would be a way where relays can capture some value.
00:24:19.740 - 00:25:15.174, Speaker A: So then to look forward with all that we have established, I think we will see that some relays will make use of a competitive advantage. And I think also once you see that these relays might have more valuable blocks than other relays, it becomes really hard to stop a relay like that. This is the relay that has a more valuable block to offer to a proposer. So I would say that you want relays that are ethereum aligned, but at the same time you want to make sure that they're very competitive. Like we at ultrasound have, for example, said we want to self cap at a third. Like we don't want to relay more blocks than that because we think that's unhealthy for decentralization. We're also hopeful that if this does go well and we manage to capture a bunch of value with the relay this way, we would be very interested in using a significant portion to fund other relays to make sure that the relay ecosystem stays healthy.
00:25:15.174 - 00:25:54.946, Speaker A: But of course, you don't have to do that. You can just come in as a competitive relay and not care about any of those things. And then again, you might rely on proposers to say, do I want to connect to this relay? But you don't want to make proposers responsible for this kind of stuff if you can help it. I'm going max fee to try and communicate all this to you. And now we want like 800,000 people to start making the right. If you can avoid it, that'd be great. So I expect then that we see that some relays are competitive and they kind of play this game and I expect that we see some relays that are less competitive but are still there and fulfill a very important decentralization role.
00:25:54.946 - 00:26:57.898, Speaker A: And if a proposer for example, wants to switch away are there as an option and of course further away, then you can expect that there's EPBs to show up. Then I'll add that this is one way in which you can earn as a relay. It definitely doesn't appear easy to find other ways, but I'm quite confident that it is possible. I think one that is already being experimented on is relays charge a fee. This is also something that's not live but that I know people are working on to see. Can just all relays or a majority of relays start charging a fee to builders and try and be more profitable or sustainable that way? Another idea that I'm actually quite excited about and I think hasn't really been tried yet, is to have a relay guild, much like Protocol Guild where you say, well, there's a lot of people that are huge beneficiaries of the current status quo around mev. Can we just get those people to throw money in a collective treasury much like protocol Guild and then distribute that over a number of relays? Yeah.
00:26:57.898 - 00:27:47.980, Speaker A: I hope that has given you some insight in what I think the relays might look like in the future. Thank you. Thanks, that was a great, I guess, talk on the relay incentivization and any questions or any, I guess things here wondering about this relay problems or any comments from you guys on this matter. Where are the slides available? I mean, we'll try to make the slides available after the talk. Probably we'll post it on Twitter, so if that's okay for you guys. And yes, we'll have the recording as well. Yes, you have the mic.
00:27:47.980 - 00:28:58.706, Speaker A: One moment. So as you mentioned, rebates are kind of centralized, right? And did you consider maybe you can see the block and you can see if you get some transfers or your fee or something? And is it possible to add latency, maybe artificial latency for the blocks that are profitable to you or like blocks that are not profitable to you add more latency to them? Yeah, I think I would for sure need more context than that. Speaking purely from the relay perspective, I'd say that's very difficult to do because generally you'd have other relays and relays, they're already pretty low latency. So I think as a relay you would start losing very quickly if you start adding artificial latency. But there might be some room there and maybe you're also thinking of more than just a relay. So be curious to hear more content. Yeah, maybe we can talk about it more.
00:28:58.706 - 00:29:43.138, Speaker A: Well, there's like what, six relays if they work together, right? Yeah, no, obviously if the relays work together, then that could be a way to do that. There's other concerns with that. If all relays agree on slowing down their blocks, for example. But I do think there's just a basic truth to if all relays start charging something, then it becomes very hard to bypass them. Yeah, I think it's profitable to all relays and it's to the right. Then maybe as a more direct ask, I can add you to the group where relay funding is discussed and then if you can convince everybody to make something like this happen, then that's great. But it turns out in practice to still be quite hard.
00:29:43.138 - 00:31:13.310, Speaker A: Yeah, I guess one question that I also want to raise a bit to, maybe also you can also on this, but essentially there's always a pull string between the competitive dynamics versus this altruistic aspect where you're kind of like self capping yourself and whatnot. But then oftentimes that is a very unstable equilibrium because if you have just one competitive but not very altruistic relay to just like boom, come into the market, then boom, it's over. Right. It's over in the sense that I'm making a lot of noises, but anyway, it's over in the sense that you have one dominant relay that could just kind of be very competitive and take the entire market share of this payroll deliveries as well as capturing some parts of the base revenue. So what do you think about that concerns around this unstable period? Yeah, just in a very practical sense, I'm hopeful that builders like myself will try to build relays that take into account these longer time horizons and broader effects on the ecosystem. But hoping is not a great way to design a strong protocol. So if we go like one step beyond that, I would say the worse it gets, the more I'm looking to ethereum protocol designers to say, okay, this is not great, we need a better design, let's somehow bring this in protocol.
00:31:13.310 - 00:31:53.130, Speaker A: So basically you're saying that in protocol PBS is from your perspective, do you think that would be the ideal solution? Yeah, again, I get real hesitant if I have to speak on how I feel about doing EPBs, especially because there's multiple designs and there's many opinions about it. But what I will say is the worse the relay ecosystem gets, the more I think we need to do that. But it's kind of open. Like the relay ecosystem still could look pretty nice or it could start looking nicer going into the future. I still think there's some pretty fundamental parts where it's just not aligned with ethereum's core values. Like, again, this post that I refer to I think is a great post to talk. About that.
00:31:53.130 - 00:32:29.334, Speaker A: Why enshrined proposal builder separation. But yeah, it could start looking better and then maybe there's not as much pressure on getting EPBs landed in protocol. Yeah, just quick one on the amount of relays. Is Titan doing relay a good or a bad? Wait, Titan here. Wait, hold on. Oh, great. Well, hold on.
00:32:29.372 - 00:32:29.734, Speaker B: Can I?
00:32:29.772 - 00:33:55.010, Speaker A: Yeah, no comment, no comment. I can make a comment. I guess from our perspective, it seems like the relay development hasn't been that great, especially catering to the builders and catering to centralized like decentralized traders. So we want to push the relays further development wise, speed wise, yeah, I guess whether it's good or not, the whole relay monetization issue that everyone's seeing, we think that makes more sense. If builders run that, then there's no issue that the relay is going to shut down because they can't afford it anymore, which seems quite realistic. Alex yeah, in general, I think more relays better, like we want to see relays that try and make sure that things are stable and that proposers get access to great blocks. I will say at the same time, it is proposer builder separation.
00:33:55.010 - 00:34:38.350, Speaker A: The reason that we created it is because there's risks when builders get really close with proposers. But also the Titans credit, they seem to be a very ecosystem aligned builder. They're definitely not just profit maxing and disregarding any of the impacts they have. Part of it remains to be seen, I would say, but so far it looks pretty good. Perhaps one could argue that if you are ecosystem aligned builder that you could bring enough credibility to your builder relay as well. So as such, maybe yeah, some form of builder relay might make sense, but there's concerns about the degree of alignment. Right, the alignment.
00:34:38.350 - 00:35:54.470, Speaker A: So that is obviously some concerns there too. But imagine some soldier builder running relay and I'll tell you, that's going to be fun. What comes afterwards? What comes after what exactly? After what you presented today, you mean that some relays start competing? Maybe a little bit more. So let's say you ship the second price auction and what comes afterwards? What's the next step role for someone? Yeah, I think that's pretty interesting, especially because it would mean for us as a relay that we get also much more aligned with some of the attendance that builders, for example, have. One of the reasons that we have not implemented very rapidly, some of the things that builders have been asking for is because it also just didn't matter to us that much. Like we were already doing almost 33% of blocked and we already said that we didn't want to do more, so why would we do more? But now we're getting reasons to do so and also ways in which we can stay below 33% and still be able to capture more value. And a lot of this has to do with latency, which I think as you've heard already is something builders really care about.
00:35:54.470 - 00:36:31.060, Speaker A: So I think we improve the relay a lot further is one of the things that's next for ultrasound, and if I can take it a little bit broader still, that's not the only thing we work on. And we also care to build lots of other things within the ethereum ecosystem. So then if our team is better funded where now we're not so funded, then we could use that to build a whole other, maybe public goods. Yeah. Why did you pick t equals zero? It just simplifies things like if proposers don't ask you at t equals zero, you need to be a lot smarter about what you're going to do with this second price auction, as you refer to it. But yeah, just things get more complicated. But I'm happy to answer more questions after.
00:36:32.550 - 00:36:33.460, Speaker C: Thank you.
00:36:38.570 - 00:37:29.166, Speaker A: Yeah, I'm going to go back on the Titan point, sorry about that. So, two things. First, I think the equilibrium is like you open the Pandora box on your builder and you're in relays and more on your side. Like Titan can just show off the 201st millisecond and we know how the thing works, the distribution, so internally, how can you even compete as a relayer? Let's say V one, and that would be V two. Yeah. Again, I hope we don't get there and there will just be a diverse relay ecosystem and it'll be fine. But if we have to and we, for example, see that Titan has a huge advantage because they now do things where it's not so much an open source relay that they run quite separately, but it's something that's heavily integrated with their builder and they have a very strong advantage.
00:37:29.166 - 00:38:08.580, Speaker A: Then we also have to start thinking about what can we do with builders to get right next to their software, maybe even run on their machine, some kind of black box that we use to relay things, but not necessarily trying to accelerate even harder. There's a question from the back here. Does Flashbot send you any order flow, yes or no? Yeah, I'm pretty sure that their builder submits to us as well. I think virtually everything. Yeah, exclusively. Oh, oh. If there's any things that are exclusive to us from flashbox? Yeah, no, I don't think so.
00:38:08.580 - 00:38:36.650, Speaker A: They may have some on their own relay, like a small amount, and it might be related to some of the products, for example, that they have, like the meth share stuff. No. Okay, then. Sounds like no, trust me, bro. So it sounds like running more relays is necessary for the ecosystem on the one hand. On the other hand, it is competitive. Right? Suppose I want to run a relay.
00:38:36.650 - 00:39:06.706, Speaker A: I have a team that kind of can kind of killed it up, but it never ran a relay. Where do I start? Yeah, I think it's very doable. Again, much credit to Flashbots there. They open source all the stuff that they built there's. Now Titan who's built a whole bunch of stuff that they made open source. We have some small parts that are open source and some small parts that are not. So I'd say go look for the open source software that exists for running a relay and then from there, I think maybe get in contact also with some of the existing relays.
00:39:06.706 - 00:39:57.710, Speaker A: I think at least we would be very willing to help you get on your way. All right, there's one more question. Hey, everyone, Murak here. I was formerly the product lead at Block Native, and you could say I was partially in charge of finding a way to monetize the relay. And I left and started Primev, which we've been building this mev commit piece of P to P software, partially because the vision was that and this came out of the MEB boost stewards first call, partially because the vision was that relays will do more in the future. This second price auction with sealed bids and things like prof and top block auctions like Manifold is doing is partially towards this vision. And we continue to feel that this is the way to kind of move forward and have relays do more across the Mev pipeline.
00:39:57.710 - 00:40:51.062, Speaker A: And think of the pipeline as like a whole set of things that any actor could sort of do modularly and hopefully relays inherit some of this. How do you think about this approach and this kind of evolution of relays from just kind of payload delivers to additional services that are maybe not fully amassed or vision? Yeah, I like it. Right. Again, I think this relay ecosystem and even the wider Mev ecosystem could flourish where people start building more things that are great for proposers and also builders. Why not? The challenge often is that you already see that updating the relay software, especially on the proposer side, is something that's very tricky to do. And now you need to get hundreds of thousands of proposers to make changes and it's changes that are critical to the functioning of the protocol. Of course, the protocol can do without mev, but we've already seen times where the protocol didn't finalize as quickly because there were issues happening.
00:40:51.062 - 00:41:27.940, Speaker A: So I think it might be quite difficult, especially if you try to do features that have something to do with how proposer side functions. But I would highly encourage relays, as we are trying to do, to develop some kind of edge, whatever that is, and try to monetize that to make themselves healthy. Great. Sorry the time has kind of ran out, but the round of applause. So I know. There's Danning from flashbox. Please give her our.
00:41:33.200 - 00:42:14.052, Speaker B: Hi, everyone. I'm Danning from flashbox. I'm a data scientist. I'm here to share like, a data analysis of order flow mental block building relevant topics relevant to everyone here. So starting with some insights or understanding from an order flow analysis to look at what's a trend and evolution happening in the order flow landscape, particularly about Dex trading and then how many from the order flow analysis we found out, a lot of the application are already adopting private mempool. Like they're sending their flow to some particular private mempool already. And then also just combine that with also today's block builders game by acquiring more private order flow share.
00:42:14.052 - 00:43:08.232, Speaker B: So let's start. So here's a chart summarizing basically past three years, how the routers are their market share volume for accounted volume. So to clarify the volume here we're accounting is like by all the routers that's labeled as acknowledged projects like say uniswap front end or like one inch front ends or like matcha or excluding all those map bots volume. So here you can see pretty much starting from DeFi summer July 2020, it was still like Dex router dominating. So basically people are still trading on AMN decks, like basically units on front end. And maybe there were some old order book model like zero X or delta IDEX, but then quickly starting after with one inch leading the aggregator game, the aggregator volume starting to take off. And at this point a Dex aggregator is starting to basically aggregate the liquidity fragmented across different pool.
00:43:08.232 - 00:43:39.584, Speaker B: So to provide a better price for user coming to the front end. And then there is meta aggregator comes out from like for example, wallets starting to introduce swaps into their front end. Basically MetaMask swaps allows you just like one click inside of your wallet. You don't need to go to any front end, you can just swap within the wallets. And so they ping all the aggregators. So we kind of call them like meta aggregator, they're aggregating across aggregators. But the volume is always small because it's mostly coming from really small trades from the retail.
00:43:39.584 - 00:44:41.290, Speaker B: And then leading by cowswap, they started the solver model which is basically trying to decentralize or basically outsourcing the solving towards a set of solvers that's trying to provide a more competitive solving or routing solution for them, for their user compared to the one entity doing the solving as DEXA creator. And there's a little tiny piece here in the end which is a new type of router. We found that's taking a volume share which is telegram bots. So this is sort of like how the trend looks like for all the routers for volume. And one trend that we've identified first is that pretty much Telegram bots is the only new novel use case or application that's been happening past year and been taking off activity. So like last chart wasn't obvious because it was about volume. But here if you look at the transaction cost so to measure basically on chain activity and it's basically Telegram bots taking more than 30% nowadays and it's like the only thing that's been popping since like past year.
00:44:41.290 - 00:45:17.616, Speaker B: But they have a fundamental issue, right, because they're asking to share your private key to them. So I believe that's also one reason why the volume is small because people wouldn't really trust to put really big funds into Telegram bots but just to test it out. The second trend here is that the solar models since cowswap started there's one there's uniswap X. They're starting to taking off by volume as well. Where really? I think it's like next level game. Dex router or Dex aggregator. Is trying to do, which is to potentially introduce competition or basically auction it off to a bunch of solvers.
00:45:17.616 - 00:46:33.496, Speaker B: So to potentially introduce more competitive routing or optimal routing as they say I think it's a question remain to be answered. It's like today we will say yeah, of course Solomon model probably have a better pricing or execution quality compared to Dex aggregator compared to AMN, but I don't think there's any concrete or solid data reports yet to really compare the difference. It's also a hard problem to compare because you have to basically have a benchmark say for example what's the best AMM liquidity back then? And then compare on top of it. And I think it's a good idea but the execution maybe remain to be answered as well because today tau swap allows the solver to answer a solution within 10 seconds and 10 seconds isn't really a fast time for the liquidity. Another problem here is that solver incentive seems broken and as we heard from some solvers, they might complain it's a lot of work with little incentive. For example this chart here, left one is all the solvers on Cowswap and you can see there's a lot of new players coming in but they churn away as well. There are still a lot of players today on Cowswap because there is still pretty good profits coming from cow token.
00:46:33.496 - 00:47:48.176, Speaker B: But we also wonder if it's going to be sustainable or like Cowswap team is basically bleeding cow token question. This is One Inch and they used to have a really good incentive model with the Inch gas token where solvers can potentially get the refunds through that. And it was, I think removed or basically deprecated things like March or earlier this year. And you can clearly see a lot of the solars kind of turn away and nowadays it's pretty much One Inch and a few other market makers. I think that's a question like if solar model can be sustainable, they will probably fix the economics. So the third trend here is that there's more and more RFQ integration coming up that we can see even though the volume here is going down because of the markets in general, the trading volume is going down but you can see the number of RFQ models coming out is growing. So in the end there's a lot more RFQ nowadays that's integrated everywhere and they prefer or basically they tend to try to disintermediate all the aggregators trying to access the flow more directly say for example, work with MetaMask directly with a direct RQ integration, et cetera.
00:47:48.176 - 00:48:33.140, Speaker B: Potentially it's a good thing for Dex trading because they're potentially introducing the private liquidity for better routing than AMM, because how it works is that on Dex aggregator, they will be like Pinging AMM. They'll also Ping private market maker. And then compare the price. And if the market maker's price is better than AMM, they will take the yeah, um, so calling back to my old slides so I was calling it out, saying like, oh, there's a lot of places that market makers plugging themselves. Like, they are the mapbots, they are the RFQ quote provider, they are the toxic flow. There are, like, block builders as well. Potentially I had a Psyop, so they probably going to be also the solver or filler on Uniswap.
00:48:33.140 - 00:49:04.300, Speaker B: And guess what, what is this? We're looking at the sankey. And so for example, this is a one inch fusion liquidity diagram and then you can clearly see like for example, from the left is the entry point where they goes into one inch website and by default fusion and it goes to the fusion system. And then there's a bunch of solver, there's one resolver like fully supplied by Winter Mood Liquidity. It's vertical integration everywhere, market mover everywhere. So psyops proven.
00:49:05.360 - 00:49:05.724, Speaker A: Yeah.
00:49:05.762 - 00:50:30.744, Speaker B: And so another view looking at the trading behavior for all these different categorized use cases that we're talking about is also interesting where if you see on the left is trade size, on the right side is trading pairs. It's like a clear train on the left that for example from top to down telephone bots are pretty much very small trade and combined with the card on the right side they are pretty much only trading loan tail because they are doing small amount of backgrounding or atomic mev on chain and then the orange one is wallet swap and they're trending to much smaller trade coming from the retail and then Dex front ends is still very dominant in terms of how to say percentage but then they're trending also to smaller trades. Meta aggregator are trending towards bigger trades because potentially they're supposed to be better as pricing for bigger trades and solver models like only in the very big trades or like well, trades. Yeah, so that's been interesting. So I have some chart to show but it's like a chart crime, it's like a lot of labels and I don't think anyone can see it clearly but I will read it out and I will have conclusion versions. I'm trying to put them into a scatter plot. On the y axis it's the trade count, on the x axis it's the median trade side.
00:50:30.744 - 00:51:27.468, Speaker B: So really just combine that two charts together and trying to locate them into a landscape, right? So you can clearly see uniswap is like the king of distribution. So by number of trades are high up. That's why I have to also make the access log scales because otherwise there you can't really see the structure and I labeled them by the use case or category they are serving. So the yellow one is banana gun like Maestro Dexview like Unibot, they're all telegram bots, they're all circling around on the top left and then most of the gray are circling around middle, but on the left like smaller trace sides, lower frequency maybe. And they're all wallet swap. And then the blue one are all the Dex aggregators and then the green one is silver and then the pink one are all the decks front end. So they're really actually clustering around their area.
00:51:27.468 - 00:51:34.620, Speaker B: So I think this is a pretty good conclusion. To conclude, what are the use cases served by DeFi today?
00:51:34.690 - 00:51:35.068, Speaker A: Right?
00:51:35.154 - 00:52:35.948, Speaker B: So like telegram bots, higher frequency bots like trades but like small trades. Today retail trades coming from the wallets are like smaller retail on the left lower bottom if you see it as like quadrants of four area, right? And then the only one decks which is uniswap is like outlier there. But most of the decks front end are in the bottom layer which are basically pretty inactive. But they are actually all over the place in terms of trade size because different AMN design might be favoring different pricing for different trading sides or pair. Bigger wheel trade always go to silver and then medium wheel trade are served mostly by tax aggregator. But this part is left blank, right? So there's nothing or really we're identifying as the application today is serving the retail on DeFi to trade, high frequency and larger trades. So what do you guys think is there? My answer is basically DeFi and the mapbox.
00:52:35.948 - 00:53:17.788, Speaker B: So yeah, I think it's still something we can fill in the blank to acknowledge. A lot of the work here is by a collaboration with my teammates Angela and she's writing a blog post coming out soon, so look forward to that. So, Next is basically looking at all those volume number we looked at by project. We noticed a really interesting trend about private mempool. So once we joined the public mempool data set and check if any of the transaction was seen in a public mempool, we noticed there are some certain applications. They're using private mempool particularly. So if we look at a few sankey, it's also impossible to read, but I'll read.
00:53:17.788 - 00:53:42.544, Speaker B: So this is a cowswap sankey in terms of their order flow. They come in from their front end and then goes to solver assistant. And then the red one shows that pretty much almost all of them are going to private mempool. And then they all go to map blockper, which is their order flow auction and they go to all the builders. The next one is one inch order flow. It's two parts. One is one inch fusion which is SolidWater.
00:53:42.544 - 00:53:49.656, Speaker B: One is the normal one inch router part and the solver part basically one inch fusion are pretty much also all.
00:53:49.678 - 00:53:51.092, Speaker A: Going to private mempool.
00:53:51.236 - 00:55:04.316, Speaker B: The normal one inch router one because the user is doing the submission, it's not really like one inch or the solver is controlling the submission mostly are still going to public mempool. And telegram bots is another big example that we're seeing. They're using private mempool Unibot is sending to all the private flow to private mempool and forwarding to flashbots protect mostly and maestro telegram bots also sending to private mempool and also to actually they are sending to both ofa like flashbots protect and also map blocker. So like they're multiplexing cross ofa which is not a great idea. We also have analysis about this because once you're multiplexing you're undermining basically the privacy setting on flashboard protect or matshare that's being shared to map blocker and then also each other can potentially invalidate the other one's valid background to be lended. So we have some more rants about that. So to conclude, the private limp adoption by application category is pretty interesting that we're seeing among all the application meta aggregator dex prompts because there mostly are still user submission and there are still some private medical adoption by user.
00:55:04.316 - 00:55:16.196, Speaker B: Basically it's around 20 or 30% which is pretty impressive by volume period though. And then Silver Model and Telegram bots are pretty much all sending to private flow. So if you're building private RPC, you know who to talk to, right?
00:55:16.218 - 00:55:17.190, Speaker A: As your customer.
00:55:17.880 - 00:55:33.704, Speaker B: Yeah, and so for all the volume we're accounting here, it are also pretty shocking that 50% of volume for all the potentially nontoxic flow is going to private member already. But imagine also adding the map bots flow, it's probably going to be even.
00:55:33.742 - 00:55:35.610, Speaker A: More leaning towards private flow, right?
00:55:37.360 - 00:56:41.772, Speaker B: But next is how does that entail to block builders who basically assumingly private order flow is the king for block building as we all think? To quick recap, the underlying data here is like today 93% of validators running Map Boost and so 93% here is like map Boost versus 7% vanilla validators. And the top builders are Beaver, rsync. TIDA and Flashbot. And I think a lot of us probably acknowledge like oh, the top ones are basically searcher builder, they have their own CFI ARB flow potentially it's much more valuable for the block bidding value. Titan is also a neutral builder but potentially also with a lot of interesting flow. So another concluding data here is that 15% of Ethereum transactions are going through private mempool. So an interesting data comparison is from this year Q one in January to March.
00:56:41.772 - 00:57:33.808, Speaker B: We looked at by different builder what's the private order flow percentage for each one? And these are all the top builders back then, right? So it's around like 3% to 5% and it's trending up. And we had a refresh on this data thanks to mempool dumpster that was built by a Flask teammate. We looked at top builders today in q Three is basically around at least around 13 or 15%. Pretty stable and even the smaller builders also have like 5%, 7%, 10% as well. It's three times basically compared to Q One. So yeah, like private mempo adoption is definitely happening and to answer the question or basically to prove the assumption, everyone has like private order flow is akin to block bidding. We're just showing it in data here.
00:57:33.808 - 00:58:25.596, Speaker B: So this chart is showing like the Y axis is number of landed blocks or winning blocks and the x axis is basically the private order flow percentage. And you can pretty much see like Beaver and Rsync has the highest number of landed blocks and also pretty much high private order flow percentage. Flashwatch also has really high private order flow percentage because we have a private RPC product called a Flashwood Protect Bureau 69. Also Gambit and F one B. There are some outliers here like block B eater. They are really high on private order flow percentage but they don't really land blocks all the time. I think they were busted a few days ago on Twitter there was a searcher builder chart showing that there is also a searcher address that's like sending flows mostly or disproportionately towards block builder.
00:58:25.596 - 00:59:18.860, Speaker B: So I think the Psyops is that they're also having a lot of private searching flow. It's proven in data, right? Private order flow is a king to block bidding and this trend is concluding summary here is like block building order flow. More order flow is definitely going to be helpful for block building. But then differentiating further order flow have private versus public, right? If the order flow is shared too many then it's probably less value for the builder. If it's more private, it's probably more value for the builder to keep. And there is further differentiating between private order flow. If it's like say it's just like private RPC product receiving users value transfer transaction, it's not really going to bring a lot of mev.
00:59:18.860 - 00:59:51.230, Speaker B: So differentiating further there's also searching flow and searching flow can also be categorized further to be like atomic searching flow or non atomic C by D by RF flow. So everything goes down. I think the value towards block reading is higher in that line. Shout out to Menpol Jumpster data and it's also on June so anyone can write SQL can analyze those stuff and join with Dex reading. Yeah. So that's my last slide and thanks for listening some memes. Any questions?
00:59:52.000 - 01:00:25.210, Speaker A: All right, thanks Danny for great. I want to open up the floor for any questions you guys may have about this great data set that she had presented. Any questions? Anyone that please resent if you could have access to any data they don't currently have.
01:00:27.740 - 01:00:53.250, Speaker B: Yes, I want to know the margin of C flow. I know there was a report from Frontier about kind of measuring the C ARB profit percentage and the number was like 37% to 77%. It's like really wide. So I want to know if we're able to measure that better. And so to account also block builders profits better. And so to also know like potentially to model it everything.
01:00:59.160 - 01:00:59.910, Speaker A: Oh.
01:01:06.280 - 01:01:07.028, Speaker B: Hi.
01:01:07.194 - 01:01:40.580, Speaker A: My question is about MAV share protocol. So there are some key transactions in blockchain such as Chaining, Transmit and this transaction are connected with. So my question is why Chain, Link and other players let them protocol?
01:01:42.440 - 01:02:22.544, Speaker B: Yeah, we actually had a very interesting conversation today at launch about some projects essentially to build a order flow auction or auctioning off these kind of price updates events through MapShare. And so basically they're able to say, post this event as a mepshare transaction or a user transaction inside of MapShare and any user or searcher who wants to background that, they can submit a bundle and then basically that money or refunds can potentially go back to say, Aave or any lending protocol. And so to gain some revenue, I think it's a great idea. I don't think our team have talked to any of them particularly, but no.
01:02:22.582 - 01:03:00.430, Speaker A: We have people are building it. I think why Chainlink doesn't do it, one reason I would imagine is that it kind of changes the incentive. Yeah. Projects are working on it, on developing this kind of thing. Why Chainlink specifically doesn't do it, I think would be that they now have additional incentives. One incentive they have right now is like give good price updates because that's their business model. If they're also being paid in mev for the updates they give, then now they have a different incentive to maybe give price feeds that liquidate some positions because that is much more profitable to them.
01:03:00.430 - 01:03:03.260, Speaker A: Thanks.
01:03:03.330 - 01:03:04.236, Speaker B: Thank you.
01:03:04.418 - 01:03:17.330, Speaker A: Any other questions? By the way, that was very interesting. That was go back to the slide about private order flow for a second.
01:03:19.560 - 01:03:21.092, Speaker C: Yes, the other one.
01:03:21.146 - 01:03:26.256, Speaker B: Sorry, which one? Right, which one particularly? Sorry, Ken.
01:03:26.368 - 01:03:26.644, Speaker C: Sorry.
01:03:26.682 - 01:03:28.580, Speaker A: The bubble pivoting.
01:03:30.940 - 01:03:33.384, Speaker B: This one just a builder one.
01:03:33.422 - 01:03:41.160, Speaker A: Yeah. Do you manage to segment that flow by whether it was coming from Sigma transaction like I-E-A user?
01:03:43.420 - 01:04:15.510, Speaker B: Yeah, that's definitely the direction we want to go further to. I'm working on this with some research fellow rather to further categorize the flow. Yeah, that's definitely a thing. And also to add like the payload e website that they just showed earlier. They also have a view that's really interesting to categorize all the transactions by a label, say if it's mev transaction, if it's a valid transfer, it's a deck swap. And you can clearly see like most of the mev or mev leaking value transactions are taking top of block position there.
01:04:17.400 - 01:04:24.920, Speaker A: Have you got a feel for how much that's going to bring down towards, let's say, Titan flashbox?
01:04:25.500 - 01:04:26.824, Speaker B: What do you mean by bring them down?
01:04:26.862 - 01:04:29.770, Speaker A: What will bring them half of their overflow is their own.
01:04:31.500 - 01:04:33.128, Speaker B: Yeah. So that's the next thing I'm going to.
01:04:33.134 - 01:04:33.672, Speaker D: Try to do.
01:04:33.726 - 01:04:59.884, Speaker A: Yeah, that's a good call actually, just quick questions from my side. So I think you have recently mentioned that there is some data suggesting that some telegram bots are submitting the order flow to both the flash bots and also the map blocker, right? Yeah. Is there any strict advantage for them to do both besides the fact that you have the second privacy compromises?
01:05:00.012 - 01:05:22.664, Speaker B: Yeah, I think the primary consideration for them is they want faster inclusion, then they should send to more. But if they want optimal output, then they should send to one particular, because otherwise the one that's including faster may. Because they didn't have a background found out valid but then the other 1 may have a valid background will not land because they will say noun is too low like user generator is already.
01:05:22.702 - 01:05:33.196, Speaker A: On chain in some way they are sort of optimizing for the bidder pool so that to get the most highest bidder possible for the background I think.
01:05:33.218 - 01:05:42.188, Speaker B: The outcome right now is they're being included faster but I don't know if that's their intention. Maybe they just didn't understand the multiplexing.
01:05:42.364 - 01:06:40.528, Speaker A: All right, I think that's it for Danny, but the round of applause for so I just wanted to quickly introduce so today's panel is going to be moderated by the Lord of the ODA Blobe Quentin. And then we'll have Speakers Barbie, julian and Thomas and Tony. All right, give a round of applause. I'll let them go. I'm rocking. Welcome everyone. So the battle is about AutoFlow and order flow auctions, but maybe to warm it up.
01:06:40.528 - 01:07:08.350, Speaker A: And the idea is that I get all the questions from the audience so please just raise your hand. I also have a couple of spicy questions but I'll reference those questions from the audience. Maybe just do a round of introductions. I'm not going to ask what people do, you can give a brief little research. It's kind of bottom line here. But I have some questions like an opening question. If you guys invite me, I'll save maybe a different question for you, but.
01:07:10.000 - 01:07:11.240, Speaker D: Just to set some context.
01:07:11.320 - 01:08:01.020, Speaker A: Ethereum, when it launched had a very specific model for how transactions would get on chain. We assume that users would submit the transactions to the Mempool and these would be picked up quite organically by miners at the time. And so the interaction was mostly between users as it's like peer to peer cost of play and clearly that is shaped. It turned out very different and there's been a lot of changes over, you know, in these discussions. We're not always sure really what we should even aim for and what the goal should be. And so maybe starting from you Tony, and moving this way, I'm curious if you guys give a very brief introduction who you are and maybe tell us what you think the goal should be. What is the ideal case for the autofoc landscape of yeah, yeah, great question.
01:08:01.020 - 01:08:51.660, Speaker A: So just to introduce myself very quickly, I'm Tony, I work at the Applied Research Group at the Berm Foundation and I've been mainly concerned with a lot of data stuff, so I've been building a lot of Pix website, maybe some of you might know them. And to answer the question so I think private order flow has definitely a lot of very strong centralizing forces, especially when it comes to the builders. So builders receiving private order flow will have an edge on other builders. And then in turn, of course, if these builders then are censoring, then we have the worst of both. So we have centralization and censorship, which is something we have to really pay attention to. Hi, I'm Thomas. I'm a researcher at robust consent group at Theorem Foundation.
01:08:51.660 - 01:10:22.410, Speaker A: Yeah, it's about the order flow. I think it's like other things, for example, latency, it's always something we have to fight against a bit because if we let the market decide, it's probably going to converge towards centralization somehow. So I've been thinking about this a bit lately, but it's basically for me, it's about designing mechanisms that are biting against all the ways that could actually lead to centralization. Just to maybe elaborate on that when we talk about a centralized order flow supply chain and maybe think a little bit down the line, if we have censorship interviews and lists and these kind of things, what is a decentralized order flow, sufficiently decentralized order flow landscape look like? Yeah, it's a broad question, but I guess it would be like a landscape where it's very hard to either pinpoint one entity, I guess shut it down and have it have an impact all the way up to the network and protocol. There'll be like one answer, but it's very broad. You can't really put a number on this. I think up to a few months ago, even the actual landscape was quite okay.
01:10:22.410 - 01:11:17.138, Speaker A: We had many relays and everything seemed quite good. But then as soon as some of the entities start to censor and turn to, I don't know, the whole landscape has centralized a bit more and then now we have to implement mechanisms to fight this. Hi, I'm Gideon. I'm also a researcher at the robust and census group. So just like Thomas, I'm mainly interested in microstructure and methods design, so interested in Amen stuff. My view on private order flow is well, if we go to what you were saying about, we assume that people submit their orders to the public. ManPool, I think there's something to be said for not doing that because as a user you give up all of your market power and you basically hand the monopoly to a proposer.
01:11:17.138 - 01:12:19.830, Speaker A: So I think if you can make efficient mechanisms that capture value, you can restore the user monopolies in some sense. So that's my hope. But how to do that is obviously very difficult. Yeah, and maybe just to elaborate on that a bit further right now. It seems like these non mempool mechanisms for capturing value, for getting users better execution end up being centralized. And I'm curious, maybe thinking in the longer term, if this trade off continues to exist, where the more centralized route does get better users, better execution, what can be done about that? Is that something that's acceptable to some degree? Is it okay if you get bad execution but always end up on chain? Or should Ethereum aim to give everyone really good execution and do the decentralized way? Yeah, so I think defining what's really good execution is not that simple because we're saying really good execution is getting the most out of your trade in pure dollar terms. But we see now that people could probably, well, certainly trade cheaper on decentralized exchange.
01:12:19.830 - 01:12:51.054, Speaker A: So maybe that's not like capturing the full utility of users. So apparently there's some utility to trade on chain that we're not capturing by using pure dollar terms. So we would have to map the utility of users. And then with respect to centralization, this is definitely very difficult. Maybe there are different approaches you can take there and different centralization degrees are acceptable. Okay, thank you. Barnaby, you're an OG.
01:12:51.054 - 01:13:52.850, Speaker A: You've been around for much longer than me at least. And so I'm curious in your time involved in the Ethereum ecosystem, and maybe from what you've spoken to people before, if you can maybe explain some of the shift in mindset and how people thought about the role of Ethereum, the role of Ethereum with regard to the order flow landscape basically. And there seems to be somewhat of a trend where execution in some sense is moving off chain and settlement is happening on chain, to use the traditional finance terms. So just generally the arc of mental model, I guess there yeah, that's a good question. To me, the goal of Ethereum has always been maximum user welfare or maximum user utility. So the means I would say we're fairly agnostic about decentralization is a means to get to that. Like the physics is that decentralized systems are better at guaranteeing maximum user welfare.
01:13:52.850 - 01:14:53.226, Speaker A: That's why we built these systems in the first place. But from my perspective as like and I'm not an OG, I arrived three years ago, so it's not like that long, but I arrived at the time that the flashboard skits were starting to wreck havoc on the network. And so I think I didn't have in mind that local block building is particularly sanctified or the only orthodox way to build your block. I think a lot of assumptions that don't necessarily happen on top of my mind. So I saw these changes happen and we've done some fairly I would call them interventionist policies, like EIP one, Title Nine is a fairly opinionated change to how the fee market works for Ethereum. So I think I've grown comfortable throughout my tenure to observe these changes and try to evaluate them always against the same benchmark, which is, are the users getting more bang for their buck when they use our system? Yeah. Thank you.
01:14:53.226 - 01:15:20.456, Speaker A: I've just got a new question, so before I go off in my interrogation, are there any questions from the audience? Okay, there we go. Can you pass the line? Curious, with EIP 4844 and Pokemon Sharding, do you believe the whole MEP landscape.
01:15:20.488 - 01:15:22.636, Speaker B: Will change in terms of competition for.
01:15:22.658 - 01:15:32.144, Speaker A: Blob space, or with the higher speeds, there'll be no changes, and will there be no MEB or will there be, like, markets for Blob space? And who's competing for that?
01:15:32.262 - 01:15:34.130, Speaker B: What's your opinion on that?
01:15:40.500 - 01:16:55.684, Speaker A: Yeah, there's been actually cool research from someone else in our team, Davide, and some folks at Offchain Labs who looked at what the market looks like post for it for so, for instance, there are reasons to believe that the Blobs are a bit too big, but it's a network constraint. So one thing that can happen that's kind of cool is roll ups could do cost sharing over the Blobs to kind of get the cost down because there is an overhead to paying for a Blob. So that's one thing. Another thing is, okay, what's mev in the block space? That's a bit more tricky to evaluate. I think the mev that is in the block space will be first, the mev that's on the roll ups, it leaks maybe onto the L one, but I think the goal of for it is to have fairly strong abstraction between the two of separation. So I'm not sure that there's a ton of MV in the block space, but I'm sure there's harder to extend that maybe you have it. I was just going to say right now, at least, the main MEB we see in interaction with other entities are with and there is not much going on in terms of L Two mev right now, but we don't know how it's going to evolve.
01:16:55.684 - 01:18:05.338, Speaker A: And I do think L Two protocols are going to have to take it very seriously and think deeply about how they want to capture mev and redistribute. Yeah. So, welcome, guys, to Istanbul. I'm from Turkey, so it's great to see you all had it's not a question, but discussion starter, but I also don't know if it's really fitting. The order flow topic is that most of you guys were involved in this latest inclusion list or censorship resistance articles on E Three search. And yeah, I just want to hear what you think about it, because Palmer recently published his simulation results with all these different configs and it's really hard to say what is the way to go with that, like, forward inclusion list, or how do you even draw a cost function to creating an inclusion list. And so, yeah, but Quintess can cut it up anyway any point, because it's not really order flow.
01:18:05.338 - 01:19:03.380, Speaker A: No, I think that's definitely relevant and maybe just to make the connection to why it's relevant before, to give the answer. When asking why order flow is important to Ethereum or what the goals are, many of the answers were about builder centralization. And one of the big reasons people talk about builder centralization is because people are worried that if one entity is building all the theorem sparks, they can sense of anything. So I think there's definitely connection there. And I'm curious if you have thoughts on I mean, when writing the post, I picked on Tony's work, also Barnaby's work published on this and on all the games that can be played. Tony proposed a design in which you can actually set the block deadline so the transactions that are in the inclusion list stay longer there until they are included or until the block deadline expires. But yeah, I mean the the obvious point when I wrote it was more like the design space is actually bigger than I thought.
01:19:03.380 - 01:20:09.906, Speaker A: There are tons of games that can be played around it. An inclusion list is a way for proposers to include transactions that the next proposer will have to include in each block as a condition for the whole block to be valid. So it imposes proposers to include a set of transactions that they wouldn't necessarily want to in the first place. But yeah, once you say this, the possibilities are basically endless. You have a lot of designs. I think the overall idea is really good, but you have straight up between. For example, I was saying if you make a very big inclusion list but no transactions are actually susceptible to be sensor, then you are leaving a lot of also training something in the protocol that's not very useful.
01:20:09.906 - 01:21:04.950, Speaker A: But on the other hand, if there are a ton of sensor transactions that are there and you don't have a mechanism for it, then it's a big issue too. So, yeah, it's a trickle question. Yeah, maybe to add on that. So basically what we achieved with Math Boost was we gave proposers this deniability so they can only see the header, what they get from the relay, they sign it off and then the builder puts it on the chain or the relay puts it on chain, doesn't matter. But the cool thing is the proposer can always be like okay, I have not seen the content of the block, so I don't know if there is any OFX transaction in there or not. And inclusion lists, the goal is more or less like to give the same properties to the builder. So the builder should be like, there's an inclusion list.
01:21:04.950 - 01:22:04.854, Speaker A: It handcuffs me. I cannot do anything else than now, including this Tornado cash transaction. So the builder could then always argue I'm actually OTEC compliant. But the consensus protocol forces me to now include this transaction because the proposer before me put it into the inclusion list. So this is kind of the goal I guess a big problem is do big validators? Will they touch inclusion lists? I think that's a really essential point there because in the end, if I'm a solo staker and I propose a block two times a year or something, then I have two inclusion lists, right? So I can force Lido, I can force Rsync VG build, flash odds and the last sensoring builder that is quite big is builder 69. Thank you very much. So I can force them to include my transaction, but I will not have a very big impact.
01:22:04.854 - 01:23:01.406, Speaker A: Right? And I think inclusion lists, if everyone would use them and we would put all the Opex sanctions transaction into the inclusion list, then we provide builders a little bit more of this deniability so they could be like, okay, I'm OPAC compliant, I still included it and everything is fine. Yeah, one last super small point on this is also it's a bit tricky because when you design incentives against entities that might just not care about the incentives and not touch it anyways because they are really scared of the regulation, it makes it even harder. Because let's say you design an incentive compatible mechanism, but people are just like, I'm just not going to use it. Because even if it's incentive committee, I don't want to be. Yes. So I just like to expand on that question and I'll give it back to the audience. So inclusion, this is one thing that we can do.
01:23:01.406 - 01:24:11.380, Speaker A: We can change the protocol. But there have been discussion about more wide ranging changes or more like in depth changes, like the Pepsi discussion, these kinds of things. I'm curious how you guys think about where the protocol ends. Yeah, this is a borderly question. Where does the protocol end after what point should ethereum researchers just not worry about? And that ends up just being like the purview of whatever the wallet and they have to think about it and what is at that end? How far can the Ethereum protocol go in actually influencing things? Yeah, that's a good question and I don't have a short answer to it, but I can say that to me it ends where the community kind of decides it ends. So there is this game of cat and mouth between what happens outside ethereum and what happens inside, which is what the protocol can see and what is legible to it. I have a post on that but yeah, I think we find ourselves often in a position where, okay, something bad is happening and we want to prevent it.
01:24:11.380 - 01:25:19.462, Speaker A: We have options, which is enshrined these mechanisms. Enshrining is not a free lunch like you do it because you believe that your protocol can make the credible claim that I'm defending against a set of outcomes that I don't want to see happening. To bring it back to more pragmatic things, I have this protocol that's called Pepsi protocol and proposal commitments. And the idea there was to say the protocol should just allow any kind of commitment, any kind of claim that proposals will be able to write in some sort of EVM language, for instance, to be taken as the letter of the protocol. So defending any kind of commitment that people would spontaneously make and this was a way of future proofing. The protocol pervert to say, well, if there are claims that we protocol designers can't really think through properly because we have limited computation power, maybe the proposals as a whole or the system can self organize around making these claims credible instead. And so yeah, it does tie into censorship.
01:25:19.462 - 01:26:34.514, Speaker A: The main idea was that the PBS auction is like this massive object. Like you sell a very valuable thing every 12 seconds, which is the block. And I felt like having just this one big sale every 12 seconds was a bit dodgy like it does, let's say, induce a lot of centralization forces. So Pepsi was a way of unbundling that auction a bit further and maybe try to give a bit more competitivity at every level of block production, including inclusion or not of sensor transactions. Could you relate that answer to order flow? Okay, so yeah, one thing we have been looking at is called Pepsi Boost and it's the idea that instead of the proposals making commitments, we need protocol changes to do that but we could try to emulate that idea by having the relays be the place where these commitments are made and sort of upheld. So one of them is we know the top of block is very valuable, this is known by everyone and we thought, what if we just separate the auction between the top of block and the rest of block? It's also something that's commonly been discussed. Skip and Cosmos has ideas around this, SMG has written a paper around this.
01:26:34.514 - 01:27:26.290, Speaker A: So these ideas are definitely kind of in the air but we've tried to relate that to this idea of commitments more generally and provide these in times overrelate. We've done some research on that and how it relates to other flow is that now as a bidder or as a searcher, you would have essentially two endpoints that you can talk to and these endpoints would be, let's say, in separation. There are a lot of bidders in the room. So I think when you hear me saying that it probably sounds like super naive, like I'm in my ivory tower and I have no idea about the realities of load building. There are so many complications. I think we're really just like scratching the surface of what's possible with us and what are the impossibility results even. But it does feel like a promising direction to maybe try to limit at least the centralizing pressures that order flow imposes on us, the protocol.
01:27:26.290 - 01:28:21.540, Speaker A: Thank you. There were a couple of questions in the front, I think. First from one, so it's kind of related to PPC single slot finality. I don't know if you've heard about Egan layer, but in a way, it's basically going to become possible to control most of the change that has to be done in protocol. So if I'm a builder, I could propose a service which with the law for single stock finality, which maybe, I don't know, speculate, if I have single stock finality, I can provide better services, more value in the block. How does that change the way you do research? Right? Because in a way, it's like you're putting in a sidecar everything that happened in the Ethereum Foundation, et cetera, and you can just be the math scientist that puts stuff in prod iterate faster and you don't really care. But the overall, the more holistic view.
01:28:21.540 - 01:29:25.874, Speaker A: Yeah, I just wanted to say I've not yet really thought deep about the topic, about combining and eiglayer and restaking with building. So I'm not sure if I'm the right one to answer. That came from me looking at Eigen layer in the first place. So, yeah, I feel like I try to understand these risks and opportunities of Eigen layer as well. As a protocol designer, I would say to me, it's pretty exciting, actually, eigen layer as an ID, and especially the idea that there are protocol changes we can test before they actually hit the protocol. As I was saying before, I don't think there's any prelaunch with any kind of change that we make. In the case of eager layer, the kind of cost that we pay is that if something goes wrong with a single capability, we don't really have control over the set of agents that are involved.
01:29:25.874 - 01:29:57.380, Speaker A: We do have control because they are validators who we stake. But that control is a little looser and maybe blurry around the edges when there's too much leverage in the system and they overcommit to different things. So, yeah, I would say an opportunity a risk. I'll talk about it a little bit at Economics tomorrow. So if you guys are around, I had a question about inclusion lists. If I can piggyback to that.
01:29:59.270 - 01:29:59.886, Speaker D: Inclusion.
01:29:59.918 - 01:30:10.386, Speaker A: List should basically solve a legal problem, right? And you say, I'm a builder, I am a proposal, whatever. I see these things, I have to include them and I'm fucked.
01:30:10.418 - 01:30:11.190, Speaker D: And I have to.
01:30:11.260 - 01:30:48.834, Speaker A: Sorry. But legally you could argue that you are kind of shifting the responsibility to whoever feels the inclusion piece. So in theory you can say, okay, I saw this transaction, I don't want to include it, I'll shuffle it to whoever comes next. But then in a way, you are including it. Not directly, but you are telling someone else you have to. So the problem is, this is up to interpretation. Obviously, you love to trust that some old fart in a court is smart enough to understand what's going on and in purposes in the right way.
01:30:48.834 - 01:30:53.446, Speaker A: So probably for 20 years you are safe. But at some point it may be that I say no.
01:30:53.468 - 01:30:54.962, Speaker D: If you really want to be OFAC.
01:30:55.026 - 01:31:43.494, Speaker A: Compliant, then you cannot even shuffle this stuff to the inclusion list whatsoever. And at that point, if I'm really paranoid and I don't want to end up in a shit situation 20 years from now or whatever, I won't touch it anyway. So how do we solve that? Right? So I'll pass it to Tonya as well. But I think one of the trick is you can make many inclusion lists as the bidder that did it for the next one. So you do have plausible deniability because you could build many of them. So I think that's like just at the for all in front of everlies, everything is illegal. What? Sorry, you can make many but you are still like, giving commitment to other people.
01:31:43.494 - 01:32:20.030, Speaker A: Yeah, no, I don't think it solves everything, but I was just saying that's an added picture that you can actually make. Yeah, just to add, I would totally agree with you. So I'm the same opinion. I also think that in front of court right. We don't know what's happening. Right. But it feels like, as you said, if you put a tornado cash transaction onto your inclusion list for the next one, and the next one is then Ofak knocks on the door of the next one and he's like, yeah, but this guy forced me, actually.
01:32:20.030 - 01:32:22.394, Speaker A: And then we don't know what's happening.
01:32:22.532 - 01:32:24.014, Speaker D: We also have legal presence.
01:32:24.062 - 01:32:49.180, Speaker A: If I tell him, please kill that guy and he died, I go to jail. Right. It's a very good point and it's very valid. And there is already a lot of discussions going on on that research. And I think it's a very good point that people have to think about, especially the researchers now trying to implement inclusionism. Because, as you say, it's not that easy. Right.
01:32:49.180 - 01:34:02.138, Speaker A: I'd like to point out something. We've got a bunch of game theorists, people with more theoretical background here, as well as more pragmatic researchers. And what's interesting is that a lot of the discussion we've been having so far hasn't been about what the equilibrium is in some sort of like rational game, but rather about legal considerations and about the more complex preferences of a handful of actors that control the majority of the stake, the majority of the building. I'm curious what you guys think about this in general and what tools we have to really reason about this environment. There's sort of this weird meme theme going out now about the loop state of blockchain ethereum. And if we're not in this super permissionless environment, we have thousands of actors, emma's acting super rationally, it becomes much harder to reason about, or at least the way you reason about it's very different. Quite an open question, but if Julian looks so are you talking about the equilibrium of how inclusionists will end up? Everything about ethereum, I guess all the kind of research you end up doing, I guess the order flow based discussion.
01:34:02.138 - 01:34:21.670, Speaker A: So inclusion list is one thing, pepsi is another. Eigen layer is another. The Eigen layer question, I think really highlights it. It's not like people will choose the mechanism that necessarily in some rational equilibrium produces the best properties. It'll also be like what vibes in meme space work? Well, what are the coinbase lawyers like?
01:34:21.820 - 01:34:22.278, Speaker D: Right?
01:34:22.364 - 01:35:18.294, Speaker A: Yeah. So I guess if I can take this on a ride, let's say, and go from it from a theory perspective, I think one interesting thing is order flow routing. So let's say that you're just looking at swaps trading on the Dex. Then we can see that either you could look at all of the DEXes beforehand and browse very efficiently, make sure that you have as little price impact as possible and make sure that all exchanges are on the equilibrium price after the fact. Or you could just let the transaction happen, do some back run, make sure that all the transactions are all the DEXes are on the same price after the fact, and then rebate. And this should be like relatively you should get to be relatively same result for the user, except there's probably a bit more cost for the background name because you're doing one more transaction. The interesting thing here is this is probably better for a Dex.
01:35:18.294 - 01:35:44.894, Speaker A: So let's say that there's some Dex who ensures that after each transaction it does a back run. Then it will attract lots of users. In the current situation, more users means more liquidity. So it gets all of these network effects. It gets lots of users, lots of liquidity. So this will be stuck in there, but it might be more efficient if there's some other optimal routing thing. But maybe this optimal routing equilibrium versus the back running equilibrium won't happen because of all of these network effects.
01:35:44.894 - 01:36:27.310, Speaker A: So maybe this is one example of how ORderflow could end up in probably a stable equilibrium, that's stuff optimal. And then with inclusion lists, I think Renee Bake talked about that probably better, I think Renabase recent posts about funny games and inclusion lists that portrays like a feature where you could subsidize transactions. So let's say that transactions don't pay the base fee. You can subsidize them to include them if they pay some mev. I think this will be very interesting to see if these markets actually pop up. And then we would probably also have to look at how our transaction fee mechanisms work. That's very cool.
01:36:27.310 - 01:37:35.400, Speaker A: Thanks. Yeah, go ahead. Yeah, I think right now when you're talking about censorship, inclusionless for inclusion, whatnot a lot of it is in protocol feature. But I was also thinking that could there be a scenario where let's say, regardless of what inclusion this actually entails legally, perhaps, that we may see one builder starting to be like, all right, I got fucked by the court because I did something as. A block builder that all the other block builder will be like, all right, this is bad. Maybe I should start figuring out how I can reduce my risk, like regulatory risk. As such, could there be a scenario where they actually flock over to out protocol implementations of some form of builders that actually better mitigates those regulatory risk for whoever that is operating it? If that makes sense.
01:37:35.400 - 01:38:50.960, Speaker A: Essentially, if there's out protocol solutions for that, for a lot of those issues, yeah, I have hard times to think about out protocol solutions in this case, because in the end, what counts is what lands on chain, right? So in the end, if someone used a mixer and for example, tornado cache and it gets on chain, no matter what happens before that, someone put it on chain, someone executed that transaction. So I think that's really hard to do. I mean, we have Censoring relays, right? This is probably the counterpart to that where you as a Validator, if you want to make sure to get Censored blocks, you can use a Censoring relay, vice versa. This doesn't hold true because if you want to make sure as a validator you're non Censoring and you're connecting to all the relays, then still 80% of the blocks are Censored because the builders are so censored. So what we could do is we could say, okay, if I want to claim I'm a non Censoring relay, then I'm only allowed to accept blocks by non Censoring builders, right? And it's very easy to determine who is a non Censoring builder. The problem, of course, is many relays would lose a lot of that market share. So I totally get why they are not happy to do so.
01:38:50.960 - 01:39:33.290, Speaker A: And also, maybe to add also to your question before, it's not like 90% of the Validators are in the US. Right? So we would still have many inclusion lists filled with OPAC sanctioned transactions. What this would mean then, certain builders that don't want to include OPAC transactions, they cannot compete for that slot. So they would be in a position where they're like, okay, the consensus now forces me to build a block with their transactions, but I can't do that. So I will just stop building for that slot. Which would in turn mean it gets very so it's not very profitable anymore. At some point, if we have, let's say it's only 30% of the Validators that are not in the US.
01:39:33.290 - 01:40:42.900, Speaker A: Then we could have 30% of the slots with inclusion lists containing Opex sanctions transactions. This would then mean the big builders, except Titan, they could not compete for these blocks anymore, which would drastically reduce their profitability and in the long term drive them out of the ecosystem, if that makes sense. Another point on this I guess is also maybe like an obvious point, but right now we are thinking about sensor transactions as Opacion, everything. But I think inclusion, this also includes other use cases that we haven't seen yet that are not necessarily tied to any legal or regulation type of stuff that might be tied to competition. I don't know, like you want to censor some transactions. So I think it's also useful, maybe in a broader perspective. And about your question originally, I think using every tool we have is pretty useful, like the game theory as well as the legal and practical stuff.
01:40:42.900 - 01:41:59.106, Speaker A: I think everyone is trying to bring all these elements together. But at the end of the day, when you design a protocol mechanism, I feel like, for example, the game theory itself is going to be limited because the mechanism is super complex. So you have to add other approaches to this. When you get an impossibility result, for example, you are already extremely happy because just like formalizing a super theoretical theory or game theory is usually not capturing what's actually going on at the mechanism level. I know there's a question in the audience, I just want to extend this. There's been a lot of discussion about censorship and I don't necessarily agree with this, but I'll make an argument which is like 1559 was a mistake because most of the censorship resistance guarantees I guess there's two kinds of censorship resistance. There's a kind that says we assume that some fraction of the validators or proposers or whatever are honest and these guys will have inclusion lists or they'll vanilla build and then eventually anyone who's being censored by the bad guys will be included.
01:41:59.106 - 01:42:52.540, Speaker A: But there's this other notion which is much more focused on a much shorter time horizon and in particular is very relevant for a lot of things pulled on ethereum, like DeFi. Someone wants to post margin so they don't get liquidated, these kind of things. Or an oracle wants to update prices. I guess the argument would be that 50 59 for these short term cases removes a lot of the momentum that takes transactions on chain because the average user pays often a very large amount that's supposed to incentivize an inclusion. Before 50 59 incentivize inclusion, and now for a blockball that could be a couple of cents. And so the economic incentive to include this transaction is very low from those actors that control inclusion. I guess you can make some other arguments as well, but let's make that one more part, right? Yeah, I can answer that.
01:42:52.540 - 01:43:22.914, Speaker A: So I actually had a longer form answer about this on the Cosmos forum where they're considering 1559. And one of the arguments that was raised against it is this idea that 1559 destroys censorship resistance or at least makes it lower. It's definitely true. Yeah. The block producer doesn't get as much fees because the fee is taken away from them. Either it's burnt or put in a pool, depending on the design. So yeah, there is less incentive for me to include your transaction when I don't get the full fee for it.
01:43:22.914 - 01:44:06.462, Speaker A: My counterargument to this is one five nine is not necessarily pricing my economic value for the transaction. The idea of one five nine is to price the provision of network resources by the protocol. So we want to price congestion, essentially. So whether your transaction is like a super important swap or liquidation, or you're registering a NS for your grandma, it doesn't really make a difference to one python. It's not trying to price the value to you, the user, for this transaction. The second thing is we aim for low fees on the network, and if we do achieve low fees, it means that every transaction is centurable by definition. Because if I'm going to be paying $0.05
01:44:06.462 - 01:44:43.034, Speaker A: for my transaction, windows can come in and tell the block producer, I'll pay you six cents to not put down a base transaction into your block. So, with or without 1559 in a low fee environment, the censorship resistance is anyway pretty low from the fee market. Another argument is if Quintus comes in and says, I'll pay $0.06 if Blanabay is ready to pay five cents to you, and if the transaction that I want to send is actually valuable for $1,000. Because let's say it's. A liquidation, and I can get a lot of mev from it. I want to come back to the block producer and say, oh, I'll give you seven cents.
01:44:43.034 - 01:45:21.686, Speaker A: And it will keep going until either printool doesn't want to censor me anymore or until I reach my willingness to pay. In either case, it's terrible. We don't want to build a system where the user is extorted for the whole welfare that they expect to get from processing their transaction. As I said in the beginning, why we are here in the first place is maximize user welfare. And if a user is constantly extorted for their full value whenever they want to do something on chain, it's just a failure of the system as a whole. So to me, the fee market is not the place to kind of get these censorship resistance properties. There have been other proposals.
01:45:21.686 - 01:46:05.698, Speaker A: So this idea of competing block proposals in a way we already have competing block proposals, but not over the same slot. We have them in time. So if I'm not included because Quintus doesn't like me and Julian is the next proposal, maybe I can be included by Julian. And so Quintus has kind of a pressure to say, well, if I'm not including it, julian will include it and get the so we have this competition. There's other mechanisms that look at competition of multiple proposals building the same slot. There are mechanisms, such as consensus over mempool contents that can also be helpful, where you actually force the block proposal to include stuff that's sitting in the mempool. It makes a lot of assumption on the absence of private order flow.
01:46:05.698 - 01:46:17.434, Speaker A: It also looks like a much stronger version of inclusion list. So, yeah, long story short, I think one side highlight is not bad or sensory. It's not there for that in the first place.
01:46:17.472 - 01:46:18.060, Speaker E: Anyway.
01:46:19.630 - 01:46:45.160, Speaker A: I remember the other argument I was going to make was which we don't have to get into it. There's another question in the audience, is that with a lot of these order flow auctions, we see that users pay $60 to have their transaction included, then get refunded 100, which is like, well, they get refunded 60. And so they had to have the $60 in their account. You're paying gas fees. It's kind of an awkward design. So that would be another counterargument, but I think your response was pretty good. So thank you.
01:46:45.160 - 01:47:52.330, Speaker A: Okay. I was just actually going to come back to Thomas'point about the multipurposeness of the inclusion list, because we thinking about censorship and so on. But if you don't limit it to, say, a certain set of transactions interacting with certain set of contracts, then again, you can use this inclusion list to kind of always deny including some certain transactions, because you can always say. Okay, I included in my inclusion list these ten transactions which fill the space, and they are apparently not touching any of the tornado cash contracts. So I did my duty, and then you can't truly tell me I'm censoring. But in fact, I was actually censoring because I on purpose selected those, like, ten transactions. So I think the point is that when you start enshrining something into the protocol, like, say, inclusion list and the rules that you define around, it can always be kind of circumventible.
01:47:52.330 - 01:49:02.642, Speaker A: And I guess it's really then hard to say determine. You can determine if someone is censoring or not, but any new feature that you bring to the protocol, I don't think there will be a single answer which will just give us free out of censorship or anything, but it will just create more games, which will then create more things to enshrine into the protocol. Yeah, I mean, your question was very specific and then very broad. It's like how inclusion list and how to design it around. I agree that, for example, it does feel like the inclusion list designs that have been out there are based on the current situation, which is like OPAC transactions and interactions and everything. Maybe if the landscape was completely different and the transactions that were censored were also completely different, the designs that would have come out will look different too. And then about the complexity and the game growing, and the protocol always has to respond to it.
01:49:02.642 - 01:49:37.834, Speaker A: It comes to different ideas about notification and when do you stop? And that's such a broad question. I feel like everyone has his own opinion on this. Do we keep going? And if yes, until when? But it's very interesting to think about. For sure. Thanks for the eugene, can I call on you for a question? Yeah. Okay. I have a question I want to ask, but actually, I just think Eugene can ask it better.
01:49:37.834 - 01:50:55.220, Speaker A: So earlier I mentioned that what seems to be happening in Ethereum is a lot of execution happens off chain, then settlement on chain. That seems to be a trend which might continue and Eugene is involved in this Lana ecosystem and where a lot of the execution still kind of happens on chain. Something you've thought about quite a bit and I've heard you be opinionated about that. So I'd love to hear your thoughts on this and maybe provide a counter perspective to the sorry, I wasn't here for the whole thing so I might be missing some of the context but happy to ask the question as the token solana guy in the room. We've seen this trend in Ethereum DeFi where yeah, more and more execution is moving off chain ostensibly to provide better execution to the end users. I think one example of this is with Unisoft X sort of enshrining this off chain Roq system that is permission. Do you view that as a vector for central censorship in the future? So I guess coming back to what's the goal of Ethereum, it would be to maximize user welfare.
01:50:55.220 - 01:52:03.588, Speaker A: So if we observe that users get more welfare by trading off chain, either this should be fine or we should be designing something that increases their welfare. From a protocol perspective I think there's less to do per se, but from an application layer perspective this seems like an issue that should be accounted for there. With respect to censorship, I think having execution of swaps off chain isn't like a huge threat to censorship of transactions if I'm getting a key question correct. And there's definitely like utility from having a separation between execution and settlement in terms of liquidity, for example. Maybe to just take the devil's advocate position here, I think there is actually an impact on censorship. Maybe not in sending trances on chain, but generally we see there's like network effects to liquidity. So if the majority of users are trading in one venue that ends up being where everyone wants to trade.
01:52:03.588 - 01:53:17.068, Speaker A: And depending on what your goal is for Ethereum, not being able to participate in that venue or having a very small number of actors being able to determine who are the winners in that venue who can participate ends up eroding the credible neutrality and decentralization of the network. I have thoughts about that. I think that with discrete time markets you can do a lot that you can't do with continuous time markets. So there's a literature from Eric Woodish on this and I think it has a lot to do with discrimination and order flow. So let's say that you're trying to look at what's an arbitrage transaction we call toxic and what's like non toxic, like whatever other transaction as a centralized exchange, it's difficult to facilitate this arbitrage because let's say that you determine there is this amount of Arbitrage. It's difficult to make this a credible claim because you have an incentive to overstayed or understate or whatever as a decentralized exchange, maybe you could do this better. So maybe order flow discrimination could maybe be done better if you improve your credibility and with discrete time markets.
01:53:17.068 - 01:53:57.630, Speaker A: So I think there are some advantages to trading on chain that you can't have. So the difference here being that you have some sort of notion of identity and that you can track people's toxicity better. Is that the point? I don't even think you need identity. So we see that arbitrage happens in first transaction. For example, you can pinpoint to this transaction happens with it like bidding in some auctions. So that gives it credibility, not per se identity. And in centralized exchanges it's more difficult to do because it's very difficult to pinpoint the transaction that's arbitraged before the fact or close after the fact and look like in an aggregate way.
01:53:57.630 - 01:54:32.490, Speaker A: Okay, that makes sense. We have time for one more question. I've got a little bit of spice, but maybe someone from the audience has more. My question is about again private version flow, but with automatic bundles. I think a few days ago a builder just unbundled a bundle. Then this create the loss for the guy who is sending the automatic bundle. So how can this be fixed on the level so that our bundles are on the hands of the builders? Right? They can just unbundle it and just make us lose money.
01:54:32.490 - 01:54:46.904, Speaker A: Can this be fixed? If I was a panelist I just show swab but I'm not maybe next time which builder is one?
01:54:46.942 - 01:54:47.530, Speaker C: It.
01:54:49.740 - 01:56:28.024, Speaker A: Begin they not directly towards bundles but I think something interesting is here we hear talked about ID layer and why would you use in prototype solutions if you could use outer prototype solutions? Outer prototype solutions generally work optimistically, so we might require some collateral and see what happens and slash if something goes wrong. And in protocol solutions, maybe easier work pessimistically. So the block is just not valid if you do something that's not allowed, even though you could maybe do some pessimistic solution, other protocol. So this is something where a pessimistic solution might be good because you could observe like a winner's curse with optimistic solutions, where if you get slashed, apparently it was working for you. With respect to bundle specifically. Yeah, I wouldn't really care if a searcher is unbundled by the builder, so I would care as of the relay because it's like we saw this unbundling attack, right, which shouldn't have happened and a builder lost a lot of money, I think 20 million or something, and I think this is really an attack, right? But there's so much trust involved already between searchers and builders and if you don't trust the builder you shouldn't just not send a transaction to them for the relays it's different, right? Because the relays are kind of this very necessary and there's no way around them at the moment. So if a relay would really unbundle a block, it would definitely be an attack, right? Because of course it's very spicy and I'm not a searcher and my bundle would be unbundled and then extracted would be very bad.
01:56:28.024 - 01:56:55.030, Speaker A: But I would say it's already depending so much on trust, right, that I don't feel bad for a searcher that trusts the builder and then gets wrecked. I just mean by the search. But both searcher and the builders builder sending the relay, I include that too when that happens. Like when builders sending relay and relay the relay unbundled. Yes. I can't believe that. Which one? It didn't happen.
01:56:55.030 - 01:57:35.508, Speaker A: Yeah. Also, the relationship between the relay and the builder is based on trust, right. So as soon as this would happen the first time, hopefully no builder would ever send to that relay anymore. And hopefully the company behind that relay would lose all the reputation. It would get away with a few million dollars, but it would lose all the reputation and probably never be able to step into the crypto market anymore. So I get unbundling is a big problem. But it's more like we have to make sure that on the proposer side, everything is safe, right? That the proposer cannot wreck the searcher, for example.
01:57:35.508 - 01:58:28.228, Speaker A: Because that would then kind of eliminate all the trust that the builder has in the whole map supply chain. In the map boost supply chain, more or less. So I would like to take this opportunity before Yuki tells us all to be quiet, to just disagree. Because a lot of the discussion today has been around the threat of having a centralized order flow supply chain or landscape or whatever the term is. And if the sort of interface between different entities in this market, like a searcher and a builder erodes, then there's an incentive or an advantage to those who are integrated along this interface. If centralization is a bad thing, then this should also be something we don't like. Which brings us to a very topical comment someone else made.
01:58:28.228 - 01:59:14.130, Speaker A: What do you guys think about builders running relays? Especially relays taking on more complicated roles in the ecosystem as opposed to just like validating blocks and passing it on. Because the relays right now are a permissioned set of entities, right? Well, not really permission. Well, permissioned by each validator, sure, but still permission. Yeah, I think there were already discussions that builders could do that as soon as, for example, relays would charge some money. Let's say relays would charge 10% of every block, right? Much too high now, but this is just an arbitrary number. Then of course builders would have an incentive to just say, okay, I will just spin up my own relay. This costs me some money, but I would still make money.
01:59:14.130 - 01:59:55.372, Speaker A: So by not paying the 10% minus the operational cost for the relay, I will still be profitable. So I totally get why builders think about that. And I totally get that builders might be prepared for a situation where relays could start taking money for their services. So I think me, personally, I think it could be very dangerous. So I don't want to see the mass supply chain be more and more vertically integrated. But I have hard times now really thinking about the concrete consequences this could have. I guess I was just on that as like Tony mentioned about, okay, it's going to cost some money to spin up, really.
01:59:55.372 - 02:01:06.880, Speaker A: But there is argument to say that, okay, how much of that cost is really in terms of is it going to be like half a million or is it going to be $10,000? Right? There's a very difference in terms of how much it cost and it could hit the scale in terms of whether they're actually going to do it or not. And getting all those validator registration itself as a heavy BD work that they have to actually do to get to where flashboards are, where ultrasounds are. To that point, there is a pretty significant, tangible and intangible cost that is associated with their builder relay. And I think just to the point that you mentioned about this relay unbundling issue. And I mean, this is exactly what builder relay could potentially do, which is to unbundle it and steal your mev. So then the question becomes, are the builder neutral enough to be even trusted to handle your order flow that you are handing over to the relays in this case? Cool to wrap it up? Yeah, well, I mean, sorry, that was my last comments, but I have to wrap it up today. Again, thank you so much for this amazing panel.
02:01:45.380 - 02:02:45.540, Speaker E: Thanks for having me. Yeah, this is our most recent paper that we put out a couple of months ago, and this is only the second group to see it presented. So relatively fresh stuff basically going to cover why we think integrated builders exist and what about the current market structure, lends itself to that. And if we have time, maybe get into some ways that we could try to shift the game so that integrated builders wouldn't have a huge advantage in terms of some of the advantages that they have in the current system. So I guess the audience here is pretty familiar with PBS, but I did put a little intro to PBS, which is most validators. Don't want to build their own block because they're not very good at it. So they sell it in an auction using mevboost, which was created by flashbots.
02:02:45.540 - 02:03:38.040, Speaker E: And we have this open English outcry auction that happens every 12 seconds. And this decides basically who builds the next ethereum block in 95% of cases. Here's a view of the bids on the right. Here is a view of those bids in one particular instance of this auction. As you can see, this block was a particularly volatile block, so the price of ETH moved a lot and that meant that winning the block gave you the right to capture a large arbitrage. And so of course, the price of the block was large for that reason because you're basically bidding on the right to capture arbitrage. So the bidders in this auction are builders.
02:03:38.040 - 02:05:18.448, Speaker E: Some of them may be in the room with you now, I'm not sure, but there's certainly been a noticeable trend within those builders. And who's winning over the last 13 months since Medboost launched, which is that we've seen a trend away from neutral builders who don't run a trading firm and towards integrated builders who not only build but also have a trading firm. And I guess the goal of this paper was to figure out why that's happening. So you can see this is a screenshot from one of our gifts. Early on, the neutral builders were kind of first of the game and so they had a lot of market share. But what we've seen happen, and continues to happen, is that these integrated builders are getting more and more market share. So I guess we sometimes have a little joke in Telegram chats like I wonder why these builders are winning so many blocks? Is it because they're better at solving the Napsack problem? Of course this is not the case if you're deeply embedded in this because I think there's little edge to be had in Napsack and a lot of edge to be had in private order flow and in the way that you structure your blocks and the way that you get private order flow.
02:05:18.448 - 02:06:29.470, Speaker E: So we'll see more about that. But the reason that Beaver and Rsync are winning is not because they have a bunch of optimization PhDs that are solving the Napsack problem better. In fact, the evidence suggests that nowadays titan actually is slightly better at the like. Nonetheless, Arsink and Beaver are winning anyway without having an advantage in that area. And so this is going to explore why. So what we wanted to explore was basically sext arbitrage, which is when the price on chain doesn't match the price off chain, which usually happens when there's a lot of realized volatility prior to the block. How does behavior differ between integrated builders and independent searchers in that case? So the key observation was that if you are integrated, then you have to choose how much you want to give away.
02:06:29.470 - 02:07:28.284, Speaker E: Sorry, if you're not integrated, you have to choose how much to give away to the builder who includes you at the time that you submit. Whereas if you're integrated, you can just put the true value of the arbitrage to you in your bundle, merge calculation, build the most valuable block according to that, and then go ahead and take profit in the PBS auction. So you're taking profit at a later stage, which is going to give you an advantage. And we're going to model it in the following way. You have a number of integrated builders NA a number of independent or non integrated searchers who are going to be NB. They each have their own densities of the value of the top of block arbitrage and all of these draws are going to be independent. So there's going to be a single item which is the top of block.
02:07:28.284 - 02:08:35.620, Speaker E: Basically all bidders are going to submit their bid simultaneously. The payment mechanics are what's interesting here. So in order to model that difference between having to put in your bid early and choose how much you want to shade your bid this is what we call it, by the way, when you're in a first price auction and you have a true value of, say, 100 and you bid 90, the difference there, the $10 difference is the shade. So the kind of independent searchers are going to have to shade their bid when they submit to the bundle merge whereas the integrated searchers can in theory, they can observe basically the next highest bid and match that. So it's closer to a second price. Now, I will say right now, because I think this is not exactly true, that it's second price because there's some latency components here but still the fact that they can choose how much to shade later is a big deal. So all models are wrong, some are useful.
02:08:35.620 - 02:10:15.364, Speaker E: I think this is a useful model to think about this one particular component of the advantage for integrated builders, which is it's kind of second price for the integrated builder and first price for the independent searcher. Um, so for the guys who are bidding in the second price version, it's going to be weekly dominant for them to just bid their true values. There's some other equilibrium here, but we're going to focus on this one because this is the one that makes sense. And then for the non integrated builders, they are going to have to solve a first price problem and they're going to have to figure out what the optimal shading is. So this lemma is basically saying the non integrated searchers have a random hidden reserve price which is the Nth order statistic of the integrated searchers bids. So think of it as like there's a random reserve price which is the best that Beaver and Rsync can do on this particular trade. Okay? So knowing that they are going to this is just a proof of knowing that we can figure out what their strategy is.
02:10:15.364 - 02:10:48.304, Speaker E: So their strategy is a mapping from the value to how much they bid. This is basically how much you shade. And of course, the strategy is strictly increasing because the more you value the item, you should bid more. That should be pretty obvious. Here is the result. So the symmetric bne for this game is that the non integrated bidders, the independent searchers bid, this expression here. And if you're familiar with.
02:10:48.304 - 02:12:09.640, Speaker E: The first price auction results. This should look very similar to the first price auction results, just this part without the kind of FN sub A terms here, these two terms that I'm highlighting, these two terms are the additional part from having this random hidden reserve price. The other part is just the standard equilibrium in a first price auction. So it's nice that if you have no integrated bidders, it just reduces first price auction. And then of course, these terms will just be one because N sub A will be zero. And then you will recover the first price formula, which is exactly what it should look like. So I guess that's a lot of math, but what does it look like? In a simple example, let's say that we use the uniform distribution as the value for the top of block and there's one integrated guy and one non integrated guy, or sorry, in this case, there's just one non integrated guy and we're going to solve the general form for uniform.
02:12:09.640 - 02:13:14.560, Speaker E: And then later I'll go in with specific numbers for how many non integrated players and integrated players. So we end up calculating this surplus. And the bottom line is, when you are not integrated, it costs you a fraction of your surplus. So the bottom line is, if you're not integrated, you have a disadvantage, which of course we started out knowing, but the goal was to quantify what that actually looks like. And here's a term that can give you that, at least in the uniform case. And of course, when you go away from uniform, it's going to be a different number there. So currently the number of integrated builders is three, right? So if we just start to plug that in, we notice that the handicap is like zero point 42, which is actually a huge handicap.
02:13:14.560 - 02:14:17.430, Speaker E: And so the non integrated guys are really losing a lot in this uniform example at least. And if you look at kind of dynamics on chain, it does seem like this is happening, right? In some cases the leading volume are from the integrated builders bots. So that would suggest that there's a big advantage to integrating. And another thing that would suggest this is builder. The builder. Builder is not very competitive, but still happens to win blocks just because it's so advantageous to have this integration that it's worth it to kind of run a suboptimal builder and lose out on some of the value from having a good block just to get this kind of early. Just so you don't have to decide how much you want to shade early.
02:14:17.430 - 02:15:02.530, Speaker E: So that was one model. This is model number two, which it was based on Julian's talk at SBC. And we decided to kind of explore it a little more in the setting. Basically, in this model we have common values, so the price follows some trend and so the arbitrage is common for everybody. There's two bidders, one is faster than the other and the price evolves according to a GBM process. And the fast bidder is going to bid delta seconds after the slow bidder. So they're going to have more information.
02:15:02.530 - 02:16:03.670, Speaker E: So what does that look like? I plotted two examples here. The price process rises at the end and one where the price process falls at the end. And if the first bidder bids naively, he will lose money in expectation because he will be outbid when the price moves favorably and he will only win when the price moves against him. So this is the classic winner's curse. Basically, when you win, you don't win the auction, so you make zero and when you lose, you're left holding the bags, basically. So we summarize this in a theorem which is an extreme unraveling. Basically, the slow bidder will always bid zero because there's no point in them trying to bid, they're always going to get winners curse adverse selected, basically.
02:16:03.670 - 02:17:12.220, Speaker E: And the fast bidder always wins, which is of course not what we want to see. Right? So this is not how it actually works in practice because there's multiple fast bidders, right? But if there was one really slow bidder and one fast bidder, I think we would expect to see a huge advantage for the faster bidders. And I don't have it in this presentation because overleaf doesn't do well with gifs. But the GIF that we put out, that was like the sound visualization, you can kind of hear that the integrated builders are much faster, much closer to the end of the auction than the non integrated ones. And I think that's kind of very clearly what this is about. So if you want the model not to completely unravel, you can add this candlestick effect, which for those of you who are familiar with the Polkadot parachain auctions, this is basically what they did. You make the end of the auction uncertain and so people have to bid earlier.
02:17:12.220 - 02:18:19.702, Speaker E: This is actually, if you look at the end of this auction, it naturally is a sort of candlestick. There's some validators who call later than others, but more generally there's just latency effects which determine when the call happens and clock synchronization and stuff. All this stuff kind of contributes to a distribution of possible auction end times. I also think this is really important for those who thinking about mevburn, which is that the auction end time is in many ways not fixed, at least right now. So right now the client is responsible for calling the block and it calls it kind of at a random time. If you do the analysis with this candlestick, then you can get it to not completely. So just a couple more points on that, which are the timing games paper talks about why it might be strategic to delay your block.
02:18:19.702 - 02:19:01.014, Speaker E: And obviously the model was first suggested by Julian in his SBC talk. So what are the takeaways here? Basically, integrated builders have a lot of advantages. One of them is kind of this latency thing that we think of, but even that is not super intuitive because it's highly nonlinear. When you get a small latency advantage, you have a huge advantage due to the adverse selection. I think a lot of people will say, oh, well, you get to observe. You're slightly faster. Well, then you must be slightly better.
02:19:01.014 - 02:19:39.300, Speaker E: No, you're slightly faster. You're much, much better because you get to put everybody else in adverse selection. Okay, that's one point. The other point is that there is this kind of complicated bid shading that has to happen. And if you have to shade your bid early because you have to bid in somebody else's block, then you're at a disadvantage and maybe a big disadvantage, depending on how efficient the PBS auction is. So those are basically the two takeaways, and if anybody has any questions, happy to take some.
02:19:39.910 - 02:19:58.966, Speaker A: Yeah, thank you very much, Max, for the talk. I guess give it a round of applause. So before I actually take some questions from the and by the way, Max.
02:19:58.998 - 02:20:00.106, Speaker D: You can hear me, correct?
02:20:00.208 - 02:20:01.482, Speaker E: Yeah, I can hear you.
02:20:01.536 - 02:20:43.640, Speaker A: Okay. One of the questions I had was when you kind of alluded to the point for something of advantages that integrated builders would have, is there any way, possibly, that non integrated builders could try to counteract with integrated builders? With, let's say, the amount and the quality of private order flows such that they can basically reduce that whatever, .4 numbers of disadvantages that you have mentioned in the paper.
02:20:44.330 - 02:21:42.746, Speaker E: Yeah, I think well, first I'll make the point that if you start entering into contracts with there's obviously optimal contracts here that you can do that will give you similar performance. At that point, isn't the builder just integrated? Right? There's one thing that you can do that would keep you neutral, which is just give more feedback about how the progress of the bundles is within the block. So the builder could submit their block, and whenever they submit a new block as a bid, they could send a little message to all of the searchers that got included, like, hey, you got included. And everybody who submitted stuff that didn't get included could also get a message you didn't get included. You need to raise your bid. And you could even if you wanted to estimate how much they need to raise their bid by. So I think that would be a slight change in favor of the independent searcher, but of course, it would not be perfect.
02:21:42.746 - 02:21:54.800, Speaker E: And then if you start to enter into contracts that make you look similar to an integrated builder, then I would just classify that as an integrated builder at that point.
02:21:55.810 - 02:22:13.300, Speaker A: And I guess just on the extension of that, do you think that there's any, I guess, neutral, non integrated builder in your perspective now.
02:22:17.910 - 02:22:19.140, Speaker E: In terms of.
02:22:19.850 - 02:22:29.690, Speaker A: In terms of the current builder landscape that you see as of right now, do you see that there's any non integrated builders under that definition?
02:22:31.150 - 02:22:52.286, Speaker E: I don't think that there is an optimal contract between from what I know, there's not a specific contract that would make Titan look like an integrated builder in Sexx Arbitrage right now, which I think is basically what you're right.
02:22:52.468 - 02:22:52.814, Speaker A: Yeah.
02:22:52.852 - 02:23:03.250, Speaker E: So I'm not aware of anything like that at the moment. Although I do think that they will probably have to do this eventually to stay competitive.
02:23:04.310 - 02:23:09.570, Speaker A: I see. Got you, I guess. Yeah. Wanted to open up the floor.
02:23:13.350 - 02:23:13.762, Speaker D: Just.
02:23:13.816 - 02:23:35.950, Speaker A: Check if you can hear, can you? Does that mean that we are going to be seeing the demise of PBS?
02:23:41.510 - 02:24:21.578, Speaker E: I mean, I think we're already seeing it. Right. But I don't know would we call that the demise or just this is the end state that this will settle into. Right. It was kind of unreasonable to expect neutral builders to win. Hindsight is 2020, obviously, but if we kind of look back at what system is in place, it obviously benefits the integrated builders. And if you went up to somebody on the street and you said, would you like to order the New York Stock Exchange? Right.
02:24:21.578 - 02:25:05.660, Speaker E: They wouldn't be able to make very much money doing it. A neutral party wouldn't be able to make very much money doing it. But a trading firm like Jane Street would be able to make a lot of money doing it. And so what we're doing here is basically giving priority rights, giving reordering rights, and of course that's going to mean that the trading firms are going to be best positioned to win. So I don't know if I would call it the demise. I think actively we are seeing the consolidation of building power among a few mostly integrated builders and we'll probably see a push towards even more of that in the coming year.
02:25:07.950 - 02:26:17.220, Speaker A: Thank you. There's one more question at the back. Speak a bit louder. Yes. We know that shorter block time generally leads to lower mev in general. What's the implication of your study of integrated I know there are saying we should just shorten block time and there are more than there are a lot of people saying probably we should study longer crop time to make it less of a latency rate. So does your study have any implication on the debate of crop time? And that's my first question and second is what are the other possible solutions maybe outside of the current mev could help alleviate this advantage of integrated builders and this inApple win of.
02:26:18.950 - 02:26:27.880, Speaker E: Yeah, so on the first point, which was sorry, can you repeat the first point?
02:26:29.530 - 02:26:54.378, Speaker A: Yeah, sure. So basically his first point was talking about if the change in the block space could potentially mitigate some of the problems that he sorry, block time. Sorry, block time. Yes, block time can mitigate some of the problems that you have mentioned that's.
02:26:54.394 - 02:26:55.854, Speaker C: The first one cool.
02:26:55.892 - 02:28:07.046, Speaker E: Yeah. So yeah, I think shorter block times would probably be better for I think basically shorter block times means less centralization because you have to carry less risk overall. And so if you look at the size of the wallets of these big sexr bots that are associated with builders, they're holding a lot of inventory and they carry a lot of inventory for quite a long time and they have to do that because the block time is so long. So the arbitrage opportunities are so big that they have to do that. If you look at the interest rate that a 69 wallet probably has to pay their opportunity cost on that capital is probably $10 million or something a year because they get pretty good returns on their capital, right? And they have like 60 million sitting in the wallet. So you have to think about how much interest they could be earning on that. Anyway, this kind of will tend to centralize the system if we have huge arbitrage opportunities that are very long.
02:28:07.046 - 02:29:23.150, Speaker E: So I also don't think the timescale on which this is all playing out right now is starting to become within the last second. So even if we went to 1 second blocks, I think you would see almost no change in the ability of these guys to compete. You might see a little bit more emphasis on relays in the right places. Like you might see more emphasis on a relay in Asia or something, which is an underserved region and you might see a little bit more clustering. But I don't think that long block times are necessarily contributing super like a large amount to the geographic decentralization. I think you already see some issues with some of the rural nodes not being able to hold the auction for as long as they would like, right, and missing slots because of it. So the second point was what can we do about this? I think my view on the end game is we need to move to a system where the block building process happens within the protocol.
02:29:23.150 - 02:30:16.080, Speaker E: Which is not to say that we should actually solve the napsack in a distributed computer, but basically we should have some predefined rules for how the block is built and we should let builders try to kind of compute that optimization problem but ultimately hold to those rules. And that requires censorship resistance as well, which I think is really important. So there's a lot of aspects there. I don't want to take too much time to get into all the weeds, but I think we need censorship resistance, shorter block times. And then once you have kind of censorship resistance in the short term for the chain, you can move this bundle, merge auction onto the chain and enforce it there, which I think is maybe the direction that we'll have to go if we don't want trading firms building our blocks in the future.
02:30:18.370 - 02:30:29.214, Speaker A: Thank you very much. One last question, because we are running out of time. So maybe this is a totally stupid.
02:30:29.262 - 02:30:31.202, Speaker D: Idea, but could we do something like.
02:30:31.256 - 02:31:04.542, Speaker A: A conditional order by other trading firms that are sent to neutral builders so where the builder puts in the transaction and the block additional gets in the last second the price. And then the price determines how that order will actually be executed and traded. So instead of going to integrated, integrated builders open it up for trading firms by allowing kind of late conditioning of the parameters of the transaction just before.
02:31:04.676 - 02:31:07.470, Speaker C: They are executed and developed.
02:31:08.290 - 02:32:21.560, Speaker E: So I guess the actual price model itself is like an enormously sensitive piece of intellectual property and that's what you would need to basically expose to the builder to do this right. You would need to say to the builder, here is my best guess on what the price is. Here is a strategy. Read from that and perform that guess and do it for me. Well, now you've exposed your price feed to the builder, which is very valuable in a lot of cases, right? And also in some ways a risk because now that I know, for example, I don't know, Wintermete was exposing their price process to me, I could go look and I could go try to find where they're consistently wrong and I could try to pick them off on the sex or on chain as well. So I think that's fundamentally an issue with privacy and sensitivity of that price process. Also, not just the binance mid price is a big part of it, but it's not the whole thing.
02:32:21.560 - 02:32:31.660, Speaker E: And so it's a little less simple than just some oracle for binance mid and then you can do everything you want to do.
02:32:33.390 - 02:32:39.530, Speaker A: Yep. Thank you very much, Max. That's it for today. For Max. Round of applause, Max.
02:32:40.270 - 02:33:23.020, Speaker F: Well, hopefully I won't take up too much of everyone's time, but happy to be here. Thank you, yuki Fenbushi for having me. I think I probably know a bunch of you there, but for those I don't know, I'm Blair Marshall, I'm a product manager at Block Native. Been with Block Native for a few years now and focus on our mev solutions, private transactions, order flow auctions, all that good stuff. And today wanted to just dive a little bit deeper into some of the observability challenges that we face at Block Native on showing our users what is happening with private transactions and particularly order flow auctions. Sound good?
02:33:24.590 - 02:33:25.386, Speaker A: Cool.
02:33:25.568 - 02:33:57.858, Speaker F: All right, let's do it. So this is a chart of private transactions since the merge. So I think we're all probably pretty aware that private transactions have grown quite a bit since the merge. They've gone from less than 5% to 15 plus percent now. And the main culprit of that, I guess you could say, is these order flow auctions. So you can see here the red and the blue. The red is mevblocker, the blue is mevshare.
02:33:57.858 - 02:34:40.846, Speaker F: And a lot of this growth happened when Mevblocker and Mevshare came online. So this is back in the April, May, June timeframe. And what's interesting is we have seen this development of users using both Mev share and mevblocker at the same time. That green section. And so I'll circle back on this point, but keep that in the back of your mind as we go through this presentation that users are using both at the same time, so they're sending their transaction to both at the same time. And so with these Order Flow auctions, it's pretty obvious that they're central entities. We're trusting these entities.
02:34:40.846 - 02:35:25.182, Speaker F: When we send our transaction to these Order Flow auctions, we know that they technically have full responsibility of our transaction. It's a signed transaction. They can do whatever they want with it. And so there's a trust relationship there. And it's easy to talk about that, but what does that actually look like in the real world? Can we observe that trust in the real world or on chain, so to speak? And so this is a pretty basic supply chain of what I like to think of as the Ofa supply chain. The transactions are sent to the auction. Auctions share something with searchers, searchers respond back or don't, depending on the transaction.
02:35:25.182 - 02:36:11.774, Speaker F: And then the auction sends the bundles and private transactions to the builder, and it goes through the Mevboost ecosystem. And the particular points that I want to focus on are these trust relationships that we have. So when you send your transaction to the Ofa, you're trusting that auctioneer to handle your transaction correctly. And the searchers, they're also trusting the auctioneer, right? The searchers are responding back with a signed transaction. And so the auctioneer can do whatever they want with that, technically. And so there's a trust relationship there. And then the real one that is interesting to me, or at least for this presentation, is this trust relationship between the Order Flow auction and the block builder, right? So there's some communication happening between the Ofa and the block builder.
02:36:11.774 - 02:37:13.010, Speaker F: And it's really only the Ofa and the block builder that know about this communication. The end users, the people sending the transactions, they have no way to observe what is going on in this arrow here in the middle between Ofas and block builders skip the so our focus at Block Native is to provide as much observability into those trust relationships as possible. And so we started transaction boost. It's pretty straightforward. So Transaction Boost is a RPC endpoint that is an aggregator of other private RPC endpoints. And so you can choose to send to all these private RPCs that you see here on the slide, or you can choose some subset of those RPCs. And then we layered on, on top of that, a status API so that you get real time notifications and updates on your transaction.
02:37:13.010 - 02:38:01.438, Speaker F: And so you can have somewhat of the same experience you have when you send it publicly, where you know what's going on. You can have that experience privately. And so it's highly customizable and we give maximal observability, but this is obviously an ongoing endeavor for us to provide more and more observability. And so we had one particular user recently in the last couple of weeks, they came to us and they said they wanted us to look at their order flow and answer a question for them. And this user, they've been using us for the last 30 days or so, and they've sent us about 5000 plus transactions. And for whatever reasons they have, they only send to mev share. And so I have like a mock RPC down here, so what that would look like for block native.
02:38:01.438 - 02:38:48.958, Speaker F: So you can specify the RPC is mevshare, you can specify some mevshare hints if you want, and you can specify some subset of builders if you want. And so this is sort of what their RPC URL would look like for this particular user. And so they're typically sending to the top five builders. But what was interesting was they said, hey, you can easily check your refunds using our status API. So they received 60 refunds from mev share, but only from four of the five builders. And so there's this fifth builder out there that produced zero refunds. And this fifth builder, they won 20% of these 5000 plus transactions, but never a refund.
02:38:48.958 - 02:39:50.370, Speaker F: And so right off the bat, you kind of think to yourself, that doesn't really pass the sniff test. Something seems odd about that. How can they win 1000 plus blocks but never have a refund? And so we decided to dig in, in particularly looking at these thousand plus transactions, and shout out to Brock at Flashbots. There's the Hindsight library. And so if you don't know what the Hindsight Library is, it is what it says it is, you can check your transaction and look with Hindsight whether that transaction generated any mev. And so we could take our thousand transactions that this block builder won with our users transactions, and we could process them again and check to see if they generated any mev. And so when we did this, when we looped through those thousand transactions, bam, bam, bam, one after another, I wouldn't say one after another, but we kept hitting several transactions that clearly generated mev.
02:39:50.370 - 02:40:45.030, Speaker F: And in all, we were able to identify 19 of those transactions that could have received a refund here. And so I emphasize could because again, we don't have the visibility or the observability into that relationship between the Ofa and the block builder. And we don't know if a refund bundle was ever generated. We don't know if that refund bundle was sent to the block builder, we don't know if the block builder received it, we don't know if the block builder processed it, in their blocks or not. So we don't have that information. All we can say is, hey, these 19 transactions hypothetically should have or could have received a refund. And when we inspected the blocks of these 19 transactions, it revealed something pretty quickly that there was a block back running bot in these blocks.
02:40:45.030 - 02:41:15.474, Speaker F: And I have an example here. So this is the user's transaction. It was somewhere in the middle of the block position 67. They're doing obviously a whale transaction here, large swap between ETH and USDC and then the second to last transaction. So this block back running bot is always the second to last transaction in the block. And they were doing the reverse here on this uniswap V three pool. And they profited some small amount of ETH on this swap here.
02:41:15.474 - 02:42:14.610, Speaker F: So they're clearly back running our user's transaction. And I checked all 19 of these manually and you could clearly see that the block back running bot was claiming this mev opportunity for themselves. And obviously there's no refund produced on these 19 transactions and in total, it was only two e. So we're not talking about huge dollars here, not a lot of refunds from an ETH standpoint, but still very interesting that this is happening on mevshare. And so I would say, astute observers would think, the following point. So, if you understand or know mevshare, mevshare is all about their privacy hints. And so a user selects privacy hints and so they can share the logs, they can share more transaction details for the searcher to search on and potentially backrun it.
02:42:14.610 - 02:43:10.486, Speaker F: And you can be completely private, so you'll only get the transaction hash. You literally know nothing about this transaction. However, if you're a block builder, you need the full transaction details in order to build a block. And so is there some information advantage game going on here where the block builder is vertically integrated and so it gets the full transaction details, but all the other searchers competing for this transaction, they get some subset of those details. And does this kind of break the promises of mevshare and what the user was intending? And so we decided to take a look. This should be viewable, this should be analyzable in the data. And when we looked at this bot's performance, it's a relatively new bot, it's only about 80 days old and it's done 1500 plus transactions in these past 80 days.
02:43:10.486 - 02:43:59.382, Speaker F: And we were able to identify the target transaction that it was back running. And 300 plus of the 1500 were mev share transactions. And of those 300 plus, surprisingly to me, 98% shared at least the logs to the searchers. So you can share the logs that this transaction generates, and most people do, but there is some small percentage that chose to share nothing, so they're perfectly private. So this block builder back running bot had the full transaction details, even though the user was intending to share nothing and 13% shared. You can share the two address call data and function selector. So the majority chose not to share that.
02:43:59.382 - 02:45:05.766, Speaker F: They typically just share the logs. And in total, this is about 16 ETH of profit on these 300 plus transactions or backruns. So it's clear that there is some information advantage here for the block builder because they're running this vertically integrated back running bot. And so when we think about taking a little bit of a step back, the challenges for the current ofas, for those who are nerding out on this and are deep in it, there are a number of challenges that these ofas face. In particular when a user sends their transaction to both Mevshare and Mevblocker. And so one of those that has kind of been known for a little while here is if you send your transaction to Mevshare and Mevblocker, they can invalidate each other's privacy guarantees. And so how does that happen? So on Mevshare, like I said before, the end user chooses the privacy hints.
02:45:05.766 - 02:46:03.674, Speaker F: So you can choose as much or as little privacy as you want, but Mevshare will always share the transaction hash. And so these are all real transactions, they're real transaction hashes and searcher that's watching Mev share will always get the transaction hash at least. And then on Mev blocker, the way the Mev blocker does privacy or prevents front running is they share a bunch of fake transactions. So if a searcher is subscribed to Mevblocker transactions, they don't know which ones are fake and which ones are real, they'll just get a whole bunch of transactions and they don't know which ones are which. However, if you are a smart searcher and you are watching mev share and Mevblocker, you can watch mev share, get the transaction hashes that are always real and then you can confirm which ones are real on Mevblocker. And then if you are watching mevblocker, they provide the full transaction details every time. And so you can line up the transaction hash on Mevblocker and look on Mevshare.
02:46:03.674 - 02:47:38.102, Speaker F: Where there are privacy hints, there are less transaction details and you can use the transaction details on Mevblocker to search on Mevshare. And so a lot of times these ofas they are built a little bit in isolation and it's hard to know or forecast how they're going to interact with each other or how users, when they send to both of them or multiple of them, how that kind of entangles their privacy guarantees. And just of note, for transaction boost, we actually made the decision to if a user decides to choose Mevshare and Mevblocker, but they share or they choose privacy hints, we will honor the privacy hints and we will not send to Mevblocker to invalidate this kind of challenge that they have. And then obviously this newer challenge that I am highlighting here, where the block builder is vertically integrated, can or will provide an unfair information advantage to the block. Builder searcher than to just regular searchers, creating more centralization at the block building level. And so what was interesting with this is that the bot passes all of the value to the block builder and so then it's up to the block builder to a win the Mev Boost auction and then figure out how much or if at all they want to take any sort of profit on this opportunity. And that's because the Mev Boost auction for a vertically integrated block builder is a second price auction versus just the bot is a first price auction.
02:47:38.102 - 02:48:32.030, Speaker F: They can't see what other bots are doing, but the block builder can see what other builders are bidding in order to win the Mev Boost auction. And so it's much easier to extract value at the builder level than it is at the bot level, so to speak. And so that will inevitably create some centralization pressure at the builder level. And then from the end user's perspective, the challenges are what we discussed earlier, that we don't know for sure of those 19 transactions, whether any bundles, refund bundles, were produced at all. So I can send this information to the user. They can decide what to do with this based on what I've told them. But there's no way for me to claim that anything was mishandled at all, because I don't know, and nobody knows, except for the ofa or the builder, whether bundles were produced at all and how they were processed between the two.
02:48:32.030 - 02:49:39.262, Speaker F: And then the other interesting aspect that we don't know is the timing of these events between ofas and block. Does. When did the private transaction get into the block builder? Is there some sort of like pause period where the ofa will pause before sending the private transaction to the block builder to give the other searchers time to search to have the host an auction? Or is that private transaction sent immediately to the block builder and there's no time advantage for the other searchers? We don't have those timestamps. So we don't know. And again, the bundles, we don't know the timestamps of when or if a bundle was created or when or if it was received by the block builder and how that was processed. And so for Block native, what's next? For Observability, we are focused on providing a full platform to help end users and other entities visualize what is going on with their public and private transactions. And so we want to show exactly what's happening with the full transaction lifecycle, whether it's public or private.
02:49:39.262 - 02:50:37.300, Speaker F: Obviously there are limitations, as we've discussed, on what we can observe on private transactions. But this analysis, it was very manual. And so we will work towards automating that and providing more of a real time view of what's happening with your private transaction instead of having to do this manual exercise. And in fact, like I said before, this bot is 80 days old, so it took 80 days for someone to notice this was happening. And we at Block Native think we can do a lot better than 80 days, and then we're going to provide more of these observability features into transaction boosts. And finally, I would just say if you have or have witnessed or have questions on areas of observability that you find challenging, we'd love to hear about them. We're building out this platform to help visualize these, so we would love to understand what gaps you're seeing and how we can possibly help.
02:50:37.300 - 02:50:49.500, Speaker F: So, yeah, that's it from me, Blair from Block Native, and love to take any questions if you guys have any. And definitely FOMOing that I'm not there with you guys in person.
02:50:50.590 - 02:51:19.540, Speaker A: Thank you very much, Blair. And I hope everyone's brains are not too fried yet, but we have a bit more thoughts going, so let's keep you rolling, I guess just hear from the audience if there's any questions on some of the order flow auction stuff that were discussed here or the observability aspects. Anyone?
02:51:21.430 - 02:51:22.226, Speaker E: No?
02:51:22.408 - 02:52:03.662, Speaker A: Well, then I guess I do have one question that I want to throw to you, Blair, which was essentially, I think, banning earlier in the talk before, you also have kind of touched briefly upon the privacy compromises on this sending to two different Ofas for Odoflow. And I think one of the questions that I had was, was there any strict sort of advantages to the people.
02:52:03.716 - 02:52:06.000, Speaker E: Who are performing this kind of like.
02:52:07.010 - 02:52:24.790, Speaker A: Double order flow submission type of thing? So kind of wanted to hear from your perspective after you're looking through all of those data points, that what you think is strictly an advantage of actually doing such kind of like, sending to both of and compromising the privacy.
02:52:25.850 - 02:53:29.500, Speaker F: Yeah, I mean, I think the advantage for an end user is these order flow auctions, in a way, have fractured the searcher market. And so if you're always sending your transaction publicly, then all searchers are participating in this market and they're watching public transactions. But now that we have all these private mempools, not necessarily all searchers are watching all private mem pools. And so as a user, I can understand if you want to maximize your rebate potential by sending to multiple ofas, like, mev share. And now I don't know if people realize, but there are a lot of ofas out there, so there's calibrio even sending to blocksroute, they operate their own Ofa. There's, like, merkel there's, blink. So these are all essentially just fractured private mempools, and they have different approaches to how you rebate the.
02:53:29.500 - 02:53:53.380, Speaker F: So, you know, this is probably a good area of research, is to see whether it is in fact true that sending to more ofas gives you a greater percentage or probability of getting a rebate. Intuitively, it feels like that would be true, and I think that's why people do it. But I don't have hard data to say, yes, or no.
02:53:54.390 - 02:53:55.194, Speaker E: Gotcha.
02:53:55.262 - 02:54:03.814, Speaker A: Sounds good. Anything else from the audience? If nothing else then I guess round of applause to Blair. Thank you very much.
02:54:04.012 - 02:54:05.702, Speaker F: Thanks everyone. Thanks for having me.
02:54:05.756 - 02:54:06.246, Speaker A: Appreciate it.
02:54:06.268 - 02:54:07.000, Speaker F: Have fun.
02:54:07.710 - 02:54:27.022, Speaker D: Hey. Hi, I'm Shabik. I'm from Eigen layer. I do generally do research at Eigen Layer. Thanks for attending this talk late at night. So today in this talk I'll be talking about stakeholder. This is a proof of stake mechanism with strong crypto economic security.
02:54:27.022 - 02:55:40.678, Speaker D: So there are a lot of words there we'll go one by one through this. What is strong crypto economic security means here? So one of the key questions that has come up in our development of Eigen Layer has been that does eigenvear over leverages ethereum or not? It has come up a lot of times people say it overloads ethereum consensus and not. So in order to answer this question, we asked a much more fundamental question. So a much more fundamental question that is ethereum over leveraged in the first place or not? So if you go to the ultrasound money website you will find that the total value logged by ethereum is around 400 $500 billion. Whereas the stake that is securing the blockchain is around $50 billion. So there is almost a ten x multiplier out there. So would you consider ethereum over leverage? Like, at first look it looks like yes, it seems like a lot less security.
02:55:40.678 - 02:56:50.160, Speaker D: $50 billion of security is securing the stake, is securing the $500 billion worth of TVL in the ethereum. But ethereum has never been attacked by anyone, so why is it even secure in the first place? So we wanted to answer that question first and before going to that whether Eigen Layer over leverages or not, second question is can we get unconditional crypto economic safety or not? We'll go to this later. Like what does unconditional crypto economic safety means? So in order to answer whether ethereum is over leveraged or not, we assume a model called economic safety model. So in traditional consensus protocol or any system analysis, there are like two existing models that people have been using. One is the BFT consensus model where people assume that one third fraction at most are adversarial nodes. There is another model where Nash equilibrium model help people think that if all other people are following the policy, should I also follow that policy or not? Generally those two are the models that people have been using. But there's a third model called economic safety model.
02:56:50.160 - 02:57:45.680, Speaker D: In this model that you assume that everyone in the protocol is colluding. So now can we say something about this protocol? If everyone colludes and they corrupt the system, how much effort they have to put in corrupting the system, and by corrupting the system, how much profit they can extract. So that means there are two important quantities that we have to measure. One is the cost of corruption. Basically how much cost it incurs for an adversary to just collude and carry out a successful attack on that system. The next one is the profit from corruption, how much profit it can extract by attacking the system. So if a system you consider a system to be crypto economically safe, if your cost of corruption is greater than profit from corruption, basically the adversary loses money or asset by attacking the system.
02:57:45.680 - 02:58:48.786, Speaker D: So this is the model we will assume for analyzing whether the Ethereum is or any proof of stake protocol. I'm just using Ethereum as an example here, but any proof of stake protocol will be safe or not, is over leveraged or not. We want to show that Ethereum why is Ethereum crypto economy safe in the first place? So to do that, we'll measure, we'll evaluate cost of corruption and the profit from corruption for attacking any proof of stake protocol. So the cost of corruption is pretty straightforward. Basically, it's one third of the stake. Why is it one third of the stake? Because to have two conflicting, finalized block on any proof of stake protocol, like a BFT protocol, you need like at least one third of the stake to double sign on blocks. So that just comes from the BFT consensus protocols and you can slash them.
02:58:48.786 - 02:59:36.446, Speaker D: So at least one third of the stake will be slashed for corrupting the system if they sign on two conflicting, finalized blocks. So that's why the cost of corruption is at least one third of the stake. Now, we want to evaluate the profit from corruption and this is the most tricky part. So we'll go in multiple steps on evaluating what is the profit from corruption here. So the first attempt is that one might just say that the profit from corruption is the whole TVL that is locked into the protocol. So if you take the Ethereum example, the profit from corruption is just all of $500 billion worth of value that is locked there. So how do you do it as an adversary? You sign on two conflicting blocks.
02:59:36.446 - 03:00:40.614, Speaker D: Suppose you have one third of the stake, at least one third of the stake you have captured somehow, and you sign on two conflicting blocks, and now you have a centralized exchange or somewhere which gives you fiat currency in exchange of those tokens. So you're double signing, double spending these tokens. And the problem is, this is not sharp enough. So if this has been the profit from corruption, this is much, much larger than Ethereum's cost of corruption, which is one third of the $50 billion, and there should have been an attack. There has not been any attack yet. So this means this is just a very loose upper bound on the profit from corruption. So can we make this sharper? So we'll make a second attempt how to make it sharper? To understand how to make it sharper, we go to the original concept of what does a finalization on a BFT proof of stake protocol means.
03:00:40.614 - 03:01:45.610, Speaker D: So whenever there is a block that gets finalized, and if there is another block that is getting finalized, then there are like at least one third of the stake is double signing and that means this one third of the stake can get slashed. But the problem is that if these two finalized, conflicting finalized block appears at the same moment, almost close by, like few seconds or some few minutes, then it leads to confusion among the social consensus. What does this mean? Whenever there are two conflicting finalized block, still the consensus has to run. The next proposers have to come and propose a block. So they have to build on top of existing blocks, right? The chain is going, so you have to build on existing blocks. So now suppose there are two conflicting finalized block. Half of the people, honest people might say that oh, I'll build on this finalized block one and the other half of the people might say that they will build on the second.
03:01:45.610 - 03:03:09.590, Speaker D: On the other conflicting finalized block, people are all confused, like what is going on? There is so much confusion in the social consensus here. Social consensus means basically the other honest proposers, the light clients, they are all confused like which is the finalized block? So this results in an ambiguity. But now if you think even sharper, if this conflicting finalized blocks had appeared after a certain time, then this confusion doesn't appear. So for example, if you have a conflicting finalized block right now and there is a finalized block right now, and if there is another conflicting finalized block 1 hour later, the social consensus will not be confused because they know that this finalized block appeared 1 hour earlier. And if there is another conflicting block is appearing at the same height, then this should not be part of a canonical fork at all because it's just coming later and it's just trying to sow confusion into the social consensus. So this time between which two conflicting finalized blocks should not appear to make the social consensus confused is called reversion. We call it reversion period.
03:03:09.590 - 03:04:08.170, Speaker D: So basically the reversion period more formally means that the block is deeply entrenched in the chain and it's already visible to everyone in the social consensus. Now this block will become part of canonical fork even if there is another conflicting finalized block appearing after the reversion period. So it does not lead to any ambiguity. Of course, the people who signed on the conflicting finalized block will get slashed, but there is no ambiguity within the social consensus. And this is the most important term here. So we talked to some of the EF people and they said the reversion period is in the order of there. If two conflicting finals work appear within one after another more than 3 seconds or 5 seconds later, then it does not lead to any confusion among the social concept, which is the canonical block.
03:04:08.170 - 03:05:37.362, Speaker D: So what does this mean? Is that the profit from corruption bound can be bounded to say that the total transaction value, the total value of the transaction that were executed by the clients within the reversion period because these transactions people were confused and now they might not become part of the canonical fork because they got reorged out in the canonical fork. So now the question is can we make this bound even sharper? So we don't have an exact estimate on this profit from corruption, but we wanted to make it even more sharper. So one crucial observation that we made is that there are certain set of transactions, we call it atomic transactions. For example, any swap transaction that you do in DEXs, they are an example of atomic transaction. And in DAX, suppose I'm paying ETH to get USDC from and suppose we all consider like this particular transaction was part of a finalized block. So I have the USDC now, you have the ETH now. Now suppose within the reversion period there is another conflicting finalized block appeared where this transaction is not included, but somehow the social consumer got confused.
03:05:37.362 - 03:06:26.142, Speaker D: But eventually this alternate fork ended up becoming the part of the canonical fork. So in this case you end up having the USDC, I end up having the ETH back. So there is no loss, you just got your USDC back, I got my ETH back, so the state got reverted, so there is no harm that happened. So that's why we say that it adds to no crypto economic load in this case. But there are certain other transactions like hybrid transaction. For example, if you want to exchange your ETH for fiat from a centralized exchange and the centralized exchange, you have to send your ETH to centralized exchange wallet and they'll give you fiat in return in the off chain format. And suppose instantaneously they give you the fiat.
03:06:26.142 - 03:07:09.620, Speaker D: But suppose because of this reorg, the on chain transaction that I paid you in ETH got reverted, so I got my ETH back. The centralized exchange does not have the ETH anymore, but the centralized exchange gave me the fiat also. So it's a loss of the centralized exchange now, so that's a problem. So instead the centralized exchange should have waited until the reversion period before paying me the fiat. So that's the important part. So this kind of hybrid transactions they add to their crypto economic load. So specifically Ethereum is trying to protect this kind of hybrid transactions in the first place.
03:07:09.620 - 03:08:08.610, Speaker D: And this kind of valuation is much significantly lower than $500 billion of TVL that we think that Ethereum is trying to secure, but they are actually trying to secure these kind of hybrid transactions in the first place. Now the question is can we reduce this profit from corruption bound? We believe that this is much sharper, but now we want to have rules in the system. The light clients they should enact these rules so that this profit from corruption bound can be reduced even further. So the way to reduce this profit from corruption is we propose a rule called secure confirmation rule. In fact, we believe that most of the centralized exchanges or bridges, they have this rule. So once I state the rule, it will become obvious. That why I'm saying that they have this rule.
03:08:08.610 - 03:09:18.650, Speaker D: So the observation is that if an hybrid transaction is confirmed only after the reversion period, so in a hybrid transaction has both an on chain part and then off chain part. So for example, the centralized exchange, I send you the ETH, I get the fiat from the centralized exchange. So the getting the fiat back from the centralized exchange is an off chain part. If the hybrid transactions are confirmed only after the reversion period, then they can never be reverted. So what does this say? That you can have a secure confirmation rule where the off chain part of the hybrid transaction should be executed only after the reversion period. So once you do that now your profit from corruption becomes even sharper. So the chain is only trying to protect only those hybrid transaction so the profit from corruption can only from those hybrid transaction that get executed within the reversion period and are basically not following the secure confirmation rule.
03:09:18.650 - 03:10:18.414, Speaker D: So basically some of the light clients might be like in a so hurry that they want to give the fiat very quickly. So now what is the problem? There is an issue with this, these bounds are not observable to everyone, so we don't know. For example, Binance might be executing many of this hybrid transaction very quickly, bridges might be executing many of these hybrid transactions very quickly, without following the secure confirmation rule, without waiting for the reversion period to finish. Coinbase might be doing it, but these are not visible to us. So can we make these things visible to everyone? Can we make these things attributable? So everyone should know that oh, Coinbase is executing this much volume of hybrid transaction without following the secure confirmation rule. This bridge is executing this much amount value of hybrid transaction without following the secure confirmation rule. So these things should be visible and attributable here attributable means that we should know the exact value.
03:10:18.414 - 03:11:11.746, Speaker D: So knowing the exact value gives the exact concrete bound on the profit from corruption because only this much value can be corrupted and can be benefited by an adversary attacking the system. So that's where this new protocol called Stakeholder we propose, basically it's an insurance protocol and to understand this protocol we assume a much stronger model. A stronger model means that we want to give a much stronger guarantee. So to recap, we were in the crypto economic safety model where we wanted a system to be called crypto economically safe. If the cost of corrupting the system is greater than profits extracted from corrupting the system. Now we want to give a much stronger guarantee. Basically no honest user in the system should suffer any loss of the fund.
03:11:11.746 - 03:12:49.950, Speaker D: So this is different from the crypto economic safety model because in the crypto economic safety model even though the adversary might be losing funds but the harm users, they are also losing money. But in the strong crypto economic safety we want to ensure that an innocent user, even if it's trying to get harmed by an adversary, should not face any loss of funds. So it's a very stronger guarantee we are trying to achieve from this stronger guarantee. We can understand the intuition that what does this system should do basically whenever in strong economic crypto income sad if the no honest user is losing any loss of fund that means they should get compensated proportional to the harm caused to them by any adversary. Now, what does this mean? Users should commit to the amount of compensation they should need and basically if you go into your day to day life it should ring alarm ring you about insurance. Like in insurance, whenever you buy a car insurance or a medical insurance you say that I want to get this much insurance amount if I ever get injured or if my car gets wrecked or something like that and you buy it beforehand right before you are getting harmed. So similar to that here, users should commit to the amount of compensation they should get paid if ever they got harmed.
03:12:49.950 - 03:14:11.030, Speaker D: So once they state clearly, right as a user you know that how much compensation you want. When you buy a medical insurance you specifically tell everyone that this is the compensation that you want. So in this case the user knows its own value and will tell what is the compensation that it wants and that sets the value for the profit from corrupting that particular user, profit from harming that user. So now if everyone does that so you get an upper bound on the profit from corruption and it's much more tighter. So what are the key ideas on this design? Basically, whenever there is a fund like in ethereum today's ethereum or any proof of stake protocol, whenever staker who has double signed gets slashed, the slash funds get burned. So instead of getting burned, that slashed money should be used for allocating through the insurance should be used for compensating the harmed users of that blockchain. And who should we allocate the slash funds to is basically those users and those users would have bought the insurance beforehand by stating what is the insurance amount they want if they ever get harmed and when to sell insurance they have to buy it beforehand.
03:14:11.030 - 03:14:45.026, Speaker D: It's just like in day to day life when you do the medical insurance or car insurance you have to state it beforehand. You don't buy insurance after you have gotten harmed. Right now you also want a rational transactor policy basically you want the rational users to hold enough insurance so that it's greater than their fund that is at risk of getting harmed, like the amount of harm that the system can do, they should buy insurance more than.
03:14:45.048 - 03:14:45.620, Speaker C: That.
03:14:47.350 - 03:15:42.102, Speaker D: And it's a self scaling security basically. Now if more and more people want to buy insurance, you have to give premium, right? When you buy a car insurance you give premium each year or each month. So basically if more and more people wants to buy insurance, they will give a premium which means that more stakers will join the system because it's additional API on top of the gas fees that they are getting anyway from the attestation fees or the proposal fees. So it's on top of that and you also want to avoid a grieving attack basically you don't want people like people can buy insurance for themselves and then they harm themselves also. So it's a pseudo anonymous economy. You don't know people are self inflecting harm on them to fraud the insurance system that happens in real world. So we want to avoid this kind of grieving attack.
03:15:42.102 - 03:16:33.814, Speaker D: So what we do, we burn a small amount of fund itself so that if it repeatedly wants to do, the adversary would eventually end up losing all of its money at certain point. So this is the key idea on the design. So now if we want to compare with traditional insurance, how is it different from traditional insurance? Basically? So there are like certain differences in the traditional insurance. It is statistical. For example, in car insurance they always put an estimate that say 1% of the people might apply for car insurance, their car might have gotten wrecked. So they always put that much amount of money at any time to pay out at least 1% of the car insurance. They don't have all the money for paying out.
03:16:33.814 - 03:17:19.720, Speaker D: Like if all 100% of the people who bought insurance got their car wrecked, the insurance people won't have enough money to pay the compensation. So it's very statistical. On the other hand, in the stakeholder protocol it's not statistical because people buy the money that they are buying. Remember the rational transaction policy, the people who are buying the insurance, it should be greater than the fund that they risk losing if there is a problem and the money that they are getting is from the stakers itself who can harm them. So it's deterministic. So there is nothing statistical going on with stake share. Second is the closed system.
03:17:19.720 - 03:18:00.462, Speaker D: In traditional insurance the insurance company don't come and just damage your car or affect you to a disease. It's something like someone else. The insurance companies are not the people who are harming you, they are just for therefore paying you insurance if your car is getting wrecked sometimes or your health is down. But in stakeholder, the people who can harm you basically the stakers of the proof of stake protocol are the people who are paying you insurance. So it's a very closed system. Third, there is no moral hazard. So this insurance policy is for the safety attacks, double signing.
03:18:00.462 - 03:19:11.500, Speaker D: So in stakehore, you know, in an objectively attributable manner that whether a double signing has happened or not in traditional insurance you don't know because many people will try to fraud the system by saying that my car got damaged or something, whereas they might have just gotten damaged their own car or something like that. But nothing like that can happen in stakeholder because double signing is visible. Now, there is no grieving because some portion of the fund gets burnt. So you can't keep grieving repeatedly. At certain point the adversary will run out of the money that it's putting. So here grieving means that the people who is putting the staker, who is behaving maliciously, is also buying insurance from itself and if certain amount of fund is getting burnt every time when the insurance is paid out, then at some point in future it will just run out of the money. But in traditional insurance you can do these grieving attacks so that's the difference.
03:19:11.500 - 03:20:17.530, Speaker D: Now, there are certain emergent properties in stakeholder that appears first is the self scaling security because you just have to buy an insurance which is greater than the fund that it's at risk. So you have to state what is the cost of profit from corrupting your fund will be what is the fund that is at risk? So it's pretty self skilled. So the crypto income security is figured out by the protocol by itself. Second, pricing the right dimension. So in today's the staking yield it is done assuming that the demand for computation is much greater than the computational that is being offered by the system. So basically the EVM execution but in an ideal world, like in today's web two world, the computation is very cheap, right? The AWS computation is significantly cheaper than doing the blockchain computation. But at certain point, you might want to have an infinite or very cheap computation.
03:20:17.530 - 03:20:55.494, Speaker D: And in that case, the gas price from computation will go down. So in that case, with an insurance policy, you can pay the gas prices, so it becomes an additional yield rate. So this separates out the dimension of computation and security. The third one is you can have insured light clients currently. There is no insured light clients. But light clients can buy insurance to pay if they are executing the hybrid transaction without waiting for the reversion period. And the fourth one is secure.
03:20:55.494 - 03:21:44.454, Speaker D: Bridging so you might have noticed in vitalik article. He says that you can't build secure bridging because of 51% attack. So the fund that was breached to the other side might become unbacked from the sending chain. But with stakeholder you can have a secure bridging because if any, at any time there's a Riog or something, the breach can just get an. Insurance fund to back up the wrapped asset that is on the receiving chain. So these are some of the emergent properties and one way to imagine the mental model is that previously blockchains were used to be things like bank lockers and Validators can steal all the money if the safe is broken. Now the new mental models like the blockchains are like FedEx and Validators can only steal the value in transit.
03:21:44.454 - 03:22:15.480, Speaker D: Now, when you send any package in FedEx or Ups or there are a lot of other package transit services, you have the capacity to buy an insurance for protecting your package and you declare that how much value of insurance you want to buy. So it is something like this is the new mental model with the insurance on proof of stake protocol will be and that's all from my side if you have any questions.
03:22:18.250 - 03:22:19.240, Speaker B: Thank you.
03:22:23.530 - 03:22:52.410, Speaker A: I think just kind of like clarify, because you have talked a lot of sexual in a more abstract level, but in essence, you're kind of like packaging what already you have with Eigen layer and then kind of offering. As a packaged product, which you kind of call it like an insurance product, but in essence, you're sort of still doing just the restake. But for, I guess, more in a concise.
03:22:52.490 - 03:23:10.520, Speaker D: Yeah. Originally we didn't propose stakeholder from a restaking perspective but from can you have this insurance directly in the Ethereum consensus itself or not? But we are planning to include that in the Eigen layer itself also.
03:23:10.970 - 03:23:49.170, Speaker A: I see. Thanks for the talk. So, just a couple of questions. The first is that you most start off on those framework about different types of profit, different types of cost. And I was expecting that you will give some quantitative analysis results showing that, for example, for Ethereum or for Eigen layer, how unique within different models, how quantitative things look like. Would you say something about that? That's kind of a suggestion that okay, so the system probably is not secure without them. So that's why you need to build them.
03:23:49.170 - 03:23:56.674, Speaker A: So that kind of seems to confirm some of my prior that they're restaking it back to their order leveraging. So could you comment on that? Yeah.
03:23:56.712 - 03:24:03.186, Speaker D: So the first question, the actual value we don't know because suppose Coinbase or.
03:24:03.208 - 03:24:06.618, Speaker A: Binance, sorry, but you have these the.
03:24:06.624 - 03:24:11.866, Speaker D: Other two models, what is the other two? You mean the I think you have.
03:24:11.888 - 03:24:16.540, Speaker A: The profit, you have three models. Yeah. Third is hybrid. The other two I think you can.
03:24:17.230 - 03:25:08.970, Speaker D: So the first one doesn't work because $500 billion, that's just the TVL of Ethereum. So if that had been the profit from corruption there would have been an attack on Ethereum which has not happened yet. The first one, most likely at least Anecdotally, has not happened yet. The second one, the model is about the atomic transactions and the hybrid transactions. So hybrid transactions we can't calculate because no one know, like Bridges or the exchanges who execute the hybrid transaction, they never broadcast that what is the hybrid transaction value they are executing within this reversion period. So that's a problem. Unless we go and talk with these exchanges, we'll never know this number.
03:25:08.970 - 03:25:24.260, Speaker D: And even if they give the numbers, it's hard to verify that whether these numbers are exactly correct or not, because they are not never declaring that. It's very hard to verify those things. So that's why we don't have the exact numbers.
03:25:25.190 - 03:25:28.702, Speaker A: It will be interesting, like you inject some of your estimates.
03:25:28.766 - 03:25:52.380, Speaker D: I see how there is. I agree. Yeah, I agree. So this is where maybe it's hard to do that experimental analysis to get those numbers. But I see what your point to have those numbers, it will be much easier to appreciate what is going on. And to answer your second question.
03:25:55.470 - 03:25:55.786, Speaker A: It.
03:25:55.808 - 03:26:44.438, Speaker D: Does not says anything about that. Eigenve is over leveraging the consensus model. It's more like if there is so much space between profit from corruption and cost of corruption and you can have insurance model to even make it better. So if there is so much gap between that, then having Eigen layer, it just does not cross that gap. It just says that because there is this one estimate that we had is that we talked to certain people who had some idea about the exchanges. They said that generally exchanges don't give you Fiat immediately. Like if you do exchange on coinbase, they don't give you the Fiat immediately.
03:26:44.438 - 03:27:28.440, Speaker D: So there's this time they always wait, they follow the secure confirmation rule. But in certain cases, like Bridges, they don't want to wait for a while, like 1 hour, 2 hours to transfer the money. So in those cases they can just buy the insurance payout. And so that means that already the profit from corruption is very low. And with Eigen layer you are not like crossing the cost of corruption, which is the 50 x $3 billion. So that is the thing. Does that answer your question? I guess having exact numbers will be much more helpful that whether it's cryptography safe or not.
03:27:30.970 - 03:27:43.790, Speaker A: I can just ask the question. It's just a clarification. So in this architecture, the reversion period is what would be like an anecdotal definition of what the reversion period could look like for this kind of system.
03:27:43.940 - 03:28:29.020, Speaker D: Yeah, when we presented this to certain folks from Ethereum Foundation, they said that we are thinking that this reversion period will be in the order of tens of minutes, like half an hour. Like say Coinbase, it will wait for half an hour before giving you the Fiat. But certain folks from EF, they mentioned that no, it could be in the order of seconds also because suppose you have a conflicting block right now and 5 seconds later you have another conflicting block finalized by the same height. Then by that time, social consensus has already made up its mind that the first one should be the part of canonical four. And because you might have seen that whenever a validator gets slashed, twitter is very active. Oh, this validator got slashed discord everywhere. It's like people have already noticed it.
03:28:29.020 - 03:28:44.130, Speaker D: That's the social consensus that people never talk about. What is the power of social consensus? We keep saying that blockchain, blockchain. But it is the coordination among the social consensus. And we are trying to leverage the power of social consensus for making the system strongly crypto economic city safe.
03:28:45.190 - 03:28:57.086, Speaker A: All right, I think that's it for Sufik. And thank you very much. It's good to see you again, Tony.
03:28:57.118 - 03:29:10.940, Speaker C: By the way, I am now about 4000 km away from where we met a few days ago. Yeah, I actually just flew back from Istanbul and I am now back home in Los Angeles. And if you can't tell from.
03:29:12.670 - 03:29:12.986, Speaker B: My.
03:29:13.008 - 03:29:20.330, Speaker C: Eyes, I'm extremely tired too. So I'm right there with you guys in spirit and also in time zone, plus a bunch of jet.
03:29:25.390 - 03:29:26.054, Speaker A: To.
03:29:26.192 - 03:30:24.682, Speaker C: I would love to just dive right into it. Bear with me 1 second. Let me share my screen. So the overall theme of what I wanted to talk to you about today, and I hope that this can be a very interactive presentation too, because I think when I look at the attendees list, I see a lot of extremely technical people and that's very exciting. It's my favorite type of presentation. So I'm actually going to try to go real fast through the first bits because I think a lot of you are very familiar already with the overall ideas and the designs behind how mev works. One of the things that I want to stress, though, in the very beginning is as I've been talking to different groups, and as I've been sharing a lot of what we're working on at Fastlane, I run into two very distinct sets of individuals.
03:30:24.682 - 03:31:08.334, Speaker C: I run into people that come from an account abstraction background, and I run into people that come from an Mev background. And the interesting thing here is that they both have a very different definition of the word bundler and they have very different definitions of the word operations. And so, for all intents and purposes, no pun intended, whenever I say a bundler, I am not referring to an Mev bundle necessarily. I'm referring to an account abstraction bundle. And we'll get into that in a little bit. But when I refer to Operations, operations had a lot of different names over the years. These have been called like signed messages, metatransactions intense, but not really gasless transactions 712 but these are all the same thing.
03:31:08.334 - 03:31:41.190, Speaker C: And what it really comes down to is a UX improvement where the user signs a message rather than a full transaction. It's a way to handle things efficiently on the front end. When it comes to mev, there are two different types of mev. There are a lot of different ways to classify mev. So there's many different ways to split up the types of mev. But the two types that I'm particularly focused on in this presentation are exogenous and endogenous mev. Exogenous mev would be mev created by something that happens off chain.
03:31:41.190 - 03:32:33.210, Speaker C: Think the binance rates change, something where the initiating factor, the thing that creates the value, is something that happens outside. This could be even something as simple as the block timestamp increasing, which is fundamentally set by the proposer by the builder. Endogenous mev, however, is something that's created on chain, typically by a transaction. Some examples of endogenous mev would be an Oracle update that lands on chain, which can then lead to a liquidation being the mev capture. A large Dex swap could be something that creates a high amount of price impact, which could then lead to a Dext atomic arbitrage or a new token mint being triggered could lead to token sniping. These would all be examples of endogenous mev. And that's what I'm primarily going to be focused on in this case.
03:32:33.210 - 03:33:11.606, Speaker C: Not the exogenous like the binance cross chain stuff, but the endogenous stuff. So you guys are all extremely sophisticated. I can kind of breeze past the majority of the mev stuff and get straight to the juicy parts. But the general idea being that builders are in control of mev because builders get to sequence transactions, but account abstraction is changing things. It's a little bit different with account abstraction. What we have are these operations, these user apps, and we have this entity called the bundler. And again, like I said before, different from what you all are used to in the mev world.
03:33:11.606 - 03:34:22.186, Speaker C: The bundler here is tasked with taking operations and putting them into a transaction which they then send on to the builder or the validator or the proposer, et cetera. Currently right now, when you're looking at user operations and the mev extracted from these user operations, the majority of it is still going to the validators. Because when these user operations generate mev, the mev created by this, the endogenous mev is typically gobbled up by the transaction that follows the transaction that included inside of it the user operation that created the mev. But this is somewhat inefficient. This is somewhat inefficient and it's not perfect. But the reason why, and this is a really key thing here, the reason why it all goes to the validator still to the proposer, is because the validator still controls which transaction goes in the slot that follows the transaction that had this profit creating, this mev creating operation. But there's nothing that says that the bundlers can't put more than one operation in a transaction.
03:34:22.186 - 03:35:39.698, Speaker C: In fact, there's some very strong gas incentives to do actually multiple operations inside of a transaction. And so what we can see in the future, and we're starting to see this now, actually, is that we can have an endogenous mev creating user operation and then we can have an endogenous mev capturing searcher operation or solver operation, whatever name we decide to go for it. I know this is like the searcher solver thing is one of those words that likes to keep changing its definition or its name every so often too. But what this means is that for endogenous mev, the vast majority of it going forwards is going not to be captured by the Validators or even by the builders necessarily, but it's going to be captured by the bundler. Because before where we had this environment where it was the Validator proposer indirectly visa vis the builder that could control which transaction goes into the valuable slot. Now all of a sudden they don't have any power at all. Because it's the bundler who can put the operation to capture the mev behind the operation that creates the mev.
03:35:39.698 - 03:36:53.434, Speaker C: Lock it all up into a singular transaction using the private key and then the builder, validator, et cetera, can't really do anything about it without a quantum computer and cracking the private key. But there's quite a few big ifs quite a few big exceptions to this and we're going to get into that a little bit because I'm sure many of you guys are thinking as to why that might not be the case. In order for that to play out the way that we're expecting it will be, we have to control the mev starting at the very tippy, tippy top of the mev supply chain, which is to say the front end. This isn't even like at the DAP, this isn't even at the smart contract level for the DAP itself. We mean like straight at the very beginning at the front end at the call data creation level, when the user is going to the front end logging into the website before they even sign their operation, which is well, before the wallet even propagates it to any sort of RPC. Before they even sign the operation, we have to know what they're signing, which means their call data has to be generated based on the inputs that they put into the front end. That is where the mev will be captured using account abstraction.
03:36:53.434 - 03:38:18.854, Speaker C: And it's all going to happen upstream of every RPC, wallet, relay builder, et cetera, as long as the front end has the ability to enforce bundling privileges. And the reason why that's a big if a big conditional is because EIP 43 37, the account abstraction EIP actually is built around this premise of bundling being permissionless. With EIP 43 37, you have this public alternate mempool. And what this public alternate mempool does is it allows anyone to be a bundler of operations. But when anyone can be a bundler of operations, the downside is that, well, who gets the actual privilege? Who gets the right to actually bundle the operation? The answer is if anyone can be a bundler, then it's whoever's going to extract the most value that will actually end up bundling the operation because they'll be able to pay the builder the most, which the builder can then pass on to the proposer the most. And then thanks to PBS, which is a system that really encourages and incentivizes maximum extraction, by definition, we're going to run into a situation where if anyone can bundle the operation and if everyone can see the operation, then over can extract the most will. But we can bypass that using smart contracts.
03:38:18.854 - 03:39:20.138, Speaker C: We can bypass that by using a non compliant EIP 43 37 spec and we can handle all of these guarantees at the smart contract level. That's not to say that builders aren't still important. Builders are still extremely important. Builders have access to information. They have the ability to see what transactions there are in front of this single transaction of bundled operations. And again, I have to be very careful with how I phrase this because I lose a lot of mev people when I'm talking about this. The general idea being that a single transaction of bundled operations not to be confused with an Mev bundle builders do still have a huge role to play because the order of transactions that happen in front of this single transaction of bundled user operations someone should come up with an acronym for that impacts how effective the operation is.
03:39:20.138 - 03:40:47.334, Speaker C: It's also going to become extremely complex once we start to look into the fact that a lot of these operations are intense and intent based. And what happens if the builder is also the party that was trying to fill the operation? Like, what if the builder just happens to be one of the prop shops that's like Winter Mute or SCP that's acting as the two main builders right now? So it's one of these situations where there is still a huge value to play and there's still a lot of considerations made for builders that has to be sort of like built around. And it's one of these things that it can be handled, but it has to be handled very explicitly and very delicately. And I think that's actually, and I'm going to get into this a little bit, that's one of the things I'm most excited about swab for. One of the other neat things about having the builders as bundlers is not only can they really handle guarantees at a better way than any other sort of bundler and more accurately handle the gas cost risk and the gas elements, builders can also handle the risk advantage. So if they, for instance, need to do any sort of eigen layer related collateral guarantees for bundling operations, then they can, they can do it knowing that it's riskless. Like we were talking before in the previous presentation about insurance, well, insurance is something I actually happen to know a pretty decent amount about.
03:40:47.334 - 03:42:05.506, Speaker C: And it's one of these things where with insurance you have this huge adversarial element to it, right? You always want to assume adverse selection, like maximal adverse selection. And the neat thing with having builders as bundlers is that the builder actually has a huge information advantage with things like insurance and posting collateral, because the builder is the only one that can post collateral risklessly if they are bundling operations that require collateral posted. Everyone else has to worry about adversarial builders, whereas the builders themselves don't, because they are obviously not going to be adversarial with themselves. They can also do last minute, just in time, collateral posting as well, which I think is something that we're extremely interested in at Fastlane. They also have speed advantages, local access to the intra block state, which means they can do two, three, four transactions deep, then simulate the state and see what the transaction would look like in that fifth slot. They can bundle the ops during block construction, and they can also are best positioned to handle the knapsack of knapsacks problem. Each of these single transactions that is composed of a bundle of operations is a knapsack, right? You've got different operations that you have to compile together, and then each of those becomes a transaction, and then you've got different transactions that you've got to compile together.
03:42:05.506 - 03:43:07.314, Speaker C: And so we have a fundamental problem. And the thought of trying to do this over some sort of TCP IP connection and introducing network delays, I'm not going to say it's impossible, but I think there's a reason why we're seeing so many GPU leaning chains pop up to handle this exact kind of thing. There's also a cost advantage extra gas. Using extra gas used during bundling is only as expensive as the block's least profitable transaction. And so if, for instance, the bundler is trying to put in some of its own stuff or to handle some extra security checks in order to capture a lot of extra value, then all it really has to do is just if it's the one paying its own gas cost, then it's really only just kicking out the transactions at the very bottom of the block. The block is the least valuable transactions because it is paying indirectly itself. And so there are some very interesting dynamics.
03:43:07.314 - 03:44:10.170, Speaker C: It's also the lowest requirement for on chain solving. And so we do think that builders have a huge advantage as bundlers, but not always, because there's a distinct rent element of adversarial building that we have to be extremely careful of. So what are some of the secondary effects of account abstraction? The good is that private order flow will be less centralizing. Endogenous mev is mev created by transactions or operations. And account abstraction means that pretty much, I would estimate, within the next five years, all endogenous mev will be captured this way. It's trustless, it's faster, it's more gas efficient, it's smoother, it's cleaner, there's really no reason not to. And it's one of these things where all of that is really great at the surface because it does solve the vast majority of the centralizing problems of order flow.
03:44:10.170 - 03:45:09.710, Speaker C: But it also creates some new problems. Unfortunately, the good problems, like the things it fixes, is that we get mev protection for unsophisticated users. For instance, any DAP that wants to can have any user, not the sophisticated ones that know how to install a custom mev blocking RPC, which like, let's be honest, the people that know how to install an mev blocking RPC aren't really the type of users that generate a lot of mev to begin with, right? Like the type of people who get sandwiched would be like me the very first time I ever used DeFi when I didn't even know what mev was. What account abstraction does and what this improved UX does for DApps is it allows DApps to give this protection to all of their users regardless of if that user has ever used DeFi before. It also reduces the reliance on centralized infrastructure. Censoring the user operation also means unfortunately, we have to censor the solver operation as well. They're fundamentally tied together.
03:45:09.710 - 03:46:07.600, Speaker C: The value creation and the value capture are fundamentally linked. And so we can put some of these transactions straight into the public mempool and that's great, that's great. And that also leads to a greater decentralization of validators for reasons that have to do with smoothing of the mev just by nature of the fact that there will be less mev to begin with and therefore less volatility. The bad news is that new centralized infrastructure may and probably will emerge. These operations still need ways to communicate with each other. And while EIP 43 37 is building a public mempool like a public alternate mempool for operations the problem is that it's going to suffer from all of the same problems that the current public mempool suffers from. Which is that if everyone can see it, if all the searchers can see it, then the bundler will just be whichever bundler can extract the most value.
03:46:07.600 - 03:46:45.174, Speaker C: The other thing is that when you start capturing mev at the front end level, that is a huge red flag for regulators. That's probably the worst possible thing you can do if you want to aggro the SEC. And so it's something that we have to be careful of. So this isn't just like speculation. A lot of this already exists in slightly diminished forms. For instance, we see uniswap x it's on centralized infrastructure now, but it'll be decentralized soon. Bundler privileges are enforced via permissioned whitelist.
03:46:45.174 - 03:47:45.150, Speaker C: Right now it's just market makers, but the overall idea is there. The proof of concept is still valid. We see things like wall chain, which is where there's not really any searchers or solvers, but there's just this central smart contract and the front end just routes all the users straight to this central smart contract that just auto backruns everybody. It misses a lot of value, but it's a very smooth UX. Then we've got our relay on Polygon, the fast lane relay on Polygon where all of the mev is searcher operations that are executed by our smart contract, which is an entry point which verifies that they paid what they said they were going to pay, it verifies that their bid was paid, otherwise it reverts their entire transaction. And then this one's actually my favorite, I mean, obviously mine is my favorite and the one that we're currently working on is my most favorite of all. But of these four, I think one of the neatest ones is API Three's OEV, which is where built into the API Three smart contract itself.
03:47:45.150 - 03:48:47.470, Speaker C: You have this auto back running system where searchers will basically get the Oracle updates and then they're bonded. I'm not fully sure how it works, but the searchers are auctioning and they are competing to backrun the Oracle updates. And that is at the smart contract level for API Three's smart contract, which is very fascinating in terms of PBS specific impact, because I saw that this was a misalignment. I wanted to make sure I took a swing or two at PBS before I start talking about some of the cool things. So one impact on PBS, I think probably the biggest one is that there's this misperception that PBS is necessary for Ofas. I've heard this repeated by multiple people and I couldn't disagree more. In fact, I would go so far as to say that PBS and Ofas are antagonistic.
03:48:47.470 - 03:49:50.040, Speaker C: I think Blair Marshall from Block native presented I think like two or three slots before me, and I think he went into a lot of the reasons why with some great examples as to why that's the case. But at the end of the day, you don't need PBS in order to do Ofas because Ofas can be handled at the bundler level. In fact, you can do so trustlessly rather than with trusted builder relationships, which by the way, it's kind of interesting that the Ofas with PBS are required to use trusted builder relationships, which is really fascinating because the builders are directly adversarial, the more they can extract, the more profitable they will be. The more profitable they are, the more likely they are to win the block. That's literally how it's designed, right? And so you've got this you're trusting a party, but it's like a definitionally adversarial party. And so it's a very interesting dynamic. It's one of those things where we see it play out in tragfi all the time.
03:49:50.040 - 03:50:50.220, Speaker C: But we can bypass all of that with account abstraction ofas, we can have smart contract guarantees over who can act as the builder or sorry, bundler the account abstraction bundler, and more specifically, even the user. The user can bundle their own transaction. The user can collect and aggregate these searcher operations and bundle them himself. And it's one of these scenarios where it's an extremely just better system. It's using smart contracts. It's using blockchains the way the blockchains were meant to be decentralized and trustless and we're not having to rely on prop chops in order to handle this because there's not really these adversarial parties in the short term. I think the biggest impact on PBS will be that we don't need PBS for Ofas and I think that that is a great thing.
03:50:50.220 - 03:51:52.106, Speaker C: I think the huge downside of this DAP value accrual is the fact that all of a sudden censorship becomes a very optimal strategy. You can see I made a little modifier right here. If Hayden over at Uniswap were to take this modifier and just plop it right down into the Uniswap V four code base and put it right in front of the swap function, then all of a sudden we would have a fully centralized builder market and there would be nothing that anyone could do about it. And because he would have not, he like Hayden would never do this. He's an extremely decentralized person. But if someone were to do something like this after having cornered the swap market, then they could censor all of the other competing DEXes very easily and it would be in their best interest to do so. And it's hugely concerning.
03:51:52.106 - 03:52:24.550, Speaker C: And it's particularly concerning because of the fact that the value isn't really coming from the smart contracts themselves, nor is it coming from people who know how to read smart contracts. All of the value here is coming from the order flow which is coming from the front end. And so we can't really rely on this. Oh, DFI would never let this happen. The social air would never let this happen. It would be great if we could. I hope that we can, but maybe I'm a little bit suspicious, but I genuinely am concerned that like ten years from now something like this might happen.
03:52:24.550 - 03:53:41.134, Speaker C: Or we might have specific builder requirements put into swap contracts which will inevitably lead to a dominating monopolizing DAP to then start censoring their competitors, which will then force all of that competitor's web traffic to go to the website of the dominant swap builder. And that's how I think Ethereum and just blockchains in general become fully centralized as a result of PBS. But I think EPBs can actually fix that and we'll get into that a little bit. So what are we working on over at Fastlane? Well, we're basically building something that'll combine all this stuff that we've talked about and the general idea being that we are building a system called Atlas that allows any front end to monetize their DeFi traffic either through intent fulfillment or mev. Either way, in a way that improves UX. Like we literally are measuring clicks because that's literally what the DApps that's literally what the front ends are measuring. And we're doing this in a way that improves UX, decreases the regulatory exposure and bypasses the solve or cold start problem by plugging into.
03:53:41.134 - 03:54:20.562, Speaker C: Our existing network of operations or operation based searchers and solvers. I've been talking about this for months now and I still don't know if I should call them searchers or solvers. I imagine the Flashbots people are eventually going to make me start calling them Geekers, which cracks me up. But that's the general idea of what we're working on. It's a modular, easy integration for DeFi apps, permissionless. It works backwards, compatible with all of the existing apps. For instance, it works for all of the chainlink mev, it works for uniswap backgrounds, it works for uniswap X, it works as a uniswap router, it works as an anti LVR hook pool.
03:54:20.562 - 03:55:06.058, Speaker C: It can do like everything. And all it does is just slap an auction on top. That's all it does. You can turn anything into an auction. And in this sort of market, just like Blockchain in general, the ability to turn anything into an auction is extremely powerful when you can use account abstraction to have explicit smart contract guarantees over who the beneficiary is. And the reason why we love this so much is because it allows us to take so many of these options that right now are giving the value to parties that quite frankly, maybe shouldn't be receiving it. And it allows us to, using smart contracts and pressless logic, redirect that value back to parties that should be receiving it.
03:55:06.058 - 03:56:09.722, Speaker C: And we think that's a very good thing. The way we do it is by using hooks. If a DAP or a DeFi app or a front end wants to use Atlas, all they do is they create a new smart contract, the very straightforward smart contract. If you've looked at the Unitswap V Four hook system, it's very similar to that, maybe slightly more lightweight, but not by much. The general idea being that the DAP can choose what happens before the user operation. And then what we do is we take these solver operations, these fulfillments, these searcher ops, and we allow hooks before the solver op, hooks after the solver op, and we do that all inside of a try catch. So if, for instance, we're looking at a swap intent, which is probably the most easy example for a swap intent using Atlas, the way it works is that for the user operation, all we do is we just verify that the user operation, that the user has their underlying tokens for the presolver hook.
03:56:09.722 - 03:56:59.566, Speaker C: What we do is we transfer the solvers or we transfer the user's tokens to the solver. Then we run the solver's operation. Then for the post solver hook, we verify that the user received what the solver promised they would receive. And if the user didn't, then we revert that entire solver up as well as the pre and post solver hook, and then we loop back and we continue the try catch loop. Now, this is arguably not the best or not the most gas efficient way to handle block space and we'll get into that in a little bit. But the important thing here is that the solvers do have to escrow their gas ahead of time, otherwise this would be a huge attack vector from solvers against bundlers. This also allows the hooks to allocate the value however they want to.
03:56:59.566 - 03:57:35.930, Speaker C: One of the neat things here is that DApps can run these auctions in any sort of currency they want. If it's a swap intent, they probably are going to want to run the auction just in whatever currency the user is trying to buy. In a sense it can be seen as like an RFQ. But let's say for instance, this is Chainlink as the user and this is a Chainlink Oracle update. Then these solvers could be competing in this auction using Link as the currency to bid up in order to compete for back running these chainlink Oracle updates. Which by the way this all works. Thank God Chainlink doesn't use TX origin which is the one thing that breaks Atlas.
03:57:35.930 - 03:58:54.202, Speaker C: And so with that like you could use Atlas to create a on chain series of hooks that capture all of the mev from all of the chainlink Oracle updates in a trustless and decentralized way that doesn't require any sort of RPC based ofa we can plop it down straight in the mempool. And the other key thing here too is because we can't assume that builders are friendly, we have to assume that builders are hostile. What that means is that we can't just use one solver operation because what if the builder is the solver? What if it is a swap intent and what if the price did move against the builder and they wants to get out of their fulfillment? Well, you could do what most people do and just whitelist the builders and have trusted builders and sign agreements off chain. We don't like that though. And so what we've done is we've made it so that it's extremely cheap to cancel a solver operation for the solvers. Not perfectly cheap, it's not free, but it's cheap ish. But even if one solver cancels their operation, then there's still the second highest biding solver and then the third highest bidding solver.
03:58:54.202 - 04:00:00.680, Speaker C: And so as long as all of the builders don't collude, then we'll still have a solver that is coming out on top, even if the builders are the solvers and the builders are trying to get out of it. So it's a really fascinating approach and it is exactly mimicking how we are currently running the mev operations on Polygon. This is also, by the way, why we're so excited about swap because the biggest criticism of Atlas, which is our system, is that it is an inefficient use of block space, which is true. We are going to have more block space used up by Atlas because we are going to have some reverted solver ops. That's true in a way you could actually argue that this is a return to Pegas, which is also true. This is also what we've seen on Vaseline, on Polygon. This is all charted territory for us.
04:00:00.680 - 04:00:03.510, Speaker C: That's why we're so excited about Suave.
04:00:03.590 - 04:00:04.182, Speaker A: On Suave.
04:00:04.246 - 04:01:18.366, Speaker C: What we can do is using the Atlas Suap. I actually have no idea how to pronounce that, by the way, so please don't correct please correct me if I'm wrong, but we can actually handle all the verifications and all of the logic on the Atlas swap on swab and then cross settle on ethereum using I'm just now realizing you guys can't see a portion of it using Permit 69, which is our novel permit system that we invented just for Atlas in order to bypass a lot of the issues that arise with having hooks and regular EOAS rather than smart wallets. Another concern is that due to PBS and 42 37 bundling, the party who can extract the most value from the user will become their bundler. Thankfully, the Atlas SUP can trustlessly bundle the user ops and match it with search resolvers and then auction it to builders, who can then return a portion of the priority gas fee to the user. And so it's one of these things where we're able to use the Suave programmable privacy to permissionlessly. I don't really know enough about SGX to know if it's like truly decentralized and permissionless or not. I assume it is because people smarter than me say that it is, but I'm just going to say permissionlessly.
04:01:18.366 - 04:02:21.026, Speaker C: And if you disagree, take it up with the Flashbots people, not me. But what I'm excited about is the ability to leverage this technology and use that to take this value that currently is going towards proposers towards builders and instead permissionlessly and trustlessly return it back to users. And I genuinely believe that you can't really fulfill an intent if the execution layer, like if the execution element of fulfillment is antagonistic towards the user's underlying intent. And I've seen a lot of intent solutions so far. I don't think I've seen one that actually fundamentally works to handle that, except for the one that we are working on and that we plan to launch on Suave. Finally, another concern that we've seen. This is more of something that I've noticed that Suave cannot be integrated into a DApp front end without impacting UX or requiring smart wallets.
04:02:21.026 - 04:03:18.810, Speaker C: And that's something that Atlas fixes. Atlas allows user operations to improve the UX, but does not require smart wallets, which we think is a distinct and intentional UX choice. Last slide. Atlas x suave predictions? This is all disclaimer, this is imo, IMHO. But in the long term, I think that Suave and Atlas on Suave's greatest strength is its ability to act as an account abstraction, bundler, paymaster and meta smart wallet in a way that is trustless, decentralized and gas efficient. And the thing that I think is most powerful about that is that that replaces so many middlemen. If you look at the account abstraction pipeline, especially if you consider the fact that that entire AA pipeline is like pre the mev pipeline that's like all stuff that happens before the mev supply chain.
04:03:18.810 - 04:04:21.100, Speaker C: There's a lot of third parties in there that are not public goods, although some might say they are. And so one of my favorite things about Atlas Xswab is that you have this ability to have a bundler. Not be this entity running infrastructure, but a bundler. Just as a smart contract on swap. Sorry, as a Suap on Suave, which is one of the things that we're going to build with Atlas and being able to handle all of this in a way that returns maximal value to users and to DApps or to liquidity providers or really wherever it is best allocated, wherever the front end wants it to be. That's something that we're extremely excited about because we think that long term, a lot of the DeFi advantages really won't benefit the user until the value and the flow actually restlessly in a decentralized manner goes back to the users. So we're very, very excited about that.
04:04:21.100 - 04:04:41.362, Speaker C: I also predict that Dan is going to somehow tag Andrew Miller maybe in the ear, maybe in the back of his neck, I don't know, but with some sort of geolocator so that people stop asking Dan so many questions that Dan ask Andrew Miller. But yeah, speaking of if any of you have seen him, tell him to.
04:04:41.416 - 04:04:42.578, Speaker A: Check his messages because I need to.
04:04:42.584 - 04:05:08.490, Speaker C: Bug him about some cryptography stuff so I can get a private key into Suave so I can actually build this. But yeah, that is the overall gist of the presentation. Apologies, my voice is usually a lot smoother but I am 14 hours flight, just landed like 8 hours ago. Thank you. I would love to take any questions that you got.
04:05:08.560 - 04:05:39.800, Speaker A: Yeah, just a quick question around the expression of preferences that we're now going to see in dense epic architecture. Do you see DApps as having control over that or do you actually see it as the wallet with which the user comes to the DAP? Obviously we haven't seen enough order flow with 43 seven Zops in D five just yet to see which direction that's going. But yeah, that's a question.
04:05:42.170 - 04:06:14.738, Speaker C: It's a really good question and what I would say is fundamentally it's both and also neither. And the reason why I say that is because it's actually the front end. And what is the front end? Well, some front ends are run by wallets. Like, for instance, MetaMask. They have a little like a swap module. You can do swaps on the MetaMask module. That's kind of a front end, right? Meanwhile, Uniswap, they've got a front end.
04:06:14.738 - 04:07:38.486, Speaker C: But the uniswap front end isn't the only front end that allows people to swap on uniswap. You've also got Uniswap Foundation has I forgot the name of the company, but there's a variety of different ways to access the same DAP and there's also a variety of different ways to access anything from the wallets. So what I would say is that fundamentally, it's the front end. And I would also say whatever is capturing the user's traffic, whatever is building the call data, whatever is building the call data that the user then signs, that is the top of the supply chain. And if the value is captured or retained is probably a better word. If the value is retained when the call data is generated, that all happens pre signing, pre transaction, and that's done by the front end, which could be the wallet or the DAP or whatever. It's tricky because I think now we're especially seeing new types of front ends starting to pop up and yeah, no, it's going to be an exciting thing to see, especially because we saw the Uniswap Labs front end with that 15 bips fee.
04:07:38.486 - 04:07:52.080, Speaker C: Yeah, it's going to be really exciting to see how the front end arms race plays out because I think user traffic really is the only thing that matters and I think that that is a front end game.
04:07:53.830 - 04:08:38.076, Speaker A: Thank you. All right, thanks Alex, again for a very thorough talk through the Mev Plus four, everything will be recorded. So we'll be publishing it on YouTube and later on we'll be distributing it across Twitter as well. So, again, thank you very much, Alex, for coming on board and take a shot at this talk. Oh, I guess your time is not that late for us. It's quite late, but yeah, I'm still.
04:08:38.098 - 04:08:44.156, Speaker C: On Turkey time though, man. I literally just landed like 8 hours ago, so for me, I'm just as sleepy as you all are.
04:08:44.178 - 04:09:03.030, Speaker A: Plus it's great to be here. Yeah, we're cognizant that it's the graveyard shift for you guys, so we'll be energetic and fast as we possibly can because we know it's late for you guys in Turkey. Yeah. Once again, thanks, Yuki, for organizing this.
04:09:03.560 - 04:09:04.900, Speaker C: Look forward to sharing.
04:09:05.480 - 04:09:24.590, Speaker A: Just going to share my screen real quick. Oh, no, that is not can you see it? Not yet. There, you should be able to see the yes, the drive? Yes.
04:09:26.080 - 04:09:27.390, Speaker C: Is it the drive?
04:09:28.480 - 04:09:44.210, Speaker A: I saw some inner strength, someone holding a sword or something. But those are beautiful guys. I've been studying the blockchain quite finally.
04:09:47.240 - 04:09:50.804, Speaker C: So, present mode. There we go.
04:09:51.002 - 04:10:13.224, Speaker A: So we'll be discussing the internalization of mev leakage in AMMS. And so I'm Ludwig and this is Karthik. We're both working at Sorella Labs and we're building Angstrom, which is kind of our response to LVR and the complete unsustainability of LPs in Amos today. Built on top of uniswap before.
04:10:13.422 - 04:10:14.072, Speaker E: Yeah.
04:10:14.206 - 04:11:05.496, Speaker A: So for the contents, this is quite a short talk. So we're going to be talking about the LP's predicament, the swappers predicament, and we're going to kind of describe the main problems and move progressively towards a solution and then how going about that? We can internalize that mev Leakage awesome. So let's start with the LPs problem. As we all know, LPs are getting mercilessly ARB right now, losing a lot of money. And why is this? Well, this is essentially loss versus rebalancing LVR, a quick primer on LVR. This is why LPs are negative in expectation and at the end of the day you can sum it up as just a function of information asymmetry right. So we have price moving homogeneously in continuous time on a centralized exchange and we have our own discrete price updates per block time on chain.
04:11:05.496 - 04:11:47.124, Speaker A: So these centralized exchanges are also deeper books than what we have on chain. So let's say during a block time there's some realized volatility on the centralized exchange which is always going to be the case for short tail high volatility tokens. As a profit maximizing arbitrager. I am going to hedge on the deeper venue, which is the centralized exchange, which has experienced a price discrepancy. And I'm going to submit an order to trade the underlying AMM from the stale price to the new true price right in the diagram on the right. This is going to be trading from A to B and this ends up being profitable for me. And as a result, arbitraged against or me arbitraging against the underlying LPs.
04:11:47.124 - 04:13:08.704, Speaker A: Because in swapping from the price at A to the price at B, I'm trading against the underlying LPs at strictly better prices for me than the final end price of the pool. So the profit maximizing ARP is not the final price of the pool which makes it so that LPs are losing a lot of money and me as an arbitrager is making a lot of money. So quite simply, the consequences of LVR for LPs are quite dramatic. LPs are like negative expectations or negative payoff and that is at its core threatening the sustainability of decentralized exchanges as a whole. If you're not profitable, if on expectation you're going to lose money, well, quite logically you're not going to be providing liquidity and you must be asking yourself, well, why do we see so many millions today in liquidity pools on uniswap B three for example? Well, nobody really knows why. And the best answer that I've received in terms of that question was that the people that were actually deploying liquidity were funds that actually collected money from various limited partners. And they advertised this high Apr to them and now realized that it was a very unprofitable strategy and are just collecting the two and 20 and see it now as an AUM game.
04:13:08.704 - 04:13:18.996, Speaker A: So I don't think that's a sustainable thing in the long term and I think we can all agree that as an industry we should strive to enable some form of profitability for the people.
04:13:19.018 - 04:13:20.432, Speaker C: That are providing that liquidity.
04:13:20.576 - 04:14:34.360, Speaker A: In terms of the transaction supply chain, what we see is that neutral search well, neutral searches, but effectively just searchers that aren't operating a builder are bidding in a blind auction while these integrated builders that are operating their searcher within their builder infrastructure bid in a second price open auction because they have the information of the incoming searcher bids. So they know exactly how much to pay for a specific access to this data. Then in terms of the builder market, well, we see clear force of centralization here because the HFTs that are vertically integrated have a structural advantage, right? They are producing this exclusive order flow that nobody has access to. And if you look at the actual flow we have mainly R Sync and then Beaver Builder and both don't actually send this sex dex flow, this stat ARB flow to each other. So it's even exclusive between them, quite logically because they don't want to give the other party that edge in terms of profitability, that extra boost in order flow. Now, looking at the swapper's predicament, well, put simply, it's just suboptimal execution. People like to present the LP problem and the swapper problems completely orthogonal.
04:14:34.360 - 04:15:56.852, Speaker A: That simply is not true and I think that is quite intuitive to anyone here. If the liquidity provider isn't profitable, they're most likely going to push that cost onto the swapper. So quite naturally they're not going to be executing at very low fees. And so how we see it exhibit itself is that compared to centralized limit order books that exist on centralized exchanges like Binance, even for these extremely deep liquidity pools, the execution quality is still worse. And for the other reasons that effectively cause this, we see it via mev, we see it in the sense of high aggregate fees where you have high gas fees, high swap fees, and then you also just see the underlying convexity of the AMM invariant which communicates itself as a slippage cost to the user. And so moving towards a solution, how can we arrive to a place that is better for the LP and the swapper? Well, the minimum viable properties, like the desirable properties for an AMM to be sustainable and for the swapper experience to be enjoyable and efficient, is that we need to contain the value within the application. So we need to capture completely the value that is extracted from the adversely selected parties.
04:15:56.852 - 04:16:49.016, Speaker A: And this is mainly in two fashions through recapturing the value of SECDEX. So recapturing the value of this arbitrage against the pools because of this exogenous price. And so we have to do that by removing the proposal monopoly on that front. And we also want to recapture background, which is kind of a flavor of the same thing in terms of swappers, we must protect them from sandwiches, so we have to remove the dependency on ordering. And the little diagram that you can see on the right shows the value flow and so as you can see, it flows up and it all flows always back to the proposal because at the end of the day they have the exclusive say on what block is finally proposed and therefore executed. So things have to be done so that all of that value doesn't just.
04:16:49.038 - 04:16:50.170, Speaker C: Trickle up to him.
04:16:51.900 - 04:18:08.128, Speaker A: So now what we have in the minimum viable solution is all good and that needs to be addressed for us to have sustainable on chain systems. But we can do much better than this, right? So we all know from traditional finance that HFTs are more than willing to provide great execution quality to user orders. So how do we actually leverage this counterparty and benefit the entire system as a whole? Well, HFTs, they are competing with each other for zero sum opportunities and they're looking for amortized profit. So this is profit minus their costs, right? And in the competitive equilibrium they're extracting this profit from these zero sum opportunities based on their spread. And the spread is being priced in with a notion of the pervasiveness of toxicity of the underlying flow that they're trading against, but also their costs, right? So these costs manifest themselves in talent. We all know quantilent is notoriously expensive and also they are latency optimized infrastructure, the other externalities that this latency optimized infrastructure and the associated cost makes this game of high frequency trading and being a counterparty to user flow very exclusive. So if we can find a way to reduce infrastructure rents, then spreads are going to be much tighter as a result and also the game is going.
04:18:08.134 - 04:18:09.876, Speaker C: To be less exclusive, which is a.
04:18:09.898 - 04:19:25.900, Speaker A: Very large benefit for the system as a whole, for execution quality as a whole. So now this is our proposed solution for internalizing MAV and it is a combination of a batch auction and an exposed realized volatility auction. So some intuition on the properties of a batch auction and kind of how it allows us to get this desired system that benefits both the passive LP and the uninformed swapper. Essentially you can think of user limit orders, uninformed trades that come within a block time as a conglomeration of supply and demand. And you can also view the AMM liquidity as an aggregation of the LP's express preferences for trades they're willing to take, so supply and demand on their side. So every block we aggregate all of these orders and the underlying AMM liquidity and we see if there's a point of intersection, that is to say that is there a quantity that we can clear such that the intents of all users are satisfied and we execute everyone at that point of intersection. This means that some users who express intents that were perhaps a bit misinformed, they get better execution quality, but everyone is guaranteed at least the execution quality of their intent.
04:19:25.900 - 04:20:22.252, Speaker A: And the thing to note, especially here is that the underlying LPs are also executed this price, right? So if you recall earlier, the problem with LBR is that as an arbitrager I'm able to trade against the pool such that the end price of the pool, which is the profit maximizing ARB, is not the average price that I trade for the entire transaction. The average price I trade for the transaction is strictly beneficial to me and not the LPs. This is essentially saying that the LPs will also execute at the final price. So to actually arbitrage against the pool now searchers will be competing against each other to improve price. We'll get into the externalities of this in the next slide. But now let's discuss the X Post auction. So we could actually do one step better than just the batch auction and institute an X Post auction to make sure that at the start of every block, price is perfectly reset to what the exogenous venue is.
04:20:22.252 - 04:20:31.116, Speaker A: And to do this, we essentially realize that the fee that is charged on swaps in a pool is something of a bid at Spread.
04:20:31.148 - 04:20:31.584, Speaker B: Right?
04:20:31.702 - 04:22:06.428, Speaker A: And we know that if there's any volatility within said spread the price is not going to be reset to what the true price is at that time. So to enable us to reset it at all times we essentially make it so that we auction off the rights to execute against the pool as the first swap with no fee. And in the competitive equilibria the value that searches will pay for this opportunity is strictly the actual value of the arbitrage opportunity which is equal to transacting against the pool at the true centralized price. Thus resetting the pool as desired and the LPs are better off as they have not been arbitraged against. So the analysis of the entire system as a whole, essentially, we have, instead of latency, where these super sophisticated high frequency players are competing against each other to fill flow, they're now competing against each other to have to offer the best execution price for all users. LP liquidity in the batch auction and this has significant positive externalities namely uniform clearing price leads to the mitigation of systemic arbitrage against LPs for the aforementioned logic that if the LPs are executing at the price that the searchers are also trading against the Pool at it's very difficult now to arbitrage against the Pool in the way that the profit maximizing arbitrage makes it so that the end price of the Pool is the profit maximizing ARB. Now, the profit maximizing ARB is to trade somewhere in the middle of what the true price is and what the current AMM price is.
04:22:06.428 - 04:22:53.004, Speaker A: But this essentially leaves something to be desired for other searchers. Now, other searchers are going to hop in, they're going to improve the price even more and more and we have convergence to what the true price is which yields convergence of the LPs to a true payoff that they would have in traditional markets. Additionally, we have complete mev protection for user orders and this is a result of the fact that executing at the uniform price. It gives a sense of commutivity to orders. So now if I have two orders transacting through an AMM, A and B, I can order A and then B or B and then A and they'll have two different payoffs. But if I have uniform clearing price A, then B equals B then A, which makes it so there's no ordering value to be extracted. Very desirable property, as we mentioned.
04:22:53.004 - 04:23:40.524, Speaker A: And then the limit order functionality. This essentially allows for really large use orders that would incur a lot of slippage to then instead of being transacted against the underlying AMM bonding curve for more sophisticated searchers with larger inventories to then match the order with their own limit orders and offer better execution quality to the whale who wants to move a lot of volume through the pool. So, yeah, this is a preliminary analysis of what we're building here at Sorella with Angstrom, and we look forward to unveiling more as we get closer launching. Thank you Yuki, and thank you all for listening. But yeah, that's all for us. All right, thank you very much. Thanks for the applause so late at night.
04:23:40.524 - 04:24:28.476, Speaker A: It's like actually 10:40 p.m. In Turkey right now. So all of the warriors standing here, sitting here is definitely desperate mev learners like myself. So I appreciate your attendance very much today and sticking around from the very beginning of the events as well. So I really appreciate that. Now, just some Q A session here and actually I will kick it off the Q A real quick with the Sorilla teams here. Obviously, there's a lot of trade offs in this DeFi space in terms of how permissionless the protocols are, how permissionless the liquidities are, how centralized the liquidity or LPs are.
04:24:28.476 - 04:25:40.390, Speaker A: Like in case of order book, you have market makers that are oftentimes centralized, maybe even in a competitive market that just participation of a market maker itself is a bit more centralizing effect. While on the extreme end you have LPs on AMMS that are fully decentralized, but at the same time you have this, I guess the six six LBRR type of things happening. So in some ways, Sorilla Labs is kind of like taking the middle approach where you have somewhat permissioned LP accesses in a way that you have maybe entities behind that is handling some liquidities. But just from the surreal apps perspective, what do you think about this trade off between the permission access to liquidity via this kind of like a hook type of implementations, potentially Kotlink or Ludwig, whoever, want to take this over. So, to summarize the question, you're referring to the permissionless access to liquidity via hooks and its centralizing effects or decentralizing effects just to make sure.
04:25:41.320 - 04:25:42.310, Speaker E: Yes, exactly.
04:25:42.920 - 04:26:51.856, Speaker A: So I think hooks are really interesting because they enable open market competition through access to this decentralized liquidity source. But I also think that for a big part of the innovation that. We're going to see it's going to happen through some form of restriction on the access to state, in the sense that to guarantee all of these properties, you obviously can't make it fully open. Not anyone can open, can access the state because then you lose all of the sophisticated, mev protected properties of the system itself. So now what's interesting is that the smart contracts, although decentralized, now shift the burden of decentralization to the underlying infrastructure, to the sequencing esque infrastructure. May that be Suave, may that be our stellgard network, may that be any other form of component that arrives as an in between point between the underlying execution and the sequencing. And so that's where I think users have to be really careful in analyzing the decentralization properties of those underlying components.
04:26:51.856 - 04:27:38.250, Speaker A: And it's also very hard because we see that there's always this huge tradeoff between efficiency in the private setting and centralization. And for example, Suave takes on the view that because they're operating in this winner takes all, there's necessarily this component of centralization that's introduced. So it's a bunch of trade offs. If you design your application where it's a collaborative game where there isn't a single winner, then I think that reliance on centralization kind of dies down quite a bit. But in the setting where it's winner takes all, it's very hard to not have centralization be introduced. Yeah, for sure. There's a question.
04:27:38.250 - 04:27:52.688, Speaker A: Hey, guys, I think my understanding yeah.
04:27:52.774 - 04:27:53.248, Speaker D: We can hear you.
04:27:53.254 - 04:27:56.290, Speaker A: It's a bit quiet, so try to speak up as loud as you can.
04:27:56.820 - 04:27:57.570, Speaker D: Okay.
04:28:01.640 - 04:28:21.556, Speaker A: We're leaning in, pretending that's going to help us. Yeah, it was like that FIFA meme when you lean into the TV where you have the video games. Anyway, we're fully engaged. So just a couple of points of clarification. I wanted to understand how you solve for the staleness of price on the deck.
04:28:21.588 - 04:28:22.696, Speaker D: So I think you mentioned over there.
04:28:22.718 - 04:28:33.480, Speaker A: That every block there is an express preference between LPs on either side that this is the price that I'm willing to execute at. So I want to understand how that staleness is solved.
04:28:33.560 - 04:28:35.504, Speaker D: And the second is, could you just.
04:28:35.542 - 04:29:07.128, Speaker A: Clarify how the HFT firms are effectively repurposed here and how the playing field becomes a little bit more level? Thanks. For sure. Yeah. So the staleness is solved really through both angles, right. Essentially what solves it directly is kind of this X post volatility realization auction, where now you say that it's Armageddon to transact with the pool to be the first one to transact to the pool. You transact with no fees. You get first rights to all the underlying at the touch liquidity at the sale price.
04:29:07.128 - 04:30:35.012, Speaker A: But you have to compete with all other sophisticated players for the right to arbitrage this pool. And in short, tail high volatility pairs. This is going to be the actual price of the bribe you have to pay to the LPs to transact against the pool is going to converge the actual price of the arbitrage opportunity, right, which is going to make sure that the LPs are compensated and price is adjusted from stale price to the true price. You also have this kind of as an emergent property of the batch auction system because the underlying LPs are going to execute at the price at which everyone trades at, right? So now let's, as a thought experiment say that price on a centralized setting at the start of the block is $1,000 e to USDC and within the 12 seconds it moves from 1000 to 500. So given that the underlying LPs are going to execute at the price of my perceived trade, me as an informed player, as a sophisticated searcher, I'm only going to want to move the price to 750 because then we all execute at 750. But since I know the true price is 500, I can still hedge it, right? But now there's something to be left for another sophisticated player, namely someone who sees that I reset price to 750 and say, no, I'm actually fine with 600, right? I'll go to 600. And then there's kind of this cascading effect where all the sophisticated players are competing against each other to make price converge to what the true price is.
04:30:35.012 - 04:31:30.564, Speaker A: And this is exactly how we're repurposing the HFT role because right now you're competing as an HFT for latency, right? If a user transaction, user market order comes in and I'm able to fill it before my counterparty, even if he was willing to fill it at a better price, I am going to fill it at my price because I'm faster. And we're basically saying we don't need these rents to be paid and it's rents paid at the cost of execution quality. Instead, let's batch, right? Let's make sure that we're not wasting rent on infrastructure, latency infrastructure, and instead make sure that those who offer the best price will be the ones who execute at that price. And to respond more practically in terms of how this actually works, it's actually really simple. You're just gating the execution of the pool. So you have your before swap hook. It calls our hook on the underlying pool that you're trying to execute on and it checks who is actually executing.
04:31:30.564 - 04:32:32.728, Speaker A: And so our system forces a multi SIG contract that is the only authorized party address to execute. So how that communicates in practice is that you have this relay network that forms consensus and they're forming consensus on that underlying bundle, on the canonical combination of the highest bidder that has paid the most to execute against the pool and the rest of the regular user swappers or market maker orders. And those orders are all signed structs. They're all intents, right? They're not actual transactions and you compose them all and compress them down to a single transaction. And the beauty there is that it's extremely efficient in terms of gas because you can compress everything down to a single swap per pool. So for any given number of interactions, it doesn't matter if there's three or 10,000, like 1000 swaps in the transaction regardless. You only actually execute a single uniswap swap.
04:32:32.728 - 04:32:56.268, Speaker A: So you only do the AMM math once, which is far cheaper in terms of gas. Yeah, well, that was some alpha drops. Well, I'm sure you guys sitting here got the alpha drops right there. So any more questions from the crowd you have?
04:32:56.294 - 04:32:56.772, Speaker D: Quick question.
04:32:56.826 - 04:33:00.612, Speaker C: Okay, just a quick clarification question.
04:33:00.746 - 04:33:03.696, Speaker A: Ludwig and Carson, you mentioned the limit.
04:33:03.728 - 04:33:07.670, Speaker C: Order functionality because is that only through the coincidence? Once.
04:33:10.220 - 04:33:47.120, Speaker A: This kind of expands on coincidence of wants, right? Coincidence of wants, very trivially is let's look at all user bids and then user asks, and then find the point of intersection or clear net them against each other. There's going to be some delta that's uncleared and then trade that through the AMM. Right? This kind of goes one step beyond that and says instead of netting what can be netted and having some delta that's traded to the AMM, let's consider the AMM as part of this underlying bid side ask, side liquidity, and let's execute everything at the canonical uniform.
04:33:56.890 - 04:33:58.230, Speaker C: I have a quick question.
04:33:58.300 - 04:34:02.060, Speaker A: Oh, yes, if you don't mind. Yes, please.
04:34:04.590 - 04:34:57.946, Speaker C: Apologies if you already answered this, but I was curious about visibility. Will the different swappers be able to see the sizes and volumes of each other's swaps? And the reason why I ask is I'm thinking for market makers initially, the huge volume is going to be for the beginning one, which is going to go back to the LPs for sure. But then let's say that there's just regular nontoxic flow that pushes the rate in one direction. There might be some competition over back running that or capturing that in the opposite direction. And I was curious as to visibility there or if maybe that'll just get pushed into being value that gets picked up by the next top of the block or how you guys see that playing out.
04:34:58.128 - 04:35:25.038, Speaker A: Yeah, great question. So, yes, they do see it because I think as we've seen, especially with the Mev share experiment, you need the information to be able to get optimal execution. Only if you can't obviously have some super sophisticated ordering that creates extreme extraction. So, yes, absolutely, they can see the orders and they will very much use that information to know what opposing limit.
04:35:25.054 - 04:35:25.938, Speaker C: Orders they should post.
04:35:26.024 - 04:35:37.350, Speaker A: And that's where this idea of repurposing the HFT comes in because they now are incentivized to offer the best price and there is no paying the rent to the proposer and front running and then back running.
04:35:37.420 - 04:35:37.894, Speaker C: Right.
04:35:38.012 - 04:36:15.860, Speaker A: You can or decide to go within that batch and then offer your liquidity for that large order that you're seeing that nontoxic order or decide that, well, that order is going to create a mispricing against the underlying pool. And then at the next block, you have to compete against other searchers to bid back that value to the LP. So regardless or the swapper is getting a way better deal, or your underlying LP gets that value as a rebate at the next block, when the auction kind of restarts because there's an underlying mispricing, which creates an even bigger gap between the centralized price and the AMM price.
04:36:16.950 - 04:36:20.966, Speaker C: Got it. That makes perfect sense. Thank you. Of course.
04:36:21.068 - 04:37:03.674, Speaker A: Thank you. Well, anyone else who have questions? All good. All right. Well, I'm quite impressed that even though we run late on the schedule for like 40 minutes and that it's almost like 11:00 P.m. Here, but we still have people hanging around at this kind of research workshop, I'm honestly quite impressed. But thanks again to our speakers, Ludwig and Karthik from Cerilla Labs as well know, Alex, who have been also sticking around to just listening on a lot of the conversation here. Thank you, Alex.
04:37:03.674 - 04:37:19.730, Speaker A: Thank you all for coming today. Thank you all the attendees who also stick around. So that would essentially conclude our MEB overflow PBS misalignment talks. Thank you all. Thank you very much. Thank you, yuki?
