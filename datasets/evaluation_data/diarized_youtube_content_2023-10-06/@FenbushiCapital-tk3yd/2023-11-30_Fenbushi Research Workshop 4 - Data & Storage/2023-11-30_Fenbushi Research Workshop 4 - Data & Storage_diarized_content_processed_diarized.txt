00:00:07.490 - 00:00:08.040, Speaker A: You.
00:00:10.970 - 00:00:12.040, Speaker B: Hello everyone.
00:00:12.410 - 00:00:15.382, Speaker C: Thank you for everyone can come to our event today.
00:00:15.436 - 00:00:21.720, Speaker B: I know it's been a long week and you guys are probably quite tired, but anyway, thank you for everyone.
00:00:23.690 - 00:00:34.446, Speaker C: Thank you. So, yeah, before our research workshop, let me give you guys a quick introduction of our fund. So we, Ben Bushi, were found in.
00:00:34.468 - 00:00:36.558, Speaker B: 2015 and we have been one of.
00:00:36.564 - 00:00:39.722, Speaker C: The oldest institutional crypto investors from Asia.
00:00:39.866 - 00:00:41.726, Speaker B: So actually we are also one of.
00:00:41.748 - 00:00:46.734, Speaker C: The very early investor five point. So we actually been introduced by data.
00:00:46.772 - 00:00:51.646, Speaker B: And service with for a very long time and actually about like two years and a half ago, we even launched.
00:00:51.678 - 00:00:58.638, Speaker C: A five point eco fund with Pro Labs together and had invested about around like ten projects directly through this five point EcoFund.
00:00:58.734 - 00:01:21.100, Speaker B: This is a workshop on data and storage, and my talk is on data availability, which is kind of funny because I know this is like a general confusion in the space Celestia. I've spent like months being like, no, DA is actually not storage. And now here I come and basically spreading misinformation, which is a great joy to me.
00:01:26.130 - 00:01:28.990, Speaker C: Yeah, exactly. Filecoin as a DA.
00:01:30.210 - 00:02:30.150, Speaker B: Yeah, my talk is going to be on filecoin as a DA later. Okay, I don't think I need a charger, but now I'm going to show the world my PR. No, it's okay. It's a really nice PR. No, and I can actually use I believe I can use this and really?
00:02:32.120 - 00:02:32.870, Speaker D: Yeah.
00:02:36.680 - 00:02:37.430, Speaker C: Cool.
00:02:38.360 - 00:03:51.600, Speaker B: Again. So at Reppy Labs, we're building a layer one blockchain called delta. I'm not going to talk that much about Delta in this talk. Beyond we're building a blockchain without consensus, which is that's exactly the reaction. Very good question. Okay, so cool. Topics of this talk, what is a blockchain? What are consensus protocols? Why do they go well together? Or do they switching over to reliable broadcast, which is this primitive, which it's not a replacement for consensus, but maybe it can mitigate it? We'll talk about a simple case of consensus list blockchains narwhal, and then we'll introduce Delta within this context.
00:03:51.600 - 00:04:27.708, Speaker B: Finally, I promise. Infinite scaling. I'm sure it's the first time you've ever heard anyone, any lay one blockchain proclaim, infinite scaling this time for real. Again, what's a blockchain? A blockchain is basically some STF. It takes in a state, transition function. Takes in some state, some block. What is a block? We'll get there output some new states and that's like half the thing.
00:04:27.708 - 00:05:19.404, Speaker B: The other half is you need some protocol to determine what this block is and that's going to be the focus of this talk. So typically that is a consensus protocol. Consensus protocols are some way for some set of participants who may or may not know of each other. There are different protocols for either case. They certainly do not trust each other and they want to agree on some value. I've just said here some field element you can kind of generalize this, but also we're working with computers and we can kind of view anything as a field element given a sufficiently big enough field. Again, also baked into the definition of the consensus protocol is some number T.
00:05:19.404 - 00:05:47.590, Speaker B: This is always some fraction of N. We typically care about Byzantine assumptions, so B of T is typically N thirds. And if this is the case, then I can have this protocol has a few different properties. We have agreement, all honest nodes agree on the same value. We have validity, which is.
00:05:50.140 - 00:05:50.616, Speaker C: If all.
00:05:50.638 - 00:05:55.720, Speaker B: Nodes propose the same value, then that value is chosen, we have integrity.
00:05:56.620 - 00:05:57.128, Speaker C: Cool.
00:05:57.214 - 00:06:34.260, Speaker B: And then finally the protocol terminates and termination is very important, right? Because, well, if we didn't have termination, you're invoking consensus time after time. Say you have some dishonest note which just says as a leader, I don't like you guys. Well then you're never going to make progress. That would be bad. Shout out Dolly three. So in a blockchain setting we use consensus to agree on subordered blocks. So we think of a block as like an ordered list of transactions.
00:06:34.260 - 00:07:21.190, Speaker B: Again, I like to think of it in terms of the hash, but that's just because I like the idea of we care about just the value, doesn't matter. Okay? And also we focus on leader based consensus in blockchain. I think all consensus protocols are leader focused or leader based. I know that in the literature generally there are leaderless consensus protocols. I don't think it really works in blockchain. So assume the existence of a leader. Now the leader is and again, shout out Dolly Three, who really seems to like this network topology and it's not representative of blockchain's IRL, they do not, whatever.
00:07:21.190 - 00:08:19.770, Speaker B: Another sort of, I think accidentally well made point by Dali Three here is that this thing, which is obviously the leader, is substantially bigger and better and more powerful than the other nodes. And in practice you choose a new leader every time. So it's obviously not the case. But that emphasizes an issue because, well, this guy needs way more bandwidth than the others. The leader needs to send out the block, it consumes way more resources. And that becomes a bombing. Because if the network is at half usage for most of the nodes and let's say the consensus protocol, or the network is such that the leader consumes double that of the replicas, well, then throughput is bounded at that point, which is bad.
00:08:19.770 - 00:08:58.390, Speaker B: The other issue is it's a single point of failure in the case of BFT. I mean, for something like Bitcoin, obviously this is different because you just have this hashing battle completely different thing. We again care about BFT because they're efficient. In BFT. It's really expensive. If the leader goes down or is malicious, you need to perform a view change. I think this is a reason why tendermint or common BFT as it now calls itself.
00:08:58.390 - 00:09:08.650, Speaker B: Why? Those blockchains have a max of like 150 validators. I think it's like N cubed cost or something. Is that right?
00:09:09.740 - 00:09:10.344, Speaker C: Yeah.
00:09:10.462 - 00:09:31.980, Speaker B: Nice. Okay, so sorry. Leader based consensus has some problems. Okay. What is a reliable broadcast? It's basically a consensus without termination. It terminates an honest node broadcast. It's not guaranteed to terminate otherwise.
00:09:31.980 - 00:10:42.972, Speaker B: And this emphasizes, again, we do not want to replace consensus with a reliable broadcast because they can literally halt the chain forever. The issue is, or the motivation, rather, the idea is a different one. You obviously have a mempool. All the transactions pass through the mempool. Well, what if we could use a reliable broadcast for the mempool so as to ensure that all the nodes receive each transaction? In the honest case, basically, going by this logic, what we would end up with would be every single validator has a copy of the same set of transactions. I emphasize set because the question here, do we need consensus? Unfortunately, yes, they have the same set, but there's no canonical ordering. What are you going to do? So you have DA, by the way, not in Texas.
00:10:42.972 - 00:11:11.140, Speaker B: I mean, people separate between the two. And this is kind of the reason. This is a great result. And I mean, this is one of the papers that I think has gone really under the radar. It's called I want to say it's called the consensus number of cryptocurrency or something like that. One takeaway from this talk. Check out this paper.
00:11:11.140 - 00:11:57.058, Speaker B: The main result is and they have like, a theoretical concert called astro, which is a bitcoin like blockchain, like very basic, no smart contracts. You can do payments, not much more. But it uses a reliable broadcast for its mental and doesn't need consensus. This is an awesome result. And the idea is basically this. You have to think about when do we need to order transactions and when do we not need to order transactions? And it's not like and I mean, this is something that they are not the first or the last to have observed. Not all transactions need to be ordered relative to each other.
00:11:57.058 - 00:12:16.470, Speaker B: So we'll go through what I claim to be the four only cases that we need to care about. First one is this. Alice sends Bob some coins. Arleigh sends Daria. Daniella is daria.
00:12:17.550 - 00:12:19.290, Speaker C: Okay. It's daria.
00:12:20.110 - 00:12:50.500, Speaker B: In this case. What do we do with ordering? It doesn't matter, right? From the system's point of view, it does not matter in which order these things take place. Next example. Both Alice and Charlie send Bob some coins. What do we do about ordering here? Doesn't matter, right? The end state is the same. Okay. Next one is a bit more interesting.
00:12:50.500 - 00:13:15.350, Speaker B: Alice has three coins. She wants to send two coins to each of Bob and Charlie. Well, this could be an issue, right? We have no sense of ordering in the system. Well, some honest notes will think, okay, alice sent Bob two coins, but then she didn't have enough to send Charlie coins.
00:13:15.510 - 00:13:16.220, Speaker A: Cool.
00:13:16.750 - 00:13:42.340, Speaker B: That transaction didn't go, you know, equally honest and valid. Nodes would think the other way around. We have a problem. So the solution is essentially Nomsis. In this case, nomsas. Are these you have some number, some natural number tied to positive integer. Sorry.
00:13:42.340 - 00:14:01.130, Speaker B: Yeah. Tied to each transaction site. Your account, it needs to go up by one for each outgoing transaction. Alice needs to order these to herself. So she needs to say, yeah, I paid Charlie first, and then Bob, or the other way around. And then this one, the most interesting.
00:14:01.200 - 00:14:02.300, Speaker C: One of them all.
00:14:03.070 - 00:14:36.110, Speaker B: So, okay, Bob has zero coins. He is broke. He wants to buy a hot dog from Daria, which costs four coins. But luckily for him, Alice is sending him six coins. Okay, so what about ordering here? Again, it obviously does matter. We cannot have this one go through before this one. So again, Bob orders the transactions.
00:14:36.110 - 00:15:24.770, Speaker B: So there's actually two rules that exist here, is each account needs to order all outgoing transactions with respect to each other and with respect to a subset of the incoming transactions. And the thing is, because we have reliable broadcasts, the availability of all of these transactions, assuming honest behavior is this is available, we have data availability. So we don't need this external step of going through consensus to reach a consistent end state of the system. Again, awesome paper, awesome work. Check it out. Called the astro paper called Consensus number of cryptocurrency.
00:15:25.670 - 00:15:26.420, Speaker C: Cool.
00:15:28.470 - 00:15:47.258, Speaker B: So Norwal is and this might be just me kind of interpreting history. I think it is valid. Basically, these Facebook people came out with FastPay, similar to the previous paper. I think the previous paper came out first. I might be wrong.
00:15:47.424 - 00:15:47.802, Speaker C: Sorry.
00:15:47.856 - 00:16:30.306, Speaker B: If so, basically, FastPay is very similar, and then they came out with Narwhal. And I think the following thing went through their head. Okay, this is very cool. We can do this payments thing without consensus. What about normal smart contract blockchains? It seems so unnecessary that in those cases, so many transactions unnecessarily have to go through consensus when we could just have a better mempool. Narwhal is a better mempool. So Narwhal is reliable broadcast as a mempool.
00:16:30.306 - 00:16:43.920, Speaker B: It can be used in any blockchain appdos. And Sui, I think at least Sui uses it now. I think Aptos is integrating it as well. All right, there you go.
00:16:45.090 - 00:16:45.790, Speaker C: Yeah.
00:16:45.940 - 00:17:22.966, Speaker B: So basically, same sort of thing. I believe in move. They have the concept of owned objects and shared objects. Owned objects are, know, Tokeny things NFTs your USDC, whatever. If you're just sending someone this stuff, they just have it go through Narwhal as a mempool. No consensus whatsoever. And then the nice thing for Narwhal is the things that do need consensus, which is when you interact with smart contract, for example, like an AMM or know two people interacting with an AMM.
00:17:22.966 - 00:18:05.480, Speaker B: How is that going to be ordered? It's no longer so simple in those cases, you just can plug in a consensus protocol on top. So it's basically you only have consensus for transactions that touch shared objects. Nice. Now, the normal thing was honestly a bit of a detour in terms of the linear development here because delta is in this sense much more similar to the work by Guerawi et al. But I wanted know historical completeness and whatnot to include it.
00:18:07.210 - 00:18:07.814, Speaker C: Okay.
00:18:07.932 - 00:18:47.480, Speaker B: So commutativity, for those who do not know, think of you have two actions. You want to change some state. Basically, the order in which you apply them doesn't matter. That's what commutativity means. Commutative ordering means we don't care about the order. So at a very high level, we separate DA and I should say we include settlement in that from execution. However, unlike something like Celestia or Ethereum, where all of these are sort of individual mix and match components, we have an integrated system and they have a global state.
00:18:47.480 - 00:19:27.460, Speaker B: Okay, so what you guys would maybe call rollouts, we call execution charts. They are really qualitatively different in the sense that they don't have their own fully owned state. They sort of rule over a subset of the global state. And I'll try to explain how also we call what you would call a sequencer, we call an executor. It does both. We just think execution is kind of more important than sequencing, although both are important and arguably this talk is about like yeah, sequencing is important.
00:19:30.710 - 00:19:31.170, Speaker C: Yeah.
00:19:31.240 - 00:20:33.654, Speaker B: So the executor essentially does and this is where it's very analogous to the work by Gurawi and co authors. The executor orders all of the spending transactions of accounts within that shard. So that is to say, sort of all outgoing transactions, instead of having those accounts have to order them themselves, the executor does it for them. And this can now include things like smart contracts within that shard and all of that. So it's essentially from this point of view of consensus listeners just replacing the need for each individual account to do this by having these big guys doing it for the entire shard. And yeah, this gives us a partial order, right? Because within each shard, the order is determined by the executor. And then between them we have mutativity because we're essentially partitioning the state.
00:20:33.654 - 00:20:45.142, Speaker B: I mean, this is like the definition of sharding, right, is you're partitioning something into different sets which make up the whole and have zero intersection. Empty intersection.
00:20:45.206 - 00:20:45.820, Speaker C: Sorry.
00:20:49.390 - 00:21:18.502, Speaker B: Okay, thanks. Yeah. Yes. This is a great illustration. I know. The next one hopefully will make it clear why I included this absolutely horrendous looking thing. The idea is basically you have these distinct accounts which are separate from each other.
00:21:18.502 - 00:22:01.458, Speaker B: They do all their ordering, they send here. It's just fun and DA and no ordering, no consensus, no nothing. This is actually whatever, but now you'll see why I have this ugly thing? So we clean it up to this, where the order is now not done by each individual guy here, but rather by these guys. And you could think of each execution shard as like roughly analogous to rope in the sense you can have smart contracts, you can have all sorts of things. For cross shard interactions, we do have that, but that is not going to be a topic of this. Such slightly more involved. Although, honestly, this paper bike where Rawi at al.
00:22:01.458 - 00:22:15.462, Speaker B: I know, I keep going on about it. It kind of offers a solution to the cross sharp thing here as well. I don't think I have the time to go into it though. Oh, did I want to say anything else?
00:22:15.516 - 00:22:16.440, Speaker D: I don't know.
00:22:18.670 - 00:22:32.682, Speaker B: Maybe one other topic I want to mention. Just to say explicitly what goes on here. There is no consensus here. These guys are just sending down their data and it's not ordered.
00:22:32.826 - 00:22:33.520, Speaker C: Okay?
00:22:34.290 - 00:22:56.286, Speaker B: It's not ordered. They only send it and essentially stream it throughout the validator set. This is different. This is new. So we can think of this as essentially streaming. You mitigate the overhead of having this is like the most literal bottleneck, by the way. And we'll get to this with the infinite scaling.
00:22:56.286 - 00:23:18.700, Speaker B: If you have sequential dependencies and the need for a total order, I mean, that's like the definition of a ballnet. You need to squeeze all the data through this one thing called consensus. Terrible. We're done with that. Not quite done. Last topic, infinite. Okay, so we have this thing that looks like this.
00:23:18.700 - 00:24:02.650, Speaker B: Obviously, execution can be scaled to whatever degree possible or necessary because you just add new shards and something you don't know, but which is the case. You can permissionlessly open up new execution shards, no problem at all. Execution is never going to be a ballneck. However, the ballneck that you do reach is these guys. They can only send so much data around. At some point you reach that limit. What are you going to do? By the way, this is not something that's specific to our chain, right? This is really like the ultimate the final boss of blockchain is the Internet.
00:24:02.650 - 00:24:51.740, Speaker B: Okay? What do we do in that case? We reach the limit of what these poor guys can send through the network. What do we do? We do this. So because we don't have the overhead of consensus, you don't have the need for this sequential dependency. You don't have the need to add relations between any of the things that come in. So what we do and by the way, if you thought this was going to happen without extra hardware, I'm sorry, you need hardware. This is also like obvious, right? If you want to squeeze like ten gigabits of data through some network, well, guess what? Each validator needs to have that Internet connection. Sorry.
00:24:51.740 - 00:25:57.840, Speaker B: Okay, so basically what you see here is each validator runs like two computers next to each other with their own Internet connection. All that and you have two X with group it and you guys are like, that sucks because you shouldn't be like that sucks. That is really cool. But not the off camps that is you're demanding more hardware harming decentralization. First of all, okay, you do need to make some trade offs in the grand scheme of things if you want a network with really high throughput well, again, they need the internet connection. Secondly though, we should assume that the profits or the profitability of running a validator is roughly proportional to the amount of data that it deals with. So I would assume that they would want to add another computer because it means they're going to make more money.
00:25:57.840 - 00:26:00.670, Speaker B: Okay, that's enough of that rant.
00:26:02.450 - 00:26:03.200, Speaker C: Cool.
00:26:04.450 - 00:26:29.020, Speaker B: Oh, another thing that I wanted to mention by the way off into this, but I'm trying to nerd snipe what do we do with Bridging here, right? Usually in safe bridges you need consensus proofs, right? Here we have no consensus. Does that mean that Bridging is suddenly trivially easy or does it mean that it's suddenly impossible? If you're interested in that question.
00:26:32.270 - 00:27:12.882, Speaker C: Yeah. Can you hear me? Well, thank you so much for introducing this concept. I remember back to 2020, I was super obsessed on finding the next Ethereum or Ethereum killer. I came across a lot of protocol consensus, some of them similar to hash graph, some of them are similar to what you just described, like reliable Byzantine broadcast. But I have one question I really want to ask. Seems like all this kind of system relies on one property, which is you always need to know the big enemy in the system. Like how many validators, the total number of validators.
00:27:12.882 - 00:27:49.220, Speaker C: Because one of the prerequisites of Byzantine broadcast algorithm is you have to reach message to every single validators. Right? So the beauty of the Ethereum and the bitcoin is you can hop on, hop off, I can run a validator and secondly, I can just drop it. How you make sure in a scalable system without the consensus, if we don't know what's the total number of N, how we can still have how this algorithm can still work.
00:27:49.590 - 00:28:13.346, Speaker B: Yeah, it's trade offs all the way down. Right? And I said this quite early on when I said we focus on BFT systems. So not like bitcoin on Ethereum. I do agree with you. So just explicitly how these things work, like reliable broadcast, you have some certificate based on the quorum. Typically that will be based on something like stake or whatever you want to have. Right.
00:28:13.346 - 00:28:21.340, Speaker B: So this is of course true. You do need to know of the other participants in the system. I agree. Would have been better if it weren't the case, but.
00:28:24.690 - 00:28:43.010, Speaker C: Seems like in this case it's probably more useful for enterprise scenario, right? Well, maybe within Amazon, you have fixed number of servers. You want to validate your data and information on the blockchain secure. Probably in that case, more useful.
00:28:43.350 - 00:29:00.600, Speaker B: I think you're underselling it. I mean, I get what you mean. But most modern consensus is like this, right? Like BFT consensus again, comet BFT or Solana or most anything that isn't Bitcoin or Ethereum. Maybe not.
00:29:04.430 - 00:29:05.180, Speaker C: Whatever.
00:29:06.430 - 00:29:18.750, Speaker B: I get your point. I think it generalizes far beyond just enterprises. Yeah. For example, Celestia is also right. Like Comet BFT.
00:29:19.330 - 00:29:23.502, Speaker C: Howdy. Howdy. Yeah, thanks for the really cool talk.
00:29:23.556 - 00:29:26.500, Speaker D: Definitely interested in reading the consensus paper.
00:29:27.110 - 00:29:28.894, Speaker C: I'm curious how this changes the typical.
00:29:28.942 - 00:29:43.894, Speaker D: Threat landscape through protocols. Especially since you're talking about, well, what? Byzantine fault tolerance. You need to be resilient to malicious nodes. Things like Celestia or other das, you say, well, we don't care about the execution, so just if it doesn't make.
00:29:43.932 - 00:29:46.360, Speaker C: Sense, ignore the message.
00:29:47.130 - 00:29:49.850, Speaker D: How has that changed in what you presented?
00:29:50.270 - 00:30:36.310, Speaker B: Yeah, so there are different kinds of nodes in our system. Maybe I should just briefly. We have these things called executors. They execute and then we use Snarks to prove the validity of execution. So they can try to pull some tomfoolery, but unless they can break the Snark, they will be caught out, these guys. The thing is, if you just have reliable broadcast, which is a strictly weaker primitive than consensus, you would assume that there are strictly fewer ways you can attack the system. So my kind of informal heuristic assumption is that these systems will end up being strictly safer and less prompt to attacks.
00:30:36.310 - 00:30:42.710, Speaker B: But it's just based on that. Heuristic right. Weaker primitive, fewer attack surfaces or less attack surface.
00:30:43.050 - 00:30:45.466, Speaker C: What do you think? So, as a follow up question then.
00:30:45.488 - 00:30:46.746, Speaker E: If you have a weaker primitive, like.
00:30:46.768 - 00:30:48.506, Speaker D: We have a stronger primitive for a.
00:30:48.528 - 00:30:50.266, Speaker C: Reason, so what are the things that.
00:30:50.288 - 00:30:51.766, Speaker E: We can throw away in the weaker.
00:30:51.798 - 00:30:54.214, Speaker D: Primitive that allows us to be more efficient?
00:30:54.342 - 00:30:59.226, Speaker B: Yeah, so as I said, reliable broadcast is really not to replace consensus.
00:30:59.258 - 00:30:59.406, Speaker A: Right?
00:30:59.428 - 00:31:33.980, Speaker B: It's not like we take out consensus and put in reliable broadcast, but rather you have the mental, which everyone has anyways. You have it work this way and then the only thing that consensus can really provide you with is ordering. So what do you lose? Well, you lose ordering. And for most blockchains, that's prohibitive, right? This isn't something that most blockchains can adopt. Ethereum cannot tomorrow say, oh, yeah, that was a cool talk, let's do it. What does the finality looks like? Should be pretty fast now.
00:31:35.790 - 00:31:36.426, Speaker A: Because there.
00:31:36.448 - 00:31:37.690, Speaker C: Is no consensus here.
00:31:37.760 - 00:32:03.442, Speaker B: Yeah, so I think this is actually a really good question and something that we need to explore more because reliable broadcast is guaranteed to terminate for honest actors. So that means if you're honest, it's guaranteed to terminate. But it doesn't say anything about when in practice. If you use these certificates, you know that two thirds of stake basically have the data, so it's easy to pull.
00:32:03.496 - 00:32:09.074, Speaker C: It because I'm trying to understand at.
00:32:09.112 - 00:32:12.486, Speaker B: What point am I because you could.
00:32:12.508 - 00:32:15.794, Speaker C: Have multiple different parties that could be competing for a different ordering.
00:32:15.842 - 00:32:16.600, Speaker D: For example.
00:32:18.650 - 00:32:25.638, Speaker C: In that case, I'm not sure what exactly would be the finality condition for determining the right ordering.
00:32:25.814 - 00:32:36.346, Speaker B: No, we wouldn't have something like that because there is no ordering whatsoever. So you only have all these nodes which just receive the copies of the transaction similar to a mempool.
00:32:36.378 - 00:32:36.526, Speaker A: Right.
00:32:36.548 - 00:32:42.350, Speaker B: So we're strictly removing the overhead of having consensus on top of the MEMP pool.
00:32:43.410 - 00:32:44.350, Speaker C: Does that make sense?
00:32:44.420 - 00:32:45.102, Speaker A: Okay, I see.
00:32:45.156 - 00:32:56.610, Speaker B: So the finality like latency should be strictly lower. Again, we haven't implemented this yet. Cannot really give you any numbers, but fingers crossed.
00:32:57.690 - 00:33:01.362, Speaker D: Sorry, I'm still confused by the execution sharding.
00:33:01.426 - 00:33:25.262, Speaker B: So if you have very complicated smart contract dependencies, how does this sharding actually works? And also I think in the talk you mentioned, for example, all spend transaction is going to be put in that shard. I'm confused. Isn't all transactions a span transaction? Yeah, so like outgoing transactions. Really? The question is can you give us some example?
00:33:25.316 - 00:33:27.674, Speaker D: How does the sharding actually work the cycle?
00:33:27.722 - 00:33:28.320, Speaker C: Example?
00:33:28.690 - 00:33:50.342, Speaker B: Yeah, it's similar to a roll up in some senses, but I guess the main case that is different is let's say you're on shard one and I'm on shard two. Here, you can just very just send me some tokens. But you just tell that to your executor. You cannot tell that to my executor. Right, but how can I make sure.
00:33:50.396 - 00:33:57.874, Speaker D: That someone send it to you and send transaction to you and some other people send transaction to me? How do we know that the transactions.
00:33:57.922 - 00:33:59.270, Speaker E: Would not have dependency?
00:34:00.250 - 00:34:01.234, Speaker B: What do you mean?
00:34:01.372 - 00:34:14.910, Speaker D: For example, my transaction is interacting with one particular DApp and your transaction is also interacting with another DAP. They send it to you as a shard executor and also one send to me as another shard executor.
00:34:15.250 - 00:34:31.042, Speaker B: Yeah, well, all of these are associated to a shard, so this gets checked explicitly. This is not like just hope and pray. It's being checked explicitly by and needs to be proven as well by the executors that this holds now.
00:34:31.176 - 00:34:37.026, Speaker D: But doesn't that checking require some communication between you and me and that kind.
00:34:37.048 - 00:34:53.260, Speaker B: Of you mean executors or yeah, executors. No, let's say you're a user in your account says shard one and I'm an executor. Then I just need to prove that it says shard one in your account.
00:34:55.310 - 00:34:58.022, Speaker D: Okay, well, maybe that's top offline.
00:34:58.166 - 00:34:59.082, Speaker B: All right, listen.
00:34:59.216 - 00:35:05.286, Speaker A: Yeah, you mentioned that executor is like a large node.
00:35:05.318 - 00:35:07.066, Speaker D: It seems like a pretty important node network.
00:35:07.098 - 00:35:08.320, Speaker A: Who's running those?
00:35:09.330 - 00:35:16.558, Speaker B: Yeah, I mean it's analogous to something like a roll up sequencer. Well, that depends, right, because some roll ups now are thinking, oh, we're going.
00:35:16.564 - 00:35:17.758, Speaker D: To split up the sequencer from the.
00:35:17.764 - 00:35:54.618, Speaker B: Proverb, from the executing client. It is a really beefy node one advantage of the network, and now I'm really going beyond the consensus thing, but they can be centralized without harming the network because you have this global state. So you can basically just switch between these and by the way, that induces more complexity, which I did not want to go into. Now basically, they can be beefy computers without harming the censorship properties of the system. You obviously have Snarks to prevent invalid computations. So no, invalid data transitions can take place. But additionally, you do have censorship resistance.
00:35:54.618 - 00:36:04.930, Speaker B: But yes, they're going to be beefy as hell. It's I wonder if you can dynamically.
00:36:05.510 - 00:36:12.946, Speaker C: Change the partition, because if you can do that, then if my state on.
00:36:12.968 - 00:36:17.266, Speaker B: The yellow partition, if I have to do some interaction with the one in.
00:36:17.288 - 00:36:21.250, Speaker C: The red partition, I don't have to talk to the executive director.
00:36:21.930 - 00:36:39.610, Speaker B: I don't think I understood the question. So that has to explicitly be done. Like, you would change the partition basically via the transaction that says, hey, I'm now on this shard, but I want to jump here, and then you do that. Did I understand your question correctly?
00:36:40.590 - 00:36:42.954, Speaker C: Yeah, I think my question is more.
00:36:43.152 - 00:36:45.040, Speaker D: Change the partition later on.
00:36:45.410 - 00:36:53.386, Speaker B: Yeah, but again, the partition only can be done explicitly. This is like sort of analysis in that sense to bridging between blockchains.
00:36:53.418 - 00:36:53.662, Speaker C: Right.
00:36:53.716 - 00:37:01.730, Speaker B: Like it will not just happen on its own. It needs to get invoked on purpose by the participants.
00:37:07.910 - 00:37:13.410, Speaker C: It's interesting. We are in an all capital city of Pyzantines.
00:37:13.930 - 00:37:15.800, Speaker B: These things are best place.
00:37:16.890 - 00:37:19.142, Speaker C: My question is, does this system work.
00:37:19.196 - 00:37:30.006, Speaker B: Under Asynchronous systems like IoT or yeah, I think they are inherently Async.
00:37:30.038 - 00:37:30.620, Speaker A: Right.
00:37:31.070 - 00:37:42.000, Speaker B: So I would say yes, but don't take that as a short answer. Not sure.
00:37:51.410 - 00:37:58.350, Speaker C: So thank so thank you, Olivia, for his presentation.
00:37:58.430 - 00:38:01.090, Speaker B: So, next. Welcome, council.
00:38:11.870 - 00:38:33.210, Speaker D: Everyone. My name is John Victor. I'm the lead at Protocol labs. So I work on IPFS and all coin things. So today I want to talk a little bit about how we go from data to storage. So before talking about filecoin, I want to talk about IPFS. For those unfamiliar, maybe the pitty liner is IPFS content addressing.
00:38:33.210 - 00:38:36.800, Speaker D: Filecoin is about markets. Sorry.
00:38:37.250 - 00:38:37.662, Speaker C: Yeah.
00:38:37.716 - 00:39:06.970, Speaker D: So maybe to preface, why do we care about content addressing? What does that enable? Maybe start with what content addressing is. Typically, when we think about how we reference data on the web, we're doing it based on locations. We're saying, go to this specific server, go to some path inside of that server. This is the data that I'm referencing. And of course, that's not a very specific way of referencing data. If we were talking about humans that way, it would be quite ridiculous to say, I'm the human that lives at 300 DRAM Avenue or works at 15 Little West twelveTH Street. You'd have no way of knowing that is the same person.
00:39:06.970 - 00:39:54.550, Speaker D: Instead, when we talk about content addressing, we're. Saying, what if we could build the web to reference pieces of data based on fingerprints, or in this case, content hashes of the data itself? And this is really the thing that IPFS is focused on. In the analogy I have with a human, instead of talking about where that human lives or where that human works, we're talking about properties of the human. It could be their eye colors, their genetic sequence, whatever, but something so specific that we know that it's that human regardless of where they are. So when we build applications with content addressing as core primitive, what does that enable? Well, thing number one is it means that we can build applications that run local first without having to go to some central server. We can have applications that just work with the data that might be sitting locally on the device. This also implicitly means that we can build applications that run at the far edge, as Boris and the Vision team will talk later on.
00:39:54.550 - 00:40:04.026, Speaker D: This can be really powerful when we have applications where you may be in bandwidth constrained environments. Maybe it's an airplane, maybe it's in some part of the world that doesn't have Internet access.
00:40:04.128 - 00:40:05.194, Speaker C: Maybe it's in space.
00:40:05.312 - 00:40:39.318, Speaker D: These are all places where you would want to be able to do things with data without having to go out to some third party server to go grab it. Third, it also means that we can make use of hardware wherever that hardware is. It could be a user's device. As the new MacBooks get more and more powerful, it could be an on prem server. So if you are a business that has your own server, it could even be a third party cloud across all of these different hardware platforms. We're still using the same content hash, which means that our application is no longer tied to one specific server, but rather the data structure itself. So why does this matter? Well, one is it gives you better security.
00:40:39.318 - 00:41:07.150, Speaker D: The data is Verifiable, which means that you can actually start building applications. Trust doesn't emanate from some arbitrary server or who owns that server, but rather from the data itself. Regardless of who serves you the data, you can always verify that the data that you asked for is the data that you got. This also means better resilience. When you have maybe one server goes offline. So long as you can ask someone for that piece of data that you're looking for and you get the right bytes back, you can still have your application run. So this gives you better resilience.
00:41:07.150 - 00:41:55.226, Speaker D: And third, and maybe this is a little bit more like future looking, but this does ride on broader trends that we're looking at in terms of hardware improvements, in terms of data generation, in terms of the requirements for Verifiability as computers and AI do. More. Now, this is all sort of like the precursors. I didn't want to go too much into IPFS, other people will talk about it later today. This may lead to a question of what does Filecoin do in this vision? And really if you sort of bought that very short and maybe too fast pitch on IPFS, the thing Filecoin is really trying to say is if we believe this world that content addressing really is the future and that there are valuable reasons why we want to build all applications irrespective of how bought into web. Three, you are with content addressing. The best that Filecoin makes is that there's two divergent paths of the Internet.
00:41:55.226 - 00:42:41.658, Speaker D: One is we use content addressing everywhere, but we still just use the hyperscalers whenever you don't use compute on a local device. The other is that we can build markets that say for the services that I go to the cloud for, can I build an open market where I can just go to whichever computer can do this service the best to provision that offering. So if we were to take a 3000 foot view of Filecoin, and for those of you who have not checked in since the 2017 white paper, this may look kind of different. But this is what we're building. Filecoin is building open services for data to serve web scale applications. And this is all shared or anchored into the same shared rails. I'll come back to this diagram, keep it in your brain, but I'll keep going through it so we can pick through where things are on each of these sort of little modules.
00:42:41.658 - 00:43:30.394, Speaker D: This is not to say that we have fully built all of this and it's 100% perfect. This is like the orienting vision of what are the core components that we need that will continue to mature, that get us to this end state of how do we build open markets for the web. So go to market. What is Filecoin's master plan? Well, we've broken it down to three main stages is to attract scalable data storage. This is really just talking about how do you build out your supply side, attract x bytes and capacity things where you can start actually competing against web two at scale, build out a network that can store large amounts of data. Part two is to start filling that capacity, start bringing more and more data into that network and leverage the cheap storage because you have massive supply in order to actually bring a competitive offering to market. This also implies needing to shave down the friction and onboarding pathways.
00:43:30.394 - 00:44:28.234, Speaker D: Because fundamentally, when you're building in these open markets as your back end, there's a lot of new problems that you have to introduce and maybe we'll talk about them at the end. The third is bringing out services that make that data useful. Once you've built out your Internet scale hardware, once you've added data on top, you want to start adding more and more services that can compound the utility of the data that's sitting on those hard drives. This includes things like Compute. So now that I have the data sitting on a hard drive, how do I run transformations? If people want to run Compute jobs, how do I make it more accessible? How do I serve the wide range of applications that have different retrieval properties to them? How do I start programming services around them? And this is where FBM and IBC come in. So now I'm going to go quickly through each of these different buckets to sort of say, where are we today and how are these things progressing? And then we'll have time at the end and sort of talk about whatever questions folks have. So part one of that vision was scalable data storage, which was attracting all of this X byte scale hardware.
00:44:28.234 - 00:44:57.510, Speaker D: So how are we doing on that? So today there's about 3400 storage provider IDs on the network. You can see this little heat map of where are these providers today. This is not perfect coverage. As you can see, we have a lot of representation in Asia, some in Australia, a lot in Europe, a lot in North America, but still some noticeable gaps in South America and in Africa. But in totality we have about ten Epabytes of storage capacity. For those unfamiliar. It goes like gigabyte terabyte petabyte x byte.
00:44:57.510 - 00:45:36.006, Speaker D: So we're large is, I guess, one takeaway from the slide. We're in 40 countries, but hopefully continuing to grow, especially as the market picks back up. We hope to see more and more storage providers start standing up operations in these areas where there's gaps. Part two was massive data onboarding. You can see at the bottom this chart, which shows since February of 2021 how data onboarding in the file coin network has grown. We've gone from under a petabyte to about that's like one exabyte and some change almost two of data onboarded into the network. And you can see that it's coming from all over.
00:45:36.006 - 00:46:08.158, Speaker D: Some of these are Web three users. They're folks like Dala Games, Openc, a bunch of NFT projects, but also some traditional players as well. You have folks like Berkeley, who's storing particle physics data. You have CERN, who is also storing particle physics data. You have New York, who's storing open data from their Open Data project, the Internet Archive and more. For a lot of these institutions, the core driving property here is not because, I mean, for some of them it's like testing the waters, see what the future of the cloud could look like. But a lot of this is also just I have a lot of data, this is a cheaper offering.
00:46:08.158 - 00:47:07.380, Speaker D: This makes sense. This slide is wrong. The third part here is like, how do we enable web scale applications? And I'm going to touch on from storage into retrieval, compute the FBM and Interplanetary consensus to move into retrieval. So when we talk about retrieval markets in 2017, if you go back to the white paper, there was a number of sections, and people really focused on the storage part, but there was a call out, actually, for retrieval markets and this vision of building basically like the other half of what you need to see. The reads and the writes. When we talk about retrieval markets, what we're really pushing for is how do we build distributed CDNS? And so why do we think retrieval markets are a good idea? Well, content addressing, as I mentioned before, means that we can have verifiable data that is served over untrusted servers. When I can verify the data itself based on what the bytes are that I get, I actually don't care which server gave me the data because I don't have to worry about you sending me malicious things.
00:47:07.380 - 00:47:37.126, Speaker D: Two crypto incentives are quite good at building up and aggregating resources. When we talk about a CDN, the thing that really matters is getting as close to users as possible. What you want is points of presence that are physically as close as possible because then the latency for delivering that content can be lower. So if you combine these things together, you get a dcdn. If you can take incentives, you can have a permissionless network of providers stand up, hopefully getting as close to the users as possible. We can then serve these Verifiable pieces.
00:47:37.158 - 00:47:38.874, Speaker B: Of data to the users themselves.
00:47:38.992 - 00:47:51.314, Speaker D: The users can trust that the thing that they got is the thing that they wanted. And you now have the magic of using the incentives of Web Three to build a CDN that might be larger than any of the ones that or hopefully at the end scale, is larger than anything that we have from Web.
00:47:51.352 - 00:47:54.610, Speaker E: Two where we are today.
00:47:54.760 - 00:48:40.740, Speaker D: This is actually just a heat map of Saturn. So Hannah from the Saturn team is here, so hopefully she'll be talking a little bit about some of this stuff too. Today there's over 2200 points of presence around the world, a median time to first byte of around 66 milliseconds. You can see some of these other stats, but I want to highlight one important piece, which is Saturn is just one of their triple market teams that's building in the filecoin ecosystem. So I think what's really cool about Web Three is when you have a lot of people working on the same shared interfaces, you have many bytes of the apple. And so what we see is like, Saturn is doing amazing work and sprinting quite far ahead, but we also have the opportunity to have experimentation on what are the other ways to fill these gaps and continue to build out to cover the rest of the world. Computer for data.
00:48:40.740 - 00:49:33.778, Speaker D: So computer for data may seem kind of weird of why should this be paired with the storage and retrieval side? And a lot of this comes back to this idea of the interplanetary principle, which is something that Juan, who founded protocol Labs talks about quite often, which is really trying to minimize these round trips and do operations wherever locally you can. And the reason for this is kind of simple and you can sort of motivate it by just thinking about data having gravity. If you have a petabyte of data, it's actually quite expensive to move that around. Like if you just think of what would it take to move it from one data center to another, you're introducing both latency and cost. And so instead you would rather move the compute to wherever the data is. Because if you have a compute job you can send whatever request you have down to the data itself, run the process, get the result and either ship the result or store it locally there for someone else to come and compute on later. When we think about compute though, of course there's different trade offs that you're going to have to make.
00:49:33.778 - 00:50:04.790, Speaker D: And so this little diagram is what Juan calls Juan's triangle. But you can sort of think of there being some contradictory properties that are sort of at play and depending on the application, there's different things you may want to optimize for. So if you want to do privacy, you might want something like fully homomorphic encryption, which is something that Sama as an example is working on. This gives you privacy. You don't have to reveal your data to the person who's doing the compute. The downside is you're adding all these extra cryptographic cycles on the other bottom left triangle. Maybe you care about verifiability.
00:50:04.790 - 00:50:48.810, Speaker D: You'll want something like recursive Snark so you can verify the compute all the way through. Teams like Lurk are working on things like that. Of course, again, you're adding in these additional cryptographic cycles and so for both privacy and Verifiability they may satisfy a condition. But if performance is what you're looking for, you may want to opt for something closer for like Bacolau, which is doing distributed compute in a more optimistic way. And so the goal with what we're building as an ecosystem is not to say like there is a single compute that will rule them all, but more observing that there are inherent trade offs you'll have to make. And ideally what we can do is build interfaces that allow us to basically say for the application or even the subpart of an application. You can make the appropriate trade off that you need to for whatever thing that you're building and compose with everything else in the ecosystem.
00:50:48.810 - 00:51:28.680, Speaker D: I'll also note the way that both the storage providers are built. I don't know if folks are aware, but Filecoin is probably the largest in production use of Snarks as it is today. All of them have a large amount of GPUs and CPUs. This also makes them a prime target for actually deploying these compute jobs, as are hopefully when Saturn goes live, the Saturn knows as well. It's interesting as a brief aside, there's a new team in the Solanic ecosystem, Ionet, who's building this large ML network. And I think actually they're sourcing their GPUs from three different networks, their own render and Filecoin. And ironically, Filecoin is the largest in terms of the GPU sources that they have.
00:51:28.680 - 00:52:02.594, Speaker D: Okay, so I have very quickly whipped through all of the things that are happening off chain. These are all the services that from the Filecoin ecosystem, we want to be invocable things that you can call out to and have different services in the network perform. So how do we actually do that? This is where the FBM comes in. So the FBM itself is semi complicated. It's a WASM based VM, but it's designed as a hypervisor. And the idea with the FBM is that you really want to bring programmability around File point state. And so this means a number of things.
00:52:02.594 - 00:53:03.506, Speaker D: The two ways I describe it is it gives you programming data. So if you want to do things like invoke a storage request and say like, someone please store this piece of data, that's something you could trigger from the FBM. It also means that you can do things in the future invoke a compute request, say, like someone who is holding on to this piece of data, please perform this computation, and then return those, store the results, and then post them proof on chain and then release the funds. So those are things that you could do with the Fem. It also gives the ability to program payment around those services. And so if you wanted to pair those two things together, you can think of things like building a perpetual storage protocol as really saying, how do I build a data endowment where I use DeFi to generate yield and make sure that the yield I'm generating is larger than the ongoing expense of my endowment? But it also gives you the programmability to say, if one of my five copies of this piece of data falls off the network, I want to set a bounty that the next person who stores a copy gets to claim a little prize. This also gives you flexibility to start doing more and more sophisticated things.
00:53:03.506 - 00:54:15.470, Speaker D: It allows for saying, how do I have multi currency payments? So if I want to have a compute network that pays it into coin, you can happily do that and then use an AMM to swap to whatever currency that storage provider wants. And of course, like, storage options are things that people have talked about. Actually, the biggest thing happening on botcoin DeFi right now are these collateral markets where storage providers are locking or using these lending markets to effectively acquire more collateral to grow their businesses faster. And it also leads to some interesting use cases for things like basically like tokenizing cash streams of these various businesses that anchor onto Balkans Rails. The last sort of piece here is Interplanetary Consensus, which is really about how do we scale sort of like the abundance of all of these things. IPC really is focused on that top right, which is solving for basically a bunch of what would seem like contradictory properties. So if you want something that is Internet scale, so you have trillions of transactions per second, and this is not for a single network, but across a network or a hierarchy of networks that is at contradiction with something that has maybe global censorship resistance.
00:54:15.470 - 00:55:09.678, Speaker D: If you want super fast local finality, that may also be in contradiction for something that has secure global finality. If you're worried about nation state attackers. And the reason I put this up is it's more starting from in the top right for what are the requirements that you want? But not saying that one network has to solve all these problems. It's just saying that across a hierarchy of networks, you need different regions of these networks to solve these problems. So at the base layer, so at the root chain of IBC, you can have in this case, this is the filecoin network. You can have the thing that we canically think of from Blockchains, which is like global censorship resistance, like all the good stuff. And you can hold your value and do all that stuff there, but you can have a pathway to get to less secure regions of the network where you can make the explicit trade off to say, hey, I don't need global, censorship resistant whatever, but I can build a local CDN region which allows me to cache data and just send it.
00:55:09.678 - 00:55:57.770, Speaker D: Like the analogy others have used is one way that you might conceive how this would work is you could have a region of a CDN that operates even in one specific geography, like AWS East. And you don't have to hold all your value there, you just have to inject small amounts of money and just spend it down. But then you have the ability to do things like have super fast delivery of content that can operate at the performance that you need to compete with a cloud layer. Some of the things that makes IBC kind of interesting is it's on demand horizontal scaling. Today it's using side chains, but the way that it's being designed, it could be roll ups in the future that's like on the roadmap, you can swap out the consensus mechanism so it doesn't have to use the stuff that cloudflare uses in its base layer. You could use things like tendermint, you could in the future theoretically use like avalanche or polkadot. There's more things that you could sort of slap in there.
00:55:57.770 - 00:56:42.874, Speaker D: And it also comes with this native messaging property, so you can send messages up and down the hierarchy. This also means because you can go up and down the hierarchy, you can go across the hierarchy as well. But that's maybe a deeper talk. But yeah, putting all this together. So Popcorn's Master plan really is around literally, how do we acquire all the hardware, how do we make that hardware useful? And then literally, what are the raw pieces that we need to do that we think of them in these three major buckets of storage retrieval compute. These are all in various degrees of completeness, but the important piece is once you have these three different types of services and have the composability through things like the FBM and Scalability through IPC, you can actually start building really complex applications. So there's some really good papers that are out there.
00:56:42.874 - 00:56:58.930, Speaker D: If you go to IPC safe you can see a really good one from the IPC team about what would this look like if you were building a fully on chain game that really relies on all these different properties? Yeah, this is sort of like our vision for how do you actually build out to web scale applications running on web3 rails.
00:56:59.750 - 00:57:01.060, Speaker E: Yeah, thanks.
00:57:09.370 - 00:57:19.942, Speaker C: For speech. So anyone have questions? So let me ask the first question. So, for the services being built like.
00:57:19.996 - 00:57:28.054, Speaker B: Storage machine, they all need to synthesize to do the job right?
00:57:28.092 - 00:57:34.590, Speaker C: So from Portfolio, are you guys want them to each have their own tokens.
00:57:37.330 - 00:57:43.454, Speaker B: Because of the price of token in some way and those server providers might.
00:57:43.492 - 00:57:51.494, Speaker C: Be incentivized to do the work that has most rewards so that the network of the service are somehow invalid.
00:57:51.642 - 00:58:32.350, Speaker D: Yeah, I don't think we are trying to specify like, oh, one tokens rule them all. I would say think of the role of Filecoin closer to the role of Ethereum where it's like you're using Filecoin to pay for gas. People can have whatever, ERC, 20s they want. They could also pay in rap fill. I also think once you have DeFi, it's a little bit less important because the miner will just convert to whatever currency they want to bolt in the end. I do think it's interesting more for things like Subnets where it's like if you're a compute network, having the ability to have your own gas token for that subnet. So as you distribute load to potentially just small clusters of SPS, that might be actually a better use case for your currency rather than just using it as a medium of exchange.
00:58:32.350 - 00:59:27.722, Speaker D: But yeah, so I think filecoin is trying to build more flexibility, not necessarily lock people into a specific mode. I imagine what we will see is things like so I mentioned this idea of data having gravity. I think one thing that's probably underweighted inside of crypto is at least for the people who are focused on deep end, that that probably means that there's going to be biding wars for GPUs. More specifically, if you were to compare a GPU that's sitting in Buenos Aires versus a hard drive that's in Scotland, that GPU, to quote a Compute job, would have to also price the bandwidth that it would take to grab the data off this hard drive, move it down, do the Compute and send it back. That's going to inherently be more expensive than the GPU that's sitting next to the hard drive. And so what's going to probably happen is the GPUs that are probably the most valuable are sitting next to the most amount of data because they will have a structural advantage of not having to pay this retrieval and distribution cost. And so I think you're going to see a consolidation of hardware.
00:59:27.722 - 01:00:14.366, Speaker D: Again, this is my perspective, but also very biased. I think you're going to see a consolidation where all of these deepen networks are going to layer on top of each other purely for cost efficiency reasons. Which is partially why things like IBC are kind of interesting, because it gives folks an opportunity to build, whether they want to be a roll up on filecoin or be a side chain to filecoin. An opportunity to be more compatible with the ecosystem. Thanks for an awesome overview. I've never seen everything put together so succinctly, especially with everything that Filecoin and IPFS does. One, I really like the idea, like you said, what instead of address based addressing like topology or whatever, where it.
01:00:14.388 - 01:00:17.246, Speaker C: Sits, the content addressing, especially with what.
01:00:17.268 - 01:00:19.274, Speaker D: You just said about the consolidation of hardware.
01:00:19.322 - 01:00:20.906, Speaker E: If you have a network where everything.
01:00:20.948 - 01:00:30.902, Speaker D: Is addressed by content and it's kind of like location agnostic, how can you ensure that the GPUs are next to whatever you need? Or especially in your thing with a dcdn is a CDN crypto and content.
01:00:30.956 - 01:00:32.518, Speaker E: Addressing all in one, how do you.
01:00:32.524 - 01:01:11.874, Speaker D: Ensure because a big thing about CDN, right, it's about putting low latencies to users. So how do you guarantee that? Yeah, so I mean, there's various ways that people discuss this. I think actually the core insight is going to come from things like these consensus hierarchies. So there's a great talk if you go to YouTube and type in IPC applications one has on how he thinks about these things and I'm totally going to just crypt from there. But I think one of the ideas is building out regions as like a concept. So one of the things that he's sort of talked about is like you could have a subnet that is just of a specific region and then you can have subnets inside of that. So that might be like a region and then go even further down even to the city level.
01:01:11.874 - 01:02:12.462, Speaker D: And you basically want to be able to index into these different subnets to place content there. And so I think there is some properties of like, okay, so how do I know that this subnet maps to the Atlanta region or something? I can imagine a few ways that you get there where you can take a hybrid of off chain and on chain information. There's a sort of example like a lot of the storage providers in Filecoins network, they are all businesses and so ernst and Young is going through and working with a number of them to do sock two certifications. And that's never something that you're going to cryptographically verify that you are all fully secure, whatever. But you could tie that with a verifiable credential to say like, hey, this storage provider ID, which is like an ID that they have to get through mining over this period of time, it's not easily replaceable. That is tied to this attestation, which is from a known set of entities, whether it's KPMG, whoever, whoever, whoever, that shows that they have this thing. And so you could think of using those identifiers tied to the network primitives.
01:02:12.462 - 01:02:59.746, Speaker D: So the network primitive of what is the storage provider ID, which is unchanging to something that is like a verifiable assertion from a number of known parties. And you can choose how many you need to be competent, but that could just then give you the delegation of like, oh, this thing is in an XYZ region. For some of the lower subnets. You may also add latency parameters where it's like you just jack up how fast need these things to run where it wouldn't be possible as possible to be in a location that's outside. Just because the time the finality would be too fast and the speed of light wouldn't let you get there. So my answer is probably like it's a mixture of things both merging on chain and off chain information. But then also if you set network parameters anchored against some of that, it would be impossible to participate in consensus if you were in a different geography and the latency isn't high enough.
01:02:59.848 - 01:03:01.522, Speaker C: Yeah, no, thank you, that's cool.
01:03:01.656 - 01:03:03.106, Speaker D: That was part of why we're all.
01:03:03.128 - 01:03:06.520, Speaker E: Hearing is tumble instead of at home. The other thing.
01:03:08.250 - 01:03:19.318, Speaker D: In your answer, you touched on Sock Two compliance, especially like with that stuff, we're starting to hear a lot about censorship and nodes and just compliance is now creeping into our beautiful Garden of Eden.
01:03:19.334 - 01:03:20.380, Speaker E: That is web3.
01:03:21.230 - 01:03:31.786, Speaker D: And if anyone else is interested, there's a phenomenal paper outside of I think it's the University of Athens that looks at malicious threat using Web Three as a base for command and control servers.
01:03:31.818 - 01:03:32.590, Speaker E: And things like that.
01:03:32.660 - 01:03:45.460, Speaker D: So I'm wondering, from the combined side, like you just said, hey, you're looking at it's no longer agnostic. People say like, hey, I want Sock Two data, sock two providers, like with my IPFS to the side.
01:03:46.710 - 01:03:49.414, Speaker E: I don't know, just do you have anything to talk to on that?
01:03:49.452 - 01:03:51.782, Speaker D: As any nation states like we have now?
01:03:51.916 - 01:03:58.514, Speaker C: What is it? Flashbot sensors and stuff?
01:03:58.632 - 01:03:59.954, Speaker D: Because based in the US.
01:03:59.992 - 01:04:00.980, Speaker E: And all that stuff.
01:04:03.110 - 01:04:03.698, Speaker B: I think this.
01:04:03.704 - 01:04:16.966, Speaker D: Is actually like a really important discussion. This is like my personal answer, but also one of the two philosophical debates I had when I joined Cos. I think it actually okay, there's two versions of the world. There's one where you don't censor anything. And one where you censor everything and.
01:04:16.988 - 01:04:18.230, Speaker B: Both of those are horrible.
01:04:18.650 - 01:04:58.934, Speaker D: I think the thing that you don't want is to say the worst things in the world are propagated. You also don't want to say you are choosing what are the worst things in the world because if someone else is choosing them, that could be quite bad. I think the only answer to that question is how do you allow individual company at different levels? And if you are a storage provider, you are a business that operates inside of a country, you will have to make whatever choices you have. But that's also part of an intentional decision then to separate one. How do you reference content that does not require you to store things on that specific storage provider? You could also store things in a different country or not even on the network itself. You could just serve it yourself if you individually want to do that. I think this is where kind of the beauty of the compatibility of the system comes in.
01:04:58.934 - 01:05:24.094, Speaker D: I do think every individual entity on the network should be able to make whatever choices make sense to them. And I wouldn't call it censorship to say if many people don't individually want to serve you, they shouldn't be required to. But then you always have the option to serve yourself or serve for yourself. And if you are offering your own content, address data into the Internet, other people can discover it and that's up to you and your prerogative. So then I guess the censorship is.
01:05:24.132 - 01:05:27.546, Speaker E: Left to the prerogative of data providers and everything that can be built upon.
01:05:27.578 - 01:06:15.840, Speaker D: Rpfs well, I mean, so like File point itself is like every individual one of those providers. And remember, if we go there, Filecoin is answering this question of what happens when you want to use a third party's server. I think it is always going to be at the prerogative of those people. But again, we're talking about an open market. Your routing path could also just push you through other countries or places which are more friendly jurisdictions. It doesn't mean that the only pathway has to be one server in whatever location. So I think this is where you get the balance, where it's like, yeah, everyone is making their own individual decision, but it also doesn't mean that you're locked into what any individual has decided, which I guess at least.
01:06:18.450 - 01:06:19.360, Speaker C: Thank you.
01:06:20.050 - 01:06:21.902, Speaker D: Yeah. So this is a more general question.
01:06:21.956 - 01:06:23.078, Speaker B: Just about the File coin.
01:06:23.114 - 01:06:47.218, Speaker D: So I've always been very passionate about Filecoin and seeing your presentation, I think just reinforces that. But I think one issue I face trying to better understand Filecoin is that, for example, for Ethereum, how we learn Ethereum, we first spend transactions, interact with smart contracts, run our own node, so figure it out. But that part is for Filecoin is.
01:06:47.244 - 01:06:49.994, Speaker E: Quite difficult because, for example, for our.
01:06:50.032 - 01:07:07.918, Speaker D: Daily average person's data usage biocoins, I don't think currently serves that I interact with the biocoints at a sales team or whatever. They said my data demand is just too small. And also I think our ordinary hardware is not able to actually do the.
01:07:08.004 - 01:07:10.814, Speaker E: Data provider, so that actually hinders a.
01:07:10.852 - 01:07:24.194, Speaker D: Deeper understanding of how things actually work. So do you have any comments on like, if we want to understand it more deeply on a technical level, like how we go through learning ethereum do you have any comments or suggestions on.
01:07:24.232 - 01:07:25.220, Speaker C: How to do that?
01:07:25.610 - 01:08:10.450, Speaker D: Yeah, for sure. One thing I'll say is alcoin as an ecosystem is almost like, I won't say entirely orthogonal, but it's a very different bet on what are blockchains good for. And I think part of what made it possible to scale size that it has also applied taking a very specific pathway, which made it not so ideal for the way that crypto normally builds up its sort of following. For what it's worth, I do actually think the network is getting easier to use. So there is a great app you can now download called Station, which is not storing your data, but it allows you to help participate in the network by doing things like uptime checks. Eventually you'll be able to run other types of jobs on Station nodes. And that's actually the sort of thing where it's getting a little bit more end user tangible.
01:08:10.450 - 01:08:52.980, Speaker D: There's teams like Banyan who are also working on different applications that work at different scales. Teams like Web Three storage which work with the Saturn folks. So there are more pathways for onboarding. But I do think the UX is a piece that has historically been really rough but is slowly getting shaved down in terms of how it gets easier in terms of learning more. I do think now that the FPM is there, there's more opportunities to build more, like consumer facing applications. Actually, I'll say the things that I would put in the bucket of what makes building more consumer facing applications easier. Once you have small data onboarding, more small data use cases, things like Saturn can help with that, web Three Storage can help with that, daniel and Co can help with that.
01:08:52.980 - 01:09:28.460, Speaker D: And then also when you have smart contracts, it gives more places where people can tactilely touch the different parts of the ball. Quinn ecosystem. But I think as more of these pieces get built out into things like patient, even honestly, where it's like you won't get every compute load because it won't make sense to you, but you might be the deploy target for something that can run on a laptop or something that can run in less connected environments. In terms of the actual content, I would encourage you to go to Filecoinclvr IO, where I write a lot of blogs which has some of this stuff. But yeah.
01:09:31.230 - 01:09:32.394, Speaker B: If you'd like to complain.
01:09:32.442 - 01:09:34.094, Speaker C: About anything that's hard to use, please.
01:09:34.132 - 01:09:35.854, Speaker D: Come see me and I will take.
01:09:35.892 - 01:09:37.886, Speaker A: Those complaints and I will feed them.
01:09:37.908 - 01:09:39.870, Speaker D: Back to the appropriate teams.
01:09:40.390 - 01:09:41.380, Speaker C: Thank you.
01:09:44.230 - 01:09:46.100, Speaker B: Okay, so thank you.
01:09:49.030 - 01:09:53.140, Speaker C: Okay, everyone, please take a seat. So next is going to start soon.
01:09:54.070 - 01:10:11.238, Speaker A: Hey, everyone. Just a quick survey because I just want to get sent to my audience a little bit of different proof that I'm used to speak to. How many people in this audience have heard of IPFS?
01:10:11.414 - 01:10:11.946, Speaker C: Yeah.
01:10:12.048 - 01:10:23.470, Speaker A: Oh, good. All right. How many people in this audience have installed on a computer some form of IPFS? Okay. How many people have heard of a content Identifier?
01:10:24.130 - 01:10:24.542, Speaker C: Okay.
01:10:24.596 - 01:11:03.130, Speaker A: All right. I just want to get a sense of what's our baseline because it's originally given an IPFA conference. My name is Hannah. I'm going to talk to you today about how we build IPFS systems that really work, that actually work. I've been working on IPFS also filecoin for about five years. This talk is my attempt to share what I think I've learned along the way. Sometimes the lessons have been painful, and I'm trying to help you avoid some pain by sticking to best practices.
01:11:03.130 - 01:11:58.140, Speaker A: Strap in. It's going to be an adventure. Also, just to let you just a heads up, I do have a sort of irreverent, joke heavy style of presenting, so let's just go with it. It's okay to laugh or not laugh, because then it's not funny. Cool. All right, part one, the dream. How do we get here? Why are we all stressed out? This doesn't have to apply to IPFS, but why are we all stressing ourselves out trying to make the whole Internet work in a completely different way than it already does? In the case of IPFS, why are we trying to make it easy content addressing, or in the case of web3 in general, why are we trying to do this completely new way of build a completely new stack for building applications of the Internet? First of all, how did you all get here? I'm going to ask you.
01:11:58.140 - 01:12:25.442, Speaker A: You can answer this in one of two ways. Because, again, it's a general audience. Why do you work on IPFS if you work on IPFS? Or why are you in web3? Like, why are you not writing, collecting a great salary, working for some gang company? Raise your hand. Yes. In the back. What? Secure? What?
01:12:25.496 - 01:12:27.430, Speaker C: No, the q PFP.
01:12:29.530 - 01:12:34.022, Speaker A: NFPs. Oh, yeah. For sure. We have better images, little images right there.
01:12:34.076 - 01:12:35.410, Speaker C: Yes. Open networks.
01:12:35.490 - 01:12:42.954, Speaker A: Open network. Yeah. We'll leave in openness, leaving things that should operate together, not work in silos. I think I had one more over here.
01:12:42.992 - 01:12:45.050, Speaker B: Yeah, I got a great salary.
01:12:47.310 - 01:13:25.378, Speaker A: You got a great salary to do something more interesting. That's actually a good reason. Well, let me tell you how I got here. I heard this is a research heavy event, and I just want to give you some of my elite and awesome credentials that prepared me to attempt to completely rewrite core infrastructure in the Internet. So, first of all, I have in the past deployed a totally awesome inventory system to help the Gap send Khakis around the world. Right? I shipped a kick ass loan qualification system for our friends on Wall Street at Moody. Yeah.
01:13:25.378 - 01:13:54.050, Speaker A: So definitely high tech stuff. I killed it working on a website. Oh, no, it's not coming up. Oh, no, hold on. It's even better when it works. Yeah, I killed it working on a pretty awesome website for bargain basement invisalign alternative. And I made the world a better place by working on an environmental, social, good investment platform for the young, liberal, and rich.
01:13:54.050 - 01:14:35.200, Speaker A: And also I've worked on failed startups. Like a lot of failed startups, I just have some great experience. Okay? But somewhere around along the line, in my case around 2017, I was inspired to do something different. Like, imagine many of you have been inspired to do something different and bizarrely, this inspiration is fairly well captured by a segment of Silicon Valley that I'd like to play for you. It's about a minute 30. Okay, hold on 1 second. I was told we have sound, but we maybe don't go back.
01:14:35.200 - 01:14:49.234, Speaker A: Okay, hold on. Yeah, we're going to do this. We're going to go back. Do we have sound here? Don't know how to do this on these touch.
01:14:49.272 - 01:14:50.900, Speaker E: But you probably know the line.
01:14:51.590 - 01:15:29.290, Speaker A: Perfect. I know the line. You know what? I'm going to try to do it all from memory, which I've watched it a few times, so maybe we'll give that a go. Let's see if this works. All right, now, I'm from Los Angeles, so this will be my attempted acting. So basically, that character, Russ Hanneman asks Richard, the other character exercise, you have infinite money and infinite time and no constraints. What do you do? And Richard says, I want to build a new Internet.
01:15:29.290 - 01:16:26.350, Speaker A: And he's he and Richard says, well, listen, I got to thinking. I was looking at my telescope the other night. Hannahman derivedly said, of course you have a telescope. And he know we got to the Moon using the computing power that fits now on a handheld calculator. And inside each of our pockets is a phone that has billions of times as much computing power as the computer that got us to the Moon. And in fact, there's billions and billions of people with these phones in their pockets basically doing nothing. So what if we took all of those phones and we used them to create a massive new network? And that network would use, well, Richard's compression software to make everything super efficient, but it would have a totally new set of properties.
01:16:26.350 - 01:16:59.766, Speaker A: It would be completely decentralized. It would be 100% private and uncensoral by any government or business, and information would truly be free. And essentially that's the pitch. Russ Hanneman says, wow, I would fund that. Anyway, there you go. I tried to reading for the part of Richard. Also, the whole clip, there's about a minute 30 that goes into that that I can play.
01:16:59.766 - 01:17:41.690, Speaker A: You should definitely look the whole clip up on the Internet because it's definitely not appropriate to play in a public setting because the jokes around it are pretty filthy, but they're hilarious. Yeah. Okay, so a new Internet, right? Something meaningful, something that actually makes the world better. Right. Not just saying it, but like, for real something we want our names as people who are working on, people who've taken up a career in this industry to have our names attached to for a long time. So I want to talk about my make all this work. Okay, so I'm going to talk about my world for a bit.
01:17:41.690 - 01:18:25.122, Speaker A: Over in the IPFS world, a lot of us set about writing a new Internet, and we built IPFS, came up with all this stuff. This is SIDS Content Identifiers. Lib P to P, a whole peer to peer networking stack. Iplb, we heard about it before. It's an entire decentralized data format. Unix FS, a way to encode traditional files and folders into content, addressing car files, a standard for encoding Verifiable data bitswap, a way to move all this stuff around, and IPNs, a way to deal with slightly more mutable data. Then, actually, each one of these is its own giant specification.
01:18:25.122 - 01:19:04.398, Speaker A: Like, all those are new technologies that go into Iplb. And actually that same is true for Lib P to P. There's like, all those things. Even the concept of a Content Identifier is four different separate technologies. And if you're working in Go and actually trying to build something, there's a whole bunch of other concepts for storing data on disk. And also we built to interface with Http, we built like, four gateways for moving content address data out to web two. These are some cons, some services an IPFS node might provide.
01:19:04.398 - 01:19:37.950, Speaker A: At one point, we even tried to write before there was Go mod in the Golanging ecosystem, we tried to write an entire Go package manager written in IPFS. So, yeah, super easy. Barely an inconvenience. Is there anyone in this room? Raise your hand if you could define every single one of the things on this list for this group. Yes, I've been working on it five years. I'm sure there's something in here I couldn't right. It's a lot of technologies.
01:19:37.950 - 01:20:00.786, Speaker A: When I first started working on this, I'm coming as like, a regular web app developer, and the first thing I notice, I'm like, there's nothing that is not built from scratch here, right? There's, like, no in use technology. It's all new, it's all custom. Wow. And that was both totally intimidating and a bit intoxicating.
01:20:00.898 - 01:20:01.222, Speaker C: Right?
01:20:01.276 - 01:20:33.362, Speaker A: Because you're like, oh, my God, I'm doing real programming now. So, yeah, I'm part of this story too, because I've been working on it for a long time. This is like an abbreviated and semi accurate list of things that I can say. Yeah, I wrote that for five years. That's pretty cool. Guess what? We built all of it and it works. Which is to say, kind of, which is to say not very much, but we're getting there.
01:20:33.362 - 01:21:01.594, Speaker A: Anyway. So our protocols, we're constantly evolving what we're doing, but we're still struggling, right? What are we trying to replace on the web? Two side of things. These are the core protocols of the web. I think you can basically run a web server and a web browser with just these four. I don't know, maybe I'm missing one. If anybody else knows something I'm missing, feel free to correct me after the talk. So it seems simple.
01:21:01.594 - 01:21:34.674, Speaker A: And obviously we have more names on our slide. So why have we won yet? So I would argue that we tried to build a new Internet with protocols. In fact, the company that started everything in the world of IPFS is called Protocol Labs. But the Internet is not protocols. The Internet is infrastructure, like a lot of infrastructure. Okay, part two trains. So I'm going to do a brief digression, if you don't mind, brief detour into another kind of infrastructure.
01:21:34.674 - 01:21:45.206, Speaker A: I want to talk about building trains. This here is a public transit map for a major American city. Anyone want to guess which one? What?
01:21:45.308 - 01:21:45.718, Speaker C: La.
01:21:45.804 - 01:21:59.470, Speaker A: It's La. Oh, man. I keep thinking I'm going to stump everyone because it's a very large network and people think La. Has no fun with trans. That's los Angeles. That's where I'm from. La.
01:21:59.470 - 01:22:22.810, Speaker A: Actually has a large rail network, and we're building a lot more. Everyone thinks we love our cars, but we actually pay a full set of sales tax to build new trains. The whole city voted for that. In fact, two thirds of us had to vote for it because that was the rule. You needed a supermajority to pass a new tax. So, like, we don't have a problem funding transit. We want to fund transit.
01:22:22.810 - 01:22:48.074, Speaker A: However, we are not actually using transit. Despite that, 73% of Angelinos Drive still drive alone to work, and 6.8%, I think, now use transit. So that's a bummer. Why? Well, for starters, we already have this. We have roads, and we have a lot of them. And once you build something, it's quite hard to do something different.
01:22:48.074 - 01:23:45.630, Speaker A: Our roads cost a lot of money to maintain. We pass a sales tax. But of that about I think almost as much goes to maintaining and expanding the highways as it does to building new rail just because they're so expensive. And also, La. Is square mile wise, like quite a large city, and so the rail network looks really big, but it's not when you consider the number of square miles involved. Eventually we might see buildings start to pop up right around the rails. But this network was built after the city was built, and so usually the hardest places to build rail are the places where the most infrastructure is already built up, where the most people are, because those people generally don't want their existence interrupted by giant trains, construction, two computers.
01:23:45.630 - 01:24:15.062, Speaker A: So you realize that typical distance from and to a rail stop is on the order of miles. So what you're really looking at is everyone has a car. You can either drive 15 minutes, find a place to park the car, which may or may not work, you may or may not be able to find free parking, take a train for 45 minutes, and then catch another Uber for like 15 minutes. Or you can drive for 30 minutes. But we're 45 in traffic. Right? And for better or worse, La. Is a car city.
01:24:15.062 - 01:25:04.742, Speaker A: It's how we're built. And it's even kind of like part of our culture. We're famous for loving our cars, so we're trying to change, but it's really slow. And I think you can learn a couple of lessons about infrastructure here, right? The first lesson is that infrastructure is harder to change than it is to build for the first time. The other thing you can learn is that infrastructure is not just what's built, it's what's built around it. So the fact that most of our living is right, not most of the places you want to go, are not right next to a rail line is pretty important and why people don't use as much rail. And then lastly, that infrastructure is not just stuff you build.
01:25:04.742 - 01:25:34.130, Speaker A: It's habits, it's culture. Right? Okay, one more rail digression and I promise we'll actually talk about web stuff. So this is China's high speed rail network. By the way. If you ever just want to have a really interesting hour on the Internet, do a deep dive on this. If you're not from China, it's shocking. It's 40,000 track, right? Larger than all the high speed speed rail networks of the world combined.
01:25:34.130 - 01:26:14.560, Speaker A: Transports about 2 billion passengers each year at about at speeds around 350. It's been built in two decades. So this is California's high speed rail. We passed the proposition to build it in 2008, about almost two decades ago. It's not operational. Probably won't be for real for at least a decade, probably more. When finished, it will maybe, possibly move riders from La to San Francisco at speeds of less than 200 mph.
01:26:14.560 - 01:26:55.850, Speaker A: So what happened? Why the discrepancy? Is this just further evidence that America is like a dying empire and its people have no shared purpose or some nebulous existential thing like that? Well, this is the US interstate highway system, which is huge. It is actually more kilometers than the high speed rail in China. And it was built in about the same amount of time, a couple of decades. So it's not like that Americans can't build things anymore. It's just that there's some things going on here. Right here's. Two real reasons, I think.
01:26:55.850 - 01:27:44.374, Speaker A: First of all, the rail network in China is driven by demand China didn't have for this rail network. China did not have infrastructure that scaled to its population. Right. And this rail network is pretty integral part of China's emergence as, like, a major, major world power. While the system itself is expensive to build, and it actually operates at a loss when you factor in the effect it's had on the Chinese economy, there's, like, one study that estimated it at about a 6% annual rate of return, which is pretty awesome. What this rail network does is it does two things. One is it connects people who live in rural areas to urban areas, allowing them to join this sort of modernization effort.
01:27:44.374 - 01:28:18.886, Speaker A: And it also helps turn small cities that aren't the major economic centers into additional economic centers where there can be additional economic growth. Right. And the net effect of this is that in the time it was built, I believe that economists now say that about half of China is in what's called the middle income group. Sometimes in the US. We call it the middle class. I'm not sure they're quite the same. But anyway, the middle income group is almost a billion people, right.
01:28:18.886 - 01:28:39.550, Speaker A: In two decades. It was like 3% before. That's, like, amazing to lift a billion people essentially out of poverty. US, on the other hand, has a road system. It has an air travel system. Right. If the California high speed rail is not built, we'll still have a pretty darn large economy in California.
01:28:39.550 - 01:29:04.454, Speaker A: So infrastructure gets built when there's a need. And we have something else in California, we have bureaucracy. We do a deep dive on California high speed rail. You'll find that there's just, like, three local politicians and communities that somewhat annihilated the potential for the entire project. It was either by refusing to let the rail go through somewhere that it.
01:29:04.492 - 01:29:05.894, Speaker D: Really needed to go through in order.
01:29:05.932 - 01:29:37.182, Speaker A: To be successful, or it was by diverting the rail to a city that really didn't need it because that local politician could say to their constituents, look, I got the rail. But then that made the whole system way harder to build. China over the last two decades has not always had this. The government has been able to sort of just mandate the building of these rails. Right. And so you get very different results. This isn't always a good thing.
01:29:37.182 - 01:30:05.366, Speaker A: Both in the case of the US. Interstate highway system and the Chinese rail system, this ability to just mandate quick building had some negative effects, at least in the US. It destroyed a number of communities as they just built highways through different areas and lives in the process. But you can just see that there's a difference here. Right. And it's about the fact that infrastructure is not just building. It's not just building objects.
01:30:05.366 - 01:30:14.586, Speaker A: It's about building consensus among people. Makes sense. Yeah. All right, cool. Back to the web.
01:30:14.688 - 01:30:15.142, Speaker C: Yay.
01:30:15.206 - 01:30:50.214, Speaker A: Thank you for going on that adventure with me. Part three building blocks. So what are the building blocks of the what's the structure of the Internet? What is the actual infrastructure? Well, for one, there's a lot of computers, big servers. There's also a lot of switches and routers. That is a picture of a Verizon switch center. It's also a whole bunch of high bandwidth data trunks that move data around very fast, literally. These data trunks are often a series of tubes for those of you guys who are in the US.
01:30:50.214 - 01:31:22.350, Speaker A: And know that that was derided is not what the Internet is, but that's what it is. Those are the underwater cables that connect across the continent through the Atlantic Ocean. So yeah, those are all the physical infrastructures. And if we take what we've learned from trades, it's not just what we built, it's what we built around it. Right. It's browsers. Browsers are like huge, highly complex and highly, highly optimized software for reading the Internet as a client.
01:31:22.350 - 01:32:00.166, Speaker A: It's also web software like Apache and NGINX, which are highly, highly optimized software to serve data on the Internet. It's standards bodies like the W three C standards bodies, where it now have a number of members who have vested interests. And sometimes you do see bureaucracy there. And it's a set of ideas and design philosophies, mostly this guy's ideas. This is Roy Fielding, the creator of Rest. If you've ever heard of a Rest API, that's what that term originated in. But it's actually a philosophy of building a massive network.
01:32:00.166 - 01:32:36.918, Speaker A: And for years I derided that word as like, that's what the purists are into. But when you get into trying to rebuild the Internet, you realize this is the caching strategy of the Internet. This is what enables data to be moved around a massive network of billions of nodes. Right? That's why it works at massive scale. So what do we have? This is going to be very IPFS specific. What actual infrastructure can we have in the world of IPFS? And you can probably generalize some of it to web3. Well, we have, for starters, 250,000 nodes that are running IPFS and the IPFS network.
01:32:36.918 - 01:32:58.670, Speaker A: That's not nothing. That's a pretty good start. Probably more aggressively. We have, as Jonathan mentioned, exabytes of hard drive space waiting to be filled up and used to store content, address data. That's pretty wild. We have a bunch of investment. Billions of dollars have been put into building a new Internet.
01:32:58.670 - 01:33:30.970, Speaker A: That's important, and there's still a lot of that to work on to try to build more things. We've got oh, you know what? I just skipped over one. I'm so sorry. You're probably noticing the bullets are off. We have hundreds of software developers working on this. Like people who've been working together for a while, starting to build really good relationships, starting to think through problems together across several companies in a somewhat decentralized way. And in IPFS land, we have ten implementations of IPFS.
01:33:30.970 - 01:34:22.250, Speaker A: We have an IPFS that goes to space. Seems like we've got a lot of software built. We're sort of interplanetary, right? This is sort of the core part of the talk. What I don't think we have, that the Web has, is a set of technologies in the world of IPFS that go together so well that people start to build killer apps on top of it. So that billions of regular people start to use IPFS without, oftentimes in like an ideal world where our content addressing or our web3 primitives are truly successful and adopted. Most people don't know that anything has changed. Most people don't know until maybe ideally, some government tries to censor their stuff and it doesn't get censored.
01:34:22.250 - 01:34:52.050, Speaker A: So that's where we want to get to. But to get there, we need people to be able to build web scale applications on top of our platform. And we're not there yet. I would argue that our problem is not that we don't have enough technology. We have a lot of technologies in content addressing and IPFS filecoin web3. This world is full of new technologies. You can spend like a week learning new jargon.
01:34:52.050 - 01:35:15.180, Speaker A: In fact, what I think is fewer technology. What we need is fewer technologies, but they should be the key. Fewer technologies that go great together to enable things you couldn't do before. So part four systems. Recently I saw I can't remember.
01:35:17.410 - 01:35:17.774, Speaker C: This.
01:35:17.812 - 01:35:59.378, Speaker A: Slide has no content. It's just me speaking. So what's the system? Recently I had a debate. There's a gentleman named Brendan who works on the rust implementation of IPFS, and he stopped talking about I wrote the name of this product as an IPFS implementation. He started saying it's an IPFS system. And we ended up having a debate about whether Iro was an IPFS system or an IPFS implementation. And he argued that Iroh is not an implementation because it does not interoperate with Kubo, which is sort of the original implementation of IPFS.
01:35:59.378 - 01:36:41.638, Speaker A: And I argued that's not what the specs say. And we've gone through a whole process where we tried to make it a lot more loose so that everyone could say they were an implementation. We went back and forth, and later I wondered about system. What I wrote actually is, which again, it's an IPFS, it's a Rust IPFS, does not interoperate with all the other IPFS. But what it is is it's a cherry picked set of IPFS technologies. Some of them are old, many of them are very new, and they go together to form a product that is really quite a great platform to build applications on. I'll talk about that in a second.
01:36:41.638 - 01:37:14.926, Speaker A: So what we might need is not IPFS implementations, but systems, ways of putting the pieces together that enable highly scalable applications to get built. And this is the core point of the talk. What IPFS systems have been built that you can build. Really amazing applications on top of. So let's look at some. These are just like a cherry picked set of things in the IPFS ecosystem that I think are really cool. The promote other people's stuff section of this presentation.
01:37:14.926 - 01:37:58.414, Speaker A: So first, I actually want to talk about this thing Iro, this Rust IPFS. Iroh is very optimized software, right? Iroh is written in rust. It's designed to run IPFS on any device. One of the chief complaints that people make if they run Kubo, the original implementation of IPFS, they say, my computer stopped running because you consumed all of the memory. And the CPU obviously not a win. IO is written to run anywhere, including specifically it's one of its primary targets is mobile phones. It has very limited ways of transferring data around.
01:37:58.414 - 01:38:32.614, Speaker A: And hashing, these are like big choices to make when you're inside of building an IPFS application. There's only one way to move data around, and there's only one kind of hashing. It's all essentially built on a type of hashing algorithm called Blake Three. I'm not going to go too deep on it, but that's sort of unusual because they're just saying there's only one way to move data and only one way to hash. Because we think this is the optimized way. They have a custom and fixed network stack. They do not actually use Lib PDP.
01:38:32.614 - 01:39:26.086, Speaker A: They essentially just build on Quick, the focus for Iroke. Iro has only one focus, which is to move and sync bytes, essentially to distribute files and then synchronize collections of files. That's it. Right? And what they managed to do with these guys put together is they built this platform that is a really awesome way to synchronize files across devices, or even to synchronize collections of data. They have essentially built an IPFS firebase, and it's pretty cool and highly performant. So that's kind of awesome. Right? Let me talk about another technology that is in the IPFS world that I think is really interesting, that's this technology called UCAN.
01:39:26.086 - 01:40:02.662, Speaker A: It was originally developed at Vision. The next speaker is going to be from Vision, and it was built out a lot by a group called Web Three Storage, which is essentially essentially UCAN is a system for tracking the chain of authority across an IPFS network. What that means, in simple terms is you can like a JWT extension, right? Raise your hand if you know what JWT is. Okay, all right. JWT is a very simple way of passing around authorization on the Internet. Raise your hand if you've heard of OAuth Two. All right.
01:40:02.662 - 01:40:29.870, Speaker A: Yeah. Okay. OAuth two is Delegated authorization. Right? It's essentially and you can is a much simpler form of OAuth Two. It's a way of verifiably saying giving someone a token that says the person doing this operation is authorized to do it. But the delegated part means they're authorized to do it by someone else. And they're only allowed to do certain operations as authorized by someone else, you get this chain of authority.
01:40:29.870 - 01:41:35.190, Speaker A: And UCAN is not just authorizing access to data or operations on data, it is actually authorizing it's authorizing the ability to perform computation. Right. It has a concept of delegated invocation, an authorized invocation. Moreover, it can actually get receipts that an authorized invocation has been run and results from that computation. So what you end up with, and I realize this is a little bit theoretical if you haven't really touched a lot of this stuff, you add one more technology which is called a decentralized Identifier or did. This is something that allows you to make your login. Just something like rather than an application having a list of users, your decentralized Identifier is simply your email with someone attesting that you actually own that email.
01:41:35.190 - 01:42:17.006, Speaker A: Right. It could also be your crypto wallet with a proof that you can sign a message on that chain. And when you add DIDs and you cans, what you end up with is this system for essentially hiring arbitrary services in the decentralized web to work for you. So if you look at Web Three storage, which has implemented all of this, they give you a command line tool that allows you to take your data and upload it into potentially any storage provider. Right now they're the only implementer, but it's a totally open spec. And Web Three Storage does not have a list of users. You are the user.
01:42:17.006 - 01:43:14.360, Speaker A: You control your decentralized Identifier, and you can associate potentially a credit card with that Identifier so that they can charge you if you want to store a lot of data. But you pack your data, you verify that it was content address. You can even use it for a whole platform where like what's it called OpenSea needs to let their users upload data without actually giving their users, all of their users their private key. Right? So they use this delegated authorization to allow users to upload data on their platform and then move it to Web Three storage with their users not actually having Web Three storage accounts. And without Web Three storage having an actual account for Openc, it's pretty wild. And then that can keep going and go all the way onto filecoin. And probably in the future we'll link it up and it can be used to authorize retrieval from Saturn, which I'm going to talk about in just a second.
01:43:14.360 - 01:43:47.738, Speaker A: I think Jonathan mentioned it briefly. It's this lovely company that I want to mention. What they're trying to do is provide an enterprise grade storage product, something that companies can hire to store terabytes of data. And what they do is they use another technology in this world called WinFS, which I think there's a talk on coming up. Yes. Oh, you're going to know what this is in 2 seconds. It's a way to take all of your data and make a content address and also encrypt it in private.
01:43:47.738 - 01:44:16.140, Speaker A: Right. So no one can read this. So they use this software to put it to make to take data and use WinFS to encrypt it and make a content address, pack it into car files. And then their actual storage network is Filecoin. They don't actually have any of their own storage. They use filecoin. They contract with specific storage providers who are willing to run custom software and sign contracts to perform really well.
01:44:16.140 - 01:45:11.290, Speaker A: And what this does is it enables a working private enterprise grade storage system that's completely decentralized. It's pretty cool, right? Finally, I want to share one shameless plug. This is the only part of the talk where I'll talk about the thing that I work on. So I work on this thing called Saturn, which is trying to we're putting components together to unlock a CDN that you just can't build on a centralized network, right? So we use, again, content addressing. All of the data we work with has to be content addressed, right? And we have a network of nodes, and these are people that have signed up to join Saturn. They have computers either in data centers or other places with high bandwidth connections. But we don't contract with them at all.
01:45:11.290 - 01:45:52.440, Speaker A: They are members of our network. We do not rent space in a data center. And they run an optimized IPFS fetching software called Lassie to get content address data from the networks, from IPFS and Filecoin. And they pull stuff in and then they cache it. Right? We have a simple piece of JavaScript to run in browsers in order to make requests to our network and get this content address data back and then verify it right in the browser. This could also happen in a native application. It's actually kind of easier if it does.
01:45:52.440 - 01:46:43.400, Speaker A: But what this means is that you request content from the Saturn network and untrusted nodes send you back data. But they can be untrusted because you immediately know if they tampered with the data. Right? Moreover, we have a crypto economic model in which essentially it's based on metrics reconciliation and fraud detection for now, similar to the way, like ad networks work. But essentially our clients and our nodes report back data on what they're serving. And we run a bunch of reconciliation on that to find out who's serving content and how well they're serving it. And we reward people based on how well they serve content. And that lives in a bit of orchestration software, which for now is centralized, but maybe not in the future.
01:46:43.400 - 01:47:28.200, Speaker A: This is what it enables. We launched one year ago. We currently have 25 terabytes terabits per second of data bandwidth. We serve about a billion requests a week. If you want a comparison, cloudflare, fairly well known CDN, probably one of the largest launched and reached 25 terabytes per second of data bandwidth in ten years. We got those in one. Exciting, right? And the reason is because anyone can just join it and come online, right.
01:47:28.200 - 01:48:03.874, Speaker A: One of our particularly enthusiastic team members says that when we go to sleep, the network increases in size and sometimes it does. Cool. So yeah. Now I don't know how many people in here are true decentralization purists in the audience, so there might be some doubts that are creeping in right. That you might be like, oh, I heard a centralized component in there about that. Oh, wait a second, I heard one of your implementations isn't interoperating. I don't know about that.
01:48:03.874 - 01:48:47.374, Speaker A: Well, yes, there are trade offs. There's not much we can do. There are trade offs and some of those trade offs are often painful in order to get towards the vision that we're building towards, even though large portions of these systems are completely decentralized. So the last thing I want to talk about is people, because we're all trying to build these new systems. If you're in Web Three, whether or not you're in the IPFS file points ecosystem, or if you're in Web Three, we're trying to build new things. It's a much harder form of software development than if you are someone like me who spent most of their time building web apps before this. And so I basically have three last qualities that I want to say.
01:48:47.374 - 01:49:18.570, Speaker A: What do we need to get there? Right? Because we have trade offs and they're painful and we need to treat each other well as a community going forward so that we can get to where we want to go. So I have three qualities that I would love to see more of in Web Three. The first is humility. This is a hard lesson that I've learned. I don't know what I'm doing. Is anyone here? There's probably like one or two people who just sort of see the future and move forward towards it. I'm not one of those people trying to build the right thing.
01:49:18.570 - 01:50:11.820, Speaker A: It takes a lot of trial and error. And so I encourage everyone, if you're in this world, learn some humility because you're going to find out you're wrong a lot. The other thing that I've noticed, I can't speak for other ecosystems in IPFS, one thing I wish we did a little more of is listening to each other. There's a healthy competition among a lot of our teams and that's great, but sometimes there's like, we're so sure that we're doing it the right way, that we don't hear the other teams and how they're working and how that makes what we could learn from them. And I think we could move a lot faster if we did that. And the last thing I think we need is patience. This is going to take some time and we need to trust each other, especially as we are all building new products and building things that enable the next version of the yeah, that's me.
01:50:11.820 - 01:50:32.590, Speaker A: That's my contact info. That's the you can feel if you want to know more since I know that wasn't the folks in the talk, feel free to talk to me after the talk. I just put this slide on here because these are some humans in my life. There are humans and animal. That's my partner Eliana and my dog Sadie. So they get a special feature because they make my life worth living. Okay.
01:50:32.590 - 01:50:34.080, Speaker A: That's it. Cool.
01:50:40.950 - 01:50:42.660, Speaker C: Does anyone have any questions?
01:50:46.170 - 01:51:30.290, Speaker A: Yeah. So when you say there are ten plus implementations of IPFS, can you define what implementations are? Yeah. Hence the debate that I have with my so technically, the IPFS spec says that sort of like, very limited scope nowadays in order to be defined as an implementation. And that is that you need to support content address data. That needs to be your primary primitive, and you need to transport data in ways that are independent of the underlying protocol. I'm not crazy about that one. So it's super abstract these days.
01:51:30.290 - 01:52:10.000, Speaker A: It used to be that IPFS define an IPFS implementation was like, you have Lib P to P, you have bitswap, you have as your way to move data. You have an HTP gateway, you have a bunch of stuff, and they cut it down significantly to allow for experimentation, but it's very poorly defined. I'm sorry that I don't have a better answer for you. And I'm like, somebody's been working on this for a while. I didn't actually write the super minimal version of an IPFS spec, but for me, anyway, it's like, if you work with content address data, then you're IPF for me, that's what I'm working on. So that's what drives me. Yeah, cool.
01:52:10.000 - 01:52:11.460, Speaker A: Other questions?
01:52:12.230 - 01:52:14.334, Speaker B: Yeah, I think you kind of raised.
01:52:14.382 - 01:52:22.610, Speaker D: A lot of important and interesting business questions at the first half of your talk, and I'm just curious about how.
01:52:22.760 - 01:52:29.462, Speaker E: Does the protocol apps how does the evolution of the business there's a lot of coordination problem.
01:52:29.516 - 01:52:34.520, Speaker D: You need to build A before you can build B. There's checking that problem. How do you actually solve these problems?
01:52:35.290 - 01:53:44.554, Speaker A: Yeah, I have to be careful here because probably what is Protocol Labs doing? Should be an authoritative answer for Juan Bennett, the boss man. But for me, I think we traditionally in Protocol Labs for many years, we would build for a while till we encountered a problem, right? And then we'd be like, oh, we need to your point? This needs to exist before this exist. So then we'd spin up a little team that was working on that, on the thing that needed to exist, and the other team would sort of, like, try to coordinate with them. It's all within a giant company roadmap, and that has been really annoying, especially for everyone in the ecosystem, because to have the context, you need to be in the company. And we're not really doing that anymore because I don't know if you've followed, but Protocol Labs is spinning off a lot of its own teams and their own little companies. I would argue that we actually in many cases, not all cases, and this is again what I'm talking about. We're not in the moment where we need to build this thing for that thing to exist.
01:53:44.554 - 01:54:41.550, Speaker A: We've got enough things to build some cool systems out of them that then allow people to build applications, right? So all the things I talked about are not actual, they are not products that are applications. They are platforms from which you could build applications even though they're cherry picked technologies. Sorry, I realize this is kind of like a lot of layers to me, that's what we need is people looking at the whole thing and saying these five things would go together really well to enable this type of application to be built. And I think that's what we could do. Again, there's a lot of centralization points that we still end up on because a thing doesn't exist like the Saturn orchestrator that coordinates the whole network. Sorry, I'm just a little bit freewheeling. This that is centralized because we don't have a super scalable, decentralized compute layer within the network at the moment.
01:54:41.550 - 01:54:58.230, Speaker A: That is Verifiable, which is what we would need in the case of Saturn because it's dealing with money. So probably we are going to wait for IPC, and now IPC is going to be a different company from Vertical Labs and Saturn is going to be a different company. So that'll be interesting to see how that plays.
01:55:01.130 - 01:55:49.000, Speaker D: On because I think Daniel's a good example of this, where it's like Daniel is working on top of a number of different teams with a number of different technologies. The thing I actually think is maybe a positive branding of that is it forces more of this coordination to happen external to PL as a group in the ecosystem. And everyone kind of knows each other. I mean, not everyone knows each other, but most people know each other. And I think the more people coordinate on these problems in public spaces, it also means that there's a broader set of input on what are the actual problems that need to be solved, not like in one specific version of solving the problem. What that means. Banyan is a good example, though, I think, of taking from the Iro team, from you guys, the one FS side, and you can even the file coin side kind of piece all these things together.
01:55:50.970 - 01:56:01.238, Speaker A: I'm not kidding you, you haven't followed major portions of the current Protocol Labs company are going to be coming independent teams and independent startups.
01:56:01.334 - 01:56:02.298, Speaker D: So we're going to be moving to.
01:56:02.304 - 01:56:15.280, Speaker A: An ecosystem, a true ecosystem model, whether we like it or not. And I think teams like Vision and Banyan are like, oh, welcome to the party, finally. Yes.
01:56:15.810 - 01:56:19.534, Speaker D: Hannah, as a former application developer, somebody.
01:56:19.572 - 01:56:34.366, Speaker E: Who'S building apps for end users and thinking about what you're talking about, infrastructure, and it's built with a need in mind? Have you imagined what types of bits.
01:56:34.398 - 01:56:45.080, Speaker D: Of IPFS, file, point, et cetera, that will be leveraged to be able to build apps that fill a need that cannot be filled by current HT? Current, like, web two style things?
01:56:50.190 - 01:58:16.678, Speaker A: Mosh michelle was here because she's got a list of like these are four categories of you put know, somebody needs to put some components together to enable this kind of application, but okay, here would be my quick take. Like, very large asset distribution is one that's in my head a lot, particularly for Saturn, because content addressing, if you think of like, we haven't really gone too deep on what exactly it is, but you can vaguely think of having content addressing being like, you have your data, then you essentially have a merkel proof on top of it. Similar to BitTorrent, right? And what we know is that BitTorrent is actually pretty good at moving things around very fast for large amounts of data. And I think that through IPFS and maybe some infrastructure like Saturn, you could do massive asset distribution, which is increasingly quite important in the world of gaming because now they're like, hey, I don't know if you follow that, but unreal is like, you know what, you can have no polygon limit if you want. You can just build assets that are like your production assets for a movie. Sorry, I'm going a little bit. My first thought when I heard that was like, well, yeah, but how are you going to pack that into like, 70 gigs of data that you can download? Right? So you're going to probably see a lot of asset distribution stuff metaverse there.
01:58:16.678 - 01:58:39.820, Speaker A: But there's a whole other class of things, I think, in censorship resistant applications. I don't know myself. I'm not sure I can tell you what the exact set of technologies you want to put together to do that, but somebody should pick that up if you want to start a startup to do that. There's a market there. There's other ones. Again. Jonathan, do you have anything?
01:58:40.590 - 01:59:17.094, Speaker D: Actually, I don't think the right frame is one of the things that are only uniquely possible because I do think one of the biggest uniquely possible things is like, can you get the cost lower? There's a lot of teams I was chatting with the High Happer team as an example, they do generate quite a lot of data, but their biggest cost is actually likewise because they're serving a lot of data as well. And so all of the content addressing stuff is really just unlocking the thing that Hannah said, which is like, how do I have magically more servers that can stand up overnight that can potentially serve as cheaper costs? I think over the long term at least, my personal bet is like, the big killer features. Like, can you just be cheaper still?
01:59:17.132 - 01:59:19.802, Speaker A: Serve these things for sure.
01:59:19.856 - 01:59:28.414, Speaker D: Money to your point, somebody had sent just passed me this article from The Verge today which says alders Gate Free.
01:59:28.452 - 01:59:33.310, Speaker E: Deluxe Edition on Xbox and PS Five is coming on a fuck ton of discs.
01:59:34.770 - 01:59:47.890, Speaker A: Yeah, it's not a CD Rom anymore. Yeah, magnetic tape. Yes, sorry, we have a question back here.
01:59:47.960 - 02:00:42.078, Speaker C: I have a very interesting idea to share actually. Last week the cloudfare got temporarily banned in China, causing Taobao like Alibaba, Amazon like bunch of websites basically down in China. The reason behind is the CDN of Code Bear, the root address is 1.1.1, right? They got banned because bunch of people are using it to bypass using protocol like V two reality to bypass the Great Firewall of China. So the interesting thing is if the Satchel can have a decentralized CDN and building some protocols to allowing people to bypass the Great Firework of China, then you can basically get killing of users. Mean, internet freedom is a thing, right?
02:00:42.264 - 02:02:03.066, Speaker A: Yeah, we've been having some conversations and also IPFS and Valflaren have tremendous interest, particularly Valve, but yeah, I'm sorry, I'm chuckling at the idea that they're like Eddie cats, they came to fit them in the butt because literally every cloudflare center has the same IP address because it's just like hack to make your content go fast. But yeah, I think there's a huge opportunity there. The interesting question is, will you be able to decentralize it enough that it can't, that people can't find ways to shut it down? So like for example, we have this orchestrator, I'm just thinking through this, we have an orchestrator that collects all the metrics and then bans nodes and perform badly. Well, the problem with that is that means that if we were to run it in China, the Chinese government could come and say to us hey, your orchestrator needs to ban these nodes. Right? There's a number of ways we could work around this, but yeah, I think there is a possibility there. But the proof would be in the pudding to see if we could do it. But I think there's a really no, I mean we've been talking to folks about running, starting to grow the Saturn presence in China because I think a lot of our stuff is killed.
02:02:03.066 - 02:02:12.020, Speaker A: Actually we do have some presence, but compared to Filecoin, which has the biggest filecoin providers are all in China. We're not quite as far longer.
02:02:12.390 - 02:02:20.386, Speaker D: So related to the censorship, was that event in Turkey like some censorship and.
02:02:20.408 - 02:02:23.218, Speaker B: Then IPFS put the Wikipedia on.
02:02:23.384 - 02:02:24.386, Speaker D: Are there any follow up?
02:02:24.408 - 02:02:25.220, Speaker E: What happens?
02:02:28.090 - 02:03:36.410, Speaker A: So that's an interesting thing. Basically what happened here is that I believe Turkey government made some edits to Wikipedia, no disrespect, Mr. Arian, and I believe at the time protocol Labs had copied and hashed a previous version of Wikipedia without those and it was essentially published IPFS. And once it's published, technically anyone can host it and it may be harder to take down. The harder thing about taking it down on IPFS is again, because it's content address. If you know that patch and then somebody hosts it, and then Turkey says, hosting provider, take it down and then somebody else hosts it, nothing changes, right? Because it can move around without anybody shutting it down. And that was how I think, another example, they basically held like, referendum on independence for Catalonia and Spain.
02:03:36.410 - 02:03:54.660, Speaker A: On IPFS Protocol Labs, the company no longer gets real deep into some of those political things because a lot of liability now that they're a little larger. But yeah, no things are possible. It's not a catch all, though. Just to be clear.
02:03:56.550 - 02:03:57.058, Speaker D: One of the.
02:03:57.064 - 02:04:13.240, Speaker A: Biggest conceptions about IPFS misconceptions is that when you put something on IPFS, it will just last forever. That's not true, unless because if only one person has the content and they go down now, it's gone, but if someone else has it, they can put it back on. Cool. Okay, sorry. Thanks for.
02:04:18.090 - 02:05:05.890, Speaker E: I am going to speed run my presentation a little bit because I know that I'm sitting between you and dinner and you really want to hear about Philip's Cryptree. So my name is Boris Van and I'm the founder and CEO of Vision, and I'm going to talk to you about life at the edge. So, Vision is a company that specializes in protocol, first applied research and software engineering. Basically, there's deep tech and other things, and software often is seen as something that's easy. But we took an incredibly hard route. Although it's been a lot of fun along the way. I wanted to kind of throw this up.
02:05:05.890 - 02:05:55.190, Speaker E: This is something that I say a fair bit, and sometimes I say it even more and I say, like, IPFS has won, but we're actually changing what that means as well. And what does winning look like? So from a Web Three perspective, ultimately, at the end of the day, there's like shared components, really low down, lots of things. We use Lib P to P that Protocol Labs helped do with the Ethereum Foundation and all these other pieces like cryptography libraries and other stuff like that. And when we pop up again, there's lots of competition and content addressing kind of sits in the middle. It's not tied to any one chain. You can produce it locally. It's locally verifiable, and we can read and write into this global, global namespace.
02:05:55.190 - 02:06:48.860, Speaker E: That's freaking amazing. It's actually a really great low level thing. And it's one of the things that Hans It, she's like, yeah, but if it goes offline and it's gone forever but it isn't, because if anyone, anywhere has a piece of content on a USB stick, on a CDROM or anything else like that, if you plug it back in again, that content address is still the same. And that is super, super cool. And if you're really deep into the IPFS ecosystem, then you know that Drake is mad at old CID versus zero QM hashes and that backy addresses are super cool. And because there's only laughter from IPFS crew, we know that you haven't been totally IPFS pilled, so we will work on that. So Phishing has been going for four years because we in fact have had to build a lot of things.
02:06:48.860 - 02:07:22.274, Speaker E: You might know some of our protocols and our cute mascots you can for decentralized auth WinFS for encrypted file systems and IPVM, the interplanetary virtual machine and we'll talk a little bit more about that. We've done a few other things securely linking devices so that you can pass keys around using something called Awake. We're going to switch to the IETS MLS at some point carmere, which is an alternative to bitswap to make things go fast and do partial syncing over Carmere. Philip, I think you're going to cover.
02:07:22.312 - 02:07:23.538, Speaker B: Some skid ratchet stuff.
02:07:23.624 - 02:08:06.190, Speaker E: Sure, great. Yeah, a pile of tech. So what's been great in working inside an ecosystem and literally everything that I have on here has been mentioned before, which is super awesome. UCAN and Winifast came first and so they've had the longest time to have adoption and diffusion throughout the PL network and beyond. So some of the teams that have implemented or integrated the protocols include Iro subconscious banyan, Web Three storage. Our friends at Saturn I think are going to end up at UCAN at some point as part of the things. And I think Valvers is in the room here today as well, who's doing some interesting stuff, plus collaborators and standards groups.
02:08:06.190 - 02:08:58.226, Speaker E: So this is not us doing it all by ourself. All of this stuff has been done in open working groups, both informally from bottom up community protocols and then standards groups like the Chain Agnostic Standards Alliance and soon some new work in the W three C. So some of those things are structures that Hannah mentioned as well. So now I'm going to tell you a little bit about Fission original Mission edition. So Brooklyn Zelinka, my CTO and co founder and good friend, started Fission after spending time in core Ethereum space. So Brooklyn was a core EVM developer and I helped run and start the early Ethereum additions and the early Unconferences and organize core dev events. What we found that a lot of people had challenges with were like, okay, there's this new tech, there's solidity and smart contracts and security and that's all brand new and we have to figure that out.
02:08:58.226 - 02:09:51.934, Speaker E: And then Web Three devs would go to build a web app and we'd be like, brooklyn and I have been building these things for decades and we're like, it's the same problem. Everyone's running straight into having to become a full stack developer and do identity data and compute and DevOps and put all that together. And we said, what if we just took a lot of these advances of Web Three of distributed systems of cryptography with content addressing as a base and we put the entire. Front end, that entire stack directly into the front end, because we do that in the browser. And it just makes no sense to have this completely decentralized back end and then run a Node JS app on top of it. Single point of failure. What are we even doing? Plus, it's just hard and expensive.
02:09:51.934 - 02:10:24.320, Speaker E: All of the scaling is on one team. So life at the edge. A lot of the stuff we've talked about with Blockchain is spending a lot of time working on logically centralized systems and global state. And you know who's at the edge? People. That's where all the stuff is. And in fact, this goes back to the Silicon Valley sort of thing, and we really think that there's an opportunity there. First of all, we have to always be thinking about that.
02:10:24.320 - 02:11:24.820, Speaker E: People want their stuff with them all the time. One of the things that really hate about IPFS is this thing called pinning. It's filled with lies, it doesn't make any sense. And most operators who run IPFS nodes at scale turn off completely the garbage collection, that was its original purpose. At the end of the day, ultimately people want to have their stuff and they want to have it available, and they want parcel sync to get to it. And the promise of the cloud within web Two version is mostly silos. So Web Two platforms where if the app developer goes away, and maybe they go away because it's really expensive to operate, so does all of the users data, and you have no idea if they're peaking right? Like, yeah, it's in a database and really, you have no idea if the developer is looking at it or not.
02:11:24.820 - 02:12:12.126, Speaker E: So really, April 2019 paper and a bunch of research by a research team called Aidan Switch about local first software. This has been done before, it's been written about before, but this really reinvigorated the space again, and had these seven ideals for local first software. Again, not something we talk a lot about in Blockchain space because there's this vast global infrastructure that's logically centralized. Plus we're trying to fix some RPC stuff, but the data at the edge is actually the important part. So I think these are really good principles to think about at the edge. And what I started thinking about is.
02:12:12.148 - 02:12:13.786, Speaker C: Like, okay, let's protect all this data.
02:12:13.828 - 02:13:16.466, Speaker E: Let's encrypt it end to end. Really, in many ways, what Fishnet has been trying to do is say, you know, that really luscious end to end Apple stack. We have to rebuild it because it's completely proprietary, it's tied to their hardware, and we can't reuse. Like this whole company is actually just me being mad at Apple because they have really good UX and good but so we have no choice but to turn these into components that we can reuse. What I'm now worried about, as we're actually getting pretty good at end to end encryption in many situations, is silos. So if we don't work on Interop, then end to end encryption of data means you've got system A that has a particular system for doing end to end encryption, and system B, doing end to end encryption and system C that does the same thing. And what does the user have to do? Decrypt it and move it between systems? Unacceptable.
02:13:16.466 - 02:14:06.018, Speaker E: The way this should work is we should have an open source Icloud dropbox as a protocol. And by the way, we should also be thinking mobile first. So many people in Web Three, so many blockchain developers sit there with giant monitors and professional desktop operating systems which are dying, and design things that's the other rant that I've had for many, many years. If we can't carry these things around in person in our phones, we're not serious about building a new Internet because the new Internet needs to be mobile. And that pattern actually already exists in native mobile. When you pull up an app, it says, hey, can I access your stuff? And we give a capability for that app to do that. And that's where I'd like to get to with encryption.
02:14:06.018 - 02:14:29.770, Speaker E: If you're working on anything related to encryption and personal data, we need to figure out how to interoperate. So I'd love to talk more about that. UCAN for capabilities, wouldn't it best for interoperable encryption if you're not following some of these standards, these are some things that are coming along that are really, really great. How many people in the room have heard of the chain agnostic standards? Linus.
02:14:31.490 - 02:14:32.590, Speaker C: Fantastic.
02:14:33.170 - 02:15:16.150, Speaker E: I'm introducing it to you. Search for Chain Agnostic. Basically, a bunch of people started with Ethereum Core Group, really as it became clear that we're going to be many EVM flavored chains and many Cosmos chains and many other chains. And in fact, what we should do instead of continuing to solve these problems independently. A lot of this came out of wallet developers. I was running the Wallet Unconference today because it sits between app developers and users and the rest of consensus core things. One of the things we figured out is Blockchain largest deployed system where people already have public private keys.
02:15:16.150 - 02:15:47.906, Speaker E: That's amazing, right? 50 million accounts, whatever we're looking like there, pretty good. And what we need, however, is we need a wallet standard, including from my friends at MetaMask, that lets us use encryption keys separate from signing keys. So end user encryption, decryption, turnkey with all of the wallets that are already deployed and we can get there. We're actually getting quite close to that. So working on that with Brave, MetaMask and other wallet devs. So that's something that's into action. How many people have heard of Paskis? Handful.
02:15:47.906 - 02:16:30.114, Speaker E: Paskis is big tech web auth n, but also standards. That also is another mechanism where direct mass market end user managed access to public private keys. There's a few little standards, it works in browsers, including on mobile, but that's coming along and a lot of people are working on in different ways, and I'm super excited by that. Something that Protocol Labs helped make happen by going back in time four years ago and spending time and advocacy and money and examples is Ed 25519 curves in all browsers, including Safari. Can I get a whoop whoop for new curves? Right? And that's what it means when we work together.
02:16:30.232 - 02:16:31.298, Speaker D: A lot of that was talking to.
02:16:31.304 - 02:17:18.626, Speaker E: People and saying, what do you need it for? And working with the W three C to say, oh yeah, this is important. So we wanted to build this stack for front end devs, to build apps that compose really nicely. But we actually find ourselves in the protocol minds because we're like, oh, there's a few pieces that need a little tweaking with IPFS and we need a decentralized auth standard just like OAuth and Twitter kick things off for APIs, rest APIs back in the mid 2000s. We need identity because we want to do encryption and you can't do encryption without identity. Then the new thing that we've gotten to now is IPVM, the whole thing.
02:17:18.648 - 02:17:19.966, Speaker C: Of identity data and compute.
02:17:19.998 - 02:17:25.138, Speaker E: We're building those pieces to get to the interplanetary virtual machine.
02:17:25.314 - 02:17:26.390, Speaker C: So this is an example.
02:17:26.460 - 02:18:08.994, Speaker E: Again, we're working with the wider PL and IPFS and Filecoin ecosystem, a specification for bringing content address execution to content address data on IPFS. So skipping a few bits in the Miller, why not build a competitor to AWS Lambda? That's the open IPBM working group. You'll see, a lot of the things that Vision works on are not under a fission namespace. They're not ours. It's work that we do in working with other people. The original spec was co written with Baciao you can Invocation, which is Hannah mentioned a ton of work with the rackley from Web three storage. And that pattern continues in everything that we do.
02:18:08.994 - 02:18:19.138, Speaker E: Banyan decided that they wanted to work on enterprise scale encrypted data and added those capabilities to WinFS. Well, we were focused more on personal scale.
02:18:19.234 - 02:18:19.638, Speaker B: Amazing.
02:18:19.724 - 02:18:22.854, Speaker E: Let's share engineering resources and make sure that it works well together.
02:18:22.972 - 02:18:24.958, Speaker C: There's a ton of video deep dives.
02:18:24.994 - 02:18:58.820, Speaker E: That you can go to at that address. This week the Vision team worked together to take all of these things, take these protocols and pieces of implementations we put together and build a platform. So this week has been the launch of the Everywhere computer. And this is kind of like the red hat of computational networks. Vision is going to run it. We're going to brand it, we're going to have a distribution or system. I think some of those two words have some things in common.
02:18:58.820 - 02:20:00.646, Speaker E: Bundles homestar are a rust implementation that focuses on WebAssembly, a control panel, a function and workflow marketplace. So think like NPM, but like filled with WebAssembly functions. It's almost like Edge smart contracts and manage compute resources. So as a developer, write functions and integrate custom services as a user, bring your own compute or rent ours as a computation provider, bring custom resources and register them with the network. So John talked about data gravity. If you have a few little bytes or maybe it's really big and it's megabytes of functions, why not send those over the network to run computation next to where the data is? So some of this capability actually starts breaking down some of those large clusters and we need to remove them on as well. So as an ecosystem, I also see basically, whether it's app chains or IPC or other specific applications, deciding that they want to federate their own network of functions, services, and resources that matter to there.
02:20:00.646 - 02:20:09.900, Speaker E: And I think that's the thing. We're turning a lot of things into markets in a good way. A little bit of a taste into future work and research.
02:20:11.470 - 02:20:11.930, Speaker C: Videos.
02:20:12.000 - 02:20:53.750, Speaker E: Oh, yeah, I'm not going to play the video, but this is about a 1015 minutes video where Brooklyn Zelenka essentially Speed runs what we've been working on, where the future is happening. Highly recommended. That was at ECC last year at BlueYard Side Event, and it's really talking about what goes on chain, what goes off chain. And I think with IPC and FBM and content address data and the stuff that fish and working on, that's really fuzzy, and that's what Content addressing gives us. When the content can't lie, it means you can move it around and do proofs around it really easily. IPFS payment channels and a Web services focused decentralized billing system. We're going to run an open web Amazon.
02:20:53.750 - 02:21:26.222, Speaker E: Then we actually need a billing system that works within a decentralized context. We designed the IPVM alongside of the FBM team. Our stuff is also written in WASM and Rust. So again, that on and off chain means we can collab really well. We're going to be working on integration into ecosystems, app chains, L two S. mainnets number of people from launching this week are super interested, and I'm excited to see where that goes. Finally, WASM from browsers to servers.
02:21:26.222 - 02:22:00.374, Speaker E: So we are going to be launching a W three C community group, and we've got teams from within intel and Google interested in participating and going through the standards based process. So I think there's this other continuum of on and off chain that is web two to web3, where everything slowly comes together as part of web3. That's the end of my talk, Speedrun. Thank you, Boris.
02:22:00.422 - 02:22:05.470, Speaker C: So, question. Perfect.
02:22:05.540 - 02:22:11.298, Speaker E: Speedrun on to you, Philip. Dor do you want to ask me a question before I run away?
02:22:11.384 - 02:22:11.682, Speaker C: Yes.
02:22:11.736 - 02:22:12.002, Speaker E: Okay.
02:22:12.056 - 02:22:14.974, Speaker C: So how about the downtime for homestar?
02:22:15.102 - 02:22:44.970, Speaker E: Downtime for homestar? How many nines are we going to get? Really interesting. I was talking to the Dran team. They have 100% uptime for four years. I don't think we're quite ready for that yet. We're just launching the first version of this. What we really see is, again, a larger, larger network of nodes available, all with the basic capabilities. So what we've decided is all homestar nodes will be able to run WASM functions.
02:22:44.970 - 02:23:14.434, Speaker E: Some nodes may have expert capabilities. So like John mentioned, some nodes may have lots of GPU, and if there aren't that many of them, your job may not be able to execute. But from a WASM perspective, even your browser can execute that WASM. So we should be able to route to someone, including just someone nearby in like a cafe or something that can execute for you TBD as we launch it.
02:23:14.632 - 02:23:18.070, Speaker C: Yeah. And there is no incentivization layer.
02:23:18.490 - 02:23:33.850, Speaker E: All of these are layered systems at the end of the day. So we think that an IPFS based payment channel is another really powerful primitive. It's essentially like the Lightning network, but.
02:23:33.920 - 02:23:35.254, Speaker B: Built on top of IPFS.
02:23:35.302 - 02:24:01.300, Speaker E: And as we heard from other speakers earlier, it's non consensus because you have content addressing and then settlement could happen into a number of different ecosystems. So, as John mentioned, a particular IPC subnet would make sense to settle against, but could also settle in Fiat or any other ecosystem that essentially talks to IPFS. And since most of Web Three has ties into IPFS, we could settle into that.
02:24:02.170 - 02:24:03.080, Speaker C: Thank you.
02:24:04.410 - 02:24:07.800, Speaker A: Do you have docs on that building?
02:24:09.610 - 02:24:12.338, Speaker E: It's the future work and research, Hannah.
02:24:12.434 - 02:24:23.980, Speaker A: I'm sorry. You should think it's chain independent, so like, maybe it doesn't need to be crypto either.
02:24:26.270 - 02:24:26.730, Speaker D: Perfect.
02:24:26.800 - 02:24:31.214, Speaker C: Thank you. Yeah, but also docs not everywhere there.
02:24:31.252 - 02:24:34.926, Speaker E: Exists something does not when can I.
02:24:34.948 - 02:24:36.320, Speaker A: Start using my idea?
02:24:36.770 - 02:24:37.278, Speaker C: Amazing.
02:24:37.364 - 02:25:01.734, Speaker E: So, basically, we did a workshop this week, all this stuff is open source and generally we have to wrap up the work we did this week. So the team crunched in a good way, we work together. It's been really amazing. And all of this stuff will release with videos and instructions and READMEs basically next week. We had it all ready this week.
02:25:01.852 - 02:25:06.680, Speaker C: Thank you. Thank you.
02:25:07.070 - 02:25:07.718, Speaker B: Speech.
02:25:07.814 - 02:25:20.734, Speaker C: So, final one. So, last talk of the day. I know, right? But I'm going to start off with some questions. Who here has built a DAP or.
02:25:20.772 - 02:25:22.080, Speaker B: Helped build a DAP before?
02:25:23.410 - 02:26:11.558, Speaker C: Who here has tried storing some data because of their DAP on a chain before? And who here has stored data that should be private on the chain somehow? So I just talked to someone today, the wallet uncont actually, about this, and essentially they were like, well, we have to store something. But we were at the hackathon and we didn't really know what to choose. And so we just stick one simple key on it and dock it on the blockchain. But then of course, that solution wasn't quite satisfying for them. And yeah, this is what the Webnar file system is also about. But first, a little bit about me. This is a link to my GitHub.
02:26:11.558 - 02:27:03.950, Speaker C: This is how you find me online. Sometimes I work at Fission, but I think that's been mentioned a bunch already, so I'm going to skip over that part. Vision. Actually little know is an acronym. It stands for Vision Internet software services for open networks. But of course, if you look at this, this doesn't quite spell out and there are some legal documents, but don't think about it too much. What I want to focus on is the open networks part because this is what I'm excited about and why this data and storage room is talking about blockchain and data availability, sampling sometimes and these things and not about AWS s three, right? Because you want to have a permissionless system and a system where you can join and help store data, right.
02:27:03.950 - 02:27:25.160, Speaker C: But if it's open, then you have this problem of developers storing data of users and having to encrypt them, right. We kind of want to avoid the situation we had with HTP and HTPs, or we kind of are in the same situation where you need to add encryption on top. But how? Yeah, and encrypt everything, right?
02:27:25.770 - 02:27:26.566, Speaker B: But how?
02:27:26.668 - 02:27:49.934, Speaker C: So some people will say, well, try TLS from HTPs. But of course TLS is like point to point. You can't really use that. So maybe you take your data cabinet, you zip it into a boulder and you age encrypt it. Who maybe someone has heard of that. That is something that people do. But you quickly run into problems, right.
02:27:49.934 - 02:28:40.446, Speaker C: How do you do editing if your data changes, how do you make that efficient? You don't want to re encrypt your whole bag of data every time you make a change. That is very small. How do you do versioning? Sometimes people want to go back in history. How do you do stuff like over secrecies, right? You want to share access to parts of your data, but not all of the history of it, maybe. How do you do fine grade access control like that in general, right? Give access to some parts of your data, but not all of it. If you have a single key for your whole encrypted thing, then, well, you can only give access to all or nothing. And how do you do stuff like concurrent edits, right? If you have things going on at the same time because you've just shared it with a bunch of people and they all make changes, how do you deal with that? All right.
02:28:40.446 - 02:29:28.422, Speaker C: With all of these questions in your mind. Also, US Vision was looking at the literature and finding, for example, the cryptree paper. So this is a paper from 2006 from some people who were storing encrypted data in an open network. And what they did is they built a file system that essentially has a fuller tree structure and very intuitive kind of sharing of data where you have a very succinct key that you share and that gives access to a whole subtree mirror file system. And like with many haters there's this one very important figure in it which basically shows how it works. This is maybe very small for you to see, but in general, this is like a diagram. You see the columns are like different directories.
02:29:28.422 - 02:30:17.514, Speaker C: So there's an image directory, holiday directory, taipei directory, and a metro picture in it. And you can see that here at the bottom there's a bunch of DKS. They stand for data keys. And in general, all of these boxes and arrows are different kinds of encryption keys that point to each other. Like if one thing points to something else, that means you can reach that in terms of information or you can decrypt something with another key, right? So you have all of these data keys, they actually encrypt the folders themselves and you have these and I want to point to that specifically. These FKs. There are so called subdirectory keys which help you just share a single key with which you can access a whole subtree in your file system.
02:30:17.514 - 02:31:04.902, Speaker C: But I want to pull this out because this is the basis of it and I'm going to ignore all of the other parts of it because this is the thing we took out of the cryptree paper and we basically did the same thing. We had this kind of hierarchy of structure of encryption keys, encrypting other keys and we added more things to it. Like for example, versioning. So if you have multiple revisions, you will use different keys for each revision so that you can give access to revision one without giving access to revision zero. And in this diagram in general, all the solid arrows, the orange ones, they are like encrypts the other thing. And these dotted arrows here, they mean this is a hash function from this key to this key. Or in general, you can kind of reach it via a very simple computation.
02:31:04.902 - 02:31:41.014, Speaker C: So in this model, you can give access to, let's say, the holiday folder in revision one, without giving access to the holiday folder in revision zero. So nice we get versioning. This is not quite what Winifs is doing. It's a little bit more complicated because one other thing we do is we have different access levels in terms of sharing access to history. So you can give access to, let's say, just a single revision and no temporal data. So you won't be able to read future updates to a file or a directory. And that we call the snapshot level.
02:31:41.014 - 02:32:36.230, Speaker C: So those are these SK, these snapshot keys. And we basically borrow the same thing from the slide before for the temporal access level. And now if you look at it, for example, if you just share this small 32 byte temporal key here and you follow all of the arrows. So you're hashing stuff or you're decrypting things by going to the right or to the bottom, you'll be able to access all of these pieces. But in contrast to that, if you just have access to this small snapshot key here, all you can reach from that is just something within this revision. But even within that case, you sometimes have this issue in this picture that let's say I have a file system on my laptop and I want to sync it with my phone for the first time. And what I want to see on my phone is the very latest state of it.
02:32:36.230 - 02:33:42.814, Speaker C: But I also want to be able to read back to the very beginning of the history on my phone. So how do I do that? Well, I would have to basically send this key over to my phone, but then have to hash this key for number of revisions that I had on my laptop times to get to the next state, which is very inefficient. And in general, this walking across the history if you're hashing linearly is kind of inefficient. So we built something else into the system that we call these skip rashes, which are another kind of buffer or layer in the system. And these skip rashes have this curious property that you could actually skip a bunch of revisions at the same time and get to the latest state much, much quicker. And I want to dive a little bit deep into that because that's one of the things that, for example, our CTO Brookslyn came up with and wrote a paper and it's published, so you can read that if you're very interested. But I also will go a little bit deeper into what's happening here.
02:33:42.814 - 02:35:13.094, Speaker C: So what you really need, or what the skip ratchet really is, is basically you can think of it as a pseudo random number generator, right? You need a bunch of random data, which is like all the different keys you're going to use across all of the visions and you need it to be deterministic so that basically you share the state of a skip. Ratchet so that my phone doesn't have to continually be in contact with my laptop to get the latest keys for the latest revisions or files. It wants to stay up to date, but you likely want it to be backtracking resistant, which means that if my phone has a much newer or like if I share something with Boris's phone and his state is a very much newer skip ratchet, he won't be able to go back in history. So for random number generators, that generally means if you observe the state of your random number generator, at some point you won't be able to compute some randomness that was prior to that state, which you get by all of the hashing, of course. And the special thing about skip ratchet is you can really efficiently, more than overn can skip forwards in time in the random number generator. So again, as with most papers, there is this great figure that basically explains the operations that exist in the skip ratchet and you can see there's a bunch of states here. Those are the states of the skip ratchet.
02:35:13.094 - 02:35:39.374, Speaker C: And at the bottom here, you see the secrets. Those secrets you can think of as the pseudo random number generated stream, right? Each one of these would essentially be the key to encrypt or decrypt some directory or whatever. And in between you can see that from state zero, you can just step once. We call this ink increment. And you get to state one and then so on. But there's also these other operations which are called skip and leap. And in this case, this is a simplified picture.
02:35:39.374 - 02:36:06.058, Speaker C: Skip, skips only two steps, or up to two steps, leap up to four steps. And all of these operations, skip, leap and ink, they're all of one. So it's not like leap actually takes the time it would take to do all of those things. It's actually faster. So how does it work? Well, let's look at the side of it. So let's look at state zero. If you actually use a library and print it out, this is what you see.
02:36:06.058 - 02:36:30.670, Speaker C: You see the state of a skip ratchet is some kind of salt, a large, a medium and a small thing. And some counters. All of these hashes are like 32 bytes. Yeah. And the way I like to think, or the most important part of the skip ratchets is actually the large, medium and small. They're called the large digit, the small digit and the medium digit. The reason they are called digits is because it's very similar to discounting.
02:36:30.670 - 02:37:41.770, Speaker C: The idea is basically when you count, you do the thing, right? I don't need to explain it to you. And at some point you'll have a carryover and you reset the small digit and you count the next digit, right? And you take this idea that you didn't really have to count through all of the small digits to get to ten and use it for the skip ratchet. So the way you count in skip ratchets is you have the small digit and you hash it and you get the next state, and then so on and so on, right? Then we also track that in the counter. And then at some point when your counter is 255, so unlike the base ten thing before, this is base 256, when you need to carry over, what you do is you hash the medium digit and reset the small digit, not to the value at the very beginning, but to a value that is derived from the previous medium value. And so that is just a hash of the solved together with the medium value before. And if you look at this, the value of the small digit does not depend on any past value of the small digit before. So that means at any time you can just skip to the next we call the medium epoch.
02:37:41.770 - 02:38:24.550, Speaker C: You arrive to this state and if you want to skip to the next large digit. You again just hash the large digit. The medium digit then gets reset and also skip forward once and the small digit gets reset essentially twice. And if you look at all of this, nothing here depends on the medium or the small values from the history of the skip ratchet before, which means you just skipped 65,000 or more of that steps and you didn't have to do all of the hashing work one by one. So, yay, this actually exists. There's an implementation of it if you care to see if you want to try it out. Basically get all of that.
02:38:24.550 - 02:39:19.334, Speaker C: So with that, with all of this key hierarchy, with all of those skip brackets, we get efficient history versioning and fine grade access control. We have access control, individual files and individual directories. We get temporal access, we get snapshot access. But there's this thing in encryption where we often use the key, the symmetric key, to encrypt stuff to also verify the integrity of what we wrote. So if you look at something that is encrypted, of course you don't know if that is just valid or not putting valid in quotation marks. And what we do is we add the authentication tag to verify that. But the authentication tag basically means someone produced the ciphertext who also knew the symmetric key to encrypt it.
02:39:19.334 - 02:40:07.358, Speaker C: And that can be verified, but it ties the symmetric key that gives read access to write access. And sometimes you don't want these two things to be the same thing, right? Sometimes you want to give somewhat right access, read access, but not write access. So there's another system you need to build on top to really separate them. And often that is something that is public key cryptography, public free, infrastructure based. So you have some public key that is the owner of the file system and you can give out access to individual subparts of your file system by signing things and saying this person has access to this subpart of the file system. Right. So how does this work? Well, if we look at a file system in Winnipeg, so this is just some kind of Pi hierarchy.
02:40:07.358 - 02:41:04.742, Speaker C: There's a bunch of things with different names and you encrypt them one by one. You lose all of the connections between them because you can't read them anymore in the encrypted form. But crucially, I want to point out, we take all of these things, put them in a big hash map, and now we need to name them somehow, right? So in this system you would basically namespace all of the names and in this system you would basically do, okay, well, if I give someone access to, let's say, the work directory, I would sign something that says that and put it in the hash map. And then every time someone provides an update, they would say, okay, well, I got that signature that I have access to the. Right work directory, right. But of course this is not really privacy preserving. You're really leaking a lot of information about the private and the work directory publicly in your open DEXA network.
02:41:04.742 - 02:41:45.846, Speaker C: So that will tell. So what you do is, well, step one, you replace all of the Identifiers with just random numbers. We call them I numbers. Again, 32 bits. This is just smaller for convenience on the site. But still you're leaking the relationship between files and directories, like what is a subdirectory of what or which files are entries of what other files. And so in that case, you can still analyze these structures of your file system to, for example, see if someone is using a certain kind of app that always has a certain kind of file hierarchy structure, right? So what we do is we actually just hash these things.
02:41:45.846 - 02:42:33.410, Speaker C: We also hash them together with a revision that is derived from the skip ratchet and take all of these things, put them into a hash map. And I'm just going to this very quickly. At the end of the day, this is an IPLD hand. You've heard the words before, get a root hash, we're happy. But what we've lost of course is publicly verifying that these things are actually subdirectories or entries of something else, right? So some magic going on in this hashing function actually, because it's just not a normal shaftrique or whatever hashing function. What we use is an RSA accumulator. So again, this is from the literature.
02:42:33.410 - 02:43:35.318, Speaker C: It's an old thing. One way accumulators is the paper where they introduced it. And then there's also the batching techniques for accumulators paper where we basically get a little bit of more privacy protocols or proofs, more privacy preserving proofs for accumulators and all of a bunch of bashing goodness that makes them faster. And the way it basically works is you need some RSA modulus and the hashing function is in practice, like taking some quadratic residue, which you call the generator g, and taking that to the power of all of the things that you are hashing and what that is. Like all of these high numbers again, these 256 bit numbers. You're taking modulus of your RC modulus. And what you get with all of this is there is a very or with those papers before there is a proof.
02:43:35.318 - 02:44:46.020, Speaker C: You can do that. Basically some parts of this, let's say the hash of I number one and I number two are part of the accumulator that came out at the end. And this is exactly what we need because now that all of our Identifiers here in the hash, in the hash map, now that they are all RSA accumulators, you can basically prove that, well, these two directory names were part of the path where the path is represented by the final accumulate them. So you can kind of construct this proof and you can also even prove at the same time that it's like part of multiple things, which is kind of neat. With the Batching Techniques paper, I don't have all of the time on the day, so I can go into all the details of this. But, yeah, basically we are using the pokeistar and the Pocpcr algorithms from the paper. So look into that if you're interested or talk to me.
02:44:46.020 - 02:45:29.894, Speaker C: All right, so what would basically happen when you give out right access now, just to wrap this up, is you sign the path that you want to give access to in the form of an accumulator. And when you verify write access and you give it to some person. And when you verify, write access. What you do is well, you see the diff between the two verses in the file system in the form of the hash map. And you go through all of the keys that changed, look at the Identifiers and make sure that there exist proofs, or that the proof that is provided basically covers all of these keys. And finally, you just verify also the signature.
02:45:29.962 - 02:45:30.660, Speaker B: Of course.
02:45:31.670 - 02:46:19.134, Speaker C: All right. What you get is actually pretty neat because we now have read write Access separation. We can actually verify writes without having read access. So think of, let's say, putting Winifs on a blockchain, or think of putting Winifs into some Hub that relays and broadcasts the latest state of it. It can actually, when it ingests the data from new clients, verify that the rights were correct before they were actually broadcast to all of those clients, before my phone is forced to also verify this again. Right. So you're saving a lot of compute there and you're making the whole system a little bit more resistant to abuse in that regard, which is kind of hard.
02:46:19.134 - 02:47:01.920, Speaker C: If you have an encrypted system, you're always forced to do all of the computation on the clients that actually decrypt. Usually, yeah. And yeah, you still have this normal system where if you have access to a certain directory, you inherit the access for all of the entries within it. So there's one last thing that I didn't touch upon which we need to talk about, and that is concurrent rights. If I'm on a plane and I'm doing stuff on my phone and someone else is also working my Winifs, we may end up doing concurrent writes. And that's the thing we want to be able to deal with. Right.
02:47:01.920 - 02:48:07.218, Speaker C: And you may have noticed that when using Dropbox or any similar kind of service, or git for that matter, I guess. How do you do that? Well, in Winnipeg, we actually keep a lot of the history, which is really good because that allows us at the end to look at what happened and reconcile changes. That is one thing, but the other thing is we're also encrypting everything. So the thing that the Hub sees, if, let's say we're doing all of the merging at the Hub is just this kind of hash map of encrypted data. So what does it do when someone else comes to it and actually does a valid write, but it writes to the same key but something different? Well, the hub can't do anything. So when it merges these two things, it just stores both merges. This is one of these cases where we can't do better, unlike with RSA accumulators sometimes, than just pushing off the computation to the client that will later have to do it when they read.
02:48:07.384 - 02:48:08.002, Speaker B: Right.
02:48:08.136 - 02:49:06.280, Speaker C: So you resolve conflicts at retime. Now, what does it look like? Well, the resolving algorithm is actually fairly simple. If you read a revision which turns out to have multiple conflicting writes, if those two are, let's say in this example, two directories, and then they, let's say, have two different things that were written into them. So one version added a file or a directory work, and the other revision added a directory life. You just merge them together and you keep work and life, both directories. But if you have a change in the same key, you need to recurse. You look at the changes inside and if you realize, oh, well, these changes are reconcilable, well, then you merge them together, you create a new revision, you write a merge directory to it and you recurse back to the top and you link back in history.
02:49:06.280 - 02:49:55.874, Speaker C: That's neat. If you actually have, let's say, a file where there's a conflict, then we can't do anything better than just tiebreak what people usually call it and you use the biggest hash, smallest hash, something like that. And so you're kind of losing data. But you're also very consistent. So you have a consistent state across all of your devices, let's say, which is important. And the other thing you get is because you're writing and persisting a history later, if you want to look at it and fix all of the things that were replaced or deleted, you can do that or even have apps that, let's say, write a JSON file as their app state. And this JSON file actually has a particular structure that say, it's a CRDT because it's an app that knows how to do that or structures their state.
02:49:55.874 - 02:50:33.666, Speaker C: That way the app can go back into history, look at different states of it and just merge them together and write a new revision if it wants to. But in principle, we need some way of producing a canonicalated state. So we tie break on hash in those cases. So that's basically the rough structure of a merge algorithm in WinFS. And yeah, where are we at in terms of implementation? What does exist and what doesn't? Well, there is an implementation in Rust rs WinFS. It exists today. You can use it, and I encourage you to do so.
02:50:33.666 - 02:51:08.106, Speaker C: There's some docs, there's some GitHub link, whatever. It is very portable. So essentially the Rust grade is just a bag of pure functions and you add some like there's an interface for a block store. You just provide essentially the networking and the storage layer via the block store, or roughly handwriting. Here you serialize deserialize some keys, provide some asymmetric encryption and decryption functionality. And that's all you need. And that's all external to ours.
02:51:08.106 - 02:51:53.434, Speaker C: WinFS. So this gives it a lot of portability. So it's very binding friendly. So, for example, it was compiled into, let's say, people who were using it in Swift, people were using it on Android from Java or just using it in the browser via Wasn't, which was one of the main things we were building it for at Vision. So it's a community project. So there's people for example, Functionand has built FX Photos, which basically takes a bunch of photos and is an Android app and iOS app where you can upload your photos and it gets stored in Winnipeg encrypted and then put into a chain there's. Banyan we've heard about them before.
02:51:53.434 - 02:52:28.262, Speaker C: They use WinFS to encrypt data and put it into filecoin. And they've actually contributed a bunch of things to Winifs. So awesome. Shout out to them. Just recently there has been a project capibilion that has been integrating with Winnifest, which is basically a mobile operating system that is building on Winifs. So the operating system underneath actually uses WinFS. And you can share data across devices between people, that kind of thing, the most recent thing of them all.
02:52:28.262 - 02:52:42.234, Speaker C: And yeah, with that, users talk to us, get involved, and thank you. So, yeah, then I guess thank you.
02:52:42.272 - 02:52:50.062, Speaker B: For all the speakers coming today. It's been good afternoon and everyone is.
02:52:50.116 - 02:52:51.534, Speaker C: Giving 40 talks today.
02:52:51.572 - 02:52:53.774, Speaker B: And I hope everyone enjoyed the time here today.
02:52:53.892 - 02:52:57.454, Speaker C: So, thank you, speakers, again, and also.
02:52:57.492 - 02:53:03.326, Speaker B: Thank you for all the panelists right here to attend our MBT Research House event.
02:53:03.428 - 02:53:04.880, Speaker C: So, yeah, thank you.
02:53:37.170 - 02:54:04.100, Speaker A: That's close.
