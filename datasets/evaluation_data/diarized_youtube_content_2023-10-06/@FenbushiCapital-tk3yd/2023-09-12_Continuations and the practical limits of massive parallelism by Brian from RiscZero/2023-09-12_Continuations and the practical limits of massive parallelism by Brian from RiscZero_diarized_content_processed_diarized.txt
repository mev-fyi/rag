00:00:03.130 - 00:00:44.860, Speaker A: We will have Brian from RISC Zero and I guess a little background on the risk Zero. So they are building zkvm. Well, as other projects have mentioned a couple of times, it's a verifiable compute primitives that is built on top of risk five and it's also a stock based VM like Maiden is, but they have different use cases and I guess assumptions around the way that people can use those risk zero VMs as well. Brian, welcome and feel free to take it over.
00:00:46.030 - 00:01:02.490, Speaker B: Glad to be here. Thanks for having me. And always great to be able to hear Bob and talk about anything. Always happy to follow him. Let's see, can we see everything on the feed? Yeah. Great. Cool.
00:01:02.490 - 00:02:13.782, Speaker B: So I'm brian redford. I'm the CEO at RISC zero. And today I want to talk about ZK parallelism, specifically in the context of continuations and some of the work that we have been doing there for people who are unfamiliar with RISC Zero. The company itself is focused on building general purpose ZK computing technologies. But our sort of main product at this point is the RISC Zero Zkvm and then the Bonsai Proving platform or ZK application Development platform that is really focused on making it easy to use ZK proofs. So the risk zero Zkvm lets you succinctly prove any computation as opposed to, say, any of these standard proving systems that you have to write circuits for or sort of Zke EVMs where you actually are proving only the EVM. So there's a lot of different things you can do with that.
00:02:13.782 - 00:03:01.590, Speaker B: But the thing we're focused on today is the sort of feature we've added to it called continuations, which lets you take a proof and split it up into tens, hundreds, thousands, really an arbitrary number of parts and prove them independently. This feature also lets you do some interesting things around sort of suspending and resuming images, but we're not really going to get into that today. So the interesting sort of data point around the usage of the Raceroz UKVM is the Zeth project. And we just open source this. The link should actually be on here, but it's just GitHub. It's the reservoir GitHub. There's a Zeph project on there, you can check it out.
00:03:01.590 - 00:04:22.320, Speaker B: But Zeph is effectively a Zke EVM that runs on top of our Zkvm and utilizes this continuations feature to prove an EVM block sort of massively in parallel. So the purpose of doing this is to let people effectively build a ZK EVM without actually needing to do any specific work for the Zkevm. So this project is based almost entirely on pre existing code. We had to modify some of the rep R EVM code to be a bit more serialization friendly for ZK. But yeah, by utilizing continuations and parallel proving, we were able to get proving an entire standard ethereum block down to about 15 minutes for 4 billion cycles. So as far as we're aware, Rckvm is sort of the first system that's actually able to stitch proofs together in this manner and actually prove an entire regular ethereum block. This is sort of part of this pattern of enabling limitless parallel ZK computing.
00:04:22.320 - 00:05:41.944, Speaker B: How does this work? The Risk Zero Zkvm actually is based on the Risk Five instruction set, or ZK, proves programs that are expressed in Risk Five. So typically this looks like you take Rust, some compiler turns it into RISC Five, and then it gets proven through our Zkvm. Now, in a world of continuations, what this actually looks like is this executor takes the Risk Five source code and then splits it into a bunch of these sort of separate segments, which are then proven separately and together are then potentially rolled up into a single proof. So the continuation feature is actually something that takes this Risk Five code, simulates it and produces, based on your configuration, a particular set of proofs that prove sort of the state transition between the first part of the code. So this is like segment receipt, one, two, three, et cetera. It's pretty straightforward to sort of think about these things. Then once you run this program through the entire executor and produce this list of sort of segment proofs, you can imagine that proof getting quite large if you have thousands of segments.
00:05:41.944 - 00:07:10.800, Speaker B: So currently the Bonsai service will let you roll those proofs up utilizing recursion so you can get a single proof and then post this on chain. So if you want to think about how this actually works at a sort of technical level, the executor, what it's doing is actually simulating the execution of a particular program. So in this case, say we're running the EVM, it's actually taking the block that we want to produce that proof for and then running like the EVM gets compiled down into Risk Five, and then that block is provided as input along with all the rest data you need. And the executor takes that program and then produces these sort of traces of executions. This is basically like this Risk Five instruction was executed and here were the results. And then it also takes and basically tracks which pages of memory have been accessed at any particular point in time and creates a sort of mercalized representation of which pages are dirty. And that sort of memory image and the data of what cycles have been run through is sort of the necessary proof data that you'd need to link these segments up to each other.
00:07:10.800 - 00:08:21.450, Speaker B: So you can imagine that this executor produces this trace that says, here's what actually happened in RISC Five. And then here's all of the memory movement through this memory management unit collapsed onto this merkel page table. As soon as the executor produces a single segment. You can imagine anyone else can take this sort of data and then actually produce a ZK proof of it. So that's what the sort of offset nature of the visual is intended to describe this notion that the executor work and the prover work are completely separable. So if we look in the context of the Zeph system and think about what parallel proving for this actually looks like sort of a demonstration time moving this direction right now. So proving an ETH block is about 4 billion cycles and by default we split programs up into segments of exactly 1 million cycles.
00:08:21.450 - 00:09:26.202, Speaker B: That's pretty arbitrary. And you can imagine that the choice of segment size actually has quite a large impact on performance depending on how a system is configured. We won't get too much into that right now. So this is just a sort of demonstration that as the executor starts chewing on this ETH block, it starts spitting out segments until it produces the sort of full 4000 segments. But you can imagine that as soon as you've produced a single segment, you can send that out to your proving cloud and sort of overlap proving of those segments with the actual generation of the segments themselves. However, if you want to get down to the single proof that you actually want to post on chain somewhere, you also need to take all these sort of initial 4000 proofs and then recursively collapse those down to 2000. Proofs and then 1000 proofs, et cetera until you've gone through sort of full log the number of segments, levels of proofing.
00:09:26.202 - 00:10:37.806, Speaker B: And at the end of all of this, you have a single proof. So right now, as I mentioned at the start, it takes about 15 minutes, 15 to 50 minutes, depending on whether you're using 3000 GPUs or 50 GPUs to sort of produce these proofs. Obviously, the faster you can produce these, the better you're going to be and the more use cases you're going to be able to support with these kinds of proofs. So 15 minutes is pretty far away from the sort of goal that Ethereum has of creating enshrined roll ups where you would actually be able to do something like this in 1 second. So when we think about the gap between where we are now at 15 minutes and moving to hopefully 1 second, eventually it's interesting to sort of think about where the different bottlenecks in the system are. So before we dive into that though, I want to talk about how you can sort of naively think of how long it takes to generate a single proof. So you have to generate all the segments first, obviously.
00:10:37.806 - 00:11:50.938, Speaker B: And then you have to take those generated segments, distribute them out to all the nodes that are going to prove them, and then they need to prove these initial sort of leaf proofs, which are the proofs of the actual traces. And then you have to produce these join proofs that collapse two proofs into one proof or three proofs into one proof or however many you choose to choose to collapse. And those are actually quite different, they end up taking quite a bit different amounts of time. On a single machine. The join proof time for two to one is about 6 seconds and then for proving a million cycles, it could be about 100 seconds, depending on which machine you're using, more or less depending on which GPU you have. So you can imagine the sort of theoretical maximum for an arrangement like this being effectively the time it takes to generate the segments, plus just one sort of segment proof and one joint proof if enough things were overlapped. If the amount of proving time were to be decreased.
00:11:50.938 - 00:13:01.134, Speaker B: Or you get enough machines to prove them all such that you can finish all of the proofs directly after the segments are generated and then also roll them up. While you're waiting for that last segment to be generated at the end, you potentially only have to do one proof. Although the sort of roll up tree in that context looks a bit different. So when we look at a system like this, there are a bunch of different bottlenecks and there's one that I didn't really list on here that we can talk about later. But obviously there's this segment generation or execution which is effectively the simulation of the Risk Five chip and then the sort of recording of all of the memory transfers. Then there's taking these kind of segments which are the full sort of memory state of the virtual machine at that point in time, and the execution trace and then distributing those to all of the provers, and then provers actually have to prove things. And then that proof needs to get distributed to a bunch of other machines potentially to be combined down to this sort of single proof.
00:13:01.134 - 00:14:33.210, Speaker B: So we see about five main points of potential bottlenecks in the system. There is a question around sort of execution right now is single threaded, whereas obviously most machines are not single threaded. And there's certainly a possibility to think about splitting up some execution into more threads of execution. And there are a variety of techniques one could use to do that which would help address sort of the execution overhead. So right now, if we look at Zeph and where most of the time is going when we run one of these massive proofs, the execution or sort of witness generation, segment generation is actually the main bottleneck, everything else can kind of be parallelized away. So at this point in time, we sort of have an interpreter for RISC Five, so we're only able to really get up to about 15 to 30 MHz, depending on the sort of circumstance, the exact thing that's being proven anyway and how much memory movement is involved. So when we look to really reduce the amount of time it's taking to build these kind of blocks, that seems to be the number one area for people to focus on, sort of improving the performance of separately, we still see like two to one recursive proving, taking about 60 seconds.
00:14:33.210 - 00:15:49.990, Speaker B: So right now we're looking at sort of a theoretical maximum of ten minutes without addressing some of these because we can actually get segment proving down to about 30 seconds. So techniques for improving some of these we can talk about. So this is kind of where the bottlenecks are in the system currently. You can imagine as some of these get addressed, the bottlenecks will move to different parts of the system. So over the medium term, once we do some of the things we're going to do to address segment generation time, you can imagine that potentially, especially if you're thinking about doing this on an ASIC or something, that distributing all of the data necessary to prove the segments might start to become the main bottleneck. And then you'd have to explore mechanisms for compressing that or reducing that, or thinking more intelligently about how you distribute these things at different points in the proof roll up process. And then obviously just the overhead approving a particular segment and this prover time itself will start to dominate again once you kind of get rid of the recursive overhead and this execution overhead.
00:15:49.990 - 00:17:14.130, Speaker B: Long term, I do think we can imagine getting to a world where if we get to 150 MHz, we're talking about 36 seconds and 1.5 MHz, or sorry, 1.5 GHz, we're looking at like 3 seconds, but that's still kind of far away from the sort of goal of 1 second. So you can imagine also getting from 115 or multiple gigahertz is going to be quite the journey. But until these other things are addressed, it doesn't really make sense to really focus on this. But long term execution or the intelligence with which we're able to split various work streams up into chunks is going to really be the sort of bottleneck, I think. And you can imagine at a certain point as well, if you can get recursive proving itself to be extremely fast, which seems totally doable with ASIC techniques, then eventually you might end up in a place where the bandwidth limitations, especially in a decentralized context, become more of a concern, which might lead one to want to research potential mechanisms for compressing some of these proofs.
00:17:14.130 - 00:18:38.846, Speaker B: So when we think about eliminating some of these bottlenecks in the system, moving forward, there are a bunch of different sort of techniques you could employ. So for execution, which is at this point, the sort of major bottleneck, rather than just simulating the execution of Risk five, you could imagine actually taking the risk five instruction set and then getting a native sort of set of code that would build that would actually execute the Risk Five instructions and sort of build the merkel tree as you go. There's a lot of work necessary to build a system like this and maybe that's going to get you sort of ten X, but because the risk zero Zkvm is based on risk Five. You can also imagine building something that increasingly looks like hardware that is specifically focused on sort of witness generation or generating these segments. So we do expect to see in the medium term some utility of FPGAs just for this part. So I haven't seen a lot of people talk about breaking down proving systems, this problem of generating proofs into sort of phases. When thinking about hardware, you tend to see people trying to tackle sort of the entire problem at once.
00:18:38.846 - 00:20:27.620, Speaker B: However, I think taking a risk five sort of core that's already designed and figuring out how to modify that to be able to spit out these segments would be a really interesting research problem that has the potential to massively speed up these kinds of very parallel proofs and then obviously you can take that a step farther and then think about actually building an ASIC that is very much focused on segment generation and that could obviously be joined to hardware that's doing proving as well. But I actually think it's really interesting to think of these problems as mostly separable. So in terms of segment distribution and generation right now when we think about the way these things sort of get mercalized, it's pretty inefficient with our current approach. We haven't really tried to optimize it, but there's a million techniques you can sort of imagine for compressing memory or becoming more efficient about which parts of prior memory that were touched you pass forward as you go through all of these segments. Additionally, you could think of becoming a lot smarter about how big these sort of segments are and start to think about potentially having dynamic segment size. So obviously the sort of fewer segments and the more compressed they are, the less data you need to spread out over the network to later prove. Yeah, and to some extent also you can imagine sort of changing the topology of the tree roll up to not necessarily just be a strict two to one tree roll up in terms of actually improving the sort of segment, improving time itself.
00:20:27.620 - 00:21:57.022, Speaker B: Obviously we're trying to stay abreast of all the developments and all of the proof systems and we'll continue to keep the sort of risk five Zkvm at the bleeding edge of performance. But there's also the ability to run this existing code on a variety of hardware and we're actually partnering with a couple of teams to work on building these more efficient hals which will help accelerate the actual proving itself. However, as we've noted, it's not really a bottleneck yet, at least for sort of the Zep scenario. We can basically prove all 3000 or 4000 segments in 30 seconds right now over time. Also, this is a Stark based proving system, so hash performance is a major component of the entire system, especially the sort of recursive proof generation. So you can imagine advances in hash functions and we've seen this with poseidon two and five, but there's probably room to go in terms of producing even more custom hash functions for systems particularly like this. Additionally, we currently have a number of accelerator circuits like Shaw 256 and ECDSA or just big integer support.
00:21:57.022 - 00:23:32.970, Speaker B: In general. You can imagine for particular problems, making it easier for people to build these acceleration circuits and incorporate them into the overall system will be a sort of big unlock for some sets of problems for recursive proving and speeding it up. You can imagine not just doing two to one sort of recursive collapse, but picking that number based on the particular characteristics of the sets of hardware that are available to your proving cloud. And then one could imagine using some of the flexibility that exists inside the Stark based systems to trade off performance time for size in the long term, if you ever get to the place where these sort of proof sizes are actually causing problems. But in any kind of parallel proofing system, you know you're going to be constantly running into sort of new bottlenecks along these kinds of lines. So, yeah, what we're doing at risk zero and what we would potentially like people's collaboration on, if you're interested, we're building out this next version of the proof system that will make it a lot easier to incorporate new accelerators and potentially different VMs and have them all kind of work together to prove a particular computation. And you can also imagine sort of parallel proving as being part of that.
00:23:32.970 - 00:24:47.600, Speaker B: And we'll be releasing that later this year along with this Zergen circuit construction or accelerator construction toolkit. I'm not sure exactly when that will get released, but hard at work building new versions of the circuit that utilize this technology. Yeah. And then all of the sort of data and diagrams that I've presented here are all this sort of AWS, centralized bonsai system which will continue to develop. But we're also very much interested in and starting to work on the decentralized version of that which is going to inevitably have a different shape and probably push some of these trade offs in different directions. And then obviously we're hoping to see more and more projects start to seriously consider Asics and Nfpgas specifically for the sort of witness generation or execution generation part of the system. Yeah, so it's kind of a speed run through the sort of massive parallel system that parallel proving system that is Bonsai.
00:24:47.600 - 00:24:49.490, Speaker B: That's actually all I have.
00:24:51.960 - 00:25:16.172, Speaker A: All right, thank you. Thank you Brian, for a very thorough overview and I guess a bit of details and a lot of the continuations and segmentation stuff that you guys are working on, I guess just to kind of dive into some questions. So one thing that I was noticing, you mentioned that you want more efficient house, I believe.
00:25:16.226 - 00:25:16.492, Speaker B: House?
00:25:16.546 - 00:25:37.904, Speaker A: You're referring to the hardware abstraction layer, right? Yeah. When you say more efficient house, if you can elaborate a little bit on, first, what kind of house that you have and then second, what's the ideal types of improvements that you want to gain from more efficient house.
00:25:38.102 - 00:26:44.536, Speaker B: So currently we have a CPU how and then we have a metal howl and a CUDA howl. The CUDA howl in particular requires a lot of effort to really optimize all the various bits and pieces of the sort of fry and overall stork algorithm. So we are working with Supra National right now on a separate CUDA implementation that has a much more optimized. Also, I think it would be really excellent to see people build out sort of the Vulcan hals and then potentially hals that target FPGAs or hardware specific to mobile phones. You have a lot of really interesting kind of acceleration targets on mobile phones and I think pushing proof generation out to the edge over time is really interesting. I don't know if people are ever going to want their mobile phones to participate in parallel proving networks, but I guess if you can earn tokens while you sleep, while your phone's recharging, maybe yeah, that's a different world. Maybe.
00:26:44.536 - 00:26:45.610, Speaker B: Got you.
00:26:47.660 - 00:27:34.052, Speaker A: And then also right now, I'm not sure how much details it would kind of get into it, but for the segment generations and also actually defining where you set the bound for each segment. Because I know for a fact that when you sort of set how much segment you want, the unit of it is based on the cycles of Compute, I believe. Right, so it's not like time or anything, just like cycles of compute. But on what standard are you saying, okay, this is the amount of cycles that I want to have per segment. Is it based on the amount of, I don't know, the states that can be stored on Ram or what's the.
00:27:34.106 - 00:27:51.112, Speaker B: Yeah, so the 1 million cycles was we just kind of picked it because it did seem like a sweet spot for a lot of hardware. Specifically, proving 1 million cycles only requires about slightly under 16 gigs of VRAM.
00:27:51.256 - 00:27:51.836, Speaker A: I see.
00:27:51.938 - 00:28:15.088, Speaker B: This actually produces a proof system or a parallel proofing system that requires consumer grade or prosumer grade GPUs. However, depending on the network topology and who's participating in it and whether they're approving it on their mobile phones or what year it is, you can imagine wanting that point to shift.
00:28:15.264 - 00:28:27.560, Speaker A: Got you. That makes sense. So basically the idea is that the more cycles you, I guess, pack into the segments, the bigger hardware requirements that you'll be requiring for those parallel approvers.
00:28:28.380 - 00:28:41.704, Speaker B: Basically, yeah, I see. The memory. If you try to approve the sort of full two to the 25th cycles that we can fit in one segment, the memory requirements can balloon up to like 200 gigs.
00:28:41.832 - 00:29:15.750, Speaker A: Got you. Actually, another thing I wanted to ask was we talked a lot about continuation here and I think the audience would know what continuation is going to be like and what it may look like in the future. But one thing that Brandon from One labs earlier today mentioned that kind of was interesting was that he claimed that there's no ZK VM in the space that is performing like recursive proof for.
00:29:17.720 - 00:29:18.180, Speaker B: Okay.
00:29:18.250 - 00:29:18.628, Speaker A: Yeah.
00:29:18.714 - 00:29:28.044, Speaker B: And I think the reason currently our source code for this is not completely open source, but it definitely works, right?
00:29:28.242 - 00:30:19.020, Speaker A: Yeah, I think the reason why he said that is because he kind of treated the continuation work to be more like a partitioning of the work and then kind of like recursively combining all those proofs together. I think maybe that's kind of like his intention where he said that no zhevian is currently doing the recursion of the actual computer or something. But I guess recursion can exist in many different ways. So in the future, how do you foresee continuations? I guess, one, have the recursions in what forms, and two, if it would be just like sort of like a mercalized proof that just sits on top of continuations, which I believe is what we have today, or if it's going to change in some shape or form going forward.
00:30:19.170 - 00:31:17.508, Speaker B: Yeah, I think whether it changes form moving forward is going to probably have a lot to do with how research progresses in terms of folding and maybe some other techniques. Although at least from our perspective, it does seem like recursion because you're utilizing the sort of succinctness property of Ztape systems at every step, it really does help address some of these sort of communication overhead issues. Right. At every sort of step of collapsing this tree, you're really only needing to pass a 200 kilobyte proof. So that's pretty appealing. Whereas I think in the sort of folding world you might end up shuttling along a lot more witness data as things sort of accumulate. But yeah, that's the only major kind of change I would expect to see, I think, other than the recursion system being fully open source.
00:31:17.704 - 00:31:44.310, Speaker A: Got you. Since you mentioned about voting, what's your current sort of prospect around actual practical usage of voting within this kind of exec VMs? And what's sort of like the bottleneck that you see imminently with using, let's say, some variations of folding schemes that we have in the space?
00:31:44.840 - 00:32:49.580, Speaker B: Yeah, obviously highly folding scheme specific, and this is definitely more of a Jeremy question than a me question, but currently it seems like the ability to actually accumulate data and share it around in a way that lets you prove across 3000 nodes seems like it's probably going to be a bottleneck for a reasonable number of systems. It's true that right now it takes 5 seconds to do this sort of two to one recursion, but there's pretty clearly some paths to make that a lot better, somewhere between somewhat better and much, much better. You could also imagine potentially creating an ASIC that's very tuned to sort of recursion as well, rather than trying to also do the sort of broad set of segment beef proofing because that's very amenable to GPUs and GPU performance will continue to increase, but this recursion sort of the join system and predicates are very constrained.
00:32:50.320 - 00:33:13.700, Speaker A: Gotcha, regarding the FGA and ASIC I think you also mentioned about the witness ASIC for segmentation work, what's your sort of general stance around usage of FPGA, usage of ASIC under different circumstances, whether it be for segmentations or whether before actually proof generations?
00:33:14.120 - 00:34:20.810, Speaker B: Yeah, I think for proof generation, I was very bearish on this in the past, but now that we're seeing systems like continuations, the ability to drive memory usage down a lot, you're going to be able to see some focused Asics, actually utilize HPM in productive ways to really accelerate NTTS. And some of the polynomial evaluation. I'm really, I guess, personally interested in and bullish in this sort of witness generation or executor executor kind of asics because, well, you can utilize the fact this is already a risk five system to kind of an instrument, an existing kind of risk five chip. And it seems likely that you're going to be able to produce something that looks like sort of risk five performant part that actually generates these kind of segments as it goes along and computes. But there's a ton of research we have to do there, somebody has to do it's more intuition and sort of follows from looking at the problem.
00:34:21.580 - 00:34:39.884, Speaker A: I see. And I guess another potential extension of research that could come out of that is perhaps we would love to hear your thoughts on this too, but how executor could be performed in a more paralyzed fashion.
00:34:39.932 - 00:35:31.820, Speaker B: Yeah, exactly. I imagine there's nothing really stopping you from having actual threading models other than it's quite complex to do this, but you could make a multithreaded executor that actually generates segments on a per thread basis and you have to deal with synchronization. So you almost certainly want that to be on one chip or one system that has very low latency links between it. But you also, for a lot of use cases, don't really need to go full threading and you can just think about graph computational models instead, which that you can kind of almost do at a higher level. So there is a question as to how low level you really want to get with parallelism. And I think that's going to be informed by how fast people can get a single thread to go honestly.
00:35:31.980 - 00:35:43.328, Speaker A: Got, you also, do you foresee that the entity who is running, or let's say that the machine who's running the executor would necessarily be separate from those who are running the provers?
00:35:43.424 - 00:37:04.140, Speaker B: Not necessarily because it's single threaded, you can imagine that there's not necessarily a reason to colocate it with provers because if you want to take a single thread of computation and spread that out over 5000. Nodes or something like this, you probably don't need to have the executor hardware in every single of one of the proving nodes. So just in terms of raw integer operations, the proving still very much dominates the sort of set of operations. It's just so embarrassingly parallel that you can use tons of hardware for it, but you could imagine trying to build hardware that can handle parts of the execution and generate some of the initial proofs or, I don't know, otherwise reduce the sort of amount of data that needs to come off the chip in order to do these recursive proofs. You also get privacy sort of as soon as you go through the ZK proving process. Once you don't seek if you're just doing the sort of witness generation and executor and building these segments and spreading them out everywhere, it's very obvious to all the parts that are proving these leaf nodes what's being proven.
00:37:04.720 - 00:38:04.048, Speaker A: I see one parallel I wanted to roll here that also love to hear your thoughts on is the fact that executor kind of looks like sequencer in terms of sure right. Which then also comes to another question which is like what is the trust assumptions around the executor? For example, could an executor be, let's say, performing some malicious chopping of the compute slots such that some provers would always get a disadvantage in some way while the others always have the advantage. And if, let's say, the provers are some sort of like a reputation based or have some performance metrics to analyze which provers is doing a good job, which proverbs doing bad jobs, then I wonder if there's some trust assumptions around how fair that the executor can actually perform those.
00:38:04.214 - 00:38:43.400, Speaker B: Really think so? Because it's not like you can reorder the transactions. So yes, it is a sequencer but the order very clearly proceeds from the program and the inputs. There's not really sources of nondeterminism there or there's not really a valid way to run a program. I guess depending on how exactly the code gets compiled you might get different results but if you have the same binary you should be getting the same results no matter what. And there's not really I guess if you start to have variables that the executor can choose from like what size chunks it wants to produce.
00:38:43.480 - 00:38:44.110, Speaker A: Right.
00:38:44.720 - 00:39:00.400, Speaker B: You can imagine somebody who's cornered the market on execution being able to tune the segments they produce to favor their provers that they also produce. Right. There is some potential for collusion there, I guess.
00:39:00.550 - 00:39:24.330, Speaker A: Yeah, because I was thinking, like, oh, what if each prover gets the same amount of rewards, but depending on the cycles of computes that they did prove for, they can reduce the compute load and maximize the reward for? Specific approvers while others maybe they would overload it so that even though they get the same reward they will have to do more compute or something.
00:39:25.820 - 00:39:29.740, Speaker B: Obviously highly dependent on how you end up splitting up the proofs, I guess.
00:39:29.810 - 00:39:30.430, Speaker A: Exactly.
00:39:30.960 - 00:39:35.244, Speaker B: A fun problem to dive into over the next couple of years for sure.
00:39:35.282 - 00:39:59.364, Speaker A: Definitely a lot more. I guess research works to be looking ahead on that as well, so interesting. All right, again, thank you very much, Brian, for the great talk about the Sigma generations and continuations and looking forward to the developments from the risk zero sides and also any future research that may come out of it as well.
00:39:59.562 - 00:40:03.236, Speaker B: Great. Yeah, thanks for having me on and organizing. Thank you.
00:40:03.258 - 00:40:03.810, Speaker A: Thank you very much.
