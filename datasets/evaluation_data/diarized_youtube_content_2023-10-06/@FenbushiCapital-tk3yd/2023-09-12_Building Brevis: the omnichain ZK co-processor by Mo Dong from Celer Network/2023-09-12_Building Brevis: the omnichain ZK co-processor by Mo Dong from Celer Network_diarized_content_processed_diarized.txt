00:00:03.050 - 00:01:09.140, Speaker A: Our next speaker will be Mo Dawn from Seller Bridge. So I guess a little background on this. So Mo has obviously built Seller Bridge and now it's up and running. It's one of the major bridge that are connecting a lot of chains in the space. In the recent times, they have also announced about Brevis, which is sort of like their ZK branch where they attempt to do a lot of ZK storage proof. They have even built ZK Lite clients as well. So that's kind of like the reason why I also invited Mo from Setter here for yeah, he will be soon joining us for the panel and share more about some of the recent works that they have been doing on the CK side of things.
00:01:09.140 - 00:01:16.200, Speaker A: And yeah, he should be joining soon, so we'll just wait for a.
00:01:37.080 - 00:01:37.588, Speaker B: Oh.
00:01:37.674 - 00:01:46.070, Speaker A: Apparently Mo's zoom crashed, so oh, there you go. We have him again. Hello Mo.
00:01:47.160 - 00:01:54.200, Speaker B: Hey. OK, sorry, just crashed while you were introducing me, but yes, I imagine.
00:01:57.020 - 00:02:02.956, Speaker A: I just saw, I was like, oh, good job. Anyway, well, I mean, the stage is yours, so feel free to take it over.
00:02:03.138 - 00:03:12.524, Speaker B: All right, well, first of all, thank you Yuki, for hosting this great session. I think this is very important as a venue for us to talk about and exchange some ideas, especially on the cutting edge of ZK research. And today I think I would just use this slide that I have for the new initiative as Yuki talked about. That is our Ziki Co processor initiative that we have been kind of focusing on recently to talk about some of the interesting challenges that we have seen on Saturn's side while building this smart coprocessor for Blockchain. So, first of all, I think there are going to be I heard the entire talk from Yi who gave an excellent talk on this topic. So there might be some overlapping materials here, but might be coming from a little bit different perspective. So I've been in this space for a while and it's great to see that the entire space definitely seeing some explosive growth, especially with the DeFi as the real product market fit moment for Blockchain.
00:03:12.524 - 00:04:21.204, Speaker B: Right. So it really showed that the trustlessness of our trustfrainness of Blockchain really made a difference in terms of changing awakes of in person meeting and negotiation in traditional finance to zero meeting and one click of mouse. And during the process of this entire DeFi growth and product marketing process, we also accumulated tons of data on the Blockchain today that are very valuable. You can extract valuable information from this. Oh, sorry, is there any issue about this? Okay, no issue. Yeah, you can extract a lot of valuable information from you can and should be able to build a lot more exciting applications on Blockchain, like using Onchain data to do data driven DeFi, user lifecycle management, protocol governance automatically risk management by just looking at the onchain data alone. And in the AI machine learning space, you should be able to do privacy preserving data exchanges moderated by the blockchain and AI agent coordination.
00:04:21.204 - 00:05:16.004, Speaker B: On metaverse gaming you should be able to just like traditional gaming do behavioral driven user engagement, a new kind of user acquisition funnels and these kind of exciting data rich and computation heavy applications. However, none of these today is seen in blockchain space. The reason for that is because the blockchain today is still fundamentally a slow web3 CPU. This is really kind of a by design because blockchain give us high trust of furnace. But at the same time, the other side of the coin being a distributed system means that it is actually low performance, high cost and insanely expensive comparing to just our traditional computing architecture. There are a lot of improvement on this front for sure, and they're very valuable. But they're still blockchains and rely on distributed consensus.
00:05:16.004 - 00:06:28.050, Speaker B: Even for layer two, they are still posting on a distributed system at data availability or other layers and therefore this kind of distributed system will always be bad at data rich and very heavy computations. A manifestation of this, as Yi also mentioned in the previous talk, is that on chain smart contract data today can only access data in a very limited form, that it can only access data on the same chain, not on every other chains. They can only access data in the current block with no view in the history, even though the data is just sitting there in the blockchain's full node and archive nodes. And they can only access data via fixed and predefined view interfaces. If they're not defined beforehand, there's no way you can actually look into another smart contract to get data from them. So there's no arbitrary access and random access capability today. An example of this is that let's say you want to build a user loyalty or VIP trader program on Uniswap, right? So let's say trader traded the 10,000 trades $30 million volume in the last 30 days and you want to give it VIP status by having 50% trading fee rebate in the next month.
00:06:28.050 - 00:07:49.780, Speaker B: You see this kind of a trading fee rebate in literally every centralized exchanges. But why are we not seeing this in uniswap or any decentralized exchanges? Because first of all, it is extremely expensive to build if you just want to build it natively in a smart contract, you basically need to track and record and do accounting of every trade for every traders in the smart contract record. Okay? What is the cumulative trading volume for this guy? What is cumulative trading volume for that guy? And in the end of day, you are incurring significantly higher cost for every trader while just maybe serving 5% of these VIP traders. So even though that the VIP trader does bring a lot of volume, they significantly incurred a cost create a very worth of social welfare. Therefore, no decentralized exchanges are implementing this today. Some of you might think, wait, you don't actually need to implement this thing directly in smart contract, you can just do retrospective on chain analysis, right? So how do you do this today is that you can actually remember all those dates rules of the blockchain by using a smart contract. Let's say that this tab is not that expensive and it can be shared by across the cost can be shared by a lot of different applications.
00:07:49.780 - 00:09:23.064, Speaker B: But the problem is that if you want to kind of prove or generate a trust rate proof for a VIP trader like the previous example mentioned, you basically need to calculate the 10,000 merkel proof of transaction inclusion on chain directly and then not only that, you need to decode that RLP, right? So you're kind of going into this transaction and look at the event, okay, this guy traded USDC to wrap east and you need to decode that and also directly on chain. And then finally you need to compute and aggregate all of these decoder trading volume into one and then kind of prove this to blockchain. So how expensive is this? Well, for this example, we actually calculate the cost on ethereum. You need to basically take 14 hours of block time if you're kind of not taking every block time, but just let's say more realistic sam and $20,000 in transaction fee to just calculate that a trader reached the VIP status. This is insane and not actually acceptable. Fortunately, in computer history, we met this kind of problem before, right? So CPU was on extremely fast growth since 1970 on Moore's Law. But even though the computation capacity is exploding, it is still fundamentally bad at certain things.
00:09:23.064 - 00:10:31.144, Speaker B: It's bad at low point arithmetics, it's bad at large degree parallel computations, it basically is bad at for anything that relate to viralization. So this is why even in the early 80s, people start to introduce a CPU coprocessor and later it get called to a GPU to basically handle these kind of cases. That is fundamentally bad for CPU architecture. And this is how we transformed from this black and white word to the word we're seeing today, that is through the CPU core processor. So history rhymes here and we think we will see similar solutions built for web3 board blockchain which act as the CPU for web3 for today also needs its own coprocessor. So the concept of coprocessor is pretty simple, right? So you have a blockchain and there are some computation tasks that is not suitable for blockchain to do. You just offload this and then the coprocessor after coprocessing will just return the result and the blockchain can just focus on transactional computation.
00:10:31.144 - 00:12:08.030, Speaker B: That is a synchronizing nature and you have to record the entire computation on chain directly because you care about the process. You not only just care about the end result, but you also care about the process of the computation and for the coprocessor it can just do all the data rate and heavy computation and there's no need to record the process of computation but just the result. And it is of course asynchronous in nature but the important thing here is that by introducing the coprocessor we should not break the trustfree nature of the blockchains. Therefore simple centralized server acting as a coprocessor definitely doesn't work and this is why we need a ZK based coprocessor like Bravius that can run arbitrary complex computation on any data and generate easy to verify test station for the computation result directly on chain. So I will just quickly go through the intuition behind using JKP for the coprocessing problem here. So let's say our goal here is to convince the blockchain x plus y equals z and let's say this computation needs 100 compute units by itself, right? So there are really two ways we can do this. The first way is that we just directly compute it on the blockchain but note that the blockchain's unit computation cost and the unit computation time are extremely much higher than the off chain computation environment and therefore we'll get a pretty bad result.
00:12:08.030 - 00:13:16.210, Speaker B: But the other way we can do this chain but at the same time also generate a ZK proof based on the computation itself. So note that the process of generating the ZK proof actually consumes a lot more computation unit than computation cells. But because the unit cost in the off chain environment is so much lower, cost and time all get improved significantly comparing to directly computed on chain. Because verifying proof on the blockchain also is much cheaper than just doing the computation itself. So even though that some of the numbers are kind of exemplary and sort of a made up but the order of magnitude is what we're shooting for for building the coprocessor world. So it seems that there are some internet problems. Okay, all right, no problem.
00:13:16.210 - 00:14:39.080, Speaker B: Now, instead of doing all of these calculations on chain directly, we can do this off chain and generate proof for that. And not only that, we can also bring other blockchains data to ethereum or ethereum's blockchain data to any other chain by relaying the entire consensus process and run the consensus computation off chain and verify the consensus process on a different blockchain using ZK proof which is essentially ZK lite or full client. It depends on the computation power you want to consume for the provers. So this is what the briefs architecture is about, right? So the lower layer is the Briefs core that is highly optimized ZK cryptoxy prover engine that can take in any kind of data and generate attestable results and then on top that there should be an API layer that is very flexible and cater to different kind of applications. Use case to build and on top of that there will be applications that can build using this kind of flexible programming layer. So now I want to kind of just briefly touch on some very interesting challenges here. There are three interesting challenges we see when building code processors.
00:14:39.080 - 00:15:48.470, Speaker B: The first part is proverb scalability, the second is API flexibility and third is Design pattern. We'll go into each of them briefly. So for proverb scalability we are looking at a lot of interesting circuits, especially for this kind of onchain data related ZK coprocessing such as Merkle Inclusion, RP decoding, various kind of signature verification with non native field issues. So how to kind of improve on these? There are various kind of different directions. So the most simple one is to use basically recursive aggregation, some sort of recursive aggregation schemes. One direction is that you go with Foolproof plus full verification for the entire kind of process, but try to somehow reduce the verification cost. Ways to do that include like two chain recursion or cycle of curves and other optimizations used in other roll apps today on the Zkvm or Zkevm space.
00:15:48.470 - 00:17:09.112, Speaker B: Or you can just directly reduce the work of proof and verification directly such as scheme fry with small field and things like that. So, the reason we want to kind of talk about this is because we see there are some very interesting collaboration effort needed in this part, right? For example, we all think that a folding based approach offers very promising path towards a highly scalable approver architecture. But we also know that folding based approach are pretty far away from production. For example, it doesn't have a very strong implementation for parallelization or it doesn't have a very strong implementation to essentially kind of change the IPA scheme to KZG to put it on know these kind of things are something that is missing in the reference code basin something that we can all work together and towards a common kind of shared good. You know, since Yoki asked us to talk a little bit more about the interesting challenges. So here's one maybe we can kind of think about working on as a collective and on the API SDK flexibility side. This is where I think a lot of these things will actually be very interesting and differs.
00:17:09.112 - 00:19:10.470, Speaker B: And whether you provide developers with simple inclusion proof or more complex analytic APIs or query DSLs is something that really decides how your developer experience is for us. We have been kind of talking to a lot of different developers and partners in the past who have need today and we're starting to forming a very interesting kind of something sitting between DSL and a lightweight VM to compose different kind of pieces together. There are also discussion about whether you do indexing abstraction or not and who is in charge of the off chain query part. The onchain smart contract should we offer developer APIs with just on chain smart contract calls or mingle off chain API calls? And what is the fee model economic structures for users and the protocols integrating with this. So this part of the design question I think is a very open and interesting question for the entire Zeke Co processor space and I think there are definitely more than one right answer in this space. And finally there are a bunch of design pattern questions, that is how does developer actually use this, right? So because all the ZK Coprocessing capability and functionality and APIs are asynchronous in nature but smart contract developers are familiar with synchronized programming patterns. How do we transform that from fully synchronized programming to asynchronous message based programming model or patterns is something that we may want to have some common practices reference kind of standard on that the entire industry can reference to and we're very interested in contributing to that effort as well.
00:19:10.470 - 00:20:38.400, Speaker B: But having said that, we have been making some progress in towards that direction to change this horrible number to this. So with enough parallelization the recursion enabled, we can actually make a very large scale trade proof, trade bottom proof, very scalable and acceptable for many different kind of application use cases. So Briefs today is launched on Minat already on Ethereum and also we're working on some other blockchain consensus as well. So what you can do is previously you can do transaction proof which just approve the transaction itself, but you can relatedly also do received proof which lets you essentially decode and approve the events in the receipts. But we also very importantly also have the capability to do message bridge because we also have a lead like client implementation of this. But this initial surface of API is usable, but not that programmable per se. But we're kind of quickly working towards both a highly scalable proof architecture as we kind of previously mentioned the current test of the numbers and also pretty fundamentally changed how the API structures for developer experiences.
00:20:38.400 - 00:22:38.008, Speaker B: Some example applications that are already live today include Setter's ZK bridge which are now using previous powered ZK Lite client from Ethereum to other blockchains and of course some demo apps are demonstrating the capability to do user lifecycle management. Like you can find your largest swap on Uniswap since June 6 and claim a very interesting NFT. But of course the interesting thing here is to actually be able to prove very large scale in terms of total number of user transactions and therefore the volume to make more sense for this kind of usage based or user lifecycle management use cases. There are other interesting use cases actually go beyond just on chain analytics or on chain kind of application use cases such as privacy preserving data marketplaces where when a data vendor can commit a crypto graphic commit of data in certain kind of data quality evaluation platform and generate a ZK proof of data quality to the data buyer and therefore kind of directly deliver data without any third party intervention or third party visibility to the data itself. And you can of course use the current existing API to do social and financial connection proofs such as proof of a friendship and user gating or anti SIBO attacks where you can generate trust free proof of your past behavior or a certain user's past behavior to construct a trust free Zkdid on chain directly. Of course Zkdid itself contains off chain information. We're collaborating with several Zkdid providers to kind of make that on chain and off chain combination work really well.
00:22:38.008 - 00:23:57.328, Speaker B: And at the same time for gaming application, I actually came from a gaming background before. So the very important aspect for gaming is something called Live Ops that is your hand holding the entire user life journey from the moment they install the app to the end of lifecycle for the entire game. So these kind of things have very dynamic requirement of how you classify category and incentivize different users. So this will be perfect use case for Zeke co processing because Zeki co processing doesn't require application developers to pre build any tracking or accounting logic in their smart contract but instead have a retrospective view in the user history and do future proof, dynamic user classification or user based incentive for the user base. And of course there are interesting user acquisition value attribution that you can build with also related to user behavior in the application and games as well. Of course you can also do social recovery, wallet and many other different use cases. I guess I'm running off of time but yeah, we are also looking for early partners here.
00:23:57.328 - 00:24:15.236, Speaker B: So if you're interested in this, just DM our twitter handle at previous underscore ZK and we're pretty well connected so you probably can find us pretty easily. So yeah, with that I will give the stage back to you.
00:24:15.258 - 00:25:49.430, Speaker A: Yuki all right, thank you very much. So I'll jump straight into some questions, and I think Mo has mentioned a lot of different aspects of Zkbridge as well as, like, Storage cruise. But the first thing I kind of wanted to touch upon was a very interesting, I guess, challenge questions that you have raised about the SDK and maybe some APIs, like how users are interacting essentially with the storage pools and so forth. So I believe that from the more like web two space, the program interactions has been mostly dominated with a lot of API interactions and I think having a bunch of SDK or even to some extent DSL and Maximizing Expressivity, I personally feel it's a lot more common in the crypto space than the web two space. In general. Maybe in the web two space you have gaming languages that are specifically to developing games, et cetera, which is some scenarios that I see or maybe some languages specifically for data analytics. But in itself it's like a very big categories versus in ZK space it may be a bit more specific.
00:25:49.430 - 00:26:07.960, Speaker A: So I kind of wanted to hear from your perspective how do you see this kind of comparison between the web two sort of devtoolings or the SDK and API interactions versus the web3 would kind of shape out differently?
00:26:08.640 - 00:26:52.280, Speaker B: Yeah, I think this is a great question. I think the ultimate goal for web two and web3 are all the same. That is cater to the developer, what developer are familiar with and also cater to and try to make their life easy. Right. So essentially for web two, for example, like for Unity, for Cocos, you have different kind of gaming developer languages. The reason for that is because they're essentially kind of syntax trigger for a lot of kind of complex visualization packages and libraries. I think what we're trying to do here is somewhat similar.
00:26:52.280 - 00:28:07.788, Speaker B: Right? So like, you know, for example, if you want to kind of prove, I guess at this point, the zinc coprocessor space is still trying to find what is the right abstraction level for developer experiences. I don't claim that we have found that, first of all. Right, so we're still trying to find our guess is that there are some important elemental kind of operations such as merkle inclusion, such as proving or decoding certain RoPS. And there are certain points that we want to allow developers to customize. For example, the developer may want to customize the logic to decode certain events, but they still want to have the decoding of events in Carding Vacate because they don't want to kind of have 10,000 transaction receives approved on chain and then kind of decode each of them on chain and directly add them. So they may not want to want that. So let's say we want to build a special DSL for ROP decoding.
00:28:07.788 - 00:29:10.790, Speaker B: That would be pretty simple, right? So basically something like the user writes a Raster program or whatever the language, they want to decode the ROP and we translate that into a Za circuit. That should be pretty simple, right? And then there will also be some other elementary operations such as adding or subtracting and do statistics on a batch of numbers stuff, basically, and operations. So these things can be integrated either as API or as some sort of Dslvm construct. So for example, you can prove a lot of inclusion, merkle inclusion and stuff like that, and then have very lightweight VM just to full calculations. So the VM is super dumb. They just do number of calculations, nothing else. They're stateless it doesn't store any other kind of complex stuff.
00:29:10.790 - 00:29:25.050, Speaker B: So you can do that. The developer service there is pretty flexible, I would say. At this point we're still in the process of searching which one is the best.
00:29:25.420 - 00:29:58.900, Speaker A: Got you something. I also noticed in one of your slides was that you made a distinction between the light node and the full node. Yeah. Could you also elaborate a little bit on that in terms of like first of all, what's the implications you have being a light node versus a full node and then what kind of different trust assumptions that we are playing with by actually having a full node versus a light node.
00:29:59.480 - 00:31:06.532, Speaker B: Yeah, sure. So I think the light node full node part is about Zaki lite client or Zeki in general for blockchain consensus. So the goal here is to verify consensus of another blockchain of one blockchain on a different blockchain. So the way to do this is that you have to convince, let's say this is Ethereum and BNB. So you want to convince the BNB blockchain that the Ethereum has reached the consensus based on the Ethereum consensus rule, right? And you obviously don't want to do this because signatures on chain, which are expensive and all that stuff. So you may want to just directly do this in the ZK space. Now, the difference between light client versus full client, ZK Light clients and ZK full client is exactly the same difference of full client or like full node versus light client node.
00:31:06.532 - 00:31:53.256, Speaker B: For Ethereum blockchains for Light client, you're verifying a selective signatures. You're verifying just enough BLS signature aggregations to prove that, okay, the entire sync committee actually reached consensus on the blockchain itself. And then they will claim, okay, this is good enough, this is how like client verify things. But is it 100% foolproof? Is it like entirely fork preventable? It is not, but it is scalable. So you can build a single machine, no parallelization, no aggregation, none of that stuff, and still catch up the entire Ethereum block pretty quickly, right?
00:31:53.278 - 00:31:53.850, Speaker A: So.
00:31:55.900 - 00:32:51.260, Speaker B: For proving each of the Ethereum block, you may just do this under 10 seconds or under 5 seconds, even if you just use light client logic. But if you want to expand it to essentially full node, you're adding a lot more kind of Shard operations. This is why we are interested in Shar in our benchmarks to basically kind of approve entire full consensus of Ethereum, which is feasible, but requires parallelization, requires aggregation, requires recursion. It really depends on what level of cost you want to pay for the corresponding level of trust. We believe for most of use cases they can like client level verification or trust is good enough. But there are maybe some use cases that really require full node level kind of a decade proof.
00:32:52.080 - 00:33:56.140, Speaker A: Got you. I see. I think it is interesting because I have previously also done quite a bit of research around the ZK based phone node and also ZK like client base for a lot of the Bridging and I guess relaying the state validities and whatnot using ZK verified proof. So I think that aspect is definitely interesting to consider and also how scalable it is and whatnot is definitely a big consideration there as well. I guess. Another thing I wanted to also dive deeper into was sort of like the positioning of a ZK bridge in this way. So, as you may know, some other projects have adopted ZK bridges as sort of like an additive security on top of their existing bridges.
00:33:56.140 - 00:35:06.250, Speaker A: And I think a very good example I believe is like a layer zero where they sort of treat the polyhedral Zeke client as a sort of additive security. Now, I think the concept of additive security sort of makes sense. I believe it's probably going to prevail across multiple different bridges in the space, including likes of Wormhole and others as well. So do you foresee a lot of the ZK bridges projects to remain as an additive security on top of their existing, let's say, multisig based bridges or some sort of like a Guardian based bridges? Or do you think that this is more or less just a transition period where people are building confidence around the ZK bridges that hopefully eventually that we can get rid of all the construct around multisigs and sort of like N out of M assumptions type of honest majority assumption type of construct and then eventually move towards more trust? Minimized ZK bridge implementations completely.
00:35:08.380 - 00:36:13.692, Speaker B: Yeah, I think this is also a great question. I think there are a little bit of both. Right today, everyone is adding ZK bridge, including us, as an additive model, mainly because that the ZK side of things are very new, while the guardian validator based external validation based bridges has been running for multiple years, has been kind of scrutinized more closely. So there is definitely that element in it. But whether this is just a transition period and then this entire kind of model of guardian based invalidator based bridge is going to be gone, I don't think so, at least not in the medium term. The reason for that is because still we're basically kind of looking at the entire trade off space. So yes, ZK bridges are in general more on the trust and minimized side.
00:36:13.692 - 00:37:15.520, Speaker B: But as of today and also foreseeable future, it still incurs pretty significant cost and complex trade off nuanced trade offs to actually kind of use essentially. So it makes perfect sense for low frequency and high importance application use cases such as governance, cross chain governance such as uniswaps cars and governance definitely makes sense to use ZK based bridges. But for high frequency use cases, let's say I'm just doing a cross chain bridge for like $5 or $10, I may not want to actually incur the cost of ZK bridges. So I think from kind of an interoperability point of view, by introducing ZK bridge, we're expanding the trade off plane so we're kind of making the entire trade off more complete so that for different use cases you can fit in different kind of solutions or underlying technologies.
00:37:16.260 - 00:37:33.270, Speaker A: Got you. In an e term, what does the cost look like between the normal bridge that you have with Stellars and then the Bravest bridge, like the ZK bridge that you have with right now? What's the differences that we are talking about? Here.
00:37:36.200 - 00:38:57.730, Speaker B: So I think this is going to be a more nuanced discussion because there are going to be several different cost perspective. So the first is on chain costing. Cost for decade based bridges are definitely going to be the verification cost. But the verification cost is kind of tricky thing to discuss because you can argue that there can be aggregation of different proofs together and then aggregate into a single and then kind of you kind of ironize the cost. But if you aggregate then you essentially trade off latency for a lot of users to kind of wait for enough proof to come or enough blocks to pass. But fundamentally for ZK bridges to work you really need to kind of do this kind of a ZK verification for every single block. So you cannot skip that because if you skip a single block, especially for Ethereum based consensus, things just doesn't actually work because just like full node or like plan you kind of need to verify the entire history to make sure things are secure or substantial part of the history.
00:38:57.730 - 00:40:40.716, Speaker B: From that point of view this definitely much more expensive in terms of off chain competition cost. For off chain composition cost it's going to be order magnitude more expensive than current bridge solutions because current Bridging solutions just observing running funnels for different blockchains and kind of observing events and rich consensus which are relatively light computation itself. But the ZK proof process for light client, the full client is actually quite expensive and have a lot of weird trade offs here and there. And, you know, more importantly, if we're going beyond just Ethereum, let's say we're talking about Cosmos, which use Ed two 5119 signature scheme then if you want to verify Cosmos consensus on Ethereum, you have to deal with non data field operations, which is a nightmare to do in ZK and also kind of finally verify on chain. Then the combination cost for any Cosmos based chain underlying such as Polygon, such as BNB chain to other blockchains is going to be insanely expensive. Now, you also have some very weird, I wouldn't say weird but like very zak unfriendly consensus algorithms such as avalanche's consensus algorithm to deal with if you want to kind of achieve really full different chain consensus and all that stuff. So on the different consensus part, the development cost is also a cost that need to be considered.
00:40:40.716 - 00:41:29.420, Speaker B: So for today's Guardian or validator based bridges it's really about the VM itself. But for ZTE based bridges you really need to deal with each consensus separately even though they're running exactly the same VM. So even from kind of an off chain computation cost will be I think at least three orders magnet more expensive. And then for development cost it's even hard to count because some countless might be even invisible to actually implement in CK completely. And then for onchain verification costs, if you're doing per block verification it's going to be, I would say like one to two orders in magnetic field more expensive than the validator based approach. So overall, pretty expensive.
00:41:29.920 - 00:41:51.410, Speaker A: Gotcha. Gotcha. Thank you. Thank you very much, Mo, for today's. Great talk. And for anyone who's interested in the ZK bridges and storage proof again, please go follow Mo on Twitter and also the setter network and the previous on Twitter as well.
