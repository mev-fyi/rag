00:00:03.210 - 00:01:01.470, Speaker A: Next up we have Diego from So, a little bit about Lambda class. So, they are one of the major, I'm sure maybe the biggest venture studio that are working on a lot of both general web3 applications engineering problems, as well as targeting a lot of ZK engineering work. So, they work with a lot of ZK projects, they have worked with a lot of ZK projects, and they also write a lot of public, I guess, open source codes that are already running for, let's say, Stocknet and potentially more project in the future for a lot of the stacks for those CTA projects. So, again, please welcome Diego, and the stage is yours.
00:01:02.290 - 00:02:17.298, Speaker B: Okay. Thank you, Yuki. Very happy to be here. And today I will be discussing a little bit the security and optimization of the Fry protocol, which is at the core of Starks. And as many of you know, Starks have gained widespread acceptance among many projects to build roll ups thanks to its properties. So we wanted to discuss a little bit what is the protocol and what kind of optimizations you can do and a little bit on the security and whether it can resist or not quantum computers and see what advantages and disadvantages it offers with respect to other protocols or commitment schemes because, well, Fry can be used as a polynomial commitment scheme, but it also is a protocol by itself that allows for low degree testing. So this is just a brief outline of my presentation.
00:02:17.298 - 00:03:39.730, Speaker B: I will present the main motivation for the Fry protocol. Then we will discuss a little bit the protocol and overview. Then we'll show how to turn this protocol together with Mercury into a polynomial commitment scheme and then some security bounds characteristics and then some optimizations for the protocol. So what is just it stands for Fast Bridge, Solomon, interactive Oracle proofs of proximity. And it allows us to test whether a given function is close to a low degree polynomial. And this is useful in particular for Starks and as a polynomial commitment scheme, because we will show briefly that, for example, if the constraints hold in some program, that is to say, all the transitions in the execution trace are valid, then we can write that as the quotient of some polynomial and a polynomial that is equal to zero on the points where the constraints hold. And so if the constraints are valid, then that quotient is not a rational function, but a polynomial.
00:03:39.730 - 00:04:46.200, Speaker B: So there are some ingredients we need to make this work. We will have an evaluation domain with certain properties that will make evaluation very amenable and fast. Then we need linear codes. In particular, it uses Ritz Solomon codes and of course, Merkel trees and, well, the security of the scheme and everything will depend on the characteristics of the Hash function. So compared to other schemes, it has like less requirements. Of course, there are some people who will criticize that maybe we tend to take some optimistic bounds on the security of Pry, but as regards the security assumptions, it only requires that the hash function be collision resistant. That is a less stringent assumption than considering, for example, that the discrete log problem is hard.
00:04:46.200 - 00:06:05.710, Speaker B: So, as I was telling you, in Starks, you want to show that the constraint polynomials are enforced over the computation trace. And so what you do is you interpolate the trace to obtain the trace polynomials and then you compose them with the constraint polynomials and that generates a univaried polynomial. And then if you want to show that that polynomial evaluates to zero over some domain, you just calculate the quotient. Of course, in Starks you have many constraints, especially if you're doing like a VM. So what you do is you can see all those polynomials by thinking a random linear combination. And well, if we can show that that random linear combination is in fact a polynomial, then we know that the computation was carried out correctly. Of course, one easy way to show that we have a polynomial is that we could send all the coefficients of p zero to the verifier and say, okay, check that this is in fact a polynomial and this is consistent with the trace.
00:06:05.710 - 00:07:14.310, Speaker B: The problem is that in general, the polynomial has, for example, degree 1 million. So it means passing the verifier over a million coefficients. So that wouldn't make the proof short and easy to verify, which is something that we effectively want to be able to scale blockchains, for example. So here is where the principle comes into play. What we can do is kind of try to reduce the polynomial into a smaller one and that way we would have to send less coefficients. Of course, we would have to send some additional information for the verifier to see that we reduce the polynomial correctly, but luckily, that would require sending less information. So what we will see is that in general, the precise is of the order, the square of the logarithm of the calculation, the size of the calculation.
00:07:14.310 - 00:08:43.038, Speaker B: So FryWhat will do is kind of randomly fold the polynomial until it reaches degree zero. So very shortly the idea of how to fold a polynomial is, well, we can split it into the OD terms and the even terms. So that way we can express it as the sum of two polynomials in x squared and we can kind of randomly fold just by choosing or having the verifier, in this case choose a random beta zero and then we can define a new polynomial in terms of the variable y, which is simply x squared. The good thing is that this reduces the degree of the polynomial in half. So luckily, if we want to show that this is a polynomial, we will have to send only half the coefficients and of course, some information regarding the first polynomial. So the proverb this case starts with the polynomial and it evaluates p zero over some domain and builds a merkle tree to commit to those evaluations. Of course, in general, what we try to choose as evaluation domain are the n roots of unity.
00:08:43.038 - 00:09:43.906, Speaker B: Why? Because we can do FFT to evaluate efficiently. And also we have both. If X is in the domain, then minus x is in the domain. And so we can really evaluate things and have the Verifier check everything in an easier way. So until the degree of the polynomial is zero, what we are going to do is have the Verifier sample a random beta. Of course, when we want to render this in an interactive, we just apply via shamir. And then what we do is we split into odd and even coefficients, we fold, and of course we then commit to the evaluations over the domain tk plus one, which is given by the square of the values in the previous domain, because we choose the nth roots of unity.
00:09:43.906 - 00:11:22.006, Speaker B: When we square, we only have like half different elements, so that reduces the evaluation domain size. So luckily, every time we have to commit to less evaluation, so we have less degree and we have to commit to less evaluation. So each step is faster. So if we want to reach degree zero, then this takes log two n steps, where n is the degree of the polynomial. And then if we want the Verifier to be able to check everything, well, we can tell him like, okay, we will show this to you by having you query certain values, which is why this is an Oracle proof. And so, well, the Verifier will query some positions in the original domain and then the proverb will have to provide him with the evaluation at the point he wants and also the evaluation at minus x zero, so that he can then perform the calculations and go into the next layer. And of course, for each layer he will have to provide two values and also show that those two values belong to the merkel tree that they correspond to, so that we are not cheating or anything.
00:11:22.006 - 00:12:20.140, Speaker B: So the proof would consist in these two pairs and of course their respective authentication paths. We will see later that we can shorten this a lot by doing some clever tricks. In the case of the Verifier, of course, what he would have to do is first sample all the random challenges. Of course, if it's interactive, he already knows them. If not, he has to do fiat shamir again. And well, for each query, what he does is take the two pairs of values for p zero, check that they are in the merkel tree, and then what he can do is calculate this value l by taking this truth. And if everything is right, then that value should correspond to the evaluation of p one at x zero square.
00:12:20.140 - 00:13:34.530, Speaker B: And well, then he can take the values corresponding to p one and get the value that should correspond to p two. Check it. So what we have to do to check the proof is like checking that they belong to the Merkel tree and then doing this collinearity test. If it passes for all the layers, then of course everything was done properly. Of course, checking for just one point is not enough, because the prover could have gotten lucky, and even if the computation is not right, maybe it passes. So of course, to be sure that the prover cannot cheat, we have to repeat these queries maybe many times to be sure we will see more or less how many times we have to do this to be able to have a decent security level. And maybe we have some way of making the proverb pay a high cost if he wants to draw bad query positions.
00:13:34.530 - 00:14:50.410, Speaker B: Of course we can also turn this low degree test into a polynomial commitment scheme with Merkel trees. To that end, what we can do is if we want to convert to a polynomial and show that the polynomial evaluates to some value v at point z, of course what we can do is create the rational function p of x minus v divided by x minus d. If the evaluation is right, then of course that is a polynomial. If not, that is a rational function and it should not satisfy the low degree test. And of course then what we do is, okay, we apply the freight protocol to this function f, and that's the key idea of how to use it as a polynomial scheme. If we want to analyze the soundness error, then if we wish for a single polynomial, of course the error will depend on the size of the finite field and the rate, or if you want to see it another way, the blow up factor. Because when we evaluate, we have to add like some redundancy.
00:14:50.410 - 00:16:05.140, Speaker B: And so what we do is choose a larger domain to evaluate. And of course the more points we add to the domain, the less likely the proverb is to cheat. And then of course we have another error which will also depend on the glow up factor, and that depends on the number of queries. So we can think that each query roughly corresponds to a certain amount of bits. If we want to batch many polynomials, that is to say taking a random linear combination, then there is another error bound which is a little bit larger. But luckily, if we choose large enough fields that can be controlled in general, we will have to choose a field that is much larger than the evaluation domain and the number of bits we want to achieve. So if we want to have like 128 bits of security and our evaluation domain is two to the 20, then our field should be like two to the 148, more or less.
00:16:05.140 - 00:17:54.942, Speaker B: Of course, there are some other tricks if we want to reduce communication and instead of using n coefficients for a random linear combination, we can just have one and take the powers of that scalar as random elements and well, that increases a little bit more the error bound. If we want to batch s polynomials, then the bounds are multiplied by s, but well, luckily we can always try to reduce the error by choosing large enough fields and also taking more queries. And well, the best attacks known against phi at least now are interpolation attacks where the proverb is free to choose up to different points and try to interpolate a polynomial that tries to mimic the real polynomial and well, if he gets lucky, maybe he queries on those points and of course the low degree test will pass. That doesn't mean that there aren't any other optimal attacks, but at least now this is one of the best known. Of course there are some optimizations we can take to improve the Fry protocol. Many have to do with reducing proof size by not having the proverbs send redundant information to the verifier. For example, if we want to use the two values from the upper layer to get to the next layer, well, one of those two values can be reduced from the above layer so we don't have to pass it.
00:17:54.942 - 00:19:22.250, Speaker B: And we can use the test in the Merkel trees to see that, in fact, we arrived at the correct evaluation. Because if not, the Merkel authentication proof would not pass. Another strategy is to avoid sending many authentication paths that would have many elements in common is batch proving in Merkel trees. So instead of sending one proof for each element in one Merkel tree, we say okay, we want to query all these elements and so we just generate one proof and then evaluate it altogether. Another point is if we continue the Fry protocol until we reach degree zero, we have to build lots of smaller mercury trees and send lots of authentication paths. And maybe that is more expensive than just sending the coefficients of a polynomial that has nonzero degree. So we can say okay, we fold until we reach degree 128 and well, sending 128 values is less than all the information we would have to pass from the authentication paths of the Merkel trees and all the values and so on.
00:19:22.250 - 00:20:24.018, Speaker B: Of course another important trick is skipping layers. Instead of committing to every layer, we can commit to every second layer or every third layer. Of course that forces us to send more values from each layer, but we don't have to pass all those authentication paths for each mercury tree. So luckily if we want to do then recursive probing or something, we don't have to do as many hashes. Another important optimization is related to using smaller fields and with maybe nicer arithmetic. For example MERS and primes or seldomars and primes. Of course if we try to use fields that are that small we need to work on extension fields to reach the required security level.
00:20:24.018 - 00:21:42.750, Speaker B: So maybe we'll have to use degree two or degree theory extensions or even more in the case of the 31 bit merson prime. Of course then we have to pay like okay, costlier extension field arithmetic, but maybe that is better than working with a very large field and with only Montgomery reduction instead of some friendlier strategy. Of course there is also a performance trade off when trying to do recursion because we can use for example slower hash functions but with nice arithmetications such as poseidon. So then doing recursion is less expensive. But the problem is that the Fry protocol is slower. Typically poseidon is at least one order of magnitude slower than commonly used hash functions like Blake three or Chef three. Of course that doesn't mean that maybe we can get better hash functions or that we can have some hardware specific instructions or something to speed up poseidon.
00:21:42.750 - 00:22:54.846, Speaker B: But of course we have this trade off between speed but not nice arithmetic, that is to say costly recursion or slow hash functions, but nice arithmetic. So of course there are some cases where one would want to use maybe for the lower layers, one fast hash function and then for the upper layers then do or use a slower hash function. And of course well we can use some strategies to reduce some communication costs. Another strategy is ordering the merkel leaves so that we always have the pairs we want to say to send in adjacent leaves. And so that reduces also the proof size so that's more or less where different teams are working and maybe looking at some other opportunities. As regards the security, it depends mainly as we talked on four parameters. First is the field size.
00:22:54.846 - 00:24:10.140, Speaker B: So as long as we choose a large enough field size or an extension field then that is not the problem. In general we should aim at least for 100 and 5160 bit fields or extension fields. The other point has to do with the hash function. But as long as we choose the right hash function, the hash functions typically offer security level of N over two where N is the digest size. Of course if quantum computers were present that reduces further the security level to N over four. But then to get to the same security level we would just have to double the digest size. Well of course that would impact directly in the proof size because it would double, but it's not that it is completely broken as would happen for example with commitment schemes built on top of the discrete log problem.
00:24:10.140 - 00:25:23.298, Speaker B: Of course then the security level depends mainly on the number of queries and we can think that each query corresponds to around pbits of security. That depends mainly on the blow up factor we use. Of course there is a trade off between using a larger blow up factor which gives more bits per query, but that increases the proverb work, and simply using a smaller blow up factor which reduces prover time and memory, but that then makes the verifier have to check more queries. But luckily that is a more parallelizable task. And of course we can also put some additional cost on the proverb by introducing proof of work before sampling the positions, we can make him do a small proof of work and luckily that adds few bits of security. So we can reduce the number of queries and consequently proof size. So that's mostly it.
00:25:23.298 - 00:25:33.640, Speaker B: So thank you very much for your attention and of course special thanks to Elevant, Sasson and Starkware for introducing us to these topics. Thank you.
00:25:36.090 - 00:25:49.340, Speaker A: All right. Thank you, Diego. So, few questions on my end. Well, the first thing was something I was kind of curious was like you mentioned on the optimization side that.
00:25:51.490 - 00:25:51.806, Speaker B: The.
00:25:51.828 - 00:26:11.460, Speaker A: Smaller the field, the more efficient that you can run the fry, but also at the same time the security would be much better with the bigger field. And I was kind of wondering what's the trade off between the field size, between the optimizations and also the security of it?
00:26:11.990 - 00:26:59.842, Speaker B: The problem is, in general, the security is not given by the field. You just have to use a large enough field. So provided that you have like 100 and 5160 bits with an extension field or with an ordinary field, then you are covered. In general, what limits the security is the number of queries you use and the blow up factor. So in the expression you had like it's the maximum of these two values, typically the field size, you try to use a very small field if we want to have like fast arithmetic. But then of course you have to make sure that at least you reach the number of bits you need. Typically 100 and 5160 would be enough.
00:26:59.842 - 00:27:14.120, Speaker B: So that's why they use, for example, mini goldilocks with degree three extension or maybe Mars and prime with degree five or degree six. So in general, that is not what limits the security.
00:27:14.490 - 00:27:27.546, Speaker A: I see. So basically you're just like the security is in that sense, is that fair to say? It's going to be the minimum of the two, yes.
00:27:27.648 - 00:27:35.870, Speaker B: So in general, you just have to focus more on the number of queries, maybe on adding proof of work and the blow up factor.
00:27:38.530 - 00:28:22.270, Speaker A: I see. Actually another thing I wanted to also ask about was the block factor. So privacy, I've seen different benchmarks that are done on some stock based proof schemes and obviously what kind of block factors that the benchmark teams have set can be a bit subjective. Like maybe they follow some different standards and whatnot. But what are some of the considerations around the efficiencies and the securities around setting the blow up factors for your proof?
00:28:23.970 - 00:29:34.260, Speaker B: Well, if you use larger blow up factors then you have to sample less queries and then proof size is smaller so in many cases one would use that. Besides, you also have to choose the blowout factor according to the highest degree constraint. So for example if you have like a reconstraint two or something maybe using a blow up factor four is enough. Of course in many cases they prefer using larger blow up factor because then that reduces proof size. Of course if you are doing them recursion maybe it doesn't matter much, maybe for the last proof to use a larger glow of factor but on the other side if you have larger growth factors then prover time increases and also the memory use. So if you are a little bit more memory constraint or anything, maybe it's better to use a smaller block factor and maybe increase the number of queries for example.
00:29:35.530 - 00:30:31.158, Speaker A: I see, got you. I think you also mentioned that it would also alter the hardware requirements or I guess more on the compute load for the provers and a verifier depending on, let's say, what kind of flow factors that you set. And maybe if you set it such that the proofs can be smaller, then maybe verifier have to do a lot less work. But then at the same time proverb might have to do, I guess, more work to some extent. So you kind of have balancing between okay, should we make it cheaper to verify on chain or should we maybe reduce the hardware requirements so that the prover can do kind of less work and then just leave the more to the verifier to handle. I presume that's kind of like the trade off that we're making here, right?
00:30:31.244 - 00:30:55.534, Speaker B: Yes, but in general it has to do most on the prover side because the verifier is really quite fast. We did some benchmarks increasing the number of queries and everything and the verifier is really fast so maybe it doesn't pay that much like having more queries, less queries. To the verifier it's more or less the same, it runs in a few.
00:30:55.572 - 00:31:05.250, Speaker A: Milliseconds but wouldn't it matter from the perspective of gas cost, let's say on ethereum?
00:31:07.270 - 00:32:06.562, Speaker B: Well, I haven't checked that much but of course then maybe you would want to minimize the amount of queries that we have to check but luckily the main cost maybe there is related to calculation of maybe some hash function but then the others are like simple operations. The nice thing in fries that you are always doing operations over some finite field, in the worst case over extension of some finite field so they are less costly than for example doing elliptic curve operations. So that is I think one of the nice things about this protocol and also I think it's easier to understand has less assumptions. So I don't know, maybe it's one of the reasons why many teams have decided to go for this approach gotcha.
00:32:06.626 - 00:32:22.920, Speaker A: Gotcha. Well, thank you very much, Diego, for your time and explaining a lot of the weeds around the fries protocol and especially the consideration on the security of it as well. So thank you again, adiego, for the talk.
00:32:23.450 - 00:32:25.986, Speaker B: Thank you. Thank you. Bye.
