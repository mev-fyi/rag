00:00:00.470 - 00:01:00.300, Speaker A: You. All right, cool. So I guess I'll get first begin with opening remarks and then later on I will just briefly go through the schedule for today and then we'll have our first speaker, E, to hop on and basically start our research webinar. So to kind of kick it off, I just first introduce myself. I'm Yuki and I lead a lot of the research effort as well as some investments at Fembush Capital. And I guess I will be the host, the moderator for today. And I thank you everyone for coming and attending the Open Talk today.
00:01:00.300 - 00:02:31.878, Speaker A: I guess a bit more introductions, a little on the Fembushi side. We are a venture capital firms that were founded back in 2015 and we have started off on the ethereum ecosystem with a very strong focus on infrastructure investments. And then ever since we have gradually grown our portfolio and also our sort of team. And now we have a distributed team across the globe and also companies that we are supporting that varies across many different sectors in the space and we believe in the very strong fundamentals of the projects and the long term potentials for projects that we support as well. So that's kind of like the background on Fembucci. Now, I also wanted to talk a little bit about why I wanted to host this space. So one thing that I was very interested and I am always interested in as a researcher at Fabuchi was that I wanted to learn more about a lot of the open challenges and the research problems that a lot of the projects are facing and especially now in the sort of emerging space in zero knowledge.
00:02:31.878 - 00:04:09.430, Speaker A: There are a lot of issues that oftentimes get brought up in Telegram groups or maybe in some niche. Part of the Twitter. However, I wanted to kind of bring this more into the public light so that with the hope that we can, I guess, foster more community driven development and research, sort of like what we see already on the more sort of ethereum landscapes, but hopefully more so on the zero knowledge landscape. So that's kind of like the reason why I wanted to bring the researchers and founders from different projects and have them present different perspectives, research problems that they are facing today and have everyone be involved in that conversations going forward. So that's kind of like the intentions for Open Talk. And also this is our first time hosting the Open Talk, so there might be a bit of experiments and here and there just trying things out. But, yeah, hopefully things would go smoothly and you guys would also enjoy the conversations for today that's I guess, the intentions for the entire events now, just to kind of roughly go through the schedules for this half day webinar.
00:04:09.430 - 00:05:17.742, Speaker A: So after this opening ceremony, we will have E for Axim to start with the kickoff today and then from there onwards, we would have around like 20 minutes ish for the presentations and then ten minutes ish for the Q and A sessions. And of course the ten minutes can be a little bit flexible. Plus at the end we would also have a five minutes extra just for some buffer and also like changing of the speakers and whatnot. So that's kind of like our plan. And then we'll basically repeat that for the rest of the nine speakers. So in total we will have ten speakers for today and I believe you guys are already aware of which projects and what talks they will be giving from the agenda that we have shared previously. So hopefully you guys will have some fun times.
00:05:17.742 - 00:06:22.224, Speaker A: And again, feel free to ask questions or drop questions in the Q A sections for the webinar and I would pick it up and answer one by one during the Q A sessions. So yeah, that's the format of the Open talk. So now I guess we'll get started with the talk. Let me introduce from Axiom, he's the founder of Axiom. And just briefly on what Axiom does, they are building Zhe Coprocessor and they're essentially trying to build a Verifiable data, have developer access Verifiable data and be able to perform arbitrary complex compute on top of it as well. So ISU do you want to take it off from here? Awesome. Yeah.
00:06:22.262 - 00:07:40.020, Speaker B: Thanks for the introduction Yuki, and thanks for organizing this wonderful event. So today I wanted to talk about slightly off the beaten track topic, which is what DK can do for developer experience for on chain applications and our perspective on that at Akio. So to kick, I wanted to talk about some of the motivation we had for even starting Axiom, which is when we looked at smart contract applications, we found that they're really data starved. What that means is that smart contracts really can access a limited fraction of all data that's even on chain today. So if you look at a listing page on OpenSea and you try to identify which pieces of on chain data are actually accessible to smart contracts from that page, you'll quickly realize that it's actually just the present owner. So for this Puzzi penguin, the fact that it's owned by Zac Efron is usable on chain, but the rich transaction history and previous prices are not, although they are trustlessly available to any observer of the chain. And the reason for this is to preserve the decentralization of the chain and allow full nodes to validate transactions easily.
00:07:40.020 - 00:08:43.660, Speaker B: So this isn't just a failing of ethereum, but more of a fundamental trade off that any blockchain must make. So as a consequence, developers have a pretty painful trade off to make today. If they want to access more data trustlessly, then what they need to do is pay more and store the data in state. So this has two problems. The first set is that it obviously costs more. What that means is that every user has a bit of a negative externality but actually more concerningly it introduces a lot of complexity on the part of the developer. You have to actually couple your business critical systems, for example, maybe your core matching algorithm on chain with things that are less critical, for example, how you maybe reward users or your loyalty system and this violates some basic principles of software modularity and also introduces some security risk.
00:08:43.660 - 00:08:46.328, Speaker B: Now, your other choice as a developer.
00:08:46.424 - 00:08:48.108, Speaker A: Is basically to reduce the security of.
00:08:48.114 - 00:09:57.940, Speaker B: Your on gate application. You can use a trusted oracle to introduce data to your application, which will definitely work, but will introduce additional trust assumptions for your users. And those trust assumptions might not scale as you compose your application into others. And so, as a consequence, today developers basically have to choose between increased cost and complexity or reduced security. And so we asked ourselves how we can simplify the design of smart contract applications so that developers don't have to think about all future uses of their onchain data when writing their initial applications and our answer to this is that we need to dramatically scale data access for smart contracts. So why do we even think this is possible? Well, blockchains actually offer a different way to access data outside of the consensus that's provided by the EVM. So blockchains all have a property that the current block commits to the entire history of chain.
00:09:57.940 - 00:11:26.660, Speaker B: So for Ethereum this means that the current block commits to all past blocks and each block commits to the state transactions and receipts inside that block. By decommitting all these commitments we can access the chain history using cryptography instead of consensus. Now, how does this work? Well, to actually show that you possess a valid transaction from a block that's 1 million blocks ago, what you have to do is demonstrate first that the block header of that block is valid. To do that you have to verify a Ketchup chain of block headers from that previous block header to the current block namely you need to show that the path block has a header whose hash appears in the next block and that block has a header whose hash appears in the next block and so on. After you've proven that the path block header is valid you actually have to give a merkel pushes to try proof for the inclusion of your transaction in the previous transaction trial. And so you might notice this involves a chain of hashes of 1 million block headers which is prohibitively expensive in the EVM. So instead by using the power of ZK, what we do at Astiom is to essentially check this proof in ZK.
00:11:26.660 - 00:12:35.880, Speaker B: So we take this large computation and make a ZK snark that the merkel Patricia Tri proof was valid and also that we possess a valid chain of block headers from a past block all the way to the current block because DK allows us to compress the verification of this to a succinct computation. This allows us to do this large scale computation in a way which is verifiable on Chain. Furthermore, it allows us to compose such computations for downstream usage. So we patched this up at Akium into something we're calling a ZK coprocessor for Ethereum. What that means is that smart contracts can query into Axiom to perform two operations first, to read Historic on Chain data and second, perform verified compute on that data. Once we come to a result, we post it on Chain and verify a zero knowledge proof that it's valid. After that, a downstream application can use that result with the same security guarantee as data that it reads from Ethereum itself.
00:12:35.880 - 00:13:54.782, Speaker B: And our philosophy at Axiom is that we should always have security that's cryptographically equivalent to Ethereum without imposing any additional trust assumptions on our users. So what's the advantage of this type of design? So first, by reading from Historic ontoing data, we allow Astium to actually trustlessly interoperate with existing applications. What that means is that when you're designing a smart contract application, you actually don't need to think about what are the later usages of the data that you're catching in your application. So today applications have to really think carefully about what aspects and side effects of their transactions they might need to reference later. If you don't store that in state, then it's sort of gone forever. For the on chain usage with Ascim, you can exactly read that data and later make use of it even if you didn't anticipate that usage at the time of deployment. After reading that Historic data, we allow you to compute without being limited by the blockchain VM.
00:13:54.782 - 00:15:11.560, Speaker B: Because we're verifying compute in ZK, this doesn't need to respect gas limits or the specific computations that are cheap in the EVM. So let me give a rundown of how zko processing with Axiom works today and how we think it's actually improving the developer experience. So what we're offering is trustless reads to Historic block headers accounts and contract storage variables on mainnet today and transactions and receipts on Testnet. And on top of that, we're allowing to compute the accustomed decay circuits. So I'll go through a few examples of how this might change your actual experience as a developer. So today if you're only using Celibity, then to track the user's participation in governance, what you have to do is that on every single time a user votes, you have to actually track the cumulative number of times they participated in governance. Then later, if you want to reward users for the number of times they voted, you have to read that cumulative participation count from your local contract storage.
00:15:11.560 - 00:16:22.760, Speaker B: The disadvantage of this is that you have to pay additional gas to actually update the participation count every single time your users vote on Chain. This obviously increases cost to voting and this can decrease voter participation in onchain governance which is already challenged due to the gas cost. By using ZK to read the history, we can change the design of this application. Instead of actually tracking the cumulative vote counts, all we have to do is trustlessly read the historic vote events for each user. Then, when a user wants to claim a reward for historic voting, all they have to do is read these events and compute the number of times they actually voted. After proving that to a smart contract, the contract can reward the user or give them special rights depending on whatever the developer wants to do after the fact. In a second scenario, suppose you wanted to make uniswap swap fees dependent on the volume that a user has traded in the last week.
00:16:22.760 - 00:17:38.160, Speaker B: Today, to make that possible, you'd have to track for each user their cumulative volume on every trade. What that means is that every uniswap pool would have to maintain a list for every user of the total volume and keep that updated every single time it makes a swap. What that means is that every swap is going to be more expensive for each user essentially because the volume updates might be comparably expensive to the actual swapping itself. Obviously that's unacceptable, but as a result so if you were to make such a system you'd be able to read the cumulative user volume and then compute fees dynamically. If we use ZK to implement such a system we can invert the architecture. This would allow us to make no changes to the current uniswap system, but instead trustlessly read historic trade events for each user. By processing those events, we can compute the total volume that a user has traded and add up that volume to determine the fee rebate that a user is entitled to.
00:17:38.160 - 00:19:11.790, Speaker B: Once every user does this, they can submit a proof of their historic activity to a contract and trustlessly obtain whatever volume rebate that the system decides they are entitled to. So as you can see in this case, typically in smart contracts today, you actually have to bake into your core application logic of swapping the idea that you might care about volumes in the future. But by using ZK we can remove the need for developers to actually do that instead later on, once we decide to develop this volume based fee rebate system, we can go back and retrofit it onto our existing application without making security critical code changes to the core swapping logic. And just to give one more example that I'm pretty excited about, if you look at Airdrops today, if you want to make Airdrops fully trustless, namely not dependent on the actions of a centralized team or token distributor, what you actually need to do is tally all potentially useful user actions on chain. What that means is that when you deploy your smart contract you need to know the space of actions that you might later want to reward or incentivize and track for each user the extent to which that user actually took those actions. That might be pretty expensive. And it might also be hard as a developer for you to anticipate.
00:19:11.790 - 00:20:33.030, Speaker B: Typically, when you're shipping the first versions of your application, you're more concerned with your core business logic and with just making sure everything is secure and probably less thinking about the space of future incentivization you might want to enable. As a consequence, if you later want to compute rewards on chain, the only things you're able to use are the information that's being tracked in the state of your contact. You see this today in the form that most rewards are simply done by staking some type of fungible, ERC, 20 token. Often that doesn't actually match the incentives that application developers want to set, but it's frankly the only information that they have available to make these choices. Using ZK, we offer smart contract developers much more flexibility. Instead of specifying the space of data that they want to take into account, the developers can trustlessly read all historic user activity on their application. And after the fact, when they want to design an incentive or rewards program, they can simply compute rewards based on arbitrary custom criteria over the entire history of activity in their application.
00:20:33.030 - 00:21:32.020, Speaker B: So we think this is a much more flexible approach and also enables developers to have a much more modular design to their application architecture. So let me close by just talking about where we think ZK coprocessing is going. So we're already offering trustless reads to all on chain historic data. And we think the next step is to offer view function simulation by running a ZKE EVM proof on top of the storage reads that we're already providing together that will create a ZK archive node or indexer, depending on your terminology, on top of the data returned by this system. We think it makes sense to offer users arbitrary computation via ZK native V app. So we'll have more to say in this direction in the next few months. So watch out for our upcoming releases.
00:21:32.020 - 00:22:06.910, Speaker B: So that's all I have for today. We're Axiom. We have a ZK co processor for Ethereum Live on mainnet and you can check out our docs and demos. I wanted to mention at the end that we're running a new program called the Axiom ZK Intensive, which is targeted at helping smart contract developers learn to develop ZK enabled applications. So applications just opened for this program and are closing September 18 and you can apply at the link below. Thanks for the time and happy to take any questions.
00:22:08.720 - 00:22:47.550, Speaker A: Thank you. Thank you very much. So I guess to kind of just kick it off and on the questions. One thing I was kind of curious was like for the arbitrary compute right now you have specific circuits for maybe specific functions, maybe some hashing functions, some arithmetic operations. And whatnot, what is the sort of like the thinking behind shifting towards more like a ZK native VM as an approach to enable those compute instead of, let's say what you're doing right now is just like building more specific circuit. What is the consideration there?
00:22:48.640 - 00:23:24.020, Speaker B: Yeah, we're always actually driven by the developer experience. I actually don't necessarily think that SDK native VM is always easier for developers, but we do think in some cases it's going to be a little easier. So our point of view is that actually we think about what are the developer frictions in deploying actual functions that they might want to use on Chain today, I actually think it's probably not a VM. It's more just having all the primitives that they're used to being implemented in ZK and also being able to easily express those primitives.
00:23:24.860 - 00:23:41.260, Speaker A: I see. So in that case you could see a scenario where with enough specialized circuits that perhaps the ZK native VM wouldn't be as useful in that kind of scenario.
00:23:42.160 - 00:23:58.368, Speaker B: Yeah, my feeling is the main criterion is actually not necessarily whether you have a VM or not, but more how fast you can implement and also how fast you can have your circuit audited and be really sure that what's being proven on Chain actually matches the application.
00:23:58.454 - 00:23:59.920, Speaker A: Logic you're interested in.
00:24:00.070 - 00:24:08.500, Speaker B: So VM definitely helps for that, but I think having a very clear sort of circuit language and framework can also achieve very similar goal.
00:24:09.480 - 00:24:29.800, Speaker A: Gotcha and also one more thing that I also wanted to ask about was the Zkevm proof that you mentioned towards, I guess, the second to last slides. Could you elaborate a little bit more on what do you exactly mean by zkvm proofs?
00:24:30.640 - 00:25:28.270, Speaker B: Yeah, so right now when you use ZK to read data from your application, you kind of are introspecting about your application state, but what you have to do is operate on the raw data underlying your application. So in some cases, this is very straightforward. If you have a mapping, for example, for balances, that's a mapping from addresses to numerical values and that can be read very directly. But actually for some contracts in the wild, the balance computation or other computations are implemented via function view functions that do some pretty simple computation on top of the storage access. And we think the ultimate developer experience will be to exactly run the solidity or EVM computation that's deployed on chain on a historic state that unfortunately is much more complex. It requires you to actually simulate the full Zkbm on a historic state.
00:25:29.120 - 00:26:02.200, Speaker A: Gotcha and then I guess one question that I saw was asking, will this benefit stateless Ethereum? So I think the question is probably more so asking about whether this is applied to a stateless chains or if it is just specifically Ethereum itself, or if you have any plan around working outside of Ethereum or if it's just constrained within Ethereum frameworks.
00:26:02.700 - 00:26:50.900, Speaker B: Yeah, we actually spoke a bit with the ethereum's stateless client team. And I think what we're doing is somehow parallel to the goal of statelessness. But some of the constraints are a little different. So you can view the data we provide as we're using ZK to give a cryptographic proof of the validity of that data. And in a stateless client you would use probably not ZK to give a proof of every data access. I think the differences in having a stateless client versus what we're doing in ASEAN is the stateless client would probably access a lot less data per block, but the latency requirements are much harsher. So in a stateless client you have to be able to generate the witnesses in under one block.
00:26:50.900 - 00:27:02.540, Speaker B: So you actually have a time budget of maybe seconds. Whereas at Axiom we're enabling contracts to do much larger scale computations and okay. Does not take just seconds.
00:27:04.320 - 00:27:54.940, Speaker A: Gotcha. Gotcha. And then also, I guess one thing that I was kind of curious because I think when I go through the decks you also talk about arbitrarily complex compute on top of verifiable data. Now, obviously that is a pretty broad statement, but at the same time I believe that's also what it's intended for. But when I was thinking about arbitrarily large compute, what sort of the bound are we looking at when we say arbitrary large? Are we constrained by, let's say, the amount of provers that we have to support this kind of compute? Or what are the constraints here that we're talking about that sort of access the bound for those kind of like, arbitrary large compute?
00:27:55.520 - 00:28:25.560, Speaker B: Yeah, generally we find actually the bound is just how much latency people are willing to tolerate. Obviously, if you're doing more compute, proving its validity will take longer. I actually think that typically the number of provers isn't necessarily a bottleneck just because the function of proving isn't fully parallel. So you can definitely put some good number of provers to use. But before you hit your parallelism bound, you're probably going to hit your latency bound.
00:28:26.460 - 00:28:27.370, Speaker A: Got you.
00:28:27.740 - 00:28:58.160, Speaker B: There's not really a conceptual bottleneck as we expect proof systems to keep getting faster. We think one of the main advantages of this type of architecture is that the compute isn't going to be gas limited. Like fundamentally, if you run compute on a roll up or any blockchain VM, you're limited by things like the Sequencer and DA and this type of thing. Whereas in this sort of architecture those aren't really factors because you're not operating in this shared compute context.
00:28:59.300 - 00:29:53.190, Speaker A: Gotcha. And then one more thing that I wanted to also discuss briefly was about the I guess, as you mentioned, a lot of the latency aspect. As you have said, latency is a major concern for users who are using Axiom and essentially kind of trying to see what kind of latency is acceptable for certain applications. And what kind of latency may not be the case because I presume that some application might be more latency sensitive while others maybe not so much. So I guess in that spectrum, do you foresee that Axioms to be initially catering for more like, let's say latency sensitive ones, lesser latency sensitive ones? And how do you think that the latency game will kind of change over time?
00:29:54.360 - 00:30:19.640, Speaker B: Yeah, in general our feeling is that as proof systems get better, latencies will go down across the board. I think that's one important distinction is that any sort of ZK based data access or computation will always be asynchronous. So the advantage you get from running something on chain is that you really get access to synchronous changes in the state in the previous transaction.
00:30:19.720 - 00:30:21.324, Speaker A: And that's something that no ZK based.
00:30:21.362 - 00:31:32.310, Speaker B: System will be able to do simply because you have to generate a proof relative to some change state. And obviously you don't know the change state of an included transaction when you're submitting your transaction. Now in terms of applications today, we actually found that things aren't necessarily so latency sensitive and I think the reason for that is that if you think about non blockchain computation, the main synchronous latency sensitive computations which exist in the world are actually in financial exchanges like at Nasdaq or CME. Whereas if you think about almost all other computation, it's actually asynchronous. Like if you hit a web server in the microservice model, it's going to make a ton of async calls to other services and sort of react on callbacks to them. And we think that Axiom is fundamentally building that sort of system. But on Chain now of course users don't want to waste, let's say an hour for results and so we're tackling more use cases where the user has already accomplished something and they're getting a reward or requesting some sort of message that already takes a significant amount of time in alternative as well.
00:31:33.400 - 00:32:09.010, Speaker A: Got you. That makes sense. Another aspect that I wanted to also touch upon was that the proof scheme. So I think today you might have not discussed too much about a lot of the proof scheme aspects, but I think would love to love for you to share a little bit more about what kind of proof scheme that you are using right now that you will be using in the future. Or perhaps different ones or the same ones or whatnot if you could elaborate on that as well?
00:32:09.700 - 00:32:41.850, Speaker B: Yeah, so today we're using halo two with a KZP back end. So that's the general clonkish arithmetization on the front end and KZP commitments on the back end for the polynomial commitment scheme. Yeah. So we're looking into a variety of more modern proof systems including things like folding large lookup schemes like CQ or Lasso and also Fry with small fields. So hope to have more to share about that in the next few months.
00:32:42.220 - 00:33:16.996, Speaker A: Gotcha, looking forward to that as well. So I guess on my end that's it on the questions and stuff. But again, thank you E for coming to the talk and sharing about the Axiom. And at the same time, for all the attendees here, please feel free to follow E on Twitter and also check Axiom stock and maybe play around with their stuff as well. So again, Ethan, thank you very much for coming today. Thank you.
00:33:17.178 - 00:33:18.820, Speaker B: Yeah, thanks for inviting.
00:33:20.200 - 00:34:27.310, Speaker A: So our next speaker will be Mo Dawn from Seller Bridge. So I guess a little background on this. So Mo has obviously built Seller Bridge and now it's up and running. It's one of the major bridge that are connecting a lot of chains in the space. In the recent times, they have also announced about Brevis, which is sort of like their ZK branch where they attempt to do a lot of ZK storage proof. They have even built ZK Lite clients as well. So that's kind of like the reason why I also invited Mo from setter here for yeah, he will be soon joining us for the panel and share more about some of the recent works that they have been doing on the CK side of things.
00:34:27.310 - 00:35:05.634, Speaker A: And yeah, he should be joining soon, so we'll just wait for a oh, apparently Mo's zoom crashed. There you go. We have him again. Hello, Mo. Hey.
00:35:05.672 - 00:35:11.140, Speaker B: OK, sorry, just crashed while you were introducing me, but yes, thanks.
00:35:15.130 - 00:35:21.066, Speaker A: I just saw. I was like, oh, good job. Anyway, well, I mean, the stage is yours, so feel free to take it over.
00:35:21.248 - 00:36:30.634, Speaker B: All right, well, first of all, thank you Yuki, for hosting this great session. I think this is very important as a venue for us to talk about and exchange some ideas, especially on the cutting edge of Ziki research. And today I think I would just use this slide that I have for the new initiative as Yuki talked about. That is our Ziki Co processor initiative that we have been kind of focusing on recently to talk about some of the interesting challenges that we have seen on Saturday side while building this smart coprocessor for Blockchain. So, first of all, I think there are going to be I heard the entire talk from Yi who gave an excellent talk on this topic. So there might be some overlapping materials here, but might be coming from a little bit different perspective. So I've been in this space for a while and it's great to see that the entire space definitely seeing some explosive growth, especially with the DeFi as the real product market fit moment for blockchain.
00:36:30.682 - 00:36:30.846, Speaker C: Right?
00:36:30.868 - 00:36:55.202, Speaker B: So it really showed that the trustlessness of our trustfreeness of Blockchain really made a difference in terms of changing weeks of in person meeting and negotiation in traditional finance to zero meeting and one click of mouse. And during the process of this entire DeFi growth and product marketing process, we also accumulated tons of data on the blockchain today that are very valuable.
00:36:55.266 - 00:36:55.526, Speaker A: You can.
00:36:55.548 - 00:37:00.934, Speaker B: Extract valuable information from this. Oh, sorry, is there any issue about this?
00:37:00.972 - 00:37:07.738, Speaker A: Okay, let me show yeah, you can.
00:37:07.824 - 00:38:24.450, Speaker B: Extract a lot of valuable information from you can and should be able to build a lot more exciting applications on blockchain, like using onchain data to do data driven DeFi, user lifecycle management, protocol governance automatically risk management by just looking at the onchain data alone. And in the AI machine learning space, you should be able to do privacy preserving data exchanges moderated by the blockchain and AI agent coordination. On metaverse gaming, you should be able to just like traditional gaming do behavioral driven user engagement, a new kind of user acquisition funnels and these kind of exciting data rich and computation heavy applications. However, none of these today is seen in the blockchain space. The reason for that is because the blockchain today is still fundamentally a slow web three CPU. This is really kind of a by design because blockchain give us high trust of furnace. But at the same time, the other side of the coin being a distributed system means that it is actually low performance, high cost, and insanely expensive comparing to just our traditional computing architecture.
00:38:24.450 - 00:39:14.462, Speaker B: There are a lot of improvement on this front for sure, and they're very valuable. But they're still blockchains and rely on distributed consensus. Even for layer twos, they're still posting on a distributed system at data availability or other layers. And therefore this kind of distributed system will always be bad at data rich and very heavy computations. A manifestation of this, as Yi also mentioned in the previous talk, is that on chain smart contract data today can only access data in a very limited form, that it can only access data on the same chain, not on every other chains. They can only access data in the current block with no view in the history, even though the data is just sitting there in the blockchain's full node and archive nodes. And they can only access data via fixed and predefined view interfaces.
00:39:14.462 - 00:40:39.322, Speaker B: If they're not defined beforehand, there's no way you can actually look into another smart contract and get data from them. So there's no arbitrary access and random access capability today. An example of this is that let's say you want to build a user loyalty or VIP trader program on Uniswap, right? So let's say trader traded the 10,000 trades $30 million volume in the last 30 days and you want to give it VIP status by having 50% trading fee rebate in the next month. You see this kind of a trading fee rebate in literally every centralized exchanges. But why are you not seeing this in uniswap or any decentralized exchanges? Because first of all, it is extremely expensive to build if you just want to build it natively in a smart contract, you basically need to track and record and do accounting of every trade for every traders in the smart contract record. Okay, what is the cumulative trading volume for this guy? What is cumulative trading volume for that guy? And in the end of day, you are incurring significantly higher cost for every trader while just maybe serving 5% of these VIP traders. So even though that the VIP trader does bring a lot of volume, the significantly incurred cost create a very worse off social welfare.
00:40:39.322 - 00:41:57.970, Speaker B: Therefore, no decentralized exchanges are implementing this today. Some of you might think, wait, you don't actually need to reimplement this indirectly. In smart contract you can just do retrospective on chain analysis, right? So how do you do this today is that you can actually remember all the state rules of the blockchain by using a smart contract. Let's say this step is not that expensive and it can be shared by across the cost can be shared by a lot of different applications. But the problem with that is if you want to kind of prove or generate a trustpray proof for a VIP trader, like the previous example mentioned, you basically need to calculate the 10,000 merkel proof of transaction inclusion on chain directly and then not only that, you need to decode that RLP, right? So you're kind of going into this transaction and look at the event, okay, this guy traded USDC to wrap east and you need to decode that and also directly on chain. And then finally you need to compute and aggregate all of these decoder trading volume into one and then kind of prove this to blockchain. So how expensive is this? Well, for this example, we actually calculate the cost on ethereum.
00:41:57.970 - 00:43:05.530, Speaker B: You need to basically take 14 hours of block time if you're kind of not taking every block time, but just let's say in a more realistic sense and $20,000 in transaction fee to just calculate that a trader reached the VIP status. This is insane and not actually acceptable. Fortunately, in computer history, we met this kind of problem before, right? So CPU was on extremely fast growth since 1970 on Moore's Law. But even though the computation capacity is exploding, it is still fundamentally bad at certain things. It's bad at low point arithmetics, it's bad at large degree parallel computations, it basically is bad at for anything that relate to viralization. So this is why even in the early 80s people start to introduce a CPU coprocessor and later it get called to a GPU to basically handle these kind of cases. That is fundamentally bad for CPU architecture.
00:43:05.530 - 00:44:46.258, Speaker B: And this is how we transformed from this black and white word to the word we're seeing today, that is through the CPU core processor. So history rhymes here and we think we will see similar solutions built for web three board blockchain which act as the CPU for web three for today, also needs its own coprocessor. So the concept of coprocessor is pretty simple, right? So you have a blockchain and there are some computation tasks that is not suitable for blockchain to do you just offload this and then the coprocessor after coprocessing will just return the result and the blockchain can just focus on transactional computation that is a synchronized in nature and you have to record the entire computation on chain directly because you care about the process you not only just care about the end result but you also care about the process of the computation and for the coprocessor it can just do all the data rate and heavy computation and there's no need to record the process computation, but just the result and it is of course asynchronous in nature. But the important thing here is that by introducing the coprocessor, we should not break the trust free nature of the blockchains. Therefore, simple centralized server acting as a coprocessor definitely doesn't work. And this is why we need a ZK based coprocessor like Bravius that can run arbitrary complex computation on any data and generate easy to verify testation for the computation result directly on chain. So I will just quickly go through the intuition behind using ZKP.
00:44:46.258 - 00:45:51.470, Speaker B: For the coprocessing problem here. So let's say our goal here is to convince the blockchain x plus y equals Z. And let's say this computation need 100 compute units by itself, right? So there are really two ways we can do this. The first way that we just directly compute it on the blockchain. But note that the blockchain's unit computation cost and the unit computation time are extremely much higher than the off chain computation environment and therefore we'll get a pretty bad result. But the other way we can do this chain but at the same time also generate a ZK proof based on the computation itself. So note that the process of generating the ZK proof actually consumes a lot more computation unit than the computation cells.
00:45:51.470 - 00:46:28.478, Speaker B: But because the unit cost in the off chain environment is so much lower, cost and time all get improved significantly comparing to directly computed on chain. Because verifying proof on the blockchain also is much cheaper than just doing the computation itself. So even though that some of the numbers are kind of exemplary and sort of a made up but the order of magnitude is what we're shooting for, for building the coprocessor world. So it seems that there are some internet problems.
00:46:28.644 - 00:46:29.360, Speaker A: Okay.
00:46:32.930 - 00:47:46.870, Speaker B: All right, no problem. So now instead of doing all of these calculations on chain directly, we can do this off chain and generate proof for that. And not only that, we can also bring other blockchains data to Ethereum or ethereum's blockchain data to any other chain by relaying the entire consensus process and run the consensus computation off chain and verify the consensus process on a different blockchain using ZK proof which is essentially ZK lite or full client. It depends on the computation power you want to consume for the provers. So this is what the previous architecture is about. So the lower layer is a brief core that is highly optimized ZK cryptoxy prover engine that can take in any kind of data and generate attestable results. And then on top of that there should be an API layer that is very flexible and cater to different kind of applications use case to build and on top of that there will be applications that can build using this kind of flexible programming layer.
00:47:46.870 - 00:48:44.970, Speaker B: So now I want to kind of just briefly touch on some very interesting challenges here. There are three interesting challenges we see when building code processors. The first part is Proverbial scalability, the second is API flexibility and third is Design Pattern. We'll go into each of them briefly. So for proverbial scalability we are looking at a lot of interesting circuits, especially for this kind of onchain data related ZK coprocessing such as Merkle Inclusion, RP decoding, various kind of signature verification with non native field issues. So how to kind of improve on these? There are various kind of different directions. So the most simple one is to use basically recursive aggregation, some sort of recursive aggregation schemes.
00:48:44.970 - 00:49:57.310, Speaker B: One direction is that you go with Foolproof plus full verification for the entire kind of process, but try to somehow reduce the verification cost. Ways to do that include like two chain recursion or cycle of curves and other optimizations used in other roll apps today on the Zkvm or Zkevm space. Or you can just directly reduce the work of proof and verification directly such as folding based scheme fry with small field and things like that. So the reason we want to kind of talk about this is because we see there are some very interesting collaboration effort needed in this part. For example, we all think that a folding based approach offers very promising path towards a highly scalable approver architecture. But we also know that folding based approach are pretty far away from production. For example, it doesn't have a very strong implementation for parallelization or it doesn't have a very strong implementation to essentially kind of change the IPA scheme to KZG to put it on chain.
00:49:57.310 - 00:51:11.820, Speaker B: These kind of things are something that is missing in the reference code basement, something that we can all work together and towards a common kind of a shared good. You know, since Yoki asked us to talk a little bit more about the interesting challenges. So here's one maybe we can kind of think about working on as a collective and on the API SDK flexibility side. This is where I think a lot of things will actually be very interesting and differs. And whether you provide developers with simple inclusion proof or more complex analytic APIs or query DSLs is something that really decides how your developer experience is for us. We have been kind of talking to a lot of different developers and partners in the past who have need today and we're starting to forming a very interesting kind of something sitting between DSL and a lightweight VM to compose different kind of pieces together. There are also discussion about whether you do indexing abstraction or not and who is in charge of the off chain query part.
00:51:11.820 - 00:53:20.398, Speaker B: The onchain smart contract should we offer developer APIs with just on chain smart contract calls or mingle off chain API calls and what is the fee model economic structures for users and the protocols integrating with this? So this part of the design question I think is a very open and interesting question for the entire Zeke Coprocessor space and I think there are definitely more than one right answer in this space. And finally there are a bunch of design pattern questions that is how does developer actually use this right? So because all the ZK Coprocessing capability and functionality and APIs are asynchronous in nature but smart contract developers are familiar with synchronized programming patterns. How do we transform that from fully synchronized programming to asynchronous message based programming model or patterns is something that we may want to have some common practices reference kind of standard on that the entire industry can reference to and we're very interested in contributing to that effort as well. But having said that, we have been making some progress in towards that direction to change this horrible number to this. So with enough parallelization recursion enabled, we can actually make a very large scale trade proof trade bottom proof, very scalable and acceptable for many different kind of application use cases. So Briefs today is launched on Minat, already on Ethereum and also we're working on some other blockchains consensus as well. So what you can do with Briefs, you can do transaction proof which just approve the transaction itself, but you can relatedly also do received proof which lets you essentially decode and approve the events in the receipts.
00:53:20.398 - 00:54:41.130, Speaker B: But we also very importantly also have the capability to do message bridge because we also have a lead like client implementation of this. But this initial surface of API is useful, but not that programmable per se. But we're kind of quickly working towards both a highly scalable proof architecture as we kind of previously mentioned the current test of the numbers and also pretty fundamentally changed how the API structures for developer experiences. Some example applications that are already live today include Setter's Zkbridge which are now using previous powered ZK lite client from Ethereum to other blockchains. And of course some demo apps are demonstrating the capability to do user lifecycle management. Like you can find your largest swap on Uniswap since June 6 and claim a very interesting NFT. But of course the interesting thing here is to actually be able to prove very large scale in terms of like a total number of user transactions and therefore the volume to make more sense for this kind of usage based or user lifecycle management use cases.
00:54:41.130 - 00:56:26.426, Speaker B: There are other interesting use cases actually go beyond just Onching analytics or on chain kind of application use cases such as privacy preserving data marketplaces where when a data vendor can commit a cryptographic commitment of data in certain kind of data quality evaluation platform and generate a ZK proof of data quality to the data buyer and therefore kind of directly deliver data without any third party intervention or third party visibility to the data itself. And you can of course use the current existing API to do social and financial connection proofs such as proof of a know and user gating or NTC attacks where you can generate trust free proof of your past behavior or a certain user's past behavior to construct a trust free Zkdid on chain directly. Of course Zakdid itself contains off chain information. We're collaborating with several Zakd providers to kind of make that on chain and off chain combination work really well. And at the same time for gaming application, I actually came from a gaming background before. So the very important aspect for gaming is something called Live Ops that is your hand holding the entire user life journey from the moment they install the app to the end of lifecycle for the entire game. So these kind of things have very dynamic requirement of how you classify category and incentivize different users.
00:56:26.426 - 00:57:33.346, Speaker B: So this will be perfect use case for Zeke co processing because Ziki Coprocessing doesn't require application developers to pre build any tracking or accounting logic in their smart contract but instead have a retrospective view in the user history and do future proof, dynamic user classification or user based incentive for the user base. And of course there are interesting user acquisition value attribution that you can build with also related to user behavior in the application and games as well. Of course you can also do social recovery work and many other different use cases. I guess I'm running off of time but yeah, we are also looking for early partners here. So if you're interested in this, just DM our twitter handle at underscore ZK and we're pretty well connected so you probably can find us pretty easily. So yeah, with that I will give the stage back to you.
00:57:33.368 - 00:59:07.540, Speaker A: Yuki all right, thank you very much. So I'll jump straight into some questions and I think Mo has mentioned a lot of different aspects of Zkbridge as well as like storage cruise. But the first thing I kind of wanted to touch upon was a very interesting, I guess challenge questions that you have raised about the SDK and maybe some APIs like how users are interacting essentially with the storage pools and so forth. So I believe that from the more like web two space, the program interactions has been mostly dominated with a lot of API interactions. And I think having a bunch of SDK, or even to some extent DSL and Maximizing Expressivity, I personally feel it's a lot more common in the crypto space than the web two space in general. Maybe in the web two space you have gaming languages that are specifically to developing games, et cetera, which is some scenarios that I see, or maybe some languages specifically for data analytics. But in itself it's like a very big categories versus in ZK space it may be a bit more specific.
00:59:07.540 - 00:59:26.070, Speaker A: So I kind of wanted to hear from your perspective, how do you see this kind of comparison between the web two sort of devtoolings or the SDK API interactions versus the web three would kind of shape out differently?
00:59:26.730 - 01:00:30.362, Speaker B: Yeah, I think this is a great question. I think the ultimate goal for web two and web three are all the same. That is cater to the developer, what developer are familiar with and also cater to and try to make their life easy, right? So essentially for web two, for example, for Unity, for Cocos, you have different kind of gaming developer languages. The reason for that is because they're essentially kind of syntax trigger for a lot of kind of complex viralization packages and libraries. I think what we're trying to do here is somewhat similar. For example, if you want to kind of proof, I guess at this point the Zinc Coprocessor space is still trying to find what is the right abstraction level for developer experiences. I don't claim that we have found that, first of all.
01:00:30.362 - 01:02:07.610, Speaker B: So we're still trying to find our guess is that there are some important elemental kind of operations such as merkle inclusion, such as proving or decoding certain RoPS. And there are certain points that we want to allow developers to customize. For example, the developer may want to customize the logic to decode certain events, but they still want to have the decoding of events in tarding Vacate because they don't want to kind of have 10,000 transaction receives approved on chain and then kind of decode each of them on chain and directly add them. So they may not want to want that. So let's say we want to build a special DSL for ROP decoding. That would be pretty simple, right? So basically something like the user writes a Rasa program or whatever the language, they want to decode the ROP and we translate that into a circuit. That should be pretty simple, right? And then there will also be some other elementary operations such as adding or subtracting and do statistics on a batch of numbers stuff basically, and operations so these things can be integrated either as API or as some sort of Dslvm construct.
01:02:07.610 - 01:02:43.160, Speaker B: So for example, you can prove a lot of inclusion, merkle inclusion and stuff like that, and then have very lightweight VM just to full calculations. So the VM is super dumb. They just do number of calculations, nothing else. They're stateless it doesn't store any other kind of complex stuff. So you can do that. The developer service there is pretty flexible, I would say. At this point we're still in the process of searching which one is the best.
01:02:43.530 - 01:03:17.070, Speaker A: Got you something. I also noticed in one of your slides was that you made a distinction between the light node and a full node. Yeah. Could you also elaborate a little bit on that in terms of like first of all, what's the implications you have being a light node versus a full node and then what kind of different trust assumptions that we are playing with by actually having a full node versus a light node?
01:03:17.670 - 01:04:09.314, Speaker B: Yeah, sure. So I think the light node full node part is about Zaki lite client or Zeki in general for blockchain consensus. So the goal here is to verify consensus of another blockchain of one blockchain on a different blockchain. So the way to do this is that you have to convince, let's say this is Ethereum and BNB. So you want to convince the BNB blockchain that the Ethereum has reached the consensus based on the Ethereum consensus rule. Right? You obviously don't want to do this because verify a bunch of signatures on chain, which are expensive and all that stuff. So you may want to just directly do this in the ZK space.
01:04:09.314 - 01:05:24.838, Speaker B: Now, the difference between light client versus full client, ZK lite clients and ZK full clients is exactly the same difference of full client or like full node client node. For Ethereum blockchains for light client, you're verifying a selective signatures. You're verifying just enough BLS signature aggregations to prove that, okay, the entire sync committee actually reached consensus on the blockchain itself. And then they will claim, okay, this is good enough, this is how like client verify things. But is it 100% foolproof? Is it like entirely fork preventable? It is not, but it is scalable. So you can build a single machine, no parallelization, no aggregation, none of that stuff, and still catch up the entire Ethereum block pretty quickly. So for proving each of the Ethereum block, you may just do this under 10 seconds or under 5 seconds, even if you just use light client logic.
01:05:24.838 - 01:06:09.350, Speaker B: But if you want to expand it to essentially full node, you're adding a lot more kind of Shard operations. This is why we are interested in sharding our benchmarks to basically kind of approve an entire full consensus of Ethereum, which is feasible, but requires parallelization, requires aggregation, requires recursion. It really depends on what level of cost you want to pay for the corresponding level of trust. We believe for most of use cases they cannot client level verification or trust is good enough. But there may be some use cases that really require full node level kind of a decade proof.
01:06:10.170 - 01:07:25.570, Speaker A: Got you. I see. I think it is interesting because I have previously also done quite a bit of research around the ZK based phone node and also ZK like client base for a lot of the bridging and I guess relaying the state validities and whatnot using ZK verified proof. So I think that aspect is definitely interesting to consider and also how scalable it is and whatnot is definitely a big consideration there as well. I guess another thing I wanted to also dive deeper into was sort of like the positioning of ZK bridge in this way. So as you may know, some other projects have adopted ZK bridges as sort of like an additive security on top of their existing bridges. And I think a very good example I believe is layer zero where they sort of treat the polyhedral Zeke client as a sort of additive security.
01:07:25.570 - 01:08:24.420, Speaker A: Now I think the concept of additive security sort of makes sense. I believe it's probably going to prevail across multiple different bridges in the space, including likes of Wormhole and others as well. So do you foresee a lot of the ZK bridges projects to remain as an additive security on top of their existing, let's say, multi SIG based bridges or some sort of guardian based bridges? Or do you think that this is more or less just a transition period where people are building confidence around the ZK bridges that hopefully eventually that we can get rid of all the construct around multistigs and sort of like N out of M assumptions type of honest majority assumption type of constructor and then eventually move towards more trust? Minimized ZK bridge implementations completely.
01:08:26.470 - 01:09:31.802, Speaker B: Yeah, I think this is also a great question. I think there are a little bit of both. Right today, everyone is adding ZK bridge, including us, as an additive model, mainly because that the ZK side of things are very new, while the guardian validator based external validation based bridges has been running for multiple years, has been kind of scrutinized more closely. So there is definitely that element in it. But whether this is just a transition period and then this entire kind of model of guardian based and validator based bridge is going to be gone, I don't think so, at least not in the medium term. The reason for that is because still we're basically kind of looking at the entire trade off space. So yes, ZK bridges are in general more on the trust and minimized side.
01:09:31.802 - 01:10:33.630, Speaker B: But as of today and also foreseeable future, it still incurs pretty significant cost and complex trade off nuanced trade offs to actually kind of use essentially. So it makes perfect sense for low frequency and high importance application use cases such as governance, crosschain governance such as uniswap cars and governance definitely makes sense to use ZK based bridges. But for high frequency use cases, let's say I'm just doing a crosschain bridge for like $5 or $10, I may not want to actually incur the cost of ZK bridges. So I think from kind of an interoperability point of view, by introducing ZK bridge, we're expanding the trade off plane so we're kind of making the entire trade off more complete so that for different use cases you can fit in different kind of solutions or underlying technologies.
01:10:34.370 - 01:10:51.380, Speaker A: Got you in an e term, what does the cost look like between the normal bridge that you have with Stellars and then the Bravest bridge like the ZK bridge that you have with right now? What's the differences that we are talking about here?
01:10:54.310 - 01:12:15.840, Speaker B: So I think this is going to be a more nuanced discussion because there are going to be several different cost perspective. So the first is Onching costing. Cost for ticket based bridges are definitely going to be the verification cost. But the verification cost is kind of tricky thing to discuss because you can argue that there can be aggregation of different proofs together and then aggregate into a single and then you kind of amortize the cost. But if you aggregate, then you essentially trade off latency for a lot of users to kind of wait for enough proof to come or enough blocks to pass. But fundamentally for ZK bridges to work, you really need to kind of do this kind of a ZK verification for every single block. So you cannot skip that because if you skip a single block, especially for Ethereum based consensus, things just doesn't actually work because just like full node or like plan, you kind of need to verify the entire history to make sure things are secure or substantial part of the history.
01:12:15.840 - 01:13:58.826, Speaker B: From that point of view, this definitely much more expensive in terms of off chain competition cost. For option competition cost, it's going to be orders magnitude more expensive than current bridge solutions because current Bridging solutions, just observing running funnels for different blockchains and kind of observing events and rich consensus which are relatively light computation itself. But the ZK proof process for light client, the full client is actually quite expensive and have a lot of weird trade offs here and there. And more importantly, if we're going beyond just Ethereum, let's say we're talking about Cosmos which use EV two five 5119 signature scheme, then if you want to verify Cosmos consensus on Ethereum, you have to deal with non data field operations, which is a nightmare to do in ZK. And also kind of finally verify on chain then the combination cost for any cost more space chain underlying such as Polygon, such as BNB chain to other blockchains is going to be insanely expensive. Now, you also have some very weird, I wouldn't say weird, but like very zaki unfriendly consensus algorithms such as Avalanche's consensus algorithm to deal with if you want to kind of achieve really full different chain consensus and all that stuff. So on the different consensus part, the development cost is also a cost that need to be considered.
01:13:58.826 - 01:14:47.550, Speaker B: So for today's Guardian or validator based bridges, it's really about the VM itself. But for ZTE based bridges you really need to deal with each consensus separately even though they're running exactly the same VM. So even from kind of an off chain computation cost will be, I think at. Least three orders magnitude more expensive. And then for development costs, it's even hard to count because some complexes might be even invisible to actually implement in CK completely. And then for onchain verification costs, if you're doing per block verification, it's going to be, I would say, like one to two orders in magnitude more expensive than the metadata based approach. So overall, pretty expensive.
01:14:48.050 - 01:15:16.200, Speaker A: Gotcha. Gotcha. Thank you. Thank you very much, Mo, for today's great talk. And for anyone who's interested in the ZK bridges and storage proof, again, please go follow Mo on Twitter and also the Setter network and the previous on Twitter as well. So, again, thank you, Mo, for the great talk. Thank you.
01:15:16.200 - 01:16:17.514, Speaker A: We have our next guest here. We have Brandon from One Labs and just a little background on One Labs. So they have incubated Mina Protocol, which is the Succinct bridge that we all know, hopefully. And then One Labs have also developed a lot of primitive in the ZKS space, including likes of Snarky, JS and so forth. So they are definitely one of the powerholes, also building a lot of the proof schemes and innovating on top of that in the space. So please welcome Brandon. And we'll have the stage for yours, so please take it over from here.
01:16:17.712 - 01:16:21.930, Speaker D: Thank you. Okay, let's see if I can share my screen successfully.
01:16:24.210 - 01:16:27.360, Speaker A: Here. Okay.
01:16:28.770 - 01:16:32.238, Speaker D: Do you see my mouse? Great.
01:16:32.404 - 01:16:34.350, Speaker A: Yes. Great.
01:16:34.420 - 01:17:01.474, Speaker D: I'm going to start. Okay, so hi. Yeah, thank you. Thank you for the introduction. I'm the CTO and founding engineer of One labs. And as we heard, we build what was called Snarky JS until yesterday. We just renamed to One JS and also incubated the Mina Protocol and continued to work on it and other ZK tools.
01:17:01.474 - 01:18:08.346, Speaker D: And we have a proof system called Kimchi and a recursive layer called Pickles and lots of various things in the ZK space. For the purposes of this talk, I guess I should say the title is ZK DSL Usability Challenges. So we are going to talk about those things. And so the context under which we're talking is within a Zkdsl, such as on JS. So what is this thing? It is a tool for building private applications. So it's both a tool that you can use to write circuits directly against the Kimchi proof system and with recursive layer. And it's a tool upon which you can build client side ZK applications that use the Mina protocol layer as a sort of state coordination layer and manipulate the blockchain.
01:18:08.346 - 01:19:10.178, Speaker D: Okay, so it's a tool, it's also a Zkdsl on top of ZK Snarks. And when building it, when designing it, as we continue to iterate on it, there's lots of interesting usability challenges that arise. So I'm going to talk about that today. Okay, so a ZK snark A few years ago, this was a very important slide, or there was a series of slides where you have to explain to people what ZK snarks were these days at least, this room probably has a good understanding. But just in a sentence, Azksnarc allows us to argue that we know a thing to a counterparty without that counterparty learning about the thing. And they can verify that we really knew the thing very efficiently in space and time. Okay, so what is this thing?
01:19:10.264 - 01:19:10.900, Speaker A: Right.
01:19:11.910 - 01:20:21.100, Speaker D: The statements that we're arguing, I like to use this particular sort of mental model. I mean, in different contexts we use different mental models. But this is one such way to reason about these statements. So there exists some data, some private, some public, such that a bunch of predicates hold on the data. That's sort of one way to think about it. If we think at runtime, how are these statements represented in the kinds of proof systems that most folks are building on top of today? It boils down to representing that as a series of constraints on top of some kind of constraint system like R One CS or some Plonkish system. And the fact that I just showed you two such ways to represent things, and there's a lot of different ways this creates a question as we're designing a DSL, at what level or levels should we be targeting and what are the trade offs? So I guess here's one way to sort of lay it out.
01:20:21.100 - 01:21:06.082, Speaker D: You could just directly manipulate constraints against the lowest level proof system. And sort of one small jump from that is basically what Arc works and Halo two, the kinds of interfaces that those systems provide. And some people build not just frameworks for others to build applications, but actually the applications themselves. On this layer it's great. You're close to the metal. You can poke at things and do things and there's nothing kind of in your way. If we go a little bit higher, this is where I would put Circom.
01:21:06.082 - 01:22:36.850, Speaker D: So Circom again, it's fairly low level in the sense that you're really looking at constraints all the time, but you can express them in a more concise way. You can package things up nicely and sort of there's language affordances for doing things with ZK circuits effectively, for describing your constraints. If we go another layer up, this is where one JS sits and where other tools like Noir sit, where we're starting to get something that kind of feels familiar to software developers who write software for Turing machines. So there's functions, there's types. If we provide a bijection between field elements, which is sort of the native primitive of these systems, and some complex structure, then we can just reason about the more interesting structure that maps our domain and the system will automatically convert it to the field elements for us and a bunch of other nice affordances. But fundamentally we're still controlling the actual thing that gets turned into the Snark. Whereas if you go sort of the next level up, you're instead building a ZK VM in one of these other layers.
01:22:36.850 - 01:24:02.960, Speaker D: And then the software that folks write, the applications that people write get run on the VM, so you're so far away from the actual runtime. And it's great because in that world, the DSL developer, the language developer, has so much control over the kinds of things that the developers can do, or that the developers, the kinds of interfaces that are exposed to developers, but in contrast, developers have less power to go and poke things at the underlying internals. So this is kind of the tension I kind of got ahead of myself. But as I was saying, the more indirect, the more flexibility the DSL developer has and the less power that the end user has. But it's easier for end users to jump in and get started and keep a lot of context in their head. And on the other side, the deeper you are, the more power that the user has, but the more complicated and there's sort of more complexity that has to be floating around in developers heads and you need different skills. So just to highlight this, if we look at recursion recursive proofs, this is a diagram taken from the Halo Two book.
01:24:02.960 - 01:25:42.798, Speaker D: There's great documentation on how to create recursive proofs with Halo Two. And if you have the right skills and you spend enough time, you can do it. And building such a system at this layer gives you a lot of flexibility on the kinds of around performance throughput latency, like the sort of desired well, anyway, you can tune a lot of things well. If you think about it though, for developers or development team that wants to focus on building products that users can use to solve a problem that's sort of more directly, I don't know, a social application or a DeFi application or whatever, maybe that team doesn't want to have to think about all these things. So at the complete other side of the scale, true recursion at the Zkvm layer, it's totally unexplored, actually, as far as I know. So I'm not just talking about folding instructions, but actually recursively verifying ZK proofs of other Zkvms within yourself. I think the way that this would have to work is it would involve two different proof systems, at least, at least two, because the trade offs that you need to get Zkvms to work are different from the trade offs you need to get recursion to work.
01:25:42.798 - 01:26:22.010, Speaker D: And anyway, there's a lot of infrastructure to build and then there would be a lot of compute required to make everything fit together. But if someone were to spend time building such a system, it would be interesting, I guess. So there's kind of an open, interesting system. But anyway, let's look at something that actually exists at the one JS level. This is a snippet of JS. Recursive proofs are represented by recursive functions. And that's I think, a good abstraction.
01:26:22.010 - 01:27:13.710, Speaker D: So the power of recursive proofs is given to developers. So developers can build blockchains that incrementally grow forever. That's what mina is. You can create a tree of proofs in parallel or you can kind of use a tree of proofs to incrementally update some fact as time moves forward or you could make a multiplayer game with different players creating proofs and recursively verifying each other even though they don't trust each other. There's sort of lots of interesting things that you can do, but only if you expose this tool to people. There's this tension. I guess the downside here is we've tuned recursion in a specific way and that isn't as exposed.
01:27:13.710 - 01:27:46.786, Speaker D: You'd have to dig really deep to change those defaults. But anyway so whatever, I'm belaboring point so anyway, this is where O One JS is in that space but there's another sort of interesting open challenge. This is the theme of this event like talking about challenges, there's obviously a lot of benefits to Zkvms oh yeah, go ahead. You have a question or was that a missed?
01:27:46.818 - 01:27:51.480, Speaker A: Oh, sorry, I was going to thumbs up. Okay, great.
01:27:57.150 - 01:29:44.990, Speaker D: Zkvms are really useful, especially when they target runtimes that traditional software can compile down to and actually we have at O One labs we've created a MIPS Zkbm. I guess it's in between a prototype and something that's real at this point and it's being further developed as part of an Opstack RFP. We want to take that system when it's ready and integrate it with on JS in some way and it's an interesting problem, like how do you make these things talk to each other seamlessly? So anyway, I'll just throw that into the ether that's sort of one axis of design space that we've talked about and we can think about another one, the DSL structure or I don't know, I struggled with the word for this, but what is the class of language that you are going to pick? And I'll sort of break that down more clearly. So if you're making a Zkvm, you can choose to target a real machine, like I said and then the surface language that developers interact with is actually like a programming language that people are familiar with down to the sort of details of how it runs. So we're doing that with ZK. MIPS risk zero is doing it with RISC Five. A bunch of teams are experimenting with ZK WebAssembly but if you're somewhere lower in that stack that I talked about earlier, then you'll need a new language of some kind, or domain specific language.
01:29:44.990 - 01:30:47.926, Speaker D: So you can break down DSLs into two classes external DSLs, which are ones that are a new language that has its own syntax and semantics and runtime, or an embedded DSL which is one that lives inside of a host language and inherits the features of that host. So I'll just give a quick example HTML is an external DSL. It's its own language for describing the content of web pages and it can be used for other things. An embedded DSL, in contrast, for example, is Jquery. So maybe I'm showing my age by using this as an example. But Jquery, it used to be a very popular embedded DSL for manipulating web UIs and it's embedded inside of JavaScript. Okay, so we understand now the difference between external and embedded DSLs.
01:30:47.926 - 01:31:31.390, Speaker D: Now, I have a very strong opinion about this, and I've had this opinion for a very long time, way before I was working in this space or thinking about a ZK DSL. And I'm just sorry, but I'm going to rant about it. It's really fun to build new programming languages and I love it. I love learning about programming languages and I love studying programming language theory and I love studying how programming language theory applies to engineering. It's fun. It's great. So as a language author, I'd prefer to build a standalone external DSL.
01:31:31.390 - 01:32:24.878, Speaker D: But even I, who's not a normal, not an application anyway, I have this background that is more abnormal, whatever. Even with this background, I struggle to imagine as a developer trying to build an application when I would prefer an external DSL unless it had decades and decades of personal engineering, years of effort. It's crazy. I don't know. If you're a ZK DSL author and you are designing a new system, you're immediately putting yourself years behind by choosing to build one of these external DSLs. The language itself is limited in scope by design. That's what the domain specific part means.
01:32:24.878 - 01:33:13.998, Speaker D: And so if you have a standalone DSL, even if you did a perfect job, the user would be limited in sort of what they can do. And in practice, because it takes so many years to build a system that's really nice and easy to use, you're just going to be behind. And so even things that you want to work, they won't quite work. And the tools around the language need to be rebuilt from scratch. Editor, support, package management, build, debug tools, all these things. Now, yeah, to some extent you need to build these things in both cases, but at least you can bootstrap off of the host. And that's people who like using new programming languages, application developers, or even people who like programming languages, who are putting on their hats of trying to build an application and product.
01:33:13.998 - 01:33:54.342, Speaker D: They don't want to learn a new language, especially if they're trying to build and ship a product, and they definitely don't want to be stuck with some broken tooling if it can be avoided. And then here's the thing, what is even the benefit? What do you get if statements are slightly cleaner? It's insane. It's insane. It blows my mind. So this is true always. It's an opinion I have, but it's really objective, it's a fact, but it's true for everything. And Zkdsl is a DSL, so therefore it's true for Zkdsl.
01:33:54.342 - 01:34:08.850, Speaker D: So I don't know. It's not really a choice. Like you should embed it. It's really nonsensical to not do that. I know there's lots of folks who haven't done that, but I really strongly disagree with you. So we can argue about it later.
01:34:09.000 - 01:34:09.700, Speaker A: Okay.
01:34:12.630 - 01:35:28.490, Speaker D: We want to embed our domain specific language. Where do we embed it? I guess the question to ask here is who are you targeting? Who do you want your users to be? What do you want those developers to do with your tools? So if we want to build a Zkdsl for application developers to build applications for users, and if you want to reach the most application developers, you need to target web developers. And if you're targeting web developers, then you need to embed inside TypeScript or JavaScript. And not only that, if you need your developers, not just the developers to use those tools, but the developers who are using your tools in order for them to build products that users want to use, then you need to have your runtime support web browsers. You could be using ZK for performance or Succinctness or something. But if you're using the privacy part, right, if you have a private application, then at least part of the computation needs to run client side. Otherwise you don't have privacy.
01:35:28.490 - 01:35:54.850, Speaker D: You're pushing information somewhere. And so that part has to run on the user's machine. And for desktop users, people don't want to download things anymore, they just want to use web browsers. So it has to work in a web browser, maybe on mobile. There's an argument. We can talk about it later if it matters. But yeah, it's like web web all the way.
01:35:54.850 - 01:36:44.290, Speaker D: It's the thing that makes sense if you're targeting if you want to build a tool that is for application developers to build applications for end users. Okay, so now I swear there's going to be some open challenges, but you had to work your way up. Okay, so I have no concept of time. I'm okay in time. Okay, I just looked at the clock. Now, within your language context, assuming you're somewhere on the stack above direct constraints, you have to choose where you abstract, what you abstract, how you abstract, when you abstract. And there's a thousand examples of this.
01:36:44.290 - 01:36:52.694, Speaker D: I'm just going to pick a few quickly because obviously there's not enough time to really go super deep. So I'm going to jump in.
01:36:52.732 - 01:36:53.222, Speaker A: Okay.
01:36:53.356 - 01:38:11.486, Speaker D: One category of choice that you can make is around nomenclature. So one interesting change that we made in the last few months was moving from circuit, which describes at Runtime or the kind of mathematical abstraction that is underlying the system that we're describing. We changed from circuit to Provable because Provable a Provable block of logic. It describes that the logic is to be proven. So what this does is there is a trade off for existing developers who are comfortable with ZK and who are learning about ZK. Provable communicates less information than circuit but for people who are new to the tool or new to ZK, provable sort of describes the logic in a more approachable way, I think. And there's always detailed documentation that people could dig into to understand that in the current sort of back end this means building a circuit and building up.
01:38:11.508 - 01:38:12.960, Speaker A: Constraints in a constraint system.
01:38:14.210 - 01:38:22.340, Speaker D: That's one example. I'm just going to quickly talk about this one.
01:38:24.070 - 01:38:24.820, Speaker A: Okay?
01:38:26.150 - 01:40:11.080, Speaker D: At the lowest level there's a distinction between a constraint variable and a constant when you're building these CK circuits. And that distinction is important, but it's confusing, I guess in the general sense there's this idea like you can make something more magical if you hide it and more magical things are easier for folks to onboard to a tool. But then as those people and as others become experts, it becomes something that is frustrating and annoying for people. Actually, this is an example. So in this case, what we've done is for example, for field elements there's one field representation from the user interface, the developer interface that both describes constants and variables and the system tries to figure out what you want automatically and actually that is causing problems for people. This is an example of something that might be too magical and it's something that we're thinking about revisiting and maybe removing some magic. So there's kind of I guess this open challenge of how much do you make something magic because the more magical well anyway, I just said and then sort of another category and this is the last example is this action reducer system that we to I'll do a quick diversion to explain this.
01:40:11.080 - 01:41:02.310, Speaker D: So there's this thing called the Elm architecture which it's part of the Elm programming language and it's from a while ago, but it's a mechanism for representing a state machine and a system of concurrent interrupt updates to that system. So like button presses, network events are coming in and all these things. So it's useful for user applications and actually react. The react ecosystem adopted this through first redux and then through the use reducer hooks. I don't know. It's a great system for dealing with state complexity in UIs. What's interesting is if it's in react then millions of people know how to use it.
01:41:02.310 - 01:42:01.670, Speaker D: Because when I Googled how many react developers there were, it said 29 million on Google. So millions of people are familiar with this. Okay, even if you have something that feels like a different problem. So for are imagine you're using Mina or some other blockchain system as a sort of global state layer. It's a state machine and you want to poke that state. And on Mina protocol you build ZK applications that run client side to move the system forward in some interesting way to run your application. But that means what can happen is two users can interact either with the same smart contract or different smart contracts, but touch the same state in such a way that essentially you have a race condition.
01:42:01.670 - 01:43:03.230, Speaker D: There's a concurrency issue and it's kind of similar to if there's button presses on a web page and networking events coming in at the same time. And so you can resolve these concurrency issues by decoupling. I didn't explain this earlier, but essentially the solution and what Bell Architecture does and Redux and all these things, you dispatch actions and you can do that concurrently. So these two concurrent systems sort of declare what they want to do and then later some system resolves those actions all at once against the state machine and then you can do that sort of one at a time. So once you make that realization, it means you can implement something that looks like use Reducer. And that means that the millions of React developers, the millions of people who have used React, as soon as they touch this, they need to solve this problem in their applications. They know how to use this API.
01:43:03.230 - 01:43:41.180, Speaker D: So this is something that I'm really happy about, and usually it doesn't quite work as this perfectly, but the idea is it's good to design APIs as close as possible to ones that developers already know, the developers who you're trying to target, the ones that they already know. And a lot of times you have to move things around and shape things to make that happen. But yeah, I guess those are the kinds of axes of usability that I thought about when I was creating this talk. Anyway, thank you.
01:43:42.270 - 01:44:05.742, Speaker A: Thank you, Brandon. That was a very interesting talk. And I think you have some hot takes and interesting takes here and there. So that was great. And I actually do have quite a few questions I wanted to also go through. So yeah, let's dive into some questions. Well, first of all, I'm going to just pick it off around some audiences who dropped some questions here.
01:44:05.742 - 01:44:28.470, Speaker A: So one of the questions that I have here is that do you see specific usability challenges that apply more for web two devs coming to ZK for the first time versus web three devs coming to ZK for the first time versus devs who are already familiar with ZK, like those usability challenges for those audience, for those different devs?
01:44:28.810 - 01:46:05.154, Speaker D: Yeah, I think the audience is something that you have to be cognizant of, and anytime you make any decision you're prioritizing, well, it's likely that certain audiences will have different desires and skills and expertise. And so you're making trade offs. I guess if we think through each of those categories for web two developers and web three developers, the fact that the system is embedded in TypeScript is a real help to make things better. The APIs like the one I just showed for Actions and Reducers making it feel like React, then for developers who are already familiar with ZK, you want to make sure that there's ways to discover how the system actually works so that they can map it to a mental model that they're familiar with. And I'll just give one example the way that one JS is structured. Everything is built up sort of transparently on top of this extremely low level interface of describing constraints. We have some library code that's in Rust and OCaml that's compiled to JavaScript and TypeScript, but there's also a lot in TypeScript, and we're moving more and more of it to TypeScript.
01:46:05.154 - 01:46:34.880, Speaker D: But you can actually go in and inspect how things are implemented and see it and see that, oh, this if expression is actually doing a boolean constraint of the boolean times the thing, plus one, minus the boolean times the thing. And the ability to go deep lets you optimize things when you need to, when the sort of auto generated versions of things are too slow. Yeah, that's how I think about it.
01:46:36.150 - 01:47:12.250, Speaker A: Also, one thing I was kind of interested to touch upon was the fact that when you mentioned about the current Zkvm does not have a recursion. And that actually did make me think that when I was thinking about continuation, you could argue that continuation is not a recursion, it's just a splitting of the workload. So yeah, it is true that Zkbm wouldn't have well, so far doesn't have recursion. But do you think that in the future, in what way do you think that recursion would exist for those Zkbms?
01:47:13.730 - 01:49:00.282, Speaker D: That's something that one JS will get to at some point, maybe another maybe another project sort of tries and experiments before we get there. But yeah, I think ultimately at the developer interface, you want to like the ideal design, I think is still like recursive functions. Recursive functions mapping to recursive proofs, even in a Zkbm context, you should have some notion at your software level of a function. And then if there's some way that's not too different from a function call that you can represent sort of a recursive proof request or recursive verification request, I think that is a very natural interface. And then there needs to be facilities for serializing these things and sending them over the wire, because that's when you unlock the interesting abilities of the system. If you think about what is the difference? And I'll tell you my answer, but the difference between the kind of recursion, the Nova style folding or continuations, or this kind of recursion where we're doing a bunch of things altogether with recursive proofs, you can do that. So that part of it is solved.
01:49:00.282 - 01:49:10.820, Speaker D: The ability to sort of run extra work, I guess. But at least in Nova, because I'm sort of more familiar with that.
01:49:13.590 - 01:49:13.954, Speaker A: The.
01:49:13.992 - 01:50:02.926, Speaker D: Prover has to have all of those chunks of work all at once to create the proof. So what that means is only one trusted party can do all of the work. And that immediately it removes a bunch of use cases, a bunch of kinds of applications that people can build. You can't build Mina protocol. So the way mina works is there's a proof that represents the blockchain, that proves the whole history of the blockchain, and different people, different parties consume that proof and extend it depending on who wins, whoever gets elected to make the next block. So you couldn't do that with that kind of folding style Persian.
01:50:02.958 - 01:50:03.780, Speaker A: I see.
01:50:06.870 - 01:51:01.430, Speaker D: There'S this kind of interesting space where our team is going to be helping other teams build these kinds of things more coming soon and we're going to build some more examples. But you can build these multiplayer applications where games and applications but I'll just use a game, for example, where each player takes a turn and makes a proof and they pass it to their neighbor and then they make the proof. And you can do this kind of state machine with multiple parties where you don't trust each other and then settle on chain at the end, for example. That kind of thing is also impossible. Anyway, I could keep going, but yeah, so you lose some power. I'm trying to motivate. It is interesting to explore this combination.
01:51:01.770 - 01:52:07.798, Speaker A: Yeah, it is definitely very interesting. Yeah. I mean, on one hand, you're sort of like providing a proof that is maybe a bit more heavy on the client side, but also enables new ways to interact with proofs in a sense that, as you mentioned, some games could be passing around proofs and then recursively proofs that over and over times across different untrusted entities. Right, but also at the same time, I think you raised a very interesting point around how a lot of the voting schemes are still very much rely on a centralized provers for the instance of generating many instances of combined witnesses over and over again, as you have. And that actually does raise also another interesting question of whether this witness generations and also the instance, I guess the actual voting processes itself could be potentially even distributed across different untrusted parties. And obviously that one is still very much I don't even know if that's.
01:52:07.814 - 01:52:44.370, Speaker D: Possible, but my understanding the protocol is incompatible. Like the Nova protocol, the Nova family of protocols, it's incompatible with this, with having untrusted witness. A new something new would have to be invented. But the thing is, you can mix these by mixing proof systems together. And that's something I'm really excited about. And I think we're going to see more. You're going to see more from O one labs.
01:52:44.370 - 01:52:58.934, Speaker D: But I think that you'll see more of this over time because you can have your proof system with Nova style folding and it's really good at this kind of computation crunching.
01:52:58.982 - 01:52:59.338, Speaker A: Right?
01:52:59.424 - 01:53:28.934, Speaker D: And then if you can recursively verify a foreign proof system that's good at recursive proofs in that system, then you can just use both and vice versa. So doing these foreign proof system verifications, that step is usually quite expensive. But I. Think there's a lot of use cases where you do a bunch of things over in this world, you do a bunch of things over in this world and you can kind of combine them together.
01:53:29.052 - 01:54:16.226, Speaker A: Right. So basically trying to compose the proofs together for different use cases. Got you. I guess one last thing I wanted to touch upon. We're running a bit short on time, but I remember you mentioned about sort of changing the terms from circuit to provable, and to me that intuitively kind of sounded more like basically you have something that is very low level that's maybe interacting with a bunch of different arithmetic gates, let's say. But now you're kind of abstracting that part away and say, okay, this is what I want to prove, and this is all you need to kind of put down in the code and I don't have to worry about all the details around the gates and whatnot. I presume that's kind of like, I.
01:54:16.248 - 01:55:20.778, Speaker D: Regret not having a code sample here that would have made more sense. But yeah, in one JS, you can go all the way down to the metal and manipulate constraints, but typically when you're looking at a chunk of logic, it's abstracted so much that you're looking at some piece of code that, I don't know, computes consensus logic for amina block, for example. And it looks like that the type of that computation is still a circuit or it was a circuit, but it's so far away from those details. I guess if you trace every function call, eventually you'll find constraints, but yeah, instead sort of thinking it as, oh, here's some logic that is provable, I can compose it with other provable pieces of logic and then call prove and I get back a yeah, that's kind of the idea.
01:55:20.944 - 01:56:01.478, Speaker A: Gotcha. So, yeah. Thank you very much, Brandon, for the amazing talk and a lot of the, I guess, hot takes you that the audience here would also have some thoughts and some takeaways and potentially new innovations out of the talk from what Brandon has given today. And also make sure to follow Brandon on Twitter and also One Labs looking forward to seeing all of the interesting ZK DSL and embedded DSL that will be coming out of their announcements in due time as well.
01:56:01.644 - 01:56:11.290, Speaker D: And you can download it and play with it and build applications. Build applications sort of in isolation in like, Web Two world or deploy them on Mina Protocol.
01:56:11.790 - 01:56:12.442, Speaker A: Exactly.
01:56:12.576 - 01:56:13.514, Speaker D: Thank you so much.
01:56:13.632 - 01:56:55.910, Speaker A: Thank you very much, Brandon. So next up, we have eno Kenti from Aztec protocol. And just kind of like briefly on Aztec, it's a privacy preserving L two, and within Aztec it's an end to end privacy preserved, meaning that you will have programmable and also like token level privacies for your, I guess, on chain fractions. So in Oknti, I guess feel free to take over and the stage is yours. Yeah.
01:56:55.980 - 01:56:56.600, Speaker E: Hi.
01:57:11.810 - 01:57:13.120, Speaker F: Can you see the.
01:57:15.890 - 01:57:16.350, Speaker A: Great.
01:57:16.420 - 01:57:59.014, Speaker F: So yeah, today I'll be talking about security consideration for ZK systems. First I'd like to tell a bit about myself. So right now I'm working as an applied cryptographer. That's the club. My focus is mainly on the proving system, on the underlying components of the proving system, and on writing the lowest level circuit, for example, writing such important primitives as unsigned integers, emulated fields, stuff like that. So I'm very close to the actual back end of the proving systems and all the complexity that has to do with that. In the past I worked as a reverse engineer and a digital forensic expert.
01:57:59.014 - 01:59:11.330, Speaker F: So I had to deal with security quite a lot at a very low level. And I have seen how attacks on organizations and applications can unfold. So I have a lot of experience with analyzing what has happened and what you could do to prevent that. Some extra parts of my biography are I am responsible for the cryptographic part of the CTF CTF competition which takes place every year and once was a phone qualifier. I found various cryptographic binary and one CPU bug in the past and I'm the winner of the first decay app. Just to understand what my background is with the security and with the security of protocols and with the security of cryptographic systems especially. This talk is aimed first and foremost at projects that use ZK and that use proven systems at a very low level.
01:59:11.330 - 02:00:42.606, Speaker F: So it is not aimed at projects that try to build off of such stuff as, for example, Circum, where you have a very small, maybe web application that encapsulates inside a small smart contract or a small Verifier with Tesco. So this is aimed at projects that either build their own stuff from the ground up or maybe they're using very low level stuff such as artworks and building up on top of that. So in that case, you'll probably be composing your own proven system. So you have a lot of moving parts, you'll be writing your own primitives. So there will be huge opportunity for security issues and at the same time you won't be able to share the experience and you won't be able to share the security area and the security coverage with other projects that are using the same tool, like for example, projects that use Circum. So the Template library is regularly being tested by some interested auditors. So to talk a bit about app stack and what we're trying to do to understand what kind of project we are.
02:00:42.606 - 02:01:09.720, Speaker F: So abstec is private product. Our Quarantine is composable private and public smart contracts that finally settle on Ethereum. So the idea is you have fully private functions and fully private part of the smart contract. You have interaction with public functions where you have updated all public state.
02:01:11.470 - 02:01:11.786, Speaker G: Where.
02:01:11.808 - 02:02:18.054, Speaker F: You can do operations on public data so that you don't have to work with subsections, notifiers outdate shared variables and Ethereum is used as the last layer. So we are an L two. This obviously brings a lot of problems for us because we have to be very efficient if we want to implement an operational and viable composable smart contract system, fully private smart contract system on layers. Because for us, for example, we operate under the notion of full privacy here. So we have to use the notion of zero knowledge. We can't just use the zero knowledge primitives for their compression functionality. We also, at least in the private part of the blockchain, aim to make all the functions fully private.
02:02:18.054 - 02:03:24.450, Speaker F: So nobody is supposed to know who executed this or their function. What was the result of that? You only know what's the public function in scenario to talk a bit more about what our project and what typical projects like this consists of. So obviously the most important part, especially to me is the ZK core. In our case it is a very complex piece of code which has a lot of various ZK technologies inside. I mean, some of the technologies that we created ourselves are plonk. Our researchers, especially Zach, absolutely love weird names for Pruning systems. So we use a lot of different technologies at the same time.
02:03:24.450 - 02:04:38.602, Speaker F: We try to compose them in such a way that we gain the most efficiency from each primitive. So we are one of the few projects that actually have to have this high degree of ZK composability. So apart from the ZK core itself, we need a sequence obviously for creating proofs to settle on ethereum. We also have nodes that need to store various data and we need some sort of web app so that users can interact with the system, so that at least they can maybe download and go and download some information about the system, just find out what we are. Most projects have a website and some sort of web app too. So this is quite typical for ZK contract. We can't really talk about ZK security without talking about what kind of vulnerabilities are typical in the zero dodge world.
02:04:38.602 - 02:05:40.234, Speaker F: And I think one of the most convenient ways to explain what kind of stuff happens in the DK security world is to look at it as a huge, just enormous underground system. So imagine that your goal is to go in in one place and go out there. Daddy is nobody should know how you moved between the lines, at which station you got off, where you made a connection. Daddy is you just need to get from point A to point B. So if we look at it ZK that way, there are three core issues that can happen with ZK systems. The first are thumbs issues. This means that you can manipulate the system to your advantage in a way in which it was not supposed to be manipulated.
02:05:40.234 - 02:06:23.582, Speaker F: So for example, you can jump the turn style or maybe you can go into the driver's cabin in the train and redirect the train to a completely different line. So for example, in the actual zero launch groups circuits, one of the silence issues would be if you could provide, if you could prove that some input to a hash function ends up with the hash of all zeros, this would be completely wonderful. This is extremely unlikely, but if you manage to prove that it would probably.
02:06:23.636 - 02:06:25.040, Speaker B: Be a sound issue.
02:06:26.530 - 02:07:22.940, Speaker F: It is also one of the most grievous issues that can happen because this allows you quite often to completely view the protocol. Imagine that you can prove that you have a million or a billion in your account. Then you can easily drain resources and easily drain funds from the protocol. So then there are competence issues that when you try use canceled or you can only create very proof. You can go to some of the station. So in irregular security would be the same as you trying to prove that the hash of an empty string is this value and not being able to just because for some reason something is failing in the circuit. And this particular use case, maybe it's an edge case, was not thought of by the developers.
02:07:22.940 - 02:08:23.312, Speaker F: And the last issue, which is an issue to a project like ours, but not to all the ZT projects, is the issue of zero knowledge. So if you want to preserve privacy, we actually need zero knowledge in our system. Which means that when you create a proof that you only submit information about the fact that the proof is correct. So the proof itself reveals only that the proof was correct. So this would be like somebody tracking all your movement for the underground or somebody logging the secret you were using for the hash. So how do you deal with all of these security issues both in ZK and in regular applications? Well, one of the things you could do is unit test and obviously it's just healthy hygiene. I think all of the projects are using unit test these days.
02:08:23.312 - 02:09:12.896, Speaker F: It's just completely necessary to detect the shallows of bus. Without this you can't go any further. The next stage are end to end tests. It is a mark of a healthy development culture in the company. It helps detect problems with complex interactions at a very early stage in the development of an application. One of the problems with end to end interactions is that they are not scalable and it is very hard to cover all the use cases that various users would probably trigger. So it is just great to see that components are working together the way they were intended to.
02:09:12.896 - 02:09:47.180, Speaker F: But it is not a very expensive testing. The next thing you can do, iterative audits. We've done quite a lot of these. One of the major benefits from them, apart from increasing the security of your solution, is that you also raise the level of your team because people start interacting with new parts of the code base they haven't been accustomed to, they explore new stuff. It's always nice to have redundancy.
02:09:50.160 - 02:09:50.428, Speaker A: In.
02:09:50.434 - 02:11:11.736, Speaker F: The skills of your team, so that if, for example, one member of a team decides to leave you, there's always somebody in place who knows a particular critical component. One issue with internal audits is that they consume enormous resources because basically your team has to stop developing at the time you can't audit something that is under active development, and just reading the code is not a very interesting process. So, yeah, it is quite slow. Also one of the issues is that you have to also one of the issues is that rereading the same part of the code base again and again produces diminishing returns with each new iteration. Especially if you're working on constantly on components that are constantly being developed and updated. External audits are actually worse most of the time because for an external audit to be efficient enough, your project requires very different documentation so that you can get the auditors up to speed quickly. Complexity and novelty of your project decreases efficiency for the simple reason that the auditors will have probably not seen this particular system used anywhere else.
02:11:11.736 - 02:12:10.104, Speaker F: It's a very complex system. Maybe they have seen some components, but not the whole structure. It is very pricey to reuse and yeah, once again, does work well with active development, because the changes that you've made to the code base after, for example, six months may change the whole, may have changed the code base so much that the whole previous audit would have left no profit for you at all. And also one of the issues with audits in general is that they work really well when there are repeatable patterns in various projects. For example, let's say various projects are writing signature code for ECDSA, then external orders can come and say, oh yeah, I'm accustomed to this primitive. I know what kind of stuff happens there, what kind of problems and issues can arise. Let me look.
02:12:10.104 - 02:12:54.404, Speaker F: I'll find the same problems here. If you are doing something completely novel, or if you are composing very new ideas, it means that the auditor actually has to learn what you are doing. Obviously, there are some issues. So as we can see, if we accumulate all of these four standard mechanisms together, we won't have all of the code base covered and we won't get rid of all the bugs anyway. So some of the issues are that supporters are hard to produce. They drain resources. Testing usually tends to be very shallow.
02:12:54.404 - 02:14:07.570, Speaker F: It doesn't cover zip bugs. So what can we do? Two of the deeper techniques that are used with project nowadays are button and formal verification. So one of the techniques that we're using is pattern. You can think about it. The one step forward from testing Daddy is that a program explores your program your build system, your program in such a way that it automatically finds new and interesting inputs that would change the behavior of your program. So where testing is just randomly throwing data at a function that might have issues with fuzzing, the array is that the program actively tracks which parts of the program are being touched by a particular input. So which part of the program lights up and then it remembers those particular inputs and tries to mutate them so that you uncover deeper and deeper parts of the program.
02:14:07.570 - 02:14:39.636, Speaker F: It actually proved quite efficient in our case. It's great for testing composable primitives, especially if we're talking about insert primitives, where the ide's constraints are dependent on how you compose those primitives because then you can track the composition of the circuit and use it as a metric for the father to explore deeper. It is not as good for fixed.
02:14:39.668 - 02:14:40.440, Speaker A: Circuits.
02:14:42.140 - 02:15:17.620, Speaker F: Because it has less information work. Homer replication to me works a bit like magic. The problem is sometimes it can refuse to answer you. It's one of the few methods that can detect soundless issues. There is some tooling like Barridizer speakers for example, that can be used as a reference or even circum. You can use it directly. The problem is that modern provers have either effect logic or finite field logic and it doesn't really mix very well.
02:15:17.620 - 02:16:24.920, Speaker F: And a lot of the time we are trying to approximate binary logic on top of finite fields, which turns out to be inefficient. And if your project is using Lookups, then it's basically uncharted territory. It is very hard to use Lookups with finite field theories, with finite field theorem solvers just because you have to encode your game with like hundreds of thousands of roots. I'll breeze through additional parts of the project other than decay. Other parts of the project are the infrastructure. So here you can obviously one of the issues are architecture issues with this it is probably best to go to professionals just because there are several firms on the market that actually deal with this kind of stuff. They can simulate the protocol, they can understand what the issues are and then there are the implementation bugs.
02:16:24.920 - 02:17:08.568, Speaker F: So what do we do? Yes, we document religiously, we ask auditors to simulate the protocol and we do have a definite fuzing during the development lifecycle. So to get rid of as many bugs as possible, we also have to think about the web application. Unfortunately, even for modern web three projects, there are still considerations in the web world. So not everything. For example, broken authorization is not as crucial most of the time. But several issues in our trend still can plague your application. So this is still something you have to consider.
02:17:08.568 - 02:18:02.040, Speaker F: Once again, it is probably best to go to professionals for this and not try to understand the security of web applications by yourself and your team, just because it is a very separate area of cybersecurity. So just last thing is CI CD and processes. So some of the issues that all projects are outdated components, sometimes weird stuff is found in those. Maybe they're weird paths. So you have to update them regularly but don't make them automatic because sometimes there are supply chain attacks. Let's say somebody forgot about their GitHub account or their GitHub account got hacked. There's a new shiny package, your application automatically updates to the latest version.
02:18:04.160 - 02:18:04.524, Speaker A: And.
02:18:04.562 - 02:18:56.600, Speaker F: Now your users are experienced in an attack just because you've imported this package. Then there are standard security misconfigurations in the ACI CD. So you have to be careful how you use secrets for example. So in here you can consult with professionals and actually traditional auditing teams are quite good at explaining what to do and what not to do and engaging with and you can easily engage with and yeah, not Korean job candidates turns out. Also Ethereum had this, we have this. So you always have to delineate the security in your organization. So how should you stop worrying about security from the start? It is important to assign areas of responsibility in your organization.
02:18:56.600 - 02:20:05.324, Speaker F: So somebody has to be responsible for the web application, somebody has to be responsible for the security of the proving system, somebody has to be responsible for the processes. Introduce a process for documenting the security considerations for build components. So once you build something as a top of concept, just write a few lines in the doc about what you are worried about how you should be securing this in the future just so that you start the process of thinking about security of that component if even you are just starting fixing regarding components as a treatment metric. So, let's say we have several primitives in our code base which we have to change quite often, which means that well, probably there are bugs in them at one point or other, we constantly change stuff. So it would be nice to have some more extensive testing techniques such as body for them. So if you have components such as that in your system, it would be great to look at that. Establish growth and maturity triggers for organizations.
02:20:05.324 - 02:20:39.884, Speaker F: So for example, if there are 40 people in the organization now, maybe there's a point in having a Fisher exercise just to see how good you are against North Korean candidates and consult with professionals on non ZK issues. There's no point trying to build your own security of web applications. You are probably not the best at it. Deal with professionals. So security is not just a pain. Security also helps you reduce resource waste. For example.
02:20:39.884 - 02:21:30.232, Speaker F: Once again, if you are working with a particular primitive, particular part of your code base that keeps breaking quite a lot, there are issues improving security and improving testing for It can help you reduce resource waste in the long run. Just because you don't have to keep going back to fixing it. Now you have a more mature process. Being open about this and showing others how to do this also brings the other view to your organization. Because you show that your processes are mature, you're showing that you care about code base. This is not just a one day project, so it becomes a talent magnet. And in a field where protocols are broken almost every day, having security in general and improving the security of your project is obviously a competitive advantage.
02:21:30.232 - 02:22:14.412, Speaker F: Sometimes you just need to supply the final takeaway. The security of ZK projects is not just the security of ZK proven systems, it's also traditional security. You have to think about other parts. Somebody has to be responsible for security, even in a small organization. One of the simple reason is if you get hacked, even if you're a small startup, and there are always chances that this can happen, somebody has to be responsible for picking up the phone, figuring out what to call, figuring out what to do. You shouldn't be scrambling to find the one responsible for that. And security is a pain, but it can really help you.
02:22:14.412 - 02:22:29.740, Speaker F: It can be a benefit for your organization. So it shouldn't be something left until the end, up until you've almost built everything. Thank you for your attention.
02:22:30.960 - 02:23:19.410, Speaker A: Thank you. And I guess just on the questions that I had myself was when you were talking about using Fuzzing for securities, one thing that kind of got me curious was how widely used Fuzzing right now for a lot of CK applications? And also does Fuzzing necessarily, let's say, cover a lot of the soundness aspects of the securities that formal verifications already covers? Or is that something that I guess fuzzing would not be able to achieve? I guess more in general, what's the part that Fuzzing can cover and what's the part that Fuzzing cannot cover?
02:23:21.460 - 02:24:31.152, Speaker F: So, to answer the first, how widely is Puzling used? I think at the time when I started looking at it, puzzling was used in very few projects and most of the time only rudimentary. So for example, there was a fixed circuit and Python was used to change inputs to that circuit. And this would provide a very limited use of the pattern because Python excels when stuff is changing inside the program. And if the circuit is fixed, obviously that's very difficult to explore. But yeah, after that I heard that, for example, there's a company, Parsing Labs, and they're also focusing on doing parking for their project. So they're thinking about security of the key from the passing perspective. So maybe right now it is a bit better because there's somebody focusing on it as an external audit about soundness with Fatim.
02:24:31.152 - 02:25:27.960, Speaker F: Well, I managed to find some soundness issue once with fatigue. It was more by chance. So it was kind of like some weird stuff happened and it led to completeness and soundness issues and the completeness issue showed that there was a soundness issue. But no, fuzzing by itself can't really find soundness because unfortunately, in circuit, it's very hard to find a proximity metric. So basically you could try and guide fuzing towards finding a solution that is inappropriate, that shouldn't be your circuit description, but still adheres to the Arithmetization. But the problem is, since we're operating in finite fields, there's no proximity between values. I don't know how one would do that.
02:25:27.960 - 02:25:31.704, Speaker F: Maybe somebody finds a solution to that particular issue.
02:25:31.902 - 02:25:57.776, Speaker A: Got you. I guess in a sense, you sort of also foresee buzing is great for just testing out a lot of details and making sure that you can try to cover some edge cases in a more broader programming senses. But you still do sort of see a lot of formal verification stuff to be quite necessary companions of fuzzing in.
02:25:57.798 - 02:26:01.430, Speaker F: General, formal verification and maybe some.
02:26:04.520 - 02:26:04.992, Speaker A: Static.
02:26:05.056 - 02:26:09.456, Speaker F: Analysis or intense dynamic analysis for particular properties.
02:26:09.568 - 02:26:10.230, Speaker G: Maybe.
02:26:12.380 - 02:26:46.370, Speaker F: For example, you can look at how the values change throughout your witness when you randomize the inputs. And that can also become a security metric and that could sometimes uncover soundless issues. But yeah, plotting, unfortunately, is just testing on steroids and it also helps remove programmable biases. So whether testing is I'm just testing what I imagine the circuit could do. Fuzzing is, yeah, I don't care, I'll just push everything.
02:26:47.300 - 02:26:58.340, Speaker A: Do you ever foresee a future where perhaps we may have ZK specific fuzzing where we can, let's say, cater for proximity testing, et cetera?
02:27:00.700 - 02:27:35.410, Speaker F: It is very hard for me to imagine because just the proximity testing by itself the proximity testing by itself is a very it's fundamentally it simply doesn't exist in the finite field. And we're using finite field and circuit. So that's the core issue. Maybe if we use something.
02:27:37.300 - 02:27:38.032, Speaker A: Maybe if.
02:27:38.086 - 02:28:07.820, Speaker F: Puzling was used, for example, with NZK DSL, where you have a very strong correlation between the original language tokens and the circuit, then you could kind of do sequential generation and you could try to do proximity step by step, but I assume that it would be very resource intensive.
02:28:08.560 - 02:29:01.630, Speaker A: Gotcha. Gotcha. I see. Okay, thank you very much inokenti for the great talk about security today and again yes, thank you very much for the time for so I guess moving forward, for the next speaker, we have Ben from MetaLabs to share more about roll up problems and just a little bit about MetaLabs. I'm sure a lot of people are familiar with the team behind ZK Sync, so they built the Zksyc, the biggest Zkvm L two that we have in the space. Ben do a lot of research and development there. Ben, you can take it over.
02:29:03.200 - 02:29:21.764, Speaker H: Thank you so much. Great to see you again, Yuki. And I hope you can hear me reasonably well. So today I'll basically give a slightly shorter version of the talk I gave at SBC last week. I'll try to fit this into 20 minutes. We'll see how it goes, and I'll try to leave plenty of time for questions. Can you see my slides? Okay.
02:29:21.764 - 02:29:22.870, Speaker H: Is that all working?
02:29:23.240 - 02:29:28.230, Speaker A: I think could you try share again? Because it's not showing up on my end.
02:29:32.220 - 02:29:37.770, Speaker H: Okay. It's showing sharing. Is it working for others?
02:29:41.620 - 02:29:48.050, Speaker A: It's showing that Ben's Her started screen sharing. Double click to enter the full screen.
02:29:48.740 - 02:30:45.530, Speaker H: Okay, interesting. Let me stop the video. Let me try to go to the full screen view here. One SEC. How about that? Is this working?
02:30:50.220 - 02:30:58.892, Speaker A: I see you in the camera, but I still don't see the screen here. I'm wondering, why is that still the case?
02:30:58.946 - 02:31:02.172, Speaker H: Fathers. Because I can see it as being shared here just fine.
02:31:02.226 - 02:31:12.370, Speaker A: Yeah. Chloe, do you see? Yeah, I don't see it from my side either, but you can actually just share it with me so I can be able to share it as well.
02:31:15.300 - 02:31:18.368, Speaker H: What do you mean? You mean the file itself? Is that what you mean?
02:31:18.454 - 02:31:22.628, Speaker A: Yeah, the file itself. You can share with me on Telegram. That'll be great, too.
02:31:22.714 - 02:31:24.790, Speaker H: Just 1 second. Let me do that.
02:31:25.960 - 02:31:28.150, Speaker A: Sorry, some technical issues here.
02:31:30.360 - 02:32:45.826, Speaker H: Sorry about that. Let's see if that works. Can I still drive it? Can I still click? Or does it mean that you have to click as well?
02:32:45.928 - 02:32:48.580, Speaker A: I will have to click for you, if that's okay.
02:32:48.950 - 02:32:50.914, Speaker H: All right, I'll tell you next slide, I guess.
02:32:50.952 - 02:32:51.522, Speaker A: Every time.
02:32:51.576 - 02:32:52.660, Speaker H: Sorry about that.
02:32:53.430 - 02:32:55.880, Speaker A: Okay, here 1 second.
02:32:58.330 - 02:33:04.470, Speaker H: Bit of a poor man's version of animation, but okay. I'll have to manage, I suppose.
02:33:10.460 - 02:33:27.910, Speaker A: All right, 1 second. Is it showing for everyone? Yeah, I can see it. I can see for me. Yeah, I can see. Feel free to just tell me to go to the next slide.
02:33:28.250 - 02:33:29.462, Speaker H: Sounds good. I'll do that.
02:33:29.516 - 02:33:29.782, Speaker A: Okay.
02:33:29.836 - 02:34:06.994, Speaker H: So today's talk is about some of the open problems we see in the roll up space and as we're thinking about where we go forward in the next year or so. So, next slide, please. Just a few words about myself. In addition to being the VP of Research at Meta Labs and running the research team there, I'm also professor at Imperial College London. Just a little bit about my research background. I come from a background in programming languages as well as system security. So some of the stuff that was covered by the previous speaker naked from ADSTEC Labs is actually near and dear to my heart.
02:34:06.994 - 02:34:39.296, Speaker H: But the last four or five years, I've been focusing squarely on blockchain related topics, so things related to DeFi fee mechanisms, audit. So we have a few papers coming out in the next several months alone on these kinds of topics, but the slides disappeared. Seems like the curse continues. Okay, there it is. The slides are back. Okay, next slide, please. And so just the TLDR for this presentation.
02:34:39.296 - 02:35:21.392, Speaker H: So I think we're still quite early in the design journey when it. Comes to roll ups, I think there are still more design questions than there are answers. And I'll talk about some of the answers that a variety of people in this space are proposing and I'll kind of pose probably more questions than I'll give you answers for the next 20 minutes or so. So my sense is that we as a community have not arrived at too many points of consensus when it comes to even the simpler aspects of roll up design. Next slide, please. So I think it's no secret there is a significant consolidation of strength behind Ethereum as a layer one platform. And that's due to a variety of reasons.
02:35:21.392 - 02:36:35.444, Speaker H: One of them is the fact that Ethereum has a quite an open design and development process. I think there is just a lot of ingenuity that goes into whole variety. And I'll mention some of these examples around, I don't know, account obstruction, 50, 59, some of the things that are coming out with 48, 44, a variety of EITs that really have a lot of mind share and a lot of people sort of putting their sort of thoughts as well as their implementation efforts into them. And part of the thing of course, is that given the extent of their market, oftentimes this is what happens in their markets as well. So as part of that, EVM compatibility has become virtually the norm around, I'd say, the majority of the blockchain space, despite any number of shortcomings when it comes to the EVM. So Solidity is a language as well as the EVM as a virtual machine, far from perfect. And in fact, many people have complained about these kinds of things vocally and there are definite ways to improve upon the EVM.
02:36:35.444 - 02:37:17.316, Speaker H: But at the same time there's a generation of developers who've been traded on Solidity. And as such, you basically have a supply of people who want to build a VM platform if you are EVM compatible. And so this is where we are with CK Sync. And part of it is because that's where developers are as well. So when it comes to scaling up execution, when it comes to making it more affordable as well, ethereum's plans are actually quite explicit. Their plan is to basically rely on the roll ups as a scale up facility, as a way to basically outsource execution, to be largely off chain with some elements of it that are on chain still. And that's the roll up story in a nutshell.
02:37:17.316 - 02:38:00.988, Speaker H: And that's why we're here, that's why we are focusing on this. Next slide, please. So now this is probably going to be, I'm sure pretty much everybody in this audience has seen this somewhere. So people distinguish it from optimistic, broadproof based roll ups and zero knowledge roll ups. We are seeing on the validity proof site, on the zero knowledge site of this divide, so to speak. And there are probably more companies essentially trying to bring solutions to that side of this space as well. And on top of these sort of end to end platforms, if you will, there is any number of companies trying to add solutions and kind of playing up to the modularity thesis.
02:38:00.988 - 02:39:03.336, Speaker H: So for example, there are DA solutions, data availability solutions like Celestia and Eigen layer DA. And so I'll mention those in passing in subsequent slides. Next slide please. So here's a few interesting data points and this is something I got from L to Bit, a site that basically keeps track of roll ups progress in the past several days. This is activity or growth, I should say, in the last year or so and a few things to point out. So the activity as measured by the number of settled transactions, that's what you see in the top chart and there's a scaling factor which is like how much are roll ups settling more than the main net in aggregate, right? All the roll ups that they consider, what's the scaling factor like and seems like there's a chance that that number will keep growing. That is to say, the amount of activity that's settled on the collection of roll ups we have will keep growing compared to where a main net is.
02:39:03.336 - 02:39:55.480, Speaker H: So sometimes actually will probably still be settled to mainnet but a variety of things will actually go to roll ups. Here's another sort of set of data points at the bottom here, which is to say the total value locked, which is not a perfect measure by the way, but it's an interesting one anyway to consider. Keeps going up, right? So you can probably identify individual points like if you go back to March, given that a number of ZKE EVMs, including our scheme, became publicly available, you sort of see significant growth as a result of that. Next slide, please. So I wanted to touch at least briefly on a variety of aspects of roll up design and this is what I have listed here on the slide. So I'll basically take these issues, these aspects in order and try to spend a bit of time on each of them in the next 1015 minutes, I would say. So, first decentralization.
02:39:55.480 - 02:40:39.892, Speaker H: Next slide, please. So the moment, the not so big secret is that just about everything about roll ups is mostly centralized. We have seen, I mean, firstly, there is nothing wrong with the path of progressive decentralization. That is to say, you get things to work, you build up your user base, you build out your developer community, you make sure that things are working kind of robust and so on and so forth. And then over time you basically decentralize various aspects of your system and this is a path that others have followed. And I think the roll up space will basically follow the same mantra for the foreseeable future. At the same time though, we're sort of running things with some degree of efficiency.
02:40:39.892 - 02:41:49.340, Speaker H: One could argue things are a bit different though, on the zero knowledge space where there is a lot of proof or work that needs to happen, as opposed to the fraud proof space, the optimistic space where the amount of work is actually relatively less. But at the same time, there are advantages of throughput and latency when it comes to centralized system deploying decentralized system by and large. And so how to attain these kinds of advantages in decentralized setting is actually relatively unclear, right? And the other thing is that decentralization comes in different shapes. This is kind of a multidimensional kind of property ultimately. So we can talk about node ownership, we can talk about data center diversity, we can talk about GeoDistribution and geodiversity. And then the question that begs itself is like, well, how much decentralization is good enough? Next slide, please. So if you stare at this chart, which covers the last six months of ethereum validator growth, you see that at the moment ethereum has like a gigantic number, over 800,000, like close to 900,000, give or take, in terms of number of validators.
02:41:49.340 - 02:42:41.712, Speaker H: And this is like almost a twofold increase in a relatively short period of time as well. So, well, the question is what's the big enough number? Is that too many? Is that the right number? Should we ask for more? Should we ask for fewer? Is it like over provisioning things? And I think answers are kind of elusive and actually people have looked at the ethereum validator ecosystem in quite a lot of detail, quite frankly. Right, and so they find some interesting sort of measurements. Like for example, this website Esunshine.com provides a variety of metrics and in fact, they give you sort of a health score, so to speak. I think for that time it was like 58%, right? And so they basically rate the ethereum ecosystem on a bunch of pails. And so they look at things like consensus client diversity, right? So like other things other than guests that people are running.
02:42:41.712 - 02:43:42.128, Speaker H: And how often is that happening? Things related to, let's say, geolocation diversity here on the right, staking pool diversity, right? So basically it's like a single pool dominating the ecosystem, non pool based validators, right? So what is it? Solo, staking, that sort of thing? I mean, is that present, is that prevalent, that sort of thing? So you can even for something that we genuinely believe to be quite decentralized, you can sort of go and ask a whole bunch of interesting questions when it comes to decentralization. And so next slide, please. A question that I would ask about this. Next slide, please. Yeah, thank you. Well, so what to do? Do we want to replicate the same thing for OLOPS? I mean, is it sensible to expect every validator to basically become, what, a sequencer or something like that? And that's to my mind, not entirely clear at all. I think some aspects of this perhaps could be sensible.
02:43:42.128 - 02:44:33.120, Speaker H: Like for example, maybe not everybody should be running the exact same piece of code. So we are not basically just reducing our potential availability because we are sharing the same exact bugs everywhere, right? That's just an availability argument. But the idea that you need to have, I don't know, hundreds of thousands of nodes doing your sequencing I'm not sure that I'm sold on that at all. Let's say firstly secondly, even with that really significant number of nodes we are not quite where we are or where we would have been in the web two space. Like if you go back and think about the cloud for just a second, people talk about high availability systems. Metrics like five nine of availability for example, come up quite often and there is very little of anything even in the ethereum ecosystem. But in blockchain in general that even starts to sort of resemble these numbers related to availability.
02:44:33.120 - 02:45:39.284, Speaker H: And so one could pose a question like well, if you think about sequence designs, if we're comparing different designs to each other, should we basically be aiming at these kinds of high availability outcomes and what's the right way to get them? And maybe it's actually not trying to increase the number of nodes to gigantic amounts. Maybe there should be focus on perhaps something else entirely. And so here's another question that is often conflated but frankly I think is causing more confusion than it should. So does decentralization imply permissionless participation? Can you basically have a system that's relatively centralized but where things are permissioned in some ways that the community believes is appropriate? Right? Because if things are entirely permissionless then you basically have the civil problem. You have potentially more dos related problems as well. And so is that what we want as well? Not entirely obvious, I would argue. At the bottom you see a reference to a paper that looks at measurements of bitcoin ethereum sort of in a variety of dimensions of decentralization.
02:45:39.284 - 02:46:39.032, Speaker H: If you're interested in these topics, in reading about these topics, next slide please. So let's talk about sequences just a bit more. So sequencers and this is where people spend quite a lot of time and sort of mental energy, if you will. I think the majority of work, not all of it, but the majority of work has been sort of lifted, if you will, from permissionless consensus literature. So basically people look at a bunch of variants of DfT based consensus and they basically reason about some of the latest approaches. There things like hot stuff, hot stuff, too fast, hot stuff and they look at essentially taking that and moving this into a POS setting and basically getting performance numbers that those kinds of algorithms genuinely give you, which is to say maybe latency of let's say one to 3 seconds for most distributions. The throughput though might be a little bit more limited.
02:46:39.032 - 02:48:02.324, Speaker H: Some people have suggested DAC based solutions but we have not seen those deployed or experimented with in practice. Let's say it's not obvious though that we should basically replicate the design that's designed that was there for state machine replication to the problem of sequencing. The problem of sequencing is its own specialized thing, right? I mean, nobody said that we should just basically take the latest and greatest in BFC consensus and just wholesale move it to this space. I think that's not entirely obvious at all. Another thing that's also not obvious to me as well is what's the right way to connect, if you will, or to align sequencing and data availability provisioning, right? So there's one argument for modularity where these things are provided by different vendors and so on and so forth, but the challenge is that modularity is ultimately somewhat more difficult to make more reliable, right? If you worry about five nine of availability, like how do you actually take a modular design and make it work with five nines? I would say that that's a highly non obvious kind of question to answer. Next slide, please. Another sort of thing is that comes up a lot is sequencers and mev and sequences as mev tractors and so basically there are sort of two extreme points of view that are often taken here.
02:48:02.324 - 02:49:02.452, Speaker H: So one is that prevent, like we should just stop or try to stop NAV at all costs and there's a lot of work that's focused on various committee schemes, private mempools, basically blinding transactions, things of that sort, where the sequencer is not able to really interpret what is being sequenced, right? And so such it's got a hard time basically profitably doing mev extraction, right, as well as failures, focused consensus, algorithms, things like Temis from Chronal Tech and a variety of others. But at the same time there is a lot of doubt in terms of like well, can we really suppress all dangers or all peritious forms of mev and should we even try? Right? And then you sort of go to the other side of this spectrum that we will oftentimes right, so, well, mev should be embraced so mev can play a positive role and it should just encourage node runners. And this is like, in some sense, the better way to compensate sequences than it is to pay them money or to pay them sequencer fees.
02:49:02.516 - 02:49:02.792, Speaker C: Right.
02:49:02.846 - 02:49:38.336, Speaker H: And so we should just basically take the latest and greatest and PBS and EPBs from Mainnet, from Ethereum's Mainnet and just basically repeat the whole thing and then basically have Flashbots as part of the picture as well. So my sense is that these are fairly extreme points of view. I'm not entirely sure what's in the middle. I wonder if there's something that's a little bit less extreme. Let's say that is a middle ground here. And I think this question is especially relevant when you're thinking about not only L two S but L three S hyperchains that sit on top of l two S, that's what we call them. Hyperchains for, for example, enterprise settings.
02:49:38.336 - 02:50:21.508, Speaker H: Right. There's a company and the company doesn't really want its mev to be extracted by somebody else. I think that's just inappropriate for the company setting, right? Like, how do we stop that without becoming too rigid and sort of too crazy about it, if you will. Next slide, please. I think the part that's actually receiving perhaps a little bit less attention than shoot, although it's certainly getting some these days, is Provers, right? And basically there is ongoing and frankly, nonstop progress in proof systems. I think the progress is not going to be like reaching a fixed point, reaching its limit for the foreseeable future, I think we can expect to see progress on the proof system side. We recently released Boojum as our proof system, but I think, again, that's not the end of that story.
02:50:21.508 - 02:51:17.476, Speaker H: And people will experiment with any number of things related to folding. There are exciting updates on Paralyzing provers. There was a paper that was accepted at Oakland Security and Privacy from Berkeley, from Don Song's Group on some of these things showing really exciting scale ups from proof of parallelization. So I think all of these things will be the subject of a lot of focus for the next months and years, which is to say, speed of proof production, single core, reducing hardware requirements, and then figuring out how to ultimately increase parallelism as well as to connect custom hardware to the task of proof generation proof production. And I think we actually don't know what the ultimate sort of steady state will look like at this point. We are experimenting and there's no shortage of interest from hardware makers to basically make this process a great deal more efficient as well. So next slide, please.
02:51:17.476 - 02:52:03.396, Speaker H: And that's kind of what I already said, right? So lots of players interested in producing new proof systems. We just saw updates from Fault Z on Jolt and Lasso, as well as some interesting updates from people trying to do aggressive folding. And on the right hand side, you see some names of companies that are trying to basically build advanced hardware for this. And next slide, please. Yeah, so in a sense, like Prover decentralization and opening the Prover game up for others, I think is super interesting. The moment it sort of becomes even mildly available, I think there'll be a great deal of interest. And we already speaking to people who are keen to get into this space.
02:52:03.396 - 02:52:08.948, Speaker H: But yeah, should it be permissionless? How diverse should the hardware set up be?
02:52:09.114 - 02:52:09.830, Speaker A: And.
02:52:11.740 - 02:53:08.970, Speaker H: What happens if Provers decide to do something that is slightly unpredictable, perhaps malicious, and so on and so forth. And this is where you basically have next slide, please. Any number of open issues. Next slide, please, in terms of how to before that, please. Okay, yeah, that's it. How to basically make it so that there's no, like, we don't end up with a winner take all possibility where somebody with a really advanced hardware set up that they don't share with anybody else just basically wins every block, right? That's one open question to my mind. Also another question is how to have variable pricing if I'm willing to pay more to have my proofs achieve finality faster, can I do that? And so what are the means by which I can achieve that? It's actually not entirely clear how to sort of create an alignment between incentives and prove our work.
02:53:08.970 - 02:53:39.428, Speaker H: Next slide, please. And so on to economics. Let me just say that we've seen a number of tokenomic formulas that have emerged over the last several years and some of them have come from DeFi, let's say. So stake based governance, stake based protocol participation, things of that sort. I think it's tempting to just lift them and move them into the space. I'm not sure that this is a great match, quite frankly. And even then we have seen any number of problems with stake based governance when it comes to DeFi protocols as well.
02:53:39.428 - 02:54:28.044, Speaker H: I don't think it's necessarily perfect system. We've seen any number of governance activities that have not been quite as trustworthy as perhaps they should have been, that sort of thing. And so maybe we should take a different approach here. Maybe we should sort of stop focusing on POS and staking and take more of a mining based approach where participation improving is actually more of a focus area, right? And we should actually reward provers a bit more than we reward people holding stake in the protocol. Just a thought. I don't think that we really have consensus, no pun intended on how this should be done, but next slide, please. I think it's fair to ask what kind of outcomes do we want? And I think one of the outcomes we do want in roll ups is we want to have speed.
02:54:28.044 - 02:55:29.380, Speaker H: We want to have low latency when it comes to both soft finality, which is generally what sequences give you as well as finality that you get from the prover as well as high throughput and then other properties like sensor resistance and maybe reduction in mev, but not clear to what degree as well as high network availability. And how to get these things from tokenomics I think is actually a tricky question. I'm not sure that there are immediate answers that sort of jump to my mind at least. And the other thing is that if we are sort of perhaps in the race to the bottom, I'm reluctant to use that term, but I think you know what I mean. We are in the process of reducing fees for the end user, right? We are in the process of making the blockchain more affordable. Well, how do we keep node runners interested? Like, how do we keep validators interested? And they're in the process of doing highly specialized proving how do we essentially keep them interested given that there is probably some capital expenditure that needs to go into that as well. Next slide, please.
02:55:29.380 - 02:56:14.310, Speaker H: I just wanted to really briefly touch on the topic that was actually covered in the previous presentation, which is the thing is that roll ups is not just mapped, right? I mean, roll ups at the moment are fairly complex distributed systems and there are many ways in which things can go wrong and we need know reason about these availability implementation bugs, which is what Kenzie talked about, as well as infrastructure vulnerabilities. And there's a variety of interesting projects that kind of focus on sort of all of these aspects of things. So, bugs, next slide please. Bugs improve production is something that's been the focus of a bunch of projects in the last three to six months. I think this will just keep expanding. Some interesting references for you debuting. Next slide please.
02:56:14.310 - 02:57:29.164, Speaker H: So one question that doesn't maybe get addressed quite as much is that how do we bring the right software engineering discipline? I mean, circuit generation is a pretty difficult activity, let's say, and this is where a bugs can be found. But at the same time we want to be in an environment where developers are relatively free to basically change things with quite a lot of code velocity. And so how do we basically marry this robustness with the fact that software engineering needs to happen fairly quickly? Is quite an interesting open question. And then next slide please. The last point to make here is that we see a lot of efforts on the main net, on Ethereum's main net things like Cape from Espresso Systems Railgun as well as a bunch of other projects as well as proposals when it comes to stealth addresses and things of that sort as well as more aggressive proposals from, for example, ADSTEC Three on hybrid privacy where execution can be private as well. And so the question is, well, should any of that be provided out of the box by the layer two or should the layer two just act as an exploration facility? And I think there is something there that's not entirely obvious as well. I'm not sure that anybody but Edtech has taken a strong position when it comes to these things.
02:57:29.164 - 02:57:39.824, Speaker H: Next slide please. And in fact last slide. So this is what I just covered fairly briefly given the time constraints. Happy to take whatever questions you might.
02:57:39.942 - 02:58:31.920, Speaker A: You thank you, Ben. I guess one thing I kind of also wanted to dive a little bit more into was the prover decentralization aspect and you kind of inferred a little bit on the proof of work aspect of proof of decentralization and again, sort of like bringing back the POW era. So in terms of proverb decentralization, so far from what I see, we're trying to decentralize it for better liveness assumptions. But are there other properties that you are specifically looking for from those proof of decentralization work to write, let's say meet certain criteria that you want for those decentralized proven network.
02:58:33.300 - 02:59:18.030, Speaker H: Yeah, I think it's not just liveness. I think ultimately, frankly, we approach it from the economic standpoint. It's ultimately the cost of improved production, right? Like how much does integrity cost? Where are we as a system, as a roll up compared to just draw execution and theory, right? And essentially how do we reduce it while having sufficient levels of decentralization? And that's the question that I posed early on which is like well, how many is enough? Like if we have basically five provers well, is that good enough? And in sometimes it's hard question to answer, I'd rather not answer it at the moment. I think there's quite a lot of subtlety here. So I'd say that it's the latency as well as ultimate cost of proof production that we are after in the system in a steady state.
02:59:19.280 - 03:00:04.350, Speaker A: Awesome. And also regarding the, I guess the sequencer side of things, I think you made a very interesting point about how it's not obvious to just bring in the existing BFT consensus and kind of just slap it onto whatever you call it decentralized sequencers. But then in that case, how do you foresee sort of in a near future the way that sequencer would exist for, let's say Zksync, especially now that maybe it's not that obvious that we should rely on some form of POS. What do you foresee as a way to sort of governance around the sequences for those big L two S?
03:00:05.360 - 03:01:11.120, Speaker H: Look, it's the same path of progressive decentralization, right? And the problem is that the path is not entirely linear. So my suspicion is that it's quite clear that decentralization is now future. Right? And what are the steps? I think that's the part that's a little bit more mysterious at the moment. I think we'll probably go with something that's perhaps following some of these designs, but I think we are also working on something that's a lot more adventurous, let's say. And it's a better sort of match in terms of the nature of sequencing as well as nature of what we are providing there. And also with an eye towards like well, how many of these things do you need and are you ultimately after high availability or do you really need to basically have replication? Right? If you basically look at deployment in some of these systems, be that tendermint or Louis consensus, let's say people look at 100 nodes but again the same question sort of comes out which is like well, what's special about 100 nodes? And again, maybe not too much is special about 100 nodes.
03:01:12.120 - 03:01:55.520, Speaker A: Got you. And also given that around, especially around big roll ups, there are a lot of projects that are recently trying to, I guess, push this modularity thesis and people are in some way maybe taking parts of the roll up stack and trying to build something very specific about it. From the perspective of you building ZKC, do you have any takes on those aspects of modularities that is brought in by individual projects and how you see whether it's a good aspect of it or not so good aspect of it from your perspective?
03:01:56.420 - 03:02:38.400, Speaker H: I think modularity is a great way to push a certain sort of angle, if you will. I mean, be that data availability or shared sequencing, we appreciate where those themes are coming from. I think it's an exciting exploration. I think, again, piecing these things together in a way that gives you a reliable and robust system that kind of I keep mentioning five lines of availability, but you know what I mean, right. So something that really kind of just works day in and day out, right, is a challenge. And that's true in a variety of ways, right? I mean, if you have basically, let's say, diverse, non overlapping, validator set that don't really connect to each other, I mean, everything is independent.
03:02:40.500 - 03:02:41.648, Speaker D: In a certain sense.
03:02:41.734 - 03:03:11.290, Speaker H: Everything has to work for the system to work end to end, right? So that's the one aspect. The other one is that if you have multiple tokens at play, then things are potentially greatly complicated by that as well. So my sense is that I really enjoy some of these explorations. I think they are there for very good reason. But at the same time, if our goal is to get to the end result faster, they can be somewhat challenging to build with, let's say.
03:03:11.980 - 03:03:21.404, Speaker A: Gotcha. Gotcha. Okay, thank you, Ben, for today's time and a great talk about all the roll up questions that you have.
03:03:21.442 - 03:03:22.232, Speaker H: My pleasure.
03:03:22.376 - 03:04:24.320, Speaker A: Yeah, thank you very much. So, next up we have Diego from So, a little bit about Lambda class. So, they are one of the major, I'm sure, maybe the biggest venture studio that are working on a lot of both general web three applications, engineering problems, as well as targeting a lot of ZK engineering work. So they work with a lot of ZK projects. They have worked with a lot of ZK projects. And they also write a lot of public, I guess, open source codes that are already running for, let's say, Stocknet and potentially more project in the future for a lot of the stacks for those CTA projects. So, again, please welcome Diego, and the stage is yours.
03:04:25.060 - 03:04:25.520, Speaker F: Okay.
03:04:25.590 - 03:05:40.068, Speaker G: Thank you, Yuki. Very happy to be here. And, well, today I will be discussing a little bit the security and optimization of the Fry protocol, which is at the core of Starks. And as many of you know, Starks have gained widespread acceptance among many projects to build rollups, thanks to its properties. So we wanted to discuss a little bit what is the protocol and what kind of optimizations you can do and a little bit on the security and whether it can resist or not quantum computers and see. What advantages and disadvantages it offers with respect to other protocols or commitment schemes, because, well, Fry can be used as a polynomial commitment scheme, but it also is a protocol by itself that allows for low degree testing. So this is just a brief outline of my presentation.
03:05:40.068 - 03:07:07.140, Speaker G: I will present the main motivation for the freight protocol. Then we will discuss a little bit the protocol and overview. Then we'll show how to turn this protocol together with Mercury into a polynomial commitment scheme, and then some security bounds and characteristics and then some optimizations for the protocol. So what is just it stands for Fast, Brit Solomon interactive Oracle proofs of proximity, and it allows us to test whether a given function is close to a low degree polynomial. And this is useful in particular for Starks and as a polynomial commitment scheme, because we will show briefly that, for example, if the constraints hold in some program, that is to say, all the transitions in the execution trace are valid, then we can write that as the quotient of some polynomial and a polynomial that is equal to zero on the points where the constraints hold. And so if the constraints are valid, then that quotient is not a rational function, but a polynomial. So there are some ingredients we need to make this work.
03:07:07.140 - 03:08:21.160, Speaker G: We will have an evaluation domain with certain properties that will make evaluation very amenable and fast. Then we need linear codes. In particular, it uses Ritz Solomon codes and of course, Merkel Trees and, well, the security of the scheme and everything will depend on the characteristics of the hash function. So compared to other schemes, it has less requirements. Of course, there are some people who will criticize that maybe we tend to take some optimistic bounds on the security of Pry, but as regards the security assumptions, it only requires that the hash function be collision resistant. That is a less stringent assumption than considering, for example, that the discrete log problem is hard. So, as I was telling you in Starks, you want to show that the constraint polynomials are enforced over the computation trace.
03:08:21.160 - 03:09:28.480, Speaker G: And so what you do is you interpolate the trace to obtain the trace polynomials and then you compose them with the constraint polynomials and that generates a univaried polynomial. And then if you want to show that that polynomial evaluates to zero over some domain, you just calculate the quotient. Of course, in Starks you have many constraints, especially if you're doing like a VM. So what you do is you can seep all those polynomials by taking a random linear combination. And, well, if we can show that that random linear combination is in fact a polynomial, then we know that the computation was carried out correctly. Of course, one easy way to show that we have a polynomial is that we could send all the coefficients of p zero to the verifier and say, okay, check that this is in fact a polynomial. And this is consistent with the trace.
03:09:28.480 - 03:10:37.080, Speaker G: The problem is that in general, the polynomial has, for example, degree 1 million. So it means passing the verifier over a million coefficients. So that wouldn't make the proof short and easy to verify, which is something that we effectively want to be able to scale blockchains, for example. So here is where the Fry protocol comes into play. What we can do is kind of try to reduce the polynomial into a smaller one and that way we would have to send less coefficients. Of course, we would have to send some additional information for the verifier to see that we reduced the polynomial correctly, but luckily that would require sending less information. So what we will see is that in general, the precise is of the order, the square of the logarithm of the calculation, the size of the calculation.
03:10:37.080 - 03:12:05.808, Speaker G: So FryWhat will do is kind of randomly fold the polynomial until it hits zero. So very shortly the idea of how to fold a polynomial is well, we can split it into the OD terms and the even terms. So that way we can express it as the sum of two polynomials in x squared and we can kind of randomly fold just by choosing or having the verifier, in this case choose a random beta zero and then we can define a new polynomial in terms of the variable y, which is simply x squared. The good thing is that this reduces the degree of the polynomial in half. So luckily, if we want to show that this is a polynomial, we will have to send only half the coefficients and of course some information regarding the first polynomial. So the proverb this case starts with the polynomial and it evaluates p zero over some domain and builds a merkel tree to commit to those evaluations. Of course, in general, what we try to choose as evaluation domain are the Nth roots of unity.
03:12:05.808 - 03:12:12.810, Speaker G: Why? Because we can do FFT to evaluate efficiently and also we have.
03:12:14.860 - 03:12:15.368, Speaker A: Both.
03:12:15.454 - 03:13:13.180, Speaker G: If X is in the domain, then minus x is in the domain. And so we can really evaluate things and have the verifier check everything in an easier way. So until the degree of the polynomial is zero, what we are going to do is have the verifier sample a random beta. Of course, when we want to render this in an interactive, we just apply via shamir and then what we do is we split into OD and even coefficients. We fold. And of course, we then commit to the evaluations over the domain tk plus one, which is given by the square of the values in the previous domain because we choose the nth roots of unity. When we square, we only have like half different elements.
03:13:13.180 - 03:13:52.250, Speaker G: So that reduces the evaluation domain size. So luckily, every time we have to commit to less evaluation, so we have less degree and we have to commit to less evaluation. So each step is faster. So if we want to reach degree zero, then this takes log two n steps, where n is the degree of the polynomial. And then if we want the Verifier to be able to check everything.
03:13:54.140 - 03:13:54.504, Speaker A: Well.
03:13:54.542 - 03:15:13.232, Speaker G: We can tell him like, okay, we will show this to you by having you query certain values, which is why this is an oracle proof. And so, well, the Verifier will query some positions in the original domain, and then the prover will have to provide him with the evaluation at the point he wants, and also the evaluation at minus x zero, so that he can then perform the calculations and go into the next layer. And of course, for each layer, he will have to provide two values and also show that those two values belong to the Merkel tree that they correspond to, so that we are not cheating or anything. So the proof would consist in these two pairs and of course, their respective authentication paths. We will see later that we can shorten this a lot by doing some clever tricks. In the case of the Verifier, of course, what he would have to do is first sample all the random challenges. Of course, if it's interactive, he already knows them.
03:15:13.232 - 03:16:05.184, Speaker G: If not, he has to do fiat shamir again. And well, for each query, what he does is take the two pairs of values for p zero, check that they are in the Merkel tree. And then what he can do is calculate this value l by taking this truth. And if everything is right, then that value should correspond to the evaluation of p one at x zero square. And well, then he can take the values corresponding to p one and get the value that should correspond to p two. Check it. So what we have to do to check the proof is like checking that they belong to the Merkel tree and then doing this collinearity test.
03:16:05.184 - 03:17:06.550, Speaker G: If it passes for all the layers, then of course, well, everything was done properly. Of course, checking for just one point is not enough, because the proverb could have gotten lucky, and even if the computation is not right, maybe it passes. So of course, to be sure that the proverb can achieve, we have to repeat these queries maybe many times to be sure we will see more or less how many times we have to do this to be able to have a decent security level. And maybe we have some way of making the proverb pay a high cost if he wants to draw bad query positions. Of course, we can also turn this low degree test into a polynomial commitment scheme with.
03:17:08.600 - 03:17:09.936, Speaker C: Merkel trees.
03:17:10.128 - 03:18:37.672, Speaker G: To that end, what we can do is if we want to convert to a polynomial and show that the polynomial evaluates to some value v at point z, of course, what we can do is create the rational function p of x minus v divided by x minus z. If the evaluation is right, then of course that is a polynomial. If not, that is a rational function and it should not satisfy the low degree test. And of course then what we do is, okay, we apply the spread protocol to this function f and that's the key idea of how to use it as a polynomial. If we want to analyze the soundless error, then if we wish for a single polynomial, of course the error will depend on the size of the finite field and the rate, or if you want to see it another way, the blow up factor. Because when we evaluate we have to add like some redundancy and so what we do is choose a larger domain to evaluate and of course the more points we add to the domain, the less likely the proverb is to cheat. And then of course we have another error which will also depend on the blowout factor and that depends on the number of queries.
03:18:37.672 - 03:20:27.280, Speaker G: So we can think that each query roughly corresponds to a certain amount of bits. If we want to batch many polynomials, that is to say taking a random linear combination, then there is another error bound which is a little bit larger. But luckily, if we choose large enough fields that can be controlled in general, we will have to choose a field that is much larger than the evaluation domain and then the number of bits we want to achieve. So if we want to have like 128 bits of security and our evaluation domain is two to the 20, then our field should be like two to the 148, more or less. Of course there are some other tricks if we want to reduce communication and instead of using n coefficients for a random linear combination, we can just have one and take the powers of that scalar as random elements and well, that increases a little bit more the error bound. If we want to batch S polynomials, then the bounds are multiplied by s, but well, luckily we can always try to reduce the error by choosing large enough fields and also taking more queries. And well, the best attacks known against phi at least now are interpolation attacks where the proverb is free to choose up to different points and try to interpolate a polynomial that tries to mimic the real polynomial.
03:20:27.280 - 03:21:47.032, Speaker G: Well, if he gets lucky, maybe he queries on those points and of course the low degree test will pass. That doesn't mean that there aren't any other optimal attacks, but at least now this is one of the best known. Of course there are some optimizations we can take to improve the Fry protocol. Many have to do with reducing proof size by not having the proverbs send redundant information to the verifier. For example, if we want to use the two values from the upper layer to get to the next layer. Well, one of those two values can be reduced from the above layer so we don't have to pass it and we can use the test in the merkel trees to see that in fact we arrived at the correct evaluation because if not the merkel authentication proof would not pass. Another strategy is to avoid sending many authentication paths that would have many elements in common is batch proving in merkel trees.
03:21:47.032 - 03:23:18.780, Speaker G: So instead of sending one proof for each element in one merkle tree, we say okay, we want to query all these elements and so we just generate one proof and then evaluate it altogether. Another point is if we continue the Fry protocol until we reach degree zero, we have to build lots of smaller market trees and send lots of authentication paths and maybe that is more expensive than just sending the coefficients of a polynomial that has nonzero degree. So we can say okay, we fold until we reach degree 128 and well, sending 128 values is less than all the information we would have to pass from the authentication paths of the merkle trees and all the values and so on. Of course another important trick is skipping layers. Instead of committing to every layer, we can commit to every second layer or every third layer. Of course that forces us to send more values from each layer, but we don't have to pass all those authentication paths for each mercury. So luckily if we want to do then recursive probing or something, we don't have to do as many hashes.
03:23:18.780 - 03:24:38.704, Speaker G: Another important optimization is related to using smaller fields and with maybe nicer arithmetic. For example MERS and primes or seldom Mersan primes. Of course if we try to use fields that are that small we need to work on extension fields to reach the required security level. So maybe we'll have to use degree two or degree theory extensions or even more in the case of the 31 bit merson prime. Of course then we have to pay like okay, costlier extension field arithmetic, but maybe that is better than working with a very large field and with only Montgomery reduction instead of some friendlier strategy. Of course there is also a performance trade off when trying to do recursion because we can use for example slower hash functions but with nice arithmeticization such as poseidon. So then doing recursion is less expensive.
03:24:38.704 - 03:26:06.480, Speaker G: But the problem is that the Fry protocol is slower. Typically poseidon is at least one order of magnitude slower than commonly used hash functions like Blake three or Chef three. Of course that doesn't mean that maybe we can get better hash functions or that we can have some hardware specific instructions or something to speed up poseidon. But of course we have this trade off between speed but not nice arithmetic, that is to say costly recursion or slow hash functions but nice arithmetic. So of course there are some cases where one would want to use maybe for the lower layers, one fast hash function and then for the upper layers then do or use a slower hash function. And of course well we can use some strategies to reduce some communication costs. Another strategy is ordering the mercury leaves so that we always have the pairs we want to say to send in adjacent leaves and so that reduces also the proof size so that's more or less where different teams are working and maybe looking at some other opportunities.
03:26:06.480 - 03:27:11.850, Speaker G: As regards the security it depends mainly as we talked on four parameters. First is the field size. So as long as we choose a large enough field size or an extension field then that is not the problem. In general we should aim at least for 100 and 5160 bit fields or extension fields. The other point has to do with the hash function but as long as we choose the right hash function the hash functions typically offer security level of N over two where N is the digest size. Of course if quantum computers were present that reduces further the security level to N over four. But then to get to the same security level we would just have to double the digest size.
03:27:11.850 - 03:28:25.636, Speaker G: Well of course that would impact directly in the proof size because it would double but it's not that it is completely broken as would happen for example with commitment schemes built on top of the discrete log problem. Of course then the security level depends mainly on the number of queries and we can think that each query corresponds to around Pbits of security. That depends mainly on the blow up factor we use. Of course there is a trade off between using a larger blow up factor which gives more bits per query, but that increases the prover work and simply using a smaller blow up factor which reduces prover time and memory. But that then makes the verifier have to check more queries. But luckily that is a more parallelizable task. And of course we can also put some additional cost on the proverb.
03:28:25.636 - 03:28:55.380, Speaker G: By introducing proof of work before sampling the positions we can make him do a small proof of work and luckily that adds some few bits of security. So we can reduce the number of queries and consequently proof size. So that's mostly it. So thank you very much for your attention. Of course special thanks to Elevant Sasson and Starkware for introducing us to these topics.
03:28:55.540 - 03:29:34.230, Speaker A: Thank you. All right. Thank you, Diego. So few questions on my end. Well, the first thing was something I was kind of curious was like you mentioned on the optimization side, that the smaller the field, the more efficient that you can run the fry. But also at the same time, the security would be much better with the bigger field. And I was kind of wondering, what's the trade off between the field size, between the optimizations and also the security of it?
03:29:34.760 - 03:30:32.488, Speaker G: The problem is in general the security is not given by the field. You just have to use a large enough field. So provided that you have like 100 and 5160 bits with an extension field or with an ordinary field, then you are covered in general what limits the security is the number of queries you use and the blow up factor. So in the expression you had like it's the maximum of these two values, typically the field size, you try to use a very small field if we want to have like fast arithmetic. But then of course you have to make sure that at least you reach the number of bits you need. Typically 100 and 5160 would be enough. So that's why they use for example, mini goldilocks with degree three extension or maybe Mars and prime with degree five or degree six.
03:30:32.488 - 03:30:36.890, Speaker G: So in general that is not what limits the security.
03:30:37.260 - 03:30:50.316, Speaker A: I see. So basically you're just like the security in that sense, is that fair to say? It's going to be the minimum of the two, yes.
03:30:50.418 - 03:30:58.640, Speaker G: So in general you just have to focus more on the number of queries maybe on adding proof of work and the blow up factor.
03:31:01.300 - 03:31:45.020, Speaker A: I see. Actually another thing I wanted to also ask about was the block factor. So privacy. I've seen different benchmarks that are done on some stock based proof schemes. And obviously, what kind of block factors that the benchmark teams have set can be a bit subjective. Like, maybe they follow some different standards and whatnot. But what are some of the considerations around, one, the efficiencies and the securities around setting the blow up factors for your proof?
03:31:46.800 - 03:32:47.168, Speaker G: Well, if you use larger blow factors then you have to sample less queries and then precise is smaller. So in many cases one would use that. Besides, you also have to choose the blow up factor according to the highest degree constraint. So for example, if you have like a constraint two or something, maybe using a blow up factor four is enough. Of course in many cases they prefer using larger blow up factor because then that reduces proof size. Of course if you are doing them recursion maybe it doesn't matter much, maybe for the last proof to use a larger blow up factor. But on the other side if you have larger blow factors then prover time increases and also the memory use.
03:32:47.168 - 03:32:57.030, Speaker G: So if you are a little bit more memory constraint or anything, maybe it's better to use a smaller block factor and maybe increase the number of queries for example.
03:32:58.280 - 03:33:54.008, Speaker A: I see. Got you. I think you also mentioned that it would also alter the hardware requirements or I guess more on the compute load for the provers and a verifier depending on, let's say, what kind of flow factors that you set. And maybe if you set it such that the proofs can be smaller, then maybe verifier have to do a lot less work. But then at the same time proverb might have to do, I guess, more work to some extent. So you kind of have balancing between, okay, should we make it cheaper to verify on chain or should we maybe reduce the hardware requirements so that the prover can do kind of less work and then just leave more to the Verifier to handle? I presume that's kind of like the trade off that we're making here, right?
03:33:54.094 - 03:34:19.440, Speaker G: Yes, but in general it has to do most on the proverb side because the Verifier is really quite fast. We did some benchmarks increasing the number of queries and everything and the Verifier is really fast, so maybe it doesn't pay that much. Like having more queries, less queries. To the Verifier it's more or less the same. It runs in a few milliseconds.
03:34:20.260 - 03:34:28.000, Speaker A: But wouldn't it matter from the perspective of gas cost, let's say on ethereum?
03:34:30.040 - 03:34:32.710, Speaker G: Well, I haven't checked that much.
03:34:35.240 - 03:34:35.892, Speaker B: But.
03:34:36.026 - 03:35:27.830, Speaker G: Of course then maybe you would want to minimize the amount of queries that we have to check. But luckily the main cost, maybe there is related to calculation of maybe some hash function, but then the others are like simple operations. The nice thing in Fries that you are always doing operations over some finite field, in the worst case, over extension of some finite field, so they are less costly than, for example, doing elliptic curve operations. So that is, I think, one of the nice things about this protocol and also I think it's easier to understand, has less assumptions. So I don't know, maybe it's one of the reasons why many teams have decided to go for this.
03:35:28.680 - 03:35:45.710, Speaker A: Gotcha. Gotcha. Well, thank you very much Diego, for your time and explaining a lot of the weeds around the Fries protocol and especially the consideration on the security of it as well. So thank you again, adiego for the talk.
03:35:46.240 - 03:35:47.180, Speaker H: Thank you.
03:35:47.330 - 03:35:49.384, Speaker G: Thank you. Bye bye.
03:35:49.512 - 03:36:23.930, Speaker A: Bye. So next up, we have shravan from LaGrange protocol. And just to kind of introduce a little LaGrange protocol is building storage proof solutions across L one and L two S. And some of you guys might have seen them presenting also at the SPC, but they recently released a paper about updatable vector commitment or something. So I guess we'll learn more about it during the talk. Feel free to take it over.
03:36:24.700 - 03:36:25.800, Speaker E: Am I audible?
03:36:26.620 - 03:36:28.584, Speaker A: Yes. I can hear you. Yes.
03:36:28.782 - 03:37:08.992, Speaker E: Thank you for the introduction, folks. I'm super excited to present our work in progress rec proofs. It's a Merkle tree based vector commitment scheme which has updatable batch proofs. So let's start from the basics. So what exactly is a Merkel tree? So a Merkel tree is a data structure that lets you compute a commitment to an ordered sequence of values. Say you have values a zero to a 15 and a collision resistant hash function hedge. In Merkel trees, you compute a commitment by recursively computing the hash of the adjacent pair of values.
03:37:08.992 - 03:38:27.740, Speaker E: So you would take a zero and a one, compute a hash of it concatenate and get a hash of it and you so on do this for the other adjacent pairs. And once you have this, you kind of do this for this level of the hash values until you just end up with just one value, a single hash value. This single hash value is called as the root of the merkel tree. Merkel trees are ubiquitous and it has countless applications in blockchains and beyond. So in addition to just computing the commitment to an audit sequence of values, merkel trees can help you prove that a certain leaf is in fact as a member of this tree. Say for the example you want to prove that a phi is a member of this merkel tree, then what you would do is you would give out these specific values which are highlighted in green, which is a four, the commitment clr and so on and all the way until to the top cr. So once you have these green values, any verifier can take this and can reconstruct the path from the leaf all the way to the root and can reconstruct the digest of the tree.
03:38:27.740 - 03:39:23.916, Speaker E: Given unknown digest, a verifier can check if this reconstructed digest is indeed matches with what the verifier already has and can decide to accept or reject this membership proof. The main focus of my talk today is about updates in merkle trees and so on. So say that an arbitrary element changes in the merkel tree. So this in turn affects the proof of, let's say for this example. In this example it's going to be a phi. So whenever let's say, for example, a twelve changes by delta, all we need to do is take the membership proof of eightwell and recompute the path from 812 to the root. And in places where it intersects with the proof of a file, we just have to update those hash values.
03:39:23.916 - 03:40:42.132, Speaker E: So thus in logarithmic time, we can update individual membership proofs whenever there's an update in the merkle tree. So we can continue to update all the way into the root and we can also recompute the updated digest in the similar fashion. So let's delve into a special type of membership proofs called range proofs. So a range proof is essentially going to essentially going to simultaneously prove the membership of multiple contiguous elements in the mercury. Say you want a single proof which says that a three to a eight is in fact in this merkle tree. So to do this, first you would give out the membership proof of a three, which is going to be these values highlighted in green, and you will give out the membership proof of a eight, which is going to be these values which are also highlighted in green. So once you have this logarithmic size proof what you can do is a verifier can take these proofs and the values ace a three to a eight and can check if in fact if this range proof is valid by reconstructing the hash paths all the way until the root.
03:40:42.132 - 03:41:34.670, Speaker E: So just like an individual membership proof you can see if the reconstructed root is in fact is what you already know. If that is the case, you accept the range proof, else you can reject it. Yeah, range proofs are also updatable in merkel proofs and in merkel trees. So whenever an arbitrary element a twelve changes it is very similar to updating individual membership proofs. So let's say a twelve changes by delta you update the hash paths from a twelve to the root and the places where the green node intersects with the path of 812 to the root. Those values of the range proofs have to be just updated. So once you update these values, you have the most up to date range proof which incorporates the change that has happened in position twelve.
03:41:34.670 - 03:42:25.550, Speaker E: The focus of the stock is more about batch proofs. So a batch proof is a single proof to open k arbitrary elements simultaneously. So the key difference between a batch proof and a range proof is in batch proof it's going to be k arbitrary elements. However, in range proof it's going to be a consecutive set of elements. A crucial efficiency property which we want from a batch proof is it has to be smaller than k individual proofs as a new way of doing batch proof is you can simply concatenate the individual membership proofs of k elements and call it as a batch proof. However, that's why we want this special efficiency requirement. Existing approaches to batch merkel proofs are not updatable even when element in the batch changes or even when some of the element outside the batch changes.
03:42:25.550 - 03:43:39.456, Speaker E: Another thing which we touched in this work is aggregation. Aggregation is the process of simply combining multiple individual membership proofs into a single batch proof. So the more about what we have done in this work so we present updatable batch proof using recursive snarks for merkel trees and our batch proofs can be updated in logarithmic time where N is the size of number of leaves in the mercury. And we generalize this idea to do something called as verify MapReduce style computation even when the data changes. Say that you have a bunch of leaves and you want to have some sort of predicate applied on top of these leaves, then using our approach, you can compute a simple Snark proof which essentially says which can essentially attest to the predicate that was applied over these leaves. Specifically, we show how to do this MapReduce style computation in two specific applications. One is how to aggregate BLS keys even when the data is changing and how do we do a notion of the digest translation.
03:43:39.456 - 03:44:18.828, Speaker E: This essentially argues that the mercury computed. Using hash function, say sha two, five, six are perseid over the same set of values. You can compute a simple proof to say that these two values are consistent. I'll dwell more into the details of these applications later in the talk. But the key point to note here is we can have all of these techniques work even when the data changes. So let me start with a notion of the Nave batch proofs. Say that you want to compute a batch proof for elements a two, a four, a five and a 15.
03:44:18.828 - 03:45:12.396, Speaker E: So the most obvious approach to the Nave approach to do is to just to concatenate the membership proofs of a two, membership proofs of a four and a five, and membership proofs of membership proofs of A 15. So this is going to be a name batch proof, but however, it doesn't meet the efficiency requirement. However, I'm going to use this as a running example for the sake of illustration for the rest of my talk. Once you have these name proofs, we need to verify if this batch proof is valid. Any batch proof verification algorithm broadly takes in certain set of inputs. These are first, what is the digest of the merkel tree? Then let's set S be the what. What is the subset of the claimed elements in this merkle tree and some characteristic about the tree.
03:45:12.396 - 03:45:54.440, Speaker E: In this case, it's going to be the height of the tree and some sort of auxiliary witness. So this is going to be the broad template almost. You need to verify any batch proof regardless of how they are constructed. And the verification algorithm should spit out if and only if s is indeed some subset of the merkel tree of L height and whose merkel root digest is C. And it should spit out no if S is not indeed a subset of this tree. In this talk, for the illustration purposes, I'm going to present two algorithms. The first one is the standard approach where you have individual membership merkel membership proofs.
03:45:54.440 - 03:46:41.768, Speaker E: Using that as a witness, you can verify if the batch proof is in fact correct. And the second one is our approach. We use this approach in addition to the membership proofs, we also use something called as a node subsets, which I'll specifically precisely define. Later, we use additional witnesses to verify the same merkel batch proof. So in the standard version, what you're going to do is you have the a two, a four, a five and a 15 as a part of the batch and you have the individual membership proofs. First, you check if a two is in fact is valid. So you do this by reconstructing the path from a two to the root.
03:46:41.768 - 03:47:24.692, Speaker E: And you can check if this reconstructed root is what you expect as a verifier. Then you also verify individually a four and a five by reconstructing the route and check if this is in fact valid. And you do this so on for the other members of the badge, in this case A 15. And you check if this in fact is the merkel route which you expect. So obviously this namely checks one proof at a time. Can we do better? Is what this book broadly tries to look. So this inefficient batch proof, which is essentially a concatenation of all the individual proofs, can be made into an efficient version.
03:47:24.692 - 03:48:39.744, Speaker E: All you have to do is to just take the verification algorithm and you can just simply snack it. So something like Grad 16 would give you a small batch proof. And this single batch proof is essentially going to attest to the fact that certain elements were part of the batch and these elements are part of a merkel tree which has some merkel root by just C and so on. However, the drawback of such approaches are so whenever there are changes to the elements in the batch are outside the batch, you have to recompute this proof from scratch. And another drawback of these standard Snark based approaches are you also need to have an April bound on total number of elements that are going to be part of the batch. So whenever your bound increases, you have to rerun the setup and have a fresh circuit for it and so on, which is an additional overhead in these approaches. A big picture motivation of our approach is how do we solve certain limitations which are there in the previous known constructions and what can we do better is what is a big picture motivation.
03:48:39.744 - 03:49:37.140, Speaker E: So say S is the set of elements that are in the batch. So we define something called as a node subset. So a node subset essentially is defined with respect to the set S over which the batched elements are proved. So every node in the merkel tree is going to have this value called node subset. So the way it goes about is let's say that it's going to be a subset of values S, where at node B it's going to be the values that are part of the subtree which is rooted at V. I'll show this with a figure which will become much more clear. So at the high level, what it node subset does is it tries to have a set of values in each node and these values are going to be a subset of the set S and the values that are part of the subset are the ones that are present in that subtree.
03:49:37.140 - 03:50:34.936, Speaker E: Yeah. An obvious example of a node subset is going to be the root, which is essentially going to be the entire set. S to see this in a little bit more detail, you can consider this figure where let's say that as I mentioned, the node subset of the root is a two, a four, a five and a 15, which are these elements inside the batch. And let's say you take the right subtree, the node subset of this is just going to be the A 15, as A 15 is the only element from the set S that is part of this subtree. Similarly, for the left one, it's going to be a two, a four and a five. And it does not contain A 15 because a two, A four and a five are the only elements that are part of this subtree and which is part of the batch. Once you have this.
03:50:34.936 - 03:51:45.740, Speaker E: Key thing to note here is in addition to having the merkle hash values nodes, we also have those node subset values in our construction. So the way we verify a batch proof navy in our scheme is first we are going to prune out the set of nodes that are of interest to us in this merkel tree. So these values are the ones that are from the root to all the way to the leaf that are part of the batch. So in this case, I'm going to iterate through an algorithm which is essentially going to look at all the nodes that are from the root and to values a two, a four, a five and a 15. In this example, when we start from the root, first we check if the merkel hash is indeed correct by hashing the left child and the right child. And we also check if the union of the node subsets of the left and the right make up the root. So in this case it's going to be a two, a three, a two, a four and a five union A 15 and we check this against the root and so on.
03:51:45.740 - 03:52:41.840, Speaker E: So we can do this recursively to all the nodes that is are of interest to us. And at each position we just make sure that the merkel root matches and the information about the subsets that is part of the node subsets are also indeed consistent. So we can do this all the way to the bottom. However, at the base case, the key thing to note here is we will have to check if the node subset of that specific leaf is in fact the content of that leaf. And this is the only particular check which we have that ensures that the base case is correct. And you do this for all the elements that are part of the batch. So just like the way we made the standard nave merkel proofs efficient by Snarking, we use recursive Snarks to do this.
03:52:41.840 - 03:53:42.416, Speaker E: So I'm going to present you the circuit of how we do it. So it's going to take in the public input as the merkel digest C, the Bat subset S and the height of the merkel tree. And it's also going to take in two witnesses, specifically one which attests to the fact that it's going to be the merkel digest of the right subtree, the node subset of the right subtree and the height of the right subtree and a snark proof which says that this public statement with respect to the right subtree is in fact valid. And similarly for the left subtree as well. So the verification algorithm goes this way. First we check if the height of the root is in fact is one above the height of its left and right child. And we also check if the Merkel hash value is in fact consistent.
03:53:42.416 - 03:54:38.468, Speaker E: And we also check if the node subset of the right and the left tree when they are put together is in fact the what you expect at the root. At the base case, we check if the leaf data is in fact the node subset. And in the recursive case we check if these are the internal nodes. In the recursive case, we check if the proofs that came in as the witnesses, the Pi l and Pi r, if those proofs are indeed correct. And we do this recursively for all the nodes of interest in the Merkel tree. To see this a little bit more pictorially, recall that this is the tree I have which consists of Merkel hash values and the node subsets. What I've been showing before the batch that is of interest to us is a two, a four, a five and a 15.
03:54:38.468 - 03:55:53.656, Speaker E: So at each level of this tree we are except the leaf level, we are going to have a Snark proof which essentially attests to the consistency of the values that are in that specific subtree. So in this case, I'm not sure if you can see the pointer in this case, this proof Pi LLR is going to attest to the fact that whatever went inside in this subtree is in fact consistent and this is going to make sure that this subtree is consistent and so on. So we can recursively compute such Snark proofs which is going to attest to the subtrees at which it's rooted. And we can do this all the way to the top. And at the root you have the root Snark proof, which essentially is which acts as a batch proof for the elements a two, a five, a four and a 15. So we call this data structure as a batch proof data structure. Once you have this data structure, whenever there is an element that changes in the vector, changes in the vector, you can simply recompute the updated proof without computing from scratch, say AFI changes by value delta.
03:55:53.656 - 03:56:52.832, Speaker E: All you have to do is to follow the path from AFI to the root and update the witnesses and the Snark proof that is there in each specific node. And we can do this all the way to the top and you can compute the updated batch proof which is pi prime in this example. In this way you can update proofs in logarithmic time rather than recomputing everything from scratch. Key optimizations which we use are rather than using passing around subsets assets, we use set accumulation and we use a hash based set accumulation for efficiency reasons and we don't need specific properties from it. So a hash based set accumulation works perfectly fine for us. And we also have two separate circuits, one for the leaf nodes. So, as you can recall, the computation and the leaf level was fairly simple.
03:56:52.832 - 03:57:56.244, Speaker E: However, at the internal node you also had to invoke expensive Snark verification. So in our experiments, we actually break it down into two specific circuits, one for the leaf and the one for the internal. And we put them together in a manner that consists it's cryptographically sound and there are no scopes for attack. So this general idea of batch proofs, this idea can be generalized to perform MapReduce style computation. So let's say that you have a zero to a seven, these values are committed using merkel trees and let the commitment be C. We can take the Map values, we can map the leaf values based on some predicate that is encoded in the computation of Map and it returns the intermediate values to the level level. The reduce operation takes these values together and spits out another value which can be further reduced all the way to the top.
03:57:56.244 - 03:58:35.730, Speaker E: So at each level, the reduce operation is also attached with a snatched proof which essentially says that this reduce operation was done correctly over the data which has the merkel commitment at that specific root value, that specific node. And we can do this recursively all the way to the top. And in the end you can simply perform the reduce operation. This way you get the result of the Map reduce computation. At the same time, you also have a Snot proof which says that the merkel digest c the digest of the values a zero to a seven.
03:58:38.740 - 03:58:39.068, Speaker A: And.
03:58:39.094 - 04:00:02.828, Speaker E: The computation was performed correctly and that is artisted using the Snark proof pipe. So, to see this a little bit more in the context of a real example, say let's say that you have a smart contract with N memory slots and each of that has NBLs keys. The goal is to compute the aggregated public key of a subset of people who have signed some particular message. And we also want to know that number of people who have signed so that we know that sufficiently many people have signed a certain message. A key thing to note here is this subset can change over such such computations are useful in emerging blockchains and so on. So the exact statement which we prove is APK is the aggregated BLS public key of T public Keys that belong to some mercury of N public Keys. And the root digest of that BLS public keys is actually c so say in this example, let's say that we have merkel digest over the BLS public keys of a slot a zero to seven, and say position two, four and phi sign a specific message.
04:00:02.828 - 04:00:53.576, Speaker E: Then what we do in the map operation is first we compute the number of people have signed and at the leaf level it's just going to be the one. And the key that was used to sign public key that was used to sign is g raised to the specific secret key and we can do this. And once you have mapped this value, you can perform the reduce operation at the successive level. So in this case if you look at this specific example so since two keys have signed under the subtree, so the count is going to be two and the public key that is going to be there is going to be the product of these two keys. And this is not proof at each level which will attest to the fact that this computation was done correctly. And you can do this all the way to the top. You can keep on aggregating values at each level of the tree and in the end you just have three people who have signed.
04:00:53.576 - 04:01:36.060, Speaker E: So the count is going to be three and the product of these three public keys. And it comes with a snat proof to say that everything was fine and everything was done correctly. So whenever a specific position in the vector changes, we can update it by rerunning the path from the leaf to the root. This way we don't have to recompute the entire proof from scratch. All you have to do is just perform login size updates to update any change that happens to this data. Another application is what we call as digest translation. Say that there are N values in a vector and you compute the merkel digest using hash function one.
04:01:36.060 - 04:02:35.836, Speaker E: For example, it could be Sha two, five, six and you compute the merkle touches c two using cosyn based hash function. And we want a proof which essentially says that c one and c two were computed over the same set of values. Such proofs are useful in roll ups where it's difficult to prove membership over an expensive hash function like Sha. However, you can simply attach a proof saying that hey, here is the snack proof which essentially says that these values correspond to the same set of n values that were committed in the vector. So the exact statement which we are trying to prove here is mercury digest c one and c two correspond to computed using hash functions, functions h one and h two respectively. And these were computed over the scene and values. So just like the previous BLS key approach, you can first compute the merkle touches using the hash function h one.
04:02:35.836 - 04:03:31.676, Speaker E: And the map operation in this case is going to just be an identity operator, it's just going to return the same value. And at each level you can compute the you have the result of the hash computed using the hash function h two. And there's a snark proof that attests to this correct computation and you can have this all the way until the top. And essentially pi would show as the proof that these were computed over the same set of values, but two different hash values, two different hash functions were used. So we performed a preliminary implementation of our approach using plonky two. So recall that the tree like structure of our approach makes it easy to parallelize. And we notice that our distributed version can be up to ten x faster than the sequential version of our rec proofs.
04:03:31.676 - 04:04:23.810, Speaker E: The proof size is 45 KB. However, our aggregation is slower than standard merkel snarks, where by Merkel snarks I mean that you use Graph 16 and posted and hash function to prove that there's a set of elements that went inside the batch. Our verification is up to 40 x faster than this approach, however, and we can update a single Merkel proof, the batch proof, in 35 seconds, and it is 43 times faster than Merkel Snarks, and it can be up to 135 times faster than Merckle Snarks. Whenever there is a change in the size of the batch, that triggers a new setup and a new circuit and so on. So here is the link to our paper. With this, I'll stop and take questions.
04:04:27.530 - 04:05:48.930, Speaker A: Hey, Srivan, thank you very much for very well explained rec proof that you guys have published recently. So something that I also wanted to kind of touch upon, as you were kind of explaining how the rec proof works and how they kind of combine the leaf nodes and then sort of perform a snark on top. Of it. And then you kind of stack it up into an entire merkle tree with, at the end, a single snark that represents the entire batch. One thing I was kind of curious was like if I wanted to, let's say, reduce the amount of recompute that I want to do whenever I'm updating the batch, right? Let's say the BLS key example that you mentioned. Whenever there's a new BLS signatures that is coming in or that the Validators have changed, that some disappeared and some popped up, for example, there's always some, I guess, a minimum bound of recompute that I have to do. Because if you follow back the trees, it's like, oh, I have to recompute this stock, then I have to recompute that stock, and so forth.
04:05:48.930 - 04:06:13.734, Speaker A: I was thinking then, in that case, would it make sense, tell me if this is some weird idea, but would it make sense to have a tree that is non binary, meaning that you could have maybe more than two nodes within following the one node? Wouldn't that kind of saves a bit more extra?
04:06:13.852 - 04:06:53.622, Speaker E: Yeah. We are in fact working on the generalized version of this. We are working on a KRE tree, MPTS and other variants of this tree, and we are trying to get a good sense of how the performance compares. So, regarding the stuff you mentioned, about multiple updates coming in. You don't have to do each update sequentially. You can put all the updates together and you can just perform the start computation once you have you can do multiple updates simultaneously is the point I'm trying to make. You can also think about doing this for different branching factor and so on.
04:06:53.622 - 04:06:56.840, Speaker E: So that's something which we are currently working on right now.
04:06:57.530 - 04:07:03.462, Speaker A: And when you say different branching factors, what kind of trade offs do you think you would be making with this direct proof?
04:07:03.606 - 04:07:28.258, Speaker E: So the work at each level could become more because you'll have to keep track of multiple things that are happening. But let's say the tree height could change, right? But at the same time that's more realistic to what people use in the real world, like MPT tries and so on. So that will be closer to that. And yeah, this is how it's going to go.
04:07:28.424 - 04:07:38.322, Speaker A: Got you. And then is it also fair to say that the deeper the tree that you have, the more computer overhead that you may have.
04:07:38.456 - 04:08:13.278, Speaker E: So at each level you also have to do the key thing to notice here is at each level you also have a bunch of Snot proofs coming in. So if your Snot proof verification, recursive verification is a lot, then having a large branching factor has to be carefully thought out because you could have more than two proofs to verify that could come in. And that could be if verifying that snap proof is the primary overhead, then that could dominate the cost of doing it.
04:08:13.444 - 04:08:38.406, Speaker A: Got you. Okay, sounds good. I think that makes a lot of sense. And again, I'm looking forward to seeing your, I guess, explorations on multiple different versions of Merkel trees that you will apply rec proof on top as well. So looking forward to that. Great. I think that's all from the questions.
04:08:38.406 - 04:08:48.634, Speaker A: And again, Shrabhan, thank you very much for the time and also explaining the recproof for us and yeah, looking forward to the future works. Thank you very much.
04:08:48.752 - 04:08:50.586, Speaker E: It was nice talking to you folks. Thank you.
04:08:50.688 - 04:09:56.690, Speaker A: Thank you. I guess just quickly replies to some questions. So yes, we will publish all the recordings for the open talk and then we'll make sure to have share the links accordingly once it's ready as yeah, please keep an eye out for that. So next up, we have Bobbin from Polygon midnight. So I guess just a quick background on that. Polygon has been building a lot of things and one of the big initiatives that they have is building Ckbm. And Bobbin has been one key crucial, I guess player in sort of developing the has been my pleasure to have Bobbin also join the Talk and Talk about, I guess efficient track of Nile fire.
04:09:56.690 - 04:10:00.210, Speaker A: So I guess the stage is yours. Please take it over. Bobbin.
04:10:02.710 - 04:10:22.540, Speaker C: Thank you so much. It's a pleasure to be here and really appreciate the opportunity to talk about these interesting things. And I think the event is overall is great. I've listened to a few talks. I'm looking forward to listening to some that I've missed as well. But yeah, can everybody see my screen?
04:10:25.470 - 04:10:28.060, Speaker A: Yes, I can see your screen. I can see your screen.
04:10:29.230 - 04:10:32.426, Speaker C: If I go into slide presentation mode. Is it still good?
04:10:32.528 - 04:10:33.818, Speaker A: Yeah, it looks good.
04:10:33.984 - 04:11:16.790, Speaker C: Awesome. All right, so today my talk is going to be about this efficient tracking of nullifier sets. And as part of that, I'm going to give a kind of a brief introduction. Very high level, probably missing a lot of details, but what are the nullifiers and why do we even need them? What are the problems that arise from kind of trying to use them? How we solve these problems in polymer mitin. And then I'll kind of give a very cursory overview of how other people are thinking about solving the same problems. So let's dive into this. So why do we need nullifiers? So, in one sentence, the nullifiers are needed to break linkability between senders and receivers and privacy preserving blockchains.
04:11:16.790 - 04:11:25.674, Speaker C: So the idea is that in traditional blockchain or any public blockchain, when you send something to somebody, everybody can observe.
04:11:25.722 - 04:11:26.320, Speaker A: That.
04:11:28.290 - 04:11:48.910, Speaker C: The transaction went through and who was the sender and who's the recipient in privacy preserving blockchains. For example, like Zcash is one of the first that pioneered this, or maybe even the first one that pioneered this. We want to kind of break this linkability, like knowing who or you shouldn't.
04:11:48.990 - 04:11:50.838, Speaker G: Be able to know who sent the.
04:11:50.844 - 04:12:27.986, Speaker C: Funds to whom, or who received funds from whom and so forth. So nullifiers is a tool that is used to do this and how they work. So in the context of nullifiers, we usually think about like a utxob system or something that is close to UTXO. So Maiden, for example, is a hybrid between account and UTXO. Zcash as an example, is a utxob system. But there you have this nodes that people send to each other and you can think of node as kind of like something that carries assets and has some spend conditions associated with that. And to break linkability, there are two things that we can kind of reduce a node to.
04:12:27.986 - 04:13:02.502, Speaker C: The first one is a node commitment, which usually is like a hash of a node in some ways. And then there is a nullifier that basically also needs to be derived deterministically from a node. And it could be a different way to cache it or it could be derived in different ways. But basically it's a way to get kind of a blinded commitment to a node and then this gets recorded. And there are different ways to do it, but one of the ways to do it is to record them in different databases. So for example, whenever a node gets created, it gets recorded in this node database. This is how we do it in polygonmyland.
04:13:02.502 - 04:13:55.260, Speaker C: And then whenever a node is consumed, a nullifier for that node gets recorded in a nullifier database. So node database basically contains commitments to all the nodes that have been ever created. Nullifier database contains all the nullifiers of the nodes that have been consumed. And then the crucial property is that if we know a node commitment to a node, we should not be able to determine the nodes nullifier. So like if we know that something has been recorded in no database that should give us no information about a nullifier that this node would be reduced to. Now, the way this is used when, let's say we want to consume a node and there are two parts to this, one part is done by the user when they generate a CKP locally and send it to the network, and the other part is done by the network. And again, there are different ways to do this, so this is just one of those ways.
04:13:55.260 - 04:15:02.286, Speaker C: So the first one, let's say we start with a node and there's a node database. So the user, when they run a transaction or prove a transaction, like generate a ZKP. So as part of the ZKP that they would compute the node commitment, they would verify that this node commitment is present in a node database and node database could be represented by various data structures in midn, we use mountain range to do that. So you can think about node databases being merkel mountain range that the user has access to. And then they would kind of internally within, when they verify or prove a transaction, they would verify that a node is part of this merkel margin range and that proves that the node that they're trying to spend has been previously created. And then the other thing that they would do is that they would compute a nullifier and this nullifier would be the output of the circuit or of the GKP that gets generated or that gets generated. And one important property here is that the node database is a public input.
04:15:02.286 - 04:15:45.526, Speaker C: So everybody knows the state of the node database, but a node itself is a private input. So by the time we get to the nullifier, like whoever gamifies the ZKP, they just know that there is some node that existed in the node database that reduces to this nullifier, but they learn nothing else basically. And then what the network does is it takes the nullifier database as an input and then this nullifier that came from some GTP that the user generated and then they need to verify that this nullifier is not in the nullifier database and then they also need to insert this nullifier into nullifier database. So the next time this prevents double spend, basically you won't be able to spend a node with the same nullifier.
04:15:45.578 - 04:15:46.290, Speaker A: Twice.
04:15:48.630 - 04:16:20.970, Speaker C: Because the database gets updated. One of the outputs is this new nullifier database. As part of this. So this is just part of what's happening in transaction. This is by no means, there is a bunch of other things that need to happen, but this is what is related to the nullifiers. So like user knows what the node, all the details of a node, they can build a nullifier to it without telling anybody which node the nullifier came from. And then the network verifies that nullifier hasn't been seen before and tracks it going forward so that a node cannot be double spent.
04:16:20.970 - 04:17:12.026, Speaker C: So this works well and it's fine, but it does lead actually here's both the nullifier and nullify database are public inputs. So as I mentioned, this works fine, but it leads to one big problem that the nullifier set is infinitely growing. And the reason for this is that to make sure that we kind of verify that a nullifier has not been seen previously, we need to have access to all nullifiers that have ever been created. In some context, this is not an issue. Like if your network is running at maybe under 100 TPS, that's probably not too big of a deal. The nullifiers are usually like 32 bytes could be compressed further, or in some constructions that could take slightly more. But the nullifiers are not huge, they're fairly small.
04:17:12.026 - 04:18:03.030, Speaker C: For the purposes of this graphic here, I use 32 bytes. And so if you're only running at 100 TPS or less, this is not really a concern. Your state will not increase. But if you want to have a really high throughput network where you're at one KTPS or even at ten KTPS, the notifier set grows to very large size. So like an example here, after ten years you are at ten or 100 terabytes if you do ten KTPS on average. And this basically means every year you add ten terabytes to the state, which is a lot. Now, this is one of the main problems that we want to solve, so let's talk about how we try to do it in polygonmidon.
04:18:03.030 - 04:18:55.126, Speaker C: So in Midn, actually, to take a step back, we want to solve for a few different things. The first thing we want to solve for is we want to have nullifier set being tracked by authenticated data structure so we can have non membership proofs. And this is important because for Midn we're in l two and we need to be able to prove updates to the nullifier set to l one as well. And one of the ways to do it is if you have this non membership proofs, like if it's an authenticated structure and when you modify it, there is a way to verify that the set of updates resulted in a new state of a nullified database. And in general it's useful for a number of other use cases as well. The second thing we want to have is constant nullifier database size at a given TPS. So one of the things that we don't like it's a slightly lacked condition.
04:18:55.126 - 04:19:01.902, Speaker C: We don't want to have like a fully constant sized nullifier set because you could try to do it, but it.
04:19:01.956 - 04:19:03.322, Speaker D: Leads to some number of complications.
04:19:03.386 - 04:19:40.886, Speaker C: It also limits your anonymity set somewhat, but what we want to do is for a given TPS. So if you're running at, let's say, one k TPS, you'll have a content size nullifier set, it's not going to grow with time every year. There's not going to be additional nullifiers added to it, or there will be, but the size will not grow. So that's one of the goals. But if the GPS increases, so your nullifier set will increase as well. So that's a slightly relaxed condition and I think it's a good trade off to live with. The other thing we want to avoid is that we don't want to place any time limits on spending notes.
04:19:40.886 - 04:20:21.490, Speaker C: So we don't want to say, like, if you didn't spend your note in the last year or last month or something like that, it becomes unspendable. So regardless of when the note was generated, you should be able to spend it. We don't want to sacrifice on that. And then what we want to also ensure is that there is minimal amount of work that the user needs to do to keep track and make sure that their notes become spendable, even if they are old and things like that. As an aside, in midn design, nodes are generally meant to be like a short lived object. So they're expected to be spent relatively quickly, like maybe within days, weeks, or even months of the time when they're created. And this is because we have this hybrid structure where you have accounts.
04:20:21.490 - 04:21:04.546, Speaker C: And nodes are really not meant as a long term storage. They're meant to just carry assets between accounts. But on the off chance that somebody doesn't want to consume a node or is not able to consume a node for a long period of time, we do want to make sure that this is first of all, possible. And second of all, it requires the least amount of work that we can kind of like the less we can require users to do, the better. All right, so let's first look at one potential construction. So one is a sparse merkel tree, and this could be a compact sparse merkel tree of depth 256. The way we'll track nullifiers in this is a nullifier value, and let's think of it as 32 bytes defines a path in the tree.
04:21:04.546 - 04:21:39.346, Speaker C: So if it starts with zero, we go to the node on the left. If it starts with one, we go to the node on the right. And then we traverse kind of like the path all the way down to that leaf. And then we get to the leaf identifying a specific nullifier. And then the value in the node will indicate whether a nullifier is spent or not. So zero means the nullifier is not spent and one means the nullifier is spent. So if I go back to the previous slide, this actually addresses a lot of these things.
04:21:39.346 - 04:22:30.766, Speaker C: So it's an authenticated structure, it has no limits on when the nodes could be spent and the work that the users need to do are basically the minimal possible. But it does have one big problem, is that the size of this merkel tree, sparse merkle tree grows constantly. And there are some optimizations that we can do to like we wouldn't use the actual sparse merkel tree, we would use a compact sparse merkel tree to make this a bit more space efficient and there are other ways to compress things. But overall the size of this data structure would grow indefinitely. And this is the problem that we're trying to avoid. So our solution to this is that we'll have not a single nullifier tree, but we'll have a sequence of these nullifier trees and we will break them into epoch. So you can think about many trees being created over time and we would say that the last tree will call it a current epoch.
04:22:30.766 - 04:23:46.226, Speaker C: And then we'll also have this notion of the last two nullifier trees as being in the current window. And then again, the idea is that as one tree gets filled up, the next tree gets created and I'll define what filled up means in this case and so on and so forth. So what are the key properties of this new data structure? The first one is that we only insert nullifiers in the tree of the current epoch. All the historical trees, like once they kind of become historical, once the epoch rotates, these trees are no longer updated, they become static. And then the nodes in the network only keep track of the last two epochs, which I define as a current window. So as soon as the epoch becomes more than two epochs old, as the tree becomes more than two epochs old, we discard the tree or the nodes discard the tree and they don't need to keep track of that anymore. Now it does or the other one is like epochs last for a fixed duration of time and there is no specific kind of duration that we've picked right now yet, but it could be under order of months.
04:23:46.226 - 04:24:19.438, Speaker C: So like six months could be a good time frame for an epoch. So if you think about it, a node needs to keep track about a year's worth of nullifiers with less. If we make an epoch shorter, it would be let's say three months. Then the node needs to keep track of just half a year worth of nullifiers and so on and so forth. So it does introduce some new requirements. So the first requirement is that for every node we need to assign kind of or be able to at least determine the epoch in which that node has been created. So we'll call this creation time NC and each node.
04:24:19.438 - 04:25:25.530, Speaker C: It's actually not too difficult to do this and doesn't sacrifice much. And then the second thing that is kind of more difficult and more problematic is that we need to verify that node nullifier is not present in any of the nullifier trees with epoch that is greater than NC and greater than or equal to NC. So basically previously we only needed to check one nullifier tree. Now we need to check multiple nullifier trees. Now, if we go back to the original picture of how this works, there are some changes that we need to introduce here and I highlight those changes in bold. So first, if the nullifier has been spent or if the node has been created a long time ago, we also need to include past nullifier proof. So in addition to computing the node commitment, verifying that it's in a node database and computing the nullifier, we also need to verify that this node has not been part of any of the nullifier trees from the past nullifiers that are not part of the current window.
04:25:25.530 - 04:26:04.742, Speaker C: We still get nullifier here and in this case the past nullifier proofs could be private input. They don't need to be public input. So the user just has those merkel paths. In this case, the proofs would be merkel paths for the sparse merkel trees. So the user just has those paths and they can provide them as private input into the circuit. And the ZKP will verify that for anything that is outside of the current window, we've proved that this node hasn't been part of that nullifier trees. And then what needs to be done on the node or on the network site is there's very slight changes.
04:26:04.742 - 04:26:56.370, Speaker C: So the first thing we need to verify the nullifier that we receive from the user is not part of the nullifier trees in the current window. So the two nullifier trees and then we just need to update the nullifier tree in the current epoch. So these are the main changes and then again we get the new nullifier database. As before, these two inputs into what network needs to do are fully public. Now, what impact does it have for users and what complexities does it create? So first, if a node was created in the last two epochs, there is really no impact for the user. They don't need to do anything extra. And if you take last two epochs as being a year, if you create a node and this node gets consumed within a year, there is really no change for the user.
04:26:56.370 - 04:27:35.294, Speaker C: Nothing else needs to be done. If the node was created earlier, there is a couple of things that you need to do. First, you need to download and store non membership hooks against old nullifier trees. So let's say you have a node that is fairly kind of getting old and new epochs get created. Anytime a new epoch gets created. You need to download a non membership proof for that nullifiers for the nodes that you kind of control and expect to spend in the future. Fortunately, this only needs to be done very rarely.
04:27:35.294 - 04:28:21.138, Speaker C: So once per epoch, so like once every six months you may need to do this and it also can be delegated to someone else to do it. If you are not online, someone else can do it. You would need to give up some privacy against this third party that will kind of keep track of these nullifiers for you because you will tell them effectively which nullifiers you're interested in tracking, but you don't give up anything else. So you give up privacy in terms of for tracking your nullifiers but nothing else. They can spend your funds, they can't kind of do anything else with your wallet, something like that. So there are some trade offs here, but I think they're fairly acceptable. Again, there is only one time that per app that you need to do updates and if you are not sure you'll be online, you can delegate this someone else.
04:28:21.138 - 04:29:20.902, Speaker C: And there is an extra cost for verifying this non membership proofs. It's part of the proofs that you generate locally. So depending on how old your node is, if your node is like two years old will be four extra or I guess two extra mercury pass to verify if it's five years old, there will be like eight extra merkle pass to verify in your kind of transaction proof. But it is not too bad compared to all other things like signature verifications and other things that you need to do as part of the transaction. This is actually not too bad, very small part of the cost and then on the network side, the additional extra costs are very small. So the only extra thing that you need to do is do this extra check, verify this extra mercury path against the second nullifier tree in the current window. And this is sufficient for you to know because you verify the ZKP that the user sends you.
04:29:20.902 - 04:30:33.058, Speaker C: So you know that user checked that or attested to the fact that the node they're spending is not part of any of the previous nullifier trees starting at the time when the node was created. And then you can very easily check that the nullifier is not part of the current window by doing this two mercury path verifications instead of one. And this also very nice property of this setup is that the anemia set that you have is actually it doesn't get reduced. So the full history of your ledger is then an emitter set. So you don't have a fixed size, an image set or like a very limited an image set. So in this setup that an emitter set kind of grows as the size of the network grows because from the standpoint of the network, they don't know if they're verifying a node or kind of processing a node that has been consumed in the last year, in the last five years, in the last ten years, or whenever else. All right, so just to wrap this up and highlight some other approaches that are out there, and I'm not going to go into the detail and this is like some select alternatives.
04:30:33.058 - 04:31:27.846, Speaker C: This is not like by any means a comprehensive set of alternatives. One interesting approach that recently came out from the Triton or Neptune team is this Mutator set. One of the new things there is that using of the sliding window bloom filter for non membership proof. So instead of using sparse merkel trees and kind of this rotating epochs, they're using a sliding window bloom filter, which in some ways is similar to epoch based sparse merkel trees. But also there are some significant differences, especially in how they use a bloom filter. Polygon Zero, previously Mirror had this interesting approach where they use this liveness mask which can be compressed with using modified Kaufman coding. I think the page marks they got there is that each nullifier could be or each active nullifier.
04:31:27.846 - 04:32:07.658, Speaker C: Instant nullifier would require only five bits or something like that of storage. So it was very efficient in that. And then I think Zcash had some proposals of how they could have scalable nullifier sets. And I think the proposal was about kind of sharding nullifier spaces and basically so that not everybody needs to store full nullifier tree. You only need to store parts of the nullifier tree. And that's how you would limit the size of something that a specific node would need to store on their system. So with this yeah, this is my talk.
04:32:07.658 - 04:32:10.140, Speaker C: I think I managed to do it in 20 minutes.
04:32:11.710 - 04:32:39.102, Speaker A: Yeah. Thank you. Thank you, Bobbin. So I guess just on my side, I had a few questions I wanted to kind of ask on the nullifiers. So you kind of defined the epoch as, I believe, half a year, right? I'm just kind of curious what's the considerations behind the epoch's length? And also why do you decide that as a half a year instead of other timelines?
04:32:39.246 - 04:33:24.158, Speaker C: Yeah, so I think there are a couple of trade offs here that it's kind of a balancing act because the bigger the epoch you make, the more nullifiers a given node needs to store. So let's say if we're running at one KTPS, if you make an epoch half a year, then a node needs to store two epochs. That's roughly like close to 1 data that they need to store. And this is a stable 1 TB. Doesn't change with time, but it's still significant. So you may ask, well, do they need to store 1 TB? Can we make an epoch three months for them to store less? And then kind of the other lever that is pulling in the other direction is kind of convenience for the user. The shorter the epoch, the sooner you may need to download those extra proofs.
04:33:24.158 - 04:34:01.870, Speaker C: So if you spend your note in the first two epochs, you don't need to do anything extra, you don't need to worry about anything. But if you go beyond the first two epochs, then there's some extra work that you need to do to download those non membership proofs and keep track of them. So it is kind of annoying. So the question is if there is sufficient infrastructure to make epochs shorter. So on a safe side, I think a year is probably safe, at least in polygonmide and design, where notes are meant to be. As a short lived object, 99.9% of people should spend their notes within a year to move funds into their accounts.
04:34:01.870 - 04:34:32.630, Speaker C: But if you make it like one week, for example, it would be very efficient. From the storage standpoint of the nodes, there is very little data that you need to store, but then it would create more inconvenience for the users where you need to make sure that every two weeks you download an additional storage proof. And not everyone logs in every two weeks and spends things within every two weeks. So it's just like basically a trade off between user convenience and how much data you want to store on your nodes.
04:34:32.790 - 04:35:13.926, Speaker A: I see. And I can also say that let's say the users are not spending anything and just, I guess leaving the funds and also the notes, just like sitting on midn. You did mention, although you said not much cost, but there's some additional cost of proving. Let's say that the nodes were from epochs that were two epochs ago, like more than two epochs ago. I presume that every epochs that progress forward the node have to create a new proofs. Or I'm not sure if it's like recursively, but some sort of new proofs. Right.
04:35:13.926 - 04:35:20.034, Speaker A: So is there like a linearly increasing amount of cost the longer that the node is not touched?
04:35:20.162 - 04:35:56.100, Speaker C: Yeah, for the user the cost would be linearly increasing. But I should say that this cost is fairly small. So the constant size is small. So you could probably not spend your node for ten years and it still should not be significant cost as compared to a lot of things that happen as part of the ZKP circuit. So as I mentioned, for ten years you need to verify extra eight merkel paths, which is if you compare it, it's probably less than 10% of what you would need to do to verify a signature, for example. Actually much less than 10% that you need to do to verify a signature. So it's not a huge amount of cost.
04:35:56.100 - 04:36:23.186, Speaker C: I think the more annoying part is that they need to kind of periodically download those merkel paths, like once every six months. That's the more annoying part. In my view. The cost is there, but it's almost negligible. But the fact that you need to either log in once every six months and download that path or have someone else track this for you and then you kind of give up a little bit of privacy. That's like the most annoying part of this design in my opinion.
04:36:23.318 - 04:36:53.878, Speaker A: Right, and then in that case the users who would have to agree to delegating that kind of like a half a year downloading. But then that things itself is like a bit of a UI UX challenges there. Right? Because then you kind of have to let users know that whether you want to delegate this or not. And if you do delegate then what is the kind of like the privacy aspect that may be a bit compromised in there as well.
04:36:53.964 - 04:37:36.050, Speaker C: Yes, exactly. So that's why you don't want to make the epoch too short because again, if you make it too short these problems become much more exacerbated. I don't know if half a year is the right answer. Maybe it should be a year and then it's something that I think we'll learn again with polygonmiden design specifically we're not expecting that notes should remain in polygonmiden. You have notes and you have accounts and notes are just like I send you a note and then next day you can take the note and move it into your account. So you move funds into an account and at that point you don't need to keep track of that note anymore. But in some cases that may not happen.
04:37:36.200 - 04:38:12.830, Speaker A: Got you something that you haven't really touched upon during the talk. But I was curious to also learn a bit more was about the hardware managements around the node that are I guess hosting those notifier trees. Do you foresee that this would be basically just like a one big machine that is running this or do you foresee that you are also somehow trying to distribute some of the node hosting load across different machines across geographically so.
04:38:12.900 - 04:38:43.286, Speaker C: It doesn't necessarily need to be a big machine. So the issue here is mostly on storage side. So you can have a hard drive with five terabytes right now at the cost, I don't know but it's like a couple of something like that. So it's mostly around maybe I'm underestimating you can get like a 1 TB hard drive for $100. Maybe five terabytes is a bit more expensive than that but it's not like a huge amount of cost. So you don't need tons of CPUs, you don't need anything. It's basically storage.
04:38:43.286 - 04:38:55.754, Speaker C: And I would say if you think at which TPS we are running so if you're running at one K TPS that's when you need like 1 storage.
04:38:55.802 - 04:38:56.400, Speaker G: Right?
04:38:57.330 - 04:39:28.360, Speaker C: I think like Ethereum right now running at I guess like 15 to 20 TPS is already at close to 1 storage that you need. So it's not like hugely different and especially at the level of throughput that you get. So it is not going to be like a raspberry pi probably so unlikely to be a Raspberry Pi that you can run this node on, but it shouldn't be anything more than like a mid to high end laptop that you should be able to run this on.
04:39:29.290 - 04:39:46.430, Speaker A: Gotcha I see. And do you think that you would also prepare some sort of redundancy around the actual hosting of those data? As in you would have maybe some multiple people just hosting in case some liveness?
04:39:48.050 - 04:40:38.320, Speaker C: For sure in a decentralized setting you would have multiple nodes and each of them have copies of this nullifier trees. So that's not something that in all honesty there might be somebody chooses to be an archive node type of thing where they store, they don't even discard the previous nullifier trees and just store them and you can always go to them and ask for this nullifier. So maybe like another solution to that annoying problem that I mentioned is you don't need to download that. Somebody will give you this at extra cost later on because they will just keep those. Storage is not hugely expensive. Even you don't want everybody to have a hard drive with 50 terabytes, but some machines out there could have 50 terabytes available to them that store everything, all the historical modifiers and in that case it solves a lot of problems. So it kind of gives this flexibility there as well.
04:40:38.690 - 04:41:17.414, Speaker A: Gotcha and also in terms of the from the incentive alignments of those people who are running those nodes. Do you foresee that what Ben previously have mentioned that the bringback of the POW type of things or even maybe a parallel would be like something like a filecoin where the filecoins node operators also get some sort of like a file coins as incentive or for them actually running their machines for storing that data. In a similar manner, I think there.
04:41:17.452 - 04:42:12.514, Speaker C: Is no extra like there isn't a lot of extra work that the nodes need to do as part of just running the protocol as intended. So they should be compensated with fees that transactions. If you want to be able to process transactions and get fees for transactions, you need to keep those nullifiers, otherwise you won't be able to process transactions. So you kind of in a way are already compensated for storing those nullifiers with transaction fees. So I don't know if there needs to be an extra mechanism for doing this. I think as I mentioned just like a minute ago, there could be extra incentives for somebody to store even historical data that you are not required to store by the protocol, like something that is outside the last two epochs and then charge extra fees for that. A node could say I'm going to store the last 20 epochs and anybody can come and get the data but you have to pay extra in terms of fees.
04:42:12.514 - 04:42:21.630, Speaker C: So there could be some incentive design something extra to what the base protocol kind of like Envisions to provide that convenience.
04:42:22.290 - 04:42:42.502, Speaker A: Gotcha. Okay, great. Thank you very much, Davin, for the great talks and explanations around the design space of nullifier tracking. And it was very honored to have you on today and looking forward to any new developments from Miten team as well.
04:42:42.636 - 04:42:45.398, Speaker C: Thank you for having me and thank you for great questions.
04:42:45.564 - 04:43:43.706, Speaker A: Thank you. All right, so next up we will have Brian from Risk Zero and I guess a little background on the Risk Zero. So they are building Zkbm, as other projects have mentioned a couple of times. It's a verifiable compute primitives that is built on top of risk Five, and it's also a stock based VM, like Midnight, but they have different use cases and I guess assumptions around the way that people can use those risk Zero VMs as Brian, welcome and feel free to take it over. Glad to be here. Thanks for having me and always great to be able to hear Bob and talk about anything. Always happy to follow him.
04:43:43.706 - 04:44:12.100, Speaker A: Let's see. Can we see everything on the feed? Yeah. Great. Cool. So I'm Brian Redford. I'm the CEO at RISC Zero. And today I want to talk about ZK parallelism, specifically in the context of continuations and some of the work that we have been doing there for people who are unfamiliar with Risk Zero.
04:44:12.100 - 04:45:28.220, Speaker A: The company itself is focused on building general purpose ZK computing technologies. But our sort of main product at this point is the RISC Zero Zkvm and then the Bonsai Proving platform or ZK application development platform that is really focused on making it easy to use ZK proofs. So the Risk Zero Zkvm lets you succinctly prove any computation as opposed to, say, any of these standard proving systems that you have to write circuits for or sort of ZKE EVMs where you actually are proving only the EVM. So there's a lot of different things you can do with that. But the thing we're focused on today is this sort of feature we've added to it called Continuations, which lets you take a proof and split it up into tens, hundreds, thousands, really an arbitrary number of parts and prove them independently. Feature also lets you do some interesting things around, sort of suspending and resuming the images. But we're not really going to get into that today.
04:45:28.220 - 04:46:15.134, Speaker A: So an interesting sort of data point around the usage of the Raceroz UKVM is the Zeth project. And we just open source this. The link should actually be on here, but it's just the resiro GitHub. There's a Zeph project on there. You can check it out. But Zeph is effectively a ZKE EVM that runs on top of our Zkvm and utilizes this continuations feature to prove an EVM block sort of massively in parallel. So the purpose of doing this is to let people effectively build a ZKE EVM without actually needing to do any specific work for the Zkevm.
04:46:15.134 - 04:47:16.096, Speaker A: So this project is. Based almost entirely on pre existing code. We had to modify some of the Revm code to be a bit more serialization friendly for ZK. But yeah, by utilizing continuations and parallel proving, we were able to get proving an entire standard Ethereum block down to about 15 minutes for 4 billion cycles. So as far as we're aware, Rckvm is sort of the first system that's actually able to stitch proofs together in this manner and actually prove an entire regular Ethereum block. This is sort of part of this pattern of enabling limitless parallel ZK computing. How does this work? The risk zero zkvm actually is based on the risk five instruction.
04:47:16.096 - 04:48:29.636, Speaker A: Set, or ZK proves programs that are expressed in RISC Five. So typically this looks like you take Rust, some compiler turns it into Risk Five, and then it gets proven through our Zkvm. Now, in a world of continuations, what this actually looks like is this Executor takes the Risk Five source code and then splits it into a bunch of these sort of separate segments, which are then proven separately and together are then potentially rolled up into a single proof. So the continuation feature is actually something that takes this Risk Five code, simulates it and produces, based on your configuration, a particular set of proofs that prove sort of the state transition between the first part of the code. So this is like segment receipt, one, two, three, et cetera. It's pretty straightforward to sort of think about these things. Then once you run this program through the entire Executor and produce this list of sort of segment proofs, you can imagine that proof getting quite large if you have thousands of segments.
04:48:29.636 - 04:49:58.510, Speaker A: So currently the Bonsai service will let you roll those proofs up utilizing recursion so you can get a single proof and then post this on chain. So if you want to think about how this actually works at a sort of technical level, the Executor, what it's doing is actually simulating the execution of a particular program. So in this case, say we're running the EVM, it's actually taking the block that we want to produce a proof for and then running like the EVM gets compiled down into S Five, and then that block is provided to Zimput along with all the rest data you need. And the Executor takes that program and then produces these sort of traces of execution. So this is basically like this Risk Five instruction was executed and here were the results. And then it also takes and basically tracks which pages of memory have been accessed at any particular point in time and creates a sort of mercalized representation of which pages are dirty. And that sort of memory image and the data of what cycles have been run through is sort of the necessary proof data that you'd need to link these segments up to each other.
04:49:58.510 - 04:51:13.444, Speaker A: So you can imagine that this Executor produces this trace that says here's what actually happened in RISC Five. And then here's all of the memory movement through this memory management unit collapsed onto the smirkle page table. As soon as the executor produces a single segment, you can imagine anyone else can take this sort of data and then actually produce a ZK proof of it. So that's what the sort of offset nature of the visual is intended to describe this notion that the executor work and the prover work are completely separable. So if we look in the context of the Zeph system and think about what parallel proving for this actually looks like sort of a demonstration, I'm moving this direction right now. So proving block is about 4 billion cycles and by default we split programs up into segments of exactly 1 million cycles. That's pretty arbitrary.
04:51:13.444 - 04:52:17.260, Speaker A: And you can imagine that the choice of segment size actually has quite a large impact on performance depending on how a system is configured. We won't get too much into that right now. So this is just a sort of demonstration that as the executor starts chewing on this ETH block, it starts spitting out segments until it produces the sort of full 4000 segments. But you can imagine that as soon as you've produced a single segment, you can send that out to your proving cloud and sort of overlap proving of those segments with the actual generation of the segments themselves. However, if you want to get down to the single proof that you actually want to post on chain somewhere, you also need to take all these sort of initial 4000 proofs and then recursively collapse those down to 2000. Proofs and then 1000 proofs, et cetera until you've gone through sort of full log the number of segments, levels of proving. And at the end of all of this, you have a single proof.
04:52:17.260 - 04:53:50.984, Speaker A: So right now, as I mentioned at the start, it takes about 15 minutes, 15 to 50 minutes, depending on whether you're using 3000 GPUs or 50 GPUs to sort of produce these proofs. Obviously, the faster you can produce these, the better you're going to be and the more use cases you're going to be able to support with these kinds of proofs. So 15 minutes is pretty far away from the sort of goal that Ethereum has of creating enshrined roll ups where you would actually be able to do something like this in 1 second. So when we think about the gap between where we are now at 15 minutes and moving to hopefully 1 second, eventually it's interesting to sort of think about where the different bottlenecks in the system are. So before we dive into that though, I want to talk about how you can sort of naively think of how long it takes to generate a single proof. So you have to generate all the segments first, obviously, and then you have to take those generated segments, distribute them out to all the nodes that are going to prove them. And then they need to prove these initial sort of leaf proofs, which are the proofs of the actual traces, and then you have to produce these join proofs that collapse two proofs into one proof or three proofs into one proof or however many you choose to choose to collapse.
04:53:50.984 - 04:54:55.612, Speaker A: And those are actually quite different. They end up taking quite a bit different amounts of time. On a single machine, the join proof time for two to one is about 6 seconds, and then for proving a million cycles, it could be about 100 seconds, depending on which machine you're using, more or less depending on which GPU you have. So you can imagine the sort of theoretical maximum for an arrangement like this being effectively the time it takes to generate the segments, plus just one sort of segment proof and one joint proof if enough things were overlapped. If the amount of proving time were to be decreased. Or you get enough machines to prove them all such that you can finish all of the proofs directly after the segments are generated and then also roll them up. While you're waiting for that last segment to be generated at the end, you potentially only have to do one proof.
04:54:55.612 - 04:55:55.248, Speaker A: Although the sort of roll up tree in that context looks a bit different. So when we look at a system like this, there are a bunch of different bottlenecks and there's one that I didn't really list on here that we can talk about later. But obviously there's this segment generation or execution which is effectively the simulation of the Risk five chip and then the sort of recording of all of the memory transfers. Then there's taking these kind of segments which are the full sort of memory state of the virtual machine at that point in time, and the execution trace and then distributing those to all of the provers. And then provers actually have to prove things. And then that proof needs to get distributed to a bunch of other machines potentially to be combined down to this sort of single proof. So we see about five main points of potential bottlenecks in the system.
04:55:55.248 - 04:57:33.120, Speaker A: There is a question around sort of execution right now is single threaded, whereas obviously most machines are not single threaded. And there's certainly a possibility to think about splitting up some execution into more threads of execution. And there are a variety of techniques one could use to do that which would help address sort of the execution overhead. So right now, if we look at Zeth and where most of the time is going when we run one of these massive proofs, the execution or sort of witness generation, segment generation is actually the main bottleneck, everything else can kind of be parallelized away. So at this point in time, we sort of have an interpreter for RISC Five, so we're only able to really get up to about 15 to 30 MHz, depending on the sort of circumstance, the exact thing that's being proven anyway and how much memory movement is involved. So when we look to really reduce the amount of time it's taking to build these kind of blocks, that seems to be the number one area for people to focus on, sort of improving the performance of separately, we still see like two to one recursive proving taking about 60 seconds. So right now we're looking at sort of a theoretical maximum of ten minutes without addressing some of these because we can actually get segment proving down to about 30 seconds.
04:57:33.120 - 04:58:56.056, Speaker A: So techniques for improving some of these we can talk about. So this is kind of where the bottlenecks are in the system currently, but you can imagine as some of these get addressed, the bottlenecks will move to different parts of the system. So over the medium term, once we do some of the things we're going to do to address segment generation time, you can imagine that potentially, especially if you're thinking about doing this on an ASIC or something, that distributing all of the data necessary to prove the segments might start to become the main bottleneck. And then you have to explore mechanisms for compressing that or reducing that, or thinking more intelligently about how you distribute these things at different points in the proof roll up process. And then obviously just the overhead approving a particular segment and this prover time itself will start to dominate again once you kind of get rid of the recursive overhead and this execution overhead long term, I do think we can imagine getting to a world where if we get to 150 MHz, we're talking about 36 seconds and 1.5 sorry, 1.5 GHz, we're looking at 3 seconds.
04:58:56.056 - 05:00:15.700, Speaker A: But that's still kind of far away from the sort of goal of 1 second. So you can imagine also getting from 115, like 1.5 or multiple gigahertz is going to be quite the journey. But until these other things are addressed, it doesn't really make sense to really focus on this. But long term execution or the intelligence with which we're able to split various work streams up into chunks is going to really be the sort of bottleneck, I think. And you can imagine at a certain point as well, if you can get recursive proving itself to be extremely fast, which seems totally doable with ASIC techniques, then eventually you might end up in a place where the bandwidth limitations, especially in a decentralized context, become more of a concern, which might lead one to want to research potential mechanisms for compressing some of these proofs. So when we think about eliminating some of these bottlenecks in the system, moving forward, there are a bunch of different sort of techniques you could employ.
05:00:15.700 - 05:02:04.088, Speaker A: So for execution, which is at this point, the sort of major bottleneck, rather than just simulating the execution of Risk five, you could imagine actually taking the risk five instruction set and then getting a native sort of set of code that would build that would actually execute the Risk Five instructions and sort of build the merkel tree as you go. There's a lot of work necessary to build a system like this and maybe that's going to get you sort of ten X. But because the risk zero Zkvm is based on risk five, you can also imagine building something that increasingly looks like hardware that is specifically focused on sort of witness generation or generating these segments. So we do expect to see in the medium term some utility of FPGAs just for this part. So I haven't seen a lot of people talk about breaking down proving systems, this problem of generating proofs into sort of phases. When thinking about hardware, you tend to see people trying to tackle sort of the entire problem at once. However, I think taking a risk five sort of core that's already designed and figuring out how to modify that to be able to spit out these segments would be a really interesting research problem that has the potential to massively speed up these kinds of very parallel proofs and then obviously you can take that a step farther and then think about actually building an ASIC that is very much focused on segment generation and that could obviously be joined to hardware that's doing proving as well.
05:02:04.088 - 05:03:28.100, Speaker A: But I actually think it's really interesting to think of these problems as mostly separable. So in terms of segment distribution and generation right now when we think about the way these things sort of get merklized is pretty inefficient with our current approach. We haven't really tried to optimize it, but there's a million techniques you can sort of imagine for compressing memory or becoming more efficient about which parts of prior memory that were touched you pass forward as you go through all of these segments. Additionally, you could think of becoming a lot smarter about how big these sort of segments are and start to think about potentially having dynamic segment size. Obviously, the fewer segments and the more compressed they are, the less data you need to spread out over the network to later prove to some extent. Also you can imagine sort of changing the topology of the tree roll up to not necessarily just be a strict two to one tree roll up in terms of actually improving the sort of segment, improving time itself. Obviously we're trying to stay abreast of all the developments and all of the proof systems and we'll continue to keep the sort of risk five Zkvm at the bleeding edge of performance.
05:03:28.100 - 05:05:16.790, Speaker A: But there's also the ability to run this existing code on a variety of hardware and we're actually partnering with a couple of teams to work on building these more efficient hals which will help accelerate the actual proving itself. However, as we've noted, it's not really a bottleneck yet, at least for sort of the zep scenario, we can basically prove all 3000 or 4000 segments in 30 seconds right now over time. Also, this is a Stark based proving system, so hash performance is a major component of the entire system, especially the sort of recursive proof generation. So you can imagine advances in hash functions and we've seen this with poseidon two and five, but there's probably room to go in terms of producing even more custom hash functions for systems particularly like this. Additionally, we currently have a number of accelerator circuits like Shaw 256 and ECDSA or just big integer support in general. You can imagine for particular problems, making it easier for people to build these acceleration circuits and incorporate them into the overall system will be a sort of big unlock for some sets of problems for recursive proving and speeding it up. You can imagine not just doing two to one sort of recursive collapse, but picking that number based on the particular characteristics of the sets of hardware that are available to your proving cloud.
05:05:16.790 - 05:06:39.920, Speaker A: And then one could imagine using some of the flexibility that exists inside the Stark based systems to trade off performance time for size. In the long term, if you ever get to the place where these sort of proof sizes are actually causing problems. But in any kind of parallel proofing system, you're going to be constantly running into sort of new bottlenecks along these kinds of lines. So, yeah, what we're doing at risk zero and what we would potentially like people's collaboration on, if you're interested, we're building out this next version of the proof system that will make it a lot easier to incorporate new accelerators and potentially different VMs and have them all kind of work together to prove a particular computation. And you can also imagine sort of parallel proving as being part of that. And we'll be releasing that later this year along with this Zergen circuit construction or accelerator construction toolkit. I'm not sure exactly when that will get released, but hard at work building new versions of the circuit that utilize this technology.
05:06:39.920 - 05:07:40.802, Speaker A: Yeah. And then all of the sort of data and diagrams that I've presented here are all this sort of AWS, centralized bonsai system which will continue to develop. But we're also very much interested in and starting to work on the decentralized version of that which is going to inevitably have a different shape and probably push some of these trade offs in different directions. And then obviously we're hoping to see more and more projects start to seriously consider Asics and Nfpgas specifically for the sort of witness generation or execution generation part of the system. Yeah, so it's kind of a speed run through the sort of massive parallel system that parallel proving system that is Bonsai. That's actually all I have. All right, thank you.
05:07:40.802 - 05:08:53.750, Speaker A: Thank you Brian, for a very thorough overview and I guess a bit of details and a lot of the continuations and segmentation stuff that you guys are working on, I guess just to kind of dive into some questions. So one thing that I was noticed, you mentioned that you want more efficient house, I believe house. You're referring to the hardware abstraction layer, right? Yeah. When you say more efficient house, if you can elaborate a little bit on, first, what kind of house that you have and then second, what's the ideal types of improvements that you want to gain from more efficient halves. So currently we have a CPU howl and then we have a metal howl and a CUDA howl. The CUDA howl in particular requires a lot of effort to really optimize all the various bits and pieces of the sort of fry and overall stark algorithm. So we are working with Supra National right now on a separate CUDA implementation that has a much more optimized.
05:08:53.750 - 05:10:05.166, Speaker A: Also, I think it would be really excellent to see people build out sort of Vulcan hals and then potentially hals that target FPGAs or hardware specific to mobile phones. You have a lot of really interesting kind of acceleration targets on mobile phones and I think pushing proof generation out to the edge over time is really interesting. I don't know if people are ever going to want their mobile phones to participate in parallel proofing networks, but I guess if you can earn tokens while you sleep, while your phone's recharging, maybe yeah, that's a different world. Maybe. Got you. And then also right now, I'm not sure how much details it would kind of get into it, but for the segment generations and also actually defining where you set the bound for each segment. Because I know for a fact that when you sort of set how much segment you want, the unit of it is based on the cycles of Compute, I believe.
05:10:05.166 - 05:11:02.830, Speaker A: Right, so it's not like time or anything, just like cycles of compute. But on what standard are you saying, okay, this is the amount of cycles that I want to have per segment. Is it based on the amount of, I don't know, the states that can be stored on Ram or what's the yeah, so the 1 million cycles was we just kind of picked it because it did seem like a sweet spot for a lot of hardware. Specifically proving 1 million cycles only requires about slightly under 16 gigs of VRAM. I see. This actually produces a proof system or a parallel proofing system that requires consumer grade or prosumer grade GPUs. However, depending on the network topology and who's participating in it and whether they're approving it on their mobile phones or what year it is, you can imagine wanting that point to shift.
05:11:02.830 - 05:11:46.480, Speaker A: Got you. That makes sense. So basically the idea is that the more cycles you, I guess, pack into the segments, the bigger hardware requirements that you'll be requiring for those parallel approvers. Basically, yeah, I see the memory. If you try to approve the sort of full two to the 25th cycles that we can fit in one segment, the memory requirements can balloon up to like 200 gigs. Got you. Actually, another thing I wanted to ask was we talked a lot about continuation here and I think the audience would know what continuation is going to be like and what it may look like in the future.
05:11:46.480 - 05:13:06.730, Speaker A: But one thing that Brandon from One Labs earlier today mentioned that kind of was interesting was that he claimed that there's no ZK VM in the space that is performing like recursive proof. Okay. Yeah. And I think the reason currently our source code for this is not completely open source, but it definitely works, right? Yeah, I think the reason why he said that is because he kind of treated the continuation work to be more like a partitioning of the work and then kind of like recursively combining all those proofs together. I think maybe that's kind of like his intention where he said that no Zhevian is currently doing the recursion of the actual computer or something. But I guess recursion can exist in many different ways. So in the future, how do you foresee continuations? I guess, one, have the recursions in what forms? And two, if it would be just like sort of like a mercalized proof that just sits on top of continuations, which I believe is what we have today, or if it's going to change in some shape or form going forward.
05:13:06.730 - 05:14:05.130, Speaker A: Yeah, I think whether it changes form moving forward is going to probably have a lot to do with how research progresses in terms of folding and maybe some other techniques. Although at least from our perspective, it does seem like recursion because you're utilizing the sort of succinctness property of Ztape systems at every step, it really does help address some of these sort of communication overhead issues. Right. Like at every sort of step of collapsing this tree, you're really only needing to pass a 200 kilobyte proof. So that's pretty appealing. Whereas I think in the sort of folding world you might end up shuttling along a lot more witness data as things sort of accumulate. But yeah, that's the only major kind of change I would expect to see, I think, other than the recursion system being fully open source.
05:14:05.130 - 05:16:58.334, Speaker A: Got you. Since you mentioned about voting, what's your current sort of prospect around actual practical usage of voting within this kind of exekvms? And what's sort of like the bottleneck that you see imminently with using, let's say, some variations of folding schemes that we have in the space? Yeah, I mean, obviously highly folding scheme specific and this is definitely more of a Jeremy question than a me question, but currently it seems like the ability to actually accumulate data and share it around in a way that lets you prove across 3000 nodes seems like it's probably going to be a bottleneck for a reasonable number of systems. It's true that right now it takes 5 seconds to do this sort of two to one recursion, but there's pretty clearly some paths to make that a lot better, somewhere between somewhat better and much, much better. You could also imagine potentially creating an ASIC that's very tuned to sort of recursion as well, rather than trying to also do the sort of broad set of segment beef proofing because that's very amenable to GPUs and GPU performance will continue to increase. But this recursion sort of the join system and predicates are very constrained. Gotcha, regarding the FGA and ASIC I think you also mentioned about the witness ASIC for segmentation work, what's your sort of general stance around usage of FPGA, usage of ASIC under different circumstances, whether it be for segmentations or whether before actually proof generations? Yeah, I think for proof generation, I was very bearish on this in the past, but now that we're seeing systems like continuations, the ability to drive memory usage down a lot, you're going to be able to see some focused asics actually utilize HPM in productive ways to really accelerate NTTS and some of the polynomial evaluation. I'm really, I guess personally interested in and bullish in this sort of witness generation or executor executor kind of asics because well, you can utilize the fact this is already a risk five system to kind of instrument an existing kind of Risk five chip and it seems likely that you're going to be able to produce something that looks like sort of RISC five performant part that actually generates these kind of segments as it goes along and computes.
05:16:58.334 - 05:18:05.978, Speaker A: But there's a ton of research we have to do there or somebody has to do it's more intuition and sort of sort of follows from looking at the problem. I see. And I guess another potential extension of research that could come out of that is perhaps we would love to hear your thoughts on this too, but how executor could be performed in a more paralyzed fashion. Yeah, exactly. I imagine there's nothing really stopping you from having actual threading models other than it's quite complex to do this, but you could make a multithreaded executor that actually generates segments on a per thread basis and you have to deal with synchronization. So you almost certainly want that to be on one chip or one system that's very low latency links between it. But you also for a lot of use cases don't really need to go full threading and you can just think about graph computational models instead, which that you can kind of almost do at a higher level.
05:18:05.978 - 05:19:11.918, Speaker A: So there is a question as to how low level you really want to get with parallelism. And I think that's going to be informed by how fast people can get a single thread to go, honestly. Got, you also, do you foresee that the entity who is running or let's say that the machine who's running the executor would necessarily be separate from those who are running the provers. Not necessarily because it's single threaded. You can imagine that there's not necessarily a reason to colocate it with provers because if you want to take a single thread of computation and spread that out over 5000 nodes or something like this, you probably don't need to have the executor hardware in every single of one of the proving nodes. So just in terms of raw integer operations, the proving still very much dominates the sort of set of operations. It's just so embarrassingly parallel that you can use tons of hardware for it.
05:19:11.918 - 05:20:56.378, Speaker A: But you could imagine trying to build a hardware that can handle parts of the execution and generate some of the initial proofs or, I don't know, otherwise reduce the sort of amount of data that needs to come off the chip in order to do these recursive proofs. You also get privacy sort of as soon as you go through the ZK proving process. Once you own ZK, if you're just doing the sort of witness generation and executor and building these segments and spreading them out everywhere, it's very obvious to all the parts that are proving these leaf nodes what's being proven. I see one parallel I wanted to roll here that also love to hear your thoughts on is the fact that executor kind of looks like sequencer in terms of sure. Right. Which then also comes to another question, which is like what is the trust assumptions around the executor? For example, could an executor be, let's say, performing some malicious chopping of the compute slots such that some provers would always get a disadvantage in some way while the others always have the advantage? And if, let's say, the provers are some sort of like a reputation based or have some performance metrics to analyze which provers is doing the good job, which proverbs doing bad jobs, then I wonder if there's some trust assumptions around how fair that the executor can actually perform those. Really think so, because it's not like you can reorder the transactions.
05:20:56.378 - 05:21:42.634, Speaker A: So yes, it is a sequencer, but the order very clearly proceeds from the program and the inputs. There's not really sources of nondeterminism there or there's not really a valid way to run a program. I guess depending on how exactly the code gets compiled, you might get different results. But if you have the same binary, you should be getting the same results no matter what. And there's not really, I guess if you start to have variables that the executor can choose from, like what size chunks it wants to produce. Right. You could imagine somebody who's cornered the market on execution being able to tune the segments they produce to favor their provers that they also produce.
05:21:42.634 - 05:22:22.954, Speaker A: Right. There is some potential for collusion there, I guess. Yeah, because I was thinking like, oh, what if each prover gets the same amount of. Rewards but depending on the cycles of computes that they prove for they can reduce the compute load and maximize the reward for specific approvers while others maybe they would overload it so that even though they get the same reward they will have to do more compute or something. Obviously highly dependent on how you end up splitting up the proofs, I guess. Exactly. A fun problem to dive into over the next couple of years for sure.
05:22:22.954 - 05:22:52.078, Speaker A: Definitely a lot more. I guess research works to be looking ahead on that as well. Interesting. All right, again, thank you very much Brian, for the great talk about the segment generations and continuations and looking forward to the developments from the risk zero size and also any future research that may come out of it as well. Great, thanks for having me on and organizing. Thank you. Thank you very much.
05:22:52.078 - 05:23:40.240, Speaker A: So next up we have Toguru from scroll team and to kind of just briefly go over. So scroll is a ZK EVM that is also very much up and coming. And then Togru has been one of the passionate advocates for scroll as well as passionate advocate for the definition of roll ups in general and I'm not sure if you're going to talk about it today. I believe not so much, but he's been very active on that front. Toguru well, stage is yours, feel free to take it over. Hello. Yeah, thank you.
05:23:40.240 - 05:25:21.004, Speaker A: Let me just share the screen before I think Toguru might have just rot. We'll just wait for him a I think he should be rejoining. Oh, I think he's hello. Hello toguru you're back. Yeah, sorry about that. My laptop decided to restart Zoom at the worst moment. Yeah, no worries.
05:25:21.004 - 05:25:48.694, Speaker A: 1 second, let me share the screen. Yep. I can see. Yep. Also let me turn on my camera so you can see my tired face. Yeah. Hello, nice.
05:25:48.694 - 05:26:02.582, Speaker A: I see you man. Hello. Hello, long time no see. Hello. Yeah, can you see my yes, yes, perfect. Hello everyone. My name is Togrel, I do research at scroll.
05:26:02.582 - 05:27:16.254, Speaker A: I mostly focus on the protocol side of things so I work on the bridge design, protocol, decentralization, et cetera, et cetera. And today I'm going to be introducing and overviewing roll ups, provers and zero knowledge roll ups. So you probably have heard about roll ups, there's been a lot of talk about them in the last few years and there are two types of roll ups optimistic roll ups and zero knowledge roll ups. Or I'm kind of forced to say it because the stockware folk don't really like it when we refer to them as zero knowledge roll ups, validity roll ups. So the main difference between the two is in how the validities of the protocol is being enforced. So optimistic roll ups utilize fraud proofs. So you have a sequencer, it proposes ordering an ordering, then the ordering is published on the base layer, then the executor fetches the ordering data, it published the state route, and then if the state route is invalid, so the challenger fetches an ordering and the state route.
05:27:16.254 - 05:28:21.794, Speaker A: And then in case the state route that was committed by the executor is invalid, it just challenges the state route. And there are two ways to challenge the state route. One is a non interactive single route challenge. So essentially, you just replay the entire transaction or a subsection of the transaction from the start to end on the base layer, which is not ideal if you are not using P two P proofs, because essentially you're limited in how much gas your transaction can consume. This was originally used by optimism, but then they realized at some point that actually that's not really viable for complex transactions. And so they switched it out for Canon, which is another type of fraud proof, which is called an interactive multi round fraud proof. In this case, instead of just the challenger re executing the transaction, the challenger and the assertor, which in most cases is the executor.
05:28:21.794 - 05:29:25.450, Speaker A: But anybody can take the position of the executor, go back and forth, dissecting the state to the point where they get to just one opcode that they have to execute on chain. This is not ideal from the perspective of efficiency because you have to go back and forth. So the time consumed in the worst case scenario with their censorship is longer than just re executing the entire transaction. But the benefit of it is it's much cheaper to execute on chain. And on top of that, it's also possible that your transaction cumulative gas consumption of your transaction can be drastically higher than the block gas limit of the L one, which means that you can essentially challenge anything. Trevorly on the L one and zero knowledge roll ups utilize surprise, surprise zero knowledge proofs. So you can see there's some proposed preliminary ordering.
05:29:25.450 - 05:30:07.094, Speaker A: The ordering is published on the base layer, then the prover fetches the ordering data, it computes the proof and publishes the validity proof, and voila, a certain batch is finalized. So typically there are two protocol participants within an Ezk roll up. There are possibilities that you can have one, but usually we just use two protocol participants. So one role is a sequencer and the other is approver. And sequencer are responsible for defining the preliminary ordering. So essentially, they are in direct contact with the users. The users propagate their transactions to Sequencer.
05:30:07.094 - 05:30:47.874, Speaker A: Sequencer either provides some receipt that guarantees that they'll include that transaction or doesn't, depending on the implementation. They combine those transactions in a batch and then publish the batch. And the proofers are responsible for forcing the validity. And they do that through zero knowledge proofs or validity proof proofs. Because in reality, in this case, zero knowledge proofs. We use zero knowledge proofs for succinctness, not for the privacy aspects of zero knowledge proofs. And therefore we actually usually refer to them as validity proofs.
05:30:47.874 - 05:31:40.402, Speaker A: And basically the validity proofs allow the L one to succinctly verify that a certain state was committed without redoing re executing all the transactions. And proving can be competitive or it can be collaborative. So what I mean by competitive proving, it essentially emulates noncommodic consensus. So you have multiple different entities that compete to have their block included in case of bitcoin. And in this case, you have multiple entities competing to have their proof submitted and finalizing the block that way. There's a drastic disadvantage with this approach is in the context of provers. Competitive proving is centralizing.
05:31:40.402 - 05:33:00.478, Speaker A: And the reason for that is, unless you insert some form of randomness, unlike Nakamoto, the most efficient prover is always going to win. So in Nakamoto, your probability of having your block selected for a given height is proportional to your hash rate percentage in the network, whereas in here your probability is essentially 100%. If you have the most efficient prover, unless there's some networking issues for you, et cetera, et cetera, which means the long term everybody else is disincentivized to compute proofs, and you'll end up having one or two provers maximum competing with one another to have their proofs submitted, which is not very resilient. It doesn't really affect the security per se, but it's still not an ideal situation. Because let's say if that prover fails for one reason or another, you're stuck with a liveness failure and you need some fallback or whatever. And collaborative proving assigns approver per batch. So think of how gasper, in Ethereum works, you have slots, and for each slot you deterministically select a leader through some form of election mechanism that involves randomness.
05:33:00.478 - 05:33:51.010, Speaker A: And that leader is the sole entity responsible for proposing a block for that slot. And the same is true here. So you elect approver and that prover is responsible for producing a validity proof for that batch. And in collaborative proving you need a fallback scheme. Because let's say if the primary prover fails for a given batch, then your protocol will just stall. And therefore you have to have a fallback which elects a second prover or makes it permissionless. There are different ways that you can implement it and basically still retain your liveness guarantees.
05:33:51.010 - 05:34:53.080, Speaker A: And proving can be sequential or parallel. The problem with sequential proving is that if the proving time is longer than the block time, your finality time increases super linear. You should think of it this way. Say you have 100 blocks and the proving time is two minutes and the block time is 10 seconds. You can see if you wait until the previous block is proved before you start proving the next block. You can see how as the number of blocks grows, your finality time grows superlinearly to the point where at some point you'll have finality guarantees that are slower than the finality guarantees of optimistic roll ups, rendering zero knowledge proofs and zero knowledge roll ups pointless and therefore it's not a good way of implementing things. It can only work if your proof time is shorter than the block time, which is unlikely, at least for the foreseeable future.
05:34:53.080 - 05:36:18.880, Speaker A: And the lower bound on finality for sequential proving of N blocks is N times proof time, as I explained previously. And the lower bound on finality for parallel proving of N blocks is proof time plus n minus one times block time. Which means that essentially we can decrease the finality time drastically by assigning as many blocks to different provers in parallel as possible, and then either verifying those proofs separately, which might be costly, or taking some additional time and aggregating those proofs and basically verifying one single proof at the end attesting to the validity of end blocks. And the more blocks you add to this, the higher the throughput and the lower the cost, assuming no other bottlenecks. And you can think of parallelization in this case similar to how Celestia works with light clients. So in Celestia, the more light clients you have in the network sampling the data availability, the greater the block size can be. And the same is true here again, assuming no other bottlenecks, the more provers you have in the network that you can assign different batches to in parallel, the higher your throughput can be.
05:36:18.880 - 05:37:50.590, Speaker A: And now, an interesting question how do we incentivize provers? So we can just allocate a fixed percentage of the protocol revenue to approver of a specific batch and call it quits. But the problem is that doesn't really account for Mev and it's likely that mev is going to be a large percentage of the protocol revenue. So protocol fees may be depending on how the specific roll up is implemented, you can have token emissions, et cetera, et cetera. But long term it's likely that mev is going to be the predominant source of profitability for the protocol. And therefore you end up with a system that is completely imbalanced in terms of incentives because the sequencers control all the ordering and therefore they get to order the transactions the way they want to maximize their attractable value potential and just keep a large chunk of that profit to themselves. Which means that if people have a choice later between becoming improver and a sequencer, they're likely to choose to become a sequencer because it's just more profitable. So what was the solution? Well, I refer to it as prover sequencer separation and it's based on proposer builder separation that is proposed for ethereum.
05:37:50.590 - 05:39:05.570, Speaker A: Ethereum already has proposer builder separation that is out of. So essentially, in case of Ethereum, you have specialized entities called builders which are good at maximizing building blocks that maximize the profitability of that block through extracting as much value as possible. And then we use a first price auction for those builders to compete with one another to have their blocks selected by the proposer for a given slot. And then added to the block. And a similar thing is true here. You have sequencers who specialize in maximizing extractable value, competing with one another to have their block selected for a given batch and basically share the revenue with the prover through the process of bidding. And PSS can be enshrined, which means that it's part of the protocol, similar to how Ethereum is planning to enshrine PBS into the protocol.
05:39:05.570 - 05:40:08.860, Speaker A: Or it can be external. And there are downsides to both approaches and upsides to both approaches. So in case of enshrine, it more clearly defines the role within the protocol, meaning that it's easier for the protocol participants to know who's performing what role and how many sequencers you have at a given time, producing blocks, et cetera, et cetera. But the downside is that it's relatively complex and it requires two separate roles. An external is simpler because you just have one role internally, and then the builders, or in this case the sequencers, are external to the protocol. So from the perspective of a full node operator or a lat node operator, the sequencers don't even exist, they're just somewhere there. So approver can just be a sequencer themselves, for example.
05:40:08.860 - 05:41:45.302, Speaker A: But the downside of this approach is, as I mentioned previously in The Benefits of Entryment, is that you just don't know whether there are enough builders, et cetera, et cetera, participating in the network, which creates a situation where you can end up with, let's say one builder doing everything and it will be difficult to tell which can result in bad resilience for the protocol. And before I end, I'll shortly touch on a few other things that have emerged recently in this space. One is prover markets. So you can think of prover markets as general purpose marketplaces for provers to purchase computational power to compute proofs, similar to how nice catch works in case of proof of work. So an example of approver market would be nil protocol. They're working on a system where you can essentially generalize approver for any protocol and just have one market where anyone can just purchase computational power from this pool of provers. And then there are also prover networks which are application specific marketplaces, similar to how Lido works in case of Ethereum.
05:41:45.302 - 05:42:04.160, Speaker A: So you have a prover market that specializes only on one chain or a specific use case and it just provides the computational power for that specific use case. So thank you for your attention and feel free to ask any questions.
05:42:05.890 - 05:42:10.046, Speaker H: All right. Thank you, Taguru. I guess it had a lot of.
05:42:10.068 - 05:43:08.382, Speaker A: Touch point with PBS with PSS. And one thing I kind of also wanted to discuss briefly with you about was the fact that you did mention like the builders of the block could well, the sequencer of the block could also centralize in a similar way that the builder on L one can also centralize. So how do you foresee, I guess, the centralization factors around builder centralization or the sequencer centralization in this case, for the L two S. So in the case where you have enshrinement, centralization is more difficult because you can, let's say, set a certain minimum amount of sequences that the protocol should have. So let's say 20 or 30 or whatever. With external it's a bit more difficult because the protocol is agnostic to how the blocks are produced. So there might be only one sequencer producing all the blocks.
05:43:08.382 - 05:43:59.890, Speaker A: In case of L two S, that's not as big of a problem as, let's say in Ethereum because you still have inclusion lists through the L one. So the force inclusion, but that's not ideal because that will cause a lot of delay and it will still be costly. So in case you do external PSS, I think the best approach is to still have some form of inclusion list similar to how Ethereum is planning to have them and just enforce certain real time censorship guarantees. Obviously you can never guarantee 100% that a certain transaction will be included within a certain amount of time, but increase the resilience of real time censorship resistance through inclusion lists.
05:44:00.230 - 05:44:01.220, Speaker H: I see.
05:44:02.310 - 05:46:03.542, Speaker A: I'm trying to see from the sequencer Builder's perspective if it is possible to sort of replay the same sort of competitive dynamics that is happening within the Builder space in L One and then kind of have the same things happening L two. Meaning that if the sequencer would be possible to do things like private order flow type of things that the builders on l ones are already doing, or is that something that can be, I guess, controlled by the protocol to some extent? I feel like depending on how you do it, you could obviously, let's say, require are a sequencer or like let's say, provers to commit to a certain set of transactions on the L one by tossing a hash of their cumulative string or somewhat like that. But I feel like in general, unless you want to go that route, which I don't think would be very efficient, you would still have a possibility of given sequencers having private order flows. It's especially true with external sequencers because it's likely that you'll end up having five to ten builders max, who control the majority of the blocks, which means that users are essentially incentivized to send the transactions directly with them versus the PTP network. Got you. I see. And I guess in that case then, yeah, you will probably see some sort of like a builder competitions and potentially some sort of centralizing effect to one builder controlling like 40 50% of the order flow that is going through the network.
05:46:03.542 - 05:47:14.740, Speaker A: In that case, yeah, you can potentially end up with a single builder controlling the large percentage of the order flow. But again, there are certain mitigations here. So let's say if you have inclusion lists, then there's not a lot a builder or a sequencer can do in this case because they're essentially forced to include certain transactions even if they don't want to or they'll be just slashed. There are certain mitigating factors you can implement, but ideally, I would like to have a protocol set up that doesn't really need to do that. Because one of the goals with L two S is to minimize the overhead. Because if you're doing the same thing that an L one is doing, you're just adding so much overhead on top of what you need to do, and therefore, it just doesn't really make sense to emulate L ones completely. And do you foresee that.
05:47:17.350 - 05:47:17.778, Speaker B: A lot.
05:47:17.784 - 05:48:42.540, Speaker A: Of the PSS stuff will be sort of like how should I be built enshrined in the scroll protocol? Or do you foresee that it may kind of exist as out protocol implementations of that kind of marketplace and then maybe one day entry? I'm not sure. It depends on other factors within the protocol. So for example whether we decide to use a BFD consensus or something else. So the problem with having external PSS with a BFT consensus is that ideally we would need quite a significant number of provers, let's say a few hundred to a thousand. Which is not really great for a BFT consensus, because usually a classic BFT protocol can support up to a few hundred nodes, which means that we have to limit the number of provers that we can have in the network at any given time. We can obviously do like random comedy sampling, et cetera, but then you degrade the liveness guarantees of the protocol and therefore, at least at the moment, I'm not really sure if external will work with a classic consensus mechanism. Got you.
05:48:42.540 - 05:50:37.630, Speaker A: Also another thing that I'm curious to kind of hear your thoughts on was as you mentioned, I think the prover incentive game is something to be kind of a little bit more explored. And I think Ben from matterlabs previously, earlier in the day have also kind of alluded to the possibilities of kind of bringing back the POW like system back for the proven networks that may or may not have under the roll ups. That prospect seems like kind of like hovering around for a lot of the proven networks that may sit under those Eka roll ups. What's your general takes on those POW types of incentive mechanisms that may emerge. So let's assume that you would have some form of randomness embedded into this POW like mechanism because otherwise you end up with a system that the most efficient proverb always wins. So let's assume that even in that case I feel like especially if you want to paralyze and you kind of have to because your proverbs times are longer than your clock times, at least for the foreseeable future, it's going to be like that. And therefore if you have a POW like mechanism you would have to prove sequentially which one results in a lot of prover energy wasted, computational energy wasted, because only one proof will be used at the end and two will result in finality times growing super linearly with the number of blocks that are being proved.
05:50:37.630 - 05:52:10.580, Speaker A: So I'm not sure if that's the right path, at least for now. Long term, I'm not really sure. But again, I feel like collaborative proving where essentially provers depend on another prover to compute the proof and they collaborate together and then aggregate the proofs and post them on chain just makes more sense because it's much more energy efficient and it maximizes the computational power that you have to compute the proofs. Got you. I think it's very interesting because I've also noticed a similar kind of discussions around well, here we call it proverb sequencer separations, but I think in the lands of Aztec, I believe they also have some sort of like a similar implementation of B 52, which also kind of have this same dynamics. Do you have any sort of common sort of thoughts around what the Aztec lands are happening with that? The proverbs sequences separation work there? I actually, funnily enough, I had a call with a couple of guys from Aztec a few weeks ago about this specific thing because we were just catching up about their approach and how it differs from what we're thinking, et cetera, et cetera. Yeah, I feel like a lot of Lt's, especially ZK roll ups, are going to converge on the same problems at the end of the day.
05:52:10.580 - 05:52:22.658, Speaker A: And I feel like we'll have one or two winning solutions that everyone's going to adopt at the end. I see. So basically you're saying someone is going.
05:52:22.664 - 05:52:24.574, Speaker B: To come up with the proposals, everyone.
05:52:24.632 - 05:53:00.554, Speaker A: Will look at it and they're like, oh, maybe this would be cool to actually use. And then people start to kind of standardize across. Yeah, I feel like maybe two max, but I don't think every single roll up is going to have a drastically different decentralization approach to decentralization. Gotcha. Gotcha. Okay, well, I guess that's probably also the community driven research part that's going to hopefully come in handy for everyone in this research. Yeah, I guess that's going to be exciting.
05:53:00.554 - 05:53:34.022, Speaker A: Well, I guess in the interest of time, I will have to wrap it up and toguru thank you very much for coming today. Very honored to have you here for the talk and sharing about the PSS. And again, looking forward to any future research that may come out of scroll as well. Thanks for having me. Thank you very much. So I guess as closing remarks and special thanks. Well, this is the end of the open talk on zero knowledge.
05:53:34.022 - 05:54:53.400, Speaker A: And this is again the first one that we did and I'm very glad that it went quite smoothly. And I think we had a very fruitful conversation with a lot of the panelists and the researchers founders that came on board and was kind enough to share about their ongoing research problems that they had. So I'm very glad that you guys from the audiences also sticked around and feel free to message me at my telegram or my Twitter, which Chloe, hopefully we can also drop in the message and feel free to reach out for any questions you may have. And also please reach out to the projects teams and the speakers panels that have attended today. As yeah, just for any further questions or any research stuff that you guys want to do, please do reach out. And yeah, I guess lastly, please make sure to follow Fengushi for any future updates on events that we are planning around DevConnect and beyond. And also just like any similar types of open talk that we may host in the future as well.
05:54:53.400 - 05:55:25.070, Speaker A: So, yeah, make sure to follow the Fembushi chat as well. And lastly, I want to thank all the speakers and also Chloe from our team, as well as Fawn for all the amazing graphics here that they made as well. So thank you very much for helping organize this thing. So, yeah, that's it for today. Thank you and hopefully we'll see you guys soon at a different venues for maybe different open door.
05:55:25.650 - 05:55:28.250, Speaker H: All right then, that's it. Bye.
