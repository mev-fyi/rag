00:00:00.410 - 00:10:03.950, Speaker A: Sam. Sam don't sam WA sam sam. Sam ram SA. It Sam SA. Sam SA. Thanks for being here. I'm not going to keep it too long because we are on a very, very tight schedule.
00:10:03.950 - 00:10:44.110, Speaker A: So I'm going to call LZRS to the stage. You are building modular cloud and you're going to tell us something about the economics of modularity. So please give him a hand. Hey everybody, thank you for coming. Do I just go to the next slide with the no, don't look spoilers. Okay, so, yeah, my talk was originally called The Economics of Modular Blockchains. But I realized that was a very broad topic and it has to cover a lot of breadth and not go very deep on any specific issue.
00:10:44.110 - 00:11:55.394, Speaker A: So I thought I'd kind of apply some of the principles I was thinking about in that original topic towards answering a very practical question that I think that the industry needs. And what I want to answer is, if somebody is an app developer, meaning they're building software for users that serves a purpose, why should they consider to include modular blockchains in their tech stack choice? And before I answer that and go into this, I do want to introduce myself. I'm LZRS. I'm the founder of Modular Cloud. We have a cloud infrastructure for modular blockchains and I am on a panel in about an hour from now and I encourage you all to attend and we're going to talk more about the specifics of those services that we provide more in depth in that presentation. Okay? Now on to why should people use modular blockchains for their apps? So here's the common narrative of why somebody would want to use how a blockchain works. Essentially, there's a user and this user creates a transaction and they securely send this transaction to some sort of decentralized network.
00:11:55.394 - 00:13:04.858, Speaker A: And once this transaction is off into the Internet, on this decentralized network, there's this group of nodes that come to consensus and they verify that this transaction is valid and update the state and it's broadcast and finalized and sent out to everyone in the world. I think that this is like a common explanation, but I don't think that there's some parts of this type of common conception that I don't like. And the one I want to highlight right now is that I don't like that when people skip when they say, okay, the user uses the blockchain. Because I think that really takes out a really important point and part of the equation. And in doing so, it kind of makes people that are building apps not really understand where they fit into the picture because the reality of the situation is that the user is not using the blockchain. They're using a device like a phone or a computer. And on that is some code that's run that's written by some sort of third party, some sort of project, which that is the app and the developers that are building that app are integrating with the APIs and SDKs provided by the blockchain.
00:13:04.858 - 00:13:40.658, Speaker A: So the app developer, in a sense, is the true consumer of the blockchain. And I think as we go on, over time, we're going to transition more from everything being where we tell people, like, okay, this is a crypto app, this is a blockchain app. And a lot of stuff will be going on behind the scenes once developers realize the power that blockchains can provide them just as a technology in their technology. Stack. Okay, so imagine you're an alien and you're coming to Earth. You have no context about this, and you look at these two apps. These are undeniably apps.
00:13:40.658 - 00:14:03.566, Speaker A: They're both on the Android Play store. So you can install these apps on your phone. These are apps. These are not blockchains. They're not decentralized networks. Right? There's a venmo app and the mycelium bitcoin wallet. Now I would ask, what's the difference? I mean, you can say there's a different UI, different name, stuff like that, but by the way, sorry, I thought the time was frozen, but it was actually moving.
00:14:03.566 - 00:14:47.526, Speaker A: Never mind. Anyway, so I'm trying to be mindful of my time. Anyway, so if you compare these apps, they both connect to the Internet. One of them, you could say, connects to the bitcoin network, the other one connects to Venmo's servers. But that in and of itself is not a particularly meaningful distinction. What I would say is the difference between these two is that the bitcoin wallet is actually not only retrieving some data from the network, but it's also doing some sort of verification on that. And the way that it does this is it connects to this very powerful API that you can call the bitcoin node RPC.
00:14:47.526 - 00:15:21.538, Speaker A: And what I want to do is I want to, in this talk, propose an abstraction that we can use when we think about talking to RPCs. And this is going to apply to all blockchains, not just bitcoin. I'm starting with bitcoin because it's a very simple case. It's basically an app chain for payments and over time, we've modularized these and expanded the scope of what these protocols can do. But this is going to be an easier case to explain. So I have this abstraction that I'm going to present to you called a cost function. And this is how I think that app developers should be conceptualizing blockchains.
00:15:21.538 - 00:16:01.554, Speaker A: Like they shouldn't be thinking about the internals of the peer to peer networking and cryptography. I mean, it's important context. You need to be aware of that. But it's not the fundamental thing that you should think about when you're considering how or if you should integrate a blockchain into your app. So what is a cost function? A cost function is essentially if you give in a statement, it's the cost that the person making that statement would incur if they were to commit fraud. So in the case of bitcoin, this has a very narrow capability. So if someone says, I sent you five bitcoin and I'm going to be very much abstracting this process, I'm not talking about actually how this works.
00:16:01.554 - 00:16:41.870, Speaker A: This is a very high level description of this process. But if someone says I sent you five bitcoin, you wouldn't want to necessarily trust that that's true and do something irreversible like sell your kidney, just trusting that it's true. They could be committing fraud. And the difference between something like Venmo and Bitcoin both are hard to commit fraud on. But if you use Venmo, you don't actually have the information for how much it would cost to commit that fraud. So probably you'd probably be fine with Venmo, but there are situations where having that additional information would be good. So in this case, this person claims to have sent five bitcoin to this person in exchange for some service or goods.
00:16:41.870 - 00:17:18.186, Speaker A: And this person realizes that the cost to commit fraud in this scenario is lower than the value of the transaction. And now they can make a risk assessment and they can model the risk based on their use case, on how confident they are and how they want to interpret that data between the two. But I think that they're actually connected. At the last modular summit, I gave a talk on the subject. I encourage you to check it out if you're interested in learning more about that, but I'm not going to talk about that here. Okay. How does the cost function work? Essentially, you start off by having this assumption that you have access to the open internet.
00:17:18.186 - 00:17:59.126, Speaker A: And through accessing a peer to peer network, you're able to scan and get all the possible chains, or maybe not all the possible chains, but among the chains that you can find when you scan the internet is the one that you are looking for, which would be called the canonical chain. It would be like, for example, the longest bitcoin chain with the highest difficulty. That would be the canonical bitcoin chain. And what happens is you retrieve data from the network and then you apply a rule set to actually determine which of these potential canonical chains is the canonical chain. Because you can't of course, trust anything you hear on the internet. Just because someone says this is bitcoin doesn't mean that it's true. So the way that this works is you can break this down into two pieces.
00:17:59.126 - 00:19:15.010, Speaker A: First is validity and the second is the fork choice. So if you think of all these things as just raw bytes, like just blobs binary data, you will parse that and you'll check if the structure of that data follows the rules of the protocol that would be checking the validity. And then even if you have ten chains, maybe there's eight of them are valid and two of them are invalid, you still have to decide which one is the canonical chain. So a fork choice rule will take two potential chains A and B and it will return either chain A or chain B depending on which one it prefers. And if you go through all possible chains that are presented to you from the network with this connectivity assumption, assuming that you're connected to the internet, you should have by the end of it the preferred chain out of the whole list will be the canonical chain. And so how this plays into the cost function is that there's this thing, I don't really know what we should call it but maybe you can call it like security units. Once you have this data and you know what the canonical chain is now you can actually statically analyze that binary and you can determine what would be the security units that it would take.
00:19:15.010 - 00:20:11.282, Speaker A: If someone had these security units in an abstract sense, could they commit fraud into the system? So let's go through a concrete example of this. So what we're talking about bitcoin it's proof of work. So there's a certain hash rate. A hash rate would be a unit perhaps and if you had enough hash rate you could do a 51% attack and now in and of itself that's not actually that useful because how do you know how hard it is to do that hash rate? Maybe you can use your laptop or something and commit fraud. So there's another assumption that goes along with this assumption that you have access to the open Internet and that's access to market data. And this is not built into the protocol, so it can be kind of counterintuitive, but this is actually really key to properly utilizing blockchains. And so with this market data, basically what you want is the cost of the hash power.
00:20:11.282 - 00:20:53.602, Speaker A: So in this case essentially I don't actually know how people really calculate this but I assume it's basically like the electricity costs and maybe something to do with hardware. But there is a way that you can model the cost of getting the hash rate that you need to commit fraud. And from that, this is what you would have as the cost function. And you can now say, based on this statement that I'm verifying how expensive would it be to commit fraud in this case. And then now I can determine, based on the circumstances for my application, how I want to interpret what is on the blockchain. And then, of course, who uses proof of work anymore, I guess, right? Proof of stake is exactly the same thing. You have tokens that can be slashed and there's a price of a token, you have access to the market data, you can do the same exact thing.
00:20:53.602 - 00:21:22.540, Speaker A: It works the same way. And so I'm not going to go into this connectivity assumptions but I do think that this is one area that really does chip people up because there is an assumption when you're using blockchains that you have access to the open internet. And if you don't, actually, a lot of things break down. I think when a lot of people think about it, they just look internally into how the protocol works. And sometimes not all the answers are there. And so that might be a disconnect why some people don't understand the value prop of crypto. But I'm not going to go into this because we just don't have time.
00:21:22.540 - 00:21:59.426, Speaker A: But I do want to talk a little bit more about selecting the canonical chain, just to be a little bit more clear about how this works. Essentially, you're getting all the chains. This is the pseudocode. You're filtering out the valid one. So now you have a list of valid chains and now you're just literally running a reduce of the fork choice rule on all the valid chains. But what's interesting is you can actually collapse this down and you can combine validity into the fork choice rule itself. And it would look something along these lines where instead of a function that takes a and b as an input and returns either a or b, depending on which one is preferred, you also can return like a nil value if both chains are not valid.
00:21:59.426 - 00:23:28.686, Speaker A: So you're just basically checking the validity at the same time as determining what is your preferred fork. And so this is actually, I would argue, the most concise abstraction for how blockchain, what is a blockchain? All this other stuff is implementation details like are you using proof of work or proof of stake? Do you have merkel trees? A lot of these things are like just to achieve this. But this is the actual conceptual, the essence of what we're actually working with here. But the thing is though, is again, I want to reiterate that the fork choice rule by itself is not really useful because you say, okay, this is the valid chain, but what's the implication of that? Right? You need to have what I call this broader thing, which is like this cost function to understand. What does it mean when you find data in this chain? What kind of guarantees can you expect from it and how expensive would it be? How much can you trust this data, essentially? So I want to now build up from bitcoin to modular blockchain so we can see how this all ties together. Okay, so bitcoin had two problems, and I want to focus mostly on this first one. And essentially the problem with bitcoin is that it's hard to engage in commerce with bitcoin.
00:23:28.686 - 00:24:07.814, Speaker A: And one reason for that is, of course, you have to get a bunch of people to get bitcoin wallets. You have to get businesses to put their payroll or their point of sale system on bitcoin. That's all very hard and we've made very little progress on doing that. But there's also kind of a more fundamental reason why it's hard, and that's because it's really hard to bridge on chain and off chain events and people working in real world assets and dealing with Oracles and stuff. They face this all the time. And I won't go into because we just don't have time, I won't go into the theoretical reasons why, but I think there are some interesting things for why this is true. But I'll give you an example to make this more concrete.
00:24:07.814 - 00:24:53.214, Speaker A: So this was the example we went. So someone was going to sell their kidney or something, and this person claimed to send them five bitcoin, but really did not. And they were able to not trust that because they could calculate that it was likely that they could be profiting by committing fraud in this scenario. Now this works in this narrowly defined scope of saying I sent you five bitcoin because essentially that's what the Bitcoin app does. It's a payments app. The cost function can let you verify that the cost of how much it takes to commit fraud in this case. But it can't do other things.
00:24:53.214 - 00:25:28.906, Speaker A: Like for example, if they claim to have provided the service, you wouldn't be able to verify that that actually happened. You'd have to introduce some sort of Oracle and then that introduces additional risk into the system and all this kind of risk. The more risk that you introduce into the system, the more you have to have a margin of error on anything that you're calculating. And eventually if you have enough error, that's going to all compound and it's going to be a useless value. So you want to constrain any of those things. You want to have as much to be directly verifiable by the cost function as possible. And so there is a way that Bitcoin could have addressed this and expanded the functionality if they wanted to.
00:25:28.906 - 00:26:17.318, Speaker A: And I'm not saying that they should have, I'm just saying they could have. And the way to do this would be to hard fork and add more opcodes and increase the power of their scripting to do other things that people want to do. And the more things that you'd want to do, the more different types of statements you'd be able to verify with this cost function and understand the context around how expensive it would be to commit fraud in these scenarios. But they didn't do this. And why did they not do this? I think this is a really important point. I actually think it's exactly what you would expect. So there's this thing called Brooks Law and it talks about how you add engineers to a project and it makes it harder to actually complete the project because essentially as you add more people, the lines of communication increase combinatorially.
00:26:17.318 - 00:27:07.366, Speaker A: So what this means is as you add more people to a group, the communication overhead gets way more and more and more difficult. The implication of this is at a global scale, you really can't coordinate, it's not possible. It's prohibited. And that's why these permissionless networks are really great because you actually don't have to coordinate. And I'm not going to go into this, but I do want to say this idea that blockchains allow people to reach consensus I think is not a good way of all of looking at it. I think blockchains allow people to verify things and calculate the probability of fraud but they actually work because you can't come to consensus like for example, a 51% attack or a 66% attack. These would require consensus among the participants of the network.
00:27:07.366 - 00:28:01.050, Speaker A: And the fact that those don't happen because that's very hard and very expensive, is the reason that this works. So we're actually playing off the fact that you can't reach consensus. It's maybe not important, but I just thought it was worth mentioning. And so essentially it makes sense that Bitcoin wouldn't be able to upgrade because everyone's verifying their own thing independently for their own purpose. And you would have to get the people sending it to upgrade and the people receiving it to upgrade, because you want to be sending what someone's trying to receive and receiving what someone's trying to send, and it's very hard to do. Ethereum actually fixed this problem though, because what they did is they modulized the application logic of Bitcoin. And so if you remember, I'm not going to go back a bunch of slides, but I had this diagram where it showed that the cost function or where the canonical chain is determined by the validity rules and the fork choice rule.
00:28:01.050 - 00:28:59.390, Speaker A: Well, essentially what Ethereum did is they modularized the validity rules. And so you're able to essentially do something like this, which is where you can make a statement like I did X according to module Y. And that module Y would be a modular validity rule set that can be loaded all onto the same blockchain. But the thing is, since this is modular, you don't actually have to fork the network and breach consensus with the entire developer community and all the users that are verifying the chain. So this allows people to build this stuff and use it in a modular context without the coordination overhead. So allowing you to not just have a modular validity rules, but also a modular fork choice rules so you're able to define. So if you think about a roll up on Celestia, for example, essentially that roll up has its own fork choice rule with its own validity rules and its own fork choice.
00:28:59.390 - 00:30:06.386, Speaker A: But it also uses Celestia's fork choice rule as an input. So it's not just a function that takes A or B, it's a higher order function that takes B and another function which is a fork choice rule. And so in this kind of way you're able to import the verification capabilities of other chains into your chain and that's what a roll up essentially is in its essence. And so this allows you to express a greater number of things that can be verified and you can understand the cost of fraud in these systems. And so I'll just kind of go through this little three dimensional diagram. Initially we had like an app chain which was bitcoin and you could just basically have different wallets and they're just verifying these very narrowly useful statements such as, I sent you this many bitcoins. Then we had Ethereum that would allow for DApps and this had so many more use cases like NFTs and DeFi and stuff like this.
00:30:06.386 - 00:30:50.290, Speaker A: And there's a lot you can do with this. But now what we're going to have is with modular blockchains, we just kind of added this other dimension here. Since you can customize the full fork choice rule, you're actually able to even do more interesting types of things. Like for example, you can decrease the level of security and so but you can increase performance as a result and you can verify things. You have more customizability in actually setting the parameters for detecting fraud in the use case that you are actually building for as an app developer. And so, yeah, I think that's my time. So this is like a great place.
00:30:50.290 - 00:31:11.042, Speaker A: The conclusion slide was here. So thank you everybody. I appreciate you coming. Thank you, Liam. That was Liam. Next up is ah, there we go, the pointer from Range. And you'll be telling us something, I think tacking onto this on security of modular chains.
00:31:11.042 - 00:31:57.030, Speaker A: There you go. Hi, everyone. I'm Andres Monty, co founder of Range. And today we are going to talk about sovereignty and security and also how app specific roll ups can be the answer to the security problem in crypto. So I have a couple of questions before I start talking. Yeah. Here for you, which is who in the area in the room thinks that Ethereum would fork if the staking deposit contract would get wiped out and all the ETH was stolen? Like, raise your hands if you think would have another doubt for contests fork.
00:31:57.030 - 00:32:45.880, Speaker A: Okay, who thinks it wouldn't? Okay, maybe we have like 75% that we would fork. Who thinks here that if lido all the TBL of lido all the ETH, which I'm not sure if it's between ten and 20% of the supply got hacked, who thinks here we would fork as with the dow fork? No one? Okay, that's interesting. So then if all the TBL of MakerDAO got hacked, nobody here thinks that we would fork. Right, okay, that's interesting. We'll come back to these questions later. But yeah, let's start kind of stepping back and understanding what's the level of security in crypto we are doing. Not fine.
00:32:45.880 - 00:33:16.702, Speaker A: In 2022, more than $3 billion were affected. Some of these were recouped. But definitely we are not doing fine. This is a huge number and I always try to make analogies with kind of more real world numbers. So this is, I think, almost 20%. Like $3 billion, almost 20% of the annual defense budget of my home country in the European Union. So we are talking about very, very big numbers, right? And I think everyone in this room agrees that security is important.
00:33:16.702 - 00:34:14.020, Speaker A: I would go even farther. I would say that I'm biased, but I think that is the key unsolved problem in crypto for mainstream adoption. It's great that we are working and solving scalability. It's great that some people are looking for product market fit, but security, like nothing of that matters if we don't solve security. A good example is Axinfinity. Axiofinity was the product that kind of crossed the like and was more like mainstream adopted, like very adopted in Philippines. But then at some point, the running bridge was hacked for more than half billion dollars and all the users, all the mainstream users that were using Axie probably are not coming back because they got wrecked, right? And so I would like to ask you here if we are ready to build like secure modern blockchains and if we are ready for thousands of rollabs or tens of thousands of roll ups kind of talking to each other.
00:34:14.020 - 00:34:54.522, Speaker A: Yeah, I come from the security background. We have certainly some challenges, what I call modular nightmares. So, like, modularity implies that there is more external dependency. So we have a supply chain risk for the counterparties that we are kind of integrating our modular stack. We're also using cutting edge tech, right? Like roll ups, zero knowledge. This is super cool technology, but it's mostly not battle tested in production yet. And then as well, this world of many, many roll ups and blockchains implies that bridges are not going anywhere and cross chain is not going anywhere.
00:34:54.522 - 00:35:31.030, Speaker A: And bridges are complex, bridges are hard. So we should understand as well that not all bridges are created equal. And I think, like, as a community, and in particular this sector of the crypto community has done a lot of progress moving from centralized to trust minimized and trustless bridges. So this is great. But the reality is that most exploits that we've seen, and we've seen in 2022, more than $2.5 billion were actually due to implementation flows. So in this infographic you can see, well, the yellow bars correspond to poor designs.
00:35:31.030 - 00:36:27.994, Speaker A: So this means like, for example, the case of Ronin and Harmony, just key compromise on multisig breaches. But all the green bars are actually implementation flows. So the point here is, doesn't matter how sound and robust your breach protocol is, doesn't matter if you are using like a trustless bridge, trust minimize, like IBC or even like a canonical bridge between Ethereum main chain and an L two, there can always be an implementation flow. So I think as a community, and I think this happens already in web two, we need to internalize this statement. There will be always yet another software back. So we kind of need to work on controlling the blast radius of these incidents and particularly when we talk about bridges and roll ups that are very complex software projects, which complexity means a larger attack surface and more error prone type of code bases. But not everything is dark.
00:36:27.994 - 00:37:29.150, Speaker A: I think modularity is kind of bringing a new paradigm with all these challenges that I've been talking about, but also a lot of opportunities. And I think the opportunities definitely at weight the challenges. And I generally believe that app specific sovereign rollups might be the only possible way actually to build truly secure blockchains. And this is not talking about like five years, ten years from now, right? Like the future is now. Like projects like Celestia are going to enable very soon that there will be thousands or tens of thousands of roll ups, sovereign, app, specific rollers talking to each other and that every application is going to have its own roll up and also kind of align with this beautiful vision of community computers where all the stakeholders are aligned. So in this world sovereignty rules. And for this, I always like to use this analogy of seeing blockchains as countries and seeing bridges as sort of like international treaties or international relationships between these countries or blockchains.
00:37:29.150 - 00:38:10.490, Speaker A: We are going to talk about these three things in this context. So blockchain jurisdictions, borders and trade control and separation of powers. Who knows about the BSc hack here? So this is a very interesting exploit. It was almost the biggest exploit in the history of crypto. I think it was around like half a billion dollars hacked. But the most interesting thing about this is that the binance validators, let's say the binance validators were able to pause the chain and hold the chain very quick to control the blast radius. So like the hacker was only able to get only in quotes, $100 million out of the chain.
00:38:10.490 - 00:39:09.426, Speaker A: So was able to breach out $100 million. But all the $400 million that remain within the blockchain jurisdiction, which is the BSE chain, were able to be recouped and recovered. So this is a very interesting example. There is this another one which is this is not security related, but it's also like a proof of the power of social consensus. So the Juno blockchain community decided to confiscate tokens from a whale address that had received more than the community thought was okay in an AirDrop that was flawlessly designed. And then this was through community governance and they actually confiscated the funds. And of course the Dow fork, which I would say is top three in terms of importance of events in the history of crypto, this happened a long time ago, like seven years, which in crypto years is almost 50 years ago.
00:39:09.426 - 00:40:09.858, Speaker A: But it's also like kind of the importance of social consensus. So also regarding the Dow, I would also like to question ourselves, how would this happen if ethereum back then was connected with different bridges? So. Like if the Dow hacker was able to breach out part of the ETH or if there were stable coins that were already like in this primitive ethereum chain, right? So with these three examples, what we see is the importance of social consensus and blockchain jurisdictions, but also that the power of these jurisdictions starts and ends in the breach. And this is specifically clear in the case of the Binance hack and the importance of the breach. Bring us to this part. Some of these things are going to be a bit controversial, but yeah, I think it's worth it to explore them. So the importance of the border in blockchain context and trade control.
00:40:09.858 - 00:40:54.706, Speaker A: So I would say let's be the wall, or at least let's explore if we should build a wall. Of course, this is not a political statement, but I think that we should explore how breaching works. Right now, breaching is binary, right? It's either we connect to a chain or we don't. I think this is very primitive and we should explore ways to parameterize this, to make this more granular with mechanisms like rate limits, transaction delays like the wormhole governor has implemented, and as well with transaction screening and blocking. I'm not talking about theory here. There are many applications, cosmos chains that already do this type of blocking with blacklist. There is thousands of DeFi applications that do this as well.
00:40:54.706 - 00:41:41.330, Speaker A: But this is not explicit. And that's why you need to pause the change and hold the chain because you don't have these type of mechanisms in place. I would also like to talk about IBC rate limits. I think this is a super powerful primitive. What I think is that it's the first governance configurable rate limited standard in production for bridging fungible tokens over IBC. And this is super important because it doesn't only cover you if IBC implementation is actually exploited, but it also protects you as a community, as a chain, or as a roll up if the source chain is exploited. So in the case of Binance, if we had rate limits in place, this attacker wouldn't have been able to get $100 million, but maybe five or $10 million.
00:41:41.330 - 00:42:28.442, Speaker A: And also like the finance validator set wouldn't have needed to pause and hold the chain. And also, more importantly, it also protects if the destination chain is exploited. So it gives community like sovereign rollups and blockchains as well, sovereignty over how much you trust your counterparties. So there might be the case that you want to breach with a new fancy blockchain, but you don't really trust their technology debt or their economic security. So rate limits enable you to parameterize this and kind of position yourself in this tradeoff space between security and liquidity. And this is just the present. I think IBC rate limits in the future are going to be very different, much more complex and much more dynamic and powerful.
00:42:28.442 - 00:43:23.502, Speaker A: For example, extending beyond fungible token transfers but also being able to dynamically adjust to events that happen on chain. So something that would be very cool is that if rate limits could be integrated with on chain events like upgrades. So if a chain upgrades you can define like a small rate limits because upgrades are normally error prone and then as you get more confidence on the new version, you can gradually increase the rate limit. And then the last one is separation of powers. I think this really encapsulates very well like the power of modularity, the ability that you can kind of specialize every layer of the stack and build like customized mechanisms. In our case build customized security mechanisms across all the layers of the stack, something that I think can be super cool. I know that other teams are working on this which are modular mempools.
00:43:23.502 - 00:44:39.978, Speaker A: This is something, it's a concept that we've seen already in the wild Cosmos chains like Terra define mempools that have prioritization for different transactions. So for example, Oracle transactions are going to be always at the beginning of the block and I think this can be super powerful in the modular context, a modular paradigm, because we can really customize this combined with app specific sovereign rollups and really make this super powerful combining with security mechanisms customized for the given application. This is the case for example, for circuit breakers, so we could see a future where circuit breakers are totally adapted to the app specific roll up functionality and also have more priority in the mempool. So then we can make sure that all security related transactions get put at the beginning of every block. And this is really the future we are building at range. So we are providing the tools for blockchains, for sovereign rollups in both the Cosmos and Celeste ecosystem to express their sovereignty and also the security. And how are we doing this? So we are integrating off chain monitoring systems and on chain mitigation techniques like rate limits, circuit breakers and more.
00:44:39.978 - 00:45:39.838, Speaker A: And also we put a lot of focus on bridges. As we discussed in the call, we see bridges as the border and we should put a lot of security in the border, right? So we are building security mechanisms, security middleware around trustless bridges like IBC. As a little surprise, we have integrated Mocha in rage testnet from tomorrow, from the second day of the modular summit you'll be able to tinker with and start monitoring addresses, transactions and messages in Mocha from the yeah, that's all. Join us@range.org. I'm a Monty on Twitter and I'm always super down to discuss all these topics. Some of them are controversial, some of theirs are less controversial, but I'm always super down to discuss this in Twitter and also in the conference. I think that we have a bit of time for Q A.
00:45:39.838 - 00:46:18.250, Speaker A: So if you guys have any question or if you would like to come back to any of the slides also happy to. Thank you. So we have five minutes. So, does anyone have a question? No Questions. Your answer on whether it would be forked right now or not. Yeah, that's a good one. Yeah, I definitely believe like, if the Staking deposit contract is hacked and all the is wiped out, we definitely fork.
00:46:18.250 - 00:46:48.350, Speaker A: I think that if Lido gets hacked and all the ETH gets wiped out, I think we would probably fork. I think the room only like 25% of you said that. Or almost no one. I think it would. It's a meaningful percentage of the supply of ETH. There is a lot of quite important stakeholders as well. I think there is more than 50% chance that it would with Maker.
00:46:48.350 - 00:47:47.442, Speaker A: I don't think it would. And if I were Maker Dow and an application like that and kind of tying back with the rest of the talk, you have a TBL of I don't know what Maker Dow has in TBL, but maybe like $10 billion. I'm not sure exactly. But I should ask myself as a community that I have, like, a TBL of $10 billion, but then I don't have the ability and the power with social consensus to be able to do these type of decisions. Right? So you are handling a lot of money, but you don't even have the power to decide these type of things. And that's why app specific sovereign roll ups are so important and so powerful in the future for security. Because each application imagine like this MakerDAO app specific sovereign roll up would be definitely able to decide, like a fork if all the ETH or older TBL were affected in the hack.
00:47:47.442 - 00:48:10.034, Speaker A: So that's kind of really the connection and why teams like MakerDAO and others should definitely consider launching a certain roll up with Celeste. One more questions. Yeah, all good. All Right. Thank you, Andres. Give him one more hand. Next up.
00:48:10.034 - 00:48:34.922, Speaker A: I think we can save a few minutes here and there. So, Bunny, if you want to come to the stage bunny is building Dora, a search engine explorer for the modular ecosystem. This is your there you go. The big green button. So give a big hand. Hello. Hi, Everyone.
00:48:34.922 - 00:48:54.474, Speaker A: My name is I'll say my real name mauricio Trujillo and also known as Bunny or Koneko. Capital on Twitter. We're building a multi chain search engine at Block Explorer is called Dora. And also thank you, everyone, for being here right now. Yeah. Name of talk. What? We search for search engines in an error of modular existence.
00:48:54.474 - 00:50:27.770, Speaker A: So, first of all, what does this mean? How can Dora actually help with any of this? How can we actually use the search engine approach to approach, let's say, some stuff that have been talked about already modular applications, being able to actually interact with appic specific chains and also, how do people actually even to begin with, how do we approach infrastructure today? What Are The Different heuristics that we interact with some of the ones that we were already talked about, application specifics roll ups and so on and so forth. Intensive graders, multi consistent type of application, then search data. How do people actually relate to all of these goods and services day to day? How people start searching, let's say through brands, instead of searching for very clearly defined queries to engage with a good and service. Then externally branded monolithic chains as opposed to internally branded modular apps that may leverage infrastructure that is embedded exclusively, that is very opinionated as to what they may enable and as well as, hey, how do we ultimately end up engaging with all of this? So, first of all, I see a little story. So Jazz and Jacob, also from the Celestia team, approached me. They're like, hey Bunny, can you actually speak at the or something? I'm like, yeah, I'm more than happy to always ready to speak whenever someone asks me to. So one of the first things that I go is like, okay, I have no idea what people are going to speak talking about modular summit or then modularity in the overarching sense of the word.
00:50:27.770 - 00:51:22.030, Speaker A: So what do I do? I start searching. How do people actually start searching in their day to day lives? People start searching on their day to day lives. Let's say at least within crypto, we somehow stumble upon the semblance of having intents. Intents meaning whatever your definition of it may be, as it being bounds on the end state of an action on the state change constraints on the state change. Or also, as Vlad said, something along the lines of they are a glimpse into your deepest desire, the deepest desires of someone's brain. No matter your definition of intense, they are something that we can all clearly agree it expresses something as simple as I want to do X and that's it. So how are intents able to leverage stuff like DeFi gaming, NFTs and identity specific applications? They're able to leverage all of these because they don't necessarily make any concrete differentiator between them.
00:51:22.030 - 00:52:27.986, Speaker A: They may not necessarily measure different security assumptions. All they really care about, and that is the nature of intensity, is that your end interaction gets fulfilled and that's it. What type of world do we build with all of this type of infrastructure? The type of world that we build is one in which is composability, is pretty much unquestionable and we end up in a world in which we have an any to any type of relationship between goods and services in which, no matter what resources you have currently available, there will always be infrastructure that allows you to partake on any good and service you may wish. Though, how are we actually able to enable this? We have some multi ecosystem infrastructure. This is honestly one of my reverse stacks. We have some of the people in the room from those logos there you may recognize a couple of people from Caldera Modular Cloud that Squid skip routing mechanisms, roll up as a service indexing mechanisms, and of course intense. They're all very much needed to be able to enable this type of workflow that people are very used to in the regular world.
00:52:27.986 - 00:53:23.270, Speaker A: That regular world meaning outside of crypto. But is that how people really relate to sorry sir, what thing to go back this one, sorry. Thank you. How do people actually relate to all of these goods and services within crypto? And how do people relate to all of this outside of it? It's very clear there's a point in which every single crypto app starts pretty much looking like each other. We call this effect in seeing recognizing patterns where there may not be any pareidolia. There's a pareidolia of DApps in which people start seeing stuff like NFT Marketplaces on UNICEF, people start seeing swapping mechanisms. Within NFT marketplaces, people start seeing options where there may not necessarily be a need or a desire for someone to actually start leveraging options.
00:53:23.270 - 00:54:07.330, Speaker A: So what we find is that all of these goods and services eventually become commoditized. Something very fascinating though is that while the goods and services may be commoditized, the providers are non fungible. Every other provider will be selling you the exact same thing, but in a much different type of delivery packaging that some people may be used to. And that's actually good. That's very good actually. So how do people outside of crypto relate to searching intents and fulfilling of any desires they may have? Where can we actually look at something like this? We can look at search data, which is honestly one of the most valuable data in the world in my opinion. People don't necessarily look for goods and servicing in a very clearly defined query.
00:54:07.330 - 00:55:08.598, Speaker A: If you look at the top 100 searches on Google or even across search engines, especially if you start very heavily localizing people relate to goods and services through brands. It is very rare that actually something as concrete that is not brand heavy makes it to the top 100 searches, is usually just related to World Cup events. And even then you could argue that, okay, there are events, there are sports. So we have brands as a proxy for actions. We have antimarria in which the rare verification of nouns, not the announced NFT of course, but where we get to is that we have these two first and second degrees of antimeria where a lot of infrastructure is currently laying on, is that we have ways in which you have protocols, like not necessarily protocol in action. At this point, emailing people feel very comfortable saying, hey, email me, just send me a message. They don't necessarily specify which provider they may be engaging with and that's completely fine.
00:55:08.598 - 00:55:41.470, Speaker A: Another way of using is not exactly a protocol, but something's akin to being able to emit a message. Twitter people feel very comfortable using the anti American word Tweeted. This very easily allows us to convey hey, I want you to express an opinion. I want you to tell that opinion to the world. People feel very easy doing that. So if this is how people search, okay, we understand that brands are very important. We understand that maybe it's actually good to have branding around certain applications, maybe certain chains.
00:55:41.470 - 00:57:01.850, Speaker A: But is it enough? There's this concept of externally branding your monolithic chain in this sense, meaning that if you have, let's say, a forked EVM chain is just done, it's the exact same thing as an EVM, but you just so happen to have an NFT marketplace in that chain and that's all that you really do there. It's not necessarily optimized for NFTs, it's just a chain that has an NFT smart contract and that's completely fine. That's pretty good for most people. Where we very much like to push for though, is that we want people to be incredibly opinionated about the infrastructure that they leverage for their specific use. Case in this sense, what we are actually pushing for is app chains. We believe that very tired approach will be to create brands around a generalizable chains applications, whereas what we should be aiming for, and forgive me, there's a typo there is to create infrastructure around such chain specific applications. In this sense, if your application, let's say very high frequency trading specific application should have the shortest finality time and shouldn't be contested, maybe something like IBC should be considered, maybe something that, I don't know, dYdX is a very clear example of that.
00:57:01.850 - 00:58:10.174, Speaker A: Maybe there's applications in which you should rely more on social consensus as opposed to some very clearly defined cryptographic security mechanisms. Who knows? We can play around with this. The ethos of modularism is very much of build anything. And we strongly believe that if we should build anything, why shouldn't we actually build this in a way that we can actually start iterating on goods and services, on monolithic ecosystems, which is precisely what we've seen, and then actually go into production and optimize for service providers, for different service providers economic incentives on something like a modular stack. In this sense, one of these examples that is actually a paper that should be getting published either today or tomorrow by Turun Chitra is in how sequencing and you can model pretty much every single DeFi app as a sequencing of liquidations. In that sense, if you start considering that sequence, then ordering becomes all the incredibly important and you can start arguing for the certain sequencing of how to say it, of certain chains should be the most theoretically optimal for this specific DeFi application. That is a fascinating concept.
00:58:10.174 - 00:59:12.974, Speaker A: I think it's very interesting to say every single application that is deployed on this chain will not have a competitive edge against this one DeFi application. That's fascinating. So now every single app is a chain how do we actually go about searching every single one of these applications? How do we actually go about engaging with every single one of these providers? Once every single one of these providers starts looking the same, or hopefully they don't, and once every single one of these providers is able to leverage their specific infrastructure, that makes their ecosystems blossom as much as, almost theoretically, no one else can. What we do at Dora is we like to take the approach of a generalized search engines. We very much believe that power users will end up engaging with topic specific aggregators. We're all in Paris right now. I don't know how many people in the room are based out of Paris or even France or Europe in general, but I will bet that most of you ended up using at some point either.
00:59:12.974 - 00:59:38.590, Speaker A: Something like airline aggregator, maybe at first we started engaging when we first got into the industry. We're like, oh, I'm going to my first conference. I want to see what I can use for traveling. If we weren't very experienced travelers before, we probably use something like Google. Then in the end, we of course narrow down into, okay, I want to use my Expedia. I don't know, there's plenty. I just use expedia because it's very convenient.
00:59:38.590 - 01:00:21.420, Speaker A: But what we see for an appropriate ecosystem that lends itself to being mass adopted is that users will engage with the most generalizable meaning. The one that is able to capture the most providers has the most coverage, friendly, quite simply, just being able to understand it from the get go. I thought it was insane. The first time I got into a plane, I was like, wow, how do people actually get this? And my parents told me, oh, you just go on the Internet. And I was blown away by that. And provider acknowledge search engines. If we believe that every single one of these brands is actually incredibly meaningful to the experience of a user actually engaging with these goods and services, search engines should be incredibly intentional as to what brands they're leveraging in order to provide all these services.
01:00:21.420 - 01:01:10.090, Speaker A: What we do at Dora is we have a couple examples of it. Actually, what we do at Dora is we try to be incredibly intentional as to, hey, these are the specific tokens that you're swapping. If we click on, let's say, the crypto.com to the transaction there, and you're swapping USCC, we tell you that this USCC is being provided for Circle. If you click on every single one of the smart contracts, we try to make it abundantly clear what types of organizations these smart contracts are associated to. That way, hopefully giving people their assurance of, hey, you don't necessarily even have to trust aura, right? Whereas search engines there's a very funny thought experiment that maybe the chain is dead and the block explorers have just been producing information for you. That could be true.
01:01:10.090 - 01:01:27.122, Speaker A: Block explorers could be lying to you. But still, we don't want people to trust us. What we want is people to trust brands. That is who they seem to trust right now. That is who people I mean, Celestia is also a brand. We seem to trust them to figure out modularism. They're doing an amazing job at it.
01:01:27.122 - 01:02:04.060, Speaker A: So therefore, why should we actually change the way people just behave and think right now? And thank you so much. Love you all. Thank you for coming here. Thanks. If anyone has any questions as well, more than happy to if anyone has specific questions as to how our stack works as well because it is Infrastructure Day as well, more than happy to dive a little bit more into that. Yeah, feel free. Yeah.
01:02:04.060 - 01:02:59.610, Speaker A: I think Block is forest are actually interesting on that because we haven't found a world in which we needed to rely that heavily into our own proprietary technology. Up until recently, there was a very much around onto the bottom on stuff like indexing within EVM chains. That was something that we found as there were applied providers that ended up showing up. There's been a run on to the bottom as to how to actually go about indexing it. I think we've done a fairly wonderful job at it, not only us, but also some of our closest partners go sky. Simple hash. I think it is an interesting question as to see are we going to see a rundown to the bottom of indexing approaches on a modular ecosystem? I think that is honestly quintessential to be able to even begin to fathom whether or not we're going to be able to let these ecosystems blossom.
01:02:59.610 - 01:03:33.338, Speaker A: I think there's a couple really good teams doing amazing work at that. I think Modular Cloud is one of those teams. So let's see how we can make this ecosystem be better than everyone else. Thank you, Bonnie. Yeah, we're a few minutes ahead of time, so I don't know, looking at the panelists, I guess whether you want to jump on stage already. You're ready to continue? All right, next up is the panel on developer infrastructure. We've got again.
01:03:33.338 - 01:04:13.090, Speaker A: Bunny jordan you want to come to the stage from Astria? Celestia representative Josh LZRS, again from Modular Cloud and then Steffi from Masari is going to be moderating the panel. Are there enough chairs to sit down? Oh, there's one. No, there are plenty of chairs. All right, so please give them a hand testing's on. Yeah. Hey, guys, thanks so much for being here today. I'm super excited to talk about developer infrastructure in the modular context.
01:04:13.090 - 01:04:47.006, Speaker A: Do you want to get us started by introducing yourself and about what you're building and your protocols? I guess I have the microphone, so I'll go first. Hi, I'm LZRS. I'm the founder of Modular Cloud. So we're building cloud services for modular blockchains, and our first product is a Block Explorer. It's one of the things that we do. We have definitely a very different approach to Dora and we actually work with Dora and we're very good friends. But what we do is we index DA layers, L ones, as well as the roll ups that are on top of them.
01:04:47.006 - 01:05:29.440, Speaker A: And there's a lot of really interesting implications to doing this and a lot of different architectural decisions that we make as a blockchain cloud company that differs from ones that were previously servicing monolithic chains. So I'm excited to dive into this. More on this panel. Hi, I'm Josh and I'm a solutions engineer at Celestia Labs. And we're building Celestia, which is layer one data availability and consensus layer one blockchain. And the goal of what we're building is to make it as easy for people to deploy a blockchain as it currently is to deploy a smart contract. So yeah, really excited to talk about infrastructure for developers today.
01:05:29.440 - 01:06:19.680, Speaker A: I'm Jordan, I'm the CTO co founder of Astria Astria. We're building a modular shared sequencing chain built on top of Celestia right now using modern DA. The goal of what we're building here is to make it such that when you make deploying a roll up as easy as deploying a smart contract, that it can also be done in such a way that it's decentralized today. The L two S and whatnot that people are familiar with using all use centralized sequencers. And so we're trying to change that and hopefully create a better future for us in that regard. Hello again, my name is Bunny. Also more easy to hear you, whichever is fine.
01:06:19.680 - 01:07:00.490, Speaker A: I'm building a multi chain search engine called LSRs Said. We work very closely with them. They are building a modular block explorer, a modular indexer or also a roller power indexer, many names for it. We're building a multi chain search engine. We made some interesting infrastructural decisions, I'd say as well, in order to be able to support this modular infrastructure. We're very happy to talk about it. A lot of them is just pretty much relies on very heavy collaboration amongst all of this very composable infrastructure.
01:07:00.490 - 01:08:12.402, Speaker A: So I think a common theme that all of you guys talked about and just building modular in general is how do we make it simpler to build blockchains or roll ups for teams, right? But at the same time you have to interact with a bunch of different layers, a bunch of different piece together, a bunch of things you might not have had to before. So I guess my first question is generally how do you think that developer infrastructure and tooling can support making things easier for developers? And perhaps if you feel as though developers that are thinking of building module are a little bit intimidated in doing so and what can be done to support them, is that for me? It's for everyone. I'll take turns. Yeah, I think that this is actually a very difficult problem in engineering. But I don't think that it's specific to modular blockchains. I think the general problem is, and this is something that we face, building an indexer and a block explorer is imagine you have 1000 different chains. These are all going to be built slightly differently and there's going to be nuances in the details of each one of these systems that you can't create a common abstraction on.
01:08:12.402 - 01:08:55.262, Speaker A: You can't necessarily, for example, make an abstract block schema that you build your application on. And you can do that, but you're not going to have the depth that every single protocol, you're not going to be able to go and you're going to lose resolution on the data. So this is a hard thing and I don't think that it's necessarily a bad thing that we're facing this, but we definitely need to be making the right engineering trade offs when we're building tools. So the way we think about it is there's kind of a spectrum. Whenever you're as an engineer, when you're designing something, you're going to have two engineers in a room and one of them is going to say, let's over engineer this. Let's make this super generalizable so we can upgrade it later. It's going to be very easy.
01:08:55.262 - 01:09:52.114, Speaker A: And on the other end of the spectrum, there's going to be an engineer who's saying, wait, no, this is the requirements for the customer, so let's just build exactly what they need. And I think as tool developers and infrastructure developers in this space, we need to go way more towards that flexible side so that we can make our tools very adaptable. And so the tact that we're taking at modular cloud is we're really focused on acknowledging that we can't generalize across all protocols. But what we can do is we can make the developer experience of creating an integration which needs to be done manually, but we can make that developer experience as easy as possible. Yeah, go ahead. I was just going to say that I think that we talk about this in the context sometimes of modular blockchains. But I think it's really a whole crypto blockchain space problem where my background is more web two and big tech.
01:09:52.114 - 01:10:55.394, Speaker A: And the reality of someone in that world trying to get into this space is that there's a huge headroom to just jump in. If you're a front end dev and you say, I want to build an app and I want to try and use a blockchain, you're trying to figure out how am I going to run a local blockchain or if I'm using Ethereum, I going to pony up, whatever like $20 a transaction to dev test this application. There's all sorts of hiccups along the way and so much tooling along the way. It's very complex and it's a huge barrier to entry to actually get some of these ideas onboarded. So when we think about building in modular, we see it as an. Opportunity to kind of correct some of that to an extent make it I hear lots of people, I talk and like, well, isn't launching something modular, like more complicated? And I was like, well, we get to have a more narrow space of what we're focused on, which means we can really work on tailoring that experience, and we can make it a lot better and easier to use. And then that breadth can grow as we get more and more specializations.
01:10:55.394 - 01:11:53.660, Speaker A: And there's more pieces in this space. Much like if you use Google Cloud today, you don't really know how networking works. The fact that we're sitting here and if you want to be a smart contract developer building an app, you're like, I need to understand exactly how the core protocol works. That in my mind is an abstraction that we need to kind of move past in order to so building the tooling so that we can do that in the future is important. Yeah, I was going to say pretty much along those lines. I think one of the biggest problems while you've pointed out it's not really a modular problem, it's more of a crypto problem in terms of where the industry is at. I think the most complicated part that needs to be figured out in the next 510 years is those sort of common APIs and interfaces that allow composability but also optionality for people to pick what they want to use.
01:11:53.660 - 01:13:06.234, Speaker A: So I guess, for example, like a data availability interface that would be somewhat standard so that people can use different DA layers but also use the same tooling. I think that's something like Rollkit is working on, for example, Dimension is another example, a roll up as a service provider, they've packaged together both their rollup SDK along with the celestial lite node. So instead of having to go and start each one of those manually, you can actually do it all in one in the command line interface. So I think things like that, some of them might last, some of them might not. But I think whatever the happy middle ground with all of that is the way that this is something that we can take to the web two developers and be like, hey, do you want to try this out? It's actually fun and you're not going to spend your whole day trying to set up the light node or trying to start the roll up. So yeah, I think we have a long way to go, but it's really cool to see things like dimensions like roll up SDK, for example, that actually exist already and excited to see where that goes. I'll keep you brief.
01:13:06.234 - 01:14:00.906, Speaker A: I think first of all, we very much relate to the point of trying to create a very good, somewhat abstract, generalizable schema. It is a noble approach. It's very hard. Someone in the room is very some people in the room are very familiar with that, but yeah, very much resonate and echoing the fact that it is a lot of trying to create packages, trying to create bundles, trying to create any semblance of, hey, we maybe figure out the best way to go about building a DeFi specific app chain. NFT specific app chain, DeFi enabled NFT app chain you can leverage composability. I think it's a very wonderful thing once you are able to come into an ecosystem and come into an entire industry and say hey, there's a lot of people that have done a lot of the groundwork for me already, I can just get extremely creative with whatever I want here. Yeah, I totally agree with that as well.
01:14:00.906 - 01:15:06.194, Speaker A: And I think the challenge then is on you guys in developing your protocols to make it easier and abstract that away so they don't have to think about it. So I guess a question for all of you again is what are some of your biggest challenges in doing so? What are the biggest hurdles that you have to go through to be able to make it simple for everybody else? Things are always changing and I think in the early stages of all this we don't really know what the best way to do it is. So speaking of the DA interface with Roll Kit, for example, we might have a beta version of that, but at the same time we don't know that that's the final version. So I think it's just, I guess a factor of where we are at in the lifecycle of this modular ecosystem and crypto ecosystem as a whole. Yeah, I'll definitely echo that. I think that the amount of attempts on goal is the key. So the cool thing about this and one thing like for example, that Astria enables is for it lowers the barrier of entry of having a decentralized chain.
01:15:06.194 - 01:15:52.930, Speaker A: So it used to be really hard to deploy a blockchain based application and now with Celestia, with shared security and now shared sequencing and stuff like this, it makes it really easy. So from the perspective of people actually building applications, we're going to have so many more shots on goal, on trying to find out that one thing that hits a zeitgeist like Bitcoin did. I mean, I think people don't realize when Bitcoin came out, nobody thought that that would be something that people would actually find valuable. But that application was very valuable. And that's why we're all here today, because everyone got inspired by this. We started trying to improve it and now we're all here at the modular summit and I think that's one thing that's going to happen. We're going to find out the killer apps through making it easier to experiment.
01:15:52.930 - 01:16:28.434, Speaker A: And we're also on the tooling side also trying to build systems where basically right now the approach we're taking at Modular Cloud is we're finding services and APIs that people want. Like for example, people want NFT APIs for EVM rollups. So we have an NFT API. This is something that there's clear demand for and we're trying to build a collection of these things. And then what we're doing is we don't know what a generalized platform is. We don't know how the design should be for a generalized platform that can provide cloud services to all protocols. That's a really hard problem.
01:16:28.434 - 01:17:41.938, Speaker A: But what we're doing is by making a bunch of attempts, we're finding out these general patterns and we're discovering how these things should be built. Yeah, I think I resonate a lot with the idea that you need a lot of attempts. And I think that's one of the things that the modular space really enables and one of the things that we think about trying to enable as a shared sequencer is launching these roll ups. What's the likelihood that of the first two blockchains that really became big, that those are like the final end state and enabling more iteration on what you can actually do? And I think that's really important. The difficult part is that if you want to be the base of that foundation, you have to make it such that people can actually innovate. And so I think there's a lot to what Josh said here of we're building things for the first time and then we're building things on top of things, which are iterating pretty quickly and there's a lot of interconnectedness in bootstrapping your own project. And so I think one of the big things that we do internally is just dog fooding.
01:17:41.938 - 01:19:02.144, Speaker A: What we do it's, hey, we have this and it's like, cool, let's go build a roll up on top of it. You have to actually make sure that what you do and it's not perfect dog fooding. We come with our own biases. And you try to work through that and that's why you get product out to people and you get it in their hands and you find out the same as any Iterative development approach. It's like, okay, what was wrong with it and what can we do and learn from that? Hello, can you repeat the question again? Oh, yeah, I was just saying, what do you think are the biggest challenges for you in your protocol? Having to make things easier for the user or the developer? I think one of the biggest one is honestly, again, to echo everything that everyone said in the room, but actually, to give a very concrete example, we got a chance to because we're with a multi search engine, we got actually the chance to chat with a bunch of node operators throughout this week. They were very enlightening chats because we got to see how one of the very core entities on the stack is able to think, to reason through whether or not to support a certain ecosystem. We all form these very implicit types of equations, some more explicit than other.
01:19:02.144 - 01:19:58.240, Speaker A: Some very large node operators have a very concrete equation that they use to actually figure out whether or not to support a certain ecosystem one disclose which. But something interesting is that one of the very key components of that equation is just how much is this ecosystem? Iterating on their notes. Being able to actually tinker around with your notes is something very powerful. It's something that you can get very inclusively complex with. I mean, there's nothing necessarily intrinsically wrong with it, but there is a point in which infrastructure providers really want to feel comfortable saying, okay, this is going to be the one version of the client that I'm going to be able to spin up 28 nodes per ecosystem, per chain, whatever. And I'm going to be able to provide industry grade offerings on top of it. And yeah, that type of offering is just something that has presented to be a very insane straw for everyone.
01:19:58.240 - 01:21:02.704, Speaker A: Yeah, we might be able to center schemas and such, but still it's something as simple, quote unquote simple as defining, okay, this is the no client can prove to be a very hard thing to do. Totally. And I think going on to the idea about different building in one ecosystem first and then perhaps moving to another, I think some of you were starting as the ethereum ecosystem, others more so in the Celestia ecosystem, I guess. Why would you select one over the you? If everyone wants to let me know why you kind of went with one Direction first and if you feel there are times where it's just best to stick to one ecosystem and not try to integrate with all of them, there's just a certain level at which it just becomes too complex. Okay, so I'd say honestly, very practical things. I do tend to think a lot about infrastructure as being the byproduct of current economic incentives. In that sense, incentive alignment at this point is one in which, let's say we do incentivize a plurality of validators within the ethereum ecosystem.
01:21:02.704 - 01:21:24.460, Speaker A: You run a validator on ethereum on the active set, you can get paid to be able to secure ethereum. That's great. You run a full node, not so much. So from this standpoint, there's very much the approach of, hey, I like this technology. I find it very cool. I think it's very impressive. I think it's the most secure, the most optimized for whatever my use case is.
01:21:24.460 - 01:22:04.936, Speaker A: That's great. Is it a viable economic solution long term. And that is not to be, let's say, incredibly cynical of a lot of really wonderful technology, but that is also just to say, hey, we do need to make infrastructure is only as sustainable as the economic mechanism holding it. So from that standpoint, that is something that we took very much into consideration. We looked at all the different ecosystems that were the ones that had the most plain and simple demand, the most demand of people asking us, hey, can you build this? Can you build this other thing? I'll pay you right now. I need to get my hands on whatever you're building right now. And that is a very clear cut example for it.
01:22:04.936 - 01:22:54.004, Speaker A: However, we also know that we're in crypto. There's the ecosystems that are very wealthy and powerful still, and they may not lend themselves to give us as much flexibility as something that we can actually do within a modular ecosystem. So that is also why we decided to build on top of it. We very much think that people can get very creative with it, and there's nothing more powerful than very creative infrastructure. I think to that point, on the economics of it all being important, I think I take a slightly different approach. I 100% agree. But I go down and I look at just kind of a first principles of, like, okay, if I'm trying to build something scalable, right? We talk about like, oh, we want users, we want actual useful applications in crypto.
01:22:54.004 - 01:23:47.588, Speaker A: One of the things I always say to that is like, okay, I used to work at a web, two startup, and we had a database that had two terabytes of information. That's like six years of Ethereum data. So the question is, like, okay, if we want to actually have these useful applications, we need scalability, we need a huge multiple. And so coming from a first principles of that is like, okay, how would I build that? And that's, I think modular is just if you're building software in the world, for the most part, if you want to scale, you end up going more modular. You have specific specializations of what you're working on. And so I think that's a part of why I look at this space and then to the point of the economics, it's like, okay, well, Ethereum can also make these changes, and they're working on things that will help with scalability. But it's a question of also just, like, velocity.
01:23:47.588 - 01:24:15.140, Speaker A: You have all that economic security. It's all in this one space, and you have all this liquidity. It's difficult to move. It's a financial product with a ton of money. It's like, how quickly are banks changing their banking systems? Right? It just doesn't happen that quickly. So if you want to actually come in and challenge the incumbent and be like, hey, we offer something better, I'm not going to do the exact same thing as what the incumbent is doing. I'm going to actually try and change a lot faster.
01:24:15.140 - 01:24:45.740, Speaker A: Yeah. So we first started off supporting Celestia. That was our first integration, and that's a Cosmos chain, essentially. So when we launched our Block Explorer, we only supported Cosmos. And we've since added EVM support. And we do have proof of concepts of other VMs that are compatible with our infrastructure, but those are the two primary ones. I just think on a practical level, if you support right now, Cosmos and EVM for your tooling.
01:24:45.740 - 01:25:37.820, Speaker A: I think you're going to have compatibility with a supermajority of apps that people are using. So that's a really good starting point. But I think on a theoretical level, the reason why we started with Celestia is back to this point that we were talking about earlier, about getting a lot of shots on goal and essentially what is possible with let's say that you have the Ethereum virtual machine and you want to improve it. If you wanted to contribute that upstream to actually the Ethereum network as something I talked about in my talk earlier, coordinating with a large group is extremely hard. So that's a very difficult process. If you want to change that execution environment, it's very difficult to do. So if you look at a blockchain, there's actually a lot of things in the monolithic architecture that's all bundled together and these things, they don't actually have to be.
01:25:37.820 - 01:26:35.776, Speaker A: So what needs to be bundled? We need a lot of people to all provide security to the ordering of the data and there is a strong network effect in that. And if you subdivide that into many different L ones I'm not saying that there's not viable, I think it is very viable and there's a lot of people who are great solutions for making L ones that are other than just the major top ten chains. I think we're going to see even a lot more L ones. But you have to admit that it does sacrifice security and there is kind of like economies of scale to security as well. And especially with data availability sampling, as you add an additional byte to the network, it's actually sublinear scaling. So it actually takes like log n steps of verification, more per n bytes. And so there's actually a really great property of having an L, one that's just dominant and that most people contribute to.
01:26:35.776 - 01:27:21.644, Speaker A: But that network effect does not necessarily carry over to other parts of what the blockchain is. So in my talk, I talked about the fork choice rule. And you can a roll up is basically defining this. And this is operating on another fork choice rule, which is like the L one I gave this talk. Earlier today and essentially being able to customize a lot of that stuff. Actually, it makes it possible to make a lot of improvements to the way that these systems work. And a lot of things in Ethereum we were kind of just figuring it out as they went and it's not necessarily like I think someone had said earlier, just because what are the ODS that one of the first blockchains was optimally implemented.
01:27:21.644 - 01:28:24.772, Speaker A: I think that there's a lot of improvement but going through an upgrade path of actually convincing the community to take every single change necessary to get there is really difficult. And also you don't want to experiment at all in one of those situations because if you try a crazy idea and it destroys the network, you're going to nuke everyone's bags and we're all going to be unemployed. So we really don't want to do that. So I think what's cool about modular blockchains is you can have a bunch of roll ups on top of an L one. This L one has a giant network effect. But the actual implementations you can have these all be permissionless. And what's kind of cool about this is it actually is a better way of building a blockchain than it has a really interesting productivity scaling property where you can parallelize the iteration process because all the different people that are trying to make different roll ups are not needing to communicate with one another.
01:28:24.772 - 01:29:07.936, Speaker A: And as a result, you can have a much, much higher speed of iteration. Something it's kind of like if you have like a distributed system and your application is not designed for that, you can't just add notes. That's like trying to build something within the confines of a single project where you have to agree with everybody. You're going to have a lot of communication overhead between each person trying to put forth an idea. But if you have permissionless, innovation, you can do everything asynchronously and this can scale, you can scale the amount of innovation with the amount of people trying to innovate because everyone can take their own bets. And I think that that is why I believe that the modular ecosystem is going to be where the dominant amount of innovation happens. And as a result, I think there's going to be a large amount of activity in the future on that.
01:29:07.936 - 01:29:45.736, Speaker A: And that's why we took a strong bet entering this ecosystem very early. I guess I'm a little biased here. I work for Celestia Labs. But I think the thing to take away for me is that people building on top of Celestia and actually building on top of other DA layers. I forget the actual question, but I think the reason that I really like I know where I was going to go. The question was just why start out building in one ecosystem versus another or maybe staying in one. Yeah, so I'm definitely biased.
01:29:45.736 - 01:30:38.944, Speaker A: But I guess the thing that I do have flexibility of my choice is what execution environment I'm using. So I just think in the grand long term, having standardization to some degree between those execution environments, as great as it would be to keep them in the Celestia ecosystem, I think that having the ability to connect with other DA layers that are building the same sort of consensus, like shared consensus network, is something that can be really powerful. All right, want to jump in a little more here into your specific protocols? I'm going to start out with both Bunny and LZRS. You guys are both building block. Explorers or search engines. Would love to hear from you guys how your approaches differ or also how they could be similar or additive to each other as well. Yeah, I can go first on this.
01:30:38.944 - 01:31:23.224, Speaker A: Yeah, I think they have a fantastic block explorer. Ours has a very different angle. First of all, it's primarily for developers and we are trying to build we have an indexer that works with roll ups and lots of different chains, including DA layers. And we think of our Block Explorer as kind of like the web console. For this, you can log into AWS and set up your services and run computations in their cloud. And we think of our Block Explorer as just the web interface for our cloud. And it's also kind of a reference implementation since it is an open source Block Explorer.
01:31:23.224 - 01:32:02.120, Speaker A: It's a reference implementation. So someone who wants to build a product with our cloud can look at how we use it and they can just copy that. Exactly. And we would love for them to use our APIs because that's how we make money. And I guess the last but not least, I would just say that when we design it, our approach is that we are very developer focused because we're working mostly with long tail projects, things that are there's not any users at all. But no, it's true. The users are the companies that are developing it and that's who we're talking to a lot.
01:32:02.120 - 01:32:32.384, Speaker A: So they're debugging things right now. This is the stage that they're at. And of course, are there users on any blockchain protocol? I mean, that's a question, but I'm sure there's hundreds. But we are trying to make all the tools that developers need for building applications and debugging their protocols as they're building it live. I can confirm there's users in other blockchains. True. Yeah.
01:32:32.384 - 01:33:02.172, Speaker A: As long as the blocks are not empty, we're happy. Or even as long as there's people that need Block Explorers that are very we used to use the phrase human readable quite a bit. I feel like over time now, I've grown a little bit more detached from it. I think it's just more so from the standpoint. It's also a not so great word, trying to find a new one for it, but legible from the standpoint. That is something that people can just understand. Maybe friendly is the appropriate word for it.
01:33:02.172 - 01:33:38.520, Speaker A: Something that you can feel is approachable, something you can relate to. Something that does not assume any single level of complexity from the user prior to knowing, oh, I don't know what is a swap, but everyone can understand what is ascent and receiving what is sending and what someone is receiving in a swap or exchange transfer. Beyond that, a lot of our infrastructure is set up in such a way that we are able to actually leverage infrastructure like models to collaborate on. Let's say, hey, you have a really wonderful indexer. Can we get some of that data? Right. Also index the same way that okay. Going back to the economics of it all.
01:33:38.520 - 01:34:57.600, Speaker A: We do believe that, let's say if you're building critical infrastructure for a lot of these ecosystems, even whether or not they are long tail or very much at the center of the Velcro, you should be still getting paid. You should still be a very sustainable project on your own, right? I think we more so prioritize the fact that developer tooling can only takes us so far in such a way that we can allow people to create, we can allow providers to create goods that people can start interacting with very easily. However, just developer infrastructure stops at the point in which as soon as end user starts trying to engage with your ecosystem, that is where you don't want to show someone, hey, here's the console, right? You very much want to create a product that someone else that knows very well how to use the console can then use a product like ours and say, hey, look, you just minted this. You just engaged with this good. You just participant in the service, you just send money around the world. You were just able to bank yourself or even, let's say something as communication. I actually didn't have a phone number for an entire year and I think, yeah, very free and I highly recommend it.
01:34:57.600 - 01:35:18.170, Speaker A: But being able to leverage our infrastructure, let's say peer to peer messaging networks, I had no problem living around the world that way. That's awesome. Might try that actually one day. Just not having a phone number. It's great. I had a couple more questions for you guys, but I think I'm going to move on just so we don't run out of time here. So sorry.
01:35:18.170 - 01:35:53.008, Speaker A: Oh, I have five more minutes. Oh, okay, perfect. All right, so great. So basically the way I understand it, Bunny, you're more focused on the consumer side of things and LDRS more. So on the developer kind of backend kind of things. I would love to hear, I guess starting with you more about the cloud platform that you're building and how you can help. Yeah, I think one issue that we're going to be facing very soon is it's becoming very easy to launch a chain.
01:35:53.008 - 01:36:43.092, Speaker A: And that means there's going to be a lot of chains. And these chains are continuously producing blocks and the cost of putting data on these chains is now plummeting. So people are going to be putting a lot more data on these chains. And so one interesting thing, first of all, there's basically two, this leads to two challenges. One is, and I think I might have mentioned this earlier, but fragmentation of the different types of implementations of everything. So if you have a traditional indexer and cloud based application or tool that works with, for example, EVM chains, you're already not going to have compatibility with every non EVM chain. And as we get more chains, that's going to be a large selection of interesting projects that you're not going to have compatibility with.
01:36:43.092 - 01:37:29.010, Speaker A: And even within the EVM space there's fragmentation. So not all EVMs are the same. There's some EVMs that don't have a given RPC endpoint that you might want or they have some pre compile or it just doesn't work exactly the same way. And it's actually becoming increasingly hard to just simply make an EVM compatible tool. So one thing that we're doing is we're just trying to think from first principles, how can we rebuild these systems so that we are not making assumptions about how a protocol should work and we can just let people define that. But as I had mentioned earlier, we want to make that developer experience as easy as possible. So that's one of the things that we're doing.
01:37:29.010 - 01:38:24.772, Speaker A: And the other thing is kind of gets into the economics of actually running a cloud based company. When you have a lot of chains, you're going to have this long tail of chains that are somewhat interesting, but maybe they're just not extremely well adopted yet. Or they have potential, but there's just not that much usage yet and you might want accessing. Any one of these given chains probably doesn't give much value to people, maybe not that many people want an API for that, but a lot of people will sometimes want to access data from some of these chains. And so I can't remember the statistic off the top of my head, but last year I was quoting this thing where there's some very large percent of Google searches that are long tail. So people are searching for data, searching for information on Google, and a lot of these searches have never been searched before. It's a non trivial amount.
01:38:24.772 - 01:39:18.310, Speaker A: It's actually surprisingly high. So what that indicates is that people, when you have a distribution like you have a head of data, most people are accessing data in the head. And this long tail, there's a lot of demand to access data in that long tail. The problem is that it's really expensive to store that. And if you just say, okay, well I'll just buy all these terabyte hard drives and just put them in my basement, it's also then hard to actually run computations on that and access that and query that in a way that is performant and acceptable in a way that is useful for people building applications. So we're trying to make the infrastructure on the cloud side of things that can allow people to run computations and query this data set and make that available for everyone. So this stuff doesn't just get lost to history.
01:39:18.310 - 01:39:52.190, Speaker A: You're up. Why consumer focused? Pardon? I guess the question was just I believe that you're kind of more on the consumer side, helping users interact very easily with the Block Explorer, more so like a search engine. So I was just curious as to why you picked that direction over something like modular cloud, which is more of the back. End side of things. Sure. So I say we very much are intentional with the wording of referring to our product as a search engine as opposed to a block explorer. But I'll actually just hop off.
01:39:52.190 - 01:40:26.670, Speaker A: There's a lot of queries that do not get actually searched on Google. The statistic is insane. As you were saying, I was just thinking, okay, just think of a standard query, like five letter, like, what is and then just insert whatever word do a permutation on that, on those characters. It's going to be an insane number, and yes, they're never going to get searched. And I do often find myself finding stuff that, oh, there's no search result for page for this. I think it's kind of fun. Then how do you build a product that actually allows you to access these things? You don't necessarily build a product that allows you to access only hash data.
01:40:26.670 - 01:41:10.760, Speaker A: You may want to do that if you want to specifically, again, take the approach of letting people be extremely developer focused. That is great, that is actually the perfect product for that. That is how you want to reference stuff. Maybe blockkite. Some people do that as well. Why not support it all? But the point in which we were very intentional on us being a search engine support to blocksper is we believe people don't necessarily want to search for a specific data set or a specific how to say it factual information that all of the top 100 searches on Google every single year are all brands. They're all brands, they're all goods.
01:41:10.760 - 01:41:59.604, Speaker A: There are services. The only exceptions are stuff like weather food around me and stuff of the sort in different languages as well, because everyone searches is not something appropriate to the crypt ecosystems. Therefore, I think it's such a good approach to actually take it. Okay, if everyone in the world searches and he's saying billions of searches every single month, I forget the precise amount, but it's definitely on the two digits end of things like 36 billion searches, I think, every month. Don't quote me on that. You can Google it yourself or dori it yourself someday. And we take that approach because we want people to actually not search for this factual information on every single block, every single transaction, every single internal transactions, locks, whatever the coded locks.
01:41:59.604 - 01:42:36.960, Speaker A: What we want is people to actually search for these goods and services. And at Dora, when you search for NFT Smart contract right now, this is only bound to, let's say, Aura ERC 721 proxy contracts. But we want people to be able to mint within Dora, which you are able to do right now. If you search NFT Smart contractor proxy 721, you're able to mint it within Dora. If you search up Dex within Dora, we generate a friendly UI for you to be able to engage with the swapping widget within that. Right now, it's of course, bound to one of the most used ones. We use the uniswap widget.
01:42:36.960 - 01:43:13.328, Speaker A: In the future, we hope to have this very generalizable infrastructure allows you to leverage every single goodin application regardless of whether or not it's the first. So last time that someone's looking for it as well, we think that that is how people relate currently to goods and services. So therefore, why actually not offer that in the space? We have a lot of providers for many things. Why not actually give it to people in a way that they're already used to engaging with them? Sounds great. You guys are really additive to each other and kind of approaching it from a different angle. I think it makes a lot of sense. So thanks for going into that.
01:43:13.328 - 01:44:14.976, Speaker A: Jordan, jumping over to you on shared sequencers. There's a number of different shared sequencer networks popping up, including Astria. Would love to know what's different about Astria from both, I guess, the developer infrastructure experience for the most part or how easy will it be to plug in and use your I mean, first of all, we're really excited to see more people building the shared sequencers. We think from our perspective, it's only positive to have more people thinking on this and more people working to push us to build something great. I think the reality of this space is that the shared sequencer narrative took off in March, April, and I think Espresso yesterday launched like a demo thing that they posted, which is great. Pretty excited for them. We're going to do a demo of what we have a little bit later here today at a workshop.
01:44:14.976 - 01:45:15.236, Speaker A: So I think the reality is that it's really early and the debating over who has better product UX and whatnot it's going to be really early. I mean, to an extent where it's like I looked this morning, I was like, oh, Espresso had this post. Let me go look and see how this works because I'm going to be talking about how we're all thinking about our user experiences and it's just so early to really give you this is our differentiator in terms of how our product actually works. What I can talk to is our experience and whatnot which is that my background is in developer tooling. I worked at GCP, I worked at Meta doing their API designs and worked at a CI CD company for a while. So it's really our background is building developer tooling. And so everything we do comes from kind of that mindset of what is a user, what is their journey, how do we make it so that it works, what are all the things they're going to want to do, how do we work on those integrations? And then from here we get products out.
01:45:15.236 - 01:46:09.370, Speaker A: And then like I said earlier, it's going back, it's hearing what people have to say and continuing to improve on it before we get to a final launch. And that's kind of how we think about it. Thanks for that. I'm also kind of curious from the node operator perspective, when people are coming in to run a sequencer. Maybe it's too early to touch on that as well, but I'm just curious to hear who might be such an operator. Is it someone like a traditional validator or professional node operator that already exists for other kind of things? Yeah, I guess I have kind of a follow up question, but I can answer both of these, which is when we talk about a node operator, are we talking about validating sequencing nodes or are we talking about a node that someone runs at home so that they have access to the data? I meant someone the former. Okay.
01:46:09.370 - 01:46:54.150, Speaker A: Yeah. I think the way I think about this is similar to other specialization pieces and similar to tendermint. We'll go after traditional validators for the most part to start with. I think this is a big part of the appeal of a shared sequencer network is if you want to go and launch your blockchain, we're going to do the legwork of making sure that there's good Validators that are using it. That's part of launching this product. And that way when you go and you launch your roll up, you don't have to do that because it's just a people coordination problem. And so whether or not that's an institutional node operator or that's an individual with their special high performance setup, I think at the end of the day, it just comes down to doing it well.
01:46:54.150 - 01:47:28.544, Speaker A: I don't have any specific biases one way or the building. Our sequencing chain is built on top of Comet BFT, so lean on top of the institutional knowledge and I say, really ecosystem knowledge that people already have in terms of running those. Thanks for that. Yeah. Josh, your turn. So you're a solutions engineer at Celestia. Would love to know a little bit about what you work on day to day and how you're helping the entire Celestia ecosystem build things on top of it.
01:47:28.544 - 01:48:02.670, Speaker A: Yeah, so I guess a lot of my day to days, mostly maintenance of our documentation. So if you've run a light node, there's a good chance I've at least edited something that you followed to do that. If you're running a roll up on roll kit. I also work with their documentation as well. I guess on the integration side of things, I work on different execution layer integrations. So I worked with the Op stack integration with our engineering team. Kind of like polish the docs up for that and give it to the end developer to go ahead and play around with.
01:48:02.670 - 01:48:59.068, Speaker A: But I guess one of the most fun things that I've had enjoyed the most recently is trying to abstract away the console or the terminal for people when running a light node. So I have like, a beta version of this Mac OS app that I made that you can just download on your Mac like you would any other application and start a node up. So yeah, I kind of just experiment with things. But I think that's the beauty of where we're at in this space. We're still figuring all these things out. There's many EVMs as LZRS was talking about, that could be used as execution environments, but some of them have different pre compiles and just like different properties about them that aren't all the same. So figuring out which one of those are best in the long run I think is going to be a really important thing for this space.
01:48:59.068 - 01:50:10.112, Speaker A: I kind of got a little bit out in the end there, but yeah, you're good. A little bit all over the place, but mostly experimenting and helping people post data. Maybe this might be a bit too early of a question, so apologies if so. But going back to our first kind of question about how easy things will be, given everything you just said, how easy do you think it will be to launch a roll up using Celestia? Both from the perspective of someone just doing it themselves or developer doing themselves or leveraging a Ras solution? Yeah, so I guess one of the next ideas I have for this Mac app is to first put in a transfer function so it can act like a wallet. I want to also put in posting data to the network function, but the bigger addition to that would be something where you can go in and literally click start a roll up or initiate a roll up and maybe leverage something like Dimension, for example, to do that. So I think in the next year or so there are going to be things like that that exist maybe even sooner. Conduit is an example that has not built on top of Celestia yet, but they do have the ability.
01:50:10.112 - 01:51:05.184, Speaker A: If you go to their web interface and make an account for free, you can start your own roll up on Gurley and I think that's fascinating. They handle all the infrastructure for you. You can inspect the logs of the actual software that's running and I'm very optimistic about this basically drag and drop sort of roll up infrastructure. Like the same way that if you want to go make a web store right now, you can use Shopify and basically have no knowledge of development or anything and you have something that works and is a fully fledged product. I guess I'll stop after this, but I think that's one of the beauties of where we're at is we're going to be able to figure out all the different pieces of the stack really well. And the really optimistic vision I have is that the same way that blockchains work in the future is the same way that software works now. I know it's called the OSI model.
01:51:05.184 - 01:51:45.910, Speaker A: I don't know much about it, but there's seven different layers in the way most applications we use operate. And I think I want to get to a point where no one has to know what's going on under the hood of that roll up. They just know that they have a roll up here's, their special key to access it or whatever. But I don't know what happens when I send an email. Like every single layer of my email, I couldn't tell you what happens. So I hope that we get to a point where developer, tooling, and even just the end user products are at that place where people don't have to think about it because it's overwhelming, to be honest. And I think that's probably one of the biggest challenges of this space.
01:51:45.910 - 01:52:18.696, Speaker A: So I think we're out of time. Yeah, out of time. Thank you guys so much for being here. Would love to do it again, sometimes have some more questions for you, but it was really engaging conversation, so I really appreciate your time. Thanks again, everyone. Thank you. All right, everybody, so we have a short break now because one talk fell through, so please be back or stick around at around 12:10, I think, to see Aditi.
01:52:18.696 - 02:11:05.140, Speaker A: I think she entered the room at some point. There she is. Yeah, stick around or make sure you're back at around 12:10. Thank you guys again. You sam sam sam sam SA om. It's sam sam it it's sam SA. It's ram sam jam sam sam WAM SA sam sam me.
02:11:05.140 - 02:11:33.258, Speaker A: I'll just point to you. It is, yes. There we go. Okay. Oh, wow, this is such a beautiful picture and just image. Amazing. Okay, today I'll be talking about capsule, but the context of why this is such an important topic for modularity and this summit in general is because as we increase the surface area of chains, we actually end up with more problems.
02:11:33.258 - 02:11:50.880, Speaker A: And so this is where transaction signing infrastructure becomes really relevant. Next slide. Wait, there's a slide before that? No, keep going back. There we go. Okay, cool. Next slide. The perfect.
02:11:50.880 - 02:12:24.710, Speaker A: Okay, so congratulations. We did it. Turn to your neighbor and give them a pat on the back because hell yeah, we're on our path to solve the scalability trilemma, which was necessary to solve for so called mainstream adoption. So, yay, we're on the path there. But because of this, we've actually introduced a bunch of new problems that hinder potential mainstream adoption. So that's kind of what I'm here to talk about today. Next slide.
02:12:24.710 - 02:12:46.370, Speaker A: Thank you. So here's what MetaMask looks like today. Managing a bunch of tokens, managing a bunch of networks. It's a very complex flow to sign transactions. Next slide. Assets are basically everywhere, and essentially as chains, the number of chains grow, this problem gets much worse. Next slide.
02:12:46.370 - 02:13:25.854, Speaker A: And so, yeah, I've been thinking about blockchains in general since 2017. Started my journey at consensus, had a stint at the Graph, was recently at Celestia, and then ended up joining Nitia, the founder of Capsule, to think about this problem, because I was kind of observing that there needed to be a better way to manage transactions across all these chains. So more into that on the next slide. So here's what we know about the path to mainstream adoption. Of course there's more steps on this. It should say one, two, three, but yeah, it's fine. Yeah, there's probably more steps on here.
02:13:25.854 - 02:13:59.560, Speaker A: But for all of us, the steps to mainstream adoption are, one, we know we have to separate out execution from the L one to just scale better. Two, we have to create more compelling applications to bring people into crypto, and then somewhere in the middle, we need to profit, hopefully. Next slide. And so if we think about this in the framework of three product categories, applications, wallets, and chains, we actually end up seeing something super interesting. Next slide. Yes. Perfect.
02:13:59.560 - 02:14:26.000, Speaker A: There you go. Perfect. We've successfully added more chains to the spectrum and protocols because we've obviously been thinking about scalability a lot for the past three years, to the point that we've created more and more block space and created more forms of execution like altvms, zkvms, et cetera. So this side of the space is kind of teeming with activity. Next slide. Next. There you go.
02:14:26.000 - 02:15:11.982, Speaker A: Also, we know on the other side of the space applications, we've seen a fair amount of activity there. So we're somewhat there in terms of mainstream adoption with apps like OpenSeas or a rabbit hole lens. But frankly, it's been a challenge to see a lot of mainstream people use these applications outside of things like AirDrop farming. And so what gives? Next slide. Go back. Yeah, the Capstool thesis is that solving for mainstream adoption is not just about creating more compelling applications. I think this is a very prevalent narrative on Twitter today where people are like, devs should be building better applications and use cases suck, but we actually think that's a misguided take.
02:15:11.982 - 02:15:52.570, Speaker A: And the problem is, of course, innovating more at the application layer, but it's also managing the transacting experience across this wide surface area of blockspace we've created over the past three years. And so the key to that is the wallet layer. Next slide. Everyone should just take a moment to cringe because I know wallets are just overdone and over talked about, so let's just take a moment to accept that. And, yeah, the reason why it's been so exhausting to talk about wallets is because we've thoroughly just exhausted our mental space for wallets in general. And I'll kind of explain why that is in the next slide. There we go.
02:15:52.570 - 02:16:24.454, Speaker A: So the reason why is that over the past few years, we've innovated a ton at the UI layer, but not on the fundamental transacting layer. So this is MetaMask rainbow Phantom kind of represents like 2018 still need to switch a bunch of networks, you still need to sign transactions or messages transact. You also have to inspect the transaction to make sure you're not signing anything. Jank. And so it's still very much the same as far as, like, third party wallet applications go. Next slide. There's also been a lot of innovation at the embedded wallet side of the space.
02:16:24.454 - 02:16:38.438, Speaker A: So this is like coinbase, Wallet, circle. Magic Link. Web three. Auth. There's a bunch of players. I think Engine as well is in the space. But this wallet as a service product category is also pretty exhausting.
02:16:38.438 - 02:17:10.870, Speaker A: And the reason why is you get this clean experience of not having to deal with signing messages and signing transactions in the middle of your application's experience, but as soon as you want to take your wallet and go elsewhere. So Celestia talks a lot about this. Exiting with interoperability, it's very, very difficult. So these are the steps you have to go through to actually take an asset from an embedded wallet and actually import that into MetaMask and then list that asset on OpenSea, for example. There's a lot of steps. And so that's also very confusing and exhausting and bad. Next slide.
02:17:10.870 - 02:17:52.450, Speaker A: The third way we've exhausted ourselves is with third party app building wallets. So if you think about this, it's like Uniswap building a wallet or Robinhood building a wallet. This is awesome and actually works a lot in favor for the application. So the app now has full control over their end user experience. They no longer have to go through, like a MetaMask, they can just go through uniswap wallet. But it now requires Hayden and the team to fight the good fight with giants like Apple on App Store policies and making sure the app doesn't get delayed and kind of growing their user base from zero, which is a really tough thing to do. Next slide.
02:17:52.450 - 02:18:41.970, Speaker A: And so you can see why we're kind of exhausted of wallets and this market in general. And at a high level, this layout is kind of where all these products sit. It's crowded, it's confusing, it's not perfect. And there's an obvious gap of needing ways to transact that are embedded and don't require leveraging a third party app like a MetaMask in the middle, but also managing the signing and switching networks and making sure that you can take your keys and kind of go elsewhere and take your assets elsewhere as well. And so the crazy thing is that this is what this market looks like for just a single chain. So just within this single chain paradigm, you have all these options. And so as block space surface area increases Mochain.
02:18:41.970 - 02:18:51.750, Speaker A: No problem. There we go. You guys didn't say that. Mochain problems. There we go. Yeah. Participate.
02:18:51.750 - 02:19:33.906, Speaker A: Take two. And so next slide. Yeah, there we go. In a world where, thanks to modularity and better scaling, wallets as standalone products are constant RPC endpoints, always switching network in the middle of your flow and just shifting the user friction, which was once on the infrastructure side of things down to applications, which is not good. And so next slide. And so this gets keep clicking 12345. There we go.
02:19:33.906 - 02:20:06.350, Speaker A: Now we're done. So this gets a thousand times worse. So this is just a couple of examples of L, two S or networks that have shipped recently. So there's like the public goods network, Zora network base Claros also shipped recently. And so as we keep increasing the number of chains we're transacting with, you kind of need a better way to manage the transacting experience and the intention to transact in a clean way without making user experience really bad. Next slide. And so our core insight is forget the wallet.
02:20:06.350 - 02:20:51.418, Speaker A: I think wallet as standalone products are really tough and instead focus on the ability to natively transact. So in a world with mochains mo problems, having a third party app like a wallet just adds to the confusion. And so if you have a native way to transact within the application or the product, you remove the ability, the need to kind of switch between signing things and also needing to kind of have a browser extension to transact. Next slide? Yeah. So this is kind of how Capsule thinks about it. And we'll go through a demo as well. We basically add a notion of the image is a little messed up, but that's okay.
02:20:51.418 - 02:21:22.870, Speaker A: We add in a notion of permissions into this experience. So by permissions, we remove the discrete proposing, signing and inspecting of transactions that need to happen in today's kind of transacting paradigm. And so how we do this is we take MPC based key management and we wrap it with a permissions engine. So no more pop up switching networks. But you also get the interoperability benefits of wallets by being able to take your keys everywhere. So functionally. Next slide.
02:21:22.870 - 02:21:35.050, Speaker A: Yeah, functionally. What this looks like is a developer tool. You can click again. It changes color. Yeah, functionally. Yeah, right, functionally. What this looks like is a developer tool with a single sign on experience.
02:21:35.050 - 02:22:11.350, Speaker A: So this means theoretically, a user could onboard onto an EVM specific application and then one click onboard into a Cosmos wallet, which removes the need for the Cosmos wallet to acquire new users and also get users to bridge through the friction of going into a new ecosystem. And so we think this design kind of drives network effects by not only simplifying the customer acquisition process, but also the user's transacting experiences. Next slide. And so this is a demo. I'm not sure if it's going to work. Click again. Okay, great.
02:22:11.350 - 02:22:46.258, Speaker A: So what we're doing here is, again, the single sign on experience of creating a wallet. We can do email, phone number, WhatsApp, whatever you want. We verify that with a six digit code. Does that work? I guess we'll do it again. Okay, that's fine. Next slide. Anyway, the point of the demo is you can log in with any form of auth leverage hardware enclaves to transact and also simplify the user's transacting experience with this concept of permissions, which the demo should show you.
02:22:46.258 - 02:23:10.308, Speaker A: Next slide. No. Okay, maybe that was not on me. That's fine. No, just to go to the last slide. Previous? No. Previous? Yes.
02:23:10.308 - 02:23:52.690, Speaker A: Okay. So the takeaway, ideally post the demo, which I can send to everyone here if you're interested, is wallets aren't the problem. Our mental model of how data is written to the chain is the problem. In a world where we have tons of chains, users should not know, ideally, in my opinion, and won't know what chain they're transacting on. And in order to live in that world, you probably need a new paradigm of writing data to the chain and transacting and ideally, the last bullet. This doesn't just create more and more silos and walled gardens. This is created in an interoperable way where you have a broader abstraction layer that can manage this transacting surface area.
02:23:52.690 - 02:24:18.090, Speaker A: Next slide. Yeah. So that's a little a bit about me. If you want to get in touch and yeah. Feel free to reach out if you want to see the demo properly. Thank you so much. You want to take some questions? Maybe you have time.
02:24:18.090 - 02:24:38.304, Speaker A: Oh, wow. You want to come up for you want to come up too? No. Okay. Yeah. Any questions? Hello. Yes, we're in the process of integrating with Wallet Connect. Yeah.
02:24:38.304 - 02:24:59.256, Speaker A: Wallet aggregators are key to how this interoperability works, but generally you should be able to take your keys and go everywhere and so that's kind of how we think about it. Yeah. And we're in private beta right now. I don't know if that was clear. Yes. If the user says they're using an email and then wants to eject to a private key yeah. This has come up a lot.
02:24:59.256 - 02:25:21.872, Speaker A: We're actually working on this right now. Having the ability to eject and also go elsewhere is extremely important. And whether that's NPC to EOA or other things is kind of on our roadmap as well. Yeah. Yes. So I think we yeah. Phone number.
02:25:21.872 - 02:25:52.088, Speaker A: Apple. WhatsApp? Whatever you want. It's pretty trivial for us to add. Wow. Yes. So if there's more chains yes. Well, ideally, if you're able to manage all of this intention to transact in a clean abstraction layer that doesn't involve the Vitalik's post of, like, my ETH is all over the place and I have no way of unifying the networks that the ETH is on.
02:25:52.088 - 02:26:07.660, Speaker A: That's the intention, and that's what Capsule does. Yeah. Cool. Hey. All right, I think we're done. Thank you. Aditi.
02:26:07.660 - 02:26:29.556, Speaker A: All right, next up, we're going to have a panel on investing in modular infrastructure. Joe free agent is going to be moderating. We've got mate Kelvin and Yuri from signature. Guys, please give them a hand. Hello. I think this is on. Great to be here.
02:26:29.556 - 02:26:55.084, Speaker A: Everyone appreciate everyone attending and a big shout out to Celestia and Maven Eleven for putting this on. Had the privilege of attending last year as well, and by far one of my favorite events I've had the privilege of joining. So I'm excited to lead this panel today about really some great perspectives about investing in the modular ecosystem. So want to throw it down the line. But first, my name is Joe. I invest in the space and yeah, happy to let these guys introduce themselves. Yeah.
02:26:55.084 - 02:27:11.436, Speaker A: I'm Mathias. I'm one of the partners at Maven. Eleven. I think we were the first investor or backer of Celestia, and I hope therefore we help the modular movement move forward a little bit. That works. Yeah. I'm Yuri, I think together with Mabel.
02:27:11.436 - 02:27:40.476, Speaker A: Actually, we're the first investor second, I guess, right after you guys. When we called Legal Ledger, we're a little bit different, I guess, as a VC. So we maybe have a different perspective here because we look at the blockchain space not specifically as blockchain, but we have a broader perspective on a better Internet, let's say. So I hope we get a different perspective here between the more crypto VCs. That's great. Kelvin here, investment associate, Spartan. I guess we're the third investor in Celestia after you guys.
02:27:40.476 - 02:28:27.492, Speaker A: So, yeah, we've been investing since 2017 and looking into more modular space starting in 2019. And, yeah, we've been fairly involved pretty recently as well. That's great, guys. I think how I liked setting this conversation last year when I had the privilege of moderating is kind of getting some context into your guys history in the space. I know you shared a little bit about it, but maybe quickly, what was maybe the first project? You kind of recall giving you some of the inklings of kind of where the modular narrative is today and maybe what's one of the more recent ones as well. Yeah, I think the first one easy out there is Celestia, but I think maybe the sort of plasma or even lighting network on Bitcoin, you could argue that it moves execution off chain. Currently the most recent one, I won't name any portfolio company, but I think Eclipse is doing pretty cool.
02:28:27.492 - 02:29:20.728, Speaker A: I think it's cool to see that they bring the Solana VM to an under ecosystem, which sort of shows the power of moving that execution off. I mean, we went with Celestia the first time we saw something really different from after, like, Bitcoin ethereum, and it became clear that there's something happening where you have to actually moderate or split certain functions. And we're kind of moving slow in that direction. So we're looking now at what's happening out there. So with all the decentralized sequencer for intent based architectures with roll ups, obviously, and then roll it through the services and just looking at from a tech perspective, like, okay, when is the next time something comes up that's so big that splits again in a way, right. Where you need to really modularize in such a way that it scales up again. What's maybe like a recent project you've heard of that kind of maybe fits into some of those narratives, one that you're following? I'm following a couple of projects, but I talked yesterday with Austria guys as well.
02:29:20.728 - 02:29:38.876, Speaker A: I'm kind of interested really in decentralized sequences. Not because they're right now interesting to us, but I think they're a big thing. One rollouts become a little bit more thriving, right. Once you have this happening around more, there will actually be a thing where you say, okay, what is the next bottleneck that might break? And that might be one of those things. Cool. I'm excited to chat to more about that in a little bit. And Calvin.
02:29:38.876 - 02:30:26.076, Speaker A: What about yourself? Yeah, so that's definitely one of the first projects that actually piqued my interest in the module space as well. So back in 2021 started looking into it. It's interesting how such a minimalistic design chain could align so well with the ethos of censorship resistant blockchain. Celeste is definitely one of the first that's like the pioneer in the space as well. For a more recent project like Yuri just mentioned, with the proliferation of roll up roll ups in the space, I've been thinking more into the permissionlessness of moving liquidity across these roll ups. Right. So an example that project that we got involved in is called Catalyst.
02:30:26.076 - 02:31:12.476, Speaker A: So essentially it's a liquidity layer across all these roll ups that allows users to transfer value natively across these. Yeah, it'll definitely be an interesting challenge. I think that we'll probably get into a little bit during this conversation as well when it comes to maybe some of the trade offs and modularity as well. Yeah, I think I would also just love to kind of understand, like, we kind of hinted at it a little bit there as far as some of the recent projects that you guys are following. But when we think back to maybe twelve or 14 months ago in Amsterdam, what are some of the things that are like top of mind as far as what has changed specifically in that period of time? Yeah, I think back then it was sort of like, okay, DA and roll ups, that's all there is to the modular ecosystem. And since then, a few big trends. Optimism, Arbitrum, obviously taking off in a big way.
02:31:12.476 - 02:31:56.828, Speaker A: Also getting traction on the application level, which I think is very key. We all talk about infrastructure here all day, but it does actually matter that we have users using these things. I think that's sort of validation of thesis that roll ups will scale more and then obviously on the more technical or experimental side, you have things like modular mev, so cross chain mev, intent based architectures. Aditi just talked about that and I think developments like Sovereign roll Ups, which Sovereign Labs is obviously working on, are also like in that bucket of new experimental ideas that we love to see. Got it. And what about maybe Calvin will go to you and then to Yuri. Sorry, can you repeat the question? Yeah, maybe trends that you've seen over the last twelve to 14 months specifically that have very much piqued your interest in the modular space.
02:31:56.828 - 02:32:27.492, Speaker A: Absolutely. So I think Ras service so similar to what Mattias just mentioned as well. We've been seeing a lot of these getting competitive in space, different ways, solving the same problem as well. I think DA is also something that's interesting. So aside from Celestia, we're seeing obviously other DA solutions in the market, whether it's avail or eigen. DA great. And Yuri, maybe anything to add to that before we move on.
02:32:27.492 - 02:33:01.250, Speaker A: Yeah, I agree with the guys. Also Ras is kind of the easy way to think about. Once you think about it, there might be many roll ups, right? You need somebody to deploy them. So there's a service that's kind of an easy idea, let's say not easy to actually realize, but it's an easy idea to think about. Maybe to add to that a little bit, I'd love for you to maybe dig in on if you have any opinions on maybe some of the economics that exist at Layer. I know I've heard maybe a variety of perspectives on how sufficiently those businesses or protocols can kind of bootstrap the economic considerations around where they sit in the stack. So free to anyone to answer, but I'm just curious to I think we all have opinions on this one.
02:33:01.250 - 02:33:53.810, Speaker A: You want to go first or shall I go ahead. I think one sort of thing we hear a lot now is abundant block space. So all these DEA solutions that we hear about are like oh, there will be so much block space that it won't really be valuable anymore. Whereas I think there is a big difference in the quality of block space whether that's liquidity on that verifiability of that block space, which obviously light clients play a big role in. So that's something where the value accrual will definitely happen, in my opinion, on the most valuable block space that a user can easily verify there's a lot of liquidity on it or MVP extractable on it which could be interchangeable, and that you can easily deploy different roll ups on. And that's I think also interesting that on ethereum you still often deploy EVM roll ups, whereas on Celestia you'll probably see a proliferation of different virtual machines. A little bit of a bias, rather execution, of course.
02:33:53.810 - 02:34:41.836, Speaker A: You're anything out of that? Yeah, I think the biggest gravity at the moment, maybe it's the bear market based liquidity. It's fun to see. If you even look at AutoBeat and you think Tov, for example of the rollups, it's strange why? For example, Arbitrarily and Optimus doesn't have more of it and more of it, especially after the breakdown last year moved again to Mainnet. Maybe this is the quality of the blockchain space and you have more security or more battle testedness of the Main net. You just trust it more? I'm not sure, but it seems to me why isn't going more up, right? Why isn't going more to the roll ups to do something? Because aven units have all of this there, right? So the basic stuff basically we're using I think we discussed this one earlier or sometime ago as well, right? If I leave my assets somewhere for ten years, I'm not going to do it on the roll up right now. I will just use ethereum. Mainet or Bitcoin.
02:34:41.836 - 02:36:03.240, Speaker A: Exactly. How do you solve that? And then I thought about to a certain degree, maybe you need some sort of tool or a company or protocol that only exists in a roller like GMX for example, Arbitram Avalanche and around that it slowly will build around ecosystem and then only it becomes as trustable as a Mainet. Yeah, maybe to kind of add to that and maybe pose another question to you guys. I think it actually might be like one of the trade offs that probably is under discussed in this space specifically, which is to what degree with some of these new implementations, with these new configurations of blockchains, can institutions or potentially more advanced larger liquidity holders actually underwrite the risk associated with these architectures? So maybe we can pose that question to you guys. How do you as investors kind of underwrite that process? Kind of the activation energy or potentially some people would use the word lendingness that has to develop in order to accrue or attract that liquidity. Yeah, I think for us as venture investments, venture investors, we can underwrite that because we sort of do need to take a lot of risk to get the returns we aim for. But as like an institutional, I don't know, debt fund allocating in DeFi or something, I understand that they see more risk in experimental projects that are now going live in the modular space, et cetera, and they would maybe rather stick with a more mature DeFi stack on ethereum.
02:36:03.240 - 02:37:08.416, Speaker A: So I think there you just need to prove yourself over time that you are there to stay and that they can allocate so it's about sort of the risk appetite of the investor. Anything from you guys as far as, I don't know, maybe how you as investors kind of at least attempt to underwrite some of the risks that these more modular configurable systems or architectures can introduce. Yeah, for sure. So when it comes to assessing different projects, I can actually use perhaps like a DA layer. So there's several ways we can assess this, right? So obviously DA is a function of cost as well as transaction count. So you can do certain projections, right, given how much I guess, revenue that the DA layer could actually make and then you add on a premium to it and you can actually have thinking like how they could actually monetize on top of it. Do you think of it from more of maybe an economic security perspective or kind of where are you going with that? As far as you're thinking of it more in relation to the economic security perspective of that sort of a thing.
02:37:08.416 - 02:37:55.120, Speaker A: Would help an investor, maybe like a liquid fund. Better underwrite the risk of a modular architecture. Yeah, I think that certainly will help Matthias. I think one thing we had chatted a lot about is kind of like some of the different founder types that we see specifically in the modular space. Would love to maybe throw it to you guys as far as are there things that stand out about the types of individuals that you've had the privilege to support and invest in that are building in the space? Maybe unique characteristics, maybe bad things, who knows? Yeah, I'm happy to take some since we've backed quite a few founders in this domain. I think right now what you see is it's usually quite tech heavy people and they like to be sort of nerd sniped, so they like to think about ideas that are complex and that easily change as they move along. So they also need to be flexible in their architecture.
02:37:55.120 - 02:38:29.756, Speaker A: I think that's sort of the current founder type you see in the space. I do expect that to sort of change over the next three to four years to also more consumer facing, more commercial, business minded people who just want to build an end product for users. What do you think that shift will be like a byproduct ever? What do you think will cause that? I think just maturity of the tech stack. Right. I think to some degree so that they don't need to worry about the trade off of mev prevention or an intent or not. Right. I think somebody who just wants to build a good user app doesn't want to care about what sort of transaction type there is.
02:38:29.756 - 02:39:38.672, Speaker A: Maybe I pivot this question to you guys as well when it comes to I think everyone kind of agrees on this kind of opinion in the zeitgeist that everyone is focusing on infra there's. Probably not enough people focusing on applications. What are some of the things maybe that you guys are impressed by when it comes to making some of these stacks more accessible? We have mentioned some of the roll ups as a service providers, but I think whether it's like net new execution environments, opening up the doors for developers from other ecosystems to come into web three, is there maybe a specific highlight of those categories that you think make this technology more approachable for application and potentially consumer developers as well? I think actually what's really cool about the modular, let's say mindset is not that you built modular, but the way that you're not locked in into an ecosystem especially through a token, right? So I tried to run around Berlin and talk to people at Wise and people collaborating more together. There are a bunch of different ecosystems there. And somebody mentioned to me, well, when you develop something in your ecosystem, the incentive is really still develop it in your ecosystem because you develop it outside, well, they profit from it. Right. So unintentionally, the token provides a barrier for collaboration certain degree, because it bends so big already.
02:39:38.672 - 02:40:06.724, Speaker A: Modelerism doesn't have that because you can collaborate very rather easily. Everything is more aligned. So the room for experimentation, the whole environment is way better. So what you have is really that there's a lot of people building really cool stuff out there and they're just trying to figure out how to combine it, recombines and so on. But a lot of great ideas coming out of that. And one of these great ideas is that you, for example, can abstract away the token. I think Nick mentioned that two days ago, on the ZK day, that you can actually build directly something on Celestia and don't have a token.
02:40:06.724 - 02:40:56.200, Speaker A: This is a good thing because developers don't have to be financial anymore, they can just start building, which is really cool on the application users in the end. Right. So those kind of thinking is way more free and running more free and doing more experiments in the modular space. It's also about the founders who are really mindset like that. I don't know, we might have some people start sweating in the room when we start hearing no tokens. But another thing I'd like to post to you guys, I'm quite curious, specifically, maybe on Calvin and Matthias'perspective, on what are you bearish about in the modular ecosystem? Are there specific things that you think are currently discussed or a lot of people are excited about that? Maybe specifically as an investor you aren't that interested in or maybe even intellectually or from a product market fit perspective you think doesn't have a chance or you're not as excited about? Yeah, so I guess I can just quickly take this. Yeah, so that's a very good question.
02:40:56.200 - 02:41:31.040, Speaker A: I think moving forward, there's going to be a challenge when it comes to justifying valuation. So I feel like during the bear market, a lot of people are looking into infrastructure that squeeze up all the valuation. And in order to justify these valuations, depending on what you build, fees might eventually compress in the future. So you'll have to find other ways to accrue value to what you're building. Right. So perhaps owning more of the vertical stack, that's how you accrue more value also bearish on high valuations. Matthias, I'm curious to hear your thoughts.
02:41:31.040 - 02:42:24.310, Speaker A: Yeah, I don't know if I agree with the fees point. I think sort of fees will compress per user, but the sum of fees that we collect on the infrastructure as a whole, I hope will increase as we onboard more users. Sort of the bare case for modular crypto I think is either the complexity that you add by sort of incorporating all these different systems together. So if one of them breaks, that might not be good. And overall maybe, but that's a larger bear case on crypto that people just don't really care about end use verification. So a large part of building a modular blockchain with data availability sampling is that everybody can run a light node very easily. I mean, we see the people from Celestia especially do it everywhere, but ultimately the assumption here there is that people do want to run a light node and verify this thing themselves, whereas maybe, well, they just don't do that.
02:42:24.310 - 02:43:12.880, Speaker A: Do you think people want to run light clients? Yeah, I think otherwise you shouldn't be in crypto. But it's sort of why you built on the blockchain, right? You want to be able to verify it, otherwise you can just use Excel sheet. It reminds me of the accessibility point that we mentioned earlier. Maybe earlier we were talking about the accessibility for developers with external skill sets from crypto. But when you think about from a retail perspective, maybe what some of the purists who have grown up in this industry really kind of prioritizes is really ensuring that these networks can be robust. And a huge part of that is ensuring that you have a culture that exists around these blockchains of verifying state and having individuals capable of doing that. But maybe one thing I would pose to the group is I'm kind of convinced that we have to in the same way we have to build modular execution environments, et cetera, to make it more accessible to developers.
02:43:12.880 - 02:44:02.416, Speaker A: I think if we want people to care about Verifiability, we have to make it almost as easy as wearing a wristwatch to tell the time in order to actually see wide adoption. So maybe counter that if you guys steal man that perspective even as like a validator like I said, I actually agree with the perspective, right? And I do think people care about it. So that's sort of in your question. Like Wilde, I fundamentally do think so because the most valuable sort of crypto networks also have that culture of running their own full nodes, right? Bitcoiners go mad if you make it slightly harder to run a bitcoin full. Sort of the whole block size debate was about that. And Ethereum also puts in a lot of work into making it easy and accessible for everybody, like all over the world with not a lot of money for hardware to run a full node. Obviously you have trade offs, but I do think that's what matters.
02:44:02.416 - 02:44:32.456, Speaker A: So that's also why I'm pretty bullish on being able to verify it yourself quite easily. Maybe a question in the room how many of you guys have ever run like a full node on a network? Or maybe you even have tried or attempted to run a light clock, they're going to counter my points. Kudos to all of you. Everyone else homework. At least give it a try. I really encourage everyone to give it a try. I think, Joe, maybe there's a difference in how you phrase especially if you go outside the blockchain space, the general blockchain space where everybody's kind of excited about the tech.
02:44:32.456 - 02:45:00.264, Speaker A: What does it mean to care about running a light client, for example? Right? I think people do care about a lot of things, but they have like, let's say I care about it for 2 seconds, right? If you give it 2 seconds to me and I'm able to do it in 2 seconds, I do care. If it takes me 10 seconds, I will not care. Right? So there's a kind of a resource allocation problem here. We say, okay, can we make you care about that with that time frame? Right? And that's what you have basically in a website, right? You go on website, there's a green button saying, okay, check, done. Right? Cool. So I care for a second. That's it.
02:45:00.264 - 02:46:16.450, Speaker A: But fine, as long as you can establish that root of trust in your green SSL checkbox at your browser. I think we've seen a lot of kind of thought and probably innovation in the past twelve or 14 months on some of these more sovereign constructs that we've started to see getting constructed. Do you guys have maybe some thoughts on the economic kind of implications of those sorts of constructs when you really remove kind of the application and execution layer from maybe some of the underlying economics that a network might be inheriting from an alternative l one, despite maintaining its sovereignty? I'm happy to go. I think that especially on sovereign app chains or app specific roll ups, whatever you want to call it, I think the value capture is way more easy, right, because it's inherent to the application. So probably value will accrue to the application of the crypto stack and to the sort of bottom layer, which in my case would be a DA layer or whatnot. And I think if you have an app specific chain, then it's quite easy to capture value by just extracting a fee for offering the application. So I think the fee switch in uniswap is famous, but if they had an app specific chain, it would be quite easy to make it valuable because every time somebody uses uniswap, the block space gets used and validators capture some of that value.
02:46:16.450 - 02:46:53.580, Speaker A: Any thoughts from you guys on kind of sovereign constructs and how they could potentially modify or alter some of the economics for application developers? Yeah, so I agree with what Ty just said as well. I feel like app specific chains will probably have a better chance of accruing more value in some way. I actually feel like modularity does make certain layer two struggle to accrue sufficient value this is why we're starting to see layer threes getting built out. Right. That's how you try to find other ways to accrue them oh, sorry. Yeah. Onto your layer.
02:46:53.580 - 02:47:32.488, Speaker A: Moving forward, I think we'll continue to see more layer three spilling on top and continue to scale that way. Yeah, I don't see a difference happening. And there's a normal, let's say, traditional world where quantization means that you will less and less accrue value, and effort on top of that will accrue more value. It's just that within the innovation cycle, I like to say that every infrastructure was application ones. You start out of that even like an email protocol was an application once you started sending emails and then more and more people built on that, it compressed basically down. What Matthias said before, that the fees per user might go down, but the fees in total might go up. Right.
02:47:32.488 - 02:47:57.056, Speaker A: So it depends. What do you mean by really accrue value? But in general, as a sum, it might be that most of it at some point will be at application, but once the system is really thriving, like we have now in the web two world. Do you think uniswap is an application or infrastructure? Which version? No. Yes, sir. We argue about this at the office. Like, are you investing in applications or infrastructure? And you were also alluding to this. Right.
02:47:57.056 - 02:48:29.592, Speaker A: And I do think that in crypto it's often both because uniswap for sort of panelic as an example, they're building options using unify three liquidity. So for them, uniswap is the infrastructure and then you sort of have an infra layer again that captures value. Probably like, why the conversation? I guess that's probably part of the reason that that's become such maybe like a common theme or something that a lot of people are chatting about is because there's probably alternate definitions of those things as well. And they probably convolute over each other a little bit. I see them. They're going to aggregate a road. Right.
02:48:29.592 - 02:49:17.208, Speaker A: Any sort of X, I think is an aggregate or coming towards to be an aggregator. So they're moving toward the application now and they kind of letting go of the protocol ones. Maybe something I would like to maybe ask you guys from a more kind of personal investor perspective. I know earlier we chatted about maybe some special things that we see in modular founders, those sorts of things. Are there explicit things that you guys really enjoy seeing out of maybe the founders or even the technical teams that you have the privilege of supporting that you would wish to see from others? Are there highlights there that you would like to share? Lessons learned from some of these portfolio companies in the modular space that you think is worth sharing? Sure. I guess when you're building modular infrastructure, the founder should be, I guess, really agile as well. So in module in that sense.
02:49:17.208 - 02:50:04.024, Speaker A: But yeah, some of the backers that some of the founders that we backed yeah. We actually would like back founders that has previously built in, whether it's like a monolithic environment or modular environment. So founders actually know how to appreciate the modularity nature of blockchain. Right. So they're agile and they actually understand what's the benefit of having modular their infrastructure. Yeah. What about you, Uri? Matthias? Anything spicy as far as things you want to see more from founders? What I like about working with our founders in the motor space is I'm often the dumbest guy in the room, right.
02:50:04.024 - 02:50:25.170, Speaker A: So they're all smarter than me. I'm learning from them as well. That's the sign of a good investor. If they're willing to say that at the same time, it's not that different. Right. They do need to be smart and sort of flexible or agile, as you said, but they also struggle with the same things every other company struggles with hiring, team building. Just building a company is hard wherever you do.
02:50:25.170 - 02:50:48.216, Speaker A: I think that's sort of it. Yeah, I think that summarize it. Yeah. There might be a balance as well, probably in this space. I've found, at least during my time as an investor, that the majority of kind of the core founding team was quite technical. And oftentimes they do need kind of more, I guess, kind of like kosher traditional support from investors when it comes to the types of things you can actually do to help them win. Yeah.
02:50:48.216 - 02:51:42.024, Speaker A: I mean, these guys don't need me to tell them how to write code. I have no fucking sure. But maybe helping with a first hire, et cetera, that's more where they need to sort of be alleviated so they have their core muscle, their core strength, and we try to work sort of around that. Yeah. Interesting. I want to kind of go into some maybe potentially spicy topics now that I think are, again, quite timely when it comes to many of the ideas that are getting discussed over the course of the next two days here. What are some of your opinions on kind of rehypothecation of proof of stake assets? Specifically? Do you think there's unknown dangers that investors or founders or application builders are not considering? Are there externalities that you think long and hard about or potentially keep you up at night like I do? Yeah, we have a company in our portfolio, for example.
02:51:42.024 - 02:52:06.892, Speaker A: It's a pure bitcoin company, and they don't like it at all. Right. But there's a bitcoin maximalism involved here. So I understand that. Right. You're talking about kind of storage, right, values. But generally what I find more complicated with the infrastructure now is that you add to the financial complexity, technical complexity, and to the technical complexity, you add scalability complexity.
02:52:06.892 - 02:52:42.988, Speaker A: So you might think that, well, if something breaks down, I can all execute smart contracts and it all goes down this way, but all of a sudden, bomb it doesn't work out because everybody runs through the same gateway. And so you have all complexity, even getting bigger, and it's really hard to evaluate that. How big is that? Actually? It's a lot of buns of unknown. Unknowns. And that makes I would go slightly right. Really careful about that and see how far you can go with this because at some point we just break and then you realize what is. So maybe just do that experimentally with small sums just to trigger and figure it out a little bit.
02:52:42.988 - 02:53:03.508, Speaker A: But doing it on a big scale as we saw last year, that might be a problematic thing. Yes, definitely. As we all witnessed last year. Matthias, do you have any spicy takes on this? Yeah, I mean, I like the experimentation with it in general. I like experimentation and trying out stuff and maybe breaking on this specifically. I think the economics behind it are still quite hard. Right.
02:53:03.508 - 02:53:36.856, Speaker A: So it's hard to quantify the risk of reapplication of assets, as quantifying risk in crypto is fucking hard anyway. And then there's also not that much fees collected right now that you can sort of give enough yield to funnel to funnel to incentivize people to take that risk. And that's, I think, sort of the big question here. Like, okay, maybe there's a small group of people who will do this, but to incentivize a lot of people, you need a lot of fees. And that's just not there right now. I think I would broadly agree with that. I maybe want to pose a question to the audience.
02:53:36.856 - 02:54:24.892, Speaker A: I like opening these up to see if anyone has anything to add to the conversation or maybe things that you disagree with. Is there anything that anyone here would like to raise their hand and potentially ask the panel? I'm happy to walk up and give you the mic. I see a spartan shirt over there. Any spicy takes for your colleague? You can't put Calvin on the spot like that. I guess one spicy take that we kind of touch on was module blockchain might at some point bring us back to maximalism. Because going back to a point like my previous point, if you're trying to accrue value to a specific layer, you'll start to find other ways to accrue more value. Right.
02:54:24.892 - 02:54:59.800, Speaker A: So for example, just thing aloud here. So a deal layer, if you're trying to like fees do compress in the future and you find perhaps you're trying to own another layer instead, maybe execution layers, then you're starting to see we're going back to the cycle where we're starting to own more of the stack. So that's perhaps one spicy take. Yeah. So then Celestia would at some point be like, hey, that settlement layer on top of us is also quite valuable. Why don't we sort of enshrine it into our stack? That's a good bear case. Yeah.
02:54:59.800 - 02:55:33.700, Speaker A: Yuri, any other bear cases in regards to that spicy take? I think my biggest word with the whole ecosystem generally is that are we actually building something completely different and new or are we just rebuilding the stuff that's already out there in a little bit slightly different way and we're just not seeing it because it's so new, you haven't converged yet. It but in the end you just end up with the same system, the same centralized parts. Like a validator is so big, it just controls a bunch of stuff. Right. MV is something so big it controls a bunch of stuff. You have like a bunch of operators that are just grand and you haven't seen coming. But the economic gravity of that just made it happen.
02:55:33.700 - 02:56:33.156, Speaker A: Right? That's fair. Another Pivot, maybe in the conversation, are there certain types of applications that you guys believe are specifically empowered by modular infrastructure? I know we've kind of chatted about these concepts of kind of turning specific applications into kind of their own sovereign execution layers, et cetera, across the stack. But when you think about kind of like the second order effects, maybe this time next year or in two years, what type of applications do you think will have most benefited from this sort of architecture? Yeah, I think a downside of building a modular blockchain is I think, interoperability. And as a result of that, applications that don't need too much interoperability are going to be, I think, very interesting to begin with. A good example of this could indeed be gaming. So a gaming specific chain for just one game. When I play like FIFA and I want to sell my coins or something on there, I don't need to interact with somebody playing World of Warcraft, right? I have no need for that.
02:56:33.156 - 02:57:18.740, Speaker A: Whereas in DeFi that's very important that you can plug these different Lego blocks into each other. So I think the first wave maybe, and we saw Argus present earlier, will probably be gaming specific chains. So that's I think something modularity enables and then indeed the different execution environments, right. Stuff like the solana VM, the fuel VM, move VM, et cetera. Maybe order book type of decentralized exchanges will be enabled by that, although it remains to be seen how mev will play out there. But these type of products as well. Do you maybe want to chat on that as far as some of your cross domain mev perspective when it comes to how that conceptually alters or is modified by modular architectures? Yeah, we're talking about a lot.
02:57:18.740 - 02:58:13.364, Speaker A: One thing we are currently still maybe debating, and I don't have my mind made up yet, is where do you want the mev to sort of land? Right? So one vision is you want the DA layer to capture the mev because then it will accrue more value and you'll have more economic security. On the other side, you maybe want the mev to sort of be isolated in a separate execution environment because then if one game gets extremely attractive and a lot of people play it, it doesn't trickle down to the other roll ups. So it's a trade off, right? Do you want it to go down or not? And I think where the leader selection happens is where it will go to and I don't know what the optimal decision is. I think there's still very much an open question. The first thing it makes me think of is if and maybe I could be ignorant to this, but whether people have thought about dynamic systems that would be able to maybe modify or point where that MVV could go but between two layers like that. I don't know if that's been explored or if that's the dumbest idea that's been said here today, but I also don't know, honestly. But I would be down to sort of look into that more.
02:58:13.364 - 02:59:21.774, Speaker A: I think that's one of the more interesting areas that indeed we talked about. These modular founders, if you want to call them like that, are sort of nerd snot by and looking into and I'm sort of happy to tag along with them on doing so. Yeah, as we kind of get a little bit closer towards the end of this conversation, maybe opening it up to all of you, you have an audience here that are quite modular pilled. Are there specific types of things that you want to see built? Kind of requests for startups or any concepts that you think are rather unexplored or that you wish you had more looks at as far as different ideas? So when I was reading up, I just started really recently reading about the intent based systems and I found it's a really cool way of thinking about how it actually should act. And we have a company that's kind of in a similar space, but it's an agents based space and so intents and agents are a similar thing. But in the end, if you look at how an app is built, for example, right, it's not possible to have an app built on just smart contracts. You need some sort of in between agents to trigger certain functions such that the person can just have a phone in the pocket and just notification arrives, right, something like that.
02:59:21.774 - 03:00:13.026, Speaker A: So you need all those things and you need to cross them the chain barrier to this be off chain and on chain. So I'm excited about this stuff and experimenting more of this kind of things to really figure out, okay, where does blockchain fit into the bigger world solving problem space than just really running around the small pieces of the blockchain access themselves? Because I think we've crossed the barrier already a couple of years ago where blockchain was just a really kind of a big space where you could put all your hope in and it was not concretely there, but just bunch of hope and you can just really think about all that's. The potential, but now it's getting more concrete and it also shows you more things that breaks, that works, that can solve and cannot solve. Right. And it becomes really hard to have the hope. And so you have to really think about now, okay, what does it actually can achieve? Where does it go, and how can we go there? And I think this is kind of this intent based thing together with the agent based. I'm not sure how different they are actually from each other.
03:00:13.026 - 03:00:45.078, Speaker A: That's kind of a different conversation is kind of a really nice way to explore it. Calvin Matthias, any requests for startups or areas that you wish were maybe ideas that you saw more of as venture? Oh, yeah, I'm happy to take one. Okay, this is going to be a bit boring and somewhat contrarian now. DeFi. It's not very sexy anymore, right? Especially amongst venture investors. I do think the whole real world assets thing is actually quite cool. That we have like close to a billion of bonds.
03:00:45.078 - 03:01:18.454, Speaker A: Being on the ethereum chain right now is, I think, very interesting to see. So in that regard, stuff like Noble that aims to bring USDC onto these Celestia chains, I think it's very interesting. So these type of products that actually connect to the sort of real world or web two world or whatever you want to call it, are quite interesting. I would love to see that more moving forward as we grow more into that adoption part of the tech sector. USDC, the quintessential real world asset. I mean, I've been arguing here about end user verification and now I'm the USDC bull. But yeah, it's all fair.
03:01:18.454 - 03:02:12.200, Speaker A: And also privacy tech, but that's more generic stuff like banambra anoma NIMTECH. I think that's sort of where my sort of more crypto native heart lies. And I would love to see more of that, maybe dig into that a little bit because I actually feel quite naive as far as maybe some of the privacy considerations that are powered maybe uniquely by modular infrastructure. Is there anything that you would add to that or things you've seen recently that excited you specifically with privacy type applications or privacy based architectures? I haven't seen a lot of privacy tech on a modular ecosystem like a roll up that's only privacy or app specific tornado cache would be quite cool. I'm not sure if they're sort of and then on privacy and you have sort of two layers of it, right? I think a lot of financial institutions want privacy on their orders and don't care if regulators can see it. And then you have more the crypto anarchistic privacy which maybe darkfi NIMTECH are working on, where sort of everything has to be private. They're very purest about that.
03:02:12.200 - 03:02:36.794, Speaker A: In the modular ecosystem, I don't know. You can make a lot of design trade offs. You can maybe encrypt a mempool which for financial institutions would be enough privacy. Right. So, yeah, looking forward to seeing these founders try out stuff there. It's another one I don't want to bore people with because I feel like it can be quite cliche. But I'm curious if you guys have any perspectives specifically on maybe like non Web three native use cases for some of these modular architectures.
03:02:36.794 - 03:03:22.814, Speaker A: I think I could probably speak for everyone in the room in the sense that enterprise, blockchain, et cetera, has failed in the majority of its capacity. But I'm curious if we feel that there's any inkling of feel that the modular infrastructure could maybe help empower that in some way. I had this conversation yesterday about how do you build a social network, a new social network. And I find it's a mistake. Most people think about social networks in terms of Facebook and Instagram and YouTube's as kind of there's a graph because I think in the new way, if you have blockchain, if you have this new technology, you can rethink that. And I think in the center of that is the feed. And the feed can be, for example, modular, right? You can actually do and choose your own feet how you want to like it.
03:03:22.814 - 03:03:45.894, Speaker A: And the feed through curation connects to people not anymore through kind of a social graph that already existed and you just walled it in. It really is about like there is a centerpiece where people are curating stuff and that flows all into the feed and that connects to people. Right. And this has the merging quality of a network. Then all of you have a social network without wanting it, just like it was before. But it's a new way of thinking. And I think that's what blockchain enables us, a new way of thinking.
03:03:45.894 - 03:04:46.378, Speaker A: The old stuff, all the things that we have already, we can have it back, but we have it in a new way and build it in a different way and only then it works. I'd actually like to build on that last point that you made. Is there anything very specifically that you guys feel like maybe getting modular pilled, whatever you want to call it, or just becoming more aware of how people are thinking in the modular kind of infrastructure space? Are there explicit things that has changed around how you think that could be? I don't know from an ethical, philosophical perspective, it also be from a technical perspective as well. I don't have a whole lot I could take it back to the enterprise question. I don't think we'll see a lot of enterprises starting to build on these systems earlier. Right? You asked like, how do you underwrite the risk of it being quite experimental? I think especially enterprises are scared of that. I do think that sort of a roll up itself is very instinct for them because they can sort of keep a level of control that maybe they feel like they miss on a permissionless l one or maybe with certain regulatory yeah, exactly.
03:04:46.378 - 03:05:45.130, Speaker A: I do see a lot of experimentation happening there, but I do think it's way further out than next year or something, as you asked. Right. I don't think they'll be like, oh, we have to deploy a roll up. No, I think they're more likely to be to look at more established networks. Cool. Well, I know we have probably maybe two or so minutes left just to finalize this conversation, but what do you guys hope we are like if we return here in a year and have the privilege of meeting at the next modular summit? What is it that you hope you're seeing become successful? What are some of maybe the next milestones that you hope you see this space achieve in order to kind of get us to the next stage? I mean, Celestia launching is a good first one, right? Launching the main net successfully second, I think also more CK rollups going live, like in full. I know CK sync is sort of live, but that ecosystem maturing to a similar sort of level as maybe the Arbitram ecosystem is.
03:05:45.130 - 03:06:23.526, Speaker A: Because I know we're here at moderate something, we're talking about Celestia, but a lot of rollups also have the largest ecosystems by far on Ethereum EIP 4844. Going live successfully would be pretty cool I think, as that makes blockspace more accessible for these roll ups and maybe the first sovereign roll up on Celestia. More app specific roll ups going live. Hopefully some accelerationism around restaking as well, maybe. Yeah, fast experience experiments, but small ones hopefully. What about you guys? Yeah, aside from those, I think we want to see more of the DAP layer development as well. So I actually want to touch on onchain gaming as well.
03:06:23.526 - 03:06:51.746, Speaker A: So I feel like on chain gaming could actually take advantage of what modular blockchain could actually offer. So bringing the entire game logic as well as game economy on chain could actually bring about siloed different type of economies on site. Right. So for example, strategy game, you're trying to attack someone, right. And then someone can front run that action. It would be like it will create different new dynamics. Right.
03:06:51.746 - 03:07:22.300, Speaker A: So I feel like that's someplace that we can perhaps know. I've definitely heard Scott from Argus talk a lot about what excites him. Building modular infrastructure specifically for game designers is attempting to create something that can create very emergent net new player behaviors. Because I think that's one of the things that actually makes games exciting or when good games become exciting is when it's like something you haven't done before. I remember reselling in Game Assets for the first time. That's actually what got me into crypto in 2012. I think seeing more experiments around that will be pretty exciting as well.
03:07:22.300 - 03:07:53.438, Speaker A: Awesome guys, I think we're just at time so really appreciate the attention from everyone and everyone supporting the whole gang here at Celestia and Maven. I appreciate you guys having us and yeah, enjoy the rest of your time in Paris, everyone. Thanks. Thanks a lot. No, one more talk, I'll take it. Thank you. These are all three for you? Yeah.
03:07:53.438 - 03:08:36.690, Speaker A: There's one more speaker, right? Does he have a microphone already? There's one more speaker, guys. All right, everybody, please make your way either out of the room or into the room. Just see if we can quiet this down a little bit for you. Next up is going to be Arjun, head of Ecosystem for Mantle. So I guess I'll call you to the stage. Take some time for these people to wash out. Yes.
03:08:36.690 - 03:09:28.628, Speaker A: Is this slide loaded already? Yeah, no, this is the last. We don't have the modular Lego after. Hello. So we're all excited about model architecture, but I actually wanted to cover something which I think doesn't get discussed a lot when it comes to modeler chains and which is how the economics are going to look like. Right. When you have all of these different layers working with each other, what are the economic models which we have at play and what the incentive design is going to look like ultimately, what we've seen. So modeler technology in general when it comes to blockchains, is not something new.
03:09:28.628 - 03:09:54.780, Speaker A: We've seen this in Web Two as well. So what we've seen is that Web Two software architecture was always monolithic. It was replaced with APIs and microservices. And ultimately what we're seeing the same in Web Three is that the monolithic chain architecture is also going to be replaced with model architecture. Right? So at Mantle, for example, we built a modeler chain. We've gone live with eigen layer data availability. We've taken the Op stack and turned that into a modeler stack.
03:09:54.780 - 03:10:37.460, Speaker A: And what we've also seen is that API driven architecture led to like billions of dollars of value creation. And we expect the same to see, at least I expect the same thing to happen with the model architecture as well. So let's try to look at some of the different layers of the blockchain and try and see what the incentive design is going to look like. So ultimately, on the execution layer, right, we've seen a lot of interesting things happen. So if you've been following what some of the other layer ones have been sort of doing, for example, Aptos came up with block STM, which is parallelized processing. There's also Arbitrum, stylus and polygonmiden. And what we've seen with different execution layers is that people have started thinking a little bit beyond EVM.
03:10:37.460 - 03:11:42.140, Speaker A: So there was a time, or rather I would just say, over the last 18 months, we've seen that EVM has been the sort of execution layer of choice. But of course, now we've come to this point where we can also see that EVM is reaching a stage where either we optimize EVM or we look at a different kind of execution model where we can use possibly other languages, et cetera, to sort of write smart contracts, possibly do things more efficiently. And ultimately there are a couple of ways of doing this. At the execution layer you could possibly charge gas fees, accrue some yield to stakers. Moment you start to decentralize, you need to figure out some sort of economic model and ultimately the value will lie in very optimized execution and it has to be deterministic in nature as well. So if you can solve something like this in a decentralized manner where you've got multiple execution environments all fighting for this piece of the spy, we could see some new interesting economic models come out. So I fully expect that with more and more VMs there chains will not sort of work with only one VM, there could be multiple ones depending on the kind of outcome you're looking for when it comes to gaming or DeFi.
03:11:42.140 - 03:12:46.208, Speaker A: And ultimately, as long as it's fast, secure and deterministic and you can parallelize things, we're going to see some interesting economic models come out of the execution layer. Similarly on Settlement, right, so if you guys have been following what happened on Twitter, anatoly posted something interesting where you could take Ethereum transactions and put those on Solana. I don't know if you want to do that, but it kind of gives you this interesting choice. Right. So can we settle on other networks in the interim before we settle on ethereum these days? You must have seen so many different roll ups are coming out on a daily basis, which is great for the ecosystem because that just means that the barrier to creating a chain and running a chain has reduced so much that everyone can have their own blockchain, which is great. But this also produces other challenges where we're going to see the block space on Ethereum, which was earlier crowded by transactions, is going to be crowded out by roll ups. More and more state routes are going to eat up and state routes, while you can optimize them and we'll sort of get into that later.
03:12:46.208 - 03:13:48.680, Speaker A: But ultimately block space is again going to become a premium and we need to figure out some interesting ways in which we can settle, we can solve for incentives and we can achieve the kind of performance we need without sacrificing security assumptions. So ultimately you also have this L Three app chain narrative going on these days. Like you can build your own app chain. I think so many people want to do that. Should they settle on ethereum? Should you only settle on l two? Like, how does, for example, the HyperChain model work on ZK Sync? Like, some of these things have been discussed from a technology standpoint, but ultimately what you really need to solve for is how do you create value of the token of the chain building on top? So all of these chains, which are L Three S, all of them are going to have their own tokens, how is the underlying layer going to build? Value accrual for those tokens ultimately is going to be the main kicker. And of course, value lives sort of in derived security and faster finality, but along with ecosystem benefits. So, for example, if you're building your own L Three chain, should you build it on ZkSync? Should you do? Op collective.
03:13:48.680 - 03:14:45.370, Speaker A: How do you make that decision? And what we've seen time and time again is that the ability to get speed, execution, finality is something we've continuously become better at solving. But how do you help the ecosystem? And what does the ecosystem benefit look like ultimately, when you have all of these different sort of chains fighting with each other, you've scroll, you have Polygon, Cksync, Op, Arbitrum, Nova. How are they going to help you design your ecosystem and how are they going to help you create value for your own token? Ultimately, in my opinion, is going to be the most important factor when you make this kind of a decision. And that's kind of why ecosystem benefits ultimately are going to be a very big factor. So, for example, when we try to advanttle talk to chains who want to build L Three S, the main conversation always is around ecosystem benefits. For example, liquidity listing of tokens. And what else can we create? Markets for the tokens building on top.
03:14:45.370 - 03:15:33.284, Speaker A: And if you go to the next layer, right, so data availability, of course, is now this hot new layer, right? We've got Celestia. We have Eigen layer. So we went live with Eigen layer, but we've been talking to Celestia for a while. There's also polygon avail. So many different data availability solutions with different, let's say, security assumptions are also there. And we also have E Three Staking now as this new primitive where you can take, for example, the economic value of a token and you can use it for utility versus collateral based yields, but ultimately you again have to create utility for the chain token. So for example, in our case, what we liked about Eigen layer is the fact that we could restake ETH and get security, but we could also take our own token and stake that on the data availability nodes, thereby creating yield and value for our token.
03:15:33.284 - 03:16:17.068, Speaker A: And this is super important. So whenever you talk like, for example, about Celestia or avail, luckily all the founders are here, so I can quiz them about these things. But how are you going to help my token? And where do the ecosystem benefits lie? So with more data availability solutions, ultimately the ability to solve token related and value related problems is going to become more important than any piece of technology out there. And we're just at the start of data availability. So I expect to see more economic value, sort of, or rather more different approaches to economic value come out in the time to come. But this is like the main question I literally have when I talk to data availability players out there. And if you go to the next layer, right, the consensus layer.
03:16:17.068 - 03:16:53.008, Speaker A: Now, again, consensus layer is a total war zone right now with Zkvms optimistic roll ups, arbitram doing their own thing. The shared sequencer approach is also interesting. But again, the question is always the same. How do you create yield? Right? So you have all of these ZK provers, you have like, shared sequences, shared bridges. Fantastic. How does it help my token? How does it create value for me? So I sometimes feel that we talk a lot about technology. We talk about EIP 4844, we talk about vertical trees to reduce proof size and optimize all of these things.
03:16:53.008 - 03:17:43.116, Speaker A: But ultimately, yeah, you can find gas fee extraction models and offer yield on decentralized designs, but how does it create value? And ultimately, these are the conversations we need to start having. I think we need to start looking a little bit beyond the technology and try and see how we can solve for value in these decentralized designs. I'm just going to keep it really short and this is basically the last slide, but ultimately technology will decide if we can build modeler chains. But ultimately the incentive design is what will decide how we do these things, how these execution layers come together, how these data availability layers come together, how the consensus layer comes together. So no value, no token, equal to no bueno. And we need to really solve that. Anyway, thank you so much for listening.
03:17:43.116 - 03:18:47.770, Speaker A: I just wanted to keep it really short and want to get more people start talking more about incentive designs. And I think the key of how we're going to implement all of these things lies in the token itself. So anyway, thank you guys. That's really it. You want to maybe take some questions? Yeah. Do you guys have any questions? Anything want me to cover about incentive design? How are you all thinking about basically the ossification of incentives? Right? I can layer, not necessarily because the core security assumption or the underlying infrastructure of how eigen layer is built is already set in stone, right? Because it's in the white paper. So it's kind of already decided upon now.
03:18:47.770 - 03:19:43.192, Speaker A: Okay, so the way to think about these problems is that the way you solve modularity with technology, you cannot really change that. So I'm actually against changing technology to fit the incentive model. What we need to do is take the infrastructure, like whatever the security assumptions are, whatever the restaking layer is. And what we need to do in that is bake economics into it. So the data availability nodes, which is securing data availability for many different chains, which is ultimately secured by the underlying eigen layer restaking infrastructure, unless you can offer some token utility there, I feel chains may think twice before adopting this kind of sort of a data availability model. And similarly for Celestia, Celestia has got such an interesting model itself, which is a little more optimistic in design polygon. Avail is using cardi polynomial commitments, but ultimately how they create value for the network's token and where those benefits lie are going to be like, super important.
03:19:43.192 - 03:20:17.692, Speaker A: So I don't think these companies or anybody else building this needs to change the infrastructure. They need to just figure out how to fit an incentive design into their underlying infrastructure. So Eigen Layer is doing that using nodes. I think Celestia, I'm sure, will have some approach. I think Avail is going to be live possibly by Q one of next year. So I look forward to seeing how they sort of solve these problems for networks. Does that cover the question? Yeah, it's kind of more of a motion about bit, but how mantle themselves are thinking about incentives.
03:20:17.692 - 03:21:08.880, Speaker A: Right? Because you're using that both on the gas fee context and I guess if DA stakers are also staking, so so I would say that in terms of token, what I would say is that how we plan to structure it as an ecosystem token. So it'll be used for gas fees, it'll be used for infrastructure staking, it'll be used as the ecosystem token, it'll be used for governance, and then it will also be used in all the other protocols you're building, like the liquid staking derivative protocol, which we also plan to roll out possibly by Q four of this year. So the token will have utility across all of these sort of, you could say, infrastructure pieces. So that's kind of how we've designed the token. And whatever is produced in the future and whatever products we're going to make, we're going to ensure that we bake in some sort of incentive. All right? All right. In case you have any other questions, you can always just reach out to me on Twitter.
03:21:08.880 - 03:21:43.970, Speaker A: It's First Name, Last Name, and Telegram is also First Name, Last Name, so you can just feel free to ping me and yeah, it's only First Name, Last Name because there's another Arjun Kalsi out there who's going to ask you for like, one ETH because his dog died. So just be like really careful. Right? So it's first name, last name. All right, thank you, guys. All right, so I think that concludes the first half of the day. There's going to be lunch in the main room and we start in an hour again. So see you back then.
03:21:43.970 - 03:28:16.420, Speaker A: Sam trip. It's SA ram. And sam. SA. Sam. Sam. Sam.
03:28:16.420 - 03:31:12.490, Speaker A: Sam. Sam. Sam. Sam. Sam. Sam. Sam.
03:31:12.490 - 03:42:54.490, Speaker A: Sam. Sam. SA. I don't I sam don't. Sam you. Sam ram. Sam.
03:42:54.490 - 03:49:56.030, Speaker A: Dam. Sam. Dam. Sam. Dam. Sam. Sam ram.
03:49:56.030 - 03:59:15.070, Speaker A: Sam. Sam. Sam SA. Sam ram. Sam. Sam. Sam.
03:59:15.070 - 04:03:49.930, Speaker A: Sam. Sam. Sam. Sam. Sh. Sam. Sam.
04:03:49.930 - 04:15:12.622, Speaker A: Sam. Sam dam. SA. Sam. Sam. Sam. Sam.
04:15:12.622 - 04:20:25.186, Speaker A: It's Dam. Sam. It sam don't. Sam ram. Sam. Ram. Sam.
04:20:25.186 - 04:25:08.700, Speaker A: Jam. Ram. Sam. It. Sam. Sam. Don't, don't get up.
04:25:08.700 - 04:30:53.180, Speaker A: Don't get up. Don't get ram ram sam sam sam SA hello. Welcome back to the afternoon of day one of modular summit. Hope you've all been having a great time so far. My name is Jill Gunter, I'm with Espresso Systems and I will be your MC for the Fourier stage this afternoon. And first up, it is my honor to introduce my friend and co founder Ben Fish from Espresso Systems. We'll be having a whole track here for the next hour and a half on the topic of shared sequencing.
04:30:53.180 - 04:31:18.100, Speaker A: And first up, Ben will be talking about why dumb blockchains require smart solutions. So without further ado, I think we'll have some more people trickling in here. But I will hand over the stage to Ben. Thank you so much. Okay. Thank you, Jill. Right, so does the clicker work? Nope.
04:31:18.100 - 04:31:45.420, Speaker A: Backslide one slide. Perfect. Right. Dumb blockchains need clever titles. This talk will primarily be about shared sequencing in the modular stack and is somewhat related to the title. Or maybe not. But as it will become clear, shared sequencers are designed as dumb or lazy ledger systems that don't execute transactions and just sequence.
04:31:45.420 - 04:32:24.472, Speaker A: And that poses some challenges that require some solutions. First, just quickly, about our organization, Espresso Systems. We are a team of engineers, researchers, and designers. We are building infrastructure that is going to assist roll ups to achieve better scale, better interoperability, and better security. We are a very distributed team. I am Ben Fish and I am the CEO. And my colleague Jill was just introducing the event.
04:32:24.472 - 04:33:33.804, Speaker A: She is our tree Strategy officer and there's a number of other executives as well. So let me start talking about this problem of sequencing in the roll up ecosystem and the modular stack more generally. But first, a starting point is to talk about what exactly are roll ups doing? Okay, so roll ups are horizontally scaling the application layer of blockchains. Primarily, today roll ups are being built on Ethereum, but of course, or on top of Celestia too. So if you view Celestria as an L one as well but in general, the idea is that the layer one, all it does is just verify fraud or ZK proofs. It may also provide availability of data and some other services, but it doesn't execute transactions. And so as you add new applications, the reason why this horizontally scales is as you add new applications, you don't burden the layer one with having to execute for those applications.
04:33:33.804 - 04:34:15.816, Speaker A: You can introduce a new set of servers that executes for that application and just proves the state result. To the l one. Roll ups are applications that host other applications. So things like Optimism or ZkSync are general virtual machines which can host other applications. But now we also have app chains that just execute their own application specific states directly. And all of these are horizontally scaling the execution layer of blockchains. Another key point besides sharding of computation across applications is that roll ups leverage heterogeneity in the network.
04:34:15.816 - 04:35:11.280, Speaker A: They're leveraging the fact that realistically, in networks, not all nodes are the same. If we want to scale layer ones to be extremely decentralized so that there's 12,000 nodes participating and one of them is a Raspberry Pi, or as I heard yesterday, apparently a Turing Pi is a thing too. But then the weakest node obviously will not be able to compute transactions as fast as the strongest nodes. So roll ups allow the more powerful nodes to do even more computation. When we're talking about ZK roll ups, it's 10,000 times or more more expensive to produce a proof than to even just execute itself. So we increase even more the computation of some powerful nodes and this helps weaker nodes catch up. So the thing is, today roll ups control a lot.
04:35:11.280 - 04:36:20.630, Speaker A: They control a lot more than just doing execution and horizontally scaling ethereum. In fact, they centralize the entire process of deciding which transactions to include and in what order. And this brings us back to a world in where there are central actors that have the power to discriminate, that have the power to impose monopolistic pricing or maximally extract mev from users. And these are all the things that blockchains were promising to get us away from. If you think about it, the roll up ecosystem today is essentially web Two applications that are being audited by an l One, right? But the actual processing and everything is really just being done by the web Two application. It's just being followed and verified by an L One. So what is the solution? One solution is to separate ordering from execution so that application layer roll up servers will execute transactions and not handle any of the ordering process, not handle any of the process of even making data available.
04:36:20.630 - 04:37:39.280, Speaker A: The l one would not build or execute its roles would only include finality on transaction ordering, availability of transaction data, and the verification of state proofs. And users would submit transactions directly to this decentralized l one. So this has also been called a based roll up architecture where you have the consensus network that users submit transactions to. Roll up servers read a transaction stream from this consensus network and then post a state route and approve like then gets verified. Challenges arise from this, however. If the l one is not doing any of the execution, then how does it set prices for including transactions that doesn't execute? How does it verify that the fees that it's receiving are paid correctly? Does it need to have some kind of minimal state to do that? How is revenue that it's generating shared back with roll ups or applications? How do we prevent spam? Could the l One overload the proving network without compensation for work? All of these are challenges, not all of which I will address in this short talk today. But these are all things that need to be considered when working with such based roll up architecture, or more generally, what I would call decentralized sequencer architectures.
04:37:39.280 - 04:38:37.792, Speaker A: So we can also look at hybrid centralized base architectures where and this has also been called escape hatches or L One inboxes I would say that is a form of based roll up, or related to, maybe not exactly the same, where there is a designated roll up server that can order transactions, but users can also submit directly to the L One. So the L One orders some of the transactions, the designated roll up server orders others. And we have some way of deciding, well, maybe the transactions from the roll up server go at the head, maybe they go at the tail. There's different approaches to doing that. And this hybrid architecture can be applied to any decentralized sequencing design as well. We can mix and match. So there's also perhaps a reason to separate the order finalization layer, at least logically, from the layer one.
04:38:37.792 - 04:39:48.916, Speaker A: And why might we do that? Why would we use anything except for Ethereum's gaffer protocol today for ordering and making available transactions? Well, specialization, right? Ethereum makes some decisions on how it works to optimize for certain sets of properties. For example, Ethereum is extremely available, ethereum is extremely decentralized, right? If you wanted to design a protocol that is still decentralized but prioritizes fast finality and can give faster pre confirmations, then that could be a reason to design another layer. If perhaps it's something that's designed for high throughput data availability, that's another reason to design what I would call some layer one and a half that sits between the layer one and the layer two. So protocol modularity opportunity to make different design trade offs higher throughput lower latency, faster pre confirmations. When it comes to talking about faster pre confirmations, a common misconception is that Ethereum's finality is 12 seconds. This is not actually true. This is the average block time of Ethereum transactions must be several blocks deep to be confirmed.
04:39:48.916 - 04:40:35.508, Speaker A: And it takes, in fact, 15 minutes for transactions to finally be finalized by Casper FFG, which is the finality gadget of Ethereum. Often users may consider a transaction final after five minutes, some may consider it final after 12 seconds. But that's very, very risky. You can read more about this by looking at Ethereum's discussion. Vitalik has a post on single slot finality where he explains that Ethereum's finality is actually 15 minutes, not 12 seconds. So let's say you wanted to design a decentralized protocol that could provide faster finality than Ethereum. Why doesn't Ethereum have fast finality? Well, it runs something called a dynamically available protocol, which means that even if 10% of the network is online, not 100%, it still can process transactions, it still can make progress.
04:40:35.508 - 04:41:26.520, Speaker A: Bitcoin has this property too. This is in fact one of the innovations of consensus protocols in the last ten years that started with the Nakamoto Consensus Protocol. But we also have protocols that are on a different side of what's called the Cap theorem, meaning they cannot be dynamically available, but they can achieve what's called optimistic responsiveness, which colloquially may be called fast finality. An optimistic responsiveness is the ability to respond as fast as the network will allow. When network conditions are good, the consensus protocol can give you instant finality. It also has an asynchronous fallback path, so that if the network conditions aren't good, it will eventually make progress, but it will stall if enough nodes go offline. Right? Ethereum doesn't have that property.
04:41:26.520 - 04:42:05.216, Speaker A: Ethereum cannot achieve fast finality because it will remain live even if a small participation set is online. Whereas protocols like Hotshot developed by Espresso Systems will go stall if too many nodes go offline. But if they're all online and the network conditions are good, they can give you very, very fast finality, almost like a centralized sequencer. There are protocols that achieve neither property, like tendermint. So responsiveness. It's the idea when the sun is shining, you get this high rate instant confirmation, but it's still robust when it's raining. So we call this web two performance.
04:42:05.216 - 04:42:51.764, Speaker A: With web three security. We can also blur the lines between the physical lines, at least between the layers. When it comes to things like eigen layer, things like restaking enable you to incentivize or at least subsidize the participation of the l one nodes in this layer, one and a half, so that the same physical set of nodes are running it, and you're making ultimately the same trust assumption. But just the protocol properties are different. So, moving on to sort of a second challenge, fragmented liquidity and interoperability. It's beautiful that this application layer is being sharded by roll ups, but this fragments the interoperability. So now two applications can no longer call each other.
04:42:51.764 - 04:43:29.836, Speaker A: They can no longer make function calls to each other. Your ave liquidity pool is no longer shared across all applications if they sit on different roll ups. Bridging across roll ups is complex. Atomicity is limited. So how can we recover this? And specifically, to what degree? Does sharing an ordering layer only, and not an execution layer, which would make everything the same roll up right? Does sharing an ordering layer only help? Okay, so this is the next question I want to explore. So there are three advantages. I will only focus on one.
04:43:29.836 - 04:44:06.440, Speaker A: One advantage is simplifying cross roll up, Bridging and atomicity, because you are sharing at least the same ordering protocol, so you don't have to verify each other's consensus. The second is mitigating what I would call systemic risks of Bridging overall and the opportunities for profit that they provide to adversaries. You can mitigate that and reduce it by sharing a consensus protocol. I won't address these first two. We have blogs on it. But I will focus today on explaining why this supports what I would say cross roll of building with economic bonding. This is not the same as sharing an execution layer, but it gives you some very interesting guarantees.
04:44:06.440 - 04:45:23.296, Speaker A: So first, quickly on proposer builder separation, this is the way things work in protocols like Ethereum today, it also works like this in Hotshot, there's a proposer that's elected by the consensus protocol in every slot to propose transactions. But this proposer, while it needs to sign and broadcast information, doesn't need to come up with everything that needs to go into that block that could be outsourced. In fact, it could run an auction among competing builders who are able to build an optimized block. There are different things that this might be optimizing for. Generally, if we let the proposer run this auction, then naturally the builders who win the block auction are going to be maximizing the revenue of the proposer. And that's what motivates us to do some other designs which are designing like an ideal functionality that implements an order flow auction which is optimal for users and stable for users. Could we design something where users are getting the best execution prices available to them or guaranteed that their transactions don't fail? And this is something that teams like Flashbots and Swab have been working on and other teams too, and is sort of the future, I think, of sort of the order flow design of blockchains.
04:45:23.296 - 04:45:55.532, Speaker A: But in order for this to work, the consensus protocol needs to be extremely decentralized so that the proposer doesn't just say, okay, you know what? Screw this ideal functionality. It's not good for me, it's not maximizing my revenue. I will just run my own auction the way I want to. No, if it's extremely decentralized, the proposer gets elected once in a blue moon. It can either take transactions from the ideal functionality or have no transactions at all. It can't make a credible threat to users to abort the ideal functionality. And that's why decentralization is important.
04:45:55.532 - 04:46:27.368, Speaker A: This is in fact why EIP 1559 works. It only works because Ethereum is extremely decentralized. But with proposer builder separation over an extremely decentralized shared sequencing layer, decentralization is key. Then you can also introduce these builder functionalities on top. And these builders or ideal functionalities can also start to provide some interoperability guarantees. Example a builder could say to a user, okay, the user wants to trade on optimism and ZK sync. It sees some arbitrage opportunity.
04:46:27.368 - 04:47:21.780, Speaker A: It wants the builder to include both of these transactions in the block and ensure that they both succeed. Because the builder is basically able to win this auction of a superblock wholesale that sends blocks to both ZkSync and Arbitram. It controls basically has a lock on the state of both ZkSync and arbitram and thus can make this guarantee to the user. It can promise that it will include both trades in the same block. And it can also write this promise in a cryptographically authenticated way, so that if it violates this promise, then the user can go to some smart contract and slash it. Okay, so if I don't do this, then you can use this as evidence to slash me and it can put up a big collateral in order to give the user confidence that this promise will be satisfied. This is a simple example, but we could go through an example of even cross roll up flash loans.
04:47:21.780 - 04:48:20.860, Speaker A: It will take more time, so I don't have time for it in the short talk today. But just to summarize, so Espresso sequencer is one of these layer 1.5 sequencers. It sits between the L one ethereum and roll ups. Although we're trying to blur the physical lines by using Eigen layer and it provides this consensus on ordering, but it also supports Proposer builder separation. And we envision most blocks being built in that builder layer, which can actually provide the interoperability guarantees to users, which wouldn't be possible if you had different consensus protocols because builders would not well, builders would have to simultaneously win many auctions, and that carries a lot of risk. A common concern is how is revenue from cross roll up bundles allocated back to roll ups? Another common concern is whether this leads to builder centralization.
04:48:20.860 - 04:49:15.000, Speaker A: To address the first concern, the concern that builders will need to execute for all interoperable rollups and that may create a higher barrier to entry, leading to fewer competing builders. So first, I think it's important to recognize that builders and validators are not the same thing. And we don't require the same level of decentralization at each layer of the stack. We need to work from first principles and look at first of all, what does decentralization mean? Does it mean a lot of economic stake? Does it mean a lot of distinct physical nodes? Does it mean geographical? And what is required from each layer, given the service that it's providing? Executing for all roll ups is not actually that hard. Maybe it's hard for 12,000 nodes, including a Raspberry Pi. Maybe it's just hard for a Raspberry Pi. But executing is not a huge barrier to entry.
04:49:15.000 - 04:50:10.000, Speaker A: And we don't necessarily need 12,000 competing builders. We just need enough competing builders to create competition in the market that drives down monopoly pricing, not necessarily 12,000 of them. So it's unlike the decentralization requirements of Proposers. And in fact, if you have a highly decentralized base layer or consensus protocol, then proposed, that's what that's what enables Proposer builder separation. That's what makes these proposers more passive and able to be compatible with ideal functionalities that are designed in an ideal way for users that are thus stable for users. Furthermore, the barrier to entry is much higher without shared consensus because it requires more capital to win simultaneous auctions. The builder absorbs more risk and it can't promise user atomicity without this risk.
04:50:10.000 - 04:50:48.600, Speaker A: This phenomenon of builders executing for all roll ups is an inevitable consequence of the user desire for interoperability. It will happen no matter what. And so I would argue that designing a shared sequencing layer is going to lower the barrier rather than create more Asymmetry among builders and increase the barrier, which is the alternative of many independent roll ups with their own sequencers. This is a small meme about that. Where there's a will, there's a way. Builders that execute for all roll ups anyways will find a way to do it. Shared sequencing enables more builders.
04:50:48.600 - 04:51:30.852, Speaker A: I don't want to go into the revenue sharing problem because I'm out of time, but I just want to say it's connected to order flow auctions, and there's a cool idea of splitting up slots, so that at even slots, you don't have cross roll up bundles, and it's very easy to divide the revenue. And in OD slots, it may be trickier, but you get a good approximation from the even slots of what to do. In the OD slots announcements we have adopio testnet. We also have double shots of Espresso available at our booth. But the second testnet of Espresso sequencer, called Dopio, is now released, and it features an end to end integration with Polygon Zke EVM, a fork of Polygon ZK EVM. Next, we're working on op stack integrations. We're working with Caldera Spire catalyst injective alt layer.
04:51:30.852 - 04:51:55.680, Speaker A: We recently won an optimism sequencer, decentralization RFP, so we'll be working on that. And here's a little picture of our little growing ecosystem. If you want to join this, please let us know. We're happy to talk to everyone. So thank you so much. Great. Thank you so much, Ben.
04:51:55.680 - 04:52:19.796, Speaker A: We're going to be hearing more about shared sequencers next. We have Josh Bowen, CEO of Astria. Coming up, who's going to be talking about Astria speed running the endgame. Josh, over to you. All right. Hey, everyone, thanks for having me. So, yeah, as they said, I'm Josh.
04:52:19.796 - 04:52:49.180, Speaker A: I'm the CEO founder of Astria. Okay, which button? The green one. Okay. All right. So I actually renamed my talk Astria's Vision for the endgame. I used the talk speed running the endgame previously, so I get the slides in late. But what I'm going to walk through here really is what Astria's architecture looks like and kind of the top to bottom of why we're building the architecture we're building because as shared sequencers have gained prominence in the last five months since this has even been, like, kind of a concept.
04:52:49.180 - 04:53:20.104, Speaker A: You know, there's been a lot of questions about why it should be designed one way or another. And so we're going to give that. And so for astroding, we have two kind of key principles. We think that roll ups should be decentralized by default. This is generally a pushback on the idea of this progressive decentralization, which has been promised by a lot of different roll up networks, but quite frankly, we haven't seen it. We're 18 months into main net on a lot of roll ups and they're still just as centralized as they were day one when they launch. So we believe that if we want to see a world with many, many roll ups, they're going to be decentralized by default.
04:53:20.104 - 04:53:50.564, Speaker A: Otherwise we'll have this recurring window of we launched it, it's centralized, we'll get to the decentralization later and there will perpetually be newcomers that are just centralized. So that's principle one. Principle two is that we believe deploying a roll up should be as easy as deploying a smart contract. Am I going to get a clock time there or should I just guess? Okay, so we believe deploying a roll up should be as easy as deploying a smart contract. And now I'm just going to go into kind of architecture stuff. I have 53 slides to get to, so I'm going to talk quickly. This is how we view the general kind of transaction thing.
04:53:50.564 - 04:54:20.488, Speaker A: You could call this like the transaction order flow, but we put it into kind of four stages and noting that we are at least myself, is not in the camp of a roll up being defined by its bridge. So that's just you can argue with that. We have a panel after where we can fight, but we believe we start with unordered transactions that could be intense. We go to an ordered block. From that ordered block, we end up with an executed block that has a state DB and presumably a resulting state root. And then after that you can have a succinct proof or a ZK proof. And that is kind of what we're going to argue, the ordering.
04:54:20.488 - 04:55:09.608, Speaker A: We'll label these as order flow, sequencing, execution and proving and view those as four phases. Again, people can disagree with terminology, but I have to pick some semantic definitions to go with. Right, okay, so state of sequencers today, this is what I'll argue is kind of the most simple case for a centralized sequencer today. You have an end user, they're going to talk to a roll up sequencer, get a soft commitment there as it's one box that's a centralized sequencer. That sequencer pushes batches to the DA layer. You get a firm commitment for that. What are the problems with that? Why would we decentralize sequencers typo there? Because this roll up sequencer that is centralized has a persistent monopoly on ordering, right? And that's a problem for any number of economic reasons on transaction flow, on censorship, right? But we don't want to have one party that has a persistent monopoly.
04:55:09.608 - 04:55:50.924, Speaker A: And when we say persistent, we mean across all time, right, across blocks. So if we go to a decentralized sequencer where we'll just simplify it and say we have a leader rotation, so there's multiple entities, this is still for one roll up, but multiple entities get a chance to be the sequencer for the block and everything else is the same. We move to having a per block monopoly on ordering. This doesn't resolve everything, right? We've seen a lot of interesting designs like multiplicity for multi validator, like block proposing. But it is, we'll argue, better than having a persistent monopoly across all time. There's various research on mev things where you can maximize mev by extracting no mev in one block and putting all that mev in a second block. We remove cases like that.
04:55:50.924 - 04:56:19.910, Speaker A: When we say there's now a per block monopoly on ordering, you can't guarantee you're getting multiple blocks or at least it's probabilistic. We'll argue that's better. Right. So why shared sequencers? Why would we want to go from decentralized for one roll up and many roll ups, having many decentralized sets to shared? And this is what we're going to posit as like a model for this. You'll still have a user, they'll communicate with the RPC for a given roll up. So this is your MetaMask to like a guest node or whatever, and it will push transactions to a shared sequencer. We implicitly assume the shared sequencer is decentralized here.
04:56:19.910 - 04:56:43.788, Speaker A: And so what are we doing here right now? We're shared right? Here we have multiple roll ups. Users still can interact directly. There's more complicated designs, but for this tonko limit here, where you can have multiple roll up RPCs sharing a sequencer. So this is our kind of simplistic model. It's very similar to like, the Espresso guys, kind of more complex, like diagram. And they have a great demo visual on Twitter that you can look at. Come on.
04:56:43.788 - 04:56:59.856, Speaker A: Right. So what is a user actually getting from a sequencer commitment? When we say it's like a soft commitment, what are they actually getting out of it? So I really like this quote. I've used this a lot of times. It's from Nick Carter's article. It's the settlement assurances, stupid, from way back in 2019. It's obviously Nick Carter. Right.
04:56:59.856 - 04:57:36.568, Speaker A: It's a little bitcoin adjacent, but I think it is some of the better written work on what we're talking about when we get a commitment of finality. And so settlement assurances refers to a system's ability to grant recipients confidence that an inbound transaction will not be reversed. Right? This is usually used in the case for the specific article of bitcoin. This is the six confirmation depth thing, right? It's probabilistic finality. When do you know a transaction won't be reverted? We'll say settlement assurances or settlement is this act of not being reverted. So we'll say a sequencer guarantees a user that their transaction will not be reverted, right. And then if we know okay, is this a settlement layer? Right? We have this quote from James Fretwood.
04:57:36.568 - 04:58:23.212, Speaker A: We actually pinned this in our marketing channel in Slack. This is one of the things that was influential on moving from we're building a settlement layer to we're building a shared sequencer. The term settlement layer is very busy, muddy, right? And confusing. So we're not going to use that. But why shared sequencers, right. Come back to this question, why would you want a shared sequencer? We argue again from the Nick Carter, it's the settlement assurances post, right? Ledger costliness is the most profound and direct variable available to us to evaluate a blockchain settlement guarantees, but simply it's the equivalent to the amount paid to validators transaction selectors per unit of time. So in like a Bitcoin case, which is probably the most simplistic one to understand, this is like the amount of computational effort put into a given fork of the chain, right, and literally the money burnt to give you that layer of settlement finality, right.
04:58:23.212 - 04:59:51.680, Speaker A: In a proof of stake chain, in something like ethereum, right, where you still have a heaviest chain and it's slashable, right. It is the depth or like the weight of a stake that has been posted on one given fork of the chain, right? That's what you're going to call ledger costliness, right? So we argue that sharing a single sequencer set between multiple roll ups will increase the ledger costliness, right? What we mean by this is if we have ten roll ups and we'll assume they have kind of heterogeneous transaction flow, right? They're maybe not all EVMs, they're maybe serving different use cases and they are all relying on a single point of settlement and they are doing like a proof of stake mechanism, or even if it's something like a proof of governance or just like kind of an implicit governance kind of chain link style implicit staking, right? If you have multiple use cases and more order flow using a single point of settlement as stronger guarantees, if it is shared between all entities rather than each of those roll ups attempting to acquire its own kind of set of ledger costliness, right? So this is what we'll look at, right? We look at this architecture. We'll assume that the soft commitment has a high likelihood that transactions are settled. Specifically if it's shared, we can assume it is a higher likelihood that the transactions are settled in a shared set than if it was decentralized across any given roll up. So then we'll say why lazy shared sequencing? And so, again, I think our architecture and Espresso's architecture is implicitly. We say it's a lazy shared sequencer. And what we mean by this is it's not executing the state transitions right to the original picture.
04:59:51.680 - 05:00:35.760, Speaker A: It's doing the ordering, not the execution for all the roll ups. So why are we doing that? Right? Well, really, there's a question of why wouldn't we do the stateful shared sequence? It would be really nice if we could say, hey, we all do it in one go and we all get this commitment over a state route and we get the ledger costliness and it's nice and clean. Why are we not doing that? That seems like the obvious nice solution, right. War, you that the resource requirements for a stateful shared sequencer, one doing execution for all the roll ups scales in. Proportion to the number of roll ups. It's not explicitly the number of roll ups. Right? But if you assume that I, as a validator set, a sequencer set, need to execute three different heterogeneous state machines, I need to keep the state and I need to process and do the execution, use the computational power and the storage requirements to keep all these states in these ledgers.
05:00:35.760 - 05:01:34.564, Speaker A: That increases if I add another roll up, then I have to pay more cost, right? So going back to one of our principles, deploying a roll up should be as easy as deploying a smart contract, right? If it's a stateful shared sequencer, it prevents permissionless use of a shared sequencer. And what this means is if you are introducing every time you add a roll up to a shared sequencer, you increase the cost, the resource cost to the actors in that sequencer set. Then that cannot be permissionless because you've now introduced a Dos vector, right? I can go say, I'm going to go make ten roll ups and I'm going to push them into the shared sequencer and I've just increased the workload of all these entities, right? So if we look at things like polka dot's parachain auctions, right, that is a mechanism to create an economic cost to join the sequencer set, the validator set. If we look at interchange security with the Cosmos hub now replicated security, right? There is a governance process. You need to convince the validator set that they should allow your state machine to join the set. And we believe that it should be as easy as deploying a smart contract. You don't need to ask for permission to deploy a smart contract.
05:01:34.564 - 05:02:41.500, Speaker A: You simply go to the chain and you pay the gas fee and you deploy your contract. Right? So that's one of our arguments for why we should use a lazy shared sequencer but then going back, if we're not doing the execution right, what is a lazy shared sequencer actually guaranteeing to our end users, right? And we're going to argue that a lazy shared sequencer guarantees that a transaction's position in the canonical block ordering won't be reverted. So specifically, this is a weaker guarantee than saying the resultant state root of your roll up is X, right? And it's actually giving you this state root hash. If you're familiar with like a tendermint kind of Cosmos space, there's a distinction between tendermint giving you a block hash and the Cosmos giving you a state root, right? We are talking about a block hash, right. You're giving a commitment of the ordering of a given block, but not a state route of a post state machine execution, right? And so we argue again, right? Deterministic state machines is kind of what we're doing with blockchains generally, right? A chain state is determined by recursively applying its state transition function to an input state in an ordered block. Right? So very simplified function here, right? If we're at state zero. And we want to move to state one.
05:02:41.500 - 05:03:10.776, Speaker A: We provide block one and we just run the state transition function. Now we have state one. Right? I'm not like a math guy. I don't know what the math symbols would be to recurse this. Right. But run that every time from Genesis, you now get the resultant state root. And the important thing here, right, is that from a genesis state, if you have access to all of the ordered blocks for a roll up, but not the state commits of those roll ups, you can still, as a full node executing that deterministically reach the same end state as everyone else for a given fork of the roll up.
05:03:10.776 - 05:03:42.656, Speaker A: Right. So we still have the capacity to do this execution. But really, when I was like, when are these blocks actually being executed? Like, who's doing this execution if not the sequencer set, the validator set? Right? So this is when we go to Ethereum land, right? So this is a picture from mevboost. Pix is a great dashboard place to look for information on mevboost. When I checked on looks like the 16th, right? We're at 95% of Ethereum validators are running blocks through mevboost, right? Or like 95% of the blocks on Ethereum overall are mev boost. So we're pretty close to 100% of people are using PBS for this. Right.
05:03:42.656 - 05:04:17.656, Speaker A: And then let's look at the relay dashboard, right? So within relays, there's really two types, right? I guess there's three types if you consider the censorship ones, like a type, right. But we see the largest relayer, 28.3% is the ultrasound relay, and the ultrasound relay is considered an optimistic relay. And what we mean by that is that validators are actually not checking the block that is given to them by a builder to ensure that it is a valid state transition function. They're not re executing. The reason they're not doing that is they save some milliseconds of latency by not doing that execution. And the way that PBS works as an auction mechanism.
05:04:17.656 - 05:04:52.184, Speaker A: Builders want to be able to submit their blocks as late as possible in the block timing by shaving, let's say 100 milliseconds off of that. They notably change the amount of order flow, the amount of blocks that are sent to them as a relayer. And they're all trying to competing for order flow. Right? So by just saying will yolo, right, we're going to assume that builders are not sending us fake blocks and we will not execute that. Right. So what we argue is 28% of all blocks 28% times 95% is like 26.8% or something of all blocks going through Ethereum today, the validator is not actually executing the state transition function.
05:04:52.184 - 05:05:21.084, Speaker A: So what does that mean? Where are these blocks actually being executed? So we'll argue this is like a rough design for how we would see a shared sequencer having like a PBS mev supply chain order flow, right? Users go to roll up RPCs. Those RPCs could be mev. Supply flow. We could have like Suave, whatever, right? But they'll go to something that's order flow. And we're going to assume that order flow is a thing that takes transactions. Maybe there's bundles in the middle. Searcher builders, they're not that distinctive parties actually in actuality at scale, but it outputs blocks to the shared sequencer proposer.
05:05:21.084 - 05:05:58.444, Speaker A: Right? And the important aspect of this diagram is that to generate these bundles, to generate these blocks, execution is needed to find profitable blocks. Right? If we think about what we saw with Mev before, mev geth this is pre merge, right? What would you do? You'd do a priority gas auction. You'd spam the chain to try to get your transaction and it would revert if you didn't like it. Right. We introduce an auction mechanism and actually it's not economically desirable. It's not like game theoretically optimal to go submit non functioning blocks, right? If I'm like a PBS person, I can go grief the chain. But if I'm here to make money, like I'm a sophisticated professional actor, I'm Jane Street, I'm jump whatever, right? I want to make money.
05:05:58.444 - 05:06:49.580, Speaker A: So I want to submit good blocks. So I have to do execution. I have to run the knapsack problem across all the state machines and theoretically find what is the optimal block, right? It's like a PNP problem or whatever, right? They're going to do execution to find these profitable blocks and they're going to know whether a block works, right? And so the shared sequencer, this is the key thing defines the canonical block ordering. But really what it's going to do is it's going to choose the most profitable block that is offered to it. And structurally a shared sequencer is essentially the same thing as an optimistic relay, right? It's going to be given a bid and a block. And the economic incentives say that the people who are going to be sending it blocks are going to send it profitable blocks and profitable blocks are going to be executable blocks that are valid, not blocks that are just kind of griefing the chain and saying, oh, look, you have a block and it's just going to revert, or whatever. Right? So that's our argument of where the execution is actually happening in this stack.
05:06:49.580 - 05:07:23.076, Speaker A: We can have a lot of debates maybe in the panel on the trust assumptions you're making, whether you've moved to economic assumptions instead of programmatic assumptions or cryptographic assumptions. But remember, this is where 27 ish percent of all ethereum blocks are already being pushed this way and that's relatively stable, but we could see increase. Right. This is status quo today. So touch on Bridging really quickly here, right? We're not doing the execution. So when we go to Bridging, right, is a role defined by its bridge? Not answering that question here. But if we say three components of Bridging, right? I'm going to argue that bridging is between a sending party and a receiving party.
05:07:23.076 - 05:07:58.656, Speaker A: And the key of bridging is that sending party has to convince a receiving party that the data is available, that there is a canonical ordering. And what the post execution state is this is broadly what like a contract on the receiving side of a bridge is looking for, right? If the data is available and we have shared DA, we're going to say that's a trust minimized bridge, we're not going to go too much in depth to that. But that's generally what we're looking at in the rollup sphere. But really when we look at this kind of order flow diagram, we're going to say the soft commitments. Again, the shared sequencer is defining the canonical ordering of the chain. So we're getting a canonical ordering between all the roll ups on the shared sequencer. The firm commitment coming from the DA layer that's giving you your data availability.
05:07:58.656 - 05:08:29.992, Speaker A: So the question is who's attesting to this post commitment state route, right? Who is actually executing the chain and saying I will give some kind of guarantee cryptographic economic whatever of this actual post state route. So what's the state of the world today? Right? The status quo EVM chains by and large. So whether you want to say like a Gnosis chain, you want to look at the roll ups between each other, right? They're often not going through the optimistic bridge seven day withdrawal window. That's really a long time, right? So you have mechanism. They can use connect, they can use something like hop. But in reality a lot of them are using things like layer zero. They're using things like axilar.
05:08:29.992 - 05:08:56.404, Speaker A: They're using things like I'm forgetting the third one wormhole. Right? Fundamentally, I'll consider those third party committee bridges. You have some off chain set of actors that are making a multi SIG adaptation and staking their reputation. Maybe they have an on chain stake on the validity of that state route. Comet BFT Chains They're using a native validator set. It's still a committee bridge and still an attestation making an honest majority assumption. But they use their native validator set, right? We'll say this looks like this, right? You have sequencing, you have an ordered block.
05:08:56.404 - 05:09:35.956, Speaker A: You do execution, you get an executed block. That executed block that is implicitly ordered, right, goes into a committee. That committee generates an aggregated signature, whether that's bos, multisig, whatever, right? And that aggregated signature is sent to the receive side and that's the kind of attestation we're making. What is the next gen, right? We see Sucynct Labs working on proof of consensus of ethereum sync committee polymer is working on ZK mint, right? And so what fundamentally these are is we're going to do the whole first step, right? We're going to do an order block, we're going to do an executed block. We're going to pass it through some committee. That committee is going to have an aggregated signature pass that aggregated signature into Approver and we're going to get what we're going to call like a proof of consensus, to use the Zinc term. Right.
05:09:35.956 - 05:10:16.812, Speaker A: Why do we want this proof of consensus? It's kind of like an interoperability solution, right? Cryptographic curves are hard to verify. They're expensive to do in contract, right. ETH doesn't know how to verify an Ed 25 519 signature natively or cheaply for tendermint. So to go between tendermint Comet BFT chain to like ethereum, if you wrap it in a Snark, you essentially have very expensive yet off chain signature aggregation modification, right? So this is what we're seeing a lot of things move towards, right? But what do we think the end game is? Right. We'll argue that prover networks are going to ease the creation of Succinct state proof. I'm not a ZK person. I'm going into details of ZK stuff of a storage proof or the state proof, right? But fundamentally we have things like risk zeros, like Bonsai Network.
05:10:16.812 - 05:11:17.284, Speaker A: We have like the Nil Foundation guys, right? We have these people working on networks that say you take a state machine, you do the execution and you pass it into this generic box of proving and it's going to give you a Stark, a Snark, whatever, right? This succinct proof of execution that allows a light client to verify it. However, it's worth noting intercluster Bridging still needs this proof of Canonicity or this proof of consensus over the ordering, right? Because fundamentally you can easily generate a proof of a state machine or theoretically you can easily generate it, but you could generate a proof that is a valid state transition but is not the canonical chain state transition. So you still need both components, right? What you get with shared sequencing and it's like 1 minute. So I'm rushing one proof of candidacy for multiple roll ups because the shared sequencer defines the ordering for the chains, right? So you can generate one proof for all the chains using the shared sequencer that tells you this is a canonical chain. And then within the shared sequencer, you just need the state proof. So we'll say this looks like this, right? You order a block that's happening on the shared sequencer standard. Kind of like next gen stuff, right? This is the Succinct style stuff.
05:11:17.284 - 05:11:48.764, Speaker A: This is the comet BF or the ZK mint from the Polymer Labs guide, right? You generate this proof of consensus and then once per roll up, you do the state execution. You pass into Approver network, you get a ZK state proof. Both of these things go to your receive side chain. Now you have all the components you want. Again, we can go back and say if you're in a shared DA layer, you get something trust minimized, right? If this is everyone sharing Celeste, if this is everyone sharing Ethereum, right, you get slightly better guarantees as well. But this is how we view it at the layer above that. So again, looping back to where we started.
05:11:48.764 - 05:12:09.110, Speaker A: We believe that roll up should be decentralized by default. Come on. We're giving a workshop at 430. Shortly after the panel, in the couchy stage, we're going to show off what we have for a demo. Right now, we have a development cluster very similar to actually, like, what Espresso has. We'll show you guys how you can run this locally and how you can deploy your own roll ups using a shared sequencer. So thank you, everyone.
05:12:09.110 - 05:12:55.736, Speaker A: Awesome. Thank you, Josh. I now see why the original title was Speed Running. That was truly a speed run. Coming up next, we're going to actually have a panel on shared sequencing. I'd like to welcome Evan from Celestia up here, who is going to be moderating and hosting the other members of the panel. Please come on up as well, Evan.
05:12:55.736 - 05:13:47.064, Speaker A: Are you supposed to be on the left? We want to do in order you're the CEO of it. Hello. All right, so the whole theory that I have for this panel is that all the talks that we just saw, they're sort of like movies. This is more like the features on the DVD. This is something to go get to know these guys. This is something to sort of I hope that if we don't answer any of these questions and we just sort of go with the flow, I'm totally okay with that. This is just to go with the flow.
05:13:47.064 - 05:14:04.624, Speaker A: Just something interesting. So we already sort of have an intro, so I'll just run through these intros really quick. Right here we have Ben fish. He's the CEO. Co founder of Espresso. He's also an assistant professor at Yale. He has some incredible research that is widely used today.
05:14:04.624 - 05:14:22.648, Speaker A: So proof of replication. I just learned this for Filecoin. Literally the most used I'm supposed to talk on a microphone. Literally the most used ZK proving system today. No big deal. Also VDFS and even some economics research on mining. Really great work.
05:14:22.648 - 05:14:35.432, Speaker A: We also have Connor. So Connor is just a general purpose gigabrain. I have learned so much from him. Oh, yeah. He's the CEO of Rollkit. Apparently. He's an engineer and a researcher at Rollkit.
05:14:35.432 - 05:14:55.068, Speaker A: Personally, I've learned a ton from Connor about how you actually decentralize roll ups and without using consensus to use something like fork choice rules. It's really, really interesting work. And you've also done work with NPC and you were a guitar teacher. Yes. So he can riff. Okay. And then we have Josh Bowen.
05:14:55.068 - 05:15:17.192, Speaker A: So, Josh, you are known, I think, for your width and depth of knowledge, which is rare. You've done work again, also some research at Celestia on execution environments, fraud proofs, optimistic Rollups. And before that, you were at Edge and Node, right? Yes. And then before that we don't want to talk about that. No. Okay. As a Google.
05:15:17.192 - 05:15:34.670, Speaker A: He worked at Google. Okay. So the first question is what is love. No, I'm just kidding. This is not like the Lex Friedman podcast, but we could talk about that. Yeah. Okay.
05:15:34.670 - 05:16:47.190, Speaker A: The first topic I really want to get into is not really comparing which is best with consensus and decentralization, but one of the really interesting things with this panel is that we have three shared sequencers or with rollkit, a way to build shared sequencers as a roll up, but they're all in very, very different contexts. So it's like all of these folks and all these projects are incredibly smart, but they made different decisions based on that context. So if we can dive into the why and the how of why we use the consensus algorithms that we used, then I think that gives us a better understanding over the problems that we're trying to solve, which, of course, shows us what shared sequencers actually do. Because sometimes if you go on Twitter, one of the leading questions is just like, WTF is a shared sequencer, so we can just go in order? Ben sure. Yeah. So the way that I view it is if roll ups are built on top of the layer one, then the default shared sequencer should be the layer one. If it's not the layer one, what are you doing? Right, well, you're trying to do something that the layer one doesn't provide.
05:16:47.190 - 05:17:41.752, Speaker A: So if you're trying to build roll ups on top of Ethereum, and I'll talk about building roll ups on top of Celestia in a second, but if you're trying to build roll ups on top of Ethereum, ethereum is a dynamically available protocol, meaning it's very available. It works even if 10% of the network is online, it has very slow 15 minutes finality. So we decided that, well, what does it make sense to build for a shared sequencer that is not the layer one but sits on top? Well, something that provides a different property, and that's fast finality. So that's what Hotshot, our consensus protocol, does. It optimizes for getting 12,000 validators to be able to reach consensus within a few seconds. It sacrifices the availability guarantees that Ethereum has in the sense that if less than 75% of the network goes offline, then it may not make progress. But that is the reason we did that.
05:17:41.752 - 05:18:29.676, Speaker A: If you were building a shared sequencer on top of a different l one, like Celestia, which uses Tendermint, then it may make sense to use something else, right? Tendermint. We didn't decide to use Tendermint because tendermint is not optimistically responsive. That's like a separate discussion. It can have fast finality, but it doesn't necessarily scale to as many nodes if you want to retain those fast finality guarantees. So Hotshot is designed to scale to Ethereum's validator set because we want to be as close to the l one as possible, just providing a different property, in this case, fast finality instead of high availability. Never thought about using roll kit as a shared sequencer. That may be a possibility, but we're more interested in using shared sequencers with rollkit.
05:18:29.676 - 05:19:31.460, Speaker A: Rollkit is meant to support all different kinds of sequencer schemes because we've identified many and the trade off space between them all is unbelievably complicated. So we like shared sequencers a lot because one of our goals has been to allow people to launch chains without needing to provision a validator set. And when you've promoted that idea and got everyone excited about it and then they find out they have to have sequencers, then it doesn't seem as cool anymore. So you can do a roll up without sequencers easily if you do the base roll ups, the no sequencer roll ups, but those inherit the block time of the L one, which on Celestia is 15 seconds, 15 2nd finality, 15 2nd blocks. A lot of people want to go faster than that. When people think about roll ups, they think about arbitram and optimism which give very quick soft confirmations and provide a very fast UX to the users. And if you lose that when you go to develop a roll up, you're going to be sad.
05:19:31.460 - 05:20:26.776, Speaker A: So shared sequencers, let us do this. No validator set, easy to launch and also have the fast user experience. So shared sequencers are probably one of our highest priorities to integrate into roll kit as soon as possible. Yeah. So kind of reiterating what the other panelists have, right? Like, I take a much more kind of practical background. I'm not an academic by any means and so the evolution of ASTRI as a project came out of my work at Celestial broadly, what we were working on was like, how do I deploy a roll up on top of Celeste? What are the components necessary to kind of do that? Right? So we go look at why are there sequencers in ethereum roll ups? We've all seen the base roll up design mechanism, right? But the fundamental reality was that users would prefer faster response times than slower response times. I often reference there's some research from 1991 on how do users feel about user experiences in web applications, right? And roughly it says users want to click a button and expect it to respond within one to 4 seconds.
05:20:26.776 - 05:21:26.376, Speaker A: Shorter is better. There's probably more modern research on mobile UIs, right? Everyone's used like an older phone or like iPhones are known for having much faster latency and it feels kind of smoother, right? But users like responsive user experiences to evan's point, right? With these centralized rollups, we got these very, very fast responses and again, I'm kind of cynical or know, I view it as practical, right? If we want to say, hey, we have these centralized sequencers, we want to decentralize the sequencers. If you sacrifice the latency benefit, that is the reason they moved decentralized sequencers in the first place, users are going to say, oh, this is just like a degradation of my protocol. No one in a roll up wants to be like the first mover to say, cool, I'm going to use the shittier experience that my users like less than the centralized ones. So it's kind of our job if we say we think decentralization is important, then we need to provide a user experience that is adequate, that users watch. We use the damn decentralized thing because I am not optimistic that users are going to be just like altruistic actors who say no, I use the thing that is the most decentralized. No they don't.
05:21:26.376 - 05:22:04.620, Speaker A: They use the thing that is convenient for them, that makes their life easy. We need to provide a decentralized solution and I think shared sequencers are a way to do that while minimizing against it. Should be easy to employ a roll up, right? We can't have everyone do this problem. Like if we want 1000 roll ups, we can't have every single project need to do this process every single time. Yeah, going back to the point of taking away the fast responsiveness have you guys ever taken away ice cream from a child? That's not fun. Okay, so next topic. Another way to think about a shared sequencer is it's a committee and it can perform some sort of service for roll ups as a committee.
05:22:04.620 - 05:23:07.580, Speaker A: And specifically there's just a lot of different services that you can provide doing that. For example, like with Espresso, you can provide Ta and with Astria, as far as I understand, you can do something like IBC and Settlement, like with the EVM portion. So I'm curious to dive into what are the other committee based services that we haven't already touched on? Or what are some committee based services that you think would be useful for roll ups? So again, we can just go in order. Yeah, well, it's interesting that you think of it as a committee. If the committee is like 12,000 nodes or if there's the same set of nodes as the underlying l one, then it's not necessarily a committee, it's the same physical set of nodes. But it's logically playing a different role of offering a modular component of the overall services that a roll up needs primarily ordering. And then availability can come for free as part of that.
05:23:07.580 - 05:23:59.730, Speaker A: But then you bring up a good point like what other services can you get from an ordering layer? So we've been talking about in our talks how shared ordering layers like Espresso, like Astria also give you interoperability guarantees. Beyond that, you could think about adding validity checking as well. So what if roll ups could also just settle directly to the shared sequencing layer? Then you might ask, well, what is the layer one doing? Right? Well, the layer one is still there. Perhaps layer one is being used for availability. But the layer one and a half for some reason is better at providing. For example, it may be easier to add a custom opcode to verify ZK proofs at the layer one and a half that is faster. It really depends on what you want to move between the layers, because the layer one can provide all these services as well.
05:23:59.730 - 05:24:47.676, Speaker A: I can't think of anything we want besides a fast, dumb, decentralized ordering machine. Okay. I'll give a more optimistic take. I guess fundamentally, I view shared sequencers, and this is going to my kind of cloud SaaS, enterprise SaaS background, right? And this is a land of middleware, right? When I joined Crypto, I think one of the weird things to me was the architecture was like, oh, there's like, Ethereum, and then you just slap a UI on top of Ethereum. And I'm like, that's not what Enterprise architectures look like. There's like 13 layers between an Enterprise service bus and some Oracle DB from, like, two decades ago, right? And the mobile app you use to check your insurance, right. Layers exist, and middleware is used to kind of intermediate between these and to reduce complexity of interactions with a broader kind of ecosystem of things.
05:24:47.676 - 05:25:42.620, Speaker A: So if we think about shared sequencing, again, it's a committee. What can you get from this committee? I think of it as like we've used the term decentralization of the service, but also it's like, what can a roll up get for free by using a shared sequencer? Again, it gets these soft commitments to my talk right there's bridging benefits if you say you still need some kind of Bridging guarantee of the canonicity of this chain. This is less useful if you're doing like, a settlement to Ethereum thing, right, where you say, well, okay, is the roll up defined by the bridge? Well, the Bridge on Ethereum has control over the ordering of the roll up in some mechanism, then that's fine from a Bridging. But if a roll up wants to bridge to some extra domain space, it's going to need to make an attestation of the canonicity of its given fork to that receiving chain. We could also think about things kind of like, farther out. Like, okay, is an Oracle, is it a roll up of the shared sequencer or whatever. But you could say part of the data stream of this shared sequencer is some set of Oracle transactions from off chain.
05:25:42.620 - 05:26:29.792, Speaker A: How is that happening? Is the committee of the shared sequencer having an increased amount of state and actually making some claim over the validity of this off chain data? There's various things like that, but I think really the way to think of it is like it's a stream of data. Generally, we assume the roll up is only kind of filtering out the stream of data directly relevant to it. There is potentially data you could push into that stream that gives a shared timestamp of it across all the rollups that they may choose to filter out as well. So, again, you could have some JSON format for Oracle data and all. The roll ups of the shared sequencer can choose whether or not they want to parse out that Oracle data and use that in their state machine. There's a pretty broad design space there, but it's unclear what will be useful. And I think the research on what are the kind of trade offs and security guarantees and whatnot of these things, but I don't think we're solving the Oracle problem here.
05:26:29.792 - 05:27:17.648, Speaker A: But there's potential UX benefits as like a role up developer. Yeah, that's really good. One of the things that I was to lead into the next topic is mev and PBS specifically. So some of the things that I was trying to hint at is with these committee based services is that whenever you use a shared sequencer, and if that shared sequencer is using some sort of mev infrastructure, that mev infrastructure is sort of already set in place for you. So to get into the mev part, monopolies are bad in general. That's usually a safe thing to say amongst crypto people and most folks in general, but we don't always get rid of all of the monopolies. And I think especially from Ben's talk, you had some really, really good answers to this.
05:27:17.648 - 05:28:07.484, Speaker A: So I really want to dig into monopolies and basically profitable censorship. So we still rely. So crypto, like, we like to think about getting rid of monopolies, yet each proposer has a monopoly over which block is used for that given slot. So I kind of want to dig into who captures mev in the system and how can we change that to be more programmatic and no matter which direction we want to go in, how can we change that? So, yeah, we can just, again, go in order. Yeah, I mean, this conversation is relevant even just talking about layer ones, like without roll ups. But the same problem happens with shared sequencing layers for roll ups too. So it's interesting that you said, wait a minute.
05:28:07.484 - 05:28:32.836, Speaker A: The proposer in a consensus protocol has a monopoly on a slot. It gets to decide what goes into the slot. So this is, I think, one of the really interesting things about consensus protocols. There's a number of papers on this. Like there's a paper called Monopoly without a monopolist. A blockchain is a monopolist. A proposer has a monopoly over one single slot.
05:28:32.836 - 05:29:26.788, Speaker A: Right. Does this slot based monopoly behavior achieve monopoly pricing? Meaning, do users pay the price that maximizes the revenue of the blockchain? Or do they pay basically the market clearing price where supply equals demand? And so ignoring mev of ordering and sandwich attacks and all that, if we just look at what's the price to include a transaction, EIP. 1559 has solved this problem. Users pay even though the proposer in the consensus protocol has a monopoly, users pay the market clearing price. And the reason is that if the proposer is not willing to process the transaction of a user paying the market clearing price, the next proposer will. So having a monopoly for a short period of time does not give you a monopoly over prices. And this is the key property of blockchains.
05:29:26.788 - 05:30:11.636, Speaker A: That's why they have to be decentralized, right? Because I'm elected once in a Blue Moon to process a block. If I don't take the transactions as they are paying the market clearing price, there's someone waiting in line who will just do it. So I cannot make a credible threat to users to reject their transactions paying below market clearing price. Now, when it comes to mev, this isn't the case because the transactions are available to me, I can sandwich attack them. I could even get the help of a bunch of builders and searchers to figure out how do I extract as much value as possible by reordering the transactions in front of me. The willingness of a proposer in a later slot to do it without extracting value doesn't matter. I can do it now because the transactions are there now.
05:30:11.636 - 05:31:06.036, Speaker A: And that's why we need to move towards order flow auction design where we introduce an ideal functionality that has a privacy element to it, which shields the transactions from the proposer, builds a system which is ideal and stable for users, so that all users submit their transactions there. And then we turn the proposer back into a passive entity that can either take the block output of this ideal functionality or have nothing at all. And again, that proposer does not have the ability to make a credible threat to users to not take their transactions if they send it to the ideal functionality. They just try to take anything that's available to them at the and it's very hard to design those ideal functionalities. It involves a lot of cryptography, perhaps trusted execution environments, but that is the direction the industry is going in. That's what Flashbots and Swab are working on and those only work with highly decentralized oring protocols. That's why we're advocating for shared sequencing.
05:31:06.036 - 05:31:42.720, Speaker A: So to respond to that really quickly, in theory you have multiple different builders, so maybe multiple different Swabs or something like that. And the Proposer, since they do still have this monopoly for this one block is very short and it's very limited. They still can pick the block that gives them the most profit. So this is sort of where I was. But the block that gives them the most profit may be the one that has all the users. And the one that has all the users is the one that is shielding users from mev. So you can either design something that's stable for the proposers or you can design something that's stable for the users.
05:31:42.720 - 05:32:26.576, Speaker A: If all the users are participating only in the order flow auction that gives them the best execution price, the short lived proposer has no choice but to take from that pool. Otherwise it doesn't have any transactions to process, it can't make a credible threat to users to say, no, don't submit there, I won't take it because it has a short turn and then it's gone. Right. I guess I always propose like, the spicy counterpoint right, of dumb flow is dumb, I guess. And what I mean is that generally when we think about retail flow and users getting sandwiched, it's because they're unsophisticated actors and they're not necessarily doing the labor to find what is the optimal execution for them. Right? And then there's also like, we can think about user preferences in financial senses, but also in a product sense. Right.
05:32:26.576 - 05:32:53.180, Speaker A: So I spent years at Google, right? What does Google do? Well, Google provides you a lot of free services. Are those services free? No, Google's monetizing that on the back end with ads. And why are they doing that? Because users don't like friction and transactions are friction. Crypto talks a lot about transaction friction because if it's like two and a half dollars submit transaction, you won't do it. Well, when apps came out, like the App Store and Apple, right, it was like ninety nine cents to buy an app or whatever. That's like no money, right? Like functionally use an app for like ten minutes, you should be willing to pay a dollar for it. Right.
05:32:53.180 - 05:33:50.284, Speaker A: But people didn't, so they made them all free and then they monetize them on the back end. Users trend towards things that are low friction and then they monetize on the back end. We see this in financial systems, right, with like, Robinhood and payment for order flow, right? Users don't think and they don't read the MBA papers that say, like, hey, maybe it's suboptimal for you to actually be submitting transactions that are free and then you get marginally worse execution, though the actual quality of execution on payment for order flow seems to be like an unanswered open question from analytics and availability of data. But fundamentally, users use the thing that is low friction, not necessarily the thing that gives them optimal execution. So I think, again, I'm very positive on Suave and Flashbots, but I think it's going to be a very hard fought battle to say, we are able to convince users that they should only submit transactions to this if that may result in some marginally higher transaction cost for the user. And then someone else comes along and say, I will ruthlessly front run and sandwich you, but you will not pay any upfront transaction fees. The users might say, all right, I'm fine with that.
05:33:50.284 - 05:34:23.140, Speaker A: So I do think there is something from a product perspective that may be trickier to get from user demand rather than the kind of like again, the ideologically correct of like we just need all the user flow to go through one of these kind of like tee environments and therefore we can't do all the sandwiching attacks when you're like. What if the users just don't care? I think we're moving the target. Whatever is good for users is good. Let's define what's good for users. That's the ideal functionality. Maybe it's not paying for failed transactions, maybe it's not paying monopoly prices. Maybe it's both.
05:34:23.140 - 05:34:51.360, Speaker A: But if we design something that's stable for users, then that will be stable for a blockchain too. Only if it's decentralized. And that's the key point. Yeah, and I agree on that. It's just like defining what is actually a user preference across many parameters is like hard. Agreed. If you use a lot of people talk about roll ups inheriting liveness and censorship resistance from their base layers.
05:34:51.360 - 05:35:58.500, Speaker A: We found that if you build a sovereign roll up on a DA layer, which is built such that it inherits censorship resistance and liveness from its base layer, fundamentally you leak a form of mev to the validators of the base layer called profitable censorship. And if you don't want to leak that mev because maybe you don't want to create anything like gas wars on the base layer, then you can't inherit liveness and censorship resistance from the base layer. So if you're using a shared sequencer, and your shared sequencer has its own consensus and its own proof of stake and its own token, the cost to create a liveness failure on the shared sequencer is the cost of one third of the stake. If it's tendermint style, and that could be very high. That could be high enough that roll up developers don't feel as though they need to build a force transaction inclusion mechanism from L One. It might be censorship resistant enough on its own. And so the shared sequencers can help remove that mev from DA layers.
05:35:58.500 - 05:36:50.452, Speaker A: This is really good. Before we finish it up on mev. Do you all envision any way for a roll up to be able to capture its own value just like that? What would be more of like a mechanism design for a roll up programmatically capturing mev as opposed to only relying on something like Suave or whatever's built into the shared sequencer? I think that if a roll up is using a shared sequencer, then the shared sequencer becomes part of the roll ups protocol. And so it does need to be programmed into the shared sequencer. You can always view things as roll ups participating in some kind of order flow auction. So if you do design order flow auctions, then maybe there's a way of doing it without programming it into the shared sequencing protocol. But I think that that's the opportunity that we have right now.
05:36:50.452 - 05:37:59.084, Speaker A: So one of the ways of we didn't really touch, none of us really touched on this in our talks, but with base roll up, if you use the L One, then there isn't really a way of programming into the base layer like, oh, roll up should get back this portion of mev. We designing these decentralized protocols on top of the layer one, have that opportunity now. And I think it's important for adoption, it's important for at least the major roll ups out there to be convinced to use Espresso or ASTRI or roll kit, right? An important thing to do is to say here, here's a credible revenue sharing protocol in place that can allow you to get back the value that you're creating for the system rather than the system taking all the value. And so that is, I think, a very important research problem that I'm trying to get people excited and interested in. We have some thoughts on that. I shared a little bit in my talk. You can have like alternating blocks where in some of them you run independent auctions, some you run joint auctions, independent being one for each roll up, joint being auctioning off wholesale blocks for all the roll ups at once.
05:37:59.084 - 05:39:08.212, Speaker A: And that allows you to estimate and approximate the marginal contributions of each more directly and then use that to inform some reallocation strategy. But this is a wide open problem that welcome everyone to think about and offer better solutions on. We think a lot about how you can do mev redistribution and things like that. And we suspect that the way it might work is if whatever layer is doing your transaction ordering has really really good censorship resistance. Maybe with something like a Threshold encryption scheme or possibly with a multiplicity style scheme that duality proposed you can use Verifiable sequencing rules and maybe something like Protocol Owned Builder to capture that mev and do interesting things with it. There's also, of course, all the various EPBs mev smoothing and mev burn schemes that are coming out of the ethereum world and maybe those would have some way of ensuring that it gets captured and redistributed in some equitable way as well. I guess I'll just throw like a contrary point in here maybe.
05:39:08.212 - 05:40:04.192, Speaker A: So from our view of the idea that deploying a roll up should be as easy as deploying like a smart contract or whatever is a term we use, there is a question to me of who is the roll? Up. And what I mean by that is not the mechanism design of calculating the counterfactual of a single auction versus a combinatorial auction of what is the value that this roll up deserves. But it's a question of who is the entity that represents this roll up and how is the shared sequencer aware of that being like a distinct entity. So it's kind of this question of like and Ben, we've talked about this various research. Where do you pay back the roll up itself? What is the state machine that is actually calculating and receiving these funds and choosing to distribute like to our thing? We think from a UX or developer experience it's important that there's not like a registration phase for a roll up to sign up to the shared sequencer. That's kind of an intentional and why we're like a lazy shared sequencer. So to this degree it's unclear who would be like the valid entity that is being rebated the actual funds.
05:40:04.192 - 05:41:17.736, Speaker A: Even if we had a magical way to calculate the optimal value that should be returned, it's like, well, who are we actually giving this to? And then the other point I think of is where do we want this value to be returned to? Should we be thinking about the roll up as some nebulous entity or application developer? Should we be thinking about the users, right? What if we see some space where I didn't have a tough time in my talk to Cover? But what if users are doing something where they have some kind of front end or singular order flow thing, whether it's like Suave or some other kind of aggregation mechanism that is actually using multiple distinct state machines and then settling to a single shared sequencer? Well, that user may not have some kind of preference for one of these chains versus the other. And so should we be thinking more about rebating the mev to the users? And again, to Ben's point, right, going to order flow auctions of like well, actually we're trying to get the mev fairly back to the user and just right now in the current mev design, even on ETH, right, like 90% of that mev is captured by the proposer. So that's like the entity where you have to get that margin from to get back to the user. But it's unclear if we want like an intermediary hop to give it to the sequencer or the nebulous entity that is the roll up. Really, we just want to be giving it back to the user and give them like best execution price. Nice. Okay, so much good alpha.
05:41:17.736 - 05:42:26.610, Speaker A: Right now let's focus on base roll ups. Based roll ups are a critical part of shared sequencing. You can't really have shared sequencing without some sort of lazy execution. So I'm not really sure on the exact definition of based rollup, but effectively you agree on the transactions beforehand and then you execute them. So this is what I think what Josh was talking about earlier on his talk, where you have sort of like an optimistic relay where you're submitting a block, you're submitting some sort of transactions and then you have some sort of arbitrary execution environment on top. So another thing to do with mev is I'm curious on with this execution environment itself, on reordering the transactions or doing something to do with a based roll up capturing again, this goes back to programmatically capturing its own. Mev is can you have a based roll up that doesn't necessarily respect the ordering of the shared sequencer? And can that be beneficial in any way? Do you see that as like a viable thing or I mean, a very fair answer is no.
05:42:26.610 - 05:43:32.640, Speaker A: No but no and or is it only yes and can we do a no and okay, no anyways, it gets into the definitions of a roll up can define its own state transition function. And that could be on a block level. For example, you could have an ordering of transactions in a block, but then an application which reads all those transactions takes the average and then uses that to determine the state of an AMM. So you can define your state transition function to be not just executing transactions in the order that they appear, but it could be some deterministic function of how they appear in blocks. You have to, of course, break it up into some parts. You can't wait forever until you decide what your transition is, but you could do transitions on a block level as opposed to an individual transaction level. And that does give you some ability to define these reorderings.
05:43:32.640 - 05:43:59.656, Speaker A: But the key thing is you can't do it in a dynamic way, you can't do it in an opinionated way. Anyone looking at what the shared sequencer outputs and knowing the state transition function knows what's going to happen, there's no dynamic thing that the roll up can now do. So that's the nuanced perspective. Yeah. Doubling down. Right. Like the state transition function, like we argue is it has to be deterministic, right? How you order it, you can just say, I read it backwards.
05:43:59.656 - 05:44:12.256, Speaker A: Like why? I don't know. Because you choose to. Right. Like I think a batch auction is probably like the cleanest thing. If you have like an AMM, like a potumbra style batch auction, everyone gets the same price. It doesn't care matter if you're at the front of the block or like the back of the block. Right, but it has to be deterministic.
05:44:12.256 - 05:44:57.724, Speaker A: You're not like doing like a random shuffle because again, then the shared sequencer, again, what is the commitment giving you? If you're like someone's going to randomly shuffle it by some exogenous extra protocol rule, you're like, you won't come to consensus on it, then what are we doing here? Right, so it does have to be deterministic. Again, how you order it, how you refund it, fine. But also from the shared sequencer perspective, it's just like including the transactions, it doesn't really care what you're doing after the fact. That's on you. The name of the concept is verifiable sequencing rules, I believe, and it reduces the problem of fairness to the problem of censorship resistance. If you can get something included, then it will be ordered fairly according to the verifiable sequencing rules. There was a research day talk about it from Matthias.
05:44:57.724 - 05:45:54.976, Speaker A: It's quite good. I'll just argue I just have a general opposition to the concept of fairness being like a thing. There might be some useful economic definition for it, but I think arguing of fairness is where we get spam and whatever, right? Like all of the first come, first serve things of like, oh, is it fair and well, is it fair that the guy has faster hardware and a shorter Ethernet cable is faster than you. Well, maybe not fair, but something that you can capture and redistribute possibly. Yeah, exactly. So what I was trying to go with that is that you definitely need a deterministic function, but could you design a deterministic function that can direct value capture to somewhere where you want some sort of programmatic mev capture? So something like a batch auction, like what you were saying? Yes, but I think that can only be the value capture within the state machine of that specific roll up. That cannot be the value capture of, say, an inclusion fee within the shared sequencer like you on your roll up cannot deterministically reorder something in such that you extract value from the inclusion fee on the shared sequencer.
05:45:54.976 - 05:46:29.004, Speaker A: I mean, it's also important to know though, that if you have a deterministic function and you are the protocol or the builder that does have dynamic control over the ordering of transactions that then go into that state transition function. You are the entity that can figure out which order is going to maximize my own profit. Or which order is going to maximize some profit. So you don't really have control just by defining the state transition function over where the profit goes. And that's why you still need to get back to this order flow auction design that we were talking about. Exactly. Because the proposer has monopoly over not the proposed.
05:46:29.004 - 05:47:08.972, Speaker A: Well, again, it comes back to consensus protocol. The proposer has a monopoly for one slot, right? Which is important because that means that it will take whatever output it's given if that's the only output from some order flow auction that it can choose from. That's why consensus protocols, that's why blockchains are good, right? Blockchains turn proposers into monopolists from one slot. And that's why centralized systems are bad because you have a monopoly over all slots. And it's amazing that when going from monopoly over all slots to one slot allows you to achieve non monopolistic behavior in a system. That's like the coolest economic thing about blockchains. But you still need to solve this builder side.
05:47:08.972 - 05:47:33.732, Speaker A: You need to solve this order flow auction that designs a system which is stable for users. If you really want to mitigate or minimize mev or control where it goes, your verifiable sequencing rules can't they don't know if somebody sold on a centralized exchange after they did a swap. For example. For example. Great. Which is how all the mev is extracted. Nice.
05:47:33.732 - 05:48:00.636, Speaker A: Okay, so bear with me. We're going to bike shed on naming just a little bit. Fine. Okay. I think actually the main criticism of shared sequencers is just the name. So do you think that shared sequencing is a good name? Is it justified? Or would you rather go with something like I've heard shared proposer, I've heard shared aggregator, is it not shared? Seems reasonable, but maybe not even that. I don't know.
05:48:00.636 - 05:48:52.140, Speaker A: What do you guys think? Are we going in order here? I think that when it comes to names, there's no perfect name for anything. Blockchains aren't a great name. I'll say it they made a lot of sense for proof of work consensus. But, you know, other, other consensus protocols may not be blockchains, but but it's the thing that catches on and it's the thing that resonates with people and it makes sense. And I think that shared sequencing, we've all now been using it. And of course, once you start using something, people are going to point out reasons why it may not be the perfect term, but in the end of the day, it's a pretty decent term. And sequencing refers really to finalization of the sequence in which roll ups will execute their transactions.
05:48:52.140 - 05:49:11.670, Speaker A: We could add that to the title. That's a mouthful. And I just don't know if there's another name that's like nice and short and succinct that captures it. But if somebody comes up with one, you're the winner. I think it's a good name. A lot of people like Aggregator. I think that's a good name as well.
05:49:11.670 - 05:49:34.452, Speaker A: I guess I'll say. Yeah. We made a very intentional decision to use shared sequencer. It was like a marketing term. Right. But fundamentally, people that use the same shared sequencer have a shared view of the sequence of transactions. So certainly better than settlement layer where everyone's like, what the hell is a settlement layer? What are you settling? What is settlement? Right? We're like, no, you use the shared sequencer.
05:49:34.452 - 05:50:06.868, Speaker A: You and everyone else using it has a shared view of the sequence of transactions. That feels pretty explicit, right? Yeah. We implicitly assume that it's like, in our terminology, like a lazy, decentralized shared sequencer. But again, you just have to pick words and put it in your marketing copy. Are there any other misconceptions on shared sequencers that you guys would like to address? Now? Shared sequencing is not one roll up. Shared sequence is not shared execution. It's not shared building per se.
05:50:06.868 - 05:50:31.070, Speaker A: It makes some set of shared building easier. I don't know. Shared sequencers are real. Yeah, that's a great segue into. Does anyone know really quick how much time we have left? Zero. Oh, wow. Okay, well, okay, that's great.
05:50:31.070 - 05:50:52.100, Speaker A: No, there's no need. We don't have time. We were just going to handle if roll ups were real or not. So we'll just leave that for next time. All right, thank you everybody. I was super excited for shared sequences before. Thanks, Evan.
05:50:52.100 - 05:51:26.160, Speaker A: Now I'm extra excited for shared sequencers. Great. Thank you, Evan. Thank you all. Stick around because coming up in a moment, we are going to hear from Neil M. All right. Just going to give it a moment of transition.
05:51:26.160 - 05:52:15.100, Speaker A: Yeah. All right. Thank you all. If you want to chat, I'm going to encourage you to make your way towards the door. If you would like to stick around. However, I would encourage you to do that, please. We're going to be hearing from Neil from Eclipse in a moment.
05:52:15.100 - 05:53:07.130, Speaker A: All right, wrap it up, wrap it up. Take your seats, everyone, please. All right, I'm giving everyone a countdown to grab their seats in three, two, one. Okay, thank you so much. All right, thank you all. Coming up next, we're going to be hearing from Neil Somani, the CEO of Eclipse, and he is going to be telling us why roll ups as a service are going to zero. Very controversial, hopefully not all of them.
05:53:07.130 - 05:53:29.656, Speaker A: Over to you, Neil. Thank you. Hi. Okay, this is on. Okay, so for context, Eclipse builds roll ups as a service. So this topic might be confusing for some folks and in this presentation. So I'm Neil, I'm the founder, and what I'm going to explain is one, different economic or technical considerations for app specific roll ups.
05:53:29.656 - 05:54:08.830, Speaker A: Gonna talk about the roll up as a service landscape and yeah, let's get right into it. All right, so there's been a lot of discussions about Ras or roll ups as a service. And what I want to do is I want to first define what is a roll up as a service. And a roll up as a service is something that deploys roll up frameworks and that requires understanding. What is a roll up framework? A roll up framework is something that implements execution and settlement related capabilities for a roll up. So this is something like Op stack or Arbitrum orbit. You deploy one of these frameworks yourself, or you can use a Ras and then you can start having your own chain for your own application.
05:54:08.830 - 05:54:53.972, Speaker A: So this is like market segmentation in a Masari Think piece, which I actually signed off on at the time. So this is partly my own fault for not thinking it through fully. But what I think is more useful for this kind of segmentation is to think about what parts of this stack are cooperative with each other, which parts are competitive, and I think if that's the right way to frame it. So the way that they framed it here is they have SDKs, which I'd probably call frameworks instead, since they're not all SDKs. You have shared sequencers and you have no code deployment. And I'm going to talk about how I'd reframe this a little bit. So what is a sequencer? Sequencer? I mean, we just had a panel on sequencers right now, but it's something that just is ordering.
05:54:53.972 - 05:55:17.036, Speaker A: Ultimately with that ordering, you have to compute the actual state transition. So that's execution. So roll ups of the service as they exist right now, this is maybe a little bit too narrow. I called it isolated sequencers as a service. Usually it's also deploying the execution related capabilities, but that's essentially what roll ups of the service as they exist right now are. They might do some additional support consulting and some stuff which. I added as a footnote.
05:55:17.036 - 05:55:46.748, Speaker A: I'll talk about that a little bit more. But what I want to describe is basically what is a business model. So right now it looks like some amount of fixed fee that might be a fixed fiat charge. There's some variable components. So it's a percentage of the sequencer fee, which you have to actually define that a little bit more clearly because that could be a percentage of the congestion fee or the execution fee for a roll up. That could be just a premium that you add to every single transaction. But this is kind of what I want to address and I think that this is something that every roll up as a service should have a view on.
05:55:46.748 - 05:56:32.120, Speaker A: But the first thing is that the roll up framework to some degree actually competes with a roll up as a service and that's something that I'm going to cover in the next couple of slides, so I'm not going to talk about it too much right now. The other things that roll ups as a service have to think about is that given that they're deploying isolated sequencers as a service, there aren't actually a lot of network effects. There aren't really economies of scale. For every single isolated sequencer. For each additional roll up that you deploy, the service doesn't really get better or it doesn't get cheaper, unlike a shared sequencer, for example, for every additional roll up now, you have additional composition between those roll ups or for certain types of services like DA. The more folks that are using a single DA layer, then there's more fees coming into the system because blocks get full. Maybe it's a fee market, it goes up that incentivizes more validators to run for like Celestia or something.
05:56:32.120 - 05:56:55.276, Speaker A: So that's very synergistic. Whereas for a roll up as a service, that kind of network effect doesn't naturally exist alone. There might not even be a token. So that's something to keep in mind. I think this is the right way that I want to frame that discussion of how a roll up framework could be competitive with a roll up as a service. And the way I want to put it is by thinking about the functions of a modular blockchain. So we're here at Modular Summit.
05:56:55.276 - 05:57:22.280, Speaker A: Everyone should be aware of this stuff, but the functions, as Celestia puts in their docs, we have execution. And I threw a bunch of stuff in there. I put sequencing the actual execution of the transactions and computing the state transition function. I put proving for ZK rollups. The second part, which John Charb would say doesn't even exist, is settlement. So it's figuring out what's the canonical chain or what's the correct state of this roll up. And then there's DA, which hopefully everyone's aware of here.
05:57:22.280 - 05:58:06.756, Speaker A: But let's say you're an optimistic roll up. You're a verifier. You need to know what were those transactions that were run in order to even call for a fall proof, or even on any l one, you need data availability just so that all the full nodes can know what transactions were run, they can keep in sync with the network. So another way of thinking about these functions of a blockchain is these are the possible ways you can try to accrue value or the places where you can try to capture value, whether you're a roll up framework or some other type of part of the blockchain stack. Now I want to go a little bit more deeply into each of these three areas and think about where could a roll up framework feasibly capture value. So settlement to me is not a viable place to capture value because one, we're not even sure that it exists. But two, settlement doesn't make very much money.
05:58:06.756 - 05:58:29.884, Speaker A: So for optimus and mainnet and this is actually all about optimistic roll ups, partly because that's what we have deployed for Eclipse right now. So that's what I think about the most. But you can probably make similar arguments for ZK roll ups. Settlement costs might be a little bit higher, but even if they're 100 times higher, that's still only like $500 a day. Not a great business to be in for every single chain. Whereas optimus and mainnet, it's an optimistic roll up. So it's just basically posting state routes.
05:58:29.884 - 05:58:56.228, Speaker A: And in the happy path that's just $5 a day. So that's basically zero to me. It should be to everyone else as well. So a competitive settlement layer to Ethereum is going to make even less money because they don't have the security budget of Ethereum to justify why they would charge more. There's less congestion, fewer people would use it. So that's not a great place to play, is my point. And then DA, this would basically be the case where a roll up framework deploys their own competitive DA layer.
05:58:56.228 - 05:59:32.880, Speaker A: So this would be like if Arbitrum had a Celestia competitor or something, or if any trust became the primary business model. And my argument here is that's not a very great place for a roll up framework to play either, partly because Celestia already exists and we're all using it. And for DA in particular, there's very good reasons for all the rolloffs to try to use the same DA layer. Because you're all posting to the same place, the fees go up. Like I was saying before, it's synergistic for everyone to use the same DA layer because more people are providing security for that DA layer. Then I'm saying the only place that's viable is actually execution. And as a result, those fees that come in from the sequencer are basically zero sum.
05:59:32.880 - 06:00:16.348, Speaker A: Like you can charge more, but that just makes you have a less competitive sequencer or less competitive roll up framework or Ras. So between the roll up framework and the and I call it ISATS here isolated sequencer as a service, maybe I shouldn't have done that because it's just like additional jargon, but I just put it there for simplicity. And this is a graph on the left side. I want to think through the business model game theory of an Isas compared to a roll up framework. And those numbers might not make sense because Chat GPT generated the graph. But what I want to convey is that I want to start with the isolated sequencer as a service and think about whether it plays nice or doesn't play nice with the roll up framework that it's deploying. And playing nice means that it shares revenue back to the roll up framework.
06:00:16.348 - 06:00:48.456, Speaker A: And not playing nice means that it tries to capture all the value for itself. And in the case where it shares value, my argument is that they're subject to being undercut. Anyone can choose to not play nice and they're going to win from an economic positioning. And economics is probably the biggest reason why people deploy their own app chain. So I don't think that's a sustainable configuration. And if the roll up as a service chooses to not play nice, then the roll up frameworks themselves needs to monetize somehow. So the roll up framework likely will deploy their own Ras, similar to what optimism has done with Conduit in my opinion.
06:00:48.456 - 06:01:30.884, Speaker A: So that's kind of how I would think this through. And what I feel is the stable configuration is for roll up frameworks to have their own roll up as a service because then they have all the flexibility and business model, they're not forced to share some fixed amount of sequencer fees, they're not getting undercut by someone else, they get all the profit for themselves. So that's kind of my thinking. And then this is just considering a little bit further the case where a roll up as a service competes with a roll up framework themselves. So this is like if someone's deploying a roll up framework that they don't own versus a roll up framework deploying their own reps. And then I'm just comparing these two cases where my argument is that the roll up framework has more flexibility. They likely already have a liquid token, they can subsidize fixed costs.
06:01:30.884 - 06:02:11.844, Speaker A: So this is not a fatal thing. It's more just something that every roll up as a service should have an opinion on this and they should establish their positioning, think about what's their business model and how do they expect to sustainably capture value. And the reason why it's really important is because developers care, because developers don't want to build on a stack that they think won't exist in like one or two years and that it's probably going to happen. There's no sustainable business model. So this is how I'd reframe the Masari framing from before. And I basically renamed roll ups as a service to isolated sequencers. And what this really shows is that you can only have an isolated sequencer or a shared sequencer, but you can't really have both.
06:02:11.844 - 06:02:51.916, Speaker A: And then similarly, I renamed Rollups SDKs to roll up frameworks just because I like that better. It's a little bit more general. And then if you're in the Rollup framework category, you should probably find yourself also in isolated sequencers or shared sequencers, similar to how Op Stack or Optimism has the super chain that's likely going to manifest as a shared sequencer. All right, so this is like an economic thing and this slide's a little bit dense, so I won't go through all the specific numbers. But what I wanted to look at is let's say you ran an Op Stack chain. I use Op Stack because it's just an open source framework. So I deployed it and it costs about one ETH to deploy the smart contracts.
06:02:51.916 - 06:03:32.316, Speaker A: I consider like roughly zero, just because over a long enough it's a one time cost. So if you're running a roll up for years and years or something, it gets amortized. So the recurring costs are more interesting to me, and for a lot of optimistic roll ups, they're still posting to the DA layer even if no transactions are run. That's because the sequencer is allowed to do some additional stuff. So that's half an ETH a day of just overhead cost, which gets amortized over the number of transactions, and that brings it down. And then there's some variable cost, and that's the biggest component for all roll ups. And Sanjay at Electric actually wrote like a great piece on the cost breakdown for a given transaction on a roll up, but the DA cost is the biggest one, so I'm not going to make you do the math, but it ends up being about $0.15
06:03:32.316 - 06:04:02.468, Speaker A: on a good day. This is assuming like 25 gray gas price and then assuming $2,000 per ETH. But that's basically like the cost that a roll up transaction can never get lower than that if it's deployed to ethereum itself. So this is considering some of the other things that could potentially impact this. Like after EIP 48, 44, maybe the cost goes down by like ten times or something. That's all speculation because we'll have to wait until it's main net. Similarly, we don't really know the Celestia main net cost because once that's up we'll have to see similarly with Eigenda.
06:04:02.468 - 06:04:46.340, Speaker A: But my point is that just look at that variable cost and you should think about what applications make sense or don't make sense. Given that, and I think an obvious one, for example, a game would never make sense, like a fully on chain game couldn't make sense on the ethereum l one as a roll up because fifteen cents per transaction is likely prohibitive unless somehow the players are making money off of every transaction. But if it's fully on chain, every transaction is likely not profitable for them. So that just seems unlikely to me, whereas a DAC could make a lot more sense. So having a variety of DA options is really critical to enabling things like consumer apps. Another way of thinking about this is what does make sense. That's probably like DFI people don't really care about paying ten bips or something like that on a big DFI trade.
06:04:46.340 - 06:05:23.830, Speaker A: So this is probably reasonable given the price propensity or the marginal propensity to spend for DFI users on Ethereum. A lot of them would make sense to be on their own roll up given that they're used to paying such high fees. It's a strictly better solution for those DeFi apps. And then I just want to think about most applications. Do they have that transaction volume to justify to feasibly amortize the fixed cost associated with running a roll up? And the answer for most apps is probably no. NFT projects in particular probably doesn't make sense to be on their own chain. Small DeFi apps probably don't have enough volume.
06:05:23.830 - 06:05:52.522, Speaker A: I'm not saying this to discourage app specific roll ups. I think people can experiment. But we've seen a lot of app specific roll ups on testnet, which would never be viable on mainnet. So I just want to point that out. And then this is like one last consideration that I want to talk about. And yeah, there are some technical reasons. Like in order to do shared sequencing, it assumes that there's some builder the builder needs to generically support many I'm maybe assuming some knowledge here, but we just had the shared sequencer panel.
06:05:52.522 - 06:06:21.058, Speaker A: But builders need to be able to execute for base. They have to run a full node for all of these different chains. So that implies some sort of interface for execution. For settlement, you need to have some restrictions on how execution can occur. So those are some reasons. But the main reason to constrain a customizable roll up is really just to better motivate the use case. Because if you look at Cosmos SDK, for example, this might offend some folks in the audience, but I never really felt like there was a plethora or diversity of use cases or implementations for Cosmos SDK.
06:06:21.058 - 06:07:24.826, Speaker A: If you look at Terra, for example, without the stablecoin, it was just a regular Cosmos SDK chain. That's how a lot of the chains look at this point. So what I think makes a lot more sense is sector specific architectures, such as a DeFi specific architecture with extra short block times that might impose an additional constraint, such as the sequencer must now be centralized in order to support that short block time. Or maybe for games, for example, like we were talking about earlier, it might need centralized DA or a DAC or something like that. So this is an example of a sector specific architecture that we put together for Worlds, which is some polychain backed game fully on chain Avenue worlds previously built unreal Engine Five. So he comes from a traditional gaming background and his view is actually contrary to mine, but his view is that the purpose of the blockchain for the game is to act as a read only interface. Well, for the users it's not read only, but as a game developer, they just write to that blockchain and they make it a public API for anyone to compose on top of, similar to the Steam Items API, or to I mean, there's like a bunch of these that people can compose with.
06:07:24.826 - 06:07:49.086, Speaker A: But the advantage of being on a blockchain is that it's standardized. So that's how he thinks about it. And as a result, he had these additional constraints, which is, one, he doesn't want the users to know that they're using a blockchain. And that's particularly important because he can use Steam if he abstracts the user or if he abstracts a blockchain from the user. And Steam is the biggest distribution channel for games. So that's one thing. He doesn't want to build financialization himself such as these play to earn games.
06:07:49.086 - 06:08:04.822, Speaker A: He's not like a play to earn guy. And then he also wants to put a bunch of data on chain. And like we talked about before, that can constrain the DA options for him. So as a result, we built this chain for him. We deployed one of the standard Eclipse chains. We put an EVM on it. I don't know if everyone here is familiar with Neon.
06:08:04.822 - 06:08:32.102, Speaker A: They actually recently went mainnet, but it's like a Sputnik VM. Sputnik VM is a rust implementation of the EVM deployed as a smart contract to the SVM. So this is how they achieved this parallelized EVM higher throughput than any other EVM chain. The settlement layer. The point that's here is basically that a ZK roll up. It'll never be strictly better than an optimistic roll up. And the reason for that is that there's that additional cost for generating the ZK proof.
06:08:32.102 - 06:09:02.140, Speaker A: Whereas for an optimistic roll, if you're just posting state routes, it's always cheaper to do that. So they kept it as an optimistic roll up and then the DA layer will hopefully Celeste is cheap enough for this to be viable and then otherwise we'll switch them to a DAC. So that's what the world's chain looks like right now, and they've been using it pretty heavily. And now I want to talk about what our roll up builder looks like. And this is our self service deployment framework, or website for this. This is a soft launch. We haven't posted this anywhere, so you're getting the first glimpse at it.
06:09:02.140 - 06:09:33.758, Speaker A: This is what it looks like. It was actually a video before, but I removed that because I didn't want to risk the video not playing or something. But this is the front page. Can specify the network name, standard configuration options like the VM and things like that. Specify block times or things that are a little bit more technical. In the future, you might expect this to look like templates or just specifications based on the type of application that you're deploying, whether it's DeFi or something else, it just tells you what the roll up is going to look like. And this is what the chain looks like.
06:09:33.758 - 06:09:56.506, Speaker A: We are giving a one week free trial. So you can heavily slam the chain. You can benchmark it, you can try it out yourself, and then after a week, you can actually pay. And it's going to be NFT gated. So if you scan this QR code and I won't waste everyone's time and stay here too long so you can scan this, you can get an NFT. You'll be on the waitlist. I'll be sure to let everyone off who is here at the modular summit.
06:09:56.506 - 06:10:34.290, Speaker A: So I'll get the list later and you can try out this roll up deployment website. So I'll pause for a second. Yeah, of yeah, go for it. And one last thing. We're doing an accelerator in San Francisco. So this was inspired by modular what was it? Modular Fellows, which is actually kind of how Eclipse came about. It was Nick who reached out after Terra D Pegged, and we were just talking about how we could make Solana into a roll up.
06:10:34.290 - 06:11:06.438, Speaker A: So we wanted to do something kind of similar for Eclipse to motivate the use cases for app specific roll ups. We have some ideas on the types of apps that make sense and we'd want to see in our ecosystem. There's also a few RFPs for things that we need built into the Eclipse core product. So we'll have those out as well. We're putting aside, I think, like 203 hundred grand, something like that. We'll be funding different projects and we're actually going to host them in our office in SF. So it's an San Francisco based program, but for exceptional projects that are outside of SF, we can actually fly you in.
06:11:06.438 - 06:11:29.860, Speaker A: And this is kind of funny, but we have rooms in our office so we can house you and you'll be living with our team and you can talk with our engineers every day. That's kind of the idea. So it's smaller than modular fellow. This is only five. It might even be as small as three, depending on how much we think we can reasonably support. All right. And that's it.
06:11:29.860 - 06:12:15.324, Speaker A: I think the rest of this is just like appendix stuff, so yeah, awesome. Thank you so much, Neil. I don't know, it doesn't sound like they're all going to zero. I think there's hope. Yeah. So up next we'll hear from Matt Katz, who's the CEO of Caldera, who will be talking about the recent Slash Celestia roll up combo that they've dropped. Cool.
06:12:15.324 - 06:13:10.264, Speaker A: That's an interesting presentation as the founder of a rollups as a service company to go afterwards. Also, if anyone saw that slide, I am not the co inventor of ZkSync, just to keep that clear. Yeah, I want to talk about Caldera, our involvement with Celestia, how we're unleashing rollups with DA networks and also just want to talk about some of the motivation behind application specific rollups, why you might want to build them, why we are personally still bullish on Ras, and why we think it might make sense for some applications. So I'm just going to take you through a journey of some of our thinking around this and then talk through our involvement with Celestia and our recent developments there. So, yeah, first of all, Matt, founder of Caldera, Caldera XYZ, come check us out. We are the easiest way to launch a dedicated rollup for your application, project or ecosystem. We're also the first, as far as I know, rollups as a service company to go multi stack.
06:13:10.264 - 06:13:31.008, Speaker A: And so we really emphasize developer choice. We support optimism. Op stack chains. We also support Arbitram Orbit chains, with many more coming in the pipeline soon. We also focus on being an infrastructure layer for modular chains, so we know how hard it is for people to launch viable blockchains. And that doesn't stop at just the sequencer. So we aim to provide all sorts of different infrastructure.
06:13:31.008 - 06:14:00.376, Speaker A: And we're also a white glove service. We are a ras. Very nebulous definition, but we are one of a few companies that is defined that way. Oh, it looks like our emojis didn't render that's. All right, imagine beautiful emojis there, chosen by Chat GPT as well. We do reliable sequencer hosting. That's a huge component of it, of course, having reliable RPCs, making sure that the sequencer doesn't go down, making sure that archival data is available to users, that's a major unlock.
06:14:00.376 - 06:14:48.540, Speaker A: And that's one of the more annoying things about for folks who are trying to launch their own roll up. We also do all sorts of additional infra, as I mentioned, like Block Explorers, RPCs, interfaces for Bridging. We've actually found that is in some sense, a lot of the value add that we provide. There are some teams that are able to get started with the roll up sequencer, but they still lean on us on all that additional infrastructure and value added services that we provide as a cloud provider and as well, dedicated support. We found we are probably the top of funnel for most teams that are thinking about application specific chains or ecosystem specific chains. And so we know best what people want, what people need, and the problems that people are thinking through as they're making these decisions. And so oftentimes when we're working with teams, we're working with them as a thought partner as they think through these major decisions.
06:14:48.540 - 06:15:31.096, Speaker A: So, yeah, I first wanted to motivate just why build with an app roll up? And I wanted to start from the very beginning. In the beginning, there is ethereum, and ethereum was good, but ethereum is also slow. Whenever I talk about the speed of ethereum, I always include this photo of the Altair 8800. I personally am too young to have ever actually used one, but this is a personal computer from the mid 1970s. And the thing about this computer, it's probably the closest in terms of instructions per second, it's the closest comparable to Ethereum main net. So both this Altera 8800 and Ethereum mainnet can process about 0.2 million instructions per second in total across their virtual machines.
06:15:31.096 - 06:16:11.576, Speaker A: Now, unlike the Altera 8800, which is a personal computer, basically single threaded, only running a single application at any time, ethereum is probably the worst cloud multitenant infrastructure ever, where you have hundreds of thousands of applications that are all competing for scarce computational resources on top of them. And so that leads to high fees. This graph was a little bit cherry picked. You might notice it stops in 2021. And another thing is Ethereum is Ossified, or Ethereum is in the process of Ossifying. Ethereum as this global base layer computer for the world's new financial system. Ethereum cannot move fast and break things.
06:16:11.576 - 06:16:57.384, Speaker A: Ethereum needs to be ultra conservative. And for anyone who has done anything in the EIP process, you know this to be the case. It's basically impossible to make changes to Ethereum, and especially breaking changes are impossible. And this is generally a very, very good thing for Ethereum. But if you're building certain types of applications, you might want to make changes and that might motivate you to build your own chain as well. This is true of most L Two S that are being built on Ethereum right now. So if you've talked with anyone from these general purpose L Two chains, whether it's optimism, Arbitrum or any of the ZK teams, the Holy Grail is Ethereum equivalents, basically saying we want our roll ups to behave from a VM perspective in the same way Ethereum does.
06:16:57.384 - 06:17:38.144, Speaker A: And that makes sense for them as well because they're motivated to produce chains that are effectively successors to Ethereum's general purpose mission. But that might not be what you want if you're building an application with specific requirements. And so even if general purpose L Two S are solving the first two problems of speed and cost, they're still relatively ossified. They're going to wait for Ethereum to make changes to the VM before they make major breaking changes as well. And so that leads to why you might want to launch a dedicated chain. Again, emoji, not rendering, but you have dedicated TPS. You have a dedicated lane to scale and that TPS is all yours.
06:17:38.144 - 06:18:18.816, Speaker A: Again, going back to that multitenancy point, you're not having resource contention between a bunch of applications all on one chain. You have either just the applications of your ecosystem or your own application on that chain. You also have ultra low fees. You can explore the trade off space where you settle, where you post, DA, et cetera, to get the best fees for your application. I mean, in the last presentation, there's a lot of calculation around fees. That calculation is legitimate, but all of those variables are configurable. And so when you're building a dedicated chain, you get to choose what the inputs are to that function and then find the outputs that make sense for you.
06:18:18.816 - 06:18:43.080, Speaker A: Lastly, there's latency. I think latency is a really underappreciated aspect when you're building web three applications. I spent some time in the gaming industry. When you're in the gaming industry, if your application has like over 100 milliseconds of latency, people go into your DMs and your email, like telling you to kill yourself. Blockchain, that's not the case. You have relatively high block times. Users are used to that.
06:18:43.080 - 06:19:06.284, Speaker A: But if you're building on chain social or you're building on chain gaming, you might actually really want ultra low latency and being able to choose where you sit on that sequencer. Decentralization spectrum might be really helpful for that. Also, there's customization, the first order. There's precompiles. A lot of these precompiles might be very useful. They've also languished for quite some time. US specifically.
06:19:06.284 - 06:19:29.552, Speaker A: We worked with a team who was waiting for years on the BLS curve operations pre compile to get merged. You can find their CTO has comments on that EIP back from like four or five years ago. They're waiting on it. They needed it. It was what would have been most helpful for them to develop their application. It's not getting merged into Ethereum anytime soon. It's pretty stale.
06:19:29.552 - 06:20:03.216, Speaker A: We're able to merge it into their roll up, get that set up, and then they were able to go live like a couple of weeks after. And so there are a bunch of other really interesting ones too, like the Poseidon hash function, ZK friendly hash function. But it's not just EIPS and it's not just custom precompiles. You have basically full control over the state machine. So eventually we're going to be able to support different VMs, like the Solana VM, the Move VM, whatever crazy VMs or custom state transition functions people might want to do. There's also value capture. Imagine a token emoji over there.
06:20:03.216 - 06:20:30.990, Speaker A: You can use a custom token. You can generate sequencer fees. You can also run some lightweight mev. And so you can choose how to monetize. And especially right now, where a lot of projects are thinking about how to make money, this is a super important aspect. A lot of teams are thinking more and more about this. How can they take fees from their roll up? How do they distribute that back to their community? How do they use that to fund development? And there are going to be a lot of really interesting models on this going forward.
06:20:30.990 - 06:21:30.444, Speaker A: Lastly, wanted to defend settled roll ups a little bit. We think that the rollup space and the modular space in general is going to be a homecoming moment for a lot of these application chains to come back into the fold with Ethereum via roll up stacks. That's not to say that we're anti sovereign roll up we think makes sense for certain applications, makes. Less sense for others. But we're really into this idea of a hub and spoke L two S and L three S all eventually settled by and settling to and secured by Ethereum and also having native composability with Ethereum assets and access to liquidity on Ethereum l Ones, ethereum l One and Other, ethereum l two s, and eventually l Three S. So there's this wonderful quote from Vitalik Pression, as always back from 2021, and we're really, really aligned with this thesis. We get a multi roll up future for Ethereum which is effectively the Cosmos multi chain vision but it's on top of a shared base layer providing data availability and shared security.
06:21:30.444 - 06:22:00.244, Speaker A: That's what he said. What you might notice in that data availability point is that we haven't fully gotten there yet. Ethereum, when we're using Ethereum for DA we're just posting data to Ethereum call data that is not super efficient. It's quite costly and it's leading to some big limitations when it comes to roll ups. So yeah, that's the major blocker. When you're running a roll up on Ethereum you have a bunch of costs and then you have DA as a major bottleneck. You can split up costs into three buckets.
06:22:00.244 - 06:22:36.436, Speaker A: The off chain infrastructure costs. This is like your cloud servers, your hosting, networking, et cetera that is relatively fixed and also something that can't really change that much you have on chain fixed costs. There's a little bit of cost and settlement. There's also fixed costs when it comes to DA. So a lot of these roll up stacks they're posting a lot of data to Ethereum just like empty blocks kind of like metadata for the chain even if there are no transactions being processed. And that was talked about in the last presentation as well. And it is a major blocker for people who are launching app chains where they might be trying to bootstrap a little bit of activity but that activity hasn't come yet.
06:22:36.436 - 06:23:05.176, Speaker A: They don't have guarantees that there will be people around to pay for fees and cover costs. And so they are wondering how do they cover those costs? And then there are the variable costs. So posting transaction data that gets sent to the sequencer and then there's a major bottleneck which is data availability. We can quickly go through them. These costs are really killer. They make it hard to get off the ground. As I mentioned, when you're settling on Ethereum you might pay $100,000 per year in these fixed DA like metadata costs.
06:23:05.176 - 06:23:42.376, Speaker A: That is based off of current calculations. So if we have another bull run or gas prices get higher, greater demand for block space that number can go up. They also prevent roll ups from being strictly like cost competitive or better on cost than ALTL ones for simple transactions. Sometimes there might be cost competitiveness depending on demand on all of these chains. But as you can see generally for average transactions a lot of these Alt L ones are still slightly better than existing roll ups. And that's going to harm adoption because users really care. They don't really know the difference between a side chain and an L two.
06:23:42.376 - 06:24:36.090, Speaker A: A lot of the times users and even application developers want to go wherever's cheapest, wherever there's adoption, et cetera. They're not super aligned with Ethereum in general. And lastly, and I think this is an underappreciated point about the current Ethereum rollup space and the status quo is that we do have a DA bottleneck which limits the total throughput across all rollups that are currently built on top of Ethereum. These are some calculations, kind of rough back of the napkin math from Calvin Victor who works at Optimism. He has a blog post called TPS is Dumb. This was a little bit of a sidebar on that blog post but basically, even assuming relatively reasonable assumptions around call data compression, et cetera, current Ethereum DA only supports 360 transactions per second. And that's if we're using all of the Ethereum's network, all of the gas throughput just for posting call data for L two S.
06:24:36.090 - 06:25:15.408, Speaker A: And this is based off of the current network. So pre 4844, pre Dank sharding, but this is global across all roll ups. So even though for the compute we can horizontally scale Ethereum right now via roll ups, eventually we're going to run into this bottleneck. And so if you look at a lot of these limitations you'll find the thing that we keep going back to is DA. A lot of these on chain fixed costs are actually DA which is surprising to some people because DA is often thought as the variable costs. But there's still a ton of metadata that needs to get posted to Ethereum. There are relatively high variable costs you're paying DA when you're sending transactions.
06:25:15.408 - 06:26:14.740, Speaker A: And of course there is this bottleneck when it comes to DA limiting the total throughput of all Ethereum rollups. And so we have our saviors, all of whom I believe are sponsored at this event, Celestia Igen and Avail basically projects that are focused on providing cheap data availability. And at Caldera we were know even since founding we were really excited about these. We saw these projects as being the major unlocks for an Ethereum centric roll up future. And so we were one of the first teams to integrate altda into traditional Ethereum rollup stacks. So with Celestia we launched the first testnet of an stack plus Celestia roll up and effectively, you know, at a very high level. What it does is we're posting data frames to Celestia now rather than Ethereum natively, thus alt DA and we're only posting references to those frames that are post on Celestia to Ethereum.
06:26:14.740 - 06:26:48.212, Speaker A: So that means that we're turning that variable cost which was sending all that data to Ethereum DA into a fixed cost. We're just sending a reference. This is a diagram from Vitalik's blog. Just wanted to put this here to say, we're not really changing much of the roll up protocol at Know. We're still posting state routes to the roll up contract. We're still posting batches. The only thing that changes is I was told not to stray too far away from there, but we'll live is this part.
06:26:48.212 - 06:27:23.072, Speaker A: So these transactions, that data is being sent to Celestia, there's a little bit of changes made in this logic to post references, but effectively, the roll up protocol is still the same. We're just referencing data on Celestia rather than Ethereum, and it's live. So if you guys want to be some of the first to launch something and play around with an Op stack plus Celestia testnet, you can go to ourbubstnet.com or. And we're also really excited about this. We recently launched a testnet with Manta Network, and these guys are super early adopters. Yeah, they're great.
06:27:23.072 - 06:27:52.760, Speaker A: There's Kenny in the back from Manta network. They're, as far as I know, the earliest adopter of Celestia plus Op stack for rollups on Ethereum. So they're going to allow their users to experience way lower transaction costs than any existing roll up on Ethereum. Plus. They've also integrated a lot of ZK friendly application level ZK functions. And so if you're interested in developing on the first network to use Celestia for DA, go check them out, go talk to them. They've got a strong presence at this event, so find them afterwards.
06:27:52.760 - 06:28:16.492, Speaker A: And yeah, thanks. Feel free to contact me at any time if you're thinking about launching a roll up. If you just want to talk about the design space, always happy to chat and, yeah, stay tuned. You're not rid of me yet. We're doing a panel here on the rollups of service space, which I'm told might get spicy, so stay tuned. Thanks. Excellent.
06:28:16.492 - 06:28:48.350, Speaker A: Yeah, the spice is just starting to heat up. That was great, Matt, and thank you also for Miracle of Miracles, running on time with all of that great content. I'm excited to go in and start playing around with the Op stack on Celestia myself. It's my pleasure to introduce Tracy. Do we have Tracy here who's going to be moderating the roll ups as a service panel that's coming up. So, yeah, you're not rid of Matt yet. Neil's going to come back up and I think that we've got a couple others joining us as well.
06:28:48.350 - 06:29:54.912, Speaker A: Yeah, in order. You could sit short when micro. We don't need like I speak loud enough, regardless. Yeah. Okay. Well, everyone, good afternoon. Thank you so much for joining us for our roll ups as a service panel.
06:29:54.912 - 06:30:33.132, Speaker A: I'm Tracy Wang. I'm a deputy managing editor at CoinDesk. Excited to moderate because we have four roll ups as a service projects all on. So, you know, hopefully nobody starts fighting. But why don't we just start with if we can go down the line and have everybody introduce themselves, kind of a little bit of your background, any other previous projects you've worked on that are notable and kind of how you got to where you are in just like a 32nd intro? Yeah, sure. Hello. If you weren't just here for my talk, I'm Matt, I'm the founder of Caldera.
06:30:33.132 - 06:31:20.972, Speaker A: Got into crypto back in 2012, 2013, bounced kind of in and out of the space, did a lot of stuff around startups, some performance engineering for things like games and operating systems, and found my way back into the space working on roll ups as a service, trying to unlock scalability for novel applications on Ethereum. Yeah. Hi everyone. Yaoji here from earlier. So actually I started my research back in 2012 mainly on consensus protocol and also privacy, mainly publishing sort of research papers. But in 2016 and 17, together with some colleagues, we started to do research on scaling, mainly Sharding at that time. So we co founded Zilica and basically pushed the Midnight for the Sharding protocol.
06:31:20.972 - 06:31:54.424, Speaker A: And later on I also working on some MultiChain approach. And then around two years ago, we started earlier to build this decentralized roll up service protocol for applications to quickly spin off a roll up and scale their applications. Hey, I'm Neil. Great to see some familiar faces here. Founded Eclipse and prior to that I was working at Citadel as a Quant researcher in the commodities group. I did like power, gas, electricity, and then I left. I was briefly building an EVM for the Cosmos.
06:31:54.424 - 06:32:38.760, Speaker A: SDK. So people here might know, like Ethermint or Evmos and stuff like that. So what this was instead was you take the regular WebAssembly virtual machine and you try to run EVM bytecode through that, and the advantage of doing it that way is you get better atomic composability between EVM smart contracts and WASM smart contracts. So that was the thinking. My main reason for doing that at the time was actually Terra, because Terra had a lot of momentum. So when Terra deep, I just scrapped that project and went back to the drawing board. Was chatting with the Celestia folks for a while, was chatting with some folks at Xerox Park, and finally ended up building Eclipse because we wanted to address this gap we saw in the market for really highly parallelized high performance roll ups.
06:32:38.760 - 06:33:28.164, Speaker A: Hey everyone, my name is Kotuk. Kind of started building on Ethereum in 2018, but then I caught the AIML hype of that time, but then I realized it was not for me. And back in 2020 I came back to the space and started learning, reading and researching more about scaling in general. I worked with the Ethereum Foundation in the PSE team a little bit, working on the Hubble Roller project and the BLS Wallet project, and also worked with Life Pair for a little bit, moved their protocol from mainnet to Arbitrum. And I think last year I took inspiration from Hubble and spun up stacker and it was basically a framework for building rollups and yeah, that's pretty much about me. All right, thank you. So we are at modular summit.
06:33:28.164 - 06:34:49.004, Speaker A: And so if you guys can briefly talk about kind of how roll ups as a service kind of fits into the modular vision of the world and also, if you can give a very high level definition of what a roll up as a service is for any of our new joiners that might not know. And I think there is this simile that some people use that like Amazon Web services is to web two as like roll ups as a service is to web three. I don't know if that's directly like a good comparison and so if you could kind of explain what roll ups as a service is and kind of how that fits into the modular vision of the world, why don't we start with UIQ? Great. So for service, right, so I think we are talking about modular a lot today and probably tomorrow. So as we know, right, two years ago, like Mustafa mentioned about this celestial and also modular. At that time we are trying to move from this monolithic blockchain model to this modular model. In that case we try to decouple the monolithic sort of design from this execution layer, settlement layer, DA layer into multiple modules like Celestia handling.
06:34:49.004 - 06:35:48.244, Speaker A: This DA and execution basically belongs to different rollabs. Like we know that optimism, arbitram are all different ZK rollabs and then we also have the settlement layer, typical ethereum. So in that case when we talk about modular for rollab as service, it's mainly for us, we mainly focus on this execution layer, really run the rollabs for the applications. So that's sort of the definition for the modular when we talk about the rollab as service. But as you mentioned, if in layman terms it's really like AWS or cloud services in the crypto world. So people can really just define the parameters names and just click a button and then quickly either use us or some of our projects platform to quickly launch a rollout for their application. It's either sort of EVM compatible or it can also be WASM compatible, even SVM compatible or even JavaScript compatible.
06:35:48.244 - 06:36:09.432, Speaker A: Right. It's really like in general execution layer, but mainly for a lot of these decentralized applications. Sorry, my explanation a little bit long. It's not as short as the number one. Do you have anything to add or take issue with YQ's answer? I like that analogy. AWS for crypto. That's very flattering.
06:36:09.432 - 06:36:33.156, Speaker A: I'm going to use that. Yeah, I feel like what's interesting well, I won't get too much, I feel like I'll just say but what's interesting about all there is that it's reducing the fixed cost substantially by making it a short term roll up. That's kind of the idea, the Ephemeral roll up. But yeah, roll up to the service. It's like managed infrastructure. This way you don't have to actually do the deployment yourself. And more than that, you don't have to maintain its reliability some other team and you're outsourcing that to them.
06:36:33.156 - 06:37:13.424, Speaker A: Though I think where it can go wrong is when the roll up as a service is spending too much time on consulting or things like that because that's not scalable revenue in SaaS terms. And I mean Ras, the whole name kind of tries to piggyback off the connotations of SaaS. Yeah, of course. So also I think it is nice to make a distinction between roll up as a service and roll up frameworks for now. So as Neil mentioned, it's basically managed infrastructure for Rasp projects. So for us it's mostly around the framework model. So it's interesting that still it can be thought about as AWS because AWS has other services also that you can use to build the cloud project as well.
06:37:13.424 - 06:37:42.808, Speaker A: So for us we think of it more in terms of building tools for creating those state machines and creating those roll ups instead of having a chain which you can deploy applications on. But yeah, the AWS example still works. Well, I guess the reason why that AWS example doesn't quite work is because we don't own the hardware ourselves. At least I don't think any roll up as a service at this point owns it. And that's why AWS works so well because they get economies of scale. When more people use AWS, their data centers grow or whatever. It's like the whole Amazon flywheel.
06:37:42.808 - 06:38:07.872, Speaker A: And maybe that is the direction that roll ups as a service can move in. They start to vertically integrate even further. Yeah, to build off of that for a technical audience. The example I like to give are platform as a services. It's like a heroku or render railway. There are a bunch of these that are built effectively on top of AWS or GCP, but then offer services on top. That's effectively what I think all of us are doing.
06:38:07.872 - 06:38:54.572, Speaker A: I suspect none of us are running our own hardware. We're all on top of a public cloud. I think that the definition was good so I don't have too much to add. The one thing I'll also say is I see roll ups as service, as the distribution layer for the rest of the modular stack. So if you have the roll up frameworks themselves, stacker being one of them, you have DA shared sequencer networks, et know, but who are like the people and the projects that applications or people who want to build chains are going to come to, they're probably going to come to us. And so we get to act as kind of like the top of funnel for all the amazing projects lower in the stack and help them reach adoption as well. One thing to keep in mind is like Heroku never became a huge business though, because as a result when a company got big enough they would just move off of Heroku.
06:38:54.572 - 06:40:02.504, Speaker A: So that's something that distinguishes roll ups to the service, which is that you want to build in something that makes it a little stickier or builds in a network effect into the whole deployment. Let's back it up a little bit and kind of talk about do you think that kind of roll ups as a service only makes sense if we exist in a world where there are a lot of roll ups? This is a two part question. One, do you think we need roll ups or whether you think that there will be a future with a bunch of roll ups and why do we need them as a service? Yeah, I'm happy to go. So I think honestly there's less of a distinction between whether we need roll ups versus whether we need block space. Really. Roll ups are on some level an implementation detail. And the reason why we've seen a lot of proliferation of roll ups as a service versus side chains as a service, which could have been done back in 2017, 2018, and kind of was done with enterprise blockchain but didn't really go anywhere, is that with roll ups it's much easier to provide them as a service.
06:40:02.504 - 06:40:51.056, Speaker A: The model makes more sense and it's easier to be more aligned with a chain like Ethereum. And so that's kind of one of the reasons why we've seen proliferation of roll ups as a service. And the motivation behind that is just more block space and varying types of block space on blockchain networks. So all of us has probably spent some time talking about fully on chain games, on chain social, high throughput applications, high latency applications, all sorts of blockchain applications that are continuing to further blockchain as this platform for general compute. If you see those trends moving, you need some way to scale and roll ups as service I think are just the easiest way to get there. And then the question is why rollups as service versus just like rollups in general or hosting it yourself? I think it is similar to the cloud services model where you could run things on bare metal. There was a time when people did that.
06:40:51.056 - 06:41:39.636, Speaker A: I remember an anecdote from the early Facebook years, every time they added a new university, they physically went into a server farm and racked a new server and added that to their platform. So you could do that in a roll up world without needing roll ups service. It's just adding another layer of abstraction on top. Also to add on top of that, you mentioned do we need so many roll ups and stuff. So I personally see roll ups as more on the application side of things than chain side of things. So I think the whole theme for today has been, at least in this room has been like roll ups should be as easy to deploy as smart contracts. Right now we are building smart contracts, applications as smart contracts, but in the future, if we have better tooling and better infrastructure on that, we can just start building applications as roll up.
06:41:39.636 - 06:42:12.364, Speaker A: So instead of applications being deployed on a roll up, application becomes roll up in itself. So I think one of the most prominent examples is dYdX. So dYdX is like just doing before that, before it moved to its own chain, it was basically a stackx app. And that's something which I see happening more once you build better infrastructure to deploy apps like that. Yeah. All right. So that point on the block space being the reasoning, I guess what's interesting to me is there's a lot of commodity block space, L ones out there, and they're far from saturated.
06:42:12.364 - 06:42:52.680, Speaker A: So I actually think that the biggest reason why people deploy their own chain is economics. And that also explains why we never saw side chains as a service, because the fixed cost for a side chain, for it to actually be secure, is much, much higher. For a proof of stake chain, you actually need some amount of stake, probably millions of dollars to secure that chain. And on top of it, you need like 100 validators, each of which is burning some amount of money every month. So you're burning millions and millions of dollars, whereas for a roll up, the fixed cost is very low because you only need a small number of sequencers. So what you're inherently doing is reducing the fixed cost. And the trade off is that you've increased the marginal cost, because now you have to post every transaction to an L one you have to pay for DA, but the upside is that when there is execution, congestion and things like that, then you actually share in that upside.
06:42:52.680 - 06:43:22.624, Speaker A: So you're going from being a rent seeker to being more of a homeowner. So that's my thinking. And yeah, as far as why roll ups as a service, I'd agree. It's just too cumbersome. It's not something that I think application developers should be thinking about as far as running their own stack. Reminds me of when people used to have their own in house data centers at Citadel, we used to have that, actually, and it was actually much cheaper than using AWS if you don't account for the engineering time that went into maintaining that whole system. So eventually we pushed everyone to AWS.
06:43:22.624 - 06:44:25.368, Speaker A: That confused a lot of the portfolio managers because they're like, what? The in house thing is so much cheaper. That's because Citadel wasn't really doing the accounting properly, they should have been properly attributing the engineers that worked for the reliability team and trying to average out that cost as well. Actually, I can provide some extra real world examples because we talk about this block space in theory, right? So we know that ULAB can provide much cheaper block space. And also at the same time, if we can provide fraud proof of validity proof, we can still derive the security guarantee from the L ones like Ethereum. But beyond that we need some real world example, right? One of the example I can give is that we are working with one of the biggest gaming partners right now. So the thing like they got a lot of IPS from Saiga Ubisoft and some of them so the thing like they couldn't really do this manual consulting one by one for their partners. So what they have right now like collaborate with us and later use us to launch for the games.
06:44:25.368 - 06:45:34.896, Speaker A: They can make it scalable. Like for example, instead of assign like ten people or 20 people team like consulting company and to talk to different sort of IPS right now they can just use us to quickly just click a bunch of buttons and quickly launch that multi sequencer with fraud proof rollout for them to serve their customers. In that case they can quickly just scale the business, not just talk to them one by one, but really once they got the deal and then they can quickly smear off the rollout for them. That's something I believe probably if all of us, especially in the room, right? We believe in the Web Three future we must probably eat the pie of internet industry, right? So in that case, definitely we have to envision in the future more and more the mobile applications will use blockchain in the future, right, as a back end. So in that case we definitely need a much more scalable and high performance infrastructure. So what we can provide at this panel for the developers to use yeah, got it. Thank you.
06:45:34.896 - 06:46:31.520, Speaker A: Could we go down the line and kind of talk about now? We're all convinced that roll us as a service is the future. Why is your project the one that developers should work with? Yeah, great question. I think this is going to lead to some interesting answers for us. We're really focused on providing the most mature stacks that exist so op stack, optimism and Arbitrum orbit and also we have really high focus on customizability anything from changing the chain parameters easily, using Alt DA, choosing which stack to use all the way down to custom stateful pre compiles. We've been working on interfaces to allow developers to do that more all the way to potentially additional VM level modifications as well. We have a big focus on middleware. I think one of the things we've realized is just like what it takes to produce a viable chain doesn't stop at the sequencer.
06:46:31.520 - 06:47:39.204, Speaker A: We're really focused on providing additional infrastructure looping in folks we work with with other infrastructure partners, creating a broader ecosystem of modular services and modular infrastructure and our focus is just on devex ease of use and getting people to the market with a viable roll up in the shortest amount of time. So on our side we are a little bit more crypto native. Most of the team members join the crypto space since 2016 building these public blockchains. So for us we are mainly focused on this decentralization security. So the stuff you can use all layer via our dashboard you just click button within two minutes you can launch your roll up but what you can have is decentralized sequencer, you can choose the number of sequencers you have and also multivm support. Right now it's EVM and wasam compatible and beyond that we also have our own proof system. So you can choose a settlement layer like ethereum or the EVM chains and later if there are anything wrong, you can have Challenger Verifier to do the challenge for the proof.
06:47:39.204 - 06:48:30.004, Speaker A: So these sort of most of the advantages we have is really to highlight the decentralization and security. So that's why we spent almost two years on the implementation for this multivm and proof system. I guess my view is that all the roll up frameworks to some degree are in their infancy and that especially optimistic roll ups, none of the fall proofs are permissionless or enabled. So to me what distinguishes different roll up frameworks or roll ups of service is what they chose to focus on and consequently what they could not focus on because that implies a trade off in terms of their positioning. So for us we focused on one was high throughput and high performance. And the trade off there is that the nodes are heavier, so they're beefier nodes, but they're very powerful. So you could run just an unheard of number of transactions.
06:48:30.004 - 06:49:00.520, Speaker A: And we have games that are posting literally hundreds of transactions per second on chain. And it would only be possible with a parallelized EVM. So that's one reason. And then the other thing is avoiding vendor lock in and that's something that also implies a trade off to some degree because it means that there's no strict allegiance to any ecosystem. You can actually pick which ecosystem you want your roll up to be aligned with. But as a result, eclipse itself is not really aligned with a specific ecosystem. It's more like people when they deploy an eclipse chain, they choose the positioning for it and they have to make that positioning known.
06:49:00.520 - 06:49:43.296, Speaker A: So that's kind of how I think about it, trying to think if there's anything else that really distinguishes because I feel like all the integrations, things like that everyone has to provide a block explorer, you have to provide bridges, these are just necessary conditions to having a viable roll up. So for us, I think we stand a little different from all the other projects right now over here. So our focus is mostly on developer experience. So we want to build systems so that you have a very web two style of building web three native applications and that's what we are going towards. So it's not a chain that we are providing, it's a toolkit that we are providing. It has a bunch of library functions. So you can think of it as like how you develop, React front ends or something like that.
06:49:43.296 - 06:50:22.350, Speaker A: You have like a whole component system. You pick and place components and you just place them so that it just builds your own state Machine. So that's the thing that we are focused on making people Easy, give them Access to build their own custom state Machines. And those custom state Machines, we are calling it as Micro Roll Ups because they Are like literally what you want them to do. So if you'd want to do like you want to build a token transfers application, you just pick and place the components required to do that. If you're building a gaming Application, you just take in like an ECS system and you just build a state machine out of that. So that's the mode that we are trying to build in and Provide.
06:50:22.350 - 06:51:23.904, Speaker A: So, moving over to economics, how do you envision rolex as a service kind of monetizing? I Guess We'll Start with neil well, I don't think that there's anything realistic beyond Fixed plus Variable. The question is just like what is the variable fee charged on? Is It on the size of the Transaction or Is it based on volume or something like that? And Then The question is how much is that fixed amount? And it should be some function of the cost of running nodes. It should be some function of just like the market, it depends on where the surplus accrues, whether it's to producers, consumers, but some variation of Fixed plus Variable. So actually, I really like the data, Neil. And also Matt just presented in the talk, as we know. Right? I also give a Talk at ECC on the first day. So the thing like we have to have the breakdown of all the roll Ups either for the general purpose roll up or the application specific roll up.
06:51:23.904 - 06:52:07.632, Speaker A: So in general, it's like they all discussed, right? So, three parts. One, the execution cost, so you can charge these part of fees from the users to cover your L. Two operator costs, and probably also sometime when you post the data either to DA or to the L one. So this kind of average cost, beyond that, they are also regularly post Data to DA, right? So for games, probably they always prefer cheaper price. So then it's Celestia eigen DA and AVO probably are the better solution. But in the End, I Believe probably for all of us, we are trying to make it as mobular as possible for developers and Project. They can choose what kind of solutions or pricing plan they want.
06:52:07.632 - 06:53:03.424, Speaker A: But for either it's sort of the normal pricing plan, different tiers, or we make It like pay as you Go. But in the End, It's sort of like after the cost from running the infrastructure plus the service fees we can charge from these developers or projects. Beyond that, I think One good thing for all of us. If some of these roll ups are quite successful by running these sequencers or executions, we can sort of have some premium on the L Two fees either from the users or from the project. So how many different pricing plans do you guys have then? No, I just give one example. It's sort of like the multiple ways, right? You can try pricing plan or this pay as you go, it's different models, but in general we need to cover the cost for the L two infrastructure. Yeah, there's a lower bound on the wait.
06:53:03.424 - 06:53:49.036, Speaker A: So if you're talking about hypothetical pricing plans, I'm curious what your project currently is charging. For me, to be honest, it really depends on the sort of customer. So I can give you one example because we are providing different types of products, right? One is so called flash layer. So for some Ft, Mint and also some games, they spin off the flash layer for a few hours and then they tear down. We roll up all the Ft back to the L one, like ethereum polygon. In that case it's like a one time charge for this flash layer and another one is relatively like a persistent one. In that case it sort of depends on the number of sequencers you choose and also the DA.
06:53:49.036 - 06:54:53.140, Speaker A: So on our side we can charge sort of the infrastructure cost for the number of nodes you run by the same time. If it's really successful, there's a premium sort of from the fees we can get something like that, but still right, case by case, some game providers, we work with them, they charge nothing from the users, they really charge nothing. So it's a zero gas we provide and basically in that case we can only charge the fees from the game providers. It's really case package at the moment. Yeah, so gasless transactions are interesting because how do you prevent spam with that specific architecture for these games, right? They have typical they have the wireless, have a wireless for users, something like that. So that's why I say it's really case by case I mean a lot of these web two applications, probably stacker, has similar experience, so their requirements are quite different from the crypto native applications. They don't really know how to install MetaMask, they don't really care.
06:54:53.140 - 06:55:30.188, Speaker A: Like you tell them 15 seconds, 2 seconds, they don't care, they just want 100 millisecond. So what can you do? We have to make it very fast, but at the same time need to fulfill their demands on all this stuff. That's why it's just some examples, but in the long run, definitely we can choose our customers, right, if we're becoming very big. Yeah, well, I guess once you have a self service thing though, then you can't do it. So case by case you have to have some standard pricing. Yeah. If I might jump in here, I think one of the interesting things about the Rasp space in general is any given pricing structure has certain kind of failure modes.
06:55:30.188 - 06:56:17.650, Speaker A: If you're charging a flat fee for hosting maybe a small amount per transaction, you might end up securing billions of TVL, making a few grand a month. On the other end, if you're trying to charge based on TVL or bridge volume, you might be supporting an on chain game with no activity for very little. So I think pricing models yeah, it does sort of need to be a little bit bespoke when you're thinking about different types of applications. And really, at least for us, our North Star is just like, we want to price it so that the user feels like they're getting a fair deal with the value we're providing and that might be different for different types of applications. Yeah, that's an astute point about the failure modes of pricing models. And actually offering different pricing models makes that problem worse because then the amount you make is the minimum of all of those. It's like not a great outcome, actually.
06:56:17.650 - 06:56:41.704, Speaker A: You have to kind of accept the failure mode of your pricing model. Well, it's like something of like an Iterated game, right? Yes. You could find someone could act adversarially here, but presumably you're actually providing value to them. They have some value that they're willing to pay for. Well, I think you can at least assume that people are economically rational. Right. They're going to pick the best deal for their application.
06:56:41.704 - 06:57:11.920, Speaker A: So if you have two pricing structures, people always pick the one for their specific application that's more favorable for that app. Sure. But if the company ultimately has the decision, like the power to make a decision there, right? Yeah. Take the one that roll ups as a no, I'm saying that the roll ups of service company has the decision of how to deal with. Right? Yeah. You don't have to offer multiple options. I mean, it's just like typical enterprise sales.
06:57:11.920 - 06:58:00.588, Speaker A: Right. Also, it's a question from you guys, like how we see things differently on the XPM side, but how do you feel like the companies running the application, like putting it the charges onto the user, how do you see that happens? What are your thoughts on that? Oh, that's a good question. I think in certain cases it makes sense. You mean basically charging users transaction fees? It makes sense. In general, we as a company, we're very skeptical on business models that are entirely dependent on that on user paid transaction fees, just because there's pretty massive incentive misalignment there. Right. There are some folks working in the space who are like, they're taking some percentage of fees that users are making.
06:58:00.588 - 06:58:17.604, Speaker A: And it makes sense now. And you'll see a lot of VC tweets talk about this with optimism or Arbitrum. Right. I don't know if anyone in the audience has seen these tweets. Like, once 4844 gets merged in the margins, that optimism and Arbitrum are going to make are going to go to the moon. What's actually going to happen? Fees are going to come down. Margins aren't going to go up.
06:58:17.604 - 06:58:48.412, Speaker A: Right. Margins are probably going to stay roughly the same. Well, it depends on how that surplus is distributed, right. When costs go down for a business, they can also make more money or it can be completely passed on to the customer. Yeah, that's true. But historically, what we've seen is that these get competed down to very little. I think that's because optimism and Arbitrum, though, offer commodity block space, the user or the DAP developer doesn't care about whether they're deploying to one of those or the other, and therefore they compete directly.
06:58:48.412 - 06:59:13.160, Speaker A: Right? Yeah, that's correct. It depends on how you view the roll ups as service space. On some level, there's differentiation. I think we know that when users talk to one of us, they're talking to most of us. And so on some level, there's commoditization. Yeah, I agree with that. The biggest benefits, which is economics, are common to all chains.
06:59:13.160 - 07:00:03.132, Speaker A: All right, let's move on to developer pain points. What are some of the common pain points that teams that you work with bring up and kind of how do you plan on solving them? And then also I want to leave some time for questions from our audience after this question. It's got to be liquidity fragmentation, I think, and interoperability. And that's a tough one to answer because for a lot of apps, the right answer is that a roll up at this point is not quite right for it if it really needs to do heavy composition, especially atomic composition, and even through shared sequencers. Espresso had this awesome workshop yesterday on shared Sequencers, and they gave this toy example about a bank, and the bank contract is on two chains. It shares a sequencer. So that sequencer produces full blocks for each of the two chains.
07:00:03.132 - 07:00:35.308, Speaker A: And what you want to do is you want to put USDC on one side and get Dai out of the other. And the tricky part is making sure that it doesn't fail and you don't put USDC on one side and the Dai doesn't come out on the other side or vice versa. Dai comes out and USDC does not go in. And that basically relies on the builder providing a block, which is correct because the builder can execute all the transactions they know if the block they submitted is going to revert or not. So you want to make sure that that builder is acting honestly. So then they said, we can just have the builder put up some stake. You can slash the builder.
07:00:35.308 - 07:01:23.730, Speaker A: And to me, that was like a fun example. But at first I was like, it seems reasonable, but then for like an hour after the talk, I was like, there are so many failure cases for that, actually, because if the bank could lose out. And there's like collusion issues, the stake size, it was like the devils and the details there. And also how do you prove the transaction failure? Do you use the L one for maybe this is a tangent, but my point is that atomic composability is really hard to achieve between roll ups. So as a result, you basically have to kind of focus on apps that don't rely on that so much across chains. Okay, I will continue using the real cases we handle recently. I think the major issues right now is still about technical issues.
07:01:23.730 - 07:02:18.470, Speaker A: We have some back to the sort of the partner, right, the assets to have this kind of latency, you can have a try. It's like you must provide less than 100 millisecond and basically you play a fully on chain, give every move on chain, but at the same time it's seamless and no latency. That kind of the ridiculous requirement asked by our partner. So the deal is like can you achieve it? If you cannot achieve it, then forget about it. So we have to spend a lot of time, a few months to really optimize and reimplement the execution layer ourselves to make it EVM compatible to achieve this last 100 millisecond latency. But this is something like an extreme case, right? One case like this, another case we also got some crypto native partners. They want like super decentralized sort of roll up system.
07:02:18.470 - 07:03:00.130, Speaker A: They don't care about latency, but they want it as decentralized as possible and as secure as possible. Better. You have a lot of sequencers, but at the same time you must have a proof system. Otherwise we won't use it. Because this one is used for public good or something that so it's really from one extreme to another extreme. This includes a lot of this technical development to tackle a lot of technical challenges which the existing general purpose L Two haven't solved yet. But as sort of like a provider, we have to solve this technical question probably ahead of the big players in L Two space.
07:03:00.130 - 07:03:21.568, Speaker A: That's why I highlight there are a lot of technical challenges here. But do you think it's worth it? I believe so, because we all dream like probably for the next ten years. It's a word for web. Three. Right. So we want to handle like billions of users to use blockchain in the normal application. So in that case, latency, we need to solve it.
07:03:21.568 - 07:03:48.210, Speaker A: Decentralization security, we also need to solve it. These are all the great properties for blockchain. But at the same time you can see, right, I feel like we are still in the early stage. It's not like we are talking about Ras and we feel like we already have four great companies. But at the same time there are a lot of new problems we need to solve. And I believe it's too early. And we really need together fix all these solutions for that.
07:03:48.210 - 07:04:20.072, Speaker A: So also we see developer experience, like building in Web Three itself as a developer experience problem. And that is also something we would want to optimize for. So if you're building a game, you write some component in Solidity, then you write a front end in some other language, and then you connect them, and it's just hard. The integration is just hard. And also, like you mentioned, the latency issue. So we kind of catalyze two different things, the developer experience and the user experience. And we're kind of like optimizing for both.
07:04:20.072 - 07:04:48.310, Speaker A: So you get both the web two experience for building web three. I've been saying that over and over. So that's the kind of thing that we are solving for on the experience side of things. It's not just the developer experience, but also the user experience that needs to go along with while building applications. Yeah, I want to leave some time for questions so I won't rehash what they said. All right. Do we have any questions from the audience? No.
07:04:48.310 - 07:05:37.068, Speaker A: Yes. You just stand out, Joel. Actually, I'll come over. In the case where you have applications that are seen as killer applications, do you expect that you have to compete between the four of you and offer sort of concessions to them to get them to build on you instead of some of the other people that are there? I think there are cases when a roll up framework is uniquely suited to serve a particular application. And to me, things like virtual machine compatibility actually result. They actually represent a reduction in fixed cost or one time cost for a user to use your, for example, like if you have SVM compatibility, technically, DAP coming from Solana doesn't have to use you. Right.
07:05:37.068 - 07:06:05.308, Speaker A: They could use an EVM chain, rebuild all their smart contracts and EVM. And that's what's funny. I remember when Starkware was writing the contracts for dYdX and Cairo, people were dunking on that a lot, but actually, they were interpreted to me, Starkware was doing exactly the right thing. All they were doing was it could have been built by dYdX or it could be built by Starkware, but that's just some cost that's being incurred by someone. They're just saying, we're not going to pass it on to you. We're going to incur it ourselves. So I thought that made sense.
07:06:05.308 - 07:06:51.290, Speaker A: But yeah, there are cases where definitely we'll be talking with a few different they'll even just tell us, hey, we're talking to these other folks. These are the models that they've given us. Could you just highlight the differences rather than giving the whole rehash? Yeah, you mentioned concessions. I think people are less focused on concessions when it comes to pricing and business model than they are on which service makes the most sense for them. This is for a lot of applications, like the biggest infrastructural architectural decision they will ever make, at least for that version of the product. And so typically, we find it's more on the fundamentals than on price or anything else. Should I just shout? Thanks.
07:06:51.290 - 07:07:44.376, Speaker A: On some level, obviously, you're all kind of competing with each other, but I wanted to ask who you view as your biggest competitors, maybe in kind of an unexpected way. No, hang on. Pick someone to cage fight amongst the four of you. No. Is it like apps looking app developers looking at the Cosmos ecosystem to have more modularity and control over their stacks? Is it other l ones? What are the alternatives that your customers are looking at outside of launching their own roll ups? It depends on the stage of development that the application is in. It depends on the vertical and things like that. But overwhelmingly, the competitor is actually the status quo, just whatever they're already doing, because it just is cost for them.
07:07:44.376 - 07:08:07.184, Speaker A: But they have to put in a lot of effort to switch to another chain. They're not necessarily even going to have their own chain. I find that usually it's not between Cosmos L One versus roll up. I find that's not as common as I would have expected. And I actually thought that we'd be competing with Solana more initially, just because they're also a high throughput chain. Then I guess Solana kind of fell out of favor, became totally irrelevant. Oh, they are.
07:08:07.184 - 07:08:26.888, Speaker A: No, I think it's solana. I'm super long. Like, totally. And those guys, I think they're really smart. Yeah, but that's just reality. Who we typically find we're talking to. Yeah, I think to his point, like, the stage of which the project is at, it's the most important thing.
07:08:26.888 - 07:09:13.910, Speaker A: So some may want to spin off of an existing L One protocol to a roll up as a service, or some projects which are just starting out might just pick a chain to just build off of or a framework to build off of. So I think that's very important. All right, I think we're out of time now, but let's give a big round of applause to our panelists and a round of applause to our moderator, please. Thank you. Great. Yeah, I think it's funny because, like, I feel like, you know, we'll eventually work together. Thanks.
07:09:13.910 - 07:11:18.800, Speaker A: Just it all right. Testing, testing. Can you hear me? All right, and we're back. All right, please settle in. Take your seats. Thank you for sticking around. We've got about one more hour and one last segment of speakers here coming up.
07:11:18.800 - 07:11:40.006, Speaker A: Please settle in. If you're still in the room, take the conversations outside in the peanut gallery. Thank you. All right, we've got one more segment here of talks before happy hour, and it's going to be great. So settle in. Thank you. It's going to be on interoperability in the modular world.
07:11:40.006 - 07:12:40.274, Speaker A: And first up, we're going to be hearing from Jim Chang on the role of relayers in the modular ecosystem. So, Jim, come on up. Awesome. Well, thanks for the Celestia team and the Maven team and all the other sponsors for having me up here. Really excited to be talking about something that I don't think many people actually talk about in the interoperability space and that's relayers and how relayers are incredibly important to kind of the cross chain ecosystem, but also incredibly important as we scale into a modular world and there's more and more kind of modular layers that need more coordination. And before I start, just a little bit about me. I'm Jim, I'm the co founder of Catalyst, which is a cross chain liquidity network.
07:12:40.274 - 07:13:21.334, Speaker A: If you guys were here for the Ras panel earlier, they're talking a lot about liquidity and interoperability and how that's really important to kind of this role as an ecosystem world. And so we're trying to solve that. I'm also a DeFi and dao builder and so I've been building this space for coming up on six years now. Most recently I was a product manager at Aave and I've contributed to a few Dows, like Pleaser and Gitcoin as well. So kind of have loved and known this space for a long time. And I'm also a cross chain enthusiast, which I don't really know what that means, but I read a lot of blog posts a little bit about Catalyst, although we're not going to talk too much about it today. We call it permissionless crosschain liquidity for the modular future.
07:13:21.334 - 07:14:05.534, Speaker A: Basically what that means is we have sovereign liquidity that lives on different roll ups and they're able to actually coordinate permissionless cross chain transfers with one another and we can connect all ecosystems. So not just EVM, not just Celestia, not just move, we can connect all of it quite trivially. And we do that through integrals and other kind of complicated math, but not going to kind of go over the details of that today. Instead we're going to talk about relayers and why they're important. And so a little bit of context. I have liked relayers for a few years now. I think it's probably the unsung hero of really cross chain and interoperability.
07:14:05.534 - 07:15:03.990, Speaker A: Broadly, relayers are called a lot of different names. You can call them agents, solvers, underwriters, what have you. But basically it's just an entity that moves packets and coordinates any sort of off chain data in between two different blockchains. And so pretty important in the context of moving between blockchains, as you can tell. But why are relayers important and why are they going to be even more important? It's because of the modular blockchain stack, right? I think all of you guys are familiar with modular blockchains, hence why you're here, splitting up all these different kind of tasks that a monolithic blockchain does into three separate tasks. But I think now there is a bit of an evolution on the thinking of it and it's less about there's three different tasks, but it becomes four different tasks, right? You have transaction ordering using shared sequencers. And so now there's four parts of the modular blockchain stack.
07:15:03.990 - 07:16:01.990, Speaker A: But, uhoh, what do we have here? If you guys were here earlier in the week and you guys attended Polygon today, you would have seen that Polygon kind of unveiled their stack and their modular blockchain stack has a few other names. They have things like staking and proving and interoperability. And so if you look at it holistically and you see kind of the modularity of blockchains and more specialization and more specialization, it starts to look a little bit more like this. And so we started off with three things that modular blockchains could do. Now became four things, now become six things or seven things. You have proving, you might have liquidity stakingrestaking, you might have interoperability. And that gets a little complicated, right? And I think what becomes more complicated about it is as there's more and more modularization and more specialization of these blockchains, there's more complexity in the verification.
07:16:01.990 - 07:16:40.162, Speaker A: If you're trying to send a transaction from one chain to another or one roll up to another, you actually have to verify all the different pieces to ensure that this is a valid transaction to verify the state verification. So it used to be okay, just moving from chain A to chain B. But when you go to a monolithic blockchain, you have to make sure there's DA proof with a shared sequencer, you have to make sure there is a transaction ordering proof as well. And it gets more complicated. Now you need an execution proof, you need a settlement proof, you need interoperability. All these things need verification. And how do you do that? I think the simplest way to do it is just to have native verification.
07:16:40.162 - 07:17:49.290, Speaker A: I e you have light clients that are pointed to one another. And so if I were to send one transaction in this new modular world, I would probably have to have six forms of verification and that gets very complicated very quickly. And how do we pass all those light client header updates to all those different layers? How do we actually coordinate the communication between these execution layers and one another? It's with relayers. And so you're starting to see, okay, wait, relayers are super important, they're going to become more important and they're kind of everywhere. And so is it all relayers when you look at the modular blockchain stack? Not quite, but yes, and I would say it always has been. And so at Catalyst, when we think about building cross chain solutions, we think it's really important to build a really good relayer system. In fact, we've had the privilege of working with every interoperability protocol, assessing their relayer systems and seeing that it's not quite good enough.
07:17:49.290 - 07:18:40.634, Speaker A: And so we kind of took it upon ourselves to think about, okay, what is a really good relayer system and how do we bake in the incentives for a good relayer system into this cross chain sovereign liquidity network that we're building at Catalyst. So a little bit of a public good, a little bit of a side quest, so to speak, but something that we love and feel very nerd sniped by. And so what are good requirements for a relayer system? One, calls always arrive. It's good when you have things queued up in the outbox and you actually receive those messages. Two, pricing is transparent and fair. You actually know what you're going to be charged and you know what the margins are you're going to be charged right now, I think a lot of relayers are public goods in the sense that they're lost leaders. You have foundations that run them at a loss, but I think at scale, you probably want to bake in some economics to it.
07:18:40.634 - 07:19:40.014, Speaker A: And then lastly, relaying sufficiently decentralized and it's not decentralization for decentralization sake. There's actually a really important reason why we need that, and I can talk about that a little bit later in the presentation. And so the first thing double clicking, we think a lot about what are the big questions behind designing a really important relaying system. And then when you break down the requirement of having calls always arriving, you realize that there's kind of two pieces to it. Calls need to arrive, but calls need to arrive quickly, right? They need to arrive within a reasonable time span or else you add latency. And therefore, what is even the point of having this happen? And so at Catalyst, we were kind of under the assumption that things were just relayed, right? They just happen. And if you're in testnet, that may be the case, but if you're in production, if you're on main net, not typically a case, right? And so a little bit frustrating there.
07:19:40.014 - 07:20:49.654, Speaker A: And so we realize that calls don't always arrive within a reasonable time span. So how do we incentivize this, right? Are we building a system in which relayers are penalized for not having timely delivery of messages? Or are we building a system in which it's more of a kind of a willingness to pay, users have a willingness to pay some sort of bid ass system in order for relayers to be relayed. We're kind of still designing the system. And the second piece is, what if calls don't arrive in a timely manner, right? And so calls should be able to be canceled or timed out, but what happens when a message gets stuck? Do we need to implement some sort of recovery system? Do we need to implement some sort of bidding system? How do you design a mechanism in which people understand, or users rather understand or applications understand when relayers are taking too long and you just need to pull the whole plug and try it again. And so that's something that's really kind of top of mind for us when we think about designing a good relayer system at Catalyst. The second piece. Is pricing being transparent and fair? Like I said before, relaying is free right now.
07:20:49.654 - 07:21:32.658, Speaker A: Well, not free, but free for the user. And it's being paid by someone. And it's usually the core teams of these interoperability protocols or the foundations or subsidized by the validators or what have you. But I think in the end state it's really important to have some sort of economic system for this. And so when you're implementing economic system, you want it to be fair, you want it to be transparent and people want to know how these routes are actually priced. And so there's a couple of ways you can actually think about pricing a route for a relayer system. You can have it be an algorithm, right? You go, okay, here's all these different factors of this is what the gas limit is, this is what the gas price is, this is what the consumption is, this is what the congestion of the network is.
07:21:32.658 - 07:22:26.820, Speaker A: You sped out a number, right? And people just take it. That's totally fine. Another system is having a certain sort of bid ask kind of order book, so to speak. And so users have willingness to pay relayers have willingness to accept certain services within a certain threshold. And so it starts looking a little bit more like an order book, which has of course mev imp implications that I'll talk about in a second. Or you can have user determined bounties, right? And so I think this one's pretty popular. Within the IBC ecosystem, you have a standard called ICS 29 that has this kind of user escrowed bounty where you're saying, hey, the user knows how much they want to pay for a cross chain packet and they'll put it up and then the relayer will take it, right? And kind of the criticism of that is like, how do users know? How do the users know they're going to pay one dollars, $2? It's kind of hard to say and you're kind of leaving a lot of money on the table in that regard.
07:22:26.820 - 07:24:06.334, Speaker A: The second piece is prepaid versus pay at the end, right? Are you going to kind of load up a balance and have someone withdraw from it? Are you going to actually kind of pay them at the end once they do this delivery? If so, there's all sorts of implications, right? One is what token do you pay it in? Do you pay it in the origin chain? Do you pay it in destination chain? Do you pay in USDC? What is the price risk of these relayers? And if there is kind of a price risk, how is that fairly priced, right? What is kind of the oracle in order to have some sort of mid market price in order to do the swap, let's say if it's from USDC to ETH or it's from ETH to Matic or what have you. And so you're starting to see that, okay, so a very simple kind of idea becomes very complicated very quickly. You're like, oh yeah, we should price relayers. And now you go down this rabbit hole and you say, okay, actually it's pretty difficult to do so. Right? And then the last piece is how is Gas handled? Right? How is Gas estimated paid front end destination? And as I'm going to talk about in two slides, it's actually a pretty big trust assumption that you have no matter what kind of system that you're architecting. And so what are the assumptions on that last piece? Right? So the most important question when you think about Gas estimation is who is doing the gas estimation? Who is calculating that? And so calculating Gas on an origin chain is pretty straightforward, right? It's what any kind of client does or interface does. But doing it for the destination chain gets a little tricky, right? And so you can obviously have some sort of trusted solution.
07:24:06.334 - 07:25:18.886, Speaker A: You can ping an RPC or you can have some sort of somewhat trusted solution, you can use some sort of gas oracle. But the question is always is who's making a gas oracle? Are they taking the margin on the quote? If so, how much is it? Totally fine. But I would say from our experience of using all these different primitives is that it's pretty opaque, someone's making money, user doesn't know, and that is not really ideal for us as we're designing our relayer system. And there's also trust assumptions on the delivery, right? Let's say everything's perfect, you have this perfect price that everyone feels very comfortable with, you're leaving no kind of leakage. In this economic system you're still trusting someone, right? And so many protocols, they pride themselves, they're like, yes, we have a one of N relaying assumption, but the reality is it's not one of N, it's typically more of a one to one. And why do I say that? It's because users have to trust the relayer that they're paying at any given time. And so if you find a relayer you want to pay them, they're going to send the message, you give them the money, how do you know they send the message? Right? Oh, I think as a kind of caveat you could probably pay a lot of different relayers.
07:25:18.886 - 07:25:59.974, Speaker A: But now instead of charging one dollars to deliver one message, you are holding up $5 for one of those five relayers to deliver a message. And so I think the takeaway here is lots of trust assumptions on this real delivering the message and there needs to be some sort of mitigation for that. And so I'm posing a lot of questions with no answers, but this is probably the closest answer that we have so far. And we call it conditional payments. And so it's basically a one time payment on the source chain with some transparent pricing in mind. And we try to make it as trust minimized as possible. And so the flow for this is as.
07:25:59.974 - 07:26:51.998, Speaker A: So there's a price quote that's generated for the relayer, right? Let's say we're using some sort of top down systemic pricing algorithm. We actually use catalyst as this kind of price oracle and so we're able to calculate how much gas is on the destination chain and what is the actual fair rate between the origin and destination chain. Gas tokens for the swap. The user's payment is then escrowed on the source chain with a time decay. And so there's an incentive for these relayers to go very quickly in order to actually capture the full amount of this escrow. And then a relayer must return with some information from the destination chain in order for the escrow to be released. And so we're talking about round trips here, not just one direction but one direction and then sending an Acknowledgment back that something actually happened with the necessary mercury proof in order to prove so ends a one of end design because this bounty is open for anyone to claim.
07:26:51.998 - 07:28:34.934, Speaker A: And so we think that this is a pretty good start on what we think is a much more trust minimized and much more kind of robust and transparent relaying system and then over time we do want to overlay mev to this. And so like I said, one way you could do pricing is have some sort of bid ass system or an order book system which begs the question where's the mev kind of coming to that. And so I think the holy grail that people like to talk about when they talk about relayers and when I say people I mean me and four other people in a Telegram group because no one's really talking about this is ofa which is order flow auctions, right? As folks may be familiar with mempools and flashbots and private mempools and so ofa and so you think about this, there's an ofa for an origin chain and they're actually queued up in some sort of auction and then you have these relayers actually bidding for this order flow, right? And why would they do that? Because then they have the exclusive right to be the relayer for that order flow and they extract what we call rev, which is just relayer extractable value and they're able to bundle this order flow and have some sort of sandwiching as what Veybob from Socket kind of talks about in this screenshot here in the right and they can extract value from it. Right. And there's this kind of additional element that we call censorship mev which is okay, what is the willingness to pay of a user if their transaction that spans two different domains has a lot of mev to be extracted if all the relayers coordinate and say hey, we're actually not going to pass this message. Right? And so a good example of that is that you have a position that's about to be liquidated on Ave. You have some collateral that lives on a different chain.
07:28:34.934 - 07:29:45.134, Speaker A: The relayer recognizes that and says, hey, you're only paying me one dollars for this $100,000 liquidation position. I think you need to be charging me a little bit more or else we're not going to be delivering that message. And so really, really big design space as we look at censorship, mev and relayer extractable value, and again, me and four other people on Telegram just nerding out about it. So the short answer is yes, mev is going to be really important for incentivizing relayers and having sustainable economics through relayers, but we're not quite there yet in terms of having sufficient transaction volume or messaging volume in order to justify it. And we think that there needs to be a really kind of robust way of doing what we call self relaying, which is if there's no mev being generated by this specific message, you got to have the relay your own message, right? Because in this paradigm, no one really wants to deliver your packet if you're not generating sort of economic value on their behalf. And so before I get to the last piece, I just want to say we're thinking a lot about exactly what a super robust relayer system looks like. We care a lot about the timely delivery of these messages.
07:29:45.134 - 07:30:21.660, Speaker A: We care a lot about the transparent pricing and the incentivization and overall the future proofing by including mev into this. But lastly, and I almost think more most importantly, we want to think about the sufficient decentralization of these things. And so again, some interop protocols have this kind of one to one relaying message delivery. It could be overtly like layer zero, right, where even before you deliver a message, you have to specify a relayer. Or it could be more tacitly where you have this kind of one on one payment assumption. You already pay the relayer, you hope they do something, but you don't know for certain. Like what I talked about before.
07:30:21.660 - 07:31:04.074, Speaker A: Or you have this kind of one of N system that's slightly more robust. But the N is really small because realistically it's just these core teams are running relayers and so the N is more two, three, four, or what have you and not quite as robust as what one of n sounds like when people say one of n for a mechanism design. And then self reeling can always be a fallback. I think it needs to be much, much more robust. I think this is a very similar narrative than to what the roll up folks call forced inclusion for roll ups, right? There's censorship from the sequencer you want to kind of submit into the settlement layer. I think this is a pretty good analogy. I think self relaying needs to be much, much more robust and have a lot more research behind it.
07:31:04.074 - 07:32:12.160, Speaker A: And we at Catalyst think a lot about that, think a lot about all the different pieces of a relayer system and why is it important to us? Because ultimately we think that healthy relaying will equal a healthy modular world. It's really hard to have modularity, really hard to have state verification between DA layer, standard action ordering layer, execution layer and eventually all the other fun layers that kind of come out in the future, like a staking layer, proving layer, what have you. They need relayers, right? And so not necessarily the core focus of Catalyst. Our core focus is liquidity, which is something, again, that all the Rasp participants in the last panel said was top of mind for them as a major pain point. But relaying and decreasing the latency and decreasing the trust assumptions between moving liquidity, between roll ups is super important for us too. Therefore, we think a lot about relaying and we want to make it a mission for us in terms of research and a public good perspective to push forward the thinking on relaying research. So thank you guys so much for your time.
07:32:12.160 - 07:32:34.470, Speaker A: Thank you. Feel free to follow Catalyst for more updates on that. Thanks. Thank you so much, Jim. That was awesome. And up next, we'll be hearing from John of Hyperlane, more on interoperability in the modular world with a different take, a different approach. Thank you, Jill.
07:32:34.470 - 07:33:28.850, Speaker A: All right, let's get to it. Let's see how do we go? But I can see my face and you see it live and you can see it on a picture, which we need to do something to get the slides. There we go. I had a lot of slides skipping. All right, let's get to it. So today we're going to talk about modular interoperability sorry, modular blockchains and interoperability, specifically permissionless interoperability. So if you think about why you guys are all here today, we are here at the modular summit because for whatever reason we'll get into that very specific reason, we think that the modular construction is an ideal solution to a lot of the issues that we face with blockchains.
07:33:28.850 - 07:34:16.150, Speaker A: So over the next 20 minutes or so, we're going to get into that. We're going to cover the different moving parts, what needs to happen, what is feasible and what is not. And so at Hyperlane, we like to talk about this concept of the interchange highway. What's that? It's kind of like a big word. It's something that we like to think of as a safe, somewhat fast path to connect between the ever increasing and ever expanding universe of chains. And the modular universe is one where we're going to have an increasing number of smaller chains, smaller than the monoliths that we've become accustomed to over the last few years. Specifically, we're going to talk about how permissionless interoperability and modular security help us create this concept of the interchain highway.
07:34:16.150 - 07:35:01.974, Speaker A: So with blockchains we have these really cool internet computers. They let us do all sorts of things from starting with Bitcoin that gave us sovereign and permissionless money moving on to Ethereum that gave us permissionless access to computation, right? I think it's one of the coolest things in the world that pretty much anyone here can access this permissionless substrate and run any computations they want on it. There's some magic in that. But with these Internet computers, the way that they do this magic, it's primarily by introducing the concept of scarcity into the digital domain. We've never really had scarcity in the digital domain before. Blockchains became as common as they have. And this visualization here we have on the screen is something I really like.
07:35:01.974 - 07:35:52.998, Speaker A: It treats each block as a bus. And I think this is a really effective visualization because if you think of a block, well, there's almost so much space in it. And if a bus has a certain amount of seats, so does a block has its size. Well, so every little person here trying to get on the bus, it's an incoming transaction, and they take up a seat. But unlike a bus that has a fixed number of seats, and once it's full, it's full, our buses introduce a mechanism of surge pricing. And so what does it mean for the bus to get filled up? It means that the thing that, by and large, all of us are here to bring about, right? Like to increase the amount of demand for these Internet computers. Well, if we end up looking like the bus on the right side, right, we got a lot of passengers to get on.
07:35:52.998 - 07:37:07.840, Speaker A: We're going to introduce the surge pricing. And so this is when that magical scarcity introduces a big, big problem. And all of us, probably sometime in the last years, have felt the anguish that comes from the surge pricing, right? When you're trying to do a transaction and suddenly you're met with, like, a $70 fee on Ethereum just to do that 21,000 gas of a basic transfer on ETH, right? The gas just gets too damn high. And modular blockchains seem like an ideal solution for this. Again, if we're here, it's probably because we think there is some efficacy in this premise of we need more buses, we need more seats. And instead of just creating one massive bus that would be some magic monolithic chain, we're going to have a whole slew of smaller buses and smaller bus lines. Think of each one of these as being a new roll up, a new chain, and that's where something like hyperlane comes in, right? So here at the modular summit, we're talking about all the different ways that it's becoming easier and easier and easier to launch your own chain, to launch your own roll up.
07:37:07.840 - 07:38:16.274, Speaker A: In fact, with some of the participants who are here today, folks like Caldera and Conduit and Eclipse and Altlair, they're making it as easy to create your own chain such that you could really get something going within 1015 minutes. But if you've done that in 1015 minutes, how do you connect to everybody else? You need something that you can support yourself. And this, again, is where hyperlane come in. I like to think of hyperlane as something that, through its introduction of permissionless, interoperability acts as the final layer, the layer that completes the modular blockchain stack. If we broke up execution, settlement, consensus, and data availability, for us to actually have practical benefits from that, our modular chains need to be able to talk to each other. If, to enable that talking to each other, you need to go and convince someone on a permissioned interoperability team to add your chain, right? So you spun up your new chain in ten minutes, but now you need to spend the next ten weeks convincing someone to add you. That doesn't really cut it.
07:38:16.274 - 07:38:41.942, Speaker A: And that's to the extent that folks have been playing around in the cosmos ecosystem. The magic of IBC is its permissionless nature. It's the fact that you can spin up a tendermint chain. You can spin up a chain in the cosmos ecosystem, and through IBC, you don't have to ask anybody for permission. But it's become harder and harder. You can't really do that outside of the confines of IBC. And so we need to find a way to extend those confines.
07:38:41.942 - 07:39:29.846, Speaker A: And there's a number of people working on this problem. So what is this problem, really, when it comes to modular blockchains? It's this problem of permissioned interoperability. It's the fact that you need an interoperability provider to add you to add your chain so that your chain can communicate with other chains on their network. I think, and certainly other people might disagree, but reason for this is that by and large, most interoperability protocols, they have coupled the product that they're providing, right? The ability to communicate between chains or the messaging interface. They've tightly coupled it with the security model. They've made those two effectively into just one thing. But it doesn't have to be that way, right? It could certainly be decoupled.
07:39:29.846 - 07:40:02.710, Speaker A: You can have that product, the way to communicate between chains, that interface on one hand, and you can have security be provisioned on the other hand. And that's what we do with hyperlane. We really believe that modular chains need modular security. So we're going to get more into the specifics of that. But at a high level. Imagine most of you have had this experience of going to your bank when you try to get a nominal sum, say a few hundred dollars out. The ATM will let you do that with just your Pin number.
07:40:02.710 - 07:40:27.246, Speaker A: Don't really ask any questions beyond that. Try to do something a little bit more intense. Try to withdraw $40,000, $50,000. Try to make a large wire. Or if you dare, try and close your account at the bank and get all the money out of it in that same day, they're going to run you through a much more serious line of interrogation. They're going to want to see your ID. They're going to want to know who else is on the account with you.
07:40:27.246 - 07:41:10.410, Speaker A: They're going to ask you, do you recognize the last ten transactions? See your Social Security number, and if you're in the US. So what is the bank really doing? The bank is looking at the context of what you're trying to do, and they're running you through a different security protocol. And that seems to make sense. Most people don't really object too heavily when the bank is trying to run them through a much more stringent process, when their action is going to have a much more significant impact on the account. Don't really do that in crypto. Crypto, you try to move $100 or you try to move $100 million, we run you through the same exact system. Might want to think about why that is.
07:41:10.410 - 07:41:46.278, Speaker A: It's something that often doesn't really get questioned for us. We didn't think that necessarily needs to be that way. And so the introduction of modular security really, really lets us do that. And so we could talk briefly about different forms of interoperability to understand this concept a little bit better. The best way to verify state, if you can, is to leverage what some people call native verification. In this case, you're just leveraging the existing consensus between the connected chains. I'd say IBC is probably the gold standard in interoperability until today.
07:41:46.278 - 07:42:52.458, Speaker A: And IBC gives you native verification. When you connect two chains through IBC, you're really just trusting the two chains that are being connected, which generally is a fine assumption, right? Like if you're connecting between, say, Ethereum in Arbitrum, let's say, or say Ethereum and Avalanche, if you're using both of those chains, you're probably comfortable with the security models on both. So the fact that you don't need to introduce anything else is terrific. But the issue with native verification is that it's a very costly method because you're basically replicating the consensus of one chain on another and doing it in a place that was not constructed for that, as is the case in the EVM world, is incredibly costly. The other mode is external verification, not too complex, right? So we're still trusting our A and our B, our Ethereum and our Avalanche. But now we need to introduce someone in the middle, and hence the name external verification. So this kind of looks and feels like an Oracle.
07:42:52.458 - 07:43:38.950, Speaker A: If you're familiar with the Oracle problem, I imagine most of the people here are. And that's because external verification for interoperability, really, it's just a narrow scope of the Oracle problem. Instead of Oracleizing, any type of information in the world, you're just Oracleizing state from another chain. That makes it a slightly easier problem to solve, but it still, at the end of the day, it introduces a third element of trust. And this is where a lot of things can certainly go wrong. So now let's move on to modular blockchains, right? Again, we're here at the modular summit because we believe in this concept. And I personally believe that with modular blockchains we're not going to have a singular hub.
07:43:38.950 - 07:44:12.600, Speaker A: We're going to have a number of hubs from which we see an increasing number of smaller blockchains, smaller computers. Again, think of these as smaller buses, smaller bus lines. And so Ethereum becomes a hub, Celestia becomes a data availability hub. Cosmos is already a hub. Think of these little guys here in Optimism and Base and Arbitram. They can become little hubs too. We're going to have these hubs everywhere and more and more chains that come out of them.
07:44:12.600 - 07:45:05.240, Speaker A: There we go. They need a way to connect with each other. And every time you create another new little computer, another little bus, how does it communicate with everything else? So you do need permissionless interoperability. You need a way that this increasing universe of chains can connect to each other and can do that without having to ask anyone for permission, without having to ask anyone for help. We're in this space because we value our sovereignty, we value our freedoms, and we want to be able to act without being gatekeeped. And so a world of permissioned interoperability, where we're just waiting for people to add our chains to get this support doesn't do much for us, doesn't do much for anybody. And this is what we built hyperlane for.
07:45:05.240 - 07:45:34.080, Speaker A: So how does it work? This little schema here shows you the lifecycle of a hyperlane message as it goes from a chain A to a chain B. Let's say again. Ethan. Avalanche. Ethan, pick your poison. There are three critical parts to hyperlane. There's our mailbox simple contract that just sends and receives information, transfers bytes, encodes them, helps verify them.
07:45:34.080 - 07:46:08.982, Speaker A: Then we have our security modules. Think of it as like some logic to run a test on to basically say, how do I get comfortable with what's been relayed to me from a different place to that? We could use system of validators. We could use an optimistic system with some type of watchtower, could use ZK Magic when it becomes available, because who doesn't love ZK Magic? It's the best. I'm a big fan. Or we could even use some type of committee based approach. And lastly, we have a relayer. Relayer's open role.
07:46:08.982 - 07:46:52.490, Speaker A: You just heard Jim talk about all the difficulties in relayer systems. There's a reason why they are working on it because it's a hard problem and the relayer in hyperlane is no different, right? A lot of those open questions are things that every system with relayers needs to think about. So how does it really work? You have a contract or a user starting on the source chain. They send the message to the mailbox. They tell it they have the message body, the bytes that they're trying to transfer around in those bytes, you could reason about, oh, I want to move this asset. I want to deposit this thing in that liquidity pool. I want to borrow from this lending pool.
07:46:52.490 - 07:47:37.666, Speaker A: The mailbox takes in the information about what needs to happen, where does it need to go, and then come in one of our security settings. So in hyperlain we talked earlier about this concept of modular security. So instead of being forced to use a specific option, you're always able to choose from a set of security modules. Don't like them, could build your own. Not everyone wants to do this. It's not an easy or fun thing to do, but it does open the door to a security marketplace. In fact, where we are today is that external teams, teams outside of the hyperlane core team, are building different security modules.
07:47:37.666 - 07:48:14.162, Speaker A: Talked a little bit about ZK magic. In fact, there's one team already leveraging their ZK proof mechanism to create a hyperlain security module. And so once your message is ready to get to the destination, the relayer comes in and the relayer always observes the mailbox. Relayer is a fairly simplistic operation. Think of it like just a basic bot that looks at the mailbox and sees what you're trying to do. It sees that the message is there and it's waiting to be processed. It sees that maybe you're kind and you want to pay them for this service.
07:48:14.162 - 07:49:07.714, Speaker A: So you've included a little tip in there for them. Maybe you didn't. And maybe then the only reason that they would relay your messages because as Jim talked about earlier, is you might have some mev opportunities for them to take advantage of as they take your message to the mailbox on the destination chain, they ready it for processing. And during the processing is when the logic is tested against the interchange security module. So there you can do some really fun stuff. We talked about the bank example where the bank treats me different whether I try to withdraw $100 or if I'm trying to wire out 50 grand. You could configure your app in such a way that when a message is received, you reason about the message a little bit, oh, what's this user doing? Well, they're just trying to send, say, $100 in a small swap.
07:49:07.714 - 07:49:31.290, Speaker A: Great. Send it through the fastest run it through the fastest module that we have. No need to have maximum security because it's $100. What's the worst that could happen? $100 could be lost. Someone's now trying to move $100 million. Let me ask you, anyone ever see a nine figure transaction that wasn't actually an exploit? Rarely happens. It's always an exploit.
07:49:31.290 - 07:50:04.070, Speaker A: So a transaction of that size, we can now run it through a very slow path, a very secure path, maybe the optimistic path, which doesn't allow for the message to be processed for a very long period of time set by you as the integrating app. And during that time, anyone can observe that transaction. And if it doesn't look. Kosher shoot it down. You don't have to take it. And once it's passed through the ism the interchange security module. It's ready to go, arrives at a destination.
07:50:04.070 - 07:50:51.222, Speaker A: And so what did we just see? We just saw how we use hyperlane to communicate between chains. In the interest of time, I won't spend too much time on this slide here, but the key concept is that with hyperlain, you can choose how you want to secure your messages. Because there is no enshrined security module, you're able to leverage economic security in the form of staking. You're able to leverage that optimistic security that we just talked about. And when you're just getting started, when the stakes are low, you could just have a multisig committee, and you can imagine that multisig option, it might actually be beneficial even when the stakes are high. Maybe there are certain actions that you want specific set of people to have to approve. Maybe it's your team, your company.
07:50:51.222 - 07:51:43.640, Speaker A: Maybe it's the key portion of your community. The folks who are emerged as the most influential players think of this as something that can be used, like the emergency switch in Makerdow, right? Hopefully we all know that there's an option in Maker to shut the system down. Never been used, but it's there. You might need to hit that switch one day. And so how does hyperlain compare to kind of regular Bridging? Generally speaking, most Bridging today is done through this form of, like, an omnibus bridge. You have one contract, and it takes all the assets from all the people, and they all sit in this thing, and it kind of looks like a nice honey pot. And this is how we end up with more than $2 billion in different bridge exploits, because the more people use it, the more appealing it becomes because they're all there in the same place.
07:51:43.640 - 07:52:40.698, Speaker A: But if we're talking about modular chains, like, why do we need to have this honeypot shared between everybody? So why not use this concept of modular Bridging? Why not leverage what in hyperlane, we call the warp route concept that lets you to have individualized path for each asset. And each path can have distinct security settings. So you might want to have different settings coming into your chain, whether it's USDC or maybe when you're bridging your own native asset, you want to treat it slightly differently. And that's something that you can do once you leverage modular Bridging. So what can you do with something like hyperlane? So these are some of the obvious fun things that you can do. You can have interchange swapping, right? So why do we do Bridging? For the most part, because we want to have the asset go from one place to another, and we want to do something with it at the edge. But if the thing that we want to do is swapping, we might not need to move it after all.
07:52:40.698 - 07:53:21.874, Speaker A: If the thing already exists on both sides. Why not just transfer the logic of that action and say, hey, John is on Ethereum and Dave is on avalanche, and Dave's got 500 USD, USDC and he's willing to use it to buy some of John's ethereum. Well, great. So John, you get rid of your Ethereum. Dave, you get rid of your USDC, and we send that logic through something like Hyperlain. There's also the concept of interchange accounts, which will be familiar for anyone who comes from the cosmos world. Hyperlane recreates that in a broader context.
07:53:21.874 - 07:53:48.270, Speaker A: Some key differences there that we could get into. If you want to talk later, you can also bring interchange Oracle. So we just released this just the other day with Hyperlane. Now you can bring chainlink feeds to your chain permissionlessly. So this is a problem that a lot of modular chains, a lot of rollups face because they need access to chainlink data or other trusted Oracles. And so you can do that. Another use case I like talking about, but I'm running out of time, so I'll be kind is interchange margin.
07:53:48.270 - 07:54:28.220, Speaker A: Now, what does the end game look for? Something like Hyperlain, we could think about inner rollup networking where we have created all these roll ups. Well, they need to talk to each other more easily. And like, what network effects can you create from that? Then relaying Services, that security module marketplace that I mentioned, and ultimately this concept of interchange intents. But that was all the time we had. So if you want to learn more, you can go on the docs docs hyperlane XYZ can join our discord at just discord slash hyperlane, follow us on Twitter. Or you can just find me here, reach out on, you know, generally pretty friendly. So thank you all for having me.
07:54:28.220 - 07:55:03.240, Speaker A: Great. Thank you, John. And coming up is the last panel of the day, which you can probably guess is going to be on the topic of Bridging, which as we heard in the rollup as a service panel, is one of the most important issues facing roll ups and the modular ecosystem. So with that, I'd like to welcome Mads from Maven Eleven. Come on down and I'll let you introduce the next panel. Thank you. Thank you so much.
07:55:03.240 - 07:55:20.886, Speaker A: Yes, you got it. Hello. That's on. Hello. Perfect. Thank you. Thank you for making to the last panel of today.
07:55:20.886 - 07:55:50.440, Speaker A: I'm glad to see so many friendly faces. We have three very or four very smart people on this panel with a slight change. So we have bo from polymer, we have fig from squid. And then we have Richard from Orblabs. And then I think we had one more. We have Jim from Catalyst. A quick change.
07:55:50.440 - 07:56:18.560, Speaker A: Hey, Giorgios. So I think I would like to start with a small intro, if you can do what you're working on as well. Yeah, I can start. Hey guys, my name is Bo. I'm a co founder at Polymer. We're working on extending the IBC protocol to all chains. I'm Fig, I'm co founder of Squid, and we're doing cross chain swaps and transactions across the cosmos and EVM and soon to be many more.
07:56:18.560 - 07:56:54.886, Speaker A: I'm Richard, I'm co founder of Orblabs, and we're building a chain abstraction stack. I'm Jim, co founder of Catalyst. We're building sovereign liquidity for the modular future. Thank you very much. I think we often hear a lot of different words when we talk about Bridging. So I think we hear transport layer, cross chain merging protocol, cross chain router, token bridge. Could you sort of explain, maybe take one each and explain what that specific topic is? A specific thing is, I'd love to talk about the transport layer.
07:56:54.886 - 07:57:46.566, Speaker A: I feel like it's something that's not really considered interrog. I feel a lot of people talk about the state layer, they talk about security, they talk about how do you verify something that happened on one chain on another chain? The transport layer deals with encoding of the network, topology of the entire network on chain, all the paths between chains, all the paths between all the smart contracts that want to talk to each other. This information needs to be encoded on chain. IBC does it very few or basically no other interrupt protocol does that. I can talk about routing, and that's what Squid does. Essentially, routing is when you look at cross chain just from the lens of connecting applications between chains. So we take the view that there's liquidity all over crypto that just needs to be connected.
07:57:46.566 - 07:58:11.234, Speaker A: And we use general message passing and different infrastructure layers. We use Axla to access that liquidity. And so a user should be able to interact with any application on any chain in just a single click. And it's a little bit like Google Maps, but if you could teleport. So we always find you the way to what you want to do. But it should happen as soon as possible. Yeah.
07:58:11.234 - 07:59:00.146, Speaker A: And I think with messaging, cross chain messaging, at least, the focus is more so passing data from one chain to another. And Bo is right. The reason why most messaging protocols don't focus on the connections is because it's possible to have connectionless interoperability where you don't need to know the direct paths beforehand and still pass message from chain to chain. And so you find that for most messaging protocols, establishing those connections at least beforehand is not a main focus. And I guess I can take token bridges. That was something you said, right? As an option. So token bridges, I think, is a pretty archaic term, at least how I define it, where I think several years ago there was a need to move coins between their kind of home blockchain to a new one.
07:59:00.146 - 07:59:36.406, Speaker A: And so bridges were kind of made as a primitive in order to enable that. And so bridges for me are some function of wrapping a token or minting a new token on these kind of new destination chains in order to get access to this token. And it's backed one to one to kind of this vault of the original tokens that are stored on the home chain. And so literally a bridge. Right? It's like you're just kind of shooting an asset from its home to another location. Thank you. I think what you all sort of touched upon a bit was standardization to some extent.
07:59:36.406 - 08:00:38.350, Speaker A: So we obviously have a lot of different token standards. How do you bridge one token standard to the other, and also in messaging? So how do you view sort of standardization and how do you think we should sort of go towards that? My long term view on standardization is that at some point, chain developers, if you relate them to operating system developers, are going to want to enshrine something into the blockchain kernel itself. What that is will probably be an open standard, be net neutral, be decentralized. I mean, I slightly disagree with Richard here on the need for encoding these connections on chain because I think that if you don't encode this information on chain, you cannot prove anything about the network. In fact, you cannot prove that you have a smart contract that is bound to another smart contract across any number of chains. So I think that when these chain developers do come to make this decision, IBC is the best candidate here. I think you're going to realize Vo and I are going to be going back and forth.
08:00:38.350 - 08:01:36.914, Speaker A: We tend to have very different views on interoperability. But I think going back to standardization, I'm very pessimistic as to whether it will have all the developers aggregate underneath one standard simply because it's in their best interest not to. If you think about a world where there's even 10% chance that there's a different interrupt standard that sort of like makes interoperability slightly better for some other chain that didn't choose to enshrine a particular bridge, you're probably going to want the option for that sort of standard to come to your chain. And so what we're probably going to see instead is we're going to see some sort of wrapper protocol that comes and bundles up all these interop solutions and gives applications on top of these chains the ability to move seamlessly from chain to chain. But yeah. Going back to the options between connection based or connectionless interoperability, I'm pretty sure it's still possible to make it very easy to pass messages without sort of like, storing and encoding all that data on the chains. That you're going to.
08:01:36.914 - 08:02:38.440, Speaker A: Partly because most of that data is also already encoded in the security layer or the state layer, which what would call them? So yeah, that's the general thought there. Any more thoughts on standardization? I think what we're seeing right now is there are some projects that are just brute forcing it. Right. I think we can all agree in this panel that there's a pretty high degree of heterogeneity in the ecosystem as it pertains to varying standards or varying protocols that not only span general message passing, but also, as we were alluding before, kind of the token level as well. And so there are projects like Socket and LiFi that are just saying, hey, we'll just kind of put the man hours to aggregate all those things and then abstract it in some sort of off chain way. Right. What I have been seeing is while that works, and I'm sure will continue to work, there has been kind of a deliberate effort to make their jobs a little bit.
08:02:38.440 - 08:03:33.180, Speaker A: So, you know, there's this new ERC I'm blanking on the name that Arjun from Connects kind of offered that is kind of championing that. Right. And so it's good to see a lot of the liquidity networks like, you know, our team at Catalyst included, seeing that there needs to be some sort of standardization how we wrap tokens when we bring it to different chains. And so we allow for not only more interoperability but also kind of more shared agreement on the security assumptions of that wrapping. Yes, you sort of touched upon it a bit in the end here. Sort of if I'm a dev and I'm building application, I need to choose a question missing protocol or a bridge or whatever it might be. What kind of trust and security assumptions should I be looking at and what do you need to consider? I can go just on the standards piece as well.
08:03:33.180 - 08:04:29.580, Speaker A: I think it depends where we are in the innovation cycle because I agree that there's going to be experimentation. If there's a chance that something's going to work, someone's going to go for it and build the product. But potentially with Bridging, for example, it's quite an old technology now and we can start to standardize it. And people are like the Connects guy looking at ways of abstracting it in a way so that you can standardize it for everyone. But our job at Squid is a little bit like the aggregators, where we integrate protocols across different virtual machines, like different chains, different gas models, and we have to deal with all of that complexity. I don't know if we're anywhere close to getting standards in a lot of these places. So there's just going to be a ton of work on our end, I think, to just deal with that for now.
08:04:29.580 - 08:05:15.878, Speaker A: I think we could end up as a temporary standard and then hopefully the ecosystem finds its own way to become very similar across the board. But for now, we need to make things usable and evolution will play its course. So will the best standard win or are there other reasons why a standard might win? I don't think so. I mean, Solidity is the standard now and I don't think many people agree that it's like the best and JavaScript is maybe the standard. It's going to play into a lot of different factors, probably non technical ones. Perfect. We heard a sort of modeler ecosystem day and we obviously Celestia is a DA layer.
08:05:15.878 - 08:06:37.918, Speaker A: How do you guys think that sharing a DA layer might help in bridging between roll ups or chains or whatever else it might be? Sharing a DA layer allows you to have trust minimize interoperability between those roll ups. If you share the same DA layer, let's say they're optimistic roll ups, you have the security of that DA layer saying that I can make this data available to anyone to be able to generate a fraud proof. That means that this economic security of the DA is shared among the roll ups on top. Yeah, and I think even going back to the general idea behind roll ups so roll ups in many cases, at least for L, two roll ups and Ethereum get the states for all things happening on Ethereum sent up to them. And so in the case where you share its DA layer you can sort of run like clients that sort of look at the state coming from the DA layer and even compute a full node that computes the state of all the blocks that are basically being broadcasted down to the DA layer. And through that you can extract the message you need. So how that differs from Lite clients that sort of like rely on well, separate DA layers is you run into the problem where it's possible for the DA layer to fork and you may have sent over a state from some other chain showing a fork that ultimately was deemed invalid.
08:06:37.918 - 08:07:22.738, Speaker A: So I guess comparing shared Das with clients to not shared Das with Lite clients, what you have is just a slightly more robust system for passing messages from chain to chain. Perfect. Any more thoughts on that? Yeah, I'm not going to talk about the security properties of it. I do think the benefit of sharing DA and depending on how you use the underlying DA layer, you might also use it for canonical transaction ordering. You do allow for soft confirmations of the state verification of the messages being passed within the execution level. And so with that you have quicker finality. Right? Because you're essentially saying that because this data is made available.
08:07:22.738 - 08:08:09.682, Speaker A: Because either there's a canonical ordering of the transactions using the transaction ordering layer of this DA layer, or even if you're not doing that, you still have the data made available so that you can have fraud proofs or so that you can reconstruct some of these validity proofs. You are basically allowing for soft confirmations at the cadence in which blocks are produced at the DA level. And so fulcissia like several seconds. Right? And so that is a sufficient user experience even if there's latency introduced at the execution level for true finality. Okay, perfect. Maybe more of a question for Fig and Jim here as well. I think you've talked a little bit about the UX problems that we run into with Bridging and how do you foresee sort of how do we fix this? Can.
08:08:09.682 - 08:08:50.478, Speaker A: AA. Help. Can intent help. What kind of for future do you foresee in terms of UX? I mean, the data availability is what everyone was just talking about is a good example of UX where we just found out that I didn't know this, that Op, the optimism token is minted on the optimism roll up and not on L one. And so there's a Bridging problem like how do you bridge Op without it being fragmented now because they could have just minted it on ethereum and then suddenly you have all these canonical versions via the enshrined bridges. But it's going to be a mess. And I think you're going to need these.
08:08:50.478 - 08:09:47.822, Speaker A: That's what we do. We have these abstraction layers where you can just get whatever token you need for an application and even if the application is supporting the wrong token, but technically you can get that token, use the app in one click. And Intents are this idea that you just have something that you want to do and you send it off into the world and someone magically solves it. And I think what we're doing is sort of like a proto intense system where we're the only solver. And we have a feature called Boost, which we're really excited about. We just launched in the last week, which allows you to essentially declare your intent on the source chain and then that will get finalized over 20 minutes or however long the bridge takes. And anyone can fulfill that transaction on the destination chain immediately.
08:09:47.822 - 08:10:16.460, Speaker A: And it's fully generalizable because it's using general message passing. It can be a swap. You can buy an NFT across chain, you can do Staking across chain. And that's the user experience we want. We want people to do anything in any application in one click as soon as possible. And yeah, I feel like we've started just with something which works and then you can decentralize that more over time. And I think, yeah, really excited for Intents and to integrate them into Squid as well.
08:10:16.460 - 08:10:59.158, Speaker A: Yeah, I don't have too much to add as it pertains to Intents. I do agree that routing systems like Squid and the ability to have kind of a sophisticated entity provide the liquidity for fast liquidity soft confirmations of those transactions is really awesome. Right. I think that augments the UX considerably. I do think we still even in that paradigm. Well, two things I'll add. One is that I think Intents are really interesting, but I don't think people talk about solvers enough.
08:10:59.158 - 08:12:22.660, Speaker A: So what Fig at the Squig team and doing kind of the boosted feature? What they're doing there is really kind of starting the conversation of how do we have infrastructure that is not articulating intents, but actually fulfilling or providing the liquidity or providing the tools for these entities to fulfill those intents. So I think we need more kind of advancement and more investment focus into that piece. And I think where we come into the picture is kind of the second piece where even in that paradigm in which we have really robust infrastructure for solvers to fulfill these intents, we still kind of face what I think is a cold start problem when you look at new chains. And so I think it's pretty kind of consensus now that there's going to be lots of roll ups, right? Hundreds of thousands, if not millions, within that kind of framework. If you have a brand new chain, it's a really difficult problem. It's a really difficult problem to try to inject liquidity into a brand new chain that's basically a barren wasteland. And so how do you solve an intent for a new chain when there's no liquidity, right? And so that's a big research question that we think a lot about catalysts, and so we're hoping to slot into kind of this refocus into the solver problem space.
08:12:22.660 - 08:13:51.214, Speaker A: I do think most of the conversation around intents currently focus a lot on a very defined problem, which is swapping one asset for another. But there's a big question on if intents are going to eventually become, let's say, the end game of interoperability, then we're going to need to find ways to generalize intents to almost every possible transaction. And I think that problem is extremely difficult and maybe the main bottleneck to ever us ever getting to this final state of interoperability, partly because intents in many ways are invariant programming, you basically have to, well, configure your system to check against maybe an infinite number of invariants to make sure you don't break things. And yeah, that's not an easy problem. So I think we're going to see a lot of maybe proto intense systems that combine transactions and intents for quite a bit, and then eventually, perhaps after several years of thinking about the problem more, we might get to a generalizable intent system. Anything on that? Bo yeah, since we're talking about intents kind of on a tangent now, but we'd love to kind of talk a little bit about the work that Enoma is doing and how it can apply generally at the interoperability level. So if you think about the research that they've done to, say, allow chains to combine, or take the union or the intersection of the validator set between multiple chains, bootstrap, a temporary chain, they call it chimera chains, and be able to create cross chain atomic transactions if you were able to standardize at the networking level.
08:13:51.214 - 08:14:38.160, Speaker A: So at the interoperability level, let's say IBC everywhere, hypothetically speaking, and IBC were to have access to the consensus mechanism of all these different chains, you could apply the ideas of heterogeneous paxos and do essentially cross chain atomic IBC transactions across all IBC enabled chains. I think we'll see some of this, but I think it'll take probably quite some time to get to this end state. All right, perfect. Some of you have talked about a world of hundreds, if not thousands of different chains. Do you think we end up in a world where there's like one aggregator, aggregating, all kinds of different question measuring protocols, token bridges, whatever else it might be. So how do you foresee sort of aggregation working out? Well, yeah, definitely be squid. There'll only be one.
08:14:38.160 - 08:15:09.286, Speaker A: Everyone will use the Squid SDK to build their apps and there'll be some Squid widgets, which everyone uses in all applications. Everyone, all the wallets will have us installed. There'll be a few winners. I think in all cases, there might be a few interrupt winners, there'll be a few aggregators. It depends what you end up focusing on as well. We try to not take the approach of we're aggregating everything, we're just doing swaps, and then you can pair a swap with something. So it's more like a payments layer.
08:15:09.286 - 08:16:22.170, Speaker A: Maybe you have NFT aggregators and as we find more actual use cases for crypto, then there'll be more solutions that might be the winners. Yeah, I think aggregation theory that a lot of people talk about either on their Mirror blog posts or you can look at like strategy is probably going to play out where you see kind of like a power law distribution where there's going to be a handful of short tail suppliers for aggregation and they might say they're differentiated. But the reality I just have to use five of them to get access to everything. And that's fine. I think that's just how the economics plays out. I do have a bit of a controversial take, though. I do think the Intent future, or at least Intent Future envisioning is directly counter to this aggregation future, partly because once you introduce solvers to the mix, solvers are way more sophisticated than your end user and they will more than likely explore a wide variety of possible different places to get the assets they need to basically optimize or maximize their profits.
08:16:22.170 - 08:17:12.778, Speaker A: And so we could see a world where aggregation doesn't really take place, but we have a vast number of well, liquidity networks and token bridges that people just generally go through from one to one just to make sure they get the best possible profits. And these layers will be fighting based off of, well, who have the lowest slippage, who have the cheapest gas fees, who have the fastest times, and so on. Right, do you have any debt? Bo? Yes. Once again, I'm going to talk about the lower layers of the stack while everyone talks about the upper layers. I would say that I think at the lower layers of the stack we're going to see some consolidation, probably in the ten to 20 year time frame. I know people don't really talk about ten to 20 years in the crypto space. But it's about how long it took for the world to kind of converge on roughly a consistent networking standard, TCP IP.
08:17:12.778 - 08:18:41.018, Speaker A: In fact, in the early seventy s, eighty s, there were a number of proprietary, I would say like company owned protocols that were kind of seen as these de facto standards of the time, maybe like a layer zero, for example. And over time they congregated because if you're Apple and you've implemented Apple Talk, microsoft might not want to implement Apple Talk, but Microsoft will be a little bit more open to implementing TCP IP per se. So I think the world will converge to IBC in the long term time horizon, but in the short term we'll see a lot of aggregation at all layers. All right, perfect. So maybe to move the thoughts towards, I think a big difference in some of you and other Bridging providers as well is the fact some might be using a state or middle chain to verify messages, some don't layer zero, for example, and others as well. What is your opinions on having a state middle chain? Do you think the sort of crypto economic security provides is good or bad? Some might say that, or what are your opinions on that? So I think the idea of a stake middle chain comes with some baggage. I think it comes with the baggage of generally these protocols implement some proprietary crosschain gateway protocol, nothing wrong with that, but some other protocol that the chain or the team that owns the chain controls, I think having middle hops in network topology makes a lot of sense.
08:18:41.018 - 08:19:13.800, Speaker A: I think if you want to scale network topology and you have a lot of heterogeneous infrastructure, you do need middle hops. That being said, I think long term you still want to converge to a single standard, have middle hops in between. Obviously you want these to be secure as well. Ideally you want them to be trust minimized. If you can prove the execution of an entire path for a particular packet, then you can remove trust, minimization trust on these middle hops in the long term time horizon. So I think that the stake matters a lot now. But I think as technology advances, I don't think it will matter as much for security.
08:19:13.800 - 08:20:26.058, Speaker A: I think right now they're ticking time bumps partly because if you think about it, the whole idea is that economics will always make it, well, not profitable for a malicious party to basically exploit those chains. But as more and more chains connect to various chains, as hubs take like Axel or Polymor, which are trying to become hubs for IBC, it may just end up being the case. There's way too much value built on it that even the validators don't have enough economic incentives to keep the network as robust as possible. And the only way that changes is a world where the. Light clients they're using for passive messages from chain to chain are no longer necessarily just checking their validators to see if they provided the right signatures and more so, running complete state transition proofs of all the transactions happening on those chains. And at the point once you get to that particular level, they don't no longer need to be validated in chains, they can just become roll ups and do the exact same thing. So yeah, I think we're probably going to see a world where all the value based networks effectively start to do the exact same thing roll ups do in order to become robust or very secure or they're just gonna implode because the economic security no longer holds.
08:20:26.058 - 08:21:20.510, Speaker A: And I'm assuming you have some thoughts on this as well. Fig yeah, sure, I agree with what you guys are saying. For sure it's a temporary solution that will upgrade into a more trustless solution when the technology gets there. But for now we need a solution that we can connect the unusual chains into other networks which are maybe more interoperable with each other, like IBC. And then IBC is developing and over time it will get more capable. But for now we want to connect Ethereum to the Cosmos, for example, so that the Cosmos can get some users and build up business relationships. I think the economic security honestly is that's a ticking time bomb, but with the number of users at the moment it's more about I think there are so many other things to be worried about with security and bridges.
08:21:20.510 - 08:22:19.458, Speaker A: The Nomad hack for example, was just at the customer contract layout. In fact, I think the Salana hack was the Wormhole hack was similar, $300 million and it wasn't anything to do with economic security. But definitely Centralization is going to be the cause of a lot of hacks. And XLR having 75 validators compared to say, I won't name names, but in the five or six range, in a lot of cases it's completely different and so much harder to hack. And then these chains become like Polymer and Axla take on a different role in a future where everything is trustless. They can be routing hubs, they can potentially have a programming environment where you can build network logic for specific applications like gas networks for example, or they're a way that you can program into the networking layer. And yeah, that's it.
08:22:19.458 - 08:23:16.100, Speaker A: One tiny rebuttal on the point of, well, the number of validators being the main thing that sort of keeps things over alive. I do think it's a combination of the validators and the stakes. And when you compare most of the chains that do have these diverse sort of like validators, you still find that the stakes aggregate into some sort of like few hands just because, well, for the most part you probably find that it's probably the team that's running most of the validators on chains like Maxilar. I do think the bigger point though, that stands is for us to get to a world where we sort of are comfortable with these hubs. They're eventually going to have to change or switch to some variant of a light client system that depends almost entirely on state transition proofs as opposed to wealth of validators. That way we sort of like move the security away from the validators and the stakes to well, math and whether we're running the right computations. Are you in agreement, Jim, as well? Kind of.
08:23:16.100 - 08:24:24.698, Speaker A: I do think proof of stake or leveraging economic security is not as big of a ticking time bomb as my fellow panelists may think. It's a coordination problem, right? Like, let's say you move more economic activity than your underlying stake, which surprise, ethereum does, right? That's where decentralization matters. It's like, okay, let's say you're using vanilla tendermint or I guess Comet BFT, they call that now still a coordination problem for all these validators collude and actually have an erroneous state transition occur. But I do agree that we're probably going to move away from that model, not because it's a taking time bomb, but because it's just inefficient, right. Having a committee of validators run full nodes of God knows how many chains. Right? I would say you could probably, depending how beefy your nodes are, run 200 chains, which is higher than I think most people would probably say. But anything more than that, then it's not going to happen.
08:24:24.698 - 08:25:01.960, Speaker A: Yeah. So what are you saying here is that each of the validators would have to run a full node for all the other chains they support? Yeah, precisely. And I guess also incentivizing sort of developers as well. Could also be a problem too. We've heard a lot about CK Bridging, I think over the last couple of months and years. Do you guys have any thoughts on using CKPS in Bridging making either for trust minimization or for batching blockheaders or transactions into CKPS? I'll start mainly because I'm the least qualified, so I'll give the most high level opinion about it. I think they're important.
08:25:01.960 - 08:26:11.594, Speaker A: I think there's still a lot of open questions on what that looks like when you have heterogeneous proving schemes, when you have latency on posting proofs on chain and verifying them. But like what Richard was like, that is the end state, right? It's like we can't be verifying the consensus or said differently, we can't be snarking just the signatures of these validators of underlying execution layers, but we need to be actually verifying or proving the computation of the actual state transition occurring. Right. I think the solution to this kind of the open questions that I'm talking about is two pieces. One is there still needs to be a hub, right? It's like heterogeneity of all these proving schemes, all the additional roll ups. You probably want an approver aggregator, a proof aggregator, or like using some sort of recursive proof system in order to make sure that they become homogeneous. Right.
08:26:11.594 - 08:26:41.542, Speaker A: You probably still need a router. Honestly, you can't have millions of chains all talking each other in pairwise kind of permutations. Right? And then like what Bo was mentioning, snarking the actual path of that packet becomes really important. And then on the cost piece, I do think optimistic ZK right. I had a tweet. I was saying, I think everyone's talking about how off chain is the future, which is kind of ironic, but optimistic. ZK is interesting, which is where you snark something.
08:26:41.542 - 08:27:33.000, Speaker A: You don't verify it on chain, but you could verify it on chain and becomes like a one of N, like fraud prover kind of setup. I feel like you probably have more oh, I have a quick one. I don't have too much to say on ZK bridges other than excited for when they come live and we'll use them. But I wanted to correct Axela's team doesn't run any of the validators and it's a permissionless network that anyone can join and run a validator. That's the big difference between it and say, multi chain, which just got hacked a couple weeks ago, which literally it was an NPC protocol where the CEO owned all the instances running all the MPC nodes, which was insane. But yeah, the state middle chain approach is definitely more secure than multi SIG agreed. Yes.
08:27:33.000 - 08:28:28.440, Speaker A: On the topic of ZK clients, I would say that there's obvious performance issues that you want to work through and that will improve over time. On the front of proving the entire execution path is something I wanted to drill into. So one advantage of encoding some of this network topology on chain is that now you have a subset of the keys that are owned by each and every particular path. So you can shard proving of the execution of one particular path and separate that away from proving the path for communication between another set of smart contracts. And you can do this on a smart contract to smart contract basis. If you don't have this information on chain, it's very difficult to scale proving. Anything to add on that, richard as well? I don't know if I completely buy that, but I think I have to think about it more.
08:28:28.440 - 08:29:14.998, Speaker A: But I think, yes, light clients will be the final state interoperability. And I think coming to this event, I've realized that a lot more people think that it's further out than it actually is. I think most of the tools needed to build extensible light clients currently exist. It's just a matter of well assembling things like Lego blocks. And once we do that, we should be able to have Lite clients for most of the, I guess, popular networks and frameworks we currently use. So if we're in a future where all chains support Lite clients, do we just not use smart contracts at all and just Light clients for bridging? No, we will still use perhaps some optimistic system, partly because of the latency that comes from approving these chains. I think perhaps the end state of Bridging would be optimistic with Light clients for dispute resolution, if anything.
08:29:14.998 - 08:30:10.246, Speaker A: So imagine a world where someone well, states that something happens on the Chimp chain and some people just monitor this particular proof and if no one disputes it, then, hey, we're fine. And if someone disputes it, we check the Light clients to make sure that they're not lying. It makes more sense given that, well, until the hardware becomes really good and Approving software becomes really good, we're still going to be looking at perhaps three, four, five minutes, sometimes even up to twelve minutes worth of latency for passing messages from chain to chain. And people aren't that patient. I'm going to take a quick second to shill the work that we're doing at on since we're on the topic of optimistic ZK approach to Interoperability and forming that connection. We're working on optimistic ZK IBC connections at Polymer. So we have working Zkibc connections and we're working on overlaying optimistic verification on top.
08:30:10.246 - 08:31:36.454, Speaker A: And we expect to have that and hopefully testing in a few months. Cool. So we've talked a lot about how we can solve insolubability. So in a world, a dream world, maybe 510 years from now, what kind of applications are you looking forward to seeing? Like, what kind of cool applications can you build when you have 1000 seamlessly connected chains with relatively decent latency? I think a lot about the idea of borderless finance, where everything that we own becomes kind of aggregated within one single interface irrespective of where it lives. And this goes beyond an on chain context, right? And so my shares of certain companies that aren't doing too well right now, or my cash assets or my real estate, it's all in the same view as my crypto assets. And there is ample liquidity in moving between all of those in whatever action I want to prefer. And so Interoperability obviously is on chain right now, but I do think that there is a lot of research underway of how we kind of, whether through the use of CK snarks or what have you, bring real world assets into it as well.
08:31:36.454 - 08:32:09.578, Speaker A: And it becomes a lot more delightful, I think, to own financial assets in the meat space. Very cool. What about you, Chandra? I think when all is said and done, the main product that we'll see enabled by Interoperability will be some variant of a cross domain intent protocol. I think the end state of Interoperability will be the chains themselves sort of disappear. They only become a thing that the developers care about. Very similar to Web Two in many ways, right? Only you care about where you deploy your application. Only you care about where your software sits.
08:32:09.578 - 08:32:46.122, Speaker A: Users just care about them being serviced. And yeah, until we have that, we're probably not going to cross over that. Well, million or billion mark that everyone keeps on talking about when it comes to users. Yeah, I agree with that. We just need a way for people to build projects and be able to add. I mean, at the moment, I feel like the only use case we have is payments, but you can imagine also making the Web Two example. I'm really excited about how you can potentially bring AI into the intent system.
08:32:46.122 - 08:33:39.830, Speaker A: Like, we've actually built a proof of concept in Squid, and we're working with Dora, who are in the audience on it, and they've built a thing where you can just they're like a search engine, like the Google of crypto where you just type in what you want to do, and then it goes through the squid SDK and it finds, say you just want to mint an NFT. It finds the NFT somewhere, builds the transaction packet so you can buy it from anywhere. And you've got your NFT So you can potentially imagine a world where you have a Google, like, just a search bar, which is your entrance into getting anything or at least finding an application. And then Interoperability allows you to access that in one click from anywhere. I love what everyone said. Borderless finance is very important. Some of the early applications in crypto are financial and have priced out some other applications.
08:33:39.830 - 08:34:09.662, Speaker A: Me personally am very excited about gaming. I'm a huge nerd, and I love computer games. So I recently saw a post someone had tweeted it about having on chain, like Ovariant of Warcraft. If people are familiar with real time strategy games like Warcraft StarCraft, these are kind of like your agent empires as well. There's a number of them. These time are like classic real time strategy games, but having these on chain games and be able to approve it is very interesting. Use case to myself personally, I would love to play these games.
08:34:09.662 - 08:35:22.818, Speaker A: I would love to be incentivized to play these games as well. All right, very cool. I think we're running slightly beyond schedule, so I don't think this is correct. And I know there's a happy hour outside, so thank you very much for joining our great panelists today as well. Thank you. We got to pick up the spot like always. Sam.
08:35:22.818 - 08:39:21.550, Speaker A: Don't. Sam. Sam. Sam. Sam. Nam.
