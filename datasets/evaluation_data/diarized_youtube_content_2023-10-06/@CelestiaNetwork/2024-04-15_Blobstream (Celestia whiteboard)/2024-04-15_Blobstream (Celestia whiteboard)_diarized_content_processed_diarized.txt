00:00:02.000 - 00:01:41.504, Speaker A: Okay, so today in today's session, we're gonna go over what blobstream is, what celestials are, and we're gonna go into, deeply into explaining how you can create inclusion proofs with blobstream and how rollups can use what blobstream is. So blobstream is so traditionally what you used to have is that you had your execution layer and you had your settlement layer, which can be for example, an EVM chain, for example ethereum. So what would rollup do is that it would send its data to the settlement layer and also send all the merkle proofs to it. So for example, if you have here the signaling contract and this is the call data, so what's going to happen here in this execution layer, which is basically a roll up, is that you would be sending the Merkle route to the contract and also sending the roll up data to the call data and save them as cold data. However, with celestium, what you can do is that you can change this. So let's assume you have Celestia here. What we're going to do now if we want to use to create a celestium, we were going to be sending this roll up data here, let's call it data.
00:01:41.504 - 00:03:04.966, Speaker A: And here merkel root, we're going to be sending these, we're going to be sending this data to celestia. So now what happens is that we no longer need to rely on the call data in the settlement layer and the contract can be sure that the data exists on celestia via the bridge called blobstream. So basically what we need, basically what we are doing is that we are reducing the cost to operate your roll up your execution layer via sending the data to Celestia instead of committing to it in the call data. And here where we have our blob stream, here where blobstream is committing to celestia data and the settlement layer is going to use blobstream to read and verify the data was posted. So this is basically it, this is basically what a celestial is. You have your data, instead of sending it to your cellular layer, you send it to celestia and you verify that using blobstream. So the difference between like blobstream and data availability committees or centralized bridges is that you could be using like a data availability committee.
00:03:04.966 - 00:04:08.834, Speaker A: It's easier to use and all, but the thing that data availability committees don't give you is that it doesn't give you the ability to slash so you don't have a way to prove that your data was actually posted to the DAC or was not posted to the DAC. And also in case of something bad happens, you wouldn't be able to slash them. So this is where blobstream comes is it gives you two important features. The first thing is that it gives you an external consensus that you can use to slash in case some validator basically misbehaves. And also the second thing that it gives you is that you'll be able to verify the data yourself if you would be able to verify the data when using blobstream or even before you can use it, and that is using data availability sampling. So basically what you'd be doing is that you can verify that all the data exists without having to download most of it. So you'd only be downloading less than 1% of the data and be 100% like 99% sure that the data exists.
00:04:08.834 - 00:05:02.716, Speaker A: So basically what blobstream gives you is that it inherits all of the security guarantees of celestia and allows you to slash in case you didn't have access to the data. So now we can go to check how you can use how blobstream works on the inside. Let's assume that we have our celestia here, a celestia block. So this is a Celestia square that has data that has been posted to Celestia. So we can assume for example, that we have our roll up data here. It's been committed, for example to this place here. Celestia provides you here.
00:05:02.716 - 00:05:38.876, Speaker A: When you create the square, you have like row roots here. You have rows. This is for example, row zero, row one. And also we have columns like col one, col zero, col one. So the way that we create our data structure is that we put like the blob data in a place in the celestial square and then we commit to it. We commit to it using, first we start off by using an NMT, a namespace merkle tree. And the way that we create that is that we take for example, this row here.
00:05:38.876 - 00:06:52.294, Speaker A: Like for example, if we take this row here and we can have it, for example here where we have our roll up data here. So this is row one. So what we're going to do is that we're going to create a namespace merkle tree out of it to create the row root. The way we do that is like you can find like the namespace specification in the celestial documentation. And basically what we have here, what we're going to end up having is the row root one. Then we're going to do like the same for row zero and row two and row three, and also the same thing for column zero, column one, two and three. So once we create those, we can assume that we have them here, for example, and also the columns.
00:06:52.294 - 00:08:09.938, Speaker A: And then when we take all of these, then we're going to create a merkle tree, a binary merkle tree out of them. And the way we're going to do it is like how you basically create a merkle tree, and then we're going to create all the commitment for it to be able to have here what we call in Celestia, the data route. The data route basically commits to all of the data that has been posted to the celestial block. And for each block, you have a specific data route. So now, in this case here, if you, for example, if we want to take, if we take, for example, this chair here, which can contain, for example, your, I don't know, some, some transaction and all, and you want to prove that it exists on Celestia, you could have the data route, and then you're gonna go, you're gonna need, first you're gonna need this and this, and then you're gonna be able to have a Merkle proof from the, from this share to the route, and then you're gonna need this and this, too. And probably someone here, something here to be able to get to the data route. So once you have that Merkel.
00:08:09.938 - 00:08:46.294, Speaker A: Merkel proof, you're going to be able to prove that this transaction is part of the data route. So now when we have our data route, like, we can assume, like, we have multiple blocks here. So we're going to have multiple data routes. So let's assume we have here data root, for example, zero. So what we're going to do is that we're going to also add the height to it. So it's going to be height zero here. And then we're going to have, for example, data root one and height one.
00:08:46.294 - 00:09:26.194, Speaker A: Data root and height n. So let's assume, like, all of these represent some celestial blocks. Like, for example, this one is this one, and the others come from, like, the whole chain. So what we're going to do is that we're going to take these data root tuples. We call them data tuples because, like, it's a tuple containing the data root and the height. And then, and then we create a binary merkle tree out of them, something like this. And then what we end up with is something called the dataroot trooper route.
00:09:26.194 - 00:10:16.494, Speaker A: So basically, what this is like a dataroot tuple, which is this, and this is like the root of all of them. So if we take for example, this dataroot tuple, we can create an inclusion proof for it to this dataroot tuple route. And it's going to be basically this. You're going to need these nodes, and then you would be able to have a proof of this data root tuple to the dataroot tuple root. So what we have here is that once we create all of these, then we send this to blobstream, to the blobstream contract. For example, let's assume we have blob stream contract here, push maybe a bit like this. So what we do is that we take these data root trooper routes and then we commit to them in the blobstream contract.
00:10:16.494 - 00:11:20.942, Speaker A: So what happens is that if you have, for example, your transaction, if assume this is the roll up data and this is all the roll up data, and we have, for example, a single transaction that we are interested in proving that it was posted to Celestia, what we do is that we take, for example, this is the transaction, we create the first Merkle namespace Merkle proof from this transaction, from this share to the root one. And then we create a binary Merkle tree, merkle root, merkel proof. From this row root one to the data root. We are using these nodes, and then we take this like, this is the proof to the data route. And then we add to it the height, and then we create a merkle proof of the binary Merkel proof of this data root and height to the dataroot tuple root. And then once we have all of this, and this one is committed to by blobstream, we are able to prove that this share was part of the celestial block in blobstream. So this is basically how an inside inclusion proof would look like in blobstream.
00:11:20.942 - 00:12:43.874, Speaker A: And now we can go and see like how blobstream commits to all of this data. Okay, so we create a new page. So now at this level here, we were able to understand like how an inclusion proof would work from a single share in celestia to the data trooper route in blobstream. And now we can see how we can, like how would roll up, use blobstream to, to verify that its data was posted to celestia. Okay, so we can have here, for example, our blob stream. And we can assume, like for example, we have our roll up contract here. So the roll up contract is going to commit when, like after, like creating its data, its block, and then it's going to, after it sends it to Celestia, it's going to commit to like some information about Celestia that's going to be used to create like inclusion proof of blobstream.
00:12:43.874 - 00:14:09.722, Speaker A: And we can define here something called a span that we can use to create this proof, which can contain like a height, a data start index and a data length. And this is like, only if we want to, like, if we assume that this roll up contract is sending its data only to a single celestial block. But if it is sending this data to multiple celestial blocks, then you can generalize this to include multiple blocks. So now that we have these, then we would be able, like, for example, if you want to prove that the data is unavailable on Celestia, if we want to prove that, if we want to have a fraud proof, for example, to prove that this data was not posted or doesn't exist, that boils down to proving that this data is out of the square, celestial square. For example, if we assume that we have here celestial square and this is the erasure encoded data. So proving that the roll up data does not exist goes down to proving, for example, that this provided range here is, for example, out of the square. It's something here, it's out of bounds of the square.
00:14:09.722 - 00:15:47.330, Speaker A: And the way that you can do that is if you go back to the proofs that we just created, you can, for example, you can prove that you can create, for example, a merkle proof of, for example, this row root to the Dataroot super root. And what this would give you is that it would give you inside the proof, it would be sure that this row root was in fact, indeed like part of that celestial block at that certain height. But what it would also give you is that the numbers, the numbers of rows and columns which you can use to calculate like the size of the square. And once you calculate the size of the square, you would be able to compare it to the span here and be like, okay, so we have like, for example, a square size of 16, and then we have a start index of one and a data length of, I don't know, 60. So that means that this data span is out of bounds from this celestial block. And you can be sure that, okay, so this data was not posted to Celestia. And the second thing that you can prove, using a roll up contract, is that if you have like some invalid state transition that you want to prove occurred, what you can do is that you can go back here to these is that you can go back here and for example, if you want to prove that in here, like in here some invalid, I don't know, here some invalid state transition happened, is that you could create your merkle inclusion proofs from this share to the root, and then from the row root to the data route, and from the dataroot tuple to the data route.
00:15:47.330 - 00:16:39.150, Speaker A: And then you would be sure that this data is exactly what was described in this span, for example. And then you would be sure that this data is the roll up data. And then you can parse it, for example inside the roll up contract and then execute the logic to prove the transaction is invalid. So this is typically how a roll up would use blobstream. Then we can move to explaining how Blobstream X, an implementation of blobstream that uses CK proofs, works. So here. So basically what Blobstream does is that it takes the celestial block headers and it creates a ZK proof from them.
00:16:39.150 - 00:17:47.276, Speaker A: And then in the blobstream X contract it verifies that proof proving the gender meant lite client verification. And then you could be sure that if the proof is valid, then the celestial blocks were valid. And also what's committed to blobstream is also valid. So the way that works is that let's assume we have here a bunch of celestial blocks. So what would Blobstream X do? What would approver do is that it's going to take these headers and then create a blunky two x proof of them. The good thing about blunky two X proofs is that they are super fast to be generated and also they provide good recursion features. But the problem with them is that you can't verify them easily on EVM chains.
00:17:47.276 - 00:19:02.796, Speaker A: So what we do is that we take this proof and we wrap it inside a planky cg proof, which is easier to be verified on EVM chains. So once we have this blank kcg proof, then we send it to like we can assume here we have for example ethereum, and then we send it to blobstream contract to be verified basically via passing by some smart contracts like the gateway and the function verifier function id verifier. So once we send this proof here, blobstream X, like blobstream x verifies this proof here. That is like gender meant light client verification. And then once this verification is done, a data route trooperroot like we described earlier gets saved inside the blobstream x contract, and then you can use it for roll ups and all. I guess this is it. I guess this is it.
00:19:02.796 - 00:19:07.508, Speaker A: If you have any questions or anything. Okay, so why would you need to.
00:19:07.556 - 00:19:10.144, Speaker B: Prove things that are out of bound?
00:19:11.324 - 00:20:07.994, Speaker A: Well, yeah, well, if, for example, the roll up contract that we have some, that the roll up contains some data, and then it commits to a span that doesn't exist. So what the roll up would be having is that it would be losing its data. So if you have your chain and it doesn't have access to some part of its data, then that's bad. You would need to be able to prove that, to prove that in the roll up, so that you can slash or do something. Like, traditionally what you would do is that you had all the data being committed in cold Asia, so you could actually verify that the data exists and all. But since now it is part of Celestia, you would need to go via creating, like, an inclusion proof that gives us the square size and then verify that this data is actually part of the square size using blob stream to be able to be sure that the data exists and, like, the data is not missing. Yeah.
00:20:07.994 - 00:20:09.654, Speaker A: Can you just ask the question?
00:20:11.544 - 00:20:25.724, Speaker B: The original question was just trying to get at why we would ever want to prove that some data was outside of the square. And then I think the second topic that we could talk about maybe a little bit more was optimistic and ZK, how they actually use blobstream.
00:20:26.104 - 00:21:52.154, Speaker A: Okay, we can look at how we can do this. So, for example, if you have blobstream here and we have here a roll up, and we have here Celestia. So what's going to be here is the data is going to be sent to Celestia, and then here we're going to commit to that data, and here we can verify. So for an optimistic roll up, you can, you can get away with, like, whenever you send your data to Celestia, you could just create the span that we just talked about. And then once you have your span, then you are given like, a data pointer to people who would want to verify that the data is on Celestia, and that's it. But once someone doubts that, they can start verifying themselves, and if they find that some data does not exist, then they can create a fraud proof, verify it using blobstream, like we said, using the inclusion proofs and everything to be able to know if the data actually exists or not, and if that data contains some, like, invalid state transition for ZK rollups. They have like, wouldn't be worrying about like the integrity of the transactions inside, but we would be worrying about if the data actually exists or not.
00:21:52.154 - 00:22:25.318, Speaker A: So what ZK rollups would do is that they would create a ZK proof of their roll up state. And then once that is verified, they would also need to verify that the data was posted to Celestia and they would verify it using blobstream. So that boils down basically to prove into verifying that the height exists, the data commitment, the data route is correct and also proving that the span is pointing to valid shares and all. So this is how optimistic rollups and ZK rollups would use. Blobstream and Celestia.
00:22:25.486 - 00:22:34.314, Speaker C: From an implementation detail standpoint, can you explain where the information in the Dataroot triple route is coming from? For example.
00:22:36.494 - 00:22:39.430, Speaker A: You mean this one?
00:22:39.542 - 00:22:54.728, Speaker C: Yeah. Is the roll up running? How is the blob stream contract, for example, tracking the data routes and the heights to then be able to know how to prove which, which block it's included in.
00:22:54.856 - 00:23:55.552, Speaker A: Okay, so we can go over like this is how this, like, for example, we assume, like we have a certain, for example, we have a transaction here, we have some block data here that has been sent to celestia. So once this data here is sent to celestia, then that data is going to be put inside a square, inside a celestia square like this one. So we can assume, like for example, this is the, the roll up data that was just sent, all the transactions and all are in this place here. So once you have this, then you start creating commitments over this data. So basically, what the data tupper route attests to is that it attests like it is a 32 bytes string that you can use that commits to this data route on height and also other data roots and heights. And this one, for example, contains all of these row routes which in turn commit to the data. So basically what enblobstream is, is just this commitment.
00:23:55.552 - 00:24:41.754, Speaker A: And if you want to prove something exists or not, then you create a proof from this, from this, like Merkel inclusion proof, like using for example, these nodes here to create a proof to the, well, first these nodes here to create proof to the row root and then these nodes here to create the proof to the data route, and then these nodes to create the proof to the dataroot to perout. And that way, like if you have, if you manage to create this kind of, like this kind of structure of proof, then you can be sure that this data is, was committed to by the data retributor route. So what blobstream keeps track of is a bunch of data retrieval routes and it's up to you to create like the all the proofs or the proof that you something doesn't exist in it.
00:24:42.334 - 00:25:02.030, Speaker C: Okay, so how blobstream is like or what data root triple roots blobstream is tracking is basically just based on whenever there is a transaction that it's concerned with. I'm trying to think about thinking at a high level. Data routes are obviously created for every block in every header.
00:25:02.102 - 00:25:02.670, Speaker A: Yes.
00:25:02.822 - 00:25:18.904, Speaker C: And so I'm trying to understand what is the trigger trigger for creating a new data root truple route that Blobstream X would be tracking? Is it only when it is submitting a transaction that it's concerned with or is it on header receipt?
00:25:19.324 - 00:25:47.924, Speaker A: Well, actually we have here the traditional blob stream implementation. We were committing for every 400 blocks. So we'd be committing, if you have once blobstream it finds there is 400 blocks, then it would create like all of this for it, and then it would commit to it. But in the blobstream X implementation, we are doing like 60 minutes delays so that every 60 minutes you create a proof of everything that has been.
00:25:50.064 - 00:25:51.624, Speaker C: Preset interval.
00:25:51.664 - 00:26:02.734, Speaker A: Got you. So Blobstream X is committing since it's going to be committing to, since its deployment to the tip of the chain, like all of that, you could create proofs for it. Cool, thanks. Sure.
00:26:03.874 - 00:26:17.210, Speaker D: Is there any penalty for a validator to do a wrong relaying of the data root tuple route when it is posting to blobstream? Would it suffer any penalty for relaying.
00:26:17.242 - 00:27:14.066, Speaker A: Wrong information about root? In the old implementation, which we were actually dependent on validators to create the signatures for the data retrieval routes and then send in them, we would have liked to do a manual process of slashing them, but we didn't have implemented. But now with Blobstream X, you can't actually, if you assume that you created a correct blunky two x proof, blunky two x circuit, is that then you have your block headers and then you create a proof from them, and then this proof here gets verified here. So if you have an invalid like header or invalid signature, then it's not going to pass here, it's not going to be there. And if you have, for example, if like for example, some validator creates, for example, another block that contains, for example, two signatures and all, we should be able to, for example, take that signature and submit it in celestia and be able to slash them because, like, they equivocate. Yeah. Yeah.
00:27:14.090 - 00:27:22.414, Speaker D: I guess the second part I was answers the question, but how does it still relate from ethereum to Celestia? Because the bridge is one way.
00:27:22.714 - 00:27:34.810, Speaker A: Well, it's not, like, relayed from ethereum to Celestia. Like, you're going to have here. You're going to have to have that logic in the prover, for example, that if it finds like, a header with two signatures and all, then each would send it.
00:27:34.882 - 00:27:37.986, Speaker D: So it would be part of celestial core.
00:27:38.130 - 00:27:46.914, Speaker A: Well, it's going to be part of, it's going to be evidence. But, like, I would need confirmation from Evan and Ismael on that, maybe. Thanks.
