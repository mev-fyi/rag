00:00:00.410 - 00:00:07.582, Speaker A: At some level the storage requirements also go up with bandwidth requirements. So like how do you, how do you kind of combat that in the long run?
00:00:07.716 - 00:00:29.858, Speaker B: You care about that happening within a certain fixed time period, right? Within an epoch? Because all proof of stake chains have an epoch. If you go beyond the epoch leak keys can generate a fork, right? So you have some assumptions about like within this fixed amount of time, some, somebody's going to say hey, what the fuck? Hopefully.
00:00:30.034 - 00:00:47.740, Speaker C: I mean, it sounds to me like relying on the assumption that Moore's law will go on permanently, which I think is an assumption that has not hold true for all types of hardware over the past few years.
00:00:48.270 - 00:01:03.914, Speaker B: It has over the last 20 years, the amount of raw throughput, compute wise, that you can do on a chip, push through a nic, push through an SSD, push through the GPU that's been doubling at a steady pace.
00:01:03.962 - 00:01:04.858, Speaker C: But not for hard drive.
00:01:04.884 - 00:01:16.770, Speaker B: What we're talking about is not cycles per second on a single core. It's like raw bandwidth across an array of cores or dsps or whatever CUDA threads.
00:01:18.010 - 00:01:39.526, Speaker C: I'm pretty sure it hasn't held true for hard drives if you look at the data. But the other thing that's missing here, I think, I think you're assuming that you're ignoring the fact that people also have to sync the chain from scratch. You need to have people to be able to sync the chain and bootstrap from scratch.
00:01:39.718 - 00:01:47.870, Speaker B: Yeah, it's like 30 terabytes of transactions per year is what we're seeing right now. Dirty tb is not intractable.
00:01:50.050 - 00:01:52.046, Speaker C: And what's that going to increase to.
00:01:52.228 - 00:02:39.134, Speaker B: But you don't need the whole year, right? You need it within the epoch. So what is it over like two days, maybe 20gb or whatever? Is that so fundamentally difficult that I could do it, right and a very large number of users could do it. So is that assumption. My point is, is that assumption and that hurdle to actually do this verification, is it so much harder and such a big differentiator versus I have to have a light client constantly running. Plus I have to assume that there's an honest minority of light clients. So that's the difference, right? You can do that or you can rely on at least one out of n to provide you the data. So the larger the network gets, the more likely that one out of n is honest.
00:02:39.134 - 00:02:52.820, Speaker B: And this is where there is a clear difference between something like Salana or tendermint, where most tendermints run to like 150 or so validators. Salana is at close to 2000.
00:02:54.630 - 00:03:28.990, Speaker C: So I think we disagree on what is a large number of users. If we're talking about 30 terabytes a year, we're talking about reaching the stage where as a user, I have to effectively buy a whole bunch of hard drives, put it in my living room to verify the chain. Whereas what we're targeting is you should be able to do it on your laptop, you shouldn't have to buy specialized hardware just to be able to verify that the chain is valid.
00:03:29.890 - 00:03:34.640, Speaker B: You're participating in validation with an honest minority assumption, which is different.
00:03:36.050 - 00:03:40.730, Speaker C: Majority assumption is a much weaker assumption than honest majority assumption.
00:03:40.890 - 00:03:59.220, Speaker B: But like I said, we're not talking about honest majority assumption. We're talking about hardware costs and a one out of n assumption. I totally own that the user trading hardware costs, plus relying on at least one out of n of the validators to provide them the data.
