00:00:01.840 - 00:00:32.774, Speaker A: So, yeah, firstly, great to be at modular summit here today. I'll be talking about native intelligence through compute specific networks. Before I get started, a bit about my personal background. I'm the founder and CEO of Valence. Previously, before founding Valence, I was a research engineer in quantum finance. I was at a fund called two Sigma, where I worked on research and engineering for specifically us equity options. My personal background before that, when I was in school, is in computer engineering.
00:00:32.774 - 00:01:48.570, Speaker A: And I also spent some time doing ML research as well as HCI research. But before I dive into why we're bringing compute on chain and why that matters, the fact that compute happens on chain as opposed to off chain, I think it's important to go back to the root of the problem, which is why do blockchains exist and what problems do blockchains actually solve? So, blockchains exist today, and they're often touted as the technology that essentially allows for people to send money across the globe without any kind of friction that banks introduce. It's often seen as the world computer, the global settlement layer. It's a technology that is globally accessible and is also censorship resistant. And that's what gives a lot of value, right? Because I can send money to any random address, nobody can stop me from sending money, and it's valuable because of that. And today, with the advent of blockchain such as Ethereum, as well as Rops, people are putting more and more compute on chain. Instead of simply sending money to other addresses, people build things such as banks, lending protocols, automated market makers, and all of these protocols live on chain, and they have all of their compute verified by all the nodes on the network.
00:01:48.570 - 00:02:42.318, Speaker A: While that sounds really cool, that is actually not the case. If you look at a lot of the more popular protocols that exist. You have these protocols existing on chain with smart contracts, but you have all of the complex logic that actually power and optimize the protocol being done off chain. And the reason for this is because blockchains are replicated state machines. And what that actually means is that every single full node on the network has to replicate all the computation, all the transactions done on the network. And this makes it extremely inefficient for things such as routing orders, where you might need convex optimization, doing things such as auctions, where you have to embed a lot of off chain logic that allows people to submit orders and for you to settle orders at a specific price. That is optimized today with Uniswap X and a lot of other protocols.
00:02:42.318 - 00:03:46.254, Speaker A: All this complex logic is, you know, it happens off chain, you know, on some AWS server. And this is a problem because you now essentially are forced into this trade off, right? Where if you want to do compute on chain, going back to the problem that, you know, blockchains are solving, you, you are not able to do it efficiently due to the fact that the vms may be primitive. If you introduce a ton of compute onto the blockchain, it may be expensive, both from a computational gas perspective. Also, on an efficiency perspective, it's almost like choosing between two, I guess, not great presidential candidates. And on the other side of the other side of the coin, you have offloading all this compute off chain, which is currently how a lot of protocols approach the problem. You have two, again, ways to do this. Either you just say, hey, I'm just going to put some code on some AWS Ec two node, and you're just going to have to trust the compute that we're doing.
00:03:46.254 - 00:06:00.440, Speaker A: Or you can work with a ZK code processor, which is a technology that allows you to write these smart contracts and compile them into circuits, and then you can use these circuits to generate zero knowledge proofs that can then be verified by the blockchain. If you go the former routes, you now have this critical dependency for the protocol on some unverified code, which could be dangerous. But if you go the latter route, it then becomes very, very expensive for you to run this compute, simply because generating ZK proofs is expensive. So this kind of is a tricky catch 22 situation where you have to choose your trade offs depending on whether you want to make that trade off in terms of trust and do off chain compute, or do you want to put that on chain and make it, you know, an extremely primitive application, or whether you want to use something like a ZK coprocessor and have, again, a computationally expensive process that's done externally and brought on chain for verification. So what is the solution? How can we find a sweet spot that attempts to reconciliate all of these different trade offs into one solution that could be a potentially a one size fits all? So, compute specific networks is something we're currently exploring, along with some other teams, where we are able to take the advantages that off chain compute brings, that is efficiency, scalability, and how cheap it is with the fact that we want to have on chain verifiability and flexible ways of securing that compute, and have also importantly, a very integrated developer experience where they don't have to work with ZK coprocessors, write smart contracts, or write different pieces of code that might live on many different execution environments or many different nodes. So a bit more about compute specific networks is they will allow you to be able to build all these applications on chain that are able to do much more sophisticated things. Instead of offloading this compute off chain, you could do dynamic fee calculation on chain for amms.
00:06:00.440 - 00:07:41.862, Speaker A: You could have risk models that are able to adjust LTV ratios and letting protocols for risk management. You can do portfolio management and optimization directly on chain and have much more complex logic embedded into your smart contract. And this would allow for obviously much more complex use cases if you're able to do on chain compute from the user experience, which is intent driven execution of transactions or programs to optimize applications using machine learning all the way down to optimizations you can make on the infrastructure layer. There are many different ways we can use more sophisticated and powerful compute on chain to build the next generation of web3 applications. But now begs the question, how sure, you can have a network that is able to do more sophisticated compute in a way that is trustless, in a way that where security is flexible, but how do you actually build this network and make it happen introducing compute specific networks? So we kind of invented this novel architecture that has to do with node specialization wherever we essentially silo validation as well as computation into two different types of nodes. So you have the regular full nodes that are stateful, that are in charge of validation, and you have these specialized beefy nodes that are stateless and in charge of computation. So a regular blockchain does not have any of these nodes, it just simply has a sequencer and regular full nodes.
00:07:41.862 - 00:09:16.728, Speaker A: But with these stateless nodes that we introduced to the network, we are able to essentially offload the expensive compute that the blockchain network normally has to do to these beefy nodes in the name of specialization, and these nodes are in charge of doing the heavy compute, and in addition to that, generating any specific proofs, whether they are tee remote attestations or whether they are ZKML proofs, and essentially propagating it back to the sequencer and the rest of the network where we have validation being done. So a high level explanation for the kind of logic we've embedded into the architecture is that we have more centralized computation, but extremely decentralized validation of the proofs. And this also allows us to take advantage of heterogeneous compute architecture. We can have many different types of stateless nodes, we can have gpu's designed to do inference, we can have trusted execution environments designed to execute a certain piece of code with a model cached locally. It allows us to take advantage of the different benefits that different hardware architectures can bring. This is nice because we can embed now a load balancer into the sequencer that allows us to intelligently route these compute workloads and these transactions to whatever hardware or whatever node is most suitable for it. In addition, because we have heterogeneous compute, we can also have what we call heterogeneous security as well.
00:09:16.728 - 00:10:08.576, Speaker A: In addition to just being siloed to using ZKML, which is what a lot of ZK coprocessors would be, that's the only choice they have when it comes to true verifiability. Because we have different types of nodes, we can also use Tees and using remote attestations to verify the compute. We can also do what we call optimistic ZK as well, which is because these are all nodes on the network. A stateless node can make a commitment and say this is the results I got from running some compute job. And essentially since they are on the network, they participate in staking for the AV's that we are running. And we can, you know, optimistically generate a CK proof if someone submits a challenge, right? If someone says, hey, I think the compute you did might be incorrect. Now we prompt the inference nodes to actually, you know, generate a proof.
00:10:08.576 - 00:11:03.274, Speaker A: And depending on the challenge frequency, you know, this could reduce the compute overhead by potentially 80, 90% or even higher. So this essentially allows us, the TL doctor would be to not just use cryptographic security, but also use cheaper crypto economic security. That might make more sense and many more use cases. So a brief comparison between bringing compute on chain with compute specific networks, that is a blockchain designed to do a certain type of compute compared to off chain. So if you have a compute chain, you essentially are able to choose very flexibly from different types of security modes. And that's nice, because depending on the compute load, depending on the job, depending on whatever you're trying to run, this could make sense. Secondly, it allows you to not have to configure your own architecture.
00:11:03.274 - 00:12:10.310, Speaker A: You don't need to run your own node at home to be able to run some compute job. All you need to do is submit some job onto the network and some node on the network will take care of it for you. Thirdly, all the transactions and proofs on the compute chain are stored and recorded to a data availability layer, which allows for provenance of the verifiability and the proof. Anyone can query the DA layer and actually verify that some compute was done. And lastly, being a chain, we are able to have some notion of state, take advantage of view functions that might expose on chain data, use oracles and all the existing tools that are nice to have today. But the more pertinent question here really is what is the difference between a compute chain and a coprocessor when it comes to a ZK code processing? What they essentially do is they take some compute job, they compute it generate a proof that you can then export to some blockchain where all the nodes verify because proof generation is expensive and proof validation is cheap. But then you create this bottleneck where if your job is extremely computationally expensive, it might cost you a ton to run this job.
00:12:10.310 - 00:13:09.578, Speaker A: In addition, it might also take a reasonable amount of time. Compute chains with our existing architecture that I've walked through allows us to switch between cryptographic and crypto economic security, allows us to choose your own adventure, choose your own security depending on your job, and also allows us to run this in a permissionless, decentralized fashion. Thirdly, like I mentioned in the architecture slide allows us to take advantage of different types of security modes as well. That ultimately is a flexible solution that might be a one size fits all. Lastly, again, we also have composability between smart contracts. I can build a smart contract that interrupts with some other contract that exists on the chain and composably build more powerful use cases off of what other people have already built. So our first product that we are unveiling is a network designed for inference compute without diving into the architecture.
00:13:09.578 - 00:14:59.660, Speaker A: This is kind of like a slightly more detailed version of the dumbed down architecture diagram I showed earlier, where we have these stateless nodes here that do the heavy inference compute, the stateful nodes here, the core blockchain network where we have a load balancer embedded into the sequencer that's able to offload these jobs to these nodes here that then do the compute, generate any proofs and attestations, and then send it back to the sequencer where the batch poster posts this to DA. So in this case, we have beefy nodes doing all the work, cheap nodes doing all the validation and all the verification, ensuring that the beefy nodes are doing their jobs correctly, and a DA layer like Celestia that could potentially help ensure that we are doing everything correctly. Anyone could query the DA and verify that we're doing the compute honestly. And you know, in addition to simply just inference, we're embedding a lot of stateful pre compiles that can help make these model driven workloads even more powerful, right? Ranging from data pre processing like standardizing data feature encoding, all the way to running simple regressions, or running simple correlation tests and mutual information tests between different features, to statistical tools that could be helpful, like z scoring or calculating variance of some price series, all the way to, of course, inference, right? These are all precompiles we offer on our network that help make, you know, anyone who wants to build something more sophisticated, like a model driven workload, more powerful. And kind of, this is, this kind of embodies the general thesis that we have for compute chains, which is the fact that, you know, while the network is decentralized, the developer experience is extremely integrated. You don't need to worry about your own GPU compute, your own hardware. You don't need to worry about hosting your own model somewhere and running your own server.
00:14:59.660 - 00:16:06.638, Speaker A: You don't even need to worry about proof generation and also validation of those proofs and deploying contracts to validate it. All that is done and completely abstracted away by our architecture. You don't even need to know what ZKML is to be able to upload a model and run ZKML secured inference. And that's how easy it needs to be for us to be able to adopt these newer technologies and this more powerful compute to empower applications. We want to minimize the development complexity when it comes to something like bring ML models on chain. And something we've recognized is that while this infrastructure is still nascent and we're building it, there are a lot of research opportunities for us to actually just dive deeper into what exactly makes sense for web3. How can we use ML and more powerful compute to actually empower applications so they can actually benefit users? So we've worked on many different types of initiatives internally, including a dynamic fee model that does fee calculation for AMM swaps.
00:16:06.638 - 00:16:53.280, Speaker A: And we're currently working on that research with Uniswap. And we also have many other initiatives internally that we're working on. And we are also collaborating with a series of other decentralized AI projects to build what we see as kind of the next generation of web3 AI primitives. As an example, risk models that can help manage risk for CDP, or lending protocols, or models that can help optimize vaults. So when volatility reaches a certain level, they can pull liquidity from AmM pools to de risk. So if you're interested, please do reach out. We are working on a lot of cool, interesting research problems and would love to talk to more people and kind of with compute specific networks becoming a thing with flexible security that is made super accessible to any developer who wants to develop anything on our platform.
00:16:53.280 - 00:17:49.740, Speaker A: This allows us to enable what we call native intelligence instead of having to plug in an off chain coprocessor or have to build some off chain program that does routing for your transactions. We're able to embed this all directly on chain in a smart contract and it allows us to do things like optimize DeFi, introduce risk management all the way to building on chain AI agents, to potentially making infrastructure today smarter by building reputation models for deep ins as an example. And yeah, we are very excited to be continuing down this route of not just building infrastructure for compute specific networks, but also kind of getting into the research ourselves. Right. And building these models and doing the digging the difficult stuff that is allowing us to build more powerful web3 applications that will define the future of web3. So yeah, thank you so much.
