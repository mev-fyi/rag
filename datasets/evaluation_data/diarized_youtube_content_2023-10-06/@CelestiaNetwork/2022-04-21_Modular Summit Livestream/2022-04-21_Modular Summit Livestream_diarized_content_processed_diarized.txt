00:00:00.490 - 00:00:26.120, Speaker A: It's kind of suit your show and tell. Yeah. Whoa. Loving in your eyes that pulls me closer it's a sub. I'm in trouble, but I love to be in trouble with you. Let's mom and Gay and get it. Out it.
00:00:26.120 - 00:01:28.470, Speaker A: You got the healing that I want. Just like they say from the song Silver Dark you got to give it up to me. I'm free of mercy. Mercy, please. Just like they say in the song still. I don't and when you leave me alone I'm like a stray without a home I'm like a dog without a phone I just want you for my own I got to your babe it's a bottle. I'm in trouble.
00:01:28.470 - 00:04:01.000, Speaker A: But I rather be in trouble with you smiling gay and get it on I got that healing that you won't like to say in the song until the dawn get it on let's move again you got that healing that I want just like they say you got to give it up to me I'm screaming please just like they say until the dark just like they say in the song until the time was falling down get it? All sitting on the top of the roof the bridge is all mine steam engines roll by the bridges fall down and so do my dream boy you hear me calling your name the bridge is your time your engine rolls hot if the bridges fall down don't lose your head of steam young man, I'm counting on you and whoa young man, I'm counting on you boy I didn't make it too far but, baby, you are the family star? I'll tighten your seams? Don't lose your head of dreams, boy, you hear me calling your name? The bridge is your time? Your engine rose hot if the bridges fall down? Don't lose your hell steam young man, I'm counting on you and whoa. Young man, I'm counting on you to get me to the other side. Hey, hey. This footprint pressure is new. Whatever you do is up to you. But do me this. Do.
00:04:01.000 - 00:07:09.020, Speaker A: Don't lose your head of dream young man, I'm counting on you it young man, I'm counting on you and whoa young man, I'm counting on you and whoa young man you are never man, whoa only young man I'm a cow. You are now the man. Can you get me to the other side? Can you get me to the other side? I don't lose your head of steam don't lose your head of steam oh, I know you moved on to someone new hope life is beautiful you were the light for me to find my truth I just want to say thank you. Leaving to find my soul? Told her I had to go? And I know it ain't pretty when I hawk it? Too young to feel this old watching us both turning cold. Oh, I know what you hawk it I. Hope sit down together and left with each other by these days? These days all our trouble we laid the rest and we wish we could come back to these days? These days all these days these days from these days. These days.
00:07:09.020 - 00:11:15.644, Speaker A: Three years of up and down. Nothing to show for it now. Alano ain't pretty when the father calling me when I'm drunk. Remind me of what I've done. I don't know. It ain't pretty when you try to move on. I hope you never sit down together and left with each other by the sam sam lam om SA ramit.
00:11:15.644 - 00:11:44.610, Speaker A: What an incredible turnout we have. And we're expecting a lot more people, so this room should fill up in a few hours. I'm one of the MCs for today Aditi. I work at Celestia. This event is about mapping the next generation of blockchains. Since the agricultural revolution, humans have relied on the specialization of increased efficiency and productivity. Blockchain is no different.
00:11:44.610 - 00:12:19.140, Speaker A: Modularity is becoming a huge paradigm shift in the way we'll build and scale blockchains. And today we'll have representation from all the modular players in the ecosystem, including roll up teams, bridges, other L1s and more. I'm excited to kick this event off and I'm going to pass it off to Balder to do a proper intro. Thanks, guys. A little late, sorry. Yeah. So hello, everybody.
00:12:19.140 - 00:13:01.282, Speaker A: A very, very warm welcome here in Amsterdam. I think everybody has so far has had more than amazing week, right? I'm very surprised by the turnout this week during DevConnect and also the quality and the amount of people. Good lesson. We shouldn't start a crypto event at 930, right? That's clearly a little too early for a lot of people in the crypto scene. And also what's great is that I think Amsterdam weather is slowly sort of rug pooling the crypto scene, right? I mean, we've never had this weather and now for a full week we've had 20 degrees and sunny, sunny skies. Normally it's like a lot of clouds and very bad weather here. So great to see come over more often.
00:13:01.282 - 00:14:22.780, Speaker A: We're happy to host here, very happy to kick off this amazing event. Celestia team and ourselves, Maven Eleven have been working around the clock last eight weeks to get you set up in this amazing location and start chatting about modeler designs, modularity, optimistic execution environments, serial knowledge execution environments like the DT set bridges. And Celestia and Mavenlevan team go quite a long way back. We, Matais and myself, I remember very vividly, we read Mustafa's paper at the time called actually Lazy Ledger for the OGS in the room and we were very excited about what he has obviously been doing at the time. So we go almost three, four years back and I think I want to thank Mustafar for all the time and hours he spent with us in making us understand how this data availability and modularity should look over time. So without further ado, I would like to get Mustafar in her stage and I wish you a great day from the Maven Eleven and Celestia side here today. Hello, everyone.
00:14:22.780 - 00:15:20.030, Speaker A: Quick little background about myself. As was mentioned, I'm co founder at Celestial Labs, which is a modular data availability layer. Previously. Before that, I was doing a PhD at UCL where I was focusing on layer one scalability and I worked on the first smart contract sharding research paper called Chainspace, which was later kind of spun out into a startup that was acquihired by Facebook. But I was the only team member that did not join Facebook and I ended up working on Lazy Ledger and Celestial instead, which I think was a very interesting alternative design to blockchains. So I'm going to talk about three main points today. The first point is we're going to actually set the record straight on what is a modular blockchain stack and define it clearly and what it is not.
00:15:20.030 - 00:16:44.160, Speaker A: And then we're going to give some examples of configurations of blockchain stacks and we're going to talk about the key benefits of modular blockchains. So when Bitcoin was first proposed in 2008, it proposed a model where it was a monolithic blockchain where effectively the full nodes and the validators, they do consensus on the chain and they also execute the transactions on the chain, so they provide consensus and computation on the chain. And for the ten years after Bitcoin, most blockchains followed the same model. So when Ethereum came around in 2013, it followed a very similar model to Bitcoin, except that it replaced the execution environment of Bitcoin with a general purpose smart contract environment known as the Ethereum virtual machine. But the actual architecture of the blockchain was similar architecturally in that it was monolithic and full nodes did the same general tasks. But obviously the problem with this approach is that and full nodes to verify the chain have to do effectively two things. The first thing is they have to check if the chain has consensus.
00:16:44.160 - 00:18:02.770, Speaker A: And the second thing they have to check, they have to actually process and execute every single transaction to check that all the transactions are valid. And that obviously doesn't scale and is quite limiting because obviously it doesn't scale if full nodes have to execute the entire state and transaction history of the network. But if you think about what is the most basic blockchain you can create, like what is the most minimal layer one? If you took Bitcoin or Ethereum and stripped it back to its core components, what would you get and what is the core functions of a layer one? If you remove the execution from a layer one, what you would get is a blockchain where full nodes do two things. The first thing is consensus. They take transactions and they order them. And the second thing is that they ensure that the data of those transactions are actually available. And the reason why you want to ensure that the data of the transactions are available is because that's all you need to build an application on the blockchain to allow clients and users to know what's on the blockchain to compute the state of the application.
00:18:02.770 - 00:19:37.340, Speaker A: And I wrote a paper about this in 2019 called Laser Ledger, which was the former name of Celestia, where I proposed a paradigm for blockchains where the blockchain is only responsible for consensus and data availability and all the execution happens off chain, which I call client side smart contracts or client side or off chain execution. And that's basically the same model of how roll ups work. So for the off chain computation you can use a roll up. ZK roll ups were proposed in 2018 and optimistic roll ups was proposed three months after I released Laser Ledger paper. And in a roll up paradigm, the roll up execution happens off chain but you're only using the layer one chain only for data availability and consensus, which is exactly what the Laser Ledger kind of paradigm does. So in this kind of model, what you're effectively doing is separating consensus and execution. Whereas previous blockchains like bitcoin and ethereum nodes did consensus and execution, in this new model, layer one nodes only do consensus and data availability and the execution happens on a layer above, above the stack off chain, not on the layer one, either as a roll up or as a different kind of L2 solution.
00:19:37.340 - 00:20:52.640, Speaker A: And this actually makes a lot of sense if you think about it. So in computers you have this thing called the OSI model that proposes a modular kind of like stack for computing, where at the bottom of the stack you have the physical layer, which is the actual cables and wiring and hardware of the Internet or the network. And then if you go all the way up the stack, you get to the application layer like Facebook and Google, but in between you have all of these different layers that can be swapped. You know, I think blockchains are kind of shifting through this paradigm because it makes a lot more sense. And I'm going to talk about why. So let's talk a little bit about what are the actual layers in a modular blockchain stack once you decouple them from a monolithic blockchain. So the first layer is consensus and consensus the consensus layer is simply responsible for taking arbitrary data or arbitrary messages and then providing an ordering over those messages.
00:20:52.640 - 00:22:05.230, Speaker A: So a developer would submit a bunch of messages to the network and the blockchain or the consensus layer would tell you in which order those messages are serialized. And at a fundamental level that's what consensus does. Then above that you have data availability. And what data availability is trying to solve is once the kind of consensus nodes or the consensus layer has decided or determined what the ordering of those messages should be, they usually commit to that as a merkel route or any kind of other commitment. But they also need to actually tell the network what the actual data in those messages are, because if they don't actually publish the data that they agreed on, then users and applications would not be able to know what the state of their application is because they don't know what the transactions are in their application. And they wouldn't be able to generate things like fraud proofs. In the case of optimistic roll up, for example.
00:22:05.230 - 00:23:32.600, Speaker A: So with data availability there's some cool techniques you can do such as data availability sampling which lets you verify that the entire data is available by only downloading a very small percentage of the data. Then the third layer is the execution layer and the execution layer takes the messages that have been agreed on, ordered in the blockchain and have been made available and then does some computation or processing on those messages. So in this example on the board you can see you take a bunch of transactions in and those transactions might be payments and then once you actually compute those payments you can know what people's balances are. And in the execution layer nowadays people typically use roll ups, whether optimistic or ZK roll ups. And you can prove to users the execution was done correctly by either using ZK proofs or fraud proofs. The final layer, which kind of sits in between the execution layer and the consensus layer technically is a settlement layer. A settlement layer is basically like a special case of execution layer.
00:23:32.600 - 00:24:52.880, Speaker A: It's basically an execution layer that bridges other execution layers together. So, as I mentioned, you would typically prove in a roll up whether a computation was valid using a fraud or ZK proof. A settlement layer basically verifies there's fraud or ZK proofs on the settlement layer itself and provides dispute resolution on it. And that would basically allow different execution layers and roll ups to bridge with each other and transfer assets with each other in a trust minimized way. Now, having listed all of the kind of main components of a modular blockchain stack like how do we define what a modular blockchain is? So I define a modular blockchain as a blockchain that outsources fully at least one of the four components. It does not handle that component. So for like Solana wouldn't be a modular blockchain if you just added roll ups to Salana because Salana's l One still has a smart contract environment and its validators are doing data availability, consensus and execution and they're not specialized in a specific task and therefore it's not modular.
00:24:52.880 - 00:25:51.120, Speaker A: So what is not a modular blockchain? So there's often some confusion on this. A blockchain that handles all the components but has a modular software design is not a modular blockchain. A modular software design can be helpful to build modular blockchains. Like we use tendermint. The ABCI construction in Tendermint is very helpful to build modular blockchains but deploying a blockchain using this modular software library does not make that blockchain modular in itself. And secondly, a network of blockchains where each blockchain in the network handles all the components is also not a modular blockchain. For example, avalanche subnets are not modular blockchains because each chain in that subnet in the network handles all the components that I just described.
00:25:51.120 - 00:26:51.400, Speaker A: Therefore it's not modular. Let's go through some examples of what a modular blockchain stack could look like. So, as I mentioned, there's these four components data availability, consensus, settlement and execution. So you have monolithic blockchains like bitcoin or ethereum as it is traditionally where you basically have a general purpose smart contract environment and the validators and the full nodes handle all of those four components. Then you have roll ups. And in roll ups the L1 handles the first three components data availability, consensus and settlement but not execution. And the roll up itself handles execution of chain.
00:26:51.400 - 00:28:06.076, Speaker A: And then you have validiums. And validiums are basically roll ups but they do not have on chain data availability or they do not use the same layer one as the settlement or consensus for data availability and therefore they are not roll ups but more like side chains or balidiums as they're termed. So this is kind of like the kind of modular design in the ethereum space that has been kind of like discussed so far. But then there's also more of a celestial centric modular design or configurations of the stack. So in Celestia we're quite interested in this idea of sovereign roll ups and in a sovereign roll up, the sovereign roll up uses the L1 only for data availability and consensus but it does not have an enshrined settlement layer. Instead it does its own settlement. And the reason why it's sovereign is because it can hard fork and it can upgrade without permission from a higher execution layer.
00:28:06.076 - 00:29:08.452, Speaker A: So for example, if you have an ethereum roll up, your roll up is effectively like a baby chain to the ethereum settlement execution layer and you can't really hard fork it without convincing the ethereum kind of social consensus to do so. But with a sovereign roll up because it does its own settlement, it's effectively like its own layer one chain. It's effectively like deploying your own layer one. You can hard fork it and it can have its own social consensus. You've also got this ideal settlement roll ups. A settlement roll up is basically like a standard roll up except that it's only optimized not for general smart contracts or computation, but for settling other roll ups on top of it. And I think Yuri is going to give a talk about this today with this idea of L1s, L two S and L three S.
00:29:08.452 - 00:30:36.100, Speaker A: So, for example, you can have an L two that has L three S, but the L two might only be might only be supposed to be used for settlement for other roll ups. But you're not supposed to host actual applications on that roll up. And then you've also finally got this idea of a celestium which is basically a validium that uses Celestia for data availability on paper has similar security trade offs as a validium, except that it has slightly higher crypto economic guarantees because of Celestia's data availability, sampling and slashing. Here's the different kind of current players in all of these different stacks. As you can see, there's currently a higher focus on the execution layers in this stack. And I think that makes a lot of sense because there's a lot more kind of innovation or divergence that can happen on the execution layer of the stack than the lower layers which kind of do more of a kind of simpler role, simpler but more but important role. Let's talk about some of the benefits of a modular blockchain stack.
00:30:36.100 - 00:31:48.200, Speaker A: So first of all, obviously the big one scalability. Intuitively modular blockchain stacks are more scalable because there's a separation of resources and that means each node in each layer of the stack can be more specialized to a specific function. So for example, on the data availability layer the nodes don't do any computation, they just do data availability. And that means they can optimize their resources to having high bandwidth resources rather than computation and they can be specialized just on that one task. And the main benefit of this kind of comes from the resource pricing. You're separating the resource pricing for different resources on the network like you have different resource pricing for data than computation. And finally, in a modular blockchain stack it's quite common to use technologies like data availability sampling and fraud proofs or ZK proofs.
00:31:48.200 - 00:33:02.290, Speaker A: And this is very important for scalability because in order to scale blockchains you can't just increase throughput. You also have to increase throughput while enabling users to still gain assurances about the correctness and validity of the underlying chain. And in traditional blockchains the only way you can do that is if the users actually reexecute every single transaction, which most users cannot do. But with technologies like data availability sampling and fraud proofs and validity proofs, it allows ordinary users to effectively be first class citizens of the network and have almost the same level of security as a full node. That is actually downloading all the transactions without having to have the same resource requirements as a full node. Secondly, flexibility is a major advantage of modular blockchains. You can see there's a Cambrian explosion of different execution layers that are innovating in different ways in the ethereum roll up space, for example.
00:33:02.290 - 00:34:17.720, Speaker A: And these different execution layers have different advantages. For example, fuel's execution layer is paralyzable, for example. And this is very important because if you think about the history of the web before the cloud or virtual machines were popular, people used to just use shared web hosting providers like DreamHost or GeoCities. And that really limited the innovation of the web because you were limited to whatever execution, environment or programming languages that that host provider had on their server. But nowadays no one uses that. Nowadays people just deploy virtual machines on Amazon EC Two and they effectively have their own operating system and they can install whatever they like on it and experiment with all kinds of different programming languages and technologies. Finally, I think this is more celestial specific but if you think about the different layers in a blockchain stack you have layer one which is commonly known as the consensus layer.
00:34:17.720 - 00:35:22.300, Speaker A: But the consensus layer only has value because people agree that it has value. Like people agree that the current Ethereum chain or the current bitcoin chain has value as opposed to some fork of Ethereum because people have just agreed it by social consensus. If I fork Ethereum, no one's going to say that's the real Ethereum. So to define the real Ethereum, that requires social consensus. But applications on shared smart contract platforms and smart contracts do not have that same property. Instead they kind of borrow the social consensus of the layer one that they're using or the execution environment they're using. So if you have a smart contract in Ethereum you can't fork it without convincing the social consensus of Ethereum.
00:35:22.300 - 00:36:33.240, Speaker A: To some people that's a feature, but to others that's a bug. But with sovereign roll ups you can actually have your own layer one execution environment that your community can hard fork and that it therefore has its own social consensus. I think this is a very interesting kind of model. To me, the whole point of blockchains is that it's basically a social coordination mechanism for off chain social decisions that have been made. I think there's kind of three main values from my perspective of a modular blockchain stack or blockchain or the idea of blockchain modularism in general. Firstly, users are first class citizens of the network thanks to technologies like data availability, sampling and ZK proofs and fraud proofs. Those technologies allow users to have the same level of security as a full node or similar level of security as a full node without needing the same resource requirements and hardware as a full node.
00:36:33.240 - 00:38:33.140, Speaker A: And so you're allowing end users to actually be first class participants of the network. Secondly, I think a second important value is this idea of layer one fighting and layer one maximalism is kind of getting old. And I think modularism is much more interesting than maximalism because it's not a zero sum game. And if you allow developers should have the freedom to build their application how they want according to their use case and the more players there are in this modular blockchain ecosystem the more value there is to be created. And finally, I think an important value is that if they want to communities can choose to be sovereign by deploying sovereign roll ups. And the main difference with that, with deploying your own L One is that it's much easier to deploy a sovereign roll up than it is to launch your own new layer one chain with its own consensus because you have to bootstrap a secure and decentralized, validator set using something like proof of stake. But with sovereign roll ups, communities for the first time have the ability to create a sovereign blockchain very quickly, within minutes without having to worry about the overhead of maintaining and creating their own consensus network with a secure, validator set because they can effectively have shared security with the data availability layer without losing the sovereignty of their execution layer.
00:38:33.140 - 00:39:57.590, Speaker A: I think the reason why I think it's so important to kind of move towards a modular blockchain stack is because over the past ten years we've kind of been stuck in this kind of cycle of, like, layer one chains being created all the time because the previous layer one chains could not scale. And you can see this is not ending well because all of the chains here have also run into performance issues and gas and transaction fee issues and uptime issues. So clearly, we kind of need to escape this cycle of new layer ones with something that actually kind of works. And I think the best way to do that is to have a modular blockchain stack and use technologies like roll ups and ZK proofs and fraud proofs and data availability proofs. So if you want to free yourselves from the kind of limitations of monolithic layer ones, then build modular and gain your freedom. Thank you, everyone. Do you want to take a question? Oh, yeah, we have time for questions.
00:39:57.590 - 00:40:37.110, Speaker A: Cool. All right. Thanks, Mustafa. All right, next up we have Alex Evans, who is a partner at Bain Capital Crypto. He will be talking about why modular is now a central thesis in their new fund. So, Alex, take it away. Do I have a clicker? Oh, I think Mustafa took it with it.
00:40:37.110 - 00:41:08.264, Speaker A: Thank you. I think I'm missing a slide, but hello, everyone. I'm Alex Evans. As the DT said, I'm one of the partners at Bain Capital Crypto. We're an early stage crypto focused fund. We spend a lot of our time on research protocol, design, engineering. We have a particular obsession with modular architectures and blockchain designs.
00:41:08.264 - 00:42:02.716, Speaker A: And I guess in this presentation, I want to share maybe some of that excitement with you. In particular, I want to talk about some of the scalability properties that Mustafa sort of alluded to carefully in terms of what types of scalability modular blockchains enable and what types of scalability they don't. And I want to spend a little bit of time talking about flexibility and the acceleration of VM and execution environment design innovation. Where I want to start is not by defining monolithic versus modular blockchains. I think Mustafa did a great job doing that. But just to point out that the concepts of monolithic versus modular exist along a spectrum and that most blockchain designs are positioned somewhere along that spectrum. Most smart contract platforms as we understand them today are becoming or at least adopting some of the ideas of the modular paradigm over time.
00:42:02.716 - 00:42:53.852, Speaker A: So in some ways you could see them becoming more modular. And that, I think, paints that world in less of a binary perspective and more in one where these ideas of separating out DA from execution are permeating into the broader blockchain space. And I guess the argument for this talk is that there's a very good reason for that. And so the reasons for that are, number one, scalability, the ability to have greater performance in our systems as well as the ability to support more users while still allowing them to be first class citizens of these networks. And the second one is that modular designs unlock rapid experimentation across the stack in terms of DA layers, in terms of execution layers and so forth. Splitting up these monoliths into components allows us to experiment on each one and iterate on the best possible designs. So I'm going to take each of these in turn.
00:42:53.852 - 00:43:41.020, Speaker A: I'm going to start with scalability. It's a little bit of a pedantic approach to scalability just to make sure that in the time that we have we cover it precisely because there's a lot of confusion around what scalability means in these ecosystems and so forth. Just with a very simple rough definition of scalability. We want to increase the capacity of our systems to handle more transactions cheaper. But we want to do so in a way where users are still able to catch up to the tip of the chain are able to validate the transactions that have happened thus far, are able to reconstruct the state. Or request the state with very minimal trust assumptions such that they're able to author their own transactions, validate the transactions of others, and importantly, do so without trusting anybody else in the system. That includes miners, it includes other nodes.
00:43:41.020 - 00:44:27.644, Speaker A: That gives users full autonomy of these systems. Mustafa mentioned really eloquently this is one of the core values of the modular design ecosystem. It also turns out to be a core value. What makes to us at least blockchains and public blockchains in particular interesting, so oftentimes we hear that this chain or that chain is more scalable. Oftentimes we're simply referring to cost. I guess the point of this part of the presentation is that we also need to consider what the requirements are on users, be they full nodes or light clients. That comes with that decreased cost, right? So sometimes we hear roll ups scale ethereum or roll ups scale some other chain and I think we want to be a little bit precise about what we mean.
00:44:27.644 - 00:44:54.500, Speaker A: That is true that roll ups are a way of scaling these platforms, however, not often for the reasons that are discussed. Typically people point. To some graph like this. They show the L1 cost being significantly higher than the L two cost for the same transaction. Here is just a uniswap swap on Ethereum versus optimism and they say, okay, one is roughly in order of magnitude or more more expensive than the other. Therefore we have scaled ethereum. Of course we're only talking about cost.
00:44:54.500 - 00:45:52.120, Speaker A: We haven't considered what actual users have to do to validate these computations that are happening on L2. Right? Another thing to note pretty briefly is again, this is data from for optimism. The vast majority of the actual cost that users pay in these roll ups actually come from the data footprint on a one in particular, in this case Ethereum today that is projected to go down through all sorts of things that are in the pipeline including making data availability cheaper on Ethereum and so forth. All this is telling us is that gas is really, really cheap on L2, right? And we have to reason about whether that can continue to be the case or what. We anticipate these gas price and gas market dynamics to be down the line. And the way to do that is we want to think a little bit carefully about what the precise scalability of splitting up computation from data availability is. Right? So just for the sake of argument, let's take just like a standard monolithic blockchain.
00:45:52.120 - 00:46:36.416, Speaker A: Let's create a carbon copy of it just as a thought experiment that does the exact same execution and just split it up into two different layers. One is going to handle data availability and will also process state commitments, commitments to the state as well as proofs that state is valid in different ways. And then all the execution is going to happen on another layer. And the question we're going to ask is have we scaled anything just by doing this? And intuitively it would seem no because we're executing the exact same computation. So for instance, if we're running the EVM on another roll up it's just another blockchain. We're simply validating the same thing. We need to run a full node on layer one to validate the data, to download data and validate state commitments and so forth.
00:46:36.416 - 00:47:27.956, Speaker A: It doesn't allow us to reconstruct the state of the roll up. If we wanted to reconstruct the state of the rollup we would have to run a full node that entire roll up re executing all the compute, thus therefore not inducing a scalability benefit, right? However, we do get some other really important things just by the way. We do get cheap trust, minimized light clients, right? We can just run our full node on layer one and if we just trust one of N nodes on L2 they can provide us that state. We can verify that it is indeed the correct state that can allow us to do fast syncs. It can allow us to do trust, minimized bridges between roll ups and lowers the hardware requirements for users to participate in these systems meaningfully. But in terms of running a full node, so far we haven't achieved any scalability. The scalability comes from two factors that are a little bit more subtle than just the pure cost of looking at L2.
00:47:27.956 - 00:48:40.620, Speaker A: The first of these is horizontal scale and the second one has to do with resource pricing, right? So let's take these in turn. This is indisputably a more scalable system than a monolith for the simple reason that it's essentially a form of sharding, right? So if I want to run a if I want to validate ZK sync, I don't need to also validate a bunch of execution on optimism, right? So nodes are splitting the work between them. This is a form of scalability, right? We have strictly increased the capacity of our system without increasing the hardware requirements on any one of the nodes in the system. We're splitting up the work between us. Now that scalability benefit is somewhat muted given the fact that a lot of the cost, as I showed two slides ago, is coming from layer one, right? So we still need to validate this really heavy EVM. We need to make sure we're downloading data that corresponds to a whole bunch of other roll ups and so forth, right? Over time, as DA costs come down on layer one, call it EIP 44 80, call it data only shards and data availability, sampling and so forth, we get a lot more horizontal scale. Then you also consider pure data availability chains that don't have heavy VMs to validate and the horizontal scalability benefits can become very, very substantial.
00:48:40.620 - 00:49:56.196, Speaker A: That said, they don't come for free. They often come at the expense, not always of composability. And what we mean by composability just the fact that applications are colocated and can be atomically composed with each other. It's a really magical property of public blockchains wherein as a developer, I can take different components from different underlying pieces of software and stitch them together into an end user experience such that the end user sees these applications almost as if they're functioning as a unit, as one, right? This is really magical. It's kind of what's enabled a lot of the innovation in DFI and other areas of web3 that we find very exciting. We don't take it lightly to compromise, I suppose, on this property it's, as we understand it, at least one of the unique properties of building applications on public blockchains, right? On the other hand, we have a lot of benefits to building, splitting up execution and having multiple different roll ups handle different tasks, right? Including this horizontal scalability as well as sovereignty and flexibility in terms of being able to customize your execution environment to your end goal, right? So we have a pure classical trade off as application developers inside of the modular world between composability and horizontal scale. On the other side.
00:49:56.196 - 00:51:25.932, Speaker A: And I guess the question is going to be how do we navigate trade offs like this? One is to take the Twitterverse approach which is to take really extreme positions and say hey, we want all applications to be on the same chain and not compromise on any composability ever. Of course that will mean that our chain is going to be either really expensive to use or really centralized. And then the other approach is that every single application you could imagine should have its own layer one blockchain or sovereign roll up, right? Empirically we know that neither of these extremes are truly tenable and that the reality is somewhere in the middle and in fact the optimal point is somewhere in the middle. And the question is going to be how do we find that optimal point? Incidentally, by the way, monoliths can also do Sharding, except that you need to pre specify what each shard does a priori. The modular approach to navigating dilemmas like this and trade offs like this is to let application developers is to let users decide with their money, with their time, what trade offs work for what application. So you can imagine a game developer may want a very streamlined, self contained experience and they're very happy building a sovereign roll up or a layer one chain specifically tailored to the experience that they want their end users to have. Maybe a DeFi yield aggregator is going to want to colocate with a whole bunch of DeFi apps might be a little bit more expensive as a consequence but the composability they get out of that greatly outweighs any slightly increased cost to users, right? The modular approach is to run a whole bunch of these experiments in parallel and for each application iterate towards a more optimal design.
00:51:25.932 - 00:52:20.368, Speaker A: When we find those designs, we can continue to iterate on them and continue to explore and run experiments and get better over time as an ecosystem. Okay, so this second scalability property of modular blockchain designs comes without that many trade offs. It is essentially a pure increase in capacity that comes from more efficient resource pricing. So recognize that monolithic blockchains price resources as a bundle. So you get some gas, you can then redeem that gas for certain bytes of data availability. You may also redeem it for certain opcode executions, right? Those resources have a fixed price relative to each other. What does that mean? It means if demand for one goes up, even if supply and demand for the other does not go up, the price of it will go up, which strictly decreases the capacity of these systems.
00:52:20.368 - 00:53:28.996, Speaker A: Right? On the other side, on the modular paradigm where you split up data and execution, if demand, for instance, for running certain operations inside of, I don't know, ZK sync goes up, data availability for optimism does not go up. Which is a reasonable thing to say because those things are completely orthogonal resources. So we strictly get more capacity in our systems through more efficient resource pricing without increasing node requirements in so doing. In fact, this property, the more congested the system, the more powerful this property is. Another way to think about it is we have a constrained optimization problem to allocate resources. In such a way, to maximize the scalability of our systems, we simply remove a constraint from that optimization problem, right, with a constraint that the prices are fixed. Removing constraints from optimization problems very often, especially when constraints are hard and met, for instance, in a case where there's a lot of demand for the system greater than the capacity they can handle at any given time, strictly allows us to achieve greater values for the objective function, right? So it's relatively trade off free way of scaling blockchains.
00:53:28.996 - 00:54:06.064, Speaker A: All right, want to change gears a little bit and talk about experimentation and optimization. And this is really maybe the meat of the talk. It's what excites us the most about modular ecosystems and designs that we're starting to see percolate throughout the space. It's that by splitting up monoliths and allowing individual components to be optimized, we get a lot more experiments run in parallel, and we get a lot better data layers, we get a lot better execution layers. As a consequence, we get more scalability, but we also get new capabilities in these systems that we couldn't have imagined otherwise. Maybe those are about privacy, or maybe they have to do with application specific concerns. So just as a prequel, we've seen an incredible amount of innovation on public blockchains.
00:54:06.064 - 00:54:30.012, Speaker A: A part of it is and more of it happens higher up, right. A lot of iteration happens at the UI layer. You can push changes every hour if you want to. Smart contracts move a little bit more slowly. You have to audit them, hopefully at least. And you need to make sure that whatever you build there's hopefully some users on it and so forth. It's not as easy to just iterate on a day to day basis, maybe it's on the scale of months.
00:54:30.012 - 00:54:57.380, Speaker A: But even in spite of that, there's unbelievable amount of innovation that's happened. And the reason for that is, for instance, if I am curve and I see uniswap v One and I'm excited about that design, I can say, hey, I think I can do that better for stablecoins. I don't have to go and fork all of Ethereum underneath me. I don't need to fork the consensus layer anything. I just build a smart contract. I run that experiment. If people tend to like it, maybe I'll get a bunch of liquidity and people will like my product.
00:54:57.380 - 00:55:26.784, Speaker A: Right? Similarly, sushi swap can fork uniswap and add an incentive. Uniswap e three can take ideas from everything else that's been built, including add some of their own. And we're running a bunch of experiments in parallel, figuring out what the right way to do decentralized exchange is. And then when we find ideas that work, we double down on them. In contrast, the underlying infrastructure, consensus data and execution moves glacially. It moves kind of closer to the speed of hardware than the speed of software. And there's actually very good reasons for this.
00:55:26.784 - 00:56:08.204, Speaker A: There's like hard fork release timelines on a year scale, kind of similar to how quickly Apple releases iPhones. And the reason for that is, let's say I want to make, for instance, a change to the execution environment of Ethereum. Maybe I want to enable greater parallelization in the EVM. I can either launch my own layer one from scratch, right? I can build my own consensus system or fork the consensus system and recruit my own validators, then build the execution environment that I want. And building layer ones is really resource intensive. Or I need to get an improvement proposal, solana improvement proposal, ethereum proposal pushed through the core developer community. There's reasons why that's structurally hard to do.
00:56:08.204 - 00:57:09.036, Speaker A: The main one is the fact that a lot of people use these systems and we can't just try random crazy ideas with low probability of success because people's livelihood depend on these systems actually being resilient, right? We don't want to break them. So we want to be a little bit conservative with how we iterate on these designs because again, they're tightly coupled. And so one change, for instance, in the execution could screw up consensus and maybe performance improvements in one part of the system can cause performance degradation elsewhere, right? So structurally these things are slow moving, but we still want this capability to be able to run a bunch of experiments kind of a little bit more similar to what's happening at the smart contract layer. And I think we would benefit a lot as an ecosystem with a lot of parallel innovation happening in these systems. Okay, so I guess my point here is that modular blockchains enable this uniquely. By splitting up execution from data availability and consensus. It is now easier for me to launch a new execution layer should I choose to, right? So if I have an idea, maybe I want to enable access lists in the EVM to enable parallel reads and writes from storage.
00:57:09.036 - 00:57:29.504, Speaker A: Maybe a lot of people disagree with this decision. But I want to run the experiment. I now don't have to fork the entire DA and consensus system. I can simply launch a parallel execution environment now. It's still a little bit harder than launching a new smart contract. I need users to validate, I need nodes and so forth to be running on this. And for instance, if I need to build my own sequencers, then I need to recruit a validator network.
00:57:29.504 - 00:58:08.112, Speaker A: It's maybe not worth it if I just want to kill the self destruction instruction in the EVM that everybody hates, but it might be worth it again for something like parallelization, which is just an interesting experiment to run, right? If I happen to be right, users will use my chain. They'll validate my chain, they'll run nodes on it, liquidity will migrate to it. And if I'm not, nobody will do that. But the cost of experimentation have been brought down materially relative to launching a new layer one from scratch, it's easier to launch a L2. It's easier to launch a sovereign, modular execution environment in these contexts. The other thing is that as a developer of an execution environment, I only need to optimize for the precise use case that I care about. Somebody else is handling DA for me.
00:58:08.112 - 00:58:57.024, Speaker A: Somebody else might be handling consensus. I need to think about who my users are, who the developers on my execution environment are likely to be, and how do I best tailor to their needs. Similarly for the DA layer, DA layers can specialize and focus on providing the best possible experience for people building execution layers on top of them. Right? So what does this imply? It implies that at the same time we're able to run a whole bunch of experiments on execution layers. That means we could get execution layers that are more performant for general purpose applications, maybe special purpose execution environments for different use cases. My colleague Wei from BCC has a proposal to add an additional instruction to the EVM to process Saplink circuits and enable anonymous transactions and composable DeFi. Anonymously? There's all kinds of different things we can try.
00:58:57.024 - 00:59:18.416, Speaker A: Some of them are going to fail. Some of them are going to be terrible. Some of them are going to be centralized, scam execution environments that claim to solve everything without any trade offs. The point is, it's going to be really fun. A lot of people will try many things. Maybe your nephew will show you execution environments at your next family reunion. Maybe you yourself will build an execution environment.
00:59:18.416 - 00:59:39.280, Speaker A: And if you do, we hope you call us. Thank you very much. Pause now for questions. Do they need to get your Mic? Yeah. There we go. Come. Beautiful.
00:59:39.280 - 01:01:09.228, Speaker A: On the note of orthogonal resource pricing, if you have an L, one that supports on the central planning when it comes to essentially creating efficient markets to maximize resource allocation for L two s, is it the case that you can get quite a lot of benefits when you have multidimensional, EIP. 1559, or you reduce central planning and you say, like, have individual opcode operations to be free floating, or is that only a half hearted solution? And you really need to have complete orthogonal separation of orthogonality on the different layers versus to allow a free market for the usage of layer one resources by L2? I think both of those achieve a similar thing, right? So if you are able to price resources separately in every instance, and you have fixed absolute prices that are given by supply and demand for scarce resources, then you've accomplished this goal it's just simply much easier goal to accomplish in a modular architecture. Because in a monolithic architecture, there's other concerns to doing that. Right? And even EIP 1559 is not trade off free, right? And I'm not saying it's trade off free in the modular world either. It's a different way of exploring what those trade offs might be by just running a bunch of experiments in parallel. Right? It's a great question. And then quick follow up in terms of the trade off between having DAP chains or very specific L1s for applications with the limited code composability and then the other sense of having an extreme gas limit universal L1, for instance, that everything is operating on with limitless composability or far fewer constraints there.
01:01:09.228 - 01:02:02.328, Speaker A: Do you foresee there would be multiple optimal points rather than, say, a single optima? And then as time evolves and these interact, would you imagine it would converge to, say, a bification or a single optimum point? Or do you imagine, say, there would be benefit for extremely low gas limit L1s to live alongside, say, for instance, very secure Bridging in adapt chain environment? Right. I think we're likely to see that there's going to be different optimal points for different applications, right? By definition, some will choose to colocate, some will choose, for instance, to break off, and then, for instance, they could break off and then start to build other applications on the same chain because there happens to be a lot of liquidity there. And then these optimal points are not fixed in time. Like are not fixed across time. There might be something that works now, then there's a whole bunch of users tomorrow and there's an optimal point in the future. It's just the experiments are running in parallel. We're already starting to see this, right? There's different roll ups that are exploring different trade offs.
01:02:02.328 - 01:02:22.324, Speaker A: There's different L1s with different trade offs. There's application specific l ones. This is the world that we live in, right? And we're starting to see increasingly, like, what types of applications might work in one environment versus another. We haven't fully explored that space yet. That is for all of us here to do. Awesome, thanks. Any other questions? No.
01:02:22.324 - 01:02:40.356, Speaker A: Oh, there we go. All right. These are pretty evenly distributed across the room. I know, right? Make you exercise. Here you go. Thank you. Hey, you've spoken a lot about the technical benefits of a modular blockchain architecture.
01:02:40.356 - 01:03:30.744, Speaker A: But if I were to build my own L1, one of the things that I want to do is hold value in my token. And for something that's going to provide me this data availability layer to replace, the need for my consensus has to be paid for. Do you see that as being something where we're going to have a lot of different tokens? Something like Terra, where you get fees paid in all the manner of tokens that are being used on the network? Or is that something you see more in the kind of Ethereum space where you're paying an ETH to write back to the chain? There's a couple of questions in there just to make sure I understand it right. So one is application developers or roll up developers wanting to have their own asset. If there turns out to be some use for that asset, then presumably it has a reason to exist. Maybe it does sequencing or so forth that you care about. Maybe there's some property to sequencing that you care about.
01:03:30.744 - 01:04:02.464, Speaker A: If not, then might not have a reason to exist. And so these are the debates we have on Ethereum all the time with different applications anyway. And then in terms of for regular users, I expect. So, for instance, if there's a roll up that has its own sequencer where you pay in that native asset, then you just outsource it to the validators right. To go pay Celestia or Ethereum or any one of these data availability solutions. Or some of these operators have users paying whatever token they want. There's different trade offs that have to do with UX.
01:04:02.464 - 01:04:24.700, Speaker A: I suppose my personal bias, obviously different people will have different ones is, like, to abstract some of the complexity away from users as they're transacting. They're having to think about 50 tokens and the relative prices of each other. It's just not a fantastic experience. Does that answer your question? Yeah, it does. Thank you. As you say, abstracting away the complexity. Okay.
01:04:24.700 - 01:04:44.644, Speaker A: You use our L1 token, but it's not an L1. It's actually provided by the data availability layer. And then we handle the costs and the gassing on the underlying consensus layer through the validators. Right. That makes sense. Yeah. I didn't get that you could do that on the validator layer because obviously they're a bit more smart than I initially thought.
01:04:44.644 - 01:05:02.704, Speaker A: So thank you. Any other questions? I think you're done. Thank you. Thanks, Alex. Yeah. All right. Oh, you're already up.
01:05:02.704 - 01:05:21.316, Speaker A: Next up, we have Zucky, co founder of many projects, also one of the founding fathers of Cosmos. I'll let him take it away. He's going to talk about the modular past and future of Cosmos. Cool. Thank you. Doesn't seem like it's working. Is it working now? Cool.
01:05:21.316 - 01:06:21.348, Speaker A: Awesome. Sweet. Here we go. All right, so what does Cosmos have to do with Celestia? This, I think, is the hardest question to answer because no one knows what Cosmos is. And Cosmos is my best way of describing Cosmos is cosmos is the emergent properties of are we going to die? Are we going to die imminently. Cool. Cosmos is sort of this emergent bottom up effort or, like, emergent bottom up system that, like, emerged out of, like, a bunch of work on building blockchains in this modular way that started in 2014.
01:06:21.348 - 01:07:24.760, Speaker A: And so I kind of am frequently the face of explaining the system architecture to the world. But credit to Jay for basically foreseeing a lot of the problems that Mustafa and Alex just talked about, which was, we want to run a lot of experiments. We want infrastructure to enable running a lot of these experiments. Back in the early days, we were extremely capital constrained. You could kind of understand the scope of the problem of how do you build like a global financial system? But it was just like a bunch of people who were like it was like small groups of people who were coding. We had very little money, we had very little resources. And to build all this stuff, how could you actually achieve the system? Another sort of sort of anecdote just about how this is all destiny, it was all destined, it was all foretold.
01:07:24.760 - 01:08:32.690, Speaker A: The original idea that Jay and Ethan came to me with, which was their idea for a public blockchain, was called Supertanker. And Supertanker was basically the idea of have one tendermint chain where you write the block headers from other tendermint chains to and then you would do fraud proofs if those tendermint chains, if those validator sets for those other tendermint chains produce an invalid state transition. So doesn't this sound, like, awfully familiar? My objection back then this is 2016. My objection to the super tanker design was we don't have a solution to data availability that scales like tendermint blocks can only be so big. We can't write the transactions for hundreds or thousands of blocks to the tendermint blockchain. We should do something else. And so I kind of was very much pushing a bunch of other ideas about how you would get sovereign interoperability that I found in Mark Miller's work from the that's how we sort of got to the architecture that we sort of launched in 2021.
01:08:32.690 - 01:09:24.504, Speaker A: We had no idea that it would take that long, but there were many hijinks and adventures along the way. So Cosmos, though, has been this, like so, like Mustafa mentioned, ABCI. ABCI is sort of Cosmos's modularity. I'll get to a little bit of the origin story of that later, but largely, I would say, the modularity that we built into the system has had a number of big success stories. So there are a number of chains that have used tendermint to augment geth going back all the way back to 2016, maybe 2015, where Kobe shows up in the tendermint slack and is like, I have geth running on top of tendermint, let's call it Ether mint. And then all of this code was open source. A lot of people ran with it.
01:09:24.504 - 01:10:48.440, Speaker A: And you get things like Polygon and Binance, which are sort of elaborations on this original idea of augmenting depth with team the Cosmos, the tendermint team focused on the Cosmos SDK. And we've had a bunch of big blockchains now that have billions of dollars of value that use the Cosmos SDK chain, the sort of raw Cosmos SDK and interoperate with the whole Cosmos SDK ecosystem of wallets and block explorers and all these pieces, and you get this shared component architecture around the Cosmos SDK with strong network effects. So, like terra luna, osmosis, atom, that all works? Then we also have this explosion of smart contract VMs that have emerged in sort of this ecosystem. So you have Agoric evmos Cosmosm and probably there's going to be more. It's very exciting. And then you have people who have built entire alternate application frameworks like Oasis and Penumbra and Nomic. So this to me is like, hey, this is the shining example of sort of the social and adoption benefits and how sort of making these choices to build Tendermint through this ABCI interface as consensus separated from application has yielded enormous benefits to our entire industry.
01:10:48.440 - 01:11:40.424, Speaker A: And so this is sort of, to me, why modularism has been good, why I am really happy with how we went about building Tendermint and Cosmos because I think all of these things have been massive wins. So our sort of layers of modularity are ABCI and ABCI application blockchain interface. So there's when you build on top of Tendermint, you get this list that's probably about a page of APIs that you have to provide in your application. And it's sort of obvious things. It's like, okay, begin a block. So what should the application do when a block comes in? Deliver all the TXS, so execute all the transactions, end the block, commit the block. These are like the sort of standard APIs.
01:11:40.424 - 01:12:34.380, Speaker A: It's pretty simple. If you look under the hood of many other blockchain stacks, you see the same design patterns where basically the same application consensus interface. But what was unique about the Tendermint approach was that we took this approach of sort of standardizing that API. It hasn't changed that much since like 2015, 2016, though ABCI Plus is a new thing that's coming out of the Tendermint team to sort of enable all of the things that we didn't foresee. And hats off to Dave Ojo for sort of really extending the API in a general way. But this has enabled things like Optimint and Celestia to use ABCI in this really powerful way. But all of those chains on the previous slide are all ABCI users.
01:12:34.380 - 01:13:28.376, Speaker A: The Cosmos SDK has also this modular interface where you can extend it with various modules. This has allowed very diverse use cases like THORChain, Axilar. All of these things really have very different systems interfacing throughout this modular architecture. We also built IBC, the sort of built in Bridging protocol to the Cosmos and Tendermint ecosystem in a modular way where you could actually pull out, where you can actually extend the client interface. So, like, the transport layer, like, how do you authenticate a message coming from which blockchain? So we actually could have multiple security models. And then we've always had support for many different signing algorithms and many different sort of cryptographic primitives inside of. The Cosmos SDK has also been a huge enabler and sort of innovation and integration with different ecosystems.
01:13:28.376 - 01:14:26.080, Speaker A: Like, you see all of these things coming out of the Evmos ecosystem that support the Ethereum signing algorithm. And people have tried to build in support for Polkadot signature system as well. Okay, so these are all sounds really great, but we've been doing this a long time and obviously hasn't all been beautiful and perfect. So tendermint itself is a bit of a monolith. It has excessive interdependency between things like consensus and peer to peer layers. And though a number of teams that have tried to build experiments sort of like within that system have failed because of just sort of the overwhelming complexity of that task, we have a lot of large scale users that are also sitting on ancient versions of the system. The version of the Cosmos SDK that Polygon is using for their tendermint chain is, I think, from 2019.
01:14:26.080 - 01:15:24.710, Speaker A: It's kind of terrible. We've also seen a number of teams kind of mostly finance as the biggest one. Take tendermint, make a bunch of changes to it and then never open source or upstream their changes Phantom built. One of the dreams of Cosmos was not only could you have many applications built on top of tendermint, but you could have many consensus implementations because we don't believe that tendermint is like the end. State of consensus could also implement ABCI. And Phantom was the first blockchain to do this, but it never really shipped and it's sort of sitting on the shelf. And then we basically abandoned the idea, though there's some community members who are trying to keep it alive of making the sort of local database of tendermint pluggable because modularity frequently imposes a lot of sort of maintenance cost on the core team.
01:15:24.710 - 01:16:32.250, Speaker A: Okay, so how did this all start? So Tendermint's modularity came about because of an IP dispute. There was one set of tendermint contributors who really wanted to GPL the entire software seat and another set, MJ, wanted to Apache licensed it. And in hindsight, I think Apache licensing tendermint was like a huge reason why the modular systems, why this has become such an important modularity layer. And it's like one of the things that worries me about sort of the modular blockchain stack is like, we're playing too many games with the licensing and the IP system and it's going to hold back innovation as people do business source licensing and stuff like that. I think getting Tendermint to be Apache two license was like a huge innovation enabler, especially in those early days of blockchain. So what we did was Jay came up with ABCI as a way of separating the GPL components from what he wanted to Apache license. And this is how we got the modular system.
01:16:32.250 - 01:17:38.664, Speaker A: That sort of formative thing sort of turned into the philosophy of Cosmos, which is sort of this extremely modular system architecture that I think has really proved its roots. So this is the start of modularity in Cosmos. What's the future hold? So ABCI is this very composable thing or this very powerful addition to the modularity system of tendermint. You can do a lot of things you can do mev. Mitigations I think one of the things that you see from, for instance, Starkware has this proposal about using tendermint to decentralize the L2 sequencer. And you can use ABCI plus plus to require a set of sequencers to agree on external data availability before producing another state update. And so we sort of somewhat intuitively, somewhat informed by all of these emerging requirements as sort of the modular stack evolves.
01:17:38.664 - 01:18:46.264, Speaker A: And we want to meet the needs of the modular sort of builders of modular blockchains. We are building these functionalities sort of into core tendermint to sort of enable people to do this without forking tendermint. One of the other questions that we've been thinking a lot about and there's been a lot of conversations is, can we extend IBC itself to be sort of fit within the roll up paradigm? IBC is the sort of currently somewhat trustless in the sense of, hey, we have this thing where the packets are authenticated by the validator set of the sending chain. So if you already trust those validators, which you needed to because you were using that blockchain, there's no additional trust assumptions. But we could build and we've sort of always wondered about approaches to building actual state machine verification. So you actually could move away from trusting the majority of a minority of those validators to just being able to say, hey, the bridge itself authenticates or has fraud proofs. And all of this stuff actually composes very nicely into sort of the overall design of IBC.
01:18:46.264 - 01:20:05.868, Speaker A: And I'm very hopeful that one of the things that emerges out of the celestial work is sort of like an IBC ecosystem that does sort of trustless settlement or state machine verifiable settlement, but across like in the sort of multipolar IBC world rather than in this sort of hierarchical settlement layer up world. We've been going back to the supertanker story. We've been thinking about fraud proofs for the Cosmos SDK. Also, if you go back to the historical split between Polkadot and Cosmos, it was like we wanted to do interoperable multi chain blockchains, and Gavin was like gavin and Rob were like, yeah, we want to do all that too, but we want fraud proofs, though they didn't end up actually doing fraud. We actually I think we have a relatively good architecture for adding fraud proof support to the Cosmos SDK and that'll allow it to sort of fit start like you to build applications in the Cosmos SDK actually. So open questions. We still really haven't I've been working in the zero knowledge proof ecosystem for many, many years, but we still haven't been able to figure out exactly where zero knowledge proofs fit in.
01:20:05.868 - 01:20:36.150, Speaker A: The cosmos ecosystem. And I still think that's like an open question. It continues to be like a sort of constant error of discussion. I like, talked to the Mina team about using Tendermint for data availability. We talked to the Starkward people about using Tendermint for sequencer decentralization. It seems like there's some piece that Tendermint is supposed to plug into the Xeronology ecosystem, but we still haven't actually figured that out yet. So a little bit of predictions about the future.
01:20:36.150 - 01:21:35.412, Speaker A: Modular and monolithic blockchains are going to coexist for a very long time. There are still like there there is a kind of purity to being able to like, reimagine your entire system from the ground up. And there are things that I don't know how to build in the modular paradigm that I still think are useful in the blockchain space. But modular systems will outperform on decentralization. I think one of the coolest things about Cosmos is the sheer number of teams that contribute at every layer of the stack. And what you will see, what I expect to see with Celestia is now you will start seeing teams that are building settlement layers and roll up SDKs and zero knowledge proof execution environments that are all built on top of Celestia. But they're also going to contribute to the core data availability layer.
01:21:35.412 - 01:22:28.360, Speaker A: They're also going to find bugs. They're going to improve because as you build these modular architectures, it's very friendly to sort of modular organizational structure because you can have different organizations own different parts of the stack and you also have incentives because you're building at one layer to fix problems or improve performance at other layers of the systems. Finally I have a project called sommelier. And Sommelier is to me like the best example of a thing that doesn't work in the modular environment. It is a blockchain that is sort of coordinating DeFi strategy execution on other blockchains. And really the only way to do this is if you own consensus up in sort of your core application stack. And so this would be my counterargument to the modular blockchain thesis is going to rule them all.
01:22:28.360 - 01:23:11.370, Speaker A: There's still going to be a place for modulates. I certainly have a lot of experience building them and that will be that. Yeah. And the other thing is I think stuff like sort of like the direction that many of Mustafa's co founders on Chainspace have kind of gone down with Mistin and Swee is another great example of how you are building highly, very interesting monolithic systems are still to be built. So thank you very much. Questions? Do you want to do questions? Take some questions? Yeah. Any questions? Can't see.
01:23:11.370 - 01:23:55.216, Speaker A: All right, time for my walk. Excuse me. Here you go. Can you comment on the use of Optimt with the Celestia stack in terms of modularization? Yes. So optimint is an alternate ABCI provider. So like, Tendermint is like the normal ABCI provider and Optimint is an alternate ABCI provider. And the expectation is the way I think it's going to work is we are going to add new APIs to ABCI on Optimt.
01:23:55.216 - 01:24:47.192, Speaker A: That add what's necessary for fraud proofs in the Cosmos SDK and then we'll add them on the Cosmos SDK. And then the dream is that eventually gets upstreamed into sort of the tendermint provider and so we continue to have one ABCI, but now we have these fraud proof mechanisms. The other thing that Optimate I think we need to figure out is exactly how we're going to because right now leader election is owned inside of one of the questions I think is about this is in tendermint, leader Election is owned inside of tendermint. It's not owned inside of the application. And we either need to change ABCI to move that over or Optimate will just have like a standard leader election system for roll ups like sequencers on top of Celestial. Does that make any sense? Yeah. Thanks.
01:24:47.192 - 01:25:52.300, Speaker A: Anything else, guys? Any other questions? AHA, all right. Is that oh, there you go. Thanks for the presentation. My question is, do you think that in the future monolithic chains will be more of a niche product that solves like, let's say, only one use case? And if it is, the follow up question is how in the future you think monolithic chains can scale. So the reality of the situation is there's plenty of room to scale monolithic chains. You do have to exponentially increase the amount of resources that are involved in sort of the core engineering. But there's plenty to do in sort of monolithic scaling to get an order of magnitude to orders of magnitude throughput.
01:25:52.300 - 01:26:55.200, Speaker A: The real reason why I think modular wins or like modular is sort of so fundamentally important and is basically like the future of finance layer is because what we need is we need the infrastructure to become public goods. We need to disperse the knowledge of how to maintain and build and improve and bug fix this to a very large community. You need lots of contributors. And really the only two systems that I think that have ever done this are like the tendermint ecosystem and the Ethereum ecosystem. Everything else, like the knowledge of how to maintain the core system, how to improve the core system, how to change the core system is really siloed in a very specific group of people, typically a company. And the modular architecture is the only way to do that. And that's the only way we're going to build a system that is fundamentally different from the legacy finance systems that we're trying to replace.
01:26:55.200 - 01:28:14.260, Speaker A: And so my general bet would be you may see monolithic systems really succeeding in gaming, gaming, NFTs, all of these other entertainment, more entertainment applications. But I do think that the Ethereum sort of team is very right in their sort of directionally right that if you want to build the future of finance modular is the only way. Thank you. Questions? No, right behind you. Go ahead. How trivial or nontrivial would it be for a monolithic blockchain in the future to evolve into part of a modular one as sort of a roll up. So Alex was basically trying suggesting that it is more of a continuum than an absolute and there's an extent to which that's but like I really think about these things as the social process of building them as being what over determines the future technical direction rather than what the actual constraints of the software.
01:28:14.260 - 01:28:55.840, Speaker A: What I've yet to see is ever a project that started out building like sort of a monolithic architecture, ever ending up with the social organization to do a modular to build modular. They don't know how to run a multistakeholder process right. The success of a monolithic blockchain comes from doing the opposite. Like not running a multistakeholder process. Being like we are going to build a blockchain, we are going to make these technical trade offs. Like if these don't work for you, screw you. That is the way in which you successfully build a monolithic blockchain.
01:28:55.840 - 01:29:46.688, Speaker A: If you want to build a modular blockchain, you have to run this multi stakeholder, multi entity process which is very messy and it's harder to figure out what's going on. It requires a lot of engagement. We see it in Cosmos, we see it in Ethereum happening in real life and that is a very difficult social shift. So I think if innovations from the monolithic blockchain world actually make it into the modular blockchain world, it's mostly going to be like forking the code and rebuilding the social structure rather than the monolithic people being like, okay, we have gotten to the end of our monolithic story and we now are going to build a modular system. Thanks very much. Thanks. All right.
01:29:46.688 - 01:30:58.040, Speaker A: Anything else? No questions? Okay. There you go. That was really interesting, really interesting talk I wanted to ask you. You mentioned that there are some features or functionality in monolithic blockchains that you think would be or you don't know how you would build in a modular blockchain. So I was interested in what are the most challenging things to build in a modular system that currently exists in a monolithic. So what are the things that make some of these things really hard? One is where you want like so basically so there's like the story basically comes down to situations where you can tightly couple the properties of consensus with the properties of the application. So if you want to change the rules of block production in a very specific way that's about observing external systems.
01:30:58.040 - 01:31:39.842, Speaker A: What you need is sort of monolithic total control. You want to have encrypted transactions that are decryptable. You want to be able to control your scheduling in such a way that you react to and execute to events autonomously from other blockchains. All of this kind of tightly bound integration between consensus and the application still represents something that's very hard to build, in my mind, in the modular ecosystem. Cool. All right, I think that's it. Cool.
01:31:39.842 - 01:37:35.900, Speaker A: Thanks, Zachi. Appreciate that. All right, we're going to be taking a break until about 1120, which is when we'll hear from Uri from Starquare on L1s, L two S and L three S. So thank you all. Ram SA sam. Sai. Ram sam.
01:37:35.900 - 01:43:41.450, Speaker A: It left. Hold on. It's SA. SA. Ram sam. Ram. SA.
01:43:41.450 - 01:50:44.906, Speaker A: Jesus. SA sam. Ram sam. SA. Ram. It's oil. Sam.
01:50:44.906 - 01:55:32.670, Speaker A: Don't let it bedroom. Ow. Sam. Sam. Sam. Sam. SA.
01:55:32.670 - 01:58:20.350, Speaker A: Eight left. Eight left. Hold on. Hey, all, reminder, we're starting in ten minutes. You it's. Sam lam. All right, guys, we'll start at 1125.
01:58:20.350 - 02:06:19.252, Speaker A: Please make your way back into the atrium. We'll start at 1125 with Uri. Ram sam. Ram SA. Hope everyone is caffeinated and ready to pick up again. Cool. All right, next up, we have a pack schedule until lunch, which is at 01:00 P.m..
02:06:19.252 - 02:07:20.072, Speaker A: I'm going to kick it off to Uri, who is the CEO and co founder of Starkware, to talk about L1s, L two S and L three S. Mike, should we just spend 30 minutes asking what yes. Can you yeah. Can you hear me? Do I need this pacemaker? Does it make me look slimmer? Three tokens to the gun. Okay, so from layer one to L2 and on to layer three. Hi. So first there was nothing, right? We were all wondering aimlessly.
02:07:20.072 - 02:08:10.940, Speaker A: And then Satoshi showed up and some people were doing some useful stuff, but not many. And then Ethereum showed up a few years later and with this beautiful concept of, for the first time, allowing general computation on a public ledger. And that was very exciting for a lot of people. And this provided two massive benefits and delivered as promised, unlike many, many other projects, truly decentralized network and a secure one. This thing is messing with me. Okay, can you hear me now? There we go. So I want to donate this to science.
02:08:10.940 - 02:08:59.560, Speaker A: Thank you. Okay, so delivered on security and decentralization. So that's good. It failed massively to deliver on scale, and the year is 2022, and repeating the same numbers on 15 TPS. And the whole moving from 10 million gas per block to twelve and a half to 15. The gradual erosion there around decentralization and the high tech efforts involved in launching an EVM compatible chain with 150,000,000 gas per block, or one and a half billion or 50 inch mazillion. But scale wasn't provided, so L2 S were born.
02:08:59.560 - 02:09:45.720, Speaker A: And so L2 was introduced as this new concept that conceptually says, let's try and solve scale. And of course, the base layer, or Ethereum, was retrospectively renamed layer one. And we came on the scene in the middle of 2020, our first main net deployment with Starkx, which is a scaling service. It's a SaaS business that supports a bunch of use cases and that does solve scale. We'll give a few examples in a second. We started out with validium where data is stored off chain and this is very pertinent an event hosted by Celestial. We think that data availability is a massively interesting design space and we'll talk about that in a second.
02:09:45.720 - 02:10:46.552, Speaker A: But we launched StarkEx in validity mode. In validity mode and we launched StarkEx in roll up mode. One is with off chain data with a data availability committee, the other is with on chain data. And as of this week, we have a new mode called Volition, where the choice whether data resides on chain or off chain is actually made by the user, not the application at the single transaction level. So the nice thing about these Starkx deployments is that they actually achieved scale. To give you some numbers, starkx over the past year has settled over 150,000,000 transactions on Ethereum, over half a trillion dollars. In terms of scale, we've reduced say dYdX's gas per transaction from 250,000 300,000 gas per transaction to 480 gas.
02:10:46.552 - 02:11:29.868, Speaker A: So a 700 x reduction. We're talking about tens of millions of transactions on Ethereum, mainet, this isn't crypto Twitter and this isn't some test bed. And for so rare and immutable, for example, we reduce the cost of minting NFTs from about 200,000 gas per mint to less than ten gas per mint. So a 20,000 x reduction. So scale, that was demonstrated in a very sort of powerful way, but a few things were sort of not achieved in the process. So first of all, this isn't general purpose. Starcark supports a bunch of use cases out of the box.
02:11:29.868 - 02:12:06.230, Speaker A: So payments and transfers, spot trading minting and trading of NFTs perpetual contracts. Now a new use case called DeFi Pooling. There are actually folks in the crowd here building a business around that, but not general purpose and not composable. You could think of these Starkx instances as these islands run by these businesses. dYdX so rare, immutable, no composability. So this beautiful vision of composability was not achieved and of course not permissionless. You have to sign a contract with us.
02:12:06.230 - 02:12:48.896, Speaker A: There's an actual legal document in place. So all these things were not achieved. Now for Starkx, we developed a programming language called Cairo. There were about five druids on the planet who could take a given computational statement and translate that to a set of polynomials, all of them on the Starquare payroll. Guess what? They were terrified of their own occupation. Because when you said, how about we modify the business logic from this to that, they said, well let's not do that anytime soon, okay? Or maybe the other guy can do that. I don't want to touch the uranium and the core of this nuclear reactor.
02:12:48.896 - 02:13:24.770, Speaker A: It's just too dangerous. We need a programming language. We need to turn this from Sorcery into just plain good old programming. And that's when cairo was born and Starkx is powered by Cairo. Now, the minute we had Cairo and we said, this thing is becoming increasingly higher level, increasingly more secure just by merit of sort of being battle hardened, the obvious idea was to say, well, let's externalize this to everyone everywhere. And so StarkNet was born. We announced this in January of last year.
02:13:24.770 - 02:14:05.464, Speaker A: The Public Testnet went live in June of last year. And the Alpha went live on Ethereum, Mainet, in November. And it's getting a fair bit of attention in the ecosystem these days. And this does give it gives general purpose. It has composability. Well, right now, much like our friends at Arbitrum and optimism with the proverbial crutches, the first phase of the network is in fact, permissioned. We want to make sure that the whole thing doesn't fall apart, but the intent is within a few months time to make it truly permissionless.
02:14:05.464 - 02:14:59.340, Speaker A: The public testnet, by the way, is permissionless for both deployment of contracts and sending transactions to the network. So all these things were achieved, but still stuff was missing. Okay, what was missing? So, first of all, scale is never the thing where no one ever said, okay, that's perfect. That's enough scale for me, thank you very much. Right. Bandwidth being sort of the obvious example, but beyond that, I think it's control and privacy being one example of control. So how did this whole thing sort of evolve in our minds? How did this realization evolve in our minds? So often, the narrative of these things, you come in and you say, well, we thought about A and then we realized B.
02:14:59.340 - 02:15:27.504, Speaker A: And then we said, C is really a forced move, and the reality is far messier than that. And the reality for us was that a particular project came and said, we want to do derivative trading. We're an exchange. And we said, that's terrific. We can build for you something like what we built for dYdX, perpetual trading, perpetual contracts. And they said, no, we want every form of all sorts of derivatives. And we said, well, that's terrific.
02:15:27.504 - 02:16:11.430, Speaker A: Go do whatever you want on StarkNet. You don't need our permission for anything. And they said, well, the problem is that we're an honest to God sort of regular exchange, and there are legal contracts, God forbid, with banks on the other side. And those are denominated in that pesky fiat called US dollars. And we can't have revenue coming in in dollars and cost fluctuating with gas prices. They said, if this network of yours is anything as successful as you expect it to be, gavin sort of invented new physics. This is going to supply and demand is going to dictate the prices of gas here, and we may not have a business.
02:16:11.430 - 02:17:01.012, Speaker A: And so at that point, we said, well, how about you run your own instance of StarkNet? Okay? So think of this as a walled garden. And the beautiful thing about these metaphors is the minute you have the metaphor and you start using it, it becomes a very sort of relatable way to sort of process these ideas. So we started talking to all sorts of folks about this, and it turns out that this concept resonates in a very powerful way with web two companies. And web two companies, they understand that something massive is happening. Okay? But this Kumbaya view of and excuse the Kumbaya folks here, but of everyone going fully uniswap, and it's not going to happen. It's not going to happen. Not because we don't want it to happen.
02:17:01.012 - 02:17:45.440, Speaker A: It's just not the way of the world. Companies exist for a reason, and economic forces are in play, and all these things will continue to exist. So control for companies is something that they need. They need this because the regulator requires them or their business partner or their users. That's where layer three was born. Now, the one thing we can immediately do in terms of layer three is move the Starkx instances up there. So what does that even mean? So Starkx, much like the public StarkNet, the transactions process there, we create proofs.
02:17:45.440 - 02:18:29.760, Speaker A: Those proofs are verified on Ethereum, on layer one. If instead of verifying those proofs on Ethereum, you verify those on the public StarkNet and L2, conceptually, you could say, well, this thing just moved one layer up, okay? And that's exactly what we intend to do. This will be in production in a short number of months. The POC in house is already done. And so that's one very interesting thing. But just like the anecdote I told you about the birth of this idea, this will also allow for private Starknets to be born. And private Starknets will exist for single companies or for all sorts of consortia of all sorts.
02:18:29.760 - 02:19:17.776, Speaker A: And we expect a lot of experimentation to go on there because, say, in the context of data availability, the stuff that Celestia is working on, this is the space where you can knock yourself out. You can move faster than the public chain. You can move slower than public chain. You can do all sorts of alternative stuff without building sort of the broader consensus that is needed for something like EIP 1559. And we firmly believe that that kind of experimentation is actually critical for this ecosystem to succeed. Okay? In my opinion, if that doesn't happen, then this whole experiment will fade into the haze. So, of course, you can have multiple instances of these things, as many as you want.
02:19:17.776 - 02:20:02.588, Speaker A: You would have all these desired properties. You can, of course, recursively, apply this and say, let's go higher up if you want. There are, of course, trade offs, right? The final settlement on the base layer gets delayed the further up the stack you move. And the final step is to basically say, well, just like we're settling on Ethereum, just like there are trade offs in the context of data availability for different applications, so on and so forth. There are going to be trade offs in the context of the base layer. Not everyone and everything needs the security offered by Ethereum. And we think that other layer ones are going to eventually adopt these tools.
02:20:02.588 - 02:21:16.608, Speaker A: I think that the Solana approach of building more lanes is, well, just like we see with highways that sort of typically ends in tiers, and so they need proper scaling solutions and other layer ones as well. And I think that's sort of the way things will unfold. So this is our view, sort of our story, where we came from, how L2 was born, and how we're sort of moving to building out these layer threes in the coming months and years. That's it. I'm happy to answer any questions you any questions? AHA, all right, take it away. Thank you very much for your presentation. At which layer does it stop? How high can you go? What does it mean, a layer three? In this context, it means that I prove a computation that takes place at layer three.
02:21:16.608 - 02:22:27.336, Speaker A: And instead of verifying that proof by a verifier smart contract on Ethereum layer one, I verify it in a smart contract that's deployed on the public. StarkNet now, you can build this as high as you want at some point. The question is, why are you doing this, sir? But do you then think that layer three is sort of the general optimization point for layers? I say this with a fair bit of caution and these things go in waves, right? So, Zaki, a few years ago, he described basically this notion, different ecosystems have different names. But Zaki, in 2018, I was new to the space, this view of these autonomous environments and the need for those. He articulated that several years ago. And then everyone went sort of fully uniswap. Composability is the only thing that's the only and when we started servicing dYdX, some of our investors who actually became investors of dYdX later said, the walled garden.
02:22:27.336 - 02:22:58.570, Speaker A: Once again, that metaphor, that's so 1990, that's gone. There are no walls, and it turns out there are walls and sometimes people want them at different heights, but that's sort of it. It sounds like a human need. The metaphor I heard from one of the web two companies, they refer to this as noisy neighbors. I thought that was a cute expression. They don't want noisy neighbors. Thank you.
02:22:58.570 - 02:23:56.324, Speaker A: Yeah, I see you. Hold on. From L1 to L three, what do you think is the most underappreciated impactful economic force? What is the most underappreciated economic force? Something that might be decisive. So, say, like in Kosher's theory of the firm, how he identifies transaction costs as leading to large companies and small companies and the dynamics there and why everything might not be decided on an open market and you might want to internalize transaction costs. So, for instance, do you. Think that there's something other than transaction costs, such as privacy mev that might be decisive in the l one to L three landscape or the possibility of higher level roll ups that we're not quite paying attention to. Yeah, I think that the basic statement around this will all exist on a public chain.
02:23:56.324 - 02:24:28.580, Speaker A: First, it was layer one. Then this will all exist on a L2 period. No modifiers or qualifiers about it. And people will say, well, there is a way to achieve privacy. You can do true ZK roll ups like our friends at STACC have already implemented. Yes, that is true ZK privacy. But in many use cases, there are all sorts of other grades that are of interest and value and far simpler and computationally more efficient, et cetera.
02:24:28.580 - 02:25:27.548, Speaker A: That level of control control is the word is offered in layer three. Now, how this will unfold in terms of additional layers, I have no clue. Beautiful. Thanks. Any other questions? Cool. Hi. So, for projects that are considering deploying on their own private StarkNet or on the public StarkNet, beyond just having the scalability primary reason that you discussed, what are the specifications and factors where you would push a project towards one or another? So, I'd like to think that we don't push them, meaning that we're truly agnostic.
02:25:27.548 - 02:26:14.396, Speaker A: And I think the intent is to build a software stack that turns that into reality, meaning it would be perfectly fine for you to spin up your own private StarkNet, and then if one day, because you've concluded that composability for your business is, in fact, not terribly important, interoperability is good enough. You don't need the synchronous calls. The async stuff is fine. And, in fact, you have a beachhead. You exist in layer three, but you have a beachhead, which is your smart contract on L2 for the stuff that you do want to be composable. And then your business changes, and you say, well, I actually need to be part of the public network here, and you basically plug the logic. Everything is implemented in StarkNet contracts.
02:26:14.396 - 02:27:16.700, Speaker A: You can, at the switch of a button, become part of the public. So the intent is to reduce those switching costs to the greatest extent possible. Hey, Rory, thanks for the talk. Could you talk a little more about recursive roll ups and hyperscale when it comes to L two and the limitations that exist? Sure. So, recursive roll ups are actually intimately tied to this concept. And I'll explain the concept of layer three, and I'll explain why right now, for example, we're minting batches of 600,000 NFTs in a single proof, which is a fair number of NFTs, but that requires reasonably powerful machines in the cloud to do so. And that is, of course, conceptually.
02:27:16.700 - 02:27:36.112, Speaker A: This sort of goes against decentralization. You'd like to sort of move to lower requirements on hardware. You'd you'd like to cut down latency. Right. When I when I batch 600,000 transactions. Two things happen. A, it takes me time to batch 600,000 transactions.
02:27:36.112 - 02:28:18.780, Speaker A: Two transactions came in and another two, it's going to take a while for 600,000 to accumulate. Okay? That's one thing. The other is that once I have 600,000 transactions, proving that is a very big computation. So generating that proof takes a long time. All this builds up, adds up to significant latency, which affects the UX. So for recursive proofs, what you could do is you can take, let's say 100,000 transactions and instead of proving 100,000 transactions in one proof, I'd say let's split it up into 100 batches of 1000 transactions. And so we now can create in parallel and the emphasis here is an in parallel by much smaller machines, 100 proofs.
02:28:18.780 - 02:28:50.856, Speaker A: And these proofs from a computational they prove the same computational statement. I've verified the transactions in this batch and they all abide by the logic that we want to enforce here. Now you have 100 proofs and now is the recursive step. Okay? Now is when you say, well, let me prove something else to you. I'll prove something that actually has nothing to do with the application specific logic. I'll prove to you that I have verified 100 Stark proofs for these hundred proofs that I've generated. Okay? And now I have a single sort of master proof.
02:28:50.856 - 02:29:22.490, Speaker A: And of course you can do this in multiple steps. So this is how recursion would work and this is something that we intend to deploy very, very soon. This is my stand up meeting in Israel now. So reminder. So this ability is of course needed in order to support the verification of proofs generated at layer three on the public StarkNet. So these things sort of go hand in hand. Thanks.
02:29:22.490 - 02:30:18.250, Speaker A: Cool. That'll be the last question, by the way, we have to move on to the panel. Cool, thanks. So in your diagram you've got L3s S as sort of the hyperscale point. But to the question you answered just earlier, if composability sort of continues to be a thing very valuable, more of these private Starknets start to move on to L2. Do you think that's a concern in the long run that you will ultimately need to make the L2 the public Stocknet, sort of ultimately be the place for all the throughput? Instead of effectively having this sort of inverse upside down pyramid where you've got slow ethereum, then public start net, then hyperscale at the top, would that middle layer become really fat? Fat and what's? Yeah, I think I had you till just the very end. The subtext, I don't like it.
02:30:18.250 - 02:30:43.024, Speaker A: How about thick, heavy bodied? Right. An asterisk he's referred to as heavy bodied. So I'm not sure I understand the fat. In what sense? As in burden with a lot of computation. Correct. Because of the desirability of composability at the L two. So I don't know what the equilibrium will end up being.
02:30:43.024 - 02:31:21.372, Speaker A: So for I for one, I think we at Starquare are sort of of the opinion that Ethereum eventually will be the place where proofs are verified, period, that everyone else will be priced out of the market, like the actual transactions that they want to conduct there. Now, whether there will end up being a public StarkNet whose purpose is to support the layer threes versus a public StarkNet, which is sort of geared at composability, I don't know. We'll see. Thank you. All right. Thank you. Thank you very much, everyone.
02:31:21.372 - 02:32:00.580, Speaker A: Cool? Thank you. All right, next up, we're going to have a panel on scaling zero knowledge execution layers moderated by Zucky with Alex from ZK Sync, louis from Starquare and Togil from scroll. All right, sweet. Hey, guys. Yeah. How's it going? Hey. Cool.
02:32:00.580 - 02:32:26.060, Speaker A: I think we can sit down. Cool. So I've known two of these people for many years, but tomer, it's really nice to meet you. Nice to meet you, too. Yeah. Welcome back to the conference circuit. How's it going? All right.
02:32:26.060 - 02:32:44.396, Speaker A: Why don't we start out with introductions who are you? And a little bit about ZK. Sync and scroll. And I think Uri gave a good introduction to StarkNet. Sure. Happy to start. My name is Alex Kuhovsky. I'm CEO and co founder of Matterlabs, which is the company behind Zksync.
02:32:44.396 - 02:33:24.988, Speaker A: We have a specialized ZK roll up on mainnet now for payments and swaps. But now we're working on the EVM compatible ZK roll up, which is the version two of ZK Sync, which we have now live on testnet. So it's the first Zkavm live on testnet. I'm talrul Maharamov. I work for scroll. I'm a senior researcher there. So what we do is we're building an EVM equivalent ZK rollup, which uses Zkevm, which is implemented by US and Ethereum Foundation, as a collaborative effort.
02:33:24.988 - 02:33:56.104, Speaker A: And we're planning to launch the testnets somewhere around the end of the year and hopefully the main net by next year. So I'm Louis. I'm the ecosystem at stockware. I'm going to repeat still what my overlord Uri said before. So Stockware is a zero company. We do a scaling through the usage of proof system, specifically Starks. And we have been settling over $1.5
02:33:56.104 - 02:34:22.230, Speaker A: billion wait, much more. $350,000,000,000. Sorry, one or two order of magnitude through our customers dYdX immutable, soar and diversify. And we have been launching the first production ready VK roll up in production on Ethereum today. So if you want to try it out, have fun. Awesome. Yeah.
02:34:22.230 - 02:35:12.980, Speaker A: Okay, so let's kind of get into it. One of the questions that we were discussing in our chat is this question of decentralization. And I think Uri started this question with StarkNet has a single sequencer. How do each of you think about the sort of trajectory towards not just having scale, but decentralization in sort of general purpose zero knowledge execution? So maybe we can begin with the question, what is decentralization? Why it's important? Sure. Absolutely. At Zksync we are following the philosophy of Bitcoin Ethereum, the decentralized public blockchains, where decentralization is important to ensure that no one has complete control over the network. There is no single point of failure, there is no single point of leverage.
02:35:12.980 - 02:36:46.428, Speaker A: So we don't want the blockchain world to devolve into something that happened to Internet, which began as a decentralized network of networks, but now it's essentially controlled to a very high degree by like five corporations because of natural process of centralization, of power, of amassing of resources. So we need to prevent that. And the way to prevent that is to put the control of the system in the hands of the users and making sure that the community always have the leverage over the developers of the protocol, over the operators of the protocol. So if something happens to Ethereum and let's say miners or validators in proof of stake after the merge become malicious and have a malicious intention to control the network, extract value from the users, the community can always coordinate and fork away. And for me, this is the very fundamental thing which we want to preserve in Zksync by having permissionless software license, by having the network governed by the community but also supporting even though we hope it will never happen. Ability for mass exits, mass transfers to a fork of Ziki sync or to any other roll up which will preserve, which will keep this philosophy. But it's also important to keep decentralization in L2 itself, which is why we're working on a decentralized consensus inside L2 to make sure that the transactions also remain.
02:36:46.428 - 02:37:30.830, Speaker A: I guess maybe it would be better to start this question why is it particularly hard to descend now for ten years? What is the unique challenge that comes out of zero knowledge, that makes decentralization more of a challenge? Do you want to? Yeah, I guess I can take it. It's more that there is no real challenge. It's an engineering challenge. It's only engineering, as people say, except that it takes time. So we're going to make it right? We're all going to make it. We're going to make it decentralized, making open source, everyone will be able to run it. What's complicated though is that you have less flexibility when you build an L two than when you build an L1.
02:37:30.830 - 02:38:06.956, Speaker A: Because as opposed to an L1, at the end of the day L1 is layer relay on an L zero and you can always roll back. You can do things on an L two. The problem that you have is that you have this bridge and when something is a bridge, value the proof and the money is out. The money is out, you don't control anything anymore. And so the bridge is the most scary piece of software when it comes to these L two s. And for the funny story, the reference paper and that no one think about it, but the reference paper on L two S. It was written by Patrick McCauley and it's the name of it.
02:38:06.956 - 02:39:07.884, Speaker A: And I would think about it scaling blockchains through validating bridges as a scaling solution because at the end of the day, everything is about the bridge. And so now there is a bunch of challenges also with a proof system, which is often you don't have only executors and your sequencers, you have this proving and this principal has this latency. You don't want to you want to make the sequencing as distributed as possible and the proving also and you have a bunch of challenging that exists elsewhere. And so there is a bunch of engineering challenges that still need to be solved and we just want to get it right. So it takes time. Can I add? So I think the problem with L two specifically is that because we are already outsourcing the consensus to an L1, the goal of an L two is to minimize the overhead which is consumed by the consensus. So adding a consensus on an L two doesn't really make a lot of sense because it just will add the same overhead that you already are trying to outsource to an L1.
02:39:07.884 - 02:40:17.168, Speaker A: So the point is how do you decentralize in a way which also minimizes the overhead? And it's quite difficult to achieve, especially with ZK roll ups because the prover efficiency is not great. So it consumes quite a bit of computational energy to compute the validity proofs and we need a way to make sure that the system remains efficient, but at the same time people can just join in and participate without an issue. I just want to add we're here at the modular summit so we should think about the decentralization also in modular way. There are multiple layers that can be decentralized with regard to L2. The first is obviously the bridge itself and the general ability to control it, to upgrade, to exit and so on. Then there comes execution, which is actually easier to decentralize than layer ones. We have less challenges because we can always tap into ethereum as a court of a final appeal to resolve disputes.
02:40:17.168 - 02:41:30.104, Speaker A: So our consensus mechanism is actually simpler than the one required for layer one. And we can also have some significant advantages. We can reduce the number of validators because we don't depend with zero knowledge proofs on them for security, we don't need such a large number, we can have delegated proof of stake, so we can reduce latency a lot and we can have very fast confirmations for transactions in L2. But then there comes also zero knowledge proof generation, which is a layer on top of that, which can be decoupled from the execution layer, which can happen asynchronously in some time, which is probably going to be much lower than what we currently have. It's going down like under minutes, like some seconds, but it still needs to happen and it needs to happen in a decentralized way. We don't want to depend on a single provider, on single cloud or on a single powerful player for generating the proofs, not to have it in supply chain as a point of failure. Well so do you imagine that there's going to be economic incentives eventually in these systems for generating proofs? Like how are we going to convince somebody to spend a huge amount of money? I'm going to troll my colleagues here.
02:41:30.104 - 02:42:03.332, Speaker A: So we have this darknet research group internally where we discuss those things and this question about how you decentralize, this has been an open joke. We have ideas and direction, but an open question for nine months to a point. I dropped the meetings just for a joke. But no, there will be obviously incentive. Today we only pay the sequencer either through leader election, through proof of work or proof of stake. But there will be the same kind of mechanism. Now you're perfectly right.
02:42:03.332 - 02:42:50.116, Speaker A: The challenge there is for instance, let's give you a random idea which is, oh, let's make a competition and then if you make a competition that's great. It seems like just the fastest gets to the network, we are all happy, whatever. So the main issue you get with that is that it basically brings down to centralization because the fastest guy, the most efficient guy would always win and there is no incentive for the others to make it. And so all of a sudden now you're relying on one or two single point of failure and your system is not redundant. And while he cannot of course steal money because it's a zero knowledge proof system, whatever, because of the bridge, it still can stall the system and you don't want that property. And so you need to come up with new design and find the right trade off on this latency decentralization, which still is an open question to some extent. Cool.
02:42:50.116 - 02:44:14.268, Speaker A: But to come back to the original question, if there is a work that needs to be done, if there's something people need and they were willing to pay for, there will always be found some market mechanism to provide this value and to get compensated for this. Absolutely. I think what do you so, you know, I think the progress in, in like the fact that like we even have testnets and main nets of general purpose zero knowledge computations is just like insane. From the first time I saw the zero cash paper in 2014 it's just been like I can't believe that we've made so much progress in eight years. But what have been the challenges in building execution environments? You each have sort of very different models of sort of how the zero knowledge is actually executed. How do you differentiate them or think about them as being different? So for us our main goal is to be EVM equivalent, which means that you can copy the smart contain bytecode from Ethereum and just plop it on scroll and it'll just work without any need for transpolation or any other fancy trickery to make it work. And the challenge with that is that EVM is not really circuit friendly and zero knowledge proof friendly.
02:44:14.268 - 02:44:53.912, Speaker A: So a lot of the Opcodes and a lot of the functionality inside EVM needs to be made in a way which minimizes the overhead inside the circuit. And that's quite challenging to do while also remaining efficient. So that's one of the main challenges for us that we're tackling right now. And the way we're trying to solve it is by using hardware acceleration. So we already have a GPU acceleration implementation, accelerated implementation and we're working on FPGAs as well. And we're just going to see which. Doesn't that make decentralization more challenging? To some degree it does, but it depends on the incentives.
02:44:53.912 - 02:45:34.564, Speaker A: If you provide enough incentives, there are going to be always enough people who are going to participate in it. About the decentralization challenge, I just want to bounce on two things here. The first one is we all have the same challenges. Basically you have the same prime. We want to separate the sequencer, we want to separate approver, ideally if we're just going to merge, if there is no other way, but basically we have the same technical challenge. But now the trade off that we all have is different target. The Starkware just took a different bet to say, you know what, EVM is great, we all love it, but seriously, it's a piece of crap and let's do something that is optimizing for proof.
02:45:34.564 - 02:46:18.170, Speaker A: Right, I'm going to rephrase that. It's a great software and it could be improved. I'm making jokes, but yeah, it's darkware. Just decided to say, you know what, ZKPs are very different computing paradigm. Just deal with it, just accept it and embrace it and be as fast as you can using that paradigm and then you can retrofit stuff if you want afterwards. But optimize for the performance and scroll obviously is going for pure EVM. EVM compatibility or the ability to actually prove blocks on Ethereum and Dksync is optimizing for simplicity of developer experience based on previous experience.
02:46:18.170 - 02:47:04.052, Speaker A: If I summarize it, I would let you bronze on that. But the question now about decentralization is the decentralization, as we say, also is less important for ZK roll up than it is for Ethereum or for an L1. And because decentralization matter on the L1, because you need it for consensus, you need to know that the state you're holding is a true one. You need that. Otherwise everyone could lie to you when in a VK roll up you're like, I just need for censorship resistance. I'm not going to be lied to because they cannot lie to me because of the proof. So I just need like a bunch of incentivized people probably.
02:47:04.052 - 02:47:44.800, Speaker A: I don't even give numbers because we want as many as possible, but it could be smaller and it's not necessarily a big deal. So I think the ZK Sync CPU target is kind of very unique. You want to sort of explain? Sure. So, as Louis correctly said, we've chosen the middle ground between the two extremes of having EVM equivalents and having completely separate from scratch execution environment. What we're trying to maintain is a balance between the balance, not a balance. We're not making compromises. We want to get the best of both worlds of the super efficiency, which zero knowledge props can provide if you optimize for them specifically.
02:47:44.800 - 02:48:50.916, Speaker A: And having a fully EVM compatible chain where you can take essentially any application written for EVM chains in Solidity and Viper and other languages, just port it on Ziki Sync and it will work out of box with full web3 API access, with your deployment scripts, with all the tooling that you depend on. It must just work. But we're not willing to take the compromises in the penalty in security orders of magnitude more sorry, not security performance that you would do if you went for full EVM efficiency. So the challenge for us was there were a lot of challenges like you just go them one by one and you tackle them once you set your priorities. But one specific example would be we need to follow and fully preserve EVM security model. And what we're building is a hybrid solution. It's a volution between Ziki rollup and Ziki Porter, which is a data availability off chain solution which is connected to Ziki rollup.
02:48:50.916 - 02:50:04.510, Speaker A: So users can choose whether their account is fully secured by Ethereum or has this external data availability layer which makes some assumptions about security. But the challenge was like, how do you maintain the security model of Ethereum? Because the roll up contracts cannot depend on the Porter contracts, like if their data becomes unavailable there, how do you make sure that the users can still always withdraw their funds from the roll up and this can be enforced by Ethereum. So there were multiple layers of thinking there. One is that we need to ensure that every transaction is executable in zero knowledge proofs, even if it fails. We don't only accept transactions that are valid, we can accept invalid transactions as well without coming through priority queue on Ethereum and we will always execute them no matter what. And just if they fail, we will prove that they fail. But there was also a challenge of how do you design this interaction between roll up and Porter in such a way that to protect 100% secure roll up users, no matter what happens on the Porter, we found a solution and we're going to be happy to present it once Porter is out.
02:50:04.510 - 02:51:15.160, Speaker A: So, Louie, the challenge with having your own sort of CPU target is you've had to build a developer community around zero knowledge, right? So how is So to be like let's say that nine months ago or since I've joined stockware and we've been talking about Cairo, everyone say EVM compatibility, even compatibility like it's like the fucking gold. And the truth is at first we were very worried. Of course we have to start community from scratch and the counterintuitive thing is that it actually helped. The counterintuitive thing is that we are not gathering. I mean the whole thing we're going after today is not to cater to the, what let's say 20,003 D developer worldwide, 3000 different 3D developers. The older new guys, they are willing to accept something new, they are trying something new and even worse they feel that they're bounded by fire because Cairo is hell. And so Cairo is our CPU and a language.
02:51:15.160 - 02:52:12.120, Speaker A: The truth is we have seen a surprising interest from the dev community, from people who came from existing solid dev from other places in the world just popping up and learning it and improving the tooling and having extremely dedicated community and without sort of buying their way. We just explain them, talk to them, give them ideas, fostering the community basically helped and worked. And as of today, I would say that word wide. Right now on a daily basis there is around 300 to 500 devs nine months after inception. So that's pretty successful and the growth is not stopping at all. For now we are pretty considered, pretty safe with our bets to say people are willing to learn new things. I certainly see you tweeting some new exciting egg every single time I wake up in the morning.
02:52:12.120 - 02:53:07.080, Speaker A: Yeah, I know. At this point I'm like cheerleader like an echo chamber. I have to say if you're a dev and I always tell to the dev wants to actually break into the space that the most important thing they need to do is I have two things that I tell them. The first one is it's better to be early than to be great. Which means the following thing you're better off being early in ecosystem than coming from fucking DeepMind and going to make solidity dev right now. Why? Because you are all of a sudden facing people who have been doing that stuff for five years and you're like you don't know the food guns, you don't know all that stuff and you're going to be like competing with people that are just better than you because they've been around. On the other hand, if you come early in an environment that is a bit hellish but at least the fundamental is very interesting, then the competition is a lot easier.
02:53:07.080 - 02:53:31.170, Speaker A: Everyone is new. There is no great dev, there is no sort of magic trick that you know and so that's what I'm saying. I'm seeing people realizing oh, the tech is super cool and Web three looks amazing. But Solidity is already kind of a crowded place. It's not fully but it's a kind of crowded place. Here I am there is 500 people worldwide. If I'm bad, I'm better than anyone else.
02:53:31.170 - 02:54:01.740, Speaker A: And so that's the frequent one. And second one is I always tell them whenever you do something in this space, you're going to break in by being visible. And the only way you do to do that, and that's magic of crypto is that everyone is on Twitter and Twitter is a very easy marketing tool. So every time I'm telling them, oh, you do this paper from Paradigm, just tweet it and retweet it. And the community is being what it is. It's basically retweeting and a lot of excitement. And I got at least five or six people getting hired just because they posted three tweets.
02:54:01.740 - 02:54:19.088, Speaker A: Hired OpenSea, hire various places because they just become visible. So yes, I've become Nico Chamber and I don't do much in my life except tweeting. So people should just follow you actually don't know. Don't think you should follow all three of them. Yeah, you should, but it's really boring. Not him though. No, not me.
02:54:19.088 - 02:54:35.750, Speaker A: It's very boring. It's like it's advertisement, advertisement, advertisement. Just don't do that. But if you want, if you go bored in your life, feel free. Okay? I think we're supposed to get off the stage around now, right? You have time. Okay. I thought we were going to transition, so now I have to come out.
02:54:35.750 - 02:54:47.550, Speaker A: We can take questions. Let's do questions. Cool. Yeah. Any questions? I have one question for Louis. Oh God, I saw you first. I'll give it to you.
02:54:47.550 - 02:55:33.524, Speaker A: Sorry guys. Hey. So we discussed at some point ZK over consensus, right? So one of the properties that we have for off chain computing is that we're not time constrained. So we can perform any proof there and just use this in the chain. So for the consensus, this is more complicated since we're time constrained and maybe we cannot even benefit from some state of the art techniques such as recursions. So how do you guys see that? Should we expect some improvement on the current state of the research or the implementation to make this feasible? To summarize the question, I mean, if I may, you're asking about Ezekiel ones and recursion as a tool for improving what? Consensus. Consensus.
02:55:33.524 - 02:56:03.348, Speaker A: Okay. Anyone wants to take it or do you want to? I think those are actually orthogonal problems. Like you don't improve consensus, you improve other things. You can significantly improve decentralization if you implement, let's call it fully succinct l one, what Mina protocol is doing, for example. Right? So it's a lot easier for everyone to verify for full nodes. You don't have to re execute all the transactions. You just verify 10 knowledge proof, which will take you 30 milliseconds or something.
02:56:03.348 - 02:56:48.604, Speaker A: And the rest is just data availability. It's a great approach if the challenge would be to compete with big established till ones, but at least that's something where you can differentiate from them. But I don't see. It affecting the consensus mechanism as such in any way. Also bear in mind that a lot of consensus mechanisms are synchronous or partially synchronous and when you use validity proofs, validity proofs don't have a sense of time. So there is no way you can use validity proofs to optimize consensus in most cases. Yeah, but I mean like the added time to prove statements for the consensus.
02:56:48.604 - 02:57:16.184, Speaker A: So the proof is far more expensive than the verification. So to verify is trivial to prove it's not. So that's the main problem. As Alex said, the proof generation can improve some bits in some proof of work stake like weeks. There might be something about the weak checkpoint or weak subjectivity checkpoint or something. Yeah, thank you. There can maybe be some improvement there.
02:57:16.184 - 02:57:53.190, Speaker A: But the main thing that you break when you have a VK at the L1 level is that you are improving the topology of the network. Meaning all of a sudden you have validators that can node, non mining node, non sequencing node that can be on your phone and the miners or sequencer can be on a fucking data center for that care. And so all of a sudden you can be increasing your throughput dramatically solana style, but you still have the decentralization that you're hoping for for net one. So that is not at the consensus level though, it's at the topology of network topology. Cool. Okay, perfect. Thank you.
02:57:53.190 - 02:58:41.108, Speaker A: There was one more question, right? Yeah. Oh perfect. All right, thank you very much guys. This question is for Louis because you were Duncan on EVM. What would you change about EVM? Hula, you want all the we're going to be here for a long time, let's give it a top three. Top three, okay. You win 256 as default kitschack, as default hash function, which is like why from the get go, this is not technically EVM, I guess, but exadsimilar tree for storing state top three.
02:58:41.108 - 02:59:26.192, Speaker A: Those are all on Vitalik's list too. What? Those are also on Vitalik's list, right? I mean what do you regret? Rest in peace. Yeah, just copy paste. Exactly. They're the top three, right? What else? The choice of 80 bytes addresses if a mistake. RLP RLP, that's something I don't even know. Right, there is a bunch you can continue there, but okay, yeah, I mean I'm not even the expert in VVM, just the top three that come because when you work on VK you're like why did they use Kshak? Why did they do that? Could they just like shao oblique two s or something? Something simpler or standard? Preferably poseidon.
02:59:26.192 - 03:00:01.180, Speaker A: Yeah, but whatever. Actually we don't use poseidon for the story. We would have yeah, otherwise for the story. For the story. By that actually the hash function. So star query is very conservative in this printive function choices and so you have to understand that the hardest thing to prove or the most consuming in the proof is usually the hash function. And so most of the industry have been using Poseidon and we are using pedestrian hash which is provably secure but ten times worse to prove.
03:00:01.180 - 03:00:30.392, Speaker A: And so we kind of like to shoot ourselves in the foot with like being conservative sometimes. So here's one example. Conservative doesn't mean better or different, it's just like it's different. It's such a choice and I kind of hope one day that I will convince my engineer that Poseidon is good enough, we can adopt it. But for now we're still struggling with that. We got to wait for someone else to settle like a trillion dollars on Poseidon hashes, right? Exactly. Thank you very much.
03:00:30.392 - 03:01:15.540, Speaker A: My pleasure. Any other questions? You had a question? Yeah, right there. Okay, interesting. And Soga's question, just a question for all three of you, or four if Zach you want to throw in what proof systems are you guys kind of like most, I don't know, interested by or kind of like where it could go, take it bulletproofs, Marlin, Halo, the list goes on. Like what of those ZK proof systems are you kind of most stoked about it? I think we know your answer. It's on your shirt. Starks.
03:01:15.540 - 03:01:47.600, Speaker A: I saw Ariel yesterday and I was like you won. Like everyone is using Plonk except for the and you know, the arithmeticization inside of Plonk and Starks is very similar. So it's just like really the last step of their verifier. But yeah, congrats to Ariel for winning. I mean I just want to react to this. It's a very interesting question. My answer obviously Stark, but obviously that's not the answer for everyone.
03:01:47.600 - 03:02:50.624, Speaker A: It's obviously a trade off. What people don't know though is that every time they talk about Stark versus Starks they always care about the theoretical differences like the trusted setup or the non trusted setup, quantum resistance or not. The reality of actually why I care more about Starks to be honest is because the proving like by default which is like why scroll you said so they are looking into acceleration is because it's a lot harder to make a snark proof because it's n log n, but it's n login VPC operation when a stark proof is n log n. But hash function. So the constant here is pretty significant and does have an impact on the proof generation. So this is also why I believe that me, I mean polygon basically went only the star quotes because they believe that it's more scalable at least in the short term. But you should also bear in mind that long term we're probably going to be limited by data availability rather by proving mean we'll know as like to say I like to quote Kent in those cases in the long term we're all dead.
03:02:50.624 - 03:03:09.490, Speaker A: So from there, I don't know, I'm just a normal guy. Of course I'm not dunking here at all. I'm just saying that this is why more for Starks. Cool. All right, you guys are done. Thank you. Thank you.
03:03:09.490 - 03:03:58.812, Speaker A: Killed it. Good job. All right, last panel before we break for lunch, which will be at one right here we have Sean from Medechech, tarun from Gauntlet, alex from Flashbots, and Guillermo, who's Enroute from Baincap Crypto to talk about mev on modular blockchain. So you guys can welcome them up. Go ahead. All right. Hello, friends.
03:03:58.812 - 03:04:55.116, Speaker A: I think we're supposed to be four people, but we're currently three people. So in the middle of the panel, it's going to be interrupted by someone rushing to the front and sitting down and joining the conversation. We could also view it as my alt, actually is my alt. The proof is finally being revealed. So this is the panel on MAV on modular. I think we're going to try and split this up very plainly into three sections or questions that the panelists are going to kind of hopefully run wild with and they're related to sort of security on modular, then incentives and finally, winners and losers in the preventative game of mev. So before we get started, maybe we could do some I mean, the reason I'm excited to sort of host this panel is because actually all three of you wrote the paper on crosschain mev, right? I think not, Guillermo.
03:04:55.116 - 03:05:22.090, Speaker A: Two out of two out of three. So the people who literally are writing the book are sort of present so we can get their perspective, but maybe they have more profound sort of personal details that are relevant that they want to introduce themselves very briefly. Yeah, I'm Tarun, founder of Gauntlet, but also written a lot of papers on both mev and AMM. Hello. It works. It works. Yeah.
03:05:22.090 - 03:06:19.640, Speaker A: Oh, I'm Alex, researcher at Flashbots. Wrote a paper with Tarun as a co author on cross domain mev and did some research on mev on different before cross domain mev, I was looking at proof of stake and mev on proof of stake chains and on Ethereum in particular. Cool. Well, so what we wanted to do to start the panel, even though we're kind of midway through the day, is start out with sort of some definitions of modular just so we all use the same words when we mean the same thing or mean the same thing when we use the same words. I think that'd be helpful. And we want to sort of do that in the context of security. So we could start with the worst case of mev is these sort of long range, time banded attacks that are sort of hopefully prevented by some kind of economic security.
03:06:19.640 - 03:07:09.340, Speaker A: So maybe we could start there with asking, what is the model? What does a modular blockchain do? How is it secured? What are the assumptions? And what does that have to do with the overall stability of the system? I think the best way to think about modularity is sort of the classical computing analogy of you have many different computers. They have different resource access, so they have different compute and different sort of state that they're executing. And they have some method for communication. That method for communication might be some type of message passing system. It may end up being some type of deferred computation system. But effectively, they have a way of communicating, yet operating somewhat independently while synchronizing anything I miss because you wrote the definition for domain in that paper. Yeah.
03:07:09.340 - 03:07:39.232, Speaker A: No, I think that sounds good. I saw there was a talk earlier by Mustafa and I think you were defining what a modular blockchain is in your context. So I don't know. There's like multiple definitions floating around and I'm not entirely sure which one we should let's use the simplest one. Well, I mean, the simplest one is what Tarun mentioned. I think it's kind of like the architectural one. But then in practice, I think you have different applications, I mean, different approaches to where you want modularity on your chain.
03:07:39.232 - 03:08:29.924, Speaker A: And so there's some approaches that have been stated, I think, earlier today. Then there are other approaches as well. What Turun is referring to, to some extent is the modularity, even in parallel computing, right, where you have multiple threads, some extent, there's modularity there and how you manage communication between these threads. Sure. I think if we had slides and we don't, there'd be like some kind of diagram and there'd be actually the last presentation from the guy from ZK snarx had this photo where you have like and there's a transaction and someone executes it and then someone agrees on the order and they're not in the modular architecture. They're not necessarily the same people. Which, of course, you could start to envision how there'd be sort of a different as soon as there's communication, as soon as there's asynchronous as soon as there's layers into things.
03:08:29.924 - 03:10:16.820, Speaker A: That's kind of where Mev lives, sort of at the intersection of varying security models at these layers, you could say. So talking about security, now we have an idea if we were on Bitcoin and hopefully no one's using that anymore, you'd have some notion of how expensive it would be to rewrite the network and it would get more expensive. Do those security assumptions hold true in the marginal architecture? Does the marginular architecture, first of all, say anything about that? Is it indifferent to sort of proof of work, proof of stagger, whatever? Or does it really imply certain new security assumptions? I guess each unit has its own local security assumptions, but then there's sort of some synchronization guarantees that are held between the different units. And maybe it's because I'm older in this space, but I think of everything in terms of, like, old school parallel compute. And when you write kind of like multithreaded code or you write multi processing code, you oftentimes will have this sort of notion of independent units that have some amount of minimum shared state and they're only allowed to kind of communicate through that channel. And one of them could flood that channel and then halt execution somewhere else because it kind of DDoS the other chain, other process. You kind of have the same issues here where effectively you need to ensure that the communication channel is sort of resistant to some extent to being DDoS, that it's expressive enough that it can actually send all the messages that are needed to be concisely communicating between the two chains or two processes.
03:10:16.820 - 03:11:31.296, Speaker A: And so you basically have three sort of between in any pair of communication you have three security kind of models. You have one for each of the two sides, the two processes and then you have one for sort of the synchronization mechanism. And one of the biggest, if you look at something like the wormhole attack, these kind of like mint bugs come from the fact that the two sides may not agree on the security model for the communication layer. And that's usually what's exploited. But you could say that in some ways the offering of a project like Celestia or one instance of this modular architecture is to kind of share security so that there are sort of like something that some component that does the data availability and then state machines and executions are kind of separate from that. And there's this idea that maybe if security is cheaper, you could do more things, you could push innovation to other layers. So in this modular stack you could have one layer that is maybe super secure but not very useful in general sense and then other stuff that is much more useful and doesn't have to worry about security.
03:11:31.296 - 03:12:19.896, Speaker A: But from an mev perspective you could imagine that at some point someone has to decide on the order. And that's the bit that I would imagine you would want to be, let's say the most secure. Yeah. So if we actually think a little bit back to Linux's development it took almost ten years for Linux to actually have reasonable SMP which is like its multicore multi processing unit so that when you write some piece of code. The Linux scheduler could decide how to sort of shard the memory for each process and allocate the processes to each CPU, manage the page tables, all of that stuff. And the reason it took ten years, it's actually really hard to get this communication part right. And this is without people having economic incentive to DDoS your CPU.
03:12:19.896 - 03:13:56.600, Speaker A: It's actually that the scheduling algorithm is actually really hard to write because you need some notion of shared state and common state to all these processes. Which is where I think something like Celestia becomes useful, is actually having that initial kind of agreement on sort of data before you start trying to do these kind of like mutexes lockless queues, things like that. This is completely off topic, but interesting to me. So I'm going to say it in FreeBSD. Are you familiar with what happened in FreeBSD? So there used to be this FreeBSD and then it became Dragonfly BSD because they had different models or different models of parallelization that was sort of sufficiently different to sort of warrant the project sort of working. Do you think that this is something or these kind of disagreements, like is the intellectual space of modular blockchain sophisticated enough that there are sort of like multiple models? And how do you think those models will sort of maybe evolve in parabelle compete or eventually integrate? Yeah, I mean, I think one important piece of interoperability between all Unix like systems and POSTIX like systems is that their file systems are very similar, right? You can basically install any formatting on any of those devices versus the Windows version, where you effectively had to do all this emulation and it took forever for Linux to actually be able to read Windows formatted devices. But I think the second thing is that the memory models, while different, have some notion of translation and you can get some sort of bitwise identical behavior on both sides.
03:13:56.600 - 03:14:46.168, Speaker A: And that's the key thing, right? You want bitwise identical execution and bitwise identical sort of storage and agreement on that. And as long as you have that as a standard, then you can get a lot further than, say, what ended up happening in Windows, where parallel compute never was able to take off. And to some extent I would argue that Windows missed I mean, there's many reasons Microsoft missed the cloud computing world, but one of them is this. It was extremely hard to actually interoperate with these. And sort of alex, from your understanding and research, I guess on the Ethereum space, which is, let's say, more diverse, it's a pretty big ecosystem. Many examples of this modulus split. There's many L two S in ethereum.
03:14:46.168 - 03:15:40.172, Speaker A: They're already sort of competing, I would say, in a way. What's your sense? Which one is best for me and why? Which one is best for me? Yeah. Which party? For Bane Capital? For Bane Capital? No, but I was going to bring it back to what Tarun was talking about. If we bring it back to the space today, I think Tarun is kind of talking about standards to some extent as well, for communication, for interoperability, to some extent. I think we kind of need that for security. I think if you have completely different security models, I think it's much easier to so let's take two different worlds. One where you have one big blockchain, right, where if like a homogeneous security model, where you have security model can be understand holistically.
03:15:40.172 - 03:16:22.412, Speaker A: Understood holistically and then say you have like a patchwork of chains that have different security models, different however you want to define your security model, et cetera. Intuitively it feels like the patchwork of chains is easier to attack because you can attack like smaller units of it grow in strengths and then being able to attack more. Whereas the first one, I think, is harder to attack, maybe upfront, but if you find a flaw, the whole system can fall. And I think currently we have these patchworks of different chains and different models. Even in the modularity approaches, there's like different modularity approaches, but also in where the security assumptions lie. Same thing for the bridges, whatever. This is all over the place right now.
03:16:22.412 - 03:17:02.360, Speaker A: And I think it makes it much easier to attack to a sophisticated attacker that wants to attack the overall system and also for usefulness of the system. If there's some standard that's agreed upon that people can coordinate against, it can be a very simple tool, but something that people can coordinate with cross domain. But there are sort of like concrete examples. Like, I know that polkadot has xcmp. Cosmos has IBC. Right? There's a few, I agree, but I think maybe we need that that's more universal or more general. I think IBC is hard to integrate in some chains, as far as I know.
03:17:02.360 - 03:17:58.172, Speaker A: As hard as integrating a like client? Yeah, like clients. Non trivial. So maybe that's the thing related to mev, right? The more you modularize, the more your stack is complex, the more there's like new attack vectors that emerge and the more there's complexity in being able to reason about the economic security of that stack. I think that's important. In the case of modularity, where you use something like Celestia as your data availability layer, there's also the security budget you need is kind of dynamic, right? So the more domains you add on top that use Celestia for data availability, maybe you have to think about how that affects the security of that layer. So I think it's also dynamic in that sense. And to me, trying to reason through this, the more simple that is in how we reason about the security of these different domains and how we compose the security together, the more often we can build like a strong resilient system.
03:17:58.172 - 03:19:01.520, Speaker A: But what are the sort of like to switch topics now onto sort of incentives like as an L two creator or a roll up if you were just launching, I don't know, Sushi swap clone as a roll up beyond the incentives of the settlement layer being sort of secure. You wrote a little bit about how there is this competition from fees to move between the base layer and the sort of roll up or the application or higher up the stack. So how maybe you could elaborate a little bit. Like how do you see value accrual sort of evolving with potentially deeper stacks, L three S and so on. And what impact do you think that will have on sort of stability? Like not just having something secure at one point, but sort of long term security, predictable security. So maybe two answers and I'll pass it to Tarun as well. One in terms of value accrual from a perspective of designing the system, you want value to be where you need security, right? So like value related security budget.
03:19:01.520 - 03:19:50.812, Speaker A: So I think there's some relation there of where value could accrue. And then the second thing is owning order flow and owning where the customers are. I think to some extent there's like value there because you own the touch point that creates economic activity. These are like two ways to look at it. So to some extent the order flow part is like whoever runs the UI that users interface with because then they can redirect your transaction to some other stack very easily. But at the same time a stack that isn't secure won't be used for long and so you need that stack to be secured properly, whatever that means. In terms of value that has to back it, how much has to be staked on it, how much does it cost to attack it relative to the economic value that's on top of it? Is there an incentive to attack it because you make more money by the value that you secure on top of it, et cetera.
03:19:50.812 - 03:20:40.750, Speaker A: But there's also cases when the protocols are incentivized to sort of be attacked in the sense if you're a Dex or whatever and you're trying to bootstrap, you're trying to get liquidity, you want to pay the liquidity provider as much as possible. If someone is sandwiching or adding more value, like people are sandwiching the tack and consuming the spread. Technically that money is going from users to LPs, right? So there is this incentive play where it isn't always clear and it probably changed over time like hey, there he is, you need to have a little more of a rocky run. I was expecting music while I was coming in but clearly that didn't work out. What happened? Very nice to meet you. Sorry, yeah, I guess I'm just a paid actor. Actually.
03:20:40.750 - 03:21:54.352, Speaker A: I'm in stand in really so it's great. Anyway, sorry no, but we were talking about incentives but I think I interrupted Terra if you wanted to speak sort of about the incentive question. Yeah, I mean one very high level view of sort of the game theory difference between single chain mev and MultiChain mev and obviously this is stylized, simplified version of this is that in a single chain mev is really a congestion game. There's just like a fixed number of opportunities. There's a bunch of people who are all competing to basically capitalize on those opportunities and auctions can be designed to be like a reasonable way of dealing with congestion in most economic situations. However, in the MultiChain world it actually really becomes like a routing problem, right, of like how do I go between these different chains and maximize my value? And the routing problem is actually a much larger sort of state space of possible ways that things can go wrong, which means you need to coordinate the incentives along the whole chain. So like let's suppose I have a sandwich attack that's a sequence of sandwich attacks across bridges that on each side use an AMM.
03:21:54.352 - 03:22:23.824, Speaker A: Well, then I actually need to make sure all the LP to your point earlier. I need to make sure that all the LPs kind of end up having similar incentives. But there's not really a great way of guaranteeing that in a sort of multi chain world, right. You actually would have to synchronize those constantly across all of these or just take the risk, or be atomic. And then you say, oh, I assume that I'm using Flashbots and that I'd zero risk. But if you're like, again, Bain Capital or someone with serious money or a market maker. Oh, sorry, you're here.
03:22:23.824 - 03:23:46.552, Speaker A: I forgot. You'll just execute the trade. Your short term prediction model will say whatever is going to happen and you'll do it and you're going to be right enough times to be bank capital. So here's a fun little alpha leak actually. So here you have a model that's probabilistic yeah, turns out that problem is convex too so actually you can solve it but anyway sorry I didn't mean to detract maybe that's too much of an athlete but I guess the point is you have to coordinate these incentives a lot more. Right? And right now we don't have the fact that Flashbots can be isolated from the incentives that people are using to bootstrap pools is not really possible when you think about it in terms of modularity is that to somehow suggest that smaller players are going to have a harder time in this fragmented liquidity world? Absolutely yeah you need more capital to be able to do this, right? Yeah it's kind of rough because I was talking with Zachie about this and it's like, oh, when you have a million EV most space chains or whatever, maybe not a million like let's say a thousand. And we were trying to ask this question like oh, will it actually be possible to do these kind of things? And it's like do you think jump Capital would have any problem putting a million dollars in a thousand place? I don't think so.
03:23:46.552 - 03:24:41.880, Speaker A: I think that most market makers are big enough to make the spread to take the inventory risk and sit there and get the juiciest part when it arrives. So if someone is writing this complicated model and even if they say everything is connected over IBC and you actually can there is sort of some kind of secure transmission between these things it's not like an exchange where they could stop withdrawals or something. It's like an automated permissionless interrupt thing even if the people who are trying to move liquidity around are always going to have a harder time than someone who just keeps liquidity everywhere. I mean, I guess this gets to Alex's point about who owns the order flow owns a lot more than you think. And the question is, in this multi chain world, will the order flow actually be owned by an aggregator or will it be multiple, kind of like separate front ends that end up fragmenting the order? And then right now it looks like the latter. Right. No one's actually built a very good cross chain aggregator.
03:24:41.880 - 03:25:01.772, Speaker A: And part of the reason is that the front ends don't want to take the inventory risk, right. Like the market makers do. But the market makers also don't make products, so they're not like getting user flow. But it could be just because it's just not worth it yet. The markets might be too, or I don't know. There's some bridges that are backed by market makers, right. Wormhole being notable.
03:25:01.772 - 03:25:31.928, Speaker A: For sure. We get a sense of the size of their ambition. That by the fact that there was a hack and the next day they paid 350,000,000 back to the bridge. But they also raised that again right after. Let's not forget that it did get covered in a fair enough fundraiser. I think the order flow crosschain is at the bridge level, in my opinion. I think when Jump started Wormhole, there was kind of this sense amongst people that this was their version of the microwave tower.
03:25:31.928 - 03:25:54.530, Speaker A: And you want to win the New York Chicago microwave tower. There's a really horrible movie that if you want to watch on the history of this called The Hummingbird Project. Worst acting I've ever seen because it's a movie about HFT. So it's like can't be good, honestly. Just the eisenberg, isn't it? Yeah, exactly. And still horrible movie. I liked it.
03:25:54.530 - 03:26:42.716, Speaker A: But it is factually accurate that the effective war we see with the bridges now in the normal world converge to one thing. But here it actually seems like it's not going to be true. Right. It really does feel like there's going to be quality of service versus bridge security trade offs, and you're going to have the spectrum of that and then people are going to allocate to that and that will have a totally different microstructure than normal markets. Yes, I think that makes a lot of sense. But even in a permissionless system, like some people use bridges, but not everyone needs to. And I guess the question is when is it eventually going to or will it happen that the bridges will sort of like if you wanted atomicity across a bridge, you just have to be you're.
03:26:42.716 - 03:27:31.984, Speaker A: If you're validating, who's to say that you can't operate on multiple networks simultaneously? But as an infrastructure provider, I guess I'll talk from the Cosmos case because that's what I'm most familiar with. A lot of Validators operate on multiple networks. There's no reason if the blocks are like the leader election, the person proposed the block is predictable. If you could predict the time on both networks, right. Then you could actually do a risk free cross. Chain thing and go around the whole bridge dilemma. So how do we think about sort of the incentives for not just jump, but even hopefully smaller validators, smaller operations to start doing MultiChain operations to sort of benefit from simultaneous order flow? One thing there that's part of the paper, the cross domain mev paper.
03:27:31.984 - 03:27:54.520, Speaker A: Right. So we talk about that to happen. But then we talk about the fact that if, say, you're a leader on chain A, I mean, I'm a leader on chain A, you're a leader on chain B and we want to collude or collaborate however you want to frame it in order to get that economic atomicity across ourselves. We don't trust each other. So we need a way to trust each other. We also need a way to communicate. Right.
03:27:54.520 - 03:29:00.860, Speaker A: I think you can model all of that as some additional economic cost that a player that is the leader on A and B at the same time doesn't have. I think you can maybe point to an incentive for someone to control both of these validator sets, for example. And that's what's problematic because maybe the pursuit for alpha is such that the system equilibrium doesn't go towards many smaller validator sets that actually have a way to collude with one another. trustlessly at minimal overhead, but rather a very large player that starts owning more and more of the validator sets on many sites so that they get these leader election slots and they can reduce the overhead of communication. There's still some communication, but it's internal communication. And you don't have that trust cost as well, because you can trust your internal systems to work to some extent. And so you can lower the cost to do some interactions and you can maybe even seize pockets of opportunities that for us who don't trust each other, the cost of coordinating for that opportunity would be too high.
03:29:00.860 - 03:29:22.096, Speaker A: Well, you could put an auction between it. Right. I think that's what you're sort of alluding to. Yeah, I just think the lack of Adamicity actually changes the outcomes of these auctions. Right. So you sort of have to have an auction where you coordinate. Even in the simplest case, I have two first price auctions on both sides and I go across the bridge.
03:29:22.096 - 03:29:55.490, Speaker A: I actually have to coordinate that, the reserve price and the floor price and stuff like that. The initial setup is the same across both places and there is a cost to that. Eventually you still have to pay some amount of money to synchronize those things and that might go to zero over time. Though ideally it does. I think at least we want it to, or at least not zero. But I think ZK stuff will be the only way to get to the point that that cost gets arbitrarily small in the long run because it lowers the communication complexity. Yeah, exactly.
03:29:55.490 - 03:30:40.760, Speaker A: But you would still have like a back and forth to some extent, right? The question is I only have to send you one packet and you can verify that it's correct at the same time that it's like I did the computation correctly on the other side, so I don't have to do any interaction. In that sense, it's good question for you both, right? What if say we go back to that same example, right, and we send each other the packets, but now we need to split the profits. How do we split the profits? I mean, that you can have as a contract on both sides, right? You store the final profit on one side, right? Right. It's always on one side. It can't be on both sides. Contract that's doing that you could also do like I mean, the question I don't know if the question you're getting at is like, how much profit do you split? Yes. This is like the cake cutting.
03:30:40.760 - 03:31:45.460, Speaker A: How do you start the optimal profit split? Yeah, so this is actually a well studied problem in game theory, which is called the cake cutting problem. The pie cutting problem depends on who you ask and what your aesthetics are. So there is like a weird set of equilibrium that you can talk about here, and often they're actually quite stable too. So it is surprising, depending on who the first person is, who's taking some of the risk. Okay, how do we think about adversarial things? Where, say we're splitting and we agree on 50 50, but maybe there's someone else who's willing to do 60 40? Or maybe over time, in an Iterated game, instead of this particular opportunity, it starts making more sense for me to own more of the network rather than split with him. Because then it's like 1000 instead of 50 50. I mean, this is why proof of stake networks that draw the entire validator set over an epoch before the epoch, where you know, the blocks you can't avoid this problem, right? Because you already see this in Solana and Avalanche, right, where people are basically just paying validators ahead of their block number and co locating.
03:31:45.460 - 03:32:21.804, Speaker A: Yeah, I mean, even Dexar people are just basically paying. Do we need to avoid any kind of leader election that's in the future? Do we need like proof of work? Like leader election? Yeah. And that's a ZK thing, right? The single secret leader election kind of type of stuff is only enabled by homomorphic encryption and ZK stuff. So I think this cost goes down if cryptography gets better. It's not a purely economic can you use the algorithm leader election model that's like VRF based or something with a committee? I mean, there's still these poison committee attacks that are known. You also often grind too. Sorry to cut you guys off.
03:32:21.804 - 03:33:16.240, Speaker A: Do you guys want to take one question from the crowd before we get off? Super interesting conversation. I just want to open it up a little bit to if anyone's been waiting, has a question. I don't know how this works, but just point to the person and then Mike goes, yeah, guys, we have lunch one. So this will probably be one of the last questions. Where was the person there? There you go. This is a question for Alex about Flashbots architecture and how you see it changing in context of multi layers and modular blockchains. So, like Flashbots right now and the way it's designed is very practically helping democratize mev on Ethereum with enabling many searchers to participate and miners.
03:33:16.240 - 03:34:20.100, Speaker A: But in context of what you guys were talking about, with people with the most capital being able to get mev on cross chain world and multilayers and modular blockchains, how is Flashbots intending to change its architecture to help further democratize mev? And yeah, in context of obviously you guys are also trying to become censorship resistant and decentralized. So you seem to have a lot of competing priorities, but I think this is a big one coming up. So curious to know. Yeah, great question. We think a lot about the risk that I mentioned earlier, this centralization of validator sets across many domains and how this kind of world can be attacked. We also see it as like it's inevitable that the world, at least for the next few years, is going to be like MultiChain or whatever. Even if you focus only on Ethereum, you have many L2s that have raised like hundreds of millions are going to be able to subsidize activity for a while, even if it's artificial.
03:34:20.100 - 03:35:22.232, Speaker A: And so we need to prepare for that asynchronous world and what that means there, I don't know if looking at it as simply as like, oh, players need more capital to participate makes sense because even today on Ethereum there are Flash loans. But if you use Flashbots today, Flash loans are not gas efficient compared to having the capital directly. So a lot of players need the capital to be able to execute a lot of opportunities. So those that change massively in the cross domain world, I think what changes is maybe like the inventory management logic that you need to have and also the maybe economies of scale that if you have more capital, you can warehouse risk a lot better and so you can maybe take more opportunities or take more risks. So we think a lot about that. We think a lot about this dystopian future of very centralized blockchain world in general where you have a few Validators that run validators on 100 different chains and run bridges where they provide liquidity to users. That's like the worst thing for us and we need to adapt for what I just mentioned.
03:35:22.232 - 03:36:25.624, Speaker A: Now we're going to announce more on this relatively soon, so you'll have extra details there. But we're worried about the same thing that we were worried about when we started Flashbaults a year and a half ago, which is we want to keep the system decentralized. We want to keep it transparent as much as possible because the more you have opacity, the more it's difficult to understand what happens within it. What else do we want to keep it? We want to keep it accessible to most individuals. Yeah, I guess I'll add one point on the fairness aspect, which is that even though these bridges that have synthetic assets tend to be the ones that get exploited first. Synthetic assets? And like staking derivatives, especially in proof. Of stake networks do actually lower the cost of capital for the actual validator because validators basically smaller validators can lever up around opportunities that they want to kind of subsidize.
03:36:25.624 - 03:37:30.128, Speaker A: And so r1 question is going to be how do we balance this centralization effect versus should we allow more kind of derivative assets across bridges? And that's obviously going to be this big security question because wormhole is the greatest example of what happens when the staking derivative blows up. But it is clear that that actually does lower validator centralization in the long run. There's going to always be this kind of trade off between how much we want derivatives for equalizing access versus why does it lower the cost? Yeah. So let's say I'm a small validator and I have like 1% of stake I can borrow against my stake and basically, at least for a short amount of time get to say like three X leverage on my existing. Stake. And now I can go restake that, get 3%, increase my number of blocks and try to basically but surely if you own 50%, you can borrow three X leverage as well, right? No. How do you borrow three times 50? There's no liquidity for it.
03:37:30.128 - 03:38:01.880, Speaker A: There's no liquidity for it. There's this kind of liquidity versus security trade off that naturally comes up in stake derivatives. So it's always going to end up being this war between how much do we want these kind of very hard to price derivatives that can blow up, versus how much do we actually want centralization. Cool. I think that's a great place to end it. Thank you very much, panel. All right, guys, that wraps up our first half of the modular summit.
03:38:01.880 - 03:41:39.490, Speaker A: Lunch will be in the overflow room. So right over there. Yeah, go ahead and enjoy. It's going to be an hour long lunch, so we'll meet back here around two. Lunch will also be served in the atrium. So if you guys also want to hang back here and pick it up, that also works close. Jam.
03:41:39.490 - 03:46:21.154, Speaker A: Sam. Ram it. Sam. Sam. Rap. Bless you. Lam.
03:46:21.154 - 03:51:04.580, Speaker A: Sam eight. SA. Sam ram. WAM. Ram. Ram. Sam.
03:51:04.580 - 04:01:40.300, Speaker A: Sam and sam. Sam it. Ram SA. Sam ram. Sam. Sam. Taste left.
04:01:40.300 - 04:07:16.200, Speaker A: Taste left. Left. Hold on. Eight left. Sam ram. Ram. Ram.
04:07:16.200 - 04:11:54.860, Speaker A: Ram. SA it's. SA. Ram. SA. Sam SA. Damn it.
04:11:54.860 - 04:20:13.394, Speaker A: SA. Ram don't. Ram. Sam. Sam. Sam. Down.
04:20:13.394 - 04:32:00.368, Speaker A: Eight left. Eight left, eight left. Blessing. SA. Sam lam sam nam. Sam ram sam ram sam we sam SA I'm ram it's. Oil sat.
04:32:00.368 - 04:47:53.670, Speaker A: Sam don't you sam don't ram it. Hold on, hold on. Eight left. Om rai WAM sam sam rat up the second half. So please come and sit down. Ram, ram. Ram and sam weight.
04:47:53.670 - 04:48:30.450, Speaker A: Hey, everybody, come and find a seat. We're starting up the second half of the modular summit. I'm Nick White. I am COO of Celestia Labs. It's my pleasure to be here. I'm so thrilled to see so many people in our ecosystem, the modular ecosystem here today and the quality of conversations has been phenomenal. So we have an action packed second half of the modular summit.
04:48:30.450 - 04:49:18.082, Speaker A: I want to make a few announcements. One is that there's a research track in a room over here and we have a lot of really if you're more like research oriented and you want to see some of the most cutting edge things with roll ups, interoperability, data availability schemes that's happening over here. But don't walk through here, go around the corner, past the bathrooms. So hopefully some of you go there because I don't want them to be in an empty room. Aside from that, we have networking drinks at the end of this segment, so at 530 and we'll also be handing out T shirts. So I hope you guys stay around until the end and hang out and talk to us. So what do we have coming up? We have a bunch of really cool stuff.
04:49:18.082 - 04:50:00.000, Speaker A: We have a data availability landscape panel that will have feature ethereum, polygon avail, Celestia. We also have a debate between Mustafa and Anatoli from Salana about modular or monolithic. And that's going to be right up until 530. So anyway, it's my pleasure to announce or introduce the next guest. He goes by a few different monikers. Some people know him as Altcoin Slayer, some people know him as Erica the blogger. But he is really one of the most insightful shitposters on.
04:50:00.000 - 04:50:19.446, Speaker A: Yeah. So without any further ado, I want to introduce Eric Wall. He's going to talk about roll up security paradigms. Thank you, Nick. Thank you so much. Hello, everyone. My name is Eric Erica combination of those.
04:50:19.446 - 04:51:25.450, Speaker A: I'm a Swedish crypto fund manager, blogger, VC, but I'm also recovering bitcoin maximalist and I've been recovering for about three years now. And I want to tell a story. I think this is the story of why I'm here on this stage and the story of how I grew out of bitcoin maximalism and why I think this is one of the most important things that is happening in the blockchain space right now and why I couldn't think of any more crucial place to be. So I want to congratulate all of you guys that are here and I want to share my story with you, how I came to that conclusion. So, like Nick said, I used to be called the altcoins layer in this picture, you see some of the coins that I supposedly killed. They're still out there. I mean, it's really hard to kill coins, but supposedly I killed Iota neo heterohashgraph.
04:51:25.450 - 04:52:15.498, Speaker A: And in this journey of killing altcoins, I always had this belief from the beginning that Bitcoin was going to be the currency that was going to the network, that was going to power everything else in the Blockchain ecosystem. And the rationale for that is that if you go back, we're going to go way back in Tamina. We're going to go all the way back to 2014 when the company Blockstream was founded. So if you read the vision statement of why Blockstream was created, it is actually kind of beautiful. It's a beautiful story. So it reads that the altcoin approach of creating a new cryptocurrency just to introduce new features creates uncertainty for everyone. Looking at cryptocurrencies from the outside, there seems to be no natural stopping point.
04:52:15.498 - 04:52:48.306, Speaker A: Each fork can be forked again at infinitum. This creates both market and development fragmentation. We think that for cryptocurrencies to be successful as a whole, we must build network effect, not fragmentation. We believe everyone should enjoy freedom to innovate too, without seeking permission from us or anyone. We needed a different way to get there than by attempting to disrupt our own success. It's kind of beautiful. To accomplish this, we propose technology to enable new cryptocurrency networks that do not need new cryptocurrencies.
04:52:48.306 - 04:53:46.610, Speaker A: Delivering on this vision will require continued investment and in cooperation with the Bitcoin ecosystem as a whole, along with the full time support of many people with broad and specialized backgrounds, we felt that in the bitcoin ecosystem and in the world at large, there is a shortage of companies working on trustless cryptographic infrastructure. That's why we, along with our other co founders who share our vision, came together to establish Blockstream. So this was the picture that they shared, like their vision for how the blockchain ecosystem would grow. And I like this picture so much. This used to be the banner picture of my LinkedIn profile. I was so hyped up about that vision for Blockchains and for side chains to interact with each other. And these side chains, they weren't originally proposed as what we see today, where there's proof of authority chain with a bunch of validators.
04:53:46.610 - 04:54:55.760, Speaker A: These were supposed to be merge mind side chains, reusing the proof of work strength of Bitcoin to secure the transaction order. And you have the developers here speaking like, yeah, merge mining is an option. You could also have proof of stake and proof of authority, but merge mine side chains that reuse the hashing power of Bitcoin to secure these sidechains were a crucial element for this vision. And you can even go back and you see Greg Maxwell here talking about using Snarks to interpret the programs running inside of sidechains, which is basically what we have today with ZK roll ups. So this was already envisioned back in 2014 by the blockstream developers. And what was the purpose of being able to run a snark to interpret other programs? Like, what was the vision? Well, it was the vision already then to build things like DEXes, to build trustless, peer to peer marketplaces options, contracts. All of those things that we see blossoming in DFI were already the vision back then.
04:54:55.760 - 04:56:01.874, Speaker A: And in 2015 they said, in the coming weeks, we're going to publish proposal for a fully decentralized two way peg. The peg is how you transfer an asset from the main chain into the side chain and back. That would be completely decentralized and the side chain itself would be mergemind. But it never materialized. But in 2018, I still believe that this was the vision for bitcoin, that we would see this ecosystem of side chains emerging around bitcoin that would be trustless, merge, mind. So I would go around writing these twitter threads saying, like, in architecture, it is a no brainer that every structure of importance that we build, whether it be religious monuments, skyscrapers, pyramids, starts with the establishment of a rock solid foundation to persist through the test of time and with the billionaires to come. So I was really in this vision that bitcoin as the strongest, most solid consensus engine, would power everything else.
04:56:01.874 - 04:56:53.718, Speaker A: And around that point in time, we were thinking in terms of drive chains. Drive chains were the latest iteration of how to build these hash rate backed side chains to bitcoin. And this is pretty funny quote here. So I believe that the only way to compete with bitcoin is by figuring out how to make a more decentralized, robust consensus engine. And the problem that I had with all coins was that all of them were competing in bitcoin in the exact opposite way. So they were trading away their core robustness and simplicity for smarter contracts. And this is before I understood the concept of modularity, because if you think about Celestia, Celestia really is an even more simple base layer than bitcoin is.
04:56:53.718 - 04:57:29.914, Speaker A: Bitcoin bundles, settlement, execution, data availability together in one monolithic piece. Celestia is that piece that abstracts the data availability and the consensus away and focuses on just that most simple piece. But this was before such constructions existed. So anyway, I was really hyped up about drive chains. I wanted to drop everything that I was doing at the time to build a drive chain. And even the bitcoin core developers, a lot of them were on board with this vision. They wanted to also see drive chains come into fruition.
04:57:29.914 - 04:57:51.062, Speaker A: They thought it was a good idea. But something happened. This is just one year later. Just one year later, something changed. And I think it was the block size wars that got so intense. And what a lot of bitcoiners realized is that the miners are not our friends. The miners had previously been involved in every upgrade in bitcoin using their hash rate.
04:57:51.062 - 04:58:44.550, Speaker A: They signaled their support for upgrades, and the miners were working together with the community. But during the block size wars, we kind of realized that miners weren't our friends and we wouldn't want to give because in a merge mine sidechain, the problem is that the miners can always steal the coins from the side chain because everything is relying on the hash rate. So something changed in the sort of conversation around drive chains. We didn't want to give more control to the miners. And one thing that happens when you have a merge mine sidechain is that if you are not validating these merge mine side chains, you're not mining those side chains. You don't get the fees from those side chains. So if they have a lot of mev, only the miners that have very high capacity bandwidth are able to extract those revenues.
04:58:44.550 - 04:59:32.754, Speaker A: So something changed. And even Peter Todd, who's also one of the core contributors to bitcoin, said that merge mine side chains were Gregory Maxwell's biggest, biggest mistake. So 2019 kind of realized that it wasn't going to happen with bitcoin. Bitcoin was not going to become this piece that powers all of the blockchain ecosystem. And then roll ups entered the scene. And the powerful thing with roll ups, the really intelligent part is that the miners and the validators that process the base layer, they don't need to validate the roll up. They don't need to process the transactions.
04:59:32.754 - 05:00:38.738, Speaker A: They can only look at the fraud proofs and the validity proofs to make sure that those systems are safe. So if you are a bitcoiner and you're thinking that the only problem that we have with drive chains is that they cause centralization around mining, roll ups are that exact mechanism that you need to get away from that problem. So I sort of wanted to bring this to the bitcoin community and say, hey, look at the ethereum researchers. Look at what they have come up with that actually solves that core piece that we've been looking for all along. This should be a very interesting architectural design for how to build blockchains that can scale, that can have different execution environments, that does not cause centralization for the validator set. So I try to convince other bitcoiners, like, look at the architecture of this thing, just look at the construction. If we can just learn from this mechanism, we could integrate something similar into Bitcoin and then we would be able to realize that vision.
05:00:38.738 - 05:01:04.870, Speaker A: That was what the blockstream company was built on. That core vision was now within reach. It didn't go that way. So I tried to bring it to the bitcoin community and I asked, I'm reading this stuff on the ethereum research page. I'm reading on Vitalik's blog post. I'm finding new ways that we could reach that goal. And what does that make me? And I got told that it makes me potentially dangerous.
05:01:04.870 - 05:02:04.630, Speaker A: And a lot of this is also a Blockstream employee. So I basically fell out of favor with the Bitcoin community just for having the idea that maybe there's something interesting going on in the Ethereum community that we should learn from. So people were starting to make bets around how long I would last in the Bitcoin community. And this vision that was originally proposed about how Bitcoin would power these side chains, what it all came down to at the end was just this. The only side chain that Blockstream ever created was a single proof of authority chain that had no crypto economic guarantees. That was the culmination of half a decade of research that Blockstream was supposed to deliver to Bitcoiners. So Eric the altcoins layer turned into Eric the potentially dangerous shitcoiner.
05:02:04.630 - 05:02:42.610, Speaker A: And I was feeling miserable at this time. I started to write my own eulogy. It was kind of a sad time where I didn't know what to do because I had been building up my reputation and my community, my friends within the Bitcoin community for seven years, and this is what it all came to. But I found new life in the roll up design space. There are so many things that you can do with roll ups. You can experiment with completely new VMs. You can experiment with the sequencing logic, the fee logic, the mev.
05:02:42.610 - 05:04:09.950, Speaker A: You can make combinations of ZK roll ups and optimistic roll ups. It opens a whole new paradigm for innovation, experimentation. One of my favorite roll up constructions is the ZK Operu construction, which allows you to make Zcash style private payments inside of an optimistic roll up. So you can get almost perfect privacy within an optimistic roll up just by leveraging the fraud proof system on the base layer of Ethereum. Another thing that I think is hugely interesting is the idea that if you have a roll up and you have perhaps a more centralized sequencing mechanism, that creates an opportunity for me to be generated in a more centralized way, and you can do things with that mev. For example, one of the biggest problems that we've had in Bitcoin is how do we fund developers to keep contributing infrastructure to Bitcoin? How do we fund developers, how do we fund public goods? So this idea by optimism that we could use the mev in a chain that the sequencer extracts to fund public goods is an incredibly interesting idea. And following along that path, if you have the ability to create an entirely new VM just by because if you're running a roll up by using the fraud proof mechanism, you don't actually need to be able to interpret all the internal logic within the roll up itself.
05:04:09.950 - 05:05:05.566, Speaker A: All you need to be able to do is validate a fraud proof or validate a validity proof. Those things allows you to create entirely new VMs. So inside of Fuel, which is one of my favorite roll ups, you can run smart contracts using a UTXO data model and that allows for parallelizability. So you can parallelize transactions and get basically solana type efficiency inside the roll up. So that's a whole new paradigm of an execution environment that is enabled through the separation between the main chain and roll ups using fraud proofs or validity proofs. So I want to talk a little bit about what does this mean for security. So there is a difference between roll ups and side chains.
05:05:05.566 - 05:06:06.594, Speaker A: Even the merge mind side chains that we had roll ups are superior. Because like I said, in the roll up paradigm, what you can do is you don't need hash rate to secure a side chain because the fraud proofs that get posted to the main net and the validity proofs that's the only thing that you actually need to validate for the validators or the miners to do their job. So roll ups present a way more attractive security model inherently than sidechains were ever able to produce. But they still come with some drawbacks. There are some people who say, well, roll ups, they are L2 and the L2 inherits the security of the layer one. That is roughly accurate, it's not 100% accurate because there are of course some things that there is some level of complexity that comes with a roll up that creates security risks. So it's not like one to one perfect equivalency between the security of the layer one and the L2.
05:06:06.594 - 05:06:43.358, Speaker A: There are some differences there and I think that it's good to perhaps get a grip of what those differences are. So I mean the most obvious one is that of course you're going to run into some implementation risk. The roll up has a roll up node, that roll up node has its own software. There can be bugs in that software. It's just no way to get around that. If the roll up isn't implemented properly, even on the contracts under layer one, a bug can arise. Like if you look at most of the hacks that have happened to side chains, only a few of them actually happen because someone compromised the keys.
05:06:43.358 - 05:07:25.338, Speaker A: In many cases there are simple implementation bugs that can cause complete failures for the roll up system, for a side chain system, for any type of system. So every time that you use an external software component that can have an implementation bug and you can lose your money by using a roll up this way, there's no way to get around that. The nice thing is that we can theoretically move away further and further away from that risk by improving our systems. With a side chain, for example, you can never get away the inherent risk that either the miner steals the coins or in a proof of authority side chain. You can never get away the risk that the validators steal the coins in the side chain. But you can get away from that risk. If you design a roll.
05:07:25.338 - 05:08:00.074, Speaker A: Up really well. You also have a sequencer risk in roll up. So roll up doesn't use the same validators, they don't use the same block producers as the Layer One does. They have their own set of sequencers that produce the roll up blocks. And there are some risks there. The most common risk that you speak about is, well, what if the sequencers start to censor transactions on the roll up. Now, for optimistic rollups like Arbitrum and optimism, there's a way around that.
05:08:00.074 - 05:09:03.040, Speaker A: You can post transactions directly to the Layer One contract on the base layer and that makes the roll up process those transactions so you can get away that censorship. It's not as easy on StarkNet, for example, I haven't seen a design that allows transactions to be posted on the Layer One that will automatically be included into the StarkNet roll up. You also have mev risk. I actually prefer to think of the mev difference here as an advantage. Like I showed with optimism that you can actually use the fact that you have a different set of sequencers that can be permissioned or have priority and they can generate MAV and they can decide that this MAV is going to go to benefit the overall system. But there are still differences in how MAV works inside of a roll up compared to the Layer One chain. So that's also a risk that you got to think about.
05:09:03.040 - 05:10:05.398, Speaker A: Another thing that I think that people don't talk that frequently about is something that I call state access risk. So a lot of people believe that, well, in a roll up, all the transactions data are posted to the Layer One. And because you have full data availability provided by the Layer One system, that means that you're never going to have a problem to access the state. And you need to be able to access the state. If you want to be able to withdraw your funds from a roll up, you need to have access to not only the raw data itself, you need access to the full state. And the risk here is that, okay, well what if there aren't any nodes who are providing me with this state? The risk then is that you and if you haven't produced that state for yourself, if you're not running a roll up node which in a roll up, their execution environments are designed to be more performant. So it's going to be more resource intense to run your own roll up node versus running a Layer One node.
05:10:05.398 - 05:10:47.342, Speaker A: So if you're not doing that, which most people won't do, and for some reason there is no other node that is providing you with what the state looks like, then you can run into a situation where you actually have to generate that state yourself. And if the state takes a very long time to be computed, then let's say for example, you're getting liquidated in another roll up and now you got to withdraw your funds from that roll up into the other roll up and that takes time. Now you got to regenerate the state. The time that it takes to generate the state is a risk. It can cause you to get liquidated, it can cause you not to access your funds in time. Maybe those funds lose value. Like maybe there's something happening in the cryptocurrency space and your assets are losing value.
05:10:47.342 - 05:11:34.122, Speaker A: You need to be able to get those out and sell those. If that takes time, well then that's a risk. I'm coloring these in yellow because these are not like catastrophic risks, they're more like mediocre risks. And then in most of these roll ups, the reason that these two are white is because these problems are not inherent to roll ups themselves, but it's something that we're likely to see with most roll ups. So most roll ups in the beginning are going to have admin keys, the ability to make emergency upgrades. That of course is a risk. The idea is that we will move away from those training wheels with time and we won't really have these admin keys that can change the entire roll up system once these systems are mature.
05:11:34.122 - 05:12:29.760, Speaker A: But they're probably going to be with us for a couple of years. These types of admin keys and a lot of these roll ups are also planning to add governance tokens. And governance tokens also provide an attack vector. Like if the governance tokens has the ability to vote on something crucial for the system, then if someone is able to acquire a large amount of those governance tokens, well, they get control of a part of the roll up. So that's also a risk. The most secure roll up would probably be a roll up that doesn't use like if we had no implementation bugs, then the most secure roll up would probably be a type of roll up that has had as little as possible of these types of admin keys or governance tokens, or at least limit their ability to do anything material to the system at all. So those are some inescapable things that you sort of get with roll ups that make the security different from the layer one itself.
05:12:29.760 - 05:13:05.526, Speaker A: So then there are also differences between ZK roll ups and optimistic roll ups. I'm not going to go to get too bogged down into this, but it's good to have at least the basics of it. I think that most people in this audience probably do know the basic security assumptions that trade offs that you make between a ZK roll up and an optimistic roll up. The ZK roll up. Of course. Uses Starks or Snarks. Those have significantly higher complexity and a lot fewer people that actually understand those systems in detail to be able to audit them to a level where we have an assurance that the system is going to work as intended.
05:13:05.526 - 05:13:54.486, Speaker A: So they are more complex the probability that they have implementations bugs are significantly higher, even though in a system like Arbitrarium, for example, the Arbitrum virtual machine is also quite complex. There's also an added risk of complexity there. But I don't think it's fair to say that these risks are comparable. I think it's probably fair to say that the ZK roll ups do add a more material layer of risk. But in the optimistic roll up you have the assumption that someone is watching the chain. If the sequencers post an invalid merkel route, then a merkel route commitment, then someone needs to be able to produce a fraud proof and invalidate that update. So roll up has this other trust assumption that you need to trust that someone is watching the chain.
05:13:54.486 - 05:14:42.614, Speaker A: Either that has to be you or that has to be someone else. And ZK rollups don't really have this assumption. And then if you are using another system or another component to provide data availability, then you're always exposed to the underlying risk of that system. We used to say that validiums, for instance, a validium is a ZK roll up that does not actually post the data on the layer one. It uses another entity or a federation of entities to guarantee that that data is there. We used to say that these validiums had a low risk profile because the only thing that they could do is freeze the funds. They could freeze the system, but they couldn't steal the funds.
05:14:42.614 - 05:15:43.390, Speaker A: But we've sort of come to the conclusion that if you're able to freeze a roll up, it's very easy to turn this into a ransom situation where you're ransoming the funds. And even if you have emergency keys which allows you to roll back the state of the validity, that opens up another attack vector which allows the attacker to double spend their funds because when you roll back the attack, then the attacker can just do that attack again. So you create a double spend risk. So no matter how you want to wrap the burrito, if you're using other data availability systems, you're always going to run into this issue that you sort of have to trust that those components are functioning correctly. So I also have this picture, I saw that Mustafa had this one earlier I want to make a joke about. This is the modular conference. So we're outsourcing other people's diagram skills to the ones that are most suitable to create those.
05:15:43.390 - 05:16:33.114, Speaker A: So that's my excuse for using other people's diagrams in this talk. So we sort of covered everything except these two so far. We're talking in very, very broad stokes, but I don't think that you're going to be able to learn everything about every single trust assumption between different roll ups in a 30 minutes talk. But let's talk about the broad strokes here. So if you're running a roll up on Celestia, there is a difference between sovereign roll ups and settlement roll ups. So the sovereign roll up has its own settlement and execution environment. So that allows for a lot of flexibility because you can hard fork or soft fork the roll up itself and change the rules of that roll up without having to depend on any other community.
05:16:33.114 - 05:17:23.866, Speaker A: Celestial is not going to stop you for changing the execution environment of your roll up. So if you want to change something inside the execution environment, you can in a sovereign roll up that gives you flexibility. That's why they're called sovereign roll ups. But that does mean that the roll up can change as long as the roll up community decides that the roll up should change. And that means that if it's a small community, if it's a small roll up and they are malicious, they could hard fork and perhaps steal your funds from that roll up. So you have a different risk profile in a sovereign roll up versus a settlement roll up. The benefit of a settlement roll up is where you share an execution environment where many different types of roll ups can settle their fraud proofs and validity proofs.
05:17:23.866 - 05:18:28.606, Speaker A: And if they are all batched together in one layer, then one roll up can't go rogue and hard fork their system because that makes them incompatible with all those other roll ups that they have enjoyed the ability to interact with. So if you're using a settlement roll up for the compatibility with other roll ups, now you can't hard fork one of those. So it sort of ties all of those roll ups into that specific settlement layer. So using a settlement roll up with many roll ups sort of batches their security together in a different way. Last thing I wanted to steal another diagram here from Maven Rain and Coffee made this excellent chart. I think that shows a little bit easier how these different rollups interact with the Celestia system. So on Celestia, you can have a sovereign optimistic roll up, you can have a sovereign ZK roll up and then you can have these restricted settlement layers for roll ups like Sevmos.
05:18:28.606 - 05:19:30.440, Speaker A: But you can also have other settlement roll ups like that on which you can build other roll ups that sort of gives you a better picture of what the ecosystem looks like, I think. And if you want to get really bogged down into the very nitty gritty differences between an optimistic rollup and a validium or a ZK roll up, I think that Matter Labs did a quite good deep dive into the nitty gritty differences in terms of security, but also performance, usability and other aspects that you can look at. So I would recommend to read from this website, it's not 100% perfect. You got to realize that Matterlabs they have a ZK roll up themselves, so they might be a little bit more biased towards ZK roll ups with that. That was the end of my presentation. Thank you very much. Great to see you guys, so many of you here.
05:19:30.440 - 05:20:06.974, Speaker A: I don't know if we have any time left for we are out of time. All right, but thank you, Eric. That was awesome. Cool. Well, I'm very glad that Eric recovered from his maximalism, because in Celestia and in the modular ecosystem, we say modularism, not maximalism. So I think he's now in the modularism camp, which is good to see. The next panel is about modularity emerging in different areas of the blockchain ecosystem.
05:20:06.974 - 05:20:41.870, Speaker A: So not just modular protocols like Celestia or roll ups, but also modular bridges or modular software stacks like the Cosmos stack that Zucky was talking about earlier today. So, without any further introduction, I'm going to bring on our panel guests. So we have Ismail who's CTO of Celestia. He's a moderator. We have Marco, who is the head of Cosmos SDK. We have James Prestwich from Nomad Bo from Polymer, and of course, Alex North from protocol labs. So give a round of applause.
05:20:41.870 - 05:21:12.466, Speaker A: Hello. Can you guys hear me? Yes. So why don't we start with a brief round of introductions? Marco? Yeah, I'm Marco. I work at Interchange Gambah, and I'm the current product owner of the Cosmos SDK. I've worked from Tendermint all the way to the Cosmos SDK. So just having fun with it. I'm James Prestwich.
05:21:12.466 - 05:21:50.498, Speaker A: I'm currently CTO at Nomad, which is a modular bridge focusing on smart contract chains and arbitrary message passing. Hi, I'm Bo, co founder at Polymer Labs, and we're working on a universal IBC interoperability hub. Hi, I'm Alex. I'm an engineer at Protocol Labs. I've spent the last three years or so working on filecoin as a software and protocol engineer. Thank you. So Mustafa made this very clear distinction this morning, what a modular blockchain is and what modular software is and where the distinction? You drew a clear line.
05:21:50.498 - 05:23:06.998, Speaker A: So let's also briefly summarize what modular design in general is. Modular design means that you basically break up a system into smaller parts, so called modules that can be used in other contexts that can be changed, or basically can plug them into other systems as well. And in modular software, or like modular programming, you basically subdivide your program in modules where the modules only execute. The part that you care about module of blockchains is when you basically outsource core functionality of a blockchain. I think that was made very clear throughout the day. So I wanted to ask the panelists, where do you see modularity emerging basically in the fields that you work on in the Cosmos SDK? I think Zaki took most of it away in this talk, but maybe you have more for us. Yeah, unfortunately, I didn't see Zaki's talk, so I don't know if I'm going to say something wrong now, but the Cosmos SDK was always built or the Cosmos ecosystem was always built with the mantra of there'll be many chains in the ecosystem.
05:23:06.998 - 05:23:46.040, Speaker A: Kind of this interchange world with IBC. And so the idea of modularity in a framework to rebuild, to build new blockchains and make it faster to scale has always been at the core of Cosmos. And so with the Cosmos SDK, it's evolved over the last it's been around for like six, seven years now. And now at the most latest, I think it's the most advanced, and it's truly modular. Now we're seeing chains pop up in the Cosmos ecosystem that go from inception, and then one month later, they have main net, and we really haven't seen that. And building a modular software stack allows us allows people to accelerate their products. Thank you.
05:23:46.040 - 05:24:56.378, Speaker A: Right now in bridges, we're seeing a lot of modularity in the separation between communication channels and the applications that they run, between how you actually pass data between chains and between what's done with that data, like token bridges, NFT bridges, governance systems. One of the things we built at Nomad that we're very excited about is a strict interface for how a communication channel connects to an application. And this should let applications like bridges swap out IBC for Nomad channels, for anything else they want. In general, we think of modularity as finding the right place to separate a concept into two smaller pieces and finding the right interface for that finding the right interface for that separation and defining the contract around it, how people use it, and what they can rely on. We see IBC as defining those interfaces, like James said, in a really nice way, kind of separating the transport authentication ordering layer from some of the application semantics. You can kind of see it in the interchange standards. There's clear distinction between the two layers.
05:24:56.378 - 05:25:51.540, Speaker A: They have a number of specifications that they review by the community and across different organizations. And one more thing I would like to say about what Mark was talking about in terms of the Cosmos SDK is that I think the Cosmos SDK does a really good job of making the components so modular. You can replace certain components, like with what Celestia was able to do, come and replace tendermint core with optimate and basically do what they do and be able to innovate. And that's a really important thing that I see here, is that it allows different organizations to come together, build together, and innovate very quickly. Yeah, I mean, modularity of software is very core to how protocolabs tries to build things. IPFS, for example, has a sort of strong brand as a thing. But really IPFS is a collection of protocols and components that are put together, a data model in IPLD, some network transports from Lib P to P, a few different ways of exchanging information.
05:25:51.540 - 05:26:32.106, Speaker A: And so really, we don't think of there being a protocol as IPFS. But a node can participate in an IPFS network if it uses a similar network protocol and if it adopts one of the transport protocols, one of the data exchange protocols that one of our IPFS nodes will use. And then similarly, those pieces can be taken by other projects. And so other projects will use Lib PDP for their networking. That automatically puts them a step closer to interoperating with other projects that do the same thing. And the IPLD data model, again, is not specific to IPFS and is a foundation for other applications data models. Thank you.
05:26:32.106 - 05:27:17.350, Speaker A: Yeah. So the Cosmos SDK was mentioned several times in the Cosmos SDK and particularly in Tendermint. Do you see anything that is currently more monolithic that should be more modular? Marco thankfully, no one here who works on Tendermint core very opinionated on this. Tendermint was always built as a library. So kind of when Jquan and Ethan Buckman wrote it, the idea was, you have consensus, you have P to P and all these things. But it's like if someone wants to switch out consensus, like, from tendermint consensus to hot stuff, it should be easy enough to do it. It was always meant as, like, a consensus framework, a consensus library.
05:27:17.350 - 05:28:21.306, Speaker A: And over the years, building modularity is very difficult, especially at a low level, because you're kind of making assumptions of how the application developers will use it, how the clients will use it, is somewhat easier for application developers. The deeper they go, the more freedom they need in developing their software. And so at the lower level, tendermint has kind of shifted more less from being super modular to being a bit more opinionated, because if you make too many opinions or you don't make that many opinions, then it becomes really hard to build software. But in Tendermints in general, I just wanted to be more modular. I wanted to switch out P to P with a click of a button instead of having to fork the code and rewrite some stuff. Yeah, that's what we do at Celeste. So one aspect that was also mentioned several times is, like, also, would you also describe it as modular, or is it something specific to the Cosmos ecosystem only, maybe? James I really enjoy working with IBC.
05:28:21.306 - 05:28:49.766, Speaker A: It is modular in a sense that it can be applied to other ecosystems. I think that the amount of effort it takes to reach other non tendermint ecosystems right now is very high. Actually. It's actually a lot easier than we first thought. Okay, I'll talk to you after the panel. I'm interested. It took a number of years to develop IBC for Tendermint.
05:28:49.766 - 05:30:20.342, Speaker A: In the first place, it's going to take a little while longer to productionize it outside of the Tendermint ecosystem in, like, the E Two model, where they have a proof of stake system that admits some reorgs. We have a lot more work to do to parameterize IBC and make it work for the Casper based consensus system, which is why we focus on not just the IBC connection, but what applications can we build today that can be eventually connected by IBC once it's ready? And what channels can we build that can be used today that can be swapped out for something more secure or faster once it's ready? This is one of the great parts of modular design is it allows us to go build scout modules that go fast, break things, colonize new ecosystems. Not colonize, but explore. Explore, that's a good word. Tracer style, wander around new ecosystems, find what works, and then we can bring in these heavier and better things like IBC in the long run once we have figured out what an application developer wants in a specific chain ecosystem. Modularity allows progressive development and upgrades over time. Let's talk a bit more about Bridging, an IBC in that sense.
05:30:20.342 - 05:30:56.930, Speaker A: So modular blockchains, does it make your life much harder as a Bridging provider or like someone building bridges, thinking about bridges? I think so. Sorry, you can go. I think so. I think there's more considerations to make. For example, like when you separate consensus or perhaps execution from the data availability and consensus layers. Now you have to say like, well, on this end we have data availability headers, on this other end we have state proof headers. We have to somehow combine these things when we want to perhaps produce a fraud proof.
05:30:56.930 - 05:31:49.890, Speaker A: But yeah, I think there are more considerations to make. And how all the pieces fit together, it's not fully clear yet, but I think we're going to be able to figure those things out. Yeah, one of the interesting challenges is that the bridges like IBC, which are kind of the ideal, most secure model you can get, they mix the execution and consensus layers. They have to cross that boundary. They can't be confined to one or the others, the bridges that we work on. And that can be iterated fast, they stick in the execution layer, they don't really touch the consensus systems. So modularity might make this more challenging to reach the long term goal of having these consensus layer bridges everywhere, because there will be more consensus modules that we have to account, more execution modules as well, which will also play into these kind of bridges.
05:31:49.890 - 05:32:53.826, Speaker A: Yeah, let's take a step back and talk more about modular software as aspect. So first of all, do you see any downsides of building your software more modular? Maybe Alex or Marco? Sure, yes. There's always a trade off, I think, to building something more modular. To build a module to be reused by others is in a sense, to take on an obligation to serve others needs and you don't know who they are when you start building and they don't know who they are when you start building. But eventually in order to serve some needs well, you'll have to optimize towards those and necessarily be less good for some other use cases. And so at some point someone else coming along with a new novel application, wanting to plug it together out of modular pieces will find that they can't get the performance or the features that they need. And so at the other end of the stack is sort of vertical integration.
05:32:53.826 - 05:33:48.406, Speaker A: And there is a great place in the world for vertically integrated software and vertically integrated systems. Car manufacturing is probably a timely example now where for a long time, car manufacturers were outsourcing and modularizing and buying and putting together components from other lower level manufacturers. And there was this old joke in the auto industry that you could see the chart of the car company from the layout of the dashboard. And you could see like, there was a climate control team and a radio team and a navigation team and so on. But then Tesla came along with entirely new needs. And so they are a vertically integrated car company and they make almost all of their own components and their own manufacturing processes and can produce a different thing at a price and performance that could not be met by stitching together the modules that existed beforehand. And so I wouldn't be at all surprised if we see a continual sort of back and forth between modularity and vertical integration.
05:33:48.406 - 05:34:45.130, Speaker A: In five years time, maybe we'll be at the vertical integration summit about all these special purpose blockchains to kind of add on to that. It's like when you're building modular software, you're also providing a guarantee of security and kind of the assumptions you made when building the software. But especially you can't predict what a developer will do when they use your software. And so you providing a guarantee for this software and then maybe them taking it and changing a few variables kind of breaks that guarantee. But for them, it's like if they say, we are using the Cosmos SDK and we got hacked, it's like, no, you guys are using a fork of the Cosmos SDK that you guys rewrote a bunch of stuff. This shouldn't really fall on us because Cosmos SDK, we provide a security guarantee of the software that's in the code base. Yeah, you kind of have to teach developers not just about the system that you built, the module itself, but about the interfaces and the boundaries of that module and what they have to do to keep the module working with other modules correctly.
05:34:45.130 - 05:35:29.234, Speaker A: So there's a lot more developer mental overhead for these things. Yeah. And you end up talking with a bunch of users like, how are you going to use the software? And I think in the current blockchain space in the past year or something, we've kind of run into this cycle of like we're kind of rinse and repeating many similar products. And then, so it's like, you build your modularity to serve these products, then you all of a sudden get a team that does something out of the box and you're like, okay, wow, this is where we want to be going, because this is pushing the boundaries. This is something that's next, this is the place that we're comfortable in right now. Yeah, I think that brings up an interesting point that I want to connect to. Alex over here is we always start off building bundled projects, everything's integrated in Ethereum.
05:35:29.234 - 05:36:18.710, Speaker A: And it's because we just didn't know how to do any better at the time we built Ethereum the way we did because we had no idea what modularity was or what the right boundaries between different parts were, or where the consensus coin is even more extreme to that. It's more monolithic like more integrated. Yeah, we built it monolithic because we didn't know what the right module boundaries were. Now that we know, we can build modular systems and then eventually, once we have modular systems everywhere, people are going to want to go back to monolithic to eke out those little gains they can get by eliminating the edges between modules. Yeah, that's the typical problem. Yeah, exactly. In web development we've seen it go from monolithic services to microservices and back to monolithic because people get frustrated with whatever the current model is.
05:36:18.710 - 05:36:46.366, Speaker A: That's basically what Alex also said previously. There will be a constant back and forth. Yeah, we're in a great space now where it's all about the pace of innovation and modularity is fantastic for you can quickly plug together a few things and maybe you take some trade offs, but you can learn really fast. But once we really know what we're doing james is right. We didn't know what we were doing to start with. IPFS is just the same. Lib PDP didn't birth as an independent project, it was extracted from IPFS and so were some of the other protocols over time.
05:36:46.366 - 05:37:23.082, Speaker A: And then we realized what we're doing and what we want to plug together. And then when someone really knows what they want to do, they're going to go and do it from scratch in vertically integrated way again, where they're going to sacrifice the ability to change it later. Yeah, and we had to do it that way because we didn't know any better. Totally. We had to experiment with it first as a monolithic thing to figure out the right way to build modularity. So some theme kept popping on. Recurring theme that was mentioned on the panel is like the interaction with users and people using your modules potentially different in different ways that you didn't anticipate.
05:37:23.082 - 05:39:03.886, Speaker A: Maybe. So one thing that I wanted to ask, and I don't have an answer to it at all myself, but how do you basically maintain these modules from a governance point of view? Potentially you have tenement, maybe you have the Cosmos SDK and its modules and you have something like Lippy to P, which is widely adopted across the blockchain space, or other non crypto projects as well. How do you make sure that this is somewhat like is it like in any other open source project where you. Basically maintain it and you discuss things on GitHub and this is like you submit RFCs and you discuss and you discuss and eventually you come to an agreement and you implement this or that feature or this and that addition to the protocol. Is it exactly like this? Or are we in a space where I feel there's all these projects that are kind of like collaborating using the same software, using the same open source software and they are at the same time collaborating but at the same time somehow also competing? It's like do we need other means of governance and do we need to align incentives here or is that something that is unnecessary? I know it's like a tough question. I could probably do a whole summit about this topic, but I just want off the top of your head, what are your thoughts on this, particularly Marco and Alex? Maybe I'm going to have to think for a second. Yeah, sure.
05:39:03.886 - 05:39:46.810, Speaker A: I can start as far as I don't have anything that says it's different to other open source software yet, maybe it will be. Maybe we'll learn about something or perhaps some better way of doing things. But I think it basically comes down to open source software development. And most of my ideas here I'm like shamelessly parroting things from Nadia Egbal and her book Working in Public, which I think really sets the stage for the discussion here. Software, the code itself, once it's produced, is a public good. It can be taken and used by as many people as possible with no marginal cost. But software development and the software development resources, the people and the time and the attention is a scarce resource that is competitive.
05:39:46.810 - 05:40:40.578, Speaker A: It sort of comes down to a discussion about at what level do you want to be open and decentralized and permissionless. It doesn't scale for a team building a critical software component to accept contributions from everywhere, to answer every question, to allow anyone to have their say on how the software should be written. The team will immediately grind to a halt and be unable to make any progress on the things that are important. But we can sort of add the robustness, add the antifragility back at a higher level by agreeing perhaps on a protocol and then having multiple implementations of it. So there are multiple implementations of Lib PDP and they agree at the protocol level, but then are free to make all of their implementation decisions independently. And so it's robust in the sense that individual software libraries can fail. Most open source software libraries go nowhere.
05:40:40.578 - 05:41:20.546, Speaker A: They're pet projects, they don't gain traction, they don't go anywhere. And that's fine. And that makes the ecosystem of open source software incredibly robust because individual projects can fail on the way to us discovering and supporting the best, most useful projects and then one day forking them when we need different needs from them. So at least in the cosmo space. We're starting to get to the point where we have to put a lot of parameters that are kind of like feature flags inside modules for things that are changing to replicate the similar behavior. In the early days when the Cosmosoft first launched, and there's maybe a couple of other chains, it was easy to coordinate. Like, we go talk to these three chains.
05:41:20.546 - 05:42:09.100, Speaker A: Do they want this change? Some of them put it to governance, some of them don't. Some of them say it's fine, and then we go forward with it. Right now, there's roughly 35 chains, and if we introduce, like, a consensus breaking change that they may not read the change log for and all of a sudden have this irregular behavior that they weren't expecting, causes a huge, huge problem. And so feature flags are kind of like the worst thing you can do in software and just adding a bunch of them just because you end up having to maintain so much extra software that you don't know how. Many people are using but in an environment that you don't know how many people are using your software because they may be rebranding it as their own or they may be forking it and saying something else, then it's kind of hard to be like, okay, we're making this consensus breaking change. If you don't like it, you have to go do it on your own. Because we want to also avoid people just forking the repo just to do little changes.
05:42:09.100 - 05:43:11.254, Speaker A: This is a little embarrassing for me because I have an unmaintained Cosmos SDK module that needs to be updated for the breaking changes. Yeah, exactly. I'm feeling a little called out here. But you don't think none of you think that there needs to be basically some dow structure or something like that to maintain? Because in this space, there's always these incentives, right? Like, you have stakeholders, you have maybe even competing projects that use your stack, right? Yeah. It's a super hard question to answer. And in terms of governance and the Cosmos SDK land and Tendermint, it's unfortunate to say, but there's, like, a finite amount of people who actually understand the protocol, and there's a lot of people who use it. And so if you end up involving all these people who just use the software and maybe they write applications with the Cosmos SDK, they never have dove into tendermint.
05:43:11.254 - 05:44:04.170, Speaker A: And all of a sudden, you have to involve them in tendermint discussions on, like, oh, are we changing to live P to P, or are we changing to this? Are we changing to this? Then the process gets dragged out. Like Alex said, you end up hitting a wall, and then the spec keeps moving, and the developers kind of get burnt out because it's just kind of like you're trying to implement a moving spec, and then it just becomes an infinite loop. It's like you don't like democracy, basically, but yeah, I got the point and it makes sense, but it's a recurring thing. And you talk about governance. I think we're talking about interfaces and layers and also governance and consensus over those interfaces and layers. And Alex made a really good point about if you define certain interfaces, for example, within the Cosmos SDK, you have the ABCI. It's evolving to the ABCI plus plus there were some interface changes at the two client level to support the development of different light clients.
05:44:04.170 - 05:44:48.690, Speaker A: I think the idea is that you want to have some sort of consensus over these layers and how they're defined and then be able to have protocols come in and innovate and make trade offs beneath between the interfaces themselves. Makes sense. Do we still have time for a few questions maybe from the audience as well, if there are any? I always asked a few questions before and I was like, keep them for later, but not sure if people asked it here. Are there any questions? Don't be one over there. Where? Up there? Here we go. People are running away. Can introduce yourself? Hi, I'm Max.
05:44:48.690 - 05:46:09.280, Speaker A: Can you comment on maybe there'll be an overlap in the future where the communication mechanism, the bridges overtake the actual chain? A future where the communication layer in the bridges overtake the chain. So where we're looking at modularity, especially like with Celestia, where it sort of enables cross chain applications, would you see that the Bridging technologies become more dominant than say, on chain activity colonization? That's an interesting question. Typically the Bridging technologies can't provide what users want out of a chain. Users want to store their assets, to trade them, to lend them, borrow against them, all of these things that bridges are poorly suited for. The way we look at this future is that bridges are going to be asynchronous communication channels between applications that are homed on individual chains. And I don't see that changing in the near future. I do think that there will be some Bridging standards that are just on every chain, implement the same standard in solana rust and near rust and polkadot rust and solidity and everything else.
05:46:09.280 - 05:46:55.906, Speaker A: But I wouldn't say that they would dominate the chains. They might just be widely deployed on chain. More questions? There's another one over there. Hi. My question is more to the interoperability rather than modularity of parts. We're seeing now. A lot of the cross chain protocols are trying to build essentially similar cross chain messaging protocols as IBC is or XCM is for the substrate.
05:46:55.906 - 05:47:30.498, Speaker A: Do you see becoming they're becoming like a standard to allow them to interoperate or it's more of which ecosystem wins out. IBC is the standard. IBC is a standard. Tell this to the XCM people. We've talked with the Web Three folks and the polka dot folks about IBC. We're working with a couple of teams implementing IBC on substrates via grandpa like clients and mountain. There's some new consensus that they're doing.
05:47:30.498 - 05:48:12.430, Speaker A: And so we're working with people who are implementing both and the idea behind Cosmos and IBC. And everything is just kind of like interoperates with everyone because it's like a connected ecosystem of blockchain, not of a specific smaller ecosystem. Just makes everyone better. And so, in a way, we lift everyone up. IBC includes a lot of different parts of this bridge stack as well. So when we talk about IBC, we have to distinguish between which ICSS we're talking about, which touch different parts of the modular, like chain stack and bridge stack. So some ICSS will be the standard and some of them won't make sense in specific chain environments.
05:48:12.430 - 05:48:48.442, Speaker A: So, thinking about this a little more, to answer Max's question, I think a little more in depth. You could look at IBC as an example of the communication standard dominating the chain in that Cosmos chains are designed the way they are in order to support the current set of ICSS. Cosmos zones are designed to participate in IBC. And that is one of the defining, most important elements of Cosmos. Yeah, so you can use it off the shelf without adding another IBC client. That makes sense. There is, I think, another question over there.
05:48:48.442 - 05:49:32.714, Speaker A: Last question. Yeah. Hi, friends. Basically, I think, say all this modular stuff plays out two years from now. What are your thoughts on what this post modular world looks like? What things are we going to focus on? Is it more like application layer stuff? Yeah, that's a great question. To end the panel, may I start to step back and look at why we're here? We're trying to build the reason that blockchains and execution layers and so on are interesting is because we're providing a platform for applications to be built, software for people to use to achieve things, and in our case, ideally, things that are cooperative or collaborative in some way. And so there are a lot of modules to be built out in this platform.
05:49:32.714 - 05:50:28.298, Speaker A: With a nod to Mustafa's attempt to claim the definition of modularity earlier today. In a development, platform needs the execution layer, the CPU in the computer, and it needs some Ram, which is the state, and it needs an I O bus, which is the data availability layer. And it also needs storage, which is filecoin, and it also needs networking, talking to other computers, which is bridges. And there are lots of pieces here that need to all come together into a platform. And I think we will start to see some standard ways, some standard groupings of these modules in much the same way as, like, the IBM PC defined a standard platform, much to IBM's detriment, but which then provided the greenfield for application developers to go nuts on this platform that was well understood and widely implemented. And so hopefully, in two years time from now, that kind of platform is understood and we're talking much more at the application level. Thank you.
05:50:28.298 - 05:50:49.186, Speaker A: Okay, that was it. Thanks. That's a good way. Thank you, guys. Cool. That was really interesting. Thanks for the good questions.
05:50:49.186 - 05:51:21.606, Speaker A: It's clear that modularity has a role to play in many different ways, not just modular protocols. And I think there's a reason why we're seeing modularity emerge across the stack, because the whole, I think, blockchain ecosystem is starting to mature. So that was very insightful. Thank you again to the panelists. Our next panel is going to be about how we're scaling execution layers using optimistic constructs. So earlier in the morning, we talked about zero knowledge rollups. Now we're talking about optimistic rollups.
05:51:21.606 - 05:52:24.610, Speaker A: So leading the panel is John, who is from Delphi, and we have Proto, who is the highest IQ pineapple on Twitter, and Josh, who's representing Celestia, and Emily, who's representing Fuel. So give them a round of applause. How's my voice? You hear me? Nice. All right, so why don't we start with a brief intro of ourselves, and I'd like to also hear a short elevator pitch of what you're working on your project. I'm sure many are familiar, but it would be nice to have your own definition of it. So I'll start with myself. I'm John from Delphi Digital.
05:52:24.610 - 05:53:09.918, Speaker A: We provide cutting edge crypto research, and we are strong supporters of this modular paradigm. Hello, I'm Proto Lamta. I work at Optimism as a researcher with Optimism, we're working on a new upgrade called Bedrock, and this simplifies the protocol by modularizing by taking apart the roll up logic from the exclusion logic that we know as Gaff. And as part of this, we also have this new fault proof tech that will basically secure the withdrawals from the roll up to the layer one. All right. Hi, I'm Josh. I'm an engineer at Celestia Labs, where I lead the Sevmos team.
05:53:09.918 - 05:53:51.760, Speaker A: So I'm not participating on the L1 data layer of Celestia. I'm working on, if you recall, back to Mustafa's slides or Eric's slides, the Sevmo section, which is a sovereign roll up focused for settlement, so for roll ups to actually settle onto. And as part of that, we're using fraud proofs to actually kind of guarantee the security of that sovereign rollup. I'm Emily. I work at fuel labs. And we're building Fuel, which is the fastest modular execution layer. And we're building that using three different principles that's UTXO based parallel transaction execution, the Fuel virtual machine, and then a really superior software development experience.
05:53:51.760 - 05:55:07.458, Speaker A: Thank you, guys. Emily? I think now that we're in the roller paradigm, the differences between fraud proof and validity proof, those are kind of overplayed and well known by a lot of people. But I think what's under discussed is how dispute resolution mechanisms, fraud proofs, and particular implementations of those differ from each other. So I'd like to start with that and have your opinions on what are some design choices you've made when building these dispute resolutions mechanisms and if any, can you point out to some unique aspects of your implementations of dispute resolution? Right, so if you think of the early roll up design, everyone approached it, starting with the fraud proof and then later looking at what can we fit in into the execution? I think this really hurts. UX. Over time, we have learned that users want some on Ethereum. At least they want ethereum features.
05:55:07.458 - 05:55:50.546, Speaker A: So they're looking for an EVM or they're looking for some specific execution environment without workarounds. So to get rid of these workarounds, you have to approach it differently. You approach it with this type of VM that can run arbitrary code, maybe it can run something else than optimism and then do a fraud proof over this more generic binary of instructions. Yeah. So I kind of view fraud proof as a relatively more limited design space, I think, even than like ZK roll ups because you really have two ways of doing a fraud proof. You're either going to have reexecuting a transaction or you're going to have this bisection interactive verification game. For us, we're just using reexecution of a transaction and that's our kind of first attempt here.
05:55:50.546 - 05:56:41.842, Speaker A: And that's for purely practical know, we're building fraud proofs into the Cosmos SDK. The modular software architecture that Cosmos SDK provides is you can have consensus on one layer and then you have your state machine on another layer. And so it's a relatively clean software design to just add that execution layer, that state machine Cosmos SDK layer, and say, okay, now Light clients just have to have the ability to take your fraud proof, set up the state. The pre state that you give them right from your state witnesses and then actually re execute the transaction just as any other Cosmos SDK based execution layer would do it. So from it's just an engineering practicality question. Here at Fuel, we use a hybrid approach. So on one hand, we use UTXO based fraud proofs and then on the other hand, we also use interactive verification game fraud proofs that are at the VM level.
05:56:41.842 - 05:57:18.510, Speaker A: And we combine both of these to produce an outcome that works for our execution model. And we feel that this offers benefits because it means that we only need to necessarily rely on, say, a specification. And other designs might rely on different technologies like interpreting to WASM and then doing fraud proofs with WASM. And those technologies would then rely on the limitations of what the WASM team wants to implement for the language. And at Fuel, because we take this hybrid approach, we're just relying on the specification and that gives us some freedom. Interesting. I didn't know that about fuel.
05:57:18.510 - 05:58:52.314, Speaker A: So just to move on that, how far do you think, as an industry we are in fraud proof design space? Do you think in near future, do you think many of the problems are figured out and these implementations are going to converge on one design get more commoditized, or do you think we will see more and more some different implementations getting divergent, like some different flavors of solutions here? So in terms of research, I think we're getting there, but in terms of products, there can be so much more. So when you think about all this new tech that enables fraud proof over some arbitrary execution, you could do a lot more than just an EVM or just some specific type of smart contract VM. And so think of indexing services or other types of things that take layer one data and do these very expensive, like, huge computations and then just prove that the execution is correct. This can be very meaningful to, say, adding an indexer to a chain that wouldn't otherwise be possible with a smart contract. Yeah. From my view, I think we're pretty far along in research and like what I mentioned before, right, we have these two modes and you can kind of figure out how to implement those modes from an actual implementation perspective. I think it's a wider range because if you want to re execute transactions, right, you just pick that mode of fraud proof, then it's completely dependent on what execution environment you're using here.
05:58:52.314 - 05:59:26.258, Speaker A: Right. If you're going to use an EVM execution environment, you're going to have to have the ability to re execute EVM transactions. Same for Cosm WASM, same for Fuel, I'm assuming where you're boxed in by what your execution environment is. And so in that space, we're actually pretty narrow in what we've researched because quite frankly, blockchain right now we have a pretty minimal number of general purpose execution environments. I mean, look, we have like Solana, we have Cosm WASM, we have Fuel, we have EVM, right. Those are kind of the big four that cover most of these things. And then we have but, you know, even Team Right is working on an EVM transpiler.
05:59:26.258 - 06:00:17.594, Speaker A: So we're very small number of execution environments and that's going to actually drive what kind of fraud proofs you need to be able to generate, right? Yeah, I think we're going to see some general theoretical ideas shake out and become more standardized. But then in terms of the actual implementations, I feel like those will be specialized to the particular technology. Stack got it. Yeah. So we basically cannot consider fraud proofs in isolation from the execution models. I think everyone would agree with this. So I'd like to hear your experiences and get more familiar on what kind of constraints each pose on each other.
06:00:17.594 - 06:01:00.838, Speaker A: So your fraud proof posing on your choice of execution model and vice versa, what are some constraints that you witnessed if you could share your experiences here? Right, so we started with the wrong approach. We have a lot of constraints. Over time, we kind of freed ourselves going for more and more generic fraud proof. So we're not doing really a fraud proof over the EVM but rather we're doing a fraud proof over MIPS execution. You can target a Go program to different instruction sets. Most commonly, it's X 86. But MIPS is this simple instruction set that you can do a fault proof over.
06:01:00.838 - 06:01:53.110, Speaker A: And using this approach, you can compile the EPM implementation from GAF to this type of binary, but it doesn't mean that you cannot compile another type of GoLink program to a binary to do a fraud proof offer. So over time, I really do think that we'll start thinking outside of the box, something that's not a regular smart contract VM and do fraud proofs are for more interesting things. Would you think MIPS could become a standard here? Unfortunately, at the time of development, we didn't have the ability to target risk phi with Go. But by now, in recent updates in Go, this risk phi instruction set is now supported. So there is this more elegant instruction set that we could support. And the differences are not that large. It's the same kind of family of instruction sets.
06:01:53.110 - 06:02:14.046, Speaker A: Very interesting. Yeah. So in that vein, right, I mentioned we're limited by the execution environment. You have to re implement the execution environment and the kind of elegant solutions the optimism team has gone with. And as I understand, the Nitro VM is somewhat similar to that, where they essentially take the EVM, which is a somewhat hard to understand virtual machine. It's somewhat complex. Right.
06:02:14.046 - 06:02:44.582, Speaker A: And they compile it to this reduced instruction set computer, which is MIPS. And then RISC Five would obviously be like the ideal, more modern one, but it's just not quite kind of ready yet from, like, an infrastructure perspective. Right. And that simplifies our problem. We still need to be able to have the ability to kind of deterministically execute this thing. But then you're reduced to kind of a simpler problem, because really what you're trying to figure out with a fraud proof right. Is how do I execute the minimal amount of state transitions and give essentially the smallest amount of state to, I'll say, my Light client, right.
06:02:44.582 - 06:03:18.686, Speaker A: And then allow them do the smallest amount of work to prove that I gave them an input and an output, and they did an execution on that input, and they got a different output. Right. And I think we'll see this general purpose fraud proving hopefully kind of become the standard. And then what we have to see is this kind of software architecture on Light clients being able to execute these general purpose things, right. Where right now you have to run a Light client for your optimistic roll up that has to be able to run this MIPS VM, right. And it has to tie into that execution for us. Light clients in a Cosmos chain normally don't have any execution part in them.
06:03:18.686 - 06:04:06.014, Speaker A: They don't have a state machine. We have to attach a state machine to them and have an ability to start up this state. So I think we'll see it at this engineering level of how can you easily tie into these optimistic roll up chains to actually do more general purpose execution of a risk five or a MIPS based fraud proof. I see. Yeah. So I touched on this a little bit earlier, but because at Fuel, we have this hybrid approach, it allows us to be based on specifications instead of being based on running fraud proofs based on, say, WASM or something like that. So I feel like in general, the fraud proof schemes that we'll see are going to be tied to execution.
06:04:06.014 - 06:05:01.470, Speaker A: Right, yeah. So in your hybrid model, would you say that it relates to your data model, like the UTXO model, and would that be relate, like, your choice of a hybrid fraud proof? Would that be related to your choice of UTXO model or those irrespective of themselves? I would say that our choice of doing a UTXO model is what helped us choose to do the hybrid approach. Okay. Yeah. I see a lot of focus on the roll up space, and rightfully so, is focused on reducing this L One footprint. Right. When we move from single round fraud proof to a Bisection game, we try to simplify basically the problem to a single instruction set that should go on chain.
06:05:01.470 - 06:05:53.378, Speaker A: So this reduces the l one footprint. But when we think about scaling as a holistic thing, really the L One footprint is a part of it, and arguably it won't be the biggest part of it in the future. With solutions like Dank Sharding, like with Celestia, a part of scaling is obviously the data model and execution model. So I'd like to know what are some optimizations that you do in this respect in your particular implementations. Right. So I do think optimistic roll ups are in favor here of seeker roll ups where if data does get very cheap, then we reduce our cost by basically 100%. This is our primary cost.
06:05:53.378 - 06:06:49.666, Speaker A: Execution only happens during the fault proof in the unhappy case. So in the happy case, you'll get very, very cheap transactions. And in the best world, you would combine zika rollups and optimistic roll ups. If we had a zkap proof for arbitrary computation like we can do with fraud proofs, they can lag behind the optimistic roll up and reduce the dispute period by showing a validity proof to basically confirm the execution. Short term, I think the optimistic roll ups are more powerful here with better arbitrary execution. So I'll have to deal with the dispute period for now, I think, answering your specific question of, like, what are we doing to kind of specifically optimize our thing outside of the data layer as a project inside of Celestia, we're not really focused on that right now. We're taking a somewhat naive approach to these roll ups, to these optimistic roll ups and the fraud proof, where we're just trying to generate a fraud proof.
06:06:49.666 - 06:07:32.706, Speaker A: And we're assuming that the size will come in at such that we can post it on our data availability layer. And we're assuming our data availability layer gives us cheap enough data that we're not too worried about that. I think one of the interesting things is as data becomes cheaper, right, as Proto mentioned, optimistic roll ups start becoming more favorable because they have a higher cost of a quantity of data that needs to be posted on the L1 as an optimistic roll up. Right. And when that kind of problem becomes better, the actual cost of generating a fraud proof also, or the complexity of that, can become simpler because in this generic execution, the Nitro VM is somewhat complicated. To understand the Bisection game, it's much easier to explain to someone, oh, I'm just going to give you a pre state. You're going to do an execution, you're going to get a post state.
06:07:32.706 - 06:08:06.378, Speaker A: The reason you might want this interactive verification game is because you can get a smaller fraud proof. And if your data is expensive, you could touch a lot of state in this fraudulent transaction, you could end up with a very large fraud proof, theoretically too large to even post in one block. Right. And that's kind of an unacceptable situation. That's what the Arbitrarian people were talking about, right. They can cover fraud proof for transactions that are essentially larger than the total block space of Ethereum. But if we start having larger blocks on a data layer or really, really cheap data, you can kind of remove some of the optimizations actually in the fraud proofs because the assumption is you're not frequently posting fraud proofs.
06:08:06.378 - 06:08:52.030, Speaker A: If we're posting fraud proofs, like once a day, once a week, once a year, something is kind of wrong with the economic incentive system here. Right. So we can actually get simpler things and less optimized systems that are then easier to understand, easier to audit and verify if we get cheaper data, right? Yeah. So in terms of optimizations, there's really two things that come to mind for fuel, and that's one, we have the UTXO based model that allows us to eliminate the need for a global state merkel tree. And this allows us to scale quite generously. And the second thing is, in the actual implementation, every contract has a corresponding address. And then to refer to these contracts, you refer to them using an address.
06:08:52.030 - 06:09:33.820, Speaker A: And this allows us to take advantage of some of the properties that having a global state merkel tree would allow you while still maining the UTXO based system where we don't actually need it. Thanks for these. Yeah. I'll have one final question and then I'll open the panel to audience questions. So oftentimes in these crypto projects, there's like, misconceptions. So can you think of a common misconception related to your project or optimistic execution in general, if you have any? If you don't, feel free to. Pass this one.
06:09:33.820 - 06:10:45.202, Speaker A: This is a spicy question. Everyone thinks different things about roll ups. I think in general you could say there is a misconception that non interactive or interactive roll ups one is better for some weird reason in the end I do think it really depends on the dispute period. If you have this kind of dispute period then you will see differences or if you don't have this dispute period you'll see differences. So like in the sovereign rollup model or things closer to Celestia it will actually matter to be able to do a non interactive fraud proof. Whereas on Ethereum Mainet you always have a limited EVM and having interaction is actually a good thing because if you have a week anyway to do this anti censorship fraud proof if you take the time then you might as well use that to reduce the cost on layer one. So in the case where the fraud proof is distributed over the P two P layer am I getting if you want to be off chain you definitely want to be non interactive or something close to that.
06:10:45.202 - 06:11:26.446, Speaker A: Whereas if you want to do this on chain, interactive is actually more optimal. Makes sense. Yeah. So for the celestial model if we go into the L1 two three right, we're deploying that settlement layer which is itself a sovereign roll up. So in this way we're boxed into using this non interactive fraud proof because we don't have a place where a smart contract can live that would essentially mediate this interactive game. But to answer your question on misconceptions I think there's a little bit of this assumption that like a 14 day latency window or whatever to the verification or the trust of the roll up transactions. Some people don't know.
06:11:26.446 - 06:12:02.102, Speaker A: I think that's like a completely arbitrary number that was picked. There's not good backing research on this is what happens if you extend it to 21 days. This is what happens if you shorten it to seven days. People just picked like 14 days and good enough. That seems fine, right? But we haven't seen a live network generating large quantities of fraud proof. To see is 14 days actually long enough? And if you think about the amount of transactions going through these networks, right, if we look at Starquare we're in tens to hundreds of billions of dollars of transactions. The reality is you just need one honest full node in the network to be re executing all transactions.
06:12:02.102 - 06:12:53.258, Speaker A: They should be able to find an invalid transaction. This latency window doesn't really need to be that long for you to have a pretty large set of people that are economically incentivized to check these transactions. You could probably be order of days or even hours because these liveness failures you'd have to have the assumption of there is no honest watcher in your network executing the transactions but you're still producing transactions. It's just not a real world situation in the modern world of how trivial it is to spin up software. If the thing that can generate a fraud proof is open source software, if all of them are down on the network and you have a 24 hours window, there's like ten cloud services you can go to at any hour of the night, buy a VM, download the software and execute it against the chain in that period. I just don't see liveness issues being something where we need 14 days to resolve this. Right.
06:12:53.258 - 06:13:38.586, Speaker A: So I think the 14 days number or seven days for some it really depends on the censorship ability against the fraud proof. So if you have a fraud proof of chain, you have a very different model than if, say if you have a fraud proof that depends on miners, including the challenges. Yeah, and I guess so we're biased in this sovereign roll up environment right. Where we have a relatively easy workaround to the censorship resistance because you can pass it to the data layer. And the assumption is that the data layer is not necessarily incentivized to censor transactions for your roll up above it. Obviously that's still an area where you can have censorship if there's sufficient money to be gained by censoring. Right? Yeah.
06:13:38.586 - 06:14:43.940, Speaker A: But good color. I like the angle that you bring on this. Emily, do you have any? Yeah, so speaking particularly about Fuel, I feel like there's this misconception that Fuel is an optimistic roll up first, when in reality it's a modular execution layer first. But I don't think that this is a problem that's specific to Fuel, I think because modular layers and the modular blockchain is this new exciting thing that's becoming more prevalent in the space that we are knowing and working with. I think there may be misconceptions about what the definitions mean and I think that's why days like today are very important so that we can get excited about it and bring everyone together with the community. So if anyone have any questions, now's the time. Questions? Don't be shy.
06:14:43.940 - 06:15:03.028, Speaker A: No one. This shows that fraud proof are already answered. We've already solved all the problems. All the problems. I think there's one. Oh, here we go. Thank you for the panel.
06:15:03.028 - 06:15:52.276, Speaker A: I have a question here. I was asking few labs folks on the background, but still question for you. Let's say I have a celestial like client on a phone. It's just a data availability layer, okay? And there is a Dex deployed on a full node, basically uniswap. And I want with my phone to take like to plug any execution layer and talk to that Dex. Do you think it is viable in this year or a year after or is it just like a realm after five or six years? Thank you. So I guess when you say a phone here, it's a relatively nebulous thing because I don't think hardware limitations of phones right now are going to be a limiting factor.
06:15:52.276 - 06:16:27.332, Speaker A: I mean, what the hell does like, an iPhone 13 have from a processing power perspective that's insufficient to do validation or execution of transaction. I think your actual problem is going to be kind of from a software perspective, like, can I get my software onto iOS? Probably not. Can you get your software onto Android? Probably if you route the phone, yeah. Then you can probably jam it into Linux somehow. But then actually an interesting problem is something that I don't think we talk about that much is like cross architecture things. You can just transpile it to like, Arm, but you do get layers of different architectures in here. But I don't see any technical limitations for that.
06:16:27.332 - 06:17:14.384, Speaker A: I think we're into the limitations of Apple and Google control, what, 98, 99% of all phones, and they're relatively restrictive on the kind of software you can deploy. I don't know if anyone's gotten blockchain execution nodes onto a phone. I don't know where that falls into an app store category. Well, so if you have a phone, obviously you might want to run a full node. But this is what Lite clients are for, to reduce the resources. So if you want to submit a transaction to a Dex from your phone and if you want it to be confirmed on a data layer, then there will probably be some software roll up that will try and specialize. In giving you a soft confirmation very quickly for the dex to have a good UX and then to show you a proof that the data has been confirmed.
06:17:14.384 - 06:17:37.770, Speaker A: And I think this is really just should be a software problem. And I think this is like a question for maybe a software and roll up panel or basically like the execution layer more so than the execution fraud proof tag. We have time for one more question. We got one over there. Someone over here. Yellow. Oh, there you go.
06:17:37.770 - 06:18:22.696, Speaker A: Oh, they did work nice. So I often think about app specific chains as not really being like whole chains, but just like a roll up that's built on top of a celestium or something like that. And each application will just run their own roll up and et cetera. I wonder how much you guys have thought about that. I think especially with the people that are building more general purpose solutions. Yeah, so that's pretty easy for us to answer as trying to do fraud proof in the Cosmos SDK, right. It's just a Cosmos transaction, and as long as we can generate a fraud proof based off of the state tree that we'll get in the Cosmos SDK, then it should be fine.
06:18:22.696 - 06:18:50.200, Speaker A: For app specific chains, I think that should go for kind of all of these. It's just as long as you have the ability to re execute a transaction for that app specific chain, which really just involves a light client being able to load the state and execute whatever software makes up the app specific chain. It shouldn't be more difficult than that. All right, let's call it there. So thank you guys. That was an awesome panel. Thank you.
06:18:50.200 - 06:22:01.250, Speaker A: So we have a 15 minutes break now, so there should be some grab some tea, grab some snacks, hang out and come back in 15 minutes. We have an investing panel, we have a data availability schemes panel, and of course the debate on monolithic or modular. So see you guys in 15 minutes. You Sam. SA. Don't touch him. Don't touch him.
06:22:01.250 - 06:32:01.356, Speaker A: Sam. Sam. Safe. Sam, break it's. SA the sun, Ram. SA sam. Om.
06:32:01.356 - 06:35:12.300, Speaker A: Sam. Hey, everybody, we're about to start up. The break is wrapping up, so please come sit down. Sam, SA guys, please come sit down. We're starting up again by fractional shareholder Sam back. Hopefully you got some refreshments and you're ready for the last segment of modular summit. So we have two panels and then we have a debate to wrap things up.
06:35:12.300 - 06:35:51.910, Speaker A: So again, the debate is going to be about modular or monolithic between Mustafa, the co founder of Celestia, and Anatoli, the co founder of Solana. So that's going to be really exciting. So stick around for the end. And of course we're going to have networking drinks starting at 530, so I think that's going to be some of the most fun time here because we'll all get to hang out and connect with each other. So stick around for that. And again, there's going to be a swag drop happening at 530, so more incentive to hang around. And also, of course, there's the research track.
06:35:51.910 - 06:36:29.454, Speaker A: Again, don't forget it's cool stuff happening there. Tarun was just talking about DFI. There's going to be some more technical data availability and roll up presentations, so I encourage you to check it out. Okay, introducing the next panel is about investing in the modular stack. So you're going to hear from a group of different investors who are really betting heavily on modular as a category for their funds. So we have Joe from Framework. We have Matthias from Maven, we have Alex from Blockchain Capital.
06:36:29.454 - 06:37:09.550, Speaker A: We have Eli from Polychain, and we have Will from Galaxy Digital. So give a round of applause, help me welcome up our panelists. All righty, hello everyone. I hope all of you have been enjoying today as much as I have. I know I've been really looking forward to the summit and appreciate Maven and Celestia inviting us to speak today. But my name is Joe. I do investing over at Framework and very excited to be joined by this group today as we kind of discuss our various perspectives on supporting the modular thesis through our work as investors in this space.
06:37:09.550 - 06:37:43.210, Speaker A: So I think we'll kind of start just by kind of going around, introduce yourself where you're from and then as well, maybe the first concept or idea that introduced you to kind of the modular paradigm. Yeah. Cool. So I'm Matais, I'm a partner at Maven Eleven and part time event planner these days. We're co organizing this. So incredibly happy you're all here. I think I first ran into modularism or modular blockchain design when I ran into the Lazy Ledger paper written by Mustafa in 2019.
06:37:43.210 - 06:38:38.794, Speaker A: I reached out to John Adler with some questions and we then helped bootstrap Lazy Ledger with their initial seed round. Everyone, I'm Alex, I'm a partner with Blockchain Capital. I was first introduced to the concept of the modular blockchain paradigm probably in 2019, just reading E three research forum. So the first concept, I don't forget the specific project that I first read about, but was the optimistic roll up. So it was either optimism or fuel or Arbitrum. Hey, I'm will. I work at Galaxy Digital and invest on behalf of the team there obviously paid attention to the optimistic roll up stuff in 2019, but I think the moment where the shift to the modular sort of approach really clicked.
06:38:38.794 - 06:39:33.946, Speaker A: Was watching in October 2020 when Vitalik was posting about the sort of roll up centric roadmap and ethereum's shift from ETH 2.0 sharding to sort of like a data shard. Centric approach was the moment that it clicked that these different functions of A blockchain are going to be split up and then started reading Mustafa's papers at that point and started to work with the team around that time and then proceeded with that. Cool. My name is Eli. I'm a partner on the Polychain investment team. I think the first time that I started thinking about kind of the modular stack, I think it's hard for me to trace back exactly, but it really evolved from conversations just with brilliant people in the industry.
06:39:33.946 - 06:41:14.222, Speaker A: One of them being James Crestwich, who I was kind of, like, trying to work with, I guess, when I was, like a struggling or mediocre developer just on Interoperability. And so this evolved into this idea of this exploration of a trade off space of really you have composition which is like how do you compose different functions within the same execution environment, like how the EVM operates today. And then you have interoperability and the various kind of permutations of that with IBC or xcmp in the Polkadot ecosystem. And then we are also thinking about scaling. And so this this evolves from, you know, all these ideas around sharding and how do you appropriately communicate between these shards? Like how do you make them interoperable, how do you compose these functions? Because that's a really interesting kind of thing to discover with Flash loans or what have you. And I kind of came to the conclusion that maybe the best way forward is not to put such an emphasis on composability or kind of synchronous composability rather. And so after evaluating this for a while, you can kind of parallelize it with shards or you can kind of compress or commit data in a roll up.
06:41:14.222 - 06:42:04.000, Speaker A: And just after exploring a lot of these ideas, I actually realized that what I had kind of discovered or rediscovered was a pitch that I had heard a year before with the lazy ledger white paper. And so for those who are not aware, lazy ledger is the academic name behind Celestia. Exactly. So that's really what led me to down the rabbit hole, so to speak. Yeah, this modular kind of way of thinking. Love it. I think one thing that I'm really curious around to get some of your guys perspective on is in a broad sense, what makes yourself or kind of the investment thesis that you hold.
06:42:04.000 - 06:42:51.806, Speaker A: Why is it so bullish specifically on this modular stack versus monolithic constructs? Yeah, I think it's of course about the various parts of the stack and that you can optimize for, say, data availability or execution, et cetera, but maybe deeper down, and especially with data availability sampling, it goes about giving users or end users and their light clients the ability to be first test citizens. I think the words we use of these crypto networks so that I, with my mobile phone, have similar security guarantees or almost similar as our full mode. Whether that matters or not is maybe the bigger question. And I think we'll get into that later with Mustafa and Anatolia. But I think that's why we're building Web 2.0, to have end user verification. Absolutely.
06:42:51.806 - 06:44:01.038, Speaker A: What about you, Alex? Yeah, I mean, for me it comes back to why we're here in the first place, which is we want to build open, incorruptible, censorship resistant networks for the Web. In our view, we see the majority of the world's economic activity moving on to blockchain networks over some period of time that is probably over the next couple of decades. And in order for that to happen, you look at what the end state is, and that's millions of transactions per second. So what we have right now is definitely not going to get the job done. And what is the best proposal for how to get to that end state and to do so safely? And by safely, that would mean without breaking the guarantees that decentralized blockchain networks give you today. So censorship, resistance and security primarily. And the best proposal that I've seen for that is to scale via roll ups and to scale via a modular blockchain stack, where instead of introducing an honest majority assumption every time you scale horizontally, you can scale vertically.
06:44:01.038 - 06:46:07.350, Speaker A: And via some combination of fraud proofs, validity proofs and data availability proofs, you can introduce only an honest minority assumption, and you can significantly increase the throughput and the expressivity of new execution environments that all share some decentralized foundation. So that, to me, makes the modular blockchain stack the only viable approach right now to an end state where the world's economic activity is happening on blockchain achieved there. What about yourself? Will it's always, I think for us comes back to scaling. We just simply haven't seen like roll ups can work, but we're not operating at scale yet today, right? So the kind of scale that can onboard a global and full user base across the world. I think why we're bullish and we believe in the modular thesis is because when you split out these layers, data availability from execution, from settlement, you can optimize the three separately, right? And you can let them when they're bundled together, you fundamentally can't perform the optimizations that each one is capable of being on their own, right? So splitting out data availability in particular gives you the ability to go from basically data availability proof in O of N complexity to O of square root of N, right? And that's a phase shift in the ability to scale blockchains. Right, and we're looking, I think at times particularly investing in technology, it's like you're looking for phase shifts as an investor and going from O of N to O of square root of N is a phase shift. And so I think we think this kind of stuff will continue to emerge as this sort of thesis is played out and technologies continue to develop along these lines.
06:46:07.350 - 06:47:16.926, Speaker A: Anything to add there, Eli? I think I covered a lot of this in kind of my earlier point, kind of jumped the gun. But I think the only thing that I would kind of add was that if you look at the history of computation, maybe you started with something more primitive like Abacus and then you kind of worked your way up through application machines. There were these kind of machines or contraptions that could solve a Fourier series or something, right? But they were very application specific, they were very bulky. And then we worked our way up to mainframes or what have you, and then worked our way all the way down to kind of modularizing it right, with CPUs, GPUs, whatever is kind of in your desktop or laptop. And I think that we're seeing a similar evolution in the blockchain space. But I do think that there's a little bit of ambiguity about what modularity actually means. Right? I think that so far we've seen modularity in the Cosmos SDK or the substrate design for polkadot.
06:47:16.926 - 06:48:47.470, Speaker A: But I think that Celestia is really interesting to me because you can have all of these modules, you can build custom execution environments, but you still inherit the security of the underlying chain through roll ups or whatever. And I think this extends to other kind of other categories of investing, whether that's bridges or what have you. It's really important to further modularize some of these concepts, but also still inherit some of those security principles, elements of the kind of the security foundation I got you. What do you all think is kind of like or are some of the larger distinctions when it comes to investing in a modular thesis versus a monolithic one. And maybe we could start with yourself, Eli, on this one. Sure we've been pretty bullish on this modular paradigm for quite some time, but I think when thinking about kind of monolithic versus modular architectures, if you abstract modules in Celestia or just like roll ups on Ethereum or what have you, really you just kind of end up with a bunch of different chains if you abstracted away enough. And I think that our name is Polychain, right? We have a bunch of application specific chains in our portfolio.
06:48:47.470 - 06:50:31.362, Speaker A: And whether you're exploring that because you want to implement some kind of new execution environment or maybe you want to incorporate cheaper fees for some kind of privacy based feature or whatever it may be, we don't live in a monolithic world. It doesn't really make sense, even from a portfolio construction perspective, to invest in one monolithic chain, if you believe in that kind of I mean, I still really like Solana and everything. I really like Ethereum, and I don't think that they're going anywhere, but I kind of view them almost as their own modules. Interesting. Will, what about yourself? What do you think makes kind of the largest distinctions between investing in these two separate thesis? Yeah, it's actually, I think, maybe more challenging in theory, I think, to invest in modular, not like it's hard to decide that that should be your thesis, but thinking through what parts of that stack are going to be valuable is actually, I think, harder to do as an investor, frankly. I think when you have a sort of monolithic L1, whether it's Ethereum or something else, right, fundamentally the tokens on the L1 are being used to pay for compute bandwidth like memory and solid state storage. Fundamentally, they're provisioning those resources on the network, but that's okay because those things are all bundled together and obviously you're paying for it at the opcode level.
06:50:31.362 - 06:52:01.262, Speaker A: But at the end of the day, when you're submitting a transaction, you're paying for all in one bundle, fundamentally. And that creates a surplus, a transaction fee that produces a surplus above and beyond what the miner takes in that's kind of the EIP 1559 model. I think this is more challenging, right? Because now you have different layers where there's different fundamentally different resources being sort of like provisioned at the end of the day and understanding which of those resources, whether that's compute or bandwidth or solid state storage or memory. Understanding what people are willing to pay for each of those and then who exactly is going to pay for each one I think is pretty non obvious to me at the moment. Maybe these guys have answers, but it makes it more challenging, frankly, to really evaluate, I think, these modular projects as an investor. Alex, I know we've talked a lot about this kind of what are your thoughts, maybe, on some of what Will was speaking on or just kind of the distinctions between yeah, I mean, for me I think it comes down to end user and developer choice and iteration speed specifically. So when you build a monolithic blockchain, you're marrying obviously, consensus settlement, data availability, execution, and that constrains the directions that you can move in.
06:52:01.262 - 06:53:15.214, Speaker A: It's much more difficult to upgrade a system where all of those pieces are coupled together. Whereas in a modular blockchain you can deploy a new roll up when you want to experiment with a new execution environment. And so over time, what I'd love to see is a wallet where I can have almost like a slider for what's the level of security you need, what is the level of privacy you need, what is the speed of transactions that you need for whatever your application is, or even just as an end user doing payments. And I think if we have wallets switching between a bunch of monolithic blockchains that are specialized for each of those things, that introduces a massive security hole in bridging assets between them. And so when you have roll ups, you have these natively trust minimized bridges that are baked in and you can achieve very highly customizable security, highly customizable privacy, et cetera, for applications without compromising on security. So I think that approach is more sound, long term, almost giving developers kind of an additional choice by kind of modularizing that yeah, exactly. I mean, at the end of the day, these are open networks, it's open source software.
06:53:15.214 - 06:53:56.690, Speaker A: And so the more experiments we run, the more iterations we produce, the better the results are going to be and the faster we're going to move as a space. And so I think one of those approaches lends itself much more effectively to experimentation, to iteration. And I think that it's very likely that that'll produce more interesting applications and developer experiences long term. Anything to add on that one? No, I agree with almost everything that has been said, but two, maybe more general points. Will said earlier, right? These rollups aren't live fully yet. So I think there's still a lot of ongoing research. And when looking to back founders, I think you have to accept that there can be quite radical pivots based on the latest research.
06:53:56.690 - 06:54:25.786, Speaker A: So you need to look for people that are willing to do that, that aren't like set in stone. And I'm going to build just this. And when a new insight comes, they ignore it. In addition, and I think this is something we already see with the rise of bridges between monolithic chains, I expect these communities of the various parts of the stack to get increasingly intertwined. So we see that with Avalanche and Terra now, right. Their communities are sort of splitting, combining two bridges, I think, in the modular stack that's going to happen so much more. Yeah.
06:54:25.786 - 06:55:44.386, Speaker A: And it is an interesting thing to think about when you not only are you modularizing the technical components of these, but also the underlying social consensus of each of these layers. And it does lead to kind of some of these interesting kind of ideas to think about on a much longer term basis, as well as people are kind of choosing where to go and which layers to use under them. Do you all think there's maybe a particular layer within the modular stack that you believe is maybe underappreciated, under discussed, or maybe even potentially undervalued? Yeah, I think right now we mostly focus on execution environments that are EVM focused. And I think different types of execution environments, virtual machines I know Fuel was on here earlier, they're doing something with that are vastly undervalued because it brings in a whole new wave of developers. And I think that's something that excites me a lot about modularity sorry, front running you I know you agree. I think we're all going to agree on this point. Yeah, maybe just to expand on the intuition there, I guess the most dominant development standard today is based around the EVM, which I don't think that a lot of folks would disagree is suboptimal in a number of ways, like the way that it prices, storage, the way that it handles memories.
06:55:44.386 - 06:56:43.334, Speaker A: These things create limitations in the types of applications that you can build and the level of scalability that you get, how efficiently you use node resources at the end of the day. And so allowing the space via modular execution environments to break away from the EVM and run experiments on can we optimize a VM for a specific application? To me, that is like, we've more or less saturated, probably development efforts around the EVM, and that's important. We should continue to improve it and make it better and make EVM proliferate more. But alternative VMs are very important and have a large place in the world. And so those experiments, I think, are very welcomed. Yeah, we're all going to be on the same page on this one. I mean, ironically, it's like data availability, sampling and data availability that unlocked all of this, right? So in some ways, that should be what we're most excited about.
06:56:43.334 - 06:58:03.374, Speaker A: But at the end of the day, the real exciting part of the modular stack fundamentally, is that I think the design space for the execution layer has been blown wide open. By splitting out execution, optimizing the data availability and consensus layer, I think we haven't even started to explore the design space. And what's cool about it is you can iterate a lot faster. I mean, obviously Cosmos and Cosmos SDK and other projects have provided some outlet for this in the past, but still there is a lot involved in scaling a validator set and some of the work you have to do to spin up sort of like a Cosmos zone. So credit to where credits do to people who have gone down this road in the past. I just think. The pace of developing new rule sets for execution environments or execution layer is going to be way faster on top of Celestia and in this sort of roll up modular sort of approach things that there's a number of parameters we could sort of imagine tweaking but there's just so much you can do, whether that's increasing block intervals, trying something with longer block intervals where it allows you to do longer, sort of like compute.
06:58:03.374 - 06:58:49.938, Speaker A: Right. So that you can do write longer programs. If compute is really cheap and you could write longer programs, people might try doing something like that. You might try parallelization of transactions was mentioned before. There's so many parameters you can sort of modify and it's just a multivariate sort of like design space. And I'm excited to watch people explore this over the next couple of years because I think that's where a lot of the exciting stuff around the modular stack will happen. Yeah, I think that there's just this kind of elegance around the minimal design of Celestia specifically and when thinking about how the execution environment on Celestia or any other chain will evolve, does get really interesting.
06:58:49.938 - 07:00:30.650, Speaker A: Naturally, the first step is to have this kind of EVM environment, maybe slightly modify it, maybe it's a canary network or something like that. But I think that the next step gets even more interesting where you can take a Cosmos SDK app and maybe decouple some of the staking economics from the core business logic, right? You no longer or maybe you don't have to subsidize the validator set as much, right? Or you can defer some of those costs or just cut costs in general. I think the step after that gets even more interesting where you can kind of have this IDE where what you're really doing is deploying separate roll ups of sorts and you're no longer competing to include a transaction in this order matching logic. And that's separate from voting logic in a Governance module or something, right? So you have like a matching engine module and then you have a Governance module or something, right? And these things can seamlessly communicate between one another using Celestia or even Ethereum can do this on a long enough time horizon, I'm sure. Absolutely. I think one of the ideas that makes me think a lot about is just like I think we've covered a lot of what about this stack excites us. I'd love to hear some more about what are the implications of that on applications on end users.
07:00:30.650 - 07:02:20.210, Speaker A: I think there's an aspect of security that could get discussed here but also just the new design space that this opens for applications would be awesome to hear your thoughts, even taking a step back from just thinking of Celestia as the modular stack. I think that other applications have been really interesting to see evolve. So there are these different kind of layers that we've seen as more chains just come into existence, you no longer are just loyal to this L1 E. Right? And using a lot of L2s feels very similar to layer ones that have deployed EVM compatible environments. And so we're seeing new kind of like liquidity layers, like connect that can act as a clearinghouse between all of these different environments. And I think that when you have this clearinghouse, you don't really have to worry about certain security concerns because it's modularized in a way where it inherits the security of the local instances where it's deployed. Or you have something like Nomad that can be used for passing messages between these different modules or environments and then at the application layer, outside of just kind of communicating or transferring tokens between all of these environments, you have core applications, right? Something like dYdX.
07:02:20.210 - 07:02:59.374, Speaker A: They're deployed on StarkNet. And I think the majority of the reason they can even function is solely because of kind of the scalability increases. Exactly. Limitation. Will, what do you think about or what excites you kind of about the application space when it comes to yeah, I think we've made a lot of points and gone there's just a lot. I think that generally excites this group. One thing that I want to flag as well, while we have it's, like some of the work Celo has historically done on Plumo, which is the sort of light client that's optimized for mobile, I think there's some elements Celestia's data availability is very different.
07:02:59.374 - 07:04:21.450, Speaker A: But the modular paradigm and data availability, let's call them like an eager blockchain versus a lazy blockchain, like lazy blockchains are fundamentally pretty good at fundamentally better for sort of mobile users. And if mobile users want to be self sovereign because light clients have stronger sort of censorship resistance guarantees. So I'm excited to see if we can use the data availability paradigm and the modular paradigm to approach new applications for mobile first users. That's a little bit further down the road and that's a little more speculative, but I think that always should be a goal of the crypto and blockchain community is like, how do we build things for different sort of computing platforms like mobile? I know that's a bit of a digression from some of the stuff Eli was just talking about, but I wanted to flag it as something I find exciting and we'll see if that can sort of emerge. I'm definitely aligned with you there. I think there is an end goal for Web Three where we need to have or at least provide the optionality for end users to be verifying and making that easy on a user device like a mobile phone. Otherwise, inevitably somebody will build it will build an application for a mobile phone and you won't have the option to have self sovereignty via a really censorship resistant, like, client.
07:04:21.450 - 07:05:01.426, Speaker A: So for me, that's pretty cool. Awesome. I know we only have a few minutes left, but I'll let you guys hit on this one. Anything particularly interesting that you guys are kind of foreseeing when it comes to the application layer? Sure. So a colleague of mine, Ryan, likens it to I guess like the shift here to a breakthrough in hardware. So it's really hard to say what does this new paradigm enable but I think we have some instincts on what it might enable. So as an example, Fuel processes transactions in parallel so they can run at a significantly higher speed than a monolithic blockchain.
07:05:01.426 - 07:05:53.254, Speaker A: So that enables things like micro payments which aren't practical or economical on blockchains today. They also have a significant memory expansion at the node level and so that allows things that are memory intensive, like an on chain order book, dex for instance, which is not something that you can really practically run on today. So without any trade off. Yeah. TYs, what about yourself to follow? Yeah, I think the order books have been mentioned a few times. Right, I totally agree there also on the application layer, I think we will just see more radical innovation because it's easy to just deploy an application specific chain and try something out another but this is maybe more infrastructure if you will, is bridges that share state between optimistic rollups is an area I'm very interested in. I haven't really seen that yet.
07:05:53.254 - 07:06:15.600, Speaker A: So if someone is building it do reach out. But yeah, apart from that, agree with what they all said. Awesome guys. Well I think we are at time but really appreciate you all joining us and thanks everyone for coming out to the summit. I hope you all have enjoyed Gilbert. Thank you guys. Good job.
07:06:15.600 - 07:07:15.810, Speaker A: All right, we have one more panel and then we have the debate. So this next panel is about different approaches to solving the data availability problem. So you guys may be aware that there are a few different projects who are building data availability layers. Namely on this panel represented Celestia, polygon avail and Ethereum itself building out Dank Sharding. So this panel is meant to explore what are the different trade offs to the different data availability scheme constructs. So on this panel we have John Adler from Celestia, we have Matt Barnet from Ethereum, we have Anroag representing avail and then we have Sriram Kanan who is a researcher at University of Washington who has written a lot of very interesting research on the topic. So please give a round of applause and welcome to the stage.
07:07:15.810 - 07:08:04.182, Speaker A: Good evening everybody. Welcome to this panel discussion. It's a great pleasure to be here. Thanks to the celestial organizers and the modeler summit organizers for bringing us here. It's a pleasure to share this stage with these illustrious gentlemen here. Okay, maybe we get started with a brief introduction of the data availability stack that each of you are building. I'm just here to moderate this discussion please.
07:08:04.182 - 07:08:31.466, Speaker A: Anurag. Yeah. So I'm anurag I'm representing polywen avail. Polywen avail is a data availability focused blockchain. So we began thinking about this problem in around 2020, Celestia or Lazy Ledger. I think Mustafa had written a paper regarding that and we were interested in exploring that. But they went with the fraud proof approach.
07:08:31.466 - 07:09:32.946, Speaker A: And when KZG polynomial commitments really became better known, I would say we thought that we could structure our data validity focus solution on using KZG proofs. So the architecture is that of the data validity focus in the sense that we do focus on transaction ordering and not execution. And specifically we use validity proofs, KCG proofs for that. And the first use case which we are focusing on because we have three different projects under the polygon umbrella, namely polygon Hermes, which is focusing on zkevm, for example, polygon zero and polygon midn. So we have three zkevm solutions within the polygon suit. And so the first focus was to kind of working on validium solutions for these three solutions. So that's the focus.
07:09:32.946 - 07:09:57.290, Speaker A: And then of course, we will move to sovereign app chains going forward. So that's the rough brief. Thanks Anurag. Okay, so hi everyone. My name is John. I am the co founder and chief research officer of Celestia Labs. And we're building Celestia, the first modular blockchain network.
07:09:57.290 - 07:10:58.682, Speaker A: This was formerly known as Lazy Ledger and it's a data availability only chain that is overhead minimized, which means in order to use the chain for data availability, in order to verify that the chain is valid, you have to do a very small amount of work, ideally as minimal as possible. The particular data availability scheme that we use is actually essentially completely identical to the one that was first proposed by my co founder Mustafa, in his earlier paper on fraud and data availability proofs to secure trust minimized by clients. Effectively completely unchanged from that paper, which was three, four years ago at this point. So there aren't many cases in the blockchain space where you don't have to fundamentally pivot your technology over that many years. But in this case, it did happen. Thanks. Matt hi, my name is Matt.
07:10:58.682 - 07:12:14.162, Speaker A: I am a researcher working on Ethereum, and you've probably been hearing a lot about the Ethereum Scaling stack called Dank Sharding. And Dank Sharding is a new take on the Ethereum Sharding roadmap that exploits some of the observations that people have been making over the last couple of years related to the centralization of block builders in the ecosystem. And thanks. Sharding relies on the fact that in the future we expect that the people who are building blocks are going to be highly sophisticated, more resourced actors and the people who are proposing blocks are going to be lower resourced actors. That's coming from a more decentralized set. And so with Dank Sharding, we are able to avoid some of the clunkiness that existed in previous sharding proposals, where there were 64 shards, 64 shard proposers there was latency between creating a block on a shard and that block being checkpointed onto the beacon chain. And so now we're taking advantage of this more centralized building actors and they're able to aggregate 64 shards worth of data into a single 2D KCG scheme and provide that to the network.
07:12:14.162 - 07:13:27.326, Speaker A: And it's also inspired by this lazy ledger paper that was written four or five years ago where people are sampling the data elements from this super 2D encoded block. I would say also for the encoding scheme for the blocks is definitely more efficient because you're able to only do 75 samples of the block rather than 30 samples for each shard block. So it's a lot better than the previous proposals for how much data needs to be downloaded from each validator that exists in the validator set. Thanks. So the main agenda of this discussion is to understand the different data availability architectures. So what we will do is try to set up the axes of comparisons. One axis I think is obviously security.
07:13:27.326 - 07:14:53.900, Speaker A: What is the security model that each of these different projects are trying to optimize for? One way to phrase this question is what would it cost for me to get an unavailable data and pass it as being available? That's something maybe we can discuss about the different schemes in the KZG commitment scheme. The the scheme is set up as such that I think safety of data is not a big problem. The problem I think would be maybe insufficient number of light clients for example, sampling the data because there's no incentive as such. So the idea is that if you are building like an app specific chain, you would have the incentive to have light lands sampling the data. So I would imagine having insufficient number of light lands sampling the data or part of the data is available not from the block itself, but if you consider the entire app history. So that could be a case. But in general, due to the construction of the scheme, it's pretty nicely laid out that either data is available or not available.
07:14:53.900 - 07:16:18.566, Speaker A: So one point I want to add here is we call this data availability sampling, and when we call something data availability sampling, what we mean is any light node, like Arjun just mentioned, anurag just mentioned is any light node should be able to sample an arbitrary subset of the chunks of a given block and then verify that they are available. A very important underlying assumption here is private random sampling that a light node should be able to do check a random sample of different chunks. This I think goes back to the security question and maybe we can address this, but the question is how do we think about private random sampling? How do we make sure that light nodes actually when they're sampling it is private and it is random? Because there are all kinds of de anonymization attacks. For example, if I know that the same IP address is requesting these 32 chunks. Maybe I can just send those 32 chunks to that guy and withhold the rest of the data. So private random sampling is a very important model and I think this is not something that is part of any other blockchain architecture prior to this. So we need to take special attention as data availability builders to this.
07:16:18.566 - 07:17:32.320, Speaker A: So I think that goes back to the light client point that you mentioned, whether they're able to sample correctly. John yeah, or you want to add anything to that, please? Is the specific question still the security? And then a part of the security is private random sampling. So that's how I'm looking at, sure. So I guess this is the first data availability sampling scheme proposed and the assumptions are pretty concretely laid out in the paper, right? You need a certain minimum number of nodes, not necessarily light nodes, but nodes in general to perform sample requests. You need a synchronous communication in order to reconstruct the block. If there's no single node that starts with the entire block and you have to reconstruct it from byte nodes. And you also need, as Sriram has said, under certain adversarial assumptions, you need private sampling so that the attacker can't link together who is trying to sample, because if they can, then they can just trick that one particular node into believing that the block data is available when it's not.
07:17:32.320 - 07:18:34.446, Speaker A: And that's essentially, I mean, barring bugs in the codes and stuff, assuming that that's all correct and that's essentially the failure modes for the scheme. So a certain number of light nodes is a pretty weak assumption because there's thousands of bitcoin nodes. There's all ten, what, 10,000 ethereum nodes, there's even something like 6000 salana nodes, right? People will run nodes even if they're fairly expensive, just purely because they want to have access to the chain. So an assumption that there are 100 light nodes sampling or 200 light nodes is like a really easy assumption to satisfy. We can essentially guarantee that the synchrony assumption for being able to rebuild the block is something that you unfortunately can't avoid. And this is one of the reasons why validity proof schemes aren't inherently better because yes, you avoid a synchrony assumption for fraud proof, but you still need synchronous communication to rebuild a block anyways. So if you're not fixing the worst case, you still need that synchrony assumption.
07:18:34.446 - 07:19:25.822, Speaker A: So you don't really get too much out of validity proofs, except potentially some lower latency happy path cases, which is debatable. And yeah, that's about it. Those are essentially the failure mode. So as you can see, these are fairly weak assumptions, which is why we tend to classify the process of data availability sampling as being trust minimized because these are fairly weak assumptions. Now, in terms of private sampling, this is something that I don't think any chain that's implementing data availability sampling has done yet. Mostly because this would require a fairly well resourced attacker. But if you don't want to go through fairly advanced schemes like mixnets or whatever, or Tor, there are other alternatives.
07:19:25.822 - 07:20:56.190, Speaker A: For instance, you can have users run full nodes in multiple cloud service providers and unless every single one of those cloud service providers colludes to reveal that with the same user running those nodes, a third party observer can't actually connect that those nodes are actually run by the same person. So even if you don't directly solve it through some advanced networking, you can solve it in practice fairly easily against an attacker that is just one person observing the network, rather than literally every single cloud provider in the world all colluding together because that's a much stronger adversary which is much less practical. Thank you. John Matt, do you want to address how Ethereum is thinking about private random sampling? Right, so in the first iteration of Dink Sharding, their plan is to not do the private random sampling. And the reason being is that being able to quickly access random chunks of this data is actually a really difficult networking engineering problem to solve. And so the assumption that we'll make is that with an honest majority, we're going to be able to assign validators certain rows and columns that they'll go ahead and download and be able to, with this mechanism, propagate this. Information over the lib p to p gossip network in a lot more efficient manner than having to push things into a DHT and then retrieve them.
07:20:56.190 - 07:22:36.990, Speaker A: But this does have the assumption if you are a client who is just watching the network, you don't have a strong grasp of if that block is available or not. You're just trusting the honest majority of the network to tell you that block was available because the validator said it was available. And this is where the private random sampling goes in, where even if you have a malicious majority of the network saying that a block is available, you're able to do the samples yourself and with high confidence determine whether or not the block was actually available. Would you say in Dank Sharding, if somebody indeed wanted to do random sampling, they could just join those particular peer to peer networks where those chunks are propagated and thus check whether that is available on those peer to peer networks and then verify for themselves that the data is available? Is that something that you guys are planning for? So the way that the rows and columns will be encoded is not going to be as amenable to the proper sampling structure that people will want to do, because you'll want to have more granularity when you do these random samples. And the rows and columns won't provide that level of granularity, but you will be able to connect to as many of the gossip channels that you want to download as many rows as you want. Okay, so this is basically the level of granularity that you can achieve in the random sampling, the peer to peer network and what goes on a certain peer to peer network dictates that granularity, but it's still not everything. As a light node, you don't have to necessarily download everything.
07:22:36.990 - 07:23:26.702, Speaker A: Is that correct? Correct. Excellent. So that's the first axis that we talked about is security. And as a subset of security was the private random sampling, which is a radically different networking model than what was needed before. The next axis that we can think about is how do we think about the scaling ability of these systems? How many nodes should to make it concrete, let's say there are N nodes. All of them have a certain bandwidth. What is the net commitment rate or data availability rate that a system like yours can achieve? Is there some heuristic idea on that? Yeah.
07:23:26.702 - 07:24:02.890, Speaker A: So we are still playing with those around. But in DevNet, for example, the configuration that we have set is like two MB block sizes, like four MB, two MB coded in block time. Yeah. The DevNet config is set at around 20 seconds at the moment. But we are still not optimized too much. We have just benchmarking. We will do a lot of benchmarking in general in the future.
07:24:02.890 - 07:24:49.546, Speaker A: We will also do things like commitment generation optimization. So those kind of things are there, but the current configuration is like two MB block sizes at 20 seconds. That is what we think. We'll experiment more in the upcoming test nets. Awesome. So when you look at your avail network, do you think of this as like a peer to peer network and you're doing both consensus and data availability, or is it a pure data availability network? So of course the validators need to compute the commitments. So we are using substrate stack for that which we have like grandpa Babe for the actual commitment generation, for example.
07:24:49.546 - 07:25:33.858, Speaker A: And at the moment the scheme is such that all validators compute the commitment individually as well. So after the proposal proposes, then all to get on its majority consensus. So everyone has to commit it. But we are looking at certain alternatives in the future which might optimize that as well. So then we can experiment with block times. John, so to clarify, was your question around the bigger complexity throughput the data availability throughput that you can achieve using this kind of like an architecture? No. Is there a question around throughput and not the size of the commitment? No, not on the size of the commitment.
07:25:33.858 - 07:25:55.070, Speaker A: Okay, I must have misread. My bad. So the throughput, I think I have to refresh numbers. It might have been something like 1.4 megabytes per second, I don't exactly remember. But we're going to have a respectably large block size in the order of several megabytes and a respectably long block time. We aren't really aiming for a short block time.
07:25:55.070 - 07:27:06.546, Speaker A: The reason for this is that the longer your block time, the larger your block, even if the throughput is the same. But the larger the block, the more you can take advantage of the fact that the size of the commitment isn't linear in the size of the block, but rather is the square root of the size of the block. And therefore you get more advantage, like you have more scalability benefit if you have larger blocks and a longer block time than if you had really, really tiny blocks and a short block time. Longer block times for the base network. Well, a lot of people say you need really fast finality for the base network you need 400 milliseconds or you need 1 second, right? But it turns out that as people have used roll ups, users are perfectly happy with the fast instant soft confirmations that roll up operators provide. And as long as you're happy with that, which is perfectly fine assumption, right? As long as the roll up operators are honest, they will include the transactions they promise to include in that order. Then the base layer doesn't actually need blazing fast block times, so you can have slightly longer block times and take better advantage of this square root data commitment.
07:27:06.546 - 07:27:26.634, Speaker A: Scalability property. Matt, do you want to comment on that? Right? Yeah. So for the Ethereum Bank sharding proposal, the throughput will also be around 1.4 megabytes. I didn't want to say 1.3 because you had just said one point. Yeah.
07:27:26.634 - 07:27:55.986, Speaker A: So the throughput is supposed to be 32 megabytes for every twelve second slot. And of the 32 megabytes, 16 megabytes will be the erasure encoded data. So the actual data that will be useful will be the 16 megabytes. And so that works out to 1.3 megabytes. Something interesting here is in Dank Sharding, every node doesn't have to download all the megabytes per second. Each node can download much fewer, at least the quorum that you're certifying is not every node downloading everything.
07:27:55.986 - 07:28:20.106, Speaker A: Whereas in Celestia I understand, at least as of now, all nodes are downloading everything. But both of you have the same bandwidth. Is there? I'll let Matt answer that first. I have actually a lot to say about that, but I'll let Matt answer that first. I didn't catch what you're saying. I'm just asking. Just clarifying first that in Dank Sharding, every node, even though your total throughput is 1.3
07:28:20.106 - 07:28:58.218, Speaker A: megabyte per second, each node, each validator does not download data at like 1.3 megabyte per second. Is that correct? Right, exactly. So in the first phase where we are not doing the random sampling the current parameters to do four samples from the rows and columns. So you would download two rows and two columns and I forget exactly how much that is per validator, but that would be the current parameter. And after that, yeah, we need to look to see what the numbers is for the exact row and column throughput. Awesome.
07:28:58.218 - 07:29:54.630, Speaker A: Yeah. So for Celestia we have kind of a few different ways to answer that question essentially, which is that the first is we don't just have full nodes and light nodes and nothing in between, where you either download all blocks, like all the block data, or you just do data availability sampling. But we have a node in the middle called a partial node. And these partial nodes, rather than downloading full blocks, they can either download a subset of all blocks or they can download, they can download like entire blocks, but a subset of all the blocks, or they can download a subset of each block. This is configurable up to the end user. But these partial nodes still contribute to the security of the network because they can still produce fraud proofs. And more importantly, even if you eliminated fraud proofs and just had validity proofs, you still need that worst case of being able to reconstruct a block.
07:29:54.630 - 07:30:39.958, Speaker A: And partial nodes contribute to that because they will do things like fully download either a full block or a full row or a full column or however you want to configure it. There's an implementation detail. So there's this node type in the middle. Now, with respect to our validator nodes, if you're a validator, the initial implementation will require validators to fully download the block. And the reason for this is not that this is a fundamental thing in our technology because we use fraud proofs or anything because again, you still need a synchrony assumption for reconstructing the block. And this is kind of the key is that by requiring validators specifically to download full blocks, it means we avoid having to deal with the complexities of dealing with the synchrony assumption. If you had validators sample blocks to determine their availability, you totally could do this.
07:30:39.958 - 07:31:42.826, Speaker A: But then the networking and the assumptions around this and how tight your timing constraints are become much more complex to reason about because you have to worry about the case where the block is available if the whole network can reconstruct the block by communicating together. And that is like a much harder process to reason about. So I wish the Ethereum researchers well on implementing this. But subsequent implementations of Celestia, if these problems are ironed out, will not require validators to fully download blocks. They'll allow validators to simply sample blocks for their availability. Just to point there in the sense. So our header structure is also constructed in such a way that when we want to increase the block, the number of commitments kind of increase in the block, which is not a very it's not a linear increase in that sense, right? So we have not experimented with those at the moment, but once we get to more better testing conditions, we will experiment with those.
07:31:42.826 - 07:32:24.054, Speaker A: Second thing is, like John said, we've also not from engineering, we have not studied that. But we have experimented with the idea of columnar full nodes because that. Is also another entity taking play part in the network and that will also, I think, be more prevalent in the future. That could be a scaling path. Basically you have validators and today you have validators and you have full nodes and you have the light clients and this is like another participant into the mix storing some of the data. Great. I think we're coming to the close of the panel.
07:32:24.054 - 07:33:55.798, Speaker A: So I want to switch into a broader question on all of you have taken very deep bets on this modular world and maybe the Celestia team coined maybe the modular paradigm and polygon has a variety of projects building a whole stack and Ethereum is shifted to a rollup centric world. I want to just get your outlook on where this paradigm is headed and what you think the interesting opportunities and challenges are. So I think what we have learned a lot from our polygon journey is we have got the opportunity to talk to so many devs so many users and what we have finally, I mean, so the broader polygon thesis is in general not focusing on one particular. So if we have a sheet of scaling solutions, right? So we've invested in ZK, EVMs, M three in fact, we believe that in general developers want like different developers, different users want a variety of different use cases. So for example, when you talk to enterprise clients, right, so they want some version of the blockchain with different properties. Maybe they don't want to have the data on a shared chain, for example. Some people want private data as well.
07:33:55.798 - 07:34:23.854, Speaker A: Some people. So there's a variety of use cases. Enterprises have different DApps are different. There's a wide variety in terms of what requirements is. Right? And so what we thought is we should focus on getting customers or users or developers their choice. So if they want to run a roll up, for example, on Ethereum, that is their choice. If they want to run validiums, that is their choice.
07:34:23.854 - 07:35:30.130, Speaker A: If they want to run sovereign appchains, that is their choice. Right? It's not a one size fits all. John yeah, so throughout the years since we started building this, it's definitely been kind of a journey of learning. When we first started, a lot of people who first heard about the project didn't really understand what it was about and didn't really understand the use cases, how it would be used and so on. So it took quite a while to get to the point that we are now that there's a lot of excitement around this notion of modular blockchains and the modular blockchain stack. Yeah, I guess I should talk about kind of from the early days when it was not exactly clear what applications we had to now when there's much more concrete vision of applications. So we have things like there are ways and mechanisms to use Celestia for data availability and provide this service to roll ups on Ethereum, and we call these celestiums.
07:35:30.130 - 07:36:35.206, Speaker A: I gave a talk at Ethanver earlier this year. I think if anyone is interested in learning more and I think a colleague of mine, Evan, gave one yesterday or the day before also here at DevConnect. And there's also the notion of submos, which I think Mustafa talked about earlier, where you have a settlement layer that runs as a solvent roll up on top of Celestia and then roll ups that as they exist on Ethereum, can use this as a kind of drop in replacement to Ethereum. And then finally we've really been pushing this notion that I really believe in is the end game for roll ups. This notion of sovereign roll ups where it's roll ups that run directly on top of a data layer and therefore have the ability to hard and soft fork, to change their consensus rules, to change their block validity rules with off chain governance. Which is not something that applications can do if they're built directly into an execution layer like DApps on ethereum or roll of smart contracts on ethereum. Those to upgrade require something like a multi SIG or a Dao, some sort of on chain governance.
07:36:35.206 - 07:37:22.454, Speaker A: And the entire Ethereum and Bitcoin ethos is that on chain governance is no bueno because on chain governance can be captured by whales, right? So it's kind of weird that you have this entire notion that the core blockchain layer can't be captured by whales. It does off chain governance, but then literally every single application on it does have on chain governance because it needs to upgrade. These sovereign roll ups are really, I think, the end game of roll ups where you can have the best of both worlds. You have off chain governance and you have shared security. Thanks so much. Chung Matt yeah. So for Ethereum, I think that there's just different choices that Ethereum has to make, being a chain that already exists and a chain that's focused on decentralization at its very core.
07:37:22.454 - 07:38:40.980, Speaker A: But what I am really excited about is that even though Dank Sharding has many things on the critical path to eventually reaching that end goal, we're working on an EIP 4844 called Protodank Sharding, which hopefully will provide a lot of the facilities for roll ups to start using what will exist at some point in the future, which is basically there will exist some oracle to provide a route that says this data was available. And as long as the roll ups understand that this route was made available by the protocol, then they can trust and they can build their chain based on that. Or if there is DK roll up, use that for their ZK roll up to do synchronous communication within that. And so I think that with 44 four, which is actually very similar to what John proposed in 2019 with 22 42, I think that as things continue to change over the next couple of years, at least we can start providing that facility that primitive for roll ups to build on this data availability layer that will start to exist in the next couple of years. Awesome. I think that brings us to the conclusion of this panel. I think it's the right question to finish to lead into the next one.
07:38:40.980 - 07:39:27.570, Speaker A: Thank you so much to the panelists for taking the time to be here. Anurag John and Matt. Thank you. Thank you. Thanks, Juvent. All right, guys, this is the last, I guess, segment of the modular summit. We are having a debate between Mustafa from Celestia and Anatoly from Solana about monolithic or modular, and moderated by yours truly, Tarun, the mad scientist of crypto.
07:39:27.570 - 07:40:04.000, Speaker A: So, again, after this, stick around. We're going to have networking drinks right here. And yeah, I encourage everyone to just hold on. And if you're talking, like, move outside and give your full attention to the debate. So let's give a round of applause. I'll help you sit here. I see that Anatoly and I both have our usual drinks in hand.
07:40:04.000 - 07:41:05.710, Speaker A: I am sure you've seen all the memes of Anatoly drinking. Well, great to see you, virtually. Great to see you guys. I like networking too. Yeah, this might be the only section today that needs no introduction, so can kind of go straight to it. So I actually want to start with this intro question that I think was teased online yesterday of sort of before we kind of get into the details of modular versus monolithic, I think we should actually just say, let's suppose each of you were kind of stuck in an elevator for 30 seconds. You had a 32nd pitch, and there was an engineer who just quit Netflix because, well, I mean, stock price crashed, and they're like, I want to understand why I should build something either on Salana or Celestia.
07:41:05.710 - 07:41:54.450, Speaker A: What's your 32nd pitch? And Mustafa me. Okay, so this former Netflix engineer is presumably a Web Two engineer. So I would use a web two analogy. If you're a Web Two Engineer and you wanted to build a web application, you wouldn't want to build it on a shared hosting provider. Like, let's say, if you're familiar with DreamHost or GeoCities or Google Pages. If you use a shared web hosting provider, you're restricted in what you can deploy because you're stuck with the same execution environment that's provided to you by that shared execution provider, and also because you're sharing the same server as everyone else. You're also limited in scalability.
07:41:54.450 - 07:43:09.558, Speaker A: That is very similar to deploying your application on a shared smart contract platform that is a monolithic blockchain. Nowadays, Web Two developers build use virtual machines on systems like Amazon, EC Two or AWS, and that allows those developers to have their own virtual server and have full control over the execution environment and scalability of the application. And that is what building a roll up is like. You effectively have your own blockchain, that you can do whatever you like on it and have any execution environment you want on it, and you can scale better because you're not sharing the same execution resource pricing as everyone else. Great. I think we were traveling slightly close to speed of light right there because we went a little over 30 seconds on our pitch. But Anatoly, don't use a blockchain unless you critically need finality in a shared context with a lot of other financial applications, like exchanges, stablecoin providers, all those things.
07:43:09.558 - 07:44:15.466, Speaker A: If you don't need that, don't even use a blockchain to begin with. But if you need that, then Solana is the cheapest, fastest way to get to the largest set of these as quickly as possible. Well, just so that we get kind of both of your individual definitions, in your own words, how would you describe the other architectures? So, Mustafa, how would you describe monolithic? And Anatoly, how would you describe modular? And maybe Anatoly, you want to start? I would describe modular as an attempt to break down the different functions of the chain with some clean interfaces and build them as separate components. I would describe Onalithic as trying to build kind of a global shared computer similar to the original Ethereum World computer vision. Cool. All right, so let's start by actually starting with this idea of, like, developer costs. Developer UX.
07:44:15.466 - 07:45:25.782, Speaker A: Startup costs. So if you're talking to someone who was just getting their feet wet with developing on either Solana or Celestia, how would you kind of describe the startup costs of going from just kind of toy project to actually running something in production to maybe having a Dow having kind of longer term sustainability? And how would you describe things like, how long does it take to get familiar with the language, the architecture, development environment? Imagine you're someone who's just getting started, Antilla, you look like you. Well, I'm assuming all the cool devs already know Rust. If not, it's a language that we didn't build. It's a language that amazing other folks have built with its own community. And people out of fangs are publishing open source Rust code. So that part is pretty easy for anybody to pick up because it's a general purpose language and the development environment is really just as close to native Rust as we can make it.
07:45:25.782 - 07:46:19.366, Speaker A: So you just use cargo and use anchor to glue all your programs together. So we've seen folks go from a hackathon to a project with, like 8 billion TVL in like, six weeks. Shout out to the saber guys who slept on the floor of the Salana office to get that done Asava. So I think the general goal for modular blockchains is that we want to make it so that deploying your own roll up chain or your own blockchain should be as easy as deploying a smart contract. And we're practically almost there with the Cosmos SDK. You can deploy a blockchain in seconds. With roll up technology, you can just fork that roll up.
07:46:19.366 - 07:47:17.880, Speaker A: You can fork the smart contract and have your own roll up. And the idea is, if we make it deploying your own chain as easy as deploying a new smart contract, then why wouldn't you do that? You would have much more flexibility over the trade offs in your execution environment and you can tailor it to the application that you need. So, to that point, I think one thing historically is that the Linux took a very long time to actually get sort of the multi processing, multi app, multitenancy environments to work correctly. It took a lot of pain in changing things in the kernel, but however, once it was done, it was like a huge floodgate. Kind of opened up a ton of app development opportunities. Like Node JS would never be able to run on a Linux machine without SMP and Linux 2.6, basically.
07:47:17.880 - 07:48:49.320, Speaker A: So do you think it'll take a similar amount of time for us to get to the point where you have this sort of automatic scheduling of roll ups, automatic kind of deployment, things like that, and that the monolithic sort of version is sort of a crutch on the way there? Or do you actually think there's sort of a fundamental difference and obviously you both probably have different opinions on that. Mustafa yeah, so, I mean, I think if you take a modular blockchain stack, if you take each module in isolation, each module is simple, but there is indeed complexity in the way that the stack integrates. So yes, module blockchain stacks, if you look at the big picture, have more complexity than a monolithic stack. Not on the individual module level, but throughout the whole stack. But that's not an issue because the whole point of modularity is that you don't have to know or care about what's going on in the entire stack. We're reaching a situation where, for example, Core Dev in Geth all the Way said that the guest client has so much complexity now that no guest developer actually knows what's going on in the entire Geth. Node I think that's part of the reason why modularity is important.
07:48:49.320 - 07:49:52.474, Speaker A: It does allow you to do more and have more complicated and blockchain architectures that do more without needing a single person to understand entire monolithic stack. And so yes, to answer that question directly, it will take us longer to get there. It does take us longer than building a malithic blockchain. But since 2008, 2020, people have just been building new layer ones and we've ended up in this cycle of new layer ones. So it's now time to kind of break that cycle and kind of evolve into a stage where now we have so much traction in blockchain space, let's modularize it and make sure that all the different innovations that people are building can actually be useful. Anthony the irony is that two six once it was stabilized, killed the microkernel design and the monolith. The kernel one is true.
07:49:52.474 - 07:51:03.582, Speaker A: The rust mock kernels have never really caught on. I kind of think that it's going to depend what the use cases are. If you have dependencies on composition between applications and these applications are not static, right, you have constant kind of churn between the composition set, then you really need one giant state machine to glue them all together. Because what the users are going to care about is how quickly do I access all the state, whatever that may be. Maybe it's like positions in serum mango and UXD or whatever, and how quickly it's finalized and how quickly it's connected to all the other financial institutions that I care about from stablecoins to exchanges. And that network is not something you can spin up in a smart contract that's actually kind of the living breathing glue of the chain itself. So you could have that in a modular architecture, right? There's a bunch of tenderments, a bunch of them have different pieces.
07:51:03.582 - 07:52:50.930, Speaker A: But if you have use cases that run even 20% worse, right, have 20% higher fees, 20% higher latencies, that's like your phone running 20% slower, people are going to notice and applications and everything else will shift to that environment where everything is optimized to the help. And to do those optimizations, you don't really care about the design or how complicated it is. What you care about is how fast can we achieve these goals for the use cases? And this is why modern operating systems are all monolithic. Even when somebody tries to build a microkernel design, when it actually shipped, you look at the 80% of the use cases that people use, the frame buffers, everything else, it's all punched through directly to the kernel without using any of these modular features. So how this plays out is going to be largely like use case dependent. I personally believe that what these chains are good at is running like a Dex, right? They're good for price discovery, they're good at managing offers and bids and all this other stuff for digital items and that market function, what these things provide allow everyone else to build unpredictable use cases like NFTs, which are their own financial contract. But at the end of the day, what you need to do is synchronize offers and bids and do this as fast and as quickly as possible and connect them to all the rails.
07:52:50.930 - 07:53:30.670, Speaker A: Yeah, I want to challenge the micro kernel analogy. I don't think analogies are bad, so rip it apart. I would say it's more closure. The analogy I would use is in the Linux operating system. You have kernel space, have code that runs in kernel space and user space. I would say having a monolithic chain is like running all the code for user applications in the kernel space. But the way I see it is like having modular blockchain is having a user space, which is like the L2 and having actual applications running there rather than in the kernel.
07:53:30.670 - 07:54:31.700, Speaker A: Yeah, I feel like there's sort of merits to both of these. If we look at history, history has sort of preserved both models. There's obviously an intense amount of work on extremely low latency code kernel bypass like all the types of stuff that Antonio was mentioning. But then obviously much of the value of the consumer internet has come from realistically making it so that I don't need to know what a page table is to write a piece of code, right? Like what percent of JavaScript developers have ever heard the phrase page table probably sub 2%. So there is a sense in which developer friendliness is also quite important. And to that kind of point, another thing that of course people love talking about nowadays a lot is extractable value from validators and miners. And this is something that both developers and users of these systems have spent a lot more time thinking about.
07:54:31.700 - 07:56:01.630, Speaker A: And one thing to kind of compare and contrast between monolithic and modular architectures is the idea that modular architectures have a lot more sort of things like you have to wait for certain transactions to go through, there's certain predictable latencies that allow people to sort of find newer forms of extractable value. There's sort of this sense in which the tanatole's point earlier heterogeneous fees make it perhaps more easier to extract value. So one question is do you think this ability perhaps to extract more value from modular architectures for Arbitrariors and sort of market makers, do you think that's a feature or a bug? Because it could actually be something that drives liquidity as well as also kind of parasitic towards users and sort of where do both of you kind of see that? It depends on how your execution environment works, specifically if your execution environment has a sequencer. So in Ethereum L2 S, the mev is basically just moved to the L2 sequencers. Like in optimism, for example. For example, the Optimism roll up. They see that as a feature and not a bug because they see that the sequencer can extract the mev and use that to fund public goods.
07:56:01.630 - 07:56:58.940, Speaker A: And that also kind of goes to kind of correspond to Anatoly's point about finality. It's not necessarily the case that you need layer one finality for users to have sufficient finality on the transactions going through. So for example, like Arbitrum's inbox model or Optimism sequencer model, you can get finality by looking either at the inbox or asking the sequencer immediately in a second faster than the Ethereum block time. So there's different guarantees of finality. That's more of a spectrum than just relying on layer one finality Anatoly. So those are all kind of compromises, right? Because for it to be a true L2, it needs to be able to fail at any moment. And that means that whatever partial finality you get is then in question.
07:56:58.940 - 07:58:35.290, Speaker A: However, failure recovery works in that setup and that trade off is probably okay for applications and users in like a sandbox. But what is the whole point of a blockchain, right? Is it to build an application that could run on a database or is it a global replacement for a large chunk of finance? So we'll probably see both of these models survive because of that, because we don't really know why these things exist yet. I think with regards to mev, I tend to agree that so far we're probably the only thing that we see that actually funds and generates value in these chains is mev. And in the best of light, you have very fancy algorithms that try to predict the future and they want to improve prices and offers and everything else on this chain and they run their racks of GPUs and they extract mev. And in theory it should get competitive to the point that they should be biding for users to send their orders directly to them. You see some of that with Flashbots and some of that with Jito Network and Solana. So those are all good things, right? Because in theory, a user that doesn't care about a MAV should in fact see a better price and maybe even a rebate for their transactions instead of a fee.
07:58:35.290 - 08:00:13.930, Speaker A: But that's still kind of, I think, pretty far away. We're still not yet at the level of adoption where I could say that, yeah, this is true and this is how these chains run. So a lot of questions still remaining. But for the sake of the debate, my strong opinion is that mev will be the thing that's actually driving development and hardware upgrades and actually going after users in these spaces on blockchains. Yeah, I mean, I guess to that point, if we think about something that you said in terms of replacing existing financial systems or sort of having the ability to service sort of a larger swath in the market mev in Solana to some extent right now and then Solana and Avalanche in particular where there is a lot of validators advertising like hey, we'll sell you this much space in this block at this time and people basically are know. Do you think that this is sort of deleterious and that it'll just reduce to the existing sort of model of sort of colocation around exchanges, colocation around validators? Or do you think that you could actually sort of make this more sustainable with a different fee model or potentially better cryptographic solutions? Because I do think one thing for the monolithic world is there's a lot more probabilistic mev. There's sort of two ways of thinking about it.
08:00:13.930 - 08:01:48.530, Speaker A: Mev on single chains is a congestion game, right? There's like just a fixed amount of opportunities and way more bidders than opportunities. And it's more about just like constructing a mechanism that matches those mev and modular architectures is much more probabilistic, right? It's about routing, figuring out which path to take. And so I guess from both of your perspectives, do you think there is a way in which the economics model can thwart kind of that becoming all of the volume on the network? Or do you think that effectively there's no way of avoiding that and we're sort of stuck with the existing system? I can go first. So the way I think about it is that we want to make it as competitive as possible and as easy for anybody to enter the network and offer my whatever mev solution, because that should drive the amount of actual profit extracted by these things to as close to zero. And that should be a good thing, right? Like what you should see in the end is better prices, prices that reflect the true value of things globally, all synchronized across the same state machine. And to that regard, at least in Solana, it's possible to run multiple block producers on the exact same state concurrently at the same time. Doesn't do that yet, but in theory, that's something that is doable.
08:01:48.530 - 08:02:54.806, Speaker A: Because of the way the VDF works, we can effectively splice the final ordering using the VDF ordering between all of these. So what that should hopefully do is that instead of only having one block producer at a time, some position in the world, you now have N, and then you go talk to the closest one geographically, that reduces out latency. And then that thing is then running whatever algos and whatever it needs to optimize its order flow. And it's doing that competitively with everyone else. And in theory, that should reflect the lowest price of any good offered on chain globally within the speed of light latencies around the world. And if we can do that, then in theory, price updates moving through Solana are moving at the speed of light through fiber, which is as fast as news travels, and then it's competitive with something like Nasdaq and CME. So that's kind of the dream, the horizon.
08:02:54.806 - 08:04:02.750, Speaker A: And this is how we've been building everything, looking at that final state, how do we get there and what are the pieces we need to get to? And when you kind of take that perspective, you don't really care if the architecture is modular or monolithic. What you care about is, can we synchronize information globally as close to the speed of light as possible? And usually, at least so far, there's no gains to be had using a modular architecture. That right. Now, everything that we've been doing is around this monolithic state because that's what's going to get us closer to that final state of global information. Symmetry. All right, well, I'm excited for Mustafa's response to that. Yeah, I think this discussion kind of highlights the key kind of philosophical difference between Solana and systems like Celestial or Ethereum, which is, I think systems like Solana view blockchains more as a distributed computing platform where you can have global settlement.
08:04:02.750 - 08:06:10.570, Speaker A: Whereas we like Celestia and also other systems like Bitcoin and Ethereum, we see blockchains fundamentally, the fundamental purpose of blockchains is that it's a verifiable computing platform, not just distributed but verifiable. And that means there's a focus on decentralization in that setting means that you need to allow light nodes to verify the state of the chain. Otherwise, like, if your goal is just simply to have fast agreement on the state, how does that goal fundamentally differentiate from what Web Two achieves with a centralized or distributed database? Even in, you could argue that the fundamental difference is that you have a global set of distributed parties agreeing on that state, but without the ability for end users to verify these states. In my opinion, that takes away one of the key properties of blockchains, which is that well resourced actors cannot violate what the social consensus of the chain has agreed, what the rules of the chain are. If this Lana Valde is, for example, wanted to change the rules of the chain, how would end users verify that without running a well resourced node? Yeah, you teed up the next question, which is to those that haven't spent time sort of in the weeds on these modular and monolithic architectures force very different separations of storage and execution for light clients versus full clients. So for the audience first, let's just start with describing kind of how light clients work for Solana and Celestia and sort of where the division is and which parts the light client has to store and execute first. Not and maybe Mustafa, since you brought up light clients.
08:06:10.570 - 08:08:00.364, Speaker A: So light nodes effectively allow end users, ordinary users, whether a cheap laptop or mobile phone, to have almost the same level of security as a full node because they can get assurances about the state of the blockchain using technologies like fraud proofs or ZK proofs. And this is the fundamental reason why the Bitcoin community has decided not to increase the block size limit. In theory, Bitcoin could do a Zillion transactions per second if they increase the block size limit and optimize the node software. But the Bitcoin community has fundamentally decided not to do that because it would increase the resource requirements for end users to run full nodes and validate the chain, which is like pretty much the critical aspects of what a blockchain is supposed to guarantee that the state of the chain is correct effectively. But with new light node technology that uses fraud proofs and data availability, sampling and ZK proofs, you can now increase the block size limit without compromising the ability for end users to validate and verify the chain like clients on Salana. So the devils in the details, obviously here, when you look at E two deployment, go to nodewatch IO, there's about 6500 machines that run the 300,000 e two nodes, e two validators. There's only.
08:08:00.364 - 08:08:37.880, Speaker A: 6500 actual boxes. So the real world deployment is that you take a Salana Validator and then you load it up with like a few hundred e two nodes, right? E two validators. And that's your e two deployment. So the real world is that machines are getting bigger. And the Salana validators are not stacks of Xeon processors. They're like 32 core systems, which is what you get out of a data center right now for $800 a month. Which is not insane by any means.
08:08:37.880 - 08:10:03.280, Speaker A: It's not building racks and racks of systems like you would for centralized service to handle all those users. So there's a bit of of kind, I think. I don't know. You have to actually look at the details of how these things are deployed. I think it would be misinforming to say that the amount of hardware necessary to run Solana is that significantly different than any of the other chains when you actually look at how these things are being deployed in the real world. But light clients themselves, when you're talking about like a different system for verification, there's obviously trade offs. So whenever anyone brings this up, the question that I have is assuming all the other nodes, everything else has been destroyed, how many light clients do you need to rehypothe the network to reconstruct it? So how many nodes are you trusting when you run this thing? And those trust assumptions are important to users because the end state of a monolithic chain is when you don't run any nodes, you're trusting that at least one out of the, whatever, 3000 boxes that run the replicate salana, at least one of them is honest.
08:10:03.280 - 08:11:43.216, Speaker A: So when you run a light client, you partially participate in validation and then you assume that at least X number of other light clients are honest to actually help you get those guarantees. So it's up to the users to pick which one and if they even care. Yeah, I think one thing that's actually been an interesting development is end users have actually had to learn the difference in light client design, sort of by spam and DDoS type things, right. Where recently sort of Solana has had some issues for end users using wallets and like clients, which every chain, if we actually look at their history, has had similar issues. I guess I just want to point out that the end user does notice this, but they may only notice it after they've already adopted something. So how do you kind of view the pros and cons of the, like client design for your chain and its impact on the end user who know their friend told them to download a wallet to buy an NFT? Does this follow up to me? Yeah, well, since you said it's up to the user to care, I feel like, first of all, it takes a hell of a lot more resources to stay on tip and to run salana at the latest block. If you're talking about just verification, you can look at how Salana deployment works.
08:11:43.216 - 08:12:46.200, Speaker A: Out of the 32 cores, there's only four threads dedicated to the entire banking stage, and that includes all the votes as well. So if people don't know how Salana works, the execution of smart contracts, it is optimized to the point that we can use it as the rail for voting and consensus. So consensus runs just as a smart contract. So out of, like, 300 million or so transactions per day, 80% of them are just votes. And it doesn't matter because there's still out of those 32 cores, only four threads are dedicated to run all the transactions. So validation is actually the smallest part of the entire stack. The true cost of running Solana is how do you synchronize globally all of this information that's being propagated, and that requires a big pile of erasure, coding, and signatures and everything else to propagate this data simultaneously around the world to all the machines.
08:12:46.200 - 08:13:40.568, Speaker A: So that part is the expensive part. If we're talking about a world where users actually care about, hey, I want to make sure that the wallet and the thing that I hold hasn't had these big validators committing an invalid state transition. You can do that much, much cheaper, and you can do that with M and event schemes and a whole bunch of other stuff, and you can build those applications without actually impacting anything else about how Solana runs and how it finalizes. And those are different concerns. These are not concerns about, I want fastest finality for trading and price discovery. This is, I'm holding my assets, and I want to make sure that the big validators don't rug me. You can serve both of those, and that's something that if the users demand it, people will build it.
08:13:40.568 - 08:14:21.830, Speaker A: That's basically the answer. But what users are demanding right now is, I want the fastest, cheapest finality, because what we care about is distribution of these NFTs to the largest number of users. Mustafa yeah, so I think there's kind of two things there to unpack. First of all, there's this question, like, there's this point to be made that users don't care about validating a chain. But you can also say if you built a completely centralized blockchain with one validator, from the user experience, it looks the same. It's still got a wallet. Will users care? Maybe they will.
08:14:21.830 - 08:15:02.208, Speaker A: My point wasn't that users care about validating is that with a light client, you have honesty assumptions that you need at least ten other light clients to be honest, and that's an honest minority. With a normal, decentralized network, like a monolithic one, you only need one out of the end. So that's a real trust trade off that users will make. And it's not about going fully centralized or not, right. I'm not trusting Facebook. I'm like assuming, okay, there's a permissionless network, at least one out of the N is honest or there's a permissionless network along a like client. And I need at least 40 out of the 50,000 to be honest.
08:15:02.208 - 08:15:43.420, Speaker A: But it's not one off end because you need to trust the honest majority to make sure there's no invalid state transition. No, any one of them can detect it and flag it and say, hey, what the fuck just happened? The entire majority of the chain just did an invalid state transition and I have the record to prove it and I'll provide the data availability for it. You only need one out of N on any monolithic network. Yeah, but that is fraud proofs. What you're describing is fraud proofs and that's what roll ups do. No, it's simply here's the data to prove that the majority went rogue. Everybody can download it and then validate it locally.
08:15:43.420 - 08:16:38.290, Speaker A: But that is basically the definition of a fraud proof. You give someone one transaction that's invalid and then they can check for themselves that it's invalid. They would have to check the entire ledger history. So this is a very big fraud proof. But every chain that has full replication, like Tendermint, BFT, whatever, they all rely on this idea that if one out of the N detects an invalid state transitions, that they can PagerDuty goes off and then somebody yells, what the fuck happened? So it sounds like what you're describing is similar to the concept of alerts in the original Bitcoin white paper. But the problem with that is that it kind of simplifies back down to in a worst case scenario, you have to run a full node to verify the history. Of course, it doesn't really fix the problem with that light.
08:16:38.290 - 08:17:32.288, Speaker A: You need to be able to prove light nodes that have low resource requirements, that something is invalid. Yeah, I think this is the main I only need these resources when this happens, right? So one of the end validators say something went wrong. Everybody in the community that's holding assets says, okay, give me the data and we'll validate it. So what is the cost of that versus the cost of me running a light client? Which then depends on at least some minority of light clients being honest and running. But what's the pause there? If I'm running a SLANA light client, if that is supported in the future and then someone says alert, do I have to buy new hardware just to validate that specific alert? And that alert could be spam as well. Maybe. It's up to you.
08:17:32.288 - 08:18:18.064, Speaker A: Right. I think that validation, like I said, is much, much cheaper than running a full node. Because even in our current deployment right now, only four out of the 32 threads is dedicated to smart contracts and they're not saturated. So do you need to buy new hardware? Do you can use your laptop? Well, I mean, fundamentally that still doesn't really help users. Users might want again, light nodes need the ability to verify the chain with low resource requirements. Maybe some users are fine with buying new hardware, but I'm pretty sure a lot of them aren't. Not a single light client, which where the rest of the chain is completely dishonest, cannot validate anything.
08:18:18.064 - 08:19:18.400, Speaker A: So a user with a like client is validating in an honest minority. So that trust assumption exists, like it or not, it's there. Yeah, but honest minority assumption is inherently a significantly weaker trust assumption than an honest majority assumption, because it's not an honest majority. It's one out of N. All I care about is that one out of N of these validators is honest, and then I can validate whether they're a claim that there's been fraud in the network is true or not. But it doesn't fix the fundamental problem, which is that you need to be able to have light nodes that can verify something went wrong with low resource requirements because in your scheme, they need to upgrade their resources to validate. I don't believe that fundamental problem exists because the actual Amdold's Law of execution, looking at purely the smart contract side after everything has been validated, is 20% of the actual running a validator.
08:19:18.400 - 08:20:02.732, Speaker A: So your requirements are just naturally at least five x lower. Yeah, but the 20% is still significant. My laptop is not 20. I have a question for you. Actually, I have a question for you. The goal of Salana is to have a world computer model where you have like a single settlement layer. What's the end game? How do you scale that? Are you just going to keep increasing the node resource requirements? Because right now you're saying it's $800 and that's fine, but there's going to be more demand and what's endgame.
08:20:02.732 - 08:20:34.872, Speaker A: So that happens naturally, simply because for the same cost, you get offered twice as much hardware roughly every two years. So without even asking for it, the new machines that are being deployed right now have dual 25 gigabit. Next cost didn't change. It's just everybody upgrades. That's just part of the cycle. In two years, my laptop that I have right now is going to be twice as powerful. You believe in perpetual motion machines for Moore's Law? It's going to continue forever.
08:20:34.872 - 08:21:36.368, Speaker A: Really? So I believe that if we stop doubling the amount of vector dot products that we can do every two years, then we should all be working on bunker coin. To some extent that's true, but there's also this problem, right, from a hardware design standpoint of we're already at EUV Lithography. There's not actually that much further we can go before we're set like electrons tunneling between no, bandwidth is going to keep doubling. Maybe single core performance is going to get saturated, but you're going to keep doubling the amount of bandwidth. And what we care about is bandwidth, right? We care about can we handle 20 gigabits in a single box? Right? That would be really hard right now, but we can do one in two years we can definitely do two or maybe even eight depending on how architectures change. And within ten years you can definitely do 40 gigabits on a single machine. This is instantaneous transfer.
08:21:36.368 - 08:22:43.952, Speaker A: How much storage do I actually need locally? Because one thing I guess from kind of your argument of hey look, people will be able to effectively submit this fraud proof by validating the entire chain. Is this idea that the storage requirements are somehow static. Like yes, we're only using four cores to process but if I have to actually validate the entire history of 300 million transactions a day, that's basically like 30 billion transactions a year at some level the storage requirements also go up with bandwidth requirements. So how do you kind of combat that in the long run? You care about that happening within a certain fixed time period, right? Within an epoch. Because all proof of stake chains have an epoch. If you go beyond the epoch, leak keys can generate a fork, right? So you have some assumptions about like within this fixed amount of time somebody's going to say hey, what the fuck? Hopefully. I mean, it sounds to me like lying on the assumption that Moore's Law will go on permanently.
08:22:43.952 - 08:23:27.972, Speaker A: Which I think is an assumption that has not hold true for all types of hardware over the past few years. It has over the last 20 years. The amount of raw throughput compute wise that you can do on a chip, push through a Nick, push through an SSD, push through the GPU that's been doubling at a steady pace. But not for hard drive. What we're talking about is not cycles per second on a single core. It's like raw bandwidth across an array of cores or DSPs or whatever CUDA threads. I'm pretty sure it hasn't held true for hard drives if you look at the data.
08:23:27.972 - 08:24:33.960, Speaker A: But the other thing that's missing here I think, I think you're assuming that you're ignoring the fact that people also have to sync the chain from scratch. You need to have people to be able to sync the chain and bootstrap from scratch. Yeah, it's like 30 terabytes of transactions per year. Is what we're seeing right now. Dirty? TB is not intractable and what's that going to increase to? But you don't need the whole year, right? You need it within the epoch. So what is it over like two days, maybe 20GB or whatever? Is that so fundamentally difficult that I could do it right and a very large number of users could do it. So is that assumption? My point is is that assumption and that hurdle to actually do this verification is it so much harder and such a big differentiator versus I have to have a light client constantly running plus I have to assume that there's an honest minority of light clients.
08:24:33.960 - 08:25:26.020, Speaker A: So that's the difference, right? You can do that or you can rely on at least one out of N to provide you the data. So the larger the network gets, the more likely that one out of N is honest. And this is where there is a clear difference between something like salana or tendermint where most tendermints run to 150 or so validators. Salana is at close to 2000. So I think we disagree on what is a large number of users. If we're talking about 38 terabytes a year, we're talking about reaching the stage where as a user I have to effectively buy a whole bunch of hard drives, put it in my living room to verify the chain. Whereas what we're targeting is you should be able to do it on your laptop.
08:25:26.020 - 08:26:20.510, Speaker A: You shouldn't have to buy specialized hardware just to be able to verify that the chain is valid. You're participating in validation with an honest minority assumption, which is different. Majority assumption is a much weaker assumption than honest majority assumption. But like I said, we're not talking about honest majority assumption. We're talking about hardware costs and a one out of N assumption. I totally own that the user Strading hardware costs plus relying on at least one out of N of the validators to provide them the data. I think this is probably where it's very clear the philosophical differences between what it means to be a user and what it means to be a validator are kind of hidden here.
08:26:20.510 - 08:27:55.700, Speaker A: But I do, before we kind of wrap up, kind of want to talk a little bit about cross chain compatibility. Obviously at this point I think it's pretty much impossible to say. We're not going to have a MultiChain world with different use cases, different qualities of service, different types of guarantees for users. And one kind of advantage of modular architectures is that they do spend a lot of time standardizing interfaces for cross chain communication and they effectively build in messaging standards or rely on actually specified messaging standards. Whereas for monolithic architectures it can be quite different. As we've kind of seen, trying to basically translate between different virtual machines and get sort of bitwise reproducibility can actually be quite hard, especially when there's synthetic assets involved. So how do kind of like monolithic architectures compete with platforms that have focused on building in native messaging? And do you think it's essential or do you think that eventually there will just be a communication standard that everyone just agrees on? Well, I can start, I guess, but I think one paradoxical thing of the vision of having a shared like a single settlement layer that Solana relies on is that if you look at the blockchain system, we don't have a single settlement layer.
08:27:55.700 - 08:29:24.548, Speaker A: We do have a multi chain ecosystem with bridges like you have Wormhole Bridge between Ethereum and Solana. The market has already shown we can have multiple settlement layers. We don't need a single settlement chain if the market has already decided that. And right now we have these insecure trusted bridges that can have that have multi sigs then why don't we just embrace that and have a modular architecture that uses roll ups and has these more secure trust minimized bridges. Antoly bridges are man are scary and the part that is the scary part is not the collusion which something like a native like roll up architecture fixes. It's the smart contract risk and implementation risk because of just even the simplest piece of code can have a bug and that's really the scary part. To that extent though I think a natively baked in roll up where this is the standard implementation that is owned by the layer one is probably likely to get to a point of extremely high security and confidence in the code much, much faster than something where every user does their own thing.
08:29:24.548 - 08:30:41.472, Speaker A: Every developer does their own thing. So that part is like who knows how that will play out. The stuff that does work pretty easily and pretty well are actually like the very dumb simple things like USDC where that's effectively bridged across multiple chains and there is a very heavy trust assumption on circle to make sure that there's dollars in the bank and you saw that be pretty successful with wrap bitcoin as well on ethereum. I can't tell honestly if this is how things will play out over the next five years, but I feel like that's probably a model that if there's demand for cross chain assets in large volumes, that's a model that's going to stick around for users. And I don't know why would users would switch from that to a more permissionless one unless it takes much more time to get assets listed in these centralized systems, which is obviously a hurdle. I see a lot of advantages. So this is like a point that's really hard to fight against.
08:30:41.472 - 08:31:54.412, Speaker A: If you have very native, very baked in roll up model that is totally part of the layer one, I would tend to think that's going to have a lot more confidence in terms of code security and that's the biggest attack surface in all the bridge solutions that we have out today. Yeah, and in a lot of ways I think there's definitely been some truth in that. The optimism bug that was found was very similar to the wormhole bug and it's mainly this idea of having the synthetic asset on both sides and ensuring that they stay synchronized, supply stays synchronized, price stays roughly synchronized. That seems to be the source of these kind of bugs. Definitely. I think we're still not at the point of hey, the contract node bugs are not kind of an issue. So before we wrap up, I guess if you were to give one compliment to the other type of architecture and say something nice about it, what would you say? I like slalana's execution environment.
08:31:54.412 - 08:32:39.780, Speaker A: I think parallelization is a very good idea. I just don't like the vision of a single settlement layer, basically. Hey, something nice was still said. I really like the idea of actually separating data availability and focusing development effort on that, because I think that's easier to make it secure. And the biggest hurdle to iterating is security. So there's actually, I think, an opportunity for modular architectures if they can iterate faster because they've broken down the pieces into something more manageable. So that's like, an honest compliment.
08:32:39.780 - 08:32:55.576, Speaker A: Well, mine is honest as well. Awesome. I don't know. Are we doing questions, or no, we're good. All right, well, hey, thanks, everyone, for listening in. Thanks, Antole. I'm sure it's like, I don't know, 05:00 A.m.
08:32:55.576 - 08:33:02.910, Speaker A: For you? No, it's 09:00 a.m. Okay. It's perfect time. Still too early. Thank you, guys. Still too early. Thanks for waking up.
08:33:02.910 - 08:33:17.808, Speaker A: Yeah. All right. Thank you. Thanks, Tarun, for moderating. That was a really good job. All right, guys, that concludes the modular summit. Thank you all for coming.
08:33:17.808 - 08:33:36.210, Speaker A: I want to thank all our sponsors and speakers again. We have drinks right now, so hang out. And there's swag as well. If you want one of these t shirts, build modular, be free. We have them on a table in the back, so thank you again.
