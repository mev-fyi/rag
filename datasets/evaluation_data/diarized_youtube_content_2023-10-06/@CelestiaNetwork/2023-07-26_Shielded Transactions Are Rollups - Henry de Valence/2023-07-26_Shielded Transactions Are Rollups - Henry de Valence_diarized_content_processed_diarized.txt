00:00:00.090 - 00:00:09.738, Speaker A: You. All right. So here we are. Hello, everyone. My name is Henry. This is my talk. Shielded transactions are roll ups.
00:00:09.738 - 00:01:29.430, Speaker A: So if you're already convinced, then great. Otherwise, let me kind of go through what I mean. So I work on a project called Penumbra. What it is is a private proof of stake L, one that has an interchange shielded pool so you can record any kind of asset from any IBC compatible chain. And what can you do with those assets once you move them into the shielded pool? You have a dex that allows people to do private on chain strategies. So this talk isn't primarily a talk about what Penumbra is as a product and so on, but we've been focused on how do we solve this one really specific use case. And then from trying to solve that one specific use case, what are the kind of common features that we can generalize to more varied kinds of computation? So to start off the talk about sort of how do we view shielded transactions as being like a weird kind of roll up, why don't we say, what is a roll up in general? And I think a lot of the time people have this idea of, oh, a roll up is a way to have more copies of ethereum, right? And I think that's a pretty limited perspective of what we could do in general.
00:01:29.430 - 00:02:30.990, Speaker A: I would say a roll up is when you have one part of a system that we'll call the base and there's another part of the system that we call the roll up. And there's this kind of like, flow within this system where the base offloads execution onto the roll up. And then the roll up sends back a state route as well as some kind of reason why people should trust in that state route. Maybe there's a ZK proof, maybe there's some kind of economic model with an optimistic roll up, but fundamentally it's about having a flow of execution moving out onto the rollup and certification and kind of summary of the results coming back. So there's a super, super enormous flexible design space here. And in this talk, what I want to do is look at a shielded chain from this perspective of thinking of things as roll ups. So to start off, in order to have a shielded chain, you need to have some kind of composable state.
00:02:30.990 - 00:03:29.706, Speaker A: So you need to have the state of the chain split up into all these little fragments. And each transaction is going to consume certain existing state fragments and then produce new ones as outputs. So this is kind of like a UTXO model, although I personally am a little hesitant to use the word because it has a lot of Bitcoin related baggage. Really what's happening here is that the state is split up into fragments and transactions only operate on certain fragments of the state. Why do we need this? It's so that we can replace all of those on chain state fragments with just commitments to those states. And that way, instead of having to have the transaction actually work on all of those things directly, we can just do a ZK proof and hide the details of the state from the public chain. But when you do that, it's not really just that, oh, we add in a ZK proof.
00:03:29.706 - 00:04:42.998, Speaker A: Fundamentally, what's happening is that the execution of the state transition is moving off chain. And so effectively you can think of each individual transaction as being its own little sort of micro personal roll up. And a lot of the problems that arise in trying to build practical shielded chains, you can kind of see from this perspective, as we'll see so one big problem that comes up is I guess never mind. So yeah, before doing that, we'll give a little more detail on this perspective. Right, so what do I mean exactly when I say that a shielded transaction is a kind of a micro roll up? Let's look at some transaction on a shielded chain in general. What are the pieces of this? We have some kind of ZK proof that's going to provide trust that everything that the transaction is doing was done correctly. We're going to have some commitments to new output states that this transaction has created and then we're also going to reveal some nullifiers that consume the input states.
00:04:42.998 - 00:06:05.780, Speaker A: So you can prove in zero knowledge, hey, I know about this state that was previously included in the chain, it's valid. But you now have a problem of how do you prevent double spends. And the general technique to do that is you assign each sort of piece of state a unique serial number or nullifier that's only derivable by the user that controls that chunk of state. And that way they can reveal this random value and kind of remove that piece of state from the kind of active set without revealing exactly which state they're nullifying. Finally, a shielded transaction generally is going to have some kind of encrypted payload in it. And the reason is that in order for me to use this chain, I need to know not just the chain needs to be convinced that my transaction is valid, but as a user, I care about being able to learn what exactly my transaction is. If I go on another device, if I'm trying to sync, how do I recover my own state? So you can think of an existing shielded chain, like for instance, Zcash, as kind of bundling in this data retrieval mechanism into a monolithic chain design.
00:06:05.780 - 00:07:31.502, Speaker A: So what's kind of the problem here? This is what I was about to get to is why haven't we seen this be particularly useful or receive a lot of adoption? And I think the problem is that when you do this change, what you lose is the ability to do late binding. What I mean by that is we have this picture of, okay, here's a transaction, it has these inputs, it has these outputs and this whole state transition is this kind of sealed pre computed thing. But when you look at what people actually like to do with blockchains, they like to interact with the chain. And so that means that they need to have some kind of late binding capability. You want to say I want to do a swap and when I do my swap, I'm going to commit to these are the inputs that I want to swap, but I'm not going to sign over like here's the exact state of the uniswap reserves and here's therefore the exact amount of output that I'm going to get. I don't know that because at the time that I'm making my transaction, I don't have access to that state and I can't ask the entire world to just stop and do nothing while I submit my transaction because I'm important. So the way that this is usually done is that on a transparent chain is that when the transaction is executed, it can access the chain state and so the chain can kind of fill in the gaps and then determine what the outputs are.
00:07:31.502 - 00:08:42.254, Speaker A: And naively, when you do this sort of shielded transaction rearrangement, you lose this ability. This is one of the things that we really focused on in trying to build a Dex. This problem of how do I know what the price is before it gets executed just shows up very, very clearly at the start. And the answer that we came to is that in a sense this is a kind of like a similar problem as doing like cross roll up communication. If every user is doing their own little independent state transitions on their own end user device, then somehow those need to be able to communicate with each other and they're all going to be executing Asynchronously. So we need to have some model to do asynchronous ZK execution via message passing and a kind of schematic diagram of how this can work is I'm going to make an initial transaction that kind of sets off the action that I want to do. It's going to consume my private inputs, but because I don't know what I'm going to be sort of filling in the gaps with yet, I can't finish the computation immediately.
00:08:42.254 - 00:09:58.570, Speaker A: So instead I'm going to send a message to whatever public contract I'm interacting with maybe that's on some other shard of the state. And my output for my initial transaction is actually going to be a commitment to some future in the programming language sense of an Asynchronous computation that's waiting for some fields to get filled in and resumed. If you've done Async await programming, you can imagine sort of each await point where you're waiting for some message to come in as turning into a point where you need to pause the computation. You need to commit to all of the intermediate execution state at that moment. And the reason that you do that is so that later, once you get the message from the contract coming back, maybe that's like, hey, your swap was executed with this price, now we know what the price is. Now you can mint your outputs. The user who had created that commitment to their intermediate execution state can now resume execution by spending their commitment and slotting in the public inputs from the contract into the appropriate places and minting their private outputs.
00:09:58.570 - 00:11:01.200, Speaker A: So when we do this on Penumbra for swaps, the public input that's coming in from the contract is what was the batch price for that block? And the private outputs are then that user's pro rata share of the batch. So what I think is really interesting about this perspective is that it means that you can kind of generalize, right? You start with this idea of oh, we have a multi asset shielded pool. We can record any asset, but since those assets can be anything, they can also be assets that represent arbitrary intermediate execution states. And so now in the shielded pool, the invariant that I can't double spend funds, that I can't just print tokens is saying that I'm not allowed to just kind of restart execution. I can't clone my program. I need to sort of only act advancing my computation in the allowable way. But the shielded pool can be recording anything that I want.
00:11:01.200 - 00:11:55.806, Speaker A: And so this, I think is what kind of gives me this sense that right now there's kind of this split in the ecosystem where there's a bunch of efforts that are working on ZK for scaling. There's another collection of efforts, penumbra included, that are working on ZK for privacy. But I think we're actually going to see a kind of convergent evolution of these things into some sort know, glorious future. This is the shill portion of the talk and the perspective there is that what privacy enables is a kind of edge compute for blockchains. So in the current world, right, all of the execution is happening sequentially, one after the other. Everybody is doing a transaction, taking a lock on the entire state of the world. Everybody else has to stop everything while I run my transaction.
00:11:55.806 - 00:13:05.986, Speaker A: And in this private world where we do computation, asynchronously we can push the computation off of the base layer and out to the edges of the network. But that's not necessarily saying like, oh, let's just make a second copy of the exact same state model, but we can go much, much further and have this sort of fractal pushing all the way out until the end user device possibly over multiple hops. And then those users are going to send back their transactions that have only their proofs and data and those things can get summarized as they move toward the core of the network. So in this picture, the reason that it's sort of like, grayed out, right, is that there's no actual computation happening there. It's just certification of the data. And these are all users with their own wallets doing their own computation locally. With this perspective, there's another kind of neat thing that this enables, which is that there's new possibilities that come out for state management.
00:13:05.986 - 00:13:40.766, Speaker A: I think this is one of the hardest problems with blockchains and with scaling blockchains. Having every single person in the world just use one world computer doesn't quite scale. And if we go back to this picture of, like, okay, what is actually in this shielded transaction? We can look at it and say, what's really the expensive part here? Right? So we've got this proof. Let's say it costs, like, two milliseconds to verify. That's not a big deal. We have the commitments to the output states. Those are 32 byte values.
00:13:40.766 - 00:14:19.706, Speaker A: Clients might have to feed those into a tree that they're using locally for proving, but they can filter. Like, hey, these ones I don't really care about because they don't relate to me. The nullifiers are also 32 bytes. Really, only full nodes have to be processing those. And although there's discussion of, like, hey, there's this nullifier bloat problem where the nullifier set grows forever, I mean, realistically, you can have a lot of 32 byte values before you really run into problems. And so I don't think that's a pretty significant part either. The real problem is this encrypted payload, right? It's not 32 bytes.
00:14:19.706 - 00:15:17.026, Speaker A: It's going to be bigger than that because you need to have the full transaction data. And unless you can build some other kind of system for allowing users to identify which states are related to them, a priori would have to be scanned by every client. And most critically, if you lose this payload, it's actually equivalent to losing your funds, right? Because the chain doesn't have it has perfect privacy. There's no way to know what all of these state commitments are. And so if you don't know, like, hey, what was the note that recorded my funds that I control? There's no way that you could possibly spend it. And if we look back over the last, like, ten years of doing blockchains, managing state managing keys is very, very difficult. Even if you have fully deterministic derivation of all the key.
00:15:17.026 - 00:16:11.918, Speaker A: Seed phrases are basically a technology that was invented to solve the key loss problem. Like, let's just figure out how we can derive everything from one secret that we can somehow get a handle on these payload data. It's equivalent to a key, but it's created dynamically. And so putting it on chain is not just, oh, this is a convenient way to schlep the data around, but it's also a kind of a security feature, right? If you want to move that off chain, you have to have a really, really robust story of how does a user ensure that they have backups of all their data. So this has led us to thinking about a sub project that we called Narsil. And the idea of Narsil is to try to create a personal roll up. So using the same diagram that I had before of, okay, well we have our base chain.
00:16:11.918 - 00:17:19.194, Speaker A: It's just mostly going to be focused on doing certification and we're moving the compute out to the edges. Well, we can have this fractal perspective where those edges, maybe some of them are a web extension that's syncing your client state. Maybe there's an app on your phone or maybe one of those edges is actually its own chain. Right. So we started actually thinking about this because we wanted to have a story about how to do Threshold custody where multiple keyshard holders can collaborate to produce a signature on a particular transaction. This way you can do like multi SIG transaction authorization flows. But when you start thinking about that, you get into questions like, okay, well if we're not using the chain to coordinate, how do the signers who are participating know what all the signing requests are? How would we make those signers have a consistent view of what transactions have been requested? And we're already using tendermint comment BFT, so sounds great, we know how to do that.
00:17:19.194 - 00:18:51.842, Speaker A: Why don't we just have the shards communicate with their own con BFD network? They can run it in proof of authority mode. And what that means is that you actually not only get strong consistency of like, hey, what are these people signing between the custodians, but it also means that every custodian has a fault tolerantly replicated audit log of everything that has ever been signed by this Threshold key. And that's exactly what you need in order to be able to have assurance that I can safely move my user state off chain. I can skip posting those encrypted payloads and keep that only for myself. And I know that I'm going to be relatively secure and resilient because I'm already replicating that across my own internal network. Right, so you could imagine say, like a market maker who is going to be updating quotes on chain reasonably frequently, right? Why should every user have to be scanning that market maker's state updates? If we could have that market maker just run their own personal roll up, replicate their state internally and get less gas fees so that hopefully sounds interesting. The plug is if you want to play with any of the stuff that we're building, here are a bunch of links.
00:18:51.842 - 00:19:06.390, Speaker A: Everything we do is totally in the open. We have testnets, you can play with them. We have a discord. If anybody has any questions or wants to talk about it with us at any point, just like show up and we're always happy to chat.
