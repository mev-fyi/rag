00:00:07.930 - 00:00:28.840, Speaker A: And Eric, thank you for joining us. You know, you and I have chatted, you know, in the past before, and I know you have some really strong convictions about the space. Let's go into some introductions. I'll introduce the Celestia leadership team first. So each of you guys just give like, a quick of who you are and what you do at Celestia. Mustafa will go with you, and then we'll go to the others.
00:00:31.930 - 00:00:35.750, Speaker B: Hello, everyone. I'm the CEO, co founder of Celestia Labs.
00:00:36.570 - 00:00:37.074, Speaker A: Sweet.
00:00:37.122 - 00:00:37.720, Speaker B: John.
00:00:38.990 - 00:00:44.410, Speaker C: Hello, everyone. I'm john adler, the chief research officer and also co founder of celestial labs.
00:00:45.390 - 00:00:45.754, Speaker B: Great.
00:00:45.792 - 00:00:56.426, Speaker D: And then nick, I'm COO of Celestial Labs. And then we were supposed to have Ismail, who's the CTO, but I think they'll be joining shortly.
00:00:56.618 - 00:01:08.820, Speaker A: Cool. And then let's kick the ball over to Eric. Eric, not only introduce yourself, but tell us a little bit more about your core focus. Now, a lot of folks may not know who you are. Would love a quick Eric intro.
00:01:10.470 - 00:02:12.950, Speaker E: Okay, sure. So I'm Eric Wall. I'm the chief investment officer of Arcane Assets, which is a Scandinavian cryptocurrency investment firm. Actually, my intro into knowing about Celestia starts with knowing John. So I think John sort of popped up on my radar in was it 2018 or 2019? And I think I remember very clearly that John sort of popped up as this character that also was very interested in heavily criticizing various cryptocurrency projects. So my mission at that time, like the thing that I was focused on the most was trying to identify the absolute weakest link, in particular blockchain stacks. So we'd look at things like trying to identify where is the point of centralization in the system and where's the scalability bottlenecks in that system.
00:02:12.950 - 00:03:27.654, Speaker E: And John sort of popped up in this context and also shared that interest. And I think that was around the time where I was known as the altcoins layer was back in 2018, 2019, perhaps a bit into 2020. And I remember clearly that Vitalik, he tweeted out John's name and said, if you want to look at what a good ethereum critic looks like, you should look at this guy. And he linked to John's account. And after that, me and John, we've sort of been sharing notes on identifying we looked at, for example, like hedera hashgraph their claims about their scalability and then comparing that to what the actual reality of the situation is. And I suppose the big difference here between me and John is that John then went out to say, okay, I'm going to go and solve those problems. So John took the path of actually not only identifying the problems, but also trying to come up with what is the most logical, design wise solution to those issues as well, whereas I've just remained as this unnecessary, non productive complainer that just criticizes, I suppose.
00:03:27.654 - 00:04:24.500, Speaker E: I mean, now that we're here, I suppose that john and the rest of the Celestia team recognizes that even though I sympathize with a lot of the decisions that Celestia has made, that doesn't preclude me know? I'm going. To do the exact same thing that I do against any cryptocurrency protocol, which is I'm going to try to identify the worst thing about Celestia and then just stick on that thing and try to squeeze out as much information about how the celestial thinks about that. So I'm going in here with the gloves off. And that comes from, I think people sort of had to understand that now that I've explicitly said that I like the celestial project, it just means that I have to criticize it twice as hard in order to keep the credibility in this situation.
00:04:25.510 - 00:04:35.060, Speaker A: Thanks, Eric. And a lot of folks don't actually know what Celestia is and all that. Mustafa, can you give a quick introduction of Celestia and the.
00:04:37.750 - 00:05:28.550, Speaker B: So like pldr is, Celestia is a very basic layer one that does the core components that layer one should do and does it very well and scalably and nothing else. And that's basically consensus and data availability. So you can think of Celestia as a blockchain where you can basically dump data on it and it orders that data and makes it available and it does it very scalably using a primitive calls, data availability, sampling. And this is effectively within the modular blockchain philosophy where Celestia sits at the bottom of that stack. And then you have execution layers that can be based on roll ups on top of Celestia.
00:05:31.610 - 00:05:37.558, Speaker A: Thank you, Mustafa. And it looks like Ismail just joined. Ismail, what happened? And then give us an introduction as.
00:05:37.564 - 00:05:38.598, Speaker E: To who you are.
00:05:38.764 - 00:06:03.600, Speaker F: I'm the CTO of Celestia Labs and I'm apparently not able to join the first space as a speaker because I joined on the desktop. That happened. I was in the Twitter spaces since the moment you started it, apparently. John just wrote me that it gives you a pop up that it doesn't work on Web, but I didn't see that.
00:06:05.490 - 00:06:09.170, Speaker A: All good man. Glad you made it and let's get into it.
00:06:09.240 - 00:06:09.618, Speaker E: Yes.
00:06:09.704 - 00:06:57.554, Speaker A: So I'm going to frame the conversation and then we're straight up going into a debate and our strongest opinions. So when I look at the L One landscape, I see a lot of noise and a lot of that noise conflicts. I've gotten to the point where I don't know what to believe and not to believe. So I wanted to host this conversation to understand some of the myths and the truths going around. And from what I can tell, there's a lot more myths and misconceptions in the L One Space than there are truths. So in the next 45, 50 minutes, we will cover the major trends and forces at play that are shaping the L One Space and then we're going to talk about where everything is going. So we'll constantly be going back and forth between misconceptions and truth and then ultimately the end game.
00:06:57.554 - 00:07:31.340, Speaker A: We'll talk congestion, fees, induced demand, bottlenecks scalability, all sorts of things. Now, speaking of truths, I think it's famously said and written that the only truth and constant in life is change. And we'll begin there. And the question that I'll open up for the group is what is the single biggest change that has happened in the last twelve months in the L One space? And then what's going to be the single biggest change that will happen in the next twelve months? Eric, why don't we begin with you? What's your point of view?
00:07:31.730 - 00:08:42.894, Speaker E: Well, I think it's quite obvious and this is not a change. This is just like an outcome that was already bound to happen from the start. So I think that a lot of people in the cryptocurrency space, they noticed, of course, fees going rampant on Ethereum and then that has led to general mistrust towards the Ethereum project, whether or not it's the ideal solution to facilitate DeFi. And then we've had all these other layer ones pop up like avalanche and Solana, and many of them are based on the same premise that if you just increase the throughput capabilities of these systems, then fees will go down. So what we can already see is that fees are going up on avalanche. And I think that the general principle here is that the more that block space becomes valuable, the more demand will be created for that block space. So it's not necessarily the case that the reason that the fees are high in Ethereum is because the block space is limited.
00:08:42.894 - 00:09:49.490, Speaker E: It has more to do with every single block in there. There's a valuable transaction that you can make. You can claim an NFT AirDrop, you can liquidate someone's position and there's a dollar value into making those transactions that is worth spending in form of gas fees. And if you increase the throughput so that you'll have just bigger blocks in Ethereum, then you might decrease fees in the short term, but it's also opening up the door for more protocols to launch on Ethereum and create more Airdrops and more liquidations that you can trigger. So the conflict that is here is whether just increasing the throughput actually decreases fees. And I think that the Solana system, they have a different way of managing congestion where they don't actually have a mempool. So you won't see the same type of gas biding in the Solana mempool because they just drop the transactions if they don't get in in a timely manner.
00:09:49.490 - 00:10:55.118, Speaker E: But what happens instead then is that you can't actually use the Solana system because they're dropping the transactions and then they have outages instead of fee spikes. So they have liveness issues in two different ways. Either the fees spike on platforms like Ethereum or Avalanche, or you have outright outages. So you don't necessarily see those fee spikes. But what we can sort of understand from this is that no layer one that just focuses on scalability or just throughput at the base layer is actually meaningfully addressing a long term sustainable solution to the fee problem. And I think that this year, very recently, I think that the tone in the space and the interest is because we have seen that these layer ones don't necessarily address the problem in a sustainable like are there other ways to address this problem? And I think that Celestia is sitting at the core of that. So even though Celestia is not live yet, it doesn't exist yet.
00:10:55.118 - 00:11:39.180, Speaker E: There's so much of the intellectual of the intellectual debate that is happening around the protocol because I think that everyone that has been in the cryptocurrency space for a long time is sort of understanding that at the root, at the basis of all of these scalability issues, there is one thing that sits at the focus, and that is data availability. So if you want to scale some parameter of these systems then building a scalable data availability layer is where you should start. Now what I'm not so sure about is that is that a solution that does that solve all the problems or is that just creating the optimal solution that also fails to solve the problem?
00:11:41.710 - 00:11:45.420, Speaker A: Thank you. Eric Mustafa, we'll go with you next. What's your.
00:11:47.950 - 00:13:19.980, Speaker B: I mean in terms of the question what has changed in layer one space? So I would say if you looked there was a period back in 2017, 2018 when there was a lot of new layer one projects that kind of raised big rounds like Avalanche, Lana, Neoprotocol and so on and so forth. And a lot of them are kind of based around a similar premise, which is ethereum but more scalable or a more scalable shared smart contract platform in the sense that you have a synchronous execution environment where all transactions kind of run within this World computer model that Ethereum has kind of proposed in 2014 where there's this world Internet computer where all transactions are processed. But now we've kind of seen the results of that play out. We can see 2022 that does not really work. You just cannot have a single synchronous world computer chain as a shared smart contract platform. You've seen issues with Avalanche and Solana and binance chain all struggling to cope with demand even though they claim to be more scalable than Ethereum. So I think the biggest shift we're seeing now is this idea of a multi chain world.
00:13:19.980 - 00:14:25.150, Speaker B: It's no longer a paradigm where the single blockchain that can synchronously process all transactions. Now people are realizing that actually you have to have a multi chain world and that's similar to the that's effectively the Cosmos vision that was kind of introduced a few years ago. And Ethereum is also taking this approach because roll ups themselves are basically an implementation of a multi chain world because roll ups themselves are chains. So effectively, Ethereum is heading towards this direction as well, where all the execution happens on roll up chains that connect to the Ethereum main chain as a kind of hub. So I would say that's kind of the biggest shift, like this shift between a single world computer model to a many computer model where you have multi chain world where the chains communicate with each other. But obviously there is no perfect solution. All scalability solutions have trade offs.
00:14:25.150 - 00:15:27.220, Speaker B: And the biggest trade off of this multi chain ecosystem paradigm where everything is a roll up and all the execution happens off chain is this kind of issue of composability. The great thing about this world computer model where just run all your transactions on Solana is that you have synchronous composability and you can just write a smart contract that calls any other smart contract. But with this multi chain model, even though it's theoretically more scalable, the main drawback of it is that it makes Composability more complicated. Not impossible or not too difficult, but more complicated and more tooling is required. For example, IBC was only launched somewhat recently and only people are realizing that. For those of you that don't know, IBC is Cosmos's interchange protocol and we're only kind of starting to see that flesh out and see the potential of that.
00:15:29.670 - 00:15:43.980, Speaker A: John, let's get your point of view again. The question is, what is the single biggest change that's happened in the L one space for the last twelve months? And then what do you anticipate the next big change to be in the next twelve months? Go for it, John.
00:15:45.950 - 00:17:21.500, Speaker C: Sure. So one big change I saw in the layer, one space, or just the blockchain space in general over the past year has kind of been an acceptance of the modular blockchain vision which for those well, I mean, if you're listening to this, you should probably at least have heard of it. But for those who are not familiar, it's essentially the separation of data availability on the consensus layer and then doing execution at the higher layers. So kind of the acceptance that this is the path forward to get the best blockchain stack you can get. Best doesn't necessarily mean like it's the most scalable in all respects necessarily, but that it's the best cross section of properties. I expect in the next year, not twelve months starting from now, but next year or so to see a large proliferation of execution layers. Because as we get scalable data layers such as celestial launching, we need something to execute transactions on and the something is going to look like a multi chain world like what Mustafa said, the original Cosmos vision of multiple, not just multiple, but many, many execution layers that communicate with each other.
00:17:21.500 - 00:18:31.070, Speaker C: Now that shared data availabilities, layers like Celestia will exist shortly. This means that these execution layers can actually share security, which is an important property they were missing before Celestia came on the scene. So you kind of get the best of both worlds where you get the scalability benefits of Sharding without all the complexities and shortcomings and open research problems of Sharding, which is a large part of why various projects that were initially pursuing Sharding eventually gave up because there's various open research problems. So I expect to see in the subsequent twelve months wide proliferation of execution layers that do different things, that try different things, because now we can experiment with a variety of execution systems without having to worry about bootstrapping your own validator set, without having to worry about bootstrapping your own consensus network. You can simply deploy it as a roll up on top of Celestia and benefit from shared security, but all other rollups also benefit from additively.
00:18:33.090 - 00:18:34.980, Speaker A: Ismal give us your take.
00:18:36.550 - 00:19:26.000, Speaker F: Yeah, I think I'm pretty much aligned with what Mustafa and John said here. There isn't much to add, I guess, especially coming from the Cosmos ecosystem. For me it's like particularly powerful to see a vision coming together where you have application specific chains, but instead, as John said previously, instead of them having to bootstrap their own proof of stake BFT consensus networks, they can run as roll ups on top of a data availability layer, on top of Celestia. And yeah, for me that's going to be huge. And I think other than that, I guess everything that John and Mustafa said is just what I can align with.
00:19:27.090 - 00:19:42.470, Speaker A: Okay, yeah, Small does have a Cosmos background and it's an interesting perspective. Nick, on the other hand, is coming from Harmony. Nick, same question to you. Macro changes, last twelve months, next twelve months, what's your opinion?
00:19:44.250 - 00:20:50.810, Speaker D: I agree a lot with Eric. That Harmony and I think the whole sort of cohort of scaling solutions that were built from 2017 and 2018 until now, I think we've kind of seen in the last twelve months that they do help alleviate the demand for block space, but ultimately they don't seem to be a viable solution long term. And I think that's also kind of led to people realizing why Cosmos, for example, has had the MultiChain vision. You kind of need separate execution spaces, I think, to have a scalable sort of like blockchain infrastructure. And so I think the next twelve months are going to be sort of like the maturation and development of this modular blockchain infrastructure. I think, to be fair, it's still very early, right? So Celestia is quite far from launching main net. We have a bunch of roll ups, some of them are already live, most of them are still sort of being built.
00:20:50.810 - 00:21:19.320, Speaker D: And I think in the next twelve months we're going to see this ecosystem emerge and mature and I think a lot of people are going to start to realize that. I guess this combination of a scalable data layer with sovereign execution spaces as roll ups is at least so far, the most compelling solution to scaling and a lot of other problems that we know interoperability included in that.
00:21:20.970 - 00:21:52.990, Speaker A: So Eric and I were chatting, I think, last week, and this question came up in our conversation can Celestia address the fee problem? And it really caught my attention and ultimately helped trigger this conversation. I want to go deeper on this topic of fees and congestion. Eric, go a bit deeper. Share some of your strongest opinions on some of the misconceptions around fees and congestion and the underlying truths at play that many aren't seeing. Possibly.
00:21:53.410 - 00:22:48.438, Speaker E: Sure. I think that we failed to clarify one thing when we were talking about this previously. So we mentioned the Cosmos system and how they're doing parallel application specific blockchains. And I think we should need to clarify a little bit more how Celestia ties into that and sort of what the vision is. And I think one very interesting thing here was I was talking on Twitter about two, three months ago broadly about what's going on in the blockchain space, and Zaki Manion commented underneath the tweet and he said, didn't everyone just adopt my thesis from five years ago? And I thought that was kind of a stretch. So I responded to Zaki and I said that Cosmos felt thin until Celestia came along. And so Zaki responded that Celestia was the original idea that Jay and Ethan pitched to him and that they call it super tanker.
00:22:48.438 - 00:23:29.630, Speaker E: And then Zaki said that it wouldn't work. And then Mustafa came along and invented data availability sampling. And now you all of a sudden have exactly what Cosmos was gunning for apparently five years ago. And I think what people need to understand is that when you have in the Cosmos ecosystem, you have these zones. Each zone is a blockchain and each blockchain has its own security parameters and validator set. The problem is that if all of these are communicating with each other using IBC, if one of these chains are reorganed, then that causes problems for the other one. Basically, you can rob the bridge in these systems if you break one blockchain but not the other one.
00:23:29.630 - 00:24:54.426, Speaker E: So what you sort of need in order to build a shared security for these application specific blockchains, you need a shared security. And that is because Celestia is a data availability layer, but it's also a consensus layer. It's also the thing that can order the data that has been seen in the system into a chronological order. It means that all of these there can't be reworks in one of these application specific chains if they use Celestia as the data availability layer, because Celestia is creating this red thread for all of the zones, which means that they can get shared security. So I think that that's very important to sort of understand how these things are coming together and why Sebastian is sitting at the focus of this conversation. But now to talk mean the reason why the Cosmos project has been interesting is because the nice thing that you get from these asynchronous execution environments like these application specific blockchains, is that the fees can be very high in one of these apps, but it doesn't mean that the fees are going to be high in the other app. So if you have one blockchain specific application specific blockchain that does something like uniswap and you have another blockchain that just does payments, people are fine with paying high fees on a decentralized exchange because they're buying a token and they're hoping for the value to go up like thousands of dollars into the future.
00:24:54.426 - 00:26:43.998, Speaker E: You're fine with spending like, say, five or $20 to enter that position. But if you're making a micro payment, like a microtransaction in another chain, if you're doing it in another chain, of course you want the fees to be low in that other system. And the problem that these systems like avalanche, no avalanche has subnets, but generally speaking, monolithic blockchains that have a single execution environment in these systems, you don't want the fees of one application to drive up the fees in other applications because that's how things become unscalable. So what I want to discuss with Celestia here and what my sort of dilemma or puzzle is, if there is let's say there is an application specific, like an app that runs on Celestia through an execution environment, through a smart contract, it uses Celestia as a data availability layer. If there's tons of demand for that specific application, then that creates block space demand on the Celestia based layer. And this impacts the entirety of the Celestia system because while Celestia has tons of block space, it is not infinite. So if we get one very popular application that uses Celestia, what I'm trying to understand is whether that leaks into all other Celestia using applications such that if you have a micro payments application that also uses Celestia as the data availability layer, is that fee pressure going to leak into the other applications? I think that the obvious answer sort of is yes.
00:26:43.998 - 00:27:05.490, Speaker E: Right? But we don't really know how bad can that get? Like, what if there is one application that is so popular that it drives the fees up on Celestia to $5 per transaction? Is this a catastrophic scenario for Celestia or is that still viewed as a positive outcome?
00:27:06.630 - 00:27:08.100, Speaker A: Mustafa, please.
00:27:09.830 - 00:28:45.810, Speaker B: Yeah, I'll respond to that, but I also want john should also I'm sure John also has lots of opinions about this. So the short answer is yes. First of all, no blockchain can guarantee cheap fees. If any blockchain guaranteed cheap fees, then it would be vulnerable to denial of service attacks because someone could just spam it with free transactions, which includes it in Celestia and every other blockchain that is not vulnerable to null service attacks, there is a block size limit, and in Celestia there will be a block size limit. Now, the question is what is that block size limit? There's several questions here. So first of all, what is that block size limit and how do we pick that block size limit and how does it scale? And secondly, what is the maximum potential size for that block size limit compared to a monolithic blockchain like Solana, for example. First of all, so one thing I've said before is we define scalability as equal to the number of transactions or the throughput that blockchain can handle divided by the cost for an end user or a light client to validate the correctness of that chain.
00:28:45.810 - 00:30:20.498, Speaker B: And this is the fundamental reason why, for example Ethereum or Bitcoin would not just increase the block size to something insanely high because end users would not be able to run a full node to check that the chain is correct. Now, this is kind of contrary to solana for example, where solana have taken the approach where they don't care about end user verification and they just want to scale throughput as high as possible on the monolithic chain. So for the bottom part of the equation, which is the cost for end user to verify the chain, the property in Celestia is such that because we use a primitive called data availability sampling the more users there are in the chain the more like science there are on the chain doing data availability sampling. Then the higher the block size we can increase it to safely without compromising the ability for end users to verify the correctness of that chain or in other words, to check that the chain data is fully available. And that's because with data availability sampling any Glyte client can verify that the chain is fully available by only downloading a very small piece of that chain. So that's one side of the equation which is that we can scale with number of users or any blockchain including when Ethereum 2.0 adopts it.
00:30:20.498 - 00:32:14.870, Speaker B: Any blockchain that supports states availability sampling can scale end user verification with number of users. But then the next question is scaling. End user verification is all well and good, but how about actually increasing the throughput of the chain or the block production of the chain? Because that's the other end of the equation, which is how big of the blocks can the actual miners or the validators or the stakers in the chain actually scalably produce? So in Celestia I would argue that would be fundamentally higher than other systems like Solana. And the core reason for that is because the validators on the solicitor chain they don't do execution of the user's transaction, they just do data availability which is a very bandwidth heavy resource requirement but not computation or kind of memory heavy requirement. And the fundamental difference here is that including just providing data availability is a stateless operation which means that you don't have to remember any user state. So like for example in Salana you have to have a huge amount of memory, I think it was like 256GB of memory to run a validator because you have to remember a huge amount of state, for example, user balances to actually validate blocks. But in celestial, because we decouple data availability and execution the actual validators of the chain, they don't have to maintain state, they just have to have the bandwidth to receive transactions and put them in blocks.
00:32:14.870 - 00:32:29.530, Speaker B: And that fundamentally allows you to increase block production to much higher than if the validators also have to do execution and computational transactions.
00:32:31.890 - 00:32:33.120, Speaker A: John, please.
00:32:34.690 - 00:32:35.054, Speaker B: Yeah.
00:32:35.092 - 00:33:59.974, Speaker C: So what Mustafa said is kind of one of the key distinguishing factors that makes the modular blockchain architecture so compelling that for the first time ever we can have this decentralized network that can actually grow its capacity as the number of users grows. The monolithic blockchains that you see in like Ethereum and Solana and Bitcoin for example, these all as you add more nodes, as you add more users, you don't actually get more capacity. Now, one subtle point here is that the specific capacity that's growing as you add more users is the capacity of data availability. It's the throughput in bytes per second, not in transactions per second. And this is actually like a litmus test. If you want to see is a blockchain modular or is it monolithic? If it advertises transactions per second then it is not modular, it is monolithic or it's an execution layer as part of the modular blockchain stack, the base layer of the modular blockchain stack, the data layer does not support transactions per second, it supports bytes per second. Now, unfortunately, these bytes aren't actually interpreted by the data layer.
00:33:59.974 - 00:35:20.430, Speaker C: So even though you get more capacity in just data throughput as you add more nodes, that does not imply that you get more capacity in execution. The execution is still bottlenecked by whatever parameter decentralization you set and by the particular choice and implementation of virtual machine and transaction format and so on. So if you put Ethereum's EVM into a roll up, that does not imply magically you can get more TPS out of it. If you want to get more TPS out of that EVM in a roll up, regardless of if you use Ethereum or Celestia as a data layer, you need to sacrifice something fundamental, you need to sacrifice some security guarantees or you sacrifice decentralization, which is all not necessarily good. It could be some trade off that some users are willing to accept. But ideally we would like to have the option of a highly decentralized and highly secure the same decentralization and security that you get today blockchain or several blockchains and have that also be scalable. And you can do things like have a sharding without sharding, multiple blockchains running in parallel, multiple roll ups running in parallel.
00:35:20.430 - 00:35:56.666, Speaker C: Or you could have one roll up that is more efficiently engineered, that can provide composable, like a higher throughput of composable. Execution over this scalable data layer. So the TLDR of this is that having a scalable data layer is very important as the base. If you don't have this, then if you have a monolithic base layer that cannot grow as you add more users, you're going to be heavily restricted in the total throughput of the system. But that's only half the puzzle. You also have to look at the other half. You have to look at the execution layer.
00:35:56.666 - 00:36:28.630, Speaker C: And for that, this is why I said in my answer previously, I expect a proliferation of a large number of execution layers that all experiment with different execution models because we would like to have something that has the composability of things like ethereum and solana, but that also is also verifiable. It's an execution layer. It can have fraud or validity proofs and act as a roll up so it can share security with other execution layers on a shared security based layer like Celestia.
00:36:29.610 - 00:36:33.530, Speaker A: Eric, we'll let you react to what Mustafa and John just said.
00:36:33.680 - 00:37:26.070, Speaker E: Okay, good. Because there's one thing that I really want to bring up here, and it's something that's been bugging me for a while, and it's good that I have all the four of you here to sort of address this question. So John just said that in Celestia, capacity increases with the number of users. And this is something that I think I've heard Nick also say in the past, like he said that the more users you have, the more they sample and the more the security they give to the network. So I suppose that the way that the Celestia team is thinking about it is that if you have more users and they're doing data availability sampling on Celestia, which is the thing that you got to sample that data to know that it's there. And the more users you have, the more sampling you do. Which means that the overall data set can be larger because you now have more samplers.
00:37:26.070 - 00:38:47.694, Speaker E: So there's two things that I would really want to get help with understanding. The first thing is, I think it's an easy one. I just want to understand, okay, so if I'm a user and other users are sampling, how does my node get the information or verification that the other nodes actually have sampled the data? Are there small proofs that they deliver? Or can you just go into some detail into how that sampling goes? And then the other thing then this is the most important question I think maybe that I have in this conversation. Is that okay, yes. With more users you sample more of the data, the data can be larger. But John, you have been the person that has always been saying that unless the security assurances are constant as you increase throughput, then you're not actually scaling. So in order for a system to scale, you have to scale while maintaining the same level of security and now my issue now becomes that okay, yes, by using data availability sampling you can prove data availability, but you don't actually prove retrievability in Sebastia.
00:38:47.694 - 00:39:37.950, Speaker E: In order for the system to be secure, you actually also need to retrieve the data. So just knowing that the data was published at some point doesn't actually give you the security that you can create a fraud proof or create a validity proof and get your money out of the system. You need that data to be there. So isn't the only real full node security still based on that? You have to have all that data yourself on your node. Otherwise there are other nodes, you know that they have sampled the data. But what if they all turn against you at one point? And you mentioned that in Cosmos, in the tendermint consensus algorithm, there's 100 nodes maximum like 102 hundred nodes. Are we trusting these 102 hundred nodes to maintain that data? And if they go rogue, then the end user is fucked.
00:39:37.950 - 00:40:02.320, Speaker E: And the more data that is getting pushed into the system, then that still puts a bottleneck in terms of how many of these nodes are actually storing all the data. Because it's one thing to have the proof that the data was sampled, but it's another thing to actually know for sure that when you come to a point in time where you need to get that data out of the system, it is actually going to happen.
00:40:03.170 - 00:40:05.470, Speaker A: So I see the team who would like to respond.
00:40:06.770 - 00:41:55.330, Speaker B: I guess I can start. So maybe it'll be helpful first if I kind of give a quick overview of the properties of data availability sampling and how it kind of works in general. So the general idea of the problem that data availability sampling is trying to solve is as a user, how can I make sure that let's say some 1 MB block was actually published to the network by that block producer without me having to actually manually download that entire block myself and check that it was actually published. So this is basically preventing what's called the block withholding attack where a block producer or a miner or a validator might publish a block header, but if they don't actually publish the transactions that that block header points to, then people might not know what was actually inside that block. And so if there was an invalid transaction in that block, then they would not be able to detect that. So with data availability sampling, it basically allows a user to download a very small piece of that block and get a very high probability assurance that 100% of the block is available. And the way it roughly works is I'm not going to go too much into the weeds, but the way it roughly works is the block producer splits the block into hundreds of chunks and applies something called an erasure code on top of those chunks.
00:41:55.330 - 00:43:42.782, Speaker B: Now the end result without understanding how erasure codes work. But the end result is that as a light client, I know once you apply the erasure code, the block producer has to if they want to hide even a single byte or even a single transaction in that block, it's not sufficient just to withhold that specific transaction or byte. They would have to withhold half of the entire block because if any half of that block is available, then the entire block can be reconstructured with the available half. And so this allows light clients to kind of have this sampling primitive, where if a light client or a user samples, let's say, 30 pieces or chunks from a block, from a block, then they can be 99.99% sure that 100% of the block is available. Because if the block producer actually did this attack and they withheld half of the data and the user randomly sampled or randomly checked 30 different pieces in that block, then there's very high probability that the user would have landed on an unavailable piece or withheld piece and therefore they would know the block is unavailable. Now, this kind of scheme only works if there's enough users in the entire network such that they're all making enough samples collectively so that they can actually reconstruct or recover the entire block.
00:43:42.782 - 00:44:16.266, Speaker B: So if you have a block with 1000 chunks, it's not enough just to have one user requesting data samples. You need a bunch of users such that they all requested at least 500 samples. So you can kind of think of it like BitTorrent. If you're familiar with BitTorrent, you have peers and seeds in network and those peers might have different pieces of that file and they can all share it with each other and reconstruct it. And that's kind of the fundamental reason why do you want to interrupt?
00:44:16.458 - 00:44:38.046, Speaker E: Yeah, I was wondering, how do I, as a user doing the sampling, know that the other users are doing it? And if they because the other users, they're not involved in the consensus. They're also like sampling. How do I know, how do I get the assurance from them that their sampling is also benefiting me? How does that compound?
00:44:38.238 - 00:45:13.022, Speaker B: Like you don't know because you assume that there are and the reason why that's fine is because usually the number of light clients needed are like in the few hundreds. So if you have a 1 MB block, depending on the parameters of the construction, you wouldn't need more than a few hundred light clients for the assumption to hold true. And so if you assume that the network has sufficient number of users, or if you know that, then you would assume that there's enough people running nodes in the network such that this assumption holds true.
00:45:13.156 - 00:45:34.866, Speaker E: But that sort of assumes that if there's one node that is sampling and doesn't get the data, he's able to prove that he's not getting the data. So is that something like is there a one out of N assumption here where the one person whose sampling fails, he can flag it to the other nodes and they can try the same sample and see if they get a response.
00:45:35.058 - 00:46:35.530, Speaker B: So this scheme does not involve any kind of flagging or there's no proofs of unavailability or red alerts or flags. The scheme works without that because if the user assumes that there's enough users in the network running the same schema as them and doing the same sampling as them, then that's all they need to know that data is actually available. The reason for that is because if there's enough users in the network to request enough samples to force the Block Producer's hands such that in order for that Block Producer for that Block Producer to have to meet all of the sample requests, the Block Producer would have to release just enough of a block such that the entire block can be reconstructed.
00:46:40.270 - 00:46:43.166, Speaker C: Can I also add something, please?
00:46:43.348 - 00:46:44.080, Speaker B: Yeah.
00:46:44.610 - 00:47:54.290, Speaker C: So one thing you might be thinking of is that there's no deterministic, completely reproducible way for you to know for certain how many honest nodes are performing this data availability sampling. Like Mustava said, if there's a few hundred or whatever, then you're safe. But there's no concrete way of guaranteeing that that is true. This is one of the reasons that the kind of social consensus around Celestia is ultimately to use social coordination and off chain governance in order to control parameters like the block size limit. Because there's no way on chain of determining not just on chain, but just programmatically in general, of determining how many honest nodes there are period in the network. This is more something that the community can get together collectively and decide on these parameters implicitly deciding on how many honest nodes the community thinks are participating.
00:47:54.870 - 00:48:24.940, Speaker B: Yeah, and I should add, this is also the same as any other blockchain ecosystem. So it's like how is the maximum block size limit in Bitcoin or Ethereum decided or even polkadot effectively, it's a social contract between the participants of those networks such that they have a certain node resource requirements and they upgrade their chains and set the block size limit based on that.
00:48:26.930 - 00:49:28.340, Speaker E: I would argue that it's still fundamentally different though, because in Bitcoin I don't need to rely on anybody else's account that the data is there. Because with the Bitcoin client I download all of the data so I have it on my own computer, whereas in the celestial system I don't download all of those petabytes, I'm just sampling them. And if the data grows and your argument here is that even if the data grows, as long as the users also grow and they're also sampling, then I don't have to worry about the data growing, even though I'm only sampling the same number of chunks. So what I'm really trying to get at is that if I'm always just sampling the same number of chunks and the data grows and grows and grows, then I have less of the certainty that the data is there. And it sounds like there's this social element to understanding that there are other samplers around me, but that's not like a technical quality. So what I'm trying to get at can we really say that the security scales I mean, it scales maybe in a social sense, but does it actually scale in a technical sense?
00:49:29.110 - 00:49:29.860, Speaker B: Yeah.
00:49:31.110 - 00:49:57.770, Speaker F: Can I also quickly add something? I think I guess the sampling itself is a proof that the data is available and you don't need to know that it's a local property and you don't really need to know that others are doing sampling as well. It's more that if the sampling succeeds, it's also proof that there's enough nodes providing you with the data.
00:49:57.840 - 00:49:58.266, Speaker B: Right.
00:49:58.368 - 00:50:24.066, Speaker F: So I guess there's some confusion also. It's not the sampling only, but if you sample and also serve that data, then you provide security to the network. The sampling itself is also for yourself. But if you want to provide to the network, like the security of the network, I guess you also have to reshare these samples, right? Maybe. Does that clarify a little bit?
00:50:24.248 - 00:50:59.200, Speaker E: Yeah, but I have some other questions along that we can move on, like if you don't want to spend too much time. But I have a few questions that we can perhaps try to address. So the other question that I have is that, okay, so when you sample a block, do you ever sample it again? Or is it just when that block arrives that you sample it and then you assume, okay, it has been published and then you never look at it again. So if this block was published like two years ago, do you ever go back and sample previous blocks and get the certainty that those blocks are still there? Because your security can still depend on those blocks, right?
00:50:59.650 - 00:51:00.062, Speaker B: Yeah.
00:51:00.116 - 00:51:07.602, Speaker F: So nodes that come online will at least, like light nodes will at least.
00:51:07.656 - 00:51:08.450, Speaker B: Sample.
00:51:10.630 - 00:51:23.960, Speaker F: From their checkpoint or from the trusted header they start from. But you could also start from Genesis and sample from there. Right, even if it's like two or three or five years ago.
00:51:24.490 - 00:52:49.614, Speaker B: But I think if I understand correctly, eric's question is more about do we also guarantee the ongoing availability of the block? Is that is a very common question. The question of data retrievability, people guarantee so Celestia does not guarantee data retrievability. What Celestia guarantees is that the data was published at some point in time. The assumption that we make for data retrievability is basically the same as ethereum full node, which is that once the data has been published and if you can verify that the data was published, then from that point on there's an honest minority assumption for the ongoing retrievability of that data. Because effectively, if you can check that data was actually published Internet, then for the ongoing retrievability you just need to assume that there's at least one honest server on the internet that's willing to host and serve that data or there's at least one available data on the internet somewhere. Kind of like the streisand effects in a sense. And this is why also there's a block size limit.
00:52:49.614 - 00:53:31.070, Speaker B: Obviously that would not work if you have tens of terabytes of data because people would not be able to host that. But if you have a block size limit that scales the number of users, then you're more lucky to guarantee that assumption is more likely to hold true. In particular because the way that we want to design the system is that you can actually contribute to the storage of network without storing the entire blockchain. So we have this concept of partial storage nodes that can store parts of the blockchain and contribute to part of the storage of the blockchain. Kind of like kind of like a bit torrent files with different peers that hold different parts of the file.
00:53:31.970 - 00:54:53.642, Speaker E: Okay, so my nitpicky summary of the situation here would be that I would say that yes, if all of the data in the Celestia is 1 TB, then I would assume that this data is still retrievable. And retrievability in my opinion, is the security requirement, not just availability. So if it's just 1 would assume this data is probably there, but if it's a million terabytes, then I would assume that the actual nodes in reality actually storing all of that data is much smaller. So meanwhile, it is commonly said that as users grow you'll have more sampling and the security scales with the number of users. I think that you cannot escape this harsh reality that as data grows there will be perhaps fewer that actually stores the data and the certainty that you can retrieve it shrinks. So I think attaching when you're saying that security scales with the number of users, you're really thinking about the data availability sampling scales with the number of users. But security is retrievability and retrievability is impacted by the actual size of the data.
00:54:53.642 - 00:55:28.920, Speaker E: So that's just like a nitpicky summary of the situation. I still understand that the data availability sampling does give you favorable properties here. But if you're going to the very cold harsh truth of a situation then thousands of petabytes of data on Celestia, you're going to have to rely on that data still exists because you're not sampling it all the time and you're not getting proofs from other nodes that they have sampled it to your node all the time. At least not in the way that Sveste is designed at the moment. Do you think that that's a fair did I say anything that was incorrect there?
00:55:29.450 - 00:56:17.190, Speaker B: Well, I guess there's two things I would say to that. The first thing is what I've said is the status quo of how blockchains work. That's the same assumption that Ethereum full node makes. Obviously that assumption does work. No one complains about the assumption as you rightly say that's because we're talking about a few terabytes of data. But the question then becomes what if you have hundreds of terabytes of data? There's several things I would say to that. First of all, just because we not want to have on chain or in protocol incentives for data retrievability does not mean that there cannot be incentives for data retrievability.
00:56:17.190 - 00:57:06.510, Speaker B: It just means that the incentive layer for data retrievability and data availability is decoupled. For example, Ethereum technically does have ethereum. Today technically also has a data retrievability problem. That's why people pay infuria to get access to full nodes to get API queries from infuria to get data out of Ethereum. You can think of infuria as a centralized data retrievability layer for ethereum but obviously you can actually have kind of decentralized versions of that. For example, you've got Pocket Network which is more distributed and you have the graph which is retrievability for index data. And then you've also got data storage protocols.
00:57:06.510 - 00:58:02.200, Speaker B: For example, Filecoin has proof of retrievability. So the first thing I would say there's no inherent reason to have data retrievability incentives as core part of the protocol. The second part I would say to that is even if that's not good enough, it could be the case that the end user applications that people build on Celestia could make their own assumptions about data retrievability. Like for example, they could create data retrievability assumptions within their applications like Celestia guarantees data availability. But if those applications want higher data retrievability guarantees they could add data retrievability or proofs of reachability within their applications if they wanted to do so. Even though I don't think it's necessary, but they could.
00:58:02.730 - 00:58:26.560, Speaker A: So I'm going to move the group back to the L One topic. While I do that, we will be going into Q and A. So if anyone wants to ask a question, your chance will be soon. Eric, while we wait for Q and A, is there another major misconception in the l One space that you can point out that needs clarification or that folks need to talk about?
00:58:28.530 - 00:59:36.642, Speaker E: Really? I think it would be more interesting. I have more architectural questions about Celestia. I think one that would perhaps be more interesting to talk about, if you don't mind. I think one of the very interesting things when you're thinking about the modular blockchain stack is thinking about I mean, as investors and most people in the cryptocurrency space are investors, everyone is thinking about okay, where does value accrual happen in the system? And I don't think that we really in this conversation really gave a good clarification of the different layers in Celestia. And I think these terms are also being mixed and confused sometimes. So if we start by dissecting the stack into their appropriate layers and label them correctly and then talk about which tokens do what? I can just start by labeling and please interrupt me here and if you think I'm taking the conversation into a direction that you don't want to go. But from my perspective, at the base layer, you have consensus and data availability.
00:59:36.642 - 01:00:32.402, Speaker E: And those are actually in Celestia bundled together because the Celestia base layer does order blocks according to the tendermint consensus that it uses. And it provides data availability for that layer. And that's the data availability layer, but it's also the consensus layer. And those two terms are being thrown around a little bit confusingly, but I think that's really one layer. And then on top of that layer, you have the execution layer. And this is a system like Evmos that provides an EVM. This is an execution layer, but it's also a settlement layer, right? So the way that you're defining a settlement layer in Celestia is, for example, roll ups that need to if there's some inconsistency in a roll up at the top of the stack, then they need to be disputed in an execution environment.
01:00:32.402 - 01:01:27.682, Speaker E: Somewhere that can interpret these fraud proofs or validity proofs and sort of settle what the actual state of the execution environment is. So on top of the data availability and the consensus layer, you have the execution layer that is also a settlement layer. And I suppose that the execution layer also encompasses roll ups because a roll up is also an execution environment. So the execution environment has really two tiers to it, the settlement layer, but also the sort of roll up that all goes into the sort of execution layer. And then on top of that, you have applications like smart contracts that run inside a roll up. But the roll up itself also can have a token. So what I want to understand a little bit from how the celestial team is thinking about it, if you go from the top of the stack to all the way to the bottom of the stack.
01:01:27.682 - 01:02:27.210, Speaker E: Let's say I'm a user of Dy, DX, like the derivatives application that currently runs on Ethereum. If we're going through, like, from the top of the stack, is it there going to be a dYdX application token that runs inside of a roll up that has a roll up token that runs inside of Evmos, that has a settlement layer execution environment token that then uses Celestia as the data availability at the consensus layer that also has a token. So are there really like, four different tokens going through her here? And which ones of these are where do you think that do you see any value accrual at the Evmos layer or is the value accrual going to happen at the celestial layer? And also we need to discuss a little bit about where does fee congestion, like, where does congestion actually happen? Does it happen in the execution environment or does it happen at the celestial layer?
01:02:28.750 - 01:03:34.238, Speaker B: Yeah, so the way I see it is that, yes, there could very well be a token for every single layer. Like you might have a data availability token, you might have a settlement layer token and you might have an app token. But now that might seem like a UX nightmare, but in theory but in practice it's not. Because the way that it would work is that the end user only needs to be exposed to the top player token, which, by the way, might not even be the app token, but could be like a stable coin like USDC. Like the app itself could accept payments in USDC and convert them to the app token via a dex or something. Like then the way I see it is like a supply chain. So if you have like the operators of the app chain, they accept payments in their app token or they accept the app token for some service, but the app chain has to pay the settlement layer so they can exchange their app token for some settlement tokens and then pay the settlement layer.
01:03:34.238 - 01:04:12.010, Speaker B: And then settlement layer needs to pay the data availability layer, data availability layer token using the same process. But this is effectively like how any supply chain works, including except that the fact there's different tokens. But effectively in a normal supply chain you have a stack where the bottoms of the supply chain transfers funds to higher layers and it keeps going up the stack. Like for example, Twitter pays its Internet service provider which pays its hardware providers and so on and so forth.
01:04:14.370 - 01:04:19.200, Speaker A: We do have John from Delphi here. John, go ahead, ask a question.
01:04:21.330 - 01:05:18.178, Speaker G: Yeah, so thank you for giving me the chance to speak. So while we're talking about architecture here, there's one issue that I've been wondering actually. So I want to get Celestia team's opinion on this. Anyone feel free to step in. So if we're talking in terms of the base layer, the celestial layer, we know that there's data sampling light clients and these light clients can effectively know how big the target size of the celestial chain is. Then they know this by looking at their block header size. So they effectively have a discretion over how large the celestial block sizes are.
01:05:18.178 - 01:06:27.640, Speaker G: But what I'm wondering is for them. So in reality for them. So if we imagine these light clients are interested in a particular roll up on Celestia, for there to remain secure, they would have to assume that there is a full node in this roll up that would hand them the validity and fraud proofs. So what becomes a point of interest here is that or a question here is that these light clients, do they have any discretion over how large the roll up blocks can get or can they enforce this somehow? Because if we imagine a roll up, a giant roll up that does a lot of execution and that there's only a handful of full nodes that can actually execute them and hand over the fraud proofs, then we're looking at a centralized execution layer on top of Celestia. So I was just wondering if there's any way for data availability sampling light clients to ensure the block size of the roll ups that sit on top.
01:06:28.410 - 01:06:33.660, Speaker A: Thanks. We'll be taking more questions in the meantime, but Celestia team, go for it.
01:06:36.350 - 01:07:54.690, Speaker C: Sure, I guess I can go. So this ties into a Bit what I said earlier, which is that there's no way completely programmatically to know how many nodes are on the network. Therefore having something programmatically entirely on chain to decide a block size is probably not a good idea. And therefore this is why we use off chain governance similar to the most robust, decentralized blockchains like Bitcoin and Ethereum, because it allows end users to have sovereignty over the blockchain regardless of what the block proposers vote on, regardless of what even token holders vote on. End users running nodes are the ones that ultimately control the rules of the chain through governance. So a single node on its own acting as an island doesn't really have any control over the block size, but the community of node runners does, regardless of what the block producers or even token holders want. They can simply hard fork or soft fork or whatever to change the rules, so they can prevent block size increases that they don't want, or suggestions to increase the block size that they don't want or that they won't accept.
01:07:55.750 - 01:08:02.600, Speaker B: Or rather in the first place, that a hard fork may be necessary for block size increase. Yes.
01:08:04.170 - 01:08:05.640, Speaker C: Does that answer your question?
01:08:08.910 - 01:08:10.140, Speaker A: I think so.
01:08:11.390 - 01:08:35.200, Speaker B: Anyone else? I think John's question was also about can roll up blocks themselves have a block size limit? And yes, they can. A roll up application could also define like say, add rules that a roll up block can no longer be over a certain size. So yeah, that's fairly trivial to do.
01:08:36.050 - 01:08:40.690, Speaker A: We have another request for a question. Djenblock, go for it.
01:08:40.840 - 01:08:41.714, Speaker F: Hello guys.
01:08:41.832 - 01:08:42.434, Speaker E: Thanks a lot.
01:08:42.472 - 01:08:44.034, Speaker F: I hope you can hear me well.
01:08:44.152 - 01:08:46.438, Speaker E: So first the discussion is super interesting.
01:08:46.524 - 01:08:51.058, Speaker B: But I'm not super technical, so I'm.
01:08:51.074 - 01:08:55.046, Speaker E: Not sure I understood everything. The question I still had was the.
01:08:55.068 - 01:08:59.434, Speaker F: Following everybody's talking about modularity now, and.
01:08:59.472 - 01:09:02.586, Speaker B: The Cosmos ecosystem is often mentioned, but.
01:09:02.608 - 01:09:05.130, Speaker F: I feel like polkadot is less.
01:09:05.280 - 01:09:06.858, Speaker B: So I wanted to know, guys, what.
01:09:06.864 - 01:09:13.694, Speaker F: You thought about the design of a polkadot and could we consider the base.
01:09:13.732 - 01:09:16.026, Speaker B: Layer of polkadot as a data availability.
01:09:16.138 - 01:09:20.560, Speaker F: Layer, or is it something completely different?
01:09:21.090 - 01:09:21.840, Speaker B: Thanks.
01:09:22.770 - 01:09:25.060, Speaker A: Thanks, Chance.
01:09:29.030 - 01:10:13.360, Speaker F: I mean, every layer one is kind of a data availability layer, but I guess what polkadot does differently from as far as I understand the question, you want to understand the differences here. I guess the key difference is that by default, at least for parachains in polkadot, the validators of the main chain of the. Layer one. They also execute part of the state of the parachains, which come close to what you could think of a roll up as we've been speaking about.
01:10:17.030 - 01:10:26.840, Speaker A: Thanks. We'll be fielding more questions. In the meantime, Eric, tell us more. What else would you like to bring up and talk about?
01:10:31.960 - 01:10:33.270, Speaker E: Sorry, 1 second.
01:10:34.920 - 01:10:44.730, Speaker A: Sure. We'll go to a question. We have Brandon here. Brandon, please ask your question.
01:10:46.060 - 01:11:28.420, Speaker H: Earlier. Eric gave some really good context on what it might look like for cosmo zones to opt in here and to solve some of the data availability challenges associated with sovereign zones. I think most people are pretty familiar that if you're running a roll up, you have to make some decisions about where you're going to store that state, whether that's on an L one, like on Ethereum, or like in the validium model where you're storing that with some sort of a data availability group. Can you speak just very briefly on what it looks like to integrate a roll up or a zone with Celestia such that it can guarantee that publishing of data has occurred?
01:11:31.650 - 01:11:32.830, Speaker A: Thanks, Brandon.
01:11:34.070 - 01:12:27.970, Speaker C: Sure. I guess I can take it since no one else seems to be interjecting. So there's kind of two immediate approaches that one could use to have roll ups as we know them on Ethereum leverage the data availability guarantees of Celestia also interject brandon, if I'm not answering the question properly. So one approach is what we call Celestiums using a technology called the Quantum Gravity bridge. So in this design, you have a roll up. It operates on top of Ethereum, it does adjudication of proofs. So either validity proofs or fraud proofs on Ethereum, just as they do today, and they can deposit and withdraw coins from a bridge contract on Ethereum.
01:12:27.970 - 01:13:57.326, Speaker C: The distinction between a Celestium and a roll up on Ethereum, as we know and love, is that the data availability, instead of being posted to Ethereum is instead posted to Celestia. And the immediate effect of this is that it would dramatically reduce the fees, at least in the short term. There's a bunch of caveats around long term fee markets, but that's like a multifaceted game and stuff, so we won't go there. But at least in the short term, the immediate effect is that it would significantly reduce fees from several dollars to a fraction of a penny, all while having very strong security guarantees, because users can perform data availability sampling to know whether the data on the Celestia side is in fact available or not before deciding to use the celestium. The other immediate approach for people who want to leverage Celestia for data availability or how does it look for a roll up that wants to leverage Celestia for data availability is some project that we've been calling Sevmos, which is essentially an EVM settlement only layer that runs directly on top of Celestia. And then you can build execution layer roll ups on top of this. So you can, for instance, fork any roll up that exists on Ethereum or if you wanted to, also a side chain, but for the purposes of shared security, any roll up.
01:13:57.326 - 01:14:41.494, Speaker C: So anything like Arbitrum or Optimism or Fuel or ZKsync or Starkware, anything you want, you could just fork onto Sevmos because it's EVM compatible, and you would essentially use Sevmos as the adjudication layer. So that's the thing that would verify the proofs. Since Celestia itself cannot verify proofs, it does not do execution. And then Sevmos would then use Celestia for data availability. So these are the kind of two immediate configurations that we're seeing pop up with regards to roll ups using Celestia. There is a third configuration that I think people are starting to get excited about, which is a sovereign rollup. So Sevmos is an example of a sovereign rollup.
01:14:41.494 - 01:15:14.518, Speaker C: It's a roll up chain that operates directly on top of Celestia as opposed to requiring another blockchain to adjudicate its proofs. It actually operates directly on top of Celestia. That is the first example. But it is not the only sovereign roll up that can exist or that will exist. And I think there are some growing excitement in the community to build out Celestia first sovereign roll ups that do not rely on another blockchain to verify proofs that are hopefully going to be built in the not too distant future. Does that answer your question?
01:15:14.684 - 01:15:16.182, Speaker F: Can I add one thing?
01:15:16.316 - 01:15:16.678, Speaker B: Go ahead.
01:15:16.684 - 01:16:03.030, Speaker F: I think Brandon also mentioned how it would look like if a cosmo zone would be turned into a roll up or would be built as a roll up. And I guess that ties in well into what you just said with the soaring rollups, because you could build a Cosmos SDK chain that uses what we have a project called Optimint that essentially is compatible to Tenement. It's like a swap and replacement, and you can build, like, a Cosmos SDK app, but instead of doing BFT Consensus, it basically dumps the blocks onto Celestia. So that would give you Cosmos zones that are sovereign roll ups. That's also just one example of these sovereign rollups.
01:16:03.770 - 01:16:06.630, Speaker A: Thanks Ismal we have another question from a priori.
01:16:07.690 - 01:16:11.162, Speaker E: Hey, guys. Thank you for taking my question question.
01:16:11.216 - 01:16:32.590, Speaker D: On Sevmos as a settlement layer. So in theory, you would be able to have both EVM based roll ups and Cosmosm based roll ups settle on Sevmos. And would that induce any kind of special interoperability between those two or composability?
01:16:35.890 - 01:17:10.060, Speaker F: That's a very good question. I guess for a CosmWasm roll up you'd rather use if you want to go with the same model that John described before, where you have, like, a settlement layer, specific dedicated changes for settlement and a roll up on top, then I'd say you'd rather have a cosm wasn't settlement layer, which isn't like like which isn't EVM based. But that's really a good, interesting question. Maybe Mustafa I already heard him. Maybe he wants to say something.
01:17:11.090 - 01:18:08.550, Speaker B: Yeah. You can't settle cosmic WASM roll ups directly on Cybos because Cybos is EVM based, not based. However, as Ismail said, you might want to create a different WASM based settlement layer. One thing I've always said before, it might make sense for the Cosmos hub itself to implement to integrate Cosm WASM into the Hub and use it to support Cosm WASM rollups. Eric can convert this, but I think there's people in the Cardano ecosystem working on WASM roll ups. However, that might not be necessary because it might be possible to actually settle Bosm roll ups directly on EVM by compiling it to an intermediate language. I think Arbitrum Nitro works similarly because Arbitrum Nitro uses Bosm.
01:18:08.550 - 01:18:36.790, Speaker B: But also, like, for example, Optimism has a fraud proving project called Candon, which can compile arbitrary Go code. It can afford prove arbitrary MIPS code. So if you compile any program to MIPS, you can afford prove that. So in theory, you could compile like WASM code to MIPS, and you should be able to prove that on the EVM based roll up. Sorry, I mean the EVM based settlement layer.
01:18:38.010 - 01:18:41.466, Speaker A: Thanks. We have another question from no excuses. Go for it.
01:18:41.648 - 01:18:42.380, Speaker E: Yeah.
01:18:44.190 - 01:19:23.350, Speaker D: When I was reading Vitalik's blog post in security that the future would be multi chain but not cross chain, he specifically called out Celestia in saying that consensus and data availability can't be delegated cross chain without sacrificing consistency and security guarantees. And I wasn't aware of I haven't seen any written rebuttal to that claim, and so I'd be really interested in reading something about that. And then my second question is what's the place of Celestia or the value proposition or moat in a post shared security launch for Cosmos?
01:19:26.970 - 01:20:30.726, Speaker B: Yes. The first question, I agree, technically speaking, with Vitalik. And in fact, I've said the exact same thing as Vitalik before in a blog post in October of last year about the question ecosystem where we disagree is on the utility of what he calls chains of sovereignty or zones of sovereignty. Because what he's effectively saying is that his argument is effectively that every chain must use the same settlement layer to have shared security. And it's not really useful to have different settlement layers because you can't do a trust minimized bridge between that. So previously I've made a distinction between this idea of a trust minimized bridge, which is what roll ups are based in, and what I call a committee based bridge, which is not trust minimized because it relies on honest majority of a committee to operate that bridge. I think both have a place in the ecosystem.
01:20:30.726 - 01:21:55.990, Speaker B: I think it's very unrealistic that we will see a single settlement zone of shared security. I think in practice, we will see multiple settlement layers that have committee based bridges with each other. I can see a lot of utility in that because, as John said, there's a lot of interest in our community of building what's called sovereign roll ups, which are roll ups that don't settle to any specific settlement layer. And there's several reasons why you might want to do that, including more freedom and not having to be bounded by the social contracts of your settlement layer. In terms of the second question, which is I think the question is basically how does Sedestia compare to the Cosmos Interchange Staking or shared security model? So the Cosmos Interchange Staking model or shared security model is very similar to Polkadots where effectively you can kind of pay or request the Cosmos Hub validators to also validate your chain. But that's fundamentally not scalable because all the validators have to execute all the transactions in your chain. So for example, in polkadot you have a limited number of execution slots, but roll ups don't require that because roll ups do off chain execution.
01:21:55.990 - 01:22:22.480, Speaker B: And Zaki also agrees with me on this, but the Cosmos Interchange Staking model is more kind of geared towards or more appropriate for the Cosmos Hub having chains underneath it within the same kind of governance system. So it's kind of like having subsidiary chains under Cosmos Hub, but it's not really kind of like a long term solution to shared security.
01:22:23.750 - 01:22:30.020, Speaker A: Okay, so we have time for one last question and then we're going to go back to Eric for his closing thoughts. Alpha key, please.
01:22:35.280 - 01:22:54.544, Speaker D: Okay, so I think what's kind of being recognized here is that the on chain data costs are extremely expensive and if you want to do data intensive things, it's really expensive to do them on chain. I'm curious of your thoughts and realizations.
01:22:54.592 - 01:22:57.780, Speaker C: Around splitting out the data availability layer.
01:23:02.120 - 01:23:03.670, Speaker A: Thanks Chance.
01:23:08.220 - 01:23:56.090, Speaker C: Sure, I can take it. So the beauty of the modular blockchain paradigm is that by separating on the base consensus layer, doing just data availability and doing execution at a higher layer, this means that you can get a much higher throughput of data availability. Whether long term this leads to lower fees or potentially higher fees because of network effects. Over that is up for debate. But regardless, you end up with a much higher throughput of data availability immediately. The immediate effect of this is cheaper transactions for the foreseeable future. So that alone just separating these things.
01:23:56.090 - 01:24:49.720, Speaker C: The modular blockchain paradigm allows us to build applications that are much more data heavy, such as roll ups. Because roll ups on Ethereum today, you can see just the on chain cost and then the order of a dollar, maybe $2, just the on chain cost. And that's with Ethereum's currently not too high transaction fees and it hasn't been too high for the past couple of weeks. So this is just the on chain cost of data, a dollar or two. And this is comparing to a few years ago when people are saying things like the Internet of money should not cost more than five cents per transaction. And now we're looking at a dollar or two just for the on chain data cost, let alone the cost of execution and so on. So the advent of the modular blockchain paradigm allows for data heavy applications to ideally run with substantially lower fees.
01:24:51.100 - 01:25:19.040, Speaker A: Thanks John. So we're at the home stretch here and we'll go back to Eric. Eric, you once said to me know Celestia is one of the few projects where I feel like even if I tried my hardest to destroy it, I probably couldn't. Can you expand on that more or just tell us where you see things going and sort of your beliefs around Celestia and you can conclude your point of view as it relates to this topic and we'll segue.
01:25:21.160 - 01:26:46.300, Speaker E: Mean, I think one of the main reasons why Celestia is very difficult to criticize is because it's extremely simple. It is doing more by doing less. So by doing as little as possible, it births this whole modular blockchain stack by just focusing on one piece and doing that one piece very, very well. But throughout this conversation I've been trying to okay, but if I had to, what is the most annoying question that I can ask the celestial team? What is the one thing that I can latch onto to fuck with their heads a little bit? And I think that the thing that I'm landing on, what it's boiling down to? Like the annoying question that I sort of could ask is that we sort of admitted here during this conversation and this is nothing new. I think the celestial team has always said this, which is a very good thing, that we're being consistent. I think the annoying question that I would ask would be along the lines of okay, so let's say that there are like in Ethereum, for instance, we see that there are a couple of applications that are extremely used. Like there's an extreme demand for those applications and that would be OpenSea and uniswap and OpenSea and uniswap impacts the fee environment for all other Ethereum users.
01:26:46.300 - 01:27:29.208, Speaker E: So I was thinking like, okay, let's imagine that something similar also happens in Celestia. That you have two or three execution environments that end up, know, being extremely high demand for those and they drive up the fee levels on the Celestia based layer. Would it possible to say here that Celestia is an extremely important piece of the modular blockchain thesis stack, but in itself it is a monolithic data availability layer because all the demand for Celestria impacts all of Celestria users. So would it?
01:27:29.294 - 01:27:29.592, Speaker B: Maybe.
01:27:29.646 - 01:28:42.770, Speaker E: And the reason that I think that this is the best, most annoying question to ask perhaps John here is that I know that John is a very loud critic of the avalanche project. And avalanche, their vision for scaling is basically using different subnets which is completely separating the ecosystem into separate environments. John, would you think that it would make sense if you have one celestial layer and the fees go up to it turns out that it's very expensive to use Celestia for micro payments use cases. Do you think that it makes sense to segregate even Celestia into different subnets, so that yes, once you have one Celestia system that is cannibalized with usage, then you sort of create another parallel Celestia that would be like a subnet for Celestia where you can have Celestia two, where the fee environment could then go back to being low again. And then you can sort of scale the system by making more and more subnets as demand is cannibalized on either of these systems. That's sort of the most annoying question that I could come up with for John.
01:28:43.300 - 01:29:45.940, Speaker C: Sure, I can take it since I've been called out, but I'm going to answer half of that and leave the other half to Mustafa. So the first half is that, yes, it's true that as demand grows, there's a certain capacity of block space. Maybe it becomes expensive. But a caveat here is that there's a number of things that we can do to make capacity grow in ways that traditional monolithic blockchains cannot. The first is the scale out property of data availability sampling, which we've talked about a few times, which is that the more users there are, the more you can increase the block size securely. The second thing is because of the fact that every block modulo some tiny amount of the validator set management, which is stateful, but the rest of the block, which is the vast majority of the work that you need to do, is essentially stateless. It doesn't rely on any previous block existing.
01:29:45.940 - 01:31:01.672, Speaker C: And not only that, it's highly parallelizable because we split up each block into a square and perform operations on each row and each column of the square completely independently. So this leads to the potential for doing what we call internal sharding, where rather than sharding across different identities within the network and sharding and splitting up work among different identities or different validators, you essentially have a single validator, but that can split up its workload across multiple, not as powerful worker computers. So this allows you to essentially I don't know if you'd call this horizontal scaling, but it essentially allows you to just very easily produce very large blocks because you don't need one supercomputer. You can just spin up a cluster of very not so powerful computers and it'll still work. Now, with regards to even if we have these techniques, even if we have these mitigations, can there reach a point where there's simply too much demand for the capacity of a single celestial chain? And this is where Mustafa is probably better to answer because he actually wrote a post about this not too long ago. Segue to Mustafa.
01:31:01.736 - 01:31:04.464, Speaker A: Mustafa yeah.
01:31:04.502 - 01:31:50.080, Speaker B: So as I said before, no blockchain can guarantee cheap fees. The only thing a blockchain can guarantee is certain scalability properties and certain throughput properties. But it cannot guarantee. Ultimately the fee market is a free market. It's simply not possible for blockchain to guarantee cheap fees without conceding two things, the first of which is Dos resistance. And the second thing is the target requirement for block producers, the target hardware requirement for block producers. And this is kind of like an interesting debate.
01:31:50.080 - 01:34:11.420, Speaker B: People have said before, like Salana is centralized because it has very high block production requirements and very high requirements for users to verify the chain. Like you need 256 gigs of Ram. But then Vitalik recently put out a post which he called the End Game Post, which is this idea where he kind of had this thought experiment which said, assuming that I had a blockchain that had a very high resource requirements to produce a block, what can I do? How can I adjust that blockchain so that I'm comfortable with it? And the short answer to that is that to be comfortable with a blockchain like that, you need to allow end users to actually verify the correctness of that chain using technologies like data availability, sampling or fraud proofs or ZK proofs, which is what roll ups use because those technologies allow end users to verify the state of the chain with very low resource requirements, even though the requirements for actual block production is very high. But then the question is, how far do you want to take that endgame idea? What is your limit for block production and to what extent does that affect censorship resistance? Because in theory, sure, in theory we could have potentially infinitely scalable data availability layer that is Google scale, but then you have to have Google scale block producers. And the question is, to what extent are we comfortable with that? Because maybe you want to have block production requirements that are spawning off so that the threshold for someone else to step in in case someone senses network is not that high. That's the first thing. And the second thing is, what about sensitive resistance? So I know there are some ways to achieve sensitive resistance even if you have a somewhat centralized block production validator set.
01:34:11.420 - 01:34:45.850, Speaker B: But the question still remains is to what extent, to what extreme you want to take that. And I think that's kind of the fundamental question, because if you don't want to take it to that extreme, even though you're comfortable with the end Game, where that Vitalik has proposed, where you have high expensive block production but cheap block verification, the question is to what extreme you want to take that. And if you don't want to take it to an extreme, then you do need multiple chains and multiple data availability layers that do have different security guarantees and fragmented security.
01:34:48.140 - 01:34:54.890, Speaker A: Okay, we're coming on. Final comments. Eric, was anything else you want to say before we go off?
01:34:55.340 - 01:35:39.800, Speaker E: I just had a tiny last question, which was, I heard that Evmos running on top of Celestia would limit its use cases to only arbitrating disputes in roll ups. So the Evmos would only look at roll ups and settle the fraud and validity proofs. But the question that I had there was just it's just a small question. How do you actually limit that in practice without making the system permissioned? And what if someone wants to experiment with a new type of roll up and a new type of fraud proof? How do you enable.
01:35:46.960 - 01:36:54.700, Speaker B: Mean? There's kind of two ways that we're looking at restricting the sevmos EBM. The first way is what you mentioned, which is like you have a set of whitelisted smart contracts, so that's the dumb way of doing it. So you just say you whitelist arbitram or optimism smart contracts and you can only deploy optimism or Arbitrum roll ups. In theory, if you want to experiment with a new kind of roll up, you can still do that. You just have to deploy it as an L3s, you have to use an L two as a settlement layer, and you have to deploy your roll up as an L three. So it's still possible to experiment with different kinds of roll ups or different types of contracts or execution layers. But the second way that we're looking at it, which is kind of more interesting to us, is just to adapt or adjust the resource pricing of the EVM to make it restricted such that it's not practical to use it for on chain applications but only roll ups.
01:36:54.700 - 01:37:29.720, Speaker B: And the main way we would do that is by increasing the gas cost for state rights and state reads such that there's a limited number that you can't do more than a certain number of state reads and state rights per transaction. So if you want to do more than that, you're forced to use a roll up. But that doesn't actually place any kind of limitations on what on a contract on the chain, just on the resource requirements that each contract has such that it's only practical to deploy roll up based contracts on top of the settlement layer.
01:37:33.570 - 01:37:48.454, Speaker A: Thanks Mustafa. Well, thank you Eric. We'd like to thank you for just was is awesome. We really appreciate your time. Celestia is the first modular blockchain network. You can learn more on our Learn Modular section. Gents, thank you.
01:37:48.454 - 01:37:48.740, Speaker A: Be well.
