00:00:00.330 - 00:00:00.880, Speaker A: You.
00:00:02.050 - 00:00:04.190, Speaker B: Well, great to see you, virtually.
00:00:05.250 - 00:00:07.920, Speaker C: Great to see you guys. I like networking too.
00:00:08.370 - 00:00:59.550, Speaker B: Yeah, this might be the only section today that needs no introduction, so can kind of go straight to it. So I actually want to start with this intro question that I think was teased online yesterday of sort of but before we kind of get into the details of modular versus monolithic, I think we should actually just say, let's suppose each of you were kind of stuck in an elevator for 30 seconds. You had a 32nd pitch, and there was an Engineer who just quit Netflix because, well, I mean, stock price crashed, and they're like, I want to understand why I should build something either on Salana or Celestia. What's your 32nd pitch? And Mustafa me.
00:00:59.700 - 00:02:17.158, Speaker A: Okay, so this former Netflix engineer is presumably a web two engineer. So I would use a web two analogy. If you're a Web Two Engineer and you wanted to build a web application, you wouldn't want to build it on a shared hosting provider, like, let's say, if you're familiar with DreamHost or GeoCities or Google Pages. If you use a shared web hosting provider, you're restricted in what you can deploy because you're stuck with the same execution environment that's provided to you by that shared execution provider, and also because you're sharing the same server as everyone else. You're also limited in scalability that is very similar to deploying your application on a shared smart contract platform that is a monolithic blockchain. Nowadays, Web Two developers build use virtual machines on systems like Amazon, EC Two or AWS, and that allows those developers to have their own virtual server and have full control over the execution environment and scalability of their application. And that is what building a roll up is like.
00:02:17.158 - 00:02:33.950, Speaker A: You effectively have your own blockchain that you can do whatever you like on it and have any execution environment you want on it, and you can scale better because you're not sharing the same execution resource pricing as everyone else. Great.
00:02:34.020 - 00:02:39.754, Speaker B: I think we were traveling slightly close to speed of light right there because we went a little over 30 seconds on our pitch.
00:02:39.802 - 00:03:10.300, Speaker C: But Anatoly, don't use a blockchain unless you critically need finality in a shared context with a lot of other financial applications, like exchanges, stablecoin providers, all those things. If you don't need that, don't even use a blockchain to begin with. But if you need that, then Solana is the cheapest, fastest way to get to the largest set of these as quickly as possible.
00:03:12.190 - 00:03:29.600, Speaker B: Well, just so that we get kind of both of your individual definitions, in your own words, how would you describe the other architectures? So, Mustafa, how would you describe monolithic? And Anatoly, how would you describe modular and maybe Anatoly, you want to start?
00:03:32.130 - 00:03:44.610, Speaker C: I would describe modular as an attempt to break down the different functions of the chain with some clean interfaces and build them as separate components.
00:03:45.990 - 00:03:57.798, Speaker A: I would describe Onalithic as trying to build kind of a global shared computer similar to the original Ethereum World computer vision. Cool.
00:03:57.964 - 00:04:43.960, Speaker B: All right, so let's start by actually starting with this idea of like, developer costs. Developer UX, startup costs. So if you're talking to someone who was just getting their feet wet with developing on either Solana or Celestia, how would you kind of describe the startup costs of going from just a kind of toy project to actually running something in production to maybe having a dao having kind of longer term sustainability? And how would you describe things like how long does it take to get familiar with the language, the architecture, development environment? Just imagine you're someone who's just getting started Antilla you look like you.
00:04:44.810 - 00:05:20.370, Speaker C: Well, I'm assuming all the cool devs already know Rust. It's not it's a language that we didn't build. It's a language that amazing, other folks have built with its own community. And people out of fangs are publishing open source Rust code. So that part is pretty easy for anybody to pick up because it's a general purpose language. And the development environment is really just as close to native Rust as we can make it. So you just use cargo and use anchor to glue all your programs together.
00:05:20.370 - 00:05:36.360, Speaker C: So we've seen folks go from a hackathon to a project with like 8 billion TVL in six weeks. Shout out to the saber guys who slept on the floor of the Salana office to get that done.
00:05:38.910 - 00:06:28.940, Speaker A: Asava. So I think the general goal for modular blockchains is that we want to make it so that deploying your own roll up chain or your own blockchain should be as easy as deploying a smart contract. And we're practically almost there with the Cosmos SDK, you can deploy a blockchain in seconds with roll up technology. You can just fork that roll up. You can fork the smart contract and have your own roll up. And the idea is, if we make it deploying your own chain as easy as deploying a new smart contract, then why wouldn't you do that? You would have much more flexibility over the trade offs in your execution environment and you can tailor it to the application that you need.
00:06:30.590 - 00:07:33.180, Speaker B: So to that point, I think one thing historically is that the Linux took a very long time to actually get sort of the multi processing, multi app, multitenancy environments to work correctly. It took a long lot of pain in changing things in the kernel, but however, once it was done, it was like a huge floodgate. Kind of opened up a ton of app development opportunities. Like Node JS would never be able to run on a Linux machine without SMP and Linux 2.6 basically. So do you think it'll take a similar amount of time for us to get to the point where you have this sort of automatic scheduling of roll ups, automatic kind of deployment, things like that, and that the monolithic sort of version is sort of a crutch on the way there? Or do you actually think there's sort of a fundamental difference? And obviously you both probably have different opinions on that?
00:07:34.030 - 00:08:59.260, Speaker A: Mustafa yeah, so, I mean, I think if you take a modular blockchain stack, if you take each module in isolation, each module is simple, but there is indeed complexity in the way that the stack integrates. So yes, module blockchain stacks, if you look at the big picture, have more complexity than a monolithic stack, not on the individual module level, but throughout the whole stack. But that's not an issue because the whole point of modularity is that you don't have to know or care about what's going on in the entire stack. We're reaching a situation where, for example, core Dev in Guest other day said that now the Guest client has so much complexity now that no Guest developer actually knows what's going on in the entire. And so I think that's part of the reason why modularity is important. It does allow you to do more and have more complicated and blockchain architectures that do more without needing a single person to understand the entire monolithic stack. And so yes, to answer that question directly, it will take us longer to get there.
00:08:59.260 - 00:09:27.810, Speaker A: It does take us longer than building a monolithic blockchain. But since 2008, 2020, people have just been building new layer ones and we've ended up in this cycle of new layer ones. So it's now time to kind of break that cycle and kind of evolve into a stage where now we have so much traction in blockchain space, let's modularize it and make sure that all the different innovations that people are building can actually be useful.
00:09:30.150 - 00:09:41.846, Speaker C: Anthony the irony is that too thick, once it was stabilized, killed the microkernel design and the monolith, the kernel one is true.
00:09:41.948 - 00:09:45.160, Speaker B: The rust mock kernels have never really caught on.
00:09:48.990 - 00:11:03.686, Speaker C: I kind of think that it's going to depend what the use cases are. If you have dependencies on composition between applications and these applications are not static, right, you have constant kind of churn between the composition set, then you really need one giant state machine to glue them all together. Because what the users are going to care about is how quickly do I access all the state, whatever that may be. Maybe it's like positions in serum mango and UXD or whatever, and how quickly it's finalized and how quickly it's connected to all the other financial institutions that I care about, from stablecoins to exchanges. And that network is not something you can spin up in a smart contract that's actually kind of the living, breathing glue of the chain itself. So you could have that in a modular architecture, right? There's a bunch of tendermints, a bunch of them have different pieces. But if you have use cases that run even 20% worse right, have 20% higher fees, 20% higher latencies.
00:11:03.686 - 00:12:40.350, Speaker C: That's like your phone running 20% slower, people are going to notice and applications and everything else will shift to that environment where everything is optimized to the hilt. And to do those optimizations, you don't really care about the design or how complicated it is, right? What you care about is how fast can we achieve these goals for the use cases? And this is why modern operating systems are all monolithic. Even when somebody tries to build a microkernel design, when it actually shipped, you look at the 80% of the use cases that people use, the frame buffers, everything else, it's all punched through directly to the kernel without using any of these modular features. So how this plays out is going to be largely use case dependent. I personally believe that what these chains are good at is running like a Dex, right? They're good for price discovery, they're good at managing offers and bids and all this other stuff for digital items and that market function, what these things provide allow everyone else to build unpredictable use cases like NFTs, which are their own financial contract. But at the end of the day, what you need to do is synchronize offers and bids and do this as fast and as quickly as possible and connect them to all the rails.
00:12:42.210 - 00:12:45.310, Speaker A: Yeah, I want to challenge the micro kernel analogy.
00:12:45.810 - 00:12:49.890, Speaker C: I don't think analogies are bad, so rip it apart.
00:12:50.710 - 00:13:22.460, Speaker A: I would say it's more closure. The analogy I would use is in the Linux operating system. You have kernel space, have code that runs in kernel space and user space. I would say having a monolithic chain is like running all the code for user applications in the kernel space. But the way I see it is like having modular blockchain is having a user space which is like the L two, and having actual applications running there rather than in the kernel. Yeah.
00:13:24.610 - 00:13:52.134, Speaker B: I feel like there's sort of merits to both of these. If we look at history, history has sort of preserved both models. There's obviously an intense amount of work on extremely low latency code kernel bypass like all the types of stuff that Antonio was mentioning. But then obviously much of the value of the consumer internet has come from realistically making it so that I don't need to know what a page table is to write a piece of code.
00:13:52.172 - 00:13:52.326, Speaker C: Right?
00:13:52.348 - 00:14:57.498, Speaker B: Like what percent of JavaScript developers have ever heard the phrase page table probably sub 2%. So there is a sense in which developer friendliness is also quite important. And to that kind of point, another thing that of course people love talking about nowadays a lot is extractable value from validators and miners. And this is something that both developers and users of these systems have spent a lot more time thinking about. And one thing to kind of compare and contrast between monolithic and modular architectures is the idea that modular architectures have a lot more sort of things like you have to wait for certain transactions to go through. There's certain predictable latencies that allow people to sort of find newer forms of extractable value. There's sort of this sense in which the tanatole's point earlier heterogeneous fees make it perhaps more easier to extract value.
00:14:57.498 - 00:15:22.100, Speaker B: So one question is do you think this ability perhaps to extract more value from modular architectures for Arbitrators and sort of market makers, do you think that's a feature or a bug? Because it could actually be something that drives liquidity as well as also kind of parasitic towards users and sort of where do both of you kind of see that?
00:15:23.430 - 00:16:05.266, Speaker A: It depends on how your execution environment works, specifically if your execution environment has a sequencer. So in Ethereum L2 S, the mev is basically just moved to the L2 sequencers. Like in optimism, for example. For example, the optimism roll up. They see that as a feature and not a bug because they see that the sequencer can extract the mev and use that to fund public goods. And that also kind of goes to kind of correspond to Anatoly's point about finality. It's not necessarily the case that you need layer one finality for users to have sufficient finality on their transactions going through.
00:16:05.266 - 00:16:30.330, Speaker A: So, for example, like Arbitrum's inbox model or Optimism sequencer model, you can get a finality by looking either at the inbox or asking the sequencer immediately in a second faster than the Ethereum block time. So there's different guarantees of finality that's more for spectrum than just relying on layer one finality.
00:16:33.110 - 00:18:06.910, Speaker C: Anatoly. So those are all kind of compromises, right? Because for it to be a true L2, it needs to be able to fail at any moment. And that means that whatever partial finality you get is then in question when, however, failure recovery works in that setup and that trade off is probably okay for applications and users in like a sandbox. But what is the whole point of a blockchain, right? Is it to build an application that could run on a database or is it a global replacement for a large chunk of finance? So we'll probably see both of these models survive because of that, because we don't really know why these things exist yet. I think with regards to mev, I tend to agree that so far we're probably the only thing that we see that actually funds and generates value in these chains is mev. And in the best of light, you have very fancy algorithms that try to predict the future and they want to improve prices and offers and everything else on this chain and they run their racks of GPUs and they extract mev. And in theory, it should get competitive to the point that they should be bidding for users to send their orders directly to them.
00:18:06.910 - 00:18:58.590, Speaker C: You see some of that with Flashbots and some of that with Jito Network and Solana. So those are all good things, right? Because in theory, a user that doesn't care about a MAV should in fact see a better price and maybe even a rebate for their transactions instead of a fee. But that's still kind of, I think, pretty far away. We're still not yet at the level of adoption where I could say that, yeah, this is true and this is how these chains run. So a lot of questions still remaining. But for the sake of the debate, my strong opinion is that mev will be the thing that's actually driving development and hardware upgrades and actually going after users in these spaces on blockchains.
00:18:59.570 - 00:20:44.580, Speaker B: Yeah, I mean, I guess to that point, if we think about something that you said in terms of replacing existing financial systems or sort of having the ability to service sort of a larger swath in the market mev in Solana to some extent right now and then Solana and Avalanche in particular, where there is a lot of validators advertising like hey, we'll sell you this much space in this block at this time and people basically are colocating. Do you think that this is sort of deleterious and that it'll just reduce to the existing sort of model of sort of colocation around exchanges, colocation around validators? Or do you think that you could actually sort of make this more sustainable with a different fee model or potentially better cryptographic solutions? Because I do think one thing for the monolithic world is there's a lot more probabilistic mev. There's sort of two ways of thinking about it. Mev on single chains is a congestion game, right? There's like just a fixed amount of opportunities and way more bidders than opportunities. And it's more about just like constructing a mechanism that matches those mev. And modular architectures is much more probabilistic, right? It's about routing, figuring out which path to take. And so I guess from both your perspectives, do you think there is a way in which the economics model can thwart kind of that becoming all of the volume on the network or do you think that effectively there's no way of avoiding that and we're sort of stuck with the existing system.
00:20:47.030 - 00:21:46.200, Speaker C: I can go first. So the way I think about it is that we want to make it as competitive as possible and as easy for anybody to enter the network and offer my whatever MAV solution because that should drive the amount of actual profit extracted by these things to as close to zero and that should be a good thing, right? Like what you should see in the end is better prices. Prices that reflect the true value of things globally all synchronized across the same state machine. And to that regard, at least in Solana, it's possible to run multiple block producers on the exact same state concurrently at the same time. Doesn't do that yet. But in theory that's something that is doable. Because of the way the VDF works, we can effectively splice the final ordering using the VDF ordering between all of these.
00:21:46.200 - 00:22:58.698, Speaker C: So what that should hopefully do is that instead of only having one block producer at a time, some position in the world, you now have N, and then you go talk to the closest one geographically, that reduces out latency. And then that thing is then running whatever algoes and whatever it needs to optimize its order flow. And it's doing that competitively with everyone else. And in theory, that should reflect the lowest price of any good offered on chain globally within the speed of light latencies around the world. And if we can do that, then in theory, price updates moving through Solana are moving at the speed of light through fiber, which is as fast as news travels, and then it's competitive with something like Nasdaq and CME. So that's kind of the dream, the horizon. And this is how we've been building everything, looking at that final state, how do we get there and what are the pieces we need to get to? And when you kind of take that perspective, you don't really care if the architecture is modular or monolithic.
00:22:58.698 - 00:23:25.142, Speaker C: What you care about is can we synchronize information globally as close to the speed of light as possible? And usually, at least so far, there's no gains to be had using a modular architecture that right now, everything that we've been doing is around this monolithic state because that's what's going to get us closer to that final state of global information.
00:23:25.276 - 00:23:30.200, Speaker B: Symmetry all right, well, I'm excited for Mustafa's response to that. Yeah.
00:23:32.090 - 00:25:21.690, Speaker A: I think this discussion kind of highlights the key kind of philosophical difference between Solana and systems like Celestial or Ethereum, which is I think systems like Solana view blockchains more as a distributed computing platform where you can have global settlement. Whereas we like Celestia and also other systems like Bitcoin and Ethereum, we see blockchains. Fundamentally, the fundamental purpose of blockchains is that it's a verifiable computing platform, not just distributed but verifiable. And that means there's a focus on decentralization in that setting means that you need to allow light nodes to verify the state of the chain. Otherwise, if your goal is just simply to have fast agreement on the state, how does that goal fundamentally differentiate from what Web Two achieves with a centralized or distributed database? Even you could argue that the fundamental difference is that you have a global set of distributed parties agreeing on that state, but without the ability for end users to verify the states. In my opinion, that takes away one of the key properties of blockchains, which is that well resourced actors cannot violate what the social consensus of the chain has agreed, what the rules of the chain are. If the Solana validators, for example, wanted to change the rules of the chain, how would end users verify that without running a well resourced node.
00:25:22.270 - 00:25:59.990, Speaker B: Yeah, you teed up the next question, which is to those that haven't spent time sort of in the weeds on these modular and monolithic architectures force very different separations of storage and execution for light clients versus full clients. So for the audience first, let's just start with describing kind of how light clients work for Solana and Celestia and sort of where the division is and which parts the light client has to store and execute first. Not and me, Mustafa, since you brought up light clients.
00:26:00.890 - 00:27:24.450, Speaker A: So light nodes effectively allow end users, ordinary users, whether a cheap laptop or mobile phone, to have almost the same level of security as a full node because they can get assurances about the state of the blockchain using technologies like fraud proofs or ZK proofs. And this is the fundamental reason why the Bitcoin community has decided not to increase the block size limit. In theory, Bitcoin could do a Zillion transactions per second if they increase the block size limit and optimize the node software. But the Bitcoin community has fundamentally decided not to do that because it would increase the resource requirements for end users to run full nodes and validate the chain, which is like pretty much the critical aspects of what a blockchain is supposed to guarantee that the state of the chain is correct effectively. But with new light node technology that uses fraud proofs and data availability, sampling and ZK proofs, you can now increase the block size limit without compromising the ability for end users to validate and verify the chain.
00:27:28.180 - 00:27:30.080, Speaker B: Like clients on Salana.
00:27:31.380 - 00:28:44.164, Speaker C: So the devils and the details obviously here, when you look at E two deployment, go to Nodewatch IO, there's about 6500 machines that run the 300,000 E two nodes, e two validators, there's only 6500 actual boxes. So the real world deployment is that you take a Salana validator and then you load it up with like a few hundred nodes, right? E two validators, and that's your E two deployment. So the real world is that machines are getting bigger. And the Salana validators are not stacks of Xeon processors. They're like 32 core systems, which is what you get out of a data center right now for $800 a month, which is not insane by any means. It's not building racks and racks of systems like you would for centralized service to handle all those users. So there's a bit of kind of I think, I don't know.
00:28:44.164 - 00:30:11.430, Speaker C: You have to actually look at the details of how these things are deployed. I think it would be misinforming to say that the amount of hardware necessary to run Solana is that significantly different than any of the other chains when you actually look at how these things are being deployed in the real world. But light clients themselves, when you're talking about like a different system for verification, there's obviously trade offs. So whenever anyone brings this up the question that I have is assuming all the other nodes, everything else has been destroyed, how many light clients do you need to rehypothe the network to reconstruct it? So how many nodes are you trusting when you run this thing? And those trust assumptions are important to users because the end state of a monolithic chain is when you don't run any nodes, you're trusting that at least one out of the, whatever, 3000 boxes that run the replicate Salana, at least one of them is honest. So when you run a light client, you partially participate in validation and then you assume that at least X number of other light clients are honest to actually help you get those guarantees. So it's up to the users to pick which one and if they even care.
00:30:13.000 - 00:31:06.120, Speaker B: Yeah, I think one thing that's actually been an interesting development is end users have actually had to learn the difference in light client design, sort of by spam and DDoS type things, right? Where recently, sort of Solana has had some issues for end users using wallets and like clients, which every chain, if we actually look at their history, has had similar issues. I guess I just want to point out that the end user does notice this, but they may only notice it after they've already adopted something. So how do you kind of view the pros and cons of the like client design for your chain and its impact on the end user who know their friend told them to download a wallet to buy an NFT?
00:31:09.980 - 00:31:11.224, Speaker C: Follow up to me.
00:31:11.342 - 00:31:15.290, Speaker B: Yeah, well, since you said it's up to the user to care, I feel like.
00:31:18.540 - 00:32:11.340, Speaker C: First of all, it takes a hell of a lot more resources to stay on tip and to run Salana at the latest block. If you're talking about just verification, you can look at how Salana deployment works. Out of the 32 cores, there's only four threads dedicated to the entire banking stage, and that includes all the votes as well. So if people don't know how Salana works the execution of smart contracts, it is optimized to the point that we can use it as the rail for voting and consensus. So consensus runs just as a smart contract. So out of like 300 million or so transactions per day, 80% of them are just votes. And it doesn't matter because there's still out of those 32 cores, only four threads are dedicated to run all the transactions.
00:32:11.340 - 00:33:16.856, Speaker C: So validation is actually the smallest part of the entire stack. The true cost of running Solana is how do you synchronize globally all of this information that's being propagated and that requires a big pile of erasure, coding and signatures and everything else to propagate this data simultaneously around the world to all the machines. So that part is the expensive part. If we're talking about a world where users actually care about, hey, I want to make sure that the wallet and the thing that I hold hasn't had these big validators committing an invalid state transition. You can do that much, much cheaper, and you can do that with MN event schemes and a whole bunch of other stuff, and you can build those applications without actually impacting anything else about how solana runs and how it finalizes. And those are different concerns. These are not concerns about, I want fastest finality for trading and price discovery.
00:33:16.856 - 00:33:42.040, Speaker C: This is I'm holding my assets, and I want to make sure that the big validators don't rug me. You can serve both of those. And that's something that if the users demand it, people will build it. That's basically the answer. But what users are demanding right now is, I want the fastest, cheapest finality, because what we care about is distribution of these NFTs to the largest number of users.
00:33:42.700 - 00:34:11.250, Speaker A: Mustafa yeah, so I think there's kind of two things there to unpack. First of all, there's this question. There's this point to be made that users don't care about validating a chain. But you can also say, like, if you built a completely centralized blockchain with one validator, from the user experience, it looks the same, it's still got a wallet. Will users care? Maybe they will.
00:34:11.780 - 00:34:19.780, Speaker C: My point wasn't that users care about validating is that with a light client, you have honesty assumptions that you need at least N other light clients to be honest.
00:34:19.850 - 00:34:20.276, Speaker A: Right.
00:34:20.378 - 00:34:51.628, Speaker C: And that's an honest minority. With a normal, decentralized network, like a monolithic one, you only need one out of the end. So that's a real trust trade off that users will make. And it's not about going fully centralized or not. Right. I'm not trusting Facebook. I'm like assuming, okay, there's a permissionless network, at least one out of the N is honest, or there's a permissionless network along a like client, and I need at least 40 out of the 50,000 to be honest.
00:34:51.794 - 00:34:58.640, Speaker A: But it's not one off end because you need to trust the honest majority to make sure there's no invalid state transition.
00:34:59.860 - 00:35:18.300, Speaker C: No. Any one of them can detect it and flag it and say, hey, what the fuck just happened? The entire majority of the chain just did an invalid state transition, and I have the record to prove it, and I'll provide the data availability for it. You only need one out of N on any monolithic network.
00:35:18.480 - 00:35:23.544, Speaker A: Yeah, but that is fraud proofs. What you're describing is fraud proofs, and that's what roll ups do.
00:35:23.742 - 00:35:32.808, Speaker C: No, it's simply, here's the data to prove that the majority went rogue. Everybody can download it and then validate it locally.
00:35:32.984 - 00:35:42.080, Speaker A: But that is basically the definition of a fraud proof. You give someone one transaction that's invalid, and then they can check for themselves that it's invalid.
00:35:43.060 - 00:36:06.484, Speaker C: They would have to check the entire ledger history. So this is a very big fraud proof. But every chain that has full replication, like Tendermint, BFT, whatever, they. All rely on this idea that if one out of the N detects an invalid state transition that they can pager duty goes off and then somebody yells what the fuck happened?
00:36:06.602 - 00:36:34.588, Speaker A: So it sounds like what you're describing is similar to the concept of alerts in the original bitcoin white paper. But the problem with that is that it kind of simplifies back down to in a worst case scenario, you have to run a full node to verify the history. Of course, it doesn't really fix the problem. You need to be able to prove light nodes that have low resource requirements that something is invalid.
00:36:34.764 - 00:36:36.352, Speaker B: Yeah, I think this is the main.
00:36:36.406 - 00:37:00.090, Speaker C: I only need these resources when this happens, right? So one of the end validators say something went wrong. Everybody in the community that's holding assets says okay, give me the data and we'll validate. So like, what is the cost of that versus the cost of me running a light client? Which then depends on at least some minority of light clients being honest and running.
00:37:01.260 - 00:37:18.110, Speaker A: But what's the path there? If I'm running a solana light client, if that is supported in the future and then someone says alert, do I have to buy new hardware just to validate that specific alert? And that alert could be spam as well.
00:37:19.680 - 00:37:40.100, Speaker C: Maybe. It's up to you. Right? I think that validation, like I said, is much, much cheaper than running a full node because even in our current deployment right now, only four out of the 32 threads is dedicated to smart contracts and they're not saturated. So do you need to buy new hardware or do you can use your laptop?
00:37:41.480 - 00:37:58.620, Speaker A: But I mean, fundamentally that still doesn't really help users. Users might want again, light nodes need the ability to verify the chain with low resource requirements. Maybe some users are fine with buying new hardware, but I'm pretty sure a lot of them aren't.
00:37:59.040 - 00:38:17.568, Speaker C: Not a single light client, which where the rest of the chain is completely dishonest, cannot validate anything. So a user with a like client is validating in an honest minority. So that trust assumption exists. Like it or not, it's there.
00:38:17.734 - 00:38:28.384, Speaker A: Yeah, but honest minority assumption is inherently a significantly weaker trust assumption than honest majority assumption because it's not an honest majority.
00:38:28.432 - 00:38:39.528, Speaker C: It's one out of N. All I care about is that one out of N of these validators is honest, and then I can validate whether a claim that there's been fraud in the network is true or not.
00:38:39.614 - 00:38:52.924, Speaker A: But it doesn't fix the fundamental problem which is that you need to be able to have light nodes that can verify something went wrong with low resource requirements because in your scheme they need to upgrade their resources to validate that.
00:38:53.042 - 00:39:11.970, Speaker C: I don't believe that fundamental problem exists because the actual Amdal's law of execution, looking at purely the smart contract side after everything has been validated, is 20% of the actual running a validator. So your requirements are just naturally at least five x lower.
00:39:13.160 - 00:39:16.260, Speaker A: Yeah, but the 20% is still significant.
00:39:17.880 - 00:39:19.716, Speaker C: My laptop is not 20.
00:39:19.898 - 00:39:52.140, Speaker A: I have a question for you. Actually, I have a question for you. The goal of Salana is to have a world computer model where you have like, a single settlement layer. What's the end game? How do you scale that? Are you just going to keep increasing the node resource requirements? Because right now you're saying it's $800 and that's fine, but there's going to be more demand and what's endgame?
00:39:52.300 - 00:40:17.400, Speaker C: So that happens naturally, simply because for the same cost, you get offered twice as much hardware roughly every two years. So without even asking for it, the new machines that are being deployed right now have dual 25 gigabit. Next cost didn't change. It's just everybody upgrades. That's just part of the cycle. In two years, my laptop that I have right now is going to be twice as powerful.
00:40:18.860 - 00:40:22.600, Speaker B: You believe in perpetual motion machines for Moore's Law?
00:40:22.750 - 00:40:36.460, Speaker C: It's going to continue forever. Really? So I believe that if we stop doubling the amount of vector dot products that we can do every two years, then we should all be working on bunker coin.
00:40:38.960 - 00:40:52.236, Speaker B: To some extent that's true, but there's also this problem, right, from a hardware design standpoint of we're already at EUV Lithography. There's not actually that much further we can go before we're set, like electrons.
00:40:52.268 - 00:41:22.460, Speaker C: Tunneling between no, bandwidth is going to keep doubling. Maybe single core performance is going to get saturated, but you're going to keep doubling the amount of bandwidth. And what we care about is bandwidth, right? We care about can we handle 20 gigabits in a single box? Right? That would be really hard right now, but we can do one in two years. We can definitely do two or maybe even eight, depending on how architectures change. And within ten years, you can definitely do 40 gigabits on a single machine.
00:41:23.840 - 00:42:03.400, Speaker B: This is instantaneous transfer. How much storage do I actually need locally? Because one thing, I guess, from kind of your argument of, hey, look, people will be able to effectively submit this fraud proof by validating the entire chain. Is this idea that the storage requirements are somehow static. Like, yes, we're only using four cores to process, but if I have to actually validate the entire history of 300 million transactions a day, that's basically like 30 billion transactions a year at some level. The storage requirements also go up with bandwidth requirements. So how do you kind of combat that in the long run?
00:42:03.550 - 00:42:22.800, Speaker C: You care about that happening within a certain fixed time period, right, within an epoch, because all proof of stake chains have an epoch. If you go beyond the epoch, leak keys can generate a fork, right? So you have some assumptions about like, within this fixed amount of time, somebody's going to say, hey, what the fuck?
00:42:24.820 - 00:42:43.510, Speaker A: Hopefully, I mean, it sounds to me like relying on the assumption that Moore's Law will go on permanently. Which I think is an assumption that has not hold true for all types of hardware over the past few years.
00:42:44.120 - 00:43:12.640, Speaker C: It has over the last 20 years. The amount of raw throughput compute wise that you can do on a chip push through a Nic, push through an SSD, push through the GPU that's been doubling at a steady pace but not for hard drive. What we're talking about is not cycles per second on a single core. It's like raw bandwidth across an array of cores or DSPs or whatever CUDA threads.
00:43:13.860 - 00:43:35.388, Speaker A: I'm pretty sure it hasn't held true for hard drives if you look at the data. But the other thing that's missing here I think, I think you're assuming that you're ignoring the fact that people also have to sync the chain from scratch. You need to have people to be able to sync the chain and bootstrap from scratch.
00:43:35.584 - 00:43:43.720, Speaker C: Yeah, it's like 30 terabytes of transactions per year is what we're seeing right now. Dirty TB is not intractable.
00:43:45.900 - 00:43:47.896, Speaker A: And what's that going to increase to?
00:43:48.078 - 00:44:48.670, Speaker C: But you don't need the whole year, right? You need it within the epoch. So what is it over like two days, maybe 20GB or whatever? Is that so fundamentally difficult that I could do it right and a very large number of users could do it. So is that assumption my point is, is that assumption and that hurdle to actually do this verification is it so much harder and such a big differentiator versus I have to have a light client constantly running plus I have to assume that there's an honest minority of light clients. So that's the difference, right? You can do that or you can rely on at least one out of N to provide you the data. So the larger the network gets, the more likely that one out of N is honest. And this is where there is a clear difference between something like salana or tendermint where most tendermints run to like 150 or so validators. Salana is at close to 2000.
00:44:50.500 - 00:45:24.760, Speaker A: So I think we disagree on what is a large number of users. If we're talking about 38 terabytes a year, we're talking about reaching the stage where as a user I have to effectively buy a whole bunch of hard drives, put it in my living room to verify the chain. Whereas what we're targeting is you should be able to do it on your laptop. You shouldn't have to buy specialized hardware just to be able to verify that the chain is valid.
00:45:25.100 - 00:45:30.490, Speaker C: But you're not. You're participating in validation with an honest minority assumption which is different.
00:45:31.500 - 00:45:36.568, Speaker A: An honest majority assumption is a much weaker assumption than honest majority assumption.
00:45:36.744 - 00:45:55.510, Speaker C: But like I said, we're not talking about honest majority assumption. We're talking about hardware costs and a one out of N assumption I totally own that the user Strading hardware costs plus relying on at least one out of n of the validators to provide them the data.
00:45:57.400 - 00:47:22.590, Speaker B: I think this is probably where it's very clear the philosophical differences between what it means to be a user and what it means to be a validator are kind of hidden here. But I do, before we kind of wrap up, kind of want to talk a little bit about cross chain compatibility. Obviously at this point I think it's pretty much impossible to say. We're not going to have a MultiChain world with different use cases, different qualities of service, different types of guarantees for users. And one kind of advantage of modular architectures is that they do spend a lot of time standardizing interfaces for cross chain communication and they effectively build in messaging standards or rely on actually specified messaging standards, whereas for monolithic architectures it can be quite different. As we've kind of seen, trying to basically translate between different virtual machines and get sort of bitwise reproducibility can actually be quite hard, especially when there's synthetic assets involved. So how do kind of like monolithic architectures compete with platforms that have focused on building a native messaging and do you think it's essential or do you think that eventually there will just be a communication center that everyone just agrees on?
00:47:26.420 - 00:48:19.360, Speaker A: Well, I can start, I guess, but I think one paradoxical thing of the vision of having a shed like a single settlement layer that Solana relies on is that if you look at the blockchain system, we don't have a single settlement layer. We do have a multi chain ecosystem with bridges like you have wormhole bridge between Ethereum and like the market has already shown, we can have multiple settlement layers. We don't need a single settlement chain. If the market has already decided that and right now we have these insecure or trusted bridges that can have multi sigs, then why don't we just embrace that and have a modular architecture that uses roll ups and has these more secure trust minimized bridges.
00:48:21.940 - 00:49:21.990, Speaker C: Antoly bridges are man, are scary. And the part that is the scary part is not the collusion which something like a native roll up architecture fixes. It's the smart contract risk and implementation risk because of just even the simplest piece of code can have a bug. And that's really the scary part. To that extent though I think a natively baked in roll up where this is the standard implementation that is owned by the layer one is probably likely to get to a point of extremely high security and confidence in the code much, much faster than something where every user does their own thing. Every developer does their own thing. So that part is like who knows how that will play out.
00:49:21.990 - 00:50:50.930, Speaker C: The stuff that does work pretty easily and pretty well are actually like the very dumb simple things like USDC where that's effectively bridged across multiple chains and there is a very heavy trust assumption on circle to make sure that there's dollars in the bank and you saw that be pretty successful with wrap bitcoin as well on ethereum. I can't tell honestly, if this is how things will play out over the next five years, but I feel like that's probably a model that if there's demand for cross chain assets in large volumes, that's a model that's going to stick around for users. And I don't know why would users would switch from that to a more permissionless one unless it takes much more time to get assets listed in decentralized systems, which is obviously a hurdle. I see a lot of advantages. So this is like a point that's really hard to fight against if you have very native, very baked in roll up model that is totally part of the layer. One, I would tend to think that's going to have a lot more confidence in terms of code security and that's the biggest attack surface in all the bridge solutions that we have out today.
00:50:51.300 - 00:51:39.890, Speaker B: Yeah, and in a lot of ways I think there's definitely been some truth in that. The optimism bug that was found was very similar to the wormhole bug and it's mainly this idea of having the synthetic asset on both sides and ensuring that they stay synchronized, supply stay synchronized, price stays roughly synchronized. That seems to be the source of these kind of bugs. Definitely. I think we're still not at the point of, hey, the contract node bugs are not kind of an issue. So before we wrap up, I guess if you were to give one compliment to the other type of architecture and say something nice about it, what would you say?
00:51:41.540 - 00:51:54.196, Speaker A: I like Solana's execution environment. I think parallelization is a very good idea. I just don't like the vision of a single set of mid layer, basically.
00:51:54.378 - 00:51:56.570, Speaker B: Hey, something nice was still said.
00:52:00.460 - 00:52:29.100, Speaker C: I really like the idea of actually separating data availability and focusing development effort on that because I think that's easier to make it secure and the biggest hurdle to iterating is security. So there's actually, I think, an opportunity for modular architectures if they can iterate faster because they've broken down the pieces into something more manageable. So that's like an honest compliment.
00:52:30.080 - 00:52:36.132, Speaker A: Well, mine is honest as well, so good. Awesome.
00:52:36.186 - 00:52:45.156, Speaker B: I don't know, are we doing questions or no, we're good. All right, well hey, thanks everyone for listening in. Thanks Antoli. I'm sure it's like, I don't know. 05:00 a.m for.
