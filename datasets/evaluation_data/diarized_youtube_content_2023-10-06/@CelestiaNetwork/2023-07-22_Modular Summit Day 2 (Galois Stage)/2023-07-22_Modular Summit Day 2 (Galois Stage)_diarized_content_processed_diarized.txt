00:01:33.490 - 00:02:10.554, Speaker A: Hello, I'm Zucky. I will be your MC for this morning. And let's kick things off. First up, we have Adrian Brink, co founder of Anoma. No one knows what Anoma is, and I don't think you'll find out in this talk, but Adrian will be talking about shielded data availability. All right. Yes, indeed.
00:02:10.554 - 00:02:31.080, Speaker A: This talk isn't about Onoma. If you wanted to find out what Onoma is, you should have listened to Chris's talk yesterday. Anonoma loves Celestia. If you haven't seen it, watch the recording. It was a great talk. Cool. I will very briefly today, could I get a timer up? Well, we'll see how long it takes.
00:02:31.080 - 00:03:24.090, Speaker A: Cool. Yes, I will talk about chilled data availability on Celestia today. Or the ability of how we can retrofit privacy guarantees and also forward fit privacy guarantees into existing protocols without needing to make changes to those protocols, which, as you'll see in a little bit, is going to be quite important, especially when you start thinking on how do you want to build privacy preserving roll ups on a lot of these systems. So before I start, let me give you very quick TLDR on what Namata is, how it works, and sort of why we need unified privacy sets. So Namata, in a nutshell, it's asset agnostic, multi chain privacy. Until really, Namata, at least on the interchange, we didn't have the ability to have asset agnostic privacy. Privacy was always tied to specific assets, right? Like, when you think of what Monero or Zcash do, they provide very good privacy guarantees.
00:03:24.090 - 00:04:19.462, Speaker A: Let me scratch this. Zcash provides very good privacy guarantees, and Monero provides you should do your own research privacy guarantees. But they provide them for specific asset types. As in the case of Zcash, they have Zack, and in the case of Monero, they have XMR. And so users are now forced into this model where for some reason, they have to hold or pay for their milk in a store using ZEC, even though I think most people would much rather use USDT. And so what Namata brings, it effectively adds an extra type or an extra field to the notes in a Zcash like system to provide a unified shielded set where all assets, no matter where they were created or how they were created, can live in a unified, shielded set. This has the tremendous property that you get low volume but high value assets that inherit the privacy guarantees of the low value but high volume transfers.
00:04:19.462 - 00:05:17.466, Speaker A: So, for example, a very trivial example here is all the USDT private transfers in Amada provide privacy guarantees to $100,000 NFT. And this is really the only way to get privacy guarantees for long tail assets which don't have a lot of inherent volume. But due to this, you can inherit the privacy guarantees of sort of the entire shield set. And the other big thing is it means we don't have to keep forcing users into accepting specific assets in order to be private users can bring user generated assets. I would assume in most cases it's going to be something like USDT USDC. But if people sort of in two years from now want to create a new fancy stablecoin, that fancy stablecoin can inherit the last three years sort of shielded set activity, and as a result, has a much stronger privacy set at launch. The way this works with Amada is that Armada is a base layer chain.
00:05:17.466 - 00:05:56.780, Speaker A: It's a sovereign L, one that connects to other systems via IBC site, not on IBC. If you are thinking about building a bridge, don't read how IBC works. This mostly solves all your problems. Custom bridges are terrifying, and they're very prone to errors. So with Amada, you get permissionless connections to other chains for IBC. So in the future, if other chains pop up, or assets on other chains pop up, those assets can also freely flow into Nomada. And maybe one of the coolest things is we have shielded set rewards in Nomada, which means that the Nomada governance unit can effectively incentivize rewards to be paid for holding assets in the shield set.
00:05:56.780 - 00:06:46.780, Speaker A: And this is actually very important because most people at the moment treat their sort of privacy as in, I want to have privacy right now for my asset. But this is actually really problematic because that means you sort of go from a transparent system into a shielded system and then back out. Now, you have sort of a lot of statistical attacks that you can possibly pull off against this. So the much better idea is that you mostly want to hold all your assets in a shielded set all the time. And when you want to use them somewhere else that doesn't work with the shielded set, then you want to make them transparent. And the cool thing with Amata is you get shielded Set rewards within the shielded set. So as in, if you hold some east in the shield set, there may be incentives there that get paid out in the Shield set in the native asset of Namata just by holding assets in the shield set.
00:06:46.780 - 00:07:19.954, Speaker A: Also, if anyone here is thinking of building private lending platforms, you need the convert circuit that enables Shield Set rewards, because it's roughly the same idea. You have a transparent list of sort of conversion rates for specific asset types, and you can then upgrade specific assets in the shield set. For example, you can go from like Osmo at epoch one to Osmo at epoch five. And at the same time, when you do this upgrade, you claim the rewards for it. Yes, this convert circuit, it's fully documented. If you like cryptography, you should check it out. Cool.
00:07:19.954 - 00:08:14.150, Speaker A: Now, coming to shielded actions, once you have assets in the shield set, you want to ideally have the ability to interact with the rest of the ecosystem, whether this is on the interchain or sort of within other specific L1s from within the shield set, right? You don't want to have to take out your money from the shield set all the time in order to use, let's say, an Osmosis pool. So shielded actions is really the generalized case of how you interact from within the shield set with application logic somewhere else. And really, this can mean with specific DApps, for example, on Ethereum, but this can also mean with specific app chains like Celestia or like Osmosis. And let me quickly walk you through an example here for Osmosis, because this is sort of very instructive. I think you hold some assets in the shield set. You want to make the choice that, I don't like the atoms anymore. I want to now hold some Osmo.
00:08:14.150 - 00:09:00.014, Speaker A: Traditionally, what people would do is they would personally take the money out of the shield set, transfer it over IBC to Osmosis, swap it on Osmosis, swap it back, right? And this is sort of normally they would go to their standard Osmosis address and then sort of do the swap there. With Namata, you can instruct the Validator Set to do actions on your behalf. This is really the right way to think of this. You can instruct the Validator Set to take some action elsewhere while staying within the shield set. So you can construct effectively using a knowledge proof. In Amara, it says, I would like to swap X or Y. Then the Validator Set will take the assets via an IBC method, bring it to Osmosis on Osmosis, use the IBC memo to execute the swap.
00:09:00.014 - 00:09:44.754, Speaker A: The counterparty or the person that executed that swap is the Nomada Validator Set. It is not some specific person. It is the entire chain sort of wanted to do a specific swap on Osmosis. That swap gets executed, the assets get returned from Osmosis via IBC again, and they enter the shield set again, at which point the user has full control over those assets again. And there are some downsides here, which is specifically due to latency. If you start thinking on what does this actually mean under the hood, this means at least one IBC message means at least like one or two blocks confirmation on the origin chain, as well as one or two blocks confirmation on the receiving chain. So you're talking on like maybe 30 seconds latency here.
00:09:44.754 - 00:10:36.286, Speaker A: This is not instant. If you're on a system that has sort of like 1 second block times, it's going to be hard to get the same kind of latency guarantees as soon as you want to hop between multiple chains. But the really cool feature now is that you have shielded swaps and none of the core systems needed to actually change what they did. Osmosis provided swaps and Nomada provided privacy. But we can start integrating these systems using IBC in order to actually have very complex user flows, where we can start in a really modular way, combining all these different systems in order to get something that is like, from a user perspective, much more useful than individual components. Let me quickly also walk through the architecture, how this actually works. This is sort of the heart of Namata is the multi asset shield pool.
00:10:36.286 - 00:11:27.838, Speaker A: If you're familiar with the Sapling circuit on Zcash, this will look very familiar to you. You can really, at a high level, think, we've added an extra identify an extra field to the node types in the Sapling circuits in order to have an asset Identifier so that all assets can share a unified shield set. What happens in Sapling actually is that you can instantiate Sapling for other assets other than ZEC, but I can identify as an external observer which sort of shield set you interact with. Whereas in Armada, an external observer can only see that someone was interacting with the shield set, but they don't know what actions were taken. This could mean that I transferred this to an address that I control myself. This could mean that I transferred this to an address that someone else owns. This could mean I wanted to execute some shielded swap somewhere else, or I wanted to submit some data to Celestia in a shielded manner.
00:11:27.838 - 00:12:21.330, Speaker A: So this is really powerful, actually, as an abstraction, because it can work with arbitrary assets. Yes. And so the basic idea is that you can sort of go into Namata transparently, then go into the shield set, do whatever you want to do in the shield set, go back out if you want to, take your assets back out, and then go wherever you want to go via IBC again. Yes. So this is sort of the entire architecture, and I won't go through the entire sort of slide. But just to highlight a couple of things here, namata comes with cubic slashing, which actually means that if you should be really careful which validators, you select the high percentage of a validator that commits an infraction, the higher the slashing rate rises exponentially. So if you ever been slashed on Cosmos, for example, slashes are constant.
00:12:21.330 - 00:12:49.742, Speaker A: If 30% on Amada commits a fault, slashes are going to be tremendously high. So there's really now an incentive to actually start splitting your delegation across many different validators that run in different security models. Yes. No matter on the hood, just use common BFT so you get fast finality BFT, you get all the IBC integration. The state machine isn't using a custom SDK. It's written by hand and rust to implement a bunch of the Scriptography. Yes.
00:12:49.742 - 00:13:36.794, Speaker A: And sort of we also have a custom Ethereum bridge, mostly because we don't have IBC adapters to Ethereum yet. If anyone in the audience wants to do cool work, please work on integrating IBC into Ethereum. It's not that difficult, but it just requires engineering effort in order to get it done. And this is also true, really, like the way the interchange ecosystem or the multi chain ecosystem is going to work is we should be building IBC adapters for other systems. For example, there's no reason why something like near or Polkadot or any fast LTV model on BFT wouldn't be able to speak IBC. Yes. Now, coming to shielded data availability, how does Shieldda actually work and why is it useful? Again, the model is roughly the same thing, which is a user is in the Shield set.
00:13:36.794 - 00:14:07.640, Speaker A: They want to interact with Celestia. They want to be able to post some data to Celestia. The user has to generate zero knowledge proof, ideally on an edge device within Namata. This instructs the Namada validator set to effectively submit data to Celestia on behalf of that user. And again, the interaction that the world observes is that the Nomada validator set submitted data on Celestia. This could mean that there's an individual person. This could mean there's sort of a hundred people behind it, but the world doesn't know.
00:14:07.640 - 00:15:36.702, Speaker A: And when you start thinking on what kind of roll ups you want to be building, this is actually becoming incredibly important, especially for privacy preserving roll ups. If you have privacy preserving roll ups where individual users want to be part of this roll up and have to submit data to Celestia, it means that in the current model, without Shield data availability, they would need to figure out how to safely submit those individual transactions to Celestia without revealing who they are. So in the current model, even if you had a privacy preserving roll up on Celestia, users would need to be incredibly, incredibly careful. And this is almost impossible to achieve that they can sort of privately pay transparent fees on Celestia with no matter, Celestia doesn't have to worry about this problem anymore and how you get private fee payments working. Rather, we can say, well, we have these two systems that each do their unique thing, but by combining them, you can start submitting using the feature sets of both effectively. So whatever feature set Celestia may even offer in the future using Shield DA, you can always submit that data privately on Celestia. I think there's also an interesting other aspect to this where privacy has a lot of benefits, but it also has sort of its complexity on an end user, because all of a sudden, data moves from the center of the network, right? Like when you think of something like Ethereum data, all lives in this globally distributed, globally readable database.
00:15:36.702 - 00:16:58.186, Speaker A: In a privacy preserving world, this data starts moving to the edge devices, where individual phones need to hold this data. So one big question is, are users going to store all this data themselves, or are there going to be services where users can, for example, offload encrypted nodes, too? And this is, I think, sort of a future direction that I'm not exploring too much in this talk, but that is really interesting to think about. For future research on how can we leverage data availability solutions to provide guarantees to users that they can get back their data, for example, their encrypted notes, which they will need in order to access their funds. An interesting other research area here is oblivious message passing. Or how can you query a data source like Celestia in a privacy preserving manner? Because one of the, in my opinion, the most valuable data set, most likely in the space is actually Infuria, because everyone has access to all the rights to the chain, right? Like if you scan Ethereum, you will see all historic rights. But Infuria is like one of the only companies in the world that has access to 99% of all the reads, which is the much more difficult to obtain data set in the world of blockchains. And so with oblivious message passing, we can actually start thinking on how do we have reads against full nodes that do not reveal exactly what kind of key in a merkel tree I'm looking for.
00:16:58.186 - 00:17:49.402, Speaker A: As in like right now, I go to a full node and go, I would like the value of this key. With oblivious message passing, you can live in a world where you query a range of keys and it's not clear to the full node exactly which key I wanted to ask for. Yes, this sort of leads into the modular thesis a little bit on we should be splitting concerns. Not every chain needs to do everything. There's a clear benefit to standardization and specialization if and only if we have decent interoperability protocols that can actually make this specialization still composable. So I think we're clearly going to head into those direction and sort of outside of execution, settlement consensus and data availability. I think we're also going to see more specialized chains around privacy guarantees, around sort of transferability tradability.
00:17:49.402 - 00:19:03.646, Speaker A: I think we're clearly heading in this direction, but this becomes much easier to build for effectively as an engineer, because instead of writing everything, like if I think about this in terms of software, instead of writing everything as one large monolithic mono repo, we start writing individual libraries that individual other systems can start consuming. Yes, and this is sort of, I think, one of the last slides, privacy as a service. Namada is going to start providing this where you can use Namada to drive actions on systems that you actually want to use for other reasons, but you want to retrofit as a personal choice privacy into those systems. For example, maybe some people want to trade a lot, maybe other people want to use Celestia a lot. Namada actually doesn't care. Nomada, just wants to provide really good privacy guarantees for whatever users want to do on other systems. And I think we're going to see this more and more because there are lots of interesting ideas being built and very few people have, or very few teams have the amount of cryptography engineering required to make these systems private, as in making every system inherently private is a very difficult task.
00:19:03.646 - 00:19:31.342, Speaker A: Because all of a sudden you need to start thinking, like, how do I build proof of stake fully, privately? That gets very, very complicated. It's very interesting, but it gets very complicated. And so with privacy as a service, we can rather say we can build individual applications and individual DApps, and we can retrofit privacy into them in a very transparent way for users. Yes. If you'd like, there are more resources here. Discord twitter the nomada testnet. Yeah.
00:19:31.342 - 00:20:35.380, Speaker A: Thank you very much. I should leave the clicker here. Great job, Adrian. All right, next up we have Magnus, co founder of Skip, talking about sort of modular user experiences, and definitely like, an increasing trend of people who think about mev now coming around to being able to sort of apply that mindset to improved UX. You're taking this? Hey, everybody, nice to meet you. So I'm Magnus, first time public speaking for Skip, so we'll see how it goes if I crash and burn. But when I was a kid, I really liked this game, Neopets.
00:20:35.380 - 00:21:29.524, Speaker A: Anybody played that before me when they were a kid. Yeah, so, little game. You basically raised these animals and you did things with them. And I remember the first time that I tried to play Neopets, I went home and I just typed in Neopets into my browser, and it didn't work, right, because I missed the www dot and I missed the and I was confused. I didn't understand, why didn't this thing just work? I went back to my friend, I asked, how do you actually put this thing into your browser? And then once I learned, I was like, okay, I guess I just have to do that. That's just what you do, right, in the Internet. And then comes a time like later in your life when you understand the Internet, right? You understand TLS and you understand DNS, and you understand the name servers and how they all route together.
00:21:29.524 - 00:22:19.380, Speaker A: You probably get a little bit too lost in the sauce, so to say. Like, you understand it maybe too well, but the Internet still works, right? There's still that kid who just does that, puts it in and everything just happens. Understanding it isn't required for making the Internet work. That is to say, the Internet has great UX, right? You don't have to understand it at all for it to actually work. And I'm going to transition that into sort of like IBC, which I would argue has shit UX. Should be clicking, maybe. There we go.
00:22:19.380 - 00:23:14.352, Speaker A: IBC is shit UX. Trust me on it. And I'd say this is sort of the same thing for crypto in general, right? You really have to understand the tech in order to make it work. You have to understand wallets, you have to understand the Bridging protocol. Has anybody ever done an IBC transfer by chance? And I don't just mean, like using Osmosis, which is like easy mode. I mean, sort of like having to put in your channel ID and understand what paths it has to go through and basically having to know the underlying protocol in order to actually use it. So one of the really annoying things about Ibcux is that you have to understand this concept of pathing, right? So if you want to go from one chain to another chain, you might not be able to take the most direct route there.
00:23:14.352 - 00:24:05.990, Speaker A: So, for example, in this case, if you want to go from Alice's source to Alice's destination, you can't go directly there. You actually have to unwrap it by going initially through the token origin, right? And so it's very different than the Internet in that you really have to understand the underlying protocol in order to use it properly. And it makes sense why it's like this, right? Like, you're picking up the security assumptions of each underlying intermediate chain. Therefore, you have to unwind it through those to sort know, basically have it become the token that it once was. So you end up with this situation, right, where you have all of these IVC denoms all over the place. They're all interwoven with each other, just like, look at all those tokens, right? What are they all doing? Why are they there? I don't need them. I don't understand what they are.
00:24:05.990 - 00:25:02.088, Speaker A: So I love this quote by Chris Goes. It's, do you think God stays in heaven because he, too, lives in fear of what he's created? It's obviously not an actual Chris Goes quote, but it is from Spy Kids, and that's sometimes how I feel when I'm using IBC. So we tried to solve this. We tried to make basically what that Neopets like, experience is for IBC, where you don't have to think know, the Www, the TLS, the DNS, the name servers. You just use it, right? You just use IBC in the way that you want to, right? So this is that same issue, right? So starting on chain A, you want to go to chain D. Every path that you go through basically will change the token, right? It's like you and your friend are basically on the Upper East Side. You want to go to a bar together.
00:25:02.088 - 00:25:38.656, Speaker A: I take the four train. You take the six train. Does different number of stops, and you end up as different people, and you're just like, what's going on? That's essentially what happens to IBC tokens as they traverse the interchange. Different paths will actually change the token. And that, of course, does not intuitively make sense, and users will lose their funds if they don't follow this, right? They will end up in a denom that maybe has no liquidity and there's no decks, and maybe they don't even have the gas token. And so, basically, interchange UX is screwed by all of these different problems. And it's true for Cosmos, you guys can learn from Cosmos.
00:25:38.656 - 00:26:19.312, Speaker A: We've done many different things, and we've screwed up a couple of them, but when it comes to the roll up world, it's going to become ten times worse. Absolutely. Because you'll have way more pathing and you'll have way more gas tokens, and you expect users to understand all of that and have all those gas tokens. So basically, one does not simply send over IBC. You can't just use the protocol. You have to use it very specifically. How do we fix this? Well, basically, we need to do unwinding, right? So this concept of if I want to go in this case from chain B to C, I need to first be unwound through chain A.
00:26:19.312 - 00:26:59.352, Speaker A: And I can have somebody tell me, okay, here's a transaction that will actually unwind you through chain a or wherever you need to go automatically so that you end up in the right place and abstract that into something super easy to use. Basically, abstract that into a browser URL so that all the routing, all the DNS, everything else just happens underneath the hood. That's what we've built at Skip. Basically, this is somewhat of the structure of the API. The user gives the intent to transfer. We send back this multi hop IBC message. Oftentimes it does a massive unwinding flow.
00:26:59.352 - 00:27:59.920, Speaker A: The front end sends back the user signed receipt, and then they get a success. And so all they see on the front end is, basically, I had my token here on this random chain. I don't know how wound it was I wanted to go there, and maybe it's even a different token, right? And they just end up with that experience. But wait, there is more. So the ideal experience in our minds, and this might not be right, but this is what we believe is you want to go in the future over IBC from any chain, any token to any chain, any token, and that, of course, might require a swap, right? So now you've further complicated this workflow where you're not just routing the same token, you actually have to, at some point change into a different token. And you want to do that in one click, right? You don't want to have the user have to go to different applications and figure them out and have to use them. So you want to just have this sort of all done in one transaction.
00:27:59.920 - 00:28:52.240, Speaker A: So this also works with the Skip API. The way that we figured this out was there was some credible middleware tech that was built inside of IBC called Packet forward Middleware, which allows you to once you get to a chain, it will automatically forward it to the next chain, and you can chain this together. So you have multiple chains in a row, and then you use these things called IBC hooks that allow for arbitrary smart contract execution on any intermediate chain. So in this case, oftentimes what will happen is three unwinding flows, a swap on Osmosis, and then more unwinding all in one transaction. So this is basically what it looks like if anybody's interested in trying it out. We built this fun little front end called IBC fun, and it's just what you think it is. Basically, it will show you the different routes and swaps that it's trying to do and then it all completes in one transaction.
00:28:52.240 - 00:29:59.524, Speaker A: So I don't know if this is going to play, maybe looks like it's not going to play, but this is a demo of me actually doing it, going from Osmosis to a different token and just seeing sort of like how fast it is. It went over five different routes, which know in IBC quite complicated. And then it all sort of completes extremely quickly. So we're trying to figure out these other UX problems that we think roll ups and IBC chains already have, right? And we argue that these main ones are these gas token issues where you have to have so many different gas tokens and that's like a big ask for the users to have this concept of sponsored payers. So basically you can subscribe to a wallet and they'll just figure out some of these issues for you. Maybe they give you like a gas tank model where you have all your gas in one place and they'll just swap it out into whatever you need. Best price deck segregation, and then also instant cross chain settlement.
00:29:59.524 - 00:30:46.584, Speaker A: There's been a lot of conversations, I assume, about intents. I don't know what they are, but some people seem to, and how those might solve UX in the future where you might just be able to give this intent of this massive cross chain workflow and then a third party actor might be able to come in, solve that intent and basically give you whatever you need on the destination. So if you're interested at all, the API is free. It's something that we made sort of as a public good. I think it might be technically interesting to just delve into sort of like the reality of IBC and what you can do with it. This is a problem that will be true for all bridges, this unwinding and winding problem. It will be true for hyperlane, it will be true for many others.
00:30:46.584 - 00:31:33.690, Speaker A: And if we want to have a permissionless IBC standard or permissionless bridging standard, we really need to fix these issues, right? We really need to turn the internet or we really need to turn IBC into something like the internet, where you don't have to think about all of these different problems. You can know. Type in neopets and that's it. Thank you so much. Appreciate it. All right, that was excellent. Now we are going to pivot to a panel on bitcoin where we have Eric, Sonny, Connor, SEM and John talking it.
00:31:33.690 - 00:32:31.460, Speaker A: Yes. All right. So I don't know, has there been a lot of panels. About Bitcoin or any talks about Bitcoin so far at this conference? Has anyone seen any Bitcoin talks? Not specifically about bitcoin. So Sonny, I want to go to you first. I saw that you wrote a completely insane tweet a couple of weeks ago. I want to give you a chance to elaborate on that and feel free to introduce yourself.
00:32:31.460 - 00:33:54.136, Speaker A: Also, you wrote that proof of stake was a mistake. Was that a joke or is that something that you it was a little bit of just in a moment of frustration, I guess, but it was mostly just around like I was frustrated with staking derivatives at the time. Where I just saw that we built or how these staking derivatives protocols are evolving is we're effectively turning our systems into proof of authority systems. And because the staking derivative providers are just basically hand selecting the validators and I was like, we spent so much time designing security mechanisms into proof of stake and slashing and making sure there's bonding programs, we've just undid all of them. And what was especially frustrating was just the fact that core protocol developers are pushing these things and it's like, wait, why did we spend all that time on proof of stake? If we wanted to, we could have probably just built a proof of authority system into the protocol, like actually maybe have token holders use governance to vote in validators and stuff like that. So I think that was like the main point I was trying to get across. No, I think it's super interesting because we have in crypto lots of proof of stake systems and we have the largest cryptocurrency is Bitcoin.
00:33:54.136 - 00:35:24.940, Speaker A: It uses proof of work. And I don't think that any one of us can for sure know how the economics of these proof of stake systems are going to end up and how decentralization due to staking derivatives are going to impact these systems. So even though you can have proof of stake as your preference, you can think it's much better than proof of work. But it's kind of nice that there's a proof of work system there is there, that we can use as a backup plan if there turns out to be some issues with proof of stake. I think though, at least when I look to myself, if I think about Bitcoin and I think about how is that system going to reach, like mass adoption or scale, it feels like the Bitcoin community to today have significantly tied themselves to the mast of the lightning network. So the lightning network was invented in a white paper in 2015 by Taj Dryja and Joseph Poon that basically connects a bunch of payment channels together and leverage that system to route payments across the network. And I was listening to a podcast with David Marcus, who was the president of PayPal previously and then worked for Libre for a while and now has a company called Lightspark where they are trying to make Lightning work.
00:35:24.940 - 00:36:55.400, Speaker A: And one of the things that he said is that in Lightspark they have a functionality called Lightspark Predict, which uses AI in order to figure out how you predict which path you should try in Lightning to make a payment. And I would kind of say that if your payment system requires AI in order to just find a path that you can send a payment that doesn't sound like a functioning payment system. Is there anyone here that sort of has? Do you see necessarily a future for Lightning? Or do you think that if we want bitcoin to work, do you think that there is actually a good reason if we want to take the biggest cryptocurrency out there and we want to make it usable for payments, is there a reason to perhaps explore other L2 technologies on top of bitcoin? I will have an opinion about that. So I think Lightning and payment channels, generally speaking, make sense for high throughput kind of use cases. Fast gives you really fast high throughput payments. And I think it would be good for payments that the whole world doesn't need to know about. Because with any blockchain, whether it's a layer, one blockchain or a roll up, you kind of have to tell everybody in the network about this payment, and they have to store it forever.
00:36:55.400 - 00:38:02.440, Speaker A: And so for payments like maybe machine to machine payments or other kinds of micro payments, something like Lightning or payment channels, generally speaking, I think maybe makes more sense for that kind of use case. But yeah, it still remains to be seen what that model really gets product market fit for. I agree with that, especially because I don't know about Lightning technical problems and the progress on that too well. But definitely when we're talking about in the context of roll ups on bitcoin, at the end of the day, bitcoin has a maximum of four megabyte data throughput every ten minutes and there's a limit of even if you're leveraging a roll up, there's a limit of number of transactions you can compress squeeze into that. As a result of that. Lightning definitely serves a pretty interesting purpose, especially in the case of bitcoin. Payments are the original use case for cryptocurrency.
00:38:02.440 - 00:39:00.732, Speaker A: It's the least speculative proposed use case for cryptocurrency. And payment channels have some amazing properties. They can be extremely low cost, they are good as finalized instantly as soon as you receive it, you don't even have to wait for a block time. Even though in Ethereum and the rest of the blockchain scaling, research has sort of started to focus on things like roll ups now and we sort of left behind payment channels, those properties are lost on roll ups and it's certainly worth trying to make the technology work, if we can, to get them back. To be fair, if you can scale data throughput to the levels that your payment network can leverage it really well. And we know that we can scale data quite well in a way that we can't scale execution. So we can still have similar throughput systems with data scaled DA layers like Celestia.
00:39:00.732 - 00:39:58.016, Speaker A: But especially in the case of bitcoin, you just can't do that. So, sonny, do you care about bitcoin at all these days? Is it something that I know that you work in the cosmos ecosystem? And is there a vibe in that community that bitcoin is a lost cause? Or are there actually people that care about improving bitcoin and coming up with solutions to improve the system? And how do you feel about it personally? So, for me personally, I spent a lot of time thinking about bitcoin. I got into cosmos because I wanted to build the app layer for bitcoin. I was working at consensus for a little bit, and I was like, they're doing all this ethereum stuff is cool, but what is this ETH shitcoin? And we got to build for bitcoin. And bitcoin is an app chain. I was like, oh, okay, good. This is simple.
00:39:58.016 - 00:40:38.944, Speaker A: We're going to do payments and issuance here. But then blockchain was the one who came up with the idea of sidechains, and it's never really shipped right. And I just saw cosmos was the place where people are actually building side chains and app chains. Okay, we're going to issue bitcoin on the bitcoin blockchain, and then the BTC asset is going to flow off into the cosmos ecosystem and be the money of this ecosystem. And so that's kind of five years ago. That's how I got into cosmos, then got busy with shipping and building all the products and stuff. But now I think it's time for I personally really want to shift cosmos back towards that original mission that was there for a lot of us.
00:40:38.944 - 00:41:33.584, Speaker A: And so I really do think that the ethereum ecosystem, it has this base money and it has this huge ecosystem of applications around it. I do think that the cosmos ecosystem has the second biggest ecosystem of applications, but we lack a base is like I do think the future of cosmos is to figure out how to bring bitcoin and build this application ecosystem around bitcoin. That's why we've been working with we're working with axlar to build a property centralized bitcoin bridge that bridges not just bitcoin, but it'll bridge ordinals it'll bridge BRC 20s. We're talking with the Lightspark people about making so you can bridge via lightning and stuff. So I do think that I personally think that bitcoin is the future of cosmos. Obviously, cosmos is a very decentralized distributed group of people and community. So different people have different views, though.
00:41:33.584 - 00:42:03.212, Speaker A: I love that answer. Is it possible that I could get you to wear a bitcoin builder hat? 100%. Thank you. We used to have a culture of innovation and fun and building in bitcoin. I don't know. John, do you want to wear one too, I gladly accept. I think, honestly, the whole ordinals thing was really exciting to me.
00:42:03.212 - 00:43:27.910, Speaker A: Not because of the technology behind it, it's not that crazy or anything, but just the fact that it brought this renewed excitement to the bitcoin community again. And I think it's the first time in years I've seen bitcoiners be excited about things. And I think it's just restarted this conversation that I can reasonably see that at some point in the next two years, we're going to see more soft forks into bitcoin, get new protocol upgrades, which I'll admit I did lose faith in the rate of development on bitcoin core in the last couple years. And I do feel this sort of renewed excitement and cultural shift in the community, honestly, a big part thanks to the work that you and ud have been doing. I've heard a lot of people say that there's sort of a renaissance of building culture coming back to bitcoin. And one of the reasons that I want to talk about bitcoin rollout specifically here is because we have this massive modular ecosystem now where there's executioner environments and there's proof systems and there's all these different tools that's being massively invested in innovated upon that. If bitcoin can just tap into all that innovation and try to bring some of that home, then we could sort of leapfrog the bitcoin space several years into the future.
00:43:27.910 - 00:44:35.420, Speaker A: So on the topic of bitcoin roll up, I think maybe it's a good idea to sort of give a lay of the land so that people understand, are roll ups on bitcoin even possible? We've had this conversation in different formats and different podcasts, like 15 times, so I think we can just summarize it super quickly. So in bitcoin, what you can't do, for example, is that if you have one transaction that says x equals five in another transaction, you can't multiply that value by two, so x becomes ten. Bitcoin doesn't have, like, state and variables and values in that sense. The only thing that bitcoin has is unspent transaction outputs. So what you can do in bitcoin is you can reference an unspent transaction output, you can combine it with a script, and now you have a new unspent transaction output, but you can't really reference other variables in bitcoin. So you can't really build, like, a roll up on bitcoin today, but what you can do with bitcoin so, for example, we put JPEGs on bitcoin. So that means that bitcoin works as a data availability layer.
00:44:35.420 - 00:45:28.744, Speaker A: All you need in order to build a sovereign roll up on a blockchain is that you need some space to put that data blob. So in the context that we're talking here, initially, we're talking about using bitcoin as a data availability layer in sort of the modularity sense. We can build sovereign roll ups from that. And then there is this whole body of discussion around, okay, what upgrades would be necessary at the bitcoin based layer in order to enable roll ups. And then I think the consensus is that we would need some way to sort of store state on the bitcoin chain, either through a covenant construction or something else. And then we would need a way to validate a fraud proof or a validity proof. So we would need, like in bitcoin right now, for example, you can't look at the merkel route and verify that a value belongs in that merkel route.
00:45:28.744 - 00:46:09.672, Speaker A: You also can't validate a serial knowledge proof. Bitcoin is like a super primitive calculator. You can only do like, add two values together and you can't even store that value anywhere, but you can use it as a data availability layer. So we're going to talk a little bit about sovereign rollups. So for those of you who don't know the difference between a sovereign roll up and a real roll up, like Arbitrum or StarkNet or optimism, it's basically that the sovereign roll up doesn't have a bridge between the layer one system and their L2 system. Maybe we shouldn't even use those layer numbers anymore. I think that the modular space is trying to move away from that.
00:46:09.672 - 00:47:11.980, Speaker A: But you basically can't bridge bitcoin trustlessly into the second layer, the bitcoin asset, and bridge it back. But you can use bitcoin as a data availability system to store a bunch of transactions and execute the state from that and create expressive layers on top of bitcoin, basically. So if we put ourselves in the situation, that okay. We think that roll ups on bitcoin are interesting, and the first step to get there is to build a sovereign roll up and then build some type of bridge, some trusted or crypto economic bridge into that second layer. I think it's more interesting than going back to the conversation, like, how is it possible to build bitcoin roll ups? It's more interesting to talk about. Okay. Given the tools and applications that are out there today, if I gave you like, a challenge and I said, okay, well, by the end of this year, we need to have a first sovereign roll up on bitcoin.
00:47:11.980 - 00:48:09.984, Speaker A: If you have that as your mission right now, that that's what you have to do, and you will get fired if you don't deliver on that. What tools would you leverage from the sort of the existing domain of modularity or whatever in crypto in order to reach that goal? We can sort of start here and then make our way to the end of the row. Okay, perfect. To start, I want to deemphasize the idea of real roll ups versus sovereign roll ups. Sovereign roll ups just don't need a smart contract or any execution at the base layer to verify your fraud proof or your validity proof. But that only means that at the start, without verifying that bridge, it doesn't have that bridge. However, the security guarantees of a smart contract roll up and a sovereign roll up are the same.
00:48:09.984 - 00:49:01.520, Speaker A: We just create a proof of the sort that we read all the data from the base layer and we processed all the transactions correctly later on. These sovereign roll ups can also have bridges, so you can still derive all of the bitcoin security and build an application on top of it. And the only thing that you would be lacking is the ability to bridge bitcoin. And to be fair, that's a big missing point about the stack that I would use to build sovereign roll upon Bitcoin. I'm extremely biased. At Sovereign Labs, we work on Sovereign SDK. So there are already teams working on a Bitcoin adapter to make sovereign SDK, build roll ups, deployable to Bitcoin.
00:49:01.520 - 00:49:42.044, Speaker A: And so I would definitely go with that. And I would choose our ZK option. We provide both optimistic roll ups and ZK roll ups using ZK single round proofs or ZK proofing, everything. But I would go with ZK because the block times on Bitcoin are quite long, so you don't take the full advantage of being an optimistic roll up. So you already are working with a team that is building an adapter to leverage bitcoin as a data availability layer. Correct. So maybe to you, Connor, perhaps you could give a little bit of an intro to what Rollkit is.
00:49:42.044 - 00:50:27.276, Speaker A: And I think that are you also working on a similar problem? Yeah, roll Kit is also an SDK for provisioning sovereign rollups. It's meant to be quite customizable. You can put in any kind of virtual machine that is compatible with the ABCI interface, which should be any VM you could conceive of because it's a very flexible interface. Rollkit also wants to support all different kinds of proving systems. We have fraud proofs for Cosmos SDK already. We hope to add different kinds of ZK proving schemes as well. And also we have an integration into bitcoin already.
00:50:27.276 - 00:51:07.736, Speaker A: We developed a implementation of the DA interface for the Bitcoin taproot witness. That was done by Javed Khan on my team who should be here and unfortunately couldn't make it, but we miss him. And as for design considerations for a sovereign roll up on Bitcoin, what features would you want? What token would you use? What kind of VM would you pick? Ideally, you would use bitcoin. I think bitcoiners. Want to use bitcoin. I don't think they want any shitcoins. But you can't trustlessly do Bitcoin up to a sovereign roll up like we talked about.
00:51:07.736 - 00:52:00.036, Speaker A: So would they accept some sort of trusted bridge? I'm not sure. If the answer is yes, then that's what you should do. You should have some sort of trusted bridge or ecosystem of trusted bridges to get bitcoin up there so that they can use the money that they believe in. If they don't want to trust a bridge or a committee to attest to those bridge transactions, then you got to make sure the token on the sovereign roll up is something that aligns with bitcoin. Do something without a pre mine, do something without insiders, maybe even AirDrop it to the bitcoin UTXO set. When you launch the roll up for features, there's a bunch of things you could do. You could add payment channels, you could give a lightning network to this thing and it would have more capacity to open and settle those channels than bitcoin does because I think it has more block space with the four megabyte taproot witness.
00:52:00.036 - 00:52:26.690, Speaker A: That's enormous. That's bigger than not bigger than a celestial block I can't remember, but it's pretty huge. You can do a lot on there. You could add privacy to it. You could add zcash style privacy that bitcoin base layer doesn't have. And it would have the security of bitcoin. A sovereign roll up is basically an app chain, but with close to the full security of whatever it's deployed on.
00:52:26.690 - 00:54:21.984, Speaker A: So I just want to stop here for a brief second and reflect over the fact that there are two modular builders here that have already built hooks into the bitcoin protocol to start leveraging that as a data bank. So all of that stuff that is happening, like out there in the ethereum world, in the cosmos world, in the modular world, because we have these hooks into the bitcoin base layer as a data storage, all of that stuff can start creeping back into the bitcoin ecosystem now. So like bitcoin builders, they're going to get challenged by this whole other ecosystem that has been practicing and building applications for these layers for years now. Sunny and light, do you guys see any risks with that? Is it maybe in bitcoin's best interest to just stay as pure and as far away from all that staking derivatives and mev and sero knowledge proofs and complex math? Is there a risk that as we bring the innovation that has happened sort of outside the bitcoin space, that we also bring with us the issues? I think the point is that you keep the bitcoin core chain as simple as possible. There's a few upgrades I would like to see mostly around making bridging a little bit more trustless. But beyond that, I think all this stuff should be experimented with at the edges on these new chains. Yeah, from the research that I've seen, mev tends to stay on the layer.
00:54:21.984 - 00:55:38.640, Speaker A: That where the transactions are actually executed. And so if you have roll ups on bitcoin, the sequencers might be capturing mev, but there is not really opportunity for the bitcoin miners to get involved in that directly as the mining network. And so I'm not too worried about mev in the context of either sovereign roll ups or even like L2 roll ups on bitcoin. And like sunny, I think the changes that we want to make to bitcoin would be relatively minimal just to enable trustless bridging. And then beyond that. I think that the application design space that is opened up by Rollups is huge and would be really valuable to Bitcoin. Whether it's more private transactions like Connor was talking about or whether the focus is on Scalability because you can do massive witness aggregation into zero knowledge proofs and get transactions down to just, like, a dozen bytes or so, which would be huge for throughput and improving scaling.
00:55:38.640 - 00:56:51.396, Speaker A: So I'm not too worried about it. In terms of your original question about the stack, I think what these guys are building, it would be a really great foundation for a sovereign roll up. I also think that there are interesting possibilities that are opening up. Like if we don't have trustless layer, two bitcoins or bitcoin bridges, we do have things like crypto economic bridges, like what Threshold Network has built or nomic, which is a bitcoin side chain in the Cosmos ecosystem. Once you can get bitcoin onto, let's say, nomic, you can use IBC to connect bitcoin to any other Cosmos chain. And so you could either build like a modular system there where you move bitcoin into the cosmos, and then you could use Celestia as a data availability layer to have an even more high throughput chain on top of that. So I think there are a lot of possibilities that are either already here or just around the corner as these systems start to mature.
00:56:51.396 - 00:58:21.984, Speaker A: So Sonny, I saw that you were nodding a lot. Is there any connection between this and sort of is there a way to connect IBC to bitcoin using a roll mean? I've thought about what would IBC on bitcoin look like, and really the point of IBC is to bring the trust assumptions of your bridge as close to the trust assumptions of your consensus protocol as don't know. I think the direction that I'm most interested in right now is mostly around drive chains. I think it's basically like I think drive chains are the proof of work equivalent of IBC, where it's saying, hey, let the bridge be operated by, instead of the validators of your proof stasicum, but let it be operated by the miners of your proof of work system. Yeah, I know that we have at least three fans of drivechains on this panel. So I want to talk about something slightly different, which is that I've at least made an observation that if you look at the roll up space in the ethereum world these days, you see that for example, optimism has the Op stack and ZkSync came out recently with a ZK stack. And at the ECC event, StarkNet announced the StarkNet stack.
00:58:21.984 - 01:00:55.504, Speaker A: I think what is basically happening is that these different roll ups that exist on Ethereum today are trying to modularize their specific stack so that other builders can deploy app chains and other instances of the same rollup, which we have seen with example, with Binance launching BNB Opstack and Coinbase launching Base, which is built on Opstack. And I think that the reason that these teams are doing that is because they want other builders to sort of use their tools so that everything that they build, everything that Coinbase builds on base, everything that Binance builds on this BNB Opstack chain will become compatible with sort of the mothership. So everything that happens there, optimism will just be able to integrate with no friction at all. So I'm thinking that is that a trend that you guys have seen also that these sort of stacks are emerging? And is that maybe something that we could leverage for Bitcoin? Like could we do something like taking Opstack and putting it on Bitcoin? Could we take ZK stack and putting it on Bitcoin? Could we take the StarkNet stack and putting it on Bitcoin? And is that a direct orthogonal thing compared to what you guys are building with these SDKs that are building sovereign roll ups on Bitcoin? Or are there synergies here? Like could you take ZK stack and then use roll kit to put the data into bitcoin? Or how could you take a stack like that that I think may emerge as really powerful software frameworks? Could we leverage those with the types of tools that you guys are building so you can totally take these stacks and modify them to work on Bitcoin? The key part is rollkit and sovereign SDK is inherently being built to be like for sovereign roll ups to start and then you can add bridges to them. And that makes it really easy to integrate it with Bitcoin because we're just using bitcoin as a pure datability layer with optimism. Even with the integrations op stack, even with the integrations they did to Celestia so far, ethereum is the settlement layer and then Celestia is like the data layer. They still require that smart contract interaction.
01:00:55.504 - 01:01:50.840, Speaker A: So to derive full bitcoin security would not be possible. At least unless they do heavy modifications to the Obstac in a way that might currently might introduce security vulnerabilities, because they're trying to stay as close to geth as possible. But over long term, it's definitely possible. But they need to get out of the smart contract roll up framework. I'm actually personally a little bit less interested on using Bitcoin as a data availability layer for everything. I see. The Bitcoin proof of work is actually a really good data availability system because what it does is you put data on there and miners are incentivized to broadcast it as widely and as quickly as possible because they want as many people mining on top of their block as possible.
01:01:50.840 - 01:03:04.510, Speaker A: So it's really good as this incentivized fast propagation layer, but also as this hard timestamping layer. And I think trying to put all your data onto Bitcoin, to me at least right now, seems like not a good use of Bitcoin block space and resources. But I think putting the types of data that you want this sort of hard timestamping for is what's more important. So I'm an advisor to Babylon and what they're doing is say like, hey, okay, what can we actually benefit from putting on here? Why don't we put our proof of stake block headers on here? And that way we get this strong timestamping system and we basically make it so we can by putting proof of stake block headers on there, you basically can solve these long range attacks that you get in proof of stake, get the best of both worlds of proof of stake with proof of work. And I think being a little bit more smarter of what kind of data, I think different data availability solutions have different benefits and being smart about what type of data you put where. And so, yeah, block headers, I think that belongs on bitcoin. Putting your entire block data, I don't see the point.
01:03:04.510 - 01:03:45.512, Speaker A: Yeah, bitcoin ordering is very valuable to people. We've had stacks which is this thing that checkpoints its blocks onto bitcoin, and Babylon, which is doing something similar. And when rollkit announced the bitcoin sovereign rollups, a lot of people were confused on what the difference is. And the difference is rollkit puts the full blocks in the witness. But kind know what's the point of that. It's not so clear. Data availability is talked about a lot in regards to roll ups, sovereign and settled where there's an on or off chain light client where you have a commitment to the results of the state transition and some sort of proof that it's correct.
01:03:45.512 - 01:05:03.030, Speaker A: And then you want to verify somehow that the data is available. And if you can verify that a lot of data is available without needing to download all of it, then it can be described as very scalable. Bitcoin doesn't have sampling, so if you want to verify data availability for bitcoin, you have to download the whole block that's the whole four megabytes. So it's not like celestial where you can verify the whole block is available with only some small samples of it. So there's a good argument that it's really the ordering that we care about and not necessarily the data availability. And I like what you said previously, Connor, that if we wanted to build a roll up system natively for bitcoiners, for the bitcoin ecosystem, maybe you would try to do this even without introducing a new token. So are there other considerations, like if you were to build specifically for the bitcoin ecosystem, are there particular use cases that would maybe be more interesting to build for bitcoiners? Like, are there perhaps something on the privacy side? Maybe there are some things that doesn't have all that negative connotations that some bitcoiners are worried about.
01:05:03.030 - 01:05:59.088, Speaker A: Let's say we had an executioner environment today that had expressiveness that was secure, either had the full data on bitcoin or had some of the data but was basically secured by bitcoin and used the bitcoin native. Asset. What would be some interesting use cases for that? We start with you, John. Well, assuming a good trusted bridge for BTC, I think the use cases that are already popular on bitcoin, just being able to do those, maybe in a more scalable way. So like a lightning channel management roll up where opening, closing, rebalancing your lightning channels is like super cheap. That could be interesting. Privacy is also, I think, a significant pain point for bitcoin users.
01:05:59.088 - 01:07:23.052, Speaker A: If you've ever tried to make your bitcoin private, there's these guides on the internet that are like ten pages long and if you screw up any of the steps, then all of your work is for nothing. And so having like a zero cash just easy button for privacy that just encrypts all your transactions I think would be pretty useful. And then can't underestimate, I think, the utility of Bitcoin DFI. It's a pretty big use case. I think over 1% of the bitcoin supply is being traded or lent or borrowed against on different chains. And so, yeah, just doing that in a way that where you also get the full double spend security of bitcoin through the Bitcoin DA settlement layer I think could be useful to a lot of people. What about like a bitcoin backed stablecoin, like USD that's backed by bitcoin? Do you think that could be something that would be interesting to bitcoiners? I think it could be because right now the stablecoins that bitcoiners have to use are like fiat stablecoins backed by money in a bank somewhere.
01:07:23.052 - 01:08:08.752, Speaker A: And so if we could actually use stablecoins that are backed by bitcoin in a smart contract instead, I think that would be better. We actually built one of you can kind of consider it like a proof of concept of this on the Rustock side chain, it's the sovereign dollar, it's a bitcoin backed stable coin, and that's with over collateralization. Over collateralization. We're not getting into like a terra luna situation with this where it's going to end up with billions of bitcoins getting sold because there's an algorithmic peg that sort of detaches. Yeah, it's over collateralized. There's like efficient liquidation mechanisms, so it always stays over collateralized. At least that's the design right there could be a flash crash and maybe black swan event.
01:08:08.752 - 01:09:02.720, Speaker A: But yeah, it's not like a partially backed stablecoin, it's a fully over collateralized stablecoin. But being able to do that on a chain that gets the full double spend security of Bitcoin, unlike root stock today, which has its own independent security budget, can be reoriently of Bitcoin. I think it brings that kind of utility in a more, I think, bitcoin native way. Totally. But also, to be fair in any yes, these roll ups will introduce general purpose programming to bitcoin, like using bitcoin security. And I think the bitcoin crowd, as I've seen at Bitcoin Miami, is extremely excited for that. But at the same time, since Bridging bitcoin will definitely introduce new trust assumptions.
01:09:02.720 - 01:10:11.048, Speaker A: Any Bitcoin backed stablecoin will definitely not have the attractiveness as it would have had without any trust assumptions, for sure. I think what's a bit more interesting, something that excites me, is that in a sovereign roll up, since you're reading the whole data space of bitcoin and applying rules from there, you could also read into USDT transactions and represent USDT trustlessly on the sovereign roll up itself and introduce liquidity in that way and build a DeFi ecosystem. I'll be a little bit contrarian. Maybe we shouldn't do a general purpose VM. Maybe you just want enshrined DeFi so that way you can avoid smart contract bugs. The fear of smart contract bugs, the upgrade authorities that people introduce as central points of failure to deal with those bugs and have something that's more bitcoin aligned and without all the things that cause bitcoiners to push back so hard against DeFi on Ethereum. That's such a great point.
01:10:11.048 - 01:11:36.636, Speaker A: I actually wish that we had spent some more time talking about that, that when we're talking about building ZK roll ups and optimistic roll ups on Bitcoin, we don't necessarily need to take the entirety of the EVM. We could actually go back to that thing that Bitcoiners used to say, that we're going to analyze what all these other altcoins do, and then we're going to take sort of the best parts, the things that we really like about them, and we're going to integrate them back home. So maybe we wouldn't necessarily want to take all the complexity of Solidity and the Ethereum virtual machine and try to cram that into Bitcoin, not only because there's sort of bugs inherent with that, but also because it's really difficult to prove the EVM if you're using zero knowledge proof. It's basically a system that's hostile to be zero knowledge proof. And there are other virtual machines out there, like Cairo, for example. I had a conversation with Vitalik and asked him, what do you think would be the best sort of virtual machine and language to run on Bitcoin? And I don't want to put words in his mouth, but I think he sort of was trying to point me towards Cairo as a better language to be serial knowledge proven. So we don't necessarily I think it's sort of a mistake if people think that we're talking about taking all the complexity from these smart contract ecosystems.
01:11:36.636 - 01:12:21.490, Speaker A: There are sort of narrow versions of that that could be more focused on privacy or could be more focused on payments only taking those with that, we're kind of out of time. Is there anything that you guys feel that you want to say before we wrap this up? Okay, that's it? All right, thanks, everyone. Thanks for coming to the panel. If you guys are interested in building on Bitcoin, I'm definitely in the business of hearing your story, wanting to introduce more builders back into Bitcoin. I also have a company we're hiring. So if you want to build roll ups on top of Bitcoin, try to find me after this panel. And thank you very much for coming.
01:12:21.490 - 01:13:27.000, Speaker A: That was fantastic. Are we actually going to do something with Bitcoin or are we just selling it to BlackRock? All right, we'll let the room cycle. Bitcoin was very popular. Who would have thunk all right, we have Ethan Buckman up next, talking about modulism and money design, co founder of Cosmos, CEO of Informal systems. All right. Thanks, Zucky. Thanks, everyone.
01:13:27.000 - 01:13:57.602, Speaker A: So here we are about to talk about the future of France, and all these losers just left the room, so they're going to miss out, but lucky for you guys. So yeah. So I'm Ethan, if you follow me or know anything about know I'm really into money monetary design, trying to figure this stuff all out. When I told me we've been working on a new project that we call collaborative finance, or Ko Fi, it's our response to DeFi. When I told Nick I want to talk about Ko Fi, he said, well, that's not really on topic. You should talk about something else. So I just changed the title to modularity and monetary design.
01:13:57.602 - 01:14:25.070, Speaker A: And of course he was then thrilled. Now, the thing Nick probably didn't even understand was that Ko Fi really is about a modular payment system design based on a system of intents over which we optimize with graph solvers that publish ZK proofs of integrity. So if that's not on topic, then what is? Right. All right. Hi. Do I have a clicker or something? Yeah, don't break the third wall. Is that what they call it? Don't smile with the photographer.
01:14:25.070 - 01:15:11.914, Speaker A: All right, so I've long felt that money is the killer app. It's the ultimate multistakeholder computational use case, and yet we are still failing to actually have the impact oh, I'm supposed to stand where? Right here? All right, thank you. And yet we failed to have the impact we'd all like to have. Right? And so I'm going to talk a little bit about that. If I can get my clicker working, which I can't, technical difficulties. Just keep talking. All right, so I believe what's on the next slide is a quote from me that says, if home is where the heart is, money is where the payments are.
01:15:11.914 - 01:15:31.294, Speaker A: Because that's what I think money is for. Money is for payments. And we can talk about the modular structure of money, right? Everyone's familiar with the three functions of money. The unit of account is for denominating debt. The medium of exchange is for discharging debt here and now. And the store of value is for discharging debt elsewhere or later. It's a very clear formulation.
01:15:31.294 - 01:16:15.034, Speaker A: It'd be great if we had such a clear statement of the modular functions of blockchains. What I like to point out is that what I like to point out is that between these different modular functions are tensions right? And when we're building modular systems and modular designs, part of what they do is they allow us to focus in on the tensions between the different functions. So it's not enough to just say, okay, these pieces are modular, and so we've got this modular thing over here and that over there. There's real tensions between the functions that we need to tune to. And so I call the tension between the unit of account and the medium of exchange liquidity, which is much of what we're going to talk about today. And the other I'm writing a blog series about all this, so you can check it out, but I won't go into it because I'm probably going to be running out of time. At some point, I'm going to need my slides to work.
01:16:15.034 - 01:17:06.238, Speaker A: Otherwise, I'm just going to start happening. All right. I believe the next slide says something like, okay, armed with a unit of account, we can denominate unlimited structures of liability, right? I can say, you owe me, you owe whoever, right? We can denominate all this debt. But then the problem is how do we ensure that we can actually discharge it? All right, you go, you create all this debt structure, and then how are you going to discharge it all? This is the fundamental problem of the banking system, of money, of payments, of finance, that all this stuff is sort of sort of built on top of. Right? And next slide, it's a good thing I actually practice. So we call this problem liquidity, right? The ability to ensure that you can actually discharge your debts. That's what liquidity means.
01:17:06.238 - 01:18:00.034, Speaker A: Liquidity is not how much slippage you're going to have when you trade one shitcoin for another, right? It's not, you know how many assets are available on some AMM? We call that liquidity because the purpose of trading is to acquire an asset so that you can discharge a debt, right? If all you're doing is trading just to trade monkey pictures or to see the value of your assets go up, well, you're not doing anything real in the real world. The whole point of exchange and everything we're building ultimately needs to be in service of discharging debt because that's where we ground ourselves in the real world and real trust relationships and so on, right? So we need to keep that in mind. Now, next slide, the way we solve or address the liquidity problem. Look, they're so pretty, right? Here we go. So we did this slide. We did this slide. Just dwell on that.
01:18:00.034 - 01:18:27.818, Speaker A: Meditate on this for a while, okay? This is pretty fundamental stuff. Yeah, we did this slide. We are here. Okay, so what's a payment system? We define a payment system as a set of obligations, right? These are the debts plus a liquidity source we can use to discharge those obligations. Right? Now, if you think about that definition for a minute, you might be like, well, wait a minute. All these things I call payment systems fintech, banking blockchains, all of them are just about the liquidity source. They're just about moving assets around.
01:18:27.818 - 01:19:00.740, Speaker A: None of them actually do anything to represent the obligations, right? So none of them are actually satisfying this definition of a payment system, which is a problem because if we're going to build the next generation of payment systems, we need to address this. Okay, so let me briefly give you a critique of existing payment system. Now, we're working on a white paper for this Kofi stuff. It's really what I'm presenting here that'll be coming out in the next two weeks. TM first of all, we'll cover this very quickly. None of these payment systems are designed from first principles. They're all just like cobbled together over the years, responding to various problems.
01:19:00.740 - 01:19:31.350, Speaker A: It's all just about asset transfer and exchange. But there's so much more to the core problem of payments and finance and what we're doing than asset transfer and exchange. We have to actually look at the underlying obligations. We're talking about exchange. So much of exchange today, both in the banking system and in the real world sorry, in the banking system and in the blockchain world, it's all about market making and dealers. That's what everyone's trying to do bring dealers on board, AMMS, liquidity, all this stuff. That is a new model that developed after the collapse of the banking system in the 16th century.
01:19:31.350 - 01:20:09.238, Speaker A: Now, I promised Tebow I wouldn't talk about the 16th century today, so I'm not going to. If you think that's great, like, oh, great, he's not going to talk about history, you're part of the problem because we all need to learn a little bit more history. And fundamentally, if we're responding to this issue of central banks, we all think, oh, we're going after central banks. Central banks emerged in response to the collapse of a prior international banking system that ran in the 16th century, that cleared all the trade credit in Europe without almost any money at all. We need to understand that. We need to understand what happened, why it failed, and how central banks emerged out of that chaos to lead us from a world of clearing to a world of liquidity providing. Right? Basically, since then, society has been structurally short volatility.
01:20:09.238 - 01:20:44.982, Speaker A: All these dealers, all these market makers, they're short volatility. That is not a way to set yourself up for sustainability. It's a way to compound moral hazard at the heart of the system. So obviously they constantly fail because nature is not short volatility. Right? And so at some point you blow up and what happens? Central bank shows up, prints a bunch of money, bails you out. Right? So that leads us to the fourth problem, which is or the fourth element of the critique, which is the problem of issuance. Any payment system needs to grapple with the problem of issuance, whether you're a blockchain issuing to miners, whether you're a new currency like osmosis issuing to liquidity providers, or whether you're a central bank in a banking system doing issuance when you make loans or when you bail out the whole system.
01:20:44.982 - 01:21:35.874, Speaker A: And so what we want to do is try to understand issuance in terms of the underlying obligations, right? One way to think about it is that, hey, when a bitcoin miner does some proof of work, they have shipped a service to the blockchain and now the blockchain owes them. The blockchain has a debt to that miner, which it then discharges by issuing new bitcoin, right? And so by thinking that way, kind of opens up new possibilities for thinking about the problem of issuance. We think that's important because fundamentally we're going after two questions here, right? One is how do we design a payment system to optimally discharge debt? And two is who should have the right to issue and according to what principles? Right now, a lot of people in the blockchain space are focused on problem two. And there's a lot of concern around tokenomics and issuance and how we do all this stuff. But there's almost no concern at all with option one. No one's talking about how we discharge debt, right? Hopefully we can start to change that. Hopefully you don't walk out of here being like, oh, he's crazy.
01:21:35.874 - 01:22:05.914, Speaker A: None of that matters. And you're like, I need to focus on this debt discharging problem and issuance in that context. Because right now, who controls the issuance in the world? It's the banks. When they make loans, they issue new money, and it's the central banks. When they bail out the banks, they issue the base money. What we want to do is restore the capacity to issuance to individual communities, right? And in order to do that and do that responsibly and sustainably, we need to ground it in the actual network structure of debts. Okay? This leads us to ultimately to what we're building, which we call collaborative finance.
01:22:05.914 - 01:22:47.834, Speaker A: And collaborative finance we think of as a sort of semipermeable membrane between the real economy and the financial economy. It's that layer in the middle. The yellow MTCS is our core algorithm multilateral trade credit setoff. Of course, that's a well designed marketing term that users are going to love. But the whole idea is that we can protect people in the real economy in the bottom layer by focusing on the structure of credit and obligations, but we can still connect into the larger capitalist system so we don't have to overthrow everything at once. We can sort of do this iteratively we can build a platform that allows lenders and capital providers, liquidity providers, lending protocols, et cetera, to plug in, but still do it in a way that's responsible and takes care of the real economy. And that's really what Copy is all about and what we're building.
01:22:47.834 - 01:23:15.810, Speaker A: Okay, so let's actually talk about the design of the system this is the background. Even though I didn't talk too much about history, I'm going to briefly just talk about the missing primitive, right? The main missing primitive is the obligation, right? The debt. And we're going to use balance sheets here. The world would be a better place if everyone spent a little bit more time learning accounting and thinking about balance sheets. So we're going to do some very basic accounting here on stage. So on the left we have Alice and Bob. We have their balance sheets, assets and liabilities, right? Not very complicated.
01:23:15.810 - 01:23:41.226, Speaker A: This is a simple asset transfer, right? Alice has $20 in assets. She transfers it to Bob. Now Bob has the assets. There's no liabilities involved. There's nothing happening. This is what you do every time you send a payment to someone else, right? On the right, we have what an obligation looks like, right? So now there's a liability involved. Alice is declaring that she owes Bob, right? So she has a $20 liability to Bob, and that corresponds on Bob's balance sheet to an asset.
01:23:41.226 - 01:24:25.738, Speaker A: Right? We might call this, if you manage books at a company, accounts payable, accounts receivable. Very simple stuff, right? The reason I'm pointing this out is because we're going to build much more complicated structures that are networking these different obligations together. But there's two things I want to highlight here. One is when you do an asset transfer between two parties, right, from Alice to Bob in general, we can say that is in service of discharging a preexisting debt, right? I mean, okay, it might be a gift, but almost, almost every time. Alice already owes Bob money, and that's the reason she's going to send him the $20. Right? Now, we're not representing that liability here. We don't represent it on our blockchains, but that's exactly what we're missing, right? And that's the whole point of what we're trying to do, to bring those obligations on chain so that we can do something more constructive with them.
01:24:25.738 - 01:24:51.454, Speaker A: The other thing I want to point out is that declaring an obligation, what we're doing on the right in Part B, is really a permissionless action. It's very much like sending money. I can send Gary Gensler some shitcoins, and there's nothing he can do about it. If I have his address, I can also declare that I owe him money and there's nothing he can do about it. If I say I owe Gary money, then who is he to stop me, right? So it's a sort of permissionless declaration. It is an intent, if you will, to pay. Okay? So now we have these obligations.
01:24:51.454 - 01:25:13.994, Speaker A: We put them together into a larger network, and you get complicated structures. You got single obligations, chains, cycles. If you've seen anything about Kofi, if you're thinking about this to yourself, you're like, hey, wait a minute. If I can see the obligations and there's obligations in a cycle, then we can clear any obligations that are in a cycle, right? And you can do that without any money at all. I owe Gary, and Gary owes I don't know, janet Yellen. And Janet Yellen owes me. And we can just get together and say, hey, we all owe each other.
01:25:13.994 - 01:25:28.622, Speaker A: Let's just do set. Don't need we don't need any money. We can just do set off. And that's what's possible when you start to open up the structure of the obligation network and you look at the obligations, you can actually save everyone cash flow liquidity. You can clear more debt with less money. That's what this is all about. Yeah.
01:25:28.622 - 01:26:10.374, Speaker A: Okay, so once you have all these obligations, how do you actually discharge them? And generally speaking, there are four ways to discharge or to settle obligations, right? Almost all of modern finance is built on the fourth one. It's called novation. It's when you actually change the structure of the payments graph, right? You do securitization or factoring or clearing houses. You introduce intermediaries. You sell your debt, you factor your debt, right? All of this the economists claim this is all in service of reducing risk and making things more efficient. Obviously, all of that was bullshit, and we saw that very clearly in 2008 when this whole house of cards came tumbling down. What we want to do is as much as possible as we can using the existing structure of the payments graph before we even consider Novation.
01:26:10.374 - 01:26:24.258, Speaker A: And it turns out there's quite a bit you can do. You can do set off, which we just talked about. That's clearing, right? I owe you, you owe me. Let's just do set off. Or we can do it in cycles, arbitrary size. There's assignment, which is just normal transfer. We'll use a fancier word because that's what we like to do.
01:26:24.258 - 01:26:57.062, Speaker A: I transfer you money, that's assignment, it discharges a debt. But then there's a very interesting thing you can do, which we'll call overdraft, right? This is where you say, draw on a credit line. Someone makes some credit line available to you, and you can draw on that credit line to pay someone. So one way you can think about this is assignment is what you can do if you have a positive balance, right? You have a positive balance, up some asset, you can assign some of those assets to someone else and use that to discharge debt. Overdraft is what you can do if you have access to a negative balance, right? Someone says, okay, I will allow your balance to go negative. And so your balance can go into negative territory, you can draw on that to pay someone else. This is what happens when you take out a credit line from a bank.
01:26:57.062 - 01:27:23.486, Speaker A: It's basically a negative balance. It's what happens when you have a mutual credit system. In both of those cases, you actually have money creation happening, right? When you have a credit line from a bank and you draw down on it. The bank is actually creating new money. It's expanding its balance sheet and allowing you to use that new money to make payments. And banks are allowed to do that because they have licenses from the government. But why should they get to have all the fun, right? We want to seize the means of production of money back from the banks for communities and figure out a way to do that responsibly.
01:27:23.486 - 01:27:43.750, Speaker A: And that's really what this is all about. But we can also do overdraft without money creation, right. A pool of investors can come together and say, okay, we're going to pool our Bitcoin and our Osmo and our atoms into some pool here. We'll do it over IBC. We'll put it in this little COFI chain, and we will make it available to people to draw on to pay their bills. Right. And we can do it because we're going to have access to this network structure of obligations.
01:27:43.750 - 01:28:05.790, Speaker A: We can do it in a much more intelligent and optimized and risk reduced way. So it's important to keep these different things in mind. The last thing would be Novation. We're not going to focus on Novation because we want to avoid it. We want to do as much as possible of setoff assignment and overdraft before we hit Novation. Okay? The last thing we need to put it all together is, as promised, the system of intents. Okay? So we already touched on obligations.
01:28:05.790 - 01:28:21.026, Speaker A: We sort of beat them to death. Obligations are an intent to pay. That's kind of obvious. Yeah. So then to really round this out, we need tenders and acceptances. So a tender is an intent to use some liquidity to discharge your obligation. So I have access to some dollars or some bitcoin or some atoms.
01:28:21.026 - 01:28:44.282, Speaker A: I think I upset Sonny here. I didn't pump Osmo enough, so I have access to some Osmo. Maybe he'll stay and I say, I'm willing to use my Osmo to discharge my debts. Wouldn't that be great, Sonny, if people could use Osmo to pay their bills? And then an acceptance is the flip side of that, saying, I'm willing to accept some Osmo. I'm willing to accept some Adam or know, if you will. Right. Right, Nick? Yeah.
01:28:44.282 - 01:28:52.930, Speaker A: I'm willing to accept some Tia to discharge charge debts owed to me. Right. That'd be great. I like Tia. I like Nick. I like what they're doing at the modular summit. And so I'm willing to accept some.
01:28:52.930 - 01:29:32.046, Speaker A: So you can start to collect all these different intents, all the obligations, the network structure, the different tenders people are making, which means the different kinds of liquidity they're willing to use to pay their bills, all the acceptances, the different kinds of liquidity people are willing to accept in payment of their bills. And we can put it all together into a graph structure, and then we can run a solver over the whole graph. Right. And we can run solvers that draw on decades of advanced theory and algorithms in graph theory, essentially to find optimal results. So we can actually optimize to discharge the most amount of debt with the least amount of money while respecting everyone's preferences, right? Solving over a system of intents. You all love it. All right, so this is what it ultimately looks like, right? So we represent we have our graph here.
01:29:32.046 - 01:29:59.522, Speaker A: We have a bunch of obligations. We have at the bottom, this VB, this is our liquidity source, right? You can imagine it's Osmo, it's Tia, Adam, whatever you like. And we have the A on the left, that's the assignment source. And the O, that's the overdraft source, right? So those are positive and negative balances that people are able to draw on. And then on the right, we have repayment and deposit, right? That's where the money goes back to after it's sort of used. It either needs to be repaid or redeposited. And basically we can use the same algorithm we use to find cycles.
01:29:59.522 - 01:30:27.502, Speaker A: We can use the same algorithm with liquidity to find even more cycles, right? So the addition of liquidity into the network, even a little bit of liquidity, a small amount of tenders or a small amount of acceptances allows us to essentially close more loops, right? So more people can benefit from their debt being discharged. Okay, there's a lot to digest in this slide. You'll have to read the white paper. We'll go into a little more detail here. Here we have two different liquidity sources we're going to put overdraft aside. We'll just focus on assignment here, right? So we have AOS, b O's E. We have C.
01:30:27.502 - 01:30:43.198, Speaker A: Right? So an obligation chain. Now we have access to fiat, we have access to crypto, and we have B. B doesn't want to use dirty fiat. He doesn't want to use shitcoins. He's only willing to settle in gold bars. But he's happy to do set off, right? If you owe him and he owes you, he'll do set off with you. But he doesn't want your dirty fiat.
01:30:43.198 - 01:31:08.746, Speaker A: He doesn't want your shitty crypto. He only wants gold. Now, what's going to happen? A has a tender that says, well, I'm willing to use fiat to pay my debts and C has an acceptance saying willing to use fiat to discharge my debts. D is willing to use crypto and so is E to accept crypto. And what ends up happening is B benefits from both of these things. So B is benefiting from A and C using fiat, and from D and E using crypto without even knowing about it. D doesn't want anything to do with them.
01:31:08.746 - 01:32:05.562, Speaker A: He doesn't have to touch either of them. All he's going to handle is a setoff notice that says his debts are reduced, right? And his debts are being reduced because other people are using fiat and other people are using crypto, right? This opens very profound new ways to bring. Crypto into the real economy, where so long as there's a small number of people willing to tender crypto and accept crypto, others can benefit from that without having to touch it at all, so long as they're willing to touch a setoff notice, which is just an accounting entry. Okay? So this offers really new ways for crypto to impact the real world economy, to be used in real world payments, and to benefit real people without them even having to touch it, which is, I think, a nut that we haven't been able to crack, really, until now. This is another example. This one's a little bit more complicated, but also quite profound, where we were not able to close the loop within fiat or within crypto, we have to combine. See, on the previous slide, we didn't need both liquidity sources, right? If we just had fiat, we'd close the loop ABC.
01:32:05.562 - 01:32:53.822, Speaker A: If we just had crypto, we'd close the loop DBE, right? In this slide, we're not able to close either of those loops. We need to use both crypto and fiat together. And because the difference here is basically in the previous one, DOB, you can see there's not the same overlap, right? So here you actually need both sources of liquidity to be able to discharge everyone in the path. And so you're able to complete a new cycle over both sources. And so the more liquidity sources you bring in, even with a small amount of tenders and acceptances, assuming you have a large obligation graph, you're able to discharge way more debt, which is very, very powerful. So ultimately, you put this all together, you get something like this, right? So here's, based on real data simulated with three different liquidity sources, we could say one of these is fiat, one of these is crypto. One of these is mutual credit.
01:32:53.822 - 01:33:34.090, Speaker A: That's the Holy Grail. Mutual credit is communities being able to issue their own debt, sorry, their own currency to discharge their debt, right? And you get this stunning visualization of how everything flows in the network so that all the debt, as much debt as possible, can be cleared with the least amount of money while optimizing over and respecting everyone's preferences for which currencies they actually want to use. With that, I'm perfectly on time, which is kind of amazing. Thank you so much. I'm buck manster. You can learn more Kofi informal systems and check out the white paper when it's out. Thank you, Ethan.
01:33:34.090 - 01:34:09.030, Speaker A: All right, up next we have Ismail From, co founder of Celestia. You should all have heard of it. And he's going to be talking about the past, present, and future of the Celestia architecture. Hello, everyone. I'm ismay Kofi, co founder and CTO of Celestia. I'm going to talk a little bit about the architecture of Celestia, how it currently is, and give, like, a glimpse into the future. Whoop.
01:34:09.030 - 01:34:57.794, Speaker A: They're not mine. How do I fix this? Okay, first, so on the agenda. I wanted to talk like two parts. The first part is I'm going to recap the Celestia architecture. And in the second part, as I said, I'm going to talk a little bit about future directions. So it's not like a full fledged roadmap, but what could be on the roadmap next year or so. Okay, so first of all, what is Celestia? I think most of you know, but for those who don't know, I going to explain it from a high level, but also how it's implemented roughly.
01:34:57.794 - 01:36:21.680, Speaker A: So Celestia is like the first modular blockchain network. And what it does is it decouples consensus from execution. So what does this mean? So in classical monolithic blockchains, usually how every chain works is a client verifies two things, right? It verifies that the header has consensus, and it also verifies that all the transactions are valid, right? Like it executes the transactions, makes sure the state transitions are valid. And then in Celestia, though, it only verifies if the transactions are available, it still validates that the Heather has consensus, right? But it doesn't execute the transactions. It doesn't validate that what is posted on Chain is actually valid. It just ensures that these messages that have been posted on chain are available, right? They are OPAC blobs to Celestia. Like, every application can post their blob onto Celestia, their message, and it's just ensured that these are ordered, there's consensus on the order, and then everyone can verify themselves that these transactions are available.
01:36:21.680 - 01:37:47.370, Speaker A: So that's the main. And I'm going to explain a little bit on how this works as well. You might ask where does the execution happen? And the execution in Celestia happens in so called roll ups, right? Instead of this world computer model where all the transactions are validated on one chain, the state transitions are validated on a roll up. And you might ask how does that work in practice? The Light clients, for instance, to validate that a header contains only valid state transitions, there's two approaches to this, which is like via fraud or validity proofs, right? In the Ethereum world, these fraud or validity proofs are posted on Ethereum, or they're settled on Ethereum. But in Celestial, as there is no execution layer whatsoever on the main layer. These can be on a different layer, like on a settlement layer, or they could be on the peer to peer layer. I just wanted to stretch this as many people still perceive roll ups as enshrined with an enshrined settlement layer like on Ethereum.
01:37:47.370 - 01:39:35.550, Speaker A: So now that we covered how execution works, how the main chain works, I wanted to mention the four guiding principles with which we designed Celestia or goals that we wanted to achieve. So it should be possible that a block is deemed valid only by checking block availability, right? Which means you can verify a header as a light node without having to download the transactions. Just by verifying the availability, the second principle is application message partitioning, which means every application only has to download their data, right? Like they don't have to download all the transaction data of all applications, only the transactions that are necessary for their application. And then there's application message retrieval completeness, which is the same thing, which means that they can verify that all the application messages they downloaded, all of them and nothing is missing. And there's also application state sovereignty, which means that all they need to care about is their state transitions and basically their state portion. And they don't have to execute other transactions of other applications unless there's a direct dependency. So next I want to talk about how is this achieved roughly, I don't want to go too much into detail, but there's mainly three key ingredients here in play.
01:39:35.550 - 01:40:38.660, Speaker A: One is erasure coding of block data. Another one is a namespaced merkle tree and data availability sampling. So let's dive a little bit in. Erasure coding is a technique that is very well known, which is basically used in CDs, where you can recover data. So you add parity data and you can recover data even if parts of the data are missing, right? So how this works in Celestia is the data is encoded in a certain way, arranged in a certain way, the original block data is split into equally sized chunks. And then you add this erasure coding in this square construction that is depicted here, the details do not matter. What matter is that this enables basically that you can with a portion of the data, can always recover the whole data.
01:40:38.660 - 01:41:26.686, Speaker A: So here we also commit, when validators, create a block. They also commit to the erasure coded block and not to the original data only as it is with other chains. And so every row or column here basically forms a mercury. And these mercuries get committed to in a bigger overarching mercury. And that mercury is not like a simple traditional mercury. It is very similar though. The main difference is that a namespaced mercury is used which sorts the data according to their namespace.
01:41:26.686 - 01:42:31.446, Speaker A: Like each application can have their own namespace and the data gets posted on Celestia, gets prefixed with a namespace, right? Like it gets sorted. And this is the data structure that ensures the property that I mentioned earlier. You can only download your portion of the data, your namespace, and you get like a complete guarantee of that as well. So the third and most important ingredient that makes it possible for Celestia to ensure availability is data availability sampling. Right? So as I mentioned before, the block is erasure coded here, depicted slightly differently. And then light nodes or nodes or clients can download only a small portion, like 1% of the data due to that erasure coding, to guarantee like almost 100% certainty that the data is available. Yeah.
01:42:31.446 - 01:43:16.500, Speaker A: And so from a very high level perspective, this is how we built the system. We took tenement, right? And we modified Comet BFT called today. So Comet BFT previously Tenement, we took that, modified it, and we added basically the data commitment that I mentioned earlier. We also used the namespace Merkel tree for that data. And the third component that I mentioned, the data availability sampling, is done in a different layer, in a different peer to peer network. We call the DA networks, heavily reliant on bitswap and lip. P, two P.
01:43:16.500 - 01:44:01.198, Speaker A: Huge shout out to the node team that built this. No one has built this before. It's a huge effort. And yeah, that's a high level architecture overview. So let's go to the more interesting part, which is what are future directions for Celestia? How could a future roadmap look like? Or at least what could be some highlights here. So I mentioned this erasure coding, right? This is required for roll ups to ensure safety, right? Like in the sense that you have to be sure that the data that has published has not been tampered with. Like there's no validator.
01:44:01.198 - 01:44:58.070, Speaker A: This is like committing to garbage, essentially, or something else, or some invalid data. So how do we ensure this? Currently we have a fraud proof type that is called bad encoding fraud proof that every node can generate that downloads either the whole block or even a row, or like several rows are sufficient. They can verify that the data matches the data route and can generate a fraud proof. If that's not the case, that has the downside that theoretically light nodes need to have to wait for such bad encoding fraud proofs to not happen to ensure full finality of the block. Right. An alternative approach is to use KCG commitments. I think if you saw avails talk yesterday, that's a technique they employ which gets rid of the bad encoding fraud proofs.
01:44:58.070 - 01:45:57.690, Speaker A: The question is, can we do better than this? I think we can. So with an alternative approach here is to use a ZK proof of the erasure to use a ZK proof that proves to you that the erasure coding and the construction of the data route was done correctly. So you can still compute quickly, like the data route as before, and send around a proof if necessary if you require finality. Right? So that's a research direction. If anyone is into ZK proofs, this is something you could look at. I will later share a research forum link and the GitHub link. Another direction or another topic that we certainly will look at after launch is a trust minimized bridge, or like a two way bridge from and to Celestia.
01:45:57.690 - 01:46:58.854, Speaker A: So currently, if you wanted to use the base layer Celestia token in your rollup as a gas token, you have to use other bridges and to use other chains or third parties to get the token out and in to your roll up and back, right? Ideally, the UX of this would be more seamlessly, such that developers can use the Tia token directly. So one naive way to do this, one way to achieve this is you could enshrine a general purpose execution environment onto Celestia that acts as a bridge. Right. That has the problem that it wouldn't be really neutral in the sense that you have to choose an execution environment. And also you'd basically reintroduce. Yeah. Like you would tamper with the original vision, which is like having only data availability and consensus.
01:46:58.854 - 01:47:55.486, Speaker A: It turns out that you can do this without enshrining an execution environment. Mustafa came up with this idea where you basically leverage that ZK snark verification is more or less looks very similar to cryptographic signature verification. Right. You also have a public key. And from that, if you look at how ZK snarks work from the public key, it basically looks the same. So the user flow for how this would look like, roughly from a high level, is that a user would send a deposit of Tia to a verification key address. That's like a key address, like a public key or an address owned by a ZK program.
01:47:55.486 - 01:48:45.658, Speaker A: So you deposit Tia to there, it gets escrowed or burned, the transaction gets confirmed, and then the Tia would get credited on the roll up. And so you moved Tia into your roll up seamlessly. So this roll up would need to track the state of that verification tier. So the more interesting part is, like, how do you bridge back? You would send tier a withdrawal transaction of your tier in the rollup. Right. If the transaction gets confirmed by the state machine on the rollup, you generate a ZK proof. And that ZK proof kind of simulates or acts like a signature on the verification key address on Celestia.
01:48:45.658 - 01:49:17.000, Speaker A: So that would trigger the withdrawal of TL. That's very high level. The devil is in the detail. There's a lot of choices to made here, John would say. A lot of implementation details. And indeed, yeah, another hot topic is how to fix mev, or rather how to apply current state of the art mev research and techniques to the modular stack. Right.
01:49:17.000 - 01:50:05.206, Speaker A: There are several teams working on this. I think Skip and flashboards are the most noteworthy here. The specialty, or like, the difference here is that mev can trickle from the roll up into the base layer. Like, the base layer validators could delay or even sensor batches from the roll apps. Right. So the question really is how do we apply solutions to mev to more modular stack? Especially given that there's like a very heterogeneous application layer. I think Evan's Talk will cover this in greater detail.
01:50:05.206 - 01:50:40.974, Speaker A: Don't miss it. It's like this afternoon he will speak about how to break the proposal monopoly. Don't miss that talk. So this is like the long term goal here that we want to achieve is 1GB blocks, 1 million roll ups, and 1 billion light nodes. So often when we mention this to people, it's like 1GB blocks. That sounds insane, right? And it kind of does. But we won't achieve this immediately anyways.
01:50:40.974 - 01:51:23.920, Speaker A: It will be on demand. But we do believe that this is like as it becomes necessary, we will tackle this. And the question really is how. I think we are very confident that we can go to 100 or something very quickly. But to actually get closer to that 1GB blocks goal, we will have to optimize commit BFT's peer to peer layer. Right? Like the Mempool is not made for super large Blobs and the consensus reactor, like the gossiping mechanism is also not made for that. There's a lot of low hanging fruits and optimizations that we will do.
01:51:23.920 - 01:52:06.858, Speaker A: One of them is already started, which is like the content address pool, cat pool. Callum implemented this already. Similar, the node peer to peer layer needs a lot of improvements to achieve this. Mainly block sync and Dasync. There's a lot of optimizations to be done, mostly very implementation specific. It might turn out that in the future, in several years, we have to merge the DA layer and the consensus layer again, like architecturally, from a software perspective. And we will might have to employ a technique called internal node sharding to achieve the throughput on the consensus layer.
01:52:06.858 - 01:52:52.350, Speaker A: Another topic that I want to stretch is we want 1 billion light nodes. How do you achieve this? Right, so we want light nodes to run on all devices, like on all kind of devices. But I think the device we should target the next is the browser because this is what people are used to interact with in their wallets and they send transactions through MetaMask and everything. What we want to achieve is running celestial nodes, DA Sing Lite clients in the browser. So there's two approaches to this. One is having a different compile target for Celestia node Go implementation. Another one is having a rust implementation.
01:52:52.350 - 01:53:22.850, Speaker A: For the sake of time, I'm not going into detail, but if you want to hack on this, please talk to me. Yeah. There's plenty of other open problems and research directions. A few were already mentioned, for instance, by Yuma yesterday. The ZK quantum gravity bridge. Yeah. And I think one thing that last word that I want to mention is that we need to focus on developer usability.
01:53:22.850 - 01:53:51.290, Speaker A: And if you want to do usability research for developers in the modular stack, please also reach out to me. Yeah. You can engage here in our research forum and on GitHub. These are the main things. Thank you very much. Thank you very much. Ismail.
01:53:51.290 - 01:54:15.622, Speaker A: Yes. Gigabyte blocks. I want a gigabyte block. I don't know what I'll put in it, but seems fine. Next up, Sonny is going to be talking about mesh security. Hello. I in fact will not be talking about mesh security.
01:54:15.622 - 01:54:33.562, Speaker A: I'll be talking about some other stuff. But it'll show up a little bit at the end I think I've just been sending Mesh Security as the title for all the talks that I haven't come up with a talk for yet. And they're like, oh yeah, that sounds good. No, but I'm going to be today. Thank you guys for coming. My name is Sonny. I'm one of the co founders of a project called Osmosis.
01:54:33.562 - 01:55:05.714, Speaker A: And we're going to be talking a little bit about how Osmosis will be interacting with the modular ecosystem. So a little bit of background for anyone who's not familiar with what Osmosis is. Oh, yeah, that's better. Anyone who's not familiar with what Osmosis is. We are a Dex built on the cosmos. SDK. We have liquidity swapping ability.
01:55:05.714 - 01:56:01.574, Speaker A: We launched, like, contrary to liquidity very recently. You can do all sorts of have assets from over 50 different blockchains. On the Osmosis blockchain, we have over $100 million in liquidity. We have all sorts of pro trading tools, like basically the most advanced developed decks in the Cosmos ecosystem. And yeah, liquidity from all different places, both from within Cosmos and outside of Cosmos. Out of all of the assets, like four out of five biggest pools are from non Cosmos native chains, right? And along with just the decks itself, we have this ecosystem of applications really meant to build this full, unified DeFi experience on top. So beyond just your basic spot trading, you have your margin trading, you have your Perps, you have your launch pads, you have your Fiat on ramps, you have lending protocols.
01:56:01.574 - 01:56:40.658, Speaker A: So basically a full integrated DeFi experience. Everything. The goal here is you go to Binance.com, you have all these trading tools, you have all this things in one unified experience. Osmosis is going to provide that in a decentralized know as by being this liquidity center for the Cosmos ecosystem. We are sort of the center of IBC traffic today. So if you look know, on any metric by users or by IBC traffic by volume, osmosis is sort of the center of that and has found itself as this IBC hub and liquidity hub of the Cosmos ecosystem.
01:56:40.658 - 01:57:33.298, Speaker A: And so now, very soon, hopefully, we will have a new entrant to the Cosmos ecosystem, right? The Celestia blockchain will be launching. And as part of the Celestia blockchain launch, the goal of the Celestia blockchain node implementation is to be as simple as possible. Right? Obviously, we need this thing to with Celestia. We're going to have this beginning of all sorts of new modular chains and roll ups building on top of the Cosmos ecosystem. And there seems to be this thing about like, oh, app chains versus modular. And I think this is some weird dichotomy because really the whole point of app chains is this idea of modularity, right? It's saying like, hey, we don't need to put everything into one blockchain. We can break out what you need osmosis, we're trying to be a Dex.
01:57:33.298 - 01:57:57.600, Speaker A: We can say like, hey, we don't want to deal with Bridging. We are going to completely outsource that. That's not our problem. We're going to go find someone else to work with. We're going to go work with the Axlar team or the Wormhole team, right? We're going to go outsource Bridging completely. This is part of the modular idea, right? Or if it comes to things like data availability or if we need to base money, it's like, okay, we're not building that. We're going to outsource this.
01:57:57.600 - 01:58:52.586, Speaker A: It's all modular. So what I was saying was, yeah, the goal of the celestial blockchain is to act as this app chain and to be as simple as possible, because obviously we got to run it on our Kindles and keep the nodes as light as possible. And because of that, the Celestia code base is actually very minimalistic. You'll see that it actually imports very few modules, ishmael just talked about how can we avoid adding a general purpose VM or anything to this chain. So the one module that it does have, though, one of the few modules that it does have, obviously, is the IBC module, which is great, right? And so this is this decentralized, this permissionless Bridging protocol that allows you to send tokens data assets, anything between any IBC enabled blockchain. And so IBC is growing. It is built on the cosmos.
01:58:52.586 - 01:59:15.686, Speaker A: SDK. There's other chains as well. Now we finally have it on Polkadot, we have it on the Penumbra. And so, you know, IBC is slowly growing. But with Celestia now and this ecosystem, the modular ecosystem that's being built on top, we have a whole bunch of new frameworks being built, right? You have the Op stack now, right? You have Rollkit. You have the sovereign. S-A-K.
01:59:15.686 - 02:00:18.298, Speaker A: You have eclipse. And it is going to take us time to go and build out IBC for every single one of these, because the goal of IBC is to really build in your Bridging protocol at the same security level as your consensus system. It uses light client proofs, and it will take time for us to get there. Obviously, the goal is we want IBC everywhere. And there are people working on this, right? Like Polymer is working on adding IBC, like Zkibc to all of these sort of different stacks, but it'll take us some time to get there. And so right now, how do we do this? We're going to have to rely on other bridges, right? So the question is, how will all these roll ups and chains and sovereign roll ups and everything on all these different stacks access liquidity, especially Tia liquidity, right? Because they need that Tia to pay for their data availability. How are they going to get that Tia to there? Well, this is where Osmosis comes in.
02:00:18.298 - 02:01:05.430, Speaker A: So Osmosis is really expanding on its Bridging support. So obviously we support IBC today. We work very closely with the Axlar team. Wormhole just announced yesterday at Osmocon that they're launching Wormchain, which is going to be a Cosmos SDK based chain. And then the important one here is Hyperlane, right? So Hyperlane seems poised to be the first bridge that's available to a lot of these different stacks because of the way that it's been designed as this very modular Bridging system where you can choose what kind of security system you want. And so we are working on adding Hyperlane to the Osmosis chain because of its app defined security model. We can write it as a Cosmosm contract.
02:01:05.430 - 02:01:55.078, Speaker A: And with this, we're going to be able to use Osmosis as the connector between Hyperlane and IBC. So now you have this flow where finally you can say, hey, how do you get Tia from Celestia onto something like Eclipse or onto an Eclipse based rollup? Well, you can IBC from Celestia to Osmosis and then Hyperlane from Osmosis to and the idea here is that we're going to use Hyperlane to connect to all of these sort of different roll ups and get Tia onto them. Now, well, what do you do when you're at the center point? Know transportation routes, right? Well, you build a trading center, right. You build Constantinople, right. You're at the crossroads. You build a trading center there. And that's what we're building with Osmosis.
02:01:55.078 - 02:03:01.866, Speaker A: So we want to be the place where we build up as much Tia liquidity and liquidity for the entire modular ecosystem and provide these services and product that we've been building for the Cosmos ecosystem, but expand just beyond that. And so one of the things that I'm really interested in is protocol owned Tia, because if anyone remembers, a couple of months ago, the Arbitrum chain basically came to a halt because some dev account didn't have enough ETH in it to pay its data availability and stuff, right? So we should be moving to a world where no, you shouldn't be relying on some foundation or some dev team to be paying for your submission proofs to your DA to Celestia, right. You should be actually moving towards a system of protocol owned TL. And so there's a bunch of different ways that we can do this. So, one, we have this protocol called stream swap, which is basically auction system built on top of the Osmosis chain. And I think a lot of the new projects that are launching can do these stream swaps. It gives you this initial distribution, but you can do it for Tia.
02:03:01.866 - 02:03:41.814, Speaker A: So that way you have this initial distribution of your token. But now your protocol owns Tia in its community pool or treasury, and it can pay for its own DA without directly from itself. We also have built this thing for the Cosmos ecosystem called fee abstraction. And what fee abstraction is it's meant for transaction fees. So what you can do is you can take a chain like Stargaze, right? Stargaze is an NFT platform built on Cosmos. People can start paying their transaction fees in Stars, but they can also pay their transaction fees in any token. They can pay in USDC, they can pay in Osmo, they can pay in Bitcoin.
02:03:41.814 - 02:04:35.470, Speaker A: And what we've done with this fee abstraction module is Stargaze can accept these fees and all these different tokens. But what the chain will do by itself is it will send all the tokens to Osmosis, swap them to its own native token, and then send them back to the Stargaze chain. So that way they can be distributed as fees or burnt or whatever they want to do. So this module is available, it's available on our GitHub. You can go to lab fee abstraction, you can import it into your Cosmos chain today, and any Cosmos chain can start accepting fees in any token. Okay, what's the next step? The next thing that we're going to build with this is DA fee abstraction. How do you let all of these roll ups pay? Let's say they don't want their own protocol owned Tia in their treasury, but what you can instead do is say, hey, a chain like Manta can say, oh, we are going to hold our own native token, Manta, but we need to pay our DA.
02:04:35.470 - 02:05:24.122, Speaker A: They can send it to hyperlain, it to Osmosis, it will automatically swap it to Tia and send it to the Celestia chain in order to pay for the DA. And so we are working with some teams to actually build this out in production. Now, another way that beyond just for fee abstraction, with this whole modular idea, a lot of these chains, we work very closely with the Argus team, we can build this idea of outpost where they don't have to build their own Dex infrastructure for their own chain. They can actually provide things like swaps and everything over IBC. So let's see if this works. This is a demo that we built of the Cosmo swap, which is basically, this is a swap on the Juno chain. You're swapping Juno for Osmo, you make a single transaction on the Juno chain.
02:05:24.122 - 02:06:11.894, Speaker A: But what it's doing is it's actually sending the tokens to Osmosis, swapping them to Osmo, and then sending them back to the Juno chain. So basically Juno doesn't need its own native any. Let's say you have a Dao on Juno or whatever you're trying to do, you can tap into Osmosis's far deeper liquidity. And so the Evmos team has also built a similar version, the same, similar concept for the EVM. The Skip team know, you saw them present like an hour ago about IBC fund. You can basically swap from any chain to any token on any chain to any token on any other chain over IBC. And we'll be working on integrating hyperlane into the system as well, all through Osmosis anyways.
02:06:11.894 - 02:07:07.674, Speaker A: So these are some of the ways that Osmosis is planning on integrating more deeply into the whole modular ecosystem. One of the other things I'm. Actually really interested in is obviously I do have to talk about mesh security and how we want to incorporate data availability into mesh security. So the premise of Cosmos, and I think a lot of the module, like the thesis in a lot of the modular ecosystem, is this idea that every chain is the L1 for its own internal state and an L two for its foreign state, right? This is sort of the whole idea behind these sovereign roll ups. And so Osmosis is the settlement for the Osmo token, but it acts like an execution environment for any axle that is bridged IBC to Osmosis. And so as part of IBC today, we do these sort of light client proofs where, like, two chains are going along. You send an IBC packet that includes some sort of light client proof.
02:07:07.674 - 02:07:58.746, Speaker A: But what we want to do is eventually make know the security model between these chains better than just, like, client proofs. We want to start to incorporate things like fraud proofs. And finally, now, thanks to things like Rollkit, we actually have fraud proofs for the Cosmos SDK. Now, the thing is, fraud proofs have this huge time delay to them, right? You have to wait the entire challenge period. And the UX that people have come to expect in Cosmos is like very fast Bridging, right? You can IBC between two chains in a matter of seconds. And that's sort of one of the things that people really love about it. And so we don't want to build in this fraud proof window into the IBC Bridging protocol because it'll just massively deteriorate the UX.
02:07:58.746 - 02:08:46.762, Speaker A: But how we can use fraud proofs is in mesh security. So mesh security is this idea of restaking where you can say, like, hey, validators are going to give certain guarantees about they're going to be restaking their Osmo as part of this. What we've done is the initial version of mesh security is mostly about tenderman fraud proofs so it'll be able to detect, hey, okay, this validator did something malicious in the consensus protocol. We're going to slash them. But the way mesh security is implemented is actually as. It has this interface called a slasher contract. And you can write arbitrary slasher contracts, right? You can do it for things like, oh, we're going to do mesh security restaking for an Oracle protocol.
02:08:46.762 - 02:09:36.286, Speaker A: And if one validator's Oracle price is too far away from everyone else, that is a valid slashing condition. What we can also do is build Roll kit fraud proofs as a slashing condition, right? And so I think that eventually every Cosmos chain will start to use roll kit style fraud proofs, right? And so that's great. But then how do you make sure that you have these fraud proofs? The idea is that, okay, we're not going to block IBC Bridging on it, but if a Byzantine state machine sends an invalid packet in the future, they will be slashable for it. But to make them slashable for it, we need to make sure there's data availability. So this is what we can block for. We can at least block on data availability before accepting IBC packets. So let's say chain two wants to send some assets to chain one.
02:09:36.286 - 02:10:21.920, Speaker A: What it can do is generate a transaction. It will post the data onto something like Celestia or any DA system. And the validators of chain one will be running a data availability sampling like clients of Celestia. So in the validator software, we should be having this Dos like light client built into the software. And then what they will do is when IBC data is sent from chain two to chain one, the validators will basically use a vote extension to approve whether or not that they actually got they'll validate that the data is available. So they're not going to actually validate the fraud proof itself. They're just going to validate the data is available so that someone in the future can submit a fraud proof to the mesh security or contract if needed.
02:10:21.920 - 02:11:11.044, Speaker A: Anyway, so thank you guys so much for listening. And I hope you learned a little bit about Osmosis and mesh security and what we're building. Thank you. Thank you very much. Continuing our sort of theme of Cosmos themed panels, myself, David from Galileo, Ismail and Jack are all going to talk about the Internet of modular blockchains. Oh, and Ethan looks like yeah, there is an order. I'll take that.
02:11:11.044 - 02:12:02.944, Speaker A: Give me that one. Cool. Awesome. Happy to be here with everyone and with a few Cosmos OGS today, I kind of want to talk about the Internet of modular blockchains setting the stage in the past and talking about a little bit of history of Cosmos, which I'm sure Ethan will be happy to do because he loves history only from the 16th century. That's right. So fast forwarding from the 16th century a little bit. Cosmos came about somewhere in 2017 and set this vision for an Internet of blockchains, a future that would encompass thousands, hundreds, thousands, millions of blockchains.
02:12:02.944 - 02:13:08.004, Speaker A: And to accomplish that, they came up with the first modular framework for blockchains, which is a composable suite of software primitives in the Cosmos SDK, what's now Comet BFT and IBC so that we can live in a world of millions of interoperable blockchains. So what I'd like to do today is talk about the future of those three different components and how different new primitives like data availability and what's going on at Celestia with separating consensus and execution fit into what was the original Cosmos vision of a modular future. Maybe Zucky, if you don't mind talking about for a few moments the state of tenderment and where we see that fitting into this future. Comet. It Comet. Comet BFT, everyone. Comet.
02:13:08.004 - 02:14:15.500, Speaker A: Yeah, absolutely. All right. My main thought process is, in a lot of ways, this whole software stack was designed in an integrated way with interoperability as the end goal. So we picked a consensus algorithm that had an efficient light client that didn't need new advanced cryptography techniques or new advanced like zero knowledge proofs or new cryptographic primitives like BLS signatures, all of which were not very mature at the time, but would still give you high performance white clients and bridges. But we're also starting to see sort of the and so we built the system. I think I'm talking a little bit about lunch, about how the system was built with Bitcoin as its only real reference point. So how do you build a next generation system from Bitcoin now? We've gone a lot with Bitcoin was very much the inspiration.
02:14:15.500 - 02:15:15.968, Speaker A: We are now sort of way past that in so many ways and really seeing the limitations of that system. It's been a big breakthrough getting ABCI Plus Plus out the door, which was the first big change to the consensus API, and is a system that no one else is offering. So it is interesting that we still are able to push the limits of the possible on top of this stack, but there's a lot of work to be done. And where this stack and what Ismail was talking about right, the P to P layer was not expressive enough to build Celestia. So you had to build a Frankenstein of two P to P layers to build Celestia. And so as we kind of advance towards the future, I think we're all going to have to figure out sort of where this software goes from here and how it sort of more or natively encompasses these different use cases. Thank you.
02:15:15.968 - 02:16:06.592, Speaker A: So continuing to consider these pieces individually for the Cosmos SDK. Do we need to consider alternative hosts for the SDK? And how do we 100 x Cosmos with those primitives? Yeah want to call out Marco and his team who are doing great work on helping to expand the Cosmos SDK to help support more underlying data availability and other consensus algorithms. That works extremely important. And then on top of that, also more VMs. And when we talk about modular, this is what it means. You want to be able to pull the pieces out, plug new ones in and easily swap out those components. And I think that the sort of innovation of Celestia was making consensus, breaking that down into different parts and allowing those components to be swapped in and out.
02:16:06.592 - 02:17:04.016, Speaker A: And that does require large changes to some of the software because I think when you're building something, it's really easy to make assumptions and just bake things in and it takes a little while to unwind that. But a lot of that work has been done by the SDK team. Right now, I think it's going to be a really interesting question to see if the most value, the most number in the Cosmos SDK is coming from Comet BFT as a host orkit as a host in like a year, five years from Big. I think that's sort of like one of the existential questions. Yep. So going back to kind of base later and security, Ethan, maybe you can touch on kind of the merits of mesh security with all cosmos blockchains that are IBC connected versus kind of data availability and consensus with Celestia and how those two interlink. Sure, yeah.
02:17:04.016 - 02:18:10.776, Speaker A: I mean, I think what's happening here is, look, we had to first create the interchange, right? So we had to stand up a bunch of I mean, once upon a time, the multi chain was a sort know, blasphemous idea. We had to convince people proof of stake was going to work, there could be many chains that sovereignty mattered and so on. And that what we needed to do was sort of go bottom up, build a general purpose interoperability protocol for these individual chains to start to connect to each other. And now that we sort of have that, we can start to explore the different ways to actually share security over that interchange that now exists. And so all of these sort of different solutions, using a common data availability layer, using things like mesh security or interchain security or so on, they're all sort of different ways to explore different areas in the trade off space for sharing security over the interchange. Right? And I think the reality is we don't really know yet how it's all going to shape out. There's going to be some kind of complex arrangement of all of these different solutions and ideally, chains will be able to sort of choose for themselves which ones they want to experiment with and if they're even able to experiment with multiple of them at the same time.
02:18:10.776 - 02:19:19.724, Speaker A: Right, so mesh security certainly holds forward a promise of allowing sort of tighter economic integration between different chains and allowing them to sort of strengthen each other's economic security by sharing stake. Data availability offers a sort of different way to offload or outsource a certain element of security that's less about sort of sharing between many different chains and more like, okay, let's use the security of one big chain for a particular function, outsource to that and then focus on what we're doing. But I can even imagine that there might be some way that sovereign roll ups or something using DA Celestia as a DA layer might still find ways to sort of anchor other definitions of security to each other and share. Frankly, I think we still have very poor definitions of security. We use that word as if we know what it means, but it means so many different things and there's many different elements to it. And so I think a big part of what we need to do in this sort of new phase now that the interchange is here, is to actually get clearer definitions of what we're talking about and the kinds of security, what that means, what kinds of guarantees are really being offered and how people can actually reason about them. Yep.
02:19:19.724 - 02:19:57.390, Speaker A: Thank you. Ishmael I think your views would be really interesting here, given know you were one of the original Cosmos members and now you're internally at Celestia and how your views on the Interchange vision have evolved, being inside of Celestia now and how you see that going forward. So how I see it is that actually Celestia is like the continuation of what we built before. Right. The first component of the Interchange was the Cosmos Hub, so to say. And basically what we're building is what the Cosmos Hub could have been. Right.
02:19:57.390 - 02:20:55.384, Speaker A: It's a version of the Hub that has actually a purpose in the Interchange or in this vision of application specific chains. It actually serves a concrete purpose, which is offering the service offering of Celestia is essentially just ordering your transactions and data availability. Right. So, yeah, I think it's a very logical continuation, essentially. And I think I agree with Bucky that there will be a lot of experimentation happening. Right. And Celestia will be? Well, I'm sure Celestia will play a crucial role in there and it will enable a lot more experimentation, particularly because you offload the execution to these applications.
02:20:55.384 - 02:21:37.236, Speaker A: Or like the app layer, you enable much. More experimentation with zero knowledge stuff and different VMs, essentially, which I think will be way easier compared to having to stand up your own Cosmos zone, essentially. Yeah, I think that basically covers it. Yeah. Thank you. I think that the continuation of the experimentation is really something I'd like to press harder on, which is more is more for developers experiences, to be able to customize as granularly as possible to build the best application experiences. And I think part of that is Roll kit.
02:21:37.236 - 02:22:24.548, Speaker A: I'm wondering if you could talk a little bit about rollkit that coming out of Celestia and what that means for kind of the Internet of blockchains. Sure. So rollkit is basically a drop in replacement for Comet. So it's like it takes the same abstractions as Comet BFT or previously Tenement, which is like the main abstraction here is ABCI, which is like application blockchain interface. It is an interface between the consensus layer and the application. Right. So we took that abstraction such that Cosmos SDK developers can continue using the Cosmos SDK as they're used to, or like people who want to use ABCI and build applications on other languages.
02:22:24.548 - 02:23:04.580, Speaker A: There were teams building in Haskell in Rust and other languages. Pinumbra is building in Rust, for instance. And Anoma. All these builders that are used and familiarized themselves already with the stack, with the Cosmos SDK, they can continue doing that while are not being forced to spin up their own Cosmos zone. They don't have to spin up like a proof of stake chain. They can use Rollkit as like a sequencer software that posts data to Bitcoin, as we've seen before. In the panel or Celestia or polygon of whale.
02:23:04.580 - 02:23:57.092, Speaker A: It's a general, more general piece of infrastructure that we build such that the ecosystem can flourish and experiment with different data availability layers. Yeah, awesome. So 100 Xing Cosmos is something that I think we're all interested in doing. Part of doing that is connecting non native IBC chains. Jack, maybe you can talk about some of the work that you and the Strange Love team are doing extending IBC outside of Tendermint, and maybe we can take that a step further and talk about new clients for IBC and how we get to that future. Yeah, sure. I think also there is this strong place for Comet or other classically BFT consensuses within this world.
02:23:57.092 - 02:24:58.404, Speaker A: We're starting to talk a lot more about shared sequencers and some of these other pieces. And I think one of the beauties of the way we've built this stack is these modular pieces. These primitives can be sort of, like, mixed and matched in all kinds of different ways. And the experimentation that's happening now is helping us optimize that developer experience, make it as easy as possible for developers to spin up their own systems and then go sell services into this interchange economy. Celestia is kind of bringing new primitives to the table, but also a novel combination of existing primitives, and they're selling that as a service to chains over some of these basic interfaces. I've got some more talk this afternoon about modular IBC patterns. I'm going to dig a lot deeper into some of these topics, but in IBC, there's this concept of a client, and the client does all the authentication.
02:24:58.404 - 02:26:16.740, Speaker A: Hey, do I know which chain I'm talking to? Did users send these packets over on these other chains? And that abstraction is really general enough to put all kinds of cryptography. And whatever the trust assumptions of your system are, you can encode them into that client. So with the rollup world coming, there's all these new trust assumptions, there's all these new models for how to verify the state from these roll ups. And the work that we need to do with IBC to ensure that this Internet of blockchains continues to be connected is to build out all of these different trust assumptions into these clients so that it enables IBC everywhere. I think that with optimistic rollups, we're likely to see a lot of the IBC traffic go through shared sequencers or other committee based solutions. Compiling the fraud proofs and all those things is a little bit too heavyweight right now computationally, because you need to run the full state machine over on your counterparty, and it's a lot easier to go through a tendermint committee that's a shared sequencer. But once the ZK roll up technology matures a little bit more, I think we're going to see that become the dominant method in a lot of ways.
02:26:16.740 - 02:27:14.070, Speaker A: And that model fits much more nicely in IBC, because all you've got is this little ZK proof. You shove it into the client and you're good to go. You couple that with the Celestia data availability sampling light client as well, and then you can fully prove the state of that know, bubbling it up a little bit. Where we are right now is like all of these primitives work, they're just entering production. But I think, David, to your particular question about bringing that mass adoption that we really need to see and really enabling developers the work over the next year is to sand the rough edges off of the developer experience and allow developers to launch one of these roll ups in five minutes in any one of a number of languages. And all of the pieces that everyone on this panel here builds are a huge part of that system. And it's really cool to see it coming together.
02:27:14.070 - 02:28:10.228, Speaker A: Yeah. Thank you. So right now we're at a point in crypto where there's a lot of infrastructure, a lot of new infrastructure being built, and all of this is in service of building killer apps. And whether that killer app is money, as Ethan suggests, or it's something more web two oriented social or not, we need maybe a little bit more infrastructure to make these experiences better. So maybe we can go down the line and I'll ask you what's missing. I'm going to take a contrarian piece here and say I think a lot of the infrastructure that we need is here today. And I think especially with breaking consensus into different pieces, a lot of people have talked about celestial data availability as the end game of blockchains.
02:28:10.228 - 02:28:55.380, Speaker A: I do truly believe that that is the case. So I think really what's missing is compelling developer experiences and ease of use for all of these products. Because at the end of the day, infrastructure is a product and products need users. And if it's really hard to explain how all these things work, then we can't bring in the users. What a product does is take a really complex idea like money and COFI or data availability and puts it into a very simple developer experience where the developer, with a few lines of code can express this very complex idea in an instantiation in the real world. And I think that that's really what's missing right now. We've built out the infrastructure.
02:28:55.380 - 02:29:50.468, Speaker A: We don't need five more consensus protocols. Let's use what we have because it's plenty fast enough. Great. I would agree, Zucky, since you have such a great taxonomy of the now hundreds of well, close to 100 blockchains that use tendermint maybe 65 or so connected IBC chains application specific or not, what applications are missing for IBC chains that would bring in those new users that would then create the flywheel for more developers to come. Mean I think there's an interesting thing that's happening right now, which is like, there has been I think there was like as a practical matter, a real lack of applications in the Internet of blockchain. So we built out this interchange vision. We had staking, IBC transfers.
02:29:50.468 - 02:30:29.936, Speaker A: It was all, okay, great. You saw all of these new chains spin up. We basically had one popular application, which was Osmosis. And then there were a lot of applications also in the Terra ecosystem, and a lot of the interchange application development was happening over in the Terra ecosystem. So Terra ecosystem explodes last year, takes out a lot of the application ecosystem in the interchange, and then you end up in this sort of like we've spent this year, where there's kind of, like, nothing to do but stake. Now. I think in the last few weeks, we've seen just enormous number of new applications.
02:30:29.936 - 02:31:16.260, Speaker A: Some of them come from that Terra world and are launching on top of Osmosis. Now you have teams that are funded by various accelerators and DAOs all launching. So there's actually now quite a lot to do with your assets. There seems to be, like, a renewed enthusiasm for NFTs on Stargaze, and all of these things are maturing rapidly. Like, the user experiences are almost acceptable. They're about to become actually acceptable with sort of the APIs that Skip has built. So, on the whole, I think we're kind of at an interesting threshold point where one of the biggest challenges that Cosmos needs to overcome is sort of reactivating our user base.
02:31:16.260 - 02:32:02.240, Speaker A: We have a user base that has kind of built a relationship with the software that's basically like, oh, okay, I should stake and occasionally sell my staking rewards or whatever. Now there's all this new, exciting stuff to do about with the stealth, with your staking rewards, et cetera. Now is the time to actually get them to go out and experiment again, this is the time to go out adventuring in the interchange. And I think that's one of the biggest challenges. Another big challenge, though, is all of the growth tooling to do that, like, as a product builder, you actually have to go out and reach users and communicate with them, understand their needs, get them to engage, build growth, funnels. That infrastructure is also being built on Cosmos. Yep.
02:32:02.240 - 02:32:53.996, Speaker A: Ethan, what's your biggest hot take in the Cosmos ecosystem? Whether that's technically, architecturally, or from a user perspective, putting me on the spot. That a lot of it. I mean, it's not surprising, but I still think a lot of it is really vapid it's so much just like, assets, assets, assets. What are we doing with our assets? And we might as well be bankers. That's what bankers care about. And we're supposed to all be here to change the financial system, but we're just kind of rebuilding it. Yeah, it's transparent and it's more decentralized and open, and that's really important.
02:32:53.996 - 02:33:47.228, Speaker A: But what's even more important is the underlying structure of debt, really, that we're replicating, I think, until we really break through the number go up and have real applications that aren't just about that we're not going to make it. So maybe you all think I'm not going to make it because I'm the guy pushing back against speculation. I mean, obviously I understand speculations is important for bringing in users and excitement and hype and narrative and memes, and that's the stuff that sort of makes the world go round. But I'm still looking for applications that sort of move beyond that, that aren't just about finance. And I think there's an opportunity now with the sort of impending collapse, maybe, of the social media empires. There seems like an opportunity to get in there. Of course, it's also fragmenting, which is sort of interesting.
02:33:47.228 - 02:34:30.852, Speaker A: But I think that's an opportunity that this community needs to figure out how to address. Now, arguably, we need more scalable infrastructure to do so. What's the thing that's missing, infrastructure wise? I was going to answer for Ishmael and say celestial launch, but he can answer for me too. But otherwise I would tend to largely agree with Jack that there is a ton of infrastructure here. It's just about making it more usable and accessible for use cases that aren't just about speculation. Right? Some people I was just talking to someone yesterday at Osmocon who was telling me how they're kind of embarrassed to tell their friends that they're in crypto, right? And this is like a serious developer in crypto building infrastructure and real stuff. But it's like embarrassing.
02:34:30.852 - 02:34:56.944, Speaker A: And I still hear this from people sometimes we try to hire people from Web Two and they're like, Crypto? That's just a bunch of scammers. Some of us are proud of it. We all like, Dgens, whatever we'll show you. I think there's a real image problem. I think that image problem comes from the lack of sort of more real applications. And that's on us to find those applications. And it's hard.
02:34:56.944 - 02:36:23.064, Speaker A: I mean, the early applications are going to be speculative and that's okay. But sort of moving beyond that, I think, is the next big challenge for all of us to make this work. I do think that's actually one of the benefits of moving to this data, to modularity, that is sort of underappreciated, which is one thing that I don't think we fully grasped until we saw it live. Was that the incentive structure of building Cosmos SDK chains meant that you had to build liquidity and narrative around a token long before you had a product that was useful to anyone at scale. And so I think one of the most exciting things to me about sort of these modular architectures is you've seen this in the ethereum world where people are able to launch apps, build an application, build an environment, build a community, build a lot of excitement, typically a lot of excitement about the coming future AirDrop. But nevertheless, that's a different sort of expenditure of energy than having to sort of build liquidity around a token right from the get go the modular infrastructure and the way in which you sort of pay as you go for using that modular infrastructure rather than having to build this huge upfront cost. I think a lot of it was always described as like, oh, it's really hard to get validators.
02:36:23.064 - 02:37:33.492, Speaker A: I think most of us have experienced that getting validators is not that hard. There's like hundreds of them and they're all looking for some new thing to speculate on, which is your token and that's the structure of their business. But having to devote time, attention, energy to liquidity of an asset and narrative around an asset before you have a product is just a huge disservice to everyone and feeds into only building speculative applications. Yep, I would agree ishmael maybe you can talk a little bit about as we're seeing basically every ecosystem or L two converge on these modular frameworks. Cosmos having started from a different perspective, which was interoperability first, ethereum having started at shared security first, how you would position Celestia against other DA solutions building on Ethereum versus building on Celestia and other Cosmos related blockchain primitives. Yeah, I think Celestia is the least opinionated. Right.
02:37:33.492 - 02:38:30.836, Speaker A: It tries to be as neutral as possible by enshrining a settlement layer, for instance, for other pure data availability layers. The main differentiator, I guess, is that we probably the first that would go to market and the first that has certain features like data availability, sampling life and running on a testnet. So I can't predict the future on how these ecosystems will play out. But I'm a bit biased. But I'm obviously convinced that the feature set, the team being early. Mustafa came up with the paper in 2019. The groundwork was done in 2018.
02:38:30.836 - 02:39:04.192, Speaker A: We've been talking about modularity long before it was like a buzword or conferences being made. Right. I think that is the main differentiator is like the tech and the cloud that we have. Yeah, just to tag onto that. If you think back to the 90s we had Microsoft was the dominant operating system and that was like just one ball of closed source software that nobody really knew how it worked. And it was incredibly tightly coupled and they built this whole ecosystem around that. Linux came around.
02:39:04.192 - 02:39:42.604, Speaker A: That was a set of composable modules that allowed you to plug in any hardware and easily integrate that. And Linux grew a lot slower, but it's ended up taking over the market for operating sources and computing. And I think that this is very similar in Cosmos. We've come at this from modularity first and Celestia has kind of taken that to the next level. But in Ethereum they started with this idea and it still is the fundamental idea in Ethereum. And you see this with the L1, L two dichotomy, L three S even people are talking about now that's being pushed. The centrality of Ethereum within that vision has.
02:39:42.604 - 02:40:24.350, Speaker A: Always stayed ETH the asset and ETH the chain. Both of those instantiations are what everything else in the Ethereum ecosystem sort of aspires to filter down to. And when you build those assumptions deeply into your software, it makes building actual modularity much, much harder. And I think that that's the advantage we have is we've built from that place. From day one, we've learned all the hard lessons on how coupling makes expansion difficult and we've had to push through a lot of those software barriers and help make better interfaces. Anyway. It's a slightly unformed thought, but yeah.
02:40:24.350 - 02:41:55.112, Speaker A: Zucky what's the future for general purpose L two S on Ethereum? Say we're all in Cosmos because we think that roll ups or blockchains themselves will be application specific. So what's the relationship between Ethereum base layer L two S, l three S and why developers would need a gen per L two? I think my general view of general purpose l two S is they are meeting a need for rapid prototyping ethereum itself, as Ethereum has gotten more expensive and has developed a lot of and has, frankly developed, become a playing ground for large dollar value real use. Right? So large counterparties do use Ethereum to move large amounts of money and use a set of DeFi building blocks to do so. The rapid prototyping sort of environment of experimenting with new applications. I think one of the biggest challenges we've had for Cosmos is it's probably not the first thing that a new team should be building is an app chain. A general purpose environment is actually probably a better place because you have to sort of don't have to invent the world and you can figure out your product market fit. You can figure out who your user base is.
02:41:55.112 - 02:42:51.144, Speaker A: You can build your community. And so I think the model of dYdX is really like the model for the app chain world where the app chain is dYdX. V four and there was v one and there was v two and there was v three. And v one and v two were a general purpose product built on the Ethereum blockchain, using it as a general purpose ledger. V two is an application specific l two or v three was at application specific l two and v three is an app chain. Right. And they learned along the way what had product market fit, what were the key features, what did you actually have to target and build? And that then making that one year investment where they eventually stopped improving the existing legacy product and built the app chain version of it could make sense because they had all of this knowledge and all of this experience built in.
02:42:51.144 - 02:43:57.232, Speaker A: And we're all going to see how that turns out. But that's a more plausible way of getting there. And so I think now you have a bunch of general purpose Cosmos chains where you have functionally, where you have neutron and archway and Osmosis and all of these new general purpose chains where people are rapid prototyping new applications. And some of those new applications may eventually pave the way to be independent app chains. But that's sort of how I see the relationship. And most of the modular teams seem to have bought into it across it, where they are launching general purpose chains now and toolkits for building your future application specific chains. Zachary, do you see a path for going from a Web Two app first to then an app chain, like instead of starting from the EVM kind of environment or something like that? Yeah, I think the real challenges is in all ways building decentralized software is just a lot more difficult, a lot more.
02:43:57.232 - 02:44:47.792, Speaker A: But the biggest challenge is self custodial. Monetary systems are such an important value, and that's really what you have, whether it's the app chain world or the smart contract world, is like, can you plausibly build in the but also most of us are Americans are. Exposed to, like, sort of the western system, which takes, you know, where you have a lot of financial regulation the minute you have custody. And that's also a big sort of barrier to building. So, I don't know, you'll probably see more. I've been watching what's happening with this Unibot thing. I don't know if anyone, I think more of the Djens in the audience probably are following it.
02:44:47.792 - 02:45:31.096, Speaker A: But it's literally like it started out as an application where you're basically just like where you're just like giving your private keys to a telegram bot right, and moving money through that. But it does seem to be developing into a real thing. To come back to Bucky's point earlier, I think it would be nice if developing starting on Web Three would be as easy as on Web Two. As Bucky said, it's much more difficult currently. But as Jack also mentioned before, I think what we like, the infrastructure that is missing is to make that easier. So I believe that will be the case. There might be trade offs.
02:45:31.096 - 02:45:58.432, Speaker A: Right? There will be trade offs, 100%. But just from the usability point of view, I think it will be possible to have something like AWS kind of style where you code your thing, you click a button and your chain is running. Right. That is not too far fetched at all. There are people, there are teams working on that right now. So I think that will be possible. And then there's no reason to start at Web Two if you plan to be like a decentralized app afterwards.
02:45:58.432 - 02:46:55.896, Speaker A: Right. I think just to like there's a couple of orders of magnitude more developers outside of crypto than there are inside of crypto. I think a big part of that is the developer experience that's been most pushed in crypto is this EVM developer experience. If somebody's worked in Web Two, it's not a great programming language to work in and I think another advantage we have as an ecosystem is we've got this proliferation of other VMs and programming languages that you can write in JavaScript with the Goric go with the Cosmos, SDK, Rust with WASM and many, many others. And that is, I think, going to be key to longer term developer adoption, offering these developers something that they're familiar with, meeting them where they are. And yeah, that's also going to be key. So developer experience is one thing, user experience is another.
02:46:55.896 - 02:47:56.924, Speaker A: I'm curious how you guys see wallets playing out in an Internet of thousands and thousands of blockchains. Are wallets application specific? Is there one to rule them all? Is there a WeChat tile wallet for crypto? What are your thoughts, Sarah? I mean, a kind of far out idea is that wallets are just top of funnel for applications. So wallets go find the users and bring them in exactly how they do that. I think there's a lot of white label solutions out there already. And you could imagine a convenience store launching their own wallet that has their loyalty program that's run in USDC and that filters back down into the, you know, wallet is this term that I think we need to unbundle a little bit. But I think the way that we talk about it most is that sort of top of funnel of the users. There's obviously the custody pieces, there's the technical infrastructure that's needed to make that happen, and all of that stuff is really cool.
02:47:56.924 - 02:49:24.410, Speaker A: But at the end of the day, it's about eyes and clicks and tokens and that's users Zucky. If you could snap your fingers and make one thing happen in the Internet of blockchains to accelerate growth and adoption, what would that be? Well, I think it would have something to do with the wallet conversation. I don't feel like I have, as a builder a detailed understanding of consumer behavior regarding wallets, the way consumers behave. Wallets with wallets mostly doesn't make sense to like, things that don't make sense to me is why is MetaMask so sticky? Why is that so sticky? And why is it that the user feedback, but at the same time, why is it that users are like, hey, I use Kepler and now you want me to install MetaMask to use the sommelier app and I don't want to do that either. Very confusing to me what exactly it is because I don't personally experience it. And one of the biggest challenges I think we have in general is that a big part of where application builders lose contact with their users is in the wallet infrastructure. We have this third party piece of software that we don't really have control over.
02:49:24.410 - 02:50:19.784, Speaker A: We push a user into it and then we maybe see the output, but we don't know what goes on inside of it. And it's a real blind spot in understanding sort of consumer behavior. So what wallets have to be like such that users will mass adopt the interchain is. I think that is one of the biggest pieces and we just have lots of theories and those theories now need data to sort of validate them and it's hard to get visibility into that. I think that in the same way I was talking earlier about the Ethereum community having this core ideal of everything kind of filtering down to Ethereum and how that shapes their worldview in Web Three. So much of what we do is a reaction to what we all really don't like about Web Two. And I think we have a lot of blind spots when it comes to some of the good things about Web Two.
02:50:19.784 - 02:51:07.304, Speaker A: And what Zach is talking about with tracking users is a great example. When you're an app and a user clicks through the wallet connect, there's a lot of missing analytics in that flow that don't allow you as an application builder to understand what the user behavior is and how you can help improve those user experiences. And it's the same thing with the programming languages thing. We need to meet those users where they are. And yeah, that's a key missing. I think the thing that's skipping with IBC Fun and then their announced Kepler integration is I was like one of the biggest pain points of Cosmos right now is that there's this website, Tfm.com that no one has ever heard of, but it makes the IBC UX 100 X better.
02:51:07.304 - 02:52:13.544, Speaker A: And it's like mostly what I use to interact with IBC now. I used to use the Osmosis deposit page or whatever and those sorts of things. But now we have that experience. Much of that experience is now coming also to be integrated into Kepler. How will that change? Behavior, I think is going to be a huge data point. So how do we bring those users in for things like TFM or IBC Fun if we don't have any insights into that behavior? If we're in blockchains and most wallets are pseudonymous unless they're docs, how do we gather any insight into topple funnel business development for applications and preserving privacy on the other end, I personally think tracking users in the browser, like opt in is an option, sure, like telemetry and stuff, that would certainly give us insights. But I think it could start even much simpler.
02:52:13.544 - 02:52:56.236, Speaker A: You could just do usability research, talk to users, do focus groups. It sounds very naive, but I think you will catch most of the flaws of our thinking of how users interact with things and they're actually interacting with things. I think you'll catch like 80% of that just by doing proper usability research. And I think that should be done independently of one ecosystem. It shouldn't be driven by like, oh, we want to figure out MetaMask, we want to figure out Kepler. It should be independent of that. Here is a user, they want to interact with an app or web free or something.
02:52:56.236 - 02:53:31.428, Speaker A: And it should be independent of these ecosystems, in my opinion. I think that would already move the bar much higher. This is kind of an aside, but Zoko from Zcash is one of the people I've seen do the best job of this. Every time he meets somebody for the first time, he has them install a Zcash wallet on their phone and then sends them Zcash. And he times every step of the process and keeps a record of all of. And like, this type of focus on user experience is missing in a lot of places, I think. There's also a kind of an interesting question.
02:53:31.428 - 02:53:59.772, Speaker A: I don't think we're really clear on what the top of the funnel is for the modular ecosystem. Is the top of the funnel like ETH l one users or monolithic blockchain users? Is the top of the funnel like social media? Is the top of the funnel like Web two application users? It's Tron Payments in Asia. Quite probably. Yeah. Not a joke. You're right. A quick survey.
02:53:59.772 - 02:54:40.904, Speaker A: How many people in the crowd raise of Hens has a Kepler wallet? Pretty decent amount. Okay, maybe 80, 90% Zucky. Let's close on this design. Tell us how to get into the Cosmos ecosystem. What is the path of least resistance as a user? Cato and squid. Those are the things like Squid router is great for bringing assets from any other ecosystem, onboarding from lots of place. Kaido is a great experience for onboarding from Fiat if you're in, depending on jurisdiction, et cetera.
02:54:40.904 - 02:55:09.332, Speaker A: But I honestly think that, again, one of the missing pieces. I know a bunch of Ethereum people got excited about bad kids and wanted one. They were all like, how do I odboard onto cosmos? And I was just like squid. It's that easy an answer. I think more support with the worm chain and Noble coming online and stuff like that. There are going to be more options to that. But that's right now the best.
02:55:09.332 - 02:55:36.984, Speaker A: You couldn't get them to install Kepler. They did. No, kepler was fine. They couldn't figure out how to get assets into and you know, this is like those core pieces of infrastructure that have gotten built out. One of those is swapping forward on Osmosis where users can send an IBC packet in, have it swap automatically, and send those assets out. So in this particular flow, you want to buy bad kits on Stargaze with stars. You need those stars somewhere, and you need to make those IBC transfers.
02:55:36.984 - 02:56:03.450, Speaker A: But the automation makes that easy. It makes it a single transaction on Ethereum for the end user, and they end up on the chain they want in the asset they want. That's a key experience. Yeah. And I don't think we're that far from being like, I have ETH, I want a bad kid. And you don't have to do anything other than sign an intent or an order or whatever and that, and then there's like the infrastructure will just take care of it. We're really not that far from that future.
02:56:03.450 - 02:56:21.544, Speaker A: Cool. So if you're in Ethereum, go to Squid, transfer some assets over, buy a bad kid. If not, go to Cato. Install Kepler. Come on over to Cosmos. We have a lot of fun, boxing matches included. I'm sure everybody is hungry and looking forward to lunch.
02:56:21.544 - 02:56:29.440, Speaker A: So I want to thank everybody on the panel for all of your time and you guys for being here and have fun today. Thank you, David, for moderating.
04:01:54.690 - 04:02:21.474, Speaker B: Hello, friends. Welcome to day Two, afternoon or morning of the Modular Solomon. This is also known as PBS Day. In fact, in PBS Day, we will be talking only about peanut butter. Just kidding. Not that one. Public broadcast.
04:02:21.474 - 04:03:24.380, Speaker B: Also not that one. No copyright infringement. Proposer builder separation. We have a naughty speaker today who is recoining it into prover sequencer separation. Something along that line, not sure. So it's up to you all to listen to their digest of the architecture of a particular market structure that specifies the roles of the protocol actors and non protocol actors and trying to resolve an inherent conflict of interest of those who have the power to be the information fiduciary as well as the responsible for the execution. So without further ado, we'll first start with Tarun giving us a grand unifying theory of mev, last I heard.
04:03:24.380 - 04:03:29.340, Speaker B: And starting from there, we'll dive into PBS land.
04:03:37.390 - 04:04:30.960, Speaker A: Okay, I think maybe these will show up. There we go. Cool. So we're going to talk a little bit about something I've been probably working on for a few years at this point, which is to try to really come up with a good understanding of what the word mev means. And if you try to really formally define it mathematically, it turns out to be quite a bit difficult, but it turns out there's a bunch of different components to it. And this is the second of some number of parts. I'm not sure how many there will be in the end, but I guess those are the old ones.
04:04:30.960 - 04:05:36.178, Speaker A: So basically the idea is that understanding what mev is from an incentive mechanism standpoint involves really understanding many different applications. Right there's of course, the most common type of mev with AMMS, but AMMS, of course, add a ton of different properties that make it easy to understand them. There's mev for liquidations in lending protocols and perpetual protocols. There's mev for front running NFT auctions. All of these things have different sort of levels of understanding for the user. And in theory, right, blockchains are supposed to provide audibility properties, verifiability properties, and other things for that form to users. So you might say, how is there uncertainty that can come from these things? So I like to think about the uncertainties as falling on a spectrum, and the spectrum is sort of defined by how much the payoffs that users are taking vary and are uncertain.
04:05:36.178 - 04:06:54.462, Speaker A: And PBS is aiming to kind of change some of that uncertainty amongst the first four categories which are ordering inclusion, Adamicity and relaying. So ordering uncertainty is I submit a transaction to a blockchain and I expect a certain payoff. I expect to pay a certain price for a trade, I expect to sell an NFT for a certain price. But my realized outcome is not that inclusion is censorship resistance. Like if I send a transaction, does it actually get in in the next block and it's slightly more severe than say, just liveness, which is I eventually get my transaction in, I may want to have some time preference on how fast my transaction is accepted. Atomicity is of course the idea that I can compose two things and it's executed as sort of fill or kill, like the entire transactions executed or single transactions executed. Relay, which I'm sure you'll hear a lot about today is the idea of having a networking layer and some sort of agents who convey transactions who may be more trusted than you think.
04:06:54.462 - 04:08:28.330, Speaker A: And I think that's certainly the one part of the PBS system that we'll hear today has some of the incentive problems from many different speakers. And finally, since we're at the modular summit, we should also remember the highest payoff uncertainty for the end user can come from a DA layer. If a DA layer doesn't work, doesn't behave as expected, you can have arbitrarily bad payoffs, your roll up is not working as expected. And one question you might say is like what does it mean to move from left to right on this diagram? So just as an illustrative example and this is more a statistical learning theory perspective on this is I have sort of a class of payoffs that's the script. Curly F I have sort of some functional say like a variance or some type of deviation, and I have some sort of notion of the types of uncertainties, and I say, what is the conditional expectation of that functional? And what does it look like? What's the worst case? So one definition of MEB you could use is any sort of mechanism that creates payoff uncertainties where unsophisticated users lose value relative to sophisticated users and the uncertainty is the key to the unsophisticated users expected payoffs. So of course there have been sort of multiple different ways that people have tried to go about this. It's not lost on me that everything in the communist side is only a paper and not a product and everything in the capitalism side is a product and not a paper.
04:08:28.330 - 04:09:51.160, Speaker A: So just as a funny aside, but the communism side is things where the mechanism designers or the protocol designers are trying to enforce a lot of constraints on what types of orderings are allowed to be conveyed to the blockchain. And the capitalism side, the blue pill is how do you make a competitive market for transactions. So Flashbots sort of created that initially. I guess arguably the spam in the ethereum mempool prior to Flashbots was the first version of this and then it sort of expanded and now we see this whole ecosystem built around this, right? So we have the red pill, we have the blue pill. Are there really only these two options? Well, one thing that purveyors of all parties will tell you that hey, our mechanism reduces uncertainty for users, it reduces payoff uncertainty, it's easier for unsophisticated users or it's more efficient for the system as a whole. Neither of these types of mechanisms, neither the full ordering complexity piece nor the economic piece actually can remove these uncertainties for users. So maybe we should take a step back and try to understand what fairness looks like in general.
04:09:51.160 - 04:11:10.222, Speaker A: So I found this actually very nice old paper from 1970s, from Hal Varian who's the chief economist at Google now on distributive justice and welfare economics and theories of fairness. And the TLDR of sort of the principle of fairness uses the maximum payoff for any particular individual is not so much greater than the average payoff, average welfare of any individual. And that's sort of the maximum for how to measure this type of uncertainty. I look at the maximum payoff that any user gets, I look at sort of the average payoff people get and as long as those two things are relatively similar, you're relatively fair. And you could argue that the communists, the order fairness things focus on one side of this but they're very payoff agnostic, so they kind of can't tell you anything about whether you're getting the worst case or best case. They just say hey, well, a validator said you got this transaction first, so I guess I'm putting in first maybe. And the economic mechanisms sort of focus on maximizing payoffs, right? Maximizing revenue that goes to validators maximizing revenue that goes through the system but it doesn't really capture the externalities necessarily as well as you might think.
04:11:10.222 - 04:12:25.320, Speaker A: And so one question is how do you get this balance between maximizing revenue, which obviously can be good for the stability of these systems, especially if they rely on arbitrage, versus making sure the expected welfare for all participants on average is not so far away from the worst case. So one important thing to think about is maybe fairness should be done on an application specific level. So the analysis we'll talk about is from the perspective of there being sort of fixed universe of transactions. So adversary or malicious validator could have added or removed transactions before we analyze it and the only actions that the malicious adversary can take are reordering transactions. So maybe I have a bunch of sandwich attacks, I'm reordering them so that the most expensive ones are first so that the later ones have to pay a higher price. And the key thing that we'll talk about is we're going to view these as these kind of payoff functions that depend on a permutation and give you a value, a real number. And that's sort of the fundamental objects we're looking at.
04:12:25.320 - 04:13:17.030, Speaker A: So in AMMS you have kind of this very smooth payoff with enough liquidity reordering doesn't really change the price impact enough and you can kind of smoothly vary how that looks. But then on the other hand, we have liquidations, right? Liquidations are very not smooth. If the price never touches a certain point, there's no payoff to the user to say a searcher. Of course the price touches a single point, you have a jump and discontinuity in your payoff. And so what we'll see is that these actually turn out to be the fundamental basis of all the payoffs of this form where you're looking at reordering. So let's maybe try to demonstrate this in pictures. So we take an AMM like uniswap a CfMM and now consider a set of transactions.
04:13:17.030 - 04:14:22.394, Speaker A: There are seven transactions here and there are four transactions that are price goes up one unit. Those are the green bars. There are three transactions that are price goes down one unit. And as you can see, if I permute the green and red arrows I get different price trajectories, right? And so you can see the first one sort of is like up, down, up and then the second one is up down. And at each point you can say given a permutation here's the price trajectory created by this ordering. And so the x axis is sort of the index as the transactions are processed. So one of the first part of the towards a theory sequence sort of showed that for AMMS, this thing is actually bounded in a very interesting way, in that, provided you have a particular liquidity constraint, the worst and expected behavior are only separated multiplicatively by a factor of log n.
04:14:22.394 - 04:15:23.214, Speaker A: So that sort of is actually pretty good. It means that realistically I'm not going to get that much worse of a price even if I'm sandwich attacked. And that paper also showed some examples where people get better prices if they're sandwiches, where someone takes a worse price but the end trades in the block get better execution. And so there's sort of this somewhat surprising thing, of course that these sandwich attacks, these things that the re jewels of the world would call robbery can actually help the welfare of the system which is like a totally interesting and surprising result. But the thing is this doesn't really dictate all the other types of mev, right? It doesn't tell you anything about NFT minting, it doesn't tell you anything about liquidations, it doesn't tell you anything about cross chain. So we'll start with a gadankin, like a thought experiment. And the thought experiment is suppose we construct this is not necessarily a good DeFi protocol, but this is just one that you can analyze.
04:15:23.214 - 04:16:00.300, Speaker A: And so you'll see why this is important. But imagine I have an AMM, I have two nDEX trades. Half of the trades are plus one. So half of the green arrows, half of the trades are minus one down arrows. And given a permutation, I get one of these charts, right, one of these three things. You can see how, like permuting the indices gives you different price trajectories, right? And if the price trajectory touches the red line, the liquidation has a payoff of one. And if you don't never touch the red line, it's zero.
04:16:00.300 - 04:16:22.914, Speaker A: So the idea is, this is a very simple thing where I say I give you some trades, I look at all the permutations. If it ever touches a particular point, then you realize a gain. If you don't, you realize a loss. So what you can do is you can kind of look through the sets of permutations, say, okay, did this one give me a loss? No. Yes. Does this one give me a loss? No. Yes.
04:16:22.914 - 04:17:20.994, Speaker A: Does this one give me a loss? Yes. And I somehow found some Mortal Kombat fonts online and I wanted to write the finish him, finish her thing. I figured writing liquid data in that font would be fun. So one very, very interesting fact is any payoff function that you can write in a blockchain that is a function of these reorderings of these permutations can be written as a linear combination of these liquidation games. So any payoff that means for any application is written as a sum of these different liquidation games. That's sort of something that's very unintuitive, that these kind of very abstract things that you find in DeFi for people taking leverage somehow are a basis for the set of all payoffs you can get. But the interesting thing is the liquidations are much worse than the AMMS.
04:17:20.994 - 04:18:40.094, Speaker A: In the AMMS, we showed this thing where it's bounded by log N, but in liquidations I can construct something where it's actually the ratio of the worst case payoff to a user versus the average case is O of N factorial, which is obviously significantly worse. Needless to say, that's not really an ideal thing. So that sort of says this idea that the space of these welfare metrics that you're measuring on users can vary quite a bit. And again, a lot of the kind of things people have made and said that would fix and make things more fair don't solve this problem. Like none of the ordering schemes in the communist column actually can correct this. So you might say, okay, well, what do we do to actually measure how fair these things are? Is there a way for me to say, give me your function and tell me what the difference between the maximum payoff and the average payoff is and give you some guarantees on fairness? And so we'll see that sort of some analog of a Fourier transform is the way to do this. So one thing kind of.
04:18:40.094 - 04:19:58.970, Speaker A: Just as a reminder or if you haven't seen it, well, what is the traditional Fourier transform? So if you think about sound waves, what you're doing is you may have this turquoise curve that represents the analog sound signal you're hearing, but you can decompose it into high frequencies and low frequencies. So maybe high pitched voices and low pitched voices represent the red and blue curves and the Fourier transform lets you take different segments, break up the curve into pieces and say, okay, this is the sum of these different components. You might say, what does that have to do with mev? That's like a smooth curve and someone's singing and all sorts of things like that. But the interesting thing is it turns out all of these payoffs under reorderings you can write as these sums of liquidations. And the analog of the sinusoid curve, the single pitch, is actually a liquidation. And so the high frequency modes are liquidations where there's very few permutations that can trigger them, very few reorderings, and the low frequency modes are ones where there's many permutations that trigger them. So you can kind of think of it as when I expand out a payoff, I am looking at how constrained I realize that particular payoff.
04:19:58.970 - 04:20:37.510, Speaker A: And again, as I was saying, this is sort of a fundamentally important thing. Is that something that we understand relatively easily and that people do in practice, liquidations actually generates a set of all payoffs. Now, this expansion might be very big, right? I have a sum over a subset of subsets of permutations. There's two to the N factorial of those. So there are many of these. You might need to expand it. But the key is that this is a basis like it represents the core set of all these payoffs.
04:20:37.510 - 04:21:41.418, Speaker A: And I put examples of people involved in the algebraic versions of Fourier transforms as pictures. So one question is, again, how do we interpret this Fourier transform? What does it really mean? So one way of thinking about it is the Fourier coefficient f hat, and these are sort of heuristics the Fourier transformers over finite groups is quite a bit different than the continuous one is. If a set is large of permutations and the Fourier coefficient or that set is large, that sort of says the permutations have the function achieves its average value on those. If the set is small but has a very large Fourier coefficient, you're near the maximum value. And so the idea is that this is why the spectrum of this thing captures what the maximum and average values are of these. You know, there's a bunch of things you might need to talk about these. We're definitely not going to talk about them.
04:21:41.418 - 04:22:27.158, Speaker A: And since it's a Celestia conference, I had to put jacob always says weird aphorisms such as none of these words are in the Bible. He's mad at me, though, for picking this picture, but it's the first thing you find when you google his name. So you might say, okay, great, I have this Fourier transform. I can take any function you give me in a blockchain, I can sort of write it as a sum of liquidations. And that's like what the user gets paid. But that's great. But what does that tell me about bounding, the difference between the max and expectation? Many of you may have heard of things memetically like uncertainty principles.
04:22:27.158 - 04:23:37.870, Speaker A: But uncertainty principles exist outside of just physics. In fact, they exist in information theory in general as these sort of lower bounds on localization. So if I can squeeze a function and make it very sharp, I can't make its Fourier transform super sharp also. And a very nice thing that turns out, and this is exactly why all of the stuff we're talking about earlier is important, is A liquidations generate all the payoffs, b the Fourier transform gives you a way of going from your payoff to the payoff represented in the basis of liquidations. So the Fourier coefficients really give you how to expand it in terms of liquidations and C the uncertainty principles give you a bound on the max over the expected value, over the max value, which is the difference in payoff for a user. So this gives you an explicit way to quantify this type of fairness. And you can kind of think of this inequality as showing you fairness bounded by sort of these spectral properties.
04:23:37.870 - 04:25:20.410, Speaker A: I didn't want to go into any more details of this, but you can read the paper, but I'll just give you a very tiny kind of understanding. But the idea is, if you allow a large enough number of permutations, there's sort of a notion of the size of the number of orderings you allow is sufficiently large relative to the Fourier transform, you actually avoid the maximally unfair payoffs where the expected value is extremely small relative to the maximum. On the other hand, if your sort of sets of orderings don't have sufficient overlap with your coefficients, you'll get a lower bound on unfairness, you'll always be at least some amount unfair. And this sort of shows any of these types of ordering mechanisms, whether they're the time based ordering in arbitram, whether it's pure fair ordering sequences, whether it's sequencing rules, they all only work for certain payoff functions, right? They only work if they restrict the set of orderings to some set of order, to a certain size set of orderings. But that means they distort any payoff functions that depend on orderings with a larger size. And so you get this kind of sort of Shannon sampling limit of like the payoffs are tied to your ordering algorithm. You can't get a free lunch of offering things like first come, first serve and satisfying every possible payoff function, which is what this says.
04:25:20.410 - 04:26:00.580, Speaker A: The second thing is you could think of this as like a sampling theorem. So these sampling theorems are the most practical time you've ever encountered. One of these is when you've maybe listened to an M P three that sounds really shitty. And then you go get a vinyl and you listen to it and it sounds great. And you're like, oh, why does the MP3 sound bad? Well, the MP3 didn't sample enough frequencies for it to resolve correctly versus the analog. And this is sort of a sampling. It says like for a payoff you have to have resolved enough frequencies for you to be fair in that payoff.
04:26:00.580 - 04:27:06.896, Speaker A: So what this sort of suggests is the communist version of the world, the fair ordering type of stuff, things that are sequencing rules is always going to preference certain applications over other applications. We always talk about mev as being sophisticated users taking advantage of unsophisticated users. I would posit that the communist ordering world is sophisticated application developers taking advantage of unsophisticated application developers. All you're doing is changing who the person who's enduring the penalty is because the application developer is defining the payoff function. And if they don't understand how all this works, then they may have defined one that's extremely unfair on purpose and now you're forcing the application developer to understand that, whereas that's not as true in the unrestricted ordering case. But what this says is application specific orderings. Orderings tied to these functions can be better in practice.
04:27:06.896 - 04:27:39.484, Speaker A: And maybe that comes from marketplaces like suave things that allow each application to specify some sets of rules as opposed to having these global sets of rules. And yeah, it's much better than sort of some of the useless communist versions of things. So what's part three? Well, everything here is like a formal theoretical thing. There's a lot of N factorials everywhere. Not all of us love sharp peak complete problems. Dot, dot, dot. So the next part is approximation.
04:27:39.484 - 04:28:44.726, Speaker A: Can you come up with measurements that you can make on real functions that give you some notion of measuring how much the max and expected mev are up to some error in production and monitor these types of metrics and see how well people are performing? And it's a paper. So with that I actually finished 30 seconds early, so if any question so, cow swap. Yeah. So you gave this point about cowswap, great point. When I talked about the o of login thing, calsop is achieving that. So that's what I'm saying. Login is not that bad, right? If there's N trades and I'm able to not get front run, but I may have to pay in the worst case, log N more in price multiplicatively.
04:28:44.726 - 04:29:50.172, Speaker A: It's not that bad. Paying N factorial is much worse, right? So that's the idea. This is a way of quantifying how much the worst and average are. And the point is, once we can approximate these better, you can run these on live systems and say like, oh, actually this particular payoff is not that fair. You should loosen the orderings or shrink the orderings, and the marketplaces can then adjust after that's. Anything else? If not, thanks. I'm around all day, so I am Zucky.
04:29:50.172 - 04:30:50.432, Speaker A: You've probably seen me already. So this talk is like sort of a appeal to people who are protocol developers mostly, but a little bit of sort of philosophy of protocol design and generally my thoughts about what constitutes a full node and the sort of changing definition. So I've been working on building blockchain protocols for ten years. I've contributed and worked a lot on different layer ones. So that's basically my background. And my general thought is that, okay, so we started out with this piece of software that didn't come from essentially anyone in the present community, bitcoin. And bitcoin worked a certain way, and every blockchain network that has been created since has been in some ways inspired by the bitcoin full node, by that original bitcoin software.
04:30:50.432 - 04:31:27.250, Speaker A: And its core property of the bitcoin software was, you have this notion of a full node. And in the original bitcoin sort of concept, every full node was a miner. So you have this whole network of miners. Everyone was connected to each other in an ad hoc basis, so there were no special roles. It's just network of nodes, this flat topology. And that is how we've sort of imagined every blockchain since then. But this hasn't been a true description of blockchains basically for as long as I've been involved in them.
04:31:27.250 - 04:31:59.608, Speaker A: So instead, there have been parallel networks, like sidecar pieces of software, specialized nodes everywhere. Like, we couldn't use blockchains today if none of this existed, so blockchains would be completely unusable. That's been true since 2013. 2014. Right. You've had like bitcoin has something called the fiber network. The fiber network does fast block propagation with a completely different block propagation algorithm.
04:31:59.608 - 04:33:00.920, Speaker A: Between mining pools, there's mining pool software, which is incredibly important for the operation of bitcoin. You have things like mevboost and flashbots. So what this has emerged is an enormous difference has happened between the actual protocol, like the protocol that our users use, and what's in the full node software, and that makes it very, very difficult for anyone to reason about maintain these entire systems. Bitcoin is actually like a pool driven block propagation system. There's no widely distributed mev tooling, but mining pools are in charge. The network that actually matters is the network between mining pools. The transaction propagation system is how mining pools is how mining pools end up ingesting transaction.
04:33:00.920 - 04:34:21.744, Speaker A: The de facto ethereum network is a network that has a consensus system and a peer to peer layer, and a mem pool that does not actually matter because mevboost builds most actual blocks. And there's an entirely other network between block producers and relayers and block builders that is essential to the functioning of the system and is potentially vulnerable to exploits, but isn't part of the core protocol. And then in cosmos we have this thing called protocol owned builders where validators build and check blocks together with consensus enforced rules to do value extraction. And it's actually cool because in Cosmos we do seem to be moving to a large extent towards more parts of it, the software being in the core binary, which is good and more maintainable, but also sort of represents better the expanding complex. But we've had a lot of problems because our full node software was never designed to do this. So questions I would ask is like, why do we write blockchains that have mem pools at all? It's really unclear why we do this. Bitcoin was basically designed with this idea that any node in the network could once in a while produce a block.
04:34:21.744 - 04:35:15.968, Speaker A: So we need to disseminate by flooding transactions everywhere. We really care about the resilience, censorship, resistance and self repair properties of P, two B networks, but it's probably the most poorly tested and specified property of any piece of blockchain software. You have all of these papers that clearly specify the consensus algorithms, the rules, the attacks. No comparable sort of material exists for the P to P networks. So yes, why don't protocol nets put these over networks in the core protocol? So the networking layer is the least expressive part of the stack. We have these crazy expressive smart contract languages. We have very complex and sophisticated consensus protocols, but we don't really have much expressiveness in this part of the stack.
04:35:15.968 - 04:35:56.560, Speaker A: Every blockchain team reinvents the wheel. They run into things that they can't build with the existing software. So they'll slap on an overlay network and add it to the nodes is the way things. And sometimes those will have centralized parts. Like in Cosmos when we have Skip. And Skip has become sort of canonicalized into some of the Cosmos networks, but it's not something people think about as part of the Cosmos protocol. So, yes, we have these Frankenstein blockchains ishmael is talking about how Celestia has this Frankenstein P to P network, which is part tendermint P to P, part lib P to P, and people are always building these things with other custom components.
04:35:56.560 - 04:37:04.410, Speaker A: So I do think that PBS is just largely a good thing. And the formalization of PBS because it's driving more innovation and attention towards these areas, it's driving, I think, and forcing the P to P engineers to do more on the networking level. It's forcing us to consider the entirety of the block of the network and the protocol as a whole. And it's sort of getting us closer to a world where the entire blockchain behavior is more formally specified. But again, we've been seeing attacks. So I would argue that the direction of moving from mevboost, which is a form of protocol builder separation, but not an enshrined one, even though every single ethereum consensus client implements it, is sort of a worse solution than enshrining PBS in some form. And we've already seen attacks on nodes and losses of funds and security incidents in the mevvoo system.
04:37:04.410 - 04:38:17.692, Speaker A: And that would be better dealt with if it was part of the core protocol. Just like kind of the other thing that I would observe is that in the spirit of modularity app, chain builders want a lot more control than traditional general purpose l one builders ever had. L1 builders did not expect that they would be able to control the mev for their applications. Tarun was just talking about this about this trade off between sophisticated unsophisticated users and sophisticated and unsophisticated application developers. Well, if you have an application specific blockchain, you don't have to deal with that trade up, because this application team is the team that built the blockchain. So if they're sophisticated, they will end up wanting probably a lot of control, and they'll want to have their own sort of fairness policy, and they'll want to be able to punish and slash and observe. And so as people are building toolkits for modular blockchains, people are building toolkits for l ones, I think we have an obligation to start building in more tooling into these systems to allow this kind of control to be expressed.
04:38:17.692 - 04:39:03.150, Speaker A: And the fewer ad hoc systems that people have to build where there's no abstraction in the full node, the better things will be. So, yes, the fundamental goal modulism is lowering the cost of innovation. It will hopefully result in more shared infrastructure. There will be more libraries. There will be more binary pieces of software. There will be more examples of building systems where this block production and pre consensus part of the network actually exists in the protocol, in the core software. As we have vendors for things like shared sequencing and intense mempools and execution, the more they can share components, it'll lower the cost of all of this.
04:39:03.150 - 04:39:37.844, Speaker A: The sustainability of all of these pieces of infrastructure as the market fluctuates up and down, should be a concern to everyone and is a big part of the long term stability of the space. So that's it. Thank you very much. Thank you. I just got a bad kids shirt. You should get your own. Is it vitalik? Next, Robert.
04:39:37.844 - 04:39:51.150, Speaker A: Okay, Robert. Danny. Okay, I'm getting conflicting information. Does anybody have the what's the mev here?
04:39:52.720 - 04:40:19.610, Speaker B: The mev is here. So I think we're unfortunately, one of our speakers, due to bureaucracies, had not been successful in getting into the Schengen area, so she has to dial in for us. But I think this is an unfortunate reminder of the boundaries that we live in and the world we live in and what we need to fight for.
04:40:22.060 - 04:40:59.200, Speaker C: All right, I can see myself. Is my slides up cool now? I can see it. All right. Hi, everyone. My name is Danning. Unfortunately, I cannot be there in person, but really glad to be here to share this analysis or more like a data review about our current state of order flow block building latency game. So I'm a data scientist at Flashbots.
04:40:59.200 - 04:41:37.790, Speaker C: Cool. I'll go through my agenda next. So I'll go through how a ethereum's transaction lifecycle code today look like with an Mev supply chain theory. Or you can call it transaction supply network. And then the next is we'll look at how the order flow market looked like with CPBs or like MEB boost Me supply chain structure today. And then we'll look at how the block building market looked like and how different builders their behavior look like in data and also the latency game and time aspect of the block bidding market. Next.
04:41:37.790 - 04:42:52.656, Speaker C: So we have two diagrams here. So quick intro about mev and I assume everyone heard about mev Mpbs since you're in Mpbs day here today. But what is mev supply chain? In the past, in the proof of work day if you're making a trade the miner had the full control about if they want to insert your trade or if they can insert any other trade of their own or per their benefits to be before yours or behind yours in the proof of stake date. Nowadays with an even more concerning problem about Mev can cause centralization on stake and therefore cause centralization of the network. We introduced the PBS proposal builder separation proposal and Mev Boost is a implementation phone flashbot about it and the vanilla model of it is basically splitting the minor row into three rows from searcher. They will send bundles to builders and then the builders will bid for the full block and then propose to the validator. Validator will pick the highest value and then basically get the header of the block and then settle it to the chain.
04:42:52.656 - 04:43:57.076, Speaker C: And then nowadays with another thing called order flow auction, this structure can even look like the one below where searcher is sort of separated from this chain. Basically they take a new row as a bidder of the order flow auction. But again the flow goes like user submit a transaction to the wallet, wallet go through RPC, potentially broadcast it to a bunch of searchers or bidder and then they will try to search for the maximum extraction of this order and then submit a bundle to builder and then to validator. So this is a very how to say, important structure that we should remember about the analysis because I'll go through each layer of the market. Next one please. And so how does exact a user's transaction or order look like? On a concrete example of an order flow auction, we can use Flashbot's map share as an example here. So either user or wallet can submit it in representation of user or a RPC provider.
04:43:57.076 - 04:45:22.710, Speaker C: They can basically send to a flashbot protect RPC basically an endpoint where you submit your transactions through which then actually make your transaction go through a private mempool, and then it will broadcast to a bunch of matchmaker services. Which is the mev share services which will then broadcast the transactions information based on users privacy setting to a bunch of mev searchers and they will try their best to see okay, I'm trying to background this transaction without basically hurting users price and see if I can make any profits from making another transaction behind yours. And then they will compete to send bondos to the Mavshare services and then the service will try to compare what's the highest value so to submit it to the builders. In the past Flashbot started with submitting the bondos to Flashbot's builder but for improvement on the inclusion of the transactions. We also recently released a feature about allowing user to submit to any of the builder of their preference. In this case it will make the user settlement to be faster and better and then the builder will broadcast to a bunch of relay and then relay will then see who has the highest bid and the validator going to take the highest one. Next one please.
04:45:22.710 - 04:46:29.876, Speaker C: Another example to make it a concrete concept on a Dex orders lifecycle of today here's a lot of interesting things to point it out. So when a user comes into a Dex or a Dex aggregator, what they will do is basically they will try to check across the different liquidities on chain and off chain from their professional market makers that is quoting and making the RFQ order for the user. They will compare the price with Uniswap. They will check what's the best price for Uniswap SushiSwap or the other AMMS and then they will compare okay, this is the best order. I'm going to send this to the public mempool. Likely, as in the past, how we operate the AMM orders if they sent into public mempool then will be broadcasted to everyone who is listening to a node. And so here there is a bunch of math searchers or basically you could call them whoever is like accessing the public AMM orders will try to arbitrage you or search on this and submit bundles to builders.
04:46:29.876 - 04:47:40.028, Speaker C: Another route is that if now Dex aggregator or basically the applications submit their orders into a private mempool aka ofas then they will have their ofa searchers trying to background you in a much better informed and actually protected way and then submit through bundlers and to the block builders. So what are we seeing here? All these highlighted red blocks here if we go next slides, turns out today in our current state is that pretty much the same entities, the RFQ providers are market makers who are like professional trading firm. The toxic flow that is taking the arbitrage on AMML liquidities are market makers. Map searcher block builders are pretty much from a bunch of big trading firms. It doesn't sound so good. Next slide please. And with the coming change in our Ethereum network potentially account abstraction bundlers and maybe uniswap X fillers can again become the same players that's some psyops.
04:47:40.028 - 04:48:36.156, Speaker C: Next one please. So yeah, one conclusion or first observation is that the order is flowing right now through a bunch of monopolies who are basically accessing each layer or basically they are touching the order flow in each layer of the markets. Next please. Right. So another observation here is that with some data analysis we collected in the past, earlier this year, we were trying to do a monitoring on the public mempool, and we saw that there are a bunch of transactions that got settled through different block builders, were never seen mempool or in the public mempool. Which means there's an adoption going up for the private order flow by different builders. We can pretty much see from January to March it grow from like one, two or 4% up to 7% per block builders.
04:48:36.156 - 04:49:28.656, Speaker C: And this data I assume will be only higher after all the Ofa launch in April. Next slide please. Another angle of this data set is that when we look at different ofa order flow auctions transaction flow percentage out of all the EVM transaction percentage we can see that pretty much like map blocker and flashbots protect or flashbots map share are having around like 5% of EVM transactions. Out of all the transactions, Calibra also have a few thousands every day getting settled. So yeah, another observation is that private mempool will have adoption going up along with the adoption of order flow auction. Next one. And then we look at the block builder market share here.
04:49:28.656 - 04:50:42.392, Speaker C: The current summary will be basically it's a 2080 market where basically a few leading winners are taking the majority of the block inclusion share including Rsync builder, builder 69 Beaver Build, Flashbots Builders and along with them there are a lot of new, small and how to say strategic builders coming out including like Thailand F one B. We also have Block Sprout and block native being playing in the block bidding game for a while as well as important players. So pretty much we can see a diversifying of the participation of this block building game and then if we would summarize their behaviors in the next slide as a different type of builder. So we will be able to see that. Next slide please. So we'll be able to see that we pretty much can see a bunch of builders. They can be defined as neutral builder, flashbots runs, nonprofit builder which means pretty much it's a very naive setting in terms of how you are transferring the profits in the line.
04:50:42.392 - 04:51:32.212, Speaker C: We're basically transferring everything to the proposer and include everything our biding value. It's mostly basically a bond merging algorithm as of today. There's also strategic builder who are also neutral but they may take more approach to be more competitive. For example, they may keep some profits from some more profitable block when they know that potentially they are bidding higher than anyone else. They may subsidize the block value when they know they're maybe slightly lower, but they can win by just subsidizing a little bit. They may attract also private order flow by subsidizing and become better at inclusion rate. And then we also have another big type which are I would say like they're the most competitive today as block builders which are searcher builder.
04:51:32.212 - 04:52:36.916, Speaker C: If you remember earlier we introduced this concept of like in the MEF supply chain there is searcher who are basically checking all the order flows, mev value or potential extractable value and then trying to bundle transactions with them as their own private transaction flow. And then the next they will basically be able to submit their order flow to their own builder and basically to have a very exclusive amount of transaction which are highly valuable. So to compete with the block value, which means all those things are making them very competitive compared to others. Next slide. If you are trying to analyze their behavior on chain, they may think different. For example, if you are a typical builder, you're pretty much going to put your address on chain as a recipient first and then you will appear as a minor basically for this block as of today. But then in the end of the block, you will try to transfer as much as profit.
04:52:36.916 - 04:53:28.876, Speaker C: You are bidding for the the validator to receive as a transaction in the end of the block. But if you're a zero profit builder, you pretty much don't even need to touch any of the profits. You would maybe even just set the validator's fee recipient as the miner of the block, basically let them to receive everything to basically also save some gas. Next please. And when we look at the open data set that Mivoost is streaming for everyone from each relay, we can pretty much see that builders, they have different behavior or preference in terms of which relay they are connected to or submitting their block bidding to. But pretty much every builder would want to have a higher or better coverage of different relays. And then most of them submit to the three major relays like flashboards and builder 69.
04:53:28.876 - 04:54:23.980, Speaker C: But we also see Rsync and the block Sprout, they are submitting to five or even seven pretty much like every relay. So to make sure they're competing at their best. Next please. So there's few more metrics here that's very interesting, which is like when we talk about the time aspect of the biding, it's a whole latency game for you to be competitive and taking basically like Sniping at the very last second right before the validator is checking which block is the most profitable. So we call this validators get header timestamp which is basically whenever the validator is checking, okay, what's the available best bit I'm getting right now? And they're checking with the relays. And it's very interesting to see that different validators here, they have different behavior in terms of when they. Will prefer to check the get header.
04:54:23.980 - 04:55:04.764, Speaker C: Some may have very consistent data points of when they're going to check it. And that actually very interesting, very interestingly result into builders behavior, which is basically they will never actually bid from the very beginning of a slot, but only at the very few minutes, very few seconds, which is when the validators will check the Get header. Our flashbots builder in the past were pretty naive to check since the very beginning of the thought. But we made some improvements after this realization and we updated the biding time to be as same as we're on like -3. Seconds. Pretty much. Next one, please.
04:55:04.764 - 04:56:20.660, Speaker C: And another aspect is that if you check how frequent each builder can update their bids as basically their bidding latency next slide, please. We pretty much see that this is a box plot of different builders updating time for the latency basically bid updating latency chart. If we zoom to the next one, we can pretty much see the number here that on the very right. Beaver and Rsync are the one that has the highest performance where they can update their bids within 200 milliseconds or 100 milliseconds, which makes them to be able to react very fast towards the market. Whenever they see a price shift, say on centralized exchange, their searcher will be able to prepare a new bunch of arbitrage transactions and then update their block from their builders. This also caused a very interesting observation in the next slide is that with some improvement in some relay, like optimistic relay from ultrasound, which is basically allowing the builder to skip a few hundred milliseconds by adding some collateral. Deposits.
04:56:20.660 - 04:57:28.380, Speaker C: We are able to see that searcher builder especially can benefit from this improvement largely if we see here the blue line is a wing rate of their optimistic submission and then the orange one will be the normal one. So it can be even like seven or eight times higher when they are submitting optimistically. But yeah, again right now today only searcher builder can benefit from this improvement because they're only one that can react this fast and update their strategy. Yeah, a few last metrics about the relays relay also as of today, they are supposed to be the one that's not taking any profits from the mev supply chain, but also taking a very important role of validating the blocks. They are receiving from the builders. As of today, a bunch of players are running a bunch of relays, the biggest one are Flashpods, Gnossis and Ultrasound. We can see pretty much ultrasound is the leading one here with a bunch of improvements, taking a quarter of market share and then it's Flashbots.
04:57:28.380 - 04:58:39.008, Speaker C: Next please. One very important thing is for relay is that the adoption from the validator side, basically how many validators are checking the bids from the relay is pretty much a metrics here to show flashbots has the highest adoption as a relay and then Vlogs rod, Max profit, ultrasound, Gnosis. Next please. On the timescale, we can also see that across different slot time, we can see like per every 100 slot, we can see flash rods having around like 90 and a little bit higher adoption by all the validators across 100 slot. And then the next is blocks Route, Max profit and Gnostics and ultrasound. One last metrics here is that we can also see that the diversity of the builders, basically how many builders are submitting the bids towards a relay matters in terms of how many total submission you will get naturally. And also we can pretty much see ultrasound and flash routes are leading towards the right and also are leading towards the higher market share.
04:58:39.008 - 04:59:22.320, Speaker C: So can see a pretty strong signal here of the correlation. Yeah. So next please. Yeah, that's a data review of all the layers of the market structures. And my final note would be if you're a Ethereum user, you should definitely check out the Flashbots protect RPC today. And so to avoid uninformed mev and potentially get refunds from our ofa service mev share, if you're an Mev searcher, I have a few memes for you in the end. Yeah, so if you're an Mev searcher, please send us bondos if you click through the next four slides.
04:59:22.320 - 04:59:30.700, Speaker C: Okay, cool, thanks.
04:59:34.690 - 05:00:11.060, Speaker A: No memes today, unfortunately. Sorry, Danning. Okay. I'm Robert Miller. My slides will hopefully appear at some point in the future, and until then you are stuck in silence with me. I'm told they are running. Okay, so I'm Robert Miller.
05:00:11.060 - 05:01:46.628, Speaker A: I have a talk here, obviously, and in thinking about what to talk to you all today about, I thought first, should I say something about PBS? It's PBS day and I thought, no, you probably have heard something about PBS before. At this point, you know about it, someone else will talk about it. Then I thought, should I talk about Suave? We just released a new blog post outlining an architecture introducing the mevm. I thought, well, I'm not sure that's the most interesting thing since you can go and read my blog post on how the mevm and our latest design for Suave works. And looking at the past schedule that I had this week, it seemed that there are a lot of new chains and domains that are interested in integrating PBS and they're interested in knowing about how Suave can support them if they do support PBS. And so that's sort of the premise of my talk, is how does Suave support many different domains, heterogeneous domains that have different virtual machines, different ways of ordering, and what is the architecture that enables that? And I'm going to begin by giving you a little bit maybe these slides are a little different than I expected, but I will begin by giving you some context on the thinking behind Suave and where we were a few years ago when the early ideas of Suave started to coalesce for Flashpots, something like late 2021. So this is kind of the first idea.
05:01:46.628 - 05:02:40.470, Speaker A: Mev is fundamental, it's fundamental to every domain, and that's for a few reasons. The first is this ability that blockchains give you to make commitments and you can commit to pay for some arbitrary state and that then alters the incentives of the party that's ordering transactions. Second is that we use this ability to commit as a part of a common design pattern in building permissionless systems. So these commitments are a fundamental part of something like DeFi in liquidations. You give a permissionless bribe to anyone who will update the state of your collateral when prices have moved. Or in CryptoKitties, you give a permissionless payment to anyone who calls a function to update the state of the system. So this permissionless bribe, this permissionless commitment that anyone can capture, is kind of this common design pattern to the systems that we're building.
05:02:40.470 - 05:03:30.756, Speaker A: And lastly, mev is fundamental because even if you don't have mev on one domain, there exists mev between the boundaries of different domains and that creates value extraction opportunities. If there was no mev on Ethereum, there would still be mev between Ethereum and BSc as an example. So that was the first idea. Second idea, mev has these negative effects. So rational actors pursuing their incentives, pursuing these permissionless commitments that make you money are going to create negative externalities in their wake if you don't deal with it appropriately. Says things like priority gas auctions, spam, latency wars that lead to wasted block space network load at the limit it can destabilize consensus itself. Which is where Mev began with Flushboy 2.0
05:03:30.756 - 05:04:16.832, Speaker A: economic centralization of validators integrating with trade firms and sandwiching, et cetera. There's all sorts of ways in which mev can have negative effects if you don't properly deal with this. And we unfortunately have a slightly old version of my slides, but we will roll with it. So what do you do about this? What Flashpots is trying to do is our project called Suave. So that stands for the single unified Auction for value expression. It's our project attempting to address this really ambitious problem of mev being fundamental to every single domain. It having negative effects and there being many different types of domains in the future that have heterogeneous different designs.
05:04:16.832 - 05:05:17.476, Speaker A: So there are non EVM chains, there are chains that will order with first come, first serve and we can't just copy paste Flashbots and other people's mev infrastructure to every single domain. How do we offer mev solutions to many different domains in a way that is actually decentralized? And what I want to get across today is that there's these two key ways that Suave addresses this in the broadest way possible. And again, we're rolling with an old set of slides, but that's fine. So the first is the mevm we just announced this last week. It's this modular, expressive, composable programming framework that allows developers to leverage credible and private compute. And I'll dig into that a little bit more in a moment. But what that means is you can take what is currently centralized off chain mev infrastructure and turn it into a smart contract on a decentralized blockchain.
05:05:17.476 - 05:05:54.652, Speaker A: And the second is what we call execution predicates. This is arbitrary preference expression on the execution you desire. Again, we'll dig into that. So first the mevm, it's a modified version of the EVM that Flashbots is launching. We've taken the normal EVM, we've added new pre compiles to it for mev use cases. That's really useful because by exposing every primitive in the mev supply chain as a pre compile, you can program your mev infrastructure as smart contracts on a blockchain, which you think is really cool. And moreover, you can do it within the familiar developer tooling of the EVM.
05:05:54.652 - 05:06:28.670, Speaker A: You can do it in foundry, you can do it in your normal developer environment. And it really lowers the barriers to entry of deploying new applications because anyone who knows solidity can do it. So, how does the Mevm work? This is what the architecture looks like. There are a few stakeholders, a few components. I'll run you through each of it. So at the core of Suave is this mevm chain that we're launching. Again, it's standard EVM with extra precompiles for mev use cases that's things like take a transaction, simulate it, return me the result.
05:06:28.670 - 05:07:11.352, Speaker A: We have a confidential data store in the middle. You can think of this as a data availability layer where encrypted data is stored because there's too much data to store at all on chain. So this is decoupled from the chain state itself. And we have an execution node. This is not a consensus node, but it's tightly coupled with the chain. It's a node that provides private and credible compute that can be called by the mevm's precompiles. And the way that these different stakeholders interact with Swab is developers define contracts like order flow, auctions, block building algorithms, centralized RFQ routers, maybe users send what we call bids.
05:07:11.352 - 05:08:18.636, Speaker A: That's a combination of contracts that they authorize to access their private data and some private data like a signed transaction and executors who are your arbitrageures, your searchers builders are looking at bids, seeing how you can interact with them and trying to backrun them, merge them, do mev things. Finally, SUAVE produces blocks for proposers that are listening to them to dig into the notion of execution nodes a little bit more because this is a really important concept. The actual chain swap chain itself stores what bids are pending. It doesn't store any private data that's stored in this off chain confidential data store, but it registers a user's bid. So a user saying like hey, I have this private transaction, I want it included on chain. And it also registers these contracts that developers have programmed their mev applications in. And those contracts, when a user authorizes them are executed within what we call execution nodes, where an execution node will read the code of a smart contract and execute it accordingly.
05:08:18.636 - 05:09:16.820, Speaker A: I'll give you an example in a moment, but as an example, a smart contract might say simulate this transaction and allow a searcher to try to backrun it, and if the searcher successfully backruns it, send that on to a blockbuilding algorithm. We run these execution nodes in trusted execution environments like in SGX to provide some level of integrity and privacy to it that isn't contingent on centralized trust in a party like Flashpods. This is an example meDm contract, so it's standard Solidity with a couple extra pre compiles. So you can see here the fifth line we get ethereum state, something that you normally can't do within Solidity. We're deploying this on swab chain, but we can have access to the latest Ethereum Mainet state. It takes a bunch of bundles that are sent as inputs, simulates them, so it's simulating ordered lists of transactions. In its ethereum Mainet state.
05:09:16.820 - 05:10:36.920, Speaker A: We're getting the gas price of those, sorting them, trying to apply them to a pending block. So it looks really similar to Solidity because it literally is. But it is able to leverage these new mev precompiles that call out to the credible private compute provided by the execution node in a trusted execution environment to let someone program a block builder, which right now is this monolithic thing off chain in Solidity on soft chain, which we think is pretty cool. And one of the ways that we are thinking about scaling SUAVE to many different types of domains is that you don't need to only run EVM type execution in execution nodes. So you can have an execution node that supports WASM or you can have an execution node that supports any weird roll up VM that exists and then users can program their smart contracts to call out to that within native EVM on swab chain. So you can do block building for non EVM chains so long as you have these execution nodes for different domains. So for every given VM we'll have execution nodes that folks will run in these trusted execution environments and you have this single unified platform for writing mev infrastructure that can access many different types of execution nodes.
05:10:36.920 - 05:12:01.764, Speaker A: That's one way that we are attempting to address mev across many many different types of heterogeneous domains. The other way that I think is really interesting to talk about is this notion of execution predicates. You may have heard this as intents or preferences before, but this lets users define arbitrary conditions that they want to be executed for. So in familiar nev use cases, I want to pay x if transaction hash zero x baba is included in block 100, I want to pay only if this array is included. Or things that you can't do today, which might be interesting for some use cases like I want to pay if my contract emits a log. And why I'm talking about this is because this unlocks new searching strategies that you can't do with current Mev infrastructure today. So as an example, I want to pay one ETH if you call my contract and it emits a log, provides an incentive for either latency or spam searchers to try to hit your contract until it's successfully called, right? So you could deploy this contract on Suave, have a corresponding contract on a low latency domain where spam is the dominant strategy, and it provides an incentive for this marketplace of people to spam your contract on your behalf.
05:12:01.764 - 05:13:08.344, Speaker A: So democratizing mev search on other types of domains, even if they're not natively integrated into PBS itself, which is very interesting. Or cross domain I want to pay for x state on Ethereum and Y state on Arbitrum gives this incentive for atomic execution across two different domains. And since you are programming this in solidity as a smart contract you can get as bespoke as you want with these execution predicates which is really interesting because searchers can express or users can express conditions that are more complicated than anything the market currently supports at the moment, which either incentivizes people to collaborate together to execute on these conditions or it incentivizes the creation of new infrastructure to be able to support more complicated types of predicates. So that was my talk. Slides got a little bit screwed up, so I had a little bit more content, but that's okay. So thanks for listening. That's how we plan on addressing many different domains for Suave, regardless of whether you are PBS or not PBS.
05:13:08.344 - 05:13:34.630, Speaker A: We have a couple different call outs here, so if you're interested in integrating as a roll up, please reach out. If you're a developer interested in building mev applications on Suave, please reach out as well. Wallets if you're interested in Mev redistribution searchers for private order flow, and that is a QR code if you want to work at Flashbots. So thank you so much, really appreciate it. I think we have Mike coming and talking about PBS next.
05:13:41.500 - 05:14:09.760, Speaker B: So thank you, Bert. The correct sets of slides are up on Mev Salon, so if you go to Mev Salon, you can find the entire day's agenda with the correct version of the slides. So for the alpha that you missed next is our chapter one, the Ethereum L1 PBS, starting with Mike Neuter on PBS RND Roadmap.
05:14:18.150 - 05:14:44.606, Speaker A: Hey, everyone. How's it going? Cool. Looks like the right slides. This isn't the right slide deck, actually. Let's see. Okay, this looks promising. Oh, no, this is the wrong slide deck.
05:14:44.606 - 05:15:03.850, Speaker A: Tina, 1 second. Tina, this is the wrong slide. Our slides have been reordered.
05:15:10.450 - 05:15:18.080, Speaker B: Yeah, well, at the meantime, you can have a sneak peek preview on Me V Salon. Just click on the slides. You can help yourself.
05:15:24.700 - 05:15:26.010, Speaker A: No problem.
05:16:06.880 - 05:16:39.844, Speaker B: So while we're fixing some technical issues here, curious at modular summit. How many of you all here are working on Ethereum stuff? Raise your hand. All right. What other ecosystems are you working on? Nice. Wait. Is that ethereum ecosystem. Well, that's fair.
05:16:39.844 - 05:17:11.520, Speaker B: Anyone else? Okay, sizable contingency right there. Hi, friends. Are we ethereum aligned. Do you want to do an impromptu lightning talk of two minutes of solana PBS? Let's go. Okay, now we have our favorite malady.
05:17:14.660 - 05:17:25.160, Speaker A: I don't know about favorite malady, but a lady. All right. Hello, everyone. I'll go back one. Cool. Yeah. So, my name is Mike Neuter.
05:17:25.160 - 05:17:42.996, Speaker A: I work at the Ethereum Foundation. And today I'll be talking about ethereum. PBS. R D roadmap. So a quick sketch of my talk. I'll first go through what we define as PBS. This is kind of a buzzword lately, so I'll define it in terms of what we mean for the Ethereum consensus layer.
05:17:42.996 - 05:18:20.484, Speaker A: And then I'm going to run through kind of rapid fire style five implementations of PBS. There are some overlaps between them. We can kind of see the similarities, but I'll present each of them kind of in their own right. And then after that, I'll try and synthesize by pulling them together, explaining the design trade offs between each one, the timelines and yeah, kind of try and connect the dots in a way that hasn't been done very well yet. Because I think a lot of the proposals and the discussion has been scattered across different forum posts and Twitter threads. So, yeah, that's kind of what we're going to go through. Cool.
05:18:20.484 - 05:18:59.624, Speaker A: So, defining proposal builder separation in Ethereum l One. This is a post from Vitalik in June of 2021. So this was already over a year before the merge. And the idea here is to keep the proposer set, which is kind of the decentralized set of validators that are selected through proof of stake, to propose blocks for a given slot. And we want to keep them relatively unsophisticated. So we want it to be kind of accessible for people to run these validators at home on consumer grade hardware without much. Sophistication on the other hand, the builders are the people who are going to construct very valuable blocks.
05:18:59.624 - 05:19:44.604, Speaker A: And what this means is they're capable of extracting a lot of mev. This might require liquidity on different exchanges. It might require a lot of compute intensive algorithms because there's a lot of different permutations of transaction ordering to try. And so these will be highly sophisticated actors. And the goal is to decouple these two roles to avoid the centralization pressure. And what I mean by centralization pressure is that if we don't kind of formally decouple these two roles, then there's very likely to be a world in which builders collude with large staking pools. Staking pools are incentivized to do this because they can kind of benefit from the builders who are really specialized at extracting mev.
05:19:44.604 - 05:20:40.128, Speaker A: So they get higher rewards, and the builders basically collude with them and yeah, the reason that the collusion is able to be kind of credible is because the staking pools have reputation and this reputation isn't something that would be accessible to the kind of everyday solo staker. And this kind of fits into the broader picture of the Vitalik's endgame post, which is the idea that block production is going to be highly specialized. We kind of have to accept that and firewall off the centralization from the Decentralized validator set in some way. So that's what proposer builder separation aims to do. Cool. So this is the first of the five designs I'll run through and oh, I should say the designs I like to categorize in terms of in protocol versus out of protocol. And what that means is if it's in protocol then it's something we change in the consensus layer spec.
05:20:40.128 - 05:21:10.472, Speaker A: It's part of the honest behavior of being a validator in the Ethereum proof of stake network. Oops, sorry. Oh yeah, this is right. But if it's out of protocol, then it's kind of like the sidecar thing that the consensus layer isn't aware of but maybe many of the validators are making use of. So Mevboost is what exists today. This is the software that was written by Flashbots right after the merge. And the idea here is that there's a third party to facilitate the relationship between the builder and the proposer.
05:21:10.472 - 05:21:44.256, Speaker A: And this third party is called the relay. They broker the trust between the two and conduct this auction to give the proposer the highest paying block from the builder. So builders are sending blocks to relays. Relays forward the headers onto the proposers and the proposers commit to a header without actually seeing the block. So the trust assumption from the builder to the relay is that the relay doesn't steal the builder's mev. The proposer trusts the relay to serve them a valid header because they're signing some header to a block without actually seeing that that header corresponds to a valid list of transactions. So the trust is brokered there.
05:21:44.256 - 05:22:26.308, Speaker A: And the way the Relays interact with this is they run this sidecar software called Mevboost. So Mevboost kind of interacts with the consensus and execution layer clients and outsources the block production rights to this external block building infrastructure called the Relay. And we see very massive adoption, like great product market fit for mevboost. So 95% of Ethereum blocks are being built using this infrastructure immediately after the merge. It kind of skyrocketed from 50% all the way up to nearly every block being built through Mevboost. Cool. The next design I wanted to talk about is kind of the original two slot PBS design.
05:22:26.308 - 05:23:29.610, Speaker A: And I would say for the past two years about this has been kind of historically what people referred to as enshrined PBS or in protocol PBS. And this is a post from Vitalik and he presents this idea where you separate each slot into two blocks, basically, or two slots so there's a beacon block. The beacon block contains some commitment from a builder to build a block and the intermediate block or the builder block actually includes the set of transactions. So kind of logically we're just splitting one execution layer, update one set of transactions into two blocks and it's this kind of commit reveal scheme that we use in Mevboost. It's just authenticated and kind of authorized through the protocol itself. And the way a testing committee works is the testers are the people who vote for different blocks kind of saying, I think this is the head of the chain and the attesting committee is split between the first block and the second block. And yeah, the partition happens.
05:23:29.610 - 05:24:03.652, Speaker A: And the kind of TLDR of this is that it actually weakens the security properties of the consensus layer. And I won't go into the details of this now, but because of this we thought of a new design and this is pretty recent. This is a few weeks ago and it's called the Payload Timeliness Committee. And this is another in protocol solution. I should say the previous solution was also in protocol. So these are changes to the consensus spec that facilitate the PBS implementation. So this is quite similar to Vitalik's two slot design.
05:24:03.652 - 05:24:44.188, Speaker A: The difference here is that we have a consensus layer block similar to before. It contains a commitment to a builder bid. The builder bid just basically says like, this is how much value I'm willing to pay to produce the block in your slot. And it also contains the header, which is like a commitment to the list of the transactions. So this is voted on with the testing committee. This is all like as it happens today. The only difference is there's kind of a hole in the block where the execution payload will go and the builder reveals the execution payload once they're confident that the proposer proposed will become on chain.
05:24:44.188 - 05:25:27.156, Speaker A: And then we have this thing called the Payload Timeliness Committee, which actually makes these votes to the timeliness and the availability of the payload. So it's quite similar to before. The differences are mostly in the fork choice rule. But the important thing here is we think this has better reorg properties in terms of securing the consensus layer. Cool. Now, I wanted to present one other kind of out of protocol solution for PBS and this is what we're calling optimistic relaying. And the difference here is that instead of kind of doing the in protocol version where you go through EIP process, you modify the consensus spec, you update the client software and then you perform a hard fork.
05:25:27.156 - 05:26:26.604, Speaker A: We have this relay infrastructure that's already running. We already have 95% of blocks using the relays. So we can make the Mevboost relay ecosystem look slightly more like the Enshrined PBS version by changing the code that actually runs on the relay and reducing the relay responsibilities. So this diagram shows the block submission flow for mevboost. And you can see there's kind of a lot of tasks that the relay does and a lot of latency involved in those tasks. So the builder submits the block, the relay validates the block against the execution layer and then publishes the block through the consensus layer client. So the idea is, how can we make this look more like EPBs? And when we say EPBs, that's enshrined or in protocol PBS, like, how can we take the infrastructure today and evolve it towards what we want it to look like in protocol? And the idea is we can just work from what we have today and remove some of the relay responsibilities.
05:26:26.604 - 05:27:04.220, Speaker A: So optimistic v one, we say, okay, take the block validation out of the system. This saves some latency. It also makes the relays easier to run because now they don't have quite the operational and compute costs of simulating all these blocks very quickly. And the way we protect the proposers is we say if a builder ends up submitting an invalid block, that wins the auction, we'll have some builder collateral and we'll execute some out of protocol builder slashing. So we'll just say, okay, we're going to use the builder collateral to refund the proposer. No problem. That's how we get around removing the validity check from the submission.
05:27:04.220 - 05:27:44.772, Speaker A: The second version of the optimistic relay is actually removing the block download from the fastpath. So in the same way before, if the builder submits a block or doesn't end up sending the entire block contents to the relay, we say, okay, we're going to slash the builder. That's a penalty that will be able to use the collateral to refund the validator. And the endgame version of optimistic relaying. And these diagrams are kind of wordy, so don't worry too much about the details. But the end game here is that the builder and the proposer actually interact through the PTP layer. So the builder gossips the bid, the proposer gossips the signed bid.
05:27:44.772 - 05:28:40.460, Speaker A: And the relay essentially just takes a snapshot of the mempool of the PTP layer at a given timestamps and attests to the accuracy and the timeliness of different events. And really, if you kind of break it down, this is kind of what the payload timeliness committee does. But it's just being done by the relay itself and the relay holds the collateral rather than the protocol. So this is kind of the molding of the relay responsibilities and the relay efficiency into something that should potentially be more compatible with enshrined PBS. Okay, and the last PBS design, and again, bear with me, the remainder of the presentation is about drawing the connections between all of these. The last one I wanted to talk about is called proposer Enforced Protocol Commitments. Again, this is an in protocol implementation and it represents a much more general framework for allowing proposers to commit to things in the protocol.
05:28:40.460 - 05:29:23.720, Speaker A: And it's actively being worked. On by barnaby and Diego. And the idea is that you use block validity to enforce commitments. So proposers commit to doing something and if they don't fulfill that commitment in the block that they produce, that block just won't be valid according to consensus rules. So the kind of cool thing about this general framework is that PBS can be enshrined through these protocol commitments. So you can have a proposer commit to a builder and then if the proposer block doesn't include that builder's bid, then the block won't even be considered valid. So the difference here is we check the block validity, we use that to enforce the proposer commitments rather than the testing committee.
05:29:23.720 - 05:29:56.608, Speaker A: Cool. So that was kind of a fire hose, but hopefully the next few slides will help connect the dots and draw the relationships between all these things. So on the top row, I have two slot, PTC, payload timeliness committee and PEPC. So those are the in protocol versions of PBS. The bottom row is Mevboost and optimistic relay, which are the out of protocol versions. So PTC and two slot are really closely related, but basically we trade off what's called reorg resilience. Or this is part of like the consensus stability, the consensus safety.
05:29:56.608 - 05:30:42.012, Speaker A: We trade it off for builder safety. So we're saying, okay, if we're choosing between these two, basically we decide how much we want to give the builder assurances versus how much we want the protocol to have certain properties to go from PTC to PEPC. We use block validity instead of a committee. So this is kind of like who is doing the enforcement of these commitments and these checks and PEPC is doing the commitment at the block validity level on the bottom. Oh, I should say also this is the heart diagram, so hopefully that will become clear in the next coming clicks. The bottom we have mevboost, which evolves into the optimistic relay. And this is just relay evolution.
05:30:42.012 - 05:31:27.900, Speaker A: And this is kind of already happening. Some relays are running optimistic, some relays have private closed source versions of their software. So this is kind of like already in progress of happening. And if we want to go from two slot to PEPC, we can kind of view it as a more general version of doing PBS. So PEPC doesn't put any constraints on what the exact mechanism you use to do the PBS. It just says have some way for the protocol to enforce commitments from the proposer. Now the left side of the heart is as mevboost evolves into two slot or the payload timeliness committee, we can kind of think of that as just enshrining the existing commit reveal architecture of mevboost into the protocol.
05:31:27.900 - 05:32:06.780, Speaker A: So that's like the enshrinement path for those two things is through that evolution. And then the right hand side of the heart is evolving the optimistic relay into the payload timeliness committee. And this is essentially just as I was saying before, replacing the relay responsibilities with the payload timeliness committee responsibilities. And the cherry on top is Mev burn and Inclusionless. So these are kind of two additional features that are hopefully going to be part of the protocol. I'll give a quick overview. Mev burn is the idea that we want to take Mev and instead of giving it to the proposer, we want to burn it.
05:32:06.780 - 05:32:45.740, Speaker A: So kind of provably destroy it through the protocol. And Inclusionless is a way we can say we want a certain set of transactions to be kind of force included into a slot. So this is a mechanism to enforce censorship, resistance. And both Mevburn and Inclusion list depend on one of two slot payload Timeliness Committee or PEPC. So that's why the three dotted arrows kind of come down and show the dependency between those two and one of the in protocol solutions. And the key takeaway here is that you can't do either of those things with an out of protocol solution. So that's kind of one of the big cons of having this out of protocol software.
05:32:45.740 - 05:33:28.840, Speaker A: Cool. So that was kind of like a mind map of the relationship between all of the five designs. I also thought it would be cool to kind of go through a few specific categories and compare the designs kind of directly versus each other. I clumped optimistic relaying and mevboost into the first column. The second column is just Payload Timeliness committee or two slot and the third column is PEPC because PEPC is kind of its own variant of a design space. So the first axis I guess we can evaluate them on is the time to ship the protocol diff. So the Mevboost relay is already running.
05:33:28.840 - 05:34:04.064, Speaker A: Optimistic Relay roadmap is kind of in progress. This is something we can do now. Oh, I should say the three colors, red, yellow or sorry, green, yellow, red, green is kind of the good outcome, yellow is the medium outcome and red is the bad outcome. So that's why I call it the traffic light matrix. So yeah, Mevboost is already running. It's kind of already in existence today. PTC we think of as a relatively minor diff, like there's a big change to the consensus layer, but it's not so dramatic that it requires a whole consensus layer rehaul essentially.
05:34:04.064 - 05:34:53.860, Speaker A: And PEPC is really dramatic insofar as it allows proposers to commit to kind of arbitrary things. There's a lot of design considerations around what those commitments look like, how those commitments are encoded on chain, et cetera, et cetera. And you can kind of think of PEPC as almost like an enshrined eigen layer. So there's all sorts of kind of tail risk at least associated with kind of opening up the doors and letting people to commit to arbitrary things as proposers. The next axis I wanted to present was decentralization. So Mevboost is kind of by definition centralized. There's only about eight relays that handle all the blocks and that's kind of part of the expectation is that there's only going to be a few relays and a few builders that connect with those relays.
05:34:53.860 - 05:36:00.104, Speaker A: PTC is extremely decentralized in that it shouldn't change the validating role at all. Basically being a validator changes the role that the validator plays in the ecosystem has very minor differences. So in that regard, it has the same decentralization properties that we have today from the validator set. And PEPC I put as yellow because there's a world in which PEPC involves many more commitments. Those commitments might require higher compute, higher bandwidth, and as a result, there might be kind of a competitive advantage of joining a staking pool that is able to run more infrastructure in the same vein of arguments that present eigen layer as a potential centralization vector in the system bypassability. So this is the idea that there's kind of some way to get around the mechanism that we design and for builders to collude with proposers. Typically the process is builders colluding with proposers to avoid any kind of tax imposed by the system or any inefficiency imposed by the system.
05:36:00.104 - 05:36:52.324, Speaker A: So mevboost, I say, is not really bypassed or it's green in that regard because the builders already have close relationships with the relays. The relays kind of do the builder's job of connecting to the validators. So there's not really any incentive for builders as currently existing today to go around the relay because the relay is essentially like subsidizing the builder's job for them. For the PTC, I put it as red because there's kind of no way to enforce that the proposer actually uses the mechanism that we put into the protocol. The relay infrastructure would probably still exist. They could just outsource their block production to the relay rather than using the PTP layer to get their blocks. PEPC I put as yellow because there's a world in which the enforcement of the commitments and the set of commitments that a proposer can make makes it harder for the builder to circumvent the protocol.
05:36:52.324 - 05:37:54.444, Speaker A: And in that regard it might be better in terms of bypassability, in terms of censorship, resistance, I put mevboost as red because there's basically no way to do inclusionless, there's no way to force through transactions in today's relay ecosystem. But both PTC and PEPC are very out of the box compatible with inclusionless. And that's the censorship resistant design that we're looking forward to in the future. Yeah, the next one is flexibility. So in terms of what you can do with the relays, I put it as yellow because it seems like there's a relatively large amount of things you can do. But since it's out of protocol, you can't enforce things like mevburn, you can't enforce things like inclusion lists so the relays can evolve. And it's kind of a latency game where the relays are maybe competing with each other, maybe updating their software in a closed source way, but in the same vein.
05:37:54.444 - 05:38:49.888, Speaker A: They're not able to do anything through the protocol because it's this out of protocol software. The PTC flexibility is red because really we're just saying we're going to take an opinionated stance on what we're going to include in the protocol and it's going to be this commit reveal scheme very similar to mevboost, and we're just going to kind of make it happen in the short to medium term. PEPC, on the other hand, is maximally flexible because you're saying these proposers can commit to any arbitrary thing, essentially, and we want to give them the flexibility to decide if they commit or if they don't. And those commitments are going to be enforced at the protocol layer. Okay, two more unknown unknowns. From the relay perspective, I don't really think there's any unknown unknowns because we're already kind of in this regime for PTC. We think the spec change would be relatively minimal and something that we can envision kind of in today's world.
05:38:49.888 - 05:39:28.380, Speaker A: PEPC I put as red because I keep calling back to the Eigen layer thing, but we're just really not sure what people will commit to. We're really not sure how those enforcements will go. It has kind of increased slashing potential. There's just a lot less we know about the potential tail risk events that this type of system enables. Lastly, mev burn compatibility. This is something that I mentioned in the heart diagram, but the idea here is that in order to do mevburn, you need one of the enshrined versions so you can't use the relays as existing. Today you need to do either PTC or PEPC.
05:39:28.380 - 05:40:01.896, Speaker A: And to wrap up, I thought just kind of like a TLDR Vibe check on each of the three. I would say that the relay approach is kind of move fast and break things. This is like the Silicon Valley style of we're all going to play this game, we're going to compete, and may the best relay win. PTC is more like let's get something in the protocol. Like the relays aren't very sustainable. There's talks about relay funding issues and sustainability around the current existing framework. Like it doesn't feel like we're in an equilibrium in any way.
05:40:01.896 - 05:40:39.060, Speaker A: So if we decide to go ahead with PTC, we could get something in protocol in the next? Yeah, hopefully months. And that's the goal of the middle column. The PEPC version is this is kind of an aesthetic thing. It feels correct in terms of protocol being maximally flexible and maximally idealistic in some way of letting the proposers to commit to things. But it could be opening Pandora's box. So we're unsure what we're getting at. There's kind of these tail risk events that I mentioned earlier, but it could be the right approach in the long run.
05:40:39.060 - 05:40:44.250, Speaker A: So yeah, thank you very much and that's all.
05:40:49.900 - 05:41:38.580, Speaker B: Well, since you did very well catching up on time, let's take one question. Well, looks like everyone is an expert in the future of Ethereum. All right, well, next up, we have Vitalik giving us a talk about zooming into the builder role, what more builder can do, and the future state of PBS. What could it be? But I had to ask since Danning's memes were cut, so would you please give us a PBS meme to make it up?
05:41:40.070 - 05:42:28.580, Speaker A: Real time meme challenge. This is difficult. I do not claim guaranteed success. So to get right into things Yodog, what's up? Who here thinks it's a ceiling? Who here thinks it's the ceiling that's up? Who thinks it's the sky? Who thinks it's like a cryptocurrency? Okay, well, let's just talk about aggregation. Okay, so aggregation, basically the core idea here is you have N things and you're trying to take N things and turn them into one big thing, which is less than N times as hard to verify. Right? Sorry. It's hard to speak and be yelled at for my poor microphone performance at the same time.
05:42:28.580 - 05:43:04.126, Speaker A: Okay, no, one more reprimand that. I'm throwing this out, and you'll have to listen to me speaking quietly. Okay, so what is aggregation? Right, it's converting N things and turning them into one thing, which is less than N times is hard to verify. Now, why do we want to do this? Well, because we care about scalability. We don't want to pay like, $80 a transaction or even zero point $80 a transaction. And we want to cut down on chain verification costs. Right.
05:43:04.126 - 05:43:50.134, Speaker A: There are a lot of different use cases of aggregation. In the ethereum consensus layer. For example, we have BLS signature aggregation, and there is many kinds of aggregation that we will have in the future. So some specific areas where we are thinking about aggregate, we might have aggregation. ERC 4337. Right? So this is this off chain protocol by which users can send in user operations that have validity conditions that are different from the validity conditions of a regular Ethereum transaction. You might have a smart contract wallet validation.
05:43:50.134 - 05:44:46.826, Speaker A: It could be a multisig, could be some custom winter knits lamport signature, could be whatever, and those conditions get verified. And it's this kind of off chain ecosystem for basically making these custom smart contract wallets work. But these user operations all have to be wrapped up in a transaction, right? Because Ethereum debased protocol only accepts transactions. Now, before ERC four three seven, in the dark old days, we would have protocols, but which every single user operation would get wrapped up in a separate transaction, right? So I send a user operation and then whoever I send it to would wrap that user operation in exactly one transaction, and then the transaction would go on chain. Every transaction would have a 21,000 gas overhead. So it's not very efficient. It also was not a very open protocol.
05:44:46.826 - 05:46:03.346, Speaker A: In ERC four three seven, we have a custom mempool where users send user operations into the mempool and then bundlers exist in that mempool bundlers, grab up all the user operations they can see in that block, and then they publish that on chain. Right? So this is a form of aggregation, right, because the object that went in the cost of verifying that object by itself would be N plus 21,000, right? And whatever the cost of the user operation is, 21,000 for the wrapper. But if you take like, let's say k of these and you put them together, then the cost is just going to be N times K because you still have to process all the user operations. Actually, sometimes a bit less than N times K because you can benefit from warm, cold storage, reading, gas stuff. And then plus 21,000 only once, right? So there's some pretty significant gas savings from taking user operations and putting them all in the same transaction. This is a type of aggregation, and this is something that's actually being done even in the builder ecosystem today, BLS aggregation. So this is a thing that actually has been recently kind of in proof of concept mode.
05:46:03.346 - 05:46:39.466, Speaker A: It's been added to ERC, four, three, seven. There's a piece of code called BLS wallet that actually does this. But the basic idea is, instead of using ECDSA signatures, we use BLS signatures. So why do we want to do this? This is especially valuable in a roll up, right? Because in a roll up, the computation is done off chain. The data is still done on chain, right? And so the computation becomes vastly cheaper. The data does not become cheaper at all, and ECDSA signatures on their own are 65 times N bytes. Okay, fine.
05:46:39.466 - 05:48:20.834, Speaker A: I'm using N to refer to the number of objects now and then with a BLS signature, if you use G one points for the signature, then the signature can be 48 bytes for an Arbitrarily large number of operations, right? And so if you have like 100 of these per block, then for each one of them, the amortized cost will go down from 65 bytes per operation to like half a byte per operation. On the computation side, it actually does get a bit more expensive because instead of doing fairly easy elliptic curve stuff involving adds and multiplies, like, I think at ECDSA, recover is like something like three multiplies or somewhere in that range, and instead you do a pairing, which is like pretty heavy duty stuff. But as I said, on rollups, computation is cheap, data is expensive, we're saving on data and it's great. Right? But how do we actually do BLS aggregation, right? Because the Ethereum protocol does not currently have an execution layer BLS capability, right? And so if you imagine N different users, each of those users is going to send their own BLS user operation and their own individual BLS signature that they created separately is going to be 48 bytes long. Right? Then what we want is we want the thing that goes on chain to be 48 bytes. We don't want it to be 48 times N, right? And so there needs to be some kind of actor that is grabbing up all of these user operations, going into the user operations, grabbing out the BLS signature, adding them all together. See, that's some kind of mechanism in the middle.
05:48:20.834 - 05:49:24.514, Speaker A: It's a big plus. And create one aggregate signature, 48 bytes. And that aggregate signature can be verified against the aggregate public key, which is created by taking all the public keys of the individual users and also adding those together. Right? So this requires two types of weird stuff, right? The first type of weird stuff is the off chain adder. The second type of weird stuff is the mechanism for verifying these signatures, where basically, instead of doing what Ethereum protocol does today, where you verify each of the signatures in sequence, the protocol or whatever super protocol we use somehow would have to take these signatures. Then when you walk through each one of the user operations, don't immediately just verify pass, fail it, but actually record the BLS signature. First of all, you'd verify it locally, make sure it verifies individually.
05:49:24.514 - 05:50:58.238, Speaker A: If it doesn't, you'd throw it out, but then record it. And then if you're doing this inside the block you just recorded, then keep walking through all of the user operations, add up all of these signatures, and then at the end, do the one check at the end, right? Because adding two signatures is like very cheap. It's like hundreds of times cheaper than even an ECDSA operation, right? So this requires a different workflow where it's like walk through the operations, then do a check and then execute everything. Now, ERC, four, three, seven actually already now has an extension to do this. But this is again a type of thing that I call aggregation, snark aggregation. So this is the future, right? So BLS is like very specific. What if I told you that in the future we're going to be able to take existing ECDSA signatures, potentially even signatures that are made with existing tools, and replace them all with a snark that just says here is a signature and here is a proof that or rather, here is a proof that there exists valid signatures for all of these things? Right? So this way you could replace an unlimited number of ECDSA signatures with one Snark and you could even apply this technique to other mechanisms, right? So who here has used a privacy protocol? Privacy protocols are good, right? Now, privacy protocols cost a lot of gas.
05:50:58.238 - 05:52:06.710, Speaker A: This is one of the bigger challenges hampering their broader adoption, right? Because inside of one of these privacy protocols, every transaction requires you to have a Snark that goes on chain. Now, the Snark 500,000 gas 20 times more expensive than a regular transaction. At current gas prices, that's going to be, I guess, about 20 GWe multiplied by 500,000 gas. So that's 10 million guay, which is 0.1 e multiplied by 19 $19 transaction fee. Who here is willing to pay an extra $19 to make your coffee more privacy preserving? I'll do it as a gimmick. Yeah, this is not good, right? But what we can do is, well, we can use a Snark to prove the validity of all the snarks, right? And so even if you have 100 different users using privacy protocols within a block, you just take their proofs, you prove them all together, and you have one proof of the proofs.
05:52:06.710 - 05:52:47.570, Speaker A: And now you have all of the proofs of all the private stuff within a single block. Right. Now, what does this require? It requires aggregation, right? You need to have some actor in the middle that it basically takes all of these objects and replaces them with an object that proves the existence of all of those objects. So this is all really nice. This all increases scalability, and this is all really good. Right, so what does this all depend on? It requires a builder ecosystem. By the way, who here remembers the cartoon character that this meme is based on? Bob the builder.
05:52:47.570 - 05:53:29.126, Speaker A: Yay. Oh, no, he gets see, Bob the Builder needs more love. We have Barbie these days, and we have all the Barbie songs. And we need a Bob the Builder movie. Can we do like, an OD chain gitcoin bounty for a Bob the Builder movie? And then use Zoo Poll to vote on the best r1 world crypto applications, please? We need builders, right? Builders are the heroes of ethereum block production. Right, okay, fine. I'll see.
05:53:29.126 - 05:54:58.354, Speaker A: I wonder if this style of mic holding is better. So the block building ecosystem is actually really big, right? We all kind of know that Bob the Builder is an abstraction. And in reality, there's like dozens of construction workers, and then there is the people driving the trucks, bringing resources over to the workers, and then there's the janitor, and then there's a whole bunch of other pieces of infrastructure, and there's like, really big teams that make up a construction company. Right, so black building ecosystem looks pretty similar to that, too. Right? So basically, you have a bunch of searchers, and searchers all have their own different strategies for creating bundles that contain different types of transactions, and then they all go together to a block builder, right? And then the block builder creates a block and then publishes it today to relays in the future directly to validators. And this is roughly how things work right now. Right? And so one natural question to ask is, well, if we have this kind of ecosystem, then who's going to do the Snark aggregation or the BLS aggregation or ERC four, three, seven aggregation? All of these things.
05:54:58.354 - 05:56:39.182, Speaker A: The most natural guess for the long term is like, we'll have specialized searchers, right? So we'll have like, a searcher that speaks the 4337 protocol, and then we'll have a searcher that does BLS, we might have a searcher that does a Snark aggregation. They might even end up talking to each other. There might even be some multilayer ecosystem where first you do the aggregating for the 4337 or for the BLS and the Snarks, and then you put that together into a bigger four, three, seven aggregate and that goes into a block along with non four, three, seven stuff. But I expect it will move towards something specialized like that. But I also expect in the shorter term, there's definitely going to be builders that just do this stuff directly, right? And I expect going to be a growing array of these aggregation subprotocols that gather these kinds of aggregate messages. So challenges in these ecosystems, right? So we want an open mempool, meaning a protocol that anyone can participate in, right? Not open in the sense of anyone can see your stuff and use advanced machine learning algorithms to figure out when okay, what should I even say here? Figure stuff out about you that you don't want figure stuff out about yourself. We want a protocol that anyone could participate in for both users and for builders and searchers, right? We want it to be open for users to send in their user operations.
05:56:39.182 - 05:57:41.238, Speaker A: We want it to be open for searchers to participate, and we want it to be maximally open for people to bring in new aggregation protocols. Now, that does require them to kind of accept permission from one builder or one searcher, but it should not require global permission from the ecosystem. So that's a goal. And we want to try to avoid centralization, and if centralization happens, we want to avoid censorship. Now, one of the challenges is that economics is weird, right? So in particular, the issue here is that a lot of these types of aggregation that I talk about, they have a high fixed cost and they have a low marginal cost. So what do I mean? What I mean is that if you think about like Snark aggregation, right, if you put a Snark on chain, the cost of a Snark is about 500,000 gas. Now, that Snark can represent lots of things.
05:57:41.238 - 05:58:33.734, Speaker A: If that Snark represents one thing, it's 500,000 gas. If the Snark represents 100 things, it's also 500,000 gas, right? It's fixed cost. Now, the other thing part is that it has a low marginal cost. The marginal cost does exist, right? Because if you're a prover, then you have to run more proving cycles, right? Like, if you have to prove five more things, the cost of doing the proof is five times bigger. Now, this is a cost, but generally these off chain costs are, I believe, already lower than on chain costs. And I expect these costs to continue to trend lower as we have more specialized proving techniques, as we have more specialized hardware and all these things. And I expect gas costs to be much harder to reduce, right.
05:58:33.734 - 05:59:35.686, Speaker A: The amount of gas on the ethereum l one has increased by a factor of five over the course of seven years. But the efficiency of Provers has increased by a factor of thousands over the last maybe four years, right? So when you have things that are high fixed cost and low marginal cost, like economics get annoying, right? It's like public transit systems, right? Like when you have a fixed cost to run a bus. And sometimes it might even happen that the system can't pay for itself, even if it makes total sense for the system to run, because the system has to charge one price. And it turns out that if it charges a high price, it excludes too many people, and if it charges a low price, it doesn't capture enough of the value. Right. And you're stuck, right? And so you have weird things like this. And unfortunately, these kinds of fixed costloge, marginal cost economics do tend towards centralization.
05:59:35.686 - 06:00:23.814, Speaker A: And sometimes centralization may be an argument for saying we want to give up on this whole higher layer idea and we want to just protocolize it. But the problem is this stuff is so early that it doesn't make sense to protocolize it. And how do we navigate that trade off in the meantime? Right. Is basically the challenge. Another really fun use case proof aggregation for ZK roll ups. So let's say you have a roll up, and who here is part of a roll up team? Who here is part of a roll up team different from the roll up team that the person over there is part of? Who here is part of the roll up team that's different from both of these roll up teams? Okay, I see a few raised hands. Yeah.
06:00:23.814 - 06:01:11.750, Speaker A: So we have at least three roll up teams here, right? So the problem is each of these roll ups are going to have to submit proofs on chain. And submitting a proof on chain is itself something that has a high cost, right? So if you look at a ZK roll up, it has to submit a ZK Snark on chain 500,000 gas. If you're a Stark, then it's like closer to 5 million gas. And this is expensive. What if we had multiple roll ups? And these multiple roll ups could cooperate and basically replace all of their individual proofs with a proof of the proofs, right? So you just have one proof that proves that all of the proofs that they want to submit are actually correct. Right. So this is kind of what the diagram says.
06:01:11.750 - 06:01:53.138, Speaker A: You have three different roll ups and they make three proofs of three state transition functions. And then you just make this kind of multi proof, which just submits as a public input the claims that the three proofs that are coming in are making. And then it sends it off to a batch handler. And then the batch handler basically just tells. The roll ups like, hey yo dog, I have a proof of this, I have a proof of this, I have a proof of this. And the roll ups accept it right out. This is a kind of L2 middleware protocol that could actually be really useful and it could be a lighter way to achieve the goals of reducing costs for L2s that's lighter than layer threes and some of those similar ideas.
06:01:53.138 - 06:02:47.458, Speaker A: And it's more minimalistic. It can be made to be more permissionless. I believe within the Starcore ecosystem, there's a version of this running already, right? But it would be good to have this as a potentially protocol that's kind of shared between an even larger piece of parts of the ethereum L2 ecosystem. And this is again a type of you're supposed to say the word yay. Okay, perfect. So the goal of this is the proof singularity, right? So basically my dream is we have a block and a block contains a proof, and the proof is a proof of proofs. And each of these proofs is itself a proof of proof of proofs.
06:02:47.458 - 06:03:12.202, Speaker A: And each of those proofs is another proof. So imagine the proofs at the end could be just signatures. Then some of them are signatures, some of them are privacy protocols. Then the proofs in the middle are going to be aggregation proofs. Then the proofs on top of those are going to be L2 proofs. And then with the L2 proofs, you have a merged protocol. And then it all gets into one proof.
06:03:12.202 - 06:04:06.770, Speaker A: And then maybe over and above that, you have one big master proof to prove them all. And that one big master proof is just going to be a zkevm, right? So this way we might actually be able to get to the point where we literally have blocks coming in once every minute. That just that prove everything. And everything in the ecosystem is going to be super efficient and super optimized, right? So this is what I hope for the ecosystem and it would be amazing to see. And builders are going to be a very important part of making this all happen. So this concludes my presentation. Thank you all for listening.
06:04:09.910 - 06:04:23.960, Speaker B: Thank you. Thank you for the meme. And we have time for one question, but I could also ask our next speaker to ask the question. If you have one.
06:04:28.010 - 06:05:39.600, Speaker A: Extremely exciting talk about the future of block building, my question would be these new protocols, how should implementers be thinking about the complexity of the implementations and about shipping them in a timely manner that can fit, that can make sure that we don't end up in a dystopian centralized future? How do we think about implementing these protocols? I think just standardization is important, and cross ecosystem collaboration I think is important. So I feel like we've been having a lot of that between L2s, which is good. I would love to see more of that happen between wallets. I would love to see more of that happen between privacy protocols. I think we just need to identify some of these kind of verticals and I think it might even be the case that per vertical, we might need an intentional effort to try to really figure out what actually makes sense to do and to try to sort of front run some of the more centralized efforts at solving the same problem.
06:05:44.020 - 06:05:58.550, Speaker B: Thank you. So next we have Giorgios who will be giving us a grand overview of our next chapter, the L2 PBS, which is a brand new frontier. There you go.
06:06:03.960 - 06:07:05.504, Speaker A: You so thank you all for coming and for not coming. This has been a very exhausting week and also a week where we've made great progress on a lot of the concrete parts about where the industry is going in this talk. I would like to give a few questions for the audience to think about, given that now we're entering the L2 part of the day and L2 part of the L2 part of the day. Many people here that you're going to hear next are working on it full time. I'm no longer spending as much time on this, but I'm going to try to offer a zoomed out view and some questions for the audience to think as people do their talks and hopefully to reach some deeper insights. My name is Georgias and I'm the CEO of Fardam. So we'll start with what is NL2? I see John in the audience and Togrul in the audience as well.
06:07:05.504 - 06:08:19.550, Speaker A: We'll see how that will go. We'll talk about shortly what the differentiators between L2s can be and what are net new exciting features that can be introduced. Then I will pose some interesting hypotheticals for shared Sequencers and then I will have one slide for Prover proposer Builder Separation or whatever the canonical name ends up, which our next speaker will also tell us about. So we will talk about L2 just from the context of not bridging because there's been many debates and I have no interest in engaging in any of them. The main thing about the L2s that we care about right now is that you have some off chain state, some off chain state in another place, in another place, and then that off chain state gets posted to a layer one, which is called the data availability layer. And that ensures that anybody that wants to recreate the state of the L2, they can go and look at the layer one, they can very easily derive it. Typically there is a deterministic derivation function different in each system which allows you to do that.
06:08:19.550 - 06:10:17.540, Speaker A: Beyond that, a L2 is an L1, it's a chain, it has a database, has a runtime, it has an RPC, has a peer to peer layer, it has a bunch of cryptography, maybe it's a standard distributed system. We know how to optimize them. These systems, they bottleneck on I O and state growth. If the thing that matters to you is decentralization and the ability for individual to verify, these are the two things that matter. I O bottlenecks, how fast you can sing a chain, state Growth bottlenecks, how big your chain can be, and if you want to scale ethereum or anything else, how you do it is by launching many of these layer tools. And the roll up Centric Roadmap is all about that. Now, some problems for people to think about are one, how are we going to do composability across the same flavor of a L2? So if I have two Op Stack chains and they want to talk to each other, how are they going to communicate without necessarily going through the layer one? If I have Op Stack and Arbitrum or ZK stack, or Stark Stack or I don't know, is there some extra difficulty in making these two communicate? Are these systems EVM compatible or whatever runtime they need to support? Or are they just looking like it on the outside and things on the inside look a lot different? And the natural follow up question to that is what does change in the internals and how does that impact the externals? How did an implementation detail make its way to the interface? And one example could be that people maybe have optimized for mev extraction on Geth because they understand, okay, these are the bottlenecks, but maybe, for example, on the polygon Zkvm, maybe it's different.
06:10:17.540 - 06:11:29.652, Speaker A: I don't know. Now, let's talk a bit about what are the unique things that you can get from a L2. First and foremost, from the sideshreams paper from 2015 by Blockstream. Experimentation, experimentation, experimentation. We can try new things without breaking the base layer, and then I will pose some bullets with some questions about them. So when we have faster block times and or soft confirmations, what does that mean about the mev extracted in these blocks? Does it really mean that more frequent blocks means less or more mev? What happens outside of that system? And how does the synchronization time between these two systems change, which directly, of course, affects the mev extracted? The proposal set in roll ups can be less decentralized without losing safety, with some cost to liveness, but not total degradation. Naturally, having a distributed system means it's more higher SLA, which is good if the system allows a custom transaction pool, it also allows for custom ordering FCFS.
06:11:29.652 - 06:12:20.780, Speaker A: We'll hear Patrick McCurry talk about that in a bit. I also don't know. But all of these are levers that one can pull depending on the mev profile that they want the L2 to have. Or if you don't have a transaction pool and people just hammer the sequencer, what are you going to do? Proof of work? Are you going to add identity? How are you going to prevent this spam that happens on the wire that comes from the client to the server. And the obvious thing which is not I'm realizing the last bullet is off topic, but I'll roll with it is that we have a bundled role. A sequencer is both a proposer and a builder and naturally you want to keep the sequencer lean to allow for in the future as many of them to be around. So the same thesis for PBS on level one applies to level two separation of concerns, how you build optimized systems.
06:12:20.780 - 06:13:21.208, Speaker A: I will not talk about decentralized sequencing. I think it's kind of again an implementation detail. Any centralized system, you can replace it with a BFT version of it, put it on tournament, put it on some other consensus protocol and that gives you redundancy and some A's or B's. Here are do you enshrine the shared sequencer in your system? For example, the Op stacks super chain, is it the thing and is it something that gets you better performance than having something like an Espresso sequencer on top of it? I would think that anytime that you have a core protocol operation and you leave the metal, that means that it becomes more expensive. One or many shared sequencers do you aggregate them? How does that go? We saw the fractal aggregation of proofs that Vitalik showed earlier. The aggregation theory can play in any layer l two specific or close favor. Again, same thing and the thing I want to drive home.
06:13:21.208 - 06:14:07.048, Speaker A: Shared sequencers are not magic. They don't actually solve all the world's problems. They give you atomic top of block inclusion, the most classic example being the base roll up. What is the room for differentiation? I also don't know, but I know that nobody else either knows. So what do and these are two other questions around the bridge and the atomic success, which I still have only seen stroman answers to and I think we should do better. One concrete proposal on how to get to cross chain composability and I realize this is kind of dense and low context, so I'm happy to expand later, but we don't have enough time. Is that maybe you can have a shared sequencer that indeed only guarantees inclusion without execution.
06:14:07.048 - 06:15:02.456, Speaker A: But then you can have a bunch of builders where they cross chain simulate and Vitali kind of alluded to that, that you can cross chain simulate the broke. The builders are indeed going to be running nodes and simulation engines for every chain that they support and the builder is going to land a bundle that is going to be landing transactions for all app one, two and three together. And the insight here is that we take the mev builder role and we just bundle more functionality of it like make it heavier. And basically the shared sequencer is funny enough, the mev builder poetic. This is my final slide. I want to talk a bit about this because the topic I deeply care about on Ziki proofs and how the market will evolve the proverb right now is again run colo with the sequencer. And the sequencer, as we said, is also a builder.
06:15:02.456 - 06:15:48.988, Speaker A: We really need to unbundle these two and there's many, many ways that we could do it. My favorite work was a paper by a good friend of mine, Nakis Katis. It was called Proof of Necessary Work and described a way where you can introduce non grinding Allah proof of work in the snark generation process. And that let you have a leaderless system. Or rather a system where the leader is not known ahead of time for proof generation, for proof proposal. Naturally, anything that is like proof of Work esque, it has a lot of wasted effort, but maybe it's like very egalitarian. So maybe the next step from that is should we do a consensus protocol for the leader election? You know, let's elect the proposer, let's agree, let's put up some stake or something.
06:15:48.988 - 06:16:37.080, Speaker A: We elect the leader, the leader does it. Standard techniques that we have seen used in consensus protocols are going to be used for prover selection. And the other thing that I don't think I've seen anywhere so far and I just added it as a strawman and I realized that it's probably broken and there's probably things to do. But I wanted to say it is that as a ZK proof is generated, you can actually observe that parts of the proof are generated in a pipeline sense. The most trivial example I have a block with ten transactions. Transaction one is executed, it generates some witness data. That witness data is supposed to get fed to a snark, but while it's getting executed and it's getting proven, another transaction is also getting executed and getting proven.
06:16:37.080 - 06:17:36.000, Speaker A: And another, and another and another. Do the provers need to be the same thing? Can there not be a market for proof aggregation? The things that Italy was talking about? Can there not be something where proof one goes to John, proof two goes to Nick, proof two, three goes to know? And you can keep doing that. It seems like we can come up with collaborative proof generation protocols that abuse this pipeline structure of proof generation. I don't know how that will look like, but if somebody is interested in talking to me about it, please find me after the talk. And the final question is how do you distribute the fees? Again, the perennial question in the MAV supply chain or compositional game theory is the new term, I hear. How do you put the fees? Where does value accrue? I also don't know, but it's something interesting to think about. I have three minutes and it would be good if we can do some questions, perhaps.
06:17:36.000 - 06:17:37.650, Speaker A: Thank you.
06:17:41.620 - 06:17:49.270, Speaker B: Anyone has any questions from the audience? Oh, I see.
06:18:00.650 - 06:18:51.160, Speaker A: Can you elaborate a bit more on how crosschain or cross LTSU interaction can be facilitated by this Shared Sequencer model for those of us who are unfamiliar with the literature. Yeah, of course. Can we get the slides back up? So the Shared Sequencer is an entity that is ordering things and doesn't really know how to execute them. Or if it were to know how to execute them, it would need to have execution nodes for every chain it is ordering for because otherwise it would be ordering invalid things. And ordering invalid things might be okay. If your chain can support no ops, you just say anything that's invalid, just like skip it. This leaves junk on chain, but maybe it's fine.
06:18:51.160 - 06:20:04.240, Speaker A: So perhaps what to do is that you have the Shared Sequencer, which is basically a privileged entity for submitting data to the chain. And how does the data get ordered to get into that privileged entity? You could have builder one and builder two or builder one. For simplicity, who is running nodes for chain One and Chain two? And what they will do is that it will publish a bundle or a full block template to the layer one, where the first transactions in that bundle are going to be all the roll up A transactions and the next transactions in that bundle are going to be all the roll up B transactions. And because that person is a builder and is has run the simulations on every chain, they can guarantee that by being at top of the block, they can guarantee that the transactions are going to be valid. So by doing that, you can hack your way into shared sequencing in a very clean way. I realize this is a bit dense and I don't know if it answered the question sufficiently. Yeah, it makes sense.
06:20:04.240 - 06:20:58.960, Speaker A: This also goes back to what Vitalik says about aggregation. I'm just wondering how this helps cross chain interaction. Well, I don't think that you can do callbacks callbacks I don't know who has said that you can do contract A, calls contract B and then it calls back. It's a bit unclear because people are doing like right now the way their roll ups work is that they submit one transaction, one big bundle per block. If we could interleave bundles, if we could say top of block part A is like bundle for roll up A and then roll up B and then roll up A again, maybe we could do something where I can call chain A and then chain A and then it calls me back. The main things that we support in this design are crosschain message passing and maybe cross chain bridging. I'm saying a timeout, so I don't know if I have time for more.
06:20:58.960 - 06:21:05.106, Speaker A: Sure. I think the person in the back.
06:21:05.128 - 06:21:13.330, Speaker B: Was first, the person in the background.
06:21:18.650 - 06:21:54.526, Speaker A: So I just have a follow up question regarding the shared sequencing and also PBS for the L two. Like if we have sort of these shared sequencer. Actually, two questions. The first one, if it's really like sort of shared sequencer, do you think it will overwhelm that machine? I mean, if the sequencer used for multiple roll ups, yeah, that's the first question. Second question. Let's give it that one. Sorry, so is the question that the shared sequencer can be very heavy? I'm not sure I understood that.
06:21:54.526 - 06:22:49.390, Speaker A: If that is the case, the way that you do shared sequencers, you basically need to make them like cross chain DA aggregators before you land the chain, before you land the bundle to layer one, but you do not want them to be running the execution of these chains. So by doing that, you can keep the sequencer as a light DA consensus protocol effectively, which is what Espresso is, for example, and then you land your data to the real DA protocol. Sorry, just a quick one. But in that case, for example, if we really pass a message across roll ups, right, via the sequencing, but do we need to wait for the confirmation from one roll up before we really execute another message dependent on the for sure, yeah, obviously, of course. Any transaction needs confirmations. Zero conf. One conf is insecure.
06:22:49.390 - 06:23:09.400, Speaker A: I see. So in that case, it's like sequencing plus execution and DA. Right. Thank you. Thank you so much. The next speaker is Togrul, who I don't think we've met before and is going to talk to us about prover proposer builder separation or whatever you're going to call it.
06:23:16.810 - 06:23:18.040, Speaker B: He needs this.
06:23:27.890 - 06:24:09.050, Speaker A: Hello, everyone. Before I begin, I wanted to thank Vitalik and Georges for warming up the crowd. It's really nice from them. So, strangely enough, I'm not going to be talking about roll up definitions today, I'm talking about something else. So my name is Togril, I mostly shed posts on Twitter and sometimes pretend to do research at scroll. And today we're going to be talking about Pbsifying rollups prover sequencer separation. So you've probably heard about zero knowledge rollups or for the starquare folk in the crowd so they understand validity roll ups.
06:24:09.050 - 06:25:05.118, Speaker A: So you have sequencers and you have provers in the system and sequencers perform the role of ordering and then the provers perform the role of computing the validity proofs for the given order. And the way they work is the sequencer submits the batch. The batch is also propagated to the prover. The prover submits the validity proof and voila, you have the finalized state on the layer one. So why should we decentralize? Because in L two S in roll up specifically, you don't really care about correctness that much because it's already enforced by the L1 bridge. So why would you decentralize? It makes more sense to just be centralized and be efficient. So there are multiple reasons why you would do that.
06:25:05.118 - 06:26:13.140, Speaker A: So one, resilience, let's say if your sequencer fails, what happens then? If there's no backup or in many cases there's no decentralization, then you're stuck and waiting for that sequencer to come back. Another reason why you would need to decentralize is real time censorship resistance. So while L1 bridges guarantee long term censorship resistance for a lot of applications that's not enough. For example, let's say if your app depends on Oracle updates and you're waiting for a long time and the sequencer is censoring you, you're basically screwed in case there's a lot of volatility. There's also throughput, which is an interesting one because it might seem logical that a centralized system is more efficient because you have less overhead. But in case of zero knowledge roll ups because you can paralyze proving, you can actually increase the throughput by decentralizing the provers. And that's what I'm going to talk about later.
06:26:13.140 - 06:27:26.970, Speaker A: What are the requirements for a decentralized roll up? So first we need to minimize the overhead. It doesn't really make sense to add drastic overhead and make it really clunky and very complex. It needs to have fast pre confirmations because if your has the same finality time as the L1 then especially in L1s that are not very fast, it's not a great UX, it has to be provable on chain. So what I mean by that if you use some sort of a mechanism that agrees on off chain ordering, you need to be able to prove to the bridge later that the ordering was agreed upon via a certain mechanism. So if there's some nondeterminism in the way you agree on the ordering it might be possible that basically the bridge might diverge. And so we need a way where you can prove it on chain that it actually happened. And finally, which is the reason why I'm giving this talk is balanced incentive distribution.
06:27:26.970 - 06:28:24.874, Speaker A: I'll explain why it is important. So why is unbalanced incentive distribution an issue? So since synchronizer is in the protocol are in charge of the ordering, they get to keep all the mev profits and it's likely that in roll ups the mev profits are going to be the large share of the total protocol revenue. And if that's the case, then what's the incentive for protocol participants to become approver? Because if all the profits are collected by the sequencer then I would rather become a sequencer. But in case of ZK rollups we actually need provers sequencers. We need a few but we don't need that many. And so we need to incentivize provers. And that's where PBS or Proposal Builder Separation comes in.
06:28:24.874 - 06:29:39.750, Speaker A: So in a proposer builder separation, proposer is thought leader responsible for proposing and propagating blocks to the network and the builder is a set. There are a number of builders competing with one another in an auction to have their candidate blocks selected by the proposer to propose to the network. And basically the whole point of PBS is that it outsources the computationally intensive task of creating an optimal value extracting ordering to some specialized nodes that can actually do that efficiently and well. But in our case we don't have proposers and we don't have builders. So what should we do? So we have prover sequencer separation. So on top of doing what PBS does, PSS also ensures that protocol generated revenue is relatively fairly distributed with the provers because as I described before, the sequencers by default get to keep all the mev. And as with PBS, there are two types of PSS.
06:29:39.750 - 06:30:32.098, Speaker A: There is Enshrined PSS and external PSS. And the role of the sequencers is essentially to emulate the role of the builders on the L1. And the role of the provers is to emulate the role of the proposers in BBS. These are the steps in PSS, in enshrined PSS. So the sequencer bids in an auction, the prover selects the highest biding thing, the winning bidder reveals the contents of the block, the sequencers reach consensus, blah blah blah. That's it, we have the ordering. Well, the benefit of this is that it doesn't have a lot of overhead because we don't need a lot of sequencers.
06:30:32.098 - 06:31:37.842, Speaker A: The consensus should be relatively efficient and therefore it should be relatively easy to run. But the downside is that it actually creates sort of a PBS inside PBS, because I might be a sequencer, I might be elected as a sequencer, but I'm not really good at building, so I'm incentivized to outsource it to someone else. So it's possible that you can create a market where sequencers outsource it to someone budy else, which creates this loop in which the value is leaked to some external parties because the sequencers just don't want to do it internally. Now it's working. And the second type of PSS is external PSS and it follows a similar pattern. So sequencers bid in an auction, provers select the highest paying bid, the winning bid is revealed, blah blah, blah, blah, blah blah. But in this case, provers reach the consensus.
06:31:37.842 - 06:32:39.126, Speaker A: And with external PSS, sequencers don't actually exist in the protocol, the protocol doesn't know about them. There are some external parties that provers deal with and they can even avoid them and build the blocks themselves. But obviously it's unlikely to happen because provers are not good at building blocks and extracting value, so they're likely to outsource it. And the advantage of this scheme is that it doesn't really require two distinct roles in the protocol and therefore it makes the protocol simpler and the fee distribution much more simpler. So you don't have to deal with two different models, potentially two different staking models, et cetera, et cetera. But it also has a massive downside. It has quite a big overhead because the idea is that you should have a lot of provers and if you run the consensus amongst the provers, it's going to cost quite a lot in terms of networking, et cetera, et cetera.
06:32:39.126 - 06:33:23.082, Speaker A: There are possibilities where you can, for example, use comedies and do random sampling and do that, but the security drops that way. And basically you might get into a situation where you have a liveness failure, whereas if you use the entire validator set, then it's much less likely that you're going to end up having a liveness failure. So those are two options and we are not really sure which trade offs are better. And therefore the open question is I'll leave you with an open question. To enshrine or not to enshrine. That is the question. Thank you very much.
06:33:23.082 - 06:33:25.500, Speaker A: And if you have any questions, feel free to ask.
06:33:41.650 - 06:34:00.062, Speaker B: Well, the name of the game is that if audience have no question, we ask Patrick to see if they're actually paying attention because these talks are sequenced. And thank you, Patrick from Arbitrum Foundation.
06:34:00.126 - 06:34:25.500, Speaker A: Right, cool. Hello, Patrick. How are you doing? I was making a joke. I was going to troll him for his talk, here's my opportunity, but I'm bad at it. No, I was just going to ask, has scroll worked out how to do the incentives yet for the decentralized proving? How do you do the fee reward, for example? Not really. We're still thinking about it. I guess it's relevant to the talk.
06:34:25.500 - 06:34:35.966, Speaker A: Yeah. Now that I've warmed out the stage, you can probably welcome cheers. Awesome. Cheers, Gigi. All right, thank you. Nice little clicker. Oh, yeah, sorry.
06:34:35.966 - 06:34:48.974, Speaker A: Yeah. Oh, that was a hard one. Normally before you're talking, your heart's beating like this, you're not really listening. Okay, cool. Should I just get started then? I guess so. Hi, guys. My name is Patrick McCorry.
06:34:48.974 - 06:35:36.286, Speaker A: I work at the Arbitrum Foundation. And today, yeah, there's my really dinky slides. See, I didn't have that cool intro before and I'm just going to talk about PBS across the layers for layer one and L2. So let's just jump right in, let's have an overview of layer one and let's have a consider about how mev sort of evolved there. So at a basic level for a blockchain system, you have Alice, and Alice wants to transact and she needs to get her transaction and communicate it to the block proposers. But the question is, how does she get her transactions across? And historically speaking, you would send this to a gasa protocol or a peer to peer network. So Alice will send it to the gasa protocol, every peer will get the transaction, validate it, and then pass it on to the next peer.
06:35:36.286 - 06:36:12.062, Speaker A: And the block proposers live on the gasa protocol. So they'll hear the transaction and then include it in their block as well. Now the question is, who else lives on this peer to peer network? Searchers do. And I'm sure most people here are aware the search will hear the transaction, they'll inspect it, they'll say, oh, I can extract value from this. So they'll copy it, paste it and get it to the block proposers first. And they'll pay a higher transaction fee so it gets into the block. Proposers first or sorry, their transaction will get in first.
06:36:12.062 - 06:36:45.338, Speaker A: But now not just one searcher is there, many searchers are there and they're all typically competing for exactly the same opportunity. And we call this a priority gas auction. So maybe, let's raise our hands. Does anyone remember these gas auctions? Okay, awesome. And did anyone ever steal your transactions? Have you ever been victim to the PGA game? Okay, maybe one or two hands there. Anyway. So what is a priority gas auction? Why is it important? So what happens is that I stole this graph from someone, from this really nice article about it.
06:36:45.338 - 06:37:28.550, Speaker A: But basically within that twelve second window, one searcher will place a bid, another searcher will place a higher bid. And very rapidly throughout those 12 seconds, you could have 100 different bids that are all competing for exactly the same mev opportunity. The block proposer will simply take the bid with the highest fee because that's going to pay them the most money. And so there's two problems that happened here with these priority gas auctions. And this is sort of like 2019 2021 was wasteful and failed transactions because the successful transaction would get in alongside all the failed bids. So this is a waste of block space. And two is sort of unrestricted mev as well.
06:37:28.550 - 06:38:00.334, Speaker A: You're sort of taking the user, throwing them to the wolves, and their transaction may or may not be executed at the very end. And so when Flashbots emerged, they sort of resolved this problem. So has anyone ever used a direct transaction before with Flashbots? A few people here. Awesome, cool. So the way they solved the problem was very simple. They set up a direct communication channel between the user and the block proposers. Okay? And so you just basically bypass the peer to peer network.
06:38:00.334 - 06:39:05.426, Speaker A: You bypass the dark forest, so they cannot eavesdrop on your transaction. But what can we extract from this scenario? So first we have to consider the transaction ordering policy. So on a layer one blockchain, we typically use the highest fee first. Whoever pays the most money is going to get ordered first. The second is the communication protocol. Can a searcher eavesdrop and learn about the user's transaction and then extract value from that? And finally we have to consider is the user experience, what guarantees did the user get and how long does it take until they figure out, oh, my transaction has been confirmed or someone's extracted value from me? And so what do you do about this in the layer one world? What do you defend against mev? Or do you try to embrace it and just take the sort of, it's going to happen anyway, let's embrace it and use it to protect the protocol. So in the layer one world, we've sort of gone down the path of embracing mev and PBS games.
06:39:05.426 - 06:40:17.290, Speaker A: And why is that? And that's because to this day, I am not aware of a protocol that can defend against mev. When we take into account 500,000 validators at the protocol level, we just don't have a protocol that can defend against mev in any meaningful manner at the layer one based layer. Because after all, one of the overarching goals of a system like Ethereum is to maximize who can be a validator, maximize that set and make the operators of the system as decentralized as possible. And one of the other problems is that one of the reasons why you would embrace mev is because one of the assumptions at a layer one blockchain is that all validators roughly get the same reward. If I get the same reward as the other validator, then we'll both earn rewards at the same amount and we'll be equal partners or equal competition. If one validator can take the mev for themselves and it's not shared amongst the validators, then they'll grow bigger than everyone else because they'll earn more reward than everyone else as well. And that's a terrible outcome.
06:40:17.290 - 06:41:11.870, Speaker A: If we're not careful of how mev is embraced, we'll break the property of fair rewards for the validators and one validator will grow and basically defeat everyone else. And so that's why Flashbots and PBS sort of solves that. Problem when you think about what Flashbots have done is that they've taken this on chain auction protocol and took it off chain. And what they enabled was to allow block proposers to outsource the work of mev. And then a marketplace of searchers can all compete to extract the mev and of course share some of the rewards with the proposers and then they get some of the reward as well. And this is also acknowledging that someone in their bedroom can probably do mev better than someone who's running like a validator farm. For Ethereum, I'll source it to the best people who can do the work and share the rewards between both parties.
06:41:11.870 - 06:41:48.890, Speaker A: And that's why it's really cool. You can share the reward and you also reduce this on chain spam. Now, what about a roll up just to highlight that's why we have PBS for the layer one to protect the decentralization of the validators. But what about a roll up? Is the environment any different? And it's exceedingly different. A roll up already assumes that we have Ethereum. We already assume that we have this decentralized platform with hundreds of thousands of validators. We don't need to replicate this at the L2.
06:41:48.890 - 06:42:34.514, Speaker A: We don't need 500,000 validators. We just need a handful, maybe 12345 sequencers and that's it. And as we'll see the sequencers there to offer the user experience and the fast path, they don't actually impact the safety of the system. But that's cool if you only have a small sequencer set, that's a much more tangible problem to deal with in terms of, you know, the scenario is very straightforward. Alice has a direct communication channel with the sequencer, so Alice can give her a transaction over, then the sequencer can give an instant response. The sequencer has ample time to do whatever they want with the pending transactions. They have a list of basically like a basket of pending transactions.
06:42:34.514 - 06:43:07.700, Speaker A: They can apply any ordering policy that they want and they could do this within a few minutes or up to a few hours. They have ample time to extract as much mev as they want if they wanted to do that. And of course, eventually the sequencer will take the list of order transactions and pass it on maybe to the executors, the provers, et cetera. So all they're doing is ordering transactions and that's it. So let's again compare the core components. So L2 is very different. In L2, the sequencer can apply any ordering policy that they want.
06:43:07.700 - 06:43:48.640, Speaker A: The communication channel is very different. It's a direct communication channel to the sequencer. There's not necessarily an opportunity for a searcher, the eavesdrop on this communication and finally the user experience. The user sends a transaction, the sequencer can give instant response. It's a bit like Plymouth of the Legacy Web Two world where you get this instant response when you sort of interact with a website. But as we're going to see, as you try to improve the user experience, you're going to impact mev, its ability to be extracted. And there's this sort of weird trade off between extracting mev and how users experience using a roll up or a L2.
06:43:48.640 - 06:44:26.982, Speaker A: So, as I mentioned, lots of ordering policies you can consider has extraction, first has fee, 1st, 1st come, first serve. It could even be cal swap, that's an ordering policy, any ordering policy that you want there. But most roll ups today implement first come, first serve. What is first come, first serve? Very straightforward. I give the sequencer my transaction, they timestamp it, they respond to me, then they order the transaction by timestamp it's straightforward. And you would think with first come, first serve, there's still no mev here, so raise your hand if you think there's mev. Let's see.
06:44:26.982 - 06:45:06.694, Speaker A: There we go. Awesome. Now of course there's mev. Why? So what Arbitrum tried to do at least, is that the sequencer has a data feed. So we get user transactions, we do first come, first serve, and then we reveal that in a data feed every 250 milliseconds. And the reason for this is that we want to provide Coinbase, Ether scan and fura with up to date data so a user can send their transaction, look at RB scan and say, oh, my transaction is confirmed. Well, if you have a sequencer feed where you're giving out this pending data to these infrastructure providers, searchers pop up, of course they do.
06:45:06.694 - 06:45:45.698, Speaker A: And what do they do while they listen for the transactions? If they find a back running opportunity bomb, they send it out immediately. Oops, look at that, there's a searcher right now. Sorry. Always listening, aren't they? But this is it. You have this rise of this latency game because in Arbitrum, you didn't just get one searcher, you got 150K WebSocket connections to this data feed. And so one is a latency game because they're all competing with each other. And two, it's a bit like a denial of service attack on the sequencer because you have to maintain 150K WebSocket connections, which is not an ideal scenario.
06:45:45.698 - 06:46:21.970, Speaker A: So the off chain Labs guys came up with a really elegant solution, in my opinion. But there was some outcry. You're right, you're laughing about it, aren't you? They basically wanted to implement has cash, which is basically you create like 20 priority lanes, you do some proof of work, you have the lowest proof of work, you gain access to the priority lane for 24 hours. So reintroduce some proof of work in order to solve this problem. But as you can imagine, there was a lot of outcry. People don't want to use proof of work to solve this problem. This, of course, for me is an elegant short term solution, but it doesn't really solve the problem long term.
06:46:21.970 - 06:46:48.810, Speaker A: But this is really frustrating, right? We have this trusted sequencer, this ideal scenario where we can fully trust them not to extract mev. And the mev bots are still there. I mean, look at this guy. He just doesn't go away, does he? That's Phil Diane, by the way, from Flashbots. Don't know if you know who he is, drew your little evil eyebrows on him, but he's always there, doesn't leave us alone. Well, what a guy. The dark forest is always lurking.
06:46:48.810 - 06:47:25.842, Speaker A: So then the question is, what's more fair? Are these latency games fair? Clearly not, as a denial of service attack on the sequencer and they're just fighting amongst each other. So then it brings you back to the question, well, what about auctions? That's how it was solved in the layer one, we create this auction and let them bid for the opportunity. And that's sort of the idea behind the time boost proposal by Ed Felton from Offchain Labs. And I'll give you a high level overview of basically, I mean, it's very straightforward, as you're going to see. Maybe they have an academic paper that's slightly more complicated. But the idea is simple. Alice gives her transaction to the sequencer.
06:47:25.842 - 06:47:55.870, Speaker A: The sequencer still has this data feed where they reveal the transactions. But now the sequencers compete in these little bundle auctions, very rapid auctions. And then, of course, the winning bundle gets sent to the sequencer. Then the sequencer then decides, okay, here's the transaction batch. But the point here is that this bundle auction is configurable. It could be really rapid, it could last 500 milliseconds, it could last 3 seconds. And what you're really doing is purchasing the right, they go back in time.
06:47:55.870 - 06:48:42.926, Speaker A: So within this auction window, you're saying, I want to go back 250 milliseconds in the past. Another way to think about it is that you're buying for a specific position in the ordering. Maybe you want to be this 20th transaction, the 30th transaction, but you're buying for that opportunity and you can be selective. So in the time boost proposal, their goal is only to allow back running, because back running is not seen as bad mev. There's a lot of moral questions around that, but it can be configurable. You could allow front running, you could allow sandwiching, and because the bundles are so small and the auctions are so rapid, you could probably inspect it, look at the transaction and say, okay, I will accept that type of mev, but I will not accept this type of mev. And so my last slide is basically some interesting questions around that.
06:48:42.926 - 06:49:39.454, Speaker A: Should the sequencer be allowed to inspect an Mev bundle and then decide not to include it? This could impact the credible neutrality of a roll up, because now they're sort of being subjective over the transactions they include. But if it's better for the user, then maybe they should do that. The auction time window is set to only enable back running, but should we allow front running and summary Gin as well? And maybe the last one, because these are rapid auctions. So the whole point here is to enable a marketplace of searchers who can all compete to find these opportunities. But if it only lasts 500 milliseconds and there's only five transactions, they know there's probably only one Mev opportunity. So maybe you want to set the time window so there's 50 transactions or 100 transactions, and so then you can get more interest in Mev marketplaces out of that because they're all competing for different opportunities. But for me, the time boost paper sort of inspired this talk.
06:49:39.454 - 06:49:53.330, Speaker A: I thought it was really interesting and I've been just thinking a lot about it. So it's not really my work at all, it's just whatever I've gathered from reading. So I hope you enjoy the talk anyway. Gigi GM GM thank you, Patrick.
06:49:53.830 - 06:49:59.122, Speaker B: Well, someone wants to give you a question back. He has been waiting.
06:49:59.186 - 06:50:22.606, Speaker A: Oh, he is here. Look at this. They're going to get Roosted. Yeah. I waited for the end of the talk to ask and a very important question. I cannot board my flight without knowing the answer to it. Should we be allowed to refer to distributed ledgers as databases? Sorry, I was just getting trolled earlier by Bartick from L2 Beats, because I keep describing it.
06:50:22.606 - 06:50:45.766, Speaker A: Yeah, this should be a database. I mean, that's what they're all roles, is building databases. Okay, so a bit of an inside joke, but I've been getting trolled all day for it. Sorry, just for context. So basically, when you think of these systems, when you deploy a blockchain system, what you're really deploying is a database, and the blockchains has an audit trail for recomputing that database and that's it. And people don't like that analogy. Do you guys like it.
06:50:45.766 - 06:50:52.440, Speaker A: Raise your hand if you like that analogy. There you go. Someone take a photo and show Bartick. Thank you. Okay? I'm done. Gigi, guys.
06:50:55.630 - 06:51:20.610, Speaker B: Perfect. We're doing perfect on time. Thank you. Next we have Tomas, who wear many hats, one as a researcher at Flashpots Architect and also as the founder of Nethermind, talking about Mev Garden and cross domain markets.
06:51:25.020 - 06:51:51.020, Speaker A: Hi, everyone. So I was told to stand here. The Mev garden. So that's very sweet name because we've been talking about the dark forest for a long time. So my name is Steincheck. I'm mated flashboards founder Net of Mind, trying to wear a few hats. And we'll be talking about how to connect through PBS many different domains with swamp.
06:51:51.020 - 06:52:53.872, Speaker A: So this is not an announcement of a product, it's not a suggestion that Fashbot's going to build it. It's more like building the open, presenting you a bit more of undercooked ideas that potentially will be rejected. Like maybe something will be missing in quality, maybe it will be not really aligned with the values, all the values that are at flashbacks. So illuminating through the bringing transparency to mev activity, providing permissionless, access to mev extraction and distributing fairly the mev revenue. All right, so once again, Mev Garden will move away to something that is a bright place, clearly divided into components, PBS and with kind of conditions for everyone to participate. So we're talking about connecting everything with the existing ecosystem and making sure that there is place for everyone, especially that the users feel very good in this ecosystem. That's one of the small steps really, what we'll be showing.
06:52:53.872 - 06:53:46.772, Speaker A: And hopefully we'll end up with something like this and maybe a little survey, like how many of you know what PBS is? Okay, it's getting better and better. So now we divided it slightly more, but just slightly more. So I say the builders would divide them into the sequencers. And builders sequencers will be choosing the transactions, sequencing transactions, and the builders will be building blocks out of the transactions that are preselected. So it may happen in some of the setups and it particularly might be important if you start talking about the shared sequencers. So start to forget for a moment that sequencers are what in the roll up is like everything together and how nowadays we see the PBS done. So I'll go quickly over maybe some very simple visualizations of what we have, what we can end up with.
06:53:46.772 - 06:54:53.668, Speaker A: So nowadays many of the roll ups are like summer two and a centralized native sequencer. In it, the decentralization very often is seen as like just introduce a few sequencers and leader election and they start proposing blocks one after another, depending on some rules. With a shared sequencer, we say that the sequencer is actually shared between multiple L two S L1s and we may have some atomicity with it. And then something that is like ethereum nowadays would be you start splitting the sequencers into proposers and builders. So there's builder market, they're bidding for the, for the best block and there's proposers. So you know all of this right now I start to say that we will say that shared sequencers and native sequencers, like centralized sequencers, all of this can coexist and you can start to have various combinations of all those setups. And also very important thing is that PBS does not necessarily require that the proposers are decentralized.
06:54:53.668 - 06:55:53.640, Speaker A: You can have a centralized proposer native to the roll up and still create a builder market so they can be biding for the best block. And the proposer may decide to either capture that value for the operator of the L two or to maybe burn it for the protocol. And now, as I say, all of this may coexist. And because we can have many different protocols arriving at the solutions at very different times with different like, we see an arbitram proposal, we see what scroll was proposing, what StarkNet is working on and so on and so on. So maybe in this world for a few years where we want to create the markets, start building the markets for mev and for PBS and still connect all of those different protocols. So when we know that we want to achieve that, we set some design goals for the protocol that will allow to do the PBS with the cross domain mev. And we want to support that heterogeneous sequencer space, different designs.
06:55:53.640 - 06:56:42.536, Speaker A: We want to enable permissionless and transparent cross domain mev market. We want to be in line with whatever happens on Ethereum, but also be in line with what happens with other chains. Right? So think about not too much pushing the protocols themselves to change. We want to operate slightly outside of the protocol, maybe opposite to what Zaki was saying, or maybe not opposite, but related to what Zaki was saying. We want to respect the position of various market players that are building shared sequencers, middlewares, risk taking solutions and so on. So we want to make sure that everyone feels like oh wow, there is a marketplace arising, there is a game which I'll be able to play with, play inside. We want to make sure that it's aligned.
06:56:42.536 - 06:57:31.316, Speaker A: So that's my internal goal, like make sure that whatever we're working on at Flashbots is actually part of our greater vision that is strategically well tested and discussed. And one thing is that we keep thinking about it. We need to ensure that their equal rights are there for the solo stakers and large operators to participate in that game. And this is for the validators or proposers. And now this will be the proposal for what we really can build to try to support all of those requirements. So first of all, we want to have some kind of chain, it can be swap chain, it can be something on the side of swap chain which is just like a global ordering system. So a set of Oracles or whatever construction.
06:57:31.316 - 06:58:33.020, Speaker A: And the construction here is not really defined on the technical level, more on the vision idea. It we want to order the blocks on different chains and have a way of looking at this ordering from the outside just to use it for construction of the proofs and slashings. So I'm saying I'm looking at Etherscan, StarkNet, Polygon and many other chains potentially, and maybe even things outside of the l two or Ethereum space. And I want to keep order of everything that I can do possibly within all of those domains. And I say that I want to introduce a definition of synchronized block proposals. So if I'm a shared sequencer, for example, I'm practically in some cases, I'll be sure that I'm proposing blocks that will happen at the same time. If it was a based roll up on L1, I know that every 12 seconds I produce block, and within that block I'll be producing blocks, batching blocks for multiple roll ups.
06:58:33.020 - 06:59:32.790, Speaker A: So I can start making some promises. But to describe those promises, we need to agree on some definitions. And I'm not sure what those definitions would be over time in the market, which of those would be the most the strongest. And here, for example, seeing something like a very strict definition, like we're looking at two different domains and we don't allow anything in between. So on the screen, the Ethereum block 100 will be synchronized with the StarkNet block, but not with the following ethereum block and not with any of the blocks before the StarkNet block because they belong to a different domain and there is something in between. So I want the blocks to be exactly next to each other, but then maybe this will be difficult. And maybe I would sometimes not care about the other domains because I'm trying to synchronize to blocks to extract mev and maybe not always I will believe that through some other chains construction I can extract that mev before this happens, before the synchronized execution happens.
06:59:32.790 - 07:00:07.384, Speaker A: So maybe I want to define something that is like strict but within the domain. So as long as there is no other block that belongs to the same domain as the two blocks that I'm talking about in between, that it's synchronized. So here the ethereum block is synchronized with StarkNet block. It's also synchronized with the polygon block because there's nothing from ethereum or polygon in between. So there's only StarkNet block in between and it belongs to another domain. So I don't care. And the more domains I will see there, the more I will probably want to restrict the number of domains that I care about when defining.
07:00:07.384 - 07:00:49.272, Speaker A: What does it mean that it's synchronized? And then I may say that, well, I really care only about the situations when both domains that I'm synchronizing between or multiple domains. Like we can probably extend it to many domains. If no set of blocks happened in between that would belong to the same domains, then everything is fine. So here I really say that even one of the last polygon blocks is still synchronized with ethereum, ethereum block 100. But the StarkNet block 202 is not synchronized with that block because we see both the ethereum and StarkNet block in between. So this is not very strict. I'm just proposing something that may be relevant when making announcements.
07:00:49.272 - 07:01:55.600, Speaker A: Like if I'm a sequencer proposer, as a proposer, shared sequencer proposer, I want to make commitments to what I'm going to do and I have to describe it in somehow. And as we were working on it, we discovered that well, the definition is not that obvious. We may still need to work about which definition will speak. The communication about this. The announcements may happen through something like nowadays for the relay, but potentially may happen through the Attestations and PTP network. So there should be no difference whether we keep the existing ideas of ethereum and existing roadmap and whether we change it towards the EPBs and shrine PBS, not really to the way we approach this solution at Mivgarden. And when you look at the proposals of PEPC, like protocol enforced proposal commitments, you can think of this a bit like a cross domain PEPC.
07:01:55.600 - 07:03:07.320, Speaker A: So cross chain PBS, we're adding communication layer from proposers to builders. So the other direction proposers make announcements like in PEPC, like some announcements of what I would like to commit to and I define Xynchronization between the blogs that I'm about to commit to and I can be slashed if I don't deliver that. So we'll show how to do that and once again think of it maybe as a cross domain PEPC and this is part of the design goals. We want to create the permissionless and transparent market. So you start to feel maybe it's permissionless because we created that chain and anyone can start announcing this proposals, defining some kind of we can have some format description of what I'm going to do as a shared sequencer. You can say polygon ethereum block in ten minutes they'll be synchronized according to the strict domain rule and if I don't deliver, you can slash me. If I do deliver, then actually you can start bidding on that proposal.
07:03:07.320 - 07:04:18.296, Speaker A: And any of those shared sequencers, native sequencers, decentralized sequencers, may start judging themselves how much of the probability of delivery of that commitment they have. Do I only propose it, only announce it if it's 100%, if I'm sharing sequencer or sometimes I want to start extracting value even if I have 5% chance, 50% chance of success and I know what my deposit is, what can be slashed. So this is both the global time with the set of proposals. We look at this chain and we see that we start noticing blocks and there is a commitment from a proposer saying I'll do synchronized StarkNet ethereum blocks. And then we start seeing bids from the block builders that are now cross domain block builders and they're bidding on what will be the best payment to that synchronized proposer. The block is selected when it's accepted, when the bid is accepted, then from now on, we practically have a promise that there is some slashing. So the deposits where exactly it's deployed, it's maybe not the most relevant.
07:04:18.296 - 07:05:13.968, Speaker A: It can be something on this chain, something outside, some restaking solutions and so on. And then we see at the end, either I delivered synchronized blocks according to that definition or I failed. If I delivered, I get the beat that was promised. If I failed, I'm going to be slashed. Why is it important in the Suave Roadmap alignment? Well, if you've seen the presentation from Robert about SUAVE, we were talking about this promise that a searcher can say I pay you one if you deploy something on two different chains. So without this, maybe the one problem is that I may make this promise, but on the other side, the executor has to have some kind of expectation about how likely they are to achieve that success of landing something on two different blocks. And they want to have someone else promising that, yes, they can do that for them.
07:05:13.968 - 07:05:56.444, Speaker A: So the executor want to have a bid, but also a promise that they can operate on that bid and start simulating and building blocks. So this is the component that is needed for swap to connect for the cross domain. And this one thing is here in yellow because it's kind of missing. This is risky. It's to be explored how the Solo Stakers Solo proposers validators can compete with the large operators that have huge overlap between two networks. So shared sequencer is a simple case. But if I'm large operator two networks, for example, 70% of blocks on one network, 70% of blocks on the other network, more likely 30 or so, then I will have overlaps very often.
07:05:56.444 - 07:06:54.432, Speaker A: I can make these promises of cross domain extraction all the time, so I can capture more and more value. So within these, we have to make sure that those proposals between the Solo Stakers solo proposers can be somehow coordinated and that we know what exactly happens if the slashing happens, who is at fault for not delivering and how was the roadmap? Well, ensuring that the design is actually satisfying all those goals, supporting PBS on multiple chains so they can be connected to this solution. And this is not trivial, as we've seen. There are different questions about what consensus mechanisms are used. Like if it's tendermint, how this will work there, if it's ethereum l two, how it will work there, how it will work with provers and so on and so on. So that PBS work already started many we look at many protocols and try to help them to build POCs. So reach.
07:06:54.432 - 07:07:56.230, Speaker A: Out if you would like to have those conversations with the flashboards team. And, yes, we'd love to hear from all of the L2 S, all of the domains of what exactly works here. What doesn't work is there much Mev to make here, how the market makers think about it is really needed and is it value aligned? And just to summarize, maybe Slide was initially at the front, but we didn't know about the time. So the flashboards Nethermind collaboration started around 2021. And there's a bit of invitation for many builders teams or like, protocol engineering teams to maybe start collaboration with SWAF, with Flashbots on swap on multiple components. We do lots of research materials together, FRPS, but we also do engineering. So I hope that the journey of being totally outside of Flashbots and enjoying the fantastic time on research and building can be your journey as well.
07:07:56.230 - 07:08:00.790, Speaker A: Thank you so much. That's all from me.
07:08:02.060 - 07:08:09.800, Speaker B: Thank you. Tamash. We don't have time for question, but we have Joe coming up from Aztec.
07:08:10.300 - 07:08:39.980, Speaker A: Thanks, this one. Hey, everyone. Thanks for having me. Cool. So, my name is Joe. I'm one of the co founders of Aztec and I'm going to run through PBS, Mev and some of the considerations on L2 S. It's not quite as simple as we may think.
07:08:39.980 - 07:09:14.680, Speaker A: I've never used one of these clickers before. Yeah, it's not working. Okay, thank you. Sorry, I was blinding someone in the audience. I'm Joe. So what is Aztec? We're a fully private ZK roll up on ethereum. We have private state, and we have fully composable contract calls between contract A and contract B.
07:09:14.680 - 07:09:47.986, Speaker A: We also have hybrid execution. So there is mev. We have things like public function calls for AMMS, and as such, we're trying to think through what does decentralization look like. And we have a big focus on launching decentralized. You can follow me on Twitter and follow Aztec for more info on that. I'm going to try and get through this in 15 minutes, but we're going to go through why we should care about decentralized sequences. Differences of block building on l two versus l one.
07:09:47.986 - 07:10:45.170, Speaker A: There are some key distinctions, a walkthrough of two proposals we received in our RFP process for decentralizing the sequencer, and a little kind of some thoughts on enshrining Mev auctions in L2 protocols. So, firstly, why should we care about decentralizing sequences? I think it's fair to say the industry doesn't really care right now. L2 Beat has an amazing dashboard and I think no one, including Aztec Connect, has made it past stage zero or stage one on the Training Wheels framework. If this was kind of like a report we're doing pretty badly, you get the Meme. I think, like, some of the actual reasons we should care, a lot of roll ups are designed to be first. So we want to be the first private roll up, the first Ckevm. And this actually kind of has pretty harmful consequences.
07:10:45.170 - 07:11:23.120, Speaker A: I think we should start designing things to be last. They need to stand the test of time. Decentralized systems need to be really robust. We learned this the hard way with Aztec Connect. Trying to decentralize a system with hundreds of thousands of users is way harder than just going to the whiteboard and getting it right first time. And if you're at the whiteboard, I think we can start to solve some of the issues in the l One block supply chain and try and get to a world where maybe more than five people are producing blocks, and the last one won't be a surprise. I think regulation is a key reason to do this.
07:11:23.120 - 07:12:05.950, Speaker A: Crypto is global, but regulation definitely isn't. And having a very centralized point of failure is something we need to kind of work around, especially with headlines like this between the differences between the UK and US, for example. All right, so let's look at the differences between blocks on L One and blocks on L2. I think the first place to start is around the size of transactions. So for kind of non private chains, you just submit a signature over kind of like a request or a transaction request, but for a privacy. L2, you submit a proof of execution. So a zero knowledge proof of the transaction.
07:12:05.950 - 07:12:56.730, Speaker A: This leads to kind of much larger transactions with Aztec. If we use something like Goblin Plonk and very wide circuits, they could be kind of 20 times larger than a normal ethereum transaction. This isn't a deal breaker, but it just means that the same infrastructure or same things we need on L 1 may need to be adjusted on L2. The second area is compute. So just a quick look at kind of like the CPU cycles that are needed to actually produce a block on these different kind of domains. First of all, everyone's kind of familiar with searching. There's a lot of compute that goes into the knapsack problem, trying to figure out the correct ordering for a block on L One, and then you've got to execute all the transactions and do it doesn't really stop there.
07:12:56.730 - 07:13:45.950, Speaker A: For ZK l two S. We also have proof construction. So constructing a proof of correct execution can be a million times slower than actually executing the transaction. So when you're going to put all that together, the summary is kind of block building on L2 S is not really the same as block building on L One. So should we use the same systems to kind of produce our blocks? And I think the main point of this slide is just this is like a huge centralizing force. If you need ten times the compute, 100 times the compute to be a block builder, and you're the fifth best block builder, is it really worth it to turn on your machine? So that's something we need to design around we don't have all the answers. So we did an RFP to try and crowdsource some of them.
07:13:45.950 - 07:14:17.590, Speaker A: We had some fantastic input from teams like Espresso Systems and Flashbots and Astria. So thank you, everyone who participated. I'm going to run through two of the proposals now. One of them was written by me and one of them was kind of written internally, but also it learns a lot from some of the Espresso Systems work. They also have drinks names, which I don't know where that came from. If you want to kind of look at the in depth write ups, you can scan these QR codes. I'll just wait for 5 seconds.
07:14:17.590 - 07:15:16.680, Speaker A: So the first one for Net, it really isn't that different to Ethereum. It tries to kind of take what works on Ethereum and mold it a little bit for L2 and it really focuses on fast transaction pre confirmations. To do that, we use an L One smart contract to administer the whole process. And we have a proposer builder separation. Like we're all familiar with. A proposer, likely with the help of mev Boost will submit a block header to the L One contract and that will include commitment to the ordering of transactions, hiding the actual ordering, but a commitment to that through just a commitment scheme and a VRF output on their staking public key. All proposers have to stake in this model, and the VRF output between Proposer A and Proposer B will differ on each block.
07:15:16.680 - 07:16:16.700, Speaker A: After kind of the proposing phase ends for a given slot or epoch, the proposer broadcasts the block contents to L2 and provers who are listening to kind of the L2 can compare the VRF outputs, look up on the L One contract and see if it's a winning score for the VRF. If it is, and all the data is available, they can start constructing a proof from the contents. So the cool thing here is that anyone can submit that proof back to the L One. And it doesn't have to be kind of submitted for the chain to move forward at that point in time. It's needed to be submitted for finality. But you can kind of look at the chain and be like, if I've got all the data and this is the winning VRF output, I can move forward. And eventually the block which has a valid proof and the highest VRF output will become canonical.
07:16:16.700 - 07:16:59.720, Speaker A: This means that we can get soft finality by building or proposing blocks that build off kind of unfinalized blocks. If all the data is available. And you can use this with shared sequencer designs and kind of it's quite extensible, much more like Ethereum. Yeah. If no proof submitted, you kind of roll back to the last canonical block in B 52. This is where things get a bit more controversial. My kind of goal with it was let's try and get rid of the proposer and to do that, we enshrine an Mev Burn auction in the protocol and we instead focus on decentralizing the proven network.
07:16:59.720 - 07:17:55.284, Speaker A: So firstly, I guess, why does a proposer exist on l One, which may lead us to kind of see could we get rid of the proposal on l Two? And I think it exists because you need to check the work of other proposers. It's the main reason it exists. A proposer on l One could technically lie, and other proposers check that work. That's kind of the power of the ethereum consensus model and it also exists for censorship reasons. The wider the set of proposers, the higher the chances are that eventually someone will come along who's not willing to censor me. So it's pretty good reasons for it to exist on l One and as such the block rewards on l One kind of incentivize, creating a very large validator set on a ZKL2. My claim is that these properties don't really hold, so we don't need to check the work of other proposers.
07:17:55.284 - 07:19:06.812, Speaker A: You can't lie ZK proofs mean that if the proof system is correct, we just submit a proof to l One and we can inherit the checking work of all the proposers on l One. And we also don't need the censorship property because we can force transactions in directly from the l One smart contract. So these two kind of main reasons for having a proposer don't really exist on a ZKL2. So what's the point of enshrining a proposer into the protocol who can really just sell their right to produce blocks to someone else? Instead we could use a block reward to incentivize approving network or something else in the protocol. So how that works, we split things in B 52 into two distinct phases there's a block proposal phase and a proof acceptance phase. A bit like finet, there's no proposer here, so the builder is doing everything in this scenario the builder broadcasts a block header to the l Two network. This contains commitment to the transaction ordering like before and a bid in the Mev Burn auction.
07:19:06.812 - 07:20:12.388, Speaker A: This is the amount they're willing to burn for this block to become canonical. They also commit to payment to the Staking network who's going to help them make a proof later on. Stakers then vote on the l Two for which proposals they will construct proofs for, likely based on how much they get paid. But there could be other factors like censorship or other factors which influence this and each vote is weighted by a VRF output from the Staker's public key. The builder then kind of aggregates all these votes on the l Two and they assemble a transcript of the block. This is committed directly to l One and it kind of allocates who's going to do what in the proof construction phase and also lets us define a total score for the block. And the l One roll up contract can rank these proposals by this score, and the two weights are kind of the sum of the prover votes and the amount they're willing to burn.
07:20:12.388 - 07:21:04.840, Speaker A: We're still modeling this so it could change significantly. Again. Like with finet, because this is on l One. At this point, the prover network can just look up on the contract and see which blocks have data available for them and are they safe to start constructing proofs for anyone then can submit a proof to the L One roll up contract and the same property kind of applies. The proof with the highest score becomes ahead of the chain and you can pay a block reward to the winner plus uncle provers as well to keep liveness. So these proposals differ quite a lot on what happens to mev. And we don't know what the right answer is right now, but we can look at kind of other L2 S and see what's happening and see how we compare.
07:21:04.840 - 07:21:51.700, Speaker A: So on optimism, I think the rough model is a centralized sequencer at the moment, extracts mev to fund public goods. That's a great outcome on Arbitrum. I think with the time boost proposals, any mev that comes from that or fees will go to a dao, potentially Aztec, if we go forward with the Finet proposal, will look very much like ethereum. You'll have a proposer and a builder who can extract mev from the protocol. And this will be facilitated by a market like Flashbots, for example. If we go forward B 52, we've deleted the proposal. So it's the protocol and the builder that can extract the mev via an in protocol mev auction.
07:21:51.700 - 07:22:40.660, Speaker A: So to help us decide on these, we're doing a lot more research, but I've tried to put together some rough economics of what I think the different variables are in this. So if we look at kind of the revenue stack for a proposer today, it's roughly transaction fees, block reward and the bribe they get from the builder to get the block on chain. And the costs are pretty straightforward. There's also, obviously the builder's revenue stack. The ordering preference fee is designed to kind of just encompass all of the different arbitrage opportunities that exist for the different builders. And there's kind of the unknown relay costs. We don't really know who pays those at the moment, but I guess it's species and some costs associated with searching.
07:22:40.660 - 07:23:21.316, Speaker A: If we were to kind of remove the proposal, the builder's revenue stack could look a bit like this. So they would get the full transaction fees, the block reward and the ordering preference fee. And in return, they have to increase their costs by doing some in protocol mev burn. So, yeah, I guess my claim is that these are the same thing. So the bribe that's currently paid to the proposer could be paid to a protocol for a ZKL2. Lots more research needs to be done on this to see if this is a good idea. There is research on ethereum around maybe doing something like this.
07:23:21.316 - 07:24:02.364, Speaker A: But for a ZKL2, I think the requirements are a lot less strict because we can rely on the L One for a lot of these censorship properties. So the main takeaways so far of where we've got to in our decentralization of the sequencer journey, the two proposals we're kind of going forward with at the moment are slightly based. So it's not full anarchy. You can't just submit roll ups to L One whenever you want. But they do use an L One contract to decide which blocks canonical. And there's some scoring logic in that contract for both proposals. They also both used a VRF.
07:24:02.364 - 07:24:53.596, Speaker A: This helps us kind of protect against 51% attacks and someone kind of acquiring too much of the stake or other attack vectors here. Finet. Decentralizes proposers like ethereum, and B, 52 tries to decentralize the proving network. We need a lot more research on enshrining MEP markets, and that's pretty much all I've got time for. So instead of you guys asking me questions, I'll ask one question to you guys. Should we enshrine mev the protocol or not? If you have any thoughts, please come and talk to me afterwards or now. Thank you very much to Joe.
07:24:53.596 - 07:25:10.116, Speaker A: I mean, I think Aztec's approach to all the proposals has been one of the most thoughtful in the space when it comes to PBS. Now, though, we turn the stage to Ben Fish, applied critov professor from Yale and also the co founder of Espresso Systems. Take it away.
07:25:10.218 - 07:25:22.010, Speaker D: Thank you, Reed. Thank you very much. Right, so I modified the title a bit from the original one. Where do we there's a clicker. Let's see.
07:25:26.080 - 07:25:29.550, Speaker A: There we go. Okay.
07:25:30.640 - 07:25:46.108, Speaker D: PB Espresso. This is not my idea. This is Quintess's idea. I stole it from him. He has all the creative ideas. So I'll be talking about PBS for shared sequencing and responsive consensus protocols. These are sort of two unrelated topics.
07:25:46.108 - 07:26:29.932, Speaker D: Well, maybe not completely unrelated, because responsive consensus is Espresso's consensus protocol for shared sequencing. But the talk will be split into these two different subjects on PBS. So first, just to quickly introduce what is shared sequencing? So rollups are horizontally scaling the application layer of Ethereum. The idea is that when you add new applications, then you can have those applications add new servers that execute. Some of those applications are roll ups that support other applications. But I view roll ups as basically applications that support other applications. And the layer one is now not doing any execution.
07:26:29.932 - 07:27:10.540, Speaker D: They are just verifying fraud or ZK proofs. It's also leveraging this heterogeneity. So you can have powerful servers on the application layer. You can keep the servers on the layer one very weak. They just verify proofs. They don't need to do any extreme execution. The problem is roll ups control a lot, and that leads to the need to either decentralize the roll ups themselves and add some kind of decentralized process for each roll up to decide on the ordering of its own transactions, or it moves us towards a different model where they share the L One.
07:27:10.540 - 07:28:05.980, Speaker D: So the other problem which remains if each roll up just runs its own consensus protocol for its own transactions, is that they remain very isolated from each other. And so currently, even though we're sharding the application space of Ethereum, we're sort of ruining this amazing property that Ethereum has, which is the interoperability of contracts. One contract that you deploy can easily make function calls to another contract and they share liquidity with each other. So roll ups don't scale Ethereum without changing its fundamental properties. And that's not really scaling. That's scaling and changing the target. So how can we regain the functionality of Ethereum as it is today while still scaling Ethereum? So one idea that has come out is for roll ups to share a consensus protocol for ordering transactions.
07:28:05.980 - 07:29:00.290, Speaker D: So this is this idea of separating ordering from execution, where the L One doesn't build or execute, right? The roll ups do that, but the L1 can provide finality on transaction ordering, availability of data, verification of state proofs and transactions, submit directly to the L One, and the roll ups just read from the L One and execute. So there's the question of does this give us interoperability? It's also good to note that this layer that's shared by the roll ups for ordering doesn't need to be Ethereum itself. It could be some layer 1.5 that sits in between. And the main reason for that is protocol modularity. Perhaps we would introduce a new protocol that has the ability to provide faster pre confirmations than Ethereum does. Ethereum has slow confirmations time.
07:29:00.290 - 07:30:00.336, Speaker D: So this is a reason to design shared sequencing layers that are not Ethereum itself, but some other protocol that sits on top, whose sole purpose is to provide a different property to roll ups, like higher throughput and faster finality. The concept is, though, the same users are submitting their transactions to the consensus protocol that is independent from the rollups. Roll ups read from this consensus protocol and only execute and report state results. So, does sharing an ordering layer help interoperability if the ordering layer is not doing execution? That is the main question of the first part of this talk today. So I'll say there are three advantages, but I'll focus on one that has to do with PBS Proposer Builder separation. There are also advantages such as simplifying cross Roll up bridging Mitigating systemic security risks for bridges. We've written about this in blogs from Espresso Systems.
07:30:00.336 - 07:30:50.268, Speaker D: But I will focus on this topic today of supporting cross roll up building with economic bonding. And what is this? So first you've been hearing about proposer builder separation all day, but maybe everyone has a slightly different way of describing it. So I'll just do a quick review and present sort of my view. So the proposer is a fundamental part of the consensus protocol because all consensus protocols today are based basically on leader election. If we use consensus protocols that are not based on leader election, then we might end up with something different. But consensus protocols like Ethereum and like the consensus protocol used in Espresso or Tendermint or any of these systems, have a leader that is elected to propose transactions that then get ratified or signed by others. So there's always a proposer.
07:30:50.268 - 07:32:19.208, Speaker D: If someone says that we can remove the proposer, what it really means is that you can just use the layer one proposer and you don't need to introduce new proposers in the L2. But there's always a proposer from the system that finalizes the ordering of transactions so builders today can compete and then produce a block. The proposer doesn't need to build a block, it can play this passive role of accepting a block from builders. And if we leave it up to proposer to do this and don't design anything into this building layer, then what ends up happening is a first price auction where the proposer will just accept the most valuable block that comes out of the builder network. And so that's a reason to take a different design approach where either we might do this in terms of implementing some smart contract that introduces some rules that we hope proposers will follow, or it can be some kind of other ideal functionality similar to what Suave is designing that implements some kind of order flow auction among users and searchers. If we can design something that is not just maximizing the profit for the Proposer, but is stable and better for users and users are only submitting their transactions to this ideal functionality. Whether it's a smart contract or a protocol like Suave, then the Proposer will have no choice but to accept the block of transactions from this ideal functionality.
07:32:19.208 - 07:33:04.284, Speaker D: Because there's no transactions going anywhere else. And this only works because the consensus protocol is decentralized. If the consensus protocol was not decentralized, then a proposer could say, I'm going to ignore this smart contract, this isn't good for me, I'm going to ignore this ideal functionality. I want people to submit to me the most valuable block. I want to extract all the mev because I'm a monopoly, right? The decentralization of blockchains breaks this monopoly so that a proposer is a monopoly for only one slot. And that is why if we design an ideal functionality stable for users, the proposer cannot influence what happens. The proposer can either take it or leave it.
07:33:04.284 - 07:33:41.540, Speaker D: If it leaves it, the next proposer will pick it up. The most it can do is delay. So that's why extreme decentralization is so important from an economic perspective. This is one of the most fundamental properties of blockchains from an economic perspective. So proposer builder separation over shared sequencers looks the same as over Ethereum today because Ethereum is a shared sequencing layer. So if we introduce a layer 1.5, it's just introducing a new modular layer on top of Ethereum, which runs an order finalization protocol, but still has some proposer as part of the consensus protocol that will accept blocks from a builder network.
07:33:41.540 - 07:34:51.932, Speaker D: And we can design that builder network to be smart and do things in a way that's good for users, like Suave or some kind of smart contract design. So one of the things that I'm not going to really focus on mev as much as this problem of regaining interoperability and atomicity among roll ups that are sharing an ordering layer is that through PBS, builders can actually make promises to users about atomicity and they can be slashed if they violate those promises. So a very simple example is the following. A builder can say, I will process your trade on optimism and your trade on ZkSync, which may be taking advantage of some arbitrage opportunity to two AMMS on the different roll ups, and I promise that I will only include them. I'm building a super block for both roll ups, and I will only include these transactions together if they both succeed. If one of them fails, I won't include either. And this statement can be cryptographically signed so it can be used as evidence against the builder if it violates this promise.
07:34:51.932 - 07:35:43.730, Speaker D: So it can be slashed, it can build up a very large collateral that gives the user confidence it will respect its promise. And the builder then provides this to the proposer of a consensus protocol that's not executing at all, but is basically auctioning off wholesale a super block that produces a subblock for every roll up simultaneously to the builder network. Okay, very simple. Without a shared sequencer, this would not work. At least it would not work as well. The builder would have extremely high risk that if it tries to make this promise to the user, it's possible that the independent sequencer or consensus protocol for ZkSync will accept the transaction, but the independent sequencer or proposer consensus protocol for optimism will reject it. Right.
07:35:43.730 - 07:36:26.212, Speaker D: There is now two independent auctions and it doesn't have a guarantee it will win both. Whereas with a shared sequencer and a single proposer that is elected, there's one auction for both together. So the builder could be slashed and lose its high collateral, even if it doesn't do anything wrong. So that's why sharing a sequencer reduces the risk of builders, allows for more builders who can make these atomicity guarantees. So the builders sit on this layer on top of the shared consensus. This is how it looks in espresso. A slightly more complicated example would be a flash loan.
07:36:26.212 - 07:37:17.148, Speaker D: So let's say a user wants to get a loan from Ave on one roll up of 1 million USDC deposited into an AMM trading USDC against Dai. So now it can't immediately bridge its funds over and just deposit it directly into this Curve contract on roll up B. So we'll introduce a bank contract and the bank contract is going to have a branch on both roll ups. Okay, so there's a branch on roll up A, there's a branch on roll up B. We'll explain how the builder can ensure this atomicity of these two events. But basically, the user will send Dai to the bank contract on roll up A and it will get Dai from the bank contract on roll up B. Then it will deposit into curve, get back its USDC and do the reverse deposit into the bank contract on roll up B.
07:37:17.148 - 07:37:50.504, Speaker D: Get it on roll up A. I haven't explained how we ensure that those can happen atomically. I will in a moment and then return the USDC to Ave on roll up A and keep the profit. All of this needs to happen in a single transaction. That's a flash loan. So how does a builder enable this? Well, the builder who's building the block for both roll ups simultaneously could provide a signature, again backed by some collateral. And that just gives an instruction to this bank contract on roll up B that says this action happened on roll up A.
07:37:50.504 - 07:38:31.648, Speaker D: So you can release the funds on roll up B and the builder creates the super block. The bank contract on roll up B just trusts the collateralized builder signature. If the builder lies it's accountable and loses the collateral and Roll up B is not absorbing the risk. All the risk is absorbed into the bank contract. So it can become a business to design these contracts and specify your requirements for what builders need to put up in terms of collateral and other possible conditions. And it's assuming some risk, but it can also be taking a fee. So this is an example of something that's enabled by shared sequencing, which would not work without shared sequencing.
07:38:31.648 - 07:39:12.592, Speaker D: Of course, it's not exactly the same as what you can do on a shared execution environment, but it opens up a wide design space. And really this is just a straw man simple construction. I encourage people to come up with more creative ideas. The builder signature would do the same for roll up A bank contract as well. It's symmetric. There are some concerns that having builders execute for all roll ups may lead to builder centralization. The reality is that executing for all smart contracts sorry, for all roll ups is not actually that high a barrier to entry, right? In fact, compared with ZK Proving, it's not that high a barrier of entry.
07:39:12.592 - 07:39:53.680, Speaker D: ZK Proving is extremely expensive. It can be like 10,000 times or more expensive than executing. And the requirements of the builder market don't need to be the same exact requirements in terms of decentralization as the consensus protocol. We need many, many nodes participating in consensus so that proposers act passively and don't monopolize. We don't. Necessarily need 12,000 competing builders to have a competitive market that takes care of monopolistic forces. So it can just be sufficiently permissive and open and free entry and have low enough barriers to entry such that it's sufficiently competitive.
07:39:53.680 - 07:40:43.852, Speaker D: In fact, the barrier to entry for builders is much higher without shared consensus because builders take on more risk. As I illustrated a slide earlier, builders would require more capital to win simultaneous auctions. They would absorb more risk that can't promise user atomicity without the risk of being slashed. So where there's a will, there's a way it's better to have shared sequencing. We will lower the barrier of entry for builders rather than increase. So now for something completely different, I will talk a little bit about PBS for responsive consensus protocols. So, ethereum's consensus protocol is not responsive, meaning it has a fixed block time that is 12 seconds.
07:40:43.852 - 07:41:25.404, Speaker D: Ethereum's finality is not actually 12 seconds. It's much longer than that because the transaction needs to be several blocks deep to be confirmed. So it takes about 15 minutes to finalize. In Ethereum, a responsive protocol does not maintain the extreme availability that ethereum has. These two properties are incompatible. But what a responsive protocol can do is it can give finality as fast as the network will allow. Fundamentally, it will stall if the network is not doing so well, whereas ethereum can continue even if only 10% of the network is live.
07:41:25.404 - 07:42:14.808, Speaker D: So, due to the Cat theorem, these two properties are essentially incompatible. But optimistic responsiveness is the property that can get you as close as possible to the performance of a centralized sequencer in a decentralized way. The nice thing is you can compose them together, because you could run optimistic responsiveness on top of a dynamically available protocol. So consensus protocols that can return an answer immediately under optimistic network conditions are called responsive. There's no block time in responsive protocols. So how does this affect PBS? So, let's look a little bit at the round structure of Hotshot, which is based on PBFT and is a responsive protocol. In the first round, the leader proposes a block, collects back signatures.
07:42:14.808 - 07:42:50.984, Speaker D: Then it will aggregate these signatures, form something called a quorum certificate. It will broadcast it again, aggregate them, and collect signatures on that. So, again, this can happen if the network is very well connected, or maybe there's even a content delivery network that assists this to proceed at very high rates. Then, as soon as the signatures are collected, the protocol doesn't need to wait. It can just move on. Finality has been achieved. So under optimistic conditions, we can finalize blocks as fast as the network will allow.
07:42:50.984 - 07:44:07.312, Speaker D: The problem is, with PBS, if a leader has only one proposal before the next leader comes up, then it's incentivized to delay, because in order to run an auction, it could, for example, run the auction for longer and obtain a more valuable block from builders. So one solution is not to rotate leaders based on blocks, but rather to rotate leaders based on some time window. Okay, so we can allow the leader to propose and obtain finality on as many blocks as it wants, consecutively, until this time window expires. I'm not going to go into the details of how you do that in the consensus protocol, but that's the main idea. It doesn't completely get rid of all incentives to delay, but it goes very far to mitigating this primary concern. When is it safe for a builder to release data? So in PBS for Ethereum today, the builder releases data after receiving a block hash, and then it races with the proposer to relay that block hash in order to get that block accepted. And at this time, it releases the data because Ethereum nodes won't accept and vote for the block without receiving the data.
07:44:07.312 - 07:44:44.424, Speaker D: Now, if the proposer proposes a conflicting block hash, it could get slashed. But sometimes that slashing amount is low compared to the amount of profit it could make from stealing the mev from the builder. So they really do engage in this race. And it's even possible for the next proposer to fork the block and steal the mev. So the protection for builders today on Ethereum is not actually that high. And I'll argue that it becomes a little bit better when we're looking at a responsive protocol. With a responsive protocol, once a builder has received a signed proposal, it can ensure the network reaches strong finality within a certain time window and under good network conditions.
07:44:44.424 - 07:45:28.110, Speaker D: Of course, it's possible for the network to experience a fault. But it can gain confidence given its observation of the network, and again, a much higher confidence that it will be able to succeed in getting this finalized and not allowing the proposer to steal the mev and do something else for even stronger protection. The builder may only release the data after seeing the first quorum certificate on the original proposal. Remember, there's two rounds. This just brings it closer to the finalization gate, and it doesn't really need to release the data until this second part. There are some caveats of that. Now we only have one round to broadcast the data instead of two.
07:45:28.110 - 07:46:16.940, Speaker D: That's easily solved by broadcasting an encryption of the data starting at round one and then releasing only the decryption key starting at round two. The decryption key is small and can be broadcasted faster, so that solves that problem. It's harder to pipeline proposals, though, with this idea. So one of the nice things about protocols like Hotshot is it allows for overlapping rounds. But if we hold onto the data and we're already proposing a new block in the second round, then this gives the builder of the previous block an advantage for building the next block, since it's the only one who really knows that information ahead of time. This might be addressed by order flow auctions right? These ideal functionalities might be able to take care of that. They essentially create one builder abstractly that is the ideal functionality and have all the real builders just participating in it as a multiparty computation.
07:46:16.940 - 07:46:32.080, Speaker D: So thank you very much. That's what I had to tell you about today. And next up we have Evan Forbes who is going to tell us something exciting about ending the proposer monopoly.
07:46:40.280 - 07:47:26.680, Speaker A: Hello. Hello. Okay, so I work at Celestia, I'm a core dev. My name's Evan and I'm here to talk about exploring mev capture in modular systems and really start a discussion about a few ideas that they already exist. If we combine them, we can create a very powerful tool. And to be honest, I just don't think enough people are talking about it. And now before I get any further, this has trade offs.
07:47:26.680 - 07:47:56.884, Speaker A: Everything has trade offs only siths deal in absolutes. So just getting that out of the way. The alternative title to this talk was how to make Validators Cucks. And on one sense I'm just being playful, but on another sense I'm being very real. Weirdly real. So like the status quo of validators is capturing large amounts of value, right? This is how it works today. And this stems from the fact that they have a monopoly over that given block.
07:47:56.884 - 07:48:12.936, Speaker A: Now, it is a small monopoly, it is only for one block. But sometimes, like in the context of a builder, it's very important. We can have a builder build a block. It has three heart emojis. That's a lot. Everyone loves that block. But that's not the block that gets committed.
07:48:12.936 - 07:48:37.700, Speaker A: That's not the canonical block. That's because a different builder found a way to exploit more mev or do something else and ended up bribing the proposer more. So now we pick that block. So the proposer is getting most of the money now. That's not the end of the world. It's not even the worst thing. Ideally, the proposer can up the security budget.
07:48:37.700 - 07:49:16.290, Speaker A: It's not even sure that this is not ideal. Like all mev has to get captured somewhere. Going to the proposer is not the worst place. However, it would be really nice if it was more programmable if we had a way for L1s and L two S or roll ups to be more programmable. So we also have to consider that with roll ups this has to be scalable. Not just in the sense that there's like 1000 different instances of this setup, but also in the sense that roll up devs only have so much brain capacity. They can't be playing like these 5D chess when they're thinking about these things.
07:49:16.290 - 07:49:52.536, Speaker A: So shout out to Skip if anyone from the skip team is here for pushing the narrative of sovereign mev. Now, all I'm trying to say here with sovereign mev over solving mev is that there's many different solutions to capturing mev. There's not just one, there's many. We're picking winners and losers here, all right? So that decision is between a community and their god. That is not like a decision that you can make for everyone all at once. That's at least what I believe. So in order to solve this, to make sovereign mev, you have to move it on chain.
07:49:52.536 - 07:50:27.300, Speaker A: Because when something's on chain, it's programmatic. And when something's programmatic, it inherently in the context of a blockchain is verifiable. So if we want it to be sovereign, all we have to do is choose to change our binaries. By simply switching our binaries, right? We now have changed how mev gets captured, assuming that we can get it in protocol. So in order to do this, you have to break the proposer monopoly. This must happen. So this is a meme that I stole from Elijah.
07:50:27.300 - 07:51:04.870, Speaker A: Elijah works at Duality. He's done some amazing work in this area of ending the proposer monopoly. And if you recall the first slide, right, if we don't end the proposer monopoly, then there's always going to be some sort of censorship that they can do in the context of mev to game the system. So we can write whatever clever rules we want on chain. But it doesn't really matter because the validator can always or the proposer can always accept a block that gets built to game the system. So also shout out to special mechanisms groups. A lot of this work is just like repurposing their work.
07:51:04.870 - 07:51:33.976, Speaker A: Highly recommend. So we kind of have to think about different things differently. Instead of building a block and then coming to consensus over it, we have to come to consensus over an unbuilt block. And then we build it, right? We build it using a deterministic function. So there's two things necessary here. We need to have a way to have for proposers multiple proposers include their input for what they want to be in the next block. In the next block.
07:51:33.976 - 07:52:00.592, Speaker A: Elijah refers to this as multiplicity. So there's many different ways to do this. One of the simplest ways or one of the ways that I keep coming to is just some different flavor of narwhal and or tusk. And that is great because it makes the entire block censorship resistant. There's other ways to do this. So Skip does top of block auctions. They already have this implemented.
07:52:00.592 - 07:52:26.270, Speaker A: If you want to go try some of these ideas out, you can literally go do this by building a Cosmos chain right now, thanks to Skip. And you have self assembling blocks. So this is the second part. The second part is that you need a deterministic function to build a block. And it's very important that the order of the inputs doesn't matter, right? It's addition. You can add two numbers in any order and you'll get the same result. That's the important part here.
07:52:26.270 - 07:53:04.744, Speaker A: Once we have these things, we are now free to define mev capture. So this is again in the context of L1s at the moment, where for the simplest case, all we need to find, all we need to define is a less than or equal function of what a bundle is. Now, what's the only thing better than one? Fire emoji. Three? Fire emojis. Correct. So if we have this, we can do something that's similar to a fork choice rule, but now we have a bundle fork choice rule. So you imagine we have a less than function.
07:53:04.744 - 07:53:28.124, Speaker A: That's all you need for it to sort. Now, we can sort these transactions or these bundles, and if there are any conflicts, then we go back to that rule. Well, which one has more fire emojis? Okay, well, we're going to pick that one. And you can replace fire emojis, really with anything. But this has a cost. This has trade offs. This is not for every state machine.
07:53:28.124 - 07:53:43.168, Speaker A: This is not for every blockchain. In the context of Celestia, there's one thing you can do. You can pay for data to be included in the block. That's it. It's payments. This means that there's basically no conflicts. Conflicts are incredibly rare.
07:53:43.168 - 07:54:21.888, Speaker A: So that dramatically, that makes it so it's like very viable thing to do in Celestia. Now, if you consider other chains, like most chains today are like these permissionless Turing complete smart contract platforms that whenever you see a transaction, you can't even know what state is accessed until you execute the transaction. That's not really like a viable thing. You can't just have a deterministic function. Not only that is that if you have a lot of mev to be exploited, then it turns out that you have something that I'm referring to as death by gas auction. You just get completely overwhelmed unless you have some sort of private memple. That's not like a hypothetical.
07:54:21.888 - 07:54:46.436, Speaker A: That's like a well tested thing that happens all the time. So it's very dependent on the chain. Fortunately for Celestia, you can do this. So now the proposer monopoly has been broken. We can define how we want mev to be captured on the L. One roll up should also be able to easily define how they capture mev. And they can do this too.
07:54:46.436 - 07:55:09.676, Speaker A: Right. The proposal monopoly is broken. We can submit multiple different roll up blocks as if they were a builder, like in PBS to the same Celestia block. And now it gets included. Right. And now we can define similar how we defined a bundle fork choice rule. Now we can define a just fork choice rule, a normal fork choice rule, except for it's completely arbitrary.
07:55:09.676 - 07:55:36.612, Speaker A: We're used to fork choice rules with, like, hash power and stuff. Right? This is just completely arbitrary. What does heart means? It could mean, judging by Twitter sentiment, people love to burn tokens. I hope we get more creative than that. But that is like a perfectly functional thing. Whoever burns the most roll up token, not the L1 token, the roll up token. Whoever burns that, that's the canonical block, right? Fork choice rules rule.
07:55:36.612 - 07:56:00.416, Speaker A: Because you can just have anyone build. You can make it so that I can have like literally anyone can submit a block. Roll ups do not have the same issues that L1s have, where you have a limited set of block producers. Roll ups can have however many they want. It just has to be defined in your fork choice rule. So you can like again, this can be very arbitrary. Rock beats scissors.
07:56:00.416 - 07:56:48.690, Speaker A: But the coolest thing about this is that if we ever decide that we want scissors to beat rock, all we have to do is change our binaries, right? We can just do that. And in the context of modular blockchains, in the context of these, we have these trust to minimize Light clients, right? These Light clients are the ones that are verifying the chain, the node operators, right? So they decide light clients decide how this actually operates, how mev gets captured. Oh, and as a side know, you can decentralize how your roll up gets sequenced just by simply having a fork choice rule and relying entirely on the consensus of the base layer. No big deal. Shout out to rollkit. They're working on this. If you really want to get your minds blown, talk to a rollkit dev.
07:56:48.690 - 07:57:10.888, Speaker A: Lastly, again, not everyone can do this. This is extremely limited. It almost requires privacy tech. You can ditch sequencers entirely. You can just have a bundle fork choice rule, and the blocks just build themselves. So there are trade offs. This will not work for everything.
07:57:10.888 - 07:57:41.090, Speaker A: Okay, so it has increased demand for blockspace. Currently, that's DOA it won't work. But can we imagine a world where blockspace is abundant? Thank you, Nick. So now if blockspace is abundant, this is actually more feasible than we think. And increased full node requirements. There's no way of getting around that this has increased full node requirements. However, it's very application specific.
07:57:41.090 - 07:58:37.556, Speaker A: It's very dependent upon your fork choice rule. It's very dependent on so many different things. So we can build around this if this is a property that we want really bad, and I mean, this isn't really saying anything new, but in order to have good mev, we need more privacy tools. And same thing goes for here. So when mev is defined in protocol, and when Light clients can verify the protocol, right, that means that Light clients can determine how profits are captured in a system. That's pretty crazy, right? That's pretty damn crazy, right? Power changes hands. It is no longer just the validators, the politicians, the technocrats, and yes, even the token holders who are deciding these things.
07:58:37.556 - 07:59:03.020, Speaker A: It is light clients. They are cucking all of these people. Okay, so that is how I would like to end my talk. If you're interested in anything else. If you're interested in anything else, I have a GitHub. Evan Forbes. Or just find me on Twitter or anyone from Celestia.
07:59:03.020 - 07:59:46.120, Speaker A: Thank you very much, Evan. And next up we have John who will take the stage. Let me use this thing. Okay, cool. I've had a lot of coffee and I have a lot of slides and I don't have a lot of time, so I'm probably going to be bouncing for this a bit. I'm going to be talking about why proof of stake kind of makes sense but kind of maybe doesn't make sense. And particularly I'll be there we go.
07:59:46.120 - 08:00:30.232, Speaker A: So this will be applicable to pretty much any proof of stake type chain, but in particular, I'll have a lens on roll ups for this while I'm talking about it. So we've all heard there's like a million different sequencer, decentralization options, shared sequencer, you can run auctions proof of stake, like all these different options. In particular, I'm going to be focusing on these last kind of subcomponents of that which is applicable to a lot of places. So it's proof of stake and proof of authority and kind of like variations thereof. And my goal kind of by the end of this talk is to convince you that those two are actually kind of the same thing. When you play out how this rationally looks, at the end of the day, whether you have a proof of stake mechanism or you just literally let your governance just say, hey, these are the people that we want to be our delegates, and you just don't even bother with staking. They kind of end up in the same place.
08:00:30.232 - 08:00:57.836, Speaker A: Roughly. So starting from really basics, what is proof of stake for? It is not to slash people's money. The fundamental reason why you have proof of stake or proof of work or similar things, it is fundamentally a civil resistance mechanism. We need people to vote for us. And if you just let any single person, one person, one vote, you get civil attacks. So we need to meter who can actually contribute into this system. So the simple way to do that generally in any permissionless protocol is you tie it to some kind of real economic resource.
08:00:57.836 - 08:01:35.864, Speaker A: So proof of stake, you put up stake proof of work, you prove that you burned energy, so you've submitted some real cost the other way around. This is also you could just have basically like a permission setting, noting that the difference between permissioned and permissionless is actually kind of subjective in how people define it. But you could just basically have some form of trusted setup where we say like, hey, these are the people who are voting for us. We know who they are, we trust who they are. Okay, so looking at proof of stake, how does it play out when you actually look at what happens and where the incentives go? So simple, delegated proof of stake. If you don't do it in protocol, it'll rise out of protocol. But basically there's a clear separation between the people who have capital and people who actually want to run the operation.
08:01:35.864 - 08:02:16.180, Speaker A: So you naturally need some form of delegation. Like, I might have capital that I want to stake, but I don't want to run this stuff, so I delegate it to someone else. That's super capital inefficient, among other issues. So you end up with liquid staking tokens. Like, this is what we see on Ethereum, this is why Lido popped up. So this helps provide people liquidity do the auto protocol delegation for them hands people back like, okay, I don't want to stake, so I get my Steeth token and someone else, they go figure it out how to manage that. For me, one of the problems with liquid staking that people talk a lot now about is this kind of inherent principal agent problem between someone like Lido and someone like Ethereum or any other protocol is lido may have a profit incentive that is not aligned with my values and my governance of my chain.
08:02:16.180 - 08:02:55.590, Speaker A: So this is why something like dual governance is pretty much necessary in my mind for any kind of liquid staking to be viable in the long term. So some way for that protocol to align itself with the underlying protocol. So in the case of Lido, what they're talking about is like, okay, what if we get, instead of just letting LDO tokens decide everything, we give Steeth holders Vito rights, who are presumably a subset of this ethereum consensus that are aligned and will have a different motive than necessarily the profit seeking LDO holders, which is helpful. So then I'll get to the next thing. So proof of authority kind of is like this dirty word that whenever I try to say this, the reaction is generally, oh, that's super centralized. It's going to be way less secure. Like, this doesn't work.
08:02:55.590 - 08:03:42.456, Speaker A: I'm going to try to convince you by the end of this that I think variation of this can be more secure and more decentralized than a proof of stake mechanism very clearly. So proof of authority, I think that's a reasonable term when you're talking about a truly private permission type blockchain, potentially even like an RWA one where circle has real control over this thing. Proof of governance. I'm saying that there's a clear, incredible governance mechanism place to choose these delegates. So it's actually a bit reminiscent of the way that Mustafa has described the difference between sovereign rollups and smart contract roll ups. It may be more of like a social distinction more than a very concrete technical one, but I do think it is important. So simple example would be like, look at roll ups they'll presumably take in the simple case like, you have a smart contract bridge on ethereum that has very powerful governance for this roll up and for something like Garbage Room.
08:03:42.456 - 08:04:15.760, Speaker A: You already even see the initial form of this today where you can force inclusion of a transaction through the l one and they could forcibly remove the sequencer if they should so choose. And similarly, they could just choose like, hey, here are the ten delegates that we want to be our sequencer instead of the one. So now I'll start to go through concretely. What are the differences between all of them? So one is permissionlessness. So this is part of why I'm targeting this. Specifically at roll ups, I would say off the bat, this is obviously not an Ethereum type option. That should be very obvious why Ethereum does not want to be actively saying like, hey, here are the 30 people that we trust to do this for us.
08:04:15.760 - 08:04:58.220, Speaker A: You want to let validator number 1 million join the validator set and be non opinionated about that, which I think is very different from roll ups. And that's the point here, is that sounds like a downside, but I don't think that roll ups are optimizing for the same thing that their base layers are optimizing for. They're already paying the base layer for that decentralized consensus in the first place. And that is why they're not going to even from an economic perspective, it's not even going to make sense for them to have validator number 1 million and no one's going to want to do that. And it doesn't add anything, really. It's particularly for that kind of doomsday scenario that they're already paying the L1 for. And in addition, kind of going back to Patty's talk before, it's actually very helpful in many cases to have permissioning because you want them to do things like enforce certain transaction ordering.
08:04:58.220 - 08:05:28.792, Speaker A: Okay? And governance. This is very related to the point before ethereum does not want active opinionated governance. That's like saying, hey, we're picking all these people. I think the practical reality is that for most rollups though, they're probably going to have this. You look at the ones today in the simple examples like Optimism Arbitrum, they have this giant bridge. You're required to have some form of legible on chain governance that will be presumably powerful unless you think that they're optimizing for this very immutable type contract. And governance, which I tend to think is rather unlikely for most of these, but it does make a difference if you do so.
08:05:28.792 - 08:06:07.216, Speaker A: Accountability. This is like one of the things that everyone kind of harps against here is like, this is one of the big benefits of proof of stake is like, oh, we could slash the bad guys. And that is seen as one of the benefits. I tend to disagree with that once you actually play out what this looks like in the long run, because the reality is the large majority, if not all stake is going to be delegated. Like there's a natural economic incentive to separate capital from labor and we see that play out very, very clearly in every proof of stake system. What I think is the real advantage of proof of stake over proof of work is it's not necessarily the slashing aspect. I think it's more so particularly once you play out the economics, it's the very targeted removal of who the malicious attacker is.
08:06:07.216 - 08:06:39.852, Speaker A: Where in proof of work you nuke the hashing algorithm and you restart from scratch and you're kind of screwed. So in proof of stake it's not so much the slashing part of it that I think is necessarily the most important part of it. It's that you can very targeted remove, okay, these are the bad guys. These are the people we want to get rid of. And you still keep the same thing here. If you have your governance, just electing them. So this is why I think economic security just gets a little overhyped sometimes when you look at the simple incentives, you could have $1 billion or $100 billion if the guy who's the operator who's doing that, it's literally not his money.
08:06:39.852 - 08:07:19.550, Speaker A: This is not like an incentive compatible thing. It is just a reputation game that you are assuming that they don't want to light your money on fire, which is fine, but they also don't want to screw their business and lose out on all future profits. It's the same thing. It's an understanding that this is a reputation game and not like a cryptographic, crypto economic incentive compatible design. So the other point is so you realize, okay, so if it's not necessarily just as a punishment mechanism, you realize, but it is still important. Like okay, if I'm going to get slashed, I'm going to pick my delegator wisely. So it is an incentive mechanism for me to pick a delegate appropriately which does so moving.
08:07:19.550 - 08:08:18.716, Speaker A: Like this is from a post that Nick Carter had a while back, called it's the settlement insurance in stupid which is actually very good. So taking some pieces from it ledger costliness, you're talking about the amount paid to validators transaction selectors per unit of time. And particularly in the proof of work context, you can assume reasonably closely this reward paid is proportional to very close to the amount of energy and real cost that is spent to earn that. And a greater salary to them means a greater security because it is more costly to reorgan that chain if you're paying them more because that means is a higher cost to do so. And then on the other side is kind of badass quote from the prince that Josh sent me, which is kind of reminiscent of this. It reminds me of it a bit where if you're basically relying on mercenaries when the day comes when they're not necessarily aligned with you, you might kind of be screwed. So you probably want to have people who are aligned with you and not just always explicitly dollar for dollar profit taking in a situation like this.
08:08:18.716 - 08:08:47.192, Speaker A: So talking about settlement insurances broadly, system's ability to grant recipients confidence a transaction will not be reversed. So economic security when. We talk about it. We're talking about what is the cost to reorg the chain here. So proof of work, as I mentioned very simply, you have to pay some money to literally mine the blocks to do this proof of stake, it's costless to simulate blocks. So when we're generally talking about the economic security, we're talking about the slashing. But as I kind of just described, that slashing is not necessarily deterrent if you're delegating all of the capital to that person.
08:08:47.192 - 08:09:40.756, Speaker A: So it's not like the actual same protection there. So delegation does remove that and you start to realize it is more of a reputation game. So if we play out that scenario where you do have that separation of capital and labor, which is where the natural economic incentives lead to what is proof of stake? At the end of the day, it is back to that thing that I said in the beginning as like the fundamental thing. It is civil resistance mechanism and a mechanism for us to delegate who are the people who care about our system and those are the people we're going to delegate to vote for us. It is not necessarily a way for us to actually penalize them because there is an incentive for it to not be their money and have that separation again. So then the question is, okay, if this is a mechanism for us primarily just to actually pick people who are very aligned with us and have the right distribution of the validator set, is this the best mechanism for us to pick them? And this is where I have a rather strong opinion. I do not think that it is because it's a simple tragedy.
08:09:40.756 - 08:10:26.132, Speaker A: The Commons if you take out liquid staking protocols, I think DPOs is the worst without them, arguably because what does regular user do in this scenario? I go look for who's the biggest validator, the second biggest validator, I go delegate it to Coinbase or whoever else. It's nice that you have a permissionless long tail, but the practical reality is the large majority of stake ends up going to like two or three people versus proof of governance. You're not putting like okay, my dollars go to this person. Like I select that person. We are voting as a collective for what is the best distribution as a collective interest for all of us. So we can go intentionally pick okay, here are the 30 geographically distributed people that we like and we give them equal voting powder across them. I think that's significantly harder to corrupt and potentially more decentralized.
08:10:26.132 - 08:11:17.732, Speaker A: And that's actually what we see with moving on here. This is the decentralization which kind of falls on that. What is decentralization? It's kind of that point that I was saying there of is it validator number 1 million that makes you decentralized or is it like the composition of what do we care about the top of the stake and in particular this is kind of like the visual example of it. Which of these is decentralization? And particularly for a roll up? I would argue the second one is actually the much more important one, particularly because they are paying the L1 in this case for that doomsday type scenario of recoverability. They are very much optimizing for what are the real time short term guarantees because also their reorg cost is being immediately reset once they go back to the old one. So I think the second one is more important and the second one is actually like that is Lido's ethereum stake distribution. This is very intentionally what they do under the hood.
08:11:17.732 - 08:11:55.280, Speaker A: It is we pick these 29 delegates who are decentralized, who are responsible, and these are the people that we equally distribute votes to. And that is the kind of thing that you can basically just in house your protocol if you have the opinionated governance to do that. So in terms of capital efficiency, this should be intuitive TPOs incredibly capital inefficient. Like it's not good for your type of roll up economy or whatever it is. LST is significantly better. But you are adding on the additional risk of these financial derivatives discrepancy between different types of holders on bonding periods, et cetera. I would say proof of governance is absolutely optimal in kind of all of these dimensions.
08:11:55.280 - 08:12:32.592, Speaker A: Everything stays in the native base asset. Everyone has an equal value capture to whatever it is you decide. You want to burn it, go ahead. You want to send money to your treasury, to allocate it based on whatever your subjective governance process is, go ahead. And you just have this explicit agreement like the validators get paid X year sequencers. And so this is what it comes down to is the alignment that I was kind of touching out with the dual governance before. So this is a section taken out of Danny Ryan, his very popular post on the risks of LSD where he's talking about, okay, Lido is doing this dual governance thing which is supposed to like okay, that'll mitigate the principal agent problem.
08:12:32.592 - 08:13:14.584, Speaker A: But he's saying like, okay, but Steeth holders and ETH users, that's not the same thing. It is still like a significantly lesser alignment. And then hasu had a post after that pointing out, which I agree with is like this is not just like a Lido liquid staking thing. This is just like an inherent problem with proof of stake. People who are staking in DPOs, they are also not the entire superset of roll up chain users consensus. There is a disparity there. So what do you want to have? Do you want to have your roll up token holders just like deciding by themselves, these are the people that we delegate to? Do you want to have an LST governance deciding it completely for your chain, that's maybe a little bit better because maybe it's more decentralized, but it's also maybe worse because you're less aligned.
08:13:14.584 - 08:14:09.608, Speaker A: LST dual governance, I would say, is the best of those three because you're letting a subjective governance process intentionally decentralized but retaining power over it to veto harmful actions. And the last one is the cleanest one of just why don't you have your own chain decide again, this doesn't make sense for something like Ethereum because they're not going to have that opinionated concrete governance process to be like selecting all these different validators. But that is the exact opposite of what a lot of these roll ups are doing and I don't think it's a coincidence and happened to actually see it. I think it was like a day or two ago. Optimism is, to the first of my knowledge, the first team to announce this in their governance post that they put out the other day, that they intend to basically this is their plan, at least initially, for the sequencer. It appears that their token house will be able to initially choose who are the sequences that we want to whitelist and then their identity based citizens house would be able to veto those decisions. That is very different than just like a straight up token holders alt vote.
08:14:09.608 - 08:15:07.888, Speaker A: And so that is kind of the point here, is we all know that token voting is not necessarily the best and this lets you bring in that more subjective governance process that you may want to be creative with, even if it is just straight token holder governance. I would argue that the majority token holder governance as a collective would make a better decision than everyone doing it individually. But when you also introduce something like this, where you allow identity based governance to be able to have additional rights, that is a very clear advantage over these other ones where you're entirely subject to like, okay, what do the token holders decide? So this is kind of related to that. Like, who is the monopolist in this scenario? There are short term monopolist powers of whoever is the sequencer, whoever is the validator in these protocols. I would argue the much more important monopoly power on any rational time span is like whoever chooses those people. So whether that's the token holders, the governance, like, whatever your mechanism is. And this is like a similar quote again out of Danny Ryan's post, which is like yet Lido could become in that position.
08:15:07.888 - 08:15:47.680, Speaker A: That's an important reason for dual governance, but that is a power that you don't want to be handing over and I don't think that is the power that the Sequencer reasonably has. Yes, they could do things on a very short time horizon and then you can immediately remove them and they lose all of their future revenue. I think that latter person is the person who's actually in control there, clearly. So as a quick summary, proof of stake, it makes sense for protocols that are permissionless. They don't want opinionated governance. That makes sense for Ethereum, but clearly LSTs will rise and they do help decentralize, meaningfully. In my mind, Lido adds to the decentralization of Ethereum very clearly on just look at the operators and distribution versus if everyone just gave that stake to coinbase, it's clearly significantly better.
08:15:47.680 - 08:16:09.952, Speaker A: So some form of dual governance is also necessary in my mind. Proof of governance. I think that too many roll ups are just over indexing too. Like okay, this is what Ethereum did, so we need to do the same thing. But I think that they are fundamentally building for a different thing. Like Ethereum is building for roll ups. Roll ups are building for users primarily and they're optimizing for very different things because they're already paying their base layer for all of those types of assurances.
08:16:09.952 - 08:16:47.516, Speaker A: So I do think that this is very viable for them because they do have very different priorities and because sequencers are fundamentally just like less powerful than L. One validators in particular, they are getting this finality and immediately that cost to reorg effectually goes back to zero. When you're posting to the layer one, they are very short term guarantees. This is kind of the key, I would say synthesis of the whole argument is like as a summary, if we put things back together just like how does proof of stake end up playing out? You do some form of simple DPOs and okay, capital holders, they want some form of liquidity. We want delegation of stake away. Liquid staking arises. That's very natural.
08:16:47.516 - 08:17:29.676, Speaker A: It's capital efficient, it's better. But we have that principal agent problem. So we introduce dual governance and what did we end up with at the end of the day is a total separation of capital and labor because that's where all the economic incentives are. And we have a governance mechanism that is aligned with our chain that manages the delegate selection. So the only questions at this point remaining are really do we want to let that delegation sit outside the protocol? Do we want to have this taking mechanism? Let Lido rocket pool, whoever decide that, and then we try to rein them in with some sort of veto, right? Have a market there? Or do we just in house that because we have our own governance. We don't need to let them decide. We don't need to leak value out to some other protocol that's going to take a chunk of all the revenues.
08:17:29.676 - 08:18:06.952, Speaker A: And then why add these financial derivatives? I came from a financial derivatives background before this and I'm rather opposed to just layering on a bunch of financial derivatives on top of the consensus layer. If they're unnecessary, they tend to introduce a lot of hidden risks that we don't particularly understand. And I don't think it's a Pandora's box that needs to be open for roll ups necessarily. And in particular, if we realize that it's primarily a voting mechanism, it's not like an incentive compatible deterrent. So we don't necessarily need it. And I think that it's a risk that is better to avoid. So it's not even that proof of governance is like this perfect thing.
08:18:06.952 - 08:18:34.964, Speaker A: I think that this is inevitably where you end up if you just play out the economics. I think this is the rational market outcome of letting these economics play out, is this is where you end up. It is the governance, it is the changes that is in control in the best case. So why not just enshrine that and put it in the best way? This is the reason that I'm kind of skeptical about it. Governance is really hard. I do not doubt that it's really, really hard to do this. Just saying, like, oh yeah, go replicate everything that Lido does.
08:18:34.964 - 08:19:19.490, Speaker A: But in particular for chains like optimism, like Arbitrator, et cetera, that have an active, that have a very involved governance that is thoughtful and is able to decide these are the delegates that are aligned for us, I think that makes a lot of sense. This is PBS day, and I didn't really talk about PBS. You could do PBS with all these. And I think it actually makes it easier because when you have a permission set, the simplest example would be like, you're kind of back to the Mevgethy days more so where it's harder if you're trying to optimize for mev boost, like, oh, yeah, we need validator number 1 million to be able to do this. And we can't trust them. If you have like ten people, five people, whatever it is, that you can kind of trust them, makes it a lot easier to not have a lot of these censorship and sensorialization problems. Like the sequencer can add transactions in and you trust them kind of not to screw you.
08:19:19.490 - 08:19:30.630, Speaker A: And that is all. These are the slides. Thank you. And I have no idea where I am on time. I'm assuming I'm over and that Tina is up next.
08:19:31.080 - 08:19:48.620, Speaker B: Well, in fact, we're switching water a little bit, but before we do that, one question from the audience. Anyone? All right. Oh, it's going to be a tie.
08:19:57.600 - 08:20:38.520, Speaker A: Hi. Really enjoyed that. Thank you. And it's in line with mostly aligned with most of that. One question I do have is how do you think proof of governance affects liveness and the relationship between that and censorship resistance? Because there does seem to be some use cases. Maybe it's a governance vote, maybe it's DeFi liquidation, maybe it's sort of a game that's being built on an L three that requires very strong liveness guarantees. And it strikes me there are a lot of instances where liveness failures can actually be censorship resistance failures.
08:20:38.520 - 08:21:29.644, Speaker A: So I'd be interested to get your perspective on whether or not proof of governance changes that and your thoughts in general. Yeah, I mean, it's definitely primarily like one of the largest reasons, I think that you should at least decentralize the sequencer to some capacity. Even if you don't need to go to a meaningful extent, it is the simple fact of like, yeah, servers go down. That is not a good thing for any application, whether it's high value or low value. I would say the absolute minimum that I would want is even if you have a default centralized sequencer, is governant votes of okay, we have these five backup, backup sequencers, okay, if X number of blocks are missed, we just go to the next one. We go to the next one just like down the list and then open it up after that. So I do think it is critical, even if you want a default centralized sequencer, because it is super efficient to have one person giving you these guarantees, but be ready to go of like these are the governance selected people that we trust to fill in for us.
08:21:29.644 - 08:22:18.750, Speaker A: Similarly for censorship, I think again, it is easier in my mind to do the PBS stuff when you don't have to trust validator number 1 million. You could say like, hey, we have 20 of them. And the builder becomes less of a choke point now because, okay, they could see the block, they can add in the rest of the transactions. It looks more like mevgeth, which is helpful. And then also just the simple composition of the sequencer set themselves is I think it is much easier to get a graph that looks like the Lido distribution graph versus if you just let staking economics play out what you really care about here for censorship, resistance errors it's not like what's the composition of the last 5% of Validators? How many are in there? It's how many do you need to get to? One third, 51%? Two thirds. And I think this is the clearest way to get a very broad composition across that. And then you could also add in other mechanisms if you want, like multiplicity, et cetera, to help with that as well.
08:22:20.720 - 08:22:29.170, Speaker B: Great question and great answer. I'm really curious of Nick's question, so I'm going to let you ask away.
08:22:31.060 - 08:23:18.972, Speaker A: Can you elaborate on why you think this is more appropriate for rollouts rather than L1s? I remember on the slide you mentioned the permissionless part of it, which I do agree with, but can you elaborate more? Yeah, so I guess I would say oversimplifying there. It's not necessarily just a layer one versus L2, like as kind of shown in a lot of the screenshots there. One of the people who's been starting to tweet about this lately is like a sonny and I think this makes sense also for plenty of cosmos L1s where it's very similar to what I'm describing with the roll ups of like, okay, we have a very clear, tight knit community that's aligned on what our values are. We have an active governance process. We are able to manage this. So it's more of a neutral permissionless base layer is like, what you're going for that would not want this. It's just not an option.
08:23:18.972 - 08:24:01.816, Speaker A: So ethereum is a perfect example of obviously they cannot do what I'm describing. It would make no sense. And that is reasonable in my mind because particularly for the base layer, that is generally the area that you're looking for. Like, okay, we want you to be completely credibly neutral, like no subjective process. Even if the free market economics are kind of concerning in the long run, I think it's also the place where you have a best shot of winning that battle, I would say, quote unquote, where if the economic incentives are to centralize all of this, there has to be some kind of soft pressure against that. And I think ethereum is a type of place where even if there's a slight incentive to centralize, it more, like there is a very strong community and a very strong push for decentralization. I think that's like a winnable battle to very intentionally keep it decentralized subjectively.
08:24:01.816 - 08:24:21.300, Speaker A: So there are different optimizations for different chains. So it's not necessarily just an L1 versus an L two. I think that just, generally speaking, broad brushstrokes like L two S tend to fall in that latter category of more opinionated, more active governance, and are optimizing more for real time guarantees as opposed to neutrality and long term nuclear war type resistance.
08:24:24.520 - 08:24:53.180, Speaker B: All right. Thank you, John. Thank you for your two X speed speech. Next up, we have a panel, the End Game. How many of you here have read a blog post called Endgame? Raise your hand. Okay. Is it by Rune from MakerDAO or raise your hand if it's from Metallic.
08:24:53.180 - 08:25:02.130, Speaker B: All right, we've got an informed audience. Shall we?
08:25:15.930 - 08:25:31.946, Speaker A: Hello, everyone. I think we'll be joined shortly by Floating Avatar in the Universe. Oh, there we go. Hey. Great hand. Holy yeah. I hope you're having a good day.
08:25:31.946 - 08:26:31.550, Speaker A: Avatar, sorry I haven't watched your second movie yet. It is true that Anatoly does look like he could have been an extra in the Barbie movie, but I don't know what else to say about that right now, but nice. This is my fame and cosplay for Oppenheimer. Yeah, well, as we learned today, vitalik prefers Bob the Builder, and maybe we'll have Bob the Builder next time. But as many of you know, this panel is the Endgame panel. And I think part of this panel actually stems from last year at the last modular summit, which was sort of April 2022. So before Luna, when we were all felt safer, more comfortable, less annoyed at people in the world, I felt less safe before the Luna collapse.
08:26:31.550 - 08:27:07.750, Speaker A: Maybe that's also fair. Safer as in I think the debate was actually quite different at that time. At that time, I think there was a lot more hype about monolithic architectures, and modular architectures were sort of coming from behind. And I'd say obviously some of the stuff that happened the last year made people flip or things change. The Luna stuff, it had lots of moral lessons for me, but totally nothing to do about monolithic versus modular. So I'd love to hear that case. I think that's more like app chain versus monolithic.
08:27:07.750 - 08:27:39.902, Speaker A: Right? But okay, I guess to me, Luna was not bad because Luna was monolithic versus modular or like, on ethereum versus on bitcoin versus on cosmos versus Ursulan or whatever. Right. Luna was bad because it was a fundamentally bad design. Like the exact same design carbon copied onto ethereum would have been just as big a collapsible failure for sure. I'm more pointing out that at this time, people were much more living in 2021 land. Okay, this is true. And that was when I think the psyche was broken.
08:27:39.902 - 08:28:10.400, Speaker A: Right. So I just want to say give some context of the last time we did this. Modular was really the underdog. I feel like there were a lot of arrows. Right. Remember the scene where Boromir just gets shot down by a bunch of ORC arrows at the same time, and it's know, first you have Luna, and then you got the Celsius arrow, and then you got the big FTX arrow, and then he's kind of dying, and then there's just like one extra arrow from what was it? I guess like DCG or whatever just shoots in. Yeah.
08:28:10.400 - 08:28:13.210, Speaker A: So there's been a lot of arrows.
08:28:13.290 - 08:28:13.866, Speaker B: Indeed.
08:28:13.978 - 08:28:25.794, Speaker A: More than three. Well, may all our future arrows be in beautiful diagrams. There you go. Exactly. Commutative diagrams only. We're all secretly category theorists. I don't know.
08:28:25.794 - 08:29:09.082, Speaker A: I prefer non commutative because we want to have progress, right? We don't want to just go from the same place. All right, well, clearly there's already even debate about whether anything has changed since last year. So maybe Anatoly and Mustafa, let's start with in the last year, what in the monolithic versus modular worlds has changed from your minds? Yeah, so I think one of the biggest changes is a year ago, there wasn't really any kind of roll up frameworks. There was like, optimism, arbitram, and other kind of optimistic roll ups, but there was no kind of frameworks to kind of create your own roll up. Now. We have Opstack. We have Arbitrum orbit.
08:29:09.082 - 08:29:55.294, Speaker A: Roll Kit as well. And I think kind of last year, a lot of the emphasis was the main reason why you need roll ups is scalability, which is true. But I think what I've realized over the past year is that one of the most underappreciated aspects of roll ups and modularity is that it gives developers more choice and freedom over the execution environments. And that lets you do really cool things that just haven't fundamentally been possible before. You can modify the EVM and add a ZK opcode so you can do more efficient ZK verification than just graph 16. And that's really good for privacy. Or, for example, Curio has modified their EVM game engine to add their entire game as an opcode on the app stack, which can be fraud proven on their MIPS fraud proven system.
08:29:55.294 - 08:30:54.226, Speaker A: So I think this kind of like this flexibility of execution environments is really underappreciated and I think might be as just as bigger or as important for impacts of the space as the scalability aspects. Anatoly I think it's been really cool to see the development of roll ups and having them ship and actually seeing their performance numbers. And from my perspective, I've never thought of them as scalability technologies because fundamentally you're still limited by the bandwidth of the layer one. And if you can't exceed that, you can't really scale beyond that. We can do all sorts of other tricks, but that's kind of like the bottleneck. And when you start scaling that piece, we haven't yet seen folks go beyond what a monolithic architecture that can provide. So I think that's coming.
08:30:54.226 - 08:32:13.210, Speaker A: I actually am pretty bullish on things like light clients giving pretty close to better than honest majority assumptions between Shards and all these other things. So I'm excited to see for that next iteration in the actual base layer. But so far from my perspective, actually the main use case for roll ups is that like giving new execution environments, giving new ways for devs to build a full game as a fraud provable thing. I think that's a really cool thing. Vitalik it's interesting because I think pre 2019 vitalik was fully on board with the idea that roll ups are not real scalability technologies. But then that was when I moved pretty decisively away from that paradigm. If you read my pre 2019 stuff, I was talking about how roll ups aren't real scale because they're not a big O notation improvements, right? I talk about how regular blockchains are OFC like their capacity is linear in the capacity of a computer, but then shortening is OFC squared, right? But roll ups are of C times somewhere between ten and 500, right? And for a long time the mathematical purity side of myself got really hung up on that.
08:32:13.210 - 08:33:08.006, Speaker A: But then at some point I was like, okay, yeah, fine. Like a factor of ten to 500 is still a big deal, I guess. And at the same time we kind of exhausted state channels in plasma and all of those things that just realized some of the fundamental limitations that they have in terms of how plasma requires every object to have a logical owner. And the whole defragmentation problem that I'm sure if Carl Forsch was here, he could tell even more about because they tried to even actually implement that. But basically a gain of ten to 500 x is still a gain. I think it is true that in practice we've been slow on getting there. One of the reasons why is because close to half that gain comes from data compression.
08:33:08.006 - 08:34:10.640, Speaker A: And roll ups have been pretty slow to actually implement data compression. And there's all of these reasons of like, oh, well, we don't want to have l, two nodes have to be archive nodes and all of these things. But these are issues that can be engineered away and even EVM parallelization it can be engineered. And there's a difference between sort of solved in your head and hard engineering slog solved. And that's been one of my own big learnings about the ecosystem. So I think to me, I still look at this from the perspective of what is this going to look like even three years from now, right? Because we're definitely not nowhere close to the frontier of what even currently known approaches can provide, right? So with that actually a great thing to talk through. There have been multiple endgame blog posts all over the internet, but all three of you I'm sure, have very strong views on your views of what the end game should look like.
08:34:10.640 - 08:35:46.734, Speaker A: So maybe in your own words, define what you think the end game of the technologies you're really interested in should look like. What kind of things should they offer users? What kinds of things should they offer sophisticated actors to maintain them? What should they not offer them? And I think having an eye towards where you really think the long far future is, I think Vitalik's talk today ended with a very good picture of what he thinks the future is, at least in terms know the big proof proof. I mean, like my view on the future is definitely that we have proofs now. And I think I've used the analogy that succinct zero knowledge proofs are to cryptography what transformers are to machine learning or to AI. They're this one single technology that is just so powerful all by itself, that just completely sweeps across decades of hard application specific work by thousands of talented people and just says like, okay, bye bye, you're gone, and replaces it with a vastly better alternative, right? And that's just something that we have to adapt to, right? Anything that gets built today should be built with the assumption that proofs exist as an ingredient. And that was not really part of the mindset a few years ago. And a lot of things change when that becomes part of the mindset, right? I mean, obviously projects like Mina were, I think, pretty early to that.
08:35:46.734 - 08:36:42.930, Speaker A: And had the EVM been conceived even five years later, it would have looked very different. Had, I mean, even the beacon chain been conceived, even a few years later, it would have looked very different. Even when the beacon chain was built, it wasn't really built with this assumption of proofs in mind. And so if we have a world where we can just assume that things can be proven and there's going to be light, like everything that can be computed is going to have a light client of it and all of those things. That in some sense simplifies a lot of things, but it still leaves open a pretty interesting design space of what kinds of things are going to exist. And there's a lot of details that you can talk about. Yeah, so to piggyback on that proofs aspect, when I kind of think about the end game, I think about what is scalability.
08:36:42.930 - 08:38:03.318, Speaker A: I define scalability as throughput, divided by the cost for an end user to verify the chain. And so for the end game, I think, how can we maximize throughput transaction throughput without increasing the end cost for the user? And that's only possible thanks to taking know data availability, sampling, fraud proofs, UK proofs. And that should be the case even in the world where they're centralized block producers because you can still achieve the properties of a blockchain. You can still have censorship, resistance and verifiability even in the world where there's centralized block producers by using things like PBS and Cr lists. But to kind of go further, to me the endgame, I see a world where it will be easier in the future for developers to create their decentralized application by deploying a roll up chain than deploying a smart contract. Simply because in a world where there's proofs, there isn't a good strong reason for apps that are not too directly connected to each other to be sharing computational resources when the computation can happen on their own environment and simply have a proof that it happened correctly. All right, I think the biggest exception to that is Composability, right? Anatoly.
08:38:03.318 - 08:38:44.760, Speaker A: With that, I think we're ready for your endgame. Yeah. It's kind of interesting how different it is. So even going into the space five years ago, I always thought that the biggest challenge was the equal and fair availability of the data itself. And this came from my experiences as kind of like amateur trader through interactive brokers and all these other systems. Whenever I had an algorithm that I thought had an edge, my trades would always be a little slower and the data that it would need would always arrive a little later and I would always kind of lose. And that's because I was playing in an unfair system.
08:38:44.760 - 08:39:41.190, Speaker A: And the way that I designed Solana was with this idea that we have a single global message bus, and it tries to propagate all the information around the world simultaneously, as fast as possible, at the speed of light. With this kind of, like, end state being that imagine some crazy trade event happens in Singapore that newswire still has to travel speed of light through fiber to a Bloomberg terminal in New York. But by the time the trader looks at it, a state transition propagating through Solana already adjusted for price impact. And the price information at Nizi is exactly the same as in a market running in Solana because all that data is propagating at the same time. So this is a system that I envision back in the day. And this is kind of still what we're building. We're trying to reduce block times from 400 milliseconds to 200 milliseconds.
08:39:41.190 - 08:40:14.946, Speaker A: We're trying to make sure that there's multiple concurrent block producers so you can send your transaction to the most geographically closest one. There's competition for map. All of this stuff gets better and better and faster. And in that world, what's cool is that I think Solana obviously needs to have a lot of hardware to do that because you're sending a lot of data. You're trying to keep it all together, and roll ups are not going to help you. You're not going to improve that system by splitting up state logically. Those effectively create logical shards.
08:40:14.946 - 08:40:59.490, Speaker A: And you're not sending information globally to everyone. You're sending information only to some of the folks, which kind of breaks the whole point. But that system that we're building actually helps roll ups. Like, in my ideal world, all of these settlement chains that have roll ups should be all atomically sequenced in this one giant super optimized hyperfast synchronizing engine, right? That's one giant global atomic state machine. But obviously, we still need trustless computation. We still need to make sure that users that can't afford the hardware have some guarantees about honest execution of these systems. And I think the research in light clients that have been done by the folks, like the teams on the stage, has been awesome.
08:40:59.490 - 08:42:14.042, Speaker A: And I think in the future, Solana, for Solana to function, it has to have those capabilities as well. But the fundamental problem that I want to solve is that global state synchronization as close to the speed of light as possible, as open as possible to everyone in the world. Okay, so let's suppose that we end in a world where there's only really one settlement layer, and your chain of choice becomes an L2 of the other chain. I only asked this question because Anatoly has been writing a lot of tweets about making east an L2 of Salana. First of all, Anatoly, maybe you want to explain your tweets, because I personally was very confused when I read them. But I was trying to create, I guess, a straw man to demonstrate that what an L2 is is there's a mechanical component to it, which is a bridge with better than honest majority assumptions that functions. Because all the data that you need to prove that the smirkle route that's on your L One is correct is also on the L One.
08:42:14.042 - 08:42:53.800, Speaker A: So you can then run the fraud proofs. You can basically order whatever compute a ZK proof, whatever it is that you need to do. You have all the information to validate the smirkle route, which the route itself does not have anything else. Right? It's just a hash. So that's kind of like you can do that with anything. You can actually take ethereum L One data right now, dump it into Salana dump the state route that's computed from that Ethereum data and then run an optimism style proof to check if fraud has occurred and then do the Bisection and stuff like that. Does that make Ethereum a solono too? Mechanically, yes.
08:42:53.800 - 08:44:56.030, Speaker A: Socially, no. One reason why is because there's the saying that the sovereign is the one that decides the exception. And in blockchains the exception is bugs and 51% attacks. And the question is always like, what happens if Ethereum hard forks, for example? Right? If Ethereum hard forks, then there is a few possibilities, right? Like one possibility is that the Salana bridge and if we assume that it's a perfect ZK bridge, then according to the old ZK rules, the Ethereum chain will just start being invalid and that bridge will just basically create its own version of basically its own Ethereum Classic 2.0. Right? But then the question is like, well, do the assets what do the assets follow? Right? And realistically, in this case, the assets would follow Ethereum, right? But if you get into a world where the majority of assets are rooted onto solana and then bridged onto Ethereum, then that would look very different, right? Or another world in which and in that case, then Ethereum would not actually be capable of hard forking unless there is some kind of on chain governance, right? Or a possible Third world would be a world where the assets are all based on solana, but at the same time, the solana community is willing to hard fork whenever Ethereum is willing to hard fork. Right? And in that case, that's like a construction based off of some sort of more deeper version of on chain governance. And in that case you would still be able to call Ethereum and L2, but it would be part of this ecosystem where things are basically willing to be part of the governance of each other.
08:44:56.030 - 08:45:57.746, Speaker A: So I think you basically just have to look at what happens if one chain or the other chain gets 51% attacked and what happens if one chain or their chain gets 51% attacked and the community decides to do a user activated soft fork to kill the attackers? So not like an easy slashing hard fork, but like a censorship hard fork where the community responds by picking the minority chain. And on the minority chain, the majority would have to either skip being on that chain or get slashed. And if they skip being on that chain, then they get leaked. Yeah, and then what happens in that case? Right? So the thing is though, that there's always going to be subset of users that are hardcore cipherfunks, censorship resistant maximalists that will always pick the right hard forks. They will always fork Salana and Ethereum towards the right state. So the real one is inside of all of us. Then this gets into yet another theater of the debate we've been having since 2015.
08:45:57.746 - 08:47:23.082, Speaker A: Which chain is the real ethereum? Who here thinks Ethereum classic is the real Ethereum? That's some good bravery. That's way higher than I expected, I'm going to be honest. Who here thinks Bitcoin cash is the real Bitcoin? Yeah, I think this kind of gets to the beat of how you define L two and so on and so forth, versus what's? A side chain? But those definitions aside, I still think it's an interesting proposition or useful to have a validating the bridge between Salana and Ethereum would be validating because most bridges between trust zones are not validating. They just have a committee based assumption. But if the Solana bridge to Ethereum at least verified the solana state via Vizk proof or fraud proof and then even if the DA wasn't on Ethereum and the DA wasn't on Solana and there's a committee assumption I think that would at least be, as far as I can tell, the same security properties as a validium or an optimistic chain, which is kind of like a much better scenario for a MultiChain ecosystem. So one thing that did come up in all of these answers to whether it's possible for one settlement layer to usurp another one, and make it an L two, is the role of governance in these futures, in the modular or monolithic futures. And so I think we've seen a lot of different variants of what qualifies as governance.
08:47:23.082 - 08:48:25.794, Speaker A: I think choosing a hard fork following certain people is a sort of weak form of governance. We just heard John's talk about don't even I haven't fully processed because it was spoken at high speed, to be honest. But I think where do you view the role of governance in your preferred modality towards the end game? How minimized is it? How important is it? Where is it necessary? Where is it only sufficient? Where is it non necessary? Yeah, I really feel like this upgrade thing, roll up. Upgradability is the kind of the core nuance there's been a lot of debate about. Kevin From Optimism is arguing that all roll ups are sovereign. And there's kind of a very controversial statement. And the reason why he argued that is because he argues that, well, all roll ups can just the community for roll up can just upgrade a roll up.
08:48:25.794 - 08:49:45.130, Speaker A: Ultimately, what a roll up is is what the community defines that roll up to be, not the bridge. But I think there's some interesting nuanced questions about how do we have upgradable roll ups while keeping the security properties of a roll up? The only kind of way I know of doing so is to have an upgrade delay and allow users to mass exit, which is not clear, if not clear how practical that would be in an actual practical scenario. Yeah, this is a tough choice, right? Because I think right now, L2 roll ups are, I guess there's like two camps. There's sort of the more activist side. And if you're on the more activist side, then I think the safe and responsible way is to choose a delay number and you could be somewhere between 30 and 365 days probably. And if an upgrade happens, and if you're not happy with the upgrade, or if governance gets taken over by evil people and the upgrade involves stealing your money, then you just have to leave. Which is good enough for most users, but it's not good enough for really long term applications that want to base themselves on that roll up, right? And then on the other side, I know scroll, for example, really values the idea of neutrality and sticking to Ethereum principles and kind of seeing itself as being an extender of Ethereum land.
08:49:45.130 - 08:50:48.958, Speaker A: And something like that might be willing to say hey, at code level we're not going to be willing to deviate from anything but being a copy of the protocol spec. And if they do that, then it takes away that risk for applications. But then it also opens up this interesting question of if Ethereum itself upgrades, then how would scroll end up copying that? And then this gets into protocol questions. I know this is one of those things that's controversial within the Ethereum research team even there's some people that are in favor of enshrined roll ups or an enshrined roll up. Assisting pre compile basically imagine a pre compile that just does EVM validation and it just does whatever EVM validation means during the specific block. And maybe you could let it do either the current hard fork or the previous hard fork. And then in the back end clients would just implement that by waiting for whatever their preferred zkevm proof is, for example.
08:50:48.958 - 08:52:12.330, Speaker A: Right? And if you do something like that, then you can preserve that tight coupling for the projects that want to do that. And then if you don't do something like that, then L2s would basically need to have governance on chain governance for the sole purpose of keeping at the very least for the sole purpose of keeping up with whatever layer one does, right? So that's one of those trade offs. And then obviously the other side of this is like this kind of underexplored class of crazier ideas where someone creates a layer one where that layer one is intentionally willing to be more activist and basically hard fork in. Such a way that even L2 assisting L2s with hard forking along with those kinds of changes, there's a big set of possibilities in off chain governance. There's obviously a big set of possibilities with on chain governance as we see it. And especially once you start properly moving away from the coin voting stuff. And then there's a whole set of debates about use cases of governance like are you protecting against tax? And that's it.
08:52:12.330 - 08:53:10.640, Speaker A: Are you improving the protocol? Are you doing public goods funding? Right? And then if you do public goods funding, then do you do it kind of by enshrining an on chain mechanism? Or do you do this kind of like retroactive off chain thing the way that Zcash does? Right. It's like every four years, they basically, as far as I could tell so far, they decide who the top level public goods funders are and then they do a hard fork to give them the block reward and then that gets revisited every four years, which is kind of cool in its own way. Right. And Zcash has made the choice to be more interventionist than Ethereum and Bitcoin are. But yeah, I don't know. It's a big design space, but do you think there's a way to allow roll ups to have upgrade mechanism with the same security properties as a roll up but without coin voting and without enshrining that roll up into the L One? Because that's one of the reasons why some people are interested in sovereign roll ups. Right.
08:53:10.640 - 08:53:47.398, Speaker A: I guess the challenge with sovereign roll ups is like, if you start using any assets where those assets are not registered within that roll up, right? Then it's like, well, for those assets, you can't be sovereign. Right. Unless you have more activist off chain governance. Yeah, for sure. But I think even though L One is truly sovereign, like, you have USDC and USDT, which is kind of like a non sovereign bridge between Ethereum and Circle or Tether. So it's definitely sovereignty is a spectrum. There's definitely trade offs, though, between sovereign wallops and non sovereign.
08:53:47.398 - 08:54:21.730, Speaker A: Right, that is true. I mean, you definitely could have tried to get every application to have separate instances on all of the different roll ups. And so you don't even cross roll up kind of bridged assets don't exist anymore, and then everyone could be sovereign. Well, if you do that, though, you would not be able to ETH would not be usable on roll ups. Right. Because ETH is, by definition, home ethereum. Any thoughts on governance? The major problem are zero days.
08:54:21.730 - 08:55:13.382, Speaker A: All these systems are too complex to not have some really fast path to upgrade and deal with a zero day bug. That's kind of like the end of the day. I don't know how long it's going to take for us to construct something that is so simple that everyone agrees this can be upgraded with a 30 day delay. We're okay with that. And if you have a fast path for zero days, you're kind of introducing honest majority assumptions. And I think that's mostly fine if that's what we're dealing with. I think we then have to be very careful about how we construct those majorities should they be like, well known security firms, et cetera, et cetera? Kind of the proof of governance talk, I think went into a lot of those considerations, but I think it's really tough to avoid those.
08:55:13.382 - 08:56:39.754, Speaker A: I think a couple of nuances on that one is you don't technically need a fast path to upgrade. You need a fast path to freeze and an arbitrarily slow path to finalize whether or not you're going to upgrade. And then the other thing is I definitely do not think that upgrade buttons should merely be a majority, right? If it's merely a majority, then whatever proof system you're doing technically has no votes, right? And I think you need to have at least a 75% threshold. The thing that I've been pushing for in the stage one definition, obviously, but I do think that being greater than 50% is important because otherwise it's mathematically true that the code itself has no votes. And then, well, the other thing is, I guess one of the ways that the Ethereum ecosystem is doing this is they are doing this kind of like inversion thing, right? I know some of the roll ups want to do this, right? They want to have multiple zkvm implementations and then the Security Council only gets activated if at least one of them disagrees with the other or something like that. Which is I think an interesting path too, as long as the implementations are being built independently enough. Yeah, I think at some level it seems like everyone thinks that there is some necessity of governance at the bare lease, bare minimum.
08:56:39.754 - 08:57:43.370, Speaker A: But it may be sort of to summarize, like in some ways synchronizing multiple chains in the case of the modular world or in Anatoly's view of the world kind of response measurement. Now maybe let's take a tiny detour. Imagine you had to come up with a complement for the architecture that's the opposite of the one that you like. You had to find something really favorite. Your life depended on coming up with a really good compliment for the other architecture. What would you say to your fellow developer speaking of Solana? Specifically, the way I see it is that Anatoly and Solana are trying to build the way I see it is scalability ultimately has trade offs between scalability and composability and even ethereum centric roadmap. You have some liquidity fragmentation of, say, like optimism and arbitrarim.
08:57:43.370 - 08:59:11.234, Speaker A: But the way I see it is like Solana is trying to see how much throughput we can get as much as possible without any composability sacrifices whatsoever. That's why Anatoly doesn't see roll ups as a scaling mechanism because it defeats the properties of the system he's trying to create, which is basically, as I understand the vision, is like decentralized version of the New York Stock Exchange. What I do appreciate about Solana is that they've made some great advancements in kind of execution know, the parallelized execution know that's a massive improvement. And I appreciate that Solana is actually taking trust minimization a bit more seriously. Over the past year, there's been some work and proposals over trying to create like clients and implement fraud proofs for solana and even some proposals to see how we can add data availability sampling to mean I appreciate that. There's a strong technical team and I appreciate the willingness to just say here's some set of use cases that kind of we understand, we're passionate about and we're going to basically optimize and be willing to make tradeoffs to do the best that we can for them. Antole well, I think the data availability, sampling and the whole fraud proof construction is like beautiful design.
08:59:11.234 - 09:00:12.962, Speaker A: It's awesome. And that actually, I think came out of the research of both of the teams here. So pretty cool. And the really cool thing is that everything's open source. As soon as those folks had really good ideas, I was able to understand them and start thinking about how apply them to the Solana stock. If we fast forward to kind of applications that you find interesting either within ecosystems that you're working in or sort of maybe even in the other ecosystems, what are some applications you're interested in in 2023 and 2024 that will be used? I think a lot of ECC this year, at least from my perspective, has been quite a bit of infrastructure, seeing a lot of updates on how certain proving systems are working, understanding how those developments have moved on. But we do at some point have to have to find some other applications on them.
09:00:12.962 - 09:00:56.558, Speaker A: Obviously you all probably have tons of examples of these that you're most interested in. What are they? Yeah, I think in the short term there's kind of been some really crazy progress in gaming, kind of roll ups. Curio managed to kind of run a real time strategy game with a 0.5 2nd tick engine using a modified EVM that adds part of their game as an opcode. And there's other stuff like August and other teams are working on interesting game engines. Now. I don't think gaming is the kind of ultimate purpose or the most real world case of crypto, but I think historically what we've seen even in computing, a lot of advancements in computing has come from gaming.
09:00:56.558 - 09:01:47.346, Speaker A: For example, the concept of sharding actually originated from gaming. So yeah, I'm just kind of interested in following those developments there and I think in the long term they will have significant impacts on applications outside of gaming as well. I kind of want the OG application to come back. I want people to pay for coffees with ETH. Remember back in 2013 when this was the whole big thing? And I remember this was actually the very start of my Bitcoin trip around the world where we would have I went to Boston and I went on a pilgrimage to Veggie Galaxy and Thelonious Monkfish because those were the two restaurants that accepted Bitcoin. And then we had the Bitcoin keats over in Berlin and York. Platzer got like ten restaurants and a block to accept it.
09:01:47.346 - 09:02:18.026, Speaker A: I want that culture to come back. And I think a big part of the reason it died is obvious, which is like, fees got too high, but the original pitch back then is like, hey, this is cheaper than PayPal. But then it stopped being cheaper than PayPal. But with roll ups, it actually will be again, which is amazing. Right. And I would love to see some of those amazing OG things that we did back then come back now that the ecosystem is starting to be ready for them. I think that will definitely happen.
09:02:18.026 - 09:02:41.038, Speaker A: We're seeing banks already using USDC for settlement. I think that will eventually be exposed to the end user. Like, it's being used on the back end, but it will eventually be exposed to the end user. I'm impressed. None of these applications so far have had any advanced cryptography. The payments, privacy preserving, but okay. Anatoly.
09:02:41.038 - 09:03:27.620, Speaker A: Yeah, I would say, like, simple things like payments and DeFi. I guess DeFi can be very complicated, but I think both of those really serve the two fundamental things that crypto provides and that's self custody and this idea of a global source of truth right. For pricing or the ability to transact, I think those are very transformative at scale. If you have everyone in the world that now has cryptographic keys that they know can move value around, everything else can be built on top of that. Like real world governance. Right. If everybody knows that we can all communicate and coordinate with cryptography, we can actually build very quick, fast global political movements that can take on action and solve real world problems.
09:03:27.620 - 09:03:59.550, Speaker A: But the foundation is that cryptographic base of users that get self custody get what they're doing. They know what this thing means. So, yeah, the simple things I'm on Vitalik's Camp know we are in France, and DeFi is the French word for challenge. Okay, fine. It's pronounced daisy, but, like, come on. More generally. Also interested in experimentation around Dows and kind of like, form ways of organizing people on chain.
09:03:59.550 - 09:04:27.346, Speaker A: There's a lot of different kind of proposals. DeFi, refi, disco, DAOs. I'm kind of, like, excited to see how that goes as well. My last question, which is a false trichotomy, but you have to pick an answer anyway, which is Barbie Oppenheimer, or you think it's really bad that we're lionizing Oppenheimer via a movie? Actually, it's Bob the Builder. Sorry. I forgot. Bob the builder.
09:04:27.346 - 09:05:17.714, Speaker A: I forgot Bob the Builder. I want to see the crypto space come up with its own epic. We've had songs, right? Remember there were Bitcoin songs back then. There was that Love You like a Bitcoin parody that continues to be one of my favorite songs to this day. It's actually good as a song despite being a parody, I think. Didn't Simon de la Revere write a novel at some point that yeah, I would love to see just us come up with a movie that uses themes from these that we care about, but is actually good as a movie at the same time. I unfortunately saw that there was an online TV show about SBF.
09:05:17.714 - 09:05:42.170, Speaker A: So maybe that's not the right type of movie, but that is coming out. It's an animated short. I see. It's an Oppenheimer for a different exactly. The price is definitely Anatoli. I mean, you look like you're ready to be in the Barbie movie, so you got to have some opinions here. This is me, Cosplaying Feynman.
09:05:42.170 - 09:06:01.220, Speaker A: So definitely oppenheimer. All right, well, thanks everyone for joining us. It's been a great day of talks. We talked a lot about the Public Broadcasting System of the US. I mean, sorry, proposer Builder separation and hope you have had a good week in Paris and enjoy.
09:06:14.320 - 09:07:34.340, Speaker B: Thanks everyone for staying for the entire morning, plus an entire day that started at 02:00 p.m.. Just a few closing remarks. There are supposed to be slides and if they don't appear, I will still say the same things. So basically the reason why we had been on this PBS day journey is to seek out an answer that I wanted to know. Me as a community member, take off my flashbot's hat, what can I do to contribute to the PBS research that is critical to fight against essentially the centralizing force of MEB? And so this is something that I wanted to have as a call to action, which is to build as a community an open RND map for the ecosystem. On the topic, on the problem space of PBS, of course, I stole a few slides from every speaker today. This is one version of the end game thanks to Vitalik.
09:07:34.340 - 09:08:56.880, Speaker B: And whether you do believe in the end game of approved singularity or not, the question really is where are we now? And of course, understanding that there are a lot of ecosystems, different participants and contributors sitting here. So this may be just one of the many examples or one of the many roadmaps that you are familiar with. The one on the right was, I believe at the end of last year, vitalik posted the Ethereum roadmap that is quite modular. And so where was PBS back then? It was right there with a couple components. It looks like we're 25% there. And today we got a zoomed in version of a heart shaped roadmap from Mike Neuter. And there is still a lot of uncertainties to be figured out in terms of actually how do we actually get rid of certain intermediaries that is currently not quite accounted for in the protocol as well as many other technical challenges.
09:08:56.880 - 09:10:28.450, Speaker B: So I started to ask myself, okay, as a non EF, non core researcher, non core developer, what can I do to actually understand what is the lay of the land and what can we do individually or as part of a project? So I went on a rabbit hole of mind mapping. So of course, we have already on Ethereum Mebboost ecosystem that's in operations that flashbacks many of my teammates are working on, along with many of the EF and. The community members contributing to it and it's up and running. There's also a healthy ecosystem that's beyond just the medboost ecosystem that is doing a lot more of the R and D work. So after I actually interviewed around ten to 20 researchers who are actually working in a space, working on PBS, I realized that I really can't recognize much on this mind map anymore because it has expanded so huge. And I realized there's a lot more problems than I realized there was. And ultimately these questions are very hard auction design problems, very hard consensus design problems, and there may not be a right answer.
09:10:28.450 - 09:11:57.630, Speaker B: And when I look through from the farther away research problems, zooming into the development and finally the operational problems, I realize that the reason why I have a lot less branches there is because there's a lot more to be done by the community. But yet we kind of just relied on, hey, the EF is going to take care of it not my problem. But that shouldn't be the case. There is so much we can do just in terms of figuring out in today's up and running out of protocol, PBS metboost ecosystem, how do we keep relays economically sustainable? That's a research question. How do we actually have data transparency such that we could enable us to keep the ecosystem in check? And how do we actually contribute to the prototyping, the economic audits, the security audits of all of the early research work that is going to production before they turn into an EIP? Those are just open ended questions unaccounted for. So it's a call to action. And this doesn't just applies to L One.
09:11:57.630 - 09:12:31.190, Speaker B: Today we heard, I think for many of you here, probably for the first time, for me, probably for the second time, because I interviewed every single speaker, what would PBS on L2 look like? And this may be just a starting point. So I'll end right here with essentially the theme of today let's turn zero sum game into positive sum. Let's keep crypto decentralized. Thank you.
09:12:39.290 - 09:12:52.930, Speaker A: I'd also like to ask if we can all give Tina one more round of applause for actually curating the whole day as well. Thank you everybody. Happy hour begins.
