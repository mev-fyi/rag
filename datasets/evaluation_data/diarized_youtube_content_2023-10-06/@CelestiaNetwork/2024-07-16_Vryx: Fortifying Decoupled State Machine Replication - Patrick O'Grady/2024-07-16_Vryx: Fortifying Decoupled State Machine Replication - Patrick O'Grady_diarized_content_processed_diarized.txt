00:00:02.120 - 00:00:31.890, Speaker A: Awesome. All right, well, thanks for coming to my talk on Vrix in parentheses in practice. So today I'm going to be talking about fortifying decoupled state machine replication. I'm Patrick O'Grady. I'm the VP of engineering at Avalabs. I've been there for about three and a half years, focused on all things related to really the consensus stack. So that's networking p two, p stuff, consensus itself, and then really how to best make use of consensus to drive execution and scalability.
00:00:31.890 - 00:01:01.802, Speaker A: So why am I here and what am I talking about? So a few weeks ago, people saw this talk from Josh. Great talk about we are building the same thing. And I also saw this talk. It was awesome. I think he did a great job of trying to draw similarities between different projects. Similarly, I'd been talking to Nick from Celestia for some time about some of the stuff we were doing. He's like, hey, you should come by, talk a little about what you're doing, which we're working on similar problems, but in a very different lens for avalanche.
00:01:01.802 - 00:01:54.846, Speaker A: And that maybe tell it from the perspective of a more modular setup and maybe for this audience who might find it interesting. So at avalanche, we've always been really interested in a bunch of different chains that are connected. And so this is my attempt at a meme related to that. But the idea is that a lot of the stuff we're doing is very similar to a new l, one where there's a bias towards just a little bit bigger bias towards sovereignty and native interop. So instead of stacking everything on top, we believe more in the horizontal approach where there's just loose connectivity between the different components. And before I get started, forgive me, this was designed, I think an integrated, might be a dirty word here, but designed for an integrated blockchain. So where this all started, I don't expect you to read this, but a lot of people in the space to scale have been talking about agreeing on execution inputs instead of execution results.
00:01:54.846 - 00:03:02.912, Speaker A: So if we just agree on what's actually in the transactions and in the blocks, then if we just put those into consensus and execute later that then we can actually get the scalability and performance that we're actually looking for in building out these chains. Well, while that may sound straightforward and simple to say, in practice it can be a lot more difficult because you need to be very careful about what you actually allow to be in these blocks or what these inputs are, because they may not always be valid. So to understand a little bit more about what we're talking about. Let's take a big step back. So anyone that read John's, John Sharp's post from a few weeks ago, I saw a bunch of these diagrams I made in basically kind of setting the table around what a lot of the state machine replication work that a lot of teams are doing looks like. So to first, it starts off with this traditional world of state machine replication, which if you use bitcoin, ethereum, or most other chains, you're probably very familiar with, some validator, some block producer produces some block, they then replicate it to the rest of the network. Other people, when they get that block, verify it.
00:03:02.912 - 00:03:48.592, Speaker A: And then, let's say in this case, there's two validators. The other validator will then build on that and only include transactions that are semantically valid. So if I include a transaction, sorry, Bob pays Alice, but Bob doesn't have enough funds, we would just consider that block invalid. This is what that typically looks like and, you know, over some sort of timescale here, the issue is we have to actually verify the state and keep it up to date before we build. So people over the, in recent years have started to optimize this a bit. So again, kind of hard to see, but with the same diagram, is that people will build a little bit, then start replicating, build a little bit while they're replicating and keep going. And what we would call this is streaming state machine replication.
00:03:48.592 - 00:04:45.298, Speaker A: So people that are familiar with Solana and how Solana works follow a very similar approach to this. Well, anyone that's been following the latest consensus research has seen this phrase, decoupled state machine replication. And folks that know the movie Zoolander should appreciate that meme. But a lot of work has gone into trying to instead break this all apart. Now, why would you want to do that? Well, if you do that, you don't actually keep the state up to date while you begin to replicate its contents. Now, that may sound a little pedantic, but what that really means is you may include blocks or transactions in blocks that don't actually have funds or can't even be processed correctly. Now, while you might do that is that you can reorder these processes of building, replicating, and verifying in a way that may allow you to achieve much higher throughput with the same hardware.
00:04:45.298 - 00:05:50.776, Speaker A: But it's a little tricky. It's exciting because it allows you to achieve throughput you might not otherwise get, which is why we're seeing a lot of chains like Aptos and Sui, like what people call the latest generation of chain really start to employ these techniques. But what it comes down to is this trade off. We're not performing semantic verification prior to replication. So semantic verification here being, hey, does this transaction actually have enough funds to pay fees in this network? Now, on a standard chain, something like a bitcoin or ethereum model, this is easy to reason about. But the same thing happens actually quite a bit in the modular ecosystem, where you may have things on different layers, you may have some sort of synchrony delay between when you can actually check the balances or the validity of different parts of the stack and how things are actually included. And so ultimately your consequence, if we break transactions per second into different components, is that you have fee paying transactions per second which are less than really what your replicated transactions per second could be.
00:05:50.776 - 00:07:02.282, Speaker A: And then the negative externality of this is that you have some amount of invalid transactions per second included in different blocks. Now, if we're an adversarial user, can we cost effectively increase invalid transactions per second? Aka, will the network allow us to just stuff it full of transactions that nobody can execute? So in the Virix writeup, which you can check out at Virix XYZ, which is in the corner, there always, if there's domain available, I'm always there to try and buy it. We model this with basically a naive implementation of decoupled state machine replication. So you can imagine a standard chain, maybe there's in this case 100 validators. I think the specifics of this really aren't important, really what to focus on is that you as a user are going to send transactions to any validator. Your transactions have a nonce, and then validators of their own volition will try to include whatever transactions they think are valid. So just like a normal blockchain, you send it to some validator, they're like, great, I can collect some fees on this, I'll sequence it, replicate it, and then hopefully be able to execute it.
00:07:02.282 - 00:07:44.606, Speaker A: Well, there's a number of attacks that can really undermine this approach. The first, we'll start very trivially, let's say that I just duplicate send the same transaction to everybody at once. So I have txid one, I send it to every validator in the network, Alt 100, and I have a bunch of different accounts that do this exact same thing. Well, all of a sudden, every validator, really, if I fill every validator's queue with the same exact transaction, duplicates many fewer transactions. Get in. Now, this is an attack that's been understood, and someone thinking about this is probably the first thing that they would think about. And so there's a paper, I think I always try to give credit to whoever the first person to point this out was, and I think this was the mere BFT paper.
00:07:44.606 - 00:08:37.987, Speaker A: And they suggest that you might want to partition the transactions by validators. So in this case, let's just say that that problem solved. Well, unfortunately there's a few more issues you might want to do, or a few more approaches you might want to take. So the next one is, if we assume every transaction has some sort of nonce, what happens if you send different transaction ids, maybe with different call data to different validators? Well, if you do that, two validators may think that they have unique transactions because they're partitioned or like the txid to that validator, however, nonce is the same. So anyone that's familiar with the typical account model knows that only one of those will actually get executed. So we can perform the exact same attack we just tried to prevent using this approach. Another common approach, or simple approach an attacker might take would be fund exhaustion.
00:08:37.987 - 00:09:10.792, Speaker A: So what if I send ten transactions from my account and I only have the money to pay for one? Well, you're now going to sequence maybe all ten of them, but only the first one is going to actually deduct fees. So this one can be pretty bad. And I don't, unfortunately don't have the time to go through the math, but I have a longer video online that you can watch where I do. Sorry. This is what happens when you try to cram a hour long presentation into 20 minutes. Bear with me. So things get really bad when you combine the attacks.
00:09:10.792 - 00:10:12.530, Speaker A: So if you take conflicting transactions, one with nonsense that conflict and fund exhaustion, you can really take a naive TSMR implementation to almost no transactions per second. I think I computed it in this model to be like a 99.999% reduction in throughput. So what it really you have to think about is that this is not only the only problem. So anyone that knows, you know, most people that use blockchains want to access historical data or sync the chain from scratch. Well, if you hash the data that's included and you want to now fetch everything to re execute it, you unfortunately are indefinitely persisting all the invalid transactions over time. So if you want to sync it, you would not only at the tip are you only processing like 1% or using only 1% of your block space, but every time anyone syncs it, they're all still wasting 99% of their bandwidth syncing transactions that never actually executed.
00:10:12.530 - 00:10:38.744, Speaker A: Similarly, you might say, well, you know, we really like the idea of having dynamic fees when there's less activity. The fee should go down. Well, you might think, okay, well, I'm only going to charge fees for valid transactions. That makes sense. That's what everyone does. Well, you just incentivize an attack. Because if I'm an attacker, like, well, transaction prices are really high, I really want to get my transaction for less.
00:10:38.744 - 00:11:34.470, Speaker A: Well, if I stuff the chain with a bunch of invalid transactions, the amount of actual execution that's happening is going to fall off a cliff, which means that the fee should go down, which now all of a sudden I've made my life cheaper. So you can even actually accidentally incentivize ATT and CK, which is a whole fun journey to go down. And so when you're thinking about TSMR, my only advice is to think a little bit more before you just deploy it, because there are quite a number of nuances to think about that someone could trivially exploit without having any stake. So what can we do about it? Not a lot. We don't have the state, so we can't really do anything that's state related. We could try to implement some sort of synchrony requirement or something like that. However, that can cause a lot of stability issues, especially if the network is degraded.
00:11:34.470 - 00:12:33.510, Speaker A: So maybe we can rely on heuristics. And so what if we give validators the tools by changing some rules in execution to enable profit maximizing builders to at least minimize the number of invalid transactions per second that they actually include. Now, this may sound like a lot. I'll explain what I mean here. So Vriks is all about exploring what you could actually introduce to systemically prevent the attacks that I mentioned previously. So, four ideas I'll talk about this in future work, is that we'd love to collaborate with the ecosystem on finding a more robust or formal description for what a minimal set of protections for DSMR might be. But four that we can explore here are account fraud proofs, temporal account partitioning, nonceless transactions, and post execution transaction filtering.
00:12:33.510 - 00:13:24.860, Speaker A: The fun part of this is it's actually consensus agnostic. So if you, you know, you've heard of DSMR, you can just couple it with some existing DSMR people, usually like Narwhal and such. So first one we talked about is an attack. This is a, you know, more or less a derivative of the mere BFT work. But if you point the transactions to some set of particular validators, you can avoid this transaction duplicate issue and you can actually sub partition an account to multiple validators at once based on the TxID space, so that you can't be censored just by a single validator. So this is one approach that's interesting. I think my favorite at least is the account fraud proof approach, which allows you to have fully decoupled inclusion from execution without actually hurting the validator.
00:13:24.860 - 00:14:35.744, Speaker A: So in this world, any account that wants to actually have transactions included in the chain would lock up some amount of funds greater than or equal to the fees that they may want to spend on some number of outstanding transactions at one time. So let's say that the fee rate on some chain is 0.1. I'm going to lock up one unit that would allow me to have ten transactions in flight at any particular time. Now if I exceed that, obviously the builder can't know that I'm doing that, but because I'm uniquely partitioned to some number of builders, and this fraud proof is just divided between them, any builder knows that if I only include transactions divided by partitions at once, then I can claim my share of the fraud proof of whatever is actually outstanding. So you can give me absolute garbage where you could have no money left, and I as a validator will be no worse off. From a fee collection perspective, which is the most important part here, I can ensure that what I sequence will be valid, but I can at least ensure that I'm paid for what I actually include in a block. Nonsys transactions are something that is not very popular in the EVM ecosystem yet, but I think that they will be.
00:14:35.744 - 00:15:23.050, Speaker A: It's actually not that crazy. And the idea is you just keep some window of TX ids and just make sure that there's no duplicate TXId included over that window. The nice benefit of this is you also get transaction expree out of the box, which for many applications is super useful. And so this prevents the attack of having duplicate transactions that have different TX ids. And then lastly you can trivially filter the blocks and still keep things. So if you just require filtered chunks to be included in the chain after some diff, that will allow you to actually sync the chain at a later point in time without actually synchronizing the invalid transactions. And so if you include those in consensus you can be very sure that they're safe to sync to.
00:15:23.050 - 00:16:14.880, Speaker A: And so you might build a chain that has blocks on each block, links to some number of certificates, and then is eventually executed, and then the roots of that execution are eventually included in the chain. So how does it do two minutes left to explain. So with a lot of this stuff, until you build it, you don't really know how good it is. And I think my primary goal after writing up a lot of the virix ideas was to actually try it. So my target was, everyone always talks about 100,000 tps. Can you hit it at least with micropayments in a distributed cluster? So we did it. We set up 50 equal, eight validators across five regions, one API node per region, one transaction issuer, transactions issued randomly to API nodes, and most importantly, a ton of accounts, 10 million active accounts, 2.5
00:16:14.880 - 00:17:05.308, Speaker A: million unique per 60 seconds, and then all transactions are just transfers. So we're talking micropayments here, we're not talking smart contract interactions. So you can see big line at 100,000, nothing crazy, but you can keep it stable with minor variations in networking from our favorite cloud provider, AWS in this case. But generally speaking, what's important is that the network latency hovered around what we expected. So over like a sustained period of time, generally things stay pretty stable. And as a result, when you distribute transactions prior to execution, we're able to get attestations of their distribution within a couple hundred milliseconds. And generally things remain pretty stable.
00:17:05.308 - 00:17:46.234, Speaker A: I'll skip this, just stability. But the important part was we saw no expiry or failure for any transactions, chunks or blocks. It took on average about 230 milliseconds for a chunk. So this decoupled state machine replication to be fully tested by stake in the network. It took 700 milliseconds for the full chain from a person to send a transaction to an API to then get it tested. So this is what you would show the user, and then the finality part is more a function of the consensus, so we won't talk about that. But we achieved pretty good throughput without really stressing the workload.
00:17:46.234 - 00:18:29.202, Speaker A: Much of it was compressible because you see a power law typically of transaction inclusion. And then most importantly, Virik's focuses on symmetric bandwidth usage between the validators. So there's no like core node that is in charge of sequencing something or is like some bottleneck for bandwidth. And so like I mentioned, future work here. Let's make DSMR safer for everybody, because I think it's going to become a very popular topic over the next few years. And so it'd be great to work on formalizing the framework for minimizing what we would call invalid transactions per second. That's it.
00:18:29.202 - 00:18:29.570, Speaker A: Thank you.
