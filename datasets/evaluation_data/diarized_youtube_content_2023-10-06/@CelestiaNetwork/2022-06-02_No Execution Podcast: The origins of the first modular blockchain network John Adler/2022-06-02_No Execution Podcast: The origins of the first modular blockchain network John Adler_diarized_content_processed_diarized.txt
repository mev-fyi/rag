00:00:08.250 - 00:00:16.286, Speaker A: So how did you get into crypto? And, like, I'd love to kind of hear your story of what, what. How did you. Ended up. Ended up. End up here.
00:00:16.388 - 00:00:58.990, Speaker B: Yeah, I feel like that's a very standard intro question in the crypto space, which is interesting because you don't really hear it in other podcasts, at least from my limited experience. It's probably because there's so much diversity in terms of background in the crypto space. But yeah, my origin story is kind of not that interesting. But essentially I was in grad school. I think this was around 2016, maybe. Yeah, maybe early 2016. And my grad school advisor was very keen on cryptocurrencies, and especially on this thing called Ethereum, because he was involved in the very early stages of Ethereum.
00:00:58.990 - 00:01:33.580, Speaker B: And in fact, he's been following cryptocurrencies for a while. One of the pictures that he likes to show to all his grad students is him next to the Mount Gox guy in Japan, you know, the day Mount Gox happened. And then there's a guy with a sign back, give us back our bitcoins and stuff in Japan. He was there when that happened, and he took a selfie with the guy. So he likes showing this picture to his students. So he's been involved for a while. He talked to his students about this thing called Ethereum, and me being the youngling that I was, I didn't really know anything about blockchain at the time.
00:01:33.580 - 00:02:19.910, Speaker B: And this was when Ethereum was like one dollars or something along those lines. And then after I looked into it, I got interested. It all seemed very interesting. And the reason I think I got hooked on cryptocurrencies and this kind of permissionless consensus blockchain stuff like that, is I saw there was a lot of overlap between the things people are doing and virtual machines and compilers in that area. And it turns out that a lot of the ways that you do things that scale blockchains and whatnot revolves around building better virtual machines. And it has very little to do with the things people were talking about at the time, things like better consensus protocols and stuff like that. None of these things really make a huge difference.
00:02:19.910 - 00:02:53.860, Speaker B: It's really about, can you make a nice virtual machine? Do you have to go with a virtual machine, a nice language that compiles down to it and stuff? And I happened to be very interested in the domain of compilers, interpreters, virtual machines. So I saw there was like a large overlap and interest over there. And that's kind of where I kind of got my initial in into blockchains. From there, I joined consensus to do L2 scalability research. And then about a year ago, I left to start working on lazy ledger full time.
00:02:54.310 - 00:03:00.694, Speaker A: Man, that's an awesome story. Who was this advisor? Or can you not share the name?
00:03:00.812 - 00:03:12.990, Speaker B: I mean, I'm pretty sure this is public, because I went to a public. My advice, the fact that I have a thesis out there is public, and I'm pretty sure it's publicly searchable, but my advisor's name is Andreas Manares.
00:03:13.090 - 00:03:59.770, Speaker A: Oh, cool. And that's awesome that you got into this from the virtual machine side, because that was one of the first things that I thought was totally wrong with the stacks that people are building. Both the Ethereum side and the hyperledger side were like, made me barf. But I was like, no, this is not how. Nobody at Qualcomm would build you this thing, or anyone that's like, an HFT trader or anyone that really had to deal with milliseconds, let alone microseconds, where those things matter. So, yeah, I thought that was kind of interesting entry point. So lazy ledger.
00:03:59.770 - 00:04:25.858, Speaker A: I love it because it's one of those places where I feel like there's a pareto efficient curve, and you guys picked an extreme point. And to me, that's always an opportunity to see what breaks in the world. When you do that, can you kind of tell me, I guess, from your, what is your vision for it and how do you think about it?
00:04:25.944 - 00:04:38.470, Speaker B: Sure. I mean, you're not wrong that it's kind of taking an extreme point. Would you rather I start with a description of what laser ledger does rather than its vision? Or do you want me to start with the vision?
00:04:38.890 - 00:04:40.966, Speaker A: Let's start with what it does.
00:04:41.148 - 00:05:39.500, Speaker B: Okay, so what it does. So one thing that's interesting to note in the original bitcoin design paper written by Satoshi Nakamoto is one of the earlier sections, describes a timestamping server. And if we had a way of timestamping transactions, transactions in the database, not necessarily monetary transactions, then we would have a way of ordering potentially conflicting transactions. And if we have a way of ordering them, then you can just ignore all the conflicting transactions that come later. But to do this, we need some sort of time stamping server. And unfortunately, in a completely permissionless, trustless setting, you don't have a notion of time. There's no time, which I think also there's some overlap here with regards to time and stuff, with Solana's approach to using the proof of the time is the big thing.
00:05:39.500 - 00:06:16.950, Speaker B: How do you timestamp these transactions? And what he devised is this blockchain thing. But one of the implementation details of the bitcoin as it was deployed as opposed to bitcoin. The idea described in the design paper, which the bitcoin that is described in the design paper and the bitcoin that is currently deployed are very different because the one in the paper is a very high level approach. Right. It describes some high level ideas and it completely doesn't go into implementation details. While the bitcoin that is currently out there, obviously it's implemented. So it has implementation details.
00:06:16.950 - 00:07:06.214, Speaker B: And one of the implementation details is that invalid transactions are not in the chain. In the bitcoin chain, you cannot have two transactions that spend the same Utxo in the Ethereum chain, you cannot have two transactions that are from the same account with the same nons. And similarly for other chains that use either the Utxo or data model or potentially something else. Right? And this is kind of it's implementation detail that was carried over. It's an artifact, but you don't actually need this, as Satoshi describes in the design paper, as long as you can. And that means if you can timestamp them, you have an order. If you have an order, then you can simply ignore the conflicting transactions that come later, right?
00:07:06.412 - 00:07:07.160, Speaker A: Yes.
00:07:07.610 - 00:07:56.706, Speaker B: So this is kind of the core idea of lazy ledger is, well, let's not execute transactions, let's just order them. So the core idea is rather than having the blockchain be an execution layer, it is only an ordering layer. So currently, what blockchains currently provide with the contemporary blockchains like bitcoin and ethereum and so on, they order data, they make it available, and then they also execute the transactions. Lazy Ledger only orders data and makes it available. It doesn't execute transactions. And as it turns out, as I said before, as long as the data is available and ordered and you have the consensus on that and some timestamping, then you don't actually need to execute transactions at the consensus layer. You can do this at the application layer.
00:07:56.706 - 00:07:57.238, Speaker B: Yes.
00:07:57.324 - 00:08:18.778, Speaker A: And you can kind of base this on the assumption that computers, unless there's some weird alpha particle that hits this thing that run the same code, will come to the same conclusion. And that's all you really need. Right. Is the input. Right. Because side effect of the state, everybody will generate the exact same thing.
00:08:18.864 - 00:08:27.390, Speaker B: That's exactly correct. And if a transaction is invalid, the virtual machine will just say, this transaction is invalid and make it a no op, essentially.
00:08:27.730 - 00:09:21.360, Speaker A: Yeah. This is like, I think, a wonderful idea because I think it really challenges a lot of assumptions that people want to make about. I think code is law and all these other things that are on top of it. What I think is really neat about it is that you kind of end up with a situation where you can maintain just this bandwidth pipe where it ingresses as much stuff as it wants, and anybody can really catch up and read it and it's available. You can kind of actually easily much easier shard computation that way. But all the folks working on sharding should be building lazy ledger and then some transaction isolation to follow whatever shard they want.
00:09:22.450 - 00:09:48.066, Speaker B: Yeah, I call lazy ledger a universal blockchain because you can implement any execution system on top of it. Unlike, for example, ethereum. Right, and ethereum due to gas limits, despite the fact that it's in theory Turing complete without a gas limit. With a gas limit, it's very much not turing complete. Right. You can't do anything you would want to do. And it's especially true for cryptographic operations, even like simple verifying.
00:09:48.066 - 00:10:13.234, Speaker B: Some signature that isn't pre compiled might take you a few million gas, which is just like. It's impossible to actually do anything with that. With laser Ledger, you don't have this constraint because you don't have execution at the base layer. You don't have a concept of gas. It's just pure data. So you can actually have a bunch of data availability, the throughput of way more than you could do in a sharded blockchain, and then just build a sharded execution layer on top of that.
00:10:13.432 - 00:10:15.250, Speaker A: So what do you lose?
00:10:16.390 - 00:10:18.900, Speaker B: What do you lose? I want to say nothing.
00:10:20.710 - 00:10:25.010, Speaker A: I tend to agree with you, but what do people say that you lose?
00:10:25.610 - 00:11:28.930, Speaker B: Well, kind of the laser ledger system, to contrast it to systems like sharding. I guess I should kind of go over a bit more detail into what we do specifically as opposed to just ordering data, because that's the core idea. But there's some other techniques that make it even better than just that. Right? There's two components of lazy ledger that distinguish it from the first facet distinguishes it from regular blockchains. The second one distinguishes it from sharded blockchains as they currently are designed. So the first one is data availability checks. There's some math behind it, but essentially what you can do is you use erasure coding and plus random sampling so that a light client can convince itself with very, very high security guarantees, like just an arbitrary number of nines that the data of the block is available without having to download the full block.
00:11:28.930 - 00:12:12.418, Speaker B: And in normal blockchains, a full node will fully download a block, and a light node will just assume the data is available. But that's not very secure because a majority of hash rate or stakers or whatever could create a block and not show the data behind it, which is very bad. You don't want that to happen. And light nodes would get tricked without these data availability checks. So lazy ledger adds these data availability checks. Now, sharded blockchains also need some form of data availability check because sharded blockchains always have committees of some sort, right? Like, say you have serenity, commonly called e two. In serenity, you have your set of global validators.
00:12:12.418 - 00:12:52.526, Speaker B: This is whatever, 100,000, let's say. And then they get split up into tiny committees of size 100 or something. And then each committee votes and creates blocks on a single shard, and then they get rotated. But it means if you could corrupt the committee just one time on one shard, you could produce a block and not show the transactions in that block, and then no one would ever be able to produce a new block for that shard, and you wouldn't really be able to penalize anyone because the data is not there. Of course. Now, one thing you could do is you could say, hey, every single node in the network, every single validator in serenity, go download that shard block. But then now there's not sharding anymore.
00:12:52.526 - 00:13:12.250, Speaker B: Everyone's just downloading everything. So all sharded systems need to have these sublinear data availability checks. And lazy ledger does this at the base layer. And these kind of distinguish charted blockchains and lazyledger from current contemporary blockchains is that it allows for secure light clients.
00:13:13.150 - 00:13:22.634, Speaker A: That'S actually not too far off from, like, performance forced us to go down a similar path for data propagation.
00:13:22.762 - 00:13:56.210, Speaker B: Yeah, erasure coding is really good because you can do, it's very resilient and very robust as opposed to full replication, which is just terrible, but temporary. Blockchains do full replication as opposed to error correcting. Yeah. Now, proponents of sharding will say, well, in lazy ledger, you take your big block and you razor code the whole big block. And in sharding, you kind of razor code each individual block. Right. And razor coding an individual block is less computationally intensive.
00:13:56.210 - 00:14:50.338, Speaker B: Right. That's kind of what they would say. But this isn't really accurate because if the nodes don't check, if the erasure coding is correct, then they have to rely onto fraud proofs or potentially validity proofs in the distant future that the erasure coding was done correctly. So if you rely on those fraud proofs, then with the difference between if you're one validator in serenity, then you're basically saying, okay, the other 63 shards, I trust that the erasure coding was done correctly. I wait for fraud proof and I do my work on this one shard versus lazy ledger, where you have the equivalent of, say, 64 shards, and that you would either trust that they're correct or that you would validate it yourself. So there's not really any difference here, right. If you trust that 63 shards are correct versus 64, there's not really any difference.
00:14:50.338 - 00:15:26.182, Speaker B: Because as soon as you have cross shard transactions, you trusting your one shard is correct is meaningless. Right. As soon as you have crosshard transactions, it means that your shard is not isolated. It actually takes in input that could be malicious. There's also the fact that in general, doing the 2d erasure coding scheme is highly parallelizable. Because each, to kind of give you the one sentence overview is each row and each column of a square is erasure coded independently. So this is a completely embarrassingly parallelizable to do.
00:15:26.182 - 00:16:07.070, Speaker B: And in modern day computers with many cores, I think the Ryzen 5000 series just came out. How many cores do they have? I can't even count them on my digits. There's enough cores there to do these things in massive amounts of parallel work here. So there's kind of concerns about if it's a single monolithic chain, you won't be able to produce a block. But that's just not true at all. You will be able to produce a block. And the reason is that if you're a very small block producer, you can produce a small block.
00:16:07.070 - 00:16:10.020, Speaker B: You don't have to produce a big block. Right.
00:16:11.430 - 00:16:29.260, Speaker A: How do you guys deal with, in terms of actually coming to agreement that this data is shared for the whole chain and stake and state and actually deciding which fork of the chain to go with?
00:16:30.030 - 00:16:36.090, Speaker B: That is a very good question. Could you clarify what you mean?
00:16:36.240 - 00:16:46.350, Speaker A: So without general blockchain, that does, I guess, Nakamoto consensus. Right. I have some electricity that decides which blocks are which.
00:16:46.420 - 00:16:50.250, Speaker B: Oh, I see. Okay. What the consensus protocol is. So the consensus protocol.
00:16:50.330 - 00:16:54.074, Speaker A: Yeah. How do you define what is a supermajority?
00:16:54.202 - 00:17:07.190, Speaker B: Okay, so there's kind of separate things here. The three separate things. The first is the execution system. Right. The second is the consensus protocol. And the third thing is the civil resistance mechanism. So for execution system.
00:17:07.190 - 00:17:57.038, Speaker B: As I said, we do no execution, which, as you said, is an extreme, and it's, in fact, optimal. Right? You cannot do less execution than no execution. And as the kind of moderately known but not too famous Barbara Liskov would say, the bottleneck for these consensus protocols in practice is the execution system and not the actual consensus protocol, which is something that I've tried to bang into the heads of several academics, but they don't really want to listen. And I think we know who we're talking about. Know, the first component is the execution system. Lazy ledger does no execution well, star. In theory, it does no execution in practice, as we'll get to the other two facets, maybe it does a small amount of execution that's very tiny.
00:17:57.038 - 00:18:35.154, Speaker B: So the second component is, what is the consensus protocol? And then the third one is the Sybil resistance. So we would have liked to use proof of work, because proof of work, it's completely permissionless within the protocol. You don't need anything like stake or anything like that. You just join with hash rate. You produce what is essentially a signature over itself just by mining and finding a nonce, and then you just do your thing. But proof of work has many downsides. It has variable lock times, it has high variance in block times.
00:18:35.154 - 00:18:53.554, Speaker B: And, of course, there's electricity. We would prefer not to spend a bunch of electricity because it's bad for the environment. So the current lazy ledger design is using proof of stake. Specifically, we're using a tendermint based consensus protocol with proof of stake for a simple resistance mechanism.
00:18:53.682 - 00:18:57.410, Speaker A: But how do you know what is the stake without any execution?
00:18:57.570 - 00:19:10.858, Speaker B: Well, that's why I put a star, is that in theory, you would have no execution. If you say proof of work, in practice, you need a very tiny amount of execution. And the only thing this execution layer does is manage the validator set.
00:19:10.944 - 00:19:19.786, Speaker A: But this is like, to me, the bottleneck in censorship resistance is actually like, how do you scale this to 10,000 validators?
00:19:19.898 - 00:19:23.114, Speaker B: Oh, well, we're using tendermint, which scales to 100 validators.
00:19:23.242 - 00:19:26.958, Speaker A: Right, but is 100 enough for censorship resistance?
00:19:27.134 - 00:19:35.490, Speaker B: I would say so, yes. Well, it depends how you define censorship resistance, right? It depends how you define censorship resistance. How would you define it?
00:19:35.560 - 00:20:00.998, Speaker A: It's the number of validators that control the ordering of transactions or the ordering of blocks. So, to me, this is the liveness threshold. The minimum set that adds up to the liveness threshold is kind of the true measure of censorship resistance. How many nodes do I need to bribe for my hedge fund to prioritize my USDT transfers between binance and Coinbase.
00:20:01.174 - 00:20:18.386, Speaker B: Yeah, I mean, there's not really an easy way to do this is that if you have a very large validator set, let's say you have a million individual validators, then you would need to manage those million validators, which necessarily. You need some sort of state there, right?
00:20:18.488 - 00:20:19.140, Speaker A: Yes.
00:20:19.910 - 00:21:06.638, Speaker B: You can't really avoid this. So if you really care that much about the number of validators over anything else, which maybe that's a good metric, maybe it's not. Some might agree, some might disagree, and we can shortly talk about maybe a few other metrics. But if that's what you cared about, then you could either use proof of work where you have unlimited validators, block producers. There's no limit to the number of block producers you can have in proof of work, or there's another consensus protocol that works with proof of stake that supports a large number of validators. I think it's called avalanche, but they don't replicate state. Yeah, but that has its own set of problems that I brought up in various Twitter threads.
00:21:06.638 - 00:21:37.980, Speaker B: So it's an interesting consensus protocol in academia. In practice, it introduces a number of issues. So if you really wanted a huge number of people being able to contribute to block production, you could use proof of work. But this kind of leads nicely into the next point, which is that technically anyone in the world could do proof of work. But if you look at bitcoin, which is supposedly highly decentralized and highly censorship resistant, there's what, three, four mining pools that control 51% of the hash rate, right.
00:21:39.230 - 00:22:26.250, Speaker A: I think in my mind, there's two different kinds of censorship resistance. One is the 4k ethereum classic rollback. That's a beautiful example of La Kamoto style censorship resistance, where eventually, no matter what you do, the heaviest fork wins, and it doesn't matter what you do. Right. If there's somebody in the dark producing a bigger chain, that's the thing that's going to win. But that's not important for inner financial things that are connected to this network, for them to transfer goods and services, right? Like, I have finance and Coinbase, and I want to transfer USDT between them. I don't give a shit about the 4k rollback, right.
00:22:26.250 - 00:22:39.470, Speaker A: That censorship resistance depends on who gets to kind of decide what block gets approved and what block doesn't. And that is the minimum set of the liveness.
00:22:41.090 - 00:23:23.950, Speaker B: Yes and no. So I think there's a disagreement here in what consensus protocols actually provide. Right. And I would say that a consensus protocol, a permissionless consensus protocol on its own, just straight up, doesn't provide censorship resistance. You need something else. And in Nakamoto consensus, this was an incentive, right? Nakamoto built in an incentive that incentivized people to not censor. And the sentence that's important, from my reading, is he ought to find it more profitable to play by the rules that will favor him with more new coins than everyone else combined than to undermine the validity of his own wealth.
00:23:23.950 - 00:23:50.354, Speaker B: So there's an extra incentive here. And with that incentive, if someone tries to do something like a 51% censorship attack and just stop a transaction from being included on chain, a penalty will be applied. That's the undermining the validity of his own wealth. So a consensus protocol on its own, like, let's say, nakamuro consensus. Let's say you completely just take away the incentive. It doesn't provide censorship resistance. It does not guarantee that a transaction will make it into the chain.
00:23:50.354 - 00:23:56.530, Speaker B: It does not guarantee that if you join the mining network, that you will be able to produce a block that's accepted on the canonical chain.
00:23:56.690 - 00:24:02.858, Speaker A: Correct? Yeah, that I agree with you. I think that's where like.
00:24:03.024 - 00:24:17.342, Speaker B: Yeah, but the incentive doesn't really have anything to do with the number of nodes. Right. In fact, it has nothing to do with hash rate or staking validating power at all. It's completely independent. It's a social coordination thing. Right.
00:24:17.396 - 00:24:38.340, Speaker A: But the end result in proof of work is that you have a large kind of concentration of hash power to a few mining pools. Even if those pools, like, there's a difference between those pools corrupting the state or doing double spends or doing something catastrophic to preferring one hedge fund to another.
00:24:39.270 - 00:24:54.794, Speaker B: Well, a censorship attack is not detectable. Objectively correct. You need subjectivity to detect a censorship attack. So a censorship attack, I would say, is a case of favoring one hedge fund over another, essentially. Right?
00:24:54.912 - 00:24:55.434, Speaker A: Yeah.
00:24:55.552 - 00:24:56.666, Speaker B: Would you agree with that?
00:24:56.768 - 00:24:59.802, Speaker A: Yeah. This is, like, an impossible problem, right?
00:24:59.856 - 00:25:10.090, Speaker B: Well, it is. That's the thing, is that it is an impossible problem, which is why the consensus protocol does not provide censorship resistance. Censorship resistance comes from social coordination. It comes from the incentive.
00:25:10.170 - 00:25:18.034, Speaker A: But if your consensus can it handle, like, one approach to solve this problem is you just make it an extremely large step.
00:25:18.152 - 00:25:19.534, Speaker B: Sure. That's one approach.
00:25:19.662 - 00:25:28.254, Speaker A: So I suppose there could be other solutions. Right. But I think Tenderman is pretty limited at that capacity.
00:25:28.382 - 00:26:17.090, Speaker B: Well, so the other solution that I would suggest, which is what we've seen in practice with bitcoin, and it's what we've kind of come to understand with modern proof of stake protocol design, is that these permissionless consensus protocols are really just tools for social coordination. And none of them are objective, even proof of work, because it requires some subjective component to enforce penalties. No one would agree to changing the bitcoin proof of work hashing algorithm and make that new coin bitcoin, unless there was some subjective agreement among people to say, miners are currently attacking the system. We need to break their Asics by changing the hashing algorithm. This requires subjectivity. So proof of work was never objective in the first place, because it requires a subjectivity component for the incentives. And if we have that, then we don't need things like a large number of mining pools or as large number of validators.
00:26:17.090 - 00:26:43.360, Speaker B: What do we need is the cost of a 51% attack must be high, which means that the cost of us going from automated consensus, which is what the consensus protocol provides, to social coordination, which is required for all these permissionless consensus protocols to work that cost, we want to maximize it. And that's the only thing that matters, so long as you maximize that cost. You could even have three people doing the consensus protocol if you really wanted to.
00:26:44.210 - 00:27:06.934, Speaker A: I love that definition, actually, especially when applied to cross chain bridges, because there's a lot of weird attack vectors where some things are automatic and some things are not. But collusion surface itself seems similar, so like an interesting way to phrase it.
00:27:06.972 - 00:27:37.882, Speaker B: Yeah, because let's say banks, traditional banks nowadays, okay, let's say there's five of them. I don't know how many there are in the states, but let's say there's five of them. I think in Canada there's five, or whatever. Let's say they start colluding and they say, okay, you, John, you can't open a bank account in any of these five banks. Well, what can I do? I can't do some. If, you know, a majority of validators in some tendermint consensus protocol decide to start censoring one guy, this one guy goes and sends out his transaction to everyone and says, hey, look, I have a transaction here. It pays enough fees.
00:27:37.882 - 00:28:04.050, Speaker B: It hasn't been included in a month. These guys are censoring me. You can actually slash them and you can fork the entire system. You cannot fork traditional finance. You can fork blockchains, and that's where all the power is. If these people, if, say, the miners try to 51% attack, if validators decide to 67% attack and do some censoring, you can just fork the whole system and make some change with proof of stake. You can burn their stake.
00:28:04.050 - 00:28:20.638, Speaker B: With proof of work, you can change the hashing algorithm, and you can just make them lose money. You can't do that to banks nowadays. And this is where the power is, is it makes the cost of attacks very high. That makes the cost of us having to move from automated consensus to social coordination very high.
00:28:20.804 - 00:29:00.758, Speaker A: Do you think that I'm kind of seeing this play out in US politics right now where the automatic consensus is failing? Is that enough? I'm worried that it's not. I think proof of work has some meat to it, because at the end of the day, there is something about just the electricity backing it and the hash power wars, when picking one side or another being a form of kind of like, much simpler social consensus.
00:29:00.934 - 00:29:31.086, Speaker B: Yeah, I would say that proof of work has a lot of interesting properties. Specifically that to move from automated consensus to social coordination, you can't really fake it in any way. You need to spend the money, while with proof of stake, you can do some trickery. It's not easy trickery. People who say proof of stake is just fiat is like, they don't understand this thing. Like I just said, that you can't fork the fiat system. You can fork proof of stake protocols and burn the attacker stake.
00:29:31.086 - 00:29:55.462, Speaker B: They're fundamentally different. But you can do some trickery. It's very difficult. It requires things like prolonged eclipse attacks and whatnot. But you can do some trickery in proof of stake to get someone to, for a short time, believe something that's false without spending too much direct resources. Well, in proof of work, you have to spend the money. That being said, neither system is really perfect against state actors.
00:29:55.462 - 00:30:15.970, Speaker B: Right. If China wanted to, since the majority of hash rates in China, if they wanted to, 51% attack bitcoin for the next few months or whatever, they could do it, and it wouldn't really cost them that much money since they just take the hash rate. So neither of these systems is inherently perfect against state actors. So in that context, does it really matter?
00:30:16.120 - 00:30:45.850, Speaker A: Yeah, in the context of state actors, I think we're all screwed, I guess. Have you guys thought of dumping the data into arweave or using something similar to Arviv as the consensus mechanism? Because it seems like there's. If you could make that work, it would be kind of a beautiful combination of a proof of work style system and data availability for this base layer.
00:30:46.590 - 00:31:38.534, Speaker B: So it would be interesting. But one thing to note is that systems like rweave or Saya or Filecoin, they might be better with things like data retrievability, but they don't necessarily provide data availability guarantees given the current context. Specifically, by data availability guarantees, I don't mean is this data available, period, because that's fairly easy to do. You just try to download it, right? If you want to know if the piece of data is available, you just download it. But specifically, when I say data availability, I mean data availability by doing sublinear work, traditional blockchains, if you're using bitcoin, I can know the data is available because I run a bitcoin full node. But that just fundamentally doesn't really scale because you can't expect every single person in the world to run a full node. You would expect eventually a lot of people will run light clients.
00:31:38.534 - 00:32:06.120, Speaker B: And if the light clients can get tricked by without blocks, that is very bad. That is a very bad position to be in. And systems that deal with file or just data retrievability or storage, things like scicoin, I guess, rweave to a certain extent. Filecoin don't really provide data availability. In other words, they don't allow a light client doing sublinear work to know or to convince themselves that the data is available or not.
00:32:06.570 - 00:32:09.030, Speaker A: What about like zero knowledge systems?
00:32:10.570 - 00:32:50.660, Speaker B: Zero knowledge systems also don't help with this, because let's assume that you have this blockchain that recursively proves that the contents, like the transactions in every single block and the structure of every single block, all the way from Genesis up to the current tip, it's all valid. So you could essentially have this o of one blockchain that you just verify a single block, and it tells you the entire history. Right? But that's not true. You can't do this, because if you don't know what's inside these blocks, you can't produce a new block. You could get censored. So if the whole network kind of agrees, okay, this is the tip. But no one can produce a new block except a small handful of people.
00:32:50.660 - 00:33:10.700, Speaker B: You can't make new blocks, right? You would have to revert maybe a million blocks later on down the line when you figure this out. So all blockchains need some data availability, at least for full nodes. And ideally you want to have data availability for light nodes too. Does that make sense?
00:33:11.070 - 00:34:07.050, Speaker A: Yeah, that makes. I've tried to struggle with this problem too, because I think the way we've been building Solana is that I don't really care about the history. What I care about is the final state, and we do this BFT style agreement on the computed state and then slash everybody that disagrees. Basically you can kind of build like clients on top of that by if I know the set of validators that I care about and they all agree on the thing that I care about, I can slash anybody that produces invalid attestations of the thing that I'm asking about. But everyone always asks about like it's not a blockchain because we don't care about the history. So I almost started calling it a replicated state machine.
00:34:08.110 - 00:34:14.126, Speaker B: Which is funny. Which is funny given that blockchains are replicated state machines. Exactly.
00:34:14.228 - 00:34:53.100, Speaker A: But just leave me alone. I don't know if the history is important, but I feel like there's an opportunity for use cases that don't care about it to use something that is tailored for that. But I wondered, is it possible to build something where if not for data availability, I can at least have guarantees that this is part of the main state. We could run like an RSA accumulator that records every transaction. Then anything that you pull from history you at least can check, hey, is this thing part of the history?
00:34:53.470 - 00:35:13.982, Speaker B: You could, but I mean, technically a blockchain also allows you to do that. And if you store the hashes of previous blocks in a Merkel accumulator also like a Merkel mountain range, for example, then that also gives you the same property. It gives you the property of being able to provide logarithmically sized proofs off a transaction being included using only a single block. Like at the tip.
00:35:14.046 - 00:35:30.760, Speaker A: Yeah. Is that good enough for people that care about the historical data set? Because what do they actually care about when they want to see the full history? In your mind, what do they care about?
00:35:34.330 - 00:36:18.674, Speaker B: That's actually a very good philosophical question. And this kind of brings up the point, which is what's even the point of doing all this? Why are we doing all this? Right, presumably. Well, here's the thing. Not everyone needs to fully validate a blockchain, right? It's entirely possible to find use case and applications for a blockchain that doesn't have the same philosophy as bitcoin. Just an example of a centralized financial system, right? That works just fine. And they don't even use a blockchain. And you can have systems in between things like let's say permission blockchain that is just there for transparency.
00:36:18.674 - 00:37:02.340, Speaker B: Right? Imagine if all government spends were done through a blockchain. Then you would have some accountability and authentication for holding public servants accountable. And that would be strictly better than the status quo, despite the fact that it wouldn't be full on bitcoin. So there's like a spectrum there, and everything within the spectrum has use cases. But if you wanted to go full bitcoin bitcoiner philosophy of you want to be able to fully validate the blockchain, not that you do do it, but that you are able to do it, then you want to have the historical blocks around so that you can reconstruct the current state and make sure that nothing bad happened in between.
00:37:03.030 - 00:37:29.578, Speaker A: So this is where I feel like the objectivity maximalists, if you want to call them that, barf on anything that throws away all data, or I think in your case doesn't do the computation because they get scared that how do I know the result of this thing if I have to download a big pile of data with unbounded compute that.
00:37:29.584 - 00:38:16.438, Speaker B: I have to execute, actually. So there's a subtle point here which might have been missed, which thank you for bringing that up, or you might be talking about this, but a few of the people in the Ethereum research space have also completely misunderstood lazy ledger, probably because, as it was originally described in the academic paper, it wasn't clear on this specific point. But when you kind of synthesize it with the optimistic roll up construction which I created, you get some nice synergy, which is that lazy ledger is a universal blockchain. As I said. I said earlier, that's what I call it, because you can implement anything on top of it, and you can implement any execution system. And actually one thing you can do is you can implement another blockchain on top of it. So instead of just ordering transactions, you can actually take whole blocks and put them on lazy ledger.
00:38:16.438 - 00:38:40.500, Speaker B: So in that context, a block consists of a block header plus a block body. So if you want to know the state route, know your application on top of lazy ledger, you just download the block header for that application, and the only thing you need is that verifying whether or not someone had the right to produce this block must be exponentially cheaper than verifying the whole block. And as long as you have this property, then there's no problem.
00:38:41.190 - 00:39:00.120, Speaker A: But I still have to, like if I'm running EVM right, or a big pile of execution, somebody still has to go execute all of this. How do I trust the lazy ledger tenderman validators and any applications that use this?
00:39:00.650 - 00:40:02.814, Speaker B: Okay, so we can actually go through a more practical example. Let's say you have a lazy ledger system. Someone creates a ethereum virtual sidechain, as we call them, ethereum application that runs on top of lazy Ledger, this would look like you post just full Ethereum blocks and you just post them to lazy ledger and you post them to a particular namespace Id. This is the second facet that distinguishes lazyledger from sharded blockchains, which I forgot to talk about earlier, but I might as well talk about it briefly. Now we have this construction called the namespace Merkel tree, which is really just like a tree that lets you do range proofs. It's augmentation on things like the Merkel sum tree. And what this allows you to do is if you have multiple applications in a single lazy ledger block, each of them can be assigned to a namespace ID, and then you can provide efficient range proofs to say, here's everything inside this lazy ledger block that is from this application.
00:40:02.814 - 00:40:38.902, Speaker B: And here's some proof that I'm not withholding any of that data. This is the entire set of data for that application on this laser ledger block, right? Yes. So once you have that, now you have to ask yourself, okay, let's say you do it, Ethereum as it currently exists. So you have proof of work. What's stopping someone from spamming a bunch of blocks? The answer is nothing. Just like there's nothing stopping someone from spamming the current ethereum peer to peer network with a bunch of blocks. Right? Someone could create a block with an invalid proof of work.
00:40:38.902 - 00:41:08.354, Speaker B: Like they could just create a block with a tiny amount of difficulty. It clearly shouldn't be there. If they send it to a node, what will the node do? Will it say, well, I guess I'm done, or I guess I have to download the full block and execute the full block? Well, no, they don't. They just download the block header and check to see does this match the proof of work? If it does, then they can process it. If it doesn't, they just ignore it and potentially they ban the node. Right, but you can do that on lazy ledger too. You don't have to give you a full node for this application.
00:41:08.354 - 00:41:14.514, Speaker B: For this ethereum application, you don't have to download the fake ethereum blocks. You just download the block headers and jack their proof of work.
00:41:14.712 - 00:41:20.050, Speaker A: So you're kind of thinking that lazy ledger is going to be just a pure data availability layer.
00:41:20.130 - 00:41:28.706, Speaker B: Lazy ledger itself, yes. Lazy ledger does not provide consensus on application state. It only provides ordering and availability of data blobs.
00:41:28.818 - 00:41:34.870, Speaker A: So kind of like, imagine if I had like a censorship resistant s three bucket.
00:41:35.030 - 00:41:41.082, Speaker B: Yes. With timestamping. Does s three have timestamp? I guess it does if it runs like Linux or whatever, right?
00:41:41.136 - 00:41:51.690, Speaker A: So yeah, sure, it's got versioning, right? And you can see if everybody writes to the same s three bucket, no data is lost. You can actually see the order of whatever Amazon decided.
00:41:51.770 - 00:41:56.210, Speaker B: Sure, it's a trustless s three bucket modulo consensus protocol.
00:41:56.870 - 00:42:02.260, Speaker A: Caveats, yes, but why would I ever need to download the whole thing?
00:42:03.430 - 00:43:25.438, Speaker B: So the interesting thing is you wouldn't need to download the whole thing unless you really wanted to produce a new lazy ledger block or unless you wanted to be guaranteed that every single lazy ledger block was valid. Now here's the interesting thing. We've designed a sort of like segregated system so it segregates transactions that modify the laser ledger validator set from what we call messages, which are these data blobs. So if you wanted to produce a new lazy ledger block, and if you wanted to make sure the lazy ledger coin did not undergo undue inflation, you actually don't have to download the full lazy ledger blocks. You only have to download the very tiny set of transactions from these large blocks and then use those small number of transactions to get the new state. There's not really a case where you would really have to fully download every single lazy ledger block even if you wanted to make sure that the lazy ledger coin was not inflated. Because unlike in bitcoin where the opern is embedded into the transaction or ethereum, where the call data is embedded into the transaction, or bitcoin SV, where the oper, which is huge in their case, is embedded into the transaction, we implement a segregated scheme so that our message data is actually segregated from the transaction, which means you don't have to download them, you can just ignore them.
00:43:25.438 - 00:43:40.290, Speaker B: So if 99.99% of the block is message 0.1, I'm sure I got those number of zeros wrong. Is transaction data then really you're only downloading a tiny, tiny amount of the block to get the transactions.
00:43:43.990 - 00:43:45.540, Speaker A: When are you guys going to be live?
00:43:46.090 - 00:44:03.900, Speaker B: Months, not years? The serious answer, we expect to have a testnet out next year and we would expect maybe main net within two years or so. I'll tell you, it'll be substantially before serenity launches phase one.
00:44:06.270 - 00:44:11.030, Speaker A: Yeah, that's actually fairly decent, I think. Time horizon.
00:44:11.190 - 00:44:35.734, Speaker B: Well, unlike systems like Solana, the nice thing is we don't have to make a huge amount of really intense optimizations into a virtual machine. And we don't have to design a new consensus protocol because tendermint worked just fine given our interpretation of proof of stake protocols. So we don't have to design new consensus protocol. We don't have to design a super optimized virtual machine like you guys had to do. Which means our life is actually on the easy side.
00:44:35.852 - 00:45:27.826, Speaker A: Yeah, I think our life is on the easy side. Because we don't have to deal with sharding. I think there's much more painful paths you can take to build the layer one. But honestly, I don't fault people for trying because it would be cool if it worked. I think if you could actually build a network where you have low availability systems just pop blink in and out of existence, but somehow add anything liveness or security that isn't Nakamoto electricity based proof of work. I think that's a really cool thing. I don't think it's possible outside of what was done with Nakamoto.
00:45:27.826 - 00:45:33.720, Speaker A: But if I'm proven wrong, it's only the world is getting better, right?
00:45:35.290 - 00:46:03.786, Speaker B: I think there was a paper, I think by the trifecta guys. I want to say it's prism. I think they wrote prism and trifecta. But if I remember, it was prism that uses sortition. It uses proof of work plus sortition. So it lets you do some sort of sharding with proof of work without having to worry about the usual. A bunch of hash power goes onto one shard and dominates a shard.
00:46:03.786 - 00:46:06.206, Speaker B: Now you don't have to worry about that because it uses sortition.
00:46:06.398 - 00:46:07.790, Speaker A: No pentagrams.
00:46:07.950 - 00:46:08.514, Speaker B: Sorry.
00:46:08.632 - 00:46:11.282, Speaker A: Does it have any pentagrams in the white paper?
00:46:11.416 - 00:46:46.654, Speaker B: I don't know. This is not an endorsement of prism, but rather I read it and I thought it was interesting and I couldn't find any flaws immediately. So it actually looked fairly interesting. That being said, unless you add things like data availability plus like client sampling and all this other stuff, plus fraud proofs, then just doing sortition doesn't really help you. Having shards doesn't help you. Unless you also do statelessness plus data availability checks with like, client sampling. If you don't do those things, then sharding doesn't actually help you.
00:46:46.654 - 00:46:50.170, Speaker B: Because if you don't do those things, every node has to fully download everything anyways.
00:46:50.250 - 00:47:08.454, Speaker A: Yeah, I think the scary part is that most of these systems are going to be run by the same validators, which are all good actors just trying to keep this thing going. There's about 100 of them. That's it.
00:47:08.492 - 00:47:08.694, Speaker B: Right?
00:47:08.732 - 00:47:14.546, Speaker A: Then the intersection between all the chains is the supermajority in every chain.
00:47:14.738 - 00:47:35.214, Speaker B: Yeah. You're not wrong. And this is one of the reasons that actually initially maybe like two years ago I was very against tendermint because specifically it could only support up to 100 validators. I'm like, oh, this is terrible. But you think about it more and it's like you have maybe 100 people that have the majority of the stake. What's the big deal? And again, concentration does not imply control.
00:47:35.412 - 00:48:05.800, Speaker A: Yeah, I think this is where I think I want to see somebody else try. Pick those crazy points in the Pareto curve and try things that haven't been tried before. And I'm disappointed with how slow sharding research is going and actually getting these systems deployed. And not because it's easy and people are dumb, it's because it's really hard.
00:48:06.490 - 00:48:50.200, Speaker B: Yeah, it is. And I mean, if I can rant for a minute about sharding and how I don't think it's viable or how I think it's a terrible idea in general, at least in the context of blockchains, at least the currently contemporary sharded designs make a lot of assumptions around having a network that does certain things which seem benign if you don't think about them. Or maybe you don't even know about them because the assumptions aren't outlined. But they push a lot of the properties that you would expect from the consensus protocol and they push them onto the network and then it makes it look like, oh, our consensus protocol can support thousands of validators. Look at that. It's amazing. It's a sharded system, it can survive world or three, we're geniuses and all that.
00:48:50.200 - 00:49:49.402, Speaker B: But that's because they push a lot of the complexity, and not just complexity, but like security guarantees and stuff from the consensus protocol onto the network stack. And the network might not actually do everything they need to guarantee this. One example is things like fast rotation of gossip subnets. If you have committee based sharding and then you have a bunch of committees that need to shuffle around, then maybe you're asking people to quickly change their gossip subnets or gossip subscriptions every 6 seconds or something. And it's like, does your gossip network actually do this? If it doesn't do this, then you just assume that the gossip network is a black box that can do this, but it doesn't, right? You've moved complexity and guarantees away from the consensus protocol into the network, but the network doesn't actually provide these guarantees. So your consensus protocol is now also screwed. There's other issues around if you have a global passive adversary, they can essentially snoop in on.
00:49:49.402 - 00:50:33.080, Speaker B: They can snoop in on network traffic and figure out or they can link validator ids to ip addresses. And if you don't have systems like how tendermint has, where you expect validators to maybe be a bit larger, you expect validators to run century nodes and so on. If you expect users to open a port in their home firewall and run off of the raspberry PI, or worse yet, if they run it off of their home computer with all their passwords and stuff on that with no protection, a global passive adversary can link things like validator ids to ip addresses just by snooping a network traffic. Now, you get into the problem that it's very easy to denial of service the system. Right. Stuff like that. Yeah.
00:50:35.050 - 00:50:53.030, Speaker A: I don't disagree with you there. Do you know of anyone trying? I guess there's kind of like a couple of families that are springing up. Like, I think avalanche and dags. Avalanche is probably the dag that's going to survive, maybe.
00:50:53.120 - 00:51:03.310, Speaker B: Well, I mean, there's Iota and there's nano. I think the other two, or Nano is a block lettuce, but Iota is a dag. But, yeah, we don't talk about Iota.
00:51:04.150 - 00:51:11.106, Speaker A: I thought the tangle got untangled and it's now like he uses some kind of coordinator to linearize everything.
00:51:11.208 - 00:51:14.260, Speaker B: Wait, no, it always uses a coordinator to linearize everything.
00:51:15.110 - 00:51:16.580, Speaker A: It always has been.
00:51:16.950 - 00:51:41.950, Speaker B: I mean, I explained this to one of my friends, like, a while ago. But essentially, if you have a daG, and then if you have two transactions that are conflicting across the daG, how do you join them? Right? Because they don't have a consensus protocol for this. The answer is they use a blockchain. They use a linearizer to linearize transactions to order them. And that thing is called the coordinator. So the diagnosis should do anything. It all depends on what does the coordinator spit out in this linear blockchain.
00:51:43.330 - 00:52:10.326, Speaker A: To me, all those protocols are basically a vector clock, I guess. Iota is not a vector clock. It's a single server. But like avalanche and hashgraph. Yeah, that's the one. Right? You get a set of versions for everything from everybody. And that's the vector clock for that particular message.
00:52:10.428 - 00:52:36.286, Speaker B: Yeah, dags are interesting. I think Conflux also uses a transaction dag of sorts. They do linearize it post facto, but when they build up, they build it up as a transaction dag. I think avalanche also does linearization, but this comes with certain synchrony assumptions between the pchain and the x chain. But these systems do, like, after the fact linearization of some sort.
00:52:36.388 - 00:52:38.400, Speaker A: Do you have time to read all this stuff.
00:52:38.770 - 00:52:49.090, Speaker B: Basically all I do from the time I wake up to the time I sleep is work on laser ledger, work on optimistic roll ups, and read papers and blog posts. And also I troll on Twitter.
00:52:50.950 - 00:52:54.078, Speaker A: What is your take on optimistic roll ups?
00:52:54.174 - 00:53:32.000, Speaker B: I think optimistic roll ups are good, but obviously I'm biased because I'm the one who originally came up with the idea of optimistic roll ups. So I'm a little bit biased, but I think they're pretty good. Not because they scale the EVM to 10,000 transactions per second, but because they're essentially a way of doing like a pseudo sharding on top of a common data availability layer. You can shard your execution without sharding the data availability layer. So in that context, you can essentially get all the benefits of e two just by running a bunch of optimistic roll ups, or a bunch of the benefits of serenity just by running a bunch of optimistic roll ups on top of ethereum today.
00:53:33.110 - 00:54:24.570, Speaker A: So my view with roll ups was that it almost seemed like a perfect bridge, at least for asset transfer data. Not like arbitrary message transfer, but for moving tokens around. Because if Ethereum looks at EVM state that's executed in Solana, when Solana comes to settlement on that thing, all the other programs external to that roll up can trade against that state because it's guaranteed internally. But then when Ethereum gets that data, they can actually trust that whatever was executing on Solana didn't just mint tokens or fake signatures that can be verified. So to me, it seems like almost what bridges should have been designed.
00:54:24.650 - 00:54:55.034, Speaker B: Yeah, the funny thing is, I had talked to James Presswich about this also around the time that I was developing optimistic rollups. I don't remember the exact dates, but he had come up with an interesting way of doing a bitcoin bridge optimization that was along the same lines of doing optimistic execution, which is correct. This should have been how it's done. Well, bitcoin is a bit cheaper because of the fact that there's a SHA 256 P compile on Ethereum, but things like, let's say the near bridge. I think Solana has a bridge too. To Ethereum. Does it?
00:54:55.152 - 00:54:59.942, Speaker A: We have a very dumb one that we could ship very quickly, which is like a multi sig.
00:55:00.006 - 00:55:58.202, Speaker B: Yeah, that was it. You guys did have an initial one, and I think Kyle Salami also talked about two other designs that you guys were working on, or something along those lines, but near also have a bridge, and I assume some other protocols have bridges. Lazy Ledger is being built so that we can natively bridge to Ethereum. So that you can use lazy ledger as a data availability layer for roll ups, which some fundamental design goal here is that we want to be able to support not only native applications on top of lazy ledger, but roll ups, both optimistic and zk on top of Ethereum and things like Cosmo zones and potentially other systems that want to use lazy Ledger for shared data availability security. But there's a bunch of systems building bridges. And you're correct that there's no point to verify all these signatures when you can just do it optimistically. Because here's the thing, all the blockchain problems come down to data availability and ordering.
00:55:58.202 - 00:56:00.378, Speaker B: That's why you can build these optimistic systems.
00:56:00.474 - 00:56:07.634, Speaker A: Yeah, for the most part. And then the application problems are execution. There's no way to cheat the devil, right?
00:56:07.752 - 00:56:09.300, Speaker B: They're an implementation detail.
00:56:10.390 - 00:56:34.090, Speaker A: We don't worry about those. Cool. I mean, we kind of ran over. It's been a fascinating conversation. It's awesome that you're on the podcast, and it's really cool to see people try crazy things. I think we're still so early in the space that there's a ton of room for people to go and take these crazy ideas and see what can happen.
00:56:34.160 - 00:57:00.590, Speaker B: Yeah, thanks for having me. I'm a big fan of the idea of no sharding, and I'm also a big fan of the insane work that you guys have done into building out an optimized virtual machine and parallel execution and all this other stuff dealing with concurrency and state accesses and caches and stuff. It's like, it's top notch. And I wish more people in the blockchain space understood these things as being fundamental to blockchain scalability like you guys do.
00:57:00.660 - 00:57:01.920, Speaker A: Thank you. Yes.
