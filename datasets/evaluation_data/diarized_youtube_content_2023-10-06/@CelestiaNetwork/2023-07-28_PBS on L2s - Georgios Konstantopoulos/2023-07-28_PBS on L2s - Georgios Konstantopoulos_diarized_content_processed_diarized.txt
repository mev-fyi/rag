00:00:02.010 - 00:00:49.850, Speaker A: So thank you all for coming and for not coming. This has been a very exhausting week and also a week where we've made great progress on a lot of the concrete parts about where the industry is going in this talk. I would like to give a few questions for the audience to think about, given that now we're entering the layer two part of the day and layer two part of the layer two part of the day. Many people here that you're going to hear next are working on it full time. I'm no longer spending as much time on this, but I'm going to try to offer a zoomed out view and some questions for the audience to think as people do their talks and hopefully to reach some deeper insights. My name is Georgias. I'm the CD of fardam.
00:00:49.850 - 00:01:24.840, Speaker A: So we'll start with what is NL Two? I see John in the audience and Togul in the audience as well. We'll see how that will go. We'll talk about shortly what the differentiators between layer twos can be and what are net new exciting features that can be introduced. Then I will pose some interesting hypotheticals for Shared Sequencers and then I will have one slide for Prover Proposer Builder Separation or whatever the canonical name ends up, which our next speaker will also tell us about.
00:01:27.130 - 00:01:29.882, Speaker B: So we will talk about layer two.
00:01:29.936 - 00:01:38.300, Speaker A: Just from the context of not bridging because there's been many debates and I have no interest in engaging in any of them.
00:01:39.950 - 00:01:42.594, Speaker B: The main thing about the layer twos.
00:01:42.662 - 00:01:44.382, Speaker A: That we care about right now is.
00:01:44.436 - 00:01:49.038, Speaker B: That you have some off chain state, some off chain state in another place.
00:01:49.124 - 00:02:13.560, Speaker A: In another place, and then that off chain state gets posted to a layer one, which is called the Data Availability Layer. And that ensures that anybody that wants to recreate the state of the layer two, they can go and look at the layer one. They can very easily derive it. Typically there is a deterministic derivation function different in each system which allows you to do that.
00:02:14.250 - 00:02:20.598, Speaker B: Beyond that, a layer two is an L one, it's a chain. It has a database, has a runtime.
00:02:20.694 - 00:02:25.158, Speaker A: It has an RPC, has a peer to peer layer, it has a bunch.
00:02:25.174 - 00:02:29.226, Speaker B: Of cryptography maybe it's a standard distributed system.
00:02:29.408 - 00:02:31.354, Speaker A: We know how to optimize them.
00:02:31.552 - 00:02:48.318, Speaker B: These systems, they bottleneck on I O and state growth. If the thing that matters to you is decentralization and the ability for individual to verify these are the two things that matter I O bottlenecks, how fast you can sync a chain state growth.
00:02:48.414 - 00:02:51.300, Speaker A: Bottlenecks, how big your chain can be.
00:02:52.230 - 00:02:58.382, Speaker B: And if you want to scale ethereum or anything else. How you do it is by launching.
00:02:58.446 - 00:03:03.480, Speaker A: Many of these layer tools. And the roll up Centric Roadmap is all about that.
00:03:04.170 - 00:03:11.170, Speaker B: Now, some problems for people to think about are one, how are we going to do composability across the same flavor.
00:03:11.250 - 00:03:12.218, Speaker A: Of a layer two?
00:03:12.304 - 00:03:15.146, Speaker B: So if I have two Op Stack chains and they want to talk to.
00:03:15.168 - 00:03:20.640, Speaker A: Each other, how are they going to communicate without necessarily going through the layer one?
00:03:21.890 - 00:03:27.534, Speaker B: If I have Op Stack and Arbitrum or ZK stack, or Stark stack, or.
00:03:27.572 - 00:03:34.110, Speaker A: You know, I don't know, is there some extra difficulty in making these two communicate?
00:03:35.170 - 00:03:39.634, Speaker B: Are these systems even compatible or whatever runtime they need to support?
00:03:39.752 - 00:03:46.680, Speaker A: Or are they just looking like it on the outside and things on the inside look a lot different?
00:03:47.210 - 00:03:52.406, Speaker B: And the natural follow up question to that is what does change in the.
00:03:52.428 - 00:03:59.570, Speaker A: Internals and how does that impact the externals? How did an implementation detail make its way to the interface?
00:03:59.730 - 00:04:01.434, Speaker B: And one example could be that people.
00:04:01.472 - 00:04:08.426, Speaker A: Maybe have optimized for mev extraction on Geth because they understand, okay, these are the bottlenecks, but maybe, for example, on.
00:04:08.448 - 00:04:15.646, Speaker B: The Polgon Zkvm, maybe it's different, I don't know. Now let's talk a bit about what.
00:04:15.668 - 00:04:18.366, Speaker A: Are the unique things that you can get from a layer two.
00:04:18.468 - 00:04:25.134, Speaker B: First and foremost, from the side Chains paper from 2015 by Blockstream. Experimentation, experimentation, experimentation.
00:04:25.182 - 00:04:26.686, Speaker A: We can try new things without breaking.
00:04:26.718 - 00:04:39.510, Speaker B: The base layer, and then I will pose some bullets with some questions about them. So when we have faster block times or soft confirmations, what does that mean.
00:04:39.580 - 00:04:42.690, Speaker A: About the mev extracted in these blocks?
00:04:42.850 - 00:04:50.938, Speaker B: Does it really mean that more frequent blocks means less or more mev? What happens outside of that system?
00:04:51.024 - 00:04:58.410, Speaker A: And how does the synchronization time between these two systems change, which directly of course, affects the mev extracted?
00:04:59.390 - 00:05:14.046, Speaker B: The proposal set in roll ups can be less decentralized without losing safety, with some cost to liveness, but not total degradation. Naturally, having a distributed system means it's.
00:05:14.078 - 00:05:16.580, Speaker A: More higher SLA, which is good.
00:05:17.430 - 00:05:23.662, Speaker B: If the system allows a custom transaction pool, it also allows for custom ordering FCFS.
00:05:23.726 - 00:05:26.098, Speaker A: We'll hear Patrick McCurry talk about that in a bit.
00:05:26.264 - 00:05:31.794, Speaker B: I also don't know. But all of these are levers that one can pull depending on the mev.
00:05:31.842 - 00:05:34.518, Speaker A: Profile that they want the layer two to have.
00:05:34.684 - 00:05:36.066, Speaker B: Or if you don't have a transaction.
00:05:36.098 - 00:05:39.922, Speaker A: Pool and people just hammer the sequencer.
00:05:39.986 - 00:05:40.882, Speaker B: What are you going to do?
00:05:40.956 - 00:05:41.786, Speaker A: Proof of work?
00:05:41.888 - 00:05:47.962, Speaker B: Are you going to add identity? How are you going to prevent this spam that happens on the wire that.
00:05:48.016 - 00:05:50.490, Speaker A: Comes from the client to the server?
00:05:51.070 - 00:05:59.130, Speaker B: And the obvious thing, which is not I'm realizing the last bullet is off topic, but I'll roll with it, is that we have a bundled role.
00:05:59.290 - 00:06:07.970, Speaker A: A sequencer is both a proposer and a builder, and naturally you want to keep the sequencer lean to allow for in the future as many of them to be around.
00:06:08.120 - 00:06:21.046, Speaker B: So the same thesis for PBS on level one applies to level two separation of concerns, how you build optimized systems. I will not talk about decentralized sequencing. I think it's kind of, again, an.
00:06:21.068 - 00:06:29.354, Speaker A: Implementation detail any centralized system you can replace it with a BFT version of it, put it on tournament, put it on some other consensus protocol and that.
00:06:29.392 - 00:06:37.962, Speaker B: Gives you redundancy and some A's or B's. Here are do you enshrine the shared sequencer in your system?
00:06:38.096 - 00:06:41.654, Speaker A: For example the Op stacks super chain.
00:06:41.782 - 00:06:44.142, Speaker B: Is it the thing and is it.
00:06:44.196 - 00:06:46.174, Speaker A: Something that gets you better performance than.
00:06:46.212 - 00:06:52.078, Speaker B: Having something like an Espresso sequencer on top of it? I would think that anytime that you.
00:06:52.084 - 00:06:53.658, Speaker A: Have a core protocol operation and you.
00:06:53.684 - 00:07:01.250, Speaker B: Leave the metal that means that it becomes more expensive. One or many shared sequencers do you aggregate them?
00:07:01.400 - 00:07:07.266, Speaker A: How does that go? We saw the fractal aggregation of proofs that the vitalik showed earlier the aggregation.
00:07:07.298 - 00:07:23.302, Speaker B: Theory can play in any layer l two specific or close favor again, same thing and the thing I want to drive home shared sequencers are not magic. They don't actually solve all the world's problems. They give you atomic top of block.
00:07:23.366 - 00:07:27.340, Speaker A: Inclusion, the most classic example being the base roll up.
00:07:28.270 - 00:07:40.238, Speaker B: What is the room for differentiation? I also don't know but I know that nobody else either knows. So what do and these are two other questions around the bridge and the.
00:07:40.244 - 00:07:44.800, Speaker A: Atomic success which I still have only seen strowman answers to and I think we should do better.
00:07:46.850 - 00:07:50.590, Speaker B: One concrete proposal on how to get to cross chain composability.
00:07:50.670 - 00:07:55.682, Speaker A: And I realize this is kind of dense and low context, so I'm happy to expand later, but we don't have enough time.
00:07:55.816 - 00:07:57.026, Speaker B: Is that maybe you can have a.
00:07:57.048 - 00:07:59.866, Speaker A: Shared sequencer that indeed only guarantees inclusion.
00:07:59.918 - 00:08:03.986, Speaker B: Without execution, but then you can have a bunch of builders where they cross.
00:08:04.018 - 00:08:15.290, Speaker A: Chain simulate and vitalik kind of alluded to that, that you can cross chain simulate the broke. The builders are indeed going to be running nodes and simulation engines for every chain that they support.
00:08:15.440 - 00:08:42.514, Speaker B: And the builder is going to land a bundle that is going to be landing transactions for all up one, two and three together. And the insight here is that we take the mev builder role and we just bundle more functionality of it like make it heavier and basically the shared sequencer is funny enough, the MAV builder poetic. This is my final slide. I want to talk a bit about this because the topic I deeply care.
00:08:42.552 - 00:08:56.466, Speaker A: About on ZK proofs and how the market will evolve the proverb right now is again run colo with the sequencer and the sequencer as we said is also a builder.
00:08:56.578 - 00:09:07.066, Speaker B: We really need to unbundle these two and there's many many ways that we could do it. My favorite work was a paper by a good friend of mine, Nike Scatis. It was called Proof of necessary Work.
00:09:07.168 - 00:09:11.726, Speaker A: And described a way where you can introduce non grinding Allah proof of work.
00:09:11.828 - 00:09:13.934, Speaker B: In the Snark generation process and that.
00:09:13.972 - 00:09:26.466, Speaker A: Let you have a leaderless system or rather a system where the leader is not known ahead of time for proof generation, for proof proposal. Naturally, anything that is like proof of.
00:09:26.488 - 00:09:28.354, Speaker B: Work esque, it has a lot of.
00:09:28.392 - 00:09:31.810, Speaker A: Wasted effort, but maybe it's like very egalitarian.
00:09:32.710 - 00:09:36.446, Speaker B: So maybe like the next step from that is should we do a consensus.
00:09:36.478 - 00:09:41.298, Speaker A: Protocol for the leader election? Let's elect the proposer, let's agree, let's.
00:09:41.314 - 00:09:42.998, Speaker B: Put up some stake or something.
00:09:43.084 - 00:09:45.318, Speaker A: We elect the leader, the leader does it.
00:09:45.404 - 00:09:59.542, Speaker B: Standard techniques that we have seen used in consensus protocols are going to be used for proverbs election. And the other thing that I don't think I've seen anywhere so far and I just added it as a strawman.
00:09:59.606 - 00:10:05.374, Speaker A: And I realized that it's probably broken and there's probably things to do. But I wanted to say it is.
00:10:05.412 - 00:10:30.582, Speaker B: That as ZK proof is generated, you can actually observe that parts of the proof are generated in a pipeline sense. The most trivial example I have a block with ten transactions. Transaction one is executed, it generates some witness data. That witness data is supposed to get fed to a snark, but while it's getting executed and it's getting proven, another transaction is also getting executed and getting.
00:10:30.636 - 00:10:36.470, Speaker A: Proven and another and another and another. Do the provers need to be the same thing?
00:10:36.620 - 00:10:56.138, Speaker B: Can there not be a market for proof aggregation? The things that Italy was talking about? Can there not be something where proof one goes to John, proof two goes to Nick, proof two, three goes to know and you can keep doing that. It seems like we can come up with collaborative proof generation protocols that abuse.
00:10:56.234 - 00:11:00.350, Speaker A: This pipeline structure of like proof generation.
00:11:00.690 - 00:11:05.326, Speaker B: I don't know how that will look like, but if somebody is interested in talking to me about it, please find.
00:11:05.348 - 00:11:06.466, Speaker A: Me after the talk.
00:11:06.648 - 00:11:09.486, Speaker B: And the final question is how do you distribute the fees?
00:11:09.598 - 00:11:15.780, Speaker A: Again, the perennial question in the mev supply chain or compositional game theory is the new term, I hear.
00:11:16.730 - 00:11:17.794, Speaker B: How do you put the fees?
00:11:17.842 - 00:11:19.510, Speaker A: Where does value accrue?
00:11:20.970 - 00:11:23.254, Speaker B: I also don't know, but it's something.
00:11:23.292 - 00:11:24.520, Speaker A: Interesting to think about.
00:11:25.370 - 00:11:31.640, Speaker B: I have 3 minutes and it would be good if we can do some questions, perhaps. Thank you.
00:11:35.610 - 00:11:43.280, Speaker C: Anyone has any questions from the audience? US. Oh, I see.
00:11:54.610 - 00:12:07.110, Speaker D: Can you elaborate a bit more on how cross chain or cross LTSU interaction can be facilitated by this shared sequencer model for those of us who are unfamiliar with the literature?
00:12:08.090 - 00:12:11.800, Speaker A: Yeah, of course. Can we get the slides back up?
00:12:13.370 - 00:12:22.170, Speaker B: So the shared sequencer is an entity that is ordering things and doesn't really know how to execute them.
00:12:22.240 - 00:12:23.946, Speaker A: Or if it were to know how.
00:12:23.968 - 00:12:41.840, Speaker B: To execute them, it would need to have execution nodes for every chain it is ordering for. Because otherwise it would be ordering invalid things. And ordering invalid things might be okay. If your chain can support no Ops, you just say anything that's invalid. Just like skip it.
00:12:42.610 - 00:12:45.140, Speaker A: This leaves junk on the chain but maybe it's fine.
00:12:46.390 - 00:13:01.314, Speaker B: So perhaps what to do is that you have the shared sequencer which is basically a privileged entity for submitting data to the chain. And how does the data get ordered.
00:13:01.362 - 00:13:03.522, Speaker A: To get into that privileged entity?
00:13:03.666 - 00:13:50.226, Speaker B: You could have builder One and Builder Two basically or builder One for simplicity, who is running nodes for chain One and Chain two? And what they will do is that it will publish a bundle or a full block template to the layer one where the first transactions in that bundle are going to be all the roll up A transactions and the next transactions in that bundle are going to be all the roll up B transactions. And because that person is a builder and has run the simulations on every chain, they can guarantee that by being at top of the block they can guarantee that the transactions are going to be valid. So by doing that you can hack your way into shared sequencing in a very clean way. I realize this is a bit dense.
00:13:50.258 - 00:13:52.710, Speaker A: And I don't know if it answered the question sufficiently.
00:13:56.330 - 00:14:06.570, Speaker D: Yeah, it makes sense. This also goes back to what Vitalik says about aggregation. I'm just wondering how this helps cross chain interaction.
00:14:08.750 - 00:14:10.138, Speaker B: Well, I don't think that you can.
00:14:10.144 - 00:14:20.526, Speaker A: Do callbacks callbacks I don't know who has said that you can do contract A, call to contract B and then it calls back. It's a bit unclear because people are doing like right now the way the.
00:14:20.548 - 00:14:21.834, Speaker B: Roll ups work is that they submit.
00:14:21.882 - 00:14:24.714, Speaker A: One transaction, one big bundle per block.
00:14:24.842 - 00:14:29.458, Speaker B: If we could interliave bundles, if we could say top of block part A.
00:14:29.544 - 00:14:32.818, Speaker A: Is like bundle for roll up A and then roll up B and then.
00:14:32.824 - 00:14:34.706, Speaker B: Roll up A again, maybe we could.
00:14:34.728 - 00:14:39.480, Speaker A: Do something where I can call chain A and then chain A and then it calls me back.
00:14:39.930 - 00:14:47.270, Speaker B: The main things that we support in this design are crosschain message passing and maybe cross chain bridging.
00:14:50.090 - 00:14:59.114, Speaker A: I'm seeing a timeout so I don't know if I have time for more. Sure. I think the person in the back.
00:14:59.152 - 00:15:08.350, Speaker C: Was first, the person in the background.
00:15:12.610 - 00:15:38.200, Speaker E: So I just have a follow up question regarding the shared sequencing and also PBS for the L two. Like if we have sort of these shared sequencer, actually two questions. The first one, if it's really like sort of shared sequencer, do you think it will overwhelm that machine? I mean, if the sequencer used for multiple roll ups yeah, that's the first question. Second question.
00:15:40.250 - 00:15:44.886, Speaker A: Let'S keep it at one. Sorry. So is the question that the shared.
00:15:44.918 - 00:15:50.714, Speaker B: Sequencer can be very heavy? I'm not sure I understood that. If that is the case, the way.
00:15:50.752 - 00:16:02.126, Speaker A: That you do shared sequencers, you basically need to make them like cross chain DA aggregators before you land the chain, before you land the bundle to layer one. But you do not want them to.
00:16:02.148 - 00:16:05.002, Speaker B: Be running the execution of these chains.
00:16:05.146 - 00:16:12.340, Speaker A: So by doing that, you can keep the sequencer as a light DA consensus protocol effectively, which is what Espresso is, for example.
00:16:12.950 - 00:16:14.578, Speaker B: And then you land your data to.
00:16:14.584 - 00:16:16.630, Speaker A: The real DA protocol.
00:16:18.330 - 00:16:36.854, Speaker E: Sorry, just a quick one. But in that case, for example, if we really pass a message across roll ups right, via the sequencing. But do we need to wait for the confirmation from one roll up before we really execute another message? Dependent on the for sure.
00:16:36.892 - 00:16:38.038, Speaker B: Yeah, obviously.
00:16:38.124 - 00:16:43.370, Speaker A: Of course. Any transaction needs config zero, conf one. Conf is insecure.
00:16:43.530 - 00:16:49.100, Speaker E: I see. So in that case, it's like sequencing plus execution and DA, right?
