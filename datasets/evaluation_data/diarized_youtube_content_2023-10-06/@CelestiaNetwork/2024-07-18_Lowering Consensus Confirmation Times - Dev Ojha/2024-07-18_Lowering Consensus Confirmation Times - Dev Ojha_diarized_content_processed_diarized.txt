00:00:01.840 - 00:00:23.890, Speaker A: Hello all, I'm Dave. I'm going to be talking about how we can lower consensus commitment times. Yeah. So what's the goal here when you make a transaction? Let's make your front end update very quickly. So why do you want this? It's kind of obvious, I think when you go do a swap, you want to see what happens instantly. You don't want to wait 15 2nd block time. You don't want to.
00:00:23.890 - 00:01:11.884, Speaker A: A lot of the reason why roll ups are being so popular lately is one, low fees, but two, they're really quick. So the roadmap of this talk is going to be first chatting, what are we improving in consensus today? And then I want to tell you a bit about tendermint and how it's actually really good. And I think people have kind of forgotten about this. Then a little bit on pre conformations sums up on short term censorship resistance. I'm going to skip the bull shark misticetti stuff today because of your time. And then what I think will be really interesting is there's been a lot of discussion on solver networks or having a lot more work being done in the mempool and we can fix, there's a lot of problems there. These are actually solved with what I call provable DA ingestion.
00:01:11.884 - 00:01:35.754, Speaker A: And we can make this really quick with geo just spring da something that'll be really exciting. Yeah. So let's dive into it. So what are our big problems in consensus, it's actually kind of the same problem. Give low latency, convinced users, get high throughput and achieve short term censorship resistance guarantees. They're the same problem. Because I'm a user, I want to get transaction quickly.
00:01:35.754 - 00:02:18.962, Speaker A: If there's many other users, I need high throughput. If the next validator is offline censoring me or whatever, I need some short term censorship resistance guarantee, I guess one other thing on here that's a big problem is decentralized validators, but shut off that separately. So in this talk I'm actually only going to be parameterizing around trying to make decentralized validator sets. So I think that's our interesting goal in crypto to be make something a financial layer for the whole world. So yeah, what's a commitment? Well, okay, I've committed to something when I think that either the thing will happen or some security assumptions violated. So for consensus, we're concerned mostly with two things, inclusion commitments. I know my transaction will be included on chain.
00:02:18.962 - 00:02:58.206, Speaker A: This is sufficient for many things like limit order placements or just payments. You only need inclusion. Your execution guarantee is pretty simple, but then there's also execution commitments where not only do you know your transaction will be on chain, but you want to know the result of it. So this is like if I do a swap, what's the result of the swap? Typically, this will depend on being in a block, but we'll discuss a bit, a bit about how you can do better in decentralized environments. Okay, so warm up like, what is the traditional commitment times in cosmos? It's the time to get into a finalized block. As a user, you've got no guarantee when you'll be in a finalized block. You kind of make a transaction.
00:02:58.206 - 00:03:39.152, Speaker A: It floats around in the mempool until a proposal includes you. But if you have enough fees, in principle, you should get into the next block and a block's finalized in one height. So in practice today, this is like one and a half to 5 seconds, depending on how your favorite blockchain parameterizes the network. But what about censorship resistance? And for context, censorship is the word we use. But this includes a validator denying they saw your transaction, legitimate network delay preventing them from seeing it, or software bugs, or economic incentive attacks. But we say censorship, and it doesn't necessarily mean active malice from the validator. So this is going to matter a lot.
00:03:39.152 - 00:04:00.936, Speaker A: I'll go into the next block, next slide. But the strategies we have for solving this is one faster block times second. The second biggest one is break up the proposer's monopoly on block space. So what we're doing with tenement vote extensions, DaG based BFT is doing this. Most recently, Ethereum is considering multiproposers. Then we have encrypted mempools. I'm not going to talk about that today.
00:04:00.936 - 00:04:54.046, Speaker A: But you can see furview or scientist threshold encryption talks in general, these solutions depend on either breaking up the proposal monopoly, having a centralized trust assumption, or very cool cryptography called witness encryption that has not seen the light of day yet, and app layer incentives. Okay, so why do we care about short term censorship resistance? Well, anything that has a challenge period depends on you being able to include transactions on chain. So being able to include transactions on chain quickly needs short term censorship resistance. So every optimistic fraud proof needs this. If you have intent layers or mempool solving networks, and they want to give a user a guarantee that you'll settle within five minutes, you need this as well. Lending markets need to be sure they get on chain pretty quickly. And something I think a lot of people haven't realized is that on chain twaps are only as good as your short term sensor persistence.
00:04:54.046 - 00:05:26.746, Speaker A: So, in Ethereum, you have Coinbase routinely proposing eight blocks in a row, which is about two minutes of data. So now people have stopped using on five minute twaps because it's basically a centralized Coinbase assumption. To fix this, we need to build this back into our protocols. So, yeah, I guess if you look at the. I'm gonna skip this. Sure. You need to parameterize around what will be safe, even in the tail cases, like 0.1%
00:05:26.746 - 00:06:07.448, Speaker A: of the time. So the simplest protection you have is make faster blocks and design blockchains that get you all decentralization with this preserved. So, in Ethereum, if you lower block time from 15 seconds to two and a half, very standard across cosmos now, then ten Coinbase blocks in a row takes it from two minutes of downtime to 20 seconds. Coinbase being like Coinbase, the company, that's a huge improvement. So where are we with tendermint today? Tendermint as a protocol is actually amazing. You have three Internet latency trips. So going from eastern North America to Japan is about 150 milliseconds, mostly bound by the speed of light.
00:06:07.448 - 00:06:55.804, Speaker A: So we can get globally distributed, broadcast one way in, say, 100 to 150 milliseconds. So that means tendermint can get consensus to happen in 450 milliseconds. So if you're a user, you want to talk to the next block proposer, that's ok, you have to talk to them, which is maybe another 150 milliseconds. So that gets you 600 milliseconds, and then the closest validator can then tell you once there's two thirds of votes, or once you're finalized, this means the tenant baseline for a user guarantee is 650 milliseconds by just optimizing the algorithm, no new consensus tricks. So if we just did this, that would make our censorship resistance for all these protocols go from two minutes in the ETH example. Well, now to 5 seconds. This is a qualitatively different world.
00:06:55.804 - 00:07:27.604, Speaker A: And if we pipeline consensus, that's all, like, you can get a new block every 200 milliseconds, gets you down to two minutes to 2 seconds. So tendermint alone is actually gets us very far. You interlude here, you may say tendermint takes you 1.3 seconds, but as Zaki's aptly noted on Twitter, all this is all low level bugs. After spending a bunch of time in this, these are all very fixable. We've done actually a tremendous amount of work the last two months. Shout out to Evan if he's here.
00:07:27.604 - 00:07:53.078, Speaker A: Evan's around somewhere from Celestia, Anton Casson, and myself. And so I think, I imagine the next few months, we'll be able to hit this platonic ideal of 500 millisecond global tenement blocks. Okay. So now, next thing I wanted to hit on is that tenement is actually better than two rounds of voting. This is maybe a minor optimization to folks, but I think it's cool. I said there's two rounds of voting. There's pre votes and pre commits.
00:07:53.078 - 00:08:21.006, Speaker A: Once you've seen two thirds of pre votes on a block containing a transaction, it's almost certainly going to be finalized. There's only three cases where it won't. There's double signs. Okay, people getting slashed. Another edge case is 67% of validators are being byzantine, but in a way, it's unaccountable. But why would they do this? What do they gain? Or you have the communication graph between all of the honest validators is partitioned. But that's actually a crazy assumption.
00:08:21.006 - 00:08:59.772, Speaker A: That's like saying that you can control the flow of stopping North America from being able to communicate to Europe or Asia and Asia. I think it's safe for a user to have a have their limit order, or be, have the limit order be preemptively confirmed in this case. So we could give users confirmation at two, three pre votes. This is a much stronger guarantee than precomps, for instance. And then you'd have a user delay of, like, 500 milliseconds. Gossip to the next proposer, two rounds of gossip, then. Closest validator back to the user.
00:08:59.772 - 00:09:23.980, Speaker A: So. Okay, great. Our baseline tenement is 500 milliseconds. Can we do better for things? There's this common idea in Ethereum for proposer commitments or otherwise called pre comps. So what's a pre conf, imagine the user can gossip their transaction to the next proposer. The proposer can sign a promise saying, I promise I'll include you in the next block at position number five. And if I don't, I get slashed.
00:09:23.980 - 00:10:01.290, Speaker A: This helps you improve inclusion time if you trust the next proposer. But this has a lot of reliability issues. Roll ups are getting away with this only because they're centralizing other things. They basically centralized MEV rights and censorship with a single sequencer. If you don't want that, if you want to be a decentralized thing, you care about censorship, which is a core property, I hope, as I noted earlier, then this won't be speeding up execution guarantees. So we need to be more clever for that. So trade off of this.
00:10:01.290 - 00:10:24.676, Speaker A: A pre conf, just the next proposer gives you no censorship resistance. You get even better latency. One Internet round trip, I guess mega ETH is now saying if you're in a single sequencer chain, they can actually geo distribute their pre conf. So maybe that's 50 milliseconds. But yeah, this won't speed up execution. So here's one idea. Let's get people faster execution.
00:10:24.676 - 00:10:50.240, Speaker A: Precomps not from consensus via intent networks. So let's say that a user publishes some intent. Hey, I want to buy bitcoin at 50,000 USDC. They post this into the mempool. If there's a market maker who sees that, the market maker can pick it up instantly in and say, okay, I will fill you, user. Here is the signature saying, I will fill you. And if I don't, you get.
00:10:50.240 - 00:11:37.196, Speaker A: If I don't fill you within five minutes, I will get slashed. So the user can see this in the mempool layer before a block's been proposed and feel like they've gotten finality or been confirmed. If the market maker doesn't do this, the market maker gets slashed and the user still gets paid. So this gives the user a 50 to 100 millisecond confirmation. But this can also fail safely, that if there's no market maker, it just goes back to the chain and the user gets a 500 millisecond guarantee. So now, as long as we have short term sensor resistance, as long as the market maker can feel confident that they will be able to get onto the chain, then we can build a system like this that can give users faster confirmations if someone's willing, but otherwise fall back to the chain. This actually matters a lot.
00:11:37.196 - 00:12:12.320, Speaker A: This is going to be a big failing in a lot of intense networks if they don't fall back to the chains. Because it actually turns out, it turns out it's hard to bootstrap market makers. You're going to need to tap into on chain liquidity first. And once there's a lot of order flow, then market makers will come. So, yeah, by getting. And this will generalize not just for swaps, but to many actions people want to do. So this means that you can get external actors to give you effectively precomps with 50 millisecond timings, or just local to your own continent or geographic distribution.
00:12:12.320 - 00:12:54.160, Speaker A: And what's even nicer is this won't overload consensus at all. We just need the guarantee that the intent solver believes they can get into the chain quickly, which just depends on short term censorship resistance guarantees. Okay, I see I have five minutes left, so I'm going to kind of skip through this. We can get censorship resistant precomps via vote extensions. The idea is every validator can contribute votes. I want to shout out to Barry and Skipteam for basically building this in osmosis for the top of block auction, but I'm going to skip through this to go to the next thing. I'm going to skip bullshark SETI as well.
00:12:54.160 - 00:13:32.910, Speaker A: Here's the thing I think will be exciting for the audience here. There's this idea right now if you want to do a lot in the mempool or intense layers, the suave solver. So I'll tell you a little about this idea from DyDX that they're calling Memcloud, and it's a unique scaling improvement that's being pioneered in cosmos. Right now. The idea is that in Doydex, they're running an exchange. So they want every validator to just run an order book in their mempool, and they just want to post the finalized transactions to the chain. So the idea is that you can place and cancel limit orders with no gas and much more cheaply because these are only in the mempool.
00:13:32.910 - 00:14:08.188, Speaker A: But this has some pretty clear massive problems. Every validator gets full mev rights. So this means that every validator can basically rug in a user. Because you place a limit order and it should be filled, they can just pretend to not see it. You cancel a limit order because you think it's bad, they can just pretend to not see the cancel and still get it filled. Or if you want to do a market buy, they can pretend there's no limit orders there and give you a terrible execution price. So right now, this gives you very little guarantees.
00:14:08.188 - 00:14:53.564, Speaker A: You get much better throughput. But your censorship resistance is actually kind of worse than normal chains because nothing's getting posted to everybody. And as a user, you have no reliable confirmation your UX update is arrival into a mempool that could be ignored, and we just kind of assume things will work out due to social consensus. But there's a way to get this property using data availability and get our throughput win so we can fix this by having every validator prove that, hey, here's some DA layer here. I promise that I have seen everything in this DA layer. Why do you need a D layer? It's not good enough that a validator promises they've seen something. You need to know that all the validators are seeing things.
00:14:53.564 - 00:15:20.860, Speaker A: So a DA layer is fundamentally giving a shared view of what data has been posted to all the validators. So you need sensor resistance as well. Let's basically get your DA layers to have sensor resistance. We can achieve that, basically. Bye. Doing vote extension tricks at the DA layer as well. So your latency to the user is going to be a DA layer latency.
00:15:20.860 - 00:16:02.470, Speaker A: But here's a problem. Why would your DA layer be faster than tendermint? Like how it's still just going to be 500 milliseconds if you want globally distributed Da? And here's the cool idea that we can do as well. We can just have geo distributed DA layers, one in North America, one in Asia, one in Africa, one in Europe. And you can have every validator promise that. Before I decided that every validator promises they ingest everything from one DA layer. Instead, you can have a North America validator promise to ingest data from the NADA layer zone. So they'll include everything that was included in the NADA layer, and then you can have these DA layers sync with each other.
00:16:02.470 - 00:16:56.132, Speaker A: So the NAD layer gets consensus in like, say, 50 milliseconds, and then with a 200 millisecond latency, we'll get all data from Asia and Europe. So now, as a user, if you get posted into the, say, the NAD layer, you can get there in like 50 milliseconds, basically the speed of Internet, you and the closest data center in the region. And then as time goes on, as the speed of light delays goes on, this gets automatically propagated to Europe and Asia for the next validator that we'll post there. So now, within this example, let's say that the NAD layer finalizes every 50 milliseconds. As a user, it's pretty instant to talk to your NADA layer. Within 50 milliseconds, you know, you'll be included by the next North America validator. Then after, say, 150 milliseconds, you'll be included by any subsequent Europe validator.
00:16:56.132 - 00:17:39.760, Speaker A: And after, I don't know, 200, every subsequent Asia validator. This is a really strong guarantee that kind of strengthens with time. So I think this is going to be, this is the future of how we should get really fast inclusion guarantees with globally distributed networks. Okay, yeah, I think that's it. Maybe I'll end with some of my takes for what should cosmos do here? I think we should get ten amount block times down to the platonic ideal of 500 milliseconds. And we should just keep iterating on vote extensions for encrypted mempools and short term censorship assisted inclusion. Let's switch to getting user state execution results after two thirds of pre votes, and let's go to this world of fast geo distributed DA.
00:17:39.760 - 00:17:52.580, Speaker A: And if you want to implement a new consensus algorithm, either pipeline tenement or implement mistoceti. But I think the other the fast geodistributed Da should be our main goal. Thanks everyone for your time and let's chat about how to improve any of this stuff.
