00:00:00.090 - 00:00:10.240, Speaker A: But I had to ask since Danning's memes were cut. So would you please give us a PBS meme to make it up?
00:00:11.730 - 00:01:13.438, Speaker B: Real time meme challenge. This is difficult. I do not claim guaranteed success. So to get right into things Yodog, what's up? Who here thinks it's a ceiling? Who here thinks it's the ceiling that's up? Who thinks it's the sky? Who thinks it's like a cryptocurrency? Okay, well, let's just talk about aggregation. Okay, so aggregation, basically the core idea here is you have N things and you're trying to take N things and turn them into one big thing, which is less than N times as hard to verify, right? So sorry, it's hard to speak and be yelled at for my poor microphone performance at the same time. Okay, no one more reprimand that. I'm throwing this out, and you'll have to listen to me speaking quietly.
00:01:13.438 - 00:02:30.650, Speaker B: Okay, so what is aggregation? Right, it's converting N things and turning them into one thing, which is less than N times is hard to verify. Now, why do we want to do this? Well, because we care about scalability. We don't want to pay like $80 a transaction or even zero point $80 a transaction, and we want to cut down on chain verification costs. Right? There are a lot of different use cases of aggregation in the Ethereum consensus layer, for example, we have BLS signature aggregation, and there is many kinds of aggregation that we will have in the future. So some specific areas where we are thinking about aggregate, we might have aggregation ERC 4337. Right? So this is this off chain protocol by which users can send in user operations that have validity conditions that are different from the validity conditions of a regular Ethereum transaction. You might have a smart contract wallet validation, it could be a multisig, could be some custom Winternet lamport signature, could be whatever, and those conditions get verified.
00:02:30.650 - 00:03:27.610, Speaker B: And it's this kind of off chain ecosystem for basically making these custom smart contract wallets work. But these user operations all have to be wrapped up in a transaction, right? Because Ethereum debased protocol only accepts transactions. Now, before ERC four three seven, in the dark old days, we would have protocols, but which every single user operation would get wrapped up in a separate transaction, right? So I send a user operation, and then whoever I send it to would wrap that user operation in exactly one transaction, and then the transaction would go on chain. Every transaction would have 21,000 gas overhead. So it's not very efficient. It also was not a very open protocol. In ERC four three seven, we have a custom mempool where users send user operations into the mempool and then bundlers exist in that mempool.
00:03:27.610 - 00:04:35.006, Speaker B: Bundlers grab up all the user operations they can see in that block, and then they publish that on chain. Right, so this is a form of aggregation, right? Because the object that went in the cost of verifying that object by itself would be N plus 21,000, right? And whatever the cost of the user operation is 21,000 for the wrapper. But if you take like, let's say k of these and you put them together, then the cost is just going to be N times K because you still have to process all the user operations. Actually, sometimes a bit less than N times K because you can benefit from warm, cold storage, reading, gas stuff. And then plus 21,000 only once, right? So there's some pretty significant gas savings from taking user operations and putting them all in the same transaction. This is a type of aggregation, and this is something that's actually being done even in the builder ecosystem today, BLS aggregation. So this is a thing that actually has been recently kind of in proof of concept mode.
00:04:35.006 - 00:05:11.134, Speaker B: It's been added to ERC four, three, seven. There's a piece of code called BLS wallet that actually does this. But the basic idea is, instead of using ECDSA signatures, we use BLS signatures. So why do we want to do this? This is especially valuable in a roll up, right? Because in a roll up, the computation is done off chain. The data is still done on chain, right? And so the computation becomes vastly cheaper. The data does not become cheaper at all, and ECDSA signatures on their own are 65 times N bytes. Okay, fine.
00:05:11.134 - 00:06:52.506, Speaker B: I'm using N to refer to the number of objects now and then with a Bos signature, if you use G one points for the signature, then the signature can be 48 bytes for an Arbitrarily large number of operations, right? And so if you have like 100 of these per block, then for each one of them, the amortized cost will go down from 65 bytes per operation to like half a byte per operation. On the computation side, it actually does get a bit more expensive because instead of doing fairly easy elliptic curve stuff involving adds and multiplies, like, I think at ECDSA, recover is like something like three multiplies or somewhere in that range, and instead you do a pairing, which is like pretty heavy duty stuff. But as I said, on roll ups, computation is cheap, data is expensive. We're saving on data and it's great, right? But how do we actually do BLS aggregation, right? Because the Ethereum protocol does not currently have an execution layer BLS capability, right? And so if you imagine N different users, each of those users is going to send their own BLS user operation and their own individual BLS signature that they created separately is going to be 48 bytes long. Right? Then what we want is we want the thing that goes on chain to be 48 bytes. We don't want it to be 48 times N, right? And so there needs to be some kind of actor that is grabbing up all of these user operations going into the user operations, grabbing out the BLS signature, adding them all together. See, that's some kind of mechanism in the middle.
00:06:52.506 - 00:07:56.174, Speaker B: It's a big plus. And create one aggregate signature, 48 bytes. And that aggregate signature can be verified against the aggregate public key, which is created by taking all the public keys of the individual users and also adding those together. Right? So this requires two types of weird stuff, right? The first type of weird stuff is the off chain adder. The second type of weird stuff is the mechanism for verifying these signatures, where basically, instead of doing what Ethereum protocol does today, where you verify each of the signatures in sequence, the protocol or whatever super protocol we use, somehow would have to take these signatures. Then when you walk through each one of the user operations, don't immediately just verify pass, fail it, but actually record the BLS signature. First of all, you'd verify it locally, make sure it verifies individually.
00:07:56.174 - 00:08:43.618, Speaker B: If it doesn't, you'd throw it out, but then record it. And then if you're doing this inside the block you just recorded, then keep walking through all of the user operations, add up all of these signatures, and then at the end, do the one check at the end, right? Because adding two signatures is like very cheap. It's like hundreds of times cheaper than even an ECDSA operation, right? So this requires a different workflow where it's like walk through the operations, then do a check and then execute everything. Now, ERC, four, three, seven actually already now has an extension to do this. But this is again a type of thing that I call aggregation. Snark aggregation. So this is the future, right? So BLS is like very specific.
00:08:43.618 - 00:10:16.370, Speaker B: What if I told you that in the future we're going to be able to take existing ECDSA signatures, potentially even signatures that are made with existing tools, and replace them all with a snark that just says here is a signature and here is a proof that or rather, here is a proof that there exists valid signatures for all of these things? Right? So this way you could replace an unlimited number of ECDSA signatures with one Snark and you could even apply this technique to other mechanisms, right? So who here has used a privacy protocol? Privacy protocols are good, right? Now, privacy protocols cost a lot of gas. This is one of the bigger challenges hampering their broader adoption, right? Because inside of one of these privacy protocols, every transaction requires you to have a Snark that goes on chain. Now, the Snark 500,000 gas 20 times more expensive than a regular transaction. At current gas prices, that's going to be, I guess, about 20 GWe multiplied by 500,000 gas. So that's 10 million guay, which is 0.1 E multiplied by 19 $19 transaction fee. Who here is willing to pay an extra $19 to make your coffee more privacy preserving? I'll do it as a gimmick.
00:10:16.370 - 00:11:11.650, Speaker B: Yeah, this is not good, right? But what we can do is, well, we can use a Snark to prove the validity of all the Snarks, right? And so even if you have 100 different users using privacy protocols within a block, you just take their proofs, you prove them all together, and you have one proof of the proofs. And now you have all of the proofs of all the private stuff within a single block. Right. Now, what does this require? It requires aggregation, right? You need to have some actor in the middle that it basically takes all of these objects and replaces them with an object that proves the existence of all of those objects. So this is all really nice. This all increases scalability, and this is all really good. Right, so what does this all depend on? It requires a builder ecosystem.
00:11:11.650 - 00:11:59.234, Speaker B: By the way, who here remembers the cartoon character that this meme is based on? Bob the builder. Yay. Oh, no, he gets see, Bob the Builder needs more love. We have Barbie these days, and we have what else? Yeah, there's all the Barbie songs, and we need a Bob the Builder movie. Can we do like, an OD chain gitcoin bounty for a Bob the Builder movie? And then use Zoo Poll to vote on the best r1 world crypto applications, please. We need builders, right? Builders are the heroes of ethereum block production. Right, okay, fine.
00:11:59.234 - 00:13:06.854, Speaker B: I'll see. I wonder if this style of mic holding is better. So the block building ecosystem is actually really big, right? We all kind of know that Bob the Builder is an abstraction. And in reality, there's like dozens of construction workers, and then there is the people driving the trucks, bringing resources over to the workers, and then there's the janitor, and then there's a whole bunch of other pieces of infrastructure, and there's like, really big teams that make up a construction company. Right, so black building ecosystem looks pretty similar to that, too. Right? So basically, you have a bunch of searchers, and searchers all have their own different strategies for creating bundles that contain different types of transactions, and then they all go together to a block builder, right? And then the block builder creates a block and then publishes it today to relays in the future directly to validators. And this is roughly how things work right now.
00:13:06.854 - 00:14:56.906, Speaker B: Right, and so one natural question to ask is, well, if we have this kind of ecosystem, then who's going to do the Snark aggregation or the BLS aggregation or ERC four, three, seven aggregation or all of these things? The most natural guess for the long term is, like, we'll have specialized searchers, right? So we'll have like, a searcher that speaks the 4337 protocol, and then we'll have a searcher that does BLS. We might have a searcher that does a Snark aggregation. They might even end up talking to each other. There might even be some multilayer ecosystem where first you do the aggregating for the 4337 or for the BLS and the synarx, and then you put that together into a bigger four, three, seven aggregate, and that goes into a block along with non four, three, seven stuff. But I expect it will move towards something specialized like that. But I also expect in the shorter term, there's definitely going to be builders that just do this stuff directly, right? And I expect going to be a growing array of these aggregation subprotocols that gather these kinds of aggregate messages. So challenges in these ecosystems, right? So we want an open mempool, meaning a protocol that anyone can participate in, right? Not open in the sense of anyone can see your stuff and use advanced machine learning algorithms to figure out when okay, what should I even say here? Figure stuff out about you that you don't want.
00:14:56.906 - 00:15:57.422, Speaker B: Figure stuff out about yourself. We want a protocol that anyone could participate in for both users and for builders and searchers, right? We want it to be open for users to send in their user operations. We want it to be open for searchers to participate, and we want it to be maximally open for people to bring in new aggregation protocols. Now, that does require them to kind of accept permission from one builder or one searcher, but it should not require global permission from the yet ecosystem. So that's a goal. And we want to try to avoid centralization, and if centralization happens, we want to avoid censorship. Now, one of the challenges is that economics is weird, right? So in particular, the issue here is that a lot of these types of aggregation that I talk about, they have a high fixed cost and they have a low marginal cost.
00:15:57.422 - 00:16:51.242, Speaker B: So what do I mean? What I mean is that if you think about, like, Snark aggregation, right? If you put a Snark on chain, the cost of a Snark is about 500,000 gas. Now, that Snark can represent lots of things. If that Snark represents one thing, it's 500,000 gas. If the Snark represents 100 things, it's also 500,000 gas, right? It's fixed cost. Now, the other thing part is that it has a low marginal cost. The marginal cost does exist, right? Because if you're a prover, then you have to run more proving cycles, right? Like, if you have to prove five more things, the cost of doing the proof is five times bigger. Now, this is a cost, but generally these off chain costs are, I believe, already lower than on chain costs.
00:16:51.242 - 00:17:58.710, Speaker B: And I expect these costs to continue to trend lower as we have more specialized proving techniques, as we have more specialized hardware and all these things. And I expect gas costs to be much harder to reduce, right? The amount of gas on the ethereum, l one has increased by a factor of five over the course of seven years. But the efficiency of provers has increased by a factor of like, thousands over the last maybe four years, right? So when you have things that are high fixed cost and low marginal cost, like economics get annoying, right? It's like public transit systems, right? Like when you have a fixed cost to run a bus. And sometimes it might even happen that the system can't pay for itself, even if it makes total sense for the system to run, because the system has to charge one price. And it turns out that if it charges a high price, it excludes too many people, and if it charges a low price, it doesn't capture enough of the value. Right. And you're stuck, right? And so you have weird things like this.
00:17:58.710 - 00:18:55.254, Speaker B: And unfortunately, these kinds of fixed costologial, marginal cost economics do tend towards centralization. And sometimes centralization may be an argument for saying we want to give up on this whole higher layer idea and we want to just protocolize it. But the problem is this stuff is so early that it doesn't make sense to protocolize it. And how do we navigate that trade off in the meantime, right. Is basically the challenge. Another really fun use case proof aggregation for ZK roll ups. So let's say you have a roll up, and who here is part of a roll up team? Who here is part of a roll up team different from the roll up team that the person over there is part of? Who here is part of the roll up team that's different from both of these roll up teams? Okay, I see a few raised hands.
00:18:55.254 - 00:19:40.838, Speaker B: Yeah. So we have at least three roll up teams here, right? So the problem is each of these roll ups are going to have to submit proofs on chain. And submitting a proof on chain is itself something that has a high cost, right? So if you look at a ZK roll up, it has to submit a ZK Snark on chain, 500,000 gas. If you're a Stark, then it's like closer to 5 million gas. And this is expensive. What if we had multiple roll ups? And these multiple roll ups could cooperate and basically replace all of their individual proofs with a proof of the proofs, right? So you just have one proof that proves that all of the proofs that they want to submit are actually correct. Right.
00:19:40.838 - 00:20:24.858, Speaker B: So this is kind of what the diagram says. You have three different roll ups and they make three proofs of three state transition functions. And then you just make this kind of multi proof, which just submits as a public input the claims that the three proofs that are coming in are making. And then it sends it off to a batch handler. And then the batch handler basically just tells the roll ups like, hey, yo, dog, I have a proof of this, I have a proof of this, I have a proof of this. And the roll ups accept it. Right? So this is a kind of layer two middleware protocol that could actually be really useful and it could be like a lighter way to achieve the goals of reducing costs for layer twos that's lighter than layer threes and some of those similar ideas.
00:20:24.858 - 00:21:15.246, Speaker B: And it's more minimalistic. It can be made to be more permissionless. I believe within the Starquare ecosystem, there's a version of this running already, right? But it would be good to have this as a potentially protocol that's kind of shared between an even larger piece of parts of the ethereum layer two ecosystem. And this is again a type of you're supposed to say the word yay. Okay, perfect. So the goal of this is the proof singularity, right? So basically my dream is we have a block and that block contains a proof. And the proof is a proof of proofs.
00:21:15.246 - 00:21:41.322, Speaker B: And each of these proofs is itself a proof of proof of proofs. And each of those proofs is another proof. So imagine the proofs at the end could be just signatures. Then some of them are signatures, some of them are privacy protocols. Then the proofs in the middle are going to be aggregation proofs. Then the proofs on top of those are going to be layer two proofs. And then with the layer two proofs, you have a merged protocol.
00:21:41.322 - 00:22:38.430, Speaker B: And then it all gets into one proof. And then maybe over and above that, you have one big master proof to prove them all. And that one big master proof is just going to be a zkvm, right? So this way we might actually be able to get to the point where we literally have blocks coming in once every minute that prove everything. And everything in the ecosystem is going to be super efficient and super optimized, right? So this is what I hope for the ecosystem and it would be amazing to see. And builders are going to be a very important part of making this all happen. So this concludes my presentation. Thank you all for listening.
00:22:41.650 - 00:22:55.640, Speaker A: Thank you. Thank you for the meme. And we have time for one question, but I could also ask our next speaker to ask the question. If you have one.
00:22:59.690 - 00:23:24.640, Speaker C: Extremely exciting talk about the future of block building, my question would be these new protocols, how should implementers be thinking about the complexity of the implementations and about shipping them in a timely manner that can fit, that can make sure that we don't end up in a dystopian centralized future?
00:23:25.250 - 00:24:11.980, Speaker B: How do we think about implementing these protocols? I think just standardization is important and cross ecosystem collaboration I think is important. So I feel like we've been having a lot of that between layer twos, which is good. I would love to see more of that happen between wallets. I would love to see more of that happen between privacy protocols. I think we just need to identify some of these kind of verticals and I think it might even be the case that per vertical, we might need an intentional effort to try to really figure out what actually makes sense to do and to try to sort of front run some of the more centralized efforts solving the same problem. It.
