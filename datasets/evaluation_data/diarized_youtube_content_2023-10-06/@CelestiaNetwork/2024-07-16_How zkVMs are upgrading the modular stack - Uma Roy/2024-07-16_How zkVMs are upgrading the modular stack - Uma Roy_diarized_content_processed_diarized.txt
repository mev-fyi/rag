00:00:01.920 - 00:00:32.040, Speaker A: Hello, I'm Uma. I'm one of the co founders of Succinct. And today I'm going to be talking about how zkvms will upgrade the modular stack. So let's look at the modular stack of today. We have a lot of different execution layers, we have a lot of different choices of Da layers. We have a lot of different choices for settlement, for bridging. The beauty of the modular thesis is that you can have this flexibility and freedom, but it also comes with trade offs.
00:00:32.040 - 00:01:08.874, Speaker A: And I think a lot of what's being discussed today is the UX implications of having all these heterogeneous environments. Today, users face really difficult interoperability between all these different ecosystems. It's difficult to bridge. Liquidity can be fragmented. And overall, the UX of the modular ecosystem today needs some work. And I think that ZK is basically the only way that all of these problems will actually get solved with ZK. I think in the fullness of time, every rollup will be a ZK rollup, every bridge will be a ZK bridge.
00:01:08.874 - 00:01:56.644, Speaker A: And all parts of the modular stack that can use ZK will use Zk and solve a lot of these problems. And today we have words for this that people are working on things like chain abstraction, unified liquidity, aggregation, the integrated thesis, interoperability. And all of this terminology fundamentally has ZK underneath, if ZK was actually to be used. So ZK seems really powerful. It is the fundamental base of how the modular ecosystem will be connected and talk to each other and in the end develop, have a good ux for end users. But historically, the truth is that ZK adoption has kind of lagged behind. For example, if you look at Ethereum, the top two Ethereum roll ups right now are arbitrum and base.
00:01:56.644 - 00:03:01.474, Speaker A: And both of those are built on optimistic, fraud proof technology, not ZK. So ZK has a ton of promise, but there isn't a lot of adoption. Why is that? Historically, one of the big reasons is that ZK is really slow and very expensive. So if you've built a ZK before or have friends who've done that, you've probably seen a picture like this where you have to generate ZK proofs on a very big, very expensive AWS server with many cores and a lot of memory, and that takes up a lot of money. And then also proof generation, even with a really big computer, is still very slow. But I think what's even worse, more than the slowness and the cost is actually that the ZK developer experience is really, really bad. Today, if you want to build with ZK, whether it be a ZK rollup or a ZK bridge or any other application, you have to have a team of 30 math PhDs or cryptography PhDs, and a bunch of really smart engineers who know weird arcane languages like circum or Ponkitoo or Cairo.
00:03:01.474 - 00:03:39.444, Speaker A: And then they have to code in these languages that basically look like assembly with no print statements and really horrible developer tooling. And they have to understand all these math papers behind the scenes. And this slide looks chaotic and really horrible because that is the ZK developer experience of today. And because of these things today. If you want to build with ZK, the time for production takes months, if not years. And you can actually see this in many ecosystems where the optimistic rollups went to prod a couple years ago, but then the ZK rollups took a few more years after that to fully launch. And why is this the case? Well, it's a lot of what I was talking about before.
00:03:39.444 - 00:04:18.438, Speaker A: Right now you have to have a very specialized team, and there's probably less than 1000 people in the world who can write all this DK stuff. And then even if you have an expert, the developer velocity is extremely slow. You have to learn these custom languages. SDKs, the developer tooling, is super primitive, and it is very similar to writing a very low level language. Once you've done all of that, the end result is unmaintainable. For example, in Ethereum, Ethereum goes through a hard fork or upgrade every six months to every one year. And so if you want to be an Ethereum compatible roll up, you also have to upgrade your logic with the current ZK tooling.
00:04:18.438 - 00:04:57.924, Speaker A: This requires a huge engineering lift every single time. Furthermore, if other teams want to use your product and customize it, it's very difficult. Today you have a lot of teams taking the op stack and building their own rollup, and they often add their own precompiles, which they can simply do by writing an op geth and writing some go code. But if you have a ZK rollup and you want to customize it, good luck. It's going to be very difficult and not really something that's possible for an external team to do. And then finally, once you've done all of this stuff, you also really care about security. Now, unfortunately, today the ZK code bases are often hundreds of thousands of lines of code.
00:04:57.924 - 00:05:18.428, Speaker A: The audit surface is extremely large. There's very few firms that can actually understand it, and so it's very expensive and time consuming. To audit all this stuff and make sure it's actually secure and be ready to use in prod. So it's succinct. We've been building with ZK for a long time. We really love ZK. I think we've seen that it has tremendous potential.
00:05:18.428 - 00:05:49.188, Speaker A: It seems super foundational. And especially with the modular thesis, I think it becomes even more important to connect all these ecosystems together as we are getting thousands of roll ups. And so we really wanted to accelerate ZK. We saw that the ZK experience of today was really bad, and that's why we built SP one. So what is SP one? SP one is an open source general purpose performant. Zkvmdev. There's a lot going on there, so let's break it down.
00:05:49.188 - 00:06:28.050, Speaker A: So what is a general purpose DK VM? What it allows developers to do is developers can just write normal rust code and then get a ZK proof. So before there was this kind of nightmarish world of circuits and specialized languages, none of that anymore. You just write normal code and then it just works kind of magical. The technical architecture underneath SP one and most DK Vmsheen is that you have rust and it compiles to a normal ISA called RISC five. It's a reduced instruction set. It's a standard target of the rust compiler. And then our ZKVM SP one proves the correct execution of the RiSC V ISA.
00:06:28.050 - 00:06:55.722, Speaker A: And then underneath the hood we use starks and small field baby bear fry and log derivative lookup arguments and a bunch of other stuff. I could probably spend hours talking about that. But kind of the whole point of a ZKVM is you guys don't have to worry about that stuff, only we do. SP one is also really performant, which is very important. General purpose is obviously super great, right? Like write rust, get proof. It sounds awesome, but if it's not practical, no one's actually going to use it. And so it's very.
00:06:55.722 - 00:07:38.030, Speaker A: Performance is extremely important. And we've actually found that with SP one, the proving time and the proving cost can often be better than the customized circuits that people have written in the past. And there's been many algorithmic innovations that our team came up with for our state of the art performance that I'll go into in a little bit. And then finally, SP one is also fully open source. So all the constraints are open source and you can check them out yourselves, which is really critical for security and auditability to understand exactly what's being proven. It's also modifiable and customizable by external teams, which is really important. And we've already had several teams customize it and make different parameter choices or add their own precompiles, which I'll talk about in a little bit.
00:07:38.030 - 00:08:11.200, Speaker A: So, okay, I've told you about what SP one is, but let me show you how easy it finally makes ZK. Here is an example of a very simple fibonacci program written with SP one. As you can see, it looks like normal rust code, and that's kind of the entire point. With SP one you can use the rust standard library, so you can use vexdae and all that nice stuff. Use rust totally normally. You can use existing crates and libraries, so you can use Serda, you can use JSON, you can use rust crypto and all the cryptographic libraries. You can use things like ReVM and ref.
00:08:11.200 - 00:08:54.918, Speaker A: And I've personally found that developing with SP one leads to 100 x developer velocity and productivity. You have things like print statements and for loops and if statements, which sounds very normal. But if you've ever tried to use ZK before actually is kind of a breakthrough. And you can just have all your rust tooling and you use all these existing libraries and it feels very much like a normal programming paradigm. It's super easily maintainable and customizable. Right? External teams can just take existing implementations and then modify them when they need to. And then I think one really cool aspect is that the audit surface area is much smaller because we can reuse all these existing crates and libraries that have already been audited.
00:08:54.918 - 00:09:37.902, Speaker A: For example, ref, which is a rust Ethereum implementation, recently had their 1.0 release and they were audited. And now you can just take ref, import it into SP one. And then there's those thousands of lines of code don't need to be audited by your team because they've already been audited and used in production, and you're just good to go. So by making maximal use of existing node software and audited libraries, your life as a developer and time for production gets a lot easier. And so I like to say with SP one, using ZK goes from taking months and years to just being a weekend project. As I mentioned previously, another really important aspect of SP one and general ZK VMS is performance.
00:09:37.902 - 00:10:24.550, Speaker A: So being really easy doesn't mean anything if it takes several days or several hours to generate a proof. So at SP one or at succinct, we came up with several algorithmic innovations to make SP one very performant. I could again spend hours talking about this stuff, but, you know, a little preview. We have like novel memory checking arguments. We make heavy usage of lookups to really minimize the amount of proving overhead. But our main innovation that I want to touch on here is our precompile centric architecture. So one of our key insights while building SP one is that most cryptographic use cases that people care about for ZK, like blockchains or roll up state transition functions or bridges.
00:10:24.550 - 00:11:09.840, Speaker A: Most of the prover overhead is spent in repetitive computations like hash functions and signature verification. So that takes up a lot of risk, five cycles. And that leads to really long prover times. We thought, what if we could replace that with kind of a ZK native assembly? Or we have a specialized circuit for things like catch act or ShA or ED 25519 verification that talk to our main cpu and save time and prove our overhead. And we found that when you do have these specialized circuits, and it's hard to kind of get all the engineering right to make this work. Well, once it does work, it's actually really effective. You have programs that used to take billions of cycles go down by a factor of five or six, and in some cases even by a factor of ten or 20.
00:11:09.840 - 00:11:43.260, Speaker A: SP one currently has state of the art performance, often by an order of magnitude. And our precompiled centric architecture is really key to that. If you reduce the number of cycles by five x or ten x, that's an automatic decrease in prover time by the exact same amount. And then more recently we've been working on SP one GPU, which is a GPU implementation of our prover. In general, the proof systems that we use, they're very, it's a very embarrassingly parallel workload. There's a lot of hashing that can be done in parallel. And other parts of the prover are also very similarly parallelizable.
00:11:43.260 - 00:12:20.390, Speaker A: And our GPU implementation improves things quite significantly as well. So in all, SP one lets developers write normal code without sacrificing performance. So it's pretty awesome. But I don't want you to just take my word for it. I also want to show you with an example that we had at the. So at succinct, we have been working with the Celestia team for quite a while on making a tendermint lite client to bridge from Celestia to Ethereum. And originally we were working on this tendermint lite client implemented in circuit world, using a custom library called Plancky two.
00:12:20.390 - 00:12:37.010, Speaker A: And this is what our circuit code looks like. It looks pretty ugly. You can tell. It doesn't look like normal rust. We're using this DSL and it's pretty complicated. It's convoluted. And by the way, this is just like one small snippet of one file.
00:12:37.010 - 00:13:11.870, Speaker A: There's actually many, many files like this that we wrote. In total, I think it took us three months of time and thousands and thousands of lines of code in this really weird DSL that we had to write a whole tenurement like Klan in. It was very painful, and I can personally attest to that because I worked on this. Okay, now, recently after ASP one, we built the exact same thing. And the code that I'm showing on the screen is the entirety of the tendermint lite client. And it shows up in one file on the screen, which is pretty awesome. And so I'm going to walk through kind of the various pieces.
00:13:11.870 - 00:13:42.640, Speaker A: So first, to make a tendermint lite client in SP one, what do you do? You import the tendermint library, which is pretty awesome. The tendermint library, by the way, was not made by our team. It's maintained by another team, the cosmos ecosystem. And we can just import it and it just works. And then we import another library alloy that's maintained actually, I think in part by the next speaker, James, and some of the paradigm team. And it's also really awesome and it just works. Okay, then our main body of our function is exceedingly simple.
00:13:42.640 - 00:14:04.820, Speaker A: We just read in two blocks. We surday, deserialize them. We instantiate a verifier from the tendermint library. We call this function verify update header that does all the checks. The library has already been audited, so we can know that the code is good. And then we make sure that the verification passes. And in all, I think this ends up being like, looks like 30 lines of code.
00:14:04.820 - 00:14:33.176, Speaker A: And then finally we take the blocks, we hash them, and then we use the alloy types to basically get a solidity structural. We abi encode it and then we expose it as a public output of our proof that we want to use on chain. So let's look at the numbers. Okay. On the left hand side, we have our old system tendermint like client with circuits. As I mentioned, it took three engineers on our team five months and ten. It was very painful.
00:14:33.176 - 00:14:49.058, Speaker A: It was thousands of lines of code. And the audit costs were actually pretty high. We spent a couple hundred k all in for the whole system. And it also took a really long time to audit it. So once we got done, we couldn't use it in prod right away. We had to wait another month and a half. And it's very complex.
00:14:49.058 - 00:15:17.372, Speaker A: There's many teams that wanted to fork it and use it because a lot of teams use tendermint and they were kind of unable to really do that because it was very difficult with SP one tendermint. It was a magical developer experience. We just imported this library, it's 50 lines of code. It took one engineer on our team 3 hours I actually think is maybe our intern. And the audit cost, I mean there's barely anything to audit, maybe takes one day. And it's extremely simple. It's super forkable and customizable.
00:15:17.372 - 00:15:57.372, Speaker A: And actually recently we hosted a workshop on SP one tendermint and we had 50 people show up. And now dozens of teams are taking it and forking it and experimenting it for their own use case or their own chain, which is super awesome. And I think this customizability is really important for the modular ecosystem, especially when you have this world where people are customizing things and really taking advantage of the flexibility of the modular paradigm. They really need this ease of use to customize things to their own use case. And that's very powerful with SP one. Now you might say, ok, well the developer experience on this side seems like 100 x better. So surely that comes up with some trade offs.
00:15:57.372 - 00:16:22.562, Speaker A: Maybe the proving time is really horrible, but that actually wasn't the case. And this was even surprising to someone like myself. So on the left hand side, in our old circuit world, we spent months and months getting the proof times down to ten minutes and the proof cost down to several dollars. And we were actually really proud of that. We'd worked really hard to get to that point. Now with SP one, we thought we could maybe never achieve those numbers. We were hoping to be within a factor of two or three.
00:16:22.562 - 00:17:21.084, Speaker A: But actually to my surprise, with SP one and all the optimizations and SP one gpu, actually the proof latency ends up being two minutes, so it's actually an improvement. And the proof cost actually ends up being significantly cheaper as well. So you might ask, why is that? And the reason is because SP one is so general and can support so many different use cases by so many different teams because it's so flexible and customizable. On our end, we can spend all this time and effort making a really optimized system and really making it as performant as possible because the ROI is so high. For example, in our old system, it would have never made sense to put our plonky two tendermint lite client on GPU, it would have just been way too much engineering effort for only one use case and one project. But with SP one, we know that every time we improve it by 10%, all the teams building on top of us also benefit. And then I also want to talk about something that's very, very relevant to the modular ecosystem, which is rollups.
00:17:21.084 - 00:18:03.940, Speaker A: I think the biggest use case of ZK in the next year is going to be ZK rollups. And I think all of them are going to be built with zkvms, hopefully on SP one. So for example, we ran ref within SP one and we benchmarked the proving time and the proving cost. And we found that the proving cost per transaction currently, and this is without the GPU stuff, which makes things better by a factor of five, is around one cent per transaction already. That's getting pretty cheap, especially when you look at how much ethereum roll ups are paying to Ethereum for DA today. And so I think in the future, all modular roll ups or all rollups in the modular ecosystem will be built on top of these ZK vms. And they can customize their execution environment to whatever they want, which is really powerful.
00:18:03.940 - 00:18:44.454, Speaker A: And to wrap up, I want to say that with SP one, ZK is going to go mainstream. Finally, it's easy, it's very fast, it's cheap and it's really flexible. And I think with that we'll finally have accessible ZK and that will allow the modular stack to have good ux because ZK will be easy enough to use in all the contexts that it needs to be used. And then that will lead to great interoperability. All rollups being ZK rollups, they will all be connected and talk to each other, and the liquidity will be unified, the ecosystems will be united. And finally users will have the good experiences that will be really good for the entirety of the space. That's all.
00:18:44.454 - 00:18:44.790, Speaker A: Thank you.
