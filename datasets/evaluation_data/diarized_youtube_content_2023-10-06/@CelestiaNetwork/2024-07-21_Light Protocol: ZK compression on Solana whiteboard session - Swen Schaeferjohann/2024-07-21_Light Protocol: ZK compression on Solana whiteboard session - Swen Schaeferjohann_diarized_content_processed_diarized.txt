00:00:02.160 - 00:00:18.690, Speaker A: Hello. Welcome. In this whiteboard session, we're going to be talking about ZK compression on Solana. And I'm joined by Sven, co founder of Light Protocol, who will be presenting.
00:00:19.990 - 00:00:44.640, Speaker B: Awesome. Yeah, thanks for the intro. Pleasure to be here as well. Thanks for the invitation. As he said, my name is Sven, I'm one of the co founders of Light Protocol. And today we're going to dig into ZK compression on Solana. And I want to start with.
00:00:44.640 - 00:01:41.030, Speaker B: Well, I'm sure you've seen the debate on Twitter in the last three weeks for context. So we released ZKN compression three weeks ago, and it sort of created a debate about sort of naming nomenclature, mainly between ethereum researchers and mostly Solana enthusiasts. And for context. So my goal here is to really make sure that you kind of get away with knowing what ZK compression actually is on Solana. So that's a little scope I'm gonna. So the plan for this session is to sort of start very high level and then sort of recursively go deeper into more technical stuff. And then somewhere along that road, hopefully, everyone will sort of get the right takeaways.
00:01:41.030 - 00:02:13.420, Speaker B: Cool. So, basically. And I'll give a little. I'll start with. I'll try my best. You know, we're building on Solana, um, which sort of historically has been branded as sort of, uh, non modular, or people sort of tried to sort of create sort of a difference. I actually personally think it's very much orthogonal to, you know, sort of a modular system versus something like that is more integrated, like Solana, where you have sort of different layers of a stack all integrated into one.
00:02:13.420 - 00:03:26.944, Speaker B: But, you know, to try to give some analogies here. So what is liprocal? What is ZK compression? You can sort of think of protocol as something in between probably ZK accounts and the ag layer in the spectrum there. And then with ZK compression being sort of the core primitive underneath that, as something similar to based inscriptions using call data blobs, with the purpose of reducing data storage costs. And so I'm going to. And it's a little bit unique. So, you know, with lyproqual being sort of a scale scaling and interoperability layer on Solana, it's a little bit different, because, like on Solana, you have different challenges, a different problem set when it comes to scaling than you'd have sort of in the EVM space or in some of the other parts of the modular stack. And so with that, what I mean is, on Solana, Solana actually distinguishes between compute and data storage and state, which is a little bit different than you'd see it on the EVM, for instance.
00:03:26.944 - 00:03:58.604, Speaker B: And specifically, Solana is very cheap with regards to compute. So execution on Solana is incredibly cheap. It's also mainly because it's not fairly priced. It's just a very low flat fee, um, as a base fee. And then data storage is quite expensive actually on Solana. And so, um, uh, I want to give sort of an intro to why that is. And, um, so I'm going to start with sort of the account model of Solana.
00:03:58.604 - 00:04:35.050, Speaker B: Let me actually pick black marker. Um, accounts. Accounts are how Solana deals with state. On Solana, you can think of it as everything is an account. So it's like with Unix system, everything is a file, everything is an account. So what it basically just means is that you have a slot, a portion of bytes that are being allocated and attached to an owner. So you have 32 bytes as the owner key, and you have and pretty arbitrary data key.
00:04:35.050 - 00:05:32.508, Speaker B: So you could think of it as this bucket of state. And all accounts are these containers of state. And it's a little bit like sharding, just that when you have transactions on Solana, you can define many different accounts that you want to read from and write to. And that's how you get this sort of smooth interoperability. And this sort of account layout where you have owner a and owner b, et cetera, et cetera, has a lot of nice benefits. And one benefit is what Solana is known for, is you have these parallel rights that between different, across different accounts that you can do. And so you can evaluate account changes in parallel, which is quite nice.
00:05:32.508 - 00:06:34.688, Speaker B: But basically what happens is when I want to create a new account, and I will give an example. So if I want to send you all, I make a new token, I'll send you all some tokens, I basically have to go and I have to allocate a new account for each of you. And this allocation of bytes in this account bucket, it's quite expensive, and it's sort of like, and the reason for that is that Solana, each validator in Solana, has to keep the current state of all the accounts in active memory at all times. And there's millions of accounts being created each day on Solana, and it sort of blows up Ram, for instance. And it's quite expensive on the whole network to keep that state around. And so this cost trickles down to the user as well. So what Solana does is they charge, they call it rent, but it's sort of like a bond that you put up front, you put it up.
00:06:34.688 - 00:07:17.868, Speaker B: And if you have, for instance, a token account, it's I think, roughly 165 bytes. You know, if you have token roughly 165 bytes, that's at the current price, depends on the sole price, but it's currently slightly less than $0.50. So it's like 25 token account right now. Now you can imagine if you have millions of accounts, that's quite expensive. On the contrary, with compute, it's very cheap. So you could actually airdrop a million tokens to a million different wallets. And the compute for that would be like five Sol, which is 600, $700.
00:07:17.868 - 00:08:16.936, Speaker B: So it's quite cheap compared to the hundreds of thousands that you'd pay just for the state allocation. And so this is quite a high cost, especially for apps on salon that want to grow to a large audience of users. Maybe you want to do huge airdrops, et cetera, et cetera. And so the core innovation, I would say, or the core thing that ZK compression, as a primitive does, is it allows solana to reason about state and data that is not actually stored in on chain accounts. So you can store it, you can commit to it on chain and then can store it somewhere else. And then when you do a transaction, you want to modify that data, read from it, write to it in your Solana programs, you can pass it along in your transaction payload, provide a validity proof that the state was previously committed to, and then go ahead and use that as state. But it's not actually stored in on chain accounts.
00:08:16.936 - 00:08:44.140, Speaker B: And so this is quite powerful because essentially it removes this rent cost to near, reduces to near zero, basically. And the way that this works is now you. And that's basically, that's the compression. So the core primitive, it's not about staling compute, about scaling data storage and dealing with state growth. You can think that, ok, this is great for users, great for developers who want to have apps that touch a lot of users.
00:08:44.840 - 00:08:50.608, Speaker A: You said that the data is stored somewhere else. Where is that?
00:08:50.784 - 00:09:29.638, Speaker B: Yeah, that's actually pretty, the protocol is pretty agnostic to it. But with ZK compression, what most users will do, and sort of the default, what it does is stored on the Solana ledgere. So you can think of it as in the ledger history, so you can think of it as call data or blobs, where. And I'll give some context, it's a great question, actually. I give some context about that. On Solana, as with on other chains, the ledger history, the historical state of accounts is not actually kept around, it's pruned after a couple days. And so the validator just keeps around the current state of all the different accounts.
00:09:29.638 - 00:10:36.616, Speaker B: Now, what that means is the validator cannot really, by default, cannot reason about historical state on Solana or any state that is emitted through these call data like functions. But what you can do is you can index it, sort of like how you would index inscriptions or some of these. Yeah, runes on bitcoin, something like this. You can index it, keep it around and provide it with an RPC provider, for instance. And that's how you keep track of it. There's this element that. Now, what are the security considerations of storing the data in the historical state of Solana that's actually pruned by the validators? Thing is that, well, as long as you have at least one archive node around, you can always replay the whole ledger and then re index it.
00:10:36.616 - 00:10:46.272, Speaker B: It's quite a sort of, it's a more complex process to re index the whole ledger, but the security guarantees are there. So as long as you have the archive around.
00:10:46.296 - 00:10:53.700, Speaker A: So, yeah, would I need to have a archive node for Solana or just for that application itself?
00:10:56.200 - 00:11:34.720, Speaker B: You'll have to index the, well, you need to index the merkle tree that your application stores the data in. And there's a couple of nuances. For instance, what the protocol allows you to do is it has these bigger shared trees that you can have and everybody can use them, but you can also have program owned merkle trees that only your application is allowed to write into. So that sort of depends on what you decide to do, but you sort of just have to figure out a way to keep around the merkle tree.
00:11:34.840 - 00:11:58.930, Speaker A: And when I have my state, that's the things that I'm changing wherever I, I assume the root and the proofs over will be this one bucket. Could I just save that state for the application if I already indexed it and then just snapshot it from that point?
00:11:59.430 - 00:12:32.446, Speaker B: Yeah, you could do snapshots. We haven't built that out. So for context, the whole. So Helios labs, so we've partnered with them, they're building the open source index implementation of this. And snapshotting is not something that currently exists, but it's definitely something that would be extremely valuable to have, for sure. Yeah. So with regards to how ZK compression works here, there are a couple of pieces here.
00:12:32.446 - 00:13:34.950, Speaker B: And basically what we're doing, we're moving the state that is stored here, we're moving that onto the ledger, which is, let's say this is the ledger, and then we have each account state, right? So the account, if you look at it, the account has like an owner, it has a data field which you know, can be written to and is owned by sort of an external guest program. This would be your custom smart contract. And then there's also like a lamp words field. And then obviously this, this key has like a public key and a corresponding private key. And then if it's program owned, it's actually an off curve address that's being derived. But now all this state is basically hashed and moved into a mercury structure as a leaf. So like the hash of this, let's say this is account a, the hash of this, the current snapshot, is stored.
00:13:34.950 - 00:14:18.530, Speaker B: The hash is stored on the ledger, and then the underlying state is also emitted onto the ledger. But this can be emitted to anywhere. Importantly, the hash is stored in ledger and then all the other account states. This could be the same account after a different transaction, a different state transition. You have this huge mega tree, and then they just basically just hash up. And then this here, the root, it's a typical binary merkle tree. We have some modifications to it that make it concurrent.
00:14:18.530 - 00:14:55.716, Speaker B: And then also for storing the actual addresses to guarantee uniqueness. We have something that's called an index merkle tree, which is also a slight modification. It's a pretty cool, pretty cool modification that I think Aztec is also doing for some of their state. Now, this root has actually stored in debug and it's referred to byte hash. The bucket then has keeps around a root history for a certain amount of time. So allow for some concurrency. But essentially what you see on chain is just a small fingerprints of all the underlying account states.
00:14:55.716 - 00:15:29.040, Speaker B: Now that is sort of fundamentally what we do. It's important to know that on Ethereum, all account states are localized. On Solana. That is not the case. So this here is sort of like we've added that on top of the light protocol. And this state is hashed using a Poseidon hash function. So this makes it quite powerful for some of the other things that we can talk about towards the end of it, which goes more into the ZK account thing.
00:15:29.040 - 00:16:29.590, Speaker B: But essentially that's what it is all about. Now, when you have a transaction that interacts with compressed accounts, let's say you have, this is part of the protocol on chains, smart contracts. Let's say you have your custom smart contract here. This can be sort of an on chain STF. Let's say it's an escrow program, right? What you do is you actually, let me do an example here. Let's say you have a regular token account on Solana. The default account layout, the standard is called SPL, smaller program library token, which sort of just puts a specific layout into the data field, right? Sort of what's the mint, what's the owner of the account of the token, what's the balance and some other fields.
00:16:29.590 - 00:17:26.941, Speaker B: Now let's say you want to compress your token account, that you have your Solana token account, and let's say you want to compress it for your custom program. What happens is you have your token account here, the user sends it on chain, sends the address of the token account on chain. Now the instruction would say I want to compress this. So what happens is here we have an escrow for each mint. You can just create an escrow account, sort of like a pool account. And then the protocol mints the representation of that token account as a state into the merkle tree. And then you could say ok, so that's compression, and then you can deal with that with these state transitions.
00:17:26.941 - 00:18:19.000, Speaker B: And then you can also decompress, which would be the reverse order you call the decompress instruction. And then the state de mercantry representation gets nullified and the tokens get released back into an actual on chain token account. So that's quite a common flow compression decompression. And essentially later it's sort of like when you deposit into a roll up. Um, it's actually quite similar where the roll up keeps around. Uh, you know, if you have a roll up that is not sovereign, usually have like sort of an l one. Let's say on Ethereum you have an l one contract that is an escrow and has a keeps Merkel routes on chain and just like transactions or these rollup etches just advance the state route.
00:18:19.000 - 00:18:59.770, Speaker B: Um, so that's the, that's the overall flow. Um, now if I want to, if I have stayed in this Merkle tree as we previously touched on, I need to request that ledger data from an indexer node. So you have an indexer that keeps this state around. It's nonvoting validator, basically just indexes all those state transitions and can then provide the compressed account. It's an extension to the JSON RPC endpoint that Solana provides.
00:19:00.070 - 00:19:17.210, Speaker A: And the Merkle tree again is, you could say the state and the transactions itself where would be on the ledger? Or is the Merkle tree, the merkle tree over the transaction bytes.
00:19:18.550 - 00:19:43.924, Speaker B: The merkle tree is really just over the actual account state. So it's like snapshots and you can think of it a little bit like Utxos, where whenever you have a state transition, the old state, the current state of the account, let's say compressed account a, gets nullified, so zeroed out basically. And then the new state gets appended to the tree.
00:19:44.092 - 00:19:45.460, Speaker A: So it's append only.
00:19:45.620 - 00:20:28.602, Speaker B: Yeah, so this is append only. But then there's the nullification process. I'll get into that. So basically what happens is that when you, this process of the user interaction, there's actually a second process. And this second process we call foresting, mainly because it just deals with sort of this forest of many different local trays. Basically what you have here, let me use this here is you have accounts, you have some more accounts, there are queues. So regular on chain accounts.
00:20:28.602 - 00:21:09.152, Speaker B: Importantly they're fixed size and so you don't actually grow the state that is on chain. So what happens when the user, let's say the user wants to spend a compressed account, they need to somehow nullify that account. But notifying this account and zeroing it out is quite costly. Just because you need to advance the state route, you need to provide a Merkel proof or a ZKP. So what we actually do is we divide into separate parts. So all you do is you just insert the hash of your accounts that you want to nullify in your transaction into this queue. You append it to the queue and then they just stay in the queue for now.
00:21:09.152 - 00:22:04.080, Speaker B: Right. It's sort of immediate finality of the state transition because it's an on chain account, even though the actual Merkle tree is not yet updated and the compressed account that you spent is not actually yet zeroed out. But this is how you print double spends basically. And then there's multiple queues and there's one other queue for. If you want to create new addresses now with addresses, for instance, you want to have an NFt that has a uniqueness guarantee about it. The way that you can reason about uniqueness in these off chain data structures is you need to do an exclusion proof over the address space. And the tricky thing is you don't want to keep around a height 256 merkle tree because it's large.
00:22:04.080 - 00:22:44.612, Speaker B: So what we do is we use this index Merkle tree structure where the actual leaf is sort of like a linked list of exclusion ranges. And basically what it allows you to do is you can prove exclusion over the address space. In our case it's 248 bit with a much smaller Merkle tree. And so arbitrarily small merkle tree. And for that process of updating the address Merkle tree, we also utilize the queue structure so the user just like puts it in the queue and then there's an asynchronous foresting process that takes care of actually emptying these queues and advancing the state routes.
00:22:44.756 - 00:22:49.692, Speaker A: The queue, is that still on Solana chain. Okay.
00:22:49.756 - 00:23:53.610, Speaker B: Yeah, so it's sort of like base sequencing at the core, the ZK compression core, primitive sequencing now and I'll get into some of the sort of extensions or adjustments you can make to allow for sort of more interoperability between batches of compressed account changes and transactions. You can do that as well. But this is based, so this is all the ordering here is enforced on chain. Okay. So the off chain process, so this is the quite important, we call it foresting. So there's like configurable as a keeper node, a server that you can run that tracks these queues. And whenever it sees a queue that it is allowed to empty, then it goes ahead and sends the form transaction onto the chain, which basically empty secure.
00:23:53.610 - 00:24:36.090, Speaker B: And the same step nullifies the state routes or updates the extrusion ranges for the addresses that it inserts. And fundamentally what it can do is it can also do it with output states. So with new accounts and these nodes, the process is permissionless. For if you have your own state tree, you can self host, you can choose to do that. But importantly, if the queue is full and you don't forest it, you get a liveness failure, because these queues have fixed size. So safety is enforced by the l one. So it's not really a sequencer.
00:24:36.090 - 00:24:47.672, Speaker B: We use the l one sequencer for that. But there could be a liveness value. Or if someone just stops emptying these queues and advancing the state routes, the respective ones.
00:24:47.736 - 00:24:51.340, Speaker A: And by emptying it would be completely removing it from Solana state.
00:24:52.440 - 00:25:39.796, Speaker B: Yeah. So this is like a temporary cache of account hashes, right? So these compressed account hashes, 32 bytes each. And then when you have a transaction, just like with the, as we said in the beginning with the Solana accounts, access lists, basically. I think it's on Ethereum. I think there's a proposal about this as well that has been discussed. But you can also define many different compressed accounts across many different merkle trees in your transaction that can either read from or write to, which is quite nice, because then in the end what it allows is interoperability between different state trees in sort of this atomic transaction. Now ultimately this is limited.
00:25:39.796 - 00:25:57.692, Speaker B: Like there's a limit to it, mainly by a transaction size limit, which on Solana is quite small. It's 1.2 kb. But you know, you could figure so modifications with transaction bundles that are not enshrined, but you could add them. Yeah. Have you guys had any use cases come to you yet that have been.
00:25:57.716 - 00:26:01.400, Speaker A: Limited by that, or like what types of use cases have limited that transaction size?
00:26:03.020 - 00:26:50.586, Speaker B: So currently we don't really have that. We don't hit that constraint for sort of the most sort of the use case that we focus on. And I'll give the context here. So the use case that ZK compression, as the core primitive focuses on right now, is just reducing data storage costs. Now if you want to batch many different state transitions together, then you quickly hit that. So ideally, let's say you have an infinite amount of transaction size that you can do atomically. You could imagine many different roll ups interoperating actually through these different compressed token accounts.
00:26:50.586 - 00:27:12.768, Speaker B: You can interoperate through that directly. You know, there's a couple of workarounds you can do, like either using bundles where you, that are currently not enshrined in the protocol, in the Solana protocol, but there are services like GTO that offer transaction bundles. It's, I don't actually think they put up stake for that, but yeah.
00:27:12.864 - 00:27:18.288, Speaker A: Would a. Okay, now there's this whole question, right, if ZK compression is a roll up or not, right?
00:27:18.304 - 00:27:18.860, Speaker B: Yeah.
00:27:19.400 - 00:27:22.780, Speaker A: Would a bundler in this case be a sequencer?
00:27:25.280 - 00:28:28.020, Speaker B: Yeah. So let's say the party that would send the bundle to the l one validator would decide the order of the transactions. So that's how this primitive can support rollups and sort of different constructions like co processors or just like. And basically what it is on the high level, if you zoom out, it's just that this custom guest program state transition function is defined off chain. So if you can describe the state transition function in a circuit, and you can prove the validity of it, either through fraud proof or validity proof, then you can go and put any, you can add a state accumulator or whatnot and bundle multiple state transitions at once, and then you can have your own sequencing. But that's more sort of like, you know, the core ZK compression primitive. What we see is sort of more just like single batch transactions, more like inscriptions.
00:28:28.020 - 00:29:19.072, Speaker B: So in this case with ZK compressions or the default use case, the user decline, actually, each user of each transaction sends a validity proof on chain, which is. And the validity proof is. And that's the ZK part, because all of this right now is Merkle proofs, right? Especially if you define your state transition function of your custom program in completely on chain. All the data is published, you don't need ZK, but these merkle proof paths can become quite large. And the 1.2 kilobyte transaction size limit is quite constraining. As in, as to the degree how many compressed accounts you can spend or create in one transaction.
00:29:19.072 - 00:30:20.890, Speaker B: So what we do is we use snarks. You could also use like a Planck ACG, where we use a snark to compress many of these different sort of arbitrary amount of Merkel proofs into one snark. And then we compress the snark. We basically just use a bitmask on a g one element. That way we can cut off 128 bytes from the snark, which is a graphic steam proof over ALPN 128. So this snark that, you know, the data we sent on chain in the payload is 128 bytes flat. Now you can put a large batch of these merkel proofs into that, and then each user can basically go and create a validity proof or request it from an RPC provider and send it on chain and transaction because it's small enough to not hit the transaction size limit.
00:30:22.550 - 00:30:27.660, Speaker A: Do you also check the validity of the, the state transition function?
00:30:28.320 - 00:31:16.908, Speaker B: Yeah, so that's the next important point. The ZK compression core primitive is really just inclusion in a Merkle tree. We consciously. So we specifically move this STF out of the circuit into the on chain protocol for this core primitive, because what that allows is it allows for composability with any other on chain program that already exists. So now what on chain programs can do is because the data is published, they can now compose with compressed accounts, accounts all in their regular on chain STF. And this is sort of historically where we started with like protocol. We actually started with something like Aztec, but on Solana.
00:31:16.908 - 00:31:43.052, Speaker B: So there was originally it was much more in the circuit. We removed all that, moved it onto the on chain side, mostly demand prompt market fit question, and then to focus on just the data storage elimination. So this proof really just proves that specific account states are part that have previously been committed to on chain.
00:31:43.116 - 00:31:56.050, Speaker A: Could you maybe go through a flow? What happens when some account, or like some smart contract, interacts with a compressed account?
00:31:57.390 - 00:33:01.972, Speaker B: Sure. Yeah. So basically, let's say you have a compressed token account already, and let's say there's a smart contract that is an escrow, and then you have a client here that's the user, and the compressed token account is owned by the user. Let's assume that now the user wants to go and escrow that compressedaccount into a custom program that is outside of the Lite protocol. The user needs to go and request, first of all, this state from the indexer. And this state looks just like a regular, it's the same account layout as with the on chain accounts, right? So it's like this is the owner, this is the mint, this is the token balance, et cetera. Now I, as a client, I request that data, and I also, because I want to interact with that state on chain, I also request a validity proof from that indexer.
00:33:01.972 - 00:33:16.190, Speaker B: So the indexer provides that the indexer internally calls a prover. It's a knock prover server. I could also go and run my own prover or, you know, have a separate provider for just the proving.
00:33:16.350 - 00:33:17.730, Speaker A: How expensive is that?
00:33:18.510 - 00:34:28.169, Speaker B: Yeah, so the proving, one compressed account where I think last we measured was like 2030 milliseconds to create the snark for a single compressor count. And it's sort of the key thing that even like enables all of this. Like if we were to put more into the circuit, you'd have much more of an async execution thing. If you, you know, usually if you have, let's say you have two or three compressed accounts that you want to interact with, it's like negligible where the user doesn't actually really, you know, the biggest overhead is still the actual round trip to the FC provider. So these are incredibly fast to generate. Those are just handwritten knock circuits for this specific use case of batching milky tree proofs. So I request that proof for the accounts that I want to reach from to write to an update.
00:34:28.169 - 00:34:55.700, Speaker B: For instance. Now I have that data locally and I can call an instruction in the escrow program. This instruction would say, well, I interact with two or three on chain accounts, but I also interact with the compressed token accounts that the client sends on chain to then put them into an escrow account. For instance, this escrow account could be compressed as well, but it could also be just an on chain account.
00:34:56.080 - 00:34:59.980, Speaker A: And if it is compressed, it would be put into the queue.
00:35:01.400 - 00:35:55.604, Speaker B: Yes, to update it, then you'd have to basically, okay, so just to go on here, basically what you have is you have to write accounts here. Now the protocol is being called. And then let's say in the case of escrowing, let's say one compressed token account, you actually update both accounts. The compressed pool account or the escrow account and the compressed token account. So basically what happens is the protocol now needs to go and insert the hashes of the old state 32 byte hashes into the nullifier queue. And then also it hashes the new state based on a state transition function that you have to find on chain. So for instance, the compressed token account owner changes.
00:35:55.604 - 00:36:34.340, Speaker B: That's the simplest one. Or if you actually have a compressed token account state that says you have two USDC but you only want to escrow one, it creates a change output account. So you create the output account. The hashes of the output account currently actually get appended directly to the state tree because it's a simple appends. So you don't need to write merkle proofs of these. And yeah, the nullify state, the old state gets put into the queue and then the state transition is done. New state gets appended onto the ledger and the new state can already be spent.
00:36:35.280 - 00:36:52.830, Speaker A: If I send, let's say from, from one account to another and both are in the compressed state tree. Would the indexer provide me Merkle proofs for both?
00:36:55.250 - 00:38:02.260, Speaker B: Good question. So the merkle proofs are only needed for accounts that already exist that you want to update. So assuming that, let's say I want to create a token account for you, or it's a new account that has not previously been committed to or omitted, just because it's like a change output of the transaction, you don't need to provide the mercle proof for that because it's not part of the true yet. Yeah, so that's a flow overall. And all of this is sort of like single transaction mechanism. And that's what ZK compression basically is. And essentially allows you to not store your accounts in the on chain space, just the small root hashes in the on chain space, and still be able to let all of the on chain programs reason about those states and interact with them, compose with them atomically.
00:38:02.260 - 00:38:12.096, Speaker B: So that's the Zika compression primitive. And do you have any other questions on this?
00:38:12.128 - 00:38:20.776, Speaker A: Specifically, how often can you interact with the state route? Like how often can you change the compressed account in the Solana block?
00:38:20.888 - 00:39:16.260, Speaker B: Yeah, so there's a couple of nuances. And for the tree itself, because you can have many different accounts and different types of compressed accounts in one tree, this is concurrent. And the way that we do this is twofold. For proving inclusion, we actually keep around the history of roots, which the amount, the length of the history array is defined by parameters. So it sort of depends on what you set these parameters to. But typically you would do it such that you have a 1 minute lifetime of your validity proof until it expires, because the route is out of date. And then the second thing is that we actually keep around sort of a canopy or changelog of the latest state of the actual local path.
00:39:16.260 - 00:40:22.300, Speaker B: And so what this allows us to do is if I update the same tree, let's say I want to interact with a tree, but then in the middle it gets updated because the Forrester process happens, we can actually patch that proof on chain with some, with some overhead inside the transaction, just because we keep a current sort of like shallow Merkel proof path around. And so, you know, the actual concurrency itself sort of really depends on the parameters, on the size of how big you want these on chain accounts to be that's stored in Merkel roots and changelogs. And then there's, there's two other factors to concurrency. One other factor is, well, how, how much compute does the transaction actually use? Because on Solana, there's a per on chain account write lock limit per block that is being enforced by the protocol. And that's somewhat arbitrary. It has to do with how many cores validators run on. Currently, it's four different threads.
00:40:22.300 - 00:41:24.500, Speaker B: And so the total block limit is 48 million compute units per block that can be used, computers being just like an arbitrary number, sort of based on the milliseconds or nanoseconds and. Yeah, exactly. It's similar done in Ethereum. And these transactions on the hashing and the Snug verification, while they're not expensive in terms of cost to the user, they do take up more compute units than just regular token transfers that you do on chain. And so you have sort of weaker concurrency, as in how many of these transactions you can put into one block if they write to the same merkle tree, for instance. So there's that nuance as well. And importantly to know here is that because compute is not actually linearly priced on Solana, it's the whole arbitrage game that ZK compression does.
00:41:24.500 - 00:41:38.214, Speaker B: Basically, even though it uses more compute, data storage is much more expensive. And so we do a little bit of price arbitrage there, which gets to these crazy multiples that you get if you use compressed accounts.
00:41:38.302 - 00:41:40.650, Speaker A: You're going to force Solana to care about compute.
00:41:41.470 - 00:42:47.980, Speaker B: Yeah, I mean, there's a lot of, I mean, there's a lot of complexity in that, right? Like Solana. Solana. The way that it approaches scaling is that, you know, you have, you know, sort of as demand for block space goes up, validators will increase block size. And then the current consensus is that they don't want to tax users sort of retroactively by sort of like introducing sort of protocol base fees that grow with compute used, which means that right now there are transactions that are super small, they're very efficient, they cost the same as like huge transactions. So like if you have a Jupyter swap that interacts with many different axis, quickly becomes very, very large, costs the same amount of sole and base fees. Okay, so that's that. And then to sort of wrap up slowly.
00:42:47.980 - 00:44:12.150, Speaker B: One thing I wanted to touch on is that, well, something that I mentioned in the beginning with protocol being somewhere in between, the ZK account idea that celestia has and is planning to build, as far as I know, and something like an AG layer like polygon or the ZK sync elastic chain or whatnot, is that there are quite a couple of different ways on how you can use these primitives on DL1 to create both based rollups, but also sovereign rollups even if you wanted to. And that is basically now you have this primitive that has state routes on chain that are concurrent. There is a standardized account and token interface that you can interact with, and RPC interface that is standardized as well. So now you could go and say, well, if I move this transition function off chain, I prove it in, let's say, I don't want to write my own circuit. I can prove it in let's say, ris zero or SP one. And then you could have sort of batch logic in there, sequence it yourself. And then you still have a choice of either committing, publishing all the data in this data field here, which is specific to your application, that's on the ledger, right? Yeah, yeah.
00:44:12.150 - 00:44:54.448, Speaker B: If you publish it, it gets emitted onto the ledger. But the protocol itself, and that's the key point, only really cares about the data hash. So what that means is you can actually commit to the data, put it for your own application. We don't really care about that. You sort of have to take care of the security for that. You could encrypt it, you could store it on an alternative DA layer if you wanted to. So that allows for sort of like an immense amount of flexibility in how you use this primitive to build different, I would say more modularized layers and applications on top of Solana now.
00:44:54.544 - 00:45:10.840, Speaker A: And you think that Solana would start going into that direction where they could maybe even compute, but mostly storage off chain and let these contracts interact with each other.
00:45:11.460 - 00:46:02.340, Speaker B: So this is a great question. And I have a personal opinion on this as well. But basically, the fact is that Solana is still like an amazing execution layer and settlement layer, and the a layer in the sense that the integrated stack, I think, has a long way to go. And I think many applications will build on top of this. And that's also why I, we focus all our protocol efforts right now on the ZK compression primitive, which is sort of this just like safe, safe cost for your compressed accounts for your state that the application produces. For instance, you have a couple of these applications like heliomochito. They produce millions and millions of accounts for their reward distribution, for instance, each epoch, which is like every two days.
00:46:02.340 - 00:46:48.580, Speaker B: And so taking care of the rent cost there, you know, you can free up a lot of locked capital if you switch to compression. So there's what we feel product market fit right now with just like keeping execution in these on chain environments, especially given that computer still so cheap. Now, there's a couple of trends that I've seen, which is that, well, there's probably going to be. So currently on Solana, blocks are pretty full. The baseline is, it's like 80%, 90% full. And so there's going to be times where demand just exceeds supply to a degree where several applications will get priced out. With regards to compute.
00:46:48.580 - 00:48:03.248, Speaker B: With some, there's one called drip, they add up like millions of nets each day, and they already hit these compute questions as well. It's not just data storage. And so at the same time, there's now sort of, given the additional attention that Solana has received over the last year, there's a bunch of projects that are now trying to build Solana L two SVM forks. And I've been having a bunch of conversations, and it's about like, okay, how can you actually keep a sort of l one centric approach for most of these where, you know, you can, you know, basically application developers that then in the end have to build on these and on the l, one can interoperate and sort of can utilize a standardized interface. And, you know, sort of my goal with this is sort of without having a strong opinion on, you know, whether it's like long term sustainable, to have like, Solana roll ups or whatnot, is to make sure and maybe help shape that we don't get sort of user experience. Fragmentation, liquidity fragmentation, et cetera. So this primitive can be, can be, in my opinion, quite helpful for that.
00:48:03.384 - 00:48:11.580, Speaker A: And the data part, if you use Solana for the data part, does that contribute to the block size limit?
00:48:13.920 - 00:48:14.928, Speaker B: No, no, no.
00:48:14.984 - 00:48:21.914, Speaker A: So it could be infinite data, but it just priced against the compute part.
00:48:22.042 - 00:48:36.242, Speaker B: Yeah. So the price against the compute part. The block size limit is compute. And that's the interesting thing about Solana. Yeah, exactly. Now there's a transaction size limit, which is quite small, 1.2 kb.
00:48:36.242 - 00:49:04.008, Speaker B: So to publish all your data, if you want to batch a lot of transactions together, there's a sort of a limit. It's probably like a multiple of like 1015 that you can get an improvement. And after that, you'll probably have to move more of your state transition function off chain and commit to data somewhere else. Like commit to data on chain, but stored somewhere else.
00:49:04.184 - 00:49:12.760, Speaker A: You could commit, like you could move the data part off chain without moving the transaction station functional chain.
00:49:12.800 - 00:49:47.540, Speaker B: Right, yeah, that too. The important thing is really just that, well, if you're in an on chain environment, your application needs to reason about the data, so it needs to be published, but it depends on, like, which data you want to read. Right. And interact with. So, like, you could think of like a specific hash structure where you actually have to send only a very small amount of each piece of data. And the rest of it you can just hash together and keep somewhere else. So there's a lot of flexibility in how you use this primitive and protocol.
00:49:47.540 - 00:50:11.000, Speaker B: But, yeah, that's, in my opinion, pretty cool. And hopefully to sort of wrap this up as well. Hopefully it sort of attracts more ZK developers and just more of that. What I like about Ethereum and Celestia and the module stack is more permissionless application designs, VM designs that can just plug into this.
00:50:11.540 - 00:50:21.868, Speaker A: So thanks for having me here. Where can people find more about light protocol? Sure.
00:50:22.044 - 00:50:44.764, Speaker B: Yeah. So the documentation, we have a dedicated documentation that we maintain with Helios Labs. It's called www.zkcompression.com. should be pretty simple to find. And then light protocol on X or Twitter, whatever you want to call it. Light as in the light ray, and then that's where you find it.
00:50:44.852 - 00:50:46.520, Speaker A: Cool, thank you again.
00:50:46.860 - 00:50:47.300, Speaker B: Thanks, guys.
