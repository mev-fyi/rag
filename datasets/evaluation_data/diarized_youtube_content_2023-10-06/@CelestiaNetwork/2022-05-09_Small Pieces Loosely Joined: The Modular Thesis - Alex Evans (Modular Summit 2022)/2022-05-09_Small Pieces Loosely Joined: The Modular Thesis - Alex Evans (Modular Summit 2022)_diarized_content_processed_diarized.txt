00:00:04.790 - 00:00:30.180, Speaker A: I think I'm missing a slide, but hello everyone. I'm Alex Evans. As the DT said, I'm one of the partners at Bain Capital Crypto. We're an early stage crypto focused fund. We spend a lot of our time on research protocol design engineering. We have a particular obsession with modular architectures and blockchain designs. And guess in this presentation I want to share maybe some of that excitement with you.
00:00:30.180 - 00:01:34.662, Speaker A: In particular, I want to talk about some of the scalability properties that Mustafa sort of alluded to carefully in terms of what types of scalability modular blockchains enable and what types of scalability they don't. And I want to spend a little bit of time talking about flexibility and the acceleration of VM and execution environment design innovation. Where I want to start is not by defining monolithic versus modular blockchains. I think Mustafa did a great job doing that but just to point out that the concepts of monolithic versus modular exist along a spectrum and that most blockchain designs are positioned somewhere along that spectrum. Most smart contract platforms as we understand them today are becoming or at least adopting some of the ideas of the modular paradigm over time. So in some ways you could see them becoming more modular. And that, I think, paints that world in less of a binary perspective and more in one where these ideas of separating out DA from execution are permeating into the broader blockchain space.
00:01:34.662 - 00:02:24.770, Speaker A: And I guess the argument for this talk is that there's a very good reason for that. And so the reasons for that are, number one, scalability, the ability to have greater performance in our systems as well as the ability to support more users while still allowing them to be first class citizens of these networks. And the second one is that modular designs unlock rapid experimentation across the stack in terms of DA layers, in terms of execution layers and so forth. Splitting up these monoliths into components allows us to experiment on each one and iterate on the best possible designs. So I'm going to take each of these in turn. I'm going to start with scalability. It's a little bit of a pedantic approach to scalability just to make sure that in the time that we have we cover it precisely because there's a lot of confusion around what scalability means in these ecosystems and so forth.
00:02:24.770 - 00:03:06.830, Speaker A: Just with a very simple rough definition of scalability. We want to increase the capacity of our systems to handle more transactions cheaper. But we want to do so in a way where users are still able to catch up to the tip of the chain are able to validate the transactions that have happened thus far, are able to reconstruct the state. Or request the state with very minimal trust assumptions such that they're able to author their own transactions, validate the transactions of others, and importantly, do so without trusting anybody else in the system. That includes miners. It includes other nodes that gives users full autonomy of these systems. Mustafa mentioned really eloquently this is one of the core values of the modular design ecosystem.
00:03:06.830 - 00:03:51.806, Speaker A: It also turns out to be a core value. What makes to us at least blockchains and public blockchains in particular. Interesting, so oftentimes we hear that this chain or that chain is more scalable. Oftentimes we're simply referring to cost. I guess the point of this part of the presentation is that we also need to consider what the requirements are on users, be they full nodes or light clients. That comes with that decreased cost, right? So sometimes we hear roll ups scale Ethereum or roll ups scale some other chain and I think we want to be a little bit precise about what we mean. That is true that roll ups are a way of scaling these platforms, however, not often for the reasons that are discussed.
00:03:51.806 - 00:04:18.618, Speaker A: Typically people point to some graph like this. They show the L1 cost being significantly higher than the L two cost for the same transaction. Here is just a uniswap swap on Ethereum versus optimism. And they say, okay, one is roughly in order of magnitude or more expensive than the other. Therefore we have scaled ethereum. Of course, we're only talking about cost. We haven't considered what actual users have to do to validate these computations that are happening on L2.
00:04:18.618 - 00:05:04.166, Speaker A: Right. Another thing to note pretty briefly is again, this is data from for optimism. The vast majority of the actual cost that users pay in these roll ups actually come from the data footprint on L One in particular, in this case Ethereum today that is projected to go down through all sorts of things that are in the pipeline, including making data availability cheaper on Ethereum and so forth. All this is telling us is that gas is really, really cheap on L2. Right? And we have to reason about whether that can continue to be the case or what. We anticipate these gas price and gas market dynamics to be down the line. And the way to do that is we want to think a little bit carefully about what the precise scalability of splitting up computation from data availability is.
00:05:04.166 - 00:05:53.426, Speaker A: Right? So just for the sake of argument, let's take just like a standard monolithic blockchain, let's create a carbon copy of it just as a thought experiment that does the exact same execution and just split it up into two different layers. One is going to handle data availability and will also process state commitments, commitments to the state, as well as proofs that state is valid in different ways. And then all the execution is going to happen on another layer. The question we're going to ask is have we scaled anything just by doing this? And intuitively it would seem no, because we're executing the exact same computation. So for instance, if we're running the EVM on another roll up, it's just another blockchain. We're simply validating the same thing. We need to run a full node on layer one to validate the data, to download data and validate state commitments and so forth.
00:05:53.426 - 00:06:52.778, Speaker A: It doesn't allow us to reconstruct the state of the roll up. If we wanted to reconstruct the state of the roll up, we would have to run a full node that entire roll up re executing all the compute, thus therefore not inducing a scalability benefit, right? However, we do get some other really important things just by the way. We do get cheap trust, minimized light clients, right? We can just run our full node on layer one and if we just trust one of N nodes on L2, they can provide us that state. We can verify that it is indeed the correct state that can allow us to do fast syncs, it can allow us to do trust, minimized bridges between roll ups and lowers the hardware requirements for users to participate in these systems meaningfully. But in terms of running a full node, so far we haven't achieved any scalability. The scalability comes from two factors that are a little bit more subtle than just the pure cost of looking at L2. The first of these is horizontal scale and the second one has to do with resource pricing, right? So let's take these in turn.
00:06:52.778 - 00:07:57.730, Speaker A: This is indisputably a more scalable system than a monolith for the simple reason that it's essentially a form of sharding. So if I want to run a if I want to validate ZK sync, I don't need to also validate a bunch of execution on optimism, right? So nodes are splitting the work between them. This is a form of scalability, right? We have strictly increased the capacity of our system without increasing the hardware requirements on any one of the nodes in the system. We're splitting up the work between us. Now that scalability benefit is somewhat muted given the fact that a lot of the cost, as I showed two slides ago, is coming from layer one, right? So we still need to validate this really heavy EVM. We need to make sure we're downloading data that corresponds to a whole bunch of other roll ups and so forth, right? Over time, as DA costs come down on layer one, call it EIP 44 80, call it data only shards and data availability, sampling and so forth, we get a lot more horizontal scale. Then you also consider pure data availability chains that don't have heavy VMs to validate and the horizontal scalability benefits can become very, very substantial.
00:07:57.730 - 00:08:36.170, Speaker A: That said, they don't come for free. They often come at the expense, not always of composability. And what we mean by composability just the fact that applications are colocated and can be atomically composed with each other. It's a really magical property of public blockchains wherein, as a developer, I can take different components from different underlying pieces of software and stitch them together into an end user experience such that the end user sees these applications almost as if they're functioning as a unit, as one. Right. This is really magical. It's kind of what's enabled a lot of the innovation in DFI and other areas of Web Three that we find very exciting.
00:08:36.170 - 00:09:33.706, Speaker A: We don't take it lightly to compromise, I suppose, on this property. It's, as we understand it, at least one of the unique properties of building applications on public blockchains. Right. On the other hand, we have a lot of benefits to building, splitting up, execution and having multiple different roll ups handle different tasks, right? Including this horizontal scalability as well as sovereignty and flexibility in terms of being able to customize your execution environment to your end goal. Right? So we have a pure classical trade off as application developers inside of the modular world between composability and horizontal scale on the other side. And I guess the question is going to be how do we navigate trade offs like this? One is to take the Twitterverse approach which is to take really extreme positions and say, hey, we want all applications to be on the same chain and not compromise on any composability ever. Of course, that will mean that our chain is going to be either really expensive to use or really centralized.
00:09:33.706 - 00:11:00.886, Speaker A: And then the other approach is that every single application you could imagine should have its own layer one blockchain or sovereign roll up, right? Empirically we know that neither of these extremes are truly tenable and that the reality is somewhere in the middle and in fact the optimal point is somewhere in the middle. And the question is going to be how do we find that optimal point? Incidentally, by the way, monoliths can also do Sharding, except that you need to prespecify what each Shard does a priori. The modular approach to navigating dilemmas like this and trade offs like this is to let application developers is to let users decide with their money, with their time, what trade offs work for what application. So you can imagine a game developer may want a very streamlined, self contained experience and they're very happy building a sovereign roll up or a layer one chain specifically tailored to the experience that they want their end users to have. Maybe a DeFi yield aggregator is going to want to colocate with a whole bunch of DeFi apps might be a little bit more expensive as a consequence but the composability they get out of that greatly outweighs any slightly increased cost to users, right? The modular approach is to run a whole bunch of these experiments in parallel and for each application iterate towards a more optimal design. When we find those designs, we can continue to iterate on them and continue to explore and run experiments and get better over time as an ecosystem. Okay, so this second scalability property of modular blockchain designs comes without that many trade offs.
00:11:00.886 - 00:11:53.146, Speaker A: It is essentially a pure increase in capacity that comes from more efficient resource pricing. So recognize that monolithic blockchains price resources as a bundle. So you get some gas, you can then redeem that gas for certain bytes of data availability. You may also redeem it for certain opcode executions, right? Those resources have a fixed price relative to each other. What does that mean? It means if demand for one goes up, even if supply and demand for the other does not go up, the price of it will go up, which strictly decreases the capacity of these systems. Right. On the other side, on the modular paradigm where you split up data and execution, if demand, for instance, for running certain operations inside of, I don't know, ZK sync goes up, data availability for optimism does not go up.
00:11:53.146 - 00:12:46.006, Speaker A: Which is a reasonable thing to say, because those things are completely orthogonal resources. So we strictly get more capacity in our systems through more efficient resource pricing without increasing node requirements in so doing. In fact, this property, the more congested the system, the more powerful this property is. Another way to think about it is we have a constraint optimization problem to allocate resources. In such a way, to maximize the scalability of our systems, we simply remove a constraint from that optimization problem, right. With a constraint that the prices are fixed. Removing constraints from optimization problems very often, especially when constraints are hard and met, for instance, in a case where there's a lot of demand for the system greater than the capacity they can handle at any given time, strictly allows us to achieve greater values for the objective function, right? So it's relatively trade off free way of scaling blockchains.
00:12:46.006 - 00:13:17.110, Speaker A: All right. Want to change gears a little bit and talk about experimentation and optimization. And this is really maybe the meat of the talk. It's what excites us the most about modular ecosystems and designs that we're starting to see percolate throughout the space. It's that by splitting up monoliths and allowing individual components to be optimized, we get a lot more experiments run in parallel, and we get a lot better data layers, we get a lot better execution layers. As a consequence, we get more scalability, but we also get new capabilities in these systems that we couldn't have imagined otherwise. Maybe those are about privacy, or maybe they have to do with application specific concerns.
00:13:17.110 - 00:13:47.022, Speaker A: So just as a prequel, we've seen an incredible amount of innovation on public blockchains. A part of it is and more of it happens higher up, right? A lot of iteration happens at the UI layer. You can push changes every hour if you want to. Smart contracts move a little bit more slowly. You have to audit them, hopefully at least. And you need to make sure that whatever you build there's, hopefully some users on it and so forth, it's not as easy to just iterate on a day to day basis. Maybe it's on the scale of months.
00:13:47.022 - 00:14:22.282, Speaker A: But even in spite of that, there's unbelievable amount of innovation that's happened. And the reason for that is, for instance, if I am curve and I see uniswap v one and I'm excited about that design, I can say, hey, I think I can do that better for stablecoins. I don't have to go and fork all of Ethereum underneath me, I need to fork the consensus layer or anything. I just build a smart contract. I run that experiment. If people tend to like it, maybe I'll get a bunch of liquidity and people will like my product, right? Similarly, Sushi swap can fork uniswap and add an incentive. Uniswap e three can take ideas from everything else that's been built, including add some of their own.
00:14:22.282 - 00:14:57.426, Speaker A: And we're running a bunch of experiments in parallel, figuring out what the right way to do decentralized exchange is. And then when we find ideas that work, we double down on them. In contrast, the underlying infrastructure, consensus, data and execution moves glacially. It moves kind of closer to the speed of hardware than the speed of software. And there's actually very good reasons for this. There's like hard fork release timelines on a year scale, kind of similar to how quickly Apple releases iPhones. And the reason for that is, let's say I want to make, for instance, a change to the execution environment of Ethereum.
00:14:57.426 - 00:15:40.718, Speaker A: Maybe I want to enable greater parallelization in the EVM. I can either launch my own layer one from scratch, right? I can build my own consensus system or fork the consensus system and recruit my own validators, then build the execution environment that I want. And building layer ones is really resource intensive. Or I need to get an improvement proposal, solana improvement proposal, ethereum proposal pushed through the core developer community. There's reasons why that's structurally hard to do. The main one is the fact that a lot of people use these systems and we can't just try random crazy ideas with low probability of success because people's livelihood depend on these systems actually being resilient, right? We don't want to break them. So we want to be a little bit conservative with how we iterate on these designs because again, they're tightly coupled.
00:15:40.718 - 00:16:33.378, Speaker A: And so one change, for instance, in the execution could screw up consensus and maybe performance improvements in one part of the system can cause performance degradation elsewhere, right? So structurally these things are slow moving, but we still want this capability to be able to run a bunch of experiments kind of a little bit more similar to what's happening at the smart contract layer. And I think we would benefit a lot as an ecosystem with a lot of parallel innovation happening in these systems. Okay, so I guess my point here is that modular blockchains enable this uniquely by splitting up execution from data availability and consensus. It is now easier for me to launch a new execution layer should I choose to, right? So if I have an idea, maybe I want to enable access lists in the EVM to enable parallel reads and writes from storage. Maybe a lot of people disagree with this decision. But I want to run the experiment. I now don't have to fork the entire DA and Consensus system.
00:16:33.378 - 00:16:58.898, Speaker A: I can simply launch a parallel execution environment now. It's still a little bit harder than launching a new smart contract. I need users to validate. I need nodes and so forth to be running on this. And for instance, if I need to build my own sequencers, then I need to recruit a validator network. It's maybe not worth it if I just want to kill the self destruct instruction in the EVM that everybody hates. But it might be worth it again for something like parallelization, which is just an interesting experiment to run, right? If I happen to be right, users will use my chain.
00:16:58.898 - 00:17:25.190, Speaker A: They'll validate my chain, they'll run nodes on it, liquidity will migrate to it. And if I'm not, nobody will do that. But the cost of experimentation have been brought down materially relative to launching a new layer. One, from scratch, it's easier to launch a layer. Two, it's easier to launch a sovereign, modular execution environment in these contexts. The other thing is that as a developer of an execution environment, I only need to optimize for the precise use case that I care about. Somebody else is handling DA for me.
00:17:25.190 - 00:18:14.086, Speaker A: Somebody else might be handling Consensus. I need to think about who my users are, who the developers on my execution environment are likely to be, and how do I best tailor to their needs. Similarly for the DA layer, DA layers can specialize and focus on providing the best possible experience for people building execution layers on top of them. Right? So what does this imply? It implies that at the same time, we're able to run a whole bunch of experiments on execution layers. That means we could get execution layers that are more performant for general purpose applications, maybe special purpose execution environments for different use cases. My colleague Wei from BCC has a proposal to add an additional instruction to the EVM to process Saplink circuits and enable anonymous transactions and composable DeFi. Anonymously? There's all kinds of different things we can try.
00:18:14.086 - 00:18:35.466, Speaker A: Some of them are going to fail. Some of them are going to be terrible. Some of them are going to be centralized, scam execution environments that claim to solve everything without any trade offs. The point is, it's going to be really fun. A lot of people will try many things. Maybe your nephew will shill you execution environments at your next family reunion. Maybe you yourself will build an execution environment.
00:18:35.466 - 00:18:48.562, Speaker A: And if you do, we hope you call us. Thank you very much. Pause now for questions. Do they need to get your mic?
00:18:48.696 - 00:18:54.260, Speaker B: Yeah, there we go. You go.
00:18:55.670 - 00:18:56.370, Speaker A: Beautiful.
00:18:56.520 - 00:19:34.880, Speaker C: On the note of orthogonal resource pricing, if you have an L1 that supports on the central planning when it comes to essentially creating efficient markets to maximize resource allocation for L two s, is it the case that you can get quite a lot of benefits when you have multidimensional EIP. 1559 or you reduce central planning and you say, like, have individual opcode operations to be free floating, or is that only a half hearted solution? And you really need to have complete orthogonal separation of orthogonality on the different layers versus to allow a free market for the usage of layer one resources by L2?
00:19:35.650 - 00:20:10.490, Speaker A: I think both of those achieve a similar thing, right? So if you are able to price resources separately in every instance, and you have fixed absolute prices that are given by supply and demand for scarce resources, then you've accomplished this goal. It's just a simply much easier goal to accomplish in a modular architecture. Because in a monolithic architecture, there's other concerns to doing that, right? And even EIP 50, 59 is not trade off free, right? And I'm not saying it's trade off free in the modular world either. It's a different way of exploring what those trade offs might be by just running a bunch of experiments in parallel, right? It's a great question.
00:20:10.560 - 00:20:46.694, Speaker C: And then quick follow up in terms of the trade off between having DAP chains or very specific L1s for applications with the limited code composability, and then the other sense of having an extreme gas limit universal L1, for instance, that everything is operating on with limitless composability or far fewer constraints there. Do you foresee there would be multiple optimal points rather than, say, a single optima, and then as time evolves and these interact, would you imagine it would converge to, say, a bification or a single optimum point? Or do you imagine, say, there would be benefit for extremely low gas limit L1s to live alongside, say, for instance, very secure Bridging in adapt chain environment, right?
00:20:46.812 - 00:21:22.626, Speaker A: I think we're likely to see that there's going to be different optimal points for different applications, right? By definition, some will choose to colocate, some will choose, for instance, to break off, and then, for instance, they could break off and then start to build other applications on the same chain because there happens to be a lot of liquidity there. And then these optimal points are not fixed in time, like are not fixed across time. There might be something that works now, and then there's a whole bunch of users tomorrow and there's an optimal point in the future. It's just the experiments are running in parallel. We're already starting to see this, right? There's different roll ups that are exploring different trade offs. There's different L1s with different trade offs. There's application specific l ones.
00:21:22.626 - 00:21:34.198, Speaker A: This is the world that we live in, right? And we're starting to see increasingly, like what types of applications might work in one environment versus another. We haven't fully explored that space yet. That is for all of us here to do.
00:21:34.284 - 00:21:36.742, Speaker C: Awesome, thanks.
00:21:36.876 - 00:21:38.040, Speaker B: Any other questions?
00:21:38.810 - 00:21:39.414, Speaker A: No.
00:21:39.532 - 00:21:40.838, Speaker B: Oh, there we go.
00:21:41.004 - 00:21:45.642, Speaker A: All right. These are pretty evenly distributed across the room.
00:21:45.696 - 00:21:46.202, Speaker B: I know, right?
00:21:46.256 - 00:21:47.450, Speaker A: Make you exercise.
00:21:47.870 - 00:21:49.034, Speaker C: Here you go.
00:21:49.232 - 00:22:28.310, Speaker D: Thank you. Hey, you've spoken a lot about the technical benefits of a modular blockchain architecture, but if I were to build my own L One, one of the things that I want to do is hold value in my token. And for something that's going to provide me this data availability layer to replace, the need for my consensus has to be paid for. Do you see that as being something where we're going to have a lot of different tokens? Something like Terra, where you get fees paid in all the manner of tokens that are being used on the network? Or is that something you see more in the kind of Ethereum space where you're paying an ETH to write back to the chain?
00:22:29.690 - 00:22:55.838, Speaker A: There's a couple of questions in there just to make sure I understand it right. So one is application developers or roll up developers wanting to have their own asset. If there turns out to be some use for that asset, then presumably it has a reason to exist. Maybe it does sequencing or so forth that you care about. Maybe there's some property to sequencing that you care about. If not, then might not have a reason to exist. And so these are the debates we have on Ethereum all the time with different applications anyway.
00:22:55.838 - 00:23:35.066, Speaker A: And then in terms of for regular users, I expect so, for instance, if there's a roll up that has its own sequencer where you pay in that native asset, then you just outsource it to the validators right. To go pay Celestia or Ethereum or any one of these data availability solutions. Or some of these operators have users paying whatever token they want. There's different trade offs that have to do with UX. I suppose my personal bias, obviously different people will have different ones is like to abstract some of the complexity away from users as they're transacting. They're having to think about 50 tokens and the relative prices of each other is just not a fantastic experience. Does that answer your question?
00:23:35.168 - 00:23:54.474, Speaker D: Yeah, it does. Thank you. As you say, abstracting away the complexity. Okay. You use our L1 token, but it's not an L1, it's actually provided by the data availability layer. And then we handle the costs and the gassing on the underlying consensus layer through the validators.
00:23:54.522 - 00:23:54.698, Speaker A: Right.
00:23:54.724 - 00:24:02.980, Speaker D: That makes sense. Yeah. I didn't get that you could do that on the validator layer because obviously they're a bit more smart than I initially thought. Thank you.
00:24:04.870 - 00:24:09.866, Speaker B: Any other questions? I think you're done.
00:24:09.968 - 00:24:10.346, Speaker A: Thank you.
00:24:10.368 - 00:24:11.160, Speaker B: Thanks, Alex. Yeah.
