00:00:00.120 - 00:00:36.224, Speaker A: On top of Celestia. So some of you guys have joined the workshop yesterday with Jas about project ideation. If you not have seen it, then feel free to watch it on the YouTube. And today we're very happy to have Nick White from Celestia to do a workshop which is about the introduction to modular blockchains. So without further ado, let's welcome Nick from Celestia to start. And for the audience, if you do have any questions in the meanwhile, feel free to drop them in the YouTube comments or in the binance live chat room. So whenever you're ready, please go ahead.
00:00:37.684 - 00:01:22.478, Speaker B: Awesome. Hey, guys, what's up? I'm Nick White. I'm the COO at Celestial Labs. And it's my pleasure today to do an introduction to modular blockchain. So the goal of the presentation today is to give everyone who may not be familiar with sort of the background of modular blockchains and then talk about some of the key concepts and then maybe talk a little bit about where the modular blockchain space is going in the next few years. So with that, let's get started. So the first, sort of like part of this presentation, I want to talk about basically monolithic to modular.
00:01:22.478 - 00:02:25.706, Speaker B: So, like, where did the idea of modular blockchains come from? And what are the advantages over monolithic architectures? So, as with everything blockchain related, it starts in 2008 when Satoshi Nakamoto published the bitcoin whitepaper. And what was so novel about what Satoshi proposed in the bitcoin white paper was that he envisioned a system for the first time that could have rules without rulers. So you could have a bunch of people agree on a set of rules, and then they would be able to enforce those rules without requiring sort of like this power hierarchy. So, like, normally in some system of cooperation, a group of people want to. Want to work together. You're going to need to have a ruler. It could be like a king or it could be the president or some lawmakers or an army or military or something, that it has to enforce the rules on everyone else.
00:02:25.706 - 00:03:12.992, Speaker B: So you end up with this sort of like, uneven ground and this hierarchy. And the problem with that, with those kind of systems, is that ultimately you have to trust the rulers to uphold the rules faithfully. And essentially, those kinds of systems of cooperation just have inherent drawbacks of centralization of power and corruption and things like that. And with what Satoshi proposed, for the first time, you could have a set of rules, and then those rules could be enforced by everyone, kind of on equal footing, essentially. So you don't need to have central authority or power. It's like a very equitable approach to cooperation. And that's what blockchains are.
00:03:12.992 - 00:03:57.494, Speaker B: And that was really what was so novel about, about bitcoin at its core. And the way that this works, like blockchain cooperation works, is that the people who are participating in the network have to run a node. And what a node is, it basically is a computer program that has a copy of the rules. And as new blocks get added, as new events take place in the blockchain system, everyone inspects those transactions, inspects those blocks, and makes sure that they're following the rules. If they don't follow the rules, then they reject those blocks. And those blocks essentially just don't happen. They don't actually take place.
00:03:57.494 - 00:05:20.750, Speaker B: So running a node is essentially what enables blockchains to have this very unique property of rules without rulers. It's the verification of blocks that enforces the integrity of the blockchain. And that's what removes trust from the system. Now, the problem with that is that if you have to verify every single transaction that happens on the chain, then as the number of transactions increases, then the amount of work you have to do to run your node and to verify the chain as a participant in that system increases proportionally. So it's fine when there's, I don't know, a few hundred transactions. But then if you're starting to do like thousands of transactions or millions or in the future, maybe billions of transactions in each block, all of a sudden the amount of work you have to do to verify increases by, you know, a factor of 101,000, a million, right? So essentially what happens is, as you scale the block size and the throughput of these systems, the person who is trying to verify them has to have a bigger and bigger computer. They need more and more bandwidth, they need more and more resources.
00:05:20.750 - 00:06:00.300, Speaker B: It becomes more and more expensive for them to verify the chain. And so again, like in a monolithic blockchain, to run a full node means you have to download every single transaction and you have to verify every transaction one by one. So to illustrate my point, sort of like this is like a meme that someone made, which I really like. It was like, when the transaction data is small, it's totally fine. I can run my node on my laptop and I can make sure that the chain is authentic. No one's stealing funds or doing fishy things. But then as a transaction data increases, all of a sudden it's like, oh, I actually, I'm unable to run that on the computer that I have.
00:06:00.300 - 00:06:51.594, Speaker B: And now you fall back basically into having to trust the people who are running nodes, the people who are running the miners or the validators. You have to assume that they're going to be honest to you, essentially. And so fundamentally, and this slide is basically about the fact that so because of that constraint, you end up in a monolithic architecture. You end up with a block size that is fixed, which it gets fixed to. What is the amount of computing resources that you assume that the participant in the system should have or can have. And then when you fix the block size, as the more and more people want to send transactions, there's a limited supply and there's more and more demand. The price, the fees of using the system go up, and it makes it unusable.
00:06:51.594 - 00:07:47.094, Speaker B: And so you're stuck in a monolithic architecture. You have this trade off between big blocks. You increase the block size, and then you can keep the fees low and have more throughput, but then you lose decentralization because fewer and fewer people are able to actually verify and participate on an even footing on the network. Or you keep the blocks small, and then you just accept the fact that you're going to have high fees. But the good thing is that you have really high decentralization. So bitcoin and Ethereum people have criticized them for not scaling the block size, but that's because they have wanted to preserve the verifiability and the decentralization of their network, even if it comes at the cost of high fees. In the monolithic blockchain architecture, the fundamental trade off that you have to have to make.
00:07:47.094 - 00:08:55.197, Speaker B: And this is why, for example, there's been several different forks of bitcoin, but most notably bitcoin cash, where the fundamental argument between the two sides was in bitcoin cash, they wanted to dramatically expand the size of bitcoin blocks. And then the bitcoin core folks didn't want to increase the block size because they wanted to make it so that people can run nodes. And that brings us to sort of like stage two in the evolution of blockchain architectures, and that is the Ethereum white paper. So in 2013, when Vitalik published Ethereum white paper, it was a massive step forward in that it generalized what you could, the kinds of rules you could express on a blockchain. So, like with, you know, bitcoin, you could think of it sort of like a calculator, where it had very simple rules, a very limited expressivity. Like, you know, it's, you know, it's kind of just about transferring assets. There's scripting, but it's so limited that you.
00:08:55.197 - 00:09:40.904, Speaker B: There's not very much that you can sort of build on top of it. In Ethereum, the goal from the beginning was actually the rules that we're going to instantiate and embed into Ethereum itself are going to be rules that basically instantiate a computer. And then people can write their own rules that they deploy on top as smart contracts. So Ethereum, if bitcoins are calculated, then Ethereum is sort of like a computer calculator. You can only do a few different kinds of operations in a computer. It's fully generalized and expressive, and you can build all different kinds of applications on top of it. So this was a massive step forward because it really blew the doors open of what it was possible to do on chain.
00:09:40.904 - 00:10:39.030, Speaker B: But the issue though was that on the Ethereum computer, the Ethereum computer basically has a built in operating system or virtual machine. And that is the EVM. The issue there is that sort of like, you know, imagine you, you were given a new computer, but it only ran windows, for example. But you, you know, you wanted to write or run Mac apps or Linux applications, you wouldn't be able to do that because the computer just, you know, you couldn't, like, uninstall windows and reinstall Mac or Linux. You're basically locked into what the computer came with, right? And so that's kind of what Ethereum and EVM is. It's sort of like, yes, it's a computer. There's a lot of things you can do on it, but ultimately there are certain design decisions that were made for you as a developer that limit, and they constrain what you can do.
00:10:39.030 - 00:11:41.058, Speaker B: So even though you have these wings and you can, in theory, build all these things, in practice, there's still constraints placed on you. And so that brings us to the third stage of the evolution of blockchain architectures, which is the advent of modular blockchain. So in 2019, Mustafa al Bassam, who's the co founder of Celestia, published the lazy Ledger whitepaper. So Celestia, for those of you who don't know, Celestia, was originally called Lazy Ledger. And in that paper, he described a totally new way of thinking about how blockchains work and can be built. And essentially the keyword is modular. So in his vision, he realized that if you go back to the first principles of blockchains, there's actually a handful of different functions that are happening under the hood.
00:11:41.058 - 00:12:46.192, Speaker B: And when Satoshi and other folks first designed blockchains, they unconsciously coupled those functions together into one monolithic system. But if you go back to first principles, there's no reason that those functions all need to be bundled and coupled together. What you could do is actually unbundle the stack and build separate layers, separate protocols that specialize in those functions. And then you layer those pieces back on top of each other, and you still have the same functionality, you can still build the same applications, but all of a sudden, you solve a lot of the core problems that monolithic blockchains face. So first is scalability. So as we talked about, blockchains are all about verifiability. That's what makes them decentralized and secure, and removes trust and equalizes power.
00:12:46.192 - 00:14:05.700, Speaker B: So in a monolithic system, to do that, you need to run a node in a monolithic system, as we mentioned, running a full node means downloading all the data and verifying every single transaction in a modular blockchain system. Because of the advent of two key technologies, which we will talk about more later, but data availability, sampling and roll ups this, it totally changes the scaling properties of running a node, such that instead of downloading all the data, you can download just a tiny sample of it. And instead of verifying every single transaction one by one, you can just verify a single proof. And essentially you can have essentially the same security of running a full node with a tiny, tiny fraction of the computing cost and resources, and running a so called light node. And this is huge, because instead of in the previous example was a transaction, data increases the number of people that are using the network, it goes up. Eventually, people are no longer able to run nodes. In this modular paradigm, even as the transaction block size increases, people can still run nodes and verify the network without needing to drastically scale up their computing resources to keep up with the network growth.
00:14:05.700 - 00:15:06.784, Speaker B: So in a monolithic world, where you're constrained to a sort of fixed block size based on how many resources the node operators are meant to have in a modular world, and that fixed block size means that the fees increase. In a modular world, you don't have a fixed block size. You can increase the block size because it doesn't actually end up pricing out, like you don't need more node resources. So the block size can be flexible and scale to meet demand, and you can scale it while still preserving that really, really important property of verifiability. And so the idea with modular blockchains is that even as blocks get bigger and bigger, the or, sorry, as there's more and more people trying to use the network, the fees can still stay relatively constant. And so modular blockchains subvert this trade off of big blocks. You can have big blocks with low fees, but still maintain high decentralization.
00:15:06.784 - 00:16:09.894, Speaker B: And then the second part of the modular blockchain paradigm that solves a big problem is that in modular blockchains, execution is separated from settlement and consensus and data availability. And what that means is that you as a developer are no longer locked into one operating system, if you will. You can choose what kind of operating system you want to run your program on. So it gives you full control, basically, over the execution part of the stack, and allows you to customize things and do things that you can't do when you're building a monolithic chain. And those choices have been made for you in a modular blockchain like Celestia. There actually is no execution on Celestia. The whole point is that you get to choose what execution you want to run on top of it.
00:16:09.894 - 00:16:49.204, Speaker B: So it could be anything, it could be evm, move, Solana, Vm, wasm, cosmos, SDK, could be something totally custom. And so it's totally up to you. So the freedom, you're no longer caged or constrained at that part of the stack. You can just do whatever you want. And so it's sort of like you're no longer stuck with windows. You can have Mac or Linux or whatever. So, to extend this analogy, if bitcoin is a calculator and ethereum is a computer, then you can think of Celestia and modular blockchains as cloud computers.
00:16:49.204 - 00:18:13.376, Speaker B: The way that understand this is that in web two, the cloud stack, at the base of it, you have a data center like AWS. And what they specialize in is providing really scalable, cost effective compute, like raw compute, that developers can come to and basically pay to borrow or sort of consume that compute. And they can run customized virtual machines, virtual servers on the data center that are customized for the application that they want to run, and they can scale it up or down. It's very cost effective. The cloud stack has enabled web two to really grow in scale, has enabled developers and not have to worry about infrastructure, have all this flexibility baked in from the beginning. And so it's really unlocked like a ton of innovation. Now, if we shift to web3, modular stack, it's a very similar paradigm or design where instead of a data center, you have a data network like Celestia.
00:18:13.376 - 00:19:17.704, Speaker B: So the data network like Celestia, what it does is it provides you with the raw resource of block space. So it's just like pure raw block space that you as a blockchain developer can come to you and say, I want this much of this block space and I want to run a virtual blockchain, also known as a roll up or an l two, that is customized for the application that I want to run. And you don't have to worry about infrastructure, it's taken care of for you and you have all the flexibility of being able to define the other layers of the stack. So it's a very similar paradigm. And so just like to sum up, the benefits of building on something like Celestia are that you have this scalability where the block size is not fixed. It can scale with the number of light nodes and still preserve verifiability. You also have sort of like shared security between the different chains, which maybe we can talk about a little bit more in detail later.
00:19:17.704 - 00:20:28.148, Speaker B: But basically roll ups that share a common data availability layer have this privileged ability to have what's called trust minimized bridging, where they can more or less run light nodes of each other. That provides you with more secure bridging than the standard committee based bridging that we have today. Then last but not least is the sovereignty and flexibility that you get because you're not locked in to a specific operating system, you get the ability to choose. The other thing is just to talk a little bit more about shared security rather than having to, if you wanted to. Let's say the other alternative to deploying a roll up is you want to have your own chain, is to deploy a new monolithic l one, sort of like cosmos, for example, you could launch a new tendermint chain. The problem is you have to bootstrap the security from zero. So you would have to issue a token, you would have to get a validator set.
00:20:28.148 - 00:21:15.524, Speaker B: And then the security of your chain is basically proportional to what is the value at stakeholder. And then when you roll up and you build on something like Celestia, you actually plug into the security layer of Celestia. You don't have to bootstrap a new consensus network. You don't have to worry about how much stake you have because that you inherit from Celestia. We covered monolithic to modular, so hopefully people have some background there. Now I'm going to talk a little bit more about some of the key concepts. So one of the first things that people always get wrong about modular blockchains is that they think that data availability means data storage.
00:21:15.524 - 00:21:55.022, Speaker B: And that is not the case, and I totally get it because the name data availability sounds like storage, right? So it's really kind of like, it's just, you know, we thought about trying to change the name. It was kind of like, it's stuck. It's a technical term, and. But data availability is probably better thought of as data publishing. So the way to think about what data availability is is that it's like a. I mean, I also use the sort of analogy of like a newspaper. It's like, you know, the newspaper is all about distributing information.
00:21:55.022 - 00:22:41.908, Speaker B: It's all about distributing the most valuable, time sensitive information for everyone to see and read about and understand and know. Data availability is very similar. It's about, okay, here's all the most important things that have happened on chain, and we need to get this to as many nodes as possible so that they can see and see all these transactions and get this information. So that's what data availability is about. It's about publishing and making sure that the data behind the events, the transactions are supposedly happening on these roll ups and happening on chain is known publicly to everyone in the network. That's what data availability is about. And data storage is something that's very different.
00:22:41.908 - 00:23:41.954, Speaker B: Data storage is more like a library. It's like, okay, there's some data. It's actually not time sensitive or critical to the security of the chain, necessarily. We just want to make sure that it's stored somewhere so that if we need it in the future, we can get it. But a library is not actually taking that, let's say it's a book and then distributing it to a bunch of people. It's just taking the book and then putting it down in some file storage system and plugging it in so that, oh, when someone comes later and says, I want that book, they can pull it from the shelf, right? So it's a very different thing. So I just hope that people don't make this mistake, because this is a question that we get all the time, which is like, well, why can't you just use filecoin or some other data storage network to do what Celestia does? And that's just because they're totally different, orthogonal.
00:23:41.954 - 00:24:29.804, Speaker B: Like, storage and data availability are just orthogonal things, and then they require completely different protocol designs. So now let's talk about data valid sampling. So a very core concept of, like, what powers Celestia, like, what is the secret sauce that enables Celestia to do what it does? Well, that is this concept of data availability sampling. So what data availability sampling is, is this. So now the goal of a data availability layer like Celestia, is that, okay, we want it normally, to make sure that data has been published, you would have to just download all that data yourself. That's the only way to know is the data there. You'd have to download it, all of it.
00:24:29.804 - 00:25:37.066, Speaker B: Of course, as we discussed, that does not scale as a solution, because now, as the amount of transaction data increases, I have to download more and more data. I need more and more bandwidth, and eventually I just don't have enough bandwidth to actually keep up with the chain. I need to be in some massive data center with a gigabit connection or something to verify the chain. The goal of a data availability layer like Celestia is to find a way to enable people to verify that the data is available that has been published without having to download all of it themselves. And this, on the surface, sounds like kind of impossible, but there was a very clever technique that Mustafa came up with in collaboration with Vitalik and this other guy, Alberto Sanino, in 2018. It's a really great paper where they designed this new system called data availability sampling, that enables you to verify the data has been published with a very, very high statistical guarantee, without having to download all the data. And we'll talk about that more in a little bit.
00:25:37.066 - 00:26:36.034, Speaker B: But the point of that is that now, all of a sudden, you break that trade off. You can now have big blocks, gigabytes of data per block, let's say, and you just have to download a few kilobytes, or maybe tens of kilobytes, basically, to verify that all that data is published. Let's talk about that a little bit more. One of the core technologies behind data availability sampling is the same thing that we use in CD roms and QR codes, is called erasure coding. Basically what erasure coding is is it adds redundancy to a file, like a data file. So if you have your CD encoding, an MP3 file, now, the problem is if the CD got scratched, then all of a sudden you wouldn't be able to recover the file, you'd be missing parts of it. So what they do, and obviously it's going to get scratched, or maybe the copying of that data is not actually perfect.
00:26:36.034 - 00:27:10.948, Speaker B: What do they do? They add redundancy. So if the original file was like 1 mb, they're going to add extra data to the end of the file, such that if you lose any part of it, any smaller part of it, you can still recover the original file. All of a sudden, you have a self healing data file, if you will. Right. And the same thing is using QR codes. Like, you can try yourself and scan these two different QR codes. Even though one is missing a large chunk, it still works.
00:27:10.948 - 00:28:03.482, Speaker B: And the reason you can cover up different chunks of a QR code and it still works because it has this redundancy baked in. So basically, the way that data availability something works under the hood is that. Maybe I'll go back to this slide. You can imagine that if you have the original block data and you extend it using erasure coding, you make it bigger. Now, if someone wanted to hide even a small part, like make sure that no one could know the data that was in this portion of the block, they would have to make sure that they've, because the data would self heal if they just hid that part. The remaining extra data is going to be enough to heal and recover what was trying to be hidden. So now, all of a sudden, if you want to hide any part of the block, you need to hide a really large portion of it.
00:28:03.482 - 00:28:53.932, Speaker B: So let's say we extended the block by two. You need to hide 50% of the overall block to hide any of it. So all of a sudden, you have this property of like, okay, now if you're trying to hide data, it needs to be a large amount. Now, what you layer on top of that is a sampling scheme where you choose randomly random chunks of that block to sample and say, hey, you block producer, give me that data. Let me make sure that it's there. And every time the block producer is able to basically respond with a proper sample, then you become more and more confident that the block data is actually all available. So it's kind of like flipping a coin, where if it keeps coming up heads over and over again, you become more and more confident that it's always going to come up heads.
00:28:53.932 - 00:29:26.408, Speaker B: Right? So it's similar. Like, the more you sample, you gain more and more confidence, up until the point where eventually, after enough samples, you have a 99.99% confidence that the whole data is available. So it's like a probabilistic way of verifying data availability. And, yeah, this is the paper. I encourage everyone to read it. And, yeah, again, like, the core concept here is that now instead of running a full node, we have to download the full block.
00:29:26.408 - 00:30:27.754, Speaker B: You can run light node, which just downloads a very small portion of it, and you get the same security. Essentially, you can run even like a light node on a phone, which we do actually have an Android phone on, which I run a light node. So it's really, really light. Let's see again, another core concept of datability sampling. The way that it works is that in order for the scheme to be secure, right, you need to have enough samples. You need enough samples to be taken among all the light nodes that they can perform the self healing operation of the block data. If you don't have enough people sampling, then it's possible for the block producer to basically trick a bunch of light nodes of the day is available when it's not.
00:30:27.754 - 00:31:13.792, Speaker B: So you need to have a minimum number of light nodes such that they're going to sample enough of the block data that they can recover the original block. And so that's why you have this property where as long as there's enough light nodes in the network, you can securely increase the block size without losing the verifiability property. And again, like a light node is very different to a light client. So people may be familiar with a light client from other chains. So a light client is basically tries to, it's a similar thing. It's like, okay, let's say you don't have enough resources to run a full node. You want to run something that at least gives you some level of security to verify the chain.
00:31:13.792 - 00:32:09.188, Speaker B: And essentially what a light client does is it just looks at the block header and says like, you know, does this have enough mining weight behind it in the context of bitcoin? Or, and like, you know, there's like the consensus rules basically being followed and, or in a proof of stake thing is like, oh, enough of the validators sign off on this block header. And if they did, it just says, okay, I'm going to trust, I'm not going to actually look inside and inspect that they followed the rules in these blocks. I'm just going to assume that the validators were honest. You can verify things this way. And this is actually also how, by the way, committee based bridges work. Like for example IBC, they run a light client of another chain in their chain. And the problem is that then you're trusting the majority of validators in the other network to be honest and they could just lie to you.
00:32:09.188 - 00:33:08.040, Speaker B: They could actually say they conclude and be like, you know what, actually we're going to break all the rules and mint a billion dollars to ourselves and the other chain's not going to know about it, or this user is not going to know about it. A light node is something where you don't actually have to make that trust assumption. You are actually verifying the chain more or less directly. It's not the same security as a full node, but it's essentially the same. You're not just checking the header of the chain, you're also sampling the data behind it to make sure it's available. You're also verifying the fraud or ZK proofs of the roll up execution layer that you care about. That's why the phrase light node originated from this idea of it's portmanteau of full node and light client.
00:33:08.040 - 00:33:46.598, Speaker B: So it's like the resources of light client with the security of a full node. So it's a light node. And then another concept to think about in the modular stack is you have different flavors of rollups depending on how you mix and match the different components. So maybe the standard roll up essentially has at the bottom a consensus in the a layer like celestia or ethereum. It may have a different settlement layer, or that could be, in the case of Ethereum, it might be Ethereum itself. And you have this execution layer on top. There's a bunch of different ways you can build roll ups.
00:33:46.598 - 00:35:34.564, Speaker B: And that's part of what's so exciting about modular blockchains, is mixing and matching of different components. For example, celestium. What's most popular right now is that Celestia is being used as external data availability layer to Ethereum l two s, which is of what this celestium column represents, which is that you're running your roll up, it's posting data, celestia, but it's settling to ethereum. And then you also have these things called sovereign roll ups, which are like they do their own settlement, essentially. So rather than having their fork choice rule be embedded on a contract on another chain like Ethereum, they do their for choice rule natively on the nodes. Okay? So hopefully that covers like the key concepts of modular blockchains. I think there's probably more that we could have talked about with roll ups specifically and how they work, but essentially just to cover that really quickly, the way that a roll up works is essentially you have some kind of proof system that makes it so that rather than having to verify every transaction that happens and make sure that each transaction follows execution rules, you have someone, either, you either trust that if someone tried to publish an incorrect state transition, like tried to break the rules, that someone will generate a fraud proof and you'll be able to verify it on your little light node, or you have someone ZK prove that execution such that it's, you know, that it's correct.
00:35:34.564 - 00:36:35.472, Speaker B: So the advantage of that is basically all of a sudden you don't have to re execute every transaction if you want to verify that roll up. Okay, so now let's transition a little bit to be more forward looking. I want to talk a little bit about the path to a million roll ups. So you may have, like, seen people on Twitter, like myself or David Hoffman or other people, talking about this idea of, like, a world of a million roll ups. And, you know, you might be wondering yourself, well, like, why that sounds kind of outlandish, right? Like, right now there's maybe 50 roll ups on l two beat. You know, why a million just sounds, like, absurd, right? And why also should we want that? Aren't roll ups don't roll up just lead to fragmentation? Don't they have all these bad trade offs? And so, like, a million roll ups just sounds like this, like, chaos world where, you know, like, nothing's actually going to work. Well, I'm going to talk a little bit about why I think a million roll ups.
00:36:35.472 - 00:37:20.734, Speaker B: Getting to a million roll ups is really valuable. Then I'm going to talk a little bit about each layer of the stack. It's not going to be exhaustive because there's just too many things. The modular stack is really growing, and there's just so many different things happening that I can't cover everything in this presentation. This will give you a high level overview of where we are at each layer of the stack on the path to a million rollups. Okay, so first, why would we want a million roll ups to begin with? Well, if you think about. If you go back in time and you think about the early Internet, right, it was extremely small scale compared to today.
00:37:20.734 - 00:39:05.900, Speaker B: It was like a few hundred servers and, I don't know, a few thousand or maybe a million users or whatever in the early days, right? But that was so tiny compared to where the Internet has grown to today, where there are, I think, over 200 million active websites and applications, and there's literally billions of users around the world. And I think web3 is on a similar growth trajectory where we're still so early. There's, like, very few million users or tens of millions of users. We have a few hundred chains. But where we're going to is this really rich, diverse world with way more applications and chains and users. And I think that this is like, kind of a prerequisite to achieving the true promise of web3 is we need to, you know, part of, like, you know, why the web has been able to be successful is like, there's just so much experimentation, right? And I think cloud as I mentioned, had a big part of this where like the more like, in order for the web to get to where it is today, right, there needs to be very scalable and flexible infrastructure for developers to build on. Scalable, because like, okay, well, you know, how are you going to, how are you going to get a billion users and hundreds of millions of websites if there's not some kind of like solid infrastructure that can actually support that amount of activity and usage? And then second, flexible, because if we want to.
00:39:05.900 - 00:40:09.784, Speaker B: The way that the web two kind of unlocked all these killer use cases is that by lowering the cost of innovation experimentation, people were able to just like build stuff and ship it and iterate and experiment at a low cost. And so that's why we have all the amazing applications that we use every day, is because it became very easy to experiment thanks to the cloud. And so I think modular blockchains have a similar goal. Like a, they want to be extremely scalable for all the technology, the technical innovations I mentioned then, second of all, they want to really make it easy and flexible to experiment. So you can now go into the execution layer and try new things that have never been tried before. And by having all this flexibility, we're going to be able to find the killer use cases. It's not like if we knew what they were a priori, then they would already be built.
00:40:09.784 - 00:41:37.964, Speaker B: But the thing is, when you have something as fundamentally paradigm shifting as blockchains, right, it's not going to be clear what the killer use cases are going to be. We have to kind of discover them. So anyway, where are we sort of like on the evolution of modular blockchains? Well, and on the path to a million roll ups? Well, Celestia, you know, after being, you know, the white paper being published in 2019, launched in 2023, and this is really like a landmark launch in the sense that it's the first time that data availability sampling has actually been implemented and is working on a live network. And this is very similar to me to sort of like the advent of broadband, I guess, for the Internet, where the equivalent is that it's just increased the capacity and the bandwidth of decentralized systems because it's a step function change in the scalability properties of block space. You also think of it as this big bang for the modular ecosystem, where now that we've removed that constraint, at least of block space, all of a sudden we now have the ability to experiment and build on top of something new. We were constrained. Like there were a lot of roll ups on Ethereum but they were really constrained by the throughput of Ethereum.
00:41:37.964 - 00:43:03.412, Speaker B: And so Celestia is leading this charge of like, this is a new wave of expansion and growth in a new category of blockchains called modular blockchains. And following Celestia's launch, a bunch of different ethereum l two s basically migrated to using Celestia for Da. And the amazing thing is that it resulted in millions of dollars in cost savings across all these different roll ups and started to sort of prove out this new reality that we're in, where all of a sudden you can run an application that has price points, you can spin up basically your own blockchain extremely easily, and it can have really, really low costs for users. And so it's like, it's really opening the doors of what's possible. And I think this is really just the first step and kind of just like proving out that this system works. And now, like, sort of we're entering into another phase where we're starting to have more meaningful integrations and deployments, such as blobstream, which has been deployed now on base as well as on arbitrum. And essentially what this unlocks is the ability for people to deploy l three.
00:43:03.412 - 00:44:38.564, Speaker B: So rather than only just settling to ethereum with underneath, now you can settle to base or arbitrum, or frankly, maybe in the future, even bitcoin or, you know, insert other ecosystem or settlement layer into that mix. And what's so exciting about that is that, like, you can think of Celestia as sort of this very pluggable, modular data availability layer that can help any other ecosystem scale, because, like you, once you reach your throughput constraints or capacity on that chain, you can start to offload some of that throughput capacity to Celestia into L2s or layer threes that are building on top of celestia. So that is sort of like this one of the goals, right? And I mean, beyond this, I didn't have time to add this to the slides, but Celestia is also integrating. Currently, all those roll ups that I showed above were all op stack roll ups, but we just recently announced the integration with arbitrum orbit and the support for nitro fraud proof. So now we're actually having some of the first fraud provable data value sampling roll ups coming to market. And I just think it's an incredibly exciting time to be part of this modular blockchain movement because every month there's an insane amount of progress. By the way, I know I just talked a lot about Celestia specifically.
00:44:38.564 - 00:45:40.324, Speaker B: But if you zoom out, this is a much bigger ecosystem than just Celestia, and there's a bunch of different components and parts of the stack. And really something I want to drive home is that the modular blockchain movement is something that is so ambitious and big that there's no way any one team can build it. In general, the goal of building blockchains and web3 is so big that there's never going to a monolithic single chain. One core dev team is never going to be able to actually build it out. You're going to need to be a collaborative effort among tons of different teams and developers. What's so cool about the modular stack is that you can now identify a specific problem in the infrastructure that you want to tackle and you can just go and try to solve that problem. You can have your own opinion about what's the right solution there.
00:45:40.324 - 00:46:14.264, Speaker B: The other thing is there is no right solution. It's all subjective. And that's why we say things like build whatever is because we believe that people should have the ability, that developers should have the ability to choose from the best solution for them. One size fits all. It's about what fits you the best. So I think it's really important basically to drive home that there's a huge amount of teams building different things. And this list is growing every day.
00:46:14.264 - 00:46:39.314, Speaker B: Okay, so I have like ten more minutes. I'm going to try to get through all the rest of this stuff. Now. I want to talk a little bit about specifically what's happening at each layer of the stack. And again, this is not exhaustive. This is just like the four main layers that people talk about, but there's lots of different components that are not covered. And I'm also taking a little bit of creative freedom to interpret some of these layers a little bit.
00:46:39.314 - 00:47:21.006, Speaker B: So the first one is data availability. Data availability is essentially, as we talked about, it is one of the core scaling bottlenecks of these systems. You need, like it's where the block space comes from that you run all these chains on. And data availability is very important. It's critical to the security of your chain. And so what we want from data availability, from the data availability layer, to get to a million roll ups, is it needs to have way like so much throughput, like gigabytes of throughput. The gigabyte, I don't know, gigabytes per second.
00:47:21.006 - 00:48:15.054, Speaker B: I should actually do some back in the napkin calculations, but we need a lot, basically, and we want it to still be verifiable and secure because it's really the base of the foundation. And so where we are today is that celestial launched recently, EAP 444 launched. And that increases the throughput capacity of the DA layer quite a bit, but still orders of magnitude away from where we need to be for this future of million roll ups. The nice thing is that because there's data available in sampling, it's more about scaling. I mean, there's a lot of different scaling bottlenecks that come through to overcome. But the nice thing is that data really sampling is working in practice, and that means that we can now start to scale block space production more aggressively than we could before. What's also really nice is we have projects like Avail Eigen Da and near coming on, which are also going to increase throughput DA layer.
00:48:15.054 - 00:49:07.654, Speaker B: But one of the other things, aside from not having enough throughput, is we also need to start rolling out more light node adoption. So, for example, one of the things Celestia is doing is we are working on rust clients to be able to embed light nodes in browsers and mobile apps. So I think that's really exciting because that's how we can actually have it so that users are running light nodes by default, and we end up scaling the amount of light nodes. All right, the consensus layer, this is where ordering of transactions happens, right? So consensus is all about, like, ordering data, ordering transactions. And this is an important thing because this is where transactions either get excluded or excluded. So there's censorship, resistance. There's also, like, liveness, you know, if the consensus layer goes down, same with the data availability layer, frankly, like, you don't really, you're not able to finalize more blocks.
00:49:07.654 - 00:50:10.684, Speaker B: And the other thing that's really important about consensus layers, this is where MeV lives, because MEV is all about manipulating the ordering of transactions to extract value. So where are we with consensus? Like, you can use Ethereum and Celeste as your consensus layer as well as DA layers, but most roll ups have a centralized sequencer running on top of that. And the reality is that's just not that great, because that sequencer can censor you, or it could go down and the whole roll up can halt. Or also, there's no MEV mitigation in most of these roll ups and sequencing schemes. So, like, that sequencer, just either it captures MEV itself or you end up with these. Like, if you don't have the right tooling, you end up with really bad UX because people are spamming the sequencer, trying to front run or back run, and things like that. So there's a lot to be still desired of the sequencing there.
00:50:10.684 - 00:51:18.004, Speaker B: I will say that fortunately, a lot of these centralized sequencers do have inboxes or exit schemes where even if the sequencer acts maliciously, there's ways for you to get out without too much downside to users. But what's coming exciting is we have shared sequencers, Astria and Espresso and no kid and Radius, and I'm sure there are more. And Madara and also op stack is working on things like the super chain. We also have sovereign Labs, which is working on base sequencing, so you can actually sequence directly on Ethereum or Celestia. But the cool thing about shared sequencers is that they're initially basically like decentralized sequencing as a service. And some of these teams, like at least Astria I know, and also Radius, are building in different mev tooling. Radius has things like threshold encryption for their mempool, and then Astria has things like MeV Boost or flashboss esque mev tooling.
00:51:18.004 - 00:51:51.630, Speaker B: A lot of these are quite close to launch, and so this is going to really level up the sequencing game for the ecosystem. Next is settlement. And so settlement is basically where, you know, I'm kind of using the word settlement to mean bridging in this context. So it's basically like, well, when you have all these different roll ups, they're going to need to be able to connect with each other and interoperate with each other. Otherwise they're just islands isolated from each other. And then you don't like the functionality is isolated and, and so much smaller. We don't want to have this fragment.
00:51:51.630 - 00:52:31.164, Speaker B: It's like basically overcoming fragmentation is the way to think about the settlement layer. And we're definitely going to need that if we're in a world of a million roll ups. So where we are is like, we have a lot of cool bridge. I think there's a lot of cool committee based bridging solutions. And also I didn't mention this idea of chain abstraction, but basically, if there's a million roll ups, you can't expect a user to manage a million different wallets across all those different roll ups or accounts. And you also don't want them to have to send, if they're trying to do something, cross chain. You don't want them to have to sit there, wait for the first transaction, then go to the next chain, do the next thing and so on and so forth.
00:52:31.164 - 00:53:17.270, Speaker B: You're going to want it to be able to just like them to send one thing, have it all happen behind the scenes and get done. And so that's what chain abstraction is all about, is making it so that to the user they don't have to think about the complexity that's happening at the roll up layer. It all looks and behaves like one unified system to them. The reality is bridging actually does work in the modular stack. It's actually not as bad as people think if you try things like hyperlane or even there's another one called relay and we have across and whatever. But the reality is a lot of them are still not very secure because the proof systems are not like the bridges are all committee based, essentially. And like the problem with a committee, when your committee is a roll up sequencer or like a multisig, is just really not that secure.
00:53:17.270 - 00:53:37.944, Speaker B: So we need to get to more proof based bridging. And frankly, the UX is still clunky. We have things like skip API, which make it slightly better, but we can use intents to make things better also. One something that's really exciting is a polygon aggregation layer. Last is execution. I'm just running out of time, so I kind of go through this. Execution is where all the smart contracts live.
00:53:37.944 - 00:54:40.404, Speaker B: It's where developers build. There's a huge amount of progress on the EVM side of supporting EVM execution layers and roll ups. There hasn't been enough dedicated to Alt vms, or at least we don't have enough of them live. We have Cosmos SDK. But I'm really excited because eclipse and movement initiative fluent, all these Argus, they're all basically shipping these new, all vms like SVM move WaSM world engine with things like SP one. And I think the progress of ZK proofs has been very promising because I think we're getting close to this breakout point, essentially, where we're going to have a lot of support for a lot more different vms, and they're going to be performant and secure because we're going to have real proving systems. Anyway, all that was a ton of talk about infrastructure, and I just want to leave off by reminding people that ultimately module blockchains, there's a very common criticism which is, oh, it's just infrastructure for infrastructure's sake.
00:54:40.404 - 00:55:43.500, Speaker B: But the truth is that all this infrastructure that's being built is in the service of developers and users. The reason why Celestia was built, and why all these other different modular components are being built is to serve developers so they can build new, compelling applications and bring and onboard more users to web3 and unlock all these use cases that we don't know of yet that can often get lost if you're like, oh, whatever. You guys need to think more about users. It's like, here we are. That's the whole point of this. That's also why I want to say that it's a really exciting time to be part of the modular stack as a developer, because right now all these new components are being shipped and it's unlocking all these things that you couldn't do before on the monolithic stack. So if you're ambitious and you're willing to experiment with this stuff, I think there's a lot of alpha, there's a lot of things that you could unlock for the first time and build for the first time that no one's been able to build before.
00:55:43.500 - 00:56:23.514, Speaker B: And that's why the motto of Celestia and modular blockchains more broadly is like, build whatever. It's about the fact that you can build whatever you want. And it's like this open canvas for you to explore. There's way more block space throughput. There's all these new different things you can customize and experiment with. So the constraints you had before are blown down. Of course, that's why we're doing the infinite space bazaar is to attract and nurture the first cohort of builders in the modular stack, which is you guys really excited about working with you all and seeing what you build.
00:56:23.514 - 00:57:07.452, Speaker B: And I want to say that we're doing modular summit in July. This was already announced, but it's going to be a really exciting place to all converge and talk about the infrastructure, talk about the applications that are being built in the modular stack. And so I encourage people to come if they can. And again, remember the high level vision here. We want to get to a world of a million roll ups. And that's going to take both infrastructure as well as developers and people experimenting with the stack, and a feedback loop of learning from each other and together is how we achieve this end vision. That's all I have.
00:57:07.452 - 00:57:23.854, Speaker B: I'm on Twitter, obviously. Then here's some resources you probably already aware of. The learn modular page is a great place to learn modular concepts and then build modular, which is the developer portal. I'm sure you guys have already seen that. So thank you very much.
00:57:25.674 - 00:57:49.324, Speaker A: Yeah, thanks a lot for this great workshop. I hope the audience learned a lot more about modular blockchains, how it differentiates to monolithic blockchains, the benefits. So let's head over to the audience questions. We're a bit late in time, but there's like one question from the audience. In short, we can't use Reddit, mysql, mongodb for da.
00:57:51.144 - 00:58:29.592, Speaker B: Yeah, so good question. So DA is not like a database. It's like it needs to be decentralized. So you don't want to just, for example, upload your block data to Amazon s three and be like, oh, I'm done. The data is available because again, it's about data publishing. It's about the verifiability of the system. If we could just run web3 on centralized storage like Amazon, it would already be scalable and be trivial to build whatever we want.
00:58:29.592 - 00:59:24.722, Speaker B: The thing is, the core essence of what makes web3 web3. What makes a blockchain a blockchain is that you can verify things, right? That's what, that's where this notion of rules without rulers comes from, is the fact that anyone on the Internet with an Internet connection can run a node and verify your thing, your application that you built and know there's no funny business. The things that he, you know, what he says he's going to do with my money and what the code says is actually going to be upheld by the network. If you don't have that property, then it's like, oh, I'm not going to deposit my dollars into your Dex. Like, you could do it like, you know, if it was running on s three, you could just like literally walk away with it. Right? So, um, uh, you, you know, uh, so, so basically, yes, you need to have like a decentralized system like Celestia to do that. And the way to that, you, maybe you're asking specifically about like how do you, you know, um, like, I guess like put data onto Celestia.
00:59:24.722 - 00:59:56.968, Speaker B: And how do you get it back? Because it's a decentralized network. It's not Sql. It's like you use our node API. You have to basically send a transaction, which is a pay for blob transaction, which defines what's the data you want to put on it. Then when that data has been included in a block, there's ways for you to basically get a proof that the data is there and show that proof to other people. That's very different to the interface of the database, if that makes sense.
00:59:57.136 - 01:00:02.604, Speaker A: Yeah, I think that's a good response. Do you have any final remarks before we wrap up the session?
01:00:03.704 - 01:00:26.514, Speaker B: No, I just hope that that was interesting and useful to folks. So, yeah, if you have any questions, ping me on Twitter and I'd love to share more knowledge if I can be useful, and I just want to see what you guys build. I'm really excited. And so also, if you have ideas that you want to float, like, I'd love to hear them.
01:00:27.334 - 01:00:57.290, Speaker A: So build whatever. So thanks a lot for the session. Thanks Nick from Celestia for joining us today. There will be another workshop tomorrow at the same time, and also next week are a few other workshops. So feel free to also subscribe to our YouTube and our Binance live channel. If you not have joined the Celestia discord yet, please free please feel free to join the discord. Yeah, there's a lot of support mentorships that you can get in order to build a cool product.
01:00:57.290 - 01:01:00.174, Speaker A: So thanks a lot for joining us today and see you soon.
01:01:01.354 - 01:01:04.034, Speaker B: Thanks everyone. Thanks.
