00:00:00.810 - 00:00:16.350, Speaker A: Hi everyone. This is John Adler and Mustafa Albasam from Celestia Labs and we're joined with Togurul, a researcher at scroll who will be presenting a Scroll ZK rollup. Togarul, would you like to introduce yourself and give us an overview of the technology behind scroll?
00:00:17.250 - 00:01:13.726, Speaker B: Hello. My name is Togurul. As John already mentioned, I do research at scroll, mostly on the protocol side, don't really do the cryptography and scroll is a Zkvm based zero knowledge roll up that is going to be deployed on Ethereum at some point in the future. So what is scroll? Scroll is a zero knowledge roll up that is built on top of Ethereum and is Zkvm based. What is zkvm? So you might have heard a lot of different things about Zkvm in the past few months and there are also quite a few flavors of Zkvm. So what is Scroll Zkvm? So, firstly, Scroll Zkvm is co built together with Ethereum Foundation's privacy and Scaling explorations team. It's been an ongoing collaboration for more than a year and it's an Opcode equivalent implementation of EVM.
00:01:13.726 - 00:02:07.858, Speaker B: And what I mean by upcode equivalent is that the implementation follows the specification of the EVM opcodes from the Ethereum yellow paper with the only one missing is self destruct. We're not going to add it, but bear in mind that self destruct is going to be removed from ethereum or substituted at some point in the near future. So it doesn't really affect us. So we're going to just add the opcode with which it's going to be substituted, and we're type two and a half according to the famous Vitalik post about zkvms and different types of zkvms, despite the fact that in his article. It says that scroll is going to be type two. But we're not, because type two requires an equivalent gas metering to ethereum. And we just don't think it's practical for a couple of reasons.
00:02:07.858 - 00:03:40.210, Speaker B: One is because if we were equivalent in terms of gas metering, that would open up quite a few DDoS vectors because let's say computing a catch up hash function in circuit is much more expensive than computing addition inside the circuit. And therefore the difference between the out of circuit pricing and inside circuit in circuit pricing is going to be quite drastic. And secondly, even if we were to support it and attempt to mitigate it, that would require us to sustain the same ratios between the fastest and the slowest UpCodes as there are in Ethereum. Which means that we would have to essentially add dummy gates to the faster UpCodes to make them slow and so retain the ratio, which doesn't really make sense. So we're not going to be type two, we're going to remain type two and a half, which means that we're going to retain all the properties of EVM aside from the fact that we're going to have different gas prices per upcode. And what's Scroll's mission? So scroll is main mission is to help Ethereum onboard the next billion users. So we know that Ethereum has there's quite a demand for Ethereum block space, but Ethereum just can't facilitate and we think that with more block space there's going to be more users and also more different ideas of how to use that block space which Ethereum doesn't really allow now because of the pricing.
00:03:40.210 - 00:04:39.586, Speaker B: And we want to achieve that by offering the most Ethereum like experience to both the users and developers. And what I mean by that is that when you're deploying an application or just playing around with an application, ideally you shouldn't feel the difference between using scroll and Ethereum. I mean, there are going to be minor differences but we're going to strive to be as similar to Ethereum as we're technically allowed to be. And we've been open source since day one. And we're also community oriented, which means we're open to collaborations and we welcome people to contribute to our code or our protocol in general. So why did we choose to build a roll? So in case of Ethereum, a roll up can have two purposes. Obviously, if you're building on celestial or building somewhere else, you can build for other reasons.
00:04:39.586 - 00:05:48.470, Speaker B: But for Ethereum specifically, you would either build to extend the throughput of the base layer in a trice minimized manner or you would build it to introduce new features to the protocol in a trice minimized manner. And what I mean by trust minimize in this case is that in order to bridge between the roll up and Ethereum you need the minimum assumptions possible. So in case the assumption is that the zero knowledge proof used to compute the validity proof is secure. And in case of an optimistic roll up, you just need to assume that there's one honest operator of an optimistic roll up node that is capable of challenging it within the window, within the dispute window. And why Ztvm? So firstly, it complies with our vision of offering the most Ethereum like user and developer experience. And secondly, as Alex Kukowski from ZK Sync likes to refer to it, EVM is the lingo franca of general Perks smart contract platforms. And what it means is the majority of the developers in Blockchain are familiar with Solidity, most likely.
00:05:48.470 - 00:06:52.670, Speaker B: And if you're a newbie and you're just a developer with a traditional background and you hear about the smart contracts for the first time, if you Google developing a smart contract, the most likely thing that you're going to encounter is tutorials on how to build on Solidity and the most of documentation is available on Solidity. What's the current stage of scroll? So we just launched a pre alpha testnet a couple of months ago. A pre alpha testnet in our case means that we basically allow users to sign up and we let them into the system one by one. Also, the deployment of contracts is permissioned. We deployed a couple of forks of uniswap et cetera, on the testnet to let the users play around with it. But users can't deploy their own bytecode and that's where the public testnet comes in. So it's currently in the works.
00:06:52.670 - 00:07:36.522, Speaker B: I don't really know the exact date on when we're going to launch it, but it should be in the near future and the public testnet will be completely permissionless, meaning that you can play around with it as much as you like, you can deploy whatever you want on it, et cetera, et cetera. And now for the future. So domainet is planned to be released in two phases. So phase one will have a centralized sequencer and decentralized prover and phase two is going to have decentralized sequencer and decentralized proverb. So now a bit about the design decision that we have agreed on early on and we're going to stick with. So firstly we're going to publish transaction data on chain rather than the state devs. And the reason for that, there are two reasons.
00:07:36.522 - 00:08:11.562, Speaker B: One. We believe that publishing transaction data will allow us to decentralize the sequencers easier. And also it allows us to have a system similar to how optimistic roll ups work. From the perspective that as long as you run an l two node and the data is finalized on chain, you can essentially consider the data to be finalized. So the only reason why you would need on chain finalization is for withdrawals, which is not really possible if you go with state diffs. And secondly, because you need to one.
00:08:11.616 - 00:08:17.370, Speaker A: Quick question, but does this transaction data include signatures as well or is it just the transaction payload?
00:08:17.870 - 00:09:26.290, Speaker B: So currently it includes the signatures, but we're thinking about compressing the signatures with a validity proof and just publishing the validity proof. So long term we're thinking about doing it that but currently we don't have that implemented. And the second reason why it makes sense for us is because if you don't have the transaction data on chain, you have to trust some external data provider to provide you with the transaction data. Because you can't really derive the transaction history from the state diffs. Whereas with transaction data it's all there and you can just derive whatever you want. And the second one, which we think essentially defines a roll up and a roll up without this feature is not really a roll up, is that the user should be able to force transactions into the roll up chain. And what I mean by that is you can submit the transaction through an L one validating bridge and it will force the sequencers to include it into the follow up at some point.
00:09:26.290 - 00:10:32.962, Speaker B: So let's say you can give it a few hours and then if no batch just include that transaction within the couple of hours, no more batches are accepted by the validating bridge without that transaction. Because while some roll ups take an approach of withdrawals in case you're being censored, firstly withdrawals are nontrivial to design in a system where you have arbitrary computation because you can have a scenario, let's say when a transaction is locked, your funds are locked, let's say in an LP on uniswap and you can't really withdraw directly. You need to first wind down your position and then withdraw. And secondly, we just think that this is a more elegant approach. So the architecture so in phase one, the architecture is going to comprise of the following things. So we're going to have the Ethereum validating bridge which will comprise of the bridge contract and the roll up contract and I'll explain what they are later. And we'll have two different nodes.
00:10:32.962 - 00:11:16.462, Speaker B: One is going to be the scroll node and the other is going to be the prover which we call the roller. And a sequencer is what we refer to as a scroll node. And it's comprised of three components a sequencer, a coordinator and a relayer. So a sequencer is a modified version of Gaeth which has essentially the similar responsibilities to an Ethereum full node. We just modified a few things. For example, we changed the Merkel Patricia tree to a Merkel tree and we use Poseidon hashing everywhere outside the EVM itself. So there are a few minor changes but overall the code is quite similar.
00:11:16.462 - 00:12:22.330, Speaker B: So the difference is not really massive with the original Geth implementation. And we also have the coordinator which is essentially responsible for propagating the execution trace of a batch to an elected pooler. So how this would work is you execute a batch, let's say comprised of 20 transactions, you extract the execution trace after you complete the execution and you send it to the proverb. And that's the responsibility of the coordinator. And then the relayer is responsible for monitoring the status of the roll up blocks and deposit withdrawals events on the roll up in Ethereum for the prover. The prover architecture is comprised of these things and basically first it receives the execution trace. It then builds the inputs from the execution trace that it receives and the Zkvm circuit is split into seven different circuits.
00:12:22.330 - 00:13:02.114, Speaker B: So one for storage, one for Ram, et cetera, et cetera, one for catch. And it computes all those circuits and then all those proofs. And then it aggregates it into one proof that is basically validity proof for a block. And so the prover is responsible for the computing, as I said before. And once it computes the validity proof it sends it back to the coordinator and we refer to the proverb as the roller in our documentation. And I've already explained this. And now the validating bridge.
00:13:02.114 - 00:13:42.834, Speaker B: The validating bridge is comprised of two contracts. One is the roll up contract that handles the canonical chain and the non finalized step. So let's say if you commit the new batch, it's not finalized yet. So the roll UP's responsibilities to handle it until the validity proof is submitted and it verifies it and then it's just assumed that that tip is finalized. And then the bridge contract which handles the arbitrary messaging between the ethereum and the roll up. So it can pass both the bridge, both the standard ERC, 20 tokens, et cetera, et cetera. And also just any messages you would like to send.
00:13:42.834 - 00:14:48.386, Speaker B: And this is how the workflow is for the phase one. So essentially, as you can see, the sequencer commits data to the roll up contract and then through the coordinator it propagates the execution trace for that particular batch to the roller. So let's say roller one in case of number one, it computes the proof and then it sends it back to the coordinator. But instead of the coordinator directly submitting it to the roll up contract through the sequencer which would be quite expensive. What it does is it waits for a few more proofs for different blocks batches and then propagates it to another roller which aggregates all those validity proofs together, returns it to the coordinator, which then submits it to the sequencer, which allows us to save some cost on verifying every single batch on chain. Validity proof for every single batch on chain. And now the phase two which will have the decentralized sequencer.
00:14:48.386 - 00:15:35.362, Speaker B: So first thing to note, this is still open area of research so a lot of things may change by the time it's released. But this is what our current thinking is. We're going to deploy a PBS style model in which sequencers perform the roles of the builders and the provers perform the roles of the proposers. We're going to add sequencer committees which enable the economic finality guarantees prior to on chain finality. So let's say you could commit the data on chain and if you have enough signatures you assume that it has economic finality. I'll explain what that means later. And an on chain challenge mechanism enabling the challenge to dispute the validity of a patch commitment.
00:15:35.362 - 00:16:52.522, Speaker B: I'll also explain that later and as I said, it's still under active research so things may change. So the PBS model is the goal of it is to solve the incentive imbalance between the sequencer and approvers. And what that means is because the sequencers can extract as much mev as they like, if we don't implement the model like that, there can be a scenario where it's not as profitable to operate approver as it is to operate the sequencer. And so the incentives would dictate that it makes more sense for somebody to become a sequencer rather than a prover. And we want to avoid that. And by deploying the PBS model we can maximize the profits of the provers and essentially lead to a scenario where the incentives are more balanced between the sequencers and the brewers. So how it would work is we would pseudo randomly extract multiple sequencers per slot and then they would propose the candidates which are essentially bids with the hashes of the block to a single pseudo randomly elected prover and the prover selects the one most likely with the highest bid because there's no reason why it shouldn't.
00:16:52.522 - 00:17:51.090, Speaker B: And then the sequencer would propagate the block for the hash that it previously propagated. And then we will go to the next stage after the proverb elects the candidate and receives the block, all the sequencers vote on the selected block. So it's essentially a mini consensus and if you get enough votes there is economic finality. Because let's say if at some point somebody challenges that commitment all those sequencers can get slashed. So essentially it's a system where if you vote for an invalid out you can get slashed later point and how the challenge mechanism works. It's still an area of active research. We still don't know if that's going to be efficient enough to put everything in circuit.
00:17:51.090 - 00:18:49.960, Speaker B: But ideally how the challenge mechanism would work is you would just input the compressed batch and the circuit would deal with the compression and everything. And even if you just input an arbitrary blob and input the previous state it will just prove that the output state is equivalent to the input state. Essentially if let's say you commit an incorrect batch that has an invalid transaction in it and you say that the state route is going to be this but then you compute this validity proof and it says that no, actually the state route is going to be completely different. You can just put the validity proof on chain, the contract would verify it and it will basically dismiss the commitment batch and also slash all the nodes that have all the sequencers that have voted for it. And that's basically the whole presentation. And if you have any questions, feel free to ask.
00:18:51.130 - 00:19:22.190, Speaker A: That was awesome. Thank you. Toga roll first, I'll mention that based on history maybe it's not so good to call your last phase phase two because that didn't go well for the last project. I tried that. First of all, I'm very excited at the kind of first ZK EVM ZK roll up to be introduced. So this is all super exciting from someone who's been following the roll up space for a while. I have a few questions.
00:19:22.190 - 00:20:05.680, Speaker A: The first one is with respect to the gas schedule that you were mentioning earlier that your instantiation of the ZKE EVM has the same functionality in the instruction sets except for the gas schedule. Do you see this as kind of a fundamental limitation of ZK? I don't want to necessarily say ZK EVM but just like these kind of Turing complete ZK systems do you see it as a fundamental limitation of them that they will basically for the foreseeable future have a very different gas schedule than a system that is not ZK based?
00:20:07.010 - 00:20:40.706, Speaker B: Yes. I don't see how you could basically have an equivalent gas metering system to ethereum in circuit currently. Obviously there might be some breakthroughs and things can change. But currently the difference between, let's say, addition and hashing Ketchup hashing in circuit is so drastic that doesn't make any sense to follow the schedule of EVM, the gas metering schedule of EVM.
00:20:40.818 - 00:20:47.290, Speaker A: And can you give us kind of maybe an order of magnitude approximation of how different that ratio is compared to the EVM?
00:20:47.710 - 00:21:18.230, Speaker B: I'll be guessing at this point. Don't really have the numbers off the top of my head, but it's going to be quite different in terms of Ketchup. Most of the other opcodes should be quite similar. It's mostly the storage and catch up that is the problem. Okay. Everywhere that hashing is involved is an issue for us and everything else should be not that problematic. There might be a few exceptions, but most of the time everything else shouldn't be that problematic.
00:21:18.730 - 00:21:26.710, Speaker A: And do you guys by any chance provide something like a poseidon hash pre compile or something? Or are there no extensions to the EVM?
00:21:27.370 - 00:21:45.920, Speaker B: So currently there are no extensions. We are discussing internally about adding a few pre compiles that EVM doesn't currently have in case people would want to use it for one reason or another. But currently it's basically the plan is to have the same functionality that Ethereum has.
00:21:47.250 - 00:22:04.980, Speaker C: How do you currently verify that? So you said that you mentioned that you posted transactions on chain. How do you verify inside the ZK roll up that the import data was posted on chain to Ethereum? Because presumably you need to use some kind of commitment scheme, right?
00:22:06.090 - 00:22:15.510, Speaker B: You mean inside the roll up chain or when you compute the validity proof. I'm not sure I'm understanding the roll up chain.
00:22:19.790 - 00:22:26.300, Speaker C: How do you check that the transactions inside the roll up match to onchain data?
00:22:28.270 - 00:22:44.190, Speaker B: So the assumption is that if you're a sequencer or approver, you run both an l two node and Ethereum node. So you can just basically check that your data for roll up matches the data on Ethereum committed to Ethereum.
00:22:45.570 - 00:22:49.780, Speaker C: But if you're the previewer, wouldn't you need to download all data?
00:22:53.110 - 00:23:29.600, Speaker B: So in the current model, what we do is because the sequencer is centralized and it's trusted in a way, and it's operated by us, the provers just trust that the sequencer committed the correct data. But for decentralized sequencer yeah. We would either have to have some form of stateless clients where you would just propagate the branches that are relevant and then you would just check that, oh no, actually no, that works.
00:23:29.970 - 00:23:54.280, Speaker C: I guess what I mean is like actually forget the prefer on the Ethereum smart contract. How does the Ethereum smart contract know that the on chain data posted to that smart contract is the same data utilized or outputted by the ZK roll up? Don't you have to hash the data or something like using chart 56 or something like that?
00:23:56.250 - 00:24:21.210, Speaker B: When you put the data on chain you store the hash in contract and then when you append the validity proof to that batch you input the hash of the data that you originally the batch commitment that you put in circuit. And then you prove in circuit that the data that you're proving the validity proof for is equivalent to the data that you committed to on chain originally.
00:24:21.370 - 00:24:26.740, Speaker C: So when you store the hash in storage, what hash function are you using?
00:24:28.950 - 00:24:31.646, Speaker B: Ketchup, the one that Ethereum uses.
00:24:31.838 - 00:24:44.230, Speaker C: I see. And then you also have to use Ketchup inside the roll up as well. I see, okay. That's a significant source for the proofing cost, I guess.
00:24:44.380 - 00:24:46.790, Speaker B: Yeah. That's going to be quite expensive.
00:24:48.090 - 00:24:50.550, Speaker A: And it's just a single hash, not a merkel tree.
00:24:51.210 - 00:25:05.514, Speaker B: No, at the moment it is a single hash. I mean, we could change it in the future, but it doesn't really make sense to use a merkel tree rather than the hash because you need all the data from the commitment.
00:25:05.562 - 00:25:20.500, Speaker A: Essentially, there are applications, for instance, if you have an off chain light client, then they would want to get, for instance, merkel proof that their transaction was included in a block without having to download full block data.
00:25:21.350 - 00:25:23.540, Speaker B: Oh, fair enough. Yeah. Okay.
00:25:23.910 - 00:25:34.680, Speaker A: Which aren't relevant at this stage for a lot of roll ups. But in the future, I imagine that we'll have a lot more things like off chain, like clients being a thing. But that's the problem for future us.
00:25:35.050 - 00:25:51.340, Speaker B: Yeah, I guess. Also, bear in mind, if we were to compute a merkle tree rather than a single catch up, it's going to be much more expensive in circuit, so it would just add additional cost that is unnecessary to us at the moment.
00:25:52.670 - 00:25:59.630, Speaker C: I guess it'd be like as twice as much as expensive because you have to do twice as much hashing roughly.
00:25:59.970 - 00:26:25.190, Speaker B: It will be more than twice because in case that you're hashing once, you're not doing the entire process recursively, you're just adding the inputs into the sponge. Whereas here you have to basically do get check recursively for as many times as you need to compute the merkel tree. So it's going to be much more expensive than twice.
00:26:26.010 - 00:27:15.240, Speaker C: Right, I see. I'm asking because we're also looking about how ZK roll ups can use celestial use the celestial data route. For the A, we use sha five six for the merkel route. So the roll ups have to verify a shaft merkel tree, but it would have to verify a merkel tree rather than a linear like a single hash. And we're currently thinking about the trade off there. Presumably the bigger and leaves are the less overhead because then the smaller the tree is. So that's something we're currently exploring internally as well.
00:27:15.930 - 00:27:16.680, Speaker B: Fair.
00:27:18.750 - 00:27:27.370, Speaker A: Yeah. Okay. I guess next question, which is currently are you guys only doing CPU proving?
00:27:28.190 - 00:27:34.190, Speaker B: No, we have a GPU implementation of the proverb and we're proving on GPUs.
00:27:34.530 - 00:27:39.082, Speaker A: Mainly even today you have a parallelized GPU prover?
00:27:39.226 - 00:27:40.590, Speaker B: Yes. Awesome.
00:27:40.660 - 00:27:49.150, Speaker A: And do you foresee in the future the usage of things like FPGAs or Asics for proving ZKE EVM circuits.
00:27:51.970 - 00:28:17.290, Speaker B: We are talking with a few FPGA manufacturers that specialize in in zero knowledge proofs and we would like to experiment with it, but we're not sure if the benefit of it is going to outweigh the cost, essentially. So we're not sure that the improvement in terms of efficiency relative to GPUs is going to be drastic.
00:28:18.830 - 00:28:24.734, Speaker A: Really? Could you provide some intuitions on why that would be the case? Because Asics can often be substantially more.
00:28:24.772 - 00:28:52.550, Speaker B: Performant than GPU in the current proof system. We need a lot of Ram. Essentially we're more constrained by Ram rather than processing, we're trying to minimize it. But I think the current hardware spec that we're running the prover on has like 512GB of Ram.
00:28:55.850 - 00:29:00.186, Speaker A: Of Ram, so that's before getting to the GPUs, or.
00:29:00.208 - 00:29:03.542, Speaker B: Are you talking about the total GPU while proving?
00:29:03.686 - 00:29:41.990, Speaker A: Yeah, got it. Okay, well, I guess that does make sense. If it's all Ram bottlenecked, you probably won't gain too much by using an ASIC. Okay, so I guess not. Follow up question, but subsequent question. You mentioned earlier in your slides that you had your current phase, a centralized sequencer and a decentralized set of provers. Can you maybe expand a bit more on intuitions around things like prover markets and so on? Because it's a pretty nuanced topic that's different than say, proof of work mining.
00:29:42.490 - 00:31:06.782, Speaker B: Yeah, some of the approaches that I've seen taken by other ZK roll ups is essentially make proving each batch of competition between multiple provers. And it's essentially advertised by some as proof of work. But it's not, because in proof of work you have randomness. So basically your probability of winning basically corresponds to your hash rate relative to the total hash rate in the system, whereas in here your probability of winning is almost 100% if you have the most efficient prover. And so if you do something like that, it has a centralizing effect because it will disincentivize others to prove. And our thinking is that it was inspired to some degree by Celestia, by the way, because in Celestia you have this idea the more light nodes you have in the system, the bigger the blocks you can have. And so our idea is that the more you can parallelize the proving, the bigger throughput you can have because you can just publish, let's say, 500 batches, assign them to different provers, and then just aggregate all those validity proofs into one validity proof that goes on chain.
00:31:06.782 - 00:31:33.240, Speaker B: And so our thinking is that we need to incentivize the provers to essentially have larger throughput. And there are a few ideas that we're exploring currently. The one that I described already is a PBS like model where you share the mev profits with approvers. But there are a few other things that we're currently thinking of.
00:31:36.350 - 00:32:02.130, Speaker A: Awesome. And to tie into this, my understanding is that based in the description, you use some sort of recursive proof scheme or you have different components of the EVM and then you kind of have this aggregate proof that has recursive proofs. Could you maybe contrast this with the Cairo model, which I think has like a more monolithic CPU style thing with a single proof?
00:32:05.270 - 00:32:42.880, Speaker B: So, as I described, our Zkvm circuit has seven subsurcuits. So for example, Ketchuck is a completely separate circuit and then we have a separate circuit for storage, for memory, et cetera. And we have essentially two layers of proofing. First you prove the subsurface and then you aggregate those subsurcuits into one single validity proof, whereas in Cara we just compute the proof without needing to compute the subsurface first.
00:32:45.650 - 00:32:49.380, Speaker A: And are there any benefits, trade offs of doing one versus the other?
00:32:51.830 - 00:33:17.660, Speaker B: I think if we could do it in one circuit, we would, but it would just be too inefficient because aggregation isn't free, it costs time and also computational cycles. So if we could do it in one circuit, we would do it, but it would just be too costly to do it in a single circuit. And therefore it makes more sense for us to just split it into multiple subsurfits and then aggregate them together.
00:33:19.310 - 00:33:41.090, Speaker A: Makes sense. Okay, I guess then. Following up on my earlier bench off point, which is that you talked about having a centralized sequencer and then decentralized provers, and then in the next step to have decentralized sequencers along with also decentralized provers. What are kind of your plans considerations around the decentralization of sequencers?
00:33:42.870 - 00:34:33.540, Speaker B: So, firstly, we want to minimize the overhead as much as possible. So I know that other projects are exploring adding tendermint or hot stuff or other consensus protocols on top of it. And we are thinking about minimizing it as much as possible. So we still would want to have committees. But ideally what we would do is we would have the ethereum drive, the consensus essentially. So you would commit the signatures to ethereum and it would take care of the timing and everything. But if we were to have blocks that are shorter than the slot times for ethereum, we would need a consensus essentially because there's no avoiding that.
00:34:33.540 - 00:34:42.120, Speaker B: So in the current model, we're aiming to minimize it, but we're open to the idea of adding a consensus at some point in the future.
00:34:43.290 - 00:34:49.994, Speaker A: Got it. So it'll be something along the lines of a proof of stake consensus protocol, like tendermint or something simple like that.
00:34:50.192 - 00:34:54.010, Speaker B: Yeah, tendermint or hot stuff or something along those lines.
00:34:54.510 - 00:34:57.574, Speaker A: Okay. I mean, seems pretty reasonable. No complaints.
00:34:57.622 - 00:34:58.220, Speaker B: Here.
00:35:00.110 - 00:35:02.800, Speaker A: Another question, which is oh, go ahead.
00:35:03.330 - 00:35:48.460, Speaker C: So you mentioned before, I think. So one of the complaints about the EVM is that it's hard to parallelize what you mentioned before. There's ways to paralyze it. Maybe you could expand on that because at the moment the bottleneck for current EVM equivalent or. Compatible roll ups is the fact that the EVM is not paralyzable, and optimism and arbitram, I think well, optimism reached three foot limitations, if I remember correctly, because the execution nodes just couldn't execute enough transactions. How would you paralyze it? Because I think you mentioned before there are ways to parallelize it.
00:35:49.230 - 00:37:09.250, Speaker B: So you could have optimistic parallelization where you enforce the access lists that are currently optional in. So the model would be similar to how Solana does it, where you list all the slots that you're touching in the access list, and then if the transactions try to touch a slot that is not listed essentially in two transactions, try to touch the same slot, you revert to sequential execution. I think somebody has done experimentation with this a year or two ago, and there was even an implementation of GitHub, and in their Rudimentary test, they achieved like five x execution throughput. But I think it could probably be optimized more. That's the lowest hanging fruit that you can basically take in terms of how you can parallelize execution. But there are other approaches, but that would probably require modifying EVM, which we don't.
00:37:10.410 - 00:37:26.860, Speaker C: Would there be a gas penalty if you try to access something that's not in the access list? Yeah, I see. I think that makes sense because assuming that you have different contracts, they're probably not going to access the same state anyway.
00:37:30.110 - 00:37:54.340, Speaker B: I think in most cases that's fine, and in the cases when it's not, we can just fall back to sequential execution, which should happen rarely unless there is one dominant contract that everybody is attempting to use and one specific slot in that contract that everybody is attempting to write and read from.
00:37:56.650 - 00:38:04.120, Speaker C: Yeah, that makes sense. I guess you would have to maintain locks for each.
00:38:08.490 - 00:38:48.046, Speaker A: Yeah, I think it was Brock Elmore who developed a prototype on optimistic concurrency. I did talk to him about this not too long ago. I have concerns around optimistic concurrency because it's optimistic in the sense that it's fine when things are a happy path. But a lot of things in blockchain you have to optimize based on worst case performance. And in the non optimistic case, someone could potentially have some denial of service vector. For instance, they could have something like transactions, a bunch of transactions that all call the same sequence of contracts. So now you execute them parallel and then okay, exclude them back in series versus parallel.
00:38:48.046 - 00:39:27.570, Speaker A: And then potentially you have to attempt this multiple times and it opens the door for lots of potential denial service vectors if you're not very careful, if you take an optimistic approach. So I'm not saying it's impossible, but it's definitely something that's in the early prototype stage right now and that there's still a lot of research in there. Speaking of implementations, you mentioned earlier that you took guess, forked it, I guess, and then replaced the Merkel patricia tree with a Merkel tree. What kind of merkel tree is that? Is it just a regular binary merkel tree or is it a sparse merkel tree?
00:39:29.910 - 00:39:44.614, Speaker B: We had a couple of implementations. I'm not sure what we're currently using because I'm not really involved in the programming side of things, but I think we're just using a standard merkel tree that uses Poseidon hashing instead of get.
00:39:44.652 - 00:39:47.430, Speaker A: Check for the state tree.
00:39:47.930 - 00:39:48.680, Speaker B: Yeah.
00:39:51.290 - 00:40:11.738, Speaker A: They should definitely look into this more because merkel trees are generally not updatable in place. You can't do an insertion of a key in place. You can, I guess, update a key, but you can't insert a new key in the middle of the tree without incurring a lot of cost. So I wouldn't be surprised if it's something more along the lines of sparse medical tree.
00:40:11.754 - 00:40:34.294, Speaker B: But there's because I think it might be a sparse medical tree because I remember we wrote a document about how we implemented the merkel tree, and I remember going through it, but I don't remember the details, so I don't want to mislead anyone here.
00:40:34.492 - 00:41:29.960, Speaker A: Well, I mean, it required a whole document then it was probably something like a sparse merkel tree as opposed to okay, well, that answers that question. Okay, mystery solved. You mentioned earlier that you're not implementing the self destruct instruction. One of the reasons was that it would be remote soon. I mean, those are some famous last words based on the previous cadence of modifying the EVM. I'm not sure I'll be so optimistic about the timelines, but regardless, this kind of points to potential limitations of ZKE EVMs. Can you kind of elaborate on any potential shortcomings or limitations or challenges around why you couldn't implement the self destruct? Or was it purely just because it was just extra work that you didn't think you needed to do because it was going to get moved soon? But there was nothing fundamental preventing this.
00:41:31.130 - 00:42:04.530, Speaker B: A self destruct could have opened up the dose of actors because let's say if a contract has 1000 slots, you need to prove the removal of 1000 slots from the state tree, or if it has one slot, you need to prove the removal of one slot. But because self destruct is priced the same, no matter how many slots you're removing from the storage, you still have to prove the removal of each of them individually. And so it would be an obvious DDoS vector.
00:42:06.490 - 00:43:07.510, Speaker A: That may be the case for traditional commitment structures. One thing that we kind of talked about as part of our because Celestia Labs is developing a sparse merkel tree for usage, some parts of the Cosmo stack and whatnot that has a bunch of nice properties. But one thing that has been discussed is the notion of removing an entire subtree at once. There's no reason that you have to necessarily remove each individual leaf. You could always have an operation that says, okay, here's the root of an entire subtree, and then just remove it out. Obviously there's trickiness around the underlying implementation, but in terms of the commitment and stateless clients and stuff, for those, it's very easy because they just remove just one node from the whole tree. Obviously, implantations would have to go out and remove each leaf, like full nodes rather, but for things like light nodes or whatever, just a single node removal.
00:43:07.510 - 00:43:14.300, Speaker A: But this is kind of still kind of a more research thing and doesn't exist in production yet.
00:43:15.390 - 00:43:40.530, Speaker C: I think that would be possible though, if you use a spark mercury tree, because sparkle tree trees are hashed. Even if the nodes that you want to delete are sequential, they're going to be in different random parts of the tree. However, if you use a different tree, like if you use Iavl, for example, then you could do that because then they would be in the same subtree.
00:43:42.390 - 00:43:43.490, Speaker B: Gotcha.
00:43:45.510 - 00:44:04.330, Speaker A: Yeah. So potentially IVL is something you guys can look into as a potential tree commitment if you guys want to support that. But regardless, I don't think too many people use softest truck nowadays. It was all the rage with counterfactual contracts, I think, back in the day. But no one uses those anymore.
00:44:08.190 - 00:44:31.460, Speaker B: I was talking to somebody the other day and they said that there's a possibility that it'll be added into the Shanghai fork. EIP. What's this called? 4758 remove self destruct? I'm not sure because I think the priority right now is withdrawals and I'm not sure if the developers would want to add other things into it.
00:44:33.350 - 00:44:36.500, Speaker A: Yeah. Mustafa, did you have any further questions?
00:44:37.450 - 00:44:39.160, Speaker C: I think that's all from my side.
00:44:42.250 - 00:44:51.154, Speaker A: Yeah, I would say me as well. Togol, did you have anything to kind of add or should we wrap up this amazing mean?
00:44:51.292 - 00:44:53.340, Speaker B: I don't really have anything to add.
00:44:54.350 - 00:45:09.040, Speaker A: Awesome. Okay, in that case, we can wrap it up. Thanks togural for presenting Scroll's ZK roll up technology. It was very enlightening. And where can people go to keep updated both with yourself and with.
00:45:11.650 - 00:45:39.240, Speaker B: Scroll? Probably the best place to follow us is on Twitter. It's Scrolled underscores AKP for scroll and togral Maharam without the last two letters of my surname for me on Twitter. So you can follow us there and then you can also visit our website, scroll. IO and read our articles. And also you can sign up for the pre alpha testnet there.
00:45:39.930 - 00:45:41.480, Speaker A: Awesome. Thank you.
00:45:42.250 - 00:45:43.220, Speaker B: Thanks for having me.
