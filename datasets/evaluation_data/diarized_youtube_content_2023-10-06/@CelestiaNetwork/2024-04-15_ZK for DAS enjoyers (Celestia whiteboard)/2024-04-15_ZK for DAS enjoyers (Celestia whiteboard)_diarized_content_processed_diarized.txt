00:00:01.840 - 00:00:48.966, Speaker A: Hello today doing a talk called intro to zero knowledge proofs for data availability sampling enjoyers. There are a lot of ZK talks for experts, and there's a lot of ZK talks for babies. This is a ZK talk for an audience of software engineers who know how Das works, because as it turns out, they actually work very similarly. Pretty much all zero knowledge proofs use a very similar underlying mechanism to the one that we all know and love. So to start, talk a little bit about why zero knowledge proofs are interesting to blockchain people. Because there are two reasons why zero knowledge proofs are very interesting to blockchain people. One is verifiable computation.
00:00:48.966 - 00:01:40.790, Speaker A: You can do gigantic computations that run on like a supercomputer or something, and then prove to somebody on a tiny device like a phone that the end result of the computation is correct. This is awesome because this lets us scale the chains without raising the verification cost, which is a big thing that all of us are very interested in, presumably. And then the second cool thing that zero knowledge proofs do for blockchains is they give us privacy. Before people started cranking on zkps, all the blockchains were transparent. You could see what everyone's doing on there, and that doesn't make it a very good replacement for physical cash, which was the original goal. So let's talk a little bit about what they actually are. So I'll go with an example.
00:01:40.790 - 00:03:06.158, Speaker A: The gist of a zero knowledge proof is prove to me that you know the inputs to some program such that it results in some output without necessarily revealing what all those inputs are. So let's do a basic program hash function. So refresher on hash functions, I'm sure you all know what they are, but the gist is you can put in some data for the input, and then the hash function gives you an output which is usually 32 bytes long, and it's nearly, or else pretty much impossible to figure out what the input was that got you to the output. And what a zero knowledge proof would let you do is if I give you some random 32 byte hash, I could prove to you that I know the input that generated that output without telling you what it is. So this is pretty useful, and that is actually pretty much more or less how some of the privacy systems based on zero knowledge proofs are constructed. Another important thing that's worth mentioning is that you can do proofs of proofs. You can do zero knowledge proofs of other zero knowledge proofs.
00:03:06.158 - 00:03:41.226, Speaker A: This is called recursive proofs, and you see these all over the place. It's a big part of what the research is and what we work on. And the way it works is very similar to das. So now we'll talk about how it's similar to DAS. As we all know, data availability sampling is based on erasure coding. And this morning we talked a little bit about what erasure coding is and how it works with polynomials. I'm going to just do a refresher on that because I think it would be helpful.
00:03:41.226 - 00:04:31.887, Speaker A: So, erasure coding is like, if you have some data and some part of it's missing, you can reconstruct the part that's missing. And the way that works is with this property, here's a chart. And if you have two points on the chart, you can draw a line through them. And if you have a different two points that fall on that same line, you can draw a line through them and get the exact same line. And a line is a. I guess technically not a polynomial, but it is like a function. It's like, I guess you could say it's like a degree one polynomial.
00:04:31.887 - 00:05:19.434, Speaker A: I mean, that's not actually the definition, but like, sort of. Yeah, okay. Yeah, so it's kind of like a degree one polynomial. And so, like, that means you need two points to reconstruct the line. You can do a similar thing with parabolas. So if you have three points, you can construct a parabola that goes through it, and you might have a different three points, and you would get the same parabola, etcetera. And so, yeah, if, like, these three points are my data, and I give you six points, then you have parity data, and some of it can be lost, and you'd still be able to reconstruct the original thing.
00:05:19.434 - 00:06:23.794, Speaker A: Data availability sampling is erasure coding plus a commitment, plus random sampling. So in celestia, your block producer takes all the block data, erasure codes it, he puts all the shares into a Merkle tree, and then the light nodes randomly sample those points. And then for each point that they randomly sample, they also check that it was part of the commitment. That's an important, important part. And basically, zero knowledge proofs are the same thing, except with a few differences. Instead of data, it's a computation. So instead of representing data as a polynomial, and then taking extra parity data to make it reconstructible, we take the computation and we make the computation into a polynomial.
00:06:23.794 - 00:07:12.452, Speaker A: And then you have someone commit to the polynomial, and then the verifier randomly samples from the polynomial and then perform some tests. On the points, and then that's how they know that the proof is valid. Another difference is that DAS has to be interactive. You need every user to sample different points. You need every user to have a light node that picks different random points from the square where he is. With ZK proofs, there's actually a trick you can do that makes it non interactive. So rather than needing everyone to randomly sample different points, you can actually just do this little mathematical shortcut, which I won't get too deep into, which lets you just create a succinct non interactive proof, post it somewhere, and then people can go retrieve it from wherever you posted it and then verify it.
00:07:12.452 - 00:07:43.830, Speaker A: And you don't need this interactive thing. And if somebody tells you that Das can be made non interactive with Fiat Shamir, tell them to sit down, because many people have thought that. But it's. You can't, it's not how it works. So there are many, many different proving systems. You might have heard some of their names, you might have heard of Garroth 16, the most notorious one, Planck, Halo, Planky, two, Starks, kimchi, Marlin. There's a lot of them.
00:07:43.830 - 00:08:35.074, Speaker A: They come out constantly, and they all have different things that they do. But from my understanding, the main most common ways that different proving systems vary is one, in the way that they transform the computation into polynomials, and two, the way that they commit to those polynomials. So this is why I think a good analogy to learn how ZK proofs work is front end and back end. The front end is how you transform your computation into polynomials. And this is called arithmetization. And the back end is how you commit to those polynomials. And this is called polynomial commitments.
00:08:35.074 - 00:09:40.564, Speaker A: So for the front ends, there's three different front ends that pretty much every ZK proof system use. And these are called r one Cs, plonkish and air. So r1 Cs is like the first one. This is like kind of the old school one that you don't see it used as much because people prefer these ones. But R one Cs translates your program into polynomials by first representing it as a circuit. So you'll take your program, which might be a hash function, it might be the EVM, or it might be whatever else you're trying to ZK prove, and you represent it as something called a circuit. Don't worry about what that is for now if you don't know.
00:09:40.564 - 00:10:16.734, Speaker A: Circuit Planck ish is a newer one. And this is what almost all the cool things that people use these days are based on. It is also a circuit representation. And then air is actually not a circuit. So instead of representing your program as a circuit, you represent it as an execution trace. And by the way, this stands for r one cs. It stands for risk one constraint system.
00:10:16.734 - 00:11:02.884, Speaker A: Planckish does have an acronym, but I'm not going to say what it is. And air stands for algebraic intermediate representation. And then for the backends. Again, there's a lot of backends, there's a lot of polynomial commitment schemes, but the three that you see most commonly are based on merkle trees. This one's called fri, elliptic curves. This scheme is called IPA. And the third category is elliptic curve pairings, which is a fancier kind of elliptic curve thing.
00:11:02.884 - 00:11:35.512, Speaker A: And the most famous pairing based polynomial commitment is KZG. Oh, and it's worth mentioning for completeness, that groth 16, which is a proof system, is based on pairings, but it's not technically KZG. It is like its own thing. That's not KZG, but it is pairing based. So I just thought I'd throw that in there. Oh, yeah. And Fri stands for fast read Solomon interactive oracle proof of proximity.
00:11:35.512 - 00:12:35.234, Speaker A: IPA stands for inner product argument. KZG stands for the names of the people who invented it, very well known people. Well, the K is Katay. I don't remember Z and G. So now I'm going to show you that basically, most proving systems can be described as just a combination of some front end and back end. So we'll draw a grid of backends Fri IPA pairings, and we'll do our front ends on the other side, air r1 cs and plonkish. So if you do air with Fri, that's a stark.
00:12:35.234 - 00:13:16.074, Speaker A: If you do Fri with r1 cs, I don't think this exists. You could be the first person to invent it. If you do Fri with Planckisch, that's the Plancky trilogy, planky two, plonky three. When I talk about Planki one, if you do air with ipas, I think that's actually kimchi, which is what Mina uses. IPA with r1 cs doesn't really exist. You could create it. IPA with plonkish halo.
00:13:16.074 - 00:14:14.636, Speaker A: Is that pairings with air? I don't think that's real. Pairings with r one cs, grot 16, and pairings with Planckish, that's like OG planck. And so, yeah, that's like one way that you can just think of all these words that people say and, like, know what they mean. All right, cool. So now I'm going to show you how you actually take a program and you turn into polynomials using two of these arithmetization schemes. So let's say the program that we want to ZK prove is computing the first ten Fibonacci numbers, and we're going to, and we want to represent this using air. So remember I said that air represents your program as an execution trace.
00:14:14.636 - 00:15:07.504, Speaker A: So we're going to compute the Fibonacci numbers step by step, and we're going to make a table where it's the number of the step and then the current, like, registers of the, or the current state of the computation. So, Fibonacci 11235. 813 2134-5589. So this is called your trace. And, like, you fill it out as you're doing the computation. And the laws of computer science say that you can actually represent any computation this way. You might need more than one column.
00:15:07.504 - 00:16:03.554, Speaker A: In fact, Fibonacci is, like, the only thing that really uses one column. Like, if you tried to do Sha 256 as an error, you would need a lot of columns. You need probably hundreds of columns that would basically represent all the registers of the thing that you're trying to compute. And then, like, the gist of how this works is you then would write a constraint function, and the constraint applies to a window of points. So in our case, the window would be like three steps long, and it would be, it would be this. X one plus x two minus x three equals zero. So this would be x one, this would be x two, this would be x three.
00:16:03.554 - 00:16:22.574, Speaker A: And we can test it. We can do x one is one. One plus x two is two, minus x three is three. Does that equal zero? Chat. Does this equal zero? Yeah. Yes. Okay, so at this point, if we sampled this window randomly, it would pass the constraint.
00:16:22.574 - 00:16:47.374, Speaker A: Now we're going to randomly sample. Tell me when to stop. All right, so this is our window that we randomly sampled. Let's see if it passes the constraint. X one is eight, x two is 13, x three is 21. What do we think? Chat. Yes.
00:16:47.374 - 00:17:31.674, Speaker A: So these points pass the test. And of course, this is really small. So if you were just randomly sampling from something this small, it would be pretty easy for the prover to fool you, because there'd be a pretty high chance that you would randomly sample something that passes, and then the rest of it could be wrong. And so that's why we turn into a polynomial, so that there's way more points. And magically, the constraints will work after you turn into a polynomial, and after you process the polynomial using some math that I'm not going to get into. And then you, you randomly sample the verifier, randomly samples windows, and it works. So next, I'll show you how to arithmetize a computation using circuits.
00:17:31.674 - 00:18:12.054, Speaker A: So our computation is going to be, our computation is going to be 33 equals a plus two times a times b. And I want to prove to you that I know some values for a and b that satisfy this equation. Without telling you what my values are, even though you could probably figure them out. I don't want to tell you, by the way. Shh. My values are, my value is a equals three and b equals five. But don't tell the verifier.
00:18:12.054 - 00:19:07.804, Speaker A: So we're going to take the computation and we're going to represent it as arithmetic gates. So a, b, two. So the first, there's a couple of arithmetic operations happening here. We got two multiplications and a sum. So a and b get multiplied together. So they go into this multiplication gate, and then the result of that gets multiplied by two. And then finally we take an addition gate, and a goes into the addition gate, and it gets summed up with this one, and the end result is 33.
00:19:07.804 - 00:19:42.164, Speaker A: And again, the laws of computer science say that any program can be represented using arithmetic gates. So the only things that these gates can do are add, multiply, subtract, divide. But turns out that's all you need. You can construct bit manipulation functions from arithmetic gates. You can construct like comparators from arithmetic gates. And using all that, you could represent the EVM or a neural network as this. And then the way that you go from the circuit to polynomials depends on if you're doing r1 cs or planckish.
00:19:42.164 - 00:21:10.884, Speaker A: But the gist for Planck ish is that you have constraints of this form out of room. And then, similar to what we did with the air, we make a table. And so this table's columns are going to be your inputs to your gates. So like, you see how this is like ql AI qr bi q o c I q m abi plus qc. Each gate would be a row of this table. So this would be the a for this gate, and this would be the b for this gate. And this would be the o for this gate.
00:21:10.884 - 00:21:28.344, Speaker A: And this would be the a for this gate. This would be the b for this gate. This would be the o for this gate. A b. Oh, and you. Yeah. So a b.
00:21:28.344 - 00:22:19.108, Speaker A: There's also a c. But I don't, I don't think I need to use it in this example. A, b, c. And then there's these q values are also columns. Let's start with the first gate, this multiplication gate here. So a would be equal to. Oh, remember what our secret inputs are? I'll just write them.
00:22:19.108 - 00:23:09.308, Speaker A: It's okay. I don't think the verifier will watch this video. So the first gate, your a is going to be a, which is three. Your b is going to be b, which is five c. We're not using it in this example. And then since it's a multiplication gate and we don't want to add them, and it's left plus right, we're going to set left and right qlqr to zero. Qm is going to be one because we're multiplying them.
00:23:09.308 - 00:23:30.464, Speaker A: So we want to basically turn this term off and turn this term off. But we want this term to be included a times b. So that's going to be a one. And then the value for q for or. Sorry. This one is going to be whatever the result is. So that's a times B 15.
00:23:30.464 - 00:24:25.060, Speaker A: And then for the next gate, a is the result of the previous gate. So a is 15, b is two, c. We're not using it again. This is a multiplication gate, so l and r are zero, m is one, and then o is the result of a times B 30. And then the last gate is an addition gate. So a is a, which is three. B is the result of this gate, which was 30 not using c.
00:24:25.060 - 00:25:14.754, Speaker A: And now we actually do want to make this an addition gate. So we make qlqr one, and qo is going to be our final result. 33 Qm is zero because we're not multiplying and we're not using this gate either. Or we're not using that term either. In a planckish, you end up with a table that's like this. And there's one last thing that makes this planck and different from air, and that's what's called the copy constraints. So notice how this 30 here comes from this 30 here, and this 15 here comes from this 15 here.
00:25:14.754 - 00:26:34.354, Speaker A: If we just did this, the airway, they wouldn't be connected. So Planck adds this new thing called the premutation argument. And the permutation argument is a polynomial that's constructed differently from these polynomials. It's like a different kind of polynomial, which lets you force one cell to be equal to another cell when the verifier samples lets the verifier verify that certain cells are connected. And so you would just say, oh, yeah, and then this would be like, this would be like the x value of your, like the domain of your polynomial. So, like, 123456, however many rows you have, or gates that you have. So, like, this would just be like x one x one qo equals x two a, and then the next connection, x three b equals x two qo.
00:26:34.354 - 00:26:51.398, Speaker A: And then. Yeah, so, like, these are the main polynomials. You would. You would create polynomials from these columns. And then this is a permutation polynomial. That's Planck, that's air. That's how you arithmetize computations.
00:26:51.398 - 00:27:19.324, Speaker A: For usage in zero knowledge proofs. There's a lot more to learn. There's a lot of, like, more like mathematical transformations that I didn't get into. There's a lot more variations between the proof systems. But the point of the talk is just to give you the vibe of how it works, how this crazy, insane, magical thing that seemingly lets you get infinite verifiable capacity out of thin air works and how it's actually possible and how it's related to the thing that we all know and love. So that's pretty much the end of my talk.
00:27:20.984 - 00:27:36.634, Speaker B: Yeah, I think that was great. Now, now that we just went through that example, seeing everything on the backend, can you just quickly go back? And so what would the verifier see or have to be doing the verification.
00:27:36.934 - 00:28:20.788, Speaker A: To really accurately answer that? We'd be here all day. But I'll tell you, the gist is, depending on the details of the proof system, you might have 1234-5678 not plus nine. The permutation argument, you might have nine polynomials that all are committed to with a polynomial commitment. So maybe the verifier would have like, nine merkel roots if it's fry, or nine KZG commitments if it's KZG. And then the thing that lets it be non interactive that's different from data availability sampling, is after the prover does the commitments, he can then hash the commitments and then use that hash as randomness to choose points to give the verifier. And that works. And ZK proofs doesn't work in Das, sadly.
00:28:20.788 - 00:28:46.264, Speaker A: So the verifier would have a bunch of commitments and a bunch of points and a bunch of, like, merkle proofs if it's fry or KZG openings if it's KZG. And they would verify all the commitment openings. And then they would verify that each point passes the constraint function, which is this formula in Planck. And that is basically the gist of it, leaving out so many details, leaving out so many steps would be here all day.
00:28:46.704 - 00:29:18.642, Speaker B: One last question that might also be, we'll be here all day, but at a high level, looking at this example, would it be relatively accurate to say one of the benefits of doing recursive ZK proofs is that you can just basically do smaller segments of those smaller arithmetic circuits at a time. And so then you can take, it's basically the taking this larger arithmetic circuits and doing ZK proofs over each one, then do that in a recursive way.
00:29:18.818 - 00:29:19.610, Speaker A: Yeah.
00:29:19.802 - 00:29:22.730, Speaker B: Is that a roundabout way of kind of thinking about it?
00:29:22.762 - 00:29:53.942, Speaker A: Oh, totally. Yeah. So there's two reasons why recursion is amazing and like, shouldn't be possible, but it is, and that's amazing. Like, one of those reasons is exactly as you said. You can take a computation that would be a massive circuit or a massive trace, and you can break it up into smaller parts and then exploit parallelism on the prover side to like prove different pieces of it in parallel and then aggregate them. And then that would let you prove things way faster. The ZK evms make extensive use of this technique.
00:29:53.942 - 00:30:33.308, Speaker A: They all do a lot of that. And then the other reason why recursion is useful is because you can have computations that go on forever, and then you have like intermediately you'll have like a proof get pooped out. So that's like what the mena blockchain is. So the Mina blockchain is a blockchain that is always going, and every block gets proved with the proof from the last block. And so it's this, like, it's just this computation that's constantly generating a new proof of itself. And that is like the real, like, blockchains are kind of the only place where you ever see that use, as far as I know. But recursion, very cool.
00:30:33.308 - 00:31:31.734, Speaker A: Oh, and then another similar related thing that is good to know is you can take one proof system and verify proofs from another proof system inside of that. So starks, which have huge proof sizes, can't really verify them on the Ethereum chain you can, but it's expensive. You oftentimes see a stark verifier written in Groth 16, and then the Garroth 16 ends up on chain, and that's a lot cheaper. So that's like another thing that unlocks a ton of optimizations and makes it way more feasible. Just to go back to the example from the previous question, you said that the prover would provide samples from random samples. How does the verifier know that the prover hasn't selected those random samples for malicious proofs. So when you take an interactive random sampling and you turn it into non interactive by, like, using a hash of the commitment as randomness, that's called fiat shamir, heuristic.
00:31:31.734 - 00:31:58.024, Speaker A: And basically, if they put some, like, malicious points on the polynomial that are intended to trick the verifier, every time they construct that malicious polynomial, they would have to recompute the commitment. And so basically, they would have to grind, and they would have to, like, brute force a malicious polynomial where the random points are the malicious points, and that is, like, just computationally infeasible for them to do.
