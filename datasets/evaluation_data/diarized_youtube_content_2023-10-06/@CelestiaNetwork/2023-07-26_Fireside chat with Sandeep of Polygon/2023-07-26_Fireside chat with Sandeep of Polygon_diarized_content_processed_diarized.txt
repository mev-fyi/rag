00:00:00.250 - 00:00:01.614, Speaker A: Hello. Hello.
00:00:01.732 - 00:00:49.050, Speaker B: What's up, everyone? So, remember when Celestia and lazy Ledger started? The architecture that's proposed is one of the original Cosmos ideas from like, 2017. And, you know, it's very anticipatory of a lot of scaling bottlenecks and a solution to it. Polygon was born in much more a practical and reactionary implementation of addressing scaling problems in real time. So, yeah, we talk a little bit about sort of the story and how we kind of get to modularity, but how'd you guys kind of get started in India? Decide on architecture for the PoS chain, ZkevM, and then up to Polygon, too?
00:00:49.200 - 00:02:03.026, Speaker A: Yeah, I mean, when we started Polygon, I think the core team initially, the way we were different is that even though we started building the infrastructure, but we were, before that, we were actually building D apps. And it was very clear for me, mid 2017 itself, that building D apps on this system, on Ethereum, on a public blockchain, is not going to scale, most probably. And it was also very clear that the dev community liked Ethereum a lot, and there was already a very big community. Now it looks like, how did we see it before? Like five years back? But even then, it was very clear that eventually the Ethereum is going to emerge as the layer one. And now we are seeing like, even many layer ones, even becoming L2s and things like that. So it was very clear that we wanted to build something which DAP developers would be able to use. And that is what you see in whatever you see Polygon does.
00:02:03.026 - 00:02:49.298, Speaker A: Right? Even though now we have extremely high quality research, we almost spent like $1 billion and now have the best talent in the ZK space, and became the first project to launch a full blown ZKe EVM, which is a L2 built with ZK security on top of Ethereum. The DNA is very clear, like, we want to build for the developers where real world applications can be built. So everything we do, the core DNA is that, and we move as per that mission. The mission is that to bring millions of users in web3 and whatever needs to be done for that has to be done.
00:02:49.464 - 00:03:11.722, Speaker B: So polygons put more transactions of value and transactions through tendermint consensus than pretty much any other projects that maybe the Binance bridge and the core piece of architecture celestio is also using. What was kind of your guys decision to utilize that? And how's that experience been?
00:03:11.856 - 00:04:24.740, Speaker A: No, I mean, tendermint is, I think, as a pluggable consensus, you pick it up and build on top of it. I think it's the most evolved, most mature consensus SDK, Cosmos SDK previously, this is one of the best ones. And that's why at that point in time, and not only now, only even in 2018, when we were building it, it was the most mature and the best. And I think from our side also, although we are very deep into the eth community, care about it very deeply. But I think after ethereum, if there is an ecosystem, which I also, and a lot of people in my team, layer one ecosystem, which we respect really is cosmos ecosystem, and tendermint is an invaluable contribution from cosmos to the whole ecosystem. And even now, we are building some things where we need a single slot, finality consensus. And obviously, tendermint constantly keeps coming at the top of the evaluation list.
00:04:24.740 - 00:04:49.820, Speaker A: Now it's much more evolved. But even then, it was very clear to us that as a plug and play consensus into whatever we are building, tendermint is pretty good. And yeah, we are happy that, as you said, that we would have put in more value transactions on tendermint consensus than I think the whole of maybe orders of magnitude bigger than that, actually.
00:04:50.350 - 00:05:17.134, Speaker B: Yeah, Luna had a good run. So what was kind of, the polygon experimented with a lot of different scaling technologies as kind of knew the limitations of this plasma side chain. What was kind of the story and journey to start working with the Hermes team and get to this ZKEvm that launched in March.
00:05:17.262 - 00:06:04.414, Speaker A: Yeah, so, I mean, when we started building this at that point in time, only, like, the practical approach, as I said, that we are driven by pragmatism. We don't want to build sand castles or like some cloud castles, which nobody uses. And back in 2018 19, all these L2 approaches were not evolved. We started building plasma. We were the only team which actually delivered a plasma to the main net, but then nobody used plasma at that point in time. And then we were very clear. And then this optimistic roll ups approach came along and all those things, and we evaluated that and we realized that this approach is also, again, kind of band aid.
00:06:04.414 - 00:06:35.386, Speaker A: Right. This doesn't give you the ultimate infinite scalability on top of Ethereum. And that's why we were looking for a better approach, the end game approach, to say. And that looked to be ZK at that point in time. And that's why we focused all our energies into ZK. And here we are, I think, arguably, pound for pound, the best team in ZK with the products which are out there in production and getting traction day by day.
00:06:35.568 - 00:06:55.220, Speaker B: Yeah. So ZK technology is obviously very hot. It's definitely the core technology underpinning Polygon two. So that was recently announced in the past few months. You talk a little bit about the sort of architecture and vision and the components of Polygon two.
00:06:55.910 - 00:08:02.418, Speaker A: Yeah. So Polygon 2.0 is essentially a multichain vision wherein the goal is, as I said, that again, our goal is not to provide this blockchain scaling technology. Our goal is not to provide these fancy consensus technologies and all that stuff. Our goal is how do we get 1 billion people into web3 in next five to ten years, and what technology needs to be built for that? And at the end of the day, for the developers, if this needs to become, as we call it, Internet of value, it needs to have the similar kind of characteristics that the Internet of information that we see, and we call web two today, it needs to have similar characteristics. What are those characteristics? The current web two world is practically, I'm not saying theoretically, it's practically infinitely scalable. The more apps, more new, different kind of applications are coming in, you can spin up more servers, you can provide additional amount of computation, everything is available, and it's almost practically infinitely scalable.
00:08:02.418 - 00:08:54.854, Speaker A: Secondly, the information is seamless. Back in the day, like 10, 20, 30 years not tend to, but 30, 40 years back when the Internet was starting, these were like di separate. There used to be DaRpANet, Euronet, this like different kinds of networks, and then this whole wWW TCp IP kind of things happened and then all these individual networks got connected with each other. And today we have a seamlessly connected Internet across the world. But previously, even if you had information on one Internet, which is, let's say one network, which is on us, and you want to bring it to, let's say a Europe network, you have to do the same thing, kind of what you have to do today. Like you have to bridge the value from one asset, one chain to another chain. And right now they're not even safe security zones that from one chain you take.
00:08:54.854 - 00:09:44.518, Speaker A: But on the other side, either you are relying a bridge or you're relying in the destination chain, security and all that stuff. And that's why the value is not seamlessly, it's not easy to move the value seamlessly from one chain to another. And this Internet of value needs to have the similar kind of characteristics as the information has. Anybody can create, share and exchange information in the web two world, same way it should be possible for value in the web3 world. So the point how we have built is that this is a multichain environment like where you can spin up as many L2s you want, as many chains similar to, very similar to Cosmos vision. You can spin up as many L2s as we want. But they are all secured by the zero knowledge technology.
00:09:44.518 - 00:10:25.618, Speaker A: So all these chains provide their ZK proofs to ethereum, right? And for all of these chains to have a fast interconnectivity with each other we proposed an interoperability layer. Everything else like otherwise this cross chain lxly bridge and everything is built out from our side. But now we have proposed a fast interoperability aggregator layer. What it does is all these chains submit their ZK proofs extremely fast. Already we can do two minute proofs as per our current technology. And the new upgrades that are coming up. You will be able to, every chain will be able to share or create proofs like in five to 10 seconds and eventually going to 2 seconds also.
00:10:25.618 - 00:11:20.642, Speaker A: So every chain essentially creates 2 seconds proofs, submits to this aggregator layer. And all these proofs of different, different chains, they get aggregated and get submitted to Ethereum. So you have eventual hard finality on Ethereum. But all of these chains, let's say I am on chain number 100 and I want to interact with a transaction, a cross chain transaction that is coming in from chain number ten. Let's say the moment chain number ten submits the ZK proof on this aggregator layer on chain number 100. And all the other chains, they clearly know that, okay, this particular transaction which is coming in that has already proven by the ZK proof on this aggregator layer, right? And I can without trusting, I don't care now that whether that chain has one sequencer, two sequencer, thousand sequencers, it's a reputed validator on that chain or it's a private chain, enterprise chain, I just don't care. It's a simple value or the transaction that is coming in which is proven by ZK.
00:11:20.642 - 00:12:16.226, Speaker A: So we can prove the execution. It doesn't matter if there is a college dorm room guy who has a chain which has multimillion dollar value flowing into the public chains. The public chains can easily trust that value. So as I was saying that what this system provides is first of all it's infinitely scalable that as many layer tools you want to create like today, if we wanted to with this aggregator layer, if there were 100,000 chains, this system will still work. If there were 1 million chains this system will still work, right? And secondly, all of these chains have fast interconnectivity between them and eventually you will reach a place where users won't even realize that all this bridging and all that for them they are clicking. Or let's say I am playing some game in some chain which has a dedicated capacity to that chain, almost like a server. I play, I get some money, I want to swap it to USD on a public chain where the liquidity is there for a user.
00:12:16.226 - 00:12:30.306, Speaker A: It should be a simple click and the transaction happens. That's the ultimate vision. And again, as I said that it rolls back into our ultimate mission, which is again, how do we get massive users across chains.
00:12:30.358 - 00:12:36.270, Speaker B: Yeah, so that sounds like a fairly modular architecture. Would you kind of agree with that?
00:12:36.420 - 00:13:06.982, Speaker A: No, definitely. That's a modular architecture. And modularity is the end game for this. We can't have monolithic architectures like many layer ones without naming them. Many of the layer ones have postulated that there is one single layer. All the transactions of the world live in that. Even if you are not a technical person, you can understand that would have limits of physics on that, right? You can't have the whole world's data transactions, everything in one place.
00:13:06.982 - 00:13:54.482, Speaker A: So modularity is going to be the end game. And all kudos to Celestia team. I think you guys own the modular concept because originally coming up with this, separated parts of the stack with this idea, but then from our structure, I think it naturally evolved into that where now we have multiple roles, like Polygon 2.0 also postulates that, that basically you have multiple roles within the system. First, all the Polygon validators have hundreds and hundreds of these chains to validate on. Plus you have multiple roles. So you can either be approver, you can maybe a data availability cluster provider, know we call it local data availability clusters.
00:13:54.482 - 00:14:36.942, Speaker A: If you don't want to go into a public data availability chain. But the system is very open. If you want to have a data availability on Celestia, on avail or any other data availability provider, our system is pretty agnostic to that. And then you have the validator layer. And the way some of the strides that we have made on the ZK level right now a lot of these things are being productionized, but on the research level everything is built out and eventually we'll be able to have decentralized prover layers. And then obviously this aggregator interop layer is also one more layer where people can collaborate. So automatically, yes, this is like a fully modular vision.
00:14:36.942 - 00:14:37.358, Speaker A: Yeah.
00:14:37.384 - 00:14:56.280, Speaker B: So that aggregator layer, I mean it sounds kind of a lot like sort of the IBC vision key part of this unified liquidity piece. Can you expand a little on the bridging thought process right now and the LXly bridge that's currently proposed for polygon too.
00:14:57.050 - 00:16:05.534, Speaker A: So the functionality wise, yes, exactly like IBC, that you can accept value coming in from one particular chain on the other chain, and the whole ecosystem is interconnected seamlessly and the user experience is really good. So functionality wise, exactly like it's an interconnectivity protocol. I would say in terms of how it works, it may be actually the reverse of IBC on how IBC works is that each chain, this is as per the best of my knowledge, that how IBC works is that each of the chains is running a light node of the other chain. And then whenever any transaction is happening, you rely on the light node information that you're getting, but you have to still rely on the consensus on the other chain, but what aggregator layer is doing. So basically the block creation or verification is spread out. Like each chain has to verify the verification of the other chain separately. Whereas in the aggregator layer we are actually aggregating the proofs of all the chains.
00:16:05.534 - 00:16:22.630, Speaker A: Like all the ZK proofs are being aggregated on one layer and then every other chain can simply take that proof and rely on the transaction. So in terms of construction, it's completely the reverse, but functionality, absolutely the same.
00:16:22.780 - 00:16:36.518, Speaker B: And then would all the liquidity, the secure liquidity kind of bridged over from Ethereum, would all that sort of be aggregated and available to any of these potential two chains?
00:16:36.534 - 00:17:02.370, Speaker A: Yes, that's the whole chain. That's the whole lxly bridge where we have, you can call it like a master smart contract, non custodial contract, which all chains connect into. And that smart contract. Actually then any ZK proofs you are submitting. So you can directly move your funds from one chain to another and you can directly exit to Ethereum because the funds are that liquidity is being aggregated in one single layer.
00:17:04.950 - 00:17:55.122, Speaker B: So a lot of talk about validiums have come up. Obviously the plan right now seems to be the existing proof of stake chain will migrate to a ZKVM validium at some point. Can you kind of talk about the structure of a validium and what are some considerations for builders? It's really like Cosmos started as this hubs and zones concept where you'd have your own kind of cities and towns. Every city kind of provides the same infrastructure, but has its own culture and growth trajectory and values. And it sort of seems like that's still kind of the multichain goal though. People don't really talk about it that much. But yeah, what's sort of the architecture for validiums and why should developers consider building with them?
00:17:55.176 - 00:18:37.838, Speaker A: The architecture that I talked about, that you have multiple chains, they have prover layer, they have data availability layer, and there is an aggregation layer for everything. Validium is one special case of this whole architecture, like validium in the concept of Ethereum. Actually, in the context of Ethereum, when you have both the data as well as the proof on Ethereum, that's a roll up. And then when you have only the proof on Ethereum and data can be elsewhere. That is the validium construction for ZK roll ups. And it's actually specially applicable to ZK roll ups, because on ZK roll up, the sequencer cannot cheat once the ZK proof is submitted. That's why it's called validity proof.
00:18:37.838 - 00:19:15.774, Speaker A: Like in case of optimistic roll ups, for example, they are called optimistic roll ups because you optimistically assume everything is correct, and then you expect that somebody from the community, if there is a fraud community, will run a fraud proof. Whereas in ZK, these are mathematical proofs, these are validity proofs. Once the proof comes in, it's accepted. On Ethereum, you know that the sequencer has executed the transaction. Still this proof correctly. So that's why you don't need data on Ethereum blockchain, basically to validate the chain. In case of optimistic rollers, the data is not there.
00:19:15.774 - 00:19:56.710, Speaker A: You can't even validate the chain because you don't know what is happening. In case of ZK, the ZK proof itself is the validation of computation. You need data only when you want to exit or you want to do any kind of cross chain operations and all that. That's why what you want to do is you want this data to be fairly available. But that data doesn't contribute too much to the security of the chain per se, 100%. Of course there is a weird ransom attack and all that, but there are multiple ways to address that, using force exits and all that. I'm not going to that, but point is that with ZK, this construction is possible where you can have the execution layer submitting only the proofs and the data is elsewhere.
00:19:56.710 - 00:20:00.540, Speaker A: And that's where all the data availability chains become.
00:20:01.070 - 00:20:31.800, Speaker B: So. And I'm sure you get this question a lot, but we have a very. Cosmos has been very much known for its tech for a long time, but the community is constantly asking where we're going to find users. Polygon has been very famous for its BD efforts and output. What's kind of the secret there. How have you guys been able to get the users that you've been able to get?
00:20:32.170 - 00:20:55.406, Speaker A: I think this stems from the DNA itself. As I said, that our mission is very clear. I said that at least from my side. And now we have some of the biggest researchers, like Jordy and Daniel and Bobin from maiden and all that. So their focus is building all these research efforts and all that. And I think as a team also we are fairly modular. Right.
00:20:55.406 - 00:21:33.030, Speaker A: So there is people who are building the tech and my mission of life is how do I get 1 billion users into web3? Maybe starting with 100 million users, let's say, in next two, three, five years. Right. So as I said, from my point of view, we are not here to create fancy technology. This has this approach, that approach, all that kind of fancy stuff that goes around. I am here to make sure this trustless compute, basically, for me, it's trustless compute. This human society, currently we interact with digital systems. All these digital systems are centralized.
00:21:33.030 - 00:22:18.026, Speaker A: And that's why we are fooled with so many things in so many ways. And my mission is that how do we make this trustless world possible? Because we are spending 60, 70% of our time in these digital worlds and how do we make it more trustless? That looks like a natural evolution of humanity. And once that mission is clear, then all of these things flow through that. It's very, very clear in my mind that what we are building needs to have users. If this is something kind of extreme, which can't get users, I would probably not vote internally that we should build something like that, even though it looks very fancy. And crypto, we as crypto industry, we are very big fans of narratives and all that stuff. I don't care about that.
00:22:18.128 - 00:22:38.906, Speaker B: So in that pursuit, I'd say you probably talked to more builder teams than any founder, at least in crypto, that I've come across. What do you hear about Celestia or DA layers when any of those teams are considering their choices?
00:22:39.098 - 00:23:55.190, Speaker A: Yeah, I think right now, when I speak to a lot of developers, obviously, that 95% of them are on Ethereum only and on Ethereum as a developer, the developers don't really care that when they are building an application, where is the data going, to be honest? Right. And the celestia kind of use cases, the kind of technologies they are more relevant to, let's say the protocol builders like us. And also, I think the roll up as a service provider, the sovereign roll ups, things that you are doing because you can't go to the developers and tell them that we'll give you this data availability system, that data availability system. To the developers, it should be like, this is the environment where you can build the app and all the other things are abstracted from them. And this is the job of these system integrators or roll up service providers and all that to make sure that the better data availability products are used for that, be it Ethereum 4844, be it Celestia avail or all the different kind of database solutions that are coming in.
00:23:55.260 - 00:23:56.470, Speaker B: Yeah, I can da.
00:23:56.890 - 00:24:15.566, Speaker A: Yeah, I can da. But I don't hear too much from the developers and ideally I should not like, we should not even index on it too much. It's actually the job of the infrastructure providers who are going to provide execution environment to the developers. That should be the job of roll up as a service, sovereign rollups and things like that.
00:24:15.668 - 00:24:33.178, Speaker B: Yeah. So just make sure the SLA is there and it's as seamless to all the potential users as possible. Cool. Well, that's all we have time for. I don't know. Do you do Q and a or no? All right, thanks, Sandeep.
00:24:33.274 - 00:24:34.240, Speaker A: Thanks everyone.
