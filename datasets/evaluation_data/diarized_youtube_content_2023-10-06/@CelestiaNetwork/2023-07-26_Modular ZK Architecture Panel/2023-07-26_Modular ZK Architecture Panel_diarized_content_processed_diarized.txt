00:00:00.250 - 00:00:01.038, Speaker A: Thank you.
00:00:01.204 - 00:00:55.066, Speaker B: Okay, so we have a pretty unique set of teams here today. For the most part, over the last few years, as people have thought about ZK snarks, they've thought about how they relate to roll ups and validity roll ups. But I would say in the last twelve to 18 months, there's a handful of teams that have started exploring what it looks like to use snarks for creative applications beyond roll ups. And I think this panel represents a lot of those teams. Maybe we could talk a bit about some of the unique opportunities that introduces why we think it's interesting and compelling, but also some of the challenges that come with it. So one of the first things I think we can start with is just like a motivating question around ZK and the trust assumptions. Sort of inherent in your interest in ZK is the belief that trust is very important in blockchains.
00:00:55.066 - 00:01:12.850, Speaker B: For years we've had solutions like off chain data processing that sort of either happen optimistically or just with an assumption of trust. What about trust assumptions is important to you? And why do you think it's so important that we do it cryptographically instead of relying on some of these weaker assumptions?
00:01:13.750 - 00:01:59.620, Speaker C: What I like about ZK is it gives you an opportunity to establish a point of trust and then inherit computation from that point. And I think the notion of trustlessness is oftentimes misleading. So when we think of a ZK roll up, what you're functionally doing is you're inheriting trust in a state transition from the consensus level collateral. And when we think about coprocessing or we think about any type of access to on chain data from a contract that can't be accessed to the VM's native execution, what you're trying to do is get as close as possible to your computation running on the same trust assumptions as the underlying base layer. So it's not trustless per se, as much as it is the inheritance of trust from a very specific designated point.
00:02:00.870 - 00:02:14.502, Speaker B: Yeah, it's a different boundary box a little bit than where we were before. But I think if you look at the base layer of no trust, this moves it up a level. It is. Any other thoughts on that? Why is it important for you guys?
00:02:14.636 - 00:03:13.926, Speaker D: Yeah, we've been talking to a lot of application teams and really trying to educate them on what ZK can do for them. And as I'm sure everyone in the audience knows, although we talk about these very secure ZK based systems, in reality, a lot of things on chain today rely on development teams being honest, even a lot of the systems in production will eventually become fully secure, optimistic roll ups, but today have permission sequencers. And I think where in the next year or so I see ZK really being valuable is in cases where social consensus will not accept a trusted oracle. And those are typically cases where composition is very important. So if you're a protocol team, maybe your users will accept your trusted oracle. But where it really gets difficult is when another protocol wants to compose on top of that protocol. Will that protocol's users trust this random other protocol's team? I think the chain of trust gets very tenuous.
00:03:13.926 - 00:03:18.810, Speaker D: I think ZK really helps in establishing more clear trust abstractions and boundaries.
00:03:19.950 - 00:03:55.430, Speaker A: Yeah. To add to that, I think there are some existing situations where we've already seen trust be super problematic, abused, and then eventually leads to things like bridge hacks and results in very tangible amounts of money getting taken from users and protocols. I think we're all aware of all the bridge hacks that have happened very recently, and I think in those situations it makes it really clear why the existing trust assumptions of multisigs or whatever other mechanisms they currently use aren't acceptable, because it has already led to material loss.
00:03:56.170 - 00:04:35.410, Speaker B: I think that's a great one. Composability is often overlooked, but if you have like a key trust assumption in the middle of a chain of trust, then you can't really build another highly valuable protocol on top of something that has weak trust assumptions. So it's a good one. Yeah. Sort of implicit in that conversation was an attempt to do it a bit differently than we've done in the past, avoid some of the bridge hacks that have happened, help improve composability and protocols. How are your teams thinking about doing it a bit differently? Maybe starting decentralized or enabling that very early in your product's lifecycle?
00:04:36.490 - 00:05:33.960, Speaker C: Yeah, so what we focus on at Lagrange is supporting data, parallel computation on top of large chunks of on chain data to be provable efficiently. So broadly speaking, when we think about the space of on chain data, we're functionally constrained by the ability to access from the execution layer the majority of the state that has been created in the canonical history of a chain. And this is even more prevalent when we think about the modular context, when you have a series of different execution spaces with a series of different state structures, transaction trees, receipt trees and block histories. And so what we focus on is to allow you to treat this, what is more or less a morass of unstructured data, as if it was your data lake in web two, to run SQL Mapreduce, RDD, massively parallel processing, computational models on top of this to be able to derive and extract properties that are relevant to your application's function.
00:05:36.490 - 00:05:40.070, Speaker B: Yeah. Maybe pulling it in a bit to focus on decentralization.
00:05:40.410 - 00:06:13.380, Speaker C: Yeah. And so, I mean, I think functionally decentralization in the context of this question has to do with a few relevant vectors. So firstly, you have to think about the decentralization of the prover, which confers a liveness assumption on the overall protocols that inherit computation from it. And moreover, you have to also think about the assumption of where you're deriving data from in the single chain context. It's very straightforward when you get multi chain and some of the work that the sync does, it gets a little bit more opaque. You have to derive data ideally from the underlying consensus of a source chain or from a close approximate, a light client as you can get to that.
00:06:16.090 - 00:07:13.210, Speaker D: Yeah, I would say one really important piece for this is for users to be able to audit what every team is doing and what's actually been deployed. I think that the four of us on stage here can talk all we want about how our teams have a security mindset and all the things we're doing, but ultimately users actually have to be able to verify and put our systems to the test. So I think there are a lot of things that go into that. One is just standing the test of time in a production environment. The second is having a lot of transparency around code base, being open source, having a reproducible build for any verify or you deploy on chain. And third is adopting cutting edge security techniques like formal verification or fuzzing to give a higher degree of guarantee to users. So I think all of these systems are going to be difficult to trust at a super high level until they've been running live for years.
00:07:13.210 - 00:07:18.780, Speaker D: And whatever we can do to allow users to audit that more quickly is going to move the space as a whole forward.
00:07:20.770 - 00:08:13.310, Speaker A: Yeah. To talk more about security, I think Vitalik advocates, even in the context of zkevms for this two factor approach, where maybe you have one computation that's done in ZK, and then maybe you have a trusted tee SGX based two factor, or maybe you have many different implementations of the same function. I think one really interesting thing about ZK is that at the end of the day, all the functions we are computing are just f of x equals y, and so it's very clearly specked. And so you can actually have multiple redundant implementations of the same computation very easily because it's just inputs and outputs and the circuit has to be the same. And I think that will be really powerful in the security story of actually getting ZK adopted and for people to feel comfortable using it in their DAP in a very critical context.
00:08:13.970 - 00:08:31.326, Speaker B: Yeah, those are good points. How are you thinking about. I mean, multi prover kind of implies a separate software stack. Do any of us think maybe we would push in the direction of a full separate software stack for another prover? I mean, the T's may be a shorter path where you don't necessarily need a whole new stack.
00:08:31.518 - 00:09:11.630, Speaker A: I think you can already see that there's a few primitives that are implemented across a few different proving systems. At the end of the day, I think all of us on stage are doing pretty similar stuff with hash functions, signature schemes and other cryptographic primitives that are fundamental. And it's really not too hard to take the same primitive and reimplement it in a new stack. And we already see multiple implementations today, so I think it's quite feasible to actually have multiple implementations and something we can push towards. So I'm not advocating for a different prover for the same proving system, but more multiple redundant implementations and even different proving systems.
00:09:13.090 - 00:10:01.310, Speaker C: I think it also requires being thoughtful over the scope of what you are proving. I mean, I think if you're proving something like a light client where you have a very fixed set of parameters over what a correct execution of that is, it's more straightforward than if you're building a general purpose VM when you're incurring a significant amount of technical debt, when you're anchoring to a specific proving system, potentially, if there's a change in the state of the art and you can't have your back end be agnostic, your front end be agnostic to the back end. And so there's complexities there, I think, that are inherently incurred as you develop applications that have more and more zero knowledge intrinsic to their core purpose. And I think in those situations having multiple back ends becomes an imperative if you are trying to ensure that you can stay up to date and your performance can stay relevant.
00:10:02.130 - 00:10:26.098, Speaker B: Yeah, that makes sense. What are you doing in the meantime? We're not quite there to a multiproover world. I think most all of you are working on getting into production pretty soon. Are you putting in gates or checks in your contracts, or. Every proof needs a signature along with it for now. How are you thinking about that in the short term? Make sure you don't have a catastrophic bug in prod.
00:10:26.274 - 00:10:48.670, Speaker D: Yeah, we deployed to mainnet two weeks ago and we put in gating on the prover. So if there is a circuit bug in soundness, we certainly will not trigger that bug. We've also put in time lock upgrades on our verifiers so that we can actually fix any issues that come up. We do feel like this should be a temporary phase until we're able to introduce stronger security techniques like formal verification.
00:10:50.450 - 00:11:21.450, Speaker A: Yeah, I think we can follow a lot of the in production ZK roll ups today. They all have similar setups where they have approved provers. You have time locked upgrade and governance over the verifiers. I think all of those things are very reasonable because again, if they are in the critical path and there is a hack that can potentially be catastrophic, I think we think about following their lead and their design choices, and we think that's like a very reasonable short term trade off before we get multiprover and trusted execution environments.
00:11:22.270 - 00:11:34.510, Speaker C: We agree. I mean, I think there's good precedence over teams who have pushed large production code bases with complicated underlying circuits and have done so in a way that has to date been more or less secure.
00:11:35.890 - 00:12:00.120, Speaker B: I actually had a conversation with an auditing team that will remain unnamed, that had done some ZKe EVM audits and they were nervous. They said to themselves, like, I don't know if we understand this well enough to really put it in production. We've done our best here. Are there things beyond audits that you're trying to do internally at your companies? Maybe to have a culture of security or help ensure at the code development time that when you go to production you don't have problems?
00:12:00.890 - 00:12:30.640, Speaker C: Yeah, I think having a culture of security is very important and I think you need to be very clear on your code reviews and your best practices as you're implementing and developing your underlying infrastructure. I'd extend that broadly and say, I think just from a business operations standpoint right now, you should be resourcing and hiring people who have an understanding of the primitives that they're working with, especially when you're building these highly complicated systems. It's important that the work that's being done is done by people who have an awareness and a context for how the things they're building works.
00:12:32.450 - 00:13:11.120, Speaker D: Yeah, we think that, of course, standard security practices are very valuable, but actually I think there's some very obvious things which are very helpful. One would be having less code, just having a more minimal and well designed system so you don't have as large a security surface area. The second would be looking at the interfaces between ZK and blockchain systems, where the two systems are really of quite different natures, and we think these boundaries are places where issues are more likely to arise. So of course circuits might have bugs, but I think it's much more likely you just completely misparse part of your circuit and do something quite obviously wrong.
00:13:13.250 - 00:13:26.500, Speaker A: Yeah, you can even see this in the bridge context, where a lot of bridge hacks have been due to trust assumptions getting violated. But then other bridge hacks are simply due to smart contract bugs. I think goes to support Yi's point.
00:13:27.990 - 00:13:44.630, Speaker B: Maybe zooming out a little bit and thinking about modularity, given where we are. Uma, I guess you just announced that you're working on a Celestia bridge. Are any of the other teams thinking about sort of modular da layers in their environments, or are you mostly focused on specific chains?
00:13:45.770 - 00:14:14.770, Speaker C: We focus very heavily on modular with how we're developing our infrastructure, I think being able to permissionlessly support data access in a modular context is very important, I think, especially as we see a proliferation of new execution spaces, whether those be roll up as a service providers or l three s on top of existing scalability solutions. I think it's very important to ensure that in terms of state access, that you're not constrained by the ecosystems that you're interrupting with or interacting with, principally.
00:14:16.150 - 00:14:47.930, Speaker D: Yeah, we're currently focused on EVM, but I actually want to point out another aspect of the word modular that I think ZK is very useful for. If you are able to use ZK introspection onto the state of chain as part of your application, you can sometimes dramatically simplify the on chain architecture of your smart contracts. Basically, you don't need to be recording a lot of extraneous information at a state that you could later read using ZK. And so we think that this can contribute to a trend in smart contracts actually getting more modular.
00:14:48.830 - 00:15:11.460, Speaker C: Yeah, I agree with that. I think state access on chain has led to development practices that if you had principally better data access or in principally better compute on top of that data could be alleviated. And I think if we look at like optimistic roll ups and we look at the bisection game there, there's things there that can be simplified drastically, I would argue reducing some of the implicit security assumptions to it.
00:15:13.530 - 00:16:00.690, Speaker B: Cool. Let me think if there's anything on that topic, I guess. Yeah, one of the challenges of having proof or a light client built off of a given chain, I mean, in l one, we can kind of assume there's a lot of economic stake behind this route of trust. When we go into a modular chain ecosystem where you have a route of trust that maybe has lower economic state or longer times to finalize, I think that introduces a challenge in the level of trust you can put in that. How are you alleviating that problem? I think Lagrange, I know you've published some thoughts on state commitments, and I think this challenge will also impact axiom as you guys try to look at other chain ecosystems.
00:16:02.070 - 00:16:42.500, Speaker D: Yeah, I think the core challenge is essentially, although let's say an optimistic roll up might have a longer finality period, let's say seven days, users really demand some sort of weaker guarantee that can hold much faster. And so we think it's actually more appropriate to leave that sort of guarantee to the application. If you are trying to withdraw $100 million from optimism, maybe you should wait seven days before someone else accepts it. If you're trying to play a game on optimism, who cares? Just accept it. And so we think it's important that for the end user, the guarantee that you're precisely offering is extremely clear.
00:16:43.670 - 00:18:00.700, Speaker C: I think part of the complexity with having a clear guarantee for the end user application is that it opens the design space up for less transparent infrastructure providers to have opacity over the underlying design decisions of their protocols. And this is, I think, the concern with a lot of cross chain protocols today that originate messages from optimistic execution environments. And so one of the things that our team works on is using existing Ethereum valid asset collateral with Eigen layer to be able to assert an early degree of economic attestation behind the validity of a purported state transition for an optimistic rollup. And the reason that we think this is very valuable is you can have bridges permissionlessly consuming state from a shared layer with a clear amount of economic trust and economic security behind the state that they're using. And it means that if you want to understand how much security is behind a given attestation estate, you can very quickly look at the size of the committee and the stake within that committee, and you can derive that assertion from there. And you don't have to worry about whether or not a k of n assumption for an arbitrary bridge that could be used on some intermediary protocol has a sufficiently decentralized underlying validator set. We think of this as like very much a public good.
00:18:03.310 - 00:18:40.950, Speaker B: Great. Maybe we could spend the last few minutes here zooming out and thinking about sort of ZK beyond the blockchain. I think snarks in general just receive no attention or minimal attention outside of the crypto domain. And I think crypto served as a good incubator for this technology to kind of grow up and get mature. But over time, there's probably an intersection with ZK snarks and the broader Internet at large. Is there anything kind of exciting and interesting that starts to happen as we see more verifiable computation used throughout the Internet? Are we all just crypto maxis?
00:18:41.370 - 00:19:32.310, Speaker D: So I've done some academic work on ZKML, putting some of the largest known machine learning models into ZK. And as consequence, I've had to explain to some pretty well known machine learning professors, like, what is ZK? Their reaction is always the same. Number one is, that's impossible, like, you got something wrong. Number two, and this takes varying periods of time for different faculty members, is, okay, maybe it's possible, but it's useless for us. And number three, some of them are like, oh, maybe in this edge, edge, edge case it might be useful. And so I think it's a lot of education and finding the places where having verifiable computation actually makes sense in a non hostile environment. Typically their response, hey, like, why would I use a snark? I could just run it, or, I trust Amazon more than your weird crypto system.
00:19:32.310 - 00:20:17.880, Speaker D: And so I think it's very sobering in showing us that as a space, we need to be delivering real world value that's exogenous to this relatively insular crypto world. That said, I think one trend that pushes a lot of people I know who've been bearish on crypto, bearish on ZK forever, to be very interested is the rise of AI. People are very worried about spoofing about deepfakes, and they really want a notion of provenance to exist. Just the other day, I called my bank to verify my identity for a wire. I'm pretty sure that's just not going to be a thing in a couple of years. And I think people are very hungry for a solution, and I do think ZK can play a role in that.
00:20:19.610 - 00:20:58.020, Speaker A: I think use cases like ZKML and Worldcoin are an interesting case where they are in crypto, but they're also kind of bridging the real world, and it's a very real world application of ZKML. But of course also worldcoin is going to be settled on an op stack roll up. And so they are actually using a lot of crypto properties as well in their system. I think use cases like that, that straddle the real world and the crypto system are really important and what really excite me. And so looking forward to more of those, I'm sure there's more ZKML use cases for similar or new sorts of products like that.
00:20:58.630 - 00:21:56.760, Speaker C: And one thing I'd also add is I think when we think of verifiable computing crypto, we have a tendency to think principally about the succinctness of the computation. And I think most of us here are not talking most about the ability to compute verifiably on a private set of inputs. And in the web two context, there's a lot of examples of where that is and will be highly viable. And I would say if we think about enterprise transfer of data, and the inability for a lot of web two companies to have effective orchestration of computation across shared data assets, to mitigate fraud, to have better user experience and customer experience. The fragmentation of data within major companies today makes it very difficult in financial services, in the healthcare sector, for there to be applications that can act in the best interest of the underlying user base while still being able to preserve privacy of each of the enterprises that has that data.
00:21:58.650 - 00:22:08.694, Speaker B: Cool. Are we doing questions? I don't know. Okay. I think we're pretty close to time, but I also think we're wrapping up the end of the conference, so maybe we could do a few questions from the audience.
00:22:08.822 - 00:22:11.066, Speaker E: There's still one talk after this, but we.
00:22:11.168 - 00:22:11.674, Speaker B: Okay, sorry.
00:22:11.712 - 00:22:28.980, Speaker E: So we can do the questions. Of course. Anybody? All right, Anna, maybe you want to say something.
00:22:29.670 - 00:22:30.930, Speaker D: We don't bite.
00:22:32.950 - 00:22:34.610, Speaker B: What's storage proof?
00:22:39.530 - 00:22:41.560, Speaker F: Yeah, just a general question about.
00:22:44.890 - 00:22:45.206, Speaker B: How.
00:22:45.228 - 00:23:04.250, Speaker F: Much more efficiency gains do you think there's left for ZK proving systems from here? Is it a ten x? 100 x is like a two x? Because people are still kind of skeptical about how practical ZK is from a computational perspective.
00:23:05.470 - 00:23:32.280, Speaker C: Yeah, I mean, I think that's a good question, but I think if we were to assess the trends in computation over the last 2030 years, there are a number of buoying factors that will result in ZK becoming increasingly performant, irrespective of the underlying proving systems that we're talking about. I'd say that you can discuss improvements in the proving system, as well as improvements in underlying computing infrastructure that both likely will have positive effects over time.
00:23:33.610 - 00:24:10.610, Speaker A: Yeah, I think to add to that, currently, I think all of us perhaps run our circuits on fairly commodity hardware, and there's a lot of ZK hardware companies that are out there trying to make hardware level improvements to these proving systems and make it faster. So that's one frontier to push and then of course there's also new proving systems all the time. I think Nova came out this year, and perhaps there will be more in that line of work that make an algorithmic improvement. So I'm very optimistic that the algorithmic improvement plus the hardware improvement is going to result in huge gains to ZK.
00:24:11.510 - 00:24:45.130, Speaker D: Yeah, and if we want to think about the theoretical limit, Justin Taylor put out a blog post, I believe, last year discussing this. Essentially there are two sources of overhead. One is in converting a normal computer program into a ZK circuit. There he thinks that the limit is maybe a factor of 100, and the second is in the actual proof system, where maybe you lose another factor of ten in the limit. So that would add up to a 1000 x overhead over a normal computer. Right now, we are nowhere close to that, so I think we can easily get 100 to 1000 x improvement without any hardware.
00:24:46.110 - 00:25:01.938, Speaker B: Yeah, and one last thing I'll add on that is that doesn't account for the hardware side of things. So it's typically between ten and sometimes up to 100 times improvement depending on the specialization of the hardware. So fast proofs are the future. All right, do we have time for one more?
00:25:02.024 - 00:25:02.770, Speaker E: Yes, we do.
00:25:02.840 - 00:25:04.834, Speaker B: Okay, thank you.
00:25:04.872 - 00:25:31.420, Speaker G: So, as someone who's been on a team who's seen developers probably not elegantly put data on chain, and they're just pushing a lot there, I'm very excited for this idea of what you guys calling coprocessors. Any predictions on how long it'll take to make the switch from these very inelegant on chain protocols to these, in my mind, much more elegant protocols, a year, two years, three years. How long do you guys think that switch will take?
00:25:32.350 - 00:26:08.920, Speaker C: I think the principal constraint there is twofold. I think there's developer adoption, and there is whether or not developers feel comfortable altering the paradigm and the function of their application to now include new primitives, irrespective of how secure those primitives may be, I would say. Secondly, there's the question over whether or not there are clear instances on chain now where there are applications that could leverage better data access and will leverage better data access in changes that they will make to their underlying infrastructure in a shorter period of time.
00:26:10.010 - 00:26:30.830, Speaker D: I think another factor that comes into play is really social consensus. I think right now it's frankly still pretty difficult to have trustless versions of a lot of the things that developers just putting up the number on chain for. I think once it becomes easy enough, users will start demanding it, and it's something that we'll see slowly and then suddenly.
00:26:34.400 - 00:26:38.700, Speaker B: Okay, thank you, everyone, for your time, and you enjoy the rest of the conference.
00:26:38.706 - 00:26:39.079, Speaker D: Thank.
00:26:39.079 - 00:26:40.110, Speaker E: Thank you.
