00:00:01.930 - 00:00:06.638, Speaker A: So I think I would like to start with a small intro if you can do what you're working on as well.
00:00:06.804 - 00:00:14.270, Speaker B: Yeah, I can start. Hey guys, my name is Bo. I'm a co founder at Polymer, where we're working on extending the IBC protocol to all chains.
00:00:15.090 - 00:00:23.840, Speaker C: I'm Fig, I'm co founder of squid, and we're doing cross chain swaps and transactions across the cosmos and EVM and soon to be many more.
00:00:25.130 - 00:00:29.510, Speaker D: I'm Richard, co founder of Orblabs and we're building the chain abstraction stack.
00:00:31.290 - 00:00:37.318, Speaker E: I'm Jim, co founder of Catalyst. We're building sovereign liquidity for the modular future.
00:00:37.484 - 00:00:55.600, Speaker A: Thank you very much. I think we often hear a lot of different words when we talk about bridging. So I think we hear transport layer, crosschain merging protocol, crosschain router, token bridge. Could you sort of explain, maybe take one each and explain what that specific topic is? A specific thing is.
00:00:58.450 - 00:01:33.260, Speaker B: I'd love to talk about the transport layer. I feel like it's something that's not really considered interrog. I feel a lot of people talk about the state layer, they talk about security, they talk about how do you verify something that happened on one chain, on another chain. The transport layer deals with encoding of the network topology of the entire network on chain, all the paths between chains, all the paths between all the smart contracts that want to talk to each other. This information needs to be encoded on chain IBC, does it? Very few or basically no other interrupt protocol does that.
00:01:34.670 - 00:02:10.674, Speaker C: I can talk about routing, and that's what squid does, essentially. Routing is when you look at cross chain just from the lens of connecting applications between chains. So we take the view that there's liquidity all over crypto that just needs to be connected. And we use general message passing and different infrastructure layers. We use Axla to access that liquidity. And so a user should be able to interact with any application on any chain in just a single click. And it's a little bit like Google maps, but if you could teleport.
00:02:10.674 - 00:02:15.640, Speaker C: So we always find you the way to what you want to do, but it should happen as soon as possible.
00:02:16.250 - 00:02:16.566, Speaker A: Yeah.
00:02:16.588 - 00:02:40.400, Speaker D: And I think with messaging, cross chain messaging, at least the focus is more so passing data from one chain to another. And Bo is right. The reason why most messaging protocols don't focus on the connections is because it's possible to have connectionless interoperability, where you don't need to know the direct paths beforehand and still pass message from chain to chain. And so you find that for most messaging protocols, establishing those connections, at least beforehand, is not a main focus.
00:02:41.810 - 00:03:31.054, Speaker E: And I guess I can take token bridges. That was something you said, right, as an option. So token bridges, I think is a pretty archaic term, at least how I define it, where I think several years ago there was a need to move coins between the kind of home blockchain to a new one. And so bridges were kind of made as a primitive in order to enable that. And so bridges for me are some function of wrapping a token or minting a new token on these kind of new destination chains in order to get access to this token. And it's backed one to one to kind of this vault of the original tokens that are stored on the home chain. And so literally a bridge, right.
00:03:31.054 - 00:03:36.330, Speaker E: It's like you're just kind of shooting an asset from its home to another location.
00:03:36.490 - 00:03:52.200, Speaker A: Thank you. I think what you all sort of touched upon a bit was standardization to some extent. So we hope you have a lot of different token standards. How do you bridge one token standard to the other? And also in messaging? So how do you view sort of standardization, and how do you think we should sort of go towards that?
00:03:53.610 - 00:04:36.420, Speaker B: My long term view on standardization is that at some point chain developers, if you relate them to operating system developers, are going to want to enshrine something into the blockchain kernel itself. What that is will probably be an open standard, be net neutral, be decentralized. I mean, I slightly disagree with Richard here on the need for encoding these connections on chain, because I think that if you don't encode this information on chain, you cannot prove anything about the network. In fact, you cannot prove that you have a smart contract that is bound to another smart contract across any number of chains. So I think that when these chain developers do come to make this decision, IBC is the best candidate here.
00:04:39.830 - 00:05:50.714, Speaker D: I think you're going to realize Vo and I are going to be going back and forth. We tend to have very different views on interoperability. But I think going back to standardization, I'm very pessimistic as to whether we'll have all the developers agate underneath one standard simply because it's in their best interest not to. If you think about a world where there's even 10% chance that there's a different interrupt standard, that sort of like, makes interoperability slightly better for some other chain that didn't choose to enshrine a particular bridge, you're probably going to want the option for that sort of standard to come to your chain. And so what we're probably going to see instead is we're going to see some sort of wrapper protocol that comes and bundles up all these interop solutions and gives applications on top of these chains the ability to move seamlessly from chain to chain. But yeah, going back to the options between connection based or connectionless interoperability, I'm pretty sure it's still possible to make it very easy to pass messages without sort of like storing and encoding all that data on the chains that you're going to, partly because most of that data is also already encoded in the security layer or the state layer, which, what we'd call them. So, yeah, that's the general thought.
00:05:50.752 - 00:05:54.010, Speaker A: There any more thoughts on sanitization?
00:05:54.450 - 00:06:51.886, Speaker E: I think what we're seeing right now is there are some projects that are just brute forcing it. Right. I think we can all agree in this panel that there's a pretty high degree of heterogeneity in the ecosystem as it pertains to varying standards or varying protocols that not only span general message passing, but also, as we were alluding before, kind of the token level as well. And so there are projects like socket and Lifi that are just saying, hey, we'll just kind of put the man hours to aggregate all those things and then abstract it in some sort of off chain way. Right. What I have been seeing is, while that works, and I'm sure will continue to work, there has been kind of a deliberate effort to make their jobs a little bit. So, you know, there's this new Erc I'm blanking on the name that Arjun from connects kind of offered that is kind of championing that.
00:06:51.886 - 00:07:17.954, Speaker E: Right. And so it's good to see a lot of the liquidity networks like connect and our team at Catalyst included, seeing that there needs to be some sort of standardization, how we wrap tokens when we bring it to different chains. And so we allow for not only more interoperability, but also kind of more shared agreement on the security assumptions of that wrapping.
00:07:18.082 - 00:07:20.262, Speaker A: Yes, you sort of touched upon it a bit in the end here.
00:07:20.316 - 00:07:20.742, Speaker C: Sort of.
00:07:20.796 - 00:07:32.640, Speaker A: If I'm a dev and I'm building application, I need to choose a question missing protocol or a bridge or whatever it might be. What kind of trust and security assumptions should I be looking at and what do you need to consider?
00:07:34.530 - 00:08:34.900, Speaker C: I can go just on the standards piece as well. I think it depends where we are in the innovation cycle, because I agree that there's going to be experimentation. If there's a chance that something's going to work, someone's going to go for it and build the product, but potentially with bridging for example, it's quite an old technology now and we can start to standardize it. And people are like the connects guy, looking at ways of abstracting it in a way so that you can standardize it for everyone. But our job at squid is a little bit like the aggregators where we integrate protocols across different virtual machines like different chains, different gas models, and we have to deal with all of that complexity. I don't know if we're anywhere close to getting standards in a lot of these places. So there's just going to be a ton of work on our end, I think to just deal with that for now.
00:08:34.900 - 00:08:52.486, Speaker C: I think we could end up as a temporary standard and then hopefully the ecosystem finds its own way to become very similar across the board. But for now we need to make things usable and evolution will play its course.
00:08:52.588 - 00:08:58.998, Speaker A: So will the best standard win? Or are there other reasons why a standard might win?
00:08:59.084 - 00:09:13.040, Speaker C: I don't think so. I mean, solidity is the standard now and I don't think many people agree that it's like the best, and javascript maybe the standard, it's going to play into a lot of different factors, probably non technical ones.
00:09:13.810 - 00:09:28.020, Speaker A: Perfect. We had a sort of modeler ecosystem day and we obviously Celestia has a DA layer. How do you guys think that sharing a DA layer might help in bridging between roll ups or chains or whatever else it might be?
00:09:33.590 - 00:09:56.060, Speaker B: Sharing a DA layer allows you to have trust, minimize interoperability between those roll ups. If you share the same DA layer, let's say they're optimistic roll ups, you have the security of that DA layer saying that I can make this data available to anyone to be able to generate a fraud proof. That means that the economic security of the DA is shared among the roll ups on top.
00:09:56.910 - 00:10:53.630, Speaker D: Yeah, and I think even going back to the general idea behind roll ups. So roll ups in many cases, at least for l two roll ups and Ethereum get the states for all things happening on Ethereum sent up to them. And so in the case where you share its DA layer, you can sort of run light clients that sort of look at the state coming from the DA layer and even compute a full node that computes the state of all the blocks that are basically being broadcasted down to the DA layer. And through that you can extract the message you need. So how that differs from light clients that sort of rely on, well, separate DA layers is you run into the problem where it's possible for the DA layer to fork, and you may have sent over a state from some other chain showing a fork that ultimately was deemed invalid. So I guess comparing shared das with lite clients to not shared das with lite clients, what you have is just a slightly more robust system for passing messages from chain to chain.
00:10:54.290 - 00:10:56.142, Speaker A: Perfect. Any more thoughts on that?
00:10:56.276 - 00:11:55.666, Speaker E: Yeah, I'm not going to talk about the security properties of it. I do think the benefit of sharing DA, and depending on how you use the underlying DA layer, you might also use it for canonical transaction ordering. You do allow for soft confirmations of the state verification of the messages being passed within the execution level. And so with that you have quicker finality, right? Because you're essentially saying that because the state is made available, because there's a canonical ordering of the transactions using the transaction ordering layer of this DA layer. Or even if you're not doing that, you still have the data made available so that you can have fraud proofs or so that you can reconstruct some of these validity proofs. You are basically allowing for soft confirmations at the cadence in which blocks are produced at the DA level. And so fulsacia like several seconds, right.
00:11:55.666 - 00:12:02.850, Speaker E: And so that is a sufficient user experience even if there's latency introduced at the execution level for true finality.
00:12:03.270 - 00:12:20.490, Speaker A: Okay, perfect. Maybe more of a question for fig and Jim here as well. I think you've talked a little bit about the UX problems that we run into with bridging. And how do you foresee, sort of, how do we fix this? Can aa help? Can intent help? What kind of forfeiture do you foresee in terms of ux?
00:12:22.990 - 00:13:20.778, Speaker C: I mean, the data availability is what everyone was just talking about is a good example of ux where we just found out that I didn't know this, that op, the optimism token is minted on the optimism roll up and not on l one. And so there's a bridging problem, like how do you bridge op without it being fragmented now? Because they could have just minted it on Ethereum, and then suddenly you have all these canonical versions via the enshrined bridges. But it's going to be a mess. And I think you're going to need these. That's what we do. We have these abstraction layers where you can just get whatever token you need for an application and even if the application is supporting the wrong token, but technically you can get that token, use the app in one click. And intents are this idea that you just have something that you want to do, and you send it off into the world and someone magically solves it.
00:13:20.778 - 00:14:04.186, Speaker C: And I think what we're doing is sort of like a proto intense system where we're the only solver and we have a feature called Boost, which we're really excited about. We just launched in the last week, which allows you to essentially declare your intent on the source chain. And then that will get finalized over 20 minutes or however long the bridge takes. And anyone can fulfill that transaction on the destination chain immediately. And it's fully generalizable because it's using general message passing. It can be a swap. You can buy an NFT across chain, you can do staking across chain, and that's the user experience we want.
00:14:04.186 - 00:14:21.760, Speaker C: We want people to do anything in any application in one click as soon as possible. And yeah, I feel like we've started just with something which works and then you can decentralize that more over time. And I think, yeah, really excited for intents and to integrate them into squid as well.
00:14:23.490 - 00:14:57.994, Speaker E: Yeah, I don't have too much to add as it pertains to intents. I do agree that routing systems like squid and the ability to have kind of a sophisticated entity provide the liquidity for fast liquidity. Soft confirmations of those transactions is really awesome. Right. I think that augments the UX considerably. I do think we still even in that paradigm. Well, two things I'll add.
00:14:57.994 - 00:16:05.794, Speaker E: One is that I think intents are really interesting, but I don't think people talk about solvers enough. So what fig at the Squig team and doing kind of the boosted feature, what they're doing there is really kind of starting the conversation of how do we have infrastructure that is not articulating intents, but actually fulfilling or providing the liquidity, or providing the tools for these entities to fulfill those intents. So I think we need more kind of advancement and more investment focus into that piece. And I think where we come into the picture is kind of the second piece where even in that paradigm in which we have really robust infrastructure for solvers to fulfill these intents, we still kind of face what I think is a cold start problem when you look at new chains. And so I think it's pretty kind of consensus now that there's going to be lots of roll ups, right? Hundreds of thousands if not millions within that kind of framework. If you have a brand new chain, it's a really difficult problem.
00:16:05.912 - 00:16:06.978, Speaker A: Please try again.
00:16:07.144 - 00:16:27.960, Speaker E: It's a really difficult problem to try to inject liquidity into a brand new chain that's basically a barren wasteland. And so how do you solve an intent for a new chain when there's no liquidity? Right. And so that's a big research question that we think a lot about catalysts. And so we're hoping to slot into kind of this refocus into the solver problem space.
00:16:30.010 - 00:17:22.794, Speaker D: I do think most of the conversation around intents currently focus a lot on a very defined problem, which is swapping one asset for another. But there's a big question on if intents are going to eventually become, let's say, the end game of interoperability, then we're going to need to find ways to generalize intents to almost every possible transaction. And I think that problem is extremely difficult and maybe the main bottleneck to ever us ever getting to this final state of interoperability, partly because intents in many ways are invariant programming. You basically have to, well, configure your system to check against maybe an infinite number of invariants to make sure you don't break things. And yeah, that's not an easy problem. So I think we're going to see a lot of maybe proto intent systems that combine transactions and intents for quite a bit. And then eventually, perhaps after several years of thinking about the problem more, we might get to a generalizable intent system.
00:17:22.992 - 00:17:24.378, Speaker A: Anything on that? Bo?
00:17:24.464 - 00:18:21.420, Speaker B: Yeah, since we're talking about intents, kind of on a tangent now, but we'd love to kind of talk a little bit about the work that enoma is doing and how it can apply generally at the interoperability level. So if you think about the research that they've done to say, allow chains to combine or take the union or the intersection of the validator set between multiple chains, bootstrap a temporary chain, they call it chimera chains, and be able to create cross chain atomic transactions if you were able to standardize at the networking level. So at the interoperability level, let's say IBC everywhere, hypothetically speaking, and IBC were to have access to the consensus mechanism of all these different chains, you could apply the ideas of heterogeneous Paxos and do essentially cross chain atomic IBC transactions across all IBC enabled chains. I think we'll see some of this, but I think it'll take probably quite some time to get to this end state.
00:18:22.190 - 00:18:38.980, Speaker A: Right. Perfect. Some of you have talked about a world of hundreds, thousands of different chains. Do you think we end up in a world where there's like one aggregator aggregating all kinds of different question measuring protocols, token bridges, whatever else it might be. So how do you foresee sort of aggregation working out?
00:18:40.790 - 00:19:05.980, Speaker C: Well, yeah, definitely be squid. There'll only be one. Everyone will use the squid SDK to build their apps and there'll be some squid widgets which everyone uses in all applications. Everyone, all the wallets will have us installed. There'll be a few winners. I think in all cases there might be a few interrupt winners, there'll be a few aggregators. It depends what you end up focusing on as well.
00:19:05.980 - 00:19:23.150, Speaker C: We try to not take the approach of we're aggregating everything, we're just doing swaps and then you can pair a swap with something. So it's more like a payments layer. Maybe you have NFT aggregators and as we find more actual use cases for crypto, then there'll be more solutions that might be the winners.
00:19:30.290 - 00:20:17.860, Speaker E: Yeah, I think aggregation theory that a lot of people talk about either on their mirror blog posts or you can look at like strategy, it's probably going to play out where you see kind of like a power law distribution, where there's going to be a handful of short tail suppliers for aggregation. And they might say they're differentiated, but the reality is from a user behavior perspective, you just end up using the three of them. It's like when I get a hotel or when I want to stay somewhere, let's say for ECC, I just end up checking like five platforms. Now they're not differentiating in any, just I just have to use five of them to get access to everything and that's fine. I think that's just how the economics plays out.
00:20:18.870 - 00:21:04.640, Speaker D: I do have a bit of a controversial take, though. I do think the intent future, or at least intent future we're envisioning, is directly counter to this aggregation future. Partly because once you introduce solvers to the mix, solvers are way more sophisticated than your end user, and they will more than likely explore a wide variety of possible different places to get the assets they need to basically optimize or maximize their profits. And so we could see a world where aggregation doesn't really take place, but we have a vast number of liquidity networks and token bridges that people just generally go through from one to one just to make sure they get the best possible profits. And these layers will be fighting based off of, well, who have the lowest slippage, who have the cheapest gas fees, who have the fastest times and so on.
00:21:05.650 - 00:21:08.134, Speaker A: Right? Do you have any debt? Bow?
00:21:08.282 - 00:21:59.218, Speaker B: Yes, once again, I'm going to talk about the lower layers of the stack. While everyone talks about the upper layers, I would say that I think at the lower layers of the stack we're going to see some consolidation, probably in the ten to 20 year time frame. I know people don't really talk about ten to 20 years in the crypto space, but it's about how long it took for the world to kind of converge on roughly a consistent networking standard, TCP IP. In fact, in the early seventy s eighty s, there were a number of proprietary company owned protocols that were kind of seen as these de facto standards of the time. Maybe like a layer zero, for example. And over time they congregated. Because if you're Apple and you've implemented Apple Talk, Microsoft might not want to implement Apple Talk, but Microsoft will be a little bit more open to implementing TCP IP per se.
00:21:59.218 - 00:22:07.270, Speaker B: So I think the world will converge to IBC in the long term time horizon, but in the short term, we'll see a lot of aggregation at all layers.
00:22:08.250 - 00:22:35.440, Speaker A: All right, perfect. So maybe to move the thoughts towards. I think a big difference in some of you and other bridging providers as well, is the fact some might be using a state or middle chain to verify messages. Some don't. Layer zero, for example, and others as well. What is your opinions on having a state middle chain? Do you think the sort of crypto economic security provides is good or bad? Some might say that. Or what are your opinions on that?
00:22:36.770 - 00:23:15.722, Speaker B: So I think the idea of a stake middle chain comes with some baggage. I think it comes with the baggage of generally these protocols implement some proprietary crosschain gateway protocol. Nothing wrong with that, but some other protocol that the chain or the team that owns the chain controls. I think having middle hops in network topology makes a lot of sense. I think if you want to scale network topology and you have a lot of heterogeneous infrastructure, you do need middle hops. That being said, I think long term you still want to converge to a single standard, have middle hops in between. Obviously you want these to be secure as well.
00:23:15.722 - 00:23:34.420, Speaker B: Ideally, you want them to be trust minimized. If you can prove the execution of an entire path for a particular packet, then you can remove trust minimization trust on these middle hops in the long term time horizon. So I think that the stake matters a lot now, but I think as technology advances, I don't think it will matter as much for security.
00:23:36.390 - 00:24:32.180, Speaker D: I think right now they're ticking time bombs. Partly because if you think about it, the whole idea is that economics will always make it, well, not profitable for a malicious party. To basically exploit those chains. But as more and more chains connect to various chains, as hubs take like Axel or Polymor, which are trying to become hubs for IBC, it may just end up being the case. There's way too much value built on it that even the validators don't have enough economic incentives to keep the network as robust as possible. And the only way that changes is a world where the light clients they're using for passing messages from chain to chain are no longer necessarily just checking their validators to see if they provided the right signatures and more so running complete state transition proofs of all the transactions happening on those chains. And at the point, once you get to that particular level, they don't no longer need to be valued in chains, they can just become roll ups and do the exact same thing.
00:24:32.180 - 00:24:46.658, Speaker D: So yeah, I think we're probably going to see a world where all the value based networks effectively start to do the exact same thing roll ups do in order to become robust or very secure, or they're just going to implode because the economic security no longer holds.
00:24:46.834 - 00:24:49.274, Speaker A: And I'm assuming you have some thoughts on this as well. Fig.
00:24:49.392 - 00:25:32.914, Speaker C: Yeah, sure. Yeah, no, I agree with what you guys are saying for sure. It's a temporary solution that will upgrade into a more trustless solution when the technology gets there. But for now, we need a solution that we can connect the unusual chains into other networks which are maybe more interoperable with each other, like IBC. And then IBC is developing, and over time it will get more capable. But for now, we want to connect Ethereum to the cosmos, for example, so that the cosmos can get some users and build up business relationships. I think the economic security, honestly, is that's a ticking time bomb.
00:25:32.914 - 00:26:10.322, Speaker C: But with the number of users at the moment, it's more about. I think there are so many other things to be worried about with security and bridges. The nomad hack, for example, was just at the customer contract layout. In fact, I think the Salana hack was, the wormhole hack was similar, $300 million. And it wasn't anything to do with economic security. But definitely centralization is going to be the cause of a lot of hacks. And Axla having 75 validators compared to, say, I won't name names, but in the five or six range, in a lot of cases, it's completely different and so much heart attack.
00:26:10.322 - 00:26:40.058, Speaker C: And then these chains become like polymer and Axlar take on a different role in a future where everything is trustless. They can be routing hubs, they can potentially have a programming environment where you can build network logic for specific applications, like gas networks, for example, or there are way that you can program into the networking layer and. Yeah, that's it.
00:26:40.224 - 00:27:33.018, Speaker D: One tiny rebuttal on the point of, well, the number of validators being the main thing that sort of keeps things alive. I do think it's a combination of the validators and the stakes. And when you compare most of the chains that do have these diverse sort of like validators, you still find that the stakes aggregate into some sort of like few hands, just because for the most part, you probably find that it's probably the team that's running most of the validators on chains like Maxilar. I do think the bigger point though that stands is for us to get to a world where we sort of are comfortable with these hubs, they're eventually going to have to change or switch to some variant of a light client system that depends almost entirely on state transition proofs as opposed to wealth of validators. That way we sort of like move the security away from the validators and your stakes to, well, math and whether we're running the right computations.
00:27:33.194 - 00:27:35.120, Speaker A: Are you in agreement, Jim, as well?
00:27:35.730 - 00:28:41.970, Speaker E: Kind of. I do think proof of stake or leveraging economic security is not as big of a ticking time bomb as my fellow panelists may think. It's a coordination problem, right? Like let's say you move more economic activity than your underlying stake, which, surprise, ethereum does, right? That's where decentralization matters. It's like, okay, let's say you're using vanilla tendermint, or I guess comet BFT. They call that now still a coordination problem for all these validators collude and actually have an erroneous state transition occur. But I do agree that we're probably going to move away from that model not because it's a taking time bomb, but because it's just inefficient, right. Having a committee of validators run full nodes of God knows how many chains, right? I would say you could probably, depending how beefy your nodes are, run 200 chains, which is higher than I think most people would probably say.
00:28:41.970 - 00:28:45.298, Speaker E: But anything more than that, then it's not going to happen.
00:28:45.384 - 00:28:50.566, Speaker A: Yeah. So what are you saying is that each of the validators would have to run a full node for all the other chains they support?
00:28:50.668 - 00:28:51.938, Speaker E: Yeah, precisely.
00:28:52.034 - 00:29:12.410, Speaker A: And I guess also incentivizing sort of developers as well could also be a problem too. We've heard a lot about CK bridging, I think over the last couple of months and years. Do you guys have any thoughts on using ckps in bridging, making either for trust minimization or for batching blockheaders or transactions into ckps?
00:29:13.490 - 00:30:21.694, Speaker E: I'll start mainly because I'm the least qualified, so I'll give the most high level opinion about it. I think they're important. I think there's still a lot of open questions on what that looks like when you have heterogeneous proving schemes, when you have latency on posting proofs on chain and verifying them. But like what Richard was like, that is the end state, right? It's like we can't be verifying the consensus or said differently, we can't be snarking just the signatures of these validators of underlying execution layers, but we need to be actually verifying or proving the computation of the actual state transition occurring. Right. I think the solution to this kind of the open questions that I'm talking about is two pieces. One is there still needs to be a hub, right? It's like heterogeneity of all these proving schemes, all these different roll ups.
00:30:21.694 - 00:31:08.770, Speaker E: You probably want a prover aggregator, a proof aggregator, or using some sort of recursive proof system in order to make sure that they become homogeneous. Right. You probably still need a router. Honestly, you can't have millions of chains all talking each other in pairwise kind of permutations. Right? And then like what Bo was mentioning, snarking the actual path of that packet becomes really important. And then on the cost piece, I do think optimistic ZK, right? I had a tweet. I was saying, I think everyone's talking about how off chain is the future, which is kind of ironic but optimistic ZK is interesting, which is where you snark something, you don't verify it on chain, but you could verify it on chain and becomes like a one of n like fraud prover kind of setup.
00:31:11.990 - 00:31:13.700, Speaker D: I feel like you probably have one.
00:31:14.710 - 00:31:44.720, Speaker C: Oh, I have a quick one. I don't have too much to say on ZK bridges, other than excited for when they come live and we'll use them. But I wanted to correct, Axellar's team doesn't run any of the validators, and it's a permissionless network that anyone can join and run a validator. That's the big difference between it and say, multi chain, which just got hacked a couple of weeks ago, which literally, it was an NPC protocol where the CEO owned all the instances running all the MPC nodes, which was insane. But yeah.
00:31:46.770 - 00:31:50.830, Speaker A: The state middle chain approach is definitely more secure than a multi sick.
00:31:52.390 - 00:32:40.830, Speaker B: Agreed, yes. On the topic of ZK clients, I would say that there's obvious performance issues that you want to work through and that will improve over time. On the front of proving the entire execution path is something I wanted to drill into. So one advantage of encoding some of this network topology on chain is that now you have a subset of the keys that are owned by each and every particular path. So you can shard proving of the execution of one particular path and separate that away from proving the path for communication between another set of smart contracts. And you can do this on a smart contract to smart contract basis. If you don't have this information on chain, it's very difficult to scale proving.
00:32:43.170 - 00:32:45.358, Speaker A: Anything to add on that, Richard as well.
00:32:45.524 - 00:33:13.450, Speaker D: I don't know if I completely buy that, but I think I have to think about it more. But I think yes, light clients will be the final state interoperability. And I think coming to this event, I've realized that a lot more people think that it's further out than it actually is. I think most of the tools needed to build extensible lite clients currently exist. It's just a matter of, well, assembling things like Lego blocks. And once we do that, we should be able to have light clients for most of the, I guess, popular networks and frameworks we currently use.
00:33:13.600 - 00:33:22.090, Speaker A: So if we're in a future where all chains support light clients, do we just not use smart contracts at all and just light clients for bridging?
00:33:22.770 - 00:34:05.290, Speaker D: No, we will still use perhaps some optimistic system, partly because of the latency that comes from while proving these chains. I think perhaps the end state of bridging would be optimistic with light clients for dispute resolution, if anything. So imagine a world where someone states that something happens on the chimp chain, and some people just monitor this particular proof, and if no one disputes it, then hey, we're fine. And if someone disputes it, we check the light clients to make sure that they're not lying. It makes more sense given that, well, until the hardware becomes really good and approving software becomes really good, we're still going to be looking at perhaps three, four, 5 minutes, sometimes even up to 12 minutes worth of latency for passing messages from chain to chain. And people aren't that patient.
00:34:06.590 - 00:34:35.140, Speaker B: I'm going to take a quick second to shill the work that we're doing at Polymer. Since we're on the topic of optimistic ZK approach to interoperability and forming that connection, we're working on optimistic ZK IBC connections at Polymer. So we have working ZkiBC connections and we're working on overlaying, optimistic verification on top, and we expect to have that and hopefully testing in a few months.
00:34:35.990 - 00:34:36.450, Speaker C: Cool.
00:34:36.520 - 00:34:52.730, Speaker A: So we've talked a lot about how we can solve insolubility. So in a world, a dream world, maybe 510 years from now, what kind of applications are you looking forward to seeing? Like what kind of cool applications can you build when you have 1000 seamlessly connected chains with relatively decent latency?
00:34:54.590 - 00:35:57.022, Speaker E: I think a lot about the idea of borderless finance, where everything that we own becomes kind of aggregated within one single interface, irrespective of where it lives. And this goes beyond an on chain context. Right. And so my shares of certain companies that aren't doing too well right now, or my cash assets or my real estate, it's all in the same view as my crypto assets. And there is ample liquidity in moving between all of those in whatever action I want to prefer. And so interoperability obviously is on chain right now. But I do think that there is a lot of research underway of how we kind of, whether through the use of CK snarks or what have you, bring real world assets into it as well.
00:35:57.022 - 00:36:04.640, Speaker E: And it becomes a lot more delightful, I think, to own financial assets in the meat space.
00:36:05.170 - 00:36:07.174, Speaker A: Very cool. What about you, Chandra?
00:36:07.322 - 00:36:43.230, Speaker D: I think when all is said and done, the main product that we'll see enabled by interoperability will be some variant of a cross domain intent protocol. I think the end state of interoperability will be the chains themselves sort of disappear. They only become a thing that the developers care about. Very similar to web two in many ways. Right? Only you care about where you deploy your application, only you care about where your software sits, users just care about them being serviced. And yeah, until we have that, we're probably not going to cross over that well million or billion mark that everyone keeps on talking about when it comes to users.
00:36:43.890 - 00:37:25.814, Speaker C: Yeah, I agree with that. We just need a way for people to build projects and be able to add. I mean, at the moment I feel like the only use case we have is payments. But you can imagine also making the web. Two example, I'm really excited about how you can potentially bring AI into the intent system. Like we've actually built a proof of concept in squid and we're working with Dora who in the audience on it. And they've built a thing where you can just like a search engine, like the google of Crypto, where you just type in what you want to do and then it goes through the squid SDK and it finds, say you just want to mint an NFT.
00:37:25.814 - 00:37:45.330, Speaker C: It finds the NFT somewhere, builds the transaction packet so you can buy it from anywhere and you've got your NFT. So you can potentially imagine a world where you have a Google, like just a search bar, which is your entrance into getting anything or at least finding an application. And then interoperability allows you to access that in one click from anywhere.
00:37:47.910 - 00:38:21.870, Speaker B: I love what everyone said. Borderless finance is very important. Some of the early applications in crypto are financial and have priced out some other applications. Me personally am very excited about gaming. I'm a huge nerd and I love computer games. So I recently saw a post someone had tweeted it about having on chain like ovariant of Warcraft. If people are familiar with real time strategy games like Warcraft, Starcraft, these are kind of like your agent empires as as well, there's a number of them.
00:38:21.870 - 00:38:32.960, Speaker B: These time are like classic real time strategy games, but having these on chain games and be able to approve it is very interesting use case to myself personally, I would love to play these games. I would love to be incentivized to play these games as well.
00:38:33.730 - 00:38:44.860, Speaker A: All right, very cool. I think we're running slightly beyond schedule, so I don't think this is correct. And I know there's a happy hour outside. So thank you very much for joining our great panelists today as well. Thank.
