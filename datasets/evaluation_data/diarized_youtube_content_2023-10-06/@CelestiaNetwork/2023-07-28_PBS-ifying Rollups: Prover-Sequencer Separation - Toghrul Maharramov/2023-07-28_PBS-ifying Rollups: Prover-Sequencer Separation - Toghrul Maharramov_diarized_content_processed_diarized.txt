00:00:03.130 - 00:00:44.090, Speaker A: Hello everyone. Before I begin, I wanted to thank Vitalik and Georges for warming up the crowd. It's really nice from them. So, strangely enough, I'm not going to be talking about roll up definitions today, I'm talking about something else. So my name is Togral, I mostly shed posts on Twitter and sometimes pretend to do research at scroll. And today we're going to be talking about Pbsifying roll ups prover sequencer separation. So you've probably heard about zero knowledge roll ups or for the starquare folk in the crowd so they understand validity roll ups.
00:00:44.090 - 00:01:40.150, Speaker A: So you have sequencers and you have provers in the system and sequencers perform the role of ordering and then the provers perform the role of computing the validity proofs for the given order. And the way they work is the sequencer submits the batch. The batch is also propagated to the prover. The prover submits the velocity proof and voila, you have the finalized state on the layer one. So why should we decentralize? Because in L two s in roll up specifically, you don't really care about correctness that much because it's already enforced by the L1 bridge. So why would you decentralize? It makes more sense to just be centralized and be efficient. So there are multiple reasons why you would do that.
00:01:40.150 - 00:02:48.180, Speaker A: So one resilience, let's say if your sequencer fails, what happens then? If there's no backup or in many cases there's no decentralization, then you're stuck and waiting for that sequencer to come back. Another reason why you would need to decentralize is real time censorship resistance. So while L One bridges guarantee long term censorship resistance for a lot of applications, that's not enough. For example, let's say if your app depends on Oracle updates and you're waiting for a long time and the sequencer is censoring you, you're basically screwed in case there's a lot of volatility. There's also throughput, which is an interesting one, because it might seem logical that a centralized system is more efficient because you have less overhead. But in case of zero knowledge roll ups because you can paralyze proving, you can actually increase the throughput by decentralizing the provers. And that's what I'm going to talk about later.
00:02:48.180 - 00:03:53.080, Speaker A: What are the requirements for a decentralized roll up? So first we need to minimize the overhead. It doesn't really make sense to add drastic overhead and make it really clunky and very complex. It needs to have fast pre confirmations because if your laptop has the same finality time as the L1, then especially in L1s that are not very fast, it's not a great UX. It has to be provable on chain. So what I mean by that, if you use some sort of a mechanism that agrees on off chain ordering, you need to be able to prove to the bridge later that the ordering was agreed upon via a certain mechanism. So if there's some nondeterminism in the way you agree on the ordering, it might be possible that basically the bridge might diverge. And so we need a way where you can prove it on chain that it actually happened.
00:03:53.080 - 00:04:49.994, Speaker A: And finally, which is the reason why I'm giving this talk, is balanced incentive distribution. I'll explain why it is important. So why is unbalanced incentive distribution an issue? So, since sequencer is in, the protocol are in charge of the ordering, they get to keep all the mev profits. And it's likely that in roll ups the mev profits are going to be the large share of the total protocol revenue. And if that's the case, then what's the incentive for protocol participants to become approver? Because if all the profits are collected by the sequencer, then I would rather become a sequencer. But in case of ZK rollups, we actually need provers sequencers. We need a few, but we don't need that many.
00:04:49.994 - 00:05:45.734, Speaker A: And so we need to incentivize provers. And that's where PBS or proposal builder separation comes in. So in a proposal builder separation, proposer is a stock leader responsible for proposing and propagating blocks to the network. And the builder is a set. There are a number of builders competing with one another in an auction to have their candidate blocks selected by the proposer to propose to the network. And basically the whole point of PBS is that it outsources the computationally intensive task of creating an optimal value extracting ordering to some specialized nodes that can actually do that efficiently and well. But in our case, we don't have proposers and we don't have builders.
00:05:45.734 - 00:06:35.450, Speaker A: So what should we do? So we have prover sequencer separation. So on top of doing what PBS does, PSS also ensures that protocol generated revenue is relatively fairly distributed with the provers. Because as I described before, the sequencers by default get to keep all the mev. And as with PBS, there are two types of PSS. There is Enshrined PSS and external PSS. And the role of the sequencers is essentially to emulate the role of the builders on the L1. And the role of the provers is to emulate the role of the proposers in PBS.
00:06:35.450 - 00:07:31.566, Speaker A: These are the steps in PSS, in enshrined PSS. So the sequencer bids in an auction, the prover selects the highest bidding thing, the winning bidder reveals the contents of the block, the sequencers reach consensus, blah, blah, blah. That's it, we have the ordering. Well, the benefit of this is that it doesn't have a lot of overhead because we don't need a lot of sequencers. The consensus should be relatively efficient and therefore it should be relatively easy to run. But the downside is that it actually creates sort of a PBS inside PBS. Because I might be a sequencer, I might be elected as a sequencer, but I'm not really good at building, so I'm incentivized to outsource it to someone else.
00:07:31.566 - 00:08:34.478, Speaker A: So it's possible that you can create a market where sequencers outsource it to someone, but else which creates this loop in which the value is leaked to some external parties because the sequencers just don't want to do it internally. Now it's working and the second type of PSS is external PSS and it follows a similar pattern. So sequencers bid in an auction, provers select the highest paying bid, the winning bid is revealed, blah, blah, blah, blah, blah, blah. But in this case, provers reach the consensus and with external PSS, sequencers don't actually exist in the protocol. The protocol doesn't know about them. There are some external parties that provers deal with and they can even avoid them and build the blocks themselves. But obviously it's unlikely to happen because provers are not good at building blocks and extracting value, so they're likely to outsource it.
00:08:34.478 - 00:09:47.526, Speaker A: And the advantage of this scheme is that it doesn't really require two distinct roles in the protocol and therefore it makes the protocol simpler and the fee distribution much more simpler. So you don't have to deal with two different models, potentially two different staking models, et cetera, et cetera. But it also has a massive downside. It has quite a big overhead because the idea is that you should have a lot of provers and if you run the consensus amongst the provers, it's going to cost quite a lot in terms of networking, et cetera, et cetera. There are possibilities where you can, for example, use committees and do random sampling and do that, but the security drops that way. And basically you might get into a situation where you have a liveness failure, whereas if you use the entire validator set, then it's much less likely that you're going to end up having a liveness failure. So those are two options and we are not really sure which trade offs are better.
00:09:47.526 - 00:10:00.020, Speaker A: And therefore the open question is I'll leave you with an open question. To enshrine or not to enshrine. That is the question. Thank you very much. And if you have any questions, feel free to ask.
