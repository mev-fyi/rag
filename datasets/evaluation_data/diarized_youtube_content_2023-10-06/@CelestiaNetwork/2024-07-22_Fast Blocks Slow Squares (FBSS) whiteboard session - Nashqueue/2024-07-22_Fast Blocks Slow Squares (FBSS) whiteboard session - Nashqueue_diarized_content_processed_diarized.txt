00:00:02.640 - 00:00:07.510, Speaker A: Hello guys. I'm here with Evan Forbes from Celestial Labs.
00:00:07.590 - 00:00:07.974, Speaker B: Hello.
00:00:08.062 - 00:00:45.260, Speaker A: And myself, Nesh Q. I'm a protocol researcher. I'll be talking today about the concept of fast blocks, slow squares, and you can think of it as a brainstorming session. We will have a conversation and you can poke holes into the design or maybe improve it. You will see if there are any question in the audience. I'll try to repeat it. Okay, so the topic of conversation is, I call it FPS.
00:00:45.760 - 00:00:48.100, Speaker B: How big do we have to write for them to see it?
00:00:48.720 - 00:00:50.060, Speaker A: I think that's big enough.
00:00:53.770 - 00:00:54.082, Speaker B: What?
00:00:54.106 - 00:01:45.906, Speaker A: We can confirm this is big enough, right? Yeah, fast blocks, slow squares. And if I make a error in writing, you can also correct me. The original goal was to increase the faster confirmation times for rollups before the square is created. And currently we have a 12 seconds block time. Right. So in Celestia, every 12 seconds there is a new block. Just very quickly.
00:01:45.906 - 00:02:52.038, Speaker A: Celestia is a data availability layer, so you can think of it as putting data into Celestia and then Celestia agreeing on some random, like some bytes of data, not actually executing on that data. If you're thinking about base roll ups, then these roll ups would want to have maybe faster confirmation time than those 12 seconds. The data structure of what celestia is committing over is a square. We'll have a square. And all the data that is put into celestia is put in the upper left quadrant of the square. And you can think of it as a. You can think of it as slots and we call it shears.
00:02:52.038 - 00:03:25.150, Speaker A: And in those shears, the data is being written on. And then you eraser coded. The erasure coding part is done for the lite clients so they can sample over it and then reconstruct the data for full nodes. But this is not what the topic is about. The topic is about how to get to separate basically the creation of the square from how fast you could get a confirmation for rollups.
00:03:26.090 - 00:03:28.928, Speaker B: Separating data availability sampling from consensus.
00:03:28.994 - 00:04:34.809, Speaker A: You know, separating the availability sampling from consensus. So how you would start thinking about it is how I approach this problem is, okay, what is the square for? Right. There are some functions like why do we need a square? One of the needs, why we need the square is to sample the data. To have the data availability guarantee, you need to actually the lite clients will ask the full notes and you will get proofs of the share to the data route. So for sampling is one and the next part is block reconstruction. Right.
00:04:37.149 - 00:04:38.101, Speaker B: Sorry, continue.
00:04:38.205 - 00:04:38.749, Speaker A: Go ahead.
00:04:38.829 - 00:04:47.069, Speaker B: Why do you do those two things. This is like a higher level. To summarize it, we have the square for data availability.
00:04:47.189 - 00:04:47.929, Speaker A: Yeah.
00:04:48.510 - 00:04:50.770, Speaker B: And what is that blocking.
00:04:54.150 - 00:05:50.950, Speaker A: So when a new block is created, we basically always have to construct a square and the light node would receive the header and I start the sampling. And after the sampling is done correctly and the full nodes are under some malicious attack, they would create reconstruct the block because they need the full data to be able to serve fraud proofs. The full node if they, the data basically needs to be available to either create a fraud or ZK proof. That's why we need that guarantee. Did that answer your question? Okay, so that is why we need the square. Right.
00:05:52.490 - 00:05:54.310, Speaker B: But from a roll up perspective.
00:05:54.650 - 00:05:56.346, Speaker A: From a roll up perspective, they care.
00:05:56.378 - 00:06:08.764, Speaker B: That the data is available because you can also expand. No, you can. I'm trying to get to the point of like why we're able to disentangle these two things in the first place.
00:06:08.812 - 00:06:09.180, Speaker A: Yep.
00:06:09.260 - 00:06:18.252, Speaker B: So like if we know what's blocking, what is data availability blocking for a roll up? Like I do the sampling because I want to make sure that the roll up block is actually the valid block.
00:06:18.356 - 00:06:24.212, Speaker A: Yes. Yeah, but we are basically blocking on availability.
00:06:24.356 - 00:06:38.020, Speaker B: Yeah, but the thing that we're separating here is consensus. So we have data availability making sure that the block is the actual. Correct. Good block. But then we also have consensus, which is simply ordering guarantees.
00:06:38.100 - 00:07:10.610, Speaker A: Yes. So I can do consensus from. Right from the point of a roll up. What does, what does consensus give? It gives a inclusion and sometimes an ordering guarantee.
00:07:15.870 - 00:07:18.422, Speaker B: By sometimes do you mean the roll up can just.
00:07:18.526 - 00:07:18.958, Speaker A: Yes.
00:07:19.054 - 00:07:19.622, Speaker B: Okay.
00:07:19.726 - 00:07:26.608, Speaker A: You can like reorder, you can have another round of some form of consensus.
00:07:26.664 - 00:07:28.352, Speaker B: Who gives you a timestamp?
00:07:28.536 - 00:08:27.860, Speaker A: Yeah, it's basically just a timestamp server for bytes. So now that we separated that, the general idea from fast block, slow squares is to produce blocks as fast as you can and then afterwards combine them together into a square. So you would have, you can see this as a chain of blocks and you're going over consensus of all those chains. And let's say the block time is. I don't know, how fast do we go?
00:08:28.960 - 00:08:31.264, Speaker B: Let's be conservative with half a second.
00:08:31.432 - 00:08:53.789, Speaker A: Half a second, half a second. That's conservative. All right. Have a second for each block. And afterwards what you're, if the, at some point. Right. So there's another question.
00:08:53.789 - 00:10:12.960, Speaker A: When square, this is a, you know, like a, also like a game. Theoretical question on how much each validator is allowed to put into a square. But the after, like when, you know, when the square is, it should be a deterministic pattern. That's when you put all the blocks into the square. You create a square and then you have a chain basically of square squares. So you can think of it as say this is blue, blue, blue and this is red. Then afterwards, after the finality of this block, and we know that, let's say the square is full, we create a square where we basically put the data and each block will be part of the square.
00:10:12.960 - 00:10:15.520, Speaker A: We have here our blues.
00:10:20.780 - 00:10:24.720, Speaker B: Did we explain why we can't just sample every block?
00:10:25.340 - 00:11:25.290, Speaker A: We can do that. Thank you. So the question was, why don't we just like this. Sorry, I will just like finish the diagram that they are connected. Why don't we just sample each block? The problem here is we want to reduce the resource requirements for light nodes. The amount that they have to sample is constant to the square. So no matter how big the square will be, you will have to sample the same amount of times to have the same probability of availability.
00:11:25.290 - 00:12:25.866, Speaker A: So you can think of it as flipping a coin and to hide data, to be able to make it non reconstructable, you need to either hit one third of the day, one four of it plus one, or, or miss it. Right. And you have a 75% chance to miss what the under a data withholding attack. So this is just some quick maths, right. You're going to, every time you're going to sample, you will have a 75% chance to miss it and sample again and again. So after, let's say 16 samples, you already had a chance to miss it for like 0.01 or so.
00:12:25.866 - 00:13:34.070, Speaker A: So you have a 99% guarantee that it is available. If you want to do that for every block, then you would have to sample 16 times for every block. Now what you have to do is you only have to sample 16 times for every square. So we can collect the benefits here. So for one, we decrease the amount of light node requirements and we get that by having, you know, a constant amount of samples per square instead of block. The other thing is that you would need to download a header for each square. If you're going to sample for every block, you would have to download the header for every block.
00:13:34.070 - 00:14:32.250, Speaker A: But there is a way that you do not have to. You don't need the headers of each block and still have the availability guarantee by putting the headers or let's say proof of consensus into the square as well. So each of the blocks has a header. And I'll just expand this. We can either put the header as a whole or the proof that those headers were like that. The let consensus was good. So into a list of headers.
00:14:32.250 - 00:14:51.700, Speaker A: And we need to do that because we don't have a data availability guarantee on all of those blocks, only in the square. So if we cannot reconstruct those things deterministically we cannot create fraud proofs over them.
00:14:52.280 - 00:14:55.904, Speaker B: I totally forgot about that. It's really interesting. The headers in the square.
00:14:55.992 - 00:15:33.850, Speaker A: Yes. And now you can, now this is like, I think the great part. You can very much see that Celestia is a roll up on itself. Right. We put the transactions of Celestia into the square. We put the headers of Celestia into the square and that's how you basically get the data availability guarantee on all of the things that you need to deterministically create and keep on. So another thing that has to happen is that there needs to be a deterministic square construction from the blocks.
00:15:33.850 - 00:15:40.240, Speaker A: So now if I change anything here, it will also reflect in the square.
00:15:43.220 - 00:16:15.810, Speaker B: To finish benefits. Though we have really fast ordering guarantees. Therefore we have at least two things. You have really good base roll ups and you also have really fast bridging because this is the blocker for bridging between roll ups is this time. So hopefully half a second is fast enough for bridging and while simultaneously lowering the requirements for these light nodes significantly by reducing the number of headers and reducing the number of sampling.
00:16:27.350 - 00:17:15.690, Speaker A: Yeah. So you get less light node requirements, less samples, headers and ordering guarantees basically for fast roller bridging. And the whole construction was made with fraud proofs in mind. So all of that can be fraud provable. And because we create deterministic data routes over those headers. But now with, you know, I could explain how we did this whole construction with fraud proofs, but there is, you know, a, there's a discussion on right now if we are moving to validity proofs. So all of that might not even have to be and makes the construction much simpler.
00:17:16.510 - 00:17:18.470, Speaker B: So we can fraud proof the validity rules.
00:17:18.550 - 00:17:18.790, Speaker A: Yes.
00:17:18.830 - 00:17:19.742, Speaker B: And by that you mean the error.
00:17:19.766 - 00:17:19.926, Speaker A: Sure.
00:17:19.958 - 00:17:20.704, Speaker B: Encoding.
00:17:20.862 - 00:17:21.252, Speaker A: Yeah.
00:17:21.316 - 00:17:32.260, Speaker B: And then you also mean we do this thing where we separate the transactions from the blobs. So within this square, the reason for that is, again, celestia is a roll.
00:17:32.300 - 00:17:33.468, Speaker A: Up on top of itself.
00:17:33.524 - 00:17:47.478, Speaker B: And you don't want to roll up like a lite client to have to download these blobs or whatever, or even a full node because the blobs are extremely heavy. So you have to separate the transactions from the blobs so that you can actually sync the chain in an efficient way. We have a hand raised.
00:17:47.574 - 00:17:51.090, Speaker C: Yeah. Can the valid asset change throughout those years?
00:17:51.590 - 00:18:53.420, Speaker A: So the question was if the validator set can change inside the, between those blocks and what? Okay, so I would, I haven't thought about if they can. I would on purpose not do it because you get benefits for lite clients as well. If we think about each square construction as an epoch, you can set as a rule that all the validator set updates only happen at square construction. So they would be applied at the next block after the square should happen. That way the lite clients don't need to get Valdata set updates in between blocks, which will make the headers much smaller.
00:18:53.920 - 00:19:04.312, Speaker B: So I think they can, as long as it's less than a third and you have to sign over. You still did the epoch thing and you have to sign over the next validator set hash here.
00:19:04.376 - 00:19:18.950, Speaker A: Yes. So basically that's a connection. You're creating a separate header. They're like the square header. Yeah. They're like two chains.
00:19:22.010 - 00:19:24.030, Speaker C: Yes. You could do this without epoch.
00:19:25.210 - 00:19:33.830, Speaker A: Yeah, we do that with epoch datasets. What do you think we should do it for every block?
00:19:38.290 - 00:19:51.626, Speaker C: I think what Evan was saying. So as long as the verdict doesn't change by greater than one third, because you want to make sure that your one third guarantee holds for the people signing over the square as the people signing over all the blocks in between.
00:19:51.738 - 00:19:52.418, Speaker B: Right.
00:19:52.594 - 00:20:06.788, Speaker C: So I think that makes sense. You just can't allow it to change that much period. But what you're saying is also true that it might even just be more valuable that we just keep the value set the same throughout the entire period between one square to another.
00:20:06.924 - 00:20:36.490, Speaker A: So the proposal is we could change the validator set by at least one third in between to have that connection, but we could omit that. I don't see any value yet in changing the val data set between squares. And by not changing it, we keep the resource requirements even lower the value.
00:20:36.570 - 00:20:52.280, Speaker C: Be like more flexibility for the staking module with all these things like voting for you and whatnot. Why would we limit ourselves to have that only between squares? Strong reason to keep this.
00:20:54.060 - 00:20:55.800, Speaker A: Consistent within the.
00:20:59.140 - 00:21:00.844, Speaker C: Is there a reason to keep that?
00:21:01.012 - 00:22:14.450, Speaker A: So like, the reason why I would want to keep the validator set in between squares is to not have validator updates. You would need to have basically either a proof of all the validator updates or all validated updates itself. So you would still get part of the header for every block because the header includes not only the signatures of the validators, but also the updates of the Val data set for a tendermint lite client. And the list of Val data set changes could be quite big if the blocks are very fast. Right, you normally batch it per header, but if you increase the frequency of headers each validate, a set change won't be batched. So even in those 12 seconds time, if the same validators have changed, if you break it up.
00:22:16.310 - 00:22:17.566, Speaker C: Like the rate.
00:22:17.598 - 00:22:19.410, Speaker B: Of the changes or.
00:22:20.270 - 00:23:32.220, Speaker A: Yeah, the rate of the change would still be controlled by the staking module. So we don't get like lose any guarantees on that. You wrote something down, just notes. The next thing, I guess that you can optimize is this header itself. If we want to run our lite client on a toaster, we basically should look into them like what is the maximum that we can achieve while reducing the like, how much can we achieve with very high trust guarantees but still keeping the resource requirements low. So I'll just go into a quick tangent that this header is one, the tendermint header or comma Bft. Sorry.
00:23:32.220 - 00:24:54.588, Speaker A: And the data availability header. And this data availability header is the row and column routes of the square, right. Each row in the square we have a namespace merkle tree over it. And for all those shares, and this is a row root for example. And the data availability header contains all the row and column roots. And you might say okay, this is not much, but the, it is, let's say for eight megabytes it could be, I think like the calculation is 96 bytes per nodes times 256 shares times 2200. So it's around 40,000 know.
00:24:54.588 - 00:24:56.692, Speaker A: And maybe someone can do the calculation in there.
00:24:56.716 - 00:24:57.844, Speaker B: 40 kb.
00:24:58.012 - 00:26:40.160, Speaker A: Yeah, that's what, I'm sorry, 40 that grows. This is like constant in the amount of data. So we cannot actually increase like throughput with this in the sense that if we would have 12 seconds block time with the same amount of data, light nodes would have to download the same amount of bytes of the data availability header. If what we need, as we saw here, is that the headers have to be available, right, the guarantee is not that you have to download them because this holds true also for the data availability header because we need it to be available to create the compact fraud proofs from the full node to the light node. Otherwise we would have to give more data as a pre match for the light node to check the razor coding for example. So what we can do is we can put the headers into the square as well. So in addition to putting the headers of the blocks into the square, we can also put in the last data availability header into the next square.
00:26:40.160 - 00:26:57.050, Speaker A: So by sampling, so by sampling this square, we are also sampling the header of the last square. Does that make sense?
00:26:57.510 - 00:26:58.890, Speaker C: What does that give us?
00:27:00.790 - 00:27:26.340, Speaker A: We don't have to download the data availability header. You're increasing your, like, let's say you're basically changing a little bit your data availability guarantee, because if that square is unavailable.
00:27:28.200 - 00:27:33.088, Speaker B: It might be worthwhile to explain why you have to have the data availability header in the first place.
00:27:33.224 - 00:28:34.540, Speaker A: Yeah, so in the data availability header we have our list of row routes and column roots. And you need this row roots to prove it's basically the hash. And the pre match of that construction is the row itself. So if I want to prove to the light node that the erasure coding between shares was wrong, I would need to give them the row root with the proof to the data root basically is another merkle tree over all of that and the shares itself or half of them.
00:28:35.480 - 00:28:38.780, Speaker B: And yeah, if you ZK'd it would you need that.
00:28:40.080 - 00:28:50.920, Speaker A: If you ZK prove it, like maybe use caseg commitments, then the commitments itself would be the proof. Yes.
00:28:50.960 - 00:28:53.300, Speaker B: Yeah, ok, nice.
00:28:55.000 - 00:29:14.850, Speaker A: But you would still like, you would still need those commitments if you're sampling over those commitments. But you could have a proof over the proofs. So it is actually, there's quite different in Ozk land as well.
00:29:15.750 - 00:29:16.770, Speaker B: Interesting.
00:29:17.870 - 00:29:20.090, Speaker A: Okay, what do I want to go for?
00:29:22.030 - 00:29:25.730, Speaker B: Do we want to cover the square construction? I don't know. I don't want to.
00:29:26.990 - 00:30:55.746, Speaker A: Whatever time is, that's a good enough time. I wanted to add another, like another benefit, but I lost my plot just to, you know, recoup. We have the blocks and the blocks happen in sequence and I, the square collects the blocks and puts them into the square. One thing that it breaks is the API for rollups, right? If you're doing a namespace merkle tree construction, you want to have a range proof over your namespace. I can quickly go how a namespace looks like, like an NMT SlaV squat. The nodes are constructed with the minimum namespace id, the max id and the hash. And the hash is basically the parent of the children.
00:30:55.746 - 00:32:25.190, Speaker A: And then you get another node all the way until the end where you have a leaf node which gives you the namespace IDK and the, like the hash of the data, I think. And this is the data root and this is a leaf. No, like this. What a namespace merkle tree gives you is you can create range proofs over shares of data. So when rollups go and download their data from the data square, they can get only their data and you will get a proof of that. That proves you that it's only for your namespace. If we would just append those different blocks in this, in the square, we would break that assumption because a roll up might post in multiple blocks and those namespaces would not append anymore.
00:32:25.190 - 00:32:27.630, Speaker A: So, yeah.
00:32:29.330 - 00:32:34.120, Speaker B: Are we going to index by heighten? Was that the thing for this?
00:32:36.060 - 00:33:23.060, Speaker A: We don't have to index by height. Okay, interesting. What you can actually do is deconstruct this block into their namespaces and then pull them back together into the square. Okay, so what you actually have here is one namespace of all blocks, then the next namespace of all blocks and so forth. So when a API calls for a square, you would get the full proof over your namespace.
00:33:27.040 - 00:33:31.980, Speaker B: It seems very breaking because do you still get the ordering guarantees at that point?
00:33:32.720 - 00:34:42.720, Speaker A: You can, but you would need to add additional metadata and such as the height. Yes, not the height, but you would add the, like, okay, how do I have a, the query of give me data from this namespace. Right. And what you want to do is have a list in each block, which is a. So in here, like what this is, you would have a list. And in this list there is the commitment and the namespace. You might need some additional metadata, but let's go with that.
00:34:42.720 - 00:36:07.000, Speaker A: So having that list before the block basically gives you the same guarantees as the NMT if this list is sorted by the namespace. So when I ask, hey, give me all blobs from one namespace, there would be, you know, if this is the list, it would be maybe namespace one, one, one and then two. Two is the hash. Let's say those are different ones. Three, four, five. Right. So what you can do is still create a range proof over your data in a compact way and guarantee that it includes all namespaces.
00:36:07.000 - 00:36:15.052, Speaker A: Like includes all the data from your namespace because it is a consensus rule that it has to be sorted in that way.
00:36:15.156 - 00:36:18.636, Speaker B: That makes sense. But what if I want to know, like if I'm a base drawer?
00:36:18.668 - 00:36:18.884, Speaker A: Yes.
00:36:18.932 - 00:36:25.004, Speaker B: I'm trying to get, I want to know which blocks are different from this. From this block to this block.
00:36:25.092 - 00:36:25.572, Speaker A: Yes.
00:36:25.676 - 00:36:26.760, Speaker B: How do I do that?
00:36:27.060 - 00:36:57.216, Speaker A: You would query this block from your namespace. Normally you're basically subscribing and getting the information what's in your namespace. What you would get is a proof of the new data that it is. So where from that list and each of that hashes, is it pob commitment of the blob? That's this, yes.
00:36:57.288 - 00:36:57.904, Speaker B: Okay.
00:36:58.032 - 00:37:14.400, Speaker A: So you would get not only the proof, which normally would be the namespace proof, but from that list in addition to the blobs itself. Okay, does it make sense?
00:37:14.520 - 00:37:15.500, Speaker B: Yeah, I think so.
00:37:18.890 - 00:37:22.630, Speaker A: And that way you don't actually break the API anymore.
00:37:24.770 - 00:37:25.710, Speaker B: That's good.
00:37:28.810 - 00:37:35.378, Speaker A: Yeah, because otherwise the whole NMT construction doesn't work anymore. And we have to remember.
00:37:35.474 - 00:37:36.482, Speaker B: Slightly problematic.
00:37:36.546 - 00:38:30.846, Speaker A: Yeah. We have to remember that the transformation of those blocks into a square needs to stay deterministic for both rot and zk proofs to work. I got it right and exactly right. So every time you sample over the square, you'll be sampling over all the blocks. So if a full node just gets the squares, they will be able to still replay all the headers and check all the consensus rules in between. If everything was correct and then if the construction was correct as well, maybe we can talk about when the square should be created. Oh, that was the benefit.
00:38:30.846 - 00:39:31.710, Speaker A: Okay, so one thing that you might want to reduce is how much the ratio of erasure coded data to original data. Currently we timestamp every 12 seconds we get the square. And it could be that we have dynamic square sizes, but the square might get created at a point where we don't have a lot of data in the square. So in the worst case we, the square is a quarter full plus one. But we eraser code three pieces of data. Right. So the ratio is a quarter to three.
00:39:31.710 - 00:40:30.708, Speaker A: So the full nodes, sorry, the full nodes have to store a lot of erasure coded data, which is not the data that we need, but it's just padding. So we eraser code like we erasure code too much by making it the rule of the square. If it is full, we fill out by construction the whole data square. So the construct. So the erasure coding ratio is. I played myself one, two, three. Erasure coding ratio, right.
00:40:30.708 - 00:40:55.478, Speaker A: Which is a huge improvement on the worst case storage requirements for full nodes. So one of the rules, and you can challenge it is if it is full. So I'll expect a question, I think.
00:40:55.494 - 00:40:58.798, Speaker C: You'Re going to answer it now, which is like what square size should you pick?
00:40:58.934 - 00:41:38.738, Speaker A: Yeah, so the, so when it is full would be the max square size that we support. And that square size depends on a, like three factors. How many. So like there's a size which we most likely call at least in the fraud proof paper it's, it's k. Then you have the number of light nodes, which is c, and the number of samples, which is SDE.
00:41:38.784 - 00:41:40.470, Speaker B: That each light node is simply that.
00:41:40.510 - 00:42:29.090, Speaker A: Each per light node. Yes. And with these three numbers you can calculate the block reconstruction probability. So in the case of many light nodes sampling the same square and then those samples are in the network, a full node is getting them and then they have to reconstruct it. Because the sampling is probabilistic. The reconstruction is probabilistic as well. And we will only want to increase the block size by having a safe block reconstruction probability guarantee to get level four light node security guarantees.
00:42:29.090 - 00:42:42.900, Speaker A: So if we increase the size, we at least need more light nodes in the network or we need to increase the samples per light node. Yes.
00:42:43.560 - 00:42:57.460, Speaker C: I would argue that there needs to be another variable involved, which is time. In this instance there should be an upward bound before square is produced, right. If there's no transactions coming through, a very low throughput, eventually lineups do once onto the bike.
00:42:58.280 - 00:43:33.890, Speaker A: So the question was, or the suggestion was we should have an uptime, an upper bound on when the square is created to give the availability guarantee at some point. And this is exactly correct. There's basically a condition and one of the condition is the maximum wait time. Right. I don't know how to call it variable, I just call it Max. Max. Just max time.
00:43:33.890 - 00:44:52.926, Speaker A: Oops. Okay, this is getting very explicit. But the longest wait for availability, that is the time that would be cut off. So if you're producing blocks and you're seeing, okay, it isn't full, I would create the next square depending on the time when the last block was produced. So you would have to, consensus would be dependent on the block time. And what is interesting in like the question of when the square is happening is that you have to set up a, some limit on how much you want to add to a block. Right.
00:44:52.926 - 00:46:57.410, Speaker A: Where in a case where you're producing a block and there is some variability of, okay, when the square is created, it's nothing going to be fully full. Right. The last validator won't squeeze in like, I don't know, ten bytes or so because maybe everyone is posting a lot. So you have to have a slight difference between how much is going to be added and that difference until fullness is basically the proportion of your block time to the longest wait for availability in proportion to the max square size. So how many of full blocks could I fit in into the square? And so you cannot like the attack would be that, hey, I know that this validator will come on, so I will add a bit to trigger that to overspill and the next validator, and that basically solves it. Okay, so we're closing to our end, and I'll quickly recap why you would want to have fast block stall squares. We have a constant work that a light node has to do, so it would be incentivized to have that as slow as possible with some upper bound, but we want to have the inclusion and ordering guarantees as fast as possible for cross roll up interaction.
00:46:57.410 - 00:47:21.696, Speaker A: So you would have less samples and less headers for the same amount of data with a, like a much better eraser coding ratio. So less full note requirements. But rollups on top would have fast cross roll up bridging guarantees. Thank you, Evan and the audience for participating.
00:47:21.848 - 00:47:22.752, Speaker B: Before we end.
00:47:22.816 - 00:47:23.560, Speaker A: Before we end.
00:47:23.640 - 00:48:17.440, Speaker B: Before we end. I just think it's also just so amazing. It's totally one thing to make data available via sampling, but it's entirely another thing to make it actually usable. And you have to continually add these constraints then in order to make data availability sampling actually useful for light clients. They can't actually make assumptions on the validator set because now you're just opening attack vectors from the validator set to these late clients. So it's just, you can see these complex games being played. Nashville has done so much amazing work here because to just play these layers of games to make sure that the validators can't actually do anything to the light clients and that we're never actually overriding things like being able to reconstruct the block and change these headers and things, it's just like, it's a lot to think about.
00:48:17.440 - 00:48:20.788, Speaker B: So thank you, Nashkir.
00:48:20.844 - 00:48:22.700, Speaker A: Of course. All right, that's it.
