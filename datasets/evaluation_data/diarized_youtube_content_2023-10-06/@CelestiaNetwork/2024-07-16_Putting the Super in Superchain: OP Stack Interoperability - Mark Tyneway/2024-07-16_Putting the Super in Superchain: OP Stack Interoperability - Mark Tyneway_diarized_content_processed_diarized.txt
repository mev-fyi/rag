00:00:01.960 - 00:00:23.450, Speaker A: Hello. Hello. Hello. So my name is Mark, and I'm a contributor to the optimism collective. And today we're going to talk about putting super in the super chain. Right? So what does this mean? Right. So my claim is that we're not going to make it unless we overhaul the way that users interact with blockchains.
00:00:23.450 - 00:00:50.568, Speaker A: Right. So here we are, as ethereum as the community. Right? Vitalik is talking about all of the historical things that, you know, all the historical problems that we've solved. Right? And this is where we are now. Right? Cross l two ux is too fragmented. So that's what I mean by talking about putting the super in the super chain. So here's Peter.
00:00:50.568 - 00:01:26.526, Speaker A: He's talking about how he's having ux horrors of crossing between l two s. He currently has ether on op mainnet, and he needs base eth, and then he needs to go over to Zora to mint an NFT. So he's asking, can the devs do something? This is right here. This is our attempt to do something. So our goal is to make the super chain feel like one chain. Cool. So let's put on our archaeology hat, because they say that, you know, a lot of computer science is just archaeology.
00:01:26.526 - 00:01:45.690, Speaker A: Like, all these problems, these hard problems have just been solved. We kind of have to just go look at the past, kind of like dig up some old secrets, like, apply them to the new problems. So. Oh, okay. Like, we're in this, like, you know, mysterious old, ancient ruins and, you know, let's explore a little bit. Okay. We found a treasure chest.
00:01:45.690 - 00:02:03.614, Speaker A: Okay. So wonder what's inside. Okay. Oh, okay. Designing data intensive applications. This is like some great wisdom from the web two world that we kind of forgot about here in crypto. So I'd like to ask the question.
00:02:03.614 - 00:02:55.362, Speaker A: Maybe we should be thinking of chains as horizontally scalable microservices in a wider microservice architecture. So I'm going to be talking about interoperability today. And the first thing I'd like to talk about is the product, because one thing that this industry likes to do is we like to focus on crazy technology. We like to talk about the solutions, the infrastructure, but really we got to focus on the users. We got to solve problems for users, because otherwise our really interesting technology is never going to get used and we won't be able to continue working on it. So secure portable fungible ERC 20 tokens. So we are making yet another token standard called super chain ERC 20.
00:02:55.362 - 00:03:51.244, Speaker A: And it's a small extension to the ERC 20 standard. But the idea is that any of the assets you've deposited to op mainnet base or any of the other chains will be able to seamlessly migrate to a new ERC 20 token standard that will allow them to be fungible across all the chains that are interoperable without cross chain liquidity constraints. So you're asking like, why are these people creating yet another standard? Like, there's already a million standards, right? Why? So you can use XCRC 20. It's a free standard. It's very flexible to work with many bridges. You can easily build this right on top of the low level abstractions that we're providing, if you prefer this. Cool.
00:03:51.244 - 00:04:25.344, Speaker A: So after talking about the kind of product side of things, we want to allow cross chain portable ERC 20 tokens. Now let's talk about the secure message passing. This is where it gets fun. It's like more protocol stuff. So let's talk about our design philosophy first. One big thing that we always think about when working on the op stack is the minimal diff. And George Hotz gave us this advice.
00:04:25.344 - 00:04:49.900, Speaker A: He said, focus on the minimal diff because there's always bugs in your code. And the more lines of code that you write, the higher chance that there's going to be bugs in it. So write less lines of code. And whether you love George or not, he's an interesting and controversial person. He's out there, he's doing cool things. There should be more people doing epic things like him. Also, we want it to be flexible.
00:04:49.900 - 00:05:57.750, Speaker A: What does this mean? It means that we want it to be able to be used in many different ways. We don't want to enshrine too many opinions about the design, and we want it to just work with existing applications. What does that mean? That means that if you've already deployed applications to op mainnet base or the other chains, you should be able to instantly start exporting your data to other chains. In importing data, you don't need to necessarily plug into, say, a new system to be interoperable. And we also want it to be simple to reason about. So one good thing to focus on is if you're designing a protocol, you want the design to be very small so that you can fit it all on your head, right? Because as soon as something becomes too complicated to fit in your head, then that's where problems arise and you can introduce bugs. So we're focusing on simplicity here, and I'm going to start with some vocabulary so that we can all kind of align our mental models and think about this.
00:05:57.750 - 00:06:33.200, Speaker A: So the first thing that we want to talk about is this idea of a dependency set. The dependency set as a chain operator, you get to manage your dependency set. You get to basically say, these are the sets of chains that I would like to accept inbound messages from. And any chain has the right to define this. Then we have this initiating message. An initiating message, and there's an executing message goes along with it. These two together make the full cross chain message.
00:06:33.200 - 00:06:50.892, Speaker A: So an initiating message is simply a log. Very simple. There's nothing more to it. It's just a log ethereum in solidity. You can emit events. Those are creating logs within the protocol. And then we have this idea of an executing message.
00:06:50.892 - 00:07:31.070, Speaker A: So the executing message is represented as a log on the remote chain. We have this source chain and a remote chain. The initiating message is created on the source chain. The executing message is created on the destination chain. The idea is that the executing message includes the initiating message as well as a unique identifier for that initiating message. So getting into the user experience, there's a total of two transactions that are sent. We have this log, which is the initiating message.
00:07:31.070 - 00:08:21.710, Speaker A: It's created on the source chain. And then that initiating message is sentence into the destination chain. So you can kind of think of it as like you're able to teleport an event that your smart contract has emitted into another chain. So you can introspect on the events of other chains. And we believe that in practice, what will likely happen is a relayer system will exist and send the second transaction on behalf of users. So the ideal user experience is the user really just sends one transaction and then some sort of solver network exists and ends up sending the executing message. And the user can get this feel of just sending a single transaction.
00:08:21.710 - 00:09:04.600, Speaker A: And it feels just like how l one feels. So the dependency set, I mentioned this before, each chain can define their own dependency set. And this is all managed on chain. And this relationship is unidirectional. So what that means is it's permissionless for anyone to accept inbound messages from your chain. You don't need to convince the other chain to also accept inbound messages from you. You can just turn on your chain and then define the chains that you want to depend on as part of your dependency set and then start accepting inbound messages from them.
00:09:04.600 - 00:09:43.574, Speaker A: So the initiating message, right, we spoke about this a little bit before. Literally any log can be an initiating message. So this is very simple, very simple. You don't need to do any extra integrations all the applications today already emit logs. Now we have the executing message. So I'll start off by talking about the invariant that makes this all work. So, because it's really useful to think about if you're designing a protocol thinking in terms of invariants, everything is just invariants and state machines.
00:09:43.574 - 00:10:18.180, Speaker A: And if you define all your invariants upfront, then you can be sure that your system is secure. So we're going to think about the invariant here. The invariant is that all executing messages must have an initiating message that matches its identifier. So what does that mean? Right. So the executing message is this tuple of the initiating message and its identifier. And this is the definition of an identifier. So the identifier is able to uniquely identify a log.
00:10:18.180 - 00:11:12.268, Speaker A: So what information do you need to do this? You need a block number, you need a chain id, and you need a log index. Right? The chain id tells you where, what chain, out of all the chains emitted this log. And then the block number tells you, like, in what block was this log emitted? And then the log index tells you, because the logs are all ordered, it tells you which specific log that is corresponding to the initiating message. So given this identifier, you can look up any log across any chain. So we have this special pre deploy smart contract. The op stack loves pre deploys because they're EVM bytecode. Instead of adding new precompiles, we add new pre deploys.
00:11:12.268 - 00:11:55.800, Speaker A: And the purpose for this is to make it seamless to integrate with tools like foundry. You don't need to go and add extra code to foundry, maintain a foundry, fork all this stuff just to have your precompile work. Right? It'll just work. So, yeah, basically the node will observe all of the executing messages and enforce this invariant. So now I'm going to go through these key properties. All right, the shared proof system. Okay, so the shared proof system, this is going to be kind of bundling all the security of the interoperable chains into a single fault proof.
00:11:55.800 - 00:13:04.416, Speaker A: And why is this important? This is important to reduce platform risk and governance risk. When you have different chains that are all operating under different consensus models or have different proof systems, then there's always a risk that that chain, the governance of that chain will kind of change its rules. And under a shared proof, you kind of, you know, enforce that. The chains that you're operating with will be working under the same security properties as you. So this gives the property of, like, fungibility of, you know, ether or ERC 20s across all these chains, right? And just, you know, helpful reminder proofs are useful for withdrawals from the bridge, and the bridge is not the roll up, right. A lot of people have this misconception that, you know, if the fault proof, you know, removes a claim, you know, because people make claims when they want to withdraw, they say, hey, this is the state of the l two at this block. And, you know, a Zk roll up will be like, all right, here's my validity proof.
00:13:04.416 - 00:13:41.130, Speaker A: And then the bridge is like, okay, I accept that this claim is correct. An optimistic roll up is like, you know, truth comes from lack of disproof, right? So when somebody makes a claim, right? They're like, all right, I. I claim this is the state of the l two at this block height. And if no one is able to disprove that within a time period, then it becomes finalized. So if the fault proof removes a claim from the bridge, it does not cause an l two reorg. This is like a big misconception because the chain is not. Or the bridge is not the roll up.
00:13:41.130 - 00:14:56.320, Speaker A: Cool. Let's talk about message integrity, because it seems kind of like maybe the sequencer can spoof these cross chain transactions. Well, that is not true, because we define this in an invariant first way, where all of these executing messages events must be validated as part of the fork choice rule. Any block that produces these executing message events, if any of the tuples, the identifier and initiating message pair that's emitted from this event, if that executing message, if the values in it don't match, then the entire block is considered invalid and reorged out. So that means that the sequencers or the block builders are going to always want to only include valid cross chain transactions. And ultimately, it's like the full node software that protects users here. Also, another feature is like, we don't have this idea of force inclusion for these transactions.
00:14:56.320 - 00:15:49.980, Speaker A: There's no idea of sending a l two to l two transaction where it's automatically force included into the remote chain, right? You can kind of think of it as it's kind of like a broadcast UDP sort of system, right? Because if we had force inclusion, it would add a lot of complexity. So you would either need one of two things. You would need to have an awareness of the gas market on the remote chain, or you would need to have an inbound message queue. Both of those are very complex, and simplicity is key. We always use simplicity to drive decision making. So it also reduces this tight synchrony assumption between all the chains. And it's also backwards compatible.
00:15:49.980 - 00:17:04.040, Speaker A: All of the existing applications today already use events. So as soon as this goes live, your application that's already emitting events, all that information can be consumed on different chains. And an interesting property of this is that as the builders become more sophisticated or the sequencers kind of the same thing, but it improves the scalability of this. Right, and this very simple low level protocol could in theory be used to build a shared sequencer on top, if people want that. So yeah, we can build kind of like atomic cross chain transactions on top of this with sequencer confirmation levels of trust. So usually for an l two, l two builds a block. Then the l two has this idea of a preconfirmation or a sequencer confirmation, which is a pinky promise, saying this is the data that I'll ultimately submit to the data availability layer.
00:17:04.040 - 00:17:55.928, Speaker A: So the latency can be extremely low if you trust those sequencer confirmations. But this protocol can be completely trustless as soon as that data is published to the DA layer. So if you're consuming data from a remote chain, as soon as they batch submit the inputs, the transactions that produced the outputs, the logs, as soon as that remote chain batched their data, then you can start consuming it trustlessly and you don't need to really trust anything. Cool. So cross chain messaging. I've been talking about low level abstractions here. The lowest level abstraction is broadcast UDP.
00:17:55.928 - 00:18:31.908, Speaker A: You just emit an event and then it just goes out there and any chain can just choose to consume it if they want. Then we can build higher level abstractions on top of this really simple thing. We've built a higher level abstraction that gives both replay protection and domain binding. So basically that says you can only send this message to a particular chain once. This is very useful for sending ether or ERC 20s between chains. Cool. So now I'll talk a little bit about contributing, how to build on top or contribute.
00:18:31.908 - 00:19:05.066, Speaker A: So at op Labs we are committed to working in the public. I think it's very important. All of our specs, open source free software, all of our project management, all happening on GitHub. You can go right to our GitHub repo and go and look at all of our tickets. And we also have a contributor to Discord. So we're kind of looking at how l one operates. And we think that by opening up development it'll help to make it easier for people to contribute and understand what's going on.
00:19:05.066 - 00:19:39.764, Speaker A: So if you're interested in joining, come chat on our GitHub. Show that you're adding a little value, and then we'd love to invite you. And we're currently working on a public devnet for this, so this is coming soon. And basically my claim is that we need to enter the world of modular applications. So these are horizontally scalable smart contract systems. Like modular has been the biggest buzzword for the longest time. Everyone's talking about modular data availability, modular execution.
00:19:39.764 - 00:20:23.450, Speaker A: Well, now it's time to add another thing to the buzzword. It's modular applications. And when I say application, I mean like, DAP, smart contract. I like calling it app, because what's a DAP? It's an app, but, okay, so horizontally scalable smart contract systems, this is the. I call this like the bring your own gas model, right? So, like right now, if you turn on extra servers, you cannot, you know, you can't bring gas to the network. You can't speed up the network by turning on new servers. It's a replicated state machine, right? This is unlike the web two world, where you can just turn on more servers and, you know, process, you know, more usage of your application.
00:20:23.450 - 00:21:28.220, Speaker A: So we want to enter a post gas scarcity world. This is like the broadband moment for crypto. So the idea is just to start thinking about blockchains as cpu cores or threads or microservices. So in the world of modular apps, what we want to do is we want to look at our smart contracts, see where the gas bottlenecks are, and then we want to redesign our architecture to try to shard the most expensive parts of our applications across multiple chains so that we can easily horizontally scale our applications. This is exactly how web two is able to build for actual global scale. And in the future, we're looking at using ZK proofs to relax this dependency set constraint. And this will be realistic once the costs are low enough and we have proof generation time fast enough.
00:21:28.220 - 00:21:30.760, Speaker A: Cool, that's it. Thank you very much.
