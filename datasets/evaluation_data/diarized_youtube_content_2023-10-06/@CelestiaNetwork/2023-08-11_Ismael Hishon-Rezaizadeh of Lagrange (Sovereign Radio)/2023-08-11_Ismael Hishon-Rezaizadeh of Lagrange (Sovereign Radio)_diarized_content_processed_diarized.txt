00:00:03.580 - 00:00:09.516, Speaker A: All right, tell me about yourself. How did you get into the whole space? And what's your involvement with Lagrange?
00:00:09.628 - 00:00:32.516, Speaker B: So, I'm the founder of Lagrange Labs. We build computation that increases the expressivity of how states can be used on chain and between chains by being able to prove computation on large data sets and data parallel computation in particular, in a zero knowledge context. I was previously a venture capitalist, and before that I was an engineering leader for a financial services company focused on digital asset strategy.
00:00:32.628 - 00:00:35.096, Speaker A: In layman's terms, what does Lagrange do?
00:00:35.198 - 00:00:56.588, Speaker B: So what we do is we enable you to run the same types of data rich computation that you'd traditionally run on top of a database or a data lake in web two on top of on chain data between applications and from applications, things like SQL, things like Mapreduce, things like Spark and RDB. We let you treat data on chain as if it was structured that you would treat it the way with a web two database.
00:00:56.764 - 00:01:04.656, Speaker A: Okay, awesome. Now, if you were to give a vc the elevator pitch of what this does, what would you say?
00:01:04.758 - 00:01:43.264, Speaker B: So what I would say is that there is a lot of d five primitives game five primitives and social five primitives on chain that can't really be built in a trust minimized fashion. The example I like to give is something like volatility. If you have a modular roll up with a canonical bridge to ethereum, and you wanted to take volatility over a period of time, do a black scholar model, for instance, and you wanted to do that in a way that doesn't incur any additional trust assumptions associated with the Ethereum estate. Beyond that which is guaranteed by the canonical bridge, we allow you to run that computation on top of the underlying data commitment in a way that is fundamentally as secure as the data that's contained within it.
00:01:43.462 - 00:01:49.196, Speaker A: Okay, so how is it complementary to the tool set that already exists?
00:01:49.308 - 00:02:21.480, Speaker B: Yeah. So when we look at crosschain in general, there's really three components of how we see it. You have what is an assertion of state at the base layer. It can be a proof of consensus. It can be an assertion by a validator set, or it can just be an oracle. Then you have the transport estate moving the state from chain a to chain b, and then you finally have the computational state. And so most cross chain protocols, typically messaging, bridging, they focus on the assertion of state, the transport estate, and checking a simple property, the existence of a transaction, the existence of an event, et cetera.
00:02:21.480 - 00:02:28.380, Speaker B: What we allow you to do is take that same infrastructure and use that to pass expressive properties of data between chains.
00:02:29.440 - 00:02:42.480, Speaker A: How does it verify whether something is true or not? Because what if a sequencer lies to you? What if, like a relay or whatever?
00:02:42.630 - 00:03:11.592, Speaker B: Yeah, so it doesn't work on trying to say this is the correct data. This isn't the correct data. It says that if this block header is correct, which you've implicitly opted into as part of the security design of your protocol, then you can prove all of this other rich information about the chain in question. You can prove moving averages of price, you can prove volatility of assets, you can prove the ownership of nfts, you can prove the result of SQL cores on top of on chain data as if it was in a database, assuming.
00:03:11.656 - 00:03:15.740, Speaker A: That the relayer and the assertion of state was correct.
00:03:15.890 - 00:03:18.940, Speaker B: So it inherits from any arbitrary protocol.
00:03:19.760 - 00:03:39.888, Speaker A: If most of the CRM network depends on a single validator for state, like flashbots, and if flashbots turns malicious, then it would still inherit the security of the network.
00:03:39.984 - 00:04:27.620, Speaker B: So this is not designed to try to tell you what the correct state is and what isn't the correct state. What this is designed to do is when you have a cross chain, when you have a protocol, you typically have opted into a security model or a security design by just the network you're in and the relationships that you have with other state and other chains. If you're on a modular chain that has a canonical bridge to Ethereum, you have secure state access to Ethereum that's secured by that canonical bridge. And now we allow the computation to inherit that. If you, for example, are building with Axler, or layer zero, or polymer or polyhedra, or many of these other protocols, you typically are already opting into the security model. What we allow you to do is retain the same security model you opted into for bridging and messaging, and now use that as part of these more data rich computations.
00:04:28.200 - 00:04:30.276, Speaker A: So what's the use case?
00:04:30.378 - 00:05:24.912, Speaker B: Yeah, so the use case, there's a lot of them, right? I think when you think of general purpose computation access to data, it's important to think of what are the new primitives that can be built and will be built. One of the things we think is very interesting is complicated pricing, both for collateral as well as for options, being able to derive features of how assets are being valued on chain within existing exchanges and existing protocols, and then being able to leverage that to remove dependencies on off chain data feeds. Another thing we think is very interesting is when you build Gamefi autonomous worlds and some of these more intricate on chain compute, heavy Gamefi and NFT based applications. You now can query and interact with data that doesn't require you to compute and loop over huge amounts of on chain information on chain.
00:05:25.056 - 00:05:37.828, Speaker A: And so with respect to general message passing, how does it relate back to something like interblockship communication protocol or how Axlar has its general message passing protocol?
00:05:37.924 - 00:05:56.496, Speaker B: Yeah, we think that there are a lot of terrific designs for message passing and for the movement of state between chains. I think IBC is a great example of that. Axel is a great example of that. Layer zero is a great example of that. Hyperlink is a great example. Omni is a great example. There are a lot of great protocols that do this, and I can go down the list.
00:05:56.496 - 00:06:21.096, Speaker B: What we enable you to do is to select the one that's best suited for your application and now do more with that state. And you don't have to now rip out your cross chain protocol and pick a new one. If you want to access moving averages of price, you can opt into the one that you would like to use and have zero knowledge based and secure computation sitting on top of the data that that protocol asserts to.
00:06:21.278 - 00:06:32.028, Speaker A: Okay, so if someone were trying to query some kind of data from the entire blockchain ecosystem, they would select what their preferred transport layer is.
00:06:32.114 - 00:06:34.380, Speaker B: Yeah, and there's secure assertions of state.
00:06:34.530 - 00:06:45.010, Speaker A: I see. Okay. Do you think that that hurts progress towards standardization of what that transport layer may be? Or do you think that.
00:06:46.900 - 00:07:18.872, Speaker B: We'Re unappinionated when it comes to what a transport layer should or will look like? We think that you need to have the ability for applications to have agency over the transport layers that they would like to opt into, as well as chains themselves. And at that point, you need to have tooling that enables chains to interact and treat data as it should. And it's not the job of a protocol like me or protocol like ours or a developer like me to say, this is what your protocol should opt into, and I'm going to not support you if you don't.
00:07:19.016 - 00:07:25.596, Speaker A: Got you. What about a transport layer compels you to support it rather.
00:07:25.778 - 00:07:29.230, Speaker B: If they move state between chains, we can support them and we will.
00:07:29.600 - 00:07:30.636, Speaker A: I see.
00:07:30.818 - 00:07:34.496, Speaker B: So every transport layer, every transport layer that moves state between chains, we will support.
00:07:34.598 - 00:07:42.844, Speaker A: Okay. All right. So what was the opportunity you saw in the space to compel you to build something like grunge?
00:07:42.892 - 00:08:36.588, Speaker B: Yeah, I think when you have built a lot of on chain applications, most developers have traditionally run into limitations with how data can be accessed and treated. One of the examples I like to give is, if you have an NFT contract and you say, I want to know all NFTs that are owned by Shango, how do I do that? Well, what you actually have to do is to go on the mapping and to loop through every single mapping from your contract or from whatever contract needs to access this data, look through every single slot, and figure out whether or not there's an NFT that is owned by you there. And so you can't just very simply query your search over this data and filter through it the same way you would if this was an off chain data structure. And so what you should be able to do is to be able to treat this fundamental morass of data that we all are creating a fundamental multichain data lake as if it was a data lake in the computer for it.
00:08:36.774 - 00:09:05.790, Speaker A: Okay, zooming out to the 20,000 foot view, given that our entire world is moving closer to AI, and not everyone understands what's real or fake anymore, and the problem will only get worse. Does something like Lagrange helped to mitigate some of that in terms of proving provenance, identity, that kind of thing? Who really signed what?
00:09:06.320 - 00:09:49.960, Speaker B: That's a really good question. I think one of the most exciting things that we're seeing being developed right now is ZKML and ZKi associated technologies there. And I think one of the areas that people often overlook when looking at ZKML is what is the provenance of the input data that is being used by the model. If you can't ascertain the veracity of the input data, you really can't ascertain the veracity of the computation run on top of something, because you can always just change the input data. You can put an adversarial input into there, and then the result of the model is going to not be what you think it is, or could be malicious as well. And so what we enable you to do, especially with this type of distributed and data parallel computation, is to really start building ZKE ETL pipelines.
00:09:50.460 - 00:09:51.556, Speaker A: And what is that?
00:09:51.678 - 00:10:36.824, Speaker B: Extract, transform, load, like the way data would be processed when you would be ingesting it into a model. In a more traditional web two context, you can now do with the types of data that you'd like to be inputting into your ZKML model. So you say, I would like this user's input activity for the last two months so I can underwrite a credit instrument to them, let's say, and you say, okay, the model that's going to be used to underwrite the credit instrument is going to be ML based. Well, if I have to trust some off chain party to put the data into that model, there's no way I can trust the computation that model renders it provides. So you have two problems there. You have the proving of the model, then you have the proving of the input. And what we enable you to do is to now compute over huge amounts of on chain data as if they were in your web.
00:10:36.824 - 00:10:37.764, Speaker B: Two, data lake.
00:10:37.892 - 00:10:55.024, Speaker A: What if a machine like an AI, were able to generate a whole bunch of different addresses? It doesn't necessarily correct for that, right? To understand whether or not there's a human behind an identity, like an address identity, or if it's an AI behind it.
00:10:55.062 - 00:11:48.130, Speaker B: So this is not an inference based technology. It's a data parallel computation and data transformation data pouring technology. And so the model itself likely should hopefully be tuned to be able to discern between fake user personas and real users who are looking for credit instruments. But once again, if you have an AI that generates a bunch of fake addresses and the AI can just compromise whatever the off chain data feed is that's supposed to extract that address out, you've now incurred two sources of attack. One that the model can discern, which is what it's hopefully been developed for, which is being able to determine on chain whether or not this address belongs to a user and should be issued a credit instrument. And then the second thing, which it can never be able to discern is whether or not there's been some off chain hack that it has no knowledge of.
00:11:48.740 - 00:12:07.456, Speaker A: Yeah. And it's also not perfect, because an AI could also employ a human to create these on chain identities as well, on its behalf. It's not perfect. Okay, great. Any last thoughts about the modular stack and why you're here at modular summit?
00:12:07.568 - 00:13:06.104, Speaker B: Yeah. So, one of the things that we're very excited about in the modular ecosystem in general is the ability to have modular chains that can have access to state and storage proofs of other modular chains without latency or overhead. And generally, we're seeing a proliferation of general message passing protocols in the modular stack. But what we're not seeing and what we don't have access to right now is ubiquitous ways to compute and to transform and to interact with the underlying state between chains beyond event emissions and transaction inclusion. And so with the efficiency of computation that we can provide, we can start deploying, and we will start deploying stateless contracts on modular chains that make it very, very easy for them to be able to verify the existence or the inclusion of some property of other chains when they receive block headers from cross chain protocols or conical bridges. And so what this does is that this allows you to now, in the modular space in general, be able to have more expressive access to data.
00:13:06.302 - 00:13:18.830, Speaker A: Okay, great. So do you imagine that this thing is going to have longevity? And ten years from now, are people going to be using this to query archival blockchain data that's produced today?
00:13:19.280 - 00:14:12.216, Speaker B: Yeah, I think SQL and Spark and RDD and Mapreduce. And the way we handle data at scale is not going away. We are actually creating more data on chain than we really have the ability to effectively process from any of our state machines that we're developing. I think broadly speaking, when we think about the progression of the development of expressive state machines, we often focus on the machine aspect, the execution layer. We don't really think about how state machines are fundamentally constrained by the prefix state. And so the ability to access prefix state by the prefix, by the first word of state machine state. Because if you're developing an expressive state machine, right execution layer, then you typically say, okay, there is some data commitment storage route and we're going to execute computation over this within our execution layer.
00:14:12.216 - 00:14:40.568, Speaker B: What you don't often think about is, well, how about all of the other data that I could potentially access, data that typically you don't have access to at the point of execution of some function or state transition on chain. And so that is where we sort of think that the ability to now have additional types of data available at the point of your execution of a state transition allow you to have applications that are much more expressive with respect to the underlying data that they compute over.
00:14:40.654 - 00:14:48.360, Speaker A: What kind of data is currently available on chain that is not expressive enough for the use case that you're thinking of.
00:14:48.430 - 00:15:24.550, Speaker B: Yeah, for example, moving average of price, let's say I would like to compute a moving average of price on optimism or on an op stack roll up over three months. And I would like to factor out the top 10% about liars and bottom 10% about letters. And I wanted that block by block. What I have is 19.2, roughly million blocks over three months. And I need to do two storage proofs per block, let's say. So I have about 40 million storage proofs I have to do, and then I have to compute over all of that data to figure out what the price is to do a very simple computation of what's the moving average of price every three months.
00:15:24.550 - 00:15:50.030, Speaker B: And then you have to update that every 2 seconds whenever a new block comes in. So that's the type of thing you can't really do with onchain state right now, because you just don't have the execution layers having access to that historical data or that depth of data, nor do they have the computational ability to actually loop over all of it. And this is the type of thing where you now have compute that is verifiable data parallel that you can start doing.
00:15:51.700 - 00:16:05.536, Speaker A: So my understanding of that problem is that there's a lack of archival nodes. There's also a lack of incentive to run archival nodes. So how does Laurenge solve that?
00:16:05.638 - 00:16:38.350, Speaker B: Yeah, so I would say it's less about the archival node and more about the computation on the data. So if you have an archival node, you can extract the data off chain, right. You can do the compute off chain, but it has to do with being able to say from your contract, I would like to execute some action based on price, and then you have the question of the provenance of that data. Right. If I do that compute off chain on my archival node and feed it back on chain, there's an agency assumption and there's a trust assumption associated with who I am and what I'm inputting. So what this allows you to do is to prove compute at a very large scale on on chain data.
00:16:39.840 - 00:16:47.376, Speaker A: That's only a problem in stateful networks, in proof of stake networks, proof of.
00:16:47.398 - 00:16:51.536, Speaker B: Work, any blockchain that has state that.
00:16:51.558 - 00:16:52.640, Speaker A: Has a state, right?
00:16:52.710 - 00:17:18.010, Speaker B: Yeah, but we actually haven't explored much on like UtxO based models and whether or not this would be, this would be applicable there, like bitcoin. I'd say if you have a proof of work network that has smart contracts and programming language, and you can write and maintain a stateful representation of all the data on the chain at a point in time, then what you have, you still have the same problem set.
00:17:19.440 - 00:17:36.480, Speaker A: I wonder how layer zero does this. Layer zero. I believe they use hashes to prove state, but not to a deep level. So I wonder if they actually address this problem.
00:17:36.550 - 00:17:57.232, Speaker B: This is a perfect question. This is a really good example. We think super highly of the layer zero team. And so what they do is they have two components of their infrastructure. Broadly, they have oracles and relayers. Oracles make assertion to, this is the state of some source chain on some destination chain at some point in time. So an oracle would say at block hype x, this is the state of Ethereum.
00:17:57.232 - 00:18:42.084, Speaker B: And then the relayer moves a proof that contained within the receipt route of that block header that the oracle asserted to that there is an event, and they run that on chain as a Patricia triproof, very similar to a storage proof, but on a different route. That being said, you now have a block header that you're checking a single property in. The question is, why don't you take something like Lagrange, and why can't you take something like Lagrange and allow it to compute over that underlying data and now prove a different property instead of there's one transaction, here's the moving average of price over three months, the same security as the checking of the transaction, the same security that underlies and integrates with all of these protocols today. But now you can do more with that same security.
00:18:42.282 - 00:18:54.804, Speaker A: Okay. All right, great. Tying it back to the cosmos ecosystem by proxy of celestia. When are your plans to support IBC?
00:18:54.932 - 00:19:18.144, Speaker B: Yeah, I would say that we will likely be supporting cosmos chains, by and large, by early next year. There is no limitation for us in our underlying constructions and supporting them, it's just an order of operations and a rollout sale. We think very highly of the cosmos ecosystem, and I'd say we're very bullish on it, and we have been for quite a while.
00:19:18.342 - 00:19:36.852, Speaker A: Okay, that's great. The problem with cosmos chains, though, is that not very many people, if at all, run archival notes. So then how would you solve for that if you don't have access to that data? Yeah, I mean, the beauty data that's not even available for over a month.
00:19:36.986 - 00:20:08.160, Speaker B: Yeah. The beauty of compute, improving compute is that you only need one node. So, in theory, to support a proof of some historical data or proof of computational data for a given chain, you need one archival node. And if there's a single archival node for a chain, then you can support it. And I think there's obviously a question of incentivization of running archival nodes long term. But if you can now run an archival node and take a fee, whenever people use this proof on chain as part of computation, you've now had a cash flow writing back to the person running the data.
00:20:08.230 - 00:20:08.672, Speaker A: Fair enough.
00:20:08.726 - 00:20:12.240, Speaker B: So you now have a better economic incentive for it if there's a consumption of the data.
00:20:12.390 - 00:20:14.912, Speaker A: Great. Well, thanks for coming on the show.
00:20:14.966 - 00:20:16.860, Speaker B: Thank you very much. Bye.
