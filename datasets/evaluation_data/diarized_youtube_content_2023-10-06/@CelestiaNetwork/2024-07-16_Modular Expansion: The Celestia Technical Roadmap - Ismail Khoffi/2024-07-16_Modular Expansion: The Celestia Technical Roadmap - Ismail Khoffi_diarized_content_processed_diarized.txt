00:00:03.280 - 00:01:04.678, Speaker A: I have the honor to present you the Celestia technical roadmap. And basically that's what the community has been working on in the past months or like almost years. I want to start and motivate the roadmap with three key problems that are still, I would say, the biggest challenges for crypto or like blockchains, at least on the base layer and in the infrastructure side. So first of all, blockchains still don't scale, if you think about it, like everyone is talking about mass adoption. If that was about to happen today, actually the current infrastructure wouldn't be ready and the fragmentation problem that was mentioned before would even intensify and become a bigger issue. Also, these are old slides. Okay.
00:01:04.678 - 00:01:57.716, Speaker A: What I wanted to say is blockchains are still not verifiable by anyone. This means that a lot of teams sacrifice some throughput for sacrifice, the core principles of blockchains for some throughput gains to quickly get some users. But what we actually want is that everyone can verify chain. It's the whole purpose of why you use blockchains in the first place is you want to minimize trust. Assumptions that has been discussed, I think, on ECC in length, bridging sucks. Liquidity fragmentation is intensified through modularity. Even all these roll ups need to bridge between each other, and bridging sucks.
00:01:57.716 - 00:02:58.714, Speaker A: It's turning into a bigger problem, both for end users as developers alike. So now I set the stage for what I think are the key problems or what the celestial community thinks are the key problems that they want to attack with their technical roadmap. Damn, it's a really old slide. Okay, first of all, first goal of the celestial community is abundant block space, and it should be verifiable by anyone. So the goal here is to have 1gb blocks here. The capacity that we aim for is actually like 1gb, which can host multiple visa scale networks, right? And multiple solana networks, many evms and so on and so forth. Currently, where we stand is at two to eight megabytes.
00:02:58.714 - 00:03:45.460, Speaker A: If you assume the same compression numbers that Vitalik assumes in his blog post about rollups with like 25 bytes per transaction, two to eight megabytes already get you to visa scale. It's like, I think roughly 7500 tps to 30,000 tps with eight megabytes. And with 1gb, you get roughly 3.9 million tps. So that's a goal post. Another goal is trust, minimization and 1 billion light nodes. We want actually everyone to be able to run a light node on their phone in their browser, in their wallet, and so on.
00:03:45.460 - 00:04:46.400, Speaker A: Our answer to the fragmentation problem that's been a hot topic is lazy bridging, or what we call lazy bridging. It's a technique that leverages ZK accounts to achieve the goal of frictionless block space and 1 million roll ups. So if you have 1 million rollups, think about the fragmentation that would exist, and we want to make it remove that obstacle for developers and users alike and have frictionless block space. So let's look a little bit more into what the community has been cooking. These are the three goals that I just mentioned. Abundant block space, 1gb blocks, trust minimization, or 1 billion light nodes, and frictionless block space, or 1 million roll ups. So let's start with the frictionless block space as the fragmentation problem.
00:04:46.400 - 00:06:08.096, Speaker A: And developer experience is like a key issue. And this consists of the following tracks. One that the community coined as modular money, which is about fragmentation issues and bridging, solving bridging and fungibility of assets between roll ups, and making the roll up experience as frictionless as possible. So as I said, lazy bridging is leveraging so called Zk or snark accounts on Celestia instead of enshrining and a general purpose execution environment to have like a canonical bridge, it allows you to plug in any interop protocol or any proof aggregation protocol, and any intense layer or intents solver where like, as like everyone wants to have like this, you have an intent on one roll up, which executes in another roll up, for instance, to buy an NFT. So roughly, the architecture looks like this, right? You have Celestia underneath with very minimal changes. We just add an account type, which is snark accounts or ZK accounts. Some call them smart accounts.
00:06:08.096 - 00:07:23.680, Speaker A: And then on top of that, there's a thin layer of interoperability solutions, bridging protocols, proof aggregation, interop, and it allows you to stream any asset in and out through Celestia just by connecting to Celestia. And, yeah, basically have that frictionless experience that everyone wants, where you can like, have an intent or anything on one roll up, and it executes on the other roll up. If you want to learn more, there's a very, like, big and very unique research community. There's a blog, not a blog post, research forum post by Yuma from succint that like is the first approach on a specification for celestial snark accounts. There's a working group that meets regularly, is very active, like, I think monthly, roughly. If you want to participate, this is the best place and there's also Nash Q from celestial Labs is giving a talk about ZK accounts during modular summit. So what else is in for in the track? Modular money.
00:07:23.680 - 00:08:30.194, Speaker A: The celestial community doesn't want to wait for lazy bridging and has proposed what is called the packet forwarding middleware, which already allows you to bridge or not to bridge, but like to have IBC transactions from one chain and use Celestia as a hop. And then there's also interchain accounts which allows you to basically manage your account from another chain. You can have another chain manage your assets on Celestia, which is important, for instance for liquid staking protocols. Lazy bridging I already mentioned, and fee burning has been under research since long before the Celestia mainnet actually launched. Then in the frictionless block space goal, there's another track called Blobstream. So far it has been quite successfully deployed on Ethereum L two s and l three s. Like an arbitron base.
00:08:30.194 - 00:09:11.420, Speaker A: Any big stack, any bigger community on Ethereum already used it. Since it has been introduced, people realized that you can actually use this to scale any ecosystem. So for blobstream, the goal, or the work has been focused on using it to scale any ecosystem. And there have been proposals to use them for Mina ZK apps for bitcoin L two s and L three s. Or there's a team doing research on Solana L two s as well. And I think there's a lot more coming there. There's a lot more cooking.
00:09:11.420 - 00:10:29.870, Speaker A: So then there's another track called roll up developer experience or roll up devex. Here. Two things I want to highlight is we reworking the API currently to have a canonical stable v one blob API which allows roll up developers to submit blobs, which will incorporate all the feedback and all the learnings we gained so far from deploying our first iteration of the Blob API. If you remember, when Celestia launched, we were the only network that had blobsen. And yeah, since then a lot of networks emerged and a lot of users that want to submit blobs, so all the learnings will accumulate in the next API version. Another thing that's pretty cool is separating sampling from finality, also called fast blocks, slow squares, which is about achieving super fast finality without sacrificing data availability sampling. So how does that work without any pre conformations or anything? The idea is literally just produce the blocks as fast as you can.
00:10:29.870 - 00:11:33.736, Speaker A: Let's say have a block time of 2 seconds. That's what you see of above and then the erasure coding of the block which enables data availability sampling happens, for instance every 10th block. All the blocks that were made before get erasure coded in the next step. So you get fast finality like instance in Celestia's case because it uses tendermint or commit BFT. And then you can have light nodes who sample without being slowed down by having to download all the headers in between. They just need to care about the square header and sample and get the guarantee that all the blocks that have been produced in the meantime are available. So then additionally to that there is, I don't think you can see it here, but there is namespace programmability which allows developers to set predicates or conditions for the inclusion of blobs in a namespace.
00:11:33.736 - 00:12:33.470, Speaker A: So it allows you to program the inclusion but like very minimalistic. And then there's a dynamic base fee in the works like proposed and it's pretty straightforward. It means that the base fee matches the usage of the chain. And then let's go to the trust minimization and the 1 billion light nodes track. Here there's basically two focus points. One is about light node security and light nodes on every device. So for light node security, the bar is already quite high because light nodes in Celestia already have similar security guarantees as full nodes, in the sense that they get guarantees that the data route, the erasure coded data route has been encoded correctly by leveraging something called bad encoding fraud proofs.
00:12:33.470 - 00:13:36.380, Speaker A: But that's not enough. We want to decrease the trust assumptions and increase the security for light nodes. One focus is block reconstruction, such that light nodes in Celestia, they already play a big role in their first class citizens, but we want them to be even more important in the sense that they can be leveraged for reconstructing block data by sampling from them if there are enough of them on the network, and to know if there are enough of them on the network. We are working together with the protocol labs team called probe labs on light node observability. So actually for the community to know how many light nodes there are in the network, that's also important if you want to increase the block size. And this will also enable no, this related to that. The reconstruction protocol has to be made more robust and more performant for larger blocks.
00:13:36.380 - 00:14:51.708, Speaker A: But there's more. I think that's actually even more exciting is that validity proofs are coming to light clients and partial nodes alike. So there will be validity proofs for the state. So Celestia state transition proofs that's currently under research, probably also leveraging ZK and there's a proposal to get rid of the data embed encoding fraud proofs to have data root validity proofs. With that you get full light nodes are really on par security wise and trust assumption wise as full nodes, both when it comes to the blob space as well as the state on, on the layer one. And to raise the bar even further, there's a technique called private sampling which think about like a Tor network or something like this, where the sampling requests are unlinkable to a particular light node. And that's important for what's called a selective shared disclosure attack where full nodes would only give out shares to particular light nodes and wouldn't to others.
00:14:51.708 - 00:15:59.170, Speaker A: So to fool the network in the sense that some think that the data is available and others actually don't get any guarantee and the data is actually befalled. Okay, and then light nodes in every device, that's the 1 billion light nodes, particularly there's a rust light client or rust light node already done, but that's obviously not enough. We want an in browser and wasm light node that's also already done. If you look around for QR codes on this modular summit there you can actually run your light node in your browser. Another thing is that light nodes currently you have to wait a bit for them to sync, and that is also in the works by the community to mitigate that. And obviously the end goal is to have in wallet light nodes that everyone who's interacting with the chain in any form through a wallet. It doesn't have to use any centralized RPC, but can fully rely on the light node running in the browser or running in the wallet.
00:15:59.170 - 00:16:55.978, Speaker A: And the last point I want to mention is the 1gb blocks or the abundant block space. So there's two layers here. One is the consensus layer, the other is the data availability layer. On the consensus layer we already have blobs, obviously you can submit blobs and the erasure coded data route. What is more interesting, as I said, for two to eight megabytes, we're already there, but we have a plan, a very concrete plan to get to roughly 100 megabyte, which also already raises the bar significantly. So all these features on there, I don't want to go into detail there, but they are all about the peer to peer layer, the mempool, and making everything ready for bigger blocks, so block propagation and so on and so forth. So this is like very engineering.
00:16:55.978 - 00:17:34.874, Speaker A: It's like not research heavy. It's very clear how to get there. To get to 1gb blocks, though, we need to do what is called no charting. And it's also under research. Think about like nodes being sharded into workers internally where they would only handle parts of the data. So that technique will get us to 1gb blocks. So on the data availability layer, we have data availability sampling.
00:17:34.874 - 00:18:21.088, Speaker A: I think till today it's the only network that does this. But here the goal is about reducing resource requirements and improving efficiency. Reducing resource requirements for pruning, pruning of blob data, pruning of, or like header trimming, removing header data. And there's a more efficient data availability sampling protocol, which is essentially a rewrite of the data availability network layer. It's called Schwap, which also will be ready for 1gb blocks to further reduce the resource requirements for partial nodes. For nodes, like storage nodes, we introduce partial nodes. There's namespace.
00:18:21.088 - 00:18:58.656, Speaker A: Partial nodes which allow you to download only a namespace that you care about or that is relevant for your application or for your roll up. You can think of them as full nodes for a particular roll up, and then as uniform partial nodes, which will allow you to uniformly shard the chain, or like uniformly store data off the chain according to your research needs. That's it. Thank you. From my side. And if you want to check out the CIPs that the community has been working on for this. On the right.
00:18:58.656 - 00:19:07.400, Speaker A: Oh, okay. Yeah, on the right, that's the research forum. And on the bottom here, that's the working groups. Thank you very much.
