00:00:01.530 - 00:00:01.962, Speaker A: Seconds.
00:00:02.026 - 00:00:28.470, Speaker B: Yeah, I think it's live. Hey, everyone, please find a seat if you don't want to sit on the ground floor. There's overflow seating up above. Also, Fred, if you're here, Off Chain Labs, please come down, put on a microphone.
00:00:32.730 - 00:00:33.640, Speaker A: Give it.
00:00:41.190 - 00:01:43.964, Speaker B: You got to put on your microphone. Welcome, everyone. The title of this event is Roll Ups and Data Availability. Building a modular blockchain stack. And it's a joint event sponsored by first, I want to feature Maven Eleven, who was a seed investor in Celestia and has kindly really helped with the organization of this event. I think you'll see Balder, there's a bunch of them here. Matthias? Miguel, I think I can't remember.
00:01:43.964 - 00:02:22.360, Speaker B: Anyway, there's also Celestia, which you'll hear about from Mustafa Fuel and Off Chain Labs and Starquare. And the format of the event is that each speaker will present for ten minutes about their individual project. And then after that, we'll have a 45 minutes panel led by James Prestwich. And we'll have 30 minutes of the panel, followed by 15 minutes of audience questions. So stick around for the end and I think there'll be a really interesting conversation. So first I want to introduce Mustafa Albasam from Celestia.
00:02:28.460 - 00:03:25.100, Speaker C: Hello, everyone. My name is Mustafa. I'm the co founder and CEO of Celestia Labs. And I'm going to give you a brief introduction on Celestia and what we're building. So what is Celestia? Celestia is what we're describing as a pluggable consensus and data availability layer. And the key goal is to allow anyone to deploy their own blockchain very easily without the overhead of having to deploy their own consensus network using something like proof of stake, because they can plug in Celestia as their consensus layer. Now, our kind of key thesis here is, and what this event is about, is building a more modular blockchain.
00:03:25.100 - 00:04:32.988, Speaker C: If you look at traditional blockchains, like Ethereum, for example, okay, so if you look at traditional blockchains or existing blockchains, for example, like Ethereum have used the same kind of paradigm since 2008 that Bitcoin has introduced, which is that a blockchain provides everything. So you have an Ethereum, you have the consensus layer, and the consensus layer provides everything. It provides the execution environment, which is the Ethereum virtual machine. It provides data availability and so on and so forth. And you have the smart contracts which build on top of Ethereum. Now, the idea of a more modular blockchain stack is to decouple the consensus and execution layers. So the idea is that a blockchain or a layer on blockchain can only be responsible for the consensus, which is basically arriving on consensus on the ordering of transactions.
00:04:32.988 - 00:05:36.564, Speaker C: But the execution can happen on a different layer, above the consensus layer. And that's also a key intuition of roll ups, for example. And above that execution layer, you have the actual applications which can be smart contracts. And so the key idea of Building a more modular blockchain stack is to decouple consensus and execution. And we can think about why do this and why does this make sense? We can draw a lot of parallels with how the web developed. So if you put the early 90s when the web was still forming, if you wanted to create a website, you would typically host that website on, let's say, your university server. Or you might have a physical or you host that website on your computer, or you might have a physical server in your workspace or your home or your company's office.
00:05:36.564 - 00:06:21.616, Speaker C: And so there was a higher barrier of entry to creating a website. You had to have a physical machine. But then in the 2000s, this idea of shared web hosting came about. Geoc DreamHost bluehost. You could upload your HTML code or PHP code using FTP or SSH and your website runs on the same server as everyone else's. But obviously that was quite limiting because you're stuck with whatever programming languages that service provider provided. But then the modern web came about where virtual machines and the cloud became popular.
00:06:21.616 - 00:07:24.120, Speaker C: And that really gave you the best of both worlds because for the first time it gave you the advantages of having your own physical server without having the overhead of having to actually have a physical server. Because you have virtual machines and you can deploy those virtual machines within seconds and have functionality equivalent to having a physical server right next to you without all the overheads of having to maintain the physical server. And you can draw a similar parallel to the blockchain space and web. Three. We can see in 2008 we had Bitcoin, which was a single purpose blockchain for cryptocurrency application. Then it had also other blockchains like namecoin litecoin and so on and so forth. And then Ethereum came about in 2014 and decided to provide a shared blockchain with a shared smart contract environment so that all of these different applications could use the same chain.
00:07:24.120 - 00:08:34.880, Speaker C: And this was great because it allowed a new flurry of new decentralized application developers to very easily deploy blockchain based decentralized applications. But it was also very limited in terms of flexibility and scalability. Obviously does not make sense for every single smart contract to be running on the same world computer as every single other smart contract, both for scalability and flexibility reasons. And then if you look at what's happening now in the crypto ecosystem, we're returning back this idea where applications should have their own blockchains. So for example, you have Cosmos Polkadot and roll ups because roll ups are effectively blockchains that deposit and withdraw assets to a parent chain like Ethereum. And that kind of similar to virtual machines. That gives you the best of both worlds because you can effectively have your own blockchain but without having to maintain the consensus layer of that blockchain.
00:08:34.880 - 00:09:46.950, Speaker C: Like for example, if you deploy a roll up on Ethereum, you're using Ethereum for security and what Celestial provides is what we're building is the world's first general purpose consensus and data Availability layer to understand how that works. If you look at traditional blockchains, in the traditional blockchain, if you want to check that a block is valid, then you have to do two things. You, first of all have to check that the block has consensus using the Proof of Stake or Proof of Work algorithm. And secondly, you have to actually process every single transaction in the block to make sure that the block is valid because if there's an invalid transaction in that block, then the block has to be rejected. But what we're saying at Celestia is we're building a blockchain where anything goes, so you can post any message. And there's no such thing as an invalid transaction. So it's just a blockchain where you can post arbitrary messages onto it.
00:09:46.950 - 00:10:52.590, Speaker C: That means that if you want to check that a block is valid, then you only have to do two things. You have to check that the block has consensus by using the Proof of stake algorithm, for example. And then you have to check that all of the data in that block is actually available and has been published to the network. But you don't actually have to care about what the data actually is or verify the data. And that makes it much more scalable because to check that data is available, you can use a primitive called Data Availability Proofs. And the key idea of Data Availability Proofs is that you can check that the entire block is available by only downloading some very small random pieces of that block. Not going to go too much in detail here, but to provide some brief properties of this.
00:10:52.590 - 00:11:58.560, Speaker C: With Data Availability proofs, the number of pieces in the block you have to download to check that a block is valid is constant. So if the block size increases, that does not increase the cost to validate that block. But the way it works is that there has to be enough clients in the network such that they're downloading enough pieces from each block such that they can collectively reconstruct the entire block. And so that has two consequences. First of all, to validate a block or to check that a block has consensus. You can do that very quickly in sublim your time because you don't have to process every transaction. And secondly, the network is scalable because the more users you add, the bigger the block size you can have securely.
00:11:58.560 - 00:13:19.130, Speaker C: Now, the reason why this is interesting is because if you think about what's the world's most decentralized scalable protocol, it's effectively BitTorrent. Because at one point BitTorrent handled a quarter of the Internet's traffic. And the reason why it was so scalable is because obviously there's no execution in BitTorrent. It's just a protocol for sharing data and making data available. So our key intuition is that if you simplify block verification to checking data availability, then you can effectively achieve similar scalability properties to peer to peer file sharing systems like BitTorrent or IPFS. And so to build applications on top of Celestia, you effectively implement them as a roll up, because the whole idea of a roll up is that it's effectively its own blockchain that uses a data availability layer to post its blocks into. And that's what the next three talks are going to be about.
00:13:19.130 - 00:13:40.930, Speaker C: How much time do you have? So if you're interested in learning more, then go to our website, celestia.org. And we're also hiring so you can check out the Our Angel page if you're interested in joining and we're looking for more, go to lang engineers. Thank you, everyone.
00:13:42.900 - 00:13:50.100, Speaker B: Nice, Mustafa. Our next speaker is going to be Brandon Kite from Fuel.
00:13:59.160 - 00:14:00.150, Speaker C: Hey, everyone.
00:14:01.800 - 00:14:03.110, Speaker B: This is the right.
00:14:03.820 - 00:14:52.730, Speaker D: Here we go. Hey everyone. I'm Brandon Kite and I'm here with Fueled Apps to talk about Fuel in the modular blockchain world. So I got my start in blockchain working at a project on Disney in Seattle, and I've consulted on several blockchain projects over the years. Recently, I was leading a protocol dev team at Transparent Systems, which is a Paul Allen based company, and now I'm leading the client and infrastructure development at Fuel Labs. So Fuel Labs and Fuel have been around for some time and actually shipped the first roll up to mainnet Ethereum last year. And one big kind of difference or differentiating factor of Fuel is that it uses a UTXO based model.
00:14:52.730 - 00:15:55.704, Speaker D: So Fuel can be an execution layer to data availability layers like Celestia. We're also targeting Ethereum, of course, but it could also be used in the execution layer on top of virtually any other blockchain that provides consensus. So that could be polkadot or anything that has an. So, you know, Mustaf already kind of covered all the main points about what makes a good data availability layer, but essentially something that provides or has minimal execution overhead and has very high capacity storage because the more data we compose to it, the more roll ups we can run, more transactions, the better. So essentially, it just needs to provide consensus over the data for the roll ups to function. So what makes a good execution layer? So I kind of break this down into two categories. One is kind of the lift and shift approach where you just want to speed up your existing layer one apps.
00:15:55.704 - 00:16:57.916, Speaker D: So it's taking the EVM, putting it in a roll up porting over your solidity apps and just running them in a roll up. And then the other approach is kind of where we're going is more L2 native approach where you have developers building their applications from the ground up for L2 to be optimized to take advantage of the best performance features you can. And so one thing that we're doing with the UTXO model is essentially maximizing parallel stability. So that essentially allows us to kind of maximize our throughput compared to kind of more sequential solutions like EVM. So as an execution layer, you get basically full ethereum style smart contracts, but we get a bunch of extra features that we can support because we're not kind of tied to being compatible with the EVM. And yeah, it allows us to basically run a lot more faster and efficiently. So the Fuel VM has several built in extra functions.
00:16:57.916 - 00:18:05.600, Speaker D: So things like extra math operations that can be efficiently verified on layer one we support out of the box and built into the language. Some of these things like Mem copy, really expensive to implement in solidity, but super efficient to verify from a fraud proof perspective. So that's something that we can do very fast and efficiently in the field VM and that's a benefit to writing our own VM versus just being compatible with EVM. And then there's other kind of advanced features we get where we can let people fully tap into the UTXO capabilities such as like multi user transactions. So building on Fuel is similar to working in Rust. So we have our own language called Sway. And the reason why we have our own language is because if you're trying to write Rust on a blockchain, you have the whole overhead and barrier of using WebAssembly usually and dealing with no stood issues there and all of your dependencies making sure they're no stood.
00:18:05.600 - 00:19:13.224, Speaker D: And then you probably have a lot of macro garbage that has to be put on top of that to deal with all the blockchain specific primitives. So rather than putting people into that macro world, we decided to just build something that's basically identical to Rust. Cuts out the things that aren't needed for writing smart contracts and so simplifies some of the more complexities of Rust, but allows you to write basically smart contract native code with all of your Abi generation and tooling kind of built in. So yeah, we don't have issues with more than 16 local variables and all that kind of stuff because it's tuned for our VM and for the contract sizes. Basically you can have them up to several megabytes and not have an issue there. Another big thing is you can have in memory hash, maps, so essentially you can have this extensive standard library of functionality that people can just use like they're writing in a normal language basically. And then to go along with that, we have tooling.
00:19:13.224 - 00:20:12.600, Speaker D: So essentially, rather than trying to be similar to reusing pieces from the solidity ecosystem with like hard hat and all this and then trying to use it for a specific L2, everything comes out of the box ready to go for writing your apps on our VM. So you're going to have code formatting, all the IDE support, autocomplete, that kind of stuff. Debugging if you want to fork the chain and debug. At a certain point, that's all baked in. That's essentially trying to focus on building a good developer experience where it's all cohesively integrated together instead of kind of piecing together a bunch of different tools out there. So that's pretty much it. If you want to follow us, we're on Twitter and all the major we have a GitHub where we're launching our network pretty soon, so we'll be open sourcing that.
00:20:12.600 - 00:20:19.570, Speaker D: And if you're interested in learning more about our project or working with us, we're definitely hiring. So. Yeah.
00:20:24.980 - 00:20:35.620, Speaker B: Thanks, Brandon. Yeah, here. So next up we have Fred Lax from Off Chain Labs who's going to talk to us about Arbitrum.
00:20:38.200 - 00:21:23.888, Speaker A: Thank you. I'm Fred, nice to meet you all. And I'll be talking about Arbitrum and L two scaling with Arbitrum and how it fits into this modular blockchain idea. So I'm Fred, I'm a software engineer at Offchain Labs. And a big question is what is Arbitrum? Right, we have Arbitrum One, which is our flagship chain right now, which has been live in Mainnet since, I believe, May this year for developers and open to end users for a bit over a month, almost two. And since then we have processed over 2 million transactions with over 200 different accounts interacting with the chain. And a bunch of contracts deployed a couple that I heard are just for all different devs, deployed a lot of contracts.
00:21:23.888 - 00:22:14.820, Speaker A: Sorry, I went on a weird tangent because I was hearing earlier that maker Dow went on a weird rescue recently and deployed a lot of contracts on us. So until maybe last week, this number was around eleven k and then it jumped to around 15K. But that's a story for the drinks after weird tangent. And we spent 67 bit in ARB gas, which is how we account for computation on our L2. And yeah, I mentioned that we are an optimistic roll up that attempts to scale Ethereum. And why do we say Ethereum? Right? We're talking about this modular blockchain idea. And as Mustafa was saying earlier, there's this idea of having a consensus chain, which is the base chain where you make the data available and where you're able to send your assets, make either a deposit or a withdrawal, and where you actually enforce the correctness of this L2 protocol, too.
00:22:14.820 - 00:22:56.460, Speaker A: And we use ethereum. And why do we use Ethereum? Well, there's security. There's a wide pool of miners and validators block producers that enforce the chain security as generalized computation, which actually allows us to write our proving system, which means it allows us to prove what happens in Arbitrum is actually correct so that you can enforce it. And it has adoption. The sad face shouldn't be an adoption, it should be a bit lower. My bad for that. But by adoption it means there's a strong community, there's a lot of users, there's a lot of knowledge that was built up about how Ethereum works and how you can have a safe experience with Ethereum being from auditors helping developers through best practices and just what patterns work and what patterns don't.
00:22:56.460 - 00:23:29.272, Speaker A: And this was something that was developed over a long time. But there's a bad side, right, which is congestion. And that's why we're here. We're trying to alleviate the congestion and try to make this blockchain idea more scalable. And, yeah, what's up with Arbitrum? Right? We try to inherit the security from Ethereum, but we try to keep also this generalized computation. So that means if you're able to express a contract on Ethereum, you're able to express it also in Arbitrum. And we wanted to go a step beyond not just general computation, but actually keeping this EVM compatibility.
00:23:29.272 - 00:24:29.948, Speaker A: And what that means is if your contracts work on Ethereum, on the layer one, it also works on arbitrary. And that means that not only we inherits the security, but we inherit this body of knowledge of how to write safe patterns and what works and what doesn't. And that's something that we're committed to, to keep this compatibility and to offer scalability on top of that. So why L2? Right? We want this idea of inheriting the security from where you're based on and still being able to branch into different states. So the idea is that you have this main Ethereum chain and then you have a roll up or a different execution environment that kind of like branches off from it trying to be like we're kind of still the same, but we branched off and we can come back and have different environments. And this isn't precisely correct on a technical term, but it's an interesting intuition around how it works. And, yeah, we keep on talking about we have this L2 Arbitrum virtual machine that can be proved.
00:24:29.948 - 00:25:13.324, Speaker A: But what's up with that? When we say we have a virtual machine, it's pretty much virtual machine in the traditional sense that takes inputs and gives you outputs. But the special thing, and one of the important things here is that this virtual machine is fully deterministic. So it means that if you always put the same inputs in, you always get the same output. And, yeah, that's a key characteristic of our L2 virtual machine. But we want to get this L2 virtual machine and have it inherit the security from the layer one. So we can think about the layer one virtual machine, the Ethereum virtual machine, as being side by side with our virtual machine. And if we bring back that idea of the inputs, if we lay it out a bit differently, we can actually try to enforce the security to actually hold.
00:25:13.324 - 00:26:02.524, Speaker A: And how we do that is instead of you sending all your transactions, all your inputs straight to Arbitrum, what you do it is you send it to the consensus layer, you send it to Ethereum, the data, all the inputs get posted into Ethereum. And from there, anyone is able to read that and actually execute on their own L2 arbitrary virtual machine and everyone will have the same output. It's a fully deterministic virtual machine. You know the set of inputs, you execute it and you get an output. And the idea is that anyone can get this output but you still need to prove it back to the layer one, right? You have the data available. Anyone can know what the result is. The challenge is how do we convince Ethereum itself that this is the result? And this is where our protocol comes in in which the optimistic roll up parts proves this.
00:26:02.524 - 00:26:46.968, Speaker A: And I'll get into that a bit later. But yeah, I glossed over a very important thing here, which is the sequencer, which I won't get into very much, but I need to at least mention it. That is in this layout there is one level indirection with the product we have shipped which is the sequencer which has one superpower that it orders transactions for the users. So it lets you as a user have a much faster UX and experience when interacting with the chain. So what it does is it gives you soft confirmations in terms of the ordering and then you get a harder confirmation when it actually goes to the L One chain. So what I mean by that is before we had the inputs going straight to the L One, to Ethereum or whichever consensus layer. And now we actually have this indirection of the sequencer.
00:26:46.968 - 00:27:34.296, Speaker A: And the cool thing is that you can have this fast soft confirmations but they can never actually steal your funds. They can just front run you and extract value, which can be very bad as well, but it's limited in that sense. I was mentioning before, right, roll ups, we're a roll up. And a key thing that we need is this data availability, which is when you're talking about a roll up, we're getting many transactions together, all those inputs, we roll them into one single bank transaction and we include it to the layer one Ethereum. And then the optimistic part of it is how we prove it back, which, as you hear later, there are other ways that people do it. For example, Starkware will be talking about their zero knowledge proofs and how they actually also leverage that. I have quite a few quite a bit of content on the slide.
00:27:34.296 - 00:28:19.396, Speaker A: So I'll power through because I think I'm getting close to the time limits. Optimistic roll ups is the idea of how we prove it back and we take an optimistic approach, which means whenever you have an output of executing on this L2 virtual machine, we let anyone assert and be like, hey, this is the output of execution. This is what I believe is the result. And you do that, but you also put a stake along with it being like, I'm willing to put my money and if you prove that I'm wrong, you can take my money or part of it. And we optimistically accept these assertions. Like people saying this is the output, which they are saying. And we have this notion of fraud proofs, which means since all the data comes as the inputs, anyone can execute and see the difference on the outputs.
00:28:19.396 - 00:28:59.200, Speaker A: And then you play a Bisection game on the layer one ethereum being like, we disagree on these outputs. Let's see where we actually disagree on this. And the way we do it in a nutshell, is kind of like a binary search in which you have all the opcodes that were executed on the L2 virtual machine. And we ask, was it on the first half or the second half? And then we keep going on until we narrow down to a single opcode of execution. And then we actually prove that on the chain itself by executing that. And since all the data is actually already there on the chain, anyone can do this. Right now, we don't have this fully permissionless set yet since we're in beta, but this is part of the design of the system and how the protocol can be enforced.
00:28:59.200 - 00:29:27.548, Speaker A: But, yeah, what's actually there and what's deployed for arbitram. So we have this proving system that allows you to prove the L2 virtual machine on the L One. We have the roll up part of things, which means when you're rolling up transactions and having the game between different outputs being disputed by people, that's the logic that's there. And we have the L2 VM. And, yeah, here's a disclaimer. We're in beta. These systems are live, are functioning, but they're upgradable.
00:29:27.548 - 00:30:10.410, Speaker A: And for a while, we're going to keep these upgrade keys, but we have a route towards permissionless system and decentralization, which you can read on our blog posts. Yeah, I said what's there? And the interesting question is what's next? Right. We've recently announced Arbitrum Nitro, which is just changing a certain parts of the system. As I said before, a slide before we have the prover, the roll up and the VM. What we're doing here is we're changing the virtual machine that's actually running on the L2. You're going to have like a WASM virtual machine and a proving system that is able to prove WASM execution. And what happens in that WASM execution is fully verifiable on the L One chain itself.
00:30:10.410 - 00:31:10.576, Speaker A: And a big question that we get is, well, if you're going to have this big upgrades, should we wait until we deploy? So once you do that upgrade, and the answer is no, you can deploy right now, it's going to be a seamless upgrade that will go live on the network and users won't have to migrate. We will maintain the EVM compatibility, and your contracts will continue working fine. Yeah, two other big things that are on our roadmap are Ethereum Two, because as Mustafa was saying earlier, these data availability shards and just having the capacity to handle more data on whatever your consensus layer is always benefits roll ups, making the cost much cheaper. And that's a big thing on our roadmap. And the other one is Arbitrum channels, which is what happens when you get this idea of data availability and you make certain specific relaxations, which is, for example, instead of posting all the data on the layer one. What if you trust someone who actually give you this data and it's a trusted party, and you trust that reduces the cost of the system a lot and it works very well for certain use cases. And that's a very interesting flavor of roll up.
00:31:10.576 - 00:31:35.924, Speaker A: Well, wouldn't technically call it a roll up, but that's a very interesting flavor of tech to play around with. But yeah, if you want to learn more now, I regret not putting the actual links instead of hyperlinks, but you can check out our documentation discord or Twitter or just talk to me later on in the event. We're also hiring. That's our hiring page. This one actually got right if you want to type it in. And yeah, thank you for coming. That's my Twitter.
00:31:35.924 - 00:31:39.960, Speaker A: And I guess we don't really have times for questions, but later on after the panel.
00:31:45.280 - 00:31:59.680, Speaker B: Thank you, Fred. Super interesting. And our last speaker is Avihu from Starquare. Here he is. And then after this, we're going to have the panel.
00:32:08.650 - 00:32:45.890, Speaker E: Hi everyone. Can you hear me? Well, okay, so hi, I'm Aviyu. I don't have slides except this one single slide with my well skinnier version of me. But I do want to try and speak a couple of minutes about StarkNet. Just before I get to StarkNet, I do want to mention what we did in Starkware so far. So we started in Starkware by developing scalability engine that we call Starkx. Starkx is basically well, it's based on validity proofs.
00:32:45.890 - 00:33:35.410, Speaker E: It's based on generating and running Stark proofs. And it works for a specific operator and a specific application. And fast forward, we deployed the first version on Mainet, I think more than a year and a half ago. And fast forward for today, there are a bunch of applications running on it with almost $1 billion locked in the contracts and something between few hundred thousand to a million transactions per day. So it works pretty good. It works in a very high TPS, and it support application like dYdX, immutable diversify and soar but at some point, we realized that we want to get to something that is bigger. And by bigger, I mean it's more general.
00:33:35.410 - 00:34:43.394, Speaker E: It support all the nice features that you're used to from ethereum like composability and the ability to split logic to contracts, to have the same structure in contract like functions. And this is where we created StarkNet. So StarkNet is basically well, as part of the as net in the name says it's a network. And the idea in StarkNet is that. We will get to the same or high scale of ziki rollups, but with the ability to write general computation in a way that is very convenient and very easy and still get the scale that we all wish to achieve as a ziki roll up that is layered to on Ethereum. I think the easiest way for me to explain in what way you get scale in StarkNet is the following. So I think of StarkNet as an extension in two different dimensions.
00:34:43.394 - 00:36:07.854, Speaker E: Those dimensions are computation and what I call witness or basically the data that you need for a computation. So why do I want to point on these two specific things? Well, because StarkNet zkolap has a very important advantage there in every other solution that is not based on validity proofs, every node in the network eventually have to reexecute the computation and because it has to reexecute the computation, it has to know the data that is needed for the computation. So every time that you have a heavy computation or that you rely on a lot of data to execute it, in any other solution, it's going to cost a lot. But in StarkNet there is just a single entity at the end of the day is the sequencer that responsible on the current block that takes all the computation and generates a single proof for all the other nodes in the network. So now those nodes only need to verify the computation, they don't need to re execute everything. And I think this opens the door for a variety of new applications that we haven't seen today on Ethereum. And I can dig like way more than ten minutes on examples.
00:36:07.854 - 00:37:02.994, Speaker E: But I do want to point on just a single example. I actually chat right before the talks. I chat with one guy in the audience here and he told me that he's implementing a primitive that is called storage proofs on StarkNet. So I just want to give storage proof as an example on where is the advantage in scale of StarkNet lays. So think about the following. Let's talk about voting for a second. In voting you basically want to know in a specific point in time what is the balance of every user and you want to make sure that every user that wanted to vote, he voted and he signed on his vote, right? So if you have like thousand people that wish to vote on Ethereum, it's going to be almost impossible to verify it.
00:37:02.994 - 00:38:43.790, Speaker E: If it's going to happen each and every single time. What you basically would like to do is you would like to take the state of ethereum, prove it and prove that you basically have I don't know, for every person who votes it has some specific balance and still be able to run the entire computation of the votes and verify that everyone has this balance. So for storage proof, for example, one way to do it is to take a lot of data in the form of merkel passes and verify them on Ethereum or verify them on StarkNet. If you verify them on Ethereum or on any other solution, it's going to cost a lot of data and it's going to cost a lot of computation to do it. If you verify it on StarkNet, you don't need to relay this data to any other node and you don't need to recompute everything in every node. So basically every application that is computational heavy or requires a lot of data is something that is very interesting to try and create in Stacknet, because it wasn't possible up until Stacknet and on any other scalability solution that isn't a ZK rollup. We have a very active, I think, community of developers, and if you are interested in coming and building any type of application, a new application, something that wasn't possible to build today on any other solution, then you are more than welcome to join our discord and try and build your application on Starklist.
00:38:43.790 - 00:38:44.740, Speaker E: Thank you.
00:38:50.970 - 00:39:54.518, Speaker B: Thank you, Avihu. And now we're going to move to the panel portion of the event. And so I want to introduce James Prestwich, who is a cross chain engineer who is going to be leading the panel. And just give us a few minutes as we set up the stools. So hang in there. And after this we're going to have more food and drinks upstairs. Let's try with the lapel mics and see if did you get one? Okay, well, here, you just use this.
00:39:54.684 - 00:39:57.800, Speaker E: Easy if we need to.
00:40:02.730 - 00:40:04.780, Speaker F: Seem to be missing someone.
00:40:08.590 - 00:40:09.340, Speaker B: Okay?
00:40:13.230 - 00:40:15.100, Speaker E: They told me that I should be here.
00:40:19.790 - 00:40:25.440, Speaker C: See, we good. Everyone's good. It's good to see you all good.
00:40:26.850 - 00:41:14.480, Speaker F: So quick, funny story is I ran into Fred on the street yesterday and did not recognize him. And he just told me right now that that was him. And now I have to be super embarrassed about it for the rest of the panel. Thanks a lot of you all for coming out. We have about 45 minutes for the panel, 30 of which will be prepared questions, conversations, panelists, and then we'll open up for audience questions for the last 15 minutes if I stumble over my words, because I'm still extremely jet lagged. I just got in two days ago and I believe everyone here has already given an introduction of themselves as part of the presentations just now. So we can go ahead and skip that part of the panel and dive right into questions.
00:41:14.480 - 00:41:33.380, Speaker F: So we've all been talking about our tech and how it works in the presentation, but the name of the event is Building a Modular Blockchain Stack. So I want to give each panelist a minute to talk about what does it mean to build a modular blockchain stack? Yes.
00:41:33.830 - 00:41:34.802, Speaker A: Speak up a little bit.
00:41:34.856 - 00:41:44.920, Speaker F: I can do this. All right. I want to give each panelist a minute to talk about what it means to build a modular blockchain stack. Mustaf, do you want to?
00:41:50.590 - 00:42:54.430, Speaker C: Well, what it means to build a blockchain stack is to take all of the components that make a blockchain, separate them out into different components and make those components connected to each other and be in a way such that you can replace each component. And the main advantage of that is, intuitively, if you make each component replaceable, then there's a lot more flexibility in what you can make those components are. And so that unlocks a lot of new potential innovation. Like, let's say if you make the execution part of the stack replaceable, you get a lot of new innovation in what the execution stack is. Like we've seen with roll ups, for example, but also outside of the Ethereum ecosystem, you can have different execution environments that don't have to be EVM compatible like you have WASM or Ros based programming languages and so on and so forth.
00:42:57.890 - 00:44:03.350, Speaker A: Yeah, I think Mustafa expressed it pretty well. And another thing is, for example, if you try to solve all the problems you need to solve for execution and for consensus on the same part, you end up being much more limited in terms of what you want to do because of the trade offs that come along with that. So when you actually separate them and you're clear about the boundaries of where each one goes and what are the requirements and the limitations, you can actually make more explicit trade offs that end up like, I guess you would know best. But the trade offs you can make are much better because, for example, if you want the execution to be something that is highly efficient and paralyzable in that sense, it's something you can do. And also just the flexibility of having many different flavors that go on top of this shared security layer is very desirable. And it allows us as just builders to experiment with different ideas without having to actually make the straight off of having to make, for example, a different chain with independent consensus and independent security that actually allows us to these experimentations in a higher security environment.
00:44:05.530 - 00:44:46.130, Speaker D: Yeah, sorry, that was a little loud. Yeah, just building on what you guys said, I think the separation of concerns between data availability and execution just allows the execution layer to focus on what's most important to processing things as fast as possible. You don't need a team full of blockchain experts who know a lot about consensus to just build a virtual machine implementation that is very optimized for running on native hardware or taking advantage of what computers can do. So I think that abstraction is definitely a powerful one that's going to enable a lot of new use cases.
00:44:47.750 - 00:45:38.660, Speaker E: Thanks. I think for us, the most important part to build in a modular way is actually the data availability part. When it comes to execution, we don't have the exact same issue with all nodes needs to run execution because in any case it's more important for us to optimize just a single machine and everyone else can just verify. But on data availability side, it's very important for us to offer not just a single layer of data availability, but probably more than a single solution with possibly various trade offs, because we understand that it's important for some application to choose their data availability, not necessarily be with the same security or the same cost as other applications. So I think that's like modular part that is important.
00:45:39.110 - 00:45:54.810, Speaker F: So it sounds like we've identified data availability and execution as the main parts that people are making modular right now. Are there any other parts of the blockchain stack that should be modular, that should be swapped and replaced?
00:45:57.790 - 00:46:09.920, Speaker E: Yeah, so I guess I already spoke about it. For us, the main focus is the data availability layer, I think. Yeah, I can talk about which part. I think there shouldn't be a modeler but maybe I'll keep it for later.
00:46:12.610 - 00:47:01.760, Speaker D: Yeah, I guess from the execution layer side, I wouldn't say that there's a ton of modularity because it's already very modular on top of the data availability layer. You can just, like you said, try out a new thing experiment. But on the data availability side, I think there is value in keeping modularity there in terms of maybe you want to upgrade your consensus protocol or change different features about how the network functions and works over time or just have different flavors of data availability networks that have different kinds of properties, that have different trade offs. Like for example, what kind of finality do you want and how does that impact the roll ups when the clients are trying to decide what happens when a fork occurs and dealing with all those problems?
00:47:05.250 - 00:48:01.780, Speaker A: Well, there's one point that I kind of hinted at during my presentation which is, for example, the change we're doing for Nitro right now for Arbitrum Nitro, which is I wouldn't say necessarily modular blockchain, but modular components that make our L2 blockchain. So for example, we have this idea of a virtual machine, how to prove this virtual machine and the role of protocol that helps anyone challenge and do, for example, the Bisection and to get to an actual execution proof or the fraud proofs in parts. And having this part of the protocol and actually the optimistic role of be modular offers a lot of flexibility because you're able to have, for example, there's a lot of codes that we are not going to change. What we actually need to change is the VM running on the L two and how you prove the opcodes on the L1. Everything in between this is still the same and it's very useful to be able to, for example, plug different VMs and different flavors and everything that comes from that.
00:48:03.830 - 00:49:07.910, Speaker C: So one thing that is modularized in practice that we haven't discussed so far is this idea of dispute resolution for the roll up. So if you look at Ethereum roll ups right now, all Ethereum roll ups don't only use Ethereum for consensus and data availability, but all Ethereum roll ups also enforce a mandatory cross chain bridge with Ethereum. But our view, or the celestial view of roll ups is kind of more flexible than that. You can even have a roll up that is a standalone chain that does not necessarily need to be bridged to another chain, but can still use a data availability layer like Celestia or Ethereum. And this is particularly relevant in the Cosmos ecosystem. For example, the idea of Cosmos is that there's a Cosmos hub, but there isn't a single parent chain like the beacon chain like Ethereum. But you can have like an Internet of blockchains where the cross chain bridges are kind of like a network.
00:49:07.910 - 00:49:31.200, Speaker C: And so one project we're working on is we're trying to make it possible to deploy Cosmos zones as roll ups. So you can deploy a cosmo zone as easily as you can deploy a smart contract or a roll up on Ethereum, and your cosmo zone can connect to any other cosmo zone using a roll up based bridge that you choose to that supports that.
00:49:33.970 - 00:50:14.198, Speaker A: Yeah, I think this point of bridges is very interesting. And something that is different on rollups, especially the Ethereum flavored roll ups, is that the way the bridge works is different from the traditional sense of when a user hears like, I'm bridging my assets. And that's because it's actually a protocol enforced bridge. That means when you're sending your assets from L One to an L two, it's not some crypto economic incentive to stand alone to the bridge. It's actually if the protocol itself holds, the messaging between this bridge actually holds as well. And actually having this characteristic in a non roll up world is very hard to maintain. You have different ways you can set up protocols.
00:50:14.198 - 00:50:20.160, Speaker A: But, for example, I guess optics is a very interesting example to this to put you on the spot. I'm sorry.
00:50:22.690 - 00:50:24.960, Speaker F: I'm trying to avoid talking about myself.
00:50:25.730 - 00:50:43.586, Speaker A: Sorry. Then I'll shift it back here and yeah, I guess Cosmos does have IBC for communication and all that, but at least in the Ethereum style flavor of roll ups, I don't think IBC actually translates what we actually want for bridges. And yeah, I'm curious to hear your take.
00:50:43.688 - 00:50:43.954, Speaker E: Yeah.
00:50:43.992 - 00:51:40.434, Speaker C: To clarify, when I mean roll up bridges, I mean roll up bridges, I don't mean trusted IBC bridges like, we're working on adding IBC sorry, working on adding on fraud and validity proofs to IBC. So you would be able to use IBC to connect a roll up to another chain, which could be a roll up using roll up based trust assumptions. So the roll up does not have to be connected to some parent chain like Ethereum. A roll up could be even connected to another roll up. And that roll up can be connected to another roll up without these honest majority trust assumptions. I talk about this a little bit more in a blog post I posted recently about something. The idea of clusters, where you can have clusters of chain and you can communicate between the chains within a cluster in a trust, minimized way, like Ethereum and all of its roll ups are one cluster.
00:51:40.434 - 00:51:41.080, Speaker C: For.
00:51:46.250 - 00:52:08.080, Speaker F: Think. You know, talking about roll ups and IBC and Cosmos zones and smart contracts brings up an interesting question, which is, in these modular systems, who's writing the module and what does one of these modules look like? Is it a Cosmos zone? Is it a roll up? Is it a smart contract? Who is writing these and what do they do?
00:52:10.230 - 00:53:01.634, Speaker C: I can go unless anyone wants. So I think that question goes to what kind of APIs do these modular components have? Because it's all well and good saying that the stack is modular, but for it to be truly modular, you need to be able to replace the components in those stacks. And for that to be possible, you need to agree on some kind of shared API or shared interfaces so that you can replace the different components in the stack. At the moment, I don't think we have a shared we don't have a shared interface at the moment. Different communities have consensus on interfaces. Like, for example, the Cosmos community has this something called ABCI application blockchain. I forgot what the C stands for.
00:53:01.634 - 00:53:40.480, Speaker C: Interface that already decouples consensus and the state machine. So you have like the APCI client, which, if I remember correctly, is the consensus, and then the ABCI server, which is the state machine. And we're using that because that was already a very natural separation between consensus and execution on Ethereum. I guess that's not so much the case because you use Ethereum anyway. So if you wanted to use Ethereum roll up on a different boot layer, you have to kind of create interfaces around that I can take.
00:53:44.210 - 00:54:25.180, Speaker D: Yeah, so the question was about who's writing these modules. There's obviously like a stack here, right? So there's essentially the protocol developers writing the roll up themselves, how the fraud proofs work, the clients, all that kind of stuff in the VM. And so that's typically like the L2 protocol team. And then you're going to have the actual application developers who just both in most L2s, you have some kind of contract oriented language similar to Ethereum where people can write their apps and they don't need to build their own blockchain. Basically, they just write their code for their application.
00:54:29.230 - 00:55:50.440, Speaker E: Um, yeah, I think for us, one interesting place to look if we talk about data availability, I think we will start in StarkNet in a completely ZK roll up mode where all data goes to layer one. And you always, every time you change a state in L2 you pay the price in the layer one, which is ethereum data layer. And we feel that it's not ideal. Like we feel that at some point we want to let other option of data layers to be activated. So I don't think it's in the level of the application or the developers of the Smart contract, but I do think and hope that the protocol will offer by the time a different layer of data, that the consensus can be run just on the data and it's offered by the protocol designers. And at some point after that I think that we have the ability and there are some interesting design around enabling other entities to write their own model of data availability that is not necessarily part of the protocol or part of the layer one that the roller run on top, but it's a completely different layer. But as long as users and applications are convenient with using this layer and it doesn't break the protocol, then we can and we would like to add it as well.
00:55:52.570 - 00:56:41.160, Speaker A: I think with time these lines are going to blur a bit more. To be honest. I see what all of you are saying but at the same time if we're building modular parts, they're going to be repurposed for other things. And I think a great example is like what we're doing with WASM and for Arbitrum nitro we're actually building a proving system for WASM and even though you won't see application developers necessarily interacting with this on a protocol level for Arbitrum itself, I can see many application developers leveraging these modular aspects of modular blockchains for other things. So, for example, if you have the ability to actually prove WASM code on the layer one, that opens the door for a whole lot of other applications that can leverage that. So you start seeing a blurred line of application devs actually leveraging these protocol tools and repurposing them for different things.
00:56:42.970 - 00:56:58.060, Speaker F: Interesting. So it sounds like right now almost all of this work on modularity is being done by the teams involved, by Starware or by Fuel or Arbitrum or by Celestia. Is that about right?
00:56:59.550 - 00:57:01.266, Speaker C: What do you mean by the modularity?
00:57:01.398 - 00:57:13.460, Speaker F: When we talk about building a modular blockchain stack, we're each working on a system that has modular components and there are relatively few other people contributing modules right now.
00:57:13.910 - 00:57:24.840, Speaker C: Well, I would see it more as like we are the modules that are contributing to the border like modular stack rather than we're building something itself that is modular, if that makes sense.
00:57:25.370 - 00:57:32.994, Speaker F: Interesting, so each of us is trying to build the module that takes up that slot in the stack.
00:57:33.122 - 00:57:35.258, Speaker C: Yeah, that's how I see it.
00:57:35.424 - 00:57:50.800, Speaker F: Interesting. While we're kind of talking about what we're building concretely, are there any parts of your stacks that are not modular that would be extremely hard to replace or to substitute another technical system for?
00:57:51.890 - 00:58:43.520, Speaker C: So for Celestia. We describe ourselves as a consensus and data availability layer. Now, in theory, you could potentially decouple consensus and data availability as well. And some projects like Starquare for example, have gone somewhat down this route. So with Starquare, for example, you don't have to use ethereum as a data availability layer. You can use a volition, which is kind of like a trusted data availability oracle, but that doesn't have consensus. The reason why we're not decoupling data availability consensus is because if you just have a data availability layer without consensus, it's not census chip resistant unless you have multiple data availability layers that people can kind of use, maybe in a round robin kind of way, but we're not there yet.
00:58:43.520 - 00:59:28.480, Speaker C: The problem with making the stack more and more modular is that you start to introduce more complexity and you need to have developer tooling to address that complexity. For example, like ten years ago, it would never have been practical to do what we're doing today, which is decoupling consensus from execution, simply because there's a lot of work to be done in making roll up, like developing roll ups and tooling around that even today there's so many teams working on roll ups to make them practical and usable. They're only possible today because of that. But like for example, five years ago or ten years ago, it might not have been as easy as possible.
00:59:32.370 - 01:00:23.840, Speaker D: Sure, yeah. Specifically for fuel, I'd say some of the things that aren't very modularized are specifically things around like our VM op codes and kind of our transaction data model because those are the kind of the core decisions we made to be able to achieve the throughput and scalability and parallelism that we want. So it'd be hard to just be like, oh, you can use UTXOs or accounts, right? I mean, you can probably layer something like accounts on top of it, but just to swap something out at that core of a level doesn't really make sense because that's kind of what makes the whole product work the way it does. I think outside of that, there's lots of other things you can make modularized. Like for example, we're using our own language, but it just compiles to a bytecode for a VM. So you could use other languages, it's fine.
01:00:24.290 - 01:00:25.022, Speaker E: But yeah.
01:00:25.076 - 01:00:26.080, Speaker D: Do you have anything?
01:00:27.410 - 01:01:00.460, Speaker E: Yeah, I think for us, one thing that is not easily going to be modular is the execution layer, or in particular Cairo. Eventually we optimize not the re execution, but the generation of the single proof. So while I can think of a Verifier that knows to read different languages or even different proof systems, I think it doesn't make sense at this point. So it's probably going to stay invariant. So yeah, that's I think one piece that is going to stay for a long while.
01:01:01.950 - 01:01:47.930, Speaker A: I think for me, maybe invariance would be a very strong word to use, but I'd say the core module for Arbtrum, at least right now, is this idea of the interactive fraud proofs and what it allows to do, especially like the EVM. Whenever you need to prove something on top of not necessarily the EVM, but another VM, you always have limitations, and it allows you to put us to put an upper bound on top of the work that goes into doing all that and a lot of our secret sauce. And what actually makes Arbitrum work and be efficient comes from that, I believe, which a lot of our approach when even thinking on theoretical sign behind actually came from. Starting with this driving principle of idea of how can we prove this back and prove this efficiently. And the other modules around this are somewhat interchangeable, but I'd say that's the core.
01:01:50.590 - 01:02:27.750, Speaker F: So it sounds like for three of you, the answer is the execution model is the VM or the proving system or Cairo that is kind of driving the execution of smart contracts. I guess Mustafa building a data availability level is that layer is in a bit of a different position. So for the people building rollups, why did you choose this specific piece of technology to build around? Why choose the multi layer, the interactive prover or the Fuel VM or Stark specifically?
01:02:31.210 - 01:03:07.940, Speaker A: So I guess the interactive fraud proofs have a couple of different benefits. But the idea is that, for example, an alternative would be just reexecution on the L1, for example, but that comes with limitations. For example, you would be bound by whatever you can do within a span of a single transaction. When you have interactive fraud proofs, you actually can put that along the span of a couple of transactions or various and you narrow down. And this goes back to what I was saying before, that you have predictable amount of work of what you do to actually create these proofs. And yeah, I'd say that's one of the things. There's quite a bit that goes into this.
01:03:07.940 - 01:03:19.190, Speaker A: But actually we did a very good deep dive around interactive fraud proofs that expressed it much better than I can. Now, if you look for it on YouTube, I think it's also linked to in our documentation.
01:03:22.570 - 01:04:12.520, Speaker D: Yeah, so with the field VM we have no global state lock between each transaction. So essentially the more capacity we get from the data layer, the more we can post to it, assuming there's more cores. So that's something that really what makes the motivation for building the Fuel VM the way we did, as well as also tuning it to do the verification games. So making that modularized would be probably more overhead than it's worth in terms of the trade offs because I don't know if people would really want that. They want whatever is going to be the fastest. You can add other layers on top to abstract those details from people. But at the core level that's what you need to have to make the product good.
01:04:13.790 - 01:05:35.490, Speaker E: Yeah, I think for us the main question is about Cairo. Like why did you chose Cairo in? So I want to say that Cairo is something that we started to develop a while ago and it's actually went into production with Starkx already, so it's already running on minute for a while and we've learned a lot from that. And building a language that is capable of perform and still help you generate proofs for any computation is not something simple and in all kinds of levels. So Cairo is something that we invested a lot of time in and it paid off in the speed that now we can develop StarkNet because StarkNet then eventually is just implementing an operation system with Cairo. So I think replacing Cairo will be extremely hard at this point. And also we invested a lot of time making sure that Cairo is secure, safe and efficient and this is why StarkNet is based on this language and they will be obviously supporting all kinds of other languages, but they're going to be probably less efficient if they would be in the core.
01:05:37.350 - 01:06:05.020, Speaker F: And one more question before we move on to a new topic. Mustafa, you mentioned earlier that it is extremely difficult or impossible to separate consensus and data availability while maintaining censorship resistance. Are there any other interesting difficulties in modularizing the stack or other places where it really doesn't want to become unstuck like that?
01:06:09.330 - 01:06:53.340, Speaker C: That's as far as you can modularize it that I can think of. But one okay, one other component is this idea of composability. I think the more you modularize the stack, the more difficult it is for different applications to communicate effectively. There's a trade off between scalability and composability. We see this even in Ethereum, right? With ethereum roll ups. Ethereum roll ups are composable with each other, but they're not as easily composable as just having every smart contract on the same blockchain. You can just do a cross contract call.
01:06:53.340 - 01:08:01.666, Speaker C: So there's always this trade off between composability and modularity. And the other thing is I recently wrote a blog post about this idea of clusters. As you said, in order for two chains or two roll ups or two kind of systems to communicate with each other in a trust minimized way, without trusting some committee, you have to implement a roll up based bridge. But to do that, those two chains you're communicating with have to understand the state transition system of each other to able to verify the fraud proofs or validity proofs of each other. But then if you want to create a different kind enough execution environment that is not compatible with the execution environment of those two chains, you cannot communicate between that chain and those two chains in a trust minimized way. You instead have to use a committee based bridge which is less secure. So the way that I envision this ecosystem looking like is.
01:08:01.666 - 01:08:21.830, Speaker C: Like a cluster of chains where each in each cluster, the chains communicate with each other in a trust minimized way. But clusters communicate between each other in a trusted way. And that's totally okay. The important thing is to make sure that we maximize the scalability of each cluster before spinning up too many clusters.
01:08:23.450 - 01:08:47.680, Speaker F: So we are coming up to the end of our time. The one quick thing that I want to talk about before we go to audience questions is the idea of data availability. Three years ago, no one was talking about this. Now you see it everywhere and it's considered incredibly important to protocol design. Do we have a brief narrative of how we got from there to here?
01:08:49.170 - 01:10:01.490, Speaker C: Well, the history of the importance of availability in blockchains goes back to, I think at least 2014. There was emails on the Bitcoin mailing list of people discussing what is a blockchain, what really is a blockchain. People were arguing this and Peter Todd argued that fundamentally is core a blockchain is something which he referred to as a proof of publication system, which is like if you post a message, you can prove that you've published it to the Internet. And that's what we mean by data availability in today's terms. But the idea of data availability became popular when in Ethereum Scaling research communities, data availability was important in sharding. Because if you want to shard the blockchain, each shard has to effectively make sure that the data of each other shard has actually been made available. And the reason for that is to make sure that you can process the fraud proofs of each today's.
01:10:01.490 - 01:10:52.870, Speaker C: But in today's world, to give you some context, before we had roll ups, we had a different kind of layer, two kind of strategy called plasma. And plasma was this side chain technology that was very, very similar to roll ups, except that the data was not posted on chain. It was just controlled by the operators of the side chain. Of the side chain. But the problem is that if the side chain operators don't publish the data, then the users of the side chain are screwed because they can't prove that anything bad happened. And then for years and years, people tried to come up with a solution to this and ultimately people realized the only solution is just to post the data on chain. And that's why we have roll ups.
01:10:54.650 - 01:11:18.026, Speaker A: Yeah, I think you covered it pretty well. And the idea of plasma, I think it came with two main problems, right? One was the mass exits games. That happens that whenever someone thinks that this coordinator is actually withholding the data, the incentive is for everyone just try to run away together. And I believe the other one is Fisherman's Dilemma. I think that's the name right there's.
01:11:18.058 - 01:11:18.960, Speaker E: A nice name.
01:11:19.650 - 01:11:20.110, Speaker C: Exactly.
01:11:20.180 - 01:11:54.250, Speaker A: That is the idea that if this person is actually being malicious and withholding the data from you well, me as a user, what can I do? Right? I can ask for him the data he's not going to send to me, what do I resort to? I need to act on the chain. And this means that the cost of the coordinator being malicious falls on the user, and that's very non desirable. And this way with roll ups, the idea is we just take that out of the equation and the data is already there. So, for example, when you're designing a protocol, you can always make this assumption of the data is available and people can act on it. And that just simplifies from there on whatever you're designing.
01:11:57.630 - 01:12:30.760, Speaker D: Yeah, I mean, coming from originally, like TrueBit and all that kind of verification game stuff, I think people kind of realized what we really need are the consensus is just ordering of the data and consensus over that it exists. So with that, basically, it solves a lot of the problems with having data off chain. So I think it's been sort of in the works for a while, but, yeah, it's nice to see everyone making it happen.
01:12:34.510 - 01:13:24.374, Speaker E: I think for me, one important point for when data availability became very interesting. So when we started to build Starkx, we were actually working on it. I think at the beginning with zerox. And for me, it was clear that for a long while, using layer one data would be good enough. And it turns out that it's not the case. Like Starkx showed it with zero X and then with other systems that run on top of it, people were just like they wouldn't accept the layer one cost or the lay one linear cost as the solution for data. And for me, this was like I don't know exactly what was at this point in time, but somewhere like two and a half years ago that we realized that this is a problem that we need to solve and it cannot just rely on lay one forever.
01:13:24.374 - 01:13:31.050, Speaker E: And this is hence everything that happened since in Starkx and everything that we plan for StarkNet.
01:13:31.950 - 01:13:47.280, Speaker F: I think it's interesting that so many different threads of research converged at just about the same place. Anyways, I'd like to open it up to audience questions. Nick, do you want to handle the microphone for that?
01:13:49.110 - 01:14:09.350, Speaker B: All right, guys, now's your chance to ask a question. Yeah, go ahead. Or if you want to shout or I'll hand it to oh. And also for those who are watching in the live stream, feel free to drop your questions on the chat.
01:14:10.010 - 01:14:59.350, Speaker C: Okay, I have a question for Celestia specifically. I hope it's not too stupid. Do you have a token? And if you don't have a token, how do you ensure that the data on the data availability layer is not spam? We plan to have a token for our proof of stake protocol because Celestia is a consensus layer, and for that we're using tendermint proof of stake. So we do have a feed market. So there will be a block size limit. If people start spamming blocks, the feed market will kick in and spam will become economically impractical. Effectively.
01:14:59.350 - 01:15:47.000, Speaker C: It's the same the US. Resistance story as ethereum or bitcoin. Okay, so a very small follow up to that. Does that mean that the core data availability layer of Celestia also has to process transfer transactions for the core? The main chain of Celestia has to process transactions that pay the transaction fee for applications that post data on Celestia. But it's not necessarily the case that it's like one transaction has to be processed for each roll up transaction. It's more the case that you have to process one roll up transaction for each roll up block. Right.
01:15:47.000 - 01:16:00.860, Speaker C: The more applications you have, the more transactions you have, basically, rather than the more transactions you have on the roll ups, the more transactions you have on the main chain, if that makes sense.
01:16:02.990 - 01:16:05.660, Speaker B: Cool, thank you. More questions.
01:16:13.250 - 01:16:53.690, Speaker C: Hey, I was just curious. Is there a future in which we will see all of you guys working together in the sense that, I don't know, maybe Stackware will become an execution engine for Celestia and Arbitrum will provide the fraud proof system of sorts. Right? Because right now it just seems to me that, yeah, Aminfuel is working with Celestia, obviously, but all the other projects are trying to do more or less the same thing, but separately. Yeah. So there's definitely a future where that's possible. We're talking and working with fuel labs. Fuel Labs has plans to use Celestia as data availability layer.
01:16:53.690 - 01:17:57.070, Speaker C: Celestia would also be very natural alternatives to Volitions because the Volitions already have trust assumptions. If you use Celestia as a Volition, that's even better from a security perspective. Now, the problem with using Celestia as a data availability layer for Ethereum roll up specifically is that Ethereum does not support off chain data availability proofs. So you have to basically use a committee based assumption where you check that the celestial consensus has signed off on the block and therefore it's available. So technically it's off chain data availability because Ethereum does not currently support off chain data availability proofs. But that being said, you could use Celestia, for example, in the software context as a Volition or other context as an off chain data availability layer.
01:17:58.710 - 01:18:04.062, Speaker A: In the case. Would it be kind of like a Celestial Lights client as the Oracle for this data availability?
01:18:04.206 - 01:18:05.314, Speaker C: Yeah, exactly.
01:18:05.512 - 01:18:06.370, Speaker A: Sweet.
01:18:07.590 - 01:18:08.002, Speaker C: Yeah.
01:18:08.056 - 01:18:40.640, Speaker E: I think one other interesting direction that I don't know if exactly that's what Bartek meant with this question is that you can also think of data availability fraud proofs that can be replaced with data availability proofs. So that's, I think, another topic where some collaboration can bring to potentially improvement on both sides. And we definitely think that other data availability solutions are interesting for us.
01:18:45.250 - 01:18:54.820, Speaker B: Cool. If you're on the top row, by the way. And you want to ask a question? If you could just walk down so I can see you. We have one over here.
01:18:59.910 - 01:20:23.230, Speaker C: Yeah. Just to follow up on what you just said that Ethereum does not support off chain Availability Proofs. So does that mean that you could not build a roll up, which does not basically write the data required for the fraud proofs on Ethereum? So it's not enough to just have the data, but you actually have to have that data on Ethereum Mainet? Is that what you mean? Yeah. So if you want to build a roll up with the security properties of a roll up, which means that you don't have to trust the roll up operator or any kind of committee, and you want to completely rely inherit the security of some main chain like Ethereum, then you have to post the data on Ethereum. The problem is that if you post the data off chain on a third party availability layer, it becomes like plasma. It's basically a plasma rather than roll up. Or in the case of a ZK roll up, it will become a validium, which is fine, because the core problem is that Ethereum cannot verify that data is available on some third party chain without relying on a committee.
01:20:23.230 - 01:21:21.860, Speaker C: Because in order to verify that data is available on a third party chain, you have to use something called Data Availability Proofs. But Data Availability Proofs, you can't verify them inherently within the smart contract because they require some interactivity that just aren't possible within a general purpose execution environment like a smart contract. So if Ethereum wanted to support the verification of third party chains using Data Availability Proofs, you have to effectively fork Ethereum to add Data Availability Proof verification as a kind of like a pre compiled upcode. John Adler has a proposal for this on Ethereum research forums. Does that answer the question? Yes. Thank you. And just a small one.
01:21:21.860 - 01:22:16.100, Speaker C: Is there a particular reason why in Celestia you're using Tendermint and not Casper proof of stake? Is that just because it's simpler? Well, does Casper exist as, like, a standalone proof of stake library at the moment? I don't know if that's because the reason why we're using Tendermint is because when we first started building Celestial, at the time, tendermint was the only production ready modular proof of stake sorry, BFT protocol at the time. So that was really the only option back in late 2019, I think. Now, maybe there's some other options. I know maybe this code for Casper that can be used in a more pluggable way, and I know, like Avalanche has released a source code for the proof of stake protocol. Thank you.
01:22:17.910 - 01:22:36.170, Speaker B: Any more questions? We have one question on Live Stream, which I'm going to go to first. It's Steph is asking what are the trade offs between ZK Snarks and ZK starks and optimistic roll ups?
01:22:38.910 - 01:22:39.660, Speaker A: Yeah.
01:22:40.990 - 01:24:08.230, Speaker E: How much time do we have ten minutes? Okay, I think at this point I'll just say that the differences are like it's not just theoretical differences. There are already teams and project building very specific solutions. So I would focus on that. We started with Starks because this was first our expertise and we also noticed it operates faster and that we can rely on it on the long term and we got to very nice performance there. But right now the most important part is that it enabled us also to create Cairo which enabled us to develop really fast and basically really fast and quality solution that is darknet. So I think the differences there lie mainly on performance and the expressibility of the language and removing the need to use circuits and disenable all kind of things that relate to scale and throughput with regard to optimistic roll up, I will let my friends here to speak. One thing that I mentioned in my short talk before is that one advantage that we see is that the ability to post much less data on chain which is still very important in all kind of use cases.
01:24:08.230 - 01:24:11.960, Speaker E: So that's one thing I will mention.
01:24:13.610 - 01:25:09.820, Speaker A: Well that's true but also that as you said at the start, it's very use case specific right, because different use cases have different requirements which fits to different models differently. So for example, reducing the data you post on chain to for example state diffs is a very interesting approach but many times many applications actually needs the intermediate data in the middle of execution and that's something that if you're just posting the state diffs you're more limited because you actually need to trust this intermediate state. But many applications actually don't need that, others do and there is a trade off based on use cases. And yeah, I believe that just having many different flavors of many different scaling solutions is the best thing we can have right now. So everyone can actually experiment and we can see how these systems actually react in production. And, yeah, there are many different community resources that show the differences of what it's live and many different papers that show theoretical differences of what can be live. And, yeah, I'm going to keep it at that, otherwise it's going to be half an hour.
01:25:12.830 - 01:25:48.520, Speaker D: Yeah, that's part of the beauty of the modular stack is that you can have all these competing solutions and it really depends on the implementation of each approach to see how it stacks up. So let the numbers talk I guess. But yeah, overall there's a lot of different considerations to take into account depending on what kind of proving time you want or liveness of your transactions. So those are all different things that probably deserve like a full presentation but yeah.
01:25:53.770 - 01:26:03.050, Speaker F: I think that just about wraps it up for our panel discussion tonight. Thank you all so much for your time. We have food and drinks starting pretty soon.
01:26:03.120 - 01:26:18.698, Speaker B: Yeah. Food and drinks are upstairs. They made a really nice spread. This is a really great restaurant. There's dessert. It looks amazing. So please hang out and mingle and chat and let's learn more about data availability and roll ups.
01:26:18.698 - 01:26:25.610, Speaker B: Thanks for coming, everyone's.
