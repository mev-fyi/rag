00:00:00.170 - 00:00:00.670, Speaker A: You.
00:00:00.820 - 00:00:38.658, Speaker B: Okay, so I guess we can start. So let me first maybe introduce myself. My name is Bartek. I'm a founder of l two Beats. We are the community watchdog for all l two s right now on Ethereum, and we try to inform the users, what are the trust assumptions, the security assumptions of all these solutions, of the users are actually aware. And I've got an amazing panelist today representing, I think, three most known projects that promise to deliver. Da.
00:00:38.658 - 00:01:59.970, Speaker B: So we have Avele, we have Celestia, and we have togru from scroll, who will, I guess, have an interesting role on this panel because he will represent Ethereum. So let me just welcome my panelists, and let me just start by saying that I've watched this panel last year, it was very docile, everybody was very nice to each other. And I think it was because maybe the space was nascent, everybody was building. But now we are like almost launching, or have just launched, and things are becoming a little bit more spicy. So let's make this panel spicier and see how we go. So my first question to you guys will be, well, Ethereum community is probably considered to be one of the largest. And should Ethereum community really care about your solutions, or should they just simply wait for protodunk sharding and dunk sharding? So maybe let's start with a veil and then we just go this direction.
00:02:00.710 - 00:03:07.126, Speaker C: Yeah, I think this is a pertinent question to ask. In general, if you look at the timelines, right, protodank sharding will come maybe end of this year, early next year sometime. But dang Sharding is going to take a lot of time to come, because from our perspective, we have gone through the whole cycle of engineering the p two p on avail, as I think Celeste has been on the same journey. And think Ethereum being a system that is already securing a lot of assets on the chain, it's difficult to kind of introduce functionality that can potentially jeopardize the current state, for example. So it's going to necessarily take some time. And you're also seeing the rise of, let's say, l two systems, l three s on top. Like I said in my talk a little bit earlier, each major l two today is looking at this l three strategy, like Zk sing hyperchains, arbitram orbit and so on, right? Everyone is looking to do that.
00:03:07.126 - 00:04:06.186, Speaker C: And right now, let's say if you look at Arbitrum, which is doing a nova or a stack web, which has a DAC, so all of them are operating dacs, which are pretty decentralized, sorry, centralized. And all these l three s will require some secure da solutions, which is less expensive than what Ethereum can give at the moment. Of course the cost will go down with EIP 4844, so we acknowledge that. But in general, if you look at our architecture, and I will agree with Celestia's architecture as well, in the sense that we are able to provide far significantly less cost and quickly before taking up too much time. Data availability sampling is massively underrated. Like it's not very well understood by a lot of people. And I think unless there is data building sampling implemented on Ethereum, I mean there's a lot of way to go there.
00:04:06.186 - 00:04:51.580, Speaker C: And we are not really using this construction like things like with recursive proofs, Zk proving systems already in there, you are able to now give proofs, propagate these proofs to the p two p, for example to the users directly. So something like a starkware which puts proofs on Ethereum every six to 8 hours, someone like that can actually create intermediate proofs and pass them directly to the users. And of course they need to wait for the proof to come to Ethereum for bridging and such, but it's much faster verification time. Right, sorry, not taking up too much time, but I think all of these points are important.
00:04:53.390 - 00:05:31.826, Speaker D: Sure. So I think there's definitely place for multiple DA layers. They all have different trade offs, different use cases. For example, for Ethereum, as mentioned, put down sharding is just like a very small step to the overall roadmap of dank sharding set up different trade offs. So for example, celestial and avail are more overhead minimized. There's no state baggage already, so for example, it's more practical or overhead minimized for sovereign roll ups, for example, if that's what you want to build. There's also like various design, different design choices.
00:05:31.826 - 00:06:05.310, Speaker D: So for example, eip four four, you can only fit eight blobs without burst in a block. If you want to do app chain roll ups, you'll probably need some data aggregation service for that to be practical. Whereas for example, on selection avail, there's no specific blob size limit, minimum blob size limit, for example. And also of course, there's the fact that we have data availability sampling, and that's still kind of further on in the Ethereum roadmap.
00:06:07.090 - 00:06:11.842, Speaker B: Tucker, what do you think? I mean, should we consider using other.
00:06:11.896 - 00:06:17.974, Speaker A: DA first, should I explain why Ethereum community should care about data availability on.
00:06:18.012 - 00:06:23.302, Speaker B: Ethereum or no, I'm kind of assuming that we all know that, so let's just move on.
00:06:23.356 - 00:07:16.550, Speaker A: Yeah, that was my guess as well. I feel like there's a place for multiple data availability solutions. And one thing to understand that we're trying to solve different problems. So on Ethereum, the DA layer serves as a way to separate the markets for data and execution and also increase the capacity by separating that market. And basically that allows roll ups that are deployed on Ethereum to scale better because you can now post more data and still settle on Ethereum. For celestial, the target market is a bit different. They're targeting sovereign roll ups and protocols that don't really care about execution on the base layer, but more just want some ordering and data availability guarantees.
00:07:16.550 - 00:07:41.360, Speaker A: And I think that there's even a way where you can combine the approaches. For example, validiums can use Celestia or avail for data availability, but still settle on Ethereum. So I don't think that there's a world where only one solution is required and only one solution is needed. We need multiple solutions, especially if they solve different problems.
00:07:42.470 - 00:07:58.440, Speaker B: So it sounds like Ethereum is targeting different users, perhaps, I don't know. Mustafa Anurakamin, would you agree with that? Know, your target is slightly different than what is Ethereum trying to actually do?
00:07:58.810 - 00:08:39.330, Speaker D: Well, I mean you can technically deploy sovereign roll ups using Ethereum SDA, but I don't think that's not something that, from a social perspective that I think the Ethereum community coalesces around. And it's not overhead optimal to have a sovereign roll up on Ethereum because you have to also run a node that gets the state off the chain. But ultimately the whole point of it's modularism, not maximalism. The whole point is like it's not a zero sum game. There's different trade offs Ethereum roll ups have to have on chain da. So if you want to deploy Ethereum roll up, you have to use Eth as a DA. There's no way around that.
00:08:39.330 - 00:08:51.430, Speaker D: If you want to do a validium or celestium or optimistic chain then yeah, you can use off chain dA. But ultimately it's just all security tradeoffs and different design trade offs.
00:08:52.490 - 00:09:31.380, Speaker C: Yeah, I would sort of agree in the sense that it's very difficult to demarket the target segments in general. How would you demarket the users of a roll up with a validium with the same stack? Of course the security properties are very different, but that's the reality that we are seeing on production. Right. You have an arbitram, one with an arbitram nova, you have a stocknet with a volition built in, right? So we are increasingly seeing this polygon also announce their validium plans. They have a roll up. They have a validium. And so of course the target segments might be different.
00:09:31.380 - 00:10:04.000, Speaker C: Some apps need the security of Ethereum, for example, but the roll up developers are asking for solutions that help them target maybe different. You can say there's a different target segment, it's the same target segment, but it totally depends on what GTM these roll up developers. But yeah, I would agree that from a more sovereign roll up, kind of maybe avail and Celestia are better options at the moment.
00:10:05.090 - 00:11:02.494, Speaker B: Okay, so let's dive in a little bit into those trade offs so that it's properly understood. Because I think I'm actually taking it from Mustafa's talk in the morning that it's not easy to talk about these trade offs. So on Ethereum especially, we all know that cost of data is actually quite high for all the roll ups to pay. And obviously that translates to the cost to end users. And that's why we're exploring all these strange constructions like volitions, validiums, data availability committees, even such weird constructions like optimistic data availability schemes. So which solution do you think will be ultimately the cheapest to choose as a DA? Just you can raise your hand, who's the cheapest? That's my first question.
00:11:02.692 - 00:11:08.422, Speaker D: You mean avail versus, or like, do you mean validiums versus roll ups?
00:11:08.586 - 00:11:10.610, Speaker A: The one that has the most capacity?
00:11:11.110 - 00:11:42.640, Speaker B: Well, I'm thinking about typical dev team that is trying to deploy either server in roll up or a roll up. But for them, the current DA on Ethereum is just too expensive. Right? So they would come, let's say to us, to LTV and they would ask us what do we think, which solution might be the cheapest? And they will provide the cheapest essentially block space for data.
00:11:44.610 - 00:12:15.010, Speaker D: So I think there's two ways to look at this. The first way is you look at classes of different types of DA. So for example, you can say like ethereum, celestia and avail are like one type of on chain da. You have like a blockchain da. Then you have other kind of less secure types of da, like a decentralized, sorry, a data availability committee with like a multi sig of seven. That's what stockware and any trust arbitrum have. And then you can just have a centralized like a single server.
00:12:15.010 - 00:13:06.790, Speaker D: So ultimately the cheapest is just like have a centralized server as a DA but obviously that's not useful. So let's assume that we're talking about like on chain DA. The important thing to note here is in blockchains you cannot guarantee low cost transactions. And the reason for that is because if you can guarantee free or low cost transactions, then you have a denial of service problem. So I think it's not about framing the question in terms of cost, because ultimately there'll be fee markets and there'll be supply and demand. It's about framing the problem in how do you get the most throughput? The only thing that blockchains can guarantee is the throughput they have or the block size they have, because ultimately the fee or the pricing will be determined by supply and demand and how much demand there is for that specific block.
00:13:09.950 - 00:13:56.710, Speaker C: Mean. Again, I sort of agree with Mustafa in the sense that it's not good, not enough to just look at the cost sense. Of course there will be throughput and our architecture allows us to create high throughput as well. But also we have to look at other factors like decentralization and stuff like that, right? There are a lot of factors. It will be cheaper, of course, and we can increase throughput as such. But as you said, we cannot compete with a single server doing da, right? And there will be solutions like that. We are already seeing solutions like that, right? So the thing is, it's a trade off between cost, decentralization like throughput and lot of other factors.
00:13:56.710 - 00:14:24.640, Speaker C: And again, just coming back to the point, right, certainly cost is a factor, but we also have to look at things like data resampling lightlines, which provide new kind of powers to these roll ups, right? Like I mean, propagating the validity proofs of fraud, proofs to the users directly. Can users verify that directly? For example, how can we come up with those kind of constructions? We should consider that as well.
00:14:25.330 - 00:14:46.070, Speaker D: And the light node is also an important piece because all the roll ups that are running on Celestia right now, like the sequences running on Celestia, they're not running full nodes, they're actually running light nodes, like the sequencers themselves are running light nodes and they're not paying for RPC endpoints or running a full node. And that's just significantly cheaper than having to need access to a full node.
00:14:46.570 - 00:15:16.790, Speaker A: But I don't think that for a sequencer the cost of running a full node really matters. I run a couple of full nodes at home. And even if you assume that I run relatively high end hardware for that, the cost of one node is less than $1,000 and considering that sequencers extract so much profit, potentially can extract much profits from mev, et cetera, et cetera.
00:15:16.970 - 00:15:24.930, Speaker D: I'm not saying that's the main thing. It's just like, that's one nuance. For example, I'm not saying the main cost, ultimately, it's about the cost of the DDA.
00:15:25.290 - 00:15:26.310, Speaker A: Gotcha.
00:15:27.050 - 00:16:54.938, Speaker B: So what about the security then? I mean, the common argument that I keep hearing is that it's always a security trade off to cross the trust boundaries when you're actually using the external DA. So clearly, the security of validium is very different than the security of a rollup, right? And the cost of validium is also very different. That's why we've got arbitram, arbitrum, Nova, that's why we've got Starknet or Starkx roll ups and Starkx validiums. And I was always wondering, like end users, at the end of the day, they seem to have very little to say. If you are, let's say, dydx user, what benefit do you gain from DyDx actually posting all the transactions as opposed to them using some data availability committee? Right? You just want to trade on DYDX, and you want to have a reasonable guarantee that if things go wrong, you can always recreate the state from the data and you can always exit. And it seems that this is all about the trade off. Right? So with increased security comes, I guess, bigger cost.
00:16:54.938 - 00:17:04.260, Speaker B: So how do you, as an app developer or as a roll up developer, suppose that I'm Dydx, how do I make this trade off?
00:17:05.910 - 00:17:44.370, Speaker C: Yeah, ultimately, I think the choice will be decided by the roll up developer, and I think not by the app developer directly. I mean, unless we're talking about app chains, for example. But before that, we are seeing a lot of roll up orchestration players coming to market, for example. And I think these are the set of developers who are making the choice in terms of what DA to use. And at least from my perspective, what we are seeing is it's at the moment a cost versus security trade off kind of a thing. So of course they want cheaper costs for the users. That's kind of the primary metric.
00:17:44.370 - 00:18:31.098, Speaker C: But in general, from a roll up operational point of view, right at the moment, of course, no decentralized DA layer is in production, which is going to change pretty soon. And that's why you see these datablood committees. And right now, that may not be a problem, but I definitely see decentralization of the DAS similar to the decentralization of sequencer questions. That will come from a regulation point of view or whatever, right? In general people don't. For now, single sequencer, data ready committees are fine, but I think just to safeguard regulatory interests and stuff like that, I think for sure things will move to a more decentralized context.
00:18:31.274 - 00:18:39.410, Speaker B: Are you saying that this is the primary reason why the UIDX chose to go its own way and be more decentralized?
00:18:40.710 - 00:19:15.630, Speaker C: I can't talk to the intentions, of course, but I think in general I didn't fully comprehend the whole reason why they moved. Of course they have more flexibility in running their own app chain as such, and so they are able to customize a lot, is what I feel. I think the Starcrafts solution was a bit limited is what I may be wrong in terms of what they wanted to do. And of course Starkx has now, I mean there's new upgrade, the Starknet is much more powerful, Cairo 1.0 and such, so I can't ascribe their intentions.
00:19:18.210 - 00:20:18.818, Speaker D: Yeah, so I think there's kind of like an interesting question in here, which is about how much do users actually care about decentralization from a UX perspective? There's no UX difference, probably interacting with the AC versus interacting with the on chain DA. But consider this, users today can use Polygon, they can bridge tokens from ETH to Polygon and use it pretty much as the same as l two or roll up, even though it's not. But if that's the case, then why did Polygon spend a billion dollars on ZK roll ups? It doesn't actually make a direct difference from a user perspective. In fact, probably like slightly less TPS at the start until we optimize the ZK previewing systems. I think ultimately this is what differentiates webt from web three. From a social perspective. Ultimately people do coalesce and do care about the decentralization properties of the systems they're interacting with.
00:20:18.818 - 00:20:54.400, Speaker D: Otherwise, why has no one just created like a centralized proof of authority? L one, just ten committee of ten nodes, billion tpS? No one would take that seriously. So that's why I think, yeah, people should use the acs for certain use cases, but ultimately for an application to be kind of like credibly decentralized and credibly have social community around it, there does need to be at least a decentralized DA as an option.
00:20:56.690 - 00:21:59.490, Speaker A: I have a small question for you. With the whole validium talk, are you trying to bring back the conversation whether validiums are l two s or not? Because I had a feeling that that's what you're doing there. So I feel like it's all about the trade offs, because when you're building a certain application or a certain protocol, you need to assume what is the worst case scenario that your protocol can handle? And is that worst case scenario worth taking certain trade offs? So, for example, if you're building a game that doesn't really have any monetary value, then you probably shouldn't use on chain da, because it doesn't matter. Like worst case scenario, your game assets disappear, whatever. But if you're building an app that has billions worth of dollars deposited on it, I think then you should take more care of how you design things, because billions of dollars frozen in a contract that you can never withdraw from, that's a bigger problem than your game assets being frozen.
00:22:02.470 - 00:23:07.960, Speaker B: Okay, so let's maybe switch gears a little bit. So here's another common question that we get from users. Well, first of all, sometimes it's hard for them to differentiate between the data availability and data storage. But let's assume for the sake of this discussion that we all know, and for the audience that don't know, maybe some of you can give a very quick introduction. But the question is, on Ethereum, it seems like we have a very reasonable and strong, I guess, guarantee of the data storage, because there's this huge, vibrant ecosystem of different explorers and indexes and whatnot, and people just simply assume that it works. Right? Now, if I use avail or Celestia, how can it be guaranteed that in twelve months I will have access to all the data and I will be able to actually recreate the state?
00:23:09.470 - 00:24:05.900, Speaker D: Yeah, so, I mean, that's like a general question about the difference between data availability and data storage. Even with Protodank sharding and dang sharding. Protodank sharding also does not plan to guarantee the data forever. The current plan is to prune data, prune blobs after 30 days. And the reason for that is because data availability there, including Ethereum, it's meant to be like a real time bulletin board to allow roll ups, an opportunity to get their data, to make sure it's published so that they themselves can store it. What is the difference between data? So I actually propose renaming data availability to data publication because I feel like that's an easier to understand thing. Easier to understand data availability is about proof of publication, like proving that data was published so that people can access it.
00:24:05.900 - 00:24:54.090, Speaker D: Specifically in Celestia, at the moment, we don't prune the data blobs. They're kept around forever right now. But at some point after Mainnet, the community will need to coalesce around after what, certain point in time, data blobs will be pruned. But that being said, even on Ethereum, even on networks where blobs are pruned, I actually still expect that the data will be permanently stored somewhere and permanently accessible to the public, simply due to the streisand effect. For data storage only requires an assumption that a single person, ideally you have more, but the minimum assumption is that you have a single person storing the data, and that's extremely easy assumption to achieve on the Internet.
00:24:54.990 - 00:25:28.434, Speaker A: I think Bartek's argument was more that because Ethereum has such a vast ecosystem where you have rpcs, block explorers, et cetera, storing the data, it's highly unlikely, even if all the nodes prune the data, that the data will be. So, for example, I think it was ripple at some point, lost like a day worth of data because their servers somehow had a bug or something. So I think Bartek's question was more, sorry if I'm rephrasing it incorrectly.
00:25:28.482 - 00:26:02.480, Speaker D: Okay, I understand, but I agree that would also still happen on Celestia and other daas, including running. The cost of storing data is cheap enough that multiple people will do it, even if the ecosystem is smaller than ethereum. And also, the other very important thing to mention is that the whole point of having data availability sampling light nodes is that you're distributing data across thousands of light nodes. And that will also help somewhat with the data storage problem, assuming that those light nodes are happy to store it for a longer period of time.
00:26:02.870 - 00:27:14.710, Speaker C: Yeah, absolutely. If these light lines can make their way into wallets, for example, I think we'll have the data mirrored from the DA layer to the lightline network, and that can be propagated, kept for a long time. Second is just a point that if you look at the stack that we have built on, we've built on substrate, Celis is built on tendermint, for example. And so a lot of the tooling in these ecosystems is also compatible with the stack in general. And in fact, we are also starting to work with a couple of teams who want to put availed data onto ipfs, onto Filecoin, for example. And so, as Mustafa said, ideally you just need only one copy of the data. But we are expecting that we've been working with a few infrastructure providers in the substrate ecosystem, for example, and the ecosystem is more mature than, let's say two, three, four years ago, like in all these ecosystems, in the substrate ecosystem in the tenderman ecosystem, for example.
00:27:14.710 - 00:27:37.360, Speaker C: And so we don't really have to build all of this tooling from scratch, and there are a lot of providers out there. But as part of the architecture, we already have a lightline network that mirrors the data. We don't anticipate the kind of problems that togrul is mentioning in that sense.
00:27:38.610 - 00:28:09.160, Speaker B: Okay, so you represent, I think, to my knowledge, the three most known on chain data availability solutions. But very recently, it seems like there's new kid on the block somehow, and everyone seems to be talking about it. And I mean Eigen da. Any of you have a spicy take on Eigen Da and the trade offs and its role in this da? Kind of.
00:28:11.610 - 00:28:48.118, Speaker A: To. Before I go on to express my opinion, I would like to add that I don't represent EF. Just in case danker kills me if I say something wrong. I just accidentally stumbled on stage. I think from the perspective of Ethereum, Eigenda makes sense because it utilizes the existing validators, or a subset of existing validators to offer extra capacity that is much cheaper than on chain da. Obviously the security guarantees are much different, so it's not comparable with just.
00:28:48.204 - 00:28:54.130, Speaker B: Yeah, what are these security guarantees? Isn't it such just a fancy name for availability committee?
00:28:54.210 - 00:29:25.410, Speaker D: Yeah, I'll make things spicy. First of all, there's no docs on IgDa, so it's very hard to even compare it in the first place. People keep saying, oh, Mantool claimed to, they launched, right? They claimed to launch their main net on Ignda, but other people are saying, no, it's not. Actually, I have no way to verify that because there's no docs anywhere. And people say use, I can eat, but there's no docs. So when there's docs, it will be easy to compare. But from what I know so far, there's a various set of trade offs.
00:29:25.410 - 00:30:01.680, Speaker D: First of all, I'm very skeptical of the idea that you'll actually be okay. So first of all, I'm skeptical of the idea that there's enough demand for restake services. I think there's enough supply, there's a lot of supply of validators wanting to restake. Very skeptical. There's demand, at least. It's very hard to bootstrap your roll up if you're saying to convince validators to come and restake if the initial rewards only come from fees, which might be very small to start at the beginning. And the whole point is that the fees are supposed to be cheap in the first place.
00:30:01.680 - 00:30:23.170, Speaker D: Secondly, as far as I know, Eigenda requires a dual token model anyway, because you can't slash data on chain kind of defeats the whole point of restaking, in my opinion, because you have to restake not only eth, but you have to restake the Eigen token as well, because you can't slash data on chain.
00:30:24.230 - 00:31:20.038, Speaker A: I was about to add on this. I'm still not sure if that's a very sound model, the whole data availability based on crypto economic guarantees, because essentially the assumption is that if the data is withheld, then the price of the staked Eigen token is going to drop, and it acts as a disincentive. So essentially you can see it as implicit staking, like the one that Chainlink used before. And I'm not really sure that it's a strong enough guarantee for a lot of applications. Obviously, as I said before, some applications it's fine, but for a lot I just not sure if that's because it's also potentially vulnerable to griefing attacks, the same way Metis's DA solution is. So yeah, I'm not sure I don't.
00:31:20.044 - 00:31:23.240, Speaker D: Want to attack again the ATM watch when they're not here to defend themselves.
00:31:24.330 - 00:32:06.210, Speaker C: Yeah, that's true. I think last year's panel, Sriram was there, and I really like him, but we have yet to see a working system. I haven't seen that, so I would deserve my judgment based on that first taking. Not about Eigen DM, but restaking itself is a pretty neat idea. But I'm still skeptical of how it will actually work in production settings in terms of how many validators sign up. What is the actual security like? Griefing attacks such and such. So I would want to reserve my judgment.
00:32:06.210 - 00:32:12.678, Speaker C: At least I want to see something working before worrying too much about it.
00:32:12.844 - 00:33:28.910, Speaker B: Actually, from my point of view, what I would love to see more is more, I think, thorough security analysis of such potential attacks. Like Griffin, we had a discussion about that about two years ago, or maybe it was a year ago when Mattis launched so called smart l two and optimistic DA, which for those that, you know, they seem to be publishing data directly to the storage without actually mentioning DA layer. And if anyone notices that the data was not published to the storage, they could in theory challenge the sequencer and force somehow sequencer to post the data on chain. And the cost of such solution, as you can imagine, is extremely low. Hence fees on Matis were very low, and I was surprised that very few people actually bothered discussing that from principles, right? I mean, what are the security trade offs. And is that scheme actually viable to consider for anyone? The community was largely silent.
00:33:29.490 - 00:34:46.662, Speaker D: Are you talking about, I looked at Metis profile on LTB before this talk and the idea is to use a data availability challenge. Right? But the problem is with data availability challenges is that as far as haven't actually, there's no challenge mechanism finalized or proposed yet. But in general, the general problem with data availability challenges is that they don't solve the data availability problem. That was actually the first thing that people like Vitalik looked at to solve the data availability problem, but it doesn't solve it because of something called the fisherman's dilemma, which is where data and availability is what's called a uniquely unattributable fault, which means that the challenger who's challenging the data availability, it might be their fault that they can't access the data because maybe their network is not good. And that basically creates like a dilemma where it's like explained on the Ethereum wiki, but it's like you either have a situation where you have a DOS attack or there's not enough incentive to make a challenge. There's no incentive to make a challenge in the first place because the DA publisher might only release the data after you make the challenge. And so you're basically griefing you.
00:34:46.662 - 00:34:48.522, Speaker D: So there's no way to make it economically sound.
00:34:48.576 - 00:35:40.554, Speaker A: Basically, yeah, it's basically as Mustafa described, because default is not attributable. It's impossible to attribute default. Here you would essentially, let's say if I create a challenge and you post the data, you can both withhold it and you could have withheld it before. And that's why I initiated the challenge, or I was just not in sync with the network and that's why I didn't receive the network. And that creates a long term griefing vector where essentially you start the challenge, you stake a certain amount, I post the data. When you initiate the challenge, you get slashed, and then you continue to get slashed until nobody is willing to challenge you anymore because basically every time you slash, the sequencer just reveals the data. So there's no reason why you would long term continue challenging it.
00:35:40.554 - 00:35:52.970, Speaker A: And therefore it's fine if you consider the optimistic case, but in a worst case scenario, this whole security basically breaks apart.
00:35:54.910 - 00:37:09.590, Speaker B: Okay, so I have another question from a completely different angle. So you guys mentioned, and this is the thesis that we've been hearing and will be hearing, especially in this conference, it will be very easy for anyone very soon to launch their own construction on roll up with very custom security parameters. And as it is even today, for users to understand the security assumptions is extremely hard for us as an.org trying to understand that for us, it's a moving target that we need to chase. How do you see in the future, with potentially thousands of roll ups being launched, how do you envisage that users will be actually able to tell what are actually the security assumptions? And how do you see the role of people like us in this whole space? I'm very curious because I'm frankly terrified about the future you're describing. From my perspective.
00:37:11.690 - 00:38:20.282, Speaker C: The way I think about it is there is a wide variation in terms of roll up orchestration today. But as you're increasingly seeing, all the roll up stacks are also tending towards standardization, right? So for example, zkvms in terms of the implementations are going in a similar direction as such, for example. So that process will also apply to the entire roll up stack pretty quickly, I mean better than anticipated. And as I said, a lot of the major players are pushing their then standardized stack to these roll up as service providers or such, for example. And so what we feel is rather than having a wide variation of roll ups, there will be a set of roll up stacks that will be pretty much standardized. And the deployment itself also will be not be like 1000 different roll up configurations. It will be like maybe 510 different configurations, but like a lot of instances in that sense.
00:38:20.282 - 00:38:30.494, Speaker C: And so it'll be easy to kind of look at it. Of course it's not going to be as easy as I make it out to be, but yeah, I think people.
00:38:30.532 - 00:38:52.246, Speaker D: Should just use lt beat. That's the solution I'm relying on. You guys. You guys are doing a good job so far to be a credibly neutral and trusted way for users and developers to get quick information about the security trade offs between different the a layers and the roll ups and l two.
00:38:52.268 - 00:39:56.954, Speaker A: S. I think we opened up a can of worms by John specifically, I blame John Charbone for everything by writing that article about social consensus or how roll ups aren't real. Because now for the last three weeks or a month, I've been hearing people making ridiculous claims about Anatoli. I love him, but his argument that if I post data on Ethereum all of a sudden become an l two, it just doesn't really make sense. And so I feel like first, before we start discussing about how many roll ups are going to be built on a different protocol, we need to work together to define what a roll up actually is, because I don't really buy that whole social consensus thing because you can literally attribute it to anything. And therefore, all the concrete properties that we have about consensus protocol, signatures, et cetera, can be just handwaved away through social consensus. And so I think we need to start working on that first before we discuss what's going to be deployed where.
00:39:57.072 - 00:40:13.386, Speaker B: And we've just ran out of time. So that might actually open the discussion for entirely different panel. Thank you so much, guys. I mean, it was lovely to have you all here. Unless you've got, like, one last closing comment. Mustafa.
00:40:13.578 - 00:40:21.470, Speaker D: I mean, you've got to ask a spicy question, which is like, what's the know? But I guess we are applying for that, unfortunately.
00:40:22.050 - 00:40:24.480, Speaker B: All right. Thanks so much again, very much.
00:40:25.290 - 00:40:25.780, Speaker C: Thank you.
