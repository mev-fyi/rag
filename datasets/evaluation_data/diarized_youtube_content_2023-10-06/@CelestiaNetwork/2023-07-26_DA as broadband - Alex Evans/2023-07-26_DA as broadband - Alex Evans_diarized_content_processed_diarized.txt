00:00:00.570 - 00:01:27.366, Speaker A: Hello everyone, I'm Alex Evans. I work at Bank Capital Crypto. We talk a lot internally about applications and infrastructure and sort of the ways in which they interact, and we're particularly interested in ways in which those interactions might change, particularly with modularity being a key driver of those. So we want to share some of those ideas, the hairbrained or otherwise, with you all today. And I'd say for most of my time in the space, the nature of the interactions between applications and infrastructure have been mostly harmonious, with some key exceptions, and some of them are sort of listed in this slide and coincide with the end of the last two respective bull markets and remarkably sort of parallel stories where right at the end of a cycle, an application draws in a lot of excitement and fervor and interest, crashes all the infrastructure, a bunch of people get frustrated, we get some cool scalability ideas out of it, the application developers go off or want to launch a new chain. And I'd say that this sort of four year, four or five year transition here occurred while Ethereum got meaningfully better as infrastructure in the meantime. Right? The block limits, unlike some other blockchains, increased at roughly the rate of Moore's law, actually a little bit higher with burst limits given EIP 50 59, which was a major improvement, roughly forex reduction in call data costs on a relative basis as well.
00:01:27.366 - 00:02:44.026, Speaker A: And that's even before things like the merge, the surge, the splurge stuff that sort of started happening later in 2022. But I'd say qualitatively, the nature of interactions between applications and infrastructure during this period didn't change, right? So applications had a wholesale choice to make when choosing what infrastructure to deploy on. And what I mean by that is you embrace all the constraints as well as all the positive aspects of infrastructure by deploying on there, or you choose not to, right, and you can choose to deploy on Ethereum or Solana or both, or some combination or launch your own chain, but that's sort of the type of interaction that you have as an application developer with the underlying infrastructure that you deploy on. And we think that's about to change. And we use the term better here, not necessarily better, just there's qualitatively different types of interactions that we think will be available to both infrastructure and applications that are enabled by succinct proofs, and in particular the horizontal scalability that succinct proofs in different forms enable. So I include underneath that things like data availability, sampling, optimistic systems, snark based systems, and so forth. But just to make this idea very, very concrete.
00:02:44.026 - 00:03:26.302, Speaker A: I'm going to go through two general examples that also are a little bit of audience pandering, and that I know a lot of people in the audience work on one or the other of these two things, or maybe some combination. So I'll use snarks as an example in which this horizontal scalability of infrastructure enables new types of applications. And then I'll use data availability sampling as an example of a changing the interaction between apps and infrastructure in a qualitative way. You could swap these two, but I just want to make it concrete. So I'll just go through each one in turn and just demonstrate the principle. So starting with modularity, ZK. And by the way, I made these slides on the plane over before I spent the last two days with ZK content.
00:03:26.302 - 00:05:10.254, Speaker A: There's more ZK content today, some tomorrow, so I'll go through these relatively quickly. But I think most people at this point, even just in the last two days, have seen sort of this diagram, right? And realistically, when I was first looking at the space, you would see these papers, there'd be these entirely monolithic constructions, for the most part, at least from where I was sitting, where it was like, hey, here's my snark, or stark, and it's fully featured, and it's better than this other thing in the literature, asymptotically or concretely, or uses fewer assumptions. And please accept my paper into your conference. I'd say over time, and in particular more recently, you'll see things that come out that focus on just one of these components. What are these components? You start with a program written in some high level language, compile it through a front end to a set of constraints, use an interactive oracle proof to reduce that to checking some evaluations of different functions and commitments against some commitments that approvers made, and using sort of a functional commitment scheme or polynomial commitment scheme, and maybe Fiat Shamir, you produce this short proof that you can circulate around a p to p network or post on chain or whatever, right? So the point is, researchers, developers, can just focus on one of these components and achieve material advancements of the state of the art, even by advancing one of these subcomponents of these, right? And then these get reassembled back into more general frameworks. And I'd argue something like that happened roughly in the 2019 to 2021 era, where things like Planck came out and high degree gates became a thing. And lookups and interesting advancements in polynomial commitment schemes, they sort of got reassembled again.
00:05:10.254 - 00:06:04.986, Speaker A: Accumulation in the sense of halo, and ultimately swapping out plank and creating the framework halo two, which a lot of people use. These are general purpose frameworks that combine the modular component improvements into general frameworks that people can use. And it sort of marks a transition between, at least when I was looking at the space 2018, most people using some sort of gross 16 variant, but circuit sort of marvel computer science, ten years leading up to it, or maybe even more. Right, but circuit specific trust it set up to what people call more universal architectures. In the case of Halo or plonky, two risero things like that. And we think this sort of architectural transition has enabled two fundamentally different things. The first one is a transition from roughly more specialized architectures to more universal architectures.
00:06:04.986 - 00:06:35.754, Speaker A: I don't just mean this in the sense of like, you don't need to do circuit specific, trusted setup. I mean this in the sense of very concretely, people are building ZKE evms out of it, implementing an instruction set of the EVM inside of that's provable, or. I think I lost the slides. Okay, thank you. While they also boot up the light node in the meantime. Thank you. So anyway, the.
00:06:35.754 - 00:07:12.994, Speaker A: Oops, let's go back and I think we're missing. Okay, never mind. All right, so yeah, the key idea is this transition to people building risk, five provable risk, five chips and so forth. It's a very, very concrete zk wasm or whatever. We sort of are going from sort of more microprocessor bases. Now, these things did exist in the Groh period, but fundamentally, the way, if you look at zkvms, they utilize these modular components like not just recursion, but lookups very extensively under the hood and so forth. So that's kind of something that's been driving new types of vms and so forth.
00:07:12.994 - 00:08:06.774, Speaker A: We think the more interesting thing that's been enabled in succinct proofs has been recursion. It has enormous economic implications for the type of infrastructure and the types of applications that exist thereafter. And again, if you look at something like halo two, or you look at plonky two, you look at a lot of these sort of more second generation universal proof systems. They roughly have the performance on a single machine of something like a 1970s computer. Right? But recursion fundamentally enables you because it's proving, it's very parallel, to add lots of machines and as a consequence be able to amortize the cost of compute over a larger number of users. And so the analogy that I'm roughly drawing here is in the 1970s, like the types of applications and services that made sense in computing were mainframe applications. Hence the system 370 analogy here that I'm loosely trying to draw.
00:08:06.774 - 00:08:41.422, Speaker A: But compute is really expensive, but you can amortize it over a large number of customers in the enterprise. Right. And if you look at most of what's happening in ZK from, in terms of what's getting funded and what people are excited about, it's mostly selling succinctness in some form. In the case of roll ups or things like, a lot of these, by the way, are things that have been founded, and most of these have been found in the last two years or something. Some of them are a little bit older, especially on the roll up side. Right. So the ability to add more machines and horizontally scale without increasing trust assumptions has enabled sort of this renaissance of more applications that sell succinctness.
00:08:41.422 - 00:09:55.238, Speaker A: And this has kind of been, I'd say, the area of the largest growth in ZK in the last few years. So this includes things like bridges and coprocessors, integrated, what I call integrated roll ups that build both, quote unquote, the processor and the roll up, but then also things that take modular components and assemble them kind of in a way that an upstart pc manufacturer maybe in the 70s would have done, right? Using op stack or taking a chip from risk zero or something we happen to work with as a portfolio company for disclosure, and then using it to build some ZK roll up where before it used to take $10 million to build one of these roll ups, or maybe more, these frameworks have made it a lot easier to just assemble them as a service. So I'd like to contrast this with what's happening on sort of the client side of the market, where you can't really take advantage as much of horizontal scalability, add more machines, because fundamentally what you're selling to the client is privacy, usually. And so you're much more limited in terms of how much you can take advantage of recursion capabilities. Right. And so as a consequence, the types of applications you want to run are fundamentally things that you'd be comfortable running on a 1970s computer. Right? Roughly.
00:09:55.238 - 00:10:54.986, Speaker A: Again, the 70s hardware analogy is maybe a little bit drawn out too far here, but the idea is that application specific architectures were kind of still more useful at the time. That said, there are really interesting examples of interesting applications that people have been trying that take advantage of these new capabilities, like the attested mic that Anna and Kobe did. These things take advantage under the hood very often of things like lookups and so forth that are relatively novel capabilities in these systems. People are doing experiments in identity and shared global state with private client side information, and a whole bunch of interesting experiments are happening. But we think fundamentally we need more vertical scale to enable more interesting and expressive applications on sort of the quote unquote pc non mainframe side of the market. Here's just a couple of examples of strands of literature that are producing crazy advancements continually. These are not all compatible with each other yet, and I won't go into each of them in depth.
00:10:54.986 - 00:11:37.770, Speaker A: There's cool things on error correcting codes, there's really cool things on FFT free iops that work with Planck and customizable constraint systems. As obviously people have heard a lot, presumably about all the advancements in folding and a whole sort of strand of literature spending the last year, and big table lookups that allow you to do a bunch of pre. All these things are the only thing I want you to take away from this slide is people are sharing, are turning like square root things into log things, like they're shaving log file in a lot of more mature fields. These would be breakthroughs. Now, agree. These are like asymptotic and algorithmic things. It's like, concretely we'll have to see, and they're not fully compatible with each other.
00:11:37.770 - 00:12:47.530, Speaker A: But very often the combination of these things, once you are able to, first of all, they are becoming more compatible. Once you combine them, they often become greater than the sum of their parts, as we saw in the case of Halo two, planky two. It's been the history of the ZK space, also more generally, the history of computing. So we think what's likely to happen, this is aspirational at this point, this is not real. There's a transition upcoming if 2019 to 2021 is a guide, that these components then get reassembled modularly, get reassembled into general purpose architectures again, and that these universal architectures are then more performant than what we've seen and aspirationally, that have sort of the Macintosh taking us into the 1980s, which I've talked to some people, are working around plonky three, and they can run roughly at this level of performance, maybe a little bit slower than that. Something that is a chip that's like fully compatible, for instance, with a high level language like rust. So maybe that takes client side ZK into the 1980s, which is where pcs and so forth, and more client side things start to really take off.
00:12:47.530 - 00:13:15.194, Speaker A: Maybe. But hopefully it'd be fun if it did happen. Okay, let's switch gears and talk about data availability sampling. And one of the ways that we talk about this internally is as abundant, high assurance, unappinionated bandwidth. The word abundant should be clear. There's just more of it assurances. Nick just gave a great talk on how we gain assurances by sampling fully downloading data as a way of getting assurance as well.
00:13:15.194 - 00:14:19.120, Speaker A: Right, but maybe the point to motivate a little bit more here is this notion of unappinionated bandwidth that's available to roll ups. And again, the paradigm that we've been in in terms of how applications scale and how infrastructure scales, infrastructure updates. Solana today is better than Solana last year, hopefully next year it's even better. And then sort of applications get this choice of like, what infrastructure would I deploy on? And Ave famously had the strategy deploy on a bunch of different evms, have a bunch of horizontal deployments in a bunch of different places, so you don't miss out on users and usage and so forth. Of course, if we take web two as any guide, these are just stereotypically customer focused companies from web two that I pulled up. And of course, all of them take some advantage of integration in the vertical stack. So Netflix with open Connect, Amazon doing fulfillment, apple integrating into Apple silicon, these companies, as they scale, they have very strong opinions about how the customer interaction should work.
00:14:19.120 - 00:14:48.454, Speaker A: They don't like to import other people's or third parties opinions about how their interaction should work. They have their own. And so it's quite likely at least, that some applications, maybe not all, want to take advantage of more vertical flexibility. And importing the opinions of the infrastructure doesn't necessarily accomplish that for you. So, specifically, what I mean is, right now, Solana has a lot of bandwidth available. The way you take advantage of that is you launch an application on there. Now, this could change very soon.
00:14:48.454 - 00:15:48.854, Speaker A: And I think a lot of people are pushing to change this. It's fairly easy to do. Ethereum is becoming more unapinionated in the way that you use call data instead of use blobs. And so we're becoming less opinionated in the use of bandwidth overall. And then the other thing that's happening is the, and I promise, Nick, this slide, what's happening is the supply of unappininated bandwidth is increasing pretty rapidly in the next few months and maybe a few years, where, again, if you look at Ethereum's out like 100 kilobyte blocks and roughly the block time, you're roughly at like a 56k bit modem from 1996, and you're about to go to fairly more modern broadband connection. Of course, interesting applications come out of transitions like this, like web two coming out of similar transition to broadband within a few years of that. But we think what's fundamentally more interesting, and by the way, there's more vertical scale that can come from lots of improvements that are planned along these things, like protodank sharding to dank sharding.
00:15:48.854 - 00:17:01.426, Speaker A: There's really cool things in how you do client side decoding, really cool peer to peer and networking problems, and how you discover people that have samples in the network, cool things, and how you prove that you've done an encoding. All these things improve more vertically. But what's much more interesting to us, and Nick sort of put this much more eloquently, I think, than I did in talking about user facing wallets and applications running light clients, is there's a different qualitative interaction between apps and infrastructure in this paradigm, which is an application could garner a lot of usage and excitement, and that could lead to more security for the underlying infrastructure by drawing in more people running light nodes. And this is different than something like cryptokitties making the infrastructure worse for other applications, at least temporarily. Right. You could draw in a lot of usage and actually increase the assurances that everybody else in the network is getting, and maybe even allow you to increase block size and performance over time. So that's a much more, we think, virtuous interaction between applications and infrastructure, where the infrastructure can scale horizontally as the application chooses how much vertical integration to do over time.
00:17:01.426 - 00:17:50.540, Speaker A: Something similar, I guess we noted as well, in the case of recursion and snarks, you can add more machines, but the level of assurance doesn't decrease, and that enables more virtuous interactions between applications and infrastructure. And so aspirationally we think this is the paradigm that allows infrastructure to scale a lot in terms of capacity, but applications to have a lot more control over the stack that they ultimately deliver to the end user. And we're very excited to see what types of applications and what types of interactions between applications and infrastructure emerge from this paradigm. Are we doing questions or. I did save some time for. I think we had like a minute? No? Okay, thank you. Thank you.
