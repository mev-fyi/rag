00:00:01.600 - 00:01:18.594, Speaker A: Before we get into like, what is arbitrum? What is a roll up? Let's just define kind of like what the state machine looks like. So in the arbitrum state machine, or better said, in the arbitrum protocol, you have two entities, users and contracts. And these entities submit messages to an inbox, and then the protocol reads these messages one by one. So we'll do like a thing here. And then after finishing, after finishes reading all the messages, it generates some outputs. Right? More specifically, what makes arbitrum, arbitrum is really this part right here. And this is my best impression of the previous arbitrum docs.
00:01:18.594 - 00:02:20.764, Speaker A: So essentially, users and contracts, again, they send messages. The chain reads these messages from the inbox one to one. It just pops it from the queue, executes each message one by one, changing the state of the chain, and that produces some set of outputs. So this is the very simple explanation of what is overturned. But how does this translate into how this becomes a roll up? How does it let you use the same tooling as Ethereum? How does it let users of Ethereum that have tokens on Ethereum interact with it? How do you go from this very simple diagram to the idea and concept of an l? Two, to talk about that, we have to get into a couple more details. Yeah, arbitrum is a protocol for rollups. Um, but for the purpose of this talk, we're not going to talk about what is a roll up.
00:02:20.764 - 00:04:18.874, Speaker A: Um, you can go follow John Charv and read his articles on that one. Now let's talk about what makes Arvitrum go from this idea that we mentioned before of like a very simple state machine that reads messages from an inbox, um, into a roll up, and how it allows all these different things, right? So there's actually four big ideas, right? When it comes to arbitrum, the first big idea is sequencing. Oh wait, sequencing followed by, uh, deterministic execution. I'm not gonna write the whole thing. Then the second idea is geth at the core, right? Geth being go Ethereum, right? The most popular node software that's used to run an ethereum client or to communicate with ethereum, the chain. The third big idea is execution. How do I represent this execution separate from proving? And then the last big idea is optimistic, optimistic protocol with interactive proving.
00:04:18.874 - 00:05:46.736, Speaker A: And we're just going to write IvG being interactive verification game, right? So for, so now, like, okay, what does this mean? Like, why should I care about this? And where does Celestia come in into these ideas that essentially make up arbitrum arbitrum, right. The first one is kind of self explanatory once you look at a diagram of how it works. So let's get on to that. So what does it mean to have sequencing followed by deterministic execution? It just means users submit transactions to a sequencer. These transactions get ordered, right? And then the execution of these transactions is deterministic. Right? So if I am running a sequencer, right, like I am the arbitrary sequencer, and I am reading these messages from the inbox, executing them one by one, and generating some outputs, any other node in the network is capable of essentially following what I did, reading messages from the inbox, executing them one by one in their local machine, and realizing, oh, this is the canonical state of arbitrage from the l two. So to draw that into perspective, you have transactions just out there in the wild.
00:05:46.736 - 00:06:55.794, Speaker A: You know, they're unordered, and these get sent to the sequencer, which then does two things. The first thing is that it sequences the transactions into an order list. And the second thing is that it batches them and compresses. Batch and compress. So we're just going to do batch and compress. So it takes the data from these transactions, batches them into like a blob of data, and then uses brotli compression to compress them at some predefined setting for compression. Then something else to know is that the sequencer gives soft guarantees of finality.
00:06:55.794 - 00:08:25.174, Speaker A: That's why if you ever and interacted with arbitrum one, if you use metamask, you send a transaction. The confirmation that you get when interacting with something on arbitrum is almost instant, because the sequencer essentially has already taken your transaction, consumed it, order it, generate an output, and before it has to hit finality, it's able to essentially give you a reasonable guarantee that it will land on Ethereum and that your transaction is indeed. Finally. Now, from these order lists of transactions, you essentially take them through the state transition function and essentially changes the state by running the transactions one by one like we talked about before. And then those outputs generate the l two block, which then gets settled into Ethereum, right? So then we go here. So your l two blocks settle on Ethereum, you batch and compress the data for your transactions and post it on Ethereum, right? This is basically what the first big idea looks like. Again, sequencing followed by deterministic execution.
00:08:25.174 - 00:09:29.614, Speaker A: Any full node in the network is able to read these transactions from the inbox, execute them in order as they were sequenced by the sequencer, and determine the canonical state of the l two block. We'll get more into how do they know it's correct, and what do they do if you, if the output that they calculated turns out to not match what the sequencer posted? Right. But for this big idea, where Celestia comes in is essentially in this piece right here. So a normal arbitrum roll up batches and compresses the data and publishes that to l one. Obviously that costs gas, fees, costs a lot to publish all this data to Ethereum. And as mentioned in the nitro white paper, you can actually just not publish this data on Ethereum and make it available somewhere else. Right.
00:09:29.614 - 00:10:44.934, Speaker A: The nitro whitepaper does talk about the idea of using a data availability committee. That's what they call anytrust, which is essentially a nitro chain with the data availability committee and some other extra techniques that they use. But in the case of the integration with Celestia, we take this concept as described in the nitro whitepaper, and what we do is we take that data and we publish it in my best rendition of the Celestia logo. So we publish the data to Celestia instead of Ethereum. Now, the key here is like, how do you know the data is available? How can I still comply with this idea with big idea number one? The answer to that is your favorite blob stream. Both the relayer and contracts themselves. Blobstream living in some contract on Ethereum and also having a relayer takes the batch of data that was compressed and sent to Celestia.
00:10:44.934 - 00:12:01.294, Speaker A: And then once that batch of data makes it to Celestia, it makes it into a block in celestia, which then Blobstream relays the block header for whatever height our batch of data was posted. And then in these smart contracts for our roll up on Ethereum, we can actually check was our data included in Celestia or not? And then we can essentially know whether a batch was not fully legit, but we at least know that it made it to Celestia. And we'll get onto how this portion right here interacts with fraud proofs in the next ideas. But that's big idea number one. It's basically what demonstrates how simple it is in theory to integrate celestial with nitro. There is no changes to how the sequencer works or how transactions are ordered, what the state transition function is. That's all up to the nitro protocol or the roll up developer to define.
00:12:01.294 - 00:13:20.694, Speaker A: We really only care here about what we do with this data and how we verify that it was indeed included in Celestia and how we use it in the fraud proof system. Now, big idea number two is geth at the core. So if we've had to divide the arbitrum software into pieces, the first piece is the node software, the second piece is Arbos, and then the third piece is geth. Right? So very important three words, node, argos, and geth. That's really like how you can separate the different components of the software. Like if you go and check out the nitro codebase, while it's not perfectly separate into three folders, there's really three functionalities. Node, Arbos, and Geth.
00:13:20.694 - 00:13:59.364, Speaker A: Obviously arbitrum, the off chain labs team, didn't write Geth. They're using Geth as a library. They imported into the nitro repo with some very minimal modifications to the block header so that it can essentially comply with the roll of protocol. Um, arbos kind of functions like the glue, uh, as the glue, um, of the system is what connects the node software with Geth and, and how it allows you to have this like roll up protocol that uses the same tooling as Ethereum. Right. Not just AVM, but also RPC calls. Right.
00:13:59.364 - 00:14:38.604, Speaker A: Um, and Arbos is also responsible for a lot of the, uh, well, not a lot. I think all of the logic behind calculating how much gas should my arbitram roll up charge based on how much gas it costs to publish that same piece of data into the Ethereum L one. Right. And that really is just idea number two. Like Geth at the core. Like Geth is a core component of the Arbitram software. It's part of this like stack, right? And it's reused into, it's reusing the protocol multiple times, right.
00:14:38.604 - 00:16:12.740, Speaker A: As a dependency, you just import geth and you use the headers, or you use some logic to calculate gas, et cetera. And it's what allows it to work, in my opinion, very seamlessly. Now, to get into idea number three, you can actually use the same image to depict idea number three, which is execution separated from proving. What it means to separate execution from proving is essentially that actually these three components, or both, the entire stack come together and they are compiled into the native binary, which is used for execution. So every time you send a transaction to a node, every time you run a sequencer, or every time you run a full node, you are running all of this software together, compiled down to native to essentially connect to the sequencer, receive the feed of transactions from it, or read them from the l one, execute them, see the output, etcetera. And with the same stack, you actually also do the proving, but it's actually separate from native. So you take Geth.
00:16:12.740 - 00:17:12.622, Speaker A: And notice how I did this like ugly red square that kind of cuts arbos into half. That's because the nitro software doesn't take all of Arbos, but it uses some very small components of arbos and takes Geth, which is just a dependency, and compiles these both into WaSM, which is what you use for proving. Now. Oh, another thing. This is what is famously known in the docs as the geth sandwich, even though geth is not in the middle. But anyways, so where does Celestia come in here? Right, we have this kind of like defined stack. You know, we gone through idea number two, which is that geth is a par, a core part of the arbitrary protocol and of every nitro rollup.
00:17:12.622 - 00:18:12.844, Speaker A: And we've also discussed how the same stack is compiled in two different ways to essentially separate the execution task of the roll up from the proving task of the roll up. Right? So where Celestia comes in in this picture is actually in both components. So in the node, we actually use Celestia node through OpenRPC to ingest the. Well, not ingest. We take the batch of data from big idea number one. We take this batch of data, compress it, and then in the notes software, which is part of our native binary, which is responsible for execution, we take that data, right, and publish it to Celestia. We publish the data to Celestia.
00:18:12.844 - 00:19:12.134, Speaker A: And that gets us some data back, right. It lets us know what height our blob was posted. And from there, we also fetch some other important pieces of data for the integration, such as like shares length and blob index, shadow, slava. And we also fetch some other stuff from Celestia core, which is mostly related to blobstream. Like, we fetch Dataroot inclusion proof, which we may or may not have time to talk about it, but whatever. And you can see the rest of sort of like the interface or the block pointer on the code base, but that's essentially what happens on the native side. And we take that back.
00:19:12.134 - 00:21:22.024, Speaker A: And actually, to go back to big idea number one, because I missed something real quick, is that while we publish the full batch of data, the batch and compressed batch of data into celestia, we do publish a small pointer of or, well, a small struct that essentially allows anyone that's looking at Ethereum to know where to look in Celestia to find our data, right? That's what I mentioned before, you have your height and then you have some commitment and then other stuff, right? That's used for blob stream and other stuff. So then we go back here, we have that for Celestia, for the native side and the execution side. And while there's no celestia code per se, in the proven binary, there is a modification or an addition to the proving binary called the pre image oracle. Note I mentioned addition that was already there. Not going to go into fully explaining what a pre image oracle is, but it's essentially the way in which the wasp prover can statelessly read some piece of data from Ethereum based on a hash. That same idea is used in anytrust and we actually used the same idea to take this data root and through the use of the pre image oracle, which is just a very, in my opinion, it's just a very fancy way of saying I have a mapping that maps hashes to their preimage. Then we have essentially code that uses this idea in the proverbial to then take the data root and give us back our blob.
00:21:22.024 - 00:22:59.350, Speaker A: And that's basically how we're able to have fraud proof support for nitro. There's way more details that I could go into. It does get a little bit more complicated. That's the core idea. You have this new prematureical, right? Like there's a Ethereum prematureical and any trust premature. And now there will be a Celestia pre meteorical that has the task of essentially taking one of the data routes that was posted in the Ethereum smart contract, it takes that, it takes the data route, right? And because in the native binary we've already read information from Celestia, there is an extra job we do here where we essentially do the job of Celestia again, quote unquote, in the sense that we take these responses and we calculate the SHA 256 of this data again, but inside of our preimage oracle, which actually is also present in the native code. But we take this to essentially hash from the data routes to the row roots and column routes to only the shares for our data, such that once we are in the proving step, we can take the data root, right.
00:22:59.350 - 00:24:01.512, Speaker A: Um, and then we can essentially take row, uh, we can take our rows and our column roots. Um, and one cool trick here in the prematurical is that we don't hash the whole, we don't, we don't hash the whole square, um, because we already know exactly from posting the data on Celestia where our data lives on the square. We actually only rehash the rows. Um, in this case, assume that our data is in both, in, in both row one and two of, uh, of the square. Um, but if this square were bigger, right, if, if it had four rows and our data was only in row index zero and index one, we only hash row zero and one. And then essentially with the pre image oracle, we get our shares of data. I'm running out of space here.
00:24:01.512 - 00:24:34.764, Speaker A: But we take our shares of data, right? And boom, we have a blob. And then the prover can go ahead and do its job, which is use this data and prove over it right. Now. Lastly, big idea number four is optimistic protocol with an interactive verification game. There isn't much to draw here. Like optimistic. Well, there's a lot to draw, but I'd rather get into questions.
00:24:34.764 - 00:25:47.812, Speaker A: But let's just try to speed run this real quick. You have optimistic roll up, right? That means you post data to ethereum and you then essentially assume everything's fine, you're optimistic. But in case the sequencer lied, somebody here, in case the sequencer is bad or it gets even more extreme once you consider multiple parties attesting to a data route. But for now, we'll just keep it one to one. There's some party that's evil here. They've done something terribly wrong, right? But there's someone here, someone out here, God bless them, they're running approver. And essentially they're because of, because of these big ideas, not just big idea number three and four, but also big idea number one.
00:25:47.812 - 00:26:31.254, Speaker A: They're able to deterministically follow the chain from the contents of the inbox. So they're able to take the contents of the inbox, apply the state transition function, which is known to everyone. And they say, hey, this guy right here is bad. So I submit a fraud proof. Well, not I, this good person submits a fraud proof, saying, I have proof that this person didn't follow the protocol, basically. Meaning if you take all these messages from the inbox and apply them correctly, you arrive at state one instead of state zero, which is like our current state. And obviously the bad person is trying to get away with it.
00:26:31.254 - 00:27:46.774, Speaker A: So, um, the way you solve this, usually sort of fraud proof. Now, the difference in arbitrum, um, from other optimistic roll ups design, optimistic roll up designs that were out there before, is that the, the first ideas for optimistic roll ups require people to re execute transactions, um, which is very expensive. I'm not going to get into the whole, you know what, what does that entail, what are the economic attacks and optimistic roll ups? We're not here to talk about that. But anyways, that's how people used to think about things. That has limitations. So then arbitrum decided to do an interactive verification game, right? Which is part of big idea number four, interactive, proving. That means if we have some set of blocks, like roll up blocks that we want to go over, what happens when somebody, when this dispute occurs is that there's essentially a match between the, between the fraudster and the defender.
00:27:46.774 - 00:28:32.698, Speaker A: And what they do is essentially they do this interactive game off chain where they say, okay, they do a bisection, meaning if this sequence is of length n, then somebody starts by saying, well, I think actually you can find a fraud between this block and say, this block right here. I know this is not even, but whatever. And then the fraudster has to respond. If they don't respond, they get slash. So they have to follow with the protocol. They have to follow the game if they want to try to get away with it. So then they say, okay, between these three blocks is actually in this one, in this range right here.
00:28:32.698 - 00:29:23.810, Speaker A: And then we ultimately arrive that it's at this block. And then inside of this block, there's a set of, like, transactions, right? And we kind of repeat this same game again until we find the transaction. And then lastly, inside of that transaction, we have a bunch of, like, opcodes, right? And what we do there are not opcodes. They're correction, they're wasm instructions. In the arbitrum wasm virtual machine, we bisect over that transaction itself. And then we say, okay, this instruction right here was incorrect. And then this is followed by a one step proof, which is essentially just like a fraud proof.
00:29:23.810 - 00:30:51.232, Speaker A: It shows, like, like your typical re execution fraud proof. So you only have to re execute a single instruction. Like, instead of having to re execute a set of transactions or a single transaction with arbitrum, you only re execute a single instruction to verify whether the end state is correct or not. We verify this fact, right? And then this guy gets slashed and loses money. Now, where Celestia comes in here, how does this tie to our previous ideas? So the TLDR is that without the data being available, this entire game, this entire process just can't happen, right? Like, the bad actor can get away with, um, with being fraudulent. Because ultimately, if we don't have the data, right, if we don't know where it is, if we can't prove that it's been made available, how do we know that these two parties and by extension, um, in future iterations of the protocol, how do we know that the group of fraudulent parties, um, are incorrect and that the group of fraud proof submitters are correct? Right? Like how do, how do we know who actually is correct? You kind of need to essentially have the data for that. It needs to be easily available so that anyone can read and follow the protocol.
00:30:51.232 - 00:32:45.884, Speaker A: And that basically just connects back to both big idea number three and four and how Celestia integrates with the native binary as well as the wasm binary for proving. Like the Wasm binary for proving is what allows us to essentially read the data using the WasM proving software to arrive at this conclusion, because it allows us to unpack the celestia data route into the data that we'll use for that. And additionally, and lastly, the way in which Celestia interacts with that last idea of like optimistic protocol with interactive proving is that in order to start a challenge, the message has to be pushed into the inbox. Contract on Ethereum L one. And what we do here is that we only accept compressed batches of data or, well, we only accept compressed batches of data that have a pointer for which you can take the data in the pointer, meaning you can take the data root to high etcetera and make a data root tuple route and then verify on the Ethereum smart contract of your sequencer, which is connected to the blobstream smart contract, that the data was included or not, right. If the sequencer even tries to like submit a batch of data with a pointer that just has like dummy data, right, it has incorrect data or it has data that just simply doesn't exist at all. The contracts on Ethereum for the roll up or contracts on your settlement layer would actually do this verification so that you have a strong guarantee.
00:32:45.884 - 00:33:51.236, Speaker A: You essentially have the guarantees that Blobstream gives you that that data route was attested to by a majority of celestia validators. Does it safe to unpack the Celestia data route from Blobstream into your data into your blob, which then allows you to play this whole game and figure out who's right and who's wrong. Celestia, like for the celestial integration, there really isn't any changes to this other than the aforementioned pre immaturical. So really we don't change anything in the protocol or how the different steps in the big idea number four work. There have been some talks on how to do some slight modifications on this last idea, not in the protocol itself. We don't want to change how nitro works, but essentially, there's a whole set of rules defined by smart contracts on how these challenges are started. Right? Who can play next, etcetera.
00:33:51.236 - 00:35:19.346, Speaker A: And currently, the approach that we take, as I mentioned, is essentially you have to wait for the data to be relayed from Celestia into your settlement contract to then be able to verify some batch. Right. And that's not great, essentially what? Or, well, it does the job. It works, right? We achieved optimums like our friends at l two beats would like to say. But the problem with at least the current approach is that if I am the sequencer and I'm trying to post the pointer to Ethereum, I know I can also, through my geth client, read the contract and I know, oh, this data hasn't been relayed. So if I try to submit this transaction for this current batch, it will get rejected, even though it's valid, because there's essentially a delta in time between when the data was posted on Celestia and when it gets relayed as part of the span of data routes into blobstream. So essentially, the sequencer has to hold back on the batch until the data route for the batch has been publish on Blobstream, which adds, like, essentially minutes to the finality.
00:35:19.346 - 00:36:37.344, Speaker A: Right? Like, obviously your hard finality for optimistic roll up is seven days right after the challenges has finished, but you're now adding, like, an extra drift that might actually be unnecessary. The reason I say might be unnecessary is because based on some talks that I've had with Leet Arbitram, there are ways in which you could say, hey, this whole challenge, right, it won't happen, or, sorry, this whole challenge can't start if the data route has not, if the data refers that batch has not been relayed to the blob screen contracts. Right. And there's some other sort of, like, thoughts there and limitations and like how we can essentially improve all of this so that you can still, you know, without changing any of the core ideas, you can actually reduce finality and only verify batches in blobstream if a challenge is started instead of the current approach, which is don't even add this pointer until blobstream has relayed it. Right. Which adds some long drift to finality. But, yeah, that's pretty much it.
00:36:37.344 - 00:36:43.344, Speaker A: If there's any questions, I will take them right now.
00:36:45.084 - 00:36:53.204, Speaker B: I guess just to clarify my understanding of arbitrum and nitro, are they a Zk roll up framework or an optimistic roll up framework?
00:36:53.364 - 00:37:17.536, Speaker A: So arbitrum is an optimistic roll up framework as describing big idea number four, which maybe I didn't do the best job at, but oru, right, optimistic roll up. And my understanding is that they're not doing any sort of like hybrid ck thing arbitrum does optimistic roll up and.
00:37:17.680 - 00:37:21.496, Speaker B: Yeah, yeah, I guess I was just confused there.
00:37:21.600 - 00:37:23.674, Speaker A: No, all good. All good market.
00:37:25.814 - 00:37:40.794, Speaker B: So arbitrum and Opstack are both role frameworks on top of Ethereum. Right. So why. And maybe this is also kind of a question to Java too, but is there a reason we couldn't use blobstream for op stack as well to kind of simplify the maintenance of rstacks?
00:37:42.054 - 00:38:33.214, Speaker A: Great question. So optimism lacks two words. All right, fraud proofs. So optimism could use blobstream in their current setup without fraud proofs. That doesn't really add any extra security, something that we've talked about with other folks in the space. And essentially you don't really need to verify. So the whole purpose of blobstream, from the point of view of Ethereum aligned person is that blobstream allows me to stay on Ethereum, use geth, and read from this contract and know this data was posted on Celestia.
00:38:33.214 - 00:39:12.634, Speaker A: But that fact isn't really useful without fraud proofs. Because ultimately what happens on optimism is that you post a data route or not a data route, you post a state route. And because there's no fraud proofs and there's no challenge, that's just it. So like there's no real need to know that the data for that batch was included. You could add it to essentially give users peace of mind. It's the same way that like when you have an op stack roll up that uses a DAC, you have signature verification and it's like, oh, okay, well, my DAC signed over the data. That's cool.
00:39:12.634 - 00:40:27.350, Speaker A: That doesn't improve anything. But you could add blobstream to the op stack implementation to achieve that same, I guess, niceness. Hey, yeah, it was posted on Celestia, but ultimately the roll up without fraud proofs can still be malicious to you in one way or another. The difference with arbitrum being that it can protect you against those attacks because of fraud proofs. One thing that I did forget to mention is that usually blobstream, well, so blobstream has those functions to submit a data route and then also verify that data was included on Celestia through the use of that data route through a miracle inclusion proof that is actually kind of expensive to perform if you were to perform that for every batch, naively you would still save gas compared to just posting to Ethereum. But the cool thing about optimistic roll ups with an IVG is that you only really need to know. Sorry, you only really need to know that the hash was published on Celestia.
00:40:27.350 - 00:41:06.454, Speaker A: Because then through the use of the pre image oracle, the parties involved in the verification game can essentially walk down the tree and know, okay, based on this length of my shares and blob index, where does my data live? Does it point to anything useful? Does it not? You hoist part of the data verification into the interactive verification protocol instead of doing it on chain, which ends up saving you like gas. Right. That was, that was kind of a tangent, but yeah.
00:41:07.514 - 00:41:20.050, Speaker B: So, I mean, I think it's ultimate. It's more of like a user choice of like whether they want to use op versus arbitrary versus the technical challenge on our side.
00:41:20.242 - 00:42:38.094, Speaker A: Yeah, of course. I mean, I think ultimately there's reasons to launch either rollup, I guess, or with either stack. But ultimately a lot of the stuff isn't so much about the operator, it's about the user who wants to use the stack. I feel personally safer putting my money on arbitrum one than putting it on optimism because, you know, their social reputation at stake and both of these things. But at least one of them has this sort of protocol that is supposed to guarantee, or it's supposed to have certain guarantees around, like what happens if somebody tries to commit fraud. Right? So ultimately, when people decide to deploy arbitrum roll ups or nitro roll ups with Celestia da versus an op stack chain with Celestia da, it's more of a question of do you want to give users this idea? Not idea. Do you want users to get this feeling of safety, or do you want to just have a roll up and hope that users come over just for the vibes?
00:42:40.634 - 00:42:44.518, Speaker C: One last question, or I have a question.
00:42:44.606 - 00:42:45.234, Speaker A: Yeah.
00:42:46.054 - 00:43:05.154, Speaker C: Can you explain in the pre match Oracle verification, where does shale length and blob index and data route, how are they relate to the pre made oracle? And does it verify the data route? And if it fails, what happens?
00:43:07.174 - 00:43:26.146, Speaker A: Good question. It's not that they get relayed. So there's, there's actually, it all kind of just connects together. Oh my goodness. So if we go back to like the beginning, right. There's an inbox. Right? It's just this like queue that exists.
00:43:26.146 - 00:44:16.170, Speaker A: Right? It's not just. It's actually not just like some random idea that Ed Feldman came up with. It's actually also part of the protocol itself. It's like in the software. So the sequencer itself, like the node software, has an inbox, right? But that inbox actually just tracks the inbox in the smart contract. And in our case, like in the case of the celestial integration, that inbox has this blob pointer posted to it. And through the use of like arbos, right, the node software essentially has calls that allow you to fetch contents just from the inboxes.
00:44:16.170 - 00:45:26.380, Speaker A: Like arbitrum has a state machine, and there's instruction in the state machine for arbitrum that's called read inbox, and that allows the provers to read from the inbox on l one. And when they read from the inbox for a batch number on the l one, for a nitro Celestia chain, they're essentially reading the previous pointer that we talked about. And that pointer includes all that information that we care about. And through the use of that pointer and the data route, we unpack that into the shares. The question now to answer the, the rest of the question is like, well, how do we know that data root is correct and all that stuff? Well that's actually what we get from Blobstream, because the prover will only read things that are on the inbox and we only add these pointers to the inbox for a given batch if they can be verified on blobstream. If they can't, if the verification on Blobstream fails for the data route in this, for the data retool in this pointer, it gets discarded. So that already gets over.
00:45:26.380 - 00:45:52.294, Speaker A: The fact of that already eliminates the need to verify or look against garbage data in the software because you essentially have this assumption that everything that you read from the inbox comes from this source of trust that you're reading from, which in this case is celestia through blobstream. Right? And yeah, hope that made sense.
