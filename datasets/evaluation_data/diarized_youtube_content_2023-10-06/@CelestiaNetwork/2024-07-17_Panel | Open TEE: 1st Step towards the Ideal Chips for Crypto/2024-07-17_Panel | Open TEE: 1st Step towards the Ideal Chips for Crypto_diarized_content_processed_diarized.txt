00:00:01.840 - 00:00:32.690, Speaker A: Everyone here is interested in hardware. You know, I think one important thing about hardware and cryptography is there have been many known amazing attacks over the lifetime of custom hardware for cryptography. So why don't we start by you introducing your favorite attack ever against a hardware cryptographic system. Could be an HSM, could be a Te, could be, you know, people listening to your memory via obscure ways.
00:00:33.630 - 00:01:00.258, Speaker B: I'm just going to say my favorite one is APIC leak. And this is a bug that's not even, not even a hardware bug, it's just a software bug. And it's not even an especially devious bug. This was a bug affecting Intel SGX, this was a couple of years ago, and it's just a software bug. It's like you load some data, you're supposed to erase it afterward. And it didn't. I'm saying code, but this is like micro code in Intel SGX.
00:01:00.258 - 00:01:34.030, Speaker B: And so it just left un erased code data when switching from the secure process out to the untrusted attacker controlled os processes. So it's actually just one of a whole bunch of security vulnerabilities that have affected SGX, and then you find out about it after they've gone through the long disclosure process, and then patches to your bios are there. This was the only one that I played around with and paid attention to, but also realized that it was just one and a sequence of other kind of related ones. But that's the one I know best.
00:01:37.050 - 00:02:33.736, Speaker C: I am a little less schooled in the list of security vulnerabilities. I guess the one I know the best is Spectre and meltdown. But I think those illustrate a really interesting point, which is these server class processors like Intel Xeon CPU's are really designed for maximal performance. So they have things like branch prediction, they have things like speculative execution, they have things like even basic stuff like out of order processing. And really all of this serves to increase performance at the expense of making the tax surface exponentially larger. And I think it is really telling and a great parable for crypto to come together and create really a crypto native processor that has far less attack surface and may not be as performant. But hey, you could use this in settings where ZK and fhe may be currently, or in the end still impractical.
00:02:33.736 - 00:02:38.020, Speaker C: And so this is like something that I'm really interested in.
00:02:39.560 - 00:03:42.090, Speaker A: Yeah, this is something I've thought a lot about, because I always love reading all those proof of concept papers that are like, hey, we took 500 microphones around this particular device and we just listened to what it was. We just did some simple Fourier analysis and then, oh, we figured out the memory allocator, whenever it's actually signing, makes this particular tone. And then that always leads to weird, crazy HSM stuff like the HSM machine randomizes some of the fans and noises that exist and stuff like that to try to counter people trying to spy on you by putting microphones near your hardware. Now let's maybe talk a little bit about the two domains you both are working in. So Andrew, you're a consumer of a lot of hardware that people are making, mainly intel, maybe Nvidia. On the other hand, you're working on building AsiCs, building custom hardware. You've done that in a previous life as well.
00:03:42.090 - 00:04:23.170, Speaker A: How do you think about the end user? From the perspective of you, Andrew, you're the end user of this hardware. Theoretically, you're really the end user. No offense to the final user, they're just pressing play or submitting something on metamask versus you. As you're designing hardware, thinking about the users, the interface, the trade offs you make between hey, we can have a really efficient chip, but the programming model is too hard, and then people get really annoyed and stop using your stuff. How do you think about these two directions? How much do you care about knowing the low level guts? And how much do you care about knowing the actual end user?
00:04:23.510 - 00:05:01.724, Speaker B: I definitely care. I've only been able to have as much fun with trusted hardware because of pretty much the flexibility. And you can run legacy code that wasn't meant to run on Tes in particular, but you can make it work with a little bit of effort, make it work, port it basically to make it work in SGX. But I realized that that is also why it's so complicated. SGX has to run all of the X 86 instructions, so full flexibility and functionality of the existing chip. No extra overhead is tolerated. It has to basically be about as fast as running without SGX at all or people wouldn't use it.
00:05:01.724 - 00:05:44.010, Speaker B: So all of the goodies that make it run so fast, like all the pipelining and super scalar and speculative execution stuff, all of that's present, that's why they're able to present oh, you can use this and there's so little overhead. So it's great from the perspective of ease of porting it and making it work, but it is not. Clearly there's so little room for margin to add security features there that it's a lot of complexity. And so I wonder if maybe that's still a good way to get an introduction to it. But I could imagine that really the most secure solutions would be something that's a little bit lower on the functionality side and higher on the security side.
00:05:44.950 - 00:06:20.176, Speaker C: Yeah, absolutely. And to navigate any trade off, you have to know the other side of the equation. And so we care a lot about knowing our end users. And I think it's kind of a mistake that hardware companies often make that many hardware companies don't care about the end user. They won't know exactly what apps are running on their hardware. They'll optimize maybe a performance metric like specant. So if you start a new cpu company, typically there's some funny little Fibonacci type integer benchmark that you're like, okay, I got this many dmips per megahertz, and the other guy got this many dmips per megahertz.
00:06:20.176 - 00:06:50.750, Speaker C: And so I'm better. And really, this is what we set out to do at fabric, which is build a really mission oriented hardware company. So we absolutely care about the end user, not just the end user, but the end developer, you know, their experience, their ability to use this hardware to actually accomplish their goals and the community. Right. Like, we care about the community values. And those are built into kind of the trust assumptions, the security assumptions that we want to be able to build into our hardware. So, yeah, I think it's really important.
00:06:53.090 - 00:08:12.338, Speaker A: And, you know, I think there's always kind of this fear, I think, in the average developer's mind who is not using cryptographic hardware, cryptographically secure hardware, wherever, you know, there's always many ways to shoot myself in the foot. There's lots of ways I can leave keys in places they shouldn't have been. There's lots of ways I could, you know, accidentally leave, you know, do the simple thing of like, not caring about the, you know, to Andrew's point yesterday, taking private state and accidentally leaving it somewhere public and basically revealing things without much sort of kind of care. So because you have all these extra considerations when writing privacy preserving programs, there's a lot that goes on the end user's mind. What are some of the biggest mistakes you've seen people make, both at the hardware level where it's like, hey, someone optimized some pipeline, but in the process it leaves a copy somewhere. So now anyone who could read that gate couldn't figure out a lot of things, versus in the software side, bugs are people just kind of leaving a lot of state around in a way that causes issues. Because I think understanding these ways of shooting yourself in the foot is actually really important.
00:08:12.338 - 00:08:31.500, Speaker A: Also figuring out if it's preventable. Obviously all these mistakes are preventable. If I knew exactly the cause and the end. But which ones stick out to you? Which ones are the ones that you think are the worst types of mistakes that developers make.
00:08:31.800 - 00:09:10.442, Speaker B: There's a bunch I could go into. The first one that comes to mind is just ignoring side channels and not either noticing them or thinking that they don't affect you, or not taking the right mitigation to avoid them. The most stark examples of these are what I called spicy print offs, which was when trusted hardware based smart contracts would access an encrypted key value store. But the key value store is off the chip, it's outside the enclave. You're at least encrypting the keys and encrypting the values. So those are just ciphertexts. Are you familiar with Penguin mode encryption? You know what I mean by that? An encrypted key value store has to work by.
00:09:10.442 - 00:10:10.380, Speaker B: You store the value, even if it's with an encrypted key, to get it the value back when you want to read it. Next time you have to use the same key that you use to read back that value. So you can tell when the same value is being requested a bunch of times versus all different random values being requested. And so I think this comes from just not keeping in mind, like the interface between off the enclave and within the enclave. That's an example of something you just have to be aware of when you're considering what your program is doing when it's running in a trusted hardware enclave. And the thing that's most been on my mind recently is about to what degree even memory accesses can leave some kind of trace if the untrusted operating system out of the enclave in SGX can tell, or can set up page faults such that it can tell when you access a piece of memory. The way that I've started thinking about it is when your enclave process runs, it's not fully isolated, it's streaming a redacted debug log.
00:10:10.380 - 00:10:26.720, Speaker B: That's my mental model for it. That's how I imagine all of the information that could be extracted by watching side channels. So you kind of have to understand what that trace would look like and determine whether or not it's bad for your application and you can mitigate it. So that's the first one that comes to mind?
00:10:27.660 - 00:10:56.716, Speaker C: Yeah, absolutely. I think in the world of hardware, obviously it's, you know, easy by default to make a mistake and very costly at the same. The flip side of that is you have a good group of hardware people. They're really used to this, and they're almost just as paranoid as cryptographers, but in another way, in another direction. And so these are kind of the two flip sides. Like there's a lot of formal verification in the world of hardware. There are logical equivalence checks.
00:10:56.716 - 00:11:41.402, Speaker C: There are layout versus schematic checks, which kind of show you this gate that you wired up to. This gate exactly matches the schematic that is the net list of gates that are supposed to be connected. So there are a lot of automated tools. That being said, these tools are also a way to screw up. We've seen a lot of cases in many, many projects where you need to know how to work with the tool to use the most reliable parts of the tool. For example, in electronic design automation, the tool will quite frequently have a bug and insert a gate where it was not supposed to be or something like that. And then knowing which parts of the tools are even mature is a surprising thing that hardware engineers have to do.
00:11:41.402 - 00:12:25.512, Speaker C: So there's really a lot of things that hardware people keep in our minds. But I also think that this means that in designing tes, if we just actually have security as a mission and trustworthiness as a mission, then in the short term we can create some pretty good tradeoff points on the tradeoff curve between performance and security. It's just that most companies don't have the luxury of having this as a goal. And uniquely, if you're building for the crypto world, you do have this. So that's the really awesome thing about crypto, is it's a way not just to secure crypto, but a way to get more security, more privacy out there in the world by default.
00:12:25.636 - 00:12:33.220, Speaker B: Suppose you built a chip, give it to me, and you wanted to convince me that there was no backdoor in it. How would you go about doing that?
00:12:33.600 - 00:12:40.800, Speaker C: That's a great question. I think that I would have to spend a few hours thinking of a solution.
00:12:40.920 - 00:12:42.540, Speaker B: No, you have 30 seconds.
00:12:43.240 - 00:13:26.568, Speaker C: I think there are ways that you can target the most obvious vectors for backdoors, but ultimately it will come down to things like people looking over other people's shoulders as they use the tools. It'll come down to things like open source RTL. It'll come down to those semi social schemes where more transparency is ultimately a really good way to do that. Then ultimately, one thing that I'm a big supporter of in the long term is destructive analysis of chips, something that no traditional hardware company will do. But a crypto native hardware company must consider this as one of the things. If they're building tes, how big of.
00:13:26.584 - 00:13:38.340, Speaker B: A tool do you need to do destructive analysis? Could it fit on a stage? And it's something that could be done at a conference, or it needs an actual. I don't know. You have to take the conference to the destructive analysis zone.
00:13:39.440 - 00:14:14.740, Speaker C: You may have to take the conference to the destructive analysis zone. The tool might fit on stage, but it might require large amounts of nitrogen gas and all sorts of weird things that you wouldn't have on a stage. But I think this is like one of the, let's say, utopian visions that we have that one day there might be some sort of ceremony around this, and then everyone might have, I don't know, eat CC or SPC there, and then everyone can see it happen. I think that's the level of public scrutiny that you would really need to have one of these things be trustworthy.
00:14:16.320 - 00:14:56.986, Speaker A: Yeah, that's a funny idea for a religious ceremony. Every time you do a tape out, it's like we tape out and then we blow up the chip, and then we pray to it or something. But let's talk a little bit about supply chain attacks. I would say that a hardware supply chain attack when I worked in ASIC development is why I got into crypto a long time ago. So I have seen some very bizarro supply chain attacks. There's two types of supply chain attacks, of course. There's the hardware ones where if you're a fabless company, you don't own your own fab, you rely on a bunch of suppliers and vendors along the way.
00:14:56.986 - 00:15:22.388, Speaker A: You rely on them executing your verification correctly. If you're outsourcing that, you rely on a lot of stuff. Talk to us about supply chain attacks in the hardware world. And then, of course, in the software world, there's a lot of supply chain attacks that can also show up. So it'd be good to kind of compare and contrast and hear the difference between, you know, pushing bits and pushing pick, pushing bits and pushing atoms. In terms of supply chain attacks, I.
00:15:22.404 - 00:16:25.758, Speaker C: Think ultimately there isn't too much of a difference in that, you know, in both cases, you're swapping out components and you're basically introducing new backdoors or new side channels or new kind of bits of leakage in the hardware. I think that maybe hardware supply chain attacks are more easy to detect in certain cases and difficult to detect in other cases. For example, there are imaging tools that are super high resolution in the hardware world. So you can just take a look at the parts and do maybe a random sampling of parts and then you can do these things. Whereas in the software world, I think the use of fuzzers is less of a thing than the use in hardware of those kinds of tools already. So I think the introduction of things like computational imaging in the hardware world, a good friend of mine runs one of those kinds of companies where they have this big machine that inspects things for defects. The primitives for this kind of analysis are already widely available and already of developed.
00:16:25.758 - 00:16:37.050, Speaker C: So that's like one nice thing about hardware, I guess. The other side is geopolitics, of course. So you have to watch out for that kind of stuff. But it's just like taking these into account and being careful about things.
00:16:37.670 - 00:17:00.558, Speaker B: It's a tough question because I mean, my first reaction is, well, software supply chain security, that is an issue. But every software supply chain issue is just, it was avoidable. Like someone should have been checking the software updates, but it was unmaintained or the maintainer lost their things. And then people upstream that could have checked the software don't check the software. I mean, but maybe that is still, you know, naive because, okay, these are very complicated.
00:17:00.614 - 00:17:15.246, Speaker A: What about the XZ? When I think about supply chain attacks of recent, I think about the XZ attack, right? It's like in very malicious bytecode injection, only discovered because someone accidentally cared about their booting latency, I was like deliberately obfuscating.
00:17:15.358 - 00:17:18.286, Speaker B: Yeah, that'd be very hard. I mean, so it's, that's the hardware.
00:17:18.318 - 00:17:22.102, Speaker A: Ones I feel like are adversarial, though they're also like more, and as much.
00:17:22.126 - 00:17:29.170, Speaker B: As the XC, you, hardware is software and hardware, it's strictly harder. And yeah.
00:17:30.950 - 00:17:47.330, Speaker A: When you think about designing, especially when I think about something like MeV, where all the participants, all the end users are also adversarial. So clearly they would want to be adversarial on the supply chain side. How do you think about that when you audit and think through your design of the orl system?
00:17:48.640 - 00:17:49.560, Speaker B: Which one was that for?
00:17:49.600 - 00:17:50.296, Speaker A: Sorry for you.
00:17:50.368 - 00:18:25.130, Speaker C: For me? Yeah, I think it's a lot of techniques that you already use. Like, you know, sometimes people buy parts from places that have a lot of fake parts and then, you know, you have to detect those already. Side channels are a little more difficult. But if you're buying, like by and large, most of the parts on a piece of hardware, like won't leak that much information or marginal information, like for example, resistors and stuff like that. So you're really only talking about a couple very critical parts. And I think that because of that, it might be cost effective to find solutions to doing this at scale.
00:18:27.710 - 00:18:45.166, Speaker A: And I guess maybe given that we're kind of running out of time, final couple questions are, what are the applications you most want to see running in trusted hardware, and which ones are, are the ones that you kind of view as your sort of. If I were building it to dog.
00:18:45.198 - 00:18:53.142, Speaker B: Food, I want the Ethereum machine, not the Ethereum virtual machine, because that's virtual. I just want hardware. Ethereum contract execution.
00:18:53.206 - 00:18:59.250, Speaker A: Look at you. You're going to take an old enigma and shove a little machine underneath it.
00:19:01.710 - 00:19:47.160, Speaker C: I think what I'm really excited about is some of these applications that have actually been presented today. You know, I was looking at ZKP to P, it's things like ZK Venmo. I was looking at web two to web3, data bridging. These are all really, really compelling use cases for confidential compute as a whole, where you can actually kind of attract users from more traditional markets and really bring on new users to crypto and really introduce new ways of improving ux into the crypto world. I think that's really important, you know, for identity, for credit scores, for uncollateralized lending. Risk models. Like all of these things actually depend on a large amount of confidential compute, and I'm really excited about that.
00:19:48.820 - 00:19:52.860, Speaker A: Cool. Well, with that, maybe a round of applause for our panelists.
