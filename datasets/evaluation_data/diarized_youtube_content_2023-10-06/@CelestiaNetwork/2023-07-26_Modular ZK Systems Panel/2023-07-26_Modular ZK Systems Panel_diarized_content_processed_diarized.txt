00:00:01.370 - 00:00:25.346, Speaker A: Cool. Well, what a panel I have here. This is so awesome. This panel was inspired by an episode Benedict and I did recently in that we talked about the different parts of a ZK system. So this is going to be a pretty technical panel. I think we're going to go pretty deep in, but I think. Let's start with introduction.
00:00:25.346 - 00:00:38.970, Speaker A: So I introduced myself earlier, but I'm the host of a show called Zero Knowledge. Yeah. And I also am curating this afternoon of ZK through the ZK validator.
00:00:39.950 - 00:00:47.850, Speaker B: Hi, I'm Nico and think. I'm a researcher in cryptography, mostly applied cryptography, geometry.
00:00:48.430 - 00:01:04.066, Speaker C: Hey, my name is Benedict. I'm the co founder and chief scientist of Espresso Systems. Work on decentralized sequencing. There's a whole nother track for that. The other one, actually. But I also dabble in zero knowledge proofs. Hey, there.
00:01:04.088 - 00:01:13.810, Speaker D: I'm Zach. I'm the CEO of Aztec. We're a privacy infrastructure provider for web three. And, yeah, I also dabbled in zero knowledge proofs.
00:01:14.390 - 00:01:27.240, Speaker E: I'm Christopher. I'm one of the co founders of the Enoma project. I am not a classically trained cryptographer, so I'm probably horribly out of my depth, but perhaps I can provide a perspective on how these zero knowledge proof systems look like from the outside. We'll see.
00:01:27.610 - 00:01:40.780, Speaker A: Very cool. Oh, yeah. So I wanted in this intro. So before we dive in, I do wonder if you could choose sort of one work paper project that you think most defines you that people here may be familiar with.
00:01:42.830 - 00:01:43.580, Speaker E: Sure.
00:01:44.030 - 00:01:51.022, Speaker B: I think I'll go after Zach, actually, it makes more sense in that the work I'll talk about probably builds upon the one he's going to mention now.
00:01:51.076 - 00:01:53.280, Speaker A: Okay, why don't we actually start from that side then?
00:01:53.810 - 00:02:13.490, Speaker E: Right? I have not written any zero knowledge proof systems, sadly. I hope to someday. But a few years ago, I was starting to go into the space, and there just seemed to be a lot of brilliant people writing zero knowledge proof systems, and I figured we need people doing something else. So maybe in ten years when I'm, like, off in a cabin, I'll do that. But probably the most popular paper was IBC. Unfortunately, I think the paper is terrible.
00:02:13.570 - 00:02:14.566, Speaker A: Oh, no. Yeah.
00:02:14.588 - 00:02:18.586, Speaker E: I want to rewrite it to be clearer, but maybe I'm just biased after the fact.
00:02:18.688 - 00:02:19.340, Speaker A: Cool.
00:02:20.510 - 00:02:28.150, Speaker D: Yeah, I guess. Yeah, plonk. I guess. That's the thing. That's what I'm known. Yeah, plonk.
00:02:28.310 - 00:02:43.762, Speaker A: And does everyone know what plonk means at this point? Who here knows what actually, who here knows the Planck proving system. Yeah. Okay, so people know the proving system, but nobody knows where that word comes from.
00:02:43.816 - 00:02:57.990, Speaker D: Amazing stands for permutations over the Lagrange base for ecumenical double, the non interactive arguments of knowledge. And the a is silent.
00:03:00.010 - 00:03:01.442, Speaker E: Some creative liberty.
00:03:01.586 - 00:03:06.034, Speaker A: And the word plonk also has a double entendre. There is a definition.
00:03:06.162 - 00:03:16.822, Speaker D: Yeah. It's british slang for cheap, low quality wine. Because, like, a bottle of bad booze getting to the bottom of plonk is going to give you one hell of a headache.
00:03:16.886 - 00:03:19.370, Speaker C: Is it like a bottle or are we talking like bag?
00:03:19.710 - 00:03:24.122, Speaker A: Oh, it's box of box wine. Is box. Okay.
00:03:24.176 - 00:03:24.726, Speaker C: Box wine.
00:03:24.758 - 00:03:25.340, Speaker A: Okay.
00:03:26.690 - 00:03:40.980, Speaker C: Yeah, I guess, probably still, I'm most known for bulletproofs, which is a zero knowledge proof that is used in Monero and many other different proof systems. I think Zcash and IO, actually, as well, are parts of it.
00:03:41.590 - 00:03:57.350, Speaker B: Yeah. Not as known as these two cryptographers. But I did put out, let's say, an observation building up on plonk that some techniques that we saw elsewhere in the ZK stack were applicable to plonk. So that thing is called sangria, fittingly.
00:03:58.010 - 00:04:35.250, Speaker A: Very nice. Okay, so I wanted to start this with a little bit of a very kind of high level history of snark systems. I think these names may be familiar for anyone who's been following the space. And what I'd like to do is kind of go through them. And any of you can actually weigh in here, basically let us know what was the big change or what part of the stack was identified and optimized as we went through each one of these things. I'm going to start. And, Benedict, you can tell me if I'm wrong here, but I'd start with, like, gross 16.
00:04:36.150 - 00:04:50.290, Speaker C: Sure. This is, like, starting in 2016. We can also start in 2012 or. Well, I was going to say, like, 1980 something, but, yes, let's start with practical, approved systems.
00:04:50.370 - 00:04:57.638, Speaker A: Okay, so we're starting with graph 16 still. I mean, it's still a system that's used a lot. Right. These libraries were really well developed.
00:04:57.814 - 00:04:58.634, Speaker E: Very much.
00:04:58.752 - 00:05:10.160, Speaker B: And it's still the system with the shortest proofs that we know of. So that's sort of why it still has a place in people's heart, because if you're trying to post a proof on chain, that will be the cheapest way to do it.
00:05:11.330 - 00:05:39.500, Speaker A: Yeah. And just a note here. There's a ton of proving systems that came out in 2019. What I'm trying to identify are the proving systems that we just got, like, a lot of mind share around. They kind of won that round, in a way, and then you kind of see those be used for a few years, and then a bunch of new ones come out. One of those kind of becomes the standard in a way. This is, like, subjective, but the next one I have is plonk, actually.
00:05:39.500 - 00:05:48.026, Speaker A: So, Zach, maybe you can help. What was the big innovation? Or what was the part of the ZK stack that it.
00:05:48.128 - 00:06:34.986, Speaker D: Yeah, the key innovation with plonk was what it enabled was practical, universal ZK starks. So one of the big downsides of GOSS 16 is you need to do a trust of setup ceremony for every single circuit you make. And so we wanted to preserve the nice, succinct polylogarithmic properties of elliptic curve based snarks, but one where you didn't need per second trust setup. So it was integration of sonic, which itself was iteration on bulletproofs and a lot of other things. But the innovation was really, it was a way of efficiently validating copy constraints. So that was kind of the big bottleneck with universal snarks. It was, how do you verify that all your gates are wired up correctly? So we had an efficient way of doing that.
00:06:34.986 - 00:06:51.706, Speaker D: And then the effect of that also meant that a nice way of arithmetizing snoxokes kind of fell out of that, which was then turned into kind of the plonkish arithmetization.
00:06:51.818 - 00:07:00.850, Speaker A: And this is where people were optimizing on what you had done. But the thing they were focused on was this arithmetization part, like they were changing.
00:07:03.030 - 00:07:33.340, Speaker D: Basically. So the idea is with plonk, one of the nice things about plonk is that the algebraic expressions that you're checking on every single gate nicely map to the overall proverb and verify algorithms that are being run, which means that you can then construct relatively complex custom algebraic statements specifically for a system that you're building your snoc for. And so, yeah, that kind of took off in a big way.
00:07:35.250 - 00:08:26.458, Speaker C: There's many different ways to look at the innovations of systems. But I think one of the realizations sort of that came out in that time, and where Plonca, Sonig and Marlon and Dark and others were sort of a big part of is this modularity, right? Like the separation. So the separation in this particular part into kind of. I have a computation, and then there's something called a polynomial iop, which is basically reducing it to some polynomial checks. So I just checked that some polynomial is, at some point is evaluated to, I don't know, zero. And this basically allows you to sort of pull apart these monolithic proof systems into two components. One of them is this arithmetization component.
00:08:26.458 - 00:09:05.210, Speaker C: Something like plon, something like Marlin. Or there's others, stark like air, I guess, would be the other one. And then the polynomial commitment constraint. And now you can plug in different pieces for these different things. So you can plug in for plunk, you could plug in KZG, but you could also plug in some hash based thing like fry. And the nice thing is. And sort of, there were also form ethereums about this that basically, if you take a secure thing from one side and I take a secure thing on one side, I get, like, basically, the plugging together works, and this gives you a new proof system.
00:09:05.210 - 00:09:35.170, Speaker C: So, for example, now we have halo two, which is plonk plus, like, a bulletproof style, polynomial commitment. Or we have, I think, plonky two, which is plonk plus fry. And I'm sure we have, like, air plus KZG. And there's basically a bunch of these things. And I think this separation is what I would view as the sort of realization of that time that then enabled innovations like plank.
00:09:36.870 - 00:10:05.594, Speaker A: Yeah, so you mentioned sort of the next stage that I would say had a lot of mind share. Although I do feel like it's just part of the ecosystem. But it was like the halo work, and then the halo two work, especially got a lot of mind share. A lot of people building on it. You had just said that it was using something like plonk, had its two defined parts, and then you could take out one, put in another. Did Halo two also introduce any other places to all of a sudden break off into another module?
00:10:05.722 - 00:10:26.374, Speaker C: Okay. To be honest, I don't really know what Halo two is anymore, because it's like Halo two, to me, is one of the no offenses. Anyway, it's very confusing because it's a library. There's a paper halo, then there's, like, halo two. There's also what people think it does and what it really does. It's an arithmetization language. So it's many, many different things.
00:10:26.374 - 00:10:29.990, Speaker C: So it's, like, also names. Just, like, terribly confusing.
00:10:30.570 - 00:10:31.830, Speaker A: We're really good at that.
00:10:31.900 - 00:11:05.486, Speaker C: I think what I actually view Halo and Halo two quite separately. And what halo two does, it's really a variation on the plunk arithmetization. Like a really nice way to sort of encode the plunk arithmetization in this column format with coming with a library. A very good, seemingly good, and widely used library, like cryptographic library. With it. Do you agree?
00:11:05.668 - 00:11:19.010, Speaker D: Yeah, broadly, I think maybe I would add one other thing, which is that I think one of the things that Halo two really pioneered was basically the use of cycles of non pairing friendly curves to enable recursive precomposition.
00:11:19.690 - 00:11:27.350, Speaker C: That's just not implemented in Halo two right now. This is like the really confusing thing. Everybody thinks this is implemented in the library. It is not right now.
00:11:27.420 - 00:11:51.658, Speaker B: So I think the confusion comes from the paper that came out from the interesting Zcash people. So I think it's Dara Shanbo. And I forget there's a third author on the paper. And that paper sort of describes this. Yeah, thanks. That paper describes this idea of how can we get cheap recursion using cycles of curves. So maybe that is what people refer to as halo.
00:11:51.834 - 00:12:18.200, Speaker C: Yeah, there's the halo paper, which is about recursion, which is another interesting area, but it's not implemented in halo two. Maybe right now, maybe in the future it will be. But this is why I'm like, this is why this grinds mergers, because it's just like so confusing these things. Because cycles of curves, which is what people think halo is, which is what halo is, is not implemented in halo two.
00:12:21.850 - 00:12:47.940, Speaker D: One of thing which complicates matters, which I'm certain that I've culped in this as well, is that all these proving systems are also kind of attached to brands. Well, they become brands for the companies that the people inside this companies are political handed inventing them. And so I think that also has a big effect on the confusion aspect as well.
00:12:50.950 - 00:13:14.934, Speaker E: Yeah, I was just going to ask because I was curious in sort of sounds like you've looked at the Halo two library and there are 15 different forks. Every time I go to a crypto conference, I learn about two new Halo two forks. And at least one of them implements something I want. And I didn't know about it. It's a big counterparty discovery problem. But I'm curious. In the modular stack, we see clearly separate roles, like data availability, solving, execution.
00:13:14.934 - 00:13:25.418, Speaker E: It sounds like they're kind of two now. Like the first arithmetization part and the polynomial equipment part. Do you think there'll be two forever? Like, is this the final decomposition?
00:13:25.594 - 00:13:39.266, Speaker B: I'd wager already at three, where you have your arithmetization, then docking pot is something that your polynomial IOP can deal with, and then you can use whatever polynomial commitment scheme you want. So it's already, I think, like a.
00:13:39.288 - 00:13:41.810, Speaker D: Three layered thing, and then you add.
00:13:41.880 - 00:13:45.490, Speaker A: Folding for a fourth layer and lookup tables for optimization.
00:13:46.870 - 00:13:48.866, Speaker B: So those would fall.
00:13:49.048 - 00:13:55.878, Speaker A: Is that more of a technique? I have all of this. This is actually my next question. Maybe let's finish our history. But I'm getting into that.
00:13:55.884 - 00:13:59.106, Speaker B: Well, I guess it is the next type of the history after sort of plugging.
00:13:59.138 - 00:14:32.546, Speaker A: Well, you're right, because obviously the next one that a lot of people have been talking about for the last year is the Nova work, which also leads to the hypernova and protostar work, which introduces this technique of folding or accumulation schemes, which I've always understood as being deeply built in. But it sounds like, does it change one of those three that you just described? Is it in polynomial iops? Is it in the arithmetization? Is it in the polynomial commitments? Is it a sub?
00:14:32.728 - 00:14:57.914, Speaker B: So, similarly to how graph 16 was like this monolithic thing, and we started to take it apart. Yeah, I think this technique started from a very specific application. So in the halo paper, they were like, we are using this bulletproof style binomial commitment scheme, and we can actually defer some of this to later. And then, slowly but surely, we're picking threads out of this and got this to be very, very generic with proto star super.
00:14:57.952 - 00:15:07.230, Speaker A: Recently, I will also say, I know, Benedict, you had done work that sort of did this before Nova, but described slightly differently, am I correct?
00:15:07.380 - 00:15:11.322, Speaker C: Yeah, we just didn't give it a fancy name, so it didn't get any. That's.
00:15:11.386 - 00:15:15.186, Speaker A: But you did do bulletproof, so you knew. You knew.
00:15:15.208 - 00:15:40.380, Speaker C: That's less. No, you got to give it fancy names. No being facetious. So the most important separation for me is sometimes it's called front end and back end, where front end is what the developer interfaces. This is how you code up your computation, how you express your computation. Right. This could be.
00:15:40.380 - 00:15:52.906, Speaker C: And we have this separation in normal computing as well. We don't need to think about sort of snarks. We can think about your programming language. Right. That is the front end. Right. This is what you write.
00:15:52.906 - 00:16:24.054, Speaker C: And c, c, whatever. And for us, this is something like the halo two front end. Sometimes it's r one cs, sometimes it's like a higher level thing. Right? Like there could be also multiple levels in the front end, like circum or whatever, that then gets compiled down to something below. And on the back end, there is the proving system. And the proving system can also have multiple layers. Right.
00:16:24.054 - 00:17:04.674, Speaker C: It can have, like, as you were saying, the compilation down to a polynomial commitment and a polynomial commitment. I would say that these folding schemes give you a proving system with very particular properties. They especially work for these, or they're designed for these iterative computations. So a computation where you have like a step that you do over and over again, for example, a blockchain, it's just like one block and then another block and another block and another block, that's a computation that you do over and over again. It's a so called iterative computation. And for those we can use kind of these folding schemes or these IBC things. But this is all in the back end.
00:17:04.674 - 00:17:33.114, Speaker C: This is the sort of the technical infrastructure, the assembly, or like. No, it's really not even the assembly, it's the cpu. Right. That does the execution. And again, like CPU, we can have multiple layers in there, we can have the instruction set, we can have all of these things. And then some things fit together, some things don't fit together, and some things, you can build a compiler to make them fit together. This is why this picture looks sort of complicated.
00:17:33.114 - 00:18:28.254, Speaker C: But really, the thing that, the image that I think helps sort of people think about it is just like front end. How do you express your computation? Hopefully in the future this will get easier and easier, and you can literally just write it in, I don't know, rust or python, and then back end. Hopefully those things get more efficient and powerful as we go along, which sort of executes the computation. And both of these, within each other, within itself, can have modularity, which is the cool thing. And the more modularity, I think historically what we've seen is exactly this trend that you just said. Usually sort of the innovation comes like someone looks at something monolithically, comes up with some genius new idea, something like Halo, which is, by the way, a beautiful genius idea. And Halo is more in the nova folding kind of line of work.
00:18:28.254 - 00:18:52.520, Speaker C: That's where I see it. And then people start picking it apart and making it more modular. And this enables new innovation and this enables new sort of techniques. So we should sort of always cherish, I think the name here of the conference and the panel is very good, because the modularity seems to be extremely helpful for both new innovation and for understanding these things.
00:18:54.650 - 00:19:33.140, Speaker E: Yeah, I was just going to ask me one thing. Modularity helps me understand in the distributed systems context is what the hard axes of trade offs are. Right? Like, there's a trade off between privacy, inefficiency, in a sort of counterparty discovery matching sense. There's a trade off between trust and efficiency in a kind of, how much do you need to replicate your verification sense? And these trade offs don't change. Like, you can make the primitives faster, but the trade offs will always be there. They're just properties of how the components fit together and what kinds of properties you want out of them holistically. And I'm curious, does the modular decomposition of zero knowledge proof systems, is it yet, at a point to provide clarity on these kinds of axes, and what are they?
00:19:35.110 - 00:19:38.534, Speaker A: This is like benchmarking a lot of this stuff, I guess. I think.
00:19:38.652 - 00:20:15.250, Speaker B: Not necessarily. I'll say. Sometimes in our case, modularity comes at some cost, where, because we're not looking at things monolithically, we can't open these black boxes anymore. And there are some small tricks and optimizations or things that we could have done otherwise that we don't do anymore. Not to say that we don't do them at all. We do have systems that go in and break the black boxes, but I don't think the modularity itself. Okay, I guess, yeah, it can draw some lines of the trade offs, but it also draws them slightly, somewhat artificially.
00:20:15.590 - 00:21:19.506, Speaker C: But I guess maybe your question is, in distributed systems, we have these pretty strong lower bounds, right? Like, we have some sort of impossibility results that are pretty strong. What is interesting is that in zero knowledge proofs, there's some lower bounds, but not really that many. And that meaningful ones, it's not even clear that proving is necessarily more expensive than computing something, right? Like, if I want to compute a square root, then I need to compute the square root. But if I want to show to you that the square root is correct, I can do this with just a squaring, right? I can go in the inverse direction, which may be cheaper. So it's not even clear. Or maybe proving it, like executing this computation is sequential, but the proving can be done in parallel. As far as I'm aware of, there's some lower bounds, but these lower bounds are usually in stylized models.
00:21:19.506 - 00:21:40.330, Speaker C: And if I go out of these models, I can oftentimes even circumvent these lower bounds. So one of the beauties is we don't have strong lower bounds, but this also means that modularity does. It doesn't show. Oh, here's clear trade offs.
00:21:41.310 - 00:22:06.200, Speaker D: Yeah. Just to further that. Yeah, it's tricky because it will be nice to be able to clearly define a trade off space. But I think if you take a snapshot of the ZK landscape at any one time, then, yeah, you can probably define some kind of trade off space between all the various proving systems and all the major components. But because the low bounds have nowhere near being reached yet, yet you flash for six months and everything you've done your analysis on is now obsolete and it's been replaced by new stuff. That's better.
00:22:07.050 - 00:22:27.050, Speaker A: Justine, I still don't think we've properly placed folding schemes and lookup tables into the where are they in this stack? I've been sort of referring to them as techniques, because I'm just like something you use on top of, but that it's not its own system in its own right.
00:22:27.120 - 00:22:32.830, Speaker D: I don't know if I'm, I can take a step. I think where I would place lookup tables is they're part of the circuit arithmetic.
00:22:34.610 - 00:22:35.466, Speaker A: They're a subset.
00:22:35.498 - 00:23:42.770, Speaker D: Then it's a way of converting a lookup table into algebraic statements just like the rest of the addition, like arithmeticates of your circuit. So I think you can place it at that layer more or less. When it comes to folding schemes, I would say it's at a higher level than the underlying proving system, because there's been some work that the protestar and proto galaxy, basically the actual proving system is left as a black box, more or less. The only requirement is that you have an additive homomorphism between the protostar doesn't even define the use of polynomials, but assuming you're using polynomials, then you have some kind of additively homomorphic commitment scheme for your polynomials. So I would place it at a layer above the underlying proving system, and it's a layer at which if you system, if you need to compose, if you have some higher level architecture that composes multiple proofs together, then you apply a polling scheme on top of your proving system to get that capability.
00:23:45.510 - 00:23:51.430, Speaker A: So could it have its own category? Would you?
00:23:51.500 - 00:23:52.470, Speaker D: Yeah. Okay.
00:23:52.620 - 00:24:21.534, Speaker A: And I guess the reason, like those two that I highlighted, the reason I wanted. So we've mentioned a few of them. Each one of these sort of has a line of work, and there's researchers who are focused just on those things. Something we didn't actually say. And now I'm realizing, I'm not sure I know this at what stage. Like lookups, which I've just brought up, there was up, there had been work before, as I learned on a panel yesterday, there had been some work done before. Mary Mallor was part of some work that was doing that.
00:24:21.534 - 00:24:34.270, Speaker A: But then there was the pluckup was created. But was that like, what system does it go with plonk? Does plonk then come with lookups or like a second generation? Is that the introduction?
00:24:34.350 - 00:24:53.690, Speaker D: That's kind of what ultra plonk was. It certainly wasn't the first lookup table scheme. I think it was the first. Well, at least at the time, I thought. I don't think it was. I think it was the first that had. The access cost of your lookup tables was constant.
00:24:53.690 - 00:25:23.874, Speaker D: Most of the ones before required you, like, the number of constraints in your system that you needed to do a lookup was logarithmic in the size of your table. And Flukup is this one gate. I think there were systems before that did that, but fluke up was the kind of the, if I make the claim that it was the first constant time practical lookup scheme, is that controversial? I'm not sure, but, yeah, I think.
00:25:23.912 - 00:26:16.214, Speaker C: Also that this is another great case where sort of modularity was sort of observed later on, where a lot of these lookup schemes were invented. And they were invented sort of in the plonked land, I'll say that. But most recently, and I think, for example, this was observed in a paper about ccs and other things, is that you can pull this out, you can pull it out and reuse this component in other systems, and it is not tightly coupled with the plunk proving system. That's sort of a very nice observation where, again, we've observed some modularity, and this helped sort of improve the space.
00:26:16.332 - 00:26:43.054, Speaker A: I want to just kind of go over what we've already said, just again, just in case people aren't fully following. So, so far we have polynomial IOP, polynomial commitment scheme, arithmetization, subset lookup tables, and somewhere else in somewhere, is folding schemes beforehand. Does recursion and folding schemes get to live together?
00:26:43.172 - 00:26:43.742, Speaker D: Yeah.
00:26:43.876 - 00:26:44.222, Speaker A: Okay.
00:26:44.276 - 00:26:45.600, Speaker D: Same thing. Ready?
00:26:46.210 - 00:27:19.640, Speaker A: Okay. So that sort of maps for roughly what I had written down here, but it's nice to see it mapped, though. But is there anything else? What else is there? So, going back to your point, as we define these components, researchers can focus in and optimize them and find new combinations, find interesting properties of other parts that we just described that interact really nicely with. Yeah, I don't know if I described that so well, but we're looking for new things. We're on a search now.
00:27:21.550 - 00:27:34.298, Speaker D: I think you can add other layers of abstraction, but I think that's where you'd stop writing soundless proofs and papers. There are plenty of high level constructs, but maybe you could. Well, yeah.
00:27:34.384 - 00:27:37.978, Speaker A: Are you saying we're at the end of all modularization? This is.
00:27:38.064 - 00:27:43.410, Speaker D: No, no, we're done maybe at the end today. But I think Benedict has some words to say on that.
00:27:43.560 - 00:28:21.178, Speaker C: I don't know. I mean, you can go even lower level. Right? So one of the things that this is like another slight rent. It's not a rent. When people start learning about, sometimes I have people approach me like, oh, I want to learn about cryptography. And it's like, oh, but I've been looking at these elliptic curves and they look so complicated, and I got stuck. And I probably, after I wrote I don't really understand elliptic curves, and for at least the first three years of my phd, and I'd written bulletproofs by that, I really did not understand elliptic curves.
00:28:21.178 - 00:29:15.710, Speaker C: And that's totally okay, because, again, we can use abstraction, right? These elliptic curves are just one tool. Like, they're mathematically, they're an algebraic group with a group operation where the discrete logarithm is hard or something like this, right? We can beautifully use abstraction, and you do not need to understand how these other components work. And there's sort of this whole cryptographic layer where, for example, a lot of these proof systems use a hash function, and there's a new hash function coming out every week. And especially there's been a lot of focus on these snark friendly hash functions. So that's another component. Or there's different elliptic curves with different security and efficiency properties. That's sort of another component at the cryptography layer.
00:29:16.770 - 00:30:09.934, Speaker D: That's a really good point. Maybe just to add to that, maybe to try and systemize that a bit more, all of these snark systems, all of their security proofs, boil down to a relatively common set of computational hardness assumptions. And by that, I mean basically, you're saying, you basically prove that the only way for a versery to break a premium system is they can solve a particular problem that we assume is basically impossible to solve. And so, for example, one of the common ones is the elliptic curve discrete logarithm problem, basically saying you can't find the discrete logarithm and leptic curve. There's a whole very low level abstraction of the cryptographic primitives themselves, the actual constructions that you pull those hardness assumptions out of. And yeah, there's a whole other level field of work.
00:30:09.972 - 00:30:23.650, Speaker C: And this, again, beauty of modularization, you don't need to understand it. It's totally fine. Just sort of deal like be happy with abstraction when you're trying to look at things, really be happy and sort of embrace the abstraction, embrace the modularity.
00:30:24.550 - 00:31:01.338, Speaker B: I will say, maybe that's one of the points of friction that I want to see sort of solved. Now it's this friction between the back end and the front end, because all this Zk stack we've been talking about. So that's mostly in the back end. These things change very quickly, and we get better and better very quickly. And they also, like, you're essentially changing the interface, right. That you have with the front end. So there are new things that are available for the front end, and it's really hard to know, like, okay, can I start thinking about a good front end for this yet? Can I start thinking about a good language or a good representation of computation to throw to these backends? Or are they just going to, is the ground going to move under my feet?
00:31:01.434 - 00:31:02.874, Speaker A: Yeah. This is actually to you, Chris?
00:31:02.922 - 00:31:55.886, Speaker E: A little, yeah. I mean, looking at things a little bit more from that perspective as we do, it seems like the really hard problem is dealing with differently sized finite fields. That's like the essence of the problem that we see from the higher level language perspective is that the choice of the field, to me is like an implementation detail that should live in the back end and the front end program should be portable across this choice. So that as there are different interactions between different systems, and the underlying systems change, and certain things become cheaper with small fields or whatever, or you need certain cycles for recursion. These are like details in the back end to me that we want to abstract, but it's difficult because at the same time, in order to get efficient execution, you kind of need to know about this detail when you're writing your programs. So there's this bleed through of something that is really at the very bottom. Like this is the thing that you're abstracting over, right? There's this bleed through all the way to the front end language.
00:31:55.886 - 00:32:10.450, Speaker E: And I have not seen a convincing general approach to translating between just modular arithmetic over different finite fields. All the mathematicians I meet tell me it's really hard. It's like discrete algebraic geometry. Like hard stuff. But I don't know. I'm curious.
00:32:11.610 - 00:33:08.830, Speaker D: I think you hit the nail on the head when you said that there's an efficiency problem there where every abstraction layer has an implicit cost associated with it. And right now, over the last ten years, I'd say we've been building up a lot of abstractionists from what used to be just a core monolithic proving system, like the gloss 16 or BCGTV 14. I can't remember their names, but yeah, the one that's really missing is creating something which is field agnostic. And I don't think it's probably not going to turn up for a while because it's a hard problem. The one way to solve it is you basically, you create a virtual machine. So instead of writing turning your program into constraints for a specific priming system, you turn them into operations over some imagined virtual machine that you then prove in your underlying priming system. And then that assumingly one would then not work in finite fields, one would work in just regular base two fields well over the integers.
00:33:08.930 - 00:33:10.006, Speaker E: Those are finite fields.
00:33:10.038 - 00:34:11.310, Speaker D: Yeah, but we are not well depend. It's far too use case specific, because the slowdowns in that are gargantuan. I feel like often, I think that the history of ZK snarks and ZK proofs kind of tracks similar to the history of computing, in that if you think about where we were in 1990 is similar to where we were in 1936, when Alan Turing was writing papers saying, in theory, we could do this wonderful stuff, but God knows how. And then the very early ZK snarks, they were basically like the very first digital computers using vacuum tubes, where forget about custom programs, your programs, you had to hardwire, rewire your computer in a plugboard to get different programs because it was that low level. And then as the performance improves, you start to layer on more abstraction levels. So you start to get primitive programming languages in the computer space. And that could be translated to creating some of these very basic abstraction layers that we have today.
00:34:11.310 - 00:34:20.960, Speaker D: But we're still early. If you can draw analogues between the path of ZK proof and the path of cryptography, we're in like the 1960s at best.
00:34:22.450 - 00:34:28.766, Speaker C: Yeah, but in the 1960s, we went to the moon, so maybe we can do that again when moon.
00:34:28.878 - 00:34:35.090, Speaker E: If you draw that analogy out, I think the conclusions are pessimistic, not optimistic. We haven't been back to the moon.
00:34:36.150 - 00:34:38.718, Speaker C: Well, it was damn hard and expensive.
00:34:38.894 - 00:34:44.246, Speaker E: Right, true. Are we saying that we're doing something damn hard and expensive that won't be repeated in the future, but we did.
00:34:44.268 - 00:34:49.880, Speaker C: It on the computational power of, I don't know, probably my watch that I'm not even wearing, but. Yeah.
00:34:53.050 - 00:35:17.490, Speaker A: All right, well, I think we've covered most of the questions. I mean, I had one last one, but this kind of in the weeds, you'd sort of mentioned hash functions. And I was thinking like, hash functions, pairing based. Where do we place this? Is this under the polynomial commitment scheme? So, like, you go into the polynomial commitment, fry based, will be using hash function. KZG is using pairing based.
00:35:17.560 - 00:36:14.434, Speaker C: Yeah, it's a technique that is used. No, exactly. I think it's a technique that is used. It's very tightly coupled, usually to the polynomial commitment scheme, like each polynomial commitment scheme requires, is built on a different cryptographic primitives, which then defines a lot of the efficiency. That's the problem that you were talking about, that it bleeds up, right? Because it does define a lot of the efficiency properties, but it even defines to some way, unfortunately, it defines to some way, like how you can express your computation, but it also defines your security assumptions and your trusted setup and all of these things. And then the other thing that I was saying, the modularization, there is just that there's many different elliptic curves and KZG can be implemented on many different curves. And which one you choose.
00:36:14.434 - 00:36:16.626, Speaker C: Again, different trade offs.
00:36:16.738 - 00:36:17.510, Speaker A: All the way up.
00:36:17.580 - 00:36:22.360, Speaker C: Yeah. Then depends, like, oh, what does Ethereum have pre compiled for? Right.
00:36:24.250 - 00:37:03.246, Speaker D: And also the landscape of cryptographic primitives is much more slower moving than the high level construction. It's because these computational hardness assumptions, you can only really get consensus around them over time, because it's not like you can formally prove that a particular computation cannot be done in polynomial time. That's kind of the P equals MP problem. So, for example, pairings were around since the 1980s, elliptical pairings, they weren't used in commercial cryptography software until the mid two thousand s, I think. I believe just because people just didn't trust them. Didn't trust them. It takes about 20 years, but I'm sure we'll see new primitives.
00:37:03.246 - 00:37:08.914, Speaker D: We're going to start to see, I suspect, polynomial equipment schemes based around lattices. Turn it cropping up in ZK, say pairings.
00:37:08.962 - 00:37:10.294, Speaker C: 1980S, I think.
00:37:10.332 - 00:37:11.046, Speaker D: Pairing, what?
00:37:11.148 - 00:37:15.750, Speaker C: You said they were invented in 1980. No, they're like from like 19.
00:37:18.650 - 00:37:23.418, Speaker D: The concept of using a valencia pairing in a cryptographic protocol was 1980s.
00:37:23.504 - 00:37:26.074, Speaker A: Wait, you were about to say something else? What did you just say?
00:37:26.192 - 00:37:51.700, Speaker D: What was I going to say? That we're going to see some new primitives eventually. Although we get machines based around latices are cool. I'm kind of curious about higher level elliptic curves is like one of the most primitive algebraic structures that you can use. There's a whole field of things like Taurus based cryptography, or you can add dimensions on and you can get possibly some interesting properties. Anyway, we'll see what happens.
00:37:52.790 - 00:38:04.610, Speaker A: Yeah, maybe that's sort of the last. So the area of potential improvements or direction you're thinking like lattice, I guess that would open up all sorts of cool new techniques. There. Any other specifics?
00:38:04.690 - 00:38:05.842, Speaker D: Trilingar maps.
00:38:05.986 - 00:38:06.390, Speaker A: What?
00:38:06.460 - 00:38:07.234, Speaker D: Trilinear maps.
00:38:07.282 - 00:38:08.354, Speaker A: Trilinear maps?
00:38:08.482 - 00:38:45.358, Speaker D: Yeah, well, I lived a curved pairings are a bilinear map, which basically allows you to do quasi multiplications. But if you had a trilinear map, then that would open up. Okay, this has been a long time since I've looked into this, and I'm a bit of a diletente in this field anyways, so take this as just as like some guy on the Internet, but I think it would open up the ability to create fhe type constructions with elliptic. Well, with whatever you have, the trilinium map with the same with the elliptic curves are additively homomorphic. I think you can think you can get something which is multiplicatively homomorphic with the trilinium map, maybe completely or just partially.
00:38:45.454 - 00:38:56.834, Speaker B: So I was under the impression that bilinear maps allow you like one multiplication, and if you go trilinary, you get one extra one. So you get two multiplications, which would need probably more if you want like fully homomorphic.
00:38:56.882 - 00:39:59.306, Speaker C: So the thing that trilinear maps, like one of the key, this is outside the ZK space, but really exciting is the thing that trilinear maps, and more generally these multilinear maps give you is obfuscation program obfuscation, which is sort of the uber crypto primitive, because what it means is that basically I can have a smart contract which can have an embedded secret key, and nobody can read the secret key. So, for example, the smart contract could literally store some balance in it, and nobody could see sort of the balance, and it can update it locally, or it could have. I think we had some talks on private state and public state. I guess you talked about this, and with obfuscation, these things would become significantly more powerful. And, yeah, there's really cool things that could happen there. That's sort of the part of the crypto future out there.
00:39:59.488 - 00:40:20.922, Speaker D: Yeah, exactly. In the private transaction world, somebody has to own encrypted state and control it, and they're there. The entity that needs to be able to construct the proofs of computation over that private state, which is kind of a pain. If you need to, for example, liquidate that person, they're not going to make you a liquidation proof. But, yeah, anyway, I could waffle on for ages.
00:40:21.066 - 00:40:40.500, Speaker A: I think we're getting the sign to leave the stage, which is a bummer, because I didn't leave any time for questions. I think I had too many. But you've met these wonderful panelists, and I guess if you have questions, please come join us after. Thank you so much. Thank you to all of you for, thank you very much, panel. Thank.
