00:00:00.250 - 00:00:00.446, Speaker A: All right.
00:00:00.468 - 00:00:37.266, Speaker B: Hey, everyone, thanks for having me. So, yeah, as they said, I'm Josh, I'm the CEO founder of Astria. So I actually renamed my talk Astria's vision for the endgame. I used the talk speed running the endgame previously, so I got the slides in late. But what I'm going to walk through here really is what Astria's architecture looks like and kind of the top to bottom of why we're building the architecture we're building because, you know, as shared sequencers have gained prominence in the last, last five months, since this has even been like a kind of a concept, there's been a lot of questions about why it should be designed one way or another. And so we're going to give that. And so for Astro, we have two kind of key principles.
00:00:37.266 - 00:01:13.486, Speaker B: We think that roll ups should be decentralized by default. This is generally a pushback on the idea of this progressive decentralization, which has been promised by a lot of different roll up networks, but quite frankly, we haven't seen it. We're 18 months into main net on a lot of roll ups, and they're still just as centralized as they were day one when they launched. So we believe that if we want to see a world with many, many roll ups, they're going to need to be decentralized by default. Otherwise we'll have this recurring window of, we launched it, it's centralized, we'll get to the decentralization later and there will perpetually be newcomers that are just centralized. So that's principle one. Principle two is that we believe deploying a roll up should be as easy as deploying a smart contract.
00:01:13.486 - 00:01:39.786, Speaker B: Am I going to get a clock time there or should I just guess? Okay, so we believe deploying a roll up should be as easy as deploying a smart contract. And now I'm just going to go into kind of architecture stuff. I have 53 slides to get to, so I'm going to talk quickly. This is how we view the general kind of transaction thing. You could call this like the transaction order flow, but we put it into kind of four stages. And noting that we are, at least myself, is not in the camp of a roll up being defined by its bridge. So that's just.
00:01:39.786 - 00:02:04.686, Speaker B: You can argue with that. We have a panel after where we can fight, but we believe we start with unordered transactions that could be intense. We go to an ordered block. From that ordered block, we end up with an executed block that has a state DB and presumably a resulting state route. And then after that you can have a succinct proof or a ZK proof. And that is kind of what we're going to argue, the ordering. We'll label these as order flow, sequencing, execution and proving, and view those as four phases.
00:02:04.686 - 00:02:09.462, Speaker B: Again, people can disagree with terminology, but I have to pick some semantic definitions to go with.
00:02:09.516 - 00:02:10.120, Speaker A: Right?
00:02:11.850 - 00:02:18.626, Speaker B: Okay, so state of sequencers today, this is what I'll argue is kind of the most simple case for a centralized sequencer.
00:02:18.658 - 00:02:18.902, Speaker A: Today.
00:02:18.956 - 00:03:05.080, Speaker B: You have an end user, they're going to talk to a roll up sequencer, get a soft commitment there as it's one box that's a centralized sequencer. That sequencer pushes batches to the DA layer. You get a firm commitment for that. What are the problems with that? Why would we decentralize sequencers typo there? Because this roll up sequencer that is centralized has a persistent monopoly on ordering, right? And that's a problem for any number of economic reasons, on transaction flow, on censorship, right? But we don't want to have one party that has a persistent monopoly. And when we say persistent, we mean across all time, right, across blocks. So if we go to a decentralized sequencer where we'll just simplify and say we have a leader rotation. So there's multiple entities, this is still for one roll up, but multiple entities get a chance to be the sequencer for the block, and everything else is the same.
00:03:05.080 - 00:03:37.042, Speaker B: We move to having a per block monopoly on ordering. This doesn't resolve everything, right? We've seen a lot of interesting designs like multiplicity for multi validator like block proposing, but it is, we'll argue, better than having a persistent monopoly across all time. There's various research on MEV things where you can maximize MEV by extracting no MeV in one block and putting all that MeV in a second block. We remove cases like that. When we say there's now a per block monopoly on ordering, you can't guarantee you're getting multiple blocks, or at least it's probabilistic. We'll argue that's better.
00:03:37.096 - 00:03:37.458, Speaker A: Right?
00:03:37.544 - 00:04:12.726, Speaker B: So why shared sequencers? Why would we want to go from decentralized for one roll up and many roll ups, having many decentralized sets to shared. And this is what we're going to posit as like a model for this. You'll still have a user, they'll communicate with the RPC for a given roll up. So this is your metamask to like a guest node or whatever, and it will push transactions to a shared sequencer we implicitly assume the shared sequencer is decentralized here. And so what are we doing here right now? We're shared right here we have multiple roll ups. Users still can interact directly. There's more complicated designs, but for this tonko limit here, where you can have multiple roll up RPCs sharing a sequencer.
00:04:12.726 - 00:04:32.402, Speaker B: So this is our kind of simplistic model. It's very similar to the Espresso guys, kind of more complex, like diagram. And they have a great demo visual on Twitter that you can look at. Come on. Right. So what is a user actually getting from a sequencer commitment? When we say it's like a soft commitment, what are they actually getting out of it? So I really like this quote. I've used this a lot of times.
00:04:32.402 - 00:04:38.846, Speaker B: It's from Nick Carter's article. It's the settlement assurances, stupid. From way back in 2019. It's obviously Nick Carter.
00:04:38.878 - 00:04:39.026, Speaker A: Right.
00:04:39.048 - 00:04:52.886, Speaker B: It's a little bitcoin adjacent, but I think it is some of the better written work on what we're talking about when we get a commitment of finality. And so settlement assurances refers to a system's ability to grant recipients confidence that an inbound transaction will not be reversed.
00:04:52.918 - 00:04:53.066, Speaker A: Right.
00:04:53.088 - 00:05:11.306, Speaker B: This is usually used in the case for the specific article of bitcoin. This is the six confirmation depth thing, right? It's probabilistic finality. When do you know a transaction won't be reverted? We'll say settlement assurances or settlement is this act of not being reverted. So we'll say a sequencer guarantees a user that their transaction will not be reverted.
00:05:11.338 - 00:05:11.822, Speaker A: Right.
00:05:11.956 - 00:05:14.346, Speaker B: And then if we know, okay, is this a settlement layer?
00:05:14.378 - 00:05:14.526, Speaker A: Right.
00:05:14.548 - 00:05:27.126, Speaker B: We have this quote from James Fretwood. We actually pinned this in our marketing channel in slack. This is one of the things that was influential on moving from we're building a settlement layer to we're building a shared sequencer. The term settlement layer is very busy, muddy. Right. Confusing. So we're not going to use that.
00:05:27.126 - 00:05:28.706, Speaker B: But why shared sequencers?
00:05:28.738 - 00:05:28.886, Speaker A: Right.
00:05:28.908 - 00:05:35.014, Speaker B: Come back to this question. Why would you want a shared sequencer? We argue again from the Nick Carter, it's the settlement assurances post.
00:05:35.052 - 00:05:35.350, Speaker A: Right?
00:05:35.420 - 00:07:17.702, Speaker B: Ledger. Costliness is the most profound and direct variable available to us to evaluate a blockchain. Settlement guarantees. But simply, it's the equivalent to the amount paid to validators. Transaction selectors per unit of time. So in like a bitcoin case, which is probably the most simplistic one to understand, this is like the amount of computational effort put into a given fork of the chain, right? And literally the money burnt to give you that layer of settlement finality, right? In a proof of stake chain, in something like ethereum, right, where you still have a heaviest chain and it's slashable, right? It is the depth or like the weight of a stake that has been posted on one given fork of the chain, right? That's what you're going to call ledger costliness, right? So we argue that sharing a single sequencer set between multiple roll ups will increase the ledger costliness, right? What we mean by this is if we have ten roll ups and we'll assume they have kind of heterogeneous transaction flow, right? They're maybe not all evms, they're maybe serving different use cases and they are all relying on a single point of settlement and they are doing like a proof of stake mechanism, or even if it's something like a proof of governance, or just like kind of an implicit governance, kind of chain link style implicit staking, right? If you have multiple use cases and more order flow using a single point of settlement as stronger guarantees if it is shared between all entities, rather than each of those roll ups attempting to acquire its own kind of set of ledger costliness, right? So this is what we'll look at, right? We look at this architecture, we'll assume that the soft commitment has a high likelihood that transactions are settled. Specifically, if it's shared, we can assume it is a higher likelihood that the transactions are settled in a shared set than if it was decentralized across any given roll up.
00:07:17.702 - 00:07:35.086, Speaker B: So then we'll say, why lazy shared sequencing? And so again, I think our architecture and espresso's architecture is implicitly we say it's a lazy shared sequencer. And what we mean by this is it's not executing the state transitions right to the original picture. It's doing the ordering, not the execution, for all the roll ups. So why are we doing that?
00:07:35.108 - 00:07:35.534, Speaker A: Right?
00:07:35.652 - 00:09:01.554, Speaker B: Well really, there's a question of why wouldn't we do the stateful shared sequence? It would be really nice if we could say, hey, we all do it in one go, and we all get this commitment over a state route, and we get the ledger costliness, and it's nice and clean. Why are we not doing that? That seems like the obvious nice solution, right? We'll argue that the resource requirements for a stateful shared sequencer, one doing execution for all the roll ups, scales in proportion to the number of roll ups, it's not explicitly the number of roll ups, right? But if you assume that I, as a validator set, a sequencer set need to execute three different heterogeneous state machines. I need to keep the state and I need to process and do the execution, use the computational power and the storage requirements to keep all these states in these ledgers. That increases. If I add another roll up, then I have to pay more cost, right? So going back to one of our principles, deploying a roll up should be as easy as deploying a smart contract, right? If it's a stateful shared sequencer, it prevents permissionless use of a shared sequencer. And what this means is if you are introducing, every time you add a roll up to a shared sequencer, you increase the cost, the resource cost to the actors in that sequencer set, then that cannot be permissionless because you've now introduced a DOS vector, right? I can go say I'm going to go make ten roll ups and I'm going to push them into the shared sequencer, and I've just increased the workload of all these entities, right? So if we look at things like Polkadot's parachain auctions, right, that is a mechanism to create an economic cost to join the sequencer set, the validator set. If we look at interchange security with the Cosmos hub now, replicated security, right.
00:09:01.554 - 00:09:17.754, Speaker B: There is a governance process. You need to convince the validator set that they should allow your state machine to join the set. And we believe that it should be as easy as deploying a smart contract. You don't need to ask for permission to deploy a smart contract. You simply go to the chain and you pay the gas fee and you deploy your contract.
00:09:17.802 - 00:09:18.062, Speaker A: Right?
00:09:18.116 - 00:09:28.366, Speaker B: So that's one of our arguments for why we should use a lazy shared sequencer. But then going back, if we're not doing the execution, right, what is a lazy shared sequencer actually guaranteeing to our end users?
00:09:28.398 - 00:09:28.738, Speaker A: Right.
00:09:28.824 - 00:09:43.270, Speaker B: And we're going to argue that a lazy shared sequencer guarantees that a transaction's position in the canonical block ordering won't be reverted. So specifically, this is a weaker guarantee than saying the resultant state root of your roll up is x.
00:09:43.340 - 00:09:43.526, Speaker A: Right.
00:09:43.548 - 00:09:54.606, Speaker B: And it's actually giving you this state root hash. If you're familiar with, like a tendermint kind of cosmos space, there's a distinction between tendermint giving you a block hash and the cosmos giving you a state root.
00:09:54.658 - 00:09:54.970, Speaker A: Right.
00:09:55.040 - 00:10:02.570, Speaker B: We are talking about a block hash, right? You're giving a commitment of the ordering of a given block, but not a state route of a post state machine execution.
00:10:02.650 - 00:10:03.280, Speaker A: Right?
00:10:04.210 - 00:11:21.414, Speaker B: And so we argue again, right? Deterministic state machines is kind of what we're doing with blockchains generally, right? A chain state is determined by recursively applying its state transition function to an input state in an ordered block, right? So very simplified function here, right? If we're at state zero and we want to move to state one, we provide block one and we just run the state transition function. Now we have state one, right? I'm not like a math guy, I don't know what the math symbols would be to recurse this, right? But run that every time from Genesis, you now get the resultant state root. And the important thing here, right, is that from a genesis state, if you have access to all of the ordered blocks for a roll up, but not the state commits of those roll ups, you can still, as a full node executing that deterministically reach the same end state as everyone else for a given fork of the roll up, right? So we still have the capacity to do this execution. But really, when I was like, when are these blocks actually being executed? Like, who's doing this execution if not the sequencer set, the validator set, right, so this is when we go to Ethereum land, right? So this is a picture from Mevboost. Pix is a great dashboard place to look for information on Mevboost. When I checked on, looks like the 16th, right? We're at 95% of Ethereum validators are running blocks through Mevboost, right, or like 95% of the blocks on Ethereum overall are Mevboost. So we're pretty close to 100% of people are using PBS for this.
00:11:21.452 - 00:11:21.846, Speaker A: Right?
00:11:21.948 - 00:12:25.914, Speaker B: And then let's look at the relay dashboard, right? So within relays, there's really two types, right? I guess there's three types, if you consider the censorship ones, like a type, right? But we see the largest relayer, 28.3%, is the ultrasound relay. And the ultrasound relay is considered an optimistic relay. And what we mean by that is that validators are actually not checking the block that is given to them by a builder to ensure that it is a valid state transition function. They're not re executing. The reason they're not doing that is they save some milliseconds of latency by not doing that execution. And the way that PBS works as an auction mechanism, builders want to be able to submit their blocks as late as possible in the block timing by shaving, let's say 100 milliseconds off of that, they notably change the amount of order flow, the amount of blocks that are sent to them as a relayer and they're all trying to competing for order flow, right? So by just saying, will yoLo, right? We're going to assume that builders are not sending us fake blocks and we will not execute that, right? So what we argue is 28% of all blocks, 28% times 95% is like 26.8%
00:12:25.914 - 00:12:57.362, Speaker B: or something of all blocks going through Ethereum today, the validator is not actually executing the state transition function. So what does that mean? Where are these blocks actually being executed? So we'll argue this is like a rough design for how we would see a shared sequencer having like a PBS MeV supply chain order flow, right? Users go to roll up rpCs. Those RPCs could be Mev supply flow. We could have like, suave whatever, right? But they'll go to something that's order flow. And we're going to assume that order flow is a thing that takes transactions. Maybe there's bundles in the middle. Searcher builders, they're not that distinctive parties, actually, in actuality, at scale.
00:12:57.362 - 00:13:57.890, Speaker B: But it outputs blocks to the shared sequencer proposer, right? And the important aspect of this diagram is that to generate these bundles, to generate these blocks, execution is needed to find profitable blocks, right? If we think about what we saw with MeV before MeV Geth, this is pre merge, right? What would you do? You'd do a priority gaff auction. You'd spam the chain to try to get your transaction. It would revert if you didn't like it, right? We introduce an auction mechanism and actually, it's not economically desirable. It's not like game, theoretically optimal to go submit non functioning blocks, right? If I'm like a PBS person, I can go grief the chain. But if I'm here to make money, like, I'm a sophisticated professional actor, I'm Jane street, I'm jump whatever, right? I want to make money, so I want to submit good blocks. So I have to do execution. I have to run the knapsack problem across all the state machines and theoretically find what is the optimal block, right? It's like a PNP problem or whatever, right? They're going to do execution to find these profitable blocks, and they're going to know whether a block works, right? And so the shared sequencer, this is the key thing, defines the canonical block ordering.
00:13:57.890 - 00:14:51.618, Speaker B: But really what it's going to do is it's going to choose the most profitable block that is offered to it. And structurally, a shared sequencer is essentially the same thing as an optimistic relay, right? It's going to be given a bid and a block. And the economic incentives say that the people who are going to be sending it blocks are going to send it profitable blocks, and profitable blocks are going to be executable blocks that are valid, not blocks that are just kind of griefing the chain and saying, oh, look, you have a block and it's just going to revert or whatever, right? So that's our argument of where the execution is actually happening in this stack. We can have a lot of debates maybe in the panel on the trust assumptions you're making, whether you've moved to economic assumptions instead of programmatic assumptions or cryptographic assumptions. But remember, this is where 27 ish percent of all Ethereum blocks are already being pushed this way and that's relatively stable. But we could see increase, right? This is status quo today. So touch on bridging really quickly here, right? We're not doing the execution.
00:14:51.618 - 00:15:22.350, Speaker B: So when we go to bridging, right, is a role defined by its bridge. Not answering that question here, but if we say three components of bridging, right, I'm going to argue that bridging is between a sending party and a receiving party. And the key of bridging is that sending party has to convince a receiving party that the data is available, that there is a canonical ordering and what the post execution state is. This is broadly what, like a contract on the receiving side of a bridge is looking for, right. If the data is available and we have shared DA, we're going to say that's a trust minimized bridge. We're not going to go too much in depth to that. But that's generally what we're looking at, like the roll up sphere.
00:15:22.350 - 00:16:12.414, Speaker B: But really when we look at this kind of order flow diagram, we're going to say the soft commitments. Again, the shared sequencer is defining the canonical ordering of the chain. So we're getting a canonical ordering between all the roll ups on the shared sequencer, the firm commitment coming from the DA layer that's giving you your data availability. So the question is, who's attesting to this post commitment state route, right? Who is actually executing the chain and saying, I will give some kind of guarantee, cryptographic, economic, whatever, of this actual post state route? What's the state of the world today, right? The status quo EVM chains, by and large. So whether you want to say like a gnosis chain, you want to look at the roll ups between each other, right? They're often not going through the optimistic bridge, seven day withdrawal window. That's really a long time, right? So you have mechanism, they can use connect, they can use something like hop, but in reality a lot of them are using things like layer zero, they're using things like axilar, they're using things like, I'm forgetting the third one, wormhole.
00:16:12.462 - 00:16:12.770, Speaker A: Right.
00:16:12.840 - 00:16:40.858, Speaker B: Fundamentally, I'll consider those third party committee bridges. You have some off chain set of actors that are making a multi sig adaptation and staking their reputation. Maybe they have an on chain stake on the validity of that state route comet BFT chains. They're using a native validator set. It's still a committee bridge and still an attestation making an honest majority assumption. But they use their native validator set, right? We'll say this looks like this, right? You have sequencing, you have an ordered block, you do execution, you get an executed block. That executed block that is implicitly ordered, right.
00:16:40.858 - 00:16:52.702, Speaker B: Goes into a committee, that committee generates an aggregated signature, whether that's bos, multisig, whatever, right? And that aggregated signature is sent to the receive side. And that's the kind of attestation we're making. What is the next gen?
00:16:52.756 - 00:16:53.310, Speaker A: Right.
00:16:53.460 - 00:17:23.382, Speaker B: We see sucinct labs working on proof of consensus of Ethereum sync committee, polymer is working on ZK Mint, right? And so what fundamentally these are, is we're going to do the whole first step, right? We're going to do an order block, we're going to do an executed block, we're going to pass it through some committee. That committee is going to have an aggregated signature, pass that aggregated signature into approver, and we're going to get what we're going to call like a proof of consensus, to use the zinc term, right. Why do we want this proof of consensus? It's kind of like an interoperability solution, right? Cryptographic curves are hard to verify. They're expensive to do in contract.
00:17:23.446 - 00:17:23.866, Speaker A: Right.
00:17:23.968 - 00:17:38.618, Speaker B: East doesn't know how to verify an ED 25 519 signature natively or cheaply for tendermint. So to go between tendermint, comet, bft chain to like Ethereum, if you wrap it in a snark, you essentially have very expensive yet off chain signature aggregation modification.
00:17:38.714 - 00:17:39.118, Speaker A: Right?
00:17:39.204 - 00:17:44.334, Speaker B: So this is what we're seeing a lot of things move towards, right? But what do we think the end game is?
00:17:44.372 - 00:17:44.910, Speaker A: Right.
00:17:45.060 - 00:18:48.206, Speaker B: We'll argue that prover networks are going to ease the creation of succinct state proof. I'm not a ZK proof. I'm going to details of ZK stuff of a storage proof or the state proof, right? But fundamentally we have things like risk zeros, like Bonsai network. We have like the nil foundation guys, right? We have these people working on networks that say you take a state machine, you do the execution, and you pass it into this generic box of proving, and it's going to give you a stark, a snark, whatever, right? This succinct proof of execution that allows a light client to verify it. However, it's worth noting, intercluster bridging still needs this proof of canonicity or this proof of consensus over the ordering, right? Because fundamentally you can easily generate a proof of a state machine, or theoretically you can easily generate it, but you could generate a proof that is a valid state transition but is not the canonical chain state transition. So you still need both components, right? What you get with shared sequencing and it's like 1 minute. So I'm rushing one proof of candidacy for multiple roll ups, because the shared sequencer defines the ordering for the chains, right? So you can generate one proof for all the chains using the shared sequencer that tells you this is a canonical chain, and then within the shared sequencer you just need the state proof.
00:18:48.206 - 00:19:22.206, Speaker B: So we'll say this looks like this, right? You order a block that's happening on the shared sequencer standard. Kind of like next gen stuff, right? This is the succinct style stuff. This is the comet BF or the ZK mint from the polymer labs guide, right? You generate this proof of consensus and then once per roll up, you do the state execution. You pass into a prover network, you get a ZK state proof. Both of these things go to your receive side chain. Now you have all the components you want. Again, we can go back and say if you're in a shared DA layer, you get something trust minimized, right? If this is everyone sharing Celeste, if this is everyone sharing ethereum, right? You get slightly better guarantees as well.
00:19:22.206 - 00:19:47.780, Speaker B: But this is how we view it at like the layer above that. So again, looping back to where we started, we believe that roll up should be decentralized by default. We're giving a workshop at 430 shortly after the pan in the couchy stage, we're going to show off what we have for a demo. Right now we have a development cluster very similar to actually like what espresso has. We'll show you guys how you can run this locally and how you can deploy your own rollups using a shared sequencer. So thank you everyone.
