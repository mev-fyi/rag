00:00:02.080 - 00:00:40.170, Speaker A: My slides are a little bit lighter than everything else. So we're going to be talking about data in modular ecosystems, so not that technical, more for the consumer to make sure that everybody actually understands what's going on. So, first of all, I'm from token flow. We do data and we do data at scale. We started with doing devs and storage, and now we pivoted slightly towards the modular ecosystem in multi chain. So pretty much following the trends in what's going on today. Me personally, I'm doing product, and I've been doing data and AI since AI was called statistics and math, so way before the cool things.
00:00:40.170 - 00:01:25.524, Speaker A: Now we're at a modular summit. So these people are trying to solve this thing, the trilemma, security, decentralization and scalability. And how was done, optimistic. And ZK rollups, modular blockchains, all nice and pretty. And now the beginning, the ecosystem looked like that you had ethereum, and then a few other chains appeared, and much more other chains appear and more are coming. So we are looking at a world where we might have hundreds, if not thousands of chains. And to make it even more complicated, things are not monolithic, they start to become modular.
00:01:25.524 - 00:01:58.972, Speaker A: So we are having also data availability layers, sequencing layers, and all of these things become their own independent stacks. So it gets even more complicated to get information about what's going on. We solved modularity. Now we have another problem, visibility. Even in the before time, this was not solved. So it was very hard to understand what was going on on chain, because everything was hashed and it looked very complicated. So what's the solution? Get data and see.
00:01:58.972 - 00:02:36.920, Speaker A: Sounds very easy. Why we need to see things, because we want this pretty chart and that chart and those other things. We want to see things going up and see. Our user base is growing. You have more addresses, we have more TVL, we have a lot of activity going on on our chain. So how do you get this data? You can self index, and that's what pretty much everybody's starting to do. But to get from this, which is pretty much how the information looks on chain, very hex numbers to that, you have to go through a few steps.
00:02:36.920 - 00:03:10.766, Speaker A: You start by calling RPC endpoints, let's scale, because chains move fast, and you can get one visual, one version, but you need to update with every single new block that appears. You transform the data, then you decode, spark contracts. If you're in a chain that has smart contracts, why we need decoding? Because otherwise you don't know what events the contract is emitted. You don't know what causes are emitted. You might know gas, but that's pretty much it. You don't know which token got transferred, where and how it evolved over time. Then you need to design data for analytics, which might sound trivial, but it's nothing.
00:03:10.766 - 00:03:54.404, Speaker A: One way you design data for application, in another way, you design data for analytics because you need to think on how to aggregate all this information to make it real time or as close to real time as possible. And you don't have to use a lot of compute power. You need to manage data pipelines and storage. So all of this data has to go somewhere, it has to be processed, it has to be tested and validated, because things might not be as easy as they seem. And you need to optimize for performance for the same reason we are optimizing for the blockchain performance. We need to optimize also for this pipeline's performance, to make sure that the data ends up when it needs to get in those charts. Because if you have charts from one week ago, it might not be exactly what you want.
00:03:54.404 - 00:04:20.460, Speaker A: And you want to know your TVL today, not last week. Integrate visualization tools, because if it looks like that, it's not fun. You need to make those pretty colors in charts and dashboards because people like charts. You need to analyze. So somebody actually has to come up and think about all these metrics to know what you're talking about. It's not that easy. You need to build those for your community.
00:04:20.460 - 00:04:42.470, Speaker A: Why? Because blockchain is all about transparency. So you need to make sure that other people are able to get your data. If it's that, it's not useful. And we're all saying that blockchains are public. Data is public. Public doesn't necessarily mean useful and available. And you need to do all of these things while you're building your project.
00:04:42.470 - 00:05:35.720, Speaker A: And that costs time. It's not necessarily about the money, because you can build your team, you can have all of these fancy steps, and you have all your budget, but it's time. Why should you spend time to do something that is on the side of what you're actually building? So blockchains are, scaling analytics are not, not in the current setup. So what would be a solution? We are building this token flow, multi chain solution where pretty much we are indexing, decoding, building dashboards and data sets in under week for any chain. So the underweek depends on the size of the chain. If it's a brand new chain, it's from day one. So you start from Genesis, you start having dashboards, we have 80% decoded out of the box using existing signatures.
00:05:35.720 - 00:06:35.716, Speaker A: So if you have something very fancy where your devs are very, very creative, that might fall in that 20% that is not there, but for everything else, it's already there out of the box. We have harmonized data models across ecosystems, and this is very, very important because the moment you start to diversify your blockchains, then you start having a lot of tables and aggregating all this data in one unified ecosystem starts to become very, very complicated. But if you're talking, I don't know about op roll ups, the stack behind the scenes is the same. So why shouldn't the data be the same? Same for our bitroom for cosmos chains. So there are similarities, and we are making sure that everything is harmonized and makes analytics across multiple chains much, much easier. And we are covering your standard information blocks, transaction receipt, standard and full traces. And by full traces, what we are doing, we are extracting all the way from state and storage for smart contracts.
00:06:35.716 - 00:07:30.524, Speaker A: This is a level of detail that nothing many teams do because we believe that all the information should be extracted from chain, not from side chains. And we are building to make things even more easier, we're building ecosystem NDA connect dashboards and data set. What does that mean? Pretty much we're aggregating data across, let's say, opstack, and we're creating unified data sets so it's easier to analyze what's going on in that stack. Same for data availability. Like Celestia avail Eigen, we are extracting all of the data and bridging it to the information that happens on chain. So you actually don't know just the data availability side of things. You also know what's going on and which transaction actually generated that piece of information that ended up on the data availability layer.
00:07:30.524 - 00:08:09.220, Speaker A: This allows to do marketing besides other things, but also to understand what is the chain that is posting data on the data availability layer doing and how that is evolving. And it's all available in a tool that is called studio. So it's a SQL web browser that allows you to build queries ad hoc and to have predefined dashboards and charts. So that's pretty much what we are doing. Very, very sweet and short because to give you a bit of time, if you want to know more, that's us.
00:08:12.480 - 00:08:24.580, Speaker B: Just to clarify on the latency part, are you guys storing the data in a database? So we are essentially querying like a cloud database.
00:08:25.900 - 00:08:41.160, Speaker A: We are pre indexing everything and making everything available on demand. We have a latency of around 15 blocks from tip for the analytics side of things because it requires a bit of processing to get it there. But we're storing everything.
00:08:44.500 - 00:08:54.080, Speaker B: What are some of the current things that you can highlight right now for the platform? What are some interesting data sets that you guys.
00:08:54.850 - 00:09:39.100, Speaker A: So we have indexed Monta mode, Ethereum, the full Ethereum with full state. So we are able to know exactly what has happened on Ethereum from Genesis to now. And when we're talking about state and storage, we are able to get the balance of a certain token without having to aggregate all the events that have happened throughout time. We just query the variable balance of, of that particular token. So it's a query that is six lines of SQL instead of something very, very complicated. And that we have done for Ethereum, Optimus and base. And we are able to do it for pretty much every single EVM chain.
00:09:39.640 - 00:09:45.112, Speaker B: Because I'm just thinking like, what's the difference between using a free solution for.
00:09:45.136 - 00:09:54.476, Speaker A: This, which is dune, and is Dune free? I mean, to a certain extent that is also free should be incomplete, right?
00:09:54.628 - 00:09:56.972, Speaker C: Dune is the bane of my existence.
00:09:57.116 - 00:09:57.412, Speaker B: Yeah.
00:09:57.436 - 00:10:03.040, Speaker C: But I use it a lot. So there's also like, with missing data points, incomplete data sets.
00:10:03.900 - 00:10:07.452, Speaker B: But it's a struggle for every data set, right?
00:10:07.596 - 00:10:08.100, Speaker A: It's true.
00:10:08.140 - 00:10:10.960, Speaker C: But yeah, if you have the whole of the Ethereum chain.
00:10:11.340 - 00:10:41.420, Speaker A: So the difference between us and Dune, Dune has, I don't know, half a million tables. So to aggregate something, you need to do joins and to know the structures and to do all of this for us, this is Ethereum. It's eight tables. And that's it. All of the events, all of the smart contracts are in the same table. So it's easier to get information. So if you want to analyze one contract, everything that has happened for it, it can just go in one table and you do all of that.
00:10:41.420 - 00:11:03.510, Speaker A: And that studio has a free plan. So you said dune is free. Ours is also free. So it's available online, obviously with compute limitations, same as dune. So dune is not the, it's free. It's free for certain use cases. And your query times out about after, I think, 1 minute or a minute and a half if you're on a free plan.
00:11:04.530 - 00:11:08.906, Speaker C: Yeah, I was updates you do per.
00:11:08.938 - 00:11:28.860, Speaker A: Day for the data set. It's continuous update, it's streaming, it's live. So it's wonderful. That's the beauty of it. So we're making it as close to the tip of the chain as possible. So you don't have to worry about all of that. And it's tested to make sure that we're not missing blocks, we're not missing events.
00:11:28.860 - 00:11:44.860, Speaker A: Obviously, it's data at scale. So there will be issues here and there, because we are talking about, what, 21 million blocks on Ethereum? And if you go on Arbitrum, it's like, what, 250 plus? So it's a bit complicated.
00:11:45.680 - 00:11:53.340, Speaker C: And then, just curious, on the pricing model, is this like a token based, as in not crypto tokens, but like.
00:11:54.800 - 00:12:21.390, Speaker A: Let'S call it, let's call it credit based. So it's not token, it will be credit based. For now, we are on advanced free plan, so everything is free. And it will be free for the foreseeable future, but we will introduce paid plans for the community for more advanced compute. So you can run multiple queries in Pera, you can have scheduled dashboards that run on demand, so on and so forth.
00:12:21.690 - 00:12:25.550, Speaker C: Currently you only have EVM chains and Cosmos.
00:12:26.170 - 00:12:38.912, Speaker A: For now. Yes, EVM chains and Cosmos, but we can add Starknet. Yes, that's our CTO. He's very quiet back there, because my.
00:12:38.936 - 00:12:47.952, Speaker C: Question would have been, obviously, Solana has a lot of users, but it's also the most data heavy, one of them all. How would that work?
00:12:48.056 - 00:12:50.800, Speaker A: You two will come as well? It will come as well.
00:12:50.960 - 00:12:52.540, Speaker B: Any timeline on that?
00:12:53.880 - 00:13:01.940, Speaker A: How much are you paying for it? I don't know, to be honest, but we are working on making it available.
00:13:03.490 - 00:13:30.722, Speaker D: There was a question about the size of the data, so I just checked in the meantime for one table, which is the storage minus table, which covers every single change in history of storage. And in Ethereum, it's 14 billion rows at this moment, because now, every time when something changes, we have, it's not like the node storage data at the block level. So you see like storage before and.
00:13:30.746 - 00:13:31.946, Speaker B: After the block, but you don't see.
00:13:31.978 - 00:13:44.118, Speaker D: What happened in between every single change. Yesterday I had a talk with the securities I demand, and I will show you many, like, use cases where this.
00:13:44.174 - 00:13:46.678, Speaker A: Level of information for security researchers is.
00:13:46.694 - 00:13:48.006, Speaker B: Something that's absolutely needed.
00:13:48.038 - 00:13:55.894, Speaker D: So depending on the use cases, if you are building simple dashboards based on events, go to the old. Like, everybody uses the old, so fine.
00:13:56.022 - 00:14:08.570, Speaker A: Unless you're a small chain that is not on dune yet, and they will take forever to add it. And if they add it, it not decoded. And it's very, very general for like, complex cases.
00:14:12.630 - 00:14:13.410, Speaker D: Sorry.
00:14:16.550 - 00:14:17.062, Speaker A: Thank you.
00:14:17.086 - 00:14:17.590, Speaker B: Thank you for waiting.
