00:00:02.880 - 00:00:37.330, Speaker A: Okay, let's start. So, hello everyone, it's Vlad and Hlib, aka Wondertime. We are from celestial labs, and today we're going to talk with you about Schwab. How do we scale data availability protocol, and sampling in particular, using that protocol. So Schwap stands for share and swap. And in its core, it's a messaging protocol and a storage system that is going to scale out the celestia to its maximum. Let's start.
00:00:38.190 - 00:01:32.040, Speaker B: Ok, in order to explain Schwab, we will take a look at how sampling works, because that's the thing we are focusing on. And to explain it, we will need to take a look on what is data availability. And briefly looking into the blockchain, you can see there is like chain of blocks. And in order to verify that the block is valid, usually you need to run the full node. But the full nodes becomes more and more expensive with the size of block. And in celestia, we have ambition to go with really, really large blocks, which makes it really expensive for normal people to run the full node. And they can't have an ability to validate if the block was valid or not.
00:01:32.040 - 00:01:55.560, Speaker B: And solution for it is the light nodes. Light nodes, which in celestia allows people to have an ability to see if the data is available. And if it's available, it will be possible for the full node to verify it.
00:01:57.300 - 00:02:01.044, Speaker A: This is in contrast to full nodes that download the full data.
00:02:01.172 - 00:02:39.166, Speaker B: Yes, and light nodes doing it by performing sampling. So what is sampling? To explain sampling, we need to explain how the blocks looks like in celestia. And there's. So let's imagine the transaction of someone. The transaction is encoded into shares of fixed size. So it's put into the shares. We'll have four here, because it doesn't fit into three.
00:02:39.166 - 00:03:18.226, Speaker B: So we will add some padding on the top, and all of these shares are then encoded into the square of shares. This is the square, and our shares will land somewhere here. Then this square is erasure encoded. So we take rows in the square and encode it. Every row is encoded, so there will be another encoded square. Then we do the same for the columns. It's encoded vertically into another square.
00:03:18.226 - 00:04:25.980, Speaker B: And then it doesn't matter which one you can take. We encode either this one vertically or this one horizontally. The result will stay the same. And the nice property about it is that it allows anyone in the network to be able to recompute the data, just having one quarter of some shares in this bigger block. So what light nodes needs to do in order to verify that the data is available, it needs to make sure that there is no hidden more than quarter of data. So if we draw it a little bit like that, this is something minimal, like one share of size. Light node needs to make sure that there is enough samples in the network that will allow to recompute the whole block back.
00:04:26.320 - 00:04:48.488, Speaker A: So to elaborate here, this is the case when the data becomes unreconstructable and completely unavailable, when it's slightly more than this quarter. And this is what sampling is trying to prevent or detect. If there is this unavailability case, how.
00:04:48.504 - 00:06:08.380, Speaker B: Do we do it and what light node does it performs random, simple random sample is, it's trying to fetch one share from the network. So for example, if it was like this column, this is row, it selects random sample here. And if we look into the probability of finding the hidden part, it's something like one fourth. So in the case of data being unavailable, light node with one single random sample will be unlikely to detect it with the probability of about three, four. It's a little bit less than that, but because of the square needs to be hidden, square needs to be a little more than one quadrant. And what light node does, it performs multiple random samples over the same square. So the probability for the light node being unlucky and not find any hidden data is with n samples is this.
00:06:08.380 - 00:07:19.680, Speaker B: So for example, for 16 samples, it will have the probability of being unlucky only 1%. And 16 samples is actually really small amount because in celestia we have block size of EDS size 128 right now, and it will grow in size over the time. And the nice property about it is that this is the probabilistic model. So you can see that the size of the block is not mentioned here anywhere. So with increasing block size, light nodes have same probabilistic guarantees of finding out unavailable data, regardless of the block size, which makes it scale with the block size in a constant manner, like it's still same hardware requirements, same networking requirements, it's a constant with the size.
00:07:22.740 - 00:07:28.004, Speaker A: That's what enables blockchains to scale. By scaling data availability, we can scale blockchain.
00:07:28.132 - 00:07:30.284, Speaker B: Actually. Yeah.
00:07:30.332 - 00:07:38.156, Speaker A: And now let's like what happens if someone just gives me a share? But how do I verify that the share is correct or not?
00:07:38.348 - 00:08:51.986, Speaker B: Yeah, and we would need to look what is the actually, what is the sample? So sample, it's this data plus proof. And this data is actually shared. And this is sample. And what is proof? In celeste, we use merkel trees to prove the data. And I demirkelization happens for each row. So for each row we have, this is the row roots. And all of the row roots and column roots are communicated to each node in celestia network through the subscription.
00:08:51.986 - 00:09:13.070, Speaker B: And it goes inside of the header, which has verification guarantees and can be like verified. So every light node gets all of the row roots, it's going to be multiple of them and column roots.
00:09:18.200 - 00:09:18.584, Speaker A: For.
00:09:18.632 - 00:09:59.010, Speaker B: Every block before it starts to sample it. And then it fetches the share. It fetches the share. And server provides the lite client with the Merkel proof of it. So it will be this subroutes here. And this is important property because we would need to serve proofs with the share. So lite clients can proof can be sure that the data they get from the network is actually corresponds to the real block that was produced.
00:09:59.010 - 00:10:00.230, Speaker B: Cool.
00:10:00.390 - 00:10:09.642, Speaker A: So now we have like a little recap of what is sampling and how it works on conceptually. And let's dive into what. Schwab.
00:10:09.746 - 00:10:52.852, Speaker B: Yeah, now schwab, finally. So what is Schwab? Schwap is protocol for getting samples or any other data within the celestia network. It has the notion of ids, which is the way to identify what exactly you want to get from the network and to query basically all of the network nodes. And let's have some examples. So the simplest id of Schwab will be eds. Id. Eds.
00:10:52.852 - 00:11:05.410, Speaker B: It was the square, the big one. So if you want to get the full block, you just have to request the full EDS. And the way to find it is you can just find it by height.
00:11:05.950 - 00:11:14.158, Speaker A: A little note, we use blocks and edss interchangeably. You can think of them as the same thing mostly in this context.
00:11:14.294 - 00:11:15.838, Speaker C: What does EDS stand for?
00:11:15.974 - 00:11:17.318, Speaker A: Extended data square.
00:11:17.414 - 00:11:17.950, Speaker B: Yes.
00:11:18.070 - 00:11:20.810, Speaker A: So exactly that square that we draw earlier.
00:11:21.590 - 00:11:55.410, Speaker B: Yeah. So we usually refer to this first quadrant as ODS, which is original data square, where all of the real transactions are. And those three quadrants, they are erasure coded. And along with the ODS combined, they are eds. And this is odds. Yes. So this is the way to find your block, whole block in the network.
00:11:55.410 - 00:12:38.974, Speaker B: But sometimes you need to be more specific. You want to have some specific data within the block. And before that I mentioned there's like roles within the block. So you can add extra row filter and find out the role within the block. And you will specify, just additionally specify the index of the row. This is the row index, or you might want to find the namespace. The namespace is a different way to sort data within the block.
00:12:38.974 - 00:13:27.536, Speaker B: And all of the roll ups posting their blobs under some namespace. So it's easier for them to find out where are the blobs are within this block because blocks are really huge. So instead of providing the index, you might not know where's your data within the block in terms of indexes, but you know your namespace beforehand because you submit it with your data. And this can be a different querying parameter, namespace. So you specify namespace identify here and samples, like we started with samples. How to find a sample is you already have a row index that identifies rows. You only need to find the sample within the row.
00:13:27.536 - 00:13:30.500, Speaker B: So you just add a column and you have your sample id.
00:13:31.440 - 00:13:34.424, Speaker A: So basically it will be like a coordinate in this table.
00:13:34.512 - 00:13:35.632, Speaker B: Yeah, like x and.
00:13:35.656 - 00:13:40.240, Speaker A: Yeah.
00:13:41.490 - 00:14:18.920, Speaker B: And there are also like few other swap ids that are required for some implementation optimizations. For example, you can combine both row and namespace, which is row namespace id. And you can find your namespace within the specific role. Or for example, if you're looking only for your specific blob and you know the identifier of your blob which is can be identified by commitment, you can add to your namespace commitment and get your blob. So you just add your commitment here.
00:14:23.180 - 00:14:24.220, Speaker A: There are a few more.
00:14:24.300 - 00:14:25.260, Speaker B: Yeah, there are a few more, but.
00:14:25.300 - 00:14:52.272, Speaker A: That will be added more as well. So the goal of this is to make it extendable. And so in future we can easily add more ids and types so we can filter out and narrow down the context within an EDs and in general way to request any sort of a data within EDS. Yes, and we kind of build this independency tree and everyone just inherits properties of the other one.
00:14:52.376 - 00:15:04.122, Speaker B: Yeah, so it's like a query language. So for example for the sample you navigate by, you filter by heightendeh then.
00:15:04.266 - 00:15:09.070, Speaker A: Have I heard of SQL? It actually stands for swap query language.
00:15:13.250 - 00:16:03.284, Speaker B: Yeah, it's just the easy way to navigate the network for data you are looking for. And for each id there is a container. So the container is the response that you will get from the network for your specific id and container. It's as simple as it can be. It contains of data from the square, data is some shares from the square and proof. And there are different use cases. Sometimes you need both when you don't trust anyone.
00:16:03.284 - 00:16:20.702, Speaker B: Sometimes you already have a data locally and you only request the proof for identifier you can select which part you want. Do you want both? Do you want something? It can be optional container when it.
00:16:20.726 - 00:16:25.770, Speaker D: Contains data and proof. Do you intently use the data in the proof so it can be verified incrementally.
00:16:27.070 - 00:16:33.190, Speaker A: It's optional whether you want to have data in the proof. When you have both, is the container.
00:16:33.270 - 00:16:35.638, Speaker D: First, the data in the proof or the proof in the data?
00:16:35.694 - 00:16:44.084, Speaker A: Or do you do. Oh, they're completely separate, sort of like fields in the structure, if it makes sense.
00:16:44.182 - 00:17:16.510, Speaker B: So it's like, yeah, there are basically two fields. Yes. And each container has its own associated function that allows you, that takes both and allows you to verify if the data corresponds to the proof and corresponds to the actual block. So it will take this. This, plus some row roots and all of them combined allows you to verify if the data is actually in the block.
00:17:18.970 - 00:17:20.146, Speaker A: So it's not like a car file.
00:17:20.178 - 00:17:21.338, Speaker B: And the data is complete.
00:17:21.474 - 00:17:24.750, Speaker A: It's a completely different way of serializing the data.
00:17:25.050 - 00:17:34.910, Speaker B: Yes. No car files anymore. Yeah, we will cover the storage later, but we used car files before.
00:17:37.860 - 00:17:44.948, Speaker A: I think the next thing for us to discuss is to, we have sort of a concept of layering or composition.
00:17:45.084 - 00:17:47.640, Speaker B: Yeah. So how do you request it over the network?
00:17:48.300 - 00:18:14.434, Speaker A: It's just like high level data structures that we operate in, and we decouple those from the actual transport layer and discovery layer so that we can experiment more and investigate into research for the content discovery, for new transfer protocols, and even, in fact, an implementation of that. We'll be running two transfers at the same time, which are.
00:18:14.602 - 00:18:41.880, Speaker B: Yeah, so the Schwab is like a way to find out your data, but underneath you will have different protocols to navigate it through the network. And in our case we will have two. One is called shreds. I will draw it down there because they are swappable. There could be other implementation of it, different networking protocols. So far we'll have only two. Maybe we'll have more.
00:18:41.880 - 00:19:34.820, Speaker B: First is shreks, and a little bit overview of shreks. It doesn't do sampling, so we're not going to dive deep into it. But some properties of it is that we use shreks mostly for finding and transmitting large blocks over the network. It's usually used for full node to full node communication and for syncing full EDS full blocks. So it does propagate eds through the network of full nodes. It also allows to request namespace data, and it works over the leap peer to peer streams, just raw streams.
00:19:41.840 - 00:19:44.310, Speaker A: It's pretty simple protocol.
00:19:44.480 - 00:20:42.880, Speaker B: If we have more time, we will cover more details on that. But we don't use it for sampling. For sampling we use, and for a lot of other types of swap ids, we use bitswap. And the nice property about bitswap is that you can subscribe to certain data to the nodes that don't have it yet. So eventually every data should be available somewhere on the full nodes, but it might be not available yet. And every node in the network, upon finding out there is a new block, it requests it from another node, and if it doesn't have it, it keeps the state of.
00:20:45.660 - 00:20:46.600, Speaker A: Wantless.
00:20:46.900 - 00:21:03.400, Speaker B: Yes, it sends the I want message, and it acts as a, as a subscription to the certain data. And once this node receives the information from somewhere from the network, it can propagate it back because it knows that this node wanted it.
00:21:06.940 - 00:21:33.006, Speaker A: So to recap this important property that bitswap has that is necessary for the next thing we will discuss, which is called reconstruction. And that property is that if there's a network of nodes that are interconnected and all of them want a particular piece of data, and some peer in that network has the data, all the peers on that network will eventually receive that data.
00:21:33.118 - 00:22:11.680, Speaker B: Yes. So this piece of data will propagate through the network all the way back. And this is the crucial property we need for the reconstruction. And let's take a quick look on what is the reconstruction and why we need it for the reconstruction. We will return back to the bitswap after we cover the reconstruction. Why should you just draw it here? Yeah, that's fine. So, reconstruction is something that can help full nodes recover the data from the network in case data is not fully available.
00:22:11.680 - 00:23:14.540, Speaker B: So sometimes it's not sometimes, but some attacker may try to hide the full blocks from the full nodes to make them unable to see that something is hidden inside of it. But it will also try to make light nodes think that data is available by serving them samples. And in this situation, the full nodes are unable to get the data, but the light nodes can still sample. So the full node, what it would need to do, it would need to reconstruct the data from the light nodes. And it, for example, this will be the full nodes. It's connected to some light nodes, and each light node has several samples. In our previous example, it had 16.
00:23:14.540 - 00:24:14.030, Speaker B: So we can just draw like it had few random samples like this. And this one has another different samples like this, and this one also had few. So the full node aggregates all of the samples from the light nodes it's connected to and tries to recompute it back. So let's aggregate all of the samples back here. There's. Okay, looks correct. And in this small example, it's the full node will be able to recompute the full block.
00:24:14.030 - 00:25:10.200, Speaker B: So for example, for the first row it has more than half of the shares. It's sufficient for the erasure decoding, so it can return this one for the next row, it also can decode it. For the last row, it cannot decode it, but it can decode the third row. And now it can look for the columns and also perform the erasure decoding, which should be possible for all of the remaining columns. And full note in this example is able to recompute the full block just from aggregating the random samples the light node had, which will allow it to validate the block. But this was the really small block it was possible to recomputed from these three nodes. But in real world we have much larger blocks.
00:25:10.200 - 00:26:20.480, Speaker B: And one of the limitations we have is that each full node has the limited amount of connections. And sometimes it's not possible to recompute full block just from the light nodes that are connected to these full nodes. So what it will do, it will try to aggregate as many as possible, but it would need someone else to help it. And this is where we need the subscription model of bitswap. So the full node will establish the wantlist connection with another full node that might have another light node connected to it with a different set of samples. And by having this I want list, this full node will find out some samples and exchange it back to this full node. So our first full node will be able to see and fetch samples through other full nodes on the network.
00:26:20.480 - 00:26:43.410, Speaker B: And with the growth of the block size, it's quite important for all of the full nodes to act collectively and exchange their samples in order to collectively reconstruct the block. And that's why they need to subscribe to the data by using the bitswap.
00:26:43.570 - 00:27:25.620, Speaker A: And so here you can already kind of feel and grasp the topology of the p two p network we are building. So in the middle of it, there are multiple full nodes that are interconnected between each other with some degree of connections. They don't have to be directly connected to everyone at the same time. And every single full node has a bunch of light nodes that also connected to it. So the scaling of the system will be like adding more full nodes that serve more light nodes that are joining the network. So the next question, how do we actually make sure that we build this topology and how it's getting created?
00:27:26.280 - 00:28:01.430, Speaker B: Yes, and to make sure this topology is possible, we each full node and light node uses the DHT to find out the full node in the network and every node finds the predefined amount of full nodes and keeps the connection to them as long as it's possible. Sometimes it's not possible. Full node can go offline or it can disconnect, but in that case every node will just find another one in the network.
00:28:02.010 - 00:28:16.960, Speaker A: In other words, we use DHT for service discovery, where some peers advertise some service that others discover and take use in this sort of like altruistic manner. Yes.
00:28:17.300 - 00:28:24.080, Speaker C: So do you know how many light nodes are on average connected to a single full node or do you have a target?
00:28:24.980 - 00:28:55.090, Speaker A: So this is something that we have to do after this protocol gets fully implemented and is one of the important properties to know, because we need to know how many light nodes can a single full node handle. And basing on that, we can then configure and parameterize the topology creation so that we sure that the system can scale with more like with how many light bulbs are joining.
00:28:55.590 - 00:29:06.756, Speaker C: I guess there should be a number that is dependent on the block size. In order to be able to reconstruct the block, you need a different number of light nodes if the block is.
00:29:06.788 - 00:29:38.990, Speaker B: 1 different if it's yes, the number like since the light nodes doing random sampling, they have random samples with growth of the block size. You will need more and more random sample light nodes in order to reconstruct it. And in this case it scales in the way that you would need full nodes to be able to collectively do it. But also you need sufficient amount of light nodes in the network as a whole.
00:29:41.930 - 00:30:20.222, Speaker A: And yes, how many light nodes can a full node has, will depend on the maximum block size that network can sustain. And so we, in the network, we have like sort of a parameter that can specify what's the maximum number the block size can be. And the testing that will rely on that will use this maximum parameter that is possible at that moment. And we will basically then parameterize the topology using the maximum block size as a base that is possible at that moment.
00:30:20.326 - 00:30:40.126, Speaker B: Yeah. So for example, right now block size is two megabytes. But Celeste has an ambition to grow the block size to 1gb. And we will basically need to test it for larger blocks than we have right now. Stay tuned on that, we will post it.
00:30:40.238 - 00:31:46.234, Speaker A: Yeah, there's also a few things like to mention. For example, we just mentioned randomly bitswap, like for the audience who's watching this for video. The bitswap in itself is an existing protocol in lib system. It's coming from the IPFS world and it's a pretty mature protocol that has this property that we mentioned earlier, and that protocol does this like serves this property well enough. But however, it has some issues that we're going to optimize in the nearest future. Like it sends this once between peers for every single share, which in case of light node is a lot of traffic and there's a multiple way we can improve that and optimize this from by something like bitmap once. So you can ask your peer which samples you have in the square represented as a bitmap, and then the full node and full nodes, they like collect the data altogether.
00:31:46.234 - 00:32:01.430, Speaker A: They see which peers have what and then they accumulate it and do very strategic requests, all the light nodes for the specific data they have in order to reconstruct it. Stay tuned for more updates.
00:32:02.130 - 00:32:02.910, Speaker B: Yeah.
00:32:05.570 - 00:32:17.434, Speaker A: And also shrek stands for share exchange. Swap is share swap. Yeah, I think we want to cover storage now.
00:32:17.522 - 00:33:42.560, Speaker B: Yeah, let's see how the data is stored. Because it was one of the bottlenecks before, the way we stored the data swap allowed us to completely rewrite the storage engine and let's see how it works. To cover the storage, let's quickly recap that for serving samples, we need to serve both data and proof. So the proof is the miracle proof, it has intermediate nodes and on the leaf it has the share. So previously we were storing both data and proofs, but for the swap we now able to serve them in a single request. And we did some benchmarking and figure out that we can recalculate proofs on the fly. So instead of storing everything for each block, we will store only two quadrants of data.
00:33:42.560 - 00:35:03.714, Speaker B: So this is our eds we had, and we need to store only two quadrants. I will just show that we will store the fourth quadrant and the first one. And why so? Because sampling is random, a server doesn't know what samples it will need to serve. And for example, for samples from, let's say from the second quadrant, it would need to read the first row. So first it needs to redraw, then it needs to encode it, extend it with a razor encoding, and then it needs to build proof. And there is a problem with the fourth quadrant, because if we didn't store it, we would need to read the whole first square, then extend it fully to the second square, and then we can only calculate the extension part. So instead we will have some redundancy on the storage.
00:35:03.714 - 00:35:17.370, Speaker B: We will need to store twice as much data. But for samples from like this, for fourth quadrant, we would need only to serve this row, which half of it is already stored within the storage.
00:35:17.490 - 00:36:13.810, Speaker A: A little recap. Again, we want to read rows lazily. We don't want to for every single sample to read the whole quadrant, as the quadrants can be huge and this is actually bottleneck. The IO reads are like a real thing that we have to optimize for comparing to actually just computing the proof and storing the swarf quadrant allows us awaiting reading the whole square. It's like a trade off that we find out through the benchmarks. And nice thing is that we also build a system in a way that you can actually have an option to, as a node operator, to choose what you want to do. You may want to trade off speed of serving the data for a lesser disk usage, or you can also just store two quadrants and serve things much, much faster.
00:36:13.970 - 00:36:26.538, Speaker B: Yes, so you can trade your storage for more compute or less compute and more RP's for more costly storage with the overhead of x two.
00:36:26.674 - 00:37:27.732, Speaker A: Another interesting thing about celestial protocol, and generally like all the data availability protocols, is that we have this notion of like availability window. It's for how long the data is being kept in the network, and we usually prune the network, the data outside of the pruning window. But there's still a notion of archival nodes who can still serve the data from original data square, the actual real data that users submitted to, which is somewhere in this square. And these archival nodes, they can just also serve the first quadrant and keep it there even in more optimized way, without storing the other fourth quadrant, because sampling is not permitted outside of this availability window. So actually running an archival node can be pretty cheap as you store only one quadrant and you can like optimize it even more there.
00:37:27.836 - 00:38:15.810, Speaker B: Yeah, and on the server side, we also can do this, all of this operation in a cached manner. So on every sample request, the data that has been written from the disk will be cached. The extended part can be cached, the proof of it can be cached. So the second time request comes for the same role. It can be served from the cache. And for the most recent blocks, we store everything within the cache. For the older blocks, it's a lazy cache that stores only on demand, with the LRU notion of evicting the cache data that hasn't been requested for some time.
00:38:15.810 - 00:38:18.220, Speaker B: Cool.
00:38:19.800 - 00:38:40.280, Speaker A: And I think it's mostly it. We covered the theoretical part of it and also touched on implementation details of this protocol that we as engineers think are important to make sure the system is actually efficient as long as you don't have any more questions? We can add it up here.
00:38:40.400 - 00:38:43.430, Speaker B: Yeah, maybe, maybe somebody has any questions.
00:38:50.690 - 00:38:57.830, Speaker C: The blocks are published. When a new blog is published, it goes to the full notes initially, right?
00:38:59.250 - 00:39:45.630, Speaker A: Yes. We haven't touched the consensus network here. There's in fact two networks, which is sort of like a tag depth that we have to fix. But the way it is right now is there's core network and these consensus node, they produce these blocks, and there's DA network that listens to sort of new blocks that are being produced in this core network. And there is a gateway node type that ingests, ingress all these blocks to the DA network, which is called bridge node. And they have almost the same set of features as other full nodes besides the fact that they also ingress data in. So yeah, it's better be written.
00:39:46.810 - 00:40:23.990, Speaker B: Yes. So the bridge node is like a sidecar consensus node. It reads the data from the consensus, it doesn't send anything, and then it just popular sent it to the network, propagates it to the DA network. So this is the DA network data availability network, and this is the consensus network. So bridge node only acts as a client to the consensus, and there are full nodes and the bridge nodes, and it can also send it to the light nodes, and this can also propagate to light nodes.
00:40:26.290 - 00:41:10.982, Speaker A: And eventually we want to merge this bridge node into the consensus node and make sure that we have like one singular network instead of them being two completely one. This actually has a lot of nice optimizations besides just being like a huge stack dapp issue, because instead of like currently, the consensus nodes has to store data twice, it has to store it on a consensus and they have to store it on a bridge node that they have to run together. They also have to recompute the data twice for all the proofs, for all the extension. When the block size grows, this additional resource consumption isn't really helping them. So it might become like a scaling bottleneck at some point, which might be justification to actually merge them. Yes.
00:41:11.086 - 00:41:12.942, Speaker D: Are you familiar with pure dust?
00:41:13.086 - 00:41:13.598, Speaker A: Yes.
00:41:13.694 - 00:41:15.094, Speaker B: Can you explain the difference?
00:41:15.262 - 00:42:15.760, Speaker A: So the pure dust is in some way similar to like pit swap, but it's more optimized for the sampling use case. It's also a request response protocol that uses the same like light node to full node relation. There is, as they call them, super nodes in the theorem community, and light nodes or light clients, they sample data from these full nodes by just doing sample requests. And there are many, many more details about how did they do retries, how they request other peers if there's something going on, but it's more like implementation details at this point. But fundamentally the peer does isn't different from the system we are doing here. There will be a next iteration of peer dos, which is called subnet dos. And this is something that is more interesting, in my opinion, to look for.
00:42:15.760 - 00:43:13.580, Speaker A: Ethereum optimizes for home stakers and by definition all these homestakers are going to run partial nodes. And in Ethereum they want to have all the home stakers to never store the full block. But there is like many, many rows, right? And every single row has its own like basically index. And some peers are in this row subnetwork. So there are some peers in the network that particularly serve these rows and some other home stakers serve other rows. So never ever a single node has all the data, but all of them have it in the end. And I believe they're going to achieve it also through gossip sub by serving these roles.
00:43:13.580 - 00:44:38.666, Speaker A: And we also use gossip sub for things that are less related to that but more related for header propagation in this network and where we actually get the data ability header that was mentioned earlier with these row routes and column routes, and how do we actually refine the, the inclusion of the data. And then eventually this very important point like Kademlia is proven to be not scalable for a sampling system. And this is what Ethereum understand. And they want to look more into research of how you make this actual, some distributed table that is can handle this amount of samples that are being produced. And actually I recently just like watched the talk from Dunkrad and he mentioned that instead of looking into distributed hash tables, DHTs, we should start and research more of distributed arrays. So we actually don't need a hash table, we don't need to identify samples by hashes, we just need arrays because these are basically arrays of shares. So the next step, the next really scalable data availability content routing system will be something based on distributed arrays.
00:44:38.666 - 00:45:03.280, Speaker A: And I'm really excited to find and see someone actually coming up with a good solution for this problem. It's more important for Ethereum at this point because of this home stake requirement, while we can focus on making full nodes more scalable, and we can live with this full notes being super nodes sort of assumption for quite a while.
00:45:04.780 - 00:45:38.390, Speaker B: But also the notion of ids in Schwab, as you might remember, it allows you to navigate the data by secondary filters, which is really handy to run partial nodes. So for example, you are interested in a specific namespace and you might be want to contribute only to this specific namespace instead of running the gigantic full node with large blocks. So it would be possible through the Schwab ids to find those nodes that holds only this namespaced data only.
00:45:39.930 - 00:46:31.480, Speaker A: Yeah. So like this is the block and there's some subsection of, of this block that is completely random and it's like up to the consensus node to define how it's created in order. There's just different namespaces that are identified here with some colors. And another thing we could do is we can also scale the system by making sure that there are nodes that only store these namespaces of interest and which can also contribute to the data availability as a whole and make sure that all these data is available and constructable. Yeah, but it's something that differentiates us from Ethereum protocol.
00:46:35.990 - 00:46:51.710, Speaker D: I'm curious if you have numbers on how often reconstruction happen. It's not clear for me. It's like, it's meant that like a fire, like a firefighter solution, like how we fucked up and sadly we have to reconstruct or if it's a normal behavior.
00:46:51.830 - 00:47:38.944, Speaker A: So that's a very good question. And in current state it's actually firefighter solution. So if full node cannot get the block by asking other full nodes, it fallbacks to starting this reconstruction process. And we do this right now because reconstruction is pretty expensive operation on the bandwidth side of things. Eventually we gonna make it and optimize the reconstruction more and more and more. So it might not still be the default way of getting the data because like just getting the data from one peer sometimes is faster than like getting it from many, many, many small lights, like very resource constrained peers. But it depends.
00:47:38.944 - 00:48:04.950, Speaker A: There's still a lot of research to do and it would be interesting to see if we can ever make a system that never really needs to fetch the full blocks. And it can always sync data efficiently, relying on reconstruction. So you always like ask your peers and sing it from them in order to sync the recent data, or maybe even historical.
00:48:12.130 - 00:48:15.402, Speaker B: Okay. Okay, no questions. Then we can finish here, I think.
00:48:15.506 - 00:48:17.530, Speaker A: Yeah, thank you.
