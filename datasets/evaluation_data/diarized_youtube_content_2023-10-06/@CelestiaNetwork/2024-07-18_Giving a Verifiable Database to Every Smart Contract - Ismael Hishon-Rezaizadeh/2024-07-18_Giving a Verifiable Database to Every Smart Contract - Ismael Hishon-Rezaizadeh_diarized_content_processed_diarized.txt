00:00:01.920 - 00:00:32.780, Speaker A: Good afternoon everyone. My name is Ismael. I'm one of the co founders of Lagrange Labs. Title of our talk today is giving a verifiable database to every smart contract. So at a high level, what Lagrange focuses on is ZK co processing. It's a term that we've heard a lot of people talk about, but is often poorly defined in our view. And put simply, the ZK coprocessing is a paradigm to scale blockchains.
00:00:32.780 - 00:02:15.208, Speaker A: For the last ten years, we've been asking the very simple question, how do we scale the computation that we can run on major chains? We've looked at solutions ranging from building Alt L1 s, where we would selectively reduce the security or aliveness guarantees to gain speed. We've scaled horizontally and we've scaled vertically, with roll ups, fragmenting state and liquidity, and all the accompanying problems that that has created. But at the core, ZK coprocessing is a new approach. It's an approach predicated on using off chain verifiable compute to scale the types of computation an existing smart contract on an existing chain can access. And this is something that we're very excited about, because what it means is that existing DeFi protocols and existing applications on Ethereum, EVM compatible L two s, and all the modular execution spaces that are now being developed have access to more scale without ever having to move. So how does Lagrange intend to build this? Well, what we have is a proving network, a network of nodes run by some of the top operators in the space, ranging from Coinbase to OKX to Kraken, whose responsibility it is to generate zero knowledge proofs in a parallel fashion that we can deliver back on chain to smart contracts and applications that have requested them. We recently deployed our prover network in production and have so far generated approximately 10,000 queries per week.
00:02:15.208 - 00:03:18.920, Speaker A: A total of about 2 million proofs overall have scaled quite a bit the computation that we can leverage and we can deliver to Ethereum. And so how do we do this? Well, very simply, we have a network and an architecture with two core components. We have a gateway that allows on chain applications to request proofs, and a series of provers that, no surprise, compute the proofs. An on chain application, think a smart contract on Ethereum, think gearbox curve, or major NFT project that wants to leverage a more intensive form of compute, can simply emit an event and request that proof from our network and verify it back in their contract. So put simply, if they want to do something, they can't do on ethereum today. They don't have to redeploy elsewhere. They can simply request a proof of that computation that they can verify.
00:03:18.920 - 00:04:19.600, Speaker A: And so we've heard a few teams today talk about different approaches to ZK co processing. We have the, what I like to call historical data focused approaches. We have the ZKVM rust based approaches. But what Lagrange has, as is evidenced in the title of this talk, is a data centric approach. What we focus on is verifiable SQL queries, queries that you can define over blockchain data the same way you define them over a database and verify within your contract. So the asymptotics of this, since we're obviously specializing in a data centric approach, are highly performant for accessing large numbers of storage slots, receipt tree data, or transactions. We have about 30 seconds in our asymptotics for accessing a million storage slots.
00:04:19.600 - 00:05:33.094, Speaker A: And the concrete performance right now is about a minute 20 in the current version for computing queries. Simple select queries, of course, over a million storage slots. So how do we do this? What does Lagrange do to prove general purpose SQL for smart contracts? Well, this is based on some new work that our team recently put out that we're calling Perseus, or from a more technical standpoint, verifiable SQL via decomposition. I like to refer to this as a ZKVM, but for SQL, what we do is we take a a statement, a SQL statement, and we parse that into ten to 20 operations. For example, the select query is parsed into a create range project, sort arithmetic operations that we then can prove in parallel over the data and recursively concatenate back together. What this means is the computation you can define is computation over data. In a language designed to support computation in queries over data, you don't have to think about how do I compute over millions of on chain storage slots in SQL? Sorry.
00:05:33.094 - 00:06:15.400, Speaker A: In solidity, you can simply define them in SQL. And this is what we think of as a fundamental innovation. It's the dune analytics, but verifiable. And we do this through a number of novel tricks in cryptography that we've been able to pioneer. The first and the principal one being recltrees, a vector commitment that supports updatability. And so when we think of large computation over data, specifically blockchain data, the concept of updatability is very pertinent. Put simply, the majority of data that every query is touching doesn't actually change the blockchain data.
00:06:15.400 - 00:07:17.500, Speaker A: The state at block b is ostensibly almost identical to the state at block b plus one, except for the transactions that executed at b plus one. So that means that query based computation touches very much the same data. So the concept of taking a proof and then updating it lets us scale. And so when we think through the developers interaction with our coprocessor, we generally think of this as two stages. And you can think of this as two stages on chain, or the same two stages that you would do in web two. What you do is you define the data frame or the data on chain that you're interested in, how you want your table to be structured, what you want to be in it, what are the columns over what block, interval, etcetera. And then you define queries, queries that you can run from your smart contract and prove and verify back on chain.
00:07:17.500 - 00:08:42.380, Speaker A: And so by thinking about it in these two steps, you can really now define your computation in a way that you can think of as a database for your smart contract. And so once you define the storage slots that you care about, maybe the receipts you care about, or the transactions you care about, we enter a phase we call preprocessing. Preprocessing, very simply, is taking blockchain data and proving that a large amount of the elements contained within it exist now in an optimized structure. So for example, take all of the data from your contract from block b to b minus k, let's say it's a range of 100,000 blocks or a million blocks, and develop a new Zk optimized friendly database that is provably equivalent. Provably equivalent is a key term here. What this means is that the data contained in that new structure is identical, and is a proof that it is identical to the data on ethereum or any EVM compatible chain you might be working on. Once you prove that this new structure is equivalent, a new optimized structure, fast to compute over, we are able to structure computation over it.
00:08:42.380 - 00:09:24.812, Speaker A: This is what we call the query phase. The query phase is where your contract can emit an event. Our network can receive the event, and we can deliver it back on chain. So the developer flow very simply has two steps, first being indexing, second being querying. So from your contract, you can make a simple call to our registry contract to request that your contract is indexed and that there's a database that is created and a proof of the correctness of that database that is created. And once you make this call, you obviously have to fund it. And we will start processing.
00:09:24.812 - 00:10:10.026, Speaker A: Our network will start processing that contract. Then whenever you want, you can admit an event and say, hey, answer some query for me over this data that I requested you process. Answer for me. Who are the top users who've interacted with my contract over the last 90 days? Tell me, how many rewards has user x accumulated between two blocks? Really anything that you would ask and dune or ask your database. We let you do it from your contract and we let you do it verifiably. So end to end flow can be done on one chain or across multiple chains. All you need to verify these proofs is a block header.
00:10:10.026 - 00:11:30.076, Speaker A: So blockchain one can simply define its contract, define its database, and the coprocessor can then answer queries over it as long as the destination chain has a block header of the source chain. So it works single chain using the pre deploys block header. Or it works multiple cross chain for some chains like Op, when you have access to precompiles or pre deploys to let you access l one routes so effectively, you have now a modular and a composable, verifiable database that your contract can request proofs from whenever it needs. So where is this deployed now? Who's using it? The important questions we are deployed across four chains, a number of l two s and ethereum. And so far we've integrated with many of the top DeFi protocols, some soon to be announced, and have served 83,000 queries in the last 60 days. We're run by some of the top operators in the space and have integrated with many of the top ecosystems to be able to deliver this functionality in a differentiated way to their users. So to make it concrete, I want to touch briefly on two use cases, the first being reward contracts.
00:11:30.076 - 00:12:22.214, Speaker A: Believe it or not, reward contracts are a primitive and important one. Within defi, you can think of the sushi MasterChef standards and you can think of the points programs done by many lrts. All of these define your reward based on a simple condition. If you've held asset X and provisioned and deposited liquidity into asset X for a period of time, you can receive a reward proportionate to that amount of time. Another common condition is simply have you provided a concentrated liquidity position in uni v three continuously over some time? That is something that is intuitive to compute off chain, intensive to compute on chain. Why? Because your contract can't actually access this data. 10,000 blocks, 20,000 blocks.
00:12:22.214 - 00:13:27.746, Speaker A: Reading a single storage slot is 2100 gas. So accessing that data over a continuous and extended interval is infeasible. So the standards that exist take liberties. Some of them cut corners, some of them do it off chain, none of them do it extensibly and securely with our mechanism already today in production, you can do this. Now another very major use case that we target is nfts. Believe it or not, I like nfts and I like to think that nfts are a core part of crypto culture. But one of the problems is you can't really do much with them, right? You have an NFT on chain, but how can another on chain application that wants to give you a differentiated experience because of your ownership of that NFT do anything? How do I reward you with a new token or a new badge if you never sell your NFT? If you hold it since mint the data is encoded in a mapping but you can't really query this mapping on chain.
00:13:27.746 - 00:15:05.530, Speaker A: You can look at it off chain and try to build experiences off chain, but your on chain contracts can't really do much. And we've seen now that many of the top NFT projects are launching their own chains and are trying to take their strong ip and build some differentiated experiences using it. But they can't necessarily do this in a way that is trustless, that is verifiable and that doesn't create agency issues over the issuance of their assets and rewards with co processing. We can answer this very simply. You can ask a query, has user X owned a pudgy between a range of blocks? Has user y owned an Azuki that matches some set of conditions which lets projects that are interesting and consumer facing actually build new features that people want to use. So what's next for us? How can you get involved? And what would I suggest that people here do? So Lagrange launched our prover network very recently in production. We are adding a number of new features over the next 60 days that we expect to really push the bounds of the types of computation and more importantly the scale of computation that can be accessed on chain? We call this next launch our v one and we really expect that some of the launch partners we have coming with v one will define what it is to build within this new paradigm.
00:15:05.530 - 00:15:31.940, Speaker A: Projects who I hope many of you here are aware of and who I think will start legitimizing and start bringing eyeballs to what it is to actually build with this paradigm. So I recommend anyone here go on our GitHub, play with the code, run some queries, go on our docs and think of what you can build. And if you have any questions, please contact me. And our team would love to engage with you. Thank you so much.
