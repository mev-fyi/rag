00:12:18.840 - 00:12:50.150, Speaker A: Hello. Welcome, everyone, to the modular Summit. This is the second time we're meeting. Last year it was in Amsterdam and a lot has changed. We have so much packed in the next two days, within the next two days. And this summit captures all the change that has happened in 1215 months. I'm going to kick it off and introduce Balder from Maven Eleven for opening words.
00:12:50.150 - 00:13:26.720, Speaker A: Yeah, perfect. Thanks, Akram. Wow, what a venue. Amazing to be here today. Well, obviously grateful that you're all here after a long week in Paris. Obviously very excited about the coming two days. I remember very vividly that Celestia or actually called Lazy Ledger back in 2019 was just a research project.
00:13:26.720 - 00:13:53.670, Speaker A: And fast forward to 2023, see how far we have come. It's a big theme in the industry. Scaling blockchains is one of the most prominent themes still today. And we're very excited about being here today. We let me get this. Over the coming two days, we're going to be very excited. We're going to be listening to key speakers and key people in the industry.
00:13:53.670 - 00:14:27.310, Speaker A: But I want to have a big, big applause for both teams on the Maven Eleven side and on the Celestia side. They've been grinding the last few months of getting this together. Indeed, like acroM said, in Amsterdam last year, we had over 800 attendants. And here we are again with a lot of interest in the space. As you have seen, we have called the stages after free mathematicians, French mathematicians in this industry. They laid the groundwork for erasure coding, as we know today. And that's the basis of Celestia's data availability.
00:14:27.310 - 00:15:42.166, Speaker A: Sampling the paper written by Celestia or Mustafa. Actually, we get this over the coming two days, there will be around 100 speakers on very interesting topics like infrastructure, mev, block production and everything as we know today. I want to give it over to Acram for the next one. Our next guest is a large part of the reason why we are here. The modular blockchain was incepted by him, and the theory and the implementation of this technology is at his helm. And as I mentioned last year in Amsterdam, a lot of what we talked about was laying the thesis of the modular blockchain. And a lot has changed.
00:15:42.166 - 00:16:36.158, Speaker A: So it makes sense to start off with a modular state of the union. So I'd like to introduce our next guest, co founder and CEO of Celestia Labs, hacktivist Mustafa Albassam. Does the clicker work? Hello. Hello. Good to see everyone. Welcome to Modular Summit 2023. About a year ago at Modular Summit 2022, we hosted an event in Amsterdam where we tried to tackle a problem that has plagued us in the blockchain space for over a decade, which is this problem that monolithic blockchains don't scale.
00:16:36.158 - 00:17:32.286, Speaker A: And we've ended up in a constant, endless cycle of new monolithic layer ones. Every single cycle that fizzle out and don't live up to their promises. So to recap, I'm going to explain what modular blockchains are and what their benefits are. Then I'm going to talk into a little bit about the progress of the modular stack today, some open problems and what the destination is that we should all be aiming for. So when the Bitcoin White paper came out in 2008, it introduced a model of blockchains that kind of stuck around for the next decade which is this monolithic model of blockchains, this monolithic era. This model where a blockchain couples consensus and execution. A model where every user has to execute every transaction of every other user which we all know doesn't scale.
00:17:32.286 - 00:18:39.606, Speaker A: A model that limits flexibility because you're enshrining specific execution environment and you can't experiment with different execution environments. But in 2019 I proposed lazy ledger which is like a very simple blockchain that only does consensus and data availability. And in that model you have a very roll up centric model where you have a data and consensus layer that is only responsible for consensus and then an execution layer which could be a roll up that posts its blocks to a date layer and inherits consensus and security from the data layer. And this basically ended up in a modular blockchain ecosystem where consensus and execution are no longer coupled. So what are the layers in a modular stack? Let's very briefly go through them to recap. So the first layer is consensus. That's the layer at the bottom and with consensus provides an ordering over arbitrary messages.
00:18:39.606 - 00:20:20.570, Speaker A: So developers import messages or transactions into the system and the consensus layer simply decides what the order of those messages are. And then once those messages have been ordered users need a way to verify that they've actually been published to the network because what could happen? A validator could execute a block or data withholding attack where they only publish the metadata of the block header but they don't publish the actual data. And in that model with that attack no one will know what the actual ordered messages are and then no one will know what the state of the chain is and be able to generate fraud proofs or progress the chain. And interestingly, if you actually go back to the original Bitcoin White paper, the proposed solution to the double spend problem was this idea of a timestamping server and I'll just read that out here, which is a timestamp server works by taking a hash of block of items to be timestamped. Widely publishing the hash such as in the newspaper or usenet post the timestamp proves that the data existed at a certain time obviously in order to get into the hash. And this is basically describing what the core property or what the core thing that a blockchain provides are which is ordered data that is made available and timestamped. And if you have this basic primitive which is a timestamping server which is basically a consensus and data layer then you can pretty much build anything on top of it using any kind of execution environment.
00:20:20.570 - 00:21:26.660, Speaker A: And because if you understand that data availability consensus are basically the core primitives of blockchain, we figured out scalable ways to kind of scale that using a primitive called data availability sampling. With data availability sampling you have an over a 99% guarantee that almost all the data is available by only downloading a very small portion of the data. And with this primitive that basically means that we don't have to live in the world anymore where users have to download every other user's transaction. And so now you can scale blockchains more directly in a more practical way. And then finally, or well, not finally, there's something after this. But you have the execution layer and the execution layer sits above the data and consensus layers. And what the execution layer does is it takes a bunch of transactions and it outputs a state.
00:21:26.660 - 00:22:19.818, Speaker A: So for example, those transactions could be payments and the state is what people's account balances. And that's what a roll up does, for example, or layer two does. It provides an execution environment to process transactions and to create a state commitment to what people's balances are. And in the modular blockchain model, the consensus and execution layers are decoupled, as I mentioned. And then finally you have the settlement layer. And a settlement layer is basically just like a special case of an execution layer that is used to bridge other execution layers or roll up together. So for example, if you look at Ethereum as an execution layer you have on chain light nodes for roll ups on Ethereum which act as bridges between roll up and Ethereum.
00:22:19.818 - 00:23:18.324, Speaker A: You can bridge assets between them and they can verify. And the onchain light client accepts block headers from the roll up and verifies fraud proofs or ZK proofs. So putting that all together, like what is a modular blockchain? There's something wrong with the slides. Well, what is a modular blockchain? A modular blockchain is basically a blockchain that fully outsources at least one of the four components of a blockchain. As I mentioned, that's either consensus, data availability settlement or execution. Okay, there we go. So what are the benefits of modularity? The first one is obviously scalability.
00:23:18.324 - 00:24:19.900, Speaker A: So for several reasons, I'll just go to two reasons here. The first reason is that as I mentioned, users don't have to execute the transaction of every other user because now roll ups because they have their own execution environment. That means they have their own dedicated computational resources. Like if you spin up a roll up, the roll up has its own computational resources. So even if another roll up gets busy or has high computational requirements, that's not going to affect every other roll up in the system. And secondly, thanks to data availability sampling, you have this loop where the more light nodes you have, the more block space you can have in a secure way because the more light science you have that are sampling, the more data they can collectively reconstruct and the bigger the block size that you can have. Because in a system that does data availability sampling, the light nodes are collectively storing and making all the data available instead of just like one or a few nodes.
00:24:19.900 - 00:25:13.790, Speaker A: Secondly, you have the developers get the freedom of choice. So with Ethereum, instead of being limited with the Ethereum virtual machine, for example. Over the past few years there's been a lot of new developments and advancements in more efficient and more practical execution environments for different use cases, whether that's for scale or for know. There's execution environments that add certain ZK opcodes. And it's not really practical to deploy a new layer one just to make a modification to an execution environment. And so with the modular blockchain stack, you can now just modify EVM a little bit, add the opcode and just deploy a roll up for that instead of having to deploy a new layer one from scratch. And this is also what various projects in our ecosystem have done as well.
00:25:13.790 - 00:26:20.290, Speaker A: And you also have different types of roll ups that you can use according to your use. Case like Sovereign Roll Ups, settled roll ups, validiums and celestiums, and specifically with Sovereign Roll Ups are an interesting case of roll ups that have kind of gained traction over the past year that effectively give the community of that roll up the freedom to fork that roll up. So you basically get the freedom of a layer one, but without the overhead of a layer one, without needing to create a new consensus network or token necessarily from scratch. So, let's talk about what the modular stack looks like today. And because we've made a lot of progress over the past twelve months, this is what the modular stack looked like a year ago in 2022. A year ago it was mostly theoretical, there was various projects in the stack, but it was still very underdeveloped ecosystem. Like Ethereum was the only settlement layer, very few execution layers, not a lot of infrastructure around it.
00:26:20.290 - 00:27:21.456, Speaker A: But we've made a lot of progress in the past year and this is what the modular ecosystem looks like today. We have various new data availability, consensus, settlement and execution environments. But also interestingly, we have a lot of new infrastructure that is surrounding that modular stack. We have infrastructure like Block explorers, analytics providers and so on and so forth. We also have a surge in interest in sequencing providers, shared sequencer provider, shared sequencing and decentralized sequencing that roll ups can use that makes roll ups more sensitive, resistant or more have have better soft commitment finality. And then you have various roll up frameworks which make it very easy for developers to deploy their own new roll up without having to define to write their own roll up from scratch. Stacks like Opstack.
00:27:21.456 - 00:28:16.280, Speaker A: Sovereign Labs. Sovereign? SDK roll kit. Those stacks make it very easy for people to just write their application and deploy roll up. And then we also have roll up as a service providers that use these roll up frameworks and provide a hosted service for people to deploy their roll ups. So instead of having to maintain your own infrastructure, just like you can go to AWS or DigitalOcean today, you can deploy a virtual machine in the cloud in seconds. In the future, you'll be able to deploy a roll up in seconds with a hosted provider with your code. And the ultimate goal here from engineering perspective, should be that deploying a roll up, deploying your decentralized application as a roll up should be more easier and more convenient and more practical than deploying a new smart contract.
00:28:16.280 - 00:29:15.320, Speaker A: And that's basically what we've seen in Web Two. Like in Web Two, if you create a new Web application or you deploy a new website, you don't use WordPress usually, or you don't use, like, a hosted provider. You deploy a new virtual machine in the cloud. You have your own virtual machine because that gives you more flexibility, more scale, and more choice. You don't use a shared hosting provider or a shared platform necessarily, in many cases like a WordPress or Blogspot. Then finally, we have a various number of cross chain providers and mev providing bridging across roll ups, but across different frameworks. But the goal of this conference today is to get everyone to make connections and to talk and to discuss the future of the modulus ecosystem.
00:29:15.320 - 00:30:09.532, Speaker A: Who knows what's in store for 2024 and what new layers or what new types of tooling and infrastructure that we might have not even thought about today might exist in 2024. Like a year ago, people weren't really talking about shared sequences. Now they're older age. Quick few highlights over the past year, in the past few months a few months ago, Opstack was the first kind of ethereum focused roll up framework to add a modular Data availability API. And Vhlsj contributed that. Data availability API. And that made it possible to deploy Opstack chains using Celestia as a DA layer and other DAOs as well.
00:30:09.532 - 00:31:02.812, Speaker A: And to me, this is really the meaning of modularism and not maximalism, because this is like an example of different ecosystems working together and interoperating with each other in a beneficial way. We also have Manta, which is deploying Opstack roll up based on this interface. And then Caldera has also launched a testnet using this. About a year ago, we introduced the concept of sovereign roll ups, which are still controversial in certain communities. But the idea of a sovereign roll up is roll ups don't necessarily have to be a scaling mechanism for L One. They can also just be an interesting and more efficient way to deploy a new blockchain or sovereign chain. Like, instead of deploying a Cosmos chain, you can just deploy a Cosmos roll up.
00:31:02.812 - 00:31:44.496, Speaker A: And a year ago Sovereign roll ups didn't exist. Like there was no implementation of a Sovereign roll up. But now we have many projects actually building and working on Sovereign rollups which is really cool to see. We have Sovereign Labs building Sovereign SDK. They recently launched an Alpha release of Sovereign SDK which is a toolkit that lets you deploy and create Sovereign DK rollups. We have Eclipse, which is roll up as a service provider for Sovereign roll ups. And we also have Rollkit, which recently was able to deploy the first sovereign roll up on Bitcoin, which is really cool because Bitcoin has historically been one of the most maximalist communities.
00:31:44.496 - 00:32:28.120, Speaker A: And this is really kind of like when that was published, that was kind of relic seen as a way to kind of foster cross chain collaboration. We also had Dimension releasing the first IBC enabled roll up using the EVM which is really cool to see as well. And they also have a testnet Live as well. But there's many more highlights which I can't list all in this talk. We have a very Sudesha has a rapidly expanding ecosystem. We also have various applications including gaming providers, gaming chains. After this talk, I know that Scott is talking from August about the world's engine.
00:32:28.120 - 00:33:37.920, Speaker A: Curio did an interesting demo recently where they run a real time strategy game on a modified EVM roll up on Celestial. And there's many other as well, many other interesting pieces of infrastructure and applications on the stack. So we have made a lot of progress over the past year and we're reaching a selection point but there's still, like, a lot of open problems that we need to solve to get to where we need to go and to really make, to really defeat maximalism and to have a positive sum mindset instead of a zero sum mindset. And this conference is meant to kind of like foster these conversations and try to discuss these open problems and progress the so one of the open problems is UX for Bridging. There's still a lot of work to improve UX and Bridging, especially in a Cosmos ecosystem. Users need multiple feet tokens for example, to bridge across chains. I know there's various people also working on that, like Skip, I think they recently did a demo recently.
00:33:37.920 - 00:34:44.020, Speaker A: You can go on IBC Fun, it's a website that has a demo. We also need tooling for custody and payment systems for roll ups and to access resources across the stack. So, for example, you might have a roll up that needs to pay the DA layer or settlement layer. And there needs to be a way to kind of hold these different tokens or do exchanges or do token exchanges. In an easy way for developers without them having to maintain too much like wallet infrastructure and pricing mechanisms and so on and so forth. It's kind of like a good problem anyway, but there's a lot of choice for developers and that can be very hard for the developers to understand the trade offs between those, like different execution environments, different settlement layers, different DA layers. And so I think we need to do a better job at trying to educate developers or explain the trade offs between different components in the stack.
00:34:44.020 - 00:35:42.390, Speaker A: We also have a lot of dependencies across the stack, like a DA layer has to connect to an execution environment and so on and so forth. And there isn't really any common interfaces. Like for example, Opstack has a specific DA interface, Rollkit has a specific DA interface, tendermint has an ABCI interface that interacts with Cosmos. And these dependencies can be very hard to maintain if there's a breaking change in any of these dependencies. So I think we should have some discussion around if there's a way to create some common interfaces or to have better dependency management across the stack so things are less likely to break when improvements are made. We need better proving systems or more work on proofing systems. So fraud proofing systems are still underdeveloped because there isn't a single permissionless deployment of a fraud proof roll up, except for field V one obviously.
00:35:42.390 - 00:36:47.660, Speaker A: And then ZK proofing systems are still slow, there's still a lot of kind of optimizations they need to be made faster. I know there's a lot of work in hardware acceleration and FPGAs to make ZK proving systems faster. And also privacy. One of the reasons why the current blockchain does not have privacy is because you often need to enshrine it into the execution environment. But now we have the opportunity to do that because instead of having to deploy a new layer one just to deploy a new execution environment, people can now experiment with privacy enabled execution environments. I know that Anoma is going to be talking about some of these topics later today and tomorrow. So what we're trying to achieve, what is the destination we're trying to get to? Let's discuss some of the values of moduleism and what we're trying to achieve with a module blockchain stack.
00:36:47.660 - 00:37:32.312, Speaker A: So first of all, users should be first class citizens of the network. This is like an ideal in crypto and web three that seems to have been forgotten over the past ten years. The whole point of blockchains and the whole point of Bitcoin is that you don't have to trust middlemen and that includes Validators and miners. You shouldn't have to trust middlemen and centralized RPC endpoints and APIs because that's just web two all over again. If the main way that users are interacting with web two is just two centralized APIs, that's not like fundamentally different to web two. You're just interacting with a database. So one of the things I appreciate about Bitcoin is that it has very good light node support, light client support.
00:37:32.312 - 00:38:59.590, Speaker A: You can actually install a light client on your phone and that connects directly to the bitcoin network and can get data out of the bitcoin network and without using any centralized API endpoints. So I think we kind of need to go back to this ideal and that's why data availability sampling like finance are important to allow users to get back to the roots of web three and to really allow users to not have to rely on centralized middlemen and endpoints which are prone to censorship and corruption. Secondly, like modularism and non maximalism is one of the obvious important ideals of modulism. And this is pretty much what this whole conference is about. And the reason why this is so important is because over the past decade we've been stuck in this endless cycle of new layer one chains, every single bull run. You had Ethereum in 2014, then in 2017 you had EOS Tron cardano and they promised the world and then 2021 we had Salana and Avalanche and then this time now we have Aptos and Sui. But this is not sustainable because it was creating an endless cycle of new tribes and new ecosystems that are not collaborating with each other.
00:38:59.590 - 00:40:21.490, Speaker A: It's a very zero sum mindset that needs to be replaced with a positive sum mindset where incremental improvements can impact everyone that uses crypto. And we can replace the zero sum mindset with a positive sum mindset by adopting a modular ecosystem or modular stack where people can for example, if people make a more efficient execution environment like Aptos and Suvi have solana. Have you can just replace that layer in the stack you can replace execution environment in the stack without having to deploy a new layer one. Because it's simply not sustainable to have a constant graveyard of new layer ones that are sucking up a lot of funding but eventually extract value, then fail to get traction. Crypto is never going to mature with this cycle and it's really important that we escape this endless cycle as soon as possible to have a more positive sum crypto ecosystem that actually kind of develops into worldwide mainstream developer adoption. And finally, one of the important aspects of Montalism is that communities have the choice to be sovereign. If they want to, they don't have to, but if they want to, they can.
00:40:21.490 - 00:41:39.972, Speaker A: And sovereignty is basically their freedom to fork. Because one of the fundamental things that crypto and blockchains allow, that previous systems haven't allowed, is the ability for a group of people with a specific shared goal to kind of thrive through self organization and collective action. By effectively having a creating a contract with each other that for the first time does not need to be enforced by physical law or like police or courts, but can be enforced cryptographically on a peer to peer network. Whereas previously if you wanted to create a shared agreement, you would have to do so under a specific jurisdiction. But with blockchains you can kind of bypass all of that and have a direct top level social contract. And the top level social contract gives you the freedom to fork if the community decides that they want to change the protocol rules. So to recap the three values of blockchain modulism, users should be first class citizens as a network by focusing on light nodes and allowing people to run light nodes.
00:41:39.972 - 00:42:08.690, Speaker A: Secondly, modularism and not maximalism, it's really important that we escape the layer. One blockchain monolithic loop. Otherwise crypto will never go up. And finally, communities can choose to be sovereign because they have the right to fork if they want to. So I really hope that you enjoy the conference and a lot of interesting conversations happen. I'll be around and many of its lecture team and other teams on the ecosystem will be all around. Thank you.
00:42:08.690 - 00:43:16.936, Speaker A: There you have it folks, the modular State of the Union by Mustafa. In his presentation you may have seen the ecosystem map. And in the modular stack, one of the hottest and most innovative areas is gaming and our next guest is spearheading that. And I'd like to introduce Scott Sunarto who will be speaking about world engine horizontally scaling roll ups with shards. Cool. Hello everyone and thanks for coming today to kind of like moderate summit. And today I want to talk about a little bit of something that we've spent the past almost three quarter of a year working on.
00:43:16.936 - 00:44:03.380, Speaker A: But the story, as you will see, is much longer than it currently has, seems like. So yeah. Cool. All right, so quick introduction about myself. Before I started Argus, I was one of the co creators of Dark Force. Dark Force, for those of you who might not have known, is the first full yonchain game on Ethereum built using ZK snarks. That started with a simple question of like what if we create a game where every single game action is an onchain transaction? Back then, in 2020, this was an absurd thesis.
00:44:03.380 - 00:45:10.636, Speaker A: A lot of people was wondering why would you make fully on chain games and how is this even possible with blockchain being as slow as it is? But regardless, driven by our curiosity, driven our just like our kind of culture just fucking around and finding out, we decided to build Dark Force. And so this is like what Dark Force looks like back in 2019, 2020. And yeah, it's basically a space exploration game where thousands of players were fighting on chain to expand their empire. And within the first week of launch we had more than 10,000 players or wallets and trillions of gas spent in the Ethereum testnets, attracting a large number of players. And eventually we had to move on from testnets because other developers who want to test their applications were not able to and eventually move to a side chain. And even in a side chain it is supposed to be more scalable. It turns out that it is not as scalable as we think.
00:45:10.636 - 00:46:49.768, Speaker A: It is, we quickly filled up the entire block space, driving up gas costs and practically making side chains unusable with only a single application. And so now begs the question, right, so with all of that limitations, why are people still so excited about on chain games after Dark Forest? We've seen many people, investors, founders, builders, hackers alike, building on top of the legacy of Dark Forest with things like Lattice, building Mud, a library and framework to build on chain games easier, and also content companies like primordium building a fully on chain games and also things in auto ecosystem beyond the EVM, like Dojo on StarkNet. The key thing here is that we realize that the limitations that we have with existing blockchain comes down to the fact that we are sharing a chain with everyone else. We are sharing this very small room where there's not a lot of space with other applications that also want to use that. And so if you see on the chart there, you can quickly realize that if there's another game like Dark Forest living on that same chain, then there could not possibly be a functioning chain. And so the question is like what now? Right? Do we just give up? Do we just kind of just throw away the concept of onchain games? And so we decided that no, we want to actually explore how we can build better on chain games. And we want to make sure that the game that we're going to build next is going to be scalable.
00:46:49.768 - 00:47:27.464, Speaker A: And so we embark on a journey that starting with a one big key realization that we currently took blockchain architecture for granted. We have all these L ones and L two S, and yet they all look the same. They all will tell you very very similar things. They'll tell you that our consensus is better, we have better trade offs than the other blockchain. They'll tell you that their VM is faster, they'll tell you their VM is like paralyzable. Or they'll tell you that maybe if they're an L two, their fraud prover is better. Or how their other competitors fraud prover doesn't exist, or their other roll up is fake news.
00:47:27.464 - 00:48:12.808, Speaker A: Some people will claim that their approving system is better, like our ZK approvers are faster, although we never see the benchmark. But all this war to an end to create yet another Dex that you can deploy on any other chain, like doesn't matter what VM it is, it all just looks the same. Or to mint yet another NFT that again you can deploy on any other chain. We decided to take a step back and think differently. Think how we can actually see blockchain architecture from a different lens. We ask the question of like what if we escape the classical blockchain architecture? Every blockchain that we've seen until now, it all looks similar to Bitcoin, all looks similar to Ethereum, behaves the same way. You have.
00:48:12.808 - 00:48:37.392, Speaker A: The concept of gas. You have the concept of transactions. Like people send transactions, they submit it to the blockchain and that causes a state transition. It all looks identical to each other. And another key thing that we realize is that other people and other blockchains let it be L one or L two. They're all trying to build a blockchain for everyone. And I'm not saying this from a wholesome way.
00:48:37.392 - 00:49:43.552, Speaker A: It's more of the fact that they don't really take into consideration a specific kind of use case or a specific kind of user persona and instead they just try to build something that everyone will just assume is compatible with their product in mind. We took a different direction. We choose to build the best blockchain for a very very specific user in mind. In our case being players and game developers. And so again we asked this question and it comes down to really understanding how games are vastly different than your typical applications. So for example, application that you might use a lot is things like social media, like Twitter and the other kind of things would be games like minecraft on the other hand. And so here we can see that with Twitter, twitter acts in this very straightforward way where if you write a tweet and then you click a button, then you will have your tweet posted.
00:49:43.552 - 00:50:32.256, Speaker A: One thing to note here is that every time you click a button there is like user input event and this is what triggers the state transition. This is like what I typically like to refer to as an event driven runtime. As you can see, this is very much similar to how your typical blockchain would look like a user would want to trigger an event and they would click send a transaction and that transaction would inflict it. But if you see games on the other hand, games doesn't behave the same way as like a web application. Even without user input, even if you're AFK, if you're just like going away with your brother, state transition still happens. Like fire will continue to cause damage, water will continue to flow, wheat will continue to grow and day and nighttime will continue to move. And this is like what we like to call the loop driven runtime.
00:50:32.256 - 00:51:22.324, Speaker A: Key thing here is that no user input is needed to cause the state transition. And so drawing an example back, you will notice that the web app is again very similar to like smart contracts. Let's say in uniswap a user wants to trade token A and token B, you would submit a transaction and that trade is executed. Again, event driven Runtime we realized very quickly this event driven runtime nature of classical blockchains are just not compatible to run a game state machine. And so we kind of explore deeper the loop driven runtime that game has. And again all game engine is specifically built to support this loop driven runtime. The key thing with the loop driven runtimes is game progressions and how times are organized within the games is referred to as ticks or the atomic unit of time.
00:51:22.324 - 00:52:16.340, Speaker A: Each game loop is going to be executed in a single tick and the higher tick rate that a game has, the more that the game feels more responsive. If you played game like Counterstrike or Valoran, you'll notice that these games have 20 tick rates or 60 tick rates for the modern game servers. While games that are more old, they'll typically have lower tick rates and often you feel sluggish doesn't feel responsive, right? In blockchain you can treat these ticks to something that's analogous to a block. It's basically a single unit of time where state transition happens. If a tick or a block feels late, you visually and feel lack in the game. And if you've built games before or even you've played game, you know how suck it feel when your game move lack and you die and you get killed and they would rage onto the enemy or the game developers. It's not a plexant experience.
00:52:16.340 - 00:52:59.056, Speaker A: And so at the end of the day, we believe that games are loop driven in nature. And because a lot of game state transitions are not triggered by external input for example, a gravity doesn't rely on the user pressing the W button to move forward. Gravity will continue to exist regardless of user input. There's also the case for deterministic transaction ordering. Let's say if you want to inflict a damage to your user, should the game heal apply regeneration to the HP of the user first? Or should it inflict damage first with a traditional ordering of just like this random. Builders of typical execution layer. Let's say the EVM.
00:52:59.056 - 00:53:55.888, Speaker A: You can't predict or you can't deterministically control which state transitions are getting applied first. And as a result, you have this deterministic transaction ordering problem that causes reliability issue in the game loop itself. On top of that, you can also allow for more aggressive parallelization by using data oriented system design in these loop driven runtimes. Last but not least is that while some people might talk about it retrofitting event driven runtimes like DVM into a loop driven runtime, we've seen that this leads to a lot of issues because of the nature of how you do gas mirroring, how do you do accounts. That is not as simple as just calling a loop or calling a single smart contract function again and again at every block. And so if you have a layer two roll up with a loop driven blockchain for games, what unlocks do we get? The first thing is of course we want to maintain the composability. Making the blockchain from invent driven to loop driven doesn't mean you have to sacrifice composability.
00:53:55.888 - 00:54:58.292, Speaker A: And that's actually the reason why we want to use blockchain as a runtime for these games in the first place. On top of that, with a loop driven game runtime, you can have real time gameplay where you can now start to blur the line between a blockchain and a traditional game server and eliminating the concerns of building a game on top of these roll ups. With the Adiomatic runtime loop driven runtime, you could also build more complex games than you can before in a blockchain. There is a reason why most quote unquote games that you see on the blockchains are mostly just people minting NFTs because that's really what the easiest you can do when you can only have even driven runtimes. And last but not least, the more that you can emulate traditional game engine runtime, the more that you can just treat the user experience much like playing another game. But with all those good things in mind, we still are missing one key ingredient to build a scalable game server blockchain. And the key thing there is the fact that we also need horizontal scalability.
00:54:58.292 - 00:55:58.280, Speaker A: When you're playing a game, you are not only playing it on a single server if you play MMOs are comprised of many many servers. If you're playing Counterstrike, these Counterscribe games are spread out across different sessions, that is run across different computers. But if you realize at the end of the day a roll up runs on like a computer and so they are bound to the physical limitation of computation itself and say games and most large scale applications scales using multiple computers. But if that is the case, then why don't we just spin another roll up? Then if we do the naive approach of just spinning up another roll up, we can lead to composability fragmentation, smart contracts would stop talking to each other. And while there are kind of different construction of shared sequencers, a lot of these constructions are less than ideal for gaming use cases. For instance, you might rely have to depend on crypto economic security to prevent things like locks, Dos, vector and doing like atomic. Shared sequencer construction can also lead to constraints on execution layer.
00:55:58.280 - 00:56:56.040, Speaker A: As a result, we need a new strategy to sequence roll up transaction and in the search for that, we had a gift from the past. Again, we'll look into how traditional game servers, especially those with intensive performance expectation like massively multiple online game like World of Warcraft, RuneScape, Ultimate Online and whatsoever through the concept of Sharding. With Sharding, some of you might have known this concept from databases like MongoDB. But the reality here is there's this concept, the concept of Sharding itself comes from game servers first instead of the database first. And so how does Sharding work in game? The key thing here is that there's no one size fits all problem. Again, at the end of the day, shards are just tool in a toolbox, not a prescription of how you should build your game. So for example, in the first sharding construction you can use location based sharding where you can split, let's say, cartesian coordinate into four diagrams.
00:56:56.040 - 00:57:46.412, Speaker A: And like, when a player want to cross from one shard to another, you simply send a message to other shard, and then the player would be teleported to the other shard. The second construction is by using something that we call multiplayer charting. If you've played MMO games before, you might have seen that when you log in, you have multiple servers that you can choose. This is the same construction that you can do where you can have distinct states or distinct game worlds that the players can decide to join. So now we have all the ingredients here, right? We have the loop driven runtime, we have the horizontal scalability, and we also want awesome composability. All of this sounds great, but how do we achieve this in a roll up, right? This looks like things that on the server seemingly looks like beyond the reality of like, we couldn't get from a blockchain. But this is like why we created Roll Engine.
00:57:46.412 - 00:58:21.812, Speaker A: We realized that we can't just use a normal roll up and expect that it will run the way that we want it. We took it to our own hands to actually build the solution that we need. The same way that back in the 1990s, when you want to build a treaty game, there's no treaty game engines available out there. You have to build them by yourselves. And so the world engine is again divided into two key parts. The first one being the core, which is comprised of two key elements, which is the EVM base shard, which is a hybrid execution layer and sequencer with sharding support. And the second part is a game shard, which is a high performance game engine plus execution layer.
00:58:21.812 - 00:59:04.920, Speaker A: And on top of that, you have peripheral components like transaction relay and netcode to do the client server communication. And also things like ZK Cloud prover for you to have ZK games like Dark Forest. And so the role engine core really comes down to our very specific design of our sequencer. While other sequence like shared sequencer construction would optimize for having atomic composability, we decide that atomic composability are extremely overrated, especially when you're operating on the context of games. And that's why we went full asynchronous. And so we don't have to put locks on the runtime. In EVM base shard, we have a global EVM chain where players can deploy smart contracts to compose with games, create marketplaces DEXs.
00:59:04.920 - 01:00:08.040, Speaker A: We built this on top of Polaris, which is Cosmos SDK compatible EVM module that allows us to customize the EVM to much greater extent than what we could buy, let's say just like naively forking get. On top of that we have the Gameshart, which is running on top of the EVM base shard sequencers, which is a high tick rate mini blockchain designed to serve as a high performance game server. The gameshart is also designed to be state machine and VM agnostic. We built an abstraction layer much similar to Cosmos SDK EBCI so that you can customize a shard to your liking or build your own by implementing a standard set of interface. We've also built the first game shard implementation to provide an example. And we use an entity component system that is commonly used by game engines and with construction that prioritize entity component system as a first class citizen. So every single objects or every know primitive on the state machine itself is treated like an entity.
01:00:08.040 - 01:00:45.770, Speaker A: So things like accounts are a part of ECS. Transaction is a part of ECS system. It also has configurable tick rate. So you can customize your game to be as fast ticking as possible, or you want to slow it down to prioritize more number of entities. And the best part of this is that you don't need to rely on indexers. You can have fast reads on a blockchain without having to have this lack of eventual consistency indexers like what we currently have with Mud, Dojo and whatsoever. And my favorite part of all is that you can write your code and go, so you don't have to wrestle with smart contract languages that can be very limiting sometimes.
01:00:45.770 - 01:01:40.868, Speaker A: And so the key thing, again, as I mentioned, shards are agnostic in nature to our abstraction layer, so you can build other shards construction like a Solidity game shard to complement your cardinal game shard. You can also build NFT Minting shard that have custom rules by, let's say, custom mempool and ordering construction to solve the basic Minting noisy neighbor problem. You can also create a game identity shard to use NFT to present your game identity and allow you to trade your game identity as well. And again, we don't use slugs and as a result we don't have to block the main thread and making the game shard runtime as reliable as possible and not causing any lags. And we don't have to rely on crypto and economics construction anymore. On top of that, we also have many interesting shard properties like how each shard can have different DA batching compression strategy. And you can also geolocly shards to reduce gameplay latency.
01:01:40.868 - 01:02:23.204, Speaker A: And last but not least, you can also run game shards as an independent game server onto its own, so you don't have to worry about roll up deployments on day zero. And so we've built many various games on top of the game shards. We've built like a fully on chain agario which have traditionally not been possible using our full world engine sequencer, stack gameshart, Nakama and so on and so forth. We've also worked with a hybrid model where you would use existing game engine frameworks on Solidity and combining that with a world engine. And the future is for you to decide you're going to use our cardinal stack, you can do hybrid, you can build your own gameshart. It's basically kubernetes for on chain games. It's mix and match Lego for your games.
01:02:23.204 - 01:02:44.364, Speaker A: And so now you're able to try out the World Engine. It's now open source on our GitHub. We are welcoming you contributors for people who are interested. Feel free to reach out afterwards. And if you're interested in building your first World Engine game, we are having a workshop later today at 11:30 a.m. At the Kaji Stage. And then tomorrow we're also going to be hosting the gaming track.
01:02:44.364 - 01:03:11.672, Speaker A: We have a panel and we also have a talk about onchain games at the four year stage. And last but not least, the key takeaway from my talk would be that let's build core roll ups. We are right now at a roll up renaissance. And what we already know is that of course roll ups allow you to scale a blockchain. Of course, roll ups will allow you to tap into the security of the underlying L one. But right now we are still living on this very EVM centric conception of roll up architecture. This is the starting line and not the end.
01:03:11.672 - 01:03:57.956, Speaker A: What we want to go towards is a user and application centric roll up construction where people can build cool shit. And yeah, that wraps my talk, and thank you for listening. Thank you, Scott. The next topic covers one of the bleeding edges of crypto intense. What are they? Why do they matter? Our next guest is Chris Gose from inoma who will talk intent centric roll ups. Thank you. Let's see if I can answer any of those questions.
01:03:57.956 - 01:04:27.596, Speaker A: All right, so if you read the schedule, I think I started with initially a slightly different title, which was something like Privacy Preserving Runtime Rollups for Efficient DA Sampling. But I've been sometimes told that I use too many words, so I tried to simplify it, and now it's just Intent centric Roll ups. I'm Christopher. Thanks for coming. Wow, this is a beautiful venue. You may know me as the co founder of the Noma Project. I also worked on IBC before, but really I'm just an armchair philosopher in denial.
01:04:27.596 - 01:04:58.648, Speaker A: So this talk started by a little bit of a meditation of mine on what exactly is a roll up. And as someone maybe adjacent to, but not always part of the modular ecosystem, this has not been entirely clear to me. So on Monday I went to the Celestia.org Glossary, which seemed like a good canonical place to look this up, and found this definition. Quote a rollup is a type of blockchain that offloads some work to a layer one. Like Celestia good marketing rollups host applications and process user transactions. Once those transactions get processed, they are then published to layer one.
01:04:58.648 - 01:05:33.312, Speaker A: It's layer one's job to order those transactions and check that they are available at minimum. It's a good definition, but there are three words here that I think we might come back to later in this talk. And those three words are type of blockchain. So I ask you now just to meditate, what exactly is a type of blockchain? But for now, let's move on to a term that is even more criminally underdefined, which is intent. So what is an intent? Noma has been using this word for a while. We've been perhaps criminally vague about what exactly it means. We wrote it down in some ways, but it recently became popular.
01:05:33.312 - 01:06:10.592, Speaker A: Honestly, I think it has very little to do with us because a lot of people are using it in a lot of ways that are not perhaps exactly the ways that we went, but they seem to be sort of correlated. So here are a few takes on intents. There's Uma from Succinct talking about intents for cross chain bridging. UX very important at Research Day in New York. Here's Penumbra talking about intents for basically user wallets intents for users thinking about how to declare what they want a transaction to do or not do. Then of course, we have the radical takes tux intents, turn your front upside down, get your kids to call you back. Intense are a girl's best friend.
01:06:10.592 - 01:07:21.640, Speaker A: Okay, so maybe back off a little bit. And if we language is this like, OG decentralized coordination scheme, right? And we don't really get to pick individually what exactly terms mean. To me, the interesting question is, when you zoom out and look at how people are using words, there's some commonality here, right? There's a reason the word intent became popular so quickly, so fast, and that people can use it with each other. And they all seem to understand, like, roughly what they mean. And I think what they mean is that an intent is if you put it in kind of slightly more mathematical terms, an intent is a commitment to a preference function over the state space of a given system, right? So as opposed to a transaction which specifies a specific imperative execution path, do A, then B, then C, an intent says, I want an execution path that satisfies these constraints, right? I have these preferences over what gets executed in the state space of a given system. This is what varies, right? So when Uma is talking about cross chain bridging, the system is like the chains that you're interested in bridging between when anoma is talking about who knows what. As we'll get into a little bit later, the system is like the information flow, right? So the bounds of what the system is that the intent refers to can change.
01:07:21.640 - 01:08:02.628, Speaker A: But in all cases, intents are these sort of like commitments, credible commitments to preference functions. So I want to zoom out a little bit and kind of just analyze like, okay, so here on one side for the modular ecosystem, there's this concept of roll ups, and everything is kind of organized around roll ups. And from our land, we come from the concept of intents, and everything is organized around intents. And I think these are two interesting concepts because they come at the problem from different directions. To me, roll ups are kind of bottom up. Like you start with this modular thesis, you start with data availability as the base layer and then you sort of build roll ups on top of that. And then we see a kind of proliferation of different execution environments, different approaches to Sharding, as was covered in the last talk, stuff like this.
01:08:02.628 - 01:08:54.880, Speaker A: So roll ups come at things bottom up, to me at least, intents come at things, quote unquote, top down or like users down. Like users have intents, right? They're always going to start with intents and the system had better figure out how to do something reasonable. In some senses, as system architects, we actually don't get to choose. Users are going to have their intentions, their intents when using the system and we just have to try to build something that can satisfy those as credibly and fairly as possible. So in particular, I want to ask the question of can intents sort of help out the modular ecosystem? So Mustafa in his talk earlier brought up a bunch of challenges at the end and kind of open problems. And I have a slightly different list, but I think it shares several components. So three challenges for modular, as I see it at the moment, just kind of an architectural paradigm are these inefficient Sharding, application lock in and user facing complexity.
01:08:54.880 - 01:09:40.660, Speaker A: And by challenges I don't mean like flaws, I want to be clear. I just mean as the ecosystem is sort of moving at the moment, if we try and foresee what might happen and try and avoid some potential problems while we still have a chance to steer things, here are a few things I think we should just sort of be cognizant of. And I bring them up here because maybe I think intents can help, but let's just go over what they are first. So challenge number one is inefficient Sharding, the most expensive thing in the world of distributed systems is atomicity. Because atomicity requires that you send messages to one place and you order them there. And that always implies at least some kind of N squared consensus communication. And it just means that you have to be processing things in like one location, right? It's the typical problem of sequential behavior in concurrent systems.
01:09:40.660 - 01:10:12.488, Speaker A: Users of course, want everything. In particular, they seem to want cross roll up transactions, applications, interactions, token transfers, stuff like this. So users are not going to be thinking about like, oh, I should put all my state on roll up once that it's most efficient. No users are going to say, I have asset here, I want asset there, do something, make it work. And it's our job to try and make it work. In making it work, we want to kind of give as much freedom as possible when it's properly constrained to the operators. Of the system to make it more efficient.
01:10:12.488 - 01:11:09.808, Speaker A: So in particular, if we envision a world of roll ups and all of the roll ups have different applications, right, and the applications are like bound to specific roll ups and users want to do lots of cross application interactions, then we've added this weird constraint where it's like there's a sort of demand for atomicity, right? Like users want to interact with application A on roll up one and application B on roll up two and they want that interaction to happen atomically or there's a bunch of shared state. And if we tie applications to roll ups and kind of have separate sequencers, then we get the sort of static sharding system where we can't change the topology of which different roll ups are settled atomically, right? It's static. And if we view kind of the demand for atomicity as varying over time, perhaps dynamically, this seems to me like it's inefficient. We've sort of added this constraint. That's challenge number one. Challenge number two is application lock in. One of the great things about the modular stack is that you can build heterogeneous execution layers very cheaply because it doesn't require deploying a whole layer one.
01:11:09.808 - 01:12:19.808, Speaker A: But one challenge with heterogeneous sort of heterogeneous protocol execution layers is that they make applications less portable. So the EVM, for example, is interesting VM, of course it changes quite slowly. One thing that I would like very selfishly often is for the EVM to add new pre compiles for new curves so we can do more efficient cryptography and it would be easy to launch a roll up that forks the EVM and adds a new pre compile for a curve, right? But one disadvantage if you sort of look at this from the application sort of whole ecosystem perspective is that then if other roll ups don't also adopt this new opcode, the app is kind of locked in. Like if the sequencer of that roll up starts charging higher fees, if users can't easily switch, if there's a bunch of application state that gets tied to this very specific different execution system, the application can get kind of locked in. And this means that apps may be paying really for more and more kind of atomicity than strictly speaking, necessary. And the third challenge I see is user facing complexity. So one modular component selection and certainly modular component construction adds a lot of clarity to the design process of blockchains and allows different teams to work on different parts, which I think is very helpful from a coordination perspective.
01:12:19.808 - 01:13:16.792, Speaker A: But especially if these different parts are operated by different sequencers, different validator sets, it tends to entail some complex security assumptions. So if we think about this from the perspective of a user and what the user has to reason about in order to know whether their interaction is safe, right, something like this, if there are different parties for solving execution, data availability, all of these components of the modular stack. That's a lot for the user to think about, right? Different interactions require different safety levels. The user is not going to, every time they send an intent, send a transaction, the user is not going to reason through all of the sort of crypto economic calculus of is this thing in fact safe? Right, given all of these specifics. So I think user facing complexity can be a challenge, sort of one that requires that we come up with good standards for describing what these security assumptions are. And also we want to always maintain the know, as Mustafa mentioned, kind of sovereignty to me, sovereignty also includes the ability for users to easily switch if something goes wrong. Right? Communities like own the system.
01:13:16.792 - 01:14:11.864, Speaker A: They give it value by bringing their applications, bringing their intents to the blockchain and they should have the ability to switch away and in particular to kind of credibly threaten to switch so that they don't have to actually do it because it's cheaper. Right? So I'm going to postulate a kind of thought experiment here and see if it might help with some of these challenges. And that thought experiment is what if at the moment, applications, as I understand, and I could be slightly wrong, but as I understand, applications are kind of defined on top of roll ups in the modular stack right there's, data availability layer, some execution, there are particular roll ups. Those roll ups have state formats, they have instruction sets, they have VMs, stuff like this. Then applications are defined on top of the roll ups. Right? And I propose a different way of defining applications, which is to define applications as intents. So in anoma an, intent is kind of opinionated about some things and unappeanionated about other things.
01:14:11.864 - 01:15:00.244, Speaker A: So in particular, intents specify which parts of state they must modify atomically, right? If we think about the whole system as having a sharded state where the state is sharded by concurrency domains and different security domains, intents specify sort of in an explicitly include fashion which parts of state they must modify. The state can be held on different domains. If you require atomic settlement between two completely different validator sets, that's not possible. Right? But you can specify in the intents which things you need to be atomic and the kind of custodians or the validator sequencers in charge of that state must sign. Right? And I think one way of kind of understanding the relation of this to roll ups. So we have a concept we call partial solving. And partial solving is like if you have some intents, let's just describe the intents abstractly.
01:15:00.244 - 01:15:17.372, Speaker A: So let's say one intent, this is the one on the top left here is Alice. And Alice wants to trade star for dolphin, right? We have another intent that's Bob. Bob wants to trade dolphin for tree. Then we have the third intent, Charlie. Charlie wants to trade tree for star. Right? Okay. We have something we call solving.
01:15:17.372 - 01:15:38.572, Speaker A: Solving basically means matching intents. And solving can be done fully. When you take a bunch of intents, you match them completely. You get a fully balanced transaction. Or it can be done partially. This particular diagram gives an example of partial solving. So in this example, we take Alice's intent and Bob's intent, and Bob already has something Alice wants, right? Bob already has a dolphin.
01:15:38.572 - 01:16:37.530, Speaker A: So we take Bob's dolphin and we send it to Alice. And then we craft a new intent that now requires that we get a tree and give a star, right? So we can do this kind of partial solving by combining two intents, doing some kind of simplification, and creating a new intent that we then self send elsewhere to do some more solving later. So if you think about it abstractly, if we have, like, an A for B intent, a B for C intent, partial solving just takes those two intents, combines them, and makes an A for C intent, right? Then in this particular example, in the second stage of solving, we have this partially solved intent, and we match it then with Charlie's intent. And then we get a fully balanced transaction where everyone's assets get swapped and just a three party barter. So what is partial solving? I postulate that partial solving is a roll up. So if we think about type of blockchain, what does type of blockchain mean? I mean, I think there's some hash linking involved. There's, like some history.
01:16:37.530 - 01:17:00.764, Speaker A: We need to be able to verify this later. Partial solving satisfies all of those properties. It's just kind of on demand. Right? We look at the intent, we do some kind of partial state change sending in this case Alice's no, sorry, sending Bob's dolphin to Alice. We have some state changes that we still need to do. We commit to those. We can perhaps in Anoma's case, we can make them private.
01:17:00.764 - 01:17:24.152, Speaker A: We can even roll them up in a ZK proof. So we have zero knowledge and computational compression. So that's like a ZK roll up, and then we send it onwards. Right? Now, what is not fixed in this particular design is that we don't fix what has to happen afterwards, right? So we just take these two intents. We see that we can do some simplification. We make a roll up. This is what sometimes we've been calling runtime roll ups or on demand roll up.
01:17:24.152 - 01:17:57.552, Speaker A: And then we can send that partially solved, partially rolled up intent onwards. We can do some more rolling up. And then as soon as it's fully balanced, it's like a transaction and can be settled somewhere. Right? So in a kind of intent centric view of roll ups, partial solving and roll up creation are just the same thing. So maybe the difference with some of the current modular stack is just that roll ups are created on demand. Right? And I think that this has some advantages. It allows for this kind of global compositionality in determining the actual topology of sharding at runtime.
01:17:57.552 - 01:18:35.420, Speaker A: Right? Just like when you're processing intents around the network, it preserves local liveness. It does require standardizing on a state format. This is perhaps controversial, but I think that we can do this in a way that doesn't really constrain choices. One nice thing about the way intents work is that intents specify verification conditions, not the execution method. Which means that you could have different instruction sets. So you could preserve basically heterogeneity of execution systems as long as you have the ability to verify. Right? So think about it like if everything is a ZK roll up, then intents specify conditions for verifying, like the other guy's ZK roll up.
01:18:35.420 - 01:19:13.912, Speaker A: If the other guy uses some other opcodes internally, you don't care as long as the condition is eventually satisfied. So in some sense it's a standard which allows you to agree on as little as possible, which is always good in distributed systems. Right? So intense sender roll ups enable dynamic sharding choosing shards at Runtime, intents can specify which consensus providers they're okay with. They can specify more options. So you don't need to fix like sending your transaction to one specific roll up. You can say, okay, I want the cheapest settlement subject to these conditions and here are the security assumptions I'm okay with. This enables the network to dynamically sort into independent atomic bundles.
01:19:13.912 - 01:19:56.212, Speaker A: So it should end up being cheaper for users. Defining applications by intent formats, if done well and kind of standardized, should help a lot with application portability because applications then are not they're not tied to a specific roll up. They can sort of move freely across roll ups. And maybe heterogeneous instruction sets become more like specialized solving algorithms for different domains. The same application can shard its state according to what users want, right? So you can move code and data across chain and application portability in particular gives you as a community or as an application user a credible threat to fork out extractive operators. Right? Because it's easy to move your application code and logic somewhere else. Everything is kind of standardized to a sufficient degree.
01:19:56.212 - 01:20:50.732, Speaker A: Then if someone is charging, if they're like extracting a lot of mev, if they're charging high fees to withdraw your assets over bridges or something like this, then you have a credible threat that you can just leave. And I think you need this in order to constrain the kind of operator extraction in these systems. Then specifically in Enema, we've been spending a lot of time trying to craft a good framework for describing declaratively what information flow users want to allow in intents. So this looks basically like declarative constraints. Intents can say that in conjunction with this atomic settlement, this value X must be revealed to A and B. So x could be a note, could be a key, it could say this value y must be revealed to some other party C at block one, two, three in the future. And these kind of declarative information flow constraints enable things like cross roll up, private bridging, new auction designs, privacy preserving governments, programmatic disclosure of aggregate data.
01:20:50.732 - 01:21:32.596, Speaker A: Yeah, a lot. Information flow control, if done properly, I think can be quite general. So what is blockchain? I'm back to this question and personally, I think a blockchain is a data structure. Like if you take a piece of data and you hash another piece of data and you include it, you've just gotten a kind of partial ordering relation and this is the essential thing. And everything else perhaps can be separated with intent centric roll ups, we just create blockchains on demand. They live very ephemeral lives. A blockchain exists for a second when two intents are matched and then it's kind of rolled up and then you can verify it later, but you need to store the data somewhere there's still data availability.
01:21:32.596 - 01:21:52.430, Speaker A: But the blockchains are really quite ephemeral in whether something is an L one, an L two or L three is just an observer dependent finality choice. So shout out to John Charb from DBA. This is kind of my meme summary of this talk. Roll ups are l. Two S roll ups are just L ones. Roll ups aren't real. Blockchains are real.
01:21:52.430 - 01:23:08.032, Speaker A: Okay, so finally a few kind of grab bag slides of just interesting points that I think come up when you look at things in this way. So if we conceive of a world of like, what do the economics of these different systems look like in a world that's modular with intents? And shout out to Zaki, who I think had a Tweet that says something roughly like this. I see kind of two classes of maybe value capture or like two classes of things people will want in an intense sensor modular world. And the first class and this is like maybe this will be controversial to call it a dow, but the first class I'm going to call service provider Dows. And the reason I call them dows is because there's like a group of validators or operators who are working together and are providing services as a collective and they're like coordinating to provide that service efficiently and reliably. But users or sort of applications see it as a service provided as a whole, right? So I would say that one kind of dao you can have in an intent centric modular world is a data availability dao, which provides data availability and ordering. And there's some slight differences here, but at the moment Celestia and Ethereum and the kind of roll up centric roadmap data availability, host layer model are providing this kind of service, right? Then you could have some kind of execution DAOs.
01:23:08.032 - 01:23:35.916, Speaker A: Maybe current rollups are like this. You could have solver DAOs suave, as I understand it, is like this. And these service provider DAOs are competing on the basis of liquidity and sort of role specific optimizations, right? They're like providing really efficient data availability sampling. They're providing more private solvers. In the case of Suave using SGX, they're providing some specific service that they think people want. Then there are just assets people want. People still want bitcoin.
01:23:35.916 - 01:24:00.244, Speaker A: They want ETH. Somehow. They still want the almighty American Empire bucks on the blockchain. And those assets are competing independently of protocols, right? They're competing on the basis of distributions and how good they are at public goods funding. So three concluding thoughts here one, I think intense and modular are like a match made in heaven. They come at the problem from opposite directions. They can kind of help solve each other's problems.
01:24:00.244 - 01:24:42.400, Speaker A: An intent centric architecture. I didn't have time to go over all of it in this talk, but one challenge we've had building Anoma is simply that we don't have specialized primitives, right? Like, we don't have efficient DA sampling. We don't have these individually optimized things. So I think there's a very nice synergy. There. One thing I also really like about the modular blockchain world and kind of some of the conversations we're having here is that it seems like it's sort of a fusion of Ethereum and Cosmos of the Polycentric or self sovereign political ideology with the kind of clear architectural thinking of the Ethereum ecosystem. This also maps like Celestia anoma Many of these teams in the modular world came from Cosmos or worked on Cosmos, and we're now kind of converging with Ethereum.
01:24:42.400 - 01:25:23.740, Speaker A: And finally, I have a kind of shout out similar to something Mustafa mentioned, which is that let's please, please not remake the mistake of building a lot of transparent blockchains that are not going to work. Like, if you're trying to launch a transparent roll up and it's not for some cute game, maybe this is okay. But if you're trying to launch a transparent roll up for financial settlement and you are going to spend years on this, and you're going to spend a marketing budget, and you're going to spend go to market effort and you're going to convince a lot of people to use it, make it private, don't make it public. It's not going to work. That's it. Thank you. Thanks, Chris.
01:25:23.740 - 01:26:07.304, Speaker A: So we have had back to back to back talks, solo talks. Time to switch it up. We're going to do a Fireside chat next. And for our next guest, polygon has been in the news a lot lately, and we're fortunate enough to have Sandeep here with Mo as moderating. Please welcome Sandeep and Mo from Polygon. Moderator just hold it. Hello.
01:26:07.304 - 01:27:29.844, Speaker A: Hello. What's up, everyone? So remember when Celestia and Lazy ledger started? The architecture that's proposed is one of the original Cosmos ideas from like, 2017. And it's very anticipatory of a lot of scaling bottlenecks and a solution to it. Polygon was born in much more practical and reactionary implementation of addressing scaling problems in real time. So, yeah, we talk a little bit about sort of the story and how we kind of get to modularity, but how'd you guys kind of get started in India, decide on architecture for the POS chain Zkevm and then up to Polygon too. Yeah, I mean, when we started Polygon, I think the core team initially, the way we were different is that even though we started building the infrastructure, but we were before that, we were actually building D apps. And it was very clear for me, mid 2017 itself, that building D apps on this system on Ethereum on a public blockchain is not going to scale, most probably.
01:27:29.844 - 01:28:15.972, Speaker A: And it was also very clear that the Dev community liked Ethereum a lot. And there was already a very big community. Now it looks like how did we see it before, like five years back? But even then it was very clear that eventually the Ethereum is going to emerge as the layer one. And now we are seeing even many layer ones, even becoming layer twos and things like that. So it was very clear that we wanted to build something which DAP developers would be able to use. And that is what you see in whatever you see Polygon does. Right, even though now we have extremely high quality research.
01:28:15.972 - 01:29:17.864, Speaker A: We almost spent like $1 billion and now have the best talent in the ZK space and became the first project to launch a full blown Zkevm, which is a layer two built with ZK security on top of Ethereum. The DNA is very clear like we want to build for the developers where real world applications can be built. So everything we do, the core DNA is that and we move as per that mission. The mission is that to bring millions of users in web three and whatever needs to be done for that, that's to be done. So Polygons put more transactions of value and transactions through Tendermint consensus than pretty much any other projects. That maybe the finance bridge and core piece of architecture celestial is also using. What was kind of your guys'decision to utilize that and how's that experience been.
01:29:17.864 - 01:30:18.664, Speaker A: No, I mean, Tendermint is, I think, as a pluggable consensus, you pick it up and build on top of it. I think it's the most evolved, most mature consensus. SDK cosmos SDK previously, this is one of the best ones. And that's why at that point in time, and not only now, only even in 2018 when we were building it, it was the most mature and the best. And I think from our side also, although we are very deep into the ETH community, care about it very deeply. But I think after Ethereum, if there is an ecosystem, which I also and a lot of people in my team, layer one ecosystem, which we respect really is Cosmos ecosystem. And Tendermint is an invaluable contribution from Cosmos to the whole ecosystem.
01:30:18.664 - 01:31:13.912, Speaker A: And even now we are building some things where we need a single slot finality consensus. And obviously, tendermint constantly keeps coming at the top of the evaluation list. Now it's much more evolved. But even then it was very clear to us that as a plug and play consensus into whatever we are building, tendermint is pretty good. And yeah, we are happy that, as you said, that we would have put in more value transactions on tendermint consensus than I think the whole of maybe orders of magnitude bigger than that, actually. Yeah, Luna had a good run. So what was kind of the Polygon experimented with a lot of different scaling technologies as kind of knew the limitations of this plasma side chain.
01:31:13.912 - 01:31:54.360, Speaker A: What was kind of the story and journey to start working with the Hermes team and get to this Zkevm that launched in March. Yeah, so, I mean, when we started building this at that point in time, only like the practical approach, as I said, that we are driven by Pragmatism. We don't want to build sand castles or like some cloud castles which nobody uses. And back in 20, 18, 19, all these layer two approaches were not evolved. We started building plasma. We were the only team which actually delivered a plasma to the main net. But then nobody used plasma at that point in time.
01:31:54.360 - 01:32:30.128, Speaker A: And then we were very clear. And then this optimistic roll ups approach came along and all those things, and we evaluated that and we realized that this approach is also, again, kind of band Aid. Right. This doesn't give you the ultimate infinite scalability on top of ethereum. And that's why we were looking for a better approach, the end game approach, to say. And that looked to be ZK at that point in time. And that's why we focused all our energies into ZK.
01:32:30.128 - 01:33:02.432, Speaker A: And here we are, I think arguably pound for pound, the best team in ZK with the products which are out there in production and getting traction day by day. Yeah. So ZK technology is obviously very hot. It's definitely the core technology underpinning Polygon Two. So that was recently announced in the past few months. You talk a little bit about the sort of architecture and vision and the components of Polygon Two. Yeah.
01:33:02.432 - 01:34:11.860, Speaker A: So Polygon 2.0 is essentially a MultiChain vision, wherein the goal is, as I said that again, our goal is not to provide this blockchain scaling technology. Our goal is not to provide these fancy consensus technologies and all that stuff. Our goal is how do we get 1 billion people into web Three in next five to ten years? And what technology needs to be built for that? And at the end of the day, for the developers, if this needs to become, as we call it, internet of value, it needs to have the similar kind of characteristics that the Internet of information that we see and we call Web Two today it needs to have similar characteristics. What are those characteristics? The current web two world is practically I'm not saying theoretically it's practically infinitely scalable. The more apps, more new different kind of applications are coming in you can spin up more servers, you can provide additional amount of computation, everything is available and it's almost practically infinitely scalable. Secondly, the information is seamless.
01:34:11.860 - 01:35:06.456, Speaker A: Back in the day, like 10, 20, 30 years not tend to, but 30 40 years back when the internet was starting, these were like die separate. There used to be DARPA, net euronet this like different kinds of networks and then this whole www, TCP, IP kind of things happened and then all these individual networks got connected with each other and today we have a seamlessly connected internet across the world. But previously, even if you had information on one Internet, which is let's say one network which is on us and you want to bring it to, let's say a Europe network, you have to do the same thing. Kind of what you have to do today. Like you have to bridge the value from one asset, one chain to another chain. And right now, they're not even safe security zones. That from one chain you take, but on the other side, either you are relying a bridge or you're relying in the destination chain security and all that stuff.
01:35:06.456 - 01:36:09.052, Speaker A: And that's why the value is not seamlessly. It's not easy to move the value seamlessly from one chain to another. And this internet of value needs to have the similar kind of characteristics as the information has. Anybody can create, share and exchange information in the web two world same way it should be possible for value in the web three world. So the point how we have built is that this is a MultiChain environment like where you can spin up as many layer twos you want, as many chains similar to Cosmos Vision, you can spin up as many layer twos as we want but they are all secured by the zero knowledge technology. So all these chains provide their ZK proofs to Ethereum, right? And for all of these chains to have a fast interconnectivity with each other, we propose the interoperability layer everything else. Like otherwise this cross chain Lxly bridge and everything is built out from our side.
01:36:09.052 - 01:37:22.112, Speaker A: But now we have proposed a fast interoperability aggregator layer which what it does is all these chains submit their ZK proofs extremely fast. Already we can do two minute proofs as per our current technology and the new upgrades that are coming up you will be able to every chain will be able to share or create proofs like in five to 10 seconds and eventually going to 2 seconds also. So every chain essentially creates 2 seconds proofs submits to this aggregator layer and all these proofs of different different chains, they get aggregated and get submitted to Ethereum. So you have eventual hard finality on Ethereum. But all of these chains, let's say I am on chain number 100 and I want to interact with a transaction, a cross chain transaction that is coming in from chain number ten. Let's say the moment chain number ten, submits the ZK proof on this aggregator layer on chain number 100. And all the other chains, they clearly know that, okay? This particular transaction which is coming in, that has already proven by the ZK proof on this aggregator layer, right? And I can without trusting, I don't care now that whether that chain has one sequencer, two sequencer, 1000 sequencers, it's a reputed validator on that chain or it's a private chain enterprise chain, I just don't care.
01:37:22.112 - 01:38:14.748, Speaker A: It's a simple value or the transaction that is coming in, which is proven by ZK. So we can prove the execution. It doesn't matter if there is a college dorm room guy who has a chain which has multimillion dollar value flowing into the public chains, the public chains can easily trust that value. So, as I was saying that what this system provides is first of all, it's infinitely scalable that as many layer tools you want to create. Like today, if we wanted to with this aggregator layer, if there were 100,000 chains, this system will still work. If there were 1 million chains, this system will still work, right? And secondly, all of these chains have fast interconnectivity between them and eventually you will reach a place where users won't even realize that all this bridging and all that for them, they are clicking. Or let's say I am playing some game in some chain which has a dedicated capacity to that chain, almost like a server.
01:38:14.748 - 01:38:57.824, Speaker A: I play, I get some money, I want to swap it to USD on a public chain where the liquidity is there for a user. It should be a simple click and the transaction happens. That's the ultimate vision. And again, as I said that it rolls back into our ultimate mission, which is again, how do we get massive users across chains? Yeah, so that sounds like a fairly modular architecture. Would you kind of agree with that? No, definitely that's a modular architecture. And modularity is the end game for this. We can't have monolithic architectures like many layer ones, without naming them.
01:38:57.824 - 01:39:45.360, Speaker A: Many of the layer ones have postulated that there is one single layer. All the transactions of the world live in that even if you are not a technical person, you can understand that would have limits of physics on that, right? You can't have the whole world's, data, transactions, everything in one place. So modularity is going to be the end game and all. Kudos to Celestia team. I think you guys own the modular concept because originally coming up with this separated parts of the stack with this idea. But then from our structure, I think it naturally evolved into that where now we have multiple roles like polygon 2.0 also postulates that that basically you have multiple roles within the system.
01:39:45.360 - 01:40:32.332, Speaker A: First, all the Polygon validators have hundreds and hundreds of these chains to validate on. Plus you have multiple roles. So you can either be approver, you can maybe a data availability cluster provider know, we call it local data availability clusters. If you don't want to go into a public data availability chain. But the system is very open if you want to have a data availability on Celestia, on avail or any other data availability provider, our system is pretty agnostic to that. And then you have the validator layer. And the way some of the strides that we have made on the ZK level right now, a lot of these things are being productionized, but on the research level everything is built out and eventually we'll be able to have decentralized prover layers.
01:40:32.332 - 01:41:30.524, Speaker A: And then obviously this aggregator interop layer is also one more layer where people can collaborate so automatically yes, this is like a fully modular vision. Yeah. So that aggregator layer, I mean, it sounds kind of a lot like sort of the IBC vision key part of this unified liquidity piece. Can you expand a little on the Bridging thought process right now? And the Lxly bridge that's currently proposed for polygon too. So the functionality wise yes, is exactly like IBC, that you can accept value coming in from one particular chain on the other chain and the whole ecosystem is interconnected seamlessly and the user experience is real good. So functionality wise, exactly like it's an interconnectivity protocol. I would say in terms of how it works, it may be actually the reverse of IBC.
01:41:30.524 - 01:42:43.668, Speaker A: On how IBC works is that each chain this is as per the best of my knowledge, that how IBC works is that each of the chains is running a light node of the other chain. And then whenever any transaction is happening, you rely on the light node information that you're getting, but you have to still rely on the consensus on the other chain, but what aggregator layer is doing. So basically the block creation or verification is spread out. Like each chain has to verify the verification of the other chain separately. Whereas in the aggregator level we are actually aggregating the proofs of all the chains, like all the ZK proofs are being aggregated on one layer and then every other chain can simply take that proof and rely on the transaction. So in terms of construction, it's completely the reverse, but functionality absolutely the same. And then would all the liquidity, the secure liquidity kind of bridged over from Ethereum, would all that sort of be aggregated and available to any of these potential two chains? Yes, that's the whole chain.
01:42:43.668 - 01:43:44.136, Speaker A: That's the whole Lxly bridge where you can call it like a master smart contract, non custodial contract, which all chains connect into. And that smart contract actually then any ZK proofs you are submitting. So you can directly move your funds from one chain to another and you can directly exit to ethereum because the liquidity is being aggregated in one single layer. So a lot of talk about validiums have come up. Obviously the plan right now seems to be the existing proof of stake chain will migrate to a zkvm validium at some point. Can you kind of talk about the structure of a validium and what are some considerations for builders, really? Like Cosmos started as this hubs and zones concept where you'd have your own kind of cities and towns. Like every city kind of provides the same infrastructure but has its own culture and growth trajectory and values.
01:43:44.136 - 01:44:42.356, Speaker A: And it sort of seems like that's still kind of the MultiChain goal though people don't really talk about it that much. But yeah, what's sort of the architecture for validiums? And why should developers consider building with them the architecture that I talked about that you have multiple chains, they have prover layer, they have data availability layer and there is an aggregation layer for everything. Validium is one special case of this whole architecture. Like validium in the concept of ethereum. Actually in the context of Ethereum, when you have both the data as well as the proof on ethereum, that's a roll up. And then when you have only the proof on ethereum and data can be elsewhere, that is the validium construction for ZK roll ups. And it's actually specially applicable to ZK roll ups because on ZK roll up the sequencer cannot cheat once the ZK proof is submitted.
01:44:42.356 - 01:45:25.532, Speaker A: That's why it's called validity proof. Like in case of optimistic roll ups, for example, they are called optimistic roll ups because you optimistically assume everything is correct and then you expect that somebody from the community, if there is a fraud community, will run a fraud proof. Whereas in ZK these are mathematical proofs, these are validity proofs. Once the proof comes in, it's accepted on ethereum, you know that the sequencer has executed the transaction still this proof correctly. So that's why you don't need data on ethereum blockchain basically to validate the chain. In case of optimistic rollers, the data is not there. You can't even validate the chain because you don't know what is happening.
01:45:25.532 - 01:46:06.730, Speaker A: In case of ZK, the ZK proof itself is the validation of computation. You need data only when you want to exit or you want to do any kind of cross chain operations and all that. That's why what you want to do is you want this data to be fairly available, but that data doesn't contribute too much to the security of the chain per se. 100% of course there is a weird ransom attack and all that, but there are multiple ways to address that using force exits and all that. I'm not going to that. But point is that with ZK, this construction is possible where you can have the execution layer submitting only the proofs. And the data is elsewhere and that's where all the data availability chains become.
01:46:06.730 - 01:46:58.090, Speaker A: So, and I'm sure you get this question a lot, but we have a very cosmos has been very much known for its tech for a long time, but the community is constantly asking where we're going to find users. Polygon has been very famous for its BD efforts and output. What's kind of the secret there? How have you guys been able to get the users that you've been able to get? Yeah, I think this stems from the DNA itself. As I said that our mission is very clear. I said mean, at least from my side. And now we have some of the biggest researchers like Daniel and Bobin from Maiden and all that. So their focus is building all these research efforts and all that.
01:46:58.090 - 01:47:44.184, Speaker A: And I think as a team also, we are fairly modular, right? So there is people who are building the tech. And my mission of life is how do I get 1 billion users into web, three, maybe starting with 100 million users, let's say in next two, three, five years, right? So, as I said, from my point of view, we are not here to create fancy technology. This has this approach, that approach, all that kind of fancy stuff that goes around. I am here to make sure this trustless compute. Basically for me, it's trustless compute, this human society. Currently we interact with digital systems. All these digital systems are centralized and that's why we are fooled with so many things in so many ways.
01:47:44.184 - 01:48:35.450, Speaker A: And my mission is that how do we make this trustless world possible? Because we are spending 60, 70% of our time in these digital worlds and how do we make it more trustless? That looks like a natural evolution of humanity. And once that mission is clear, then all of these things flow through that. It's very, very clear in my mind that what we are building needs to have users. If this is something kind of extreme which can't get users, I would probably not vote internally that we should build something like that, even though it looks very fancy. And we as crypto industry, we are very big fans of narratives and all that stuff. I don't care about that. So in that pursuit, I'd say you probably talk to more builder teams than any founder, at least in crypto that I've come across.
01:48:35.450 - 01:49:46.864, Speaker A: What do you hear about Celestia or DA layers when any of those teams are considering their choices? Yeah, I think right now when I speak to a lot of developers, obviously, that 95% of them are on Ethereum only, and on Ethereum as a developer, the developers don't really care that when they are building an application, where is the data going, to be honest, right. And the Celestia kind of use cases, the kind of technologies they are more relevant to, let's say the protocol builders like us. And also I think the roll up as a service providers, the sovereign roll ups things that you are doing because you can't go to the developers and tell them that we'll give you this data availability system. That data. Availability system to the developers. It should be like this is the environment where you can build the app and all the other things are abstracted from them. And this is the job of these system integrators or roll up.
01:49:46.864 - 01:50:38.064, Speaker A: Service providers and all that to make sure that the better data availability products are used for that, be it Ethereum 4844, be it Celestia avail, or all the different kind of database solutions that are coming in. Yeah, I can ta but I don't hear too much from the developers and ideally I should not like we should not even index on it too much. It's actually the job of the infrastructure providers who are going to provide execution environment to the developers. That should be the job of roll up as a service software roll ups and things like that. Yeah, so just make sure the SLA is there and it's as seamless to all the potential users as possible. Cool, well, that's all we have time for. I don't know, do you do Q and A or no? All right.
01:50:38.064 - 01:51:11.020, Speaker A: Thanks, Sandeep. Thanks everyone. Thanks guys. Thank you. Okay, back to the solo talks. Now we're going to switch gears, do a deep dive into DA and our next guest will go through the Avail architecture and show us the nuances there. Introducing Anurag from Avel.
01:51:11.020 - 01:52:56.466, Speaker A: Yeah, good morning, very pleased to be here. Very well organized summit from the chair folks. My name is Anurag and I'm going to talk about Avail architecture today. Can I have the next slide please? How do I yeah, so just to provide some context on what Avail is, right, so Avail was started within Polygon in November 2020 and we recently spun Avail out in March 2023 to become a completely separate independent entity. What Avail is, is a data availability layer which uses combination of virus accoding, KZG polynomial commitments and data sampling essentially. And I'll provide some context on where we are in that. Some of my background is I previously founded Polygon in 2017 and I started the project with my co founder Prabhul in 2020 and we just spun it out just very recently.
01:52:56.466 - 01:53:54.646, Speaker A: So the entire team of Polygon came over to join us Avail. So that's some of the background and you can see some of the history here. So today a lot of people ask me on Twitter regarding the architecture and so that's why I wanted to focus on what Avail is, what are the use cases. And so I'll try to get a bit into the technical details as much as possible within the short time window. So before I get into the meat of the presentation, right, like some context, I know this is a modeler summit audience and so in general, no major introduction required. But essentially what I want to talk is that roll ups are now acknowledged to be the main way to do off chain execution. And if you can see the rise of Ethereum roll ups, right, like all the big activity is happening on layers like polygon zkvm or Arbitrum and optimism.
01:53:54.646 - 01:55:00.446, Speaker A: And so the roll up is now considered as the best way to do off chain execution. But if we consider that the roll up is the way to go and blockchain constructions are becoming more and more modular with the rise of roll ups, now it is important to see what these roll ups really want and what are they hungry for. And the answer is that they really want lots of DA or data availability. And that's kind of the primary reason why we are working on Avail. And I want to make a bold statement here in the sense that every base layer blockchain in the future is going to be a DA layer. Even Ethereum has already pivoted to a roll up centric roadmap and it is like prioritizing pivoting to a DA layer. If you've heard of protodank sharding, dank sharding, all of this point to the fact that the base layer is going to be a DA layer and all the execution is going to move to the rollups on top.
01:55:00.446 - 01:56:04.770, Speaker A: And that is the context in which you should view Avail, that Avail is a base layer that provides scalable data availability for roll ups. Now, what is mean? This is a Celestia is the modeler Summit and Celestia is one of the organizers. And so you'll ask what are the differences between Avail and Celestia and I'll get into that. But essentially Avail is a modular layer that focuses on datability. It does not do any execution, it accepts transactions from roll ups and makes them available via a combination of eraser coding and KZG polynomial commitments. And in a sense, what it does is it kind of orders the transactions that come to it and provides it to the light client network. In general, the mental model for looking at Avail is very similar to what an Ethereum like layer provides to the roll ups on top.
01:56:04.770 - 01:57:00.774, Speaker A: So in that case, the roll ups do the execution on layer two and then there's the base layer that does the data availability. And so you can have a variety of roll up execution environments. So this includes something like the EVM, but also more complex environments like SVM, but also app specific chains. And how we do it is a combination of erasa coding, KZG polynomial commitments and the USP of data quality sampling which allows downloading of block data within a few random samples and we'll get into how that gets done. But in general, a variety of roll ups can leverage this capability. This is the base layer architecture. And in general, if you look at it right, like I'll just go through it in a little bit of detail.
01:57:00.774 - 01:58:12.742, Speaker A: So what is happening is the primary consumers of avail are roll ups. Roll ups accept transactions and basically submit transactions directly to avail. We have this concept of application ID where each roll up corresponds to a particular application ID, and then they can submit that onto the same base layer so that we can have multiple roll ups submitting data to avail demarcated by application ID. What we then do is kind of extend the data or eraser code the data. So if you see this diagram, so the original data is then extended in general via eraser coding. And then what we do is primarily create commitments of the data. So, I mean, if you see this slide, so this is the rough structure of the blocks, right? So if you see the original data, the data from the roll ups is packaged into the block and we create polynomial KZG, polynomial commitments for the data.
01:58:12.742 - 01:59:10.122, Speaker A: And it's erasure coded in such a way. And the homomorphic property of KZG allows us to mirror the erasure coding of the encoded data on the commitments as well. So if you see in the right hand side, the C one to CN are the commitments of the original data. And because of the homophomorphic property of the KZG commitments, we are able to kind of extend that to the erasure coded data as well. And so once that happens, how do I go back this one? Yeah. Okay. In general, we are able to adjust the matrix size.
01:59:10.122 - 01:59:57.020, Speaker A: So what we really create, as I told you, is like an M cross N matrix. It is erasure coded to create in general, we double the data in general. And what we then do is take the commitment and put into the header. So the header has all the commitments to the data. It also has the app index and certain other meta information. Now this block data is then propagated to all the other validators and basically each validator at the moment in the current implementation, regenerates those commitments and comes to consensus on the block. So that's how the base layer works.
01:59:57.020 - 02:00:37.790, Speaker A: And we've already gone through the block production stack. In general, we've used substrate to build the validator node. And the consensus that we use on the network is Grandpa and Babe. So Babe is the block production mechanism and we have Grandpa as the finality gadget. So it's a hybrid ledger in that sense and protects against large number of roads crashing, et cetera. The incentive mechanism is nominated Proof of stake. Why we chose that is basically because it allows for wide stake distribution.
02:00:37.790 - 02:01:21.126, Speaker A: And so what happens in nominated Proof of stake is that you do not delegate to a single validator. You delegate or nominate to a pool, which then is fairly distributed to a large number of validators. So you can do a ranked choice of validators. And so essentially, why is this important is it allows us to have a pretty decentralized set of validators and we can have up to 1000 validators in the validator set. That's what we get from using substrate. We use substrate but of course it's a data availability layer, so there is no execution. So we've disabled all the runtimes and such.
02:01:21.126 - 02:02:53.406, Speaker A: And so it's a very light runtime on the base layer itself. Now, the beauty of this whole construction is we are able to do a pretty neat lightline network. And in general, once the blocks are finalized and the headers are propagated to the lightlines, even if a validator withholds the data the lightline networks, basically because they can sample the block pretty efficiently they can come to know if there's some withholding of data possible. And in general, we want to target a large number of light lines, but in a sense a few hundred or 1000 nodes are pretty much enough to kind of sample the block data pretty quickly to get into the lightlined architecture a little bit, right? So we started with a different implementation initially, but then we had to kind of build the lightline node from scratch. We use a Cademlia DHT implementation, distributed hash table implementation, and so the LCS basically form an overlay p two B network on top of the base layer. Initially they use the full node for the bootstrapping, but over time when you have a number of light lines on network, a new light line that enters the network essentially can start sampling from the light line. So within the light line network there's a P two P network as well as a DHT.
02:02:53.406 - 02:03:45.498, Speaker A: And so you can think of this from a mental model. The light line network almost is like a torrent like network. It's not the same, but you can think of it like that because it stores some of the sample data locally as well for a period of time. How we do the proof verification is that we generate cell level proofs. So as I mentioned, we have this M cross N matrix and so depending upon the site, we'll have these proofs generated on the cell level. And so that's why there'll be a random sampling from these light lines and they are able to verify these cell level proofs. And as I said, within a few samples, or if we have like a few hundred or even thousand light lines, the entire block gets verified.
02:03:45.498 - 02:05:24.740, Speaker A: And of course there's this property that the larger the number of light lines in the network, we can increase the size of the blocks as well in that sense, and I will say from an engineering effort perspective, bulk of the work that has gone into building a whale. And so we have been building this for more than two and a half years now, first within polygon, but now as a separate entity. And most of the work is on the bulk of the effort is on the lightline P, two P, because there are a number of issues that performance, et cetera, that have to be looked into there. This is sort of a visual representation of the data metrics that I talked about. We can play around with the number of rows, the number of columns, and you can see a reference benchmark on our performance in the sense that if you see the table on the right for a two MB block size right, the rows and columns are such and the times to generate the commitments the polynomial commitments are pretty neat. And even if we increase the block size to a 32 MB or a 128 MB, you can see that these are well within our target block time, which we have at the moment kept as 20 seconds for now. And this allows for propagation across the network as well, as well as verification of the commitments as well.
02:05:24.740 - 02:06:46.618, Speaker A: Okay, so I will have time for Q and A after the session. So in general, if you have any questions on the architecture, happy to answer post the talk. But having said that, once we've arrived at this whole construction, what is the ecosystem that we are envisioning that will be built on avail? So, as I said, this is a roll up centric blockchain, right? Our primary customers are roll up developers, infrastructure developers. And so these are the different kinds of solutions that we are kind of looking at, right? So sovereign roll ups, validiums optimistic chains and app specific chains, right? Like I think Cosmos style app chains, but more in the validity proof or optimistic construction manner. And of course general roll ups as well. The way we are thinking about this go to market, and I will get into that a little bit, is that there's been a lot of activity on the L Three area. So a lot of the L two S, all the major ethereum L two S are now looking at their own L Three initiatives.
02:06:46.618 - 02:07:49.470, Speaker A: So if you look at something like an Arbitrum orbit or a ZKsync HyperChain or a Polygon 2.0 or Starquest Fractal scaling strategy or optimism superchain strategy. And so they are basically optimizing for a lot of L three S in general because what they want is to optimize the L Two as a liquidity hub for all the L three S on top. So that's why you'll see in the coming days a lot of one click L Three deployment stacks. Now, why am I talking about this is basically because when we talk about L three S on Ethereum, right, the first thing that they need is a DLA to do dump for the datability needs and they cannot use Ethereum for that perspective. And so this is sort of the graphical representation of how avail will be used in conjunction with these L two S. And we are starting to work with a number of these to come up with these constructions.
02:07:49.470 - 02:08:50.550, Speaker A: We also released the avail attestation bridge recently. It's a pretty interesting construction in the sense that there is a data attestation Bridge between the avail based layer to Ethereum. Right now of course it's on testnet. For now we are doing an optimistic style construction of the bridge but we've been also been working on a ZK Snark based data attention bridge with our partners at Susint. In fact, we just shot a whiteboard session on the construction. It's pretty neat. And so we are going to be working with Suscent in general to create this bi directional bridge between avail and ethereum because Susint if you know, already has a telepathy bridge which proves Ethereum's proof of consensus and now we have with them like zkSNARK based construction which proves avail's proof of consensus which is Grandpa Babe.
02:08:50.550 - 02:10:32.530, Speaker A: So that's one focus area that we are going to work with and of course we are going to work with Sovereign Labs sovereign roll ups in that sense, in the sense that current roll ups are primarily implemented to be verified on a smart contract on the ethereum based layer. But with our data already sampling lightlines what is also possible and especially with ZK constructions and recursive proof mechanisms, we are also able to kind of propagate these proofs to the lightline layer. And in fact we are also talking with a bunch of wallet teams in general to kind of embed the lightline into the wallet itself. Right now lightlines are run via desktop apps or CLI or something like that. But what we envision is eventually they will make their way to wallets and so very similar to let's say Bitcoin lightlines or such right, where the user doesn't even know that the light line is working in the background. And why we are able to make this possible is basically because of the lightweight construction in which these light clients can actually even work on mobile devices and hopefully in the browser and such at some point in time. And so we envision that there will be a lot of light clients and essentially, I mean this is an underrated development but today for a user to verify the state of the blockchain, it's not that straightforward on today's blockchains.
02:10:32.530 - 02:11:34.970, Speaker A: And what we will enable with this combination of DA Lite clients plus recursive ZK proofs is that any user will be able to verify the state of the blockchain pretty easily. And as I said, modular base layers are perfect for sovereign rollups. And along with us, of course, Celeste is also taking up the mantle and so we are very happy to grow the ecosystem together. I'll quickly end with the development stage and timelines. As I said, we have been in development since two and a half years currently on our second long running testnet which is we call it the Kate testnet. It's named after Aniket Kate who was one of the researchers behind the KZG polynomial commitment. We already have a robust set of external validators already on the testnet and we are targeting 200 next month I guess.
02:11:34.970 - 02:12:40.970, Speaker A: And we want to do an incentivized testnet which we want to scale to a 5000 light client number pretty quickly. We'll have an incentivized testnet also this quarter and the main net target is end of Q four or early Q one. That's the main net target for now. We are pretty comfortable in terms of where we are in terms of development, quickly getting to the optimizations. So currently in our base layer, what happens is when the block producer creates the proposes, the block creates the commitments, we propagate this to other validators and they regenerate the commitments at their end. What we want to move to a construction in the future is a construction where other validators can just verify the commitments and not regenerate them which will make it much faster to arrive at finality and such. We are also working on a very neat construction called KZG multi proof.
02:12:40.970 - 02:13:44.020, Speaker A: So if you remember the matrix that we create, we create these proofs at the cell level. And so what we want to do is do submatrix level openings. And so we kind of reduce the complexity of verification pretty significantly. And this will create huge improvements in terms of the opening generation, the DST population and overall keeping the network streams manageable. And of course, while ensuring backward compatibility. And as I said a little bit earlier, we are working on the Zksnarc based data attestation Bridge and this is a pretty neat construction where we are able to prove avail proof of consensus which is Grandpa BAP consensus within the Snark circuit. And I think this will be pretty useful for deploying or connecting our chain to a variety of other ecosystems because of the nature of the bridge itself.
02:13:44.020 - 02:14:22.062, Speaker A: Yeah, I think I wanted to cover whatever I wanted. These are some of the links, important links. Please feel free to scan the QR code. We'll have this presentation available online as well. And generally yes, we are hiring across the board. When we started three months ago we were at 18, now we are at 27 and so we are looking for quality folks to join the team. And so if you are looking to join a growing team, please feel free to ping us.
02:14:22.062 - 02:15:41.742, Speaker A: And this is also our Twitter handle. So looking forward to talk to some of you and of course I'm available and a lot of our team is also available in the event and happy to talk to you as well. Yeah, thank you. Thanks Anurag. So we are going to stay on the topic of DA. Next up will be a panel, I think it's our first panel and I'd like to welcome back Mustafa from Celestia, anurag from Avail Togral from scroll and then Bartek who will moderate. Please welcome okay, so I guess we can start.
02:15:41.742 - 02:16:32.590, Speaker A: So let me first maybe introduce myself. My name is bartek. I'm a founder of l Two Beats. We are the community watchdog for all L Two S right now on Ethereum. And we try to inform the users what are the trust assumptions, the security assumptions of all these solutions so the users are actually aware. And I've got an amazing panelist today representing, I think, three most known projects that promise to deliver DA. So we have Avele, we have Celestia, and we have Togo from scroll, who will, I guess, have an interesting role on this panel because he will represent Ethereum.
02:16:32.590 - 02:17:42.920, Speaker A: So let me just welcome my panelists and let me just start by saying that I've watched this panel last year. It was very docile, everybody was very nice to each other. And I think it was because maybe the space was nascent, everybody was building. But now we are like almost launching or have just launched and things are becoming a little bit more spicy. So let's make this panel spicier and see how we go. So my first question to you guys will be, well, Ethereum community is probably considered to be one of the largest and should Ethereum community really care about your solutions or should they just simply wait for Protodunk Sharding and Dunk Sharding? So maybe let's start with avail and then we just go this direction. Yeah, I think this is a pertinent question to ask.
02:17:42.920 - 02:18:45.466, Speaker A: In general, if you look at the timelines, right, protodank Sharding will come maybe end of this year, early next year sometime. But dang sharding is going to take a lot of time to come because from our perspective, we have gone through the whole cycle of engineering the p, two p on avail as I think celeste has been on the same journey. And think ethereum being a system that is already securing a lot of assets on the chain, it's difficult to kind of introduce functionality that can potentially jeopardize the current state. For example. So it's going to necessarily take some time. And you're also seeing the rise of, let's say, l two systems, l Three S on top. Like I said in my talk a little bit earlier, each major L Two today is looking at this l Three strategy, like ZK, Sing, Hyperchains, Arbitrum, Orbit and so on, right? Everyone is looking to do that.
02:18:45.466 - 02:20:01.486, Speaker A: And right now, let's say if you look at Arbitrum, which is doing a Nova or a Stack web which has a DAC, so all of them are operating DACs, which are pretty decentralized, sorry, centralized. And all these l three S will require some secure DA solutions, which is less expensive than what Ethereum can give at the moment. Of course, the cost will go down with EIP 4844. So we acknowledge that. But in general, if you look at our architecture, and I will agree with Celestia's architecture as well, in the sense that we are able to provide far significantly less cost and quickly before taking up too much time data availability sampling is massively underrated. It's not very well understood by a lot of people. And I think unless there is data sampling implemented on ethereum, I mean, there's a lot of way to go there and we are not really using this construction like things like with recursive proof ZK proving systems already in there you are able to now give proofs, propagate these proofs to the P, two P, for example, to the users directly.
02:20:01.486 - 02:20:38.990, Speaker A: So something like a Starkware which puts proofs on Ethereum every six to 8 hours. Someone like that can actually create intermediate proofs and pass them directly to the users. And of course they need to wait for the proof to come to Ethereum for bridging and such, but it's much faster verification time. Right? Sorry, not taking up too much time, but I think all of these points are important. Yeah, sure. So I think there's definitely place for multiple DA layers. They all have different trade offs, different use cases.
02:20:38.990 - 02:21:11.710, Speaker A: For example, for Ethereum, as mentioned, proto dank sharding is just like a very small step to the overall roadmap of dank sharding set up different trade offs. So for example, Celestia and avail are more overhead minimized. There's no state baggage already. So for example, it's more practical or overhead minimized. For sovereign roll ups, for example, if that's what you want to build. There's also like various design, different design choices. So for example, EIP.
02:21:11.710 - 02:22:03.660, Speaker A: Four four. You can only fit eight Blobs without burst in a block. If you want to do app chain roll ups, you'll probably need some data aggregation service for that to be practical. Whereas for example, on selection avail there's no specific Blob size limit, minimum Blob size limit for example. And also, of course there's the fact that we have data availability sampling and that's still kind of further on in the Ethereum roadmap toguru what do you think? I mean, should we consider using other so first, should I explain why Ethereum community should care about data availability on Ethereum or no, I'm kind of assuming that we all know that. So let's just move on. Yeah, that was my guess as well.
02:22:03.660 - 02:23:05.342, Speaker A: I feel like there's a place for multiple data availability solutions and one thing to understand that we're trying to solve different problems. So on Ethereum, the DA layer serves as a way to separate the markets for data and execution and also increase the capacity by separating that market. And basically that allows roll ups that are deployed on Ethereum to scale better because you can now post more data and still settle on Ethereum. For celestial, the target market is a bit different. They're targeting things, sovereign roll ups and protocols that don't really care about execution on the base layer, but more just want some ordering and data availability guarantees. And I think that there's even a way where you can combine the approaches. For example, Validiums can use Celestia or avail for data.
02:23:05.342 - 02:24:01.626, Speaker A: Availability but still settle on Ethereum. So I don't think that there's a world where only one solution is required and only one solution is needed. We need multiple solutions, especially if they solve different problems. So it sounds like Ethereum is targeting different users. Perhaps, I don't know. Mustafa Anurakamin, would you agree with that? Know your target is slightly different than what is Ethereum trying to actually do? Well, I mean, you can technically deploy sovereign roll ups using Ethereum SDA, but I don't think that's not something that from a social perspective that I think the Ethereum community coalesces around. And it's not overhead optimal to have a sovereign roll up on Ethereum because you have to also run a node that gets the state off the chain.
02:24:01.626 - 02:24:29.830, Speaker A: But ultimately the whole point of modulism not maximalism, the whole point is like it's not a zero sum game. There's different trade offs. Ethereum roll ups have to have on chain DA. So if you want to deploy Ethereum roll up, you have to use ETH as a DA. There's no way around that. If you want to do a Validium or Celestium or optimistic chain, then yeah, you can use off chain DA. But ultimately it's just all security trade offs and different design trade offs.
02:24:29.830 - 02:25:13.814, Speaker A: Yeah, I would sort of agree in the sense that it's very difficult to demarket the target segments in general, to demarket the users of a roll up with a Validium with the same stack. Of course the security properties are very different, but that's the reality that we are seeing on production, right? You have an arbitram? One with an arbitram nova. You have a stacknet with a Volition built in, right? So we are increasingly seeing this. Polygon also announced their validium plans. They have a roll up, they have a validium. And so of course the target segments might be different. Some apps need the security of Ethereum, for example.
02:25:13.814 - 02:26:23.594, Speaker A: But the roll up developers are asking for solutions that help them target maybe different. You can say there's a different target segment, it's the same target segment, but it totally depends on what GTM these roll up developers. But yeah, I would agree that from a more sovereign roll up, kind of maybe avail and Celestia are better options at the moment. Okay, so let's dive in a little bit into those trade offs so that it's properly understood because I think I'm actually taking it from Mustafa's talk in the morning that it's not easy to talk about these trade offs. So on Ethereum especially, we all know that cost of data is actually quite high for all the roll ups to pay. And obviously that translates to the cost to end users. And that's why we're exploring all these strange constructions like Volitions, validiums, data availability committees, even such weird constructions like optimistic data availability schemes.
02:26:23.594 - 02:27:36.094, Speaker A: So which solution do you think will be ultimately the cheapest to choose as a DA? Just you can raise your hand. Who's the cheapest? That's my first question. You mean avail versus or like you mean validiums versus roll ups, the one that has the most capacity? Well, I'm like thinking about typical dev team that is trying to deploy either server in roll up or a roll up. But for them the current DA on ethereum is just too expensive, right? So they would come, let's say to us, to LTV and they would ask us what do we think, which solution might be the cheapest? And they will provide the cheapest essentially block space for data. So I think there's two ways to look at this. The first way is you look at classes of different types of DA. So for example, you can say like ethereum Celestia and avail are like one type of on chain DA.
02:27:36.094 - 02:28:19.634, Speaker A: You have like a blockchain DA, then you have other kind of less secure types of DA like a data availability committee with like a multi SIG of seven. That's what stockware and any trust Arbitrum have and then you can just have a centralized like a single server. So ultimately the cheapest is just like have a centralized server as a DA. But obviously that's not useful. So let's assume that we're talking about on chain DA. The important thing to note here is in blockchains you cannot guarantee low cost transactions. And the reason for that is because if you can guarantee free or low cost transactions then you have a denial of service problem.
02:28:19.634 - 02:29:15.300, Speaker A: So I think it's not about framing the question in terms of cost because ultimately there'll be fee markets and there'll be supply and demand. It's about framing the problem in how do you get the most throughput. The only thing that blockchains can guarantee is the throughput they have or the block size they have because ultimately the fee or the pricing will be determined by supply and demand and how much demand there is for that specific block. Mean, again, I sort of agree with Mustafa in the sense that it's not good, not enough to just look at the cost sense. Of course there will be throughput and our architecture allows us to create high throughput as well. But also we have to look at other factors like decentralization and stuff like that, right? There are a lot of factors. It will be cheaper of course, and we can increase throughput as such.
02:29:15.300 - 02:30:34.270, Speaker A: But as you said, we cannot compete with a single server doing DA and there will be solutions like that. We are already seeing solutions like that, right? Like so the the thing is it's a trade off between cost, decentralization, you know, like throughput and a lot of other factors. And again just coming back to the point, right, certainly cost is a factor but we also have to look at things like data resampling lightlines which provide new kind of powers to these roll ups, right? Like I mean propagating the validity proofs of fraud proofs to the users directly? Can users verify that directly? For example? How can we come up with those kind of constructions? We should consider that as well. And the light node is also an important piece because all the roll ups that are running on Celestia right now, like the sequences running on Celestia, they're not running full nodes, they're actually running light nodes. Like the sequencers themselves are running light nodes and they're not paying for RPC endpoints or running a full node. And that's just significantly cheaper than having to need access to a full node. But I don't think that for a sequencer, the cost of running a full node really matters.
02:30:34.270 - 02:31:04.600, Speaker A: I run a couple of full nodes at home. And even if you assume that I run relatively high end hardware for that, the cost of one node is less than $1,000. And considering that sequencers extract so much profit, potentially can extract much profits from mev, et cetera, et cetera. I'm not saying that's like the main thing, it's just like that's one nuance, for example. I'm not saying it's like the main cost difference. Ultimately it's about the cost of the DDA. Got you.
02:31:04.600 - 02:32:20.590, Speaker A: So what about the security then? I mean, the common argument that I keep hearing is that it's always a security trade off to cross the trust boundaries when you're actually using the external DA. So clearly the security of validium is very different than the security of a roll up. Right? And the cost of Validium is also very different. That's why we've got Arbitram, arbitram Nova, that's why we've got StarkNet or Starkx roll ups and Starkx Validiums. And I was always wondering, like end users, at the end of the day, they seem to have very little to say. If you are, let's say, the YDX user, what benefit do you gain from dYdX actually posting all the transactions as opposed to them using some data availability committee? Right. You just want to trade on dYdX and you want to have a reasonable guarantee that if things go wrong, you can always recreate the state from the data and you can always exit.
02:32:20.590 - 02:33:08.142, Speaker A: It seems that this is all about the trade off. Right. So with increased security comes, I guess, bigger costs. So how do you, as an app developer or as a roll up developer, suppose that I'm dYdX, how do I make this trade off? Yeah, ultimately I think the choice will be decided by the roll up developer and I think not by the app developer directly. I mean, unless we're talking about app chains, for example. But before that, we are seeing a lot of roll up orchestration players coming to market, for example. And I think these are the set of developers who are making the choice in terms of what DA to use.
02:33:08.142 - 02:34:28.866, Speaker A: And at least from my perspective, what we are seeing is it's at the moment a cost versus security trade off kind of a thing. So of course they want cheaper costs for the users, that's kind of the primary metric. But in general, from a roll up operational point of view right at the moment, of course no decentralized DLA is in production, which is going to change pretty soon. And that's why you see these data validity committees. And right now that may not be a problem, but I definitely see decentralization of the DS similar to the decentralization of the sequencer questions that will come from a regulation point of view or whatever. Right in general people don't for now, single sequencer datablood committees are fine, but I think just to safeguard regulatory interests and stuff like that, I think for sure things will move to a more decentralized context. Are you saying that this is the primary reason why the IDEAX chose to go its own way and be more decentralized? I can't talk to the intentions of course, but I think in general I didn't fully comprehend the whole reason why they moved.
02:34:28.866 - 02:35:13.890, Speaker A: Of course they have more flexibility in running their own app chain as such and so they are able to customize a lot is what I feel. I think the Starc solution was a bit limited is what I may be wrong in terms of what they wanted to do. And of course Starkx has now, I mean there's new upgrade, the StarkNet is much more powerful, Cairo 1.0 and such. So I can't ascribe their intentions. Yeah. So I think there's kind of like an interesting question here which is about how much do users actually care about? Sure, from a UX perspective there's no UX difference probably like interacting with the AC versus interacting with an onchain DA.
02:35:13.890 - 02:36:32.800, Speaker A: But consider this users today can use polygon, they can bridge tokens from ETH to polygon and use it pretty much as the same as l Two or a roll up, even though it's not. But if that's the case, then why did polygon spend a billion dollars on ZK roll ups? It doesn't actually make a direct difference from a user perspective. In fact probably like slightly less TPS at the start until we optimize the ZK previewing systems. I think ultimately this is what differentiates web Two from Web Three from a social perspective. Ultimately people do coalesce and do care about the decentralization properties of the systems they're interacting with. Otherwise why has no one just created a centralized proof of authority l one, just ten committee of ten nodes, billion TPS, no one would take that seriously. So that's why I think, yeah, people should use the ACS for certain use cases, but ultimately for an application to be kind of like Credibly decentralized and Credibly have social community around it, there just needs to be at least a decentralized DA as an option.
02:36:32.800 - 02:37:51.850, Speaker A: I have a small question for you with the whole validium talk, are you trying to bring back the conversation? Whether validiums are l two S or not because I had a feeling that that's where you're doing there. So I feel like from it's all about the trade offs. Because when you're building a certain application or a certain protocol, you need to assume what is the worst case scenario that your protocol can handle? And is that worst case scenario worth taking certain trade offs? So, for example, if you're building a game that doesn't really have any monetary value, then you probably shouldn't use on chain DA because it doesn't matter. Like worst case scenario, your game assets disappear, whatever. But if you're building an app that has billions worth of dollars deposited on it, I think then you should take more care of how you design things, because billions of dollars frozen in a contract that you can never withdraw from, that's a bigger problem than your game assets being frozen. Okay, so let's maybe switch gears a little bit. So here's another common question that we get from users.
02:37:51.850 - 02:39:26.760, Speaker A: Well, first of all, sometimes it's hard for them to differentiate between the data availability and data storage. But let's assume for the sake of this discussion that we all know, and for the audience that don't know, maybe some of you can give a very quick introduction. But the question is, on Ethereum, it seems like we have a very reasonable and strong, I guess, guarantee of the data storage because there's this huge, vibrant ecosystem of different explorers and indexes and whatnot, and people just simply assume that it works. Right? Now, if I use a vale or Celestia, how can it be guaranteed that in twelve months I will have access to all the data and I will be able to actually recreate the state? Yeah, so, I mean, that's like a general question about the difference between data availability and data storage. Even with Protodank Sharding and Dang Sharding, protodank Sharding also does not plan to guarantee the data forever. The current plan is to prune data prune blobs after 30 days. And the reason for that is because data availability layer, including Ethereum, it's meant to be like a real time bulletin board to allow roll ups an opportunity to get their data to make sure it's published so that they themselves can store it.
02:39:26.760 - 02:40:28.602, Speaker A: What is the difference between data? So I would actually propose, like I actually propose renaming data availability to data publication because I feel like that's an easier to understand thing. Easier to understand? Data availability is about proof of publication, like proving that data was published so that people can access it. Specifically in Celestia, at the moment, we don't prune the data blobs they're kept around forever right now. But at some point after Mainnet, the community will need to coalesce around after what certain point in time data blobs will be pruned. But that being said, even on Ethereum, even on networks where blobs are pruned, I actually still expect that the data will be permanently stored somewhere and permanently accessible to the public simply due to the streisand effect for data. Storage only requires an assumption that a single person ideally, you have more. But the minimum assumption is that you have a single person storing the data.
02:40:28.602 - 02:41:32.990, Speaker A: And that's extremely easy assumption to achieve on the internet. I think Bartek's argument was more that because Ethereum has such a vast ecosystem, where you have RPCs block, explorers, et cetera, storing the data, it's highly unlikely, even if all the nodes prune the data, that the data will be. So, for example, I think it was ripple at some point, lost like a day worth of data because their servers somehow had a bug or something. So I think Bartek's question was more sorry if I'm rephrasing it incorrectly. Okay, I understand. But I argue that would also still happen on Celestia and other daily is, including running the cost of storing data is cheap enough that multiple people will do it, even if the ecosystem is smaller than Ethereum. And also the other very important thing to mention is that the whole point of having data availability sampling light nodes is that you're distributing data across thousands of light nodes.
02:41:32.990 - 02:42:30.434, Speaker A: And that will also help somewhat with the data storage problem, assuming that those light nodes are happy to store it for a longer period of time. Yeah, absolutely. If these light lines can make their way into wallets, for example, I think we'll have the data mirrored from the DA layer to the lightline network, and that can be propagated kept for a long time. Second is just a point that if you look at the stack that we have built on, we've built on substrate, celeste is built on tendermint, for example. And so a lot of the tooling in these ecosystems is also compatible with the stack in general. And in fact, we are also starting to work with a couple of teams who want to put availed data onto IPFS, onto filecoin, for example. And so, as Mustafa said, ideally you just need only one copy of the data.
02:42:30.434 - 02:43:27.634, Speaker A: But we are expecting that. We've been working with a few infrastructure providers in the substrate ecosystem, for example, and the ecosystem is more mature than, let's say, two, three, four years ago, like in all these ecosystems, in the substrate ecosystem, in the tenderman ecosystem, for example. And so we don't really have to build all of this tooling from scratch. And there are a lot of providers out there. But as part of the architecture, we already have a lightline network that mirrors the data. We don't anticipate the kind of problems that Togrul is mentioning in that sense. Okay, so you represent, I think to my knowledge, the three most known on chain data availability solutions.
02:43:27.634 - 02:44:49.246, Speaker A: But very recently, it seems like there's new kid on the block somehow, and everyone seems to be talking about it. And I mean, Eigen, DA, any of you have a spicy take on Eigenda and the trade offs and its role in this DA kind of landscape. Just before I go on to express my opinion, I would like to add that I don't represent EF just in case Dankered kills me if I say something wrong, I just accidentally stumbled on stage. I think from the perspective of Ethereum, Eigenda makes sense because it utilizes the existing validators or a subset of existing Validators to offer extra capacity that is much cheaper than on chain DA. Obviously the security guarantees are much different so it's not comparable with just yeah, what are these security guarantees? Isn't it such just a fancy name for availability committee? Yeah, I'll make things spicy. First of all, there's no docs on Ignda, so it's very hard to even compare it in the first place. People keep saying, oh ManTool, claimed to they launched, right? They claimed to launch their main net on Ignda, but other people are saying, no, it's not.
02:44:49.246 - 02:45:36.190, Speaker A: Actually, I have no way to verify that because there's no docs anywhere and people say use agony, but there's no docs, so when there's docs it will be easy to compare. But from what I know so far, there's a various set of trade offs. First of all, I'm very skeptical of the idea that you'll actually be okay. So first of all, I'm skeptical of the idea that there's enough demand for restake services. I think there's enough supply, there's a lot of supply of Validators wanting to restake. Very skeptical. There's demand that at least it's very hard to bootstrap your roll up if you're saying to convince Validators to come and restake if the initial rewards only come from fees which might be very small to start at the beginning.
02:45:36.190 - 02:47:01.660, Speaker A: And the whole point is that the fees are supposed to be cheap in the first place. Secondly, as far as I know, agnda requires a dual token model anyway because you can't slash data on chain kind of defeats the whole point of restaking in my opinion, because you have to restake not only ETH, but you have to restake the Eigen token as well because you can't slash data on chain. I was about to add on this, I'm still not sure if that's a very sound model, the whole data availability based on crypto economic guarantees because essentially the assumption is that if the data is withheld, then the price of the staked Eigen token is going to drop and it acts as a disincentive. So essentially you can see it as implicit staking like the one that Chainlink used before. And I'm not really sure that it's a strong enough guarantee for a lot of applications. Obviously, as I said before, some applications it's fine, but for a lot I just not sure if that's because it's also potentially vulnerable to griefing attacks the same way Methus's DA solution is. So yeah, I'm not sure I don't want to attack again the ATM watch when they're not here to defend themselves.
02:47:01.660 - 02:48:02.510, Speaker A: Yeah, that's true. I think last year's Panel Streetam was there and I really like him. But we have yet to see a working system like I haven't seen that. So I would deserve my judgment based on that first taking not about Eigenda, but restaking itself is a pretty neat idea, but I'm still skeptical of how it will actually work in production settings in terms of how many validator sign up, what is the actual security like, griefing attacks, such and such. So I would want to reserve my judgment at least. I want to see something working before worrying too much about it. Actually, from my point of view, what I would love to see more is more, I think, thorough security analysis of such potential attacks like griefing.
02:48:02.510 - 02:49:15.970, Speaker A: We had a discussion about that about two years ago, or maybe it was a year ago when Mattis launched so called Smart l Two and optimistic DA, which for those that you know, they seem to be publishing data directly to the storage without actually mentioning DA layer. And if anyone notices that the data was not published to the storage, they could, in theory, challenge the sequencer and force somehow sequencer to post the data on chain. And the cost of such solution, as you can imagine, is extremely low. Hence fees on Matis were very, very low. And I was surprised that very few people actually bothered discussing that from Principles. Right? I mean, what are the security trade offs and is that scheme actually viable to consider for anyone? The community was largely silent. Are you talking about I looked at Metis profile on LTB up before this talk, and the idea is to use a data availability challenge.
02:49:15.970 - 02:50:23.110, Speaker A: Right, but the problem is with data availability challenges is that as far as haven't actually there's no challenge mechanism finalized or proposed yet. But in general, the general problem with data availability challenges is that they don't solve the data availability problem. That was actually the first thing that people like Vitalik looked at to solve the data availability problem. But it doesn't solve it because of something called the fisherman's dilemma, which is where data and availability is what's called a uniquely unattributable fault, which means that the challenger who's challenging the data availability, it might be their fault that they can't access the data because maybe their network is not good. And that basically creates like a dilemma where it's like explained on the Ethereum wiki. But it's like you either have a situation where you have a Dos attack or there's not enough incentive to make a challenge. There's no incentive to make a challenge in the first place because the DA publisher might only release the data after you make the challenge.
02:50:23.110 - 02:51:18.934, Speaker A: And so you're basically griefing you so there's no way to make it economically sound. Basically, yeah, it's basically as Mustafa described, because default is not attributable it's impossible to attribute default here. You would essentially let's say if I create a challenge and you post the data, you can both withhold it and you could have withheld it before and that's why I initiated the challenge, or I was just not in sync with the network and that's why I didn't receive the network. And that creates a long term griefing vector where essentially you start the challenge, you stake a certain amount, I post the data. When you initiate the challenge, you get slashed and then you continue to get slashed until nobody is willing to challenge you anymore. Because basically every time you slash, the sequencer just reveals the data. So there's no reason why you would long term continue challenging it.
02:51:18.934 - 02:52:55.610, Speaker A: And therefore it's fine if you consider the optimistic case, but in a worst case scenario, this whole security basically breaks apart. Okay, I have another question from a completely different angle. So you guys mentioned, and this is the thesis that we've been hearing and will be hearing especially in this conference, it will be very easy for anyone very soon to launch their own construction on roll up with very custom security parameters. And as it is even today, for users to understand the security assumptions is extremely hard for us as an.org trying to understand that for us, it's a moving target that we need to chase. How do you see in the future, with potentially thousands of roll ups being launched, how do you envisage that users will be actually able to tell what are actually the security assumptions and how do you see the role of people like us in this whole space? I'm very curious because I'm frankly terrified about the future you're describing. From my perspective, the way I think about it is there is a wide variation in terms of roll up orchestration today.
02:52:55.610 - 02:54:02.286, Speaker A: But as you're increasingly seeing, all the roll up stacks are also tending towards standardization, right? So for example, Zkevms in terms of the implementations are going in a similar direction as such, for example. So that process will also apply to the entire roll up stack pretty quickly. I mean, better than anticipated. And as I said, a lot of the major players are pushing their then standardized stack to these roll up as service providers or such, for example. And so what we feel is rather than having a wide variation of roll ups, there will be a set of roll up stacks that will be pretty much standardized. And the deployment itself also will be not be like 1000 different roll up configurations, it will be like maybe 510 different configurations, but like a lot of instances in that sense. And so it'll be easy to kind of look at it.
02:54:02.286 - 02:54:54.234, Speaker A: Of course, it's not going to be as easy as I make it out to be, but yeah, I think people should just use Lt beat. That's the solution. I'm relying on you guys. You guys are doing a good job so far to be a credibly neutral and trusted way for users and developers to get quick information about the security trade offs between different the A layers and the roll ups. And LTOs. I think we opened up a can of worms by John, specifically, I blame John Charbone for everything by writing that article about social consensus or how roll ups aren't real. Because now for the last three weeks or a month, I've been hearing people making ridiculous claims about Anatoli.
02:54:54.234 - 02:55:47.618, Speaker A: I love him, but his argument that if I post date on ethereum all of a sudden become an L two, it just doesn't really make sense. And so I feel like first, before we start discussing about how many roll ups are going to be built on a different protocol, we need to work together to define what a roll up actually is because I don't really buy that whole social consensus thing because you can literally attribute it to anything. And therefore all the concrete properties that we have about consensus, protocol signatures, et cetera, can be just hand waved away through social consensus. And so I think we need to start working on that first before we discuss what's going to be deployed where. And we've just ran out of time, so that might actually open the discussion for entirely different panel. Thank you so much, guys. I mean, it was lovely to have you all here.
02:55:47.618 - 02:56:31.290, Speaker A: Unless you've got like one last closing comment. Mustafa, I mean, you've got to ask a spiciest question which is like, what's the know? But I guess we are time for that, unfortunately. All right, thanks so much again. Thank you very much. Thank you. Thank you. Which one? Which one should I use? Yeah, where is it? Do you have it? Okay.
02:56:31.290 - 02:57:25.096, Speaker A: All right. I am really excited for this next talk. I work very closely with this next guest, and I know firsthand the passion, belief, and conviction that he has behind the topic he's about to speak about. Please welcome Nick White, COO at Celestia Labs to talk about light nodes are more than just a meme. All right. It's so nice to be around other people who are as passionate and also nerdy about modular blockchains as I am and as our team is. And it's also really humbling to have seen the growth of the modular ecosystem in the last year since the first modular summit.
02:57:25.096 - 02:58:12.486, Speaker A: And it's all thanks to you guys out there in the audience, the builders and the believers who are turning the vision of modular blockchains into reality day by day by building all of this stuff. So today my talk is going to be about light nodes and see if my clicker is working. Doesn't seem to be working. Let me try this one. I think we're having a little technical issue. Try it again. Okay.
02:58:12.486 - 02:58:40.186, Speaker A: There we go. So, yeah, my talk is going to be about light nodes. So if you're on Twitter in the last few. Months, you may have seen a bunch of these memes circulating. People were joking about running celestial light nodes in the club or on their COVID test. And there were just Twitter for like a few days, was just filled with all these memes about celestial light nodes. And honestly, it was hilarious.
02:58:40.186 - 02:59:24.730, Speaker A: It was awesome. I was stoked. I'm glad that the community was embracing light nodes, but at the same time, I had a little fear that people were going to miss the deep significance behind light nodes, what they represent. Light nodes are not just a meme, they're actually a movement. And they're crucial to the success of modular blockchains, and therefore to crypto, and also therefore to the future of mankind. And to explain why I'm saying that, we have to cover some first principles and kind of get really high level for a second. So I love this quote by Yuval Noah Harari in the book Sapiens.
02:59:24.730 - 03:00:14.254, Speaker A: He says that humans have the unprecedented ability to cooperate, flexibly in large numbers. And this is something that you might not think about on a daily basis, but if you were alone as a single human, there's not much that you could do. You probably wouldn't even survive that long. But when we get together in groups and we cooperate, we're able to achieve some pretty incredible things. And this is a feature of human beings that really sets us apart as a species and makes us as successful as we are today. One way to think about this, a good analogy, is the difference between a single celled bacterium and a multicellular organism like a human being itself. So when cells in biology get together and cooperate, they're able to build much more powerful, complex things.
03:00:14.254 - 03:01:24.142, Speaker A: And the same is true for human beings. So in nature, human beings can't actually cooperate in group sizes above 150, which is the so called Dunbar Number. And this brings us to a really important concept, which we're going to be talking about for the rest of this presentation, which is this notion of social scalability. So within any given system of cooperation, there's a limit to the maximum group size that it can support. And one of the really cool sort of arcs of history has been that over time, human beings have been able to transcend this Dunbar number and continuously find systems where they can cooperate in larger and larger groups. And with each time that we're able to innovate and sort of take the step towards larger scale cooperation and coordination, we have an increase in prosperity and all these big benefits to everyone. And so in ancient times, or prehistory, we would collaborate in the sizes of an order of magnitude of a few hundred people, like tribes.
03:01:24.142 - 03:02:14.846, Speaker A: And then as things got more advanced and we had more systems of cooperation, we could cooperate at the scale of city states, tens of thousands or maybe 100,000 people. And in modern times, we are cooperating at the scale of nation states. So millions, tens of millions, sometimes hundreds, or even in some cases a billion people within a nation. And this has kind of been this arc of progress for humanity. And so now I want to talk a bit about how does our current technology or systems of cooperation work? So the way that they currently work is everyone starts out with a set of rules. It's like laws, it's like the Constitution or the Bill of Rights or the bylaws of a corporation. You have a set of rules that everyone that's in the system is agreeing to follow, right? And this sort of sets the ground rules.
03:02:14.846 - 03:03:07.046, Speaker A: But unfortunately it doesn't end there because you can't just trust everyone else to actually follow the rules as they're written. So you need some way of enforcing the rules. So in lots of our current systems, you have to empower a person or a group of people to be the rulers, or they're given a special power that they get to enforce the rules on everyone else. And this works, but it comes with a really deep flaw. And that flaw is summed up in this question who watches the watchmen? In other words, who is actually enforcing the rules on the enforcers? And the answer is really no one is you're just trusting them. And the problem with that is that they often have an incentive to cheat. And that's what corruption is.
03:03:07.046 - 03:03:57.602, Speaker A: That's what fraud is, is when the watchmen are not actually being watched and they go off and do their own thing. And so this is a really deep problem in our current systems of cooperation. And we got a really timely reminder of that last year with what happened with FTX, unfortunately. And although a lot of people interpreted FTX as an example of why crypto is broken, it's actually the exact opposite. It's an example of why we need to build better systems of cooperation and build blockchains and decentralize things. And that is really core to this whole space, is this idea that trust doesn't scale, or at least trust in other human beings, to be honest, is not a scalable form of cooperation. And so that's why in blockchain space we use these words constantly.
03:03:57.602 - 03:04:49.798, Speaker A: Trust minimized, trustless decentralized. All these things are pointing towards the fact that the problem that blockchains are trying to solve is we're trying to remove trust from our systems of cooperation because trust represents a vulnerability, like a weak point that can be exploited. And if we want to achieve a greater degree of social scalability, we need to find something better. And the answer is trusting in cryptography rather than other people. And so how do blockchains work? How are they different to our legacy forms of cooperation? Well, they start with a set of rules, just like the previous example. And oftentimes this is called like a social contract. But instead of having a particular group or individual who gets empowered to enforce the rules on everyone else.
03:04:49.798 - 03:05:26.050, Speaker A: It's actually the people, the users of the blockchain that enforce the rules directly. And so you'll notice that there isn't this sort of like power hierarchy. It's kind of like a direct relationship between the people and the rules. And this is really powerful. And unfortunately a lot of people map the old mental model of cooperation onto blockchains and they assume that oh, it's the validators who are given this privilege. They get to enforce the rules. They're like the new rulers in this blockchain based world and that is not the case.
03:05:26.050 - 03:06:14.446, Speaker A: And if it were the case then there would be nothing special about blockchains. It'd just be a repeat of our old systems of cooperation and we could all forget about that. We're building them and go home and give up. So this is really, really not how it works and I hope this is probably one of the most widespread misconceptions in the space. So how do users actually enforce the rules directly? It sounds kind of weird or confusing how it could possibly work. Well, the way that users enforce the rules of the chain is that they run a node. And a node is a computer program that has an understanding of what the rules of the blockchain are and then it constantly audits and verifies the chain to make sure none of the rules are being broken.
03:06:14.446 - 03:07:32.730, Speaker A: And if the rules do get broken, they stop them before they happen. So running a node is how users enforce the rules of the chain and this gives blockchains their fundamental value proposition, this superpower, which is that blockchains enable rules without rulers. So using blockchains we're able to dissolve this power hierarchy. We're able to remove the need for this trusted powerful entity in the system and that enables us, now that we're rather than trusting in other human beings, to be honest, if we can just trust in cryptography, all of a sudden we can scale our cooperation. We can achieve a social scalability that extends to the entire planet because everyone can agree on entrusting in cryptography and in verifying cryptographic proofs. And now this sounds amazing, but unfortunately there is a catch. And that catch is that even though blockchains achieve very high social scalability, they don't achieve very high technical scalability.
03:07:32.730 - 03:08:27.254, Speaker A: Basically technical scalability is the fact that they can't really process that many transactions per unit time so they can't actually support that many users. This is why, for example, Ethereum is always congested. And to understand why blockchains are not very technically scalable, you have to understand what's actually happening under the hood inside of a node. So when you're running a full node and you're verifying the chain you have to download and verify every single transaction that every other user sends. And unfortunately, users are just people, right? They're normal people like you and me. They have a certain amount of bandwidth that they can download transaction data and they have a certain amount of computer power that they can use to verify these transactions. And so that sets a finite limit, an upper bound to the throughput of the chain.
03:08:27.254 - 03:09:05.670, Speaker A: If you exceed that, then all of a sudden the users can't verify the chain anymore and you lose a property of social scalability. So a lot of chains when they are quote, scaling, what they're doing is they're increasing the node requirements. So if you create node requirements, obviously you can run the chain faster and process more transactions. But then end users like you and me, average people, get priced out of the ability to verify the chain. And instead of running a full node, they can run instead a light client. And a light client is basically what it does is it assumes that the validators are honest. It assumes that the validators are honestly enforcing the rules.
03:09:05.670 - 03:09:59.430, Speaker A: And even though it's very technically scalable to run a light client, that's not very socially scalable because you're reintroducing this power dynamic and this trust vulnerability by empowering the validators once again. So that's why blockchain scaling is so difficult is because you have this dilemma between social scalability and technical scalability where you want to increase the throughput of the chain so you can support lots and lots of users. But that's what we all want. We all want mass adoption, we all want everyone to be able to use the same chain. But the problem is that in this current paradigm, we can't do that without also sacrificing social scalability. And social scalability is the whole reason we're building blockchains in the first place. This is why when bitcoin forked, the bitcoin fork that survived was the one that maintained social scalability.
03:09:59.430 - 03:11:17.086, Speaker A: That's why basically we had this promise of, hey, we can rebuild this like a trust network in cryptography that scales to the entire planet. But then actually it turns out we can't really get there because we need to achieve both technical and social scalability at the same time to actually make that practical. And that's the sort of the state of things. That's where things were up until 2018 and 19 when a galaxy brained dude named Mustafa Albasam, who was just up here, published two papers that described an entirely new way of building blockchains, which are called modular blockchains. And at their core, modular blockchains defined a new way for people to verify blockchains in a way that can be both technically and socially scalable. And the way that it works is that unlike in a monolithic chain, we have to download every transaction and verify every single one. In a modular blockchain, you just have to download a tiny sample of the transaction data and then you can verify a proof that the transactions are valid, that they followed the rules of the chain.
03:11:17.086 - 03:12:26.658, Speaker A: So in doing this, you're able to verify lots and lots and lots of transactions that the other users are sending, but without having to do so much work. And really importantly, this technique scales sublinearly to an increase in transactions. So it fundamentally solves this sort of dilemma and results in. The whole purpose of this talk is this new type of node called a light node, which has the security and decentralization properties of a full node, but it has the technical scalability of a light client. Light nodes are the core innovation of modular blockchains and they're how we can actually have blockchains that scale both technically and socially at the same time. And you can see this represented in the specs for a celestial light node as compared to nodes in other ecosystems. In many cases as orders of magnitude less resources are required to verify the chain.
03:12:26.658 - 03:13:12.466, Speaker A: And that brings me to a really important value of the entire modular blockchain movement, not just Celestia, which is that we believe anyone, anywhere should be able to verify the blockchain. They should be able to run a light node and enforce the rules of the chain themselves. And that's why we have targeted the specs of the Celestia light node to be able to be run on a smartphone. Because a smartphone is the most widely adopted device in the world. Estimated 5.3 billion people worldwide own a smartphone. So if every one of those people can run a light node, then we can achieve a maximum degree of social scalability.
03:13:12.466 - 03:13:55.730, Speaker A: All those 5.3 billion people could be connected together in a cryptographic network of trust. And so we could, with modular blockchains, actually achieve this holy grail of scaling cooperation to the entire planet. Now, again, that's a really big vision and I firmly believe in it, but I'm not going to lie to you, there's still a lot of work that needs to be done. So at the protocol level, this is sort of a call to action to all the protocol engineers and builders out there. There's a ton of work that we still need to do to make light nodes actually technically possible. I mean, they are technically possible.
03:13:55.730 - 03:14:56.194, Speaker A: We have celestial light nodes working on testnets and soon on our main net when we launch. But there's so many other layers to the stack, we need better proof systems. All these things listed here are going to be really crucial to be built out to make this vision a reality. So there's a lot of engineering still to be done and there's also a component of friction, right? We need to make it really easy and simple for users to run light nodes. And current status quo is that you have to open a terminal window and do command line and that is way too much friction for the average person, as you know. And so if that remains that way, we're not really going to get very far in terms of adoption. But fortunately there's amazing people like Josh and our team who have been building desktop apps and hopefully in the future mobile apps so that it can be just a few clicks between starting up a light node.
03:14:56.194 - 03:15:44.502, Speaker A: And I think that's really where we need to start moving towards. And the last thing is I think we need to even embed light nodes into user facing infrastructure like wallets. And that way in the same way that we can have the privacy community has this notion of private by default in the modular blockchain community we can have a notion and a value of verify by default. And lastly, we actually need to make light nodes into a meme. So light nodes need to be embedded deeply within our culture. As for those of us who believe in modular blockchains, running a light node should become a habit. It should become a ritual, even a rite of passage for being part of what we're building and what we're doing.
03:15:44.502 - 03:16:31.240, Speaker A: And if you haven't yet run a light node, we have a booth outside and we're also teaching a workshop tomorrow and so I invite you to join and give it a try and be baptized in the modular movement. So the amazing thing is that the meme is working. As you guys have seen, people are actually putting light node to use in practice, running it in all different kinds of places in the world. On planes, on boats, at the pyramids. People have been running them in all kinds of devices, kindles gaming consoles and even in this instance on their car. It's cool. There's a momentum, there's this cultural momentum that we have and we need to keep it going.
03:16:31.240 - 03:17:50.842, Speaker A: I really hope that you guys do your part and participate. Now you might be wondering though, okay, this is really cool, but why should I care? Well the answer is that why should I care and why should I go through the effort of running a light node? It's a busy world, I only have so much time in my day, why should I bother? Well the answer is you should care and you should make an effort to run a light node because we're living in a world where our rights and our freedoms are being increasingly infringed upon by people in power. And even if it's done with the best intentions, it's really dangerous and it's something that we shouldn't tolerate. And so we need to run light nodes as a way to sort of stand up for our own rights and build a system where we can't be taken advantage of. And we've seen this all over the place. We've seen in Canada the financial system was used as a way to suppress protests by freezing people's bank accounts. We've seen with Twitter files and a lot of recent news that Facebook and lots of social media companies have been censoring information because of government requests.
03:17:50.842 - 03:18:43.170, Speaker A: There's also widespread surveillance on the internet. And I don't think it's a coincidence that trust in our institutions is at all time lows. I think people are waking up to the realization that the people in power don't always have our best interests at heart. And the amazing thing is that our ancestors, when they had to defend their rights and their freedom, they had to pick up weapons and go to war, right? They had to put their lives on the line and fight for what they believed in. And the beautiful thing is that with light nodes, we have a peaceful alternative. We can build a new sort of like fabric of society and build our own rules and build our rights in a way that can't be compromised. And we don't even have to fight.
03:18:43.170 - 03:19:24.910, Speaker A: It's just software. It's just code. And last but not least, it's not just about defending your rights. Running light nodes is an opportunity for us to take this next leap to large scale cooperation and coordination, rebuild a network of cryptographic trust on which we can build a more peaceful, prosperous and democratic society. And so I actually happen to have a this is kind of complicated. I happen to have a node on my phone right here running. Hopefully you guys can see that.
03:19:24.910 - 03:20:11.160, Speaker A: And so what I want all of you to do with me is if you could reach into your pocket or your bag and pull out your phone and hold in the air with me. Let's go, guys. Put them up, keep them raised. I want you to realize that what you're holding in your hand is not just a phone or a camera or a computer. It's the key to your rights, it's the key to your freedom, and it's the key to a better future for humanity. So thank you very much. Wow, that was incredible.
03:20:11.160 - 03:20:44.420, Speaker A: Give it up for Nick to close our last session out right before lunch, das Broadband is the title. And please welcome Alex Evans from Bankap crypto to the stage. Hello everyone. I'm Alex Evans. I work at Bankapital Crypto. I'm going to first try to figure out how to work this click. I think that's fine.
03:20:44.420 - 03:21:39.904, Speaker A: Hopefully it's not running a light node or something and it freezes halfway through. We talk a lot internally about applications and infrastructure and sort of the ways in which they interact. And we're particularly interested in ways in which those interactions might change, particularly with modularity being a key driver of those. So we want to share some of those ideas, hairbrained or otherwise, with you all today. And I'd say for most of my time in the space, the nature of the interactions between applications and infrastructure have been mostly harmonious, with some key exceptions. And some of them are sort of listed in this slide and coincide with the end of the last two respective bull markets and remarkably, sort of parallel stories where right at the end of a cycle, an application draws in a lot of excitement and fervor and interest crashes all the infrastructure. A bunch of people get frustrated.
03:21:39.904 - 03:22:50.056, Speaker A: We get some cool, scalability ideas out of it. The application developers go off or want to launch a new chain. And I'd say that this sort of four year, four or five year transition here occurred while Ethereum got meaningfully better as infrastructure. In the meantime, the block limits, unlike some other blockchains, increased at roughly the rate of Moore's Law, actually a little bit higher with burst limits given, EIP. 50 59, which was a major improvement, roughly forex reduction in call data costs on a relative basis as well. And that's even before things like the merge, the surge, the splurge stuff that sort of started happening later in 2022, right? But I'd say qualitatively the nature of interactions between applications and infrastructure during this period didn't change, right? So applications had a wholesale choice to make when choosing what infrastructure to deploy on. And what I mean by that is you embrace all the constraints as well as all the positive aspects of infrastructure by deploying on there or you choose not to, right? And you can choose to deploy on Ethereum or Solana or both, or some combination or launch your own chain.
03:22:50.056 - 03:23:54.192, Speaker A: But that's sort of the type of interaction that you have as an application developer with the underlying infrastructure that you deploy on. And we think that's about to change. And we use the term better here. Not necessarily better, just there's qualitatively different types of interactions that we think will be available to both infrastructure and applications that are enabled by succinct proofs and in particular the horizontal scalability that succinct proofs in different forms enable. So that's I include underneath that things like data availability sampling, optimistic systems, Snark based systems and so forth. But just to make this idea very, very concrete, I'm going to go through two general examples that also are a little bit of audience pandering and that I know a lot of people in the audience work on one or the other of these two things or maybe some combination. So I'll use Snarks as an example in which this horizontal scalability of infrastructure enables new types of applications and then I'll use data availability sampling as an example of changing the interaction between apps and infrastructure in a qualitative way.
03:23:54.192 - 03:24:44.320, Speaker A: You could swap these two, but I just want to make it concrete. So I'll just go through each one in turn and just demonstrate the principle. So starting with modularity ZK and by the way, I made these slides on the plane over before I spent the last two days with ZK content. There's more ZK content today, some tomorrow. So I'll go through these relatively quickly. But I think most people at this point, even just in the last two days have seen sort of this diagram, right? And realistically, when I was first looking at the space, you would see these papers. There'd be these entirely monolithic constructions for the most part, at least from where I was sitting where it's like, hey, here's my Snark or Stark, and it's fully featured and it's better than this other thing in the literature asymptotically or concretely or uses fewer assumptions and please accept my paper into your conference.
03:24:44.320 - 03:26:36.708, Speaker A: I'd say over time, and in particular more recently, you'll see things that come out that focus on just one of these components. What are these components? You start with like a program written in some high level language, compile it through a front end to a set of constraints, use an interactive oracle proof to reduce that to checking some evaluations of different functions and commitments against some commitments that approvers made, and using sort of a functional commitment scheme or polynomial commitment scheme. And maybe if you had shamir, you produced this short proof that you can circulate around a P to P network or post on chain or whatever, right? So the point is researchers, developers can just focus on one of these components and achieve material advancements of the state of the art even by advancing one of these subcomponents of these, right? And then these get reassembled back into more general frameworks. And I'd argue something like that happened roughly in the 2019 to 2021 era, where things like plank came out and high degree gates became a thing. And lookups, and interesting advancements in polynomial commitment schemes, they sort of got reassembled again, accumulation in the sense of halo and ultimately swapping out plank and creating the framework Halo Two, which a lot of people use. These are general purpose frameworks that combine the modular component improvements into general frameworks that people can use. And it sort of marks a transition between at least when I was looking at the space, 2018, most people using some sort of gross 16 variant but circuit sort of Marvel computer science ten years leading up to it, or maybe even more, right? But circuit specific trust, it's set up to what people call more universal architectures in the case of Halo or Plonky two risero, things like that.
03:26:36.708 - 03:27:17.030, Speaker A: And we think this sort of architectural transition has enabled two fundamentally different things. The first one is a transition from roughly more specialized architectures to more universal architectures. I don't just mean this in the sense of like, you don't need to do circuit specific trusted setup. I mean this in the sense of very concretely people are building ZKE EVMs out of it, implementing an instruction set of the EVM inside of that's provable, or I think I lost the slides. Okay, thank you. While they also boot up the light node in the meantime. Thank you.
03:27:17.030 - 03:28:02.716, Speaker A: So anyway, the Oops let's go back and I think we're missing okay, never mind. All right, so, yeah, the key idea is this transition to people building risk five provable, risk five chips and so forth. It's a very concrete ZK WASM or whatever we sort of are going from sort of more microprocessor bases now. These things did exist in the Groath period. But fundamentally the way if you look at Zkvms, they utilize these modular components like not just recursion, but lookups very extensively under the hood and so forth. So that's kind of something that's been driving new types of VMs and so forth. We think the more interesting thing that's been enabled in succinct proofs has been recursion.
03:28:02.716 - 03:29:10.816, Speaker A: It has enormous economic implications for the type of infrastructure and the types of applications that exist thereafter. And again, if you look at something like Halo Two or you look at Plonky Two, you look at a lot of these sort of more second generation universal proof systems. They roughly have the performance on a single machine of something like a 1970s computer, right? But recursion fundamentally enables you because it's proving it's very parallel to add lots of machines and as a consequence, be able to amortize the cost of compute over a larger number of users. And so the analogy that I'm roughly drawing here is in the 1970s, like, the types of applications and services that made sense in computing were mainframe applications. Hence the System 370 analogy here that I'm loosely trying to draw. But compute is really expensive, but you can amortize it over a large number of customers in the enterprise, right? And if you look at most of what's happening in ZK from in terms of what's getting funded and what people are excited about, it's mostly selling succinctness in some form. In the case of roll ups or things like a lot of these, by the way, are things that have been founded and most of these have been found in the last two years or something.
03:29:10.816 - 03:30:23.164, Speaker A: Some of them are a little bit older, especially on the roll up side, right? So the ability to add more machines and horizontally scale without increasing trust assumptions has enabled sort of this renaissance of more applications that sell succinctness. And this has kind of been, I'd say, the area of the largest growth in ZK in the last few years. So this includes things like bridges and coprocessors integrated, what I call integrated rollups that build both, quote unquote, the processor and the roll up. But then also things that take modular components and assemble them kind of in a way that an upstart PC manufacturer, maybe in the 70s would have done, right? Using Op Stack or taking a chip from Risk Zero or something we happened to work with as a portfolio company for disclosure, and then using it to build some ZK roll up where before it used to take $10 million to build one of these roll ups or maybe more. These frameworks have made it a lot easier to just assemble them as a service. So I'd like to contrast this with what's happening on sort of the client side of the market where you can't really take advantage as much of horizontal scalability, add more machines, because fundamentally what you're selling to the. Client is privacy usually.
03:30:23.164 - 03:31:17.072, Speaker A: And so you're much more limited in terms of how much you can take advantage of recursion capabilities. Right. And so as a consequence, the types of applications you want to run are fundamentally things that you'd be comfortable running on a 1970s computer, right? Roughly. Again, the 70s hardware analogy is maybe a little bit drawn out too far here, but the idea is that application specific architectures were kind of still more useful at the time. That said, there are really interesting examples of interesting applications that people have been trying that take advantage of these new capabilities like the attested mic that Anna and Kobe did. These things take advantage under the hood very often of things like lookups and so forth that are relatively novel capabilities in these systems. People are doing experiments in identity and shared state, global state with private client side information and a whole bunch of interesting experiments are happening.
03:31:17.072 - 03:32:13.228, Speaker A: But we think fundamentally we need more vertical scale to enable more interesting and expressive applications on sort of the quote unquote PC non mainframe side of the market. Here's just a couple of examples of strands of literature that are producing crazy advancements continually. These are not all compatible with each other yet and I won't go into each of them in depth. There's cool things on error correcting codes. There's really cool things on FFT free IOPS that work with Planck and customizable constraint systems as obviously people have heard a lot presumably about all the advancements in folding and a whole sort of strand of literature spending the last year and big table lookups that allow you to do a bunch of pre. All these things are the only thing I want you to take away from this slide is people are sharing, are turning like square root things into log things like they're shaving log in a lot of more mature fields. These would be breakthroughs.
03:32:13.228 - 03:32:58.624, Speaker A: Now, I agree these are like asymptotic and algorithmic things. It's like concretely we'll have to see and they're not fully compatible with each other. But very often the combination of these things, once you are able to first of all, they are becoming more compatible. Once you combine them, they often become greater than the sum of their parts as we saw in the case of Halo two, plunky Two, it's been the history of the ZK space also more generally the history of computing. So we think what's likely to happen this is aspirational at this point, this is not real. There's a transition upcoming. If 2019 to 2021 is a guide that these components then get reassembled modularly, get reassembled into general purpose architectures again and that these universal architectures are then more performant than what we've seen.
03:32:58.624 - 03:33:49.100, Speaker A: And aspirationally that have sort of the Macintosh taking us into the 1980s, which I've talked to some people are working around Plonky Three and they can run roughly at this level of performance, maybe a little bit slower than that. Something that is a chip that's fully compatible, for instance, with a high level language like Rust. So maybe that takes client side ZK into the 1980s, which is where PCs and so forth and more client side things start to really take off. Maybe, but hopefully it'd be fun if it did happen. Okay, let's switch gears and talk about data availability, sampling. And one of the ways that we talk about this internally is as abundant high assurance, unappinionated bandwidth. The word abundant should be clear, there's just more of it assurances.
03:33:49.100 - 03:34:41.696, Speaker A: Nick just gave a great talk on how we gain assurances by sampling fully downloading data as a way of getting assurance as well. Right? But maybe the point to motivate a little bit more here is this notion of unappinionated bandwidth that's available to rollups. And again, the paradigm that we've been in in terms of how applications scale and how infrastructure scales infrastructure updates. Solana today is better than Solana last year. Hopefully next year it's even better. And then sort of applications get this choice of like, what infrastructure would I deploy on? And Ave famously had the strategy deploy on a bunch of different EVMs, have a bunch of horizontal deployments in a bunch of different places so you don't miss out on users and usage and so forth. Of course, if we take Web Two as any guide, these are just stereotypically customer focused companies from Web Two that I've pulled up.
03:34:41.696 - 03:35:26.716, Speaker A: And of course, all of them take some advantage of integration in the vertical stack. So Netflix with Open Connect amazon doing fulfillment apple integrating into Apple silicon These companies, as they scale, they have very strong opinions about how the customer interaction should work. They don't like to import other people's or third parties opinions about how their interaction should work. They have their own. And so it's quite likely, at least, that some applications, maybe not all, want to take advantage of more vertical flexibility. And importing the opinions of the infrastructure doesn't necessarily accomplish that for you. So specifically what I mean is, right now, Solana has a lot of bandwidth available.
03:35:26.716 - 03:35:56.808, Speaker A: The way you take advantage of that is you launch an application on there now, this could change very soon. And I think a lot of people are pushing to change this. It's fairly easy to do. Ethereum is becoming more unappinionated in the way that you use Call data instead of use blobs. And so we're becoming less opinionated in the use of bandwidth overall. And then the other thing that's happening is the and I promised Nick this slide. What's happening is the supply of unapinionated bandwidth is increasing pretty rapidly in the next few months and maybe a few years.
03:35:56.808 - 03:36:47.060, Speaker A: Where again, if you look at Ethereum's out like 100 kilobyte blocks and roughly the block time, you're roughly at like a 56k bit modem from 1996 and you're about to go to fairly more modern broadband connection. Of course, interesting applications come out of transitions like this, like Web Two coming out of similar transition to broadband within a few years of that. But we think what's fundamentally more interesting and by the way, there's more vertical scale that can come from lots of improvements that are planned along these things, like protodank Sharding to dank Sharding. There's really cool things in how you do client side decoding, really cool peer to peer and networking problems, and how you discover people that have samples in the network. Cool things. And how you prove that you've done an encoding. All these things improve more vertically.
03:36:47.060 - 03:37:57.360, Speaker A: But what's much more interesting to us and Nick sort of put this much more eloquently, I think, than I did in talking about user facing wallets and applications running light clients. Is there's a different qualitative? And we increase the assurances that everybody else in the network is getting and maybe even allow you to increase block size and performance over time. So that's a much more, we think, virtuous interaction between applications and infrastructure, where the infrastructure can scale horizontally as the application chooses, how much vertical integration to do over time. Something similar, I guess we noted as well, in the case of recursion and Snarks, you can add more machines, but the level of assurance doesn't decrease, and that enables more virtuous interactions between applications and infrastructure. And so, aspirationally we think, this is the paradigm that allows infrastructure to scale a lot in terms of capacity, but applications to have a lot more control over the stack that they ultimately deliver to the end user. And we're very excited to see what types of applications and what types of interactions between applications and infrastructure emerge from this paradigm. Are we doing questions or I did save some time for I think we had, like a minute.
03:37:57.360 - 04:37:47.870, Speaker A: No? Okay, thank you. Thank you. All right, our morning has concluded. We're going to break for lunch. Be back in an hour. Hello. Hi, everybody.
04:37:47.870 - 04:38:06.848, Speaker A: Welcome to the ZK track at Modular Summit. My name is Anna. I'm the host of a podcast called Zero Knowledge. But I'm also one of the co creators of the ZK Validator. And so, yeah, today we've programmed all of the talks this afternoon. Just a quick word about ZK. Validator.
04:38:06.848 - 04:38:35.160, Speaker A: We're a mission driven validator. We're on over twelve networks. We will be on Celestia. And what the ZK Validator does is we support ZK through various initiatives. We do educational initiatives like the ZK hack, investments in grants, governance, regulatory engagement, and building infrastructure. So, yeah, we curated the afternoon for you. We have some fantastic speakers talking about very, very relevant ZK topics.
04:38:35.160 - 04:39:05.690, Speaker A: And yeah, I will not be the moderator for the whole day. My colleague Agneska will be up here. But, yeah, I want to introduce our first speaker, Brian from RISC Zero, who will be talking about Bonsai, a Verifiable and ZK computing platform for Modular World. Welcome to the thanks, Anna. Oh, okay. Well, that's the old name of the talk that I was going to give. That's fine.
04:39:05.690 - 04:39:55.590, Speaker A: Anyway, I'm going to talk about Bonsai and we're also going to talk a little bit about voting. How do I advance the slideshow? Okay, yes. So I'm going to be talking about Bonsai. It's a verifiable anzk I'm trying to use the new terminology here, platform for a Modular World. So Bonsai basically lets you prove massive computations for all kinds of applications. We're going to talk about a bunch of those in this talk. Yeah.
04:39:55.590 - 04:40:57.864, Speaker A: Also it's now actually available for testing and if you want to get access to it, there's actually a link at the end and please feel free to apply and waking if you're out there. Yes, we will get you a key. So I do want to mention briefly, bonsai is built on top of the risk zero zkvm or VVM. And I don't want to explain exactly what that is because I'm sort of assuming most people know what Zkvms are. General idea is it takes a program in, produces a ZK proof and in this case the risk zero zkvm is a RISC Five chip, sort of a virtual RISC Five chip, which lets it run any kind of normal program. And people have built a whole bunch of interesting sort of hackathon applications on top of it. And we've just recently added pretty amazing capability we call continuations.
04:40:57.864 - 04:43:07.430, Speaker A: This lets you split proofs into any number of chunks and prove them independently, which lowers memory requirements and lets you roll things up in parallel and prove arbitrarily long computations so there's no longer any kind of cycle limit. So one of the things I was trying to think before I made this talk, what are the themes of the modular summit and what I think is going on in Blockchain right now? And this thought that came to me is that an ecosystem of any kind can be as decentralized as it as it can be. I think me and my friends started the first maker space in Seattle like 15 years ago. And the amount of sort of tragedy of the commons that went on in that situation and how difficult it was to manage, I really see as a lack of collaborative tooling and ways to assign values to things that are a lot more fluid than the current tools at the time supported. So I feel like the more we can all do to build capabilities in this ecosystem and especially focus on ones that allow us to collaborate with each other, the more decentralized we can be and the more we'll be able to benefit from the advantages of those things. So these are things that I think the modular ecosystem benefits from sort of roughly interoperability capability, the ability to do anything and run multiple execution layers, diversity, having lots of different projects, different clients for the protocols that are out there, et cetera and then obviously customizability. So Bonsai is a general purpose Verifiable and ZK computing platform, which turns out is very useful to sort of enhance the capability of any system or protocol to do those kinds of things.
04:43:07.430 - 04:45:12.968, Speaker A: Obviously, ZK is a huge part of many of the interoperability projects that have come out recently and you can see how critical ZK technology has been for allowing multiple chains to actually communicate with each other in a low friction and efficient manner. Obviously, ZK has brought all kinds of capabilities to different ecosystems with all of the sort of ZK coprocessing that's going on and the kinds of things you can do with Bonsai. You can build more clients with ZK and you can also build fraud proving systems without doing any work, which I think lets people have a lot more agility when it comes to getting systems that might be too complex for ZK currently out onto the market in a way that they can really serve their users. So all of the great things that you can do with ZK are obviously only useful if they're actually accessible to developers. So Bonsai is really focused on making the most advanced features in ZK and advanced cryptography available to all of the developers in the modular ecosystem across all blockchains and also off chain. Sort of always known we were going to build some kind of network and or platform as a service type offering. But I think the thing that's really crystallized to me over the past 18 months of being in this space has been how important it is to actually build an ergonomic platform for developers, especially when you're dealing with something as difficult as ZK.
04:45:12.968 - 04:46:33.440, Speaker A: So Bonsai's goal is to make it as easy as possible to do the most things that you can with ZK. So what that means for now, Bonsai is going to be a lot of things over time, initially sort of version that's available for testing now and will be at sort of 10 status in the fall. The sort of core of it is a high speed proving service that lets anyone submit proof requests for, for instance, running Linux programs on top of Cartesi, on top of RISC Five. So you can actually verifiably prove the execution of Linux using something like Bonsai. Now you can do this on your own computer, it's just very slow. So Bonsai has a very optimized machine in front of the singular executor part and then can spread computations out to thousands of machines. And with that, very recently, I think the prior fastest risk zero VM execution speed that we've seen was about 100 khz.
04:46:33.440 - 04:47:22.468, Speaker A: We're now up to 2.5 MHz, so it's about 25 times faster and we really expect to see that number continue to go up over the near term. So it's really amazing how quickly you can prove some fairly out there things and we'll talk about that in a bit. So we also have built out a proof relaying system. This is effectively focused on Ethereum chains for now, but this is a full Foundry integration and template that lets you and set of smart contracts that let you easily interact with Bonsai from your smart contract. So along with that, in the past, we're a Stark based system. We always have been.
04:47:22.468 - 04:48:04.896, Speaker A: This produces proofs that are too big to economically verify on Ethereum. So we've built this is brand new, and I'm very excited about it. A Stark to snark translator for our proving system. So you can actually just post a single gross 16 proof for any risk zero computation in the future. We'll also let people save money by aggregating a bunch of proofs and sort of posting that for people and that will still integrate completely with the sort of relaying infrastructure yeah, that will be coming up soon. We'll eventually probably make some kind of proof market. We already have a bunch of light client and ETH proving stuff in the pipeline.
04:48:04.896 - 04:49:07.800, Speaker A: Yeah. So there's going to be a lot more. These are not just early case studies. It's some of our partners and some of the, I guess, example applications and spaces we've started exploring on the application side. The first one I talked about this with, Neath Denver a little bit, was a central limit order book that utilized Bonsai to run all of the matching logic off chain. So currently, the version of this that we'll eventually release after we clean it up more basically works by having people place their vote, their orders on chain. And then the smart contract does literally nothing with those except call out to Bonsai where Bonsai ingests whatever orders have been placed, whatever orders are open, computes whatever matches exist, and then sends back effectively a set of settled orders or orders that are no longer valid, et cetera.
04:49:07.800 - 04:50:15.992, Speaker A: So by doing that, we actually found that using this pattern, it's about 100 times cheaper than sort of a pure EVM CLOB would be and actually cheaper than Uniswap V three by two to three times. So of course, people always ask, why on earth would you build an order book and ZK? It's going to be too slow, obviously. Latency is a factor and ZK is kind of slow, but this runs pretty quickly. And you're also I'll talk about one of our partners that's doing some innovative work to combine ZK with some other techniques to resolve that. So this is now available on our GitHub page under Examples Voting. I think, or Governor, we just posted this. This is a full app built on top of this foundry template I mentioned that integrates with existing Dow voting frameworks.
04:50:15.992 - 04:51:24.310, Speaker A: So it's kind of an example of how you can use Bonsai to effectively replace gas intensive components of any application. And when people wanted to do this, I was kind of skeptical that it was going to be worth a time. It's actually three x cheaper than the original application. And I was really surprised to see that if you then take this kind of concept and start integrating something like Celestia to store your votes, you start to reach the capability to do a bunch of things on chain that were difficult before, especially when you couple with the count abstraction coming online. Hopefully people have seen some of the amazing demos people have built with this. I really think there's going to be a lot more capability to build applications that actually use blockchains and people use them. So I'm really excited about the ability to have more information from people in a sort of ZK and pseudonymous manner coupled with identity feeds from Sismo or anything like that.
04:51:24.310 - 04:52:17.374, Speaker A: Yeah. So a nice thing about this, we were able to effectively build something that would interoperate with existing UIs and applications in about three weeks with one person. So this kind of demonstrates the efficiency gains of being able to use general purpose language for your ZK development. And this is a recent one that we've been very heavily working on as part of our proposal to Optimism to help them with ZK, which I'll talk about in a second. And where we've gotten to now is that you can run an entire ethereum block. This is missing some optimization for pre compiles. This is using an existing EVM.
04:52:17.374 - 04:53:08.242, Speaker A: I think we support Revm and Sputnik and we'll support the rest at some point as well. And yeah, so you can actually run an EVM that's been fully audited and is in use in many other places directly on top of risk zero. Now it takes quite a while. I think the two to 4 billion cycles takes about 15 minutes on Bonsai, but that's going to go down pretty rapidly, I think. So, yeah. We're going to be in a place where we're looking at ZK approve ETH blocks for full size ETH blocks without needing to even think about writing an EVM or about compatibility with other smart contracts. Yeah.
04:53:08.242 - 04:54:24.198, Speaker A: And we expect people to start integrating this into various rollap frameworks. Nobody's doing that quite yet though, and I'm excited about it. Okay, so I do want to talk about some integrations that we've been working on and this is sort of a general category here and I was on a panel Monday, I think, and people one of the questions was, okay, it's ten years from now or five years from now and ZK has lost to optimistic approaches. Why? What happened? And I don't think it's an either or kind of question at all. I think optimistic approaches coupled with ZK approaches can produce a lot of value for the user in terms of supporting livelier applications. And then obviously throwing ZK on top of that supports fairly quick liquidity in and out of whatever sort of applications you're using. Layer N, the team over there actually built a sort of example of a actually they're pretty far along of a fraud prover for their order book and they basically didn't have to do any work.
04:54:24.198 - 04:55:27.222, Speaker A: You just run the matching engine yourself on data and produce a proof that the results that got posted on Chain aren't the actual results. So this lets you run your order book or any kind of application sort of at the frequency that you desire and then gives an opportunity to kind of use ZK to tie it all up and give everyone confidence that everything's in the right shape moving forward. And indeed, this is a lot of what optimism wants to do with their sort of ZK support for optimism. Very excited that us, along with the awesome team over at One Labs were both selected to work on this. So very excited about that. One of our partners from sort of the earliest days of our company and theirs has been the Sovereign team. They're building the Sovereign SDK, which is a roll app framework that's highly aligned with the modular ecosystem.
04:55:27.222 - 04:56:21.802, Speaker A: I really love what they're doing. They basically made a very sensible way to program blockchain applications. Pure Rust has tons of flexibility and then they kind of take care of all of the difficulty of figuring out which parts of the modular stack to use, where and which ones. So Risero is going to be one of their first adapters that actually supports doing ZK computations for Sovereign apps. Very excited about that. And yeah, what else did I want to say? Also one of our great sort of partners and people we collaborate a lot with is the team over at Eclipse. They're building roll up framework as well.
04:56:21.802 - 04:57:33.810, Speaker A: The work we did with them so far has been limited to a pretty amazing project, which is effectively doing ZK solana executions by turning Risk Zero into an Ebbf prover. So we got pretty far with that and really looking forward to starting to give people the ability to migrate code from Solana to wherever they want it to run or doing roll ups on solana, even though in theory you don't need them. One of the most recent sort of hackathon winners, I think this is at ETH waterloo built a really amazing and fairly simple I mean, they did it in a hackathon and it's effectively a demonstration of using some new authentication standards along with ZK to let you immediately sort of make a wallet that's based on a biometric Identifier. Okay, yeah, thanks. Two minutes. Yeah. So this lets you make wallets without worrying about seed phrases or anything like that.
04:57:33.810 - 04:58:03.546, Speaker A: And you can see this is a sort of amazing entree into making crypto accessible for a lot more people. That's pretty much all I have. There's a bunch of links here if people want to get to know more about Bonsai, what we're doing. And if you want access to the Bonsai API, you can go ahead and apply at that link up there. Thank you. Thank you very much. Brian yeah, let me get that.
04:58:03.546 - 04:58:38.762, Speaker A: And now I would like to introduce to the stage Zach Williamson, who's going to talk to us about writing the code, not the circuits. This is for hello. Hello. We good? Yes, it works wonderful. Hello, everyone. So, yeah, I'm Zach. I'm the CEO of Aztec, and I'm here to talk about basically if I can kind of go back does this even work? Never mind.
04:58:38.762 - 05:00:09.080, Speaker A: Okay, we're not going backwards. Yeah, so I'm here to talk about how does one turn code into ZK circuits, specifically with the angle towards looking at privacy. So privacy is hard if you want to like the goal of what we're doing at Aztec is to enable users to write smart contracts where you can have genuinely private state variables inside of them. Where they're encrypted, you can still do logic on them, but only owners of those variables with the decryption keys can actually see what's inside. And so we spent a lot of time over the last few years basically trying to figure out how do you take this key foundational technology of zero and launch proofs and actually present it in a way that gives you the benefits of privacy whilst making it accessible to developers without needing to require cryptographic knowledge and basically boiling down all of the complexities to do with ZK Two and privacy to a relatively simple set of heuristics. And so the goal of this is to create rather composable modular abstraction layers that convert the code of a smart contract and basically the consensus algorithms that you're using to verify its correctness into algebra's and nonsproofs. So these are the abstraction layers that we've come up with, at least when it comes to how to do this.
05:00:09.080 - 05:01:12.038, Speaker A: We're taking a very different approach to Brian and RISC Zero, basically because of the private state model. Once you want to create blockchain transactions where you have private state in the mix, then you can't take an existing architecture like Ethereum and then just wrap it in a ZK proof and call a day. Because even if your state is encrypted, the act of modifying it still basically leaks the transaction graph. So we basically had to build a lot of this from the ground up, starting with what do you need to turn a Snark code into Snarks? You need a cryptographic backend, basically some kind of proving system that will construct and verify proofs. You then need some low level language that you can use to convert programs into circuits. The idea here is that we have this abstract intermediate representation, Asir, which is our attempt at LLVM for Snocs. So it basically describes generic ish constraints that are SNOC friendly.
05:01:12.038 - 05:01:53.480, Speaker A: And the idea is you can compose a program out of these. You wouldn't write a program directly in ASPHR, but the goal of a series that you take language front ends like Noir, like Circum, and those which presents a nice programming language with clean semantics and that gets converted into the intermediate representation. A bit like how you take Rust. And Rust is a language front end that compiles down to LLVM. Noir is a language front end for ZK that compiles down into Sier. And the goal of this is to be very modular so that you can swap out various proving systems. So like Haley Two or Aztec stuff, retinberg or ArcWorks to basically fit your own custom needs.
05:01:53.480 - 05:02:23.090, Speaker A: And then on top of that, you actually need tuning for your language and then you need a program execution environment and a transaction execution environment. Basically the entire network and architecture infrastructure around sending transactions to a distributed network. So, yeah, let's start at the bottom. Zero knowledge proofs. Yes. Who wants to do some math? I didn't hear you. Who wants to do some math? Yeah, I know, end of the week, we're all a bit tired.
05:02:23.090 - 05:03:05.034, Speaker A: So this is nonsense. Basically. It's not nonsense, but a zero knowledge proof is you have a proof of you have a Verifier and some kind of common state, some statement with some public input, secret inputs, and the goal is to prove that your inputs belong to some defined relation and the proverb runs some sort of algorithm. This is not a program, this is not a smart contract. This is weird messy algebra and it's a pain. It set aside these three, like, three fundamental conditions completeness, soundness or knowledge. Completeness is basically, if you an honest prover, can always make a valid proof.
05:03:05.034 - 05:03:45.254, Speaker A: The Verifier wanted to reject a good proof. Soundness means that a Verifier will always reject a bad proof and zero knowledge means that effectively the Verifier doesn't extract any useful information out of your proof. Yes, algebra is like you can't write complex algorithms in terms of algebraic equations, not practically. It's a bit like trying to write a computer program by flipping bits on a magnetic hard drive with a needle. It's not going to work. And so the basic abstraction layer that we have to move from ZK Proving Systems to Snarks is the concept of an arithmetic circuit. So instead of having an imperative program, you have these concepts of arithmetic gates.
05:03:45.254 - 05:04:25.286, Speaker A: Gates have wires, they go into the gates, they go out of the gates. The gates perform basic operations like Add and Mo. And you can use this to sort of represent a program, as in if you have an infinite number of gates, you can represent any Turing complete computation. Therefore you can sort of think of it as being slightly ish Turing complete. And we have very nice weight reductions that convert arithmetic circuits into Snark systems. And so this makes abstracts away some of the evil complexities of ZK proofs, but not very much of it because arithmetic circuits it's still algebra at the end of the day. So yeah, basically zero knowledge is annoying.
05:04:25.286 - 05:05:12.690, Speaker A: It's hard. We want people who are working with ZK Tech to not. Have to know anything about ZK or cryptography because it's an absolute nightmare. Okay, so I've described some of the basic abstraction layers that we can use to construct Snark circuits. But how do you turn a program into a Snark circuit? You know, programs are these weird, complicated laviathans of code with lots of conditional branching and predicates and working on complex data structures. How do you turn that into additions and multiplications? Well, you can do it with Noir. So this is a programming language that we're building from the ground up to be ZK friendly and support the kind of complex private state models that you need in private transaction environments.
05:05:12.690 - 05:05:55.874, Speaker A: And so we've modeled it after Rust. So it has variables, it has things like integers and booleans like you'd expect from a regular programming language. And yeah, so it's a front end, basically. It doesn't compile circus directly into constraints, it just compiles them into SA. And you can plug in any backend you want. That supports SA goal, is to be a completely open architecture so that other folks can customize it to their needs, plug in whatever cryptography they need to get their job done. So, yeah, you have even things like arrays, and you can access the arrays with non constant values, which to a programmer is like obvious to a cryptographer.
05:05:55.874 - 05:06:29.546, Speaker A: That's really hard, but we can do it and basic compound types that you would expect. So effectively, Nor is like a programming language from the 1960s, but with modern, modern semantics wrapped around it. We have even things like a module system and sub modules. Isn't it amazing? It's better than C plus plus. We even have loops and if statements, which again, as a programmer, of course you have if statements. As a cryptographer, you think if statements, they're hard, but we've got them. And so blah, blah, blah, Noir, blah, Noir, noir's li amazing.
05:06:29.546 - 05:06:48.918, Speaker A: Noir is awesome. You should use noir. So let's move on. Okay, so now you have a programming language that takes your high level pro computer program and converts it relatively efficiently into a Snark circuit. You then have a cryptography backend that turns your Snark circuit into a zero proving system. Do you have a private blockchain? No, you don't. Not whether just that.
05:06:48.918 - 05:07:26.286, Speaker A: All you have is a programming language. What we need is an execution environment. And so basically some system that doesn't need to be physical or real, but that will execute your program and perform actions as a result of what your program is saying. So Node JS basically can be considered an execution environment for JavaScript. The Ethereum network can be considered an execution environment for EVM programs. And so this is where Noir with smart contract functionality comes in. So this is basically what we're building at Aztec.
05:07:26.286 - 05:08:32.006, Speaker A: It's a way it's adding the semantics around smart contracts into Noir. So you can define contracts, you can define functions that operate on public state, private state, you can define storage slots and storage variables like you do in a regular Smart Contract language. And then on top of that is this on the next slide? No, it's not. So sneak peek of contract syntax. So this is the kind of stuff that we are developing internally that will be available externally, hopefully next month that allows so this is just some random example transfer function, but it has weird keywords like secret and a secret balance. The goal is basically all the complexity around what the hell that means, all of the encryption, all of the weird stuff you have to do with merkle trees, nullifier sets and witness encoding inside Snarks that's all abstracted away and you get nice easy storage slots. So yeah, basically taking it's an abstraction that gets rid of all of the ugly stuff on the right hand side and then we combine that with a real proper bona fide execution environment.
05:08:32.006 - 05:09:38.558, Speaker A: That is the Aztec layer two. It is a rather large collection of Snark circuits that compose a layer two network where effectively the goal of the network is to the user will send Snark proofs that represent the function calls to various smart contract functions. And the roll up circuit will one of the roll up circuits effectively will use a lot of recursive proof composition, so proof verifying proof verifying proofs to basically emulate a cool stack for the user so that you can basically have a function call stack of private functions and public functions and you can work your way through the cool stack by recursively generating these snark proofs. And then you have a roll up circuit which will take these Snark proofs and validate their correctness perform all the state updates validate or correct, do fee management, do consent, like all of the sequencer selection consensus, algorithm checking. And you end up with a proof of a block. But not just any block block, a block which has an encrypted state tree. So yeah, that's the ZKZ roll up.
05:09:38.558 - 05:10:19.114, Speaker A: It inherits ethereum's security. We're leaching from ethereum's consensus like all other layer twos. But the critical difference between Aztec and the other things is the fact that we support both private and public state. And you can use that to create hybrid applications. Something I often get asked is what can I build with privacy? Right? Privacy is like this weird abstract concept and we don't have it in web three. Not really. So how to articulate this? One of my go to examples is webulf sign in flows or specifically things like, let's say you want to sign into a Web three account using Apple ID.
05:10:19.114 - 05:11:03.814, Speaker A: Using, like, FaceTime. Not FaceTime face. ID. What will happen on your phone is that your phone is going to use its hardware security module to sign a digital signature according to a message format defined by Apple and according to a public key that is also described by your Apple ID. There's no reason why you can't verify that in a smart contract using account abstraction and that can then become the default portal to your account. However, without privacy, that means that every time you transact on chain everyone can link those transactions to your specific Apple ID, which is rather problematic if you're doing anything with any kind of real value associated with it. Maybe you don't want people to know that you're trading degen board ape NFTs with all your life savings.
05:11:03.814 - 05:11:39.450, Speaker A: So that's one example. Things like Dow governance, private voting. One of the key problems with Dow Governance right now is the massive social pressure to vote according to certain ways. And for better or worse, privacy means that people can vote according to their conscience. Perish the thought. So yes, maybe in a year or two we will be able see the true dark heart of Web Three and what our community really thinks. There's some of the things you can do with privacy, then there's the obvious user hiding properties.
05:11:39.450 - 05:12:29.852, Speaker A: You can hide your identity and you can then link your cryptocurrency account to a real world identity. So yes, so I'm actually at the end of my talk because I blitzed through all the last slides and this wasn't supposed to be a 15 minutes talk, I got the timings wrong. So we have a few minutes of questions. Thank you very much. Does anybody has a question? So you mentioned public and private applications. Do you have an example of how that would work or what are the use cases for both of those at the same time? Yeah. So the reason why you'd want composable hybrid applications is because there are a lot of decentralized applications that require global state.
05:12:29.852 - 05:14:01.212, Speaker A: And one of the difficulties with an encrypted state database is that in a private world state is owned by individuals or groups of individuals it's encrypted against their public key. And so consider a DeFi app like Uniswap or any kind of automated market maker where you have concepts like the total supply in your network, liquidity pools, things like that, that's all global state and therefore needs to be public. And so then the question becomes, well, okay, well how do you get privacy guarantees with a dex? Well, one thing you can do quite easily is you can keep the token values public but the identities private. So the idea is that you can have token contracts which have basically privacy preserving functions which allow you to hold shielded balances and then you can directly deposit those you can basically deposit those value deposit into a AMM like Uniswap where the value of your tokens is public but the identity is private. And then you would have the public uniswap algorithm execute the trade. In many ways I suspect this is going to be quite a popular model in the future because it gives privacy for the user, but it still means that you get transparency for the protocol. So you still know that whatever algorithms are being executed by the protocol have been run correctly.
05:14:01.212 - 05:14:50.812, Speaker A: There's no centers of trust. And longer term you can close the circle and make the entire system private by adding in multiparty computations so that all of the price finding algorithms that an AMM uses are executed in a multiparty way so they can actually be genuinely private. But that is a future project to be built. I think we have one more question. Time for one more question. Hey, so you had a slide there where you sort of described from ZK to Snark using a circuit, but then from a Snark to a programmable environment I'm still like, how do you turn then a circuit into a place where you can do all of this? You talked about the language, but I don't like where is it? You're right, I did skip that. So let's go back.
05:14:50.812 - 05:15:27.528, Speaker A: So the first kind of system that kind of did this was the Zexi protocol from 2016. So we have the concept of a kernel snark circuit where what a kernel circuit does is it verifies the correct execution of a single function call. So let's imagine now you have a smart contract. Your smart contract has public functions and private functions. Each function is converted into a snark circuit. And so then what a user does is they construct one of these kernel circuit proofs where the kernel circuit takes as an input a function call stack. So at the very start, that function call stack will have one entry in.
05:15:27.528 - 05:16:15.290, Speaker A: It'll be the function that you want to call. And what the kernel circuit will do is it'll pop that function call off the call stack. It will verify a proof that you've provided, assuming you want everything to succeed, you provided a proof that proves that the function call has been executed correctly. The kernel circuit will verify that the verification key belongs to a specific smart contract. And then what the kernel circuit is going to do is it's going to grab the public inputs of your inner snark circuit, the one that made the function call, and interpret those inputs according to a defined abi. And as part of that abi, that function call may spit out additional function calls to be executed. So if you have, for example, you want to call approve and then you want to call transfer from on different contracts, for example.
05:16:15.290 - 05:16:50.084, Speaker A: And so that's one iteration of the kernel circuit. But the kernel circuit is recursive as in that it verifies a previous proof of itself if one exists. And so what you can then do is you can repeatedly construct proofs of the kernel circuit. So to start with, you have one function in the cool stack that gets popped off and verified, but then more functions get pushed onto the call stack as a result of your first function call. And you just repeatedly construct proofs until your function call stack is empty. And then the output of that is, well, you now have a proof of a kernel circuit with an empty function call stack. So no one knows what functions you've called.
05:16:50.084 - 05:17:20.480, Speaker A: But also spat out of that proof are basically a bunch of encrypted state changes, state updates to perform as a result of those functions being executed. And that's kind of sort of how you can get a quasi execution environment out of a snark circuit. Okay. Thank you, Zach. Cool. Thank you. Okay, now I would like to introduce to the stage, henry Devellens from Penumbra.
05:17:20.480 - 05:17:44.052, Speaker A: All right, so here we are. Hello, everyone. My name is Henry. This is my talk. Shielded transactions are roll ups. So if you're already convinced, then great. Otherwise, let me kind of go through what I mean.
05:17:44.052 - 05:19:11.680, Speaker A: So I work on a project called Penumbra. What it is is a private proof of stake L, one that has an interchange shielded pool so you can record any kind of asset from any IBC compatible chain. And what can you do with those assets once you move them into the shielded pool? You have a dex that allows people to do private on chain strategies. So this talk isn't primarily a talk about what Penumbra is as a product and so on, but we've been focused on how do we solve this one really specific use case. And then from trying to solve that one specific use case, what are the kind of common features that we can generalize to more varied kinds of computation? So to start off the talk about sort of, how do we view shielded transactions as being like a weird kind of roll up, why don't we say, what is a roll up in general? And I think a lot of the time people have this idea of, oh, a roll up is a way to have more copies of ethereum, right? And I think that that's a pretty limited perspective of what we could do in general. I would say a roll up is when you have one part of a system that we'll call the base and there's another part of the system that we call the roll up. And there's this kind of, like, flow within this system where the base offloads execution onto the roll up.
05:19:11.680 - 05:20:22.516, Speaker A: And then the roll up sends back a state route as well as some kind of reason why people should trust in that state route. Maybe there's a ZK proof, maybe there's some kind of economic model with an optimistic roll up, but fundamentally, it's about having a flow of execution moving out onto the roll up and certification and kind of summary of the results coming back. So there's a super, super enormous flexible design space here. And in this talk, what I want to do is look at a shielded chain from this perspective of thinking of things as roll ups. So to start off, in order to have a shielded chain, you need to have some kind of composable state. So you need to have the state of the chain split up into all these little fragments and each transaction is going to consume certain existing state fragments and then produce new ones as outputs. So this is kind of like a UTXO model, although I personally am a little hesitant to use the word because it has a lot of bitcoin related baggage.
05:20:22.516 - 05:21:21.820, Speaker A: Really what's happening here is that the state is split up into fragments and transactions only operate on certain fragments of the state. Why do we need this? It's so that we can replace all of those on chain state fragments with just commitments to those states. And that way instead of having to have the transaction actually work on all those things directly, we can just do a ZK proof and hide the details of the state from the public chain. But when you do that, it's not really just that, oh, we add in a ZK proof. Fundamentally, what's happening is that the execution of the state transition is moving off chain. And so effectively you can think of each individual transaction as being its own little sort of micro personal roll up. And a lot of the problems that arise in trying to build practical shielded chains, you can kind of see from this perspective, as we'll see.
05:21:21.820 - 05:22:23.468, Speaker A: So one big problem that comes up is I guess never mind. So yeah, before doing that, we'll give a little more detail on this perspective. Right, so what do I mean exactly when I say that a shielded transaction is a kind of a micro roll up? Let's look at some transaction on a shielded chain in general. What are the pieces of this? We have some kind of ZK proof that's going to provide trust that everything that the transaction is doing was done correctly. We're going to have some commitments to new output states that this transaction has created and then we're also going to reveal some nullifiers that consume the input states. So you can prove in zero knowledge, hey, I know about this state that was previously included in the chain, it's valid. But you now have a problem of how do you prevent double spends.
05:22:23.468 - 05:23:53.912, Speaker A: And the general technique to do that is you assign each sort of piece of state a unique serial number or nullifier that's only derivable by the user that controls that chunk of state. And that way they can reveal this random value and kind of remove that piece of state from the kind of active set without revealing exactly which state they're nullifying. Finally, a shielded transaction generally is going to have some kind of encrypted payload in it. And the reason is that in order for me to use this chain, I need to know not just the chain needs to be convinced that my transaction is valid, but as a user I care about being able to learn what exactly my transaction is. If I go on another device. If I'm trying to sync, how do I recover my own state? So you can think of an existing shielded chain, like for instance, Zcash, as kind of bundling in this data retrieval mechanism into a monolithic chain design. So what's kind of the problem here, this is what I was about to get to is why haven't we seen this be particularly useful or receive a lot of adoption? And I think the problem is that when you do this change, what you lose is the ability to do late binding.
05:23:53.912 - 05:25:07.728, Speaker A: What I mean by that is we have this picture of, okay, here's a transaction, it has these inputs, it has these outputs, and this whole state transition is this kind of sealed pre computed thing. But when you look at what people actually like to do with blockchains, they like to interact with the chain. And so that means that they need to have some kind of late binding capability. You want to say, I want to do a swap and when I do my swap, I'm going to commit to, these are the inputs that I want to swap, but I'm not going to sign over like here's the exact state of the uniswap reserves and here's therefore the exact amount of output that I'm going to get. I don't know that because at the time that I'm making my transaction, I don't have access to that state and I can't ask the entire world to just stop and do nothing while I submit my transaction because I'm important. So the way that this is usually done is that on a transparent chain is that when the transaction is executed, it can access the chain state and so the chain can kind of fill in the gaps and then determine what the outputs are. And naively, when you do this sort of shielded transaction rearrangement, you lose this ability.
05:25:07.728 - 05:26:17.588, Speaker A: This is one of the things that we really focused on in trying to build a Dex. This problem of how do I know what the price is before it gets executed just shows up very, very clearly at the start. And the answer that we came to is that in a sense, this is a kind of like a similar problem as doing like cross roll up communication. If every user is doing their own little independent state transitions on their own end user device, then somehow those need to be able to communicate with each other and they're all going to be executing asynchronously. So we need to have some model to do asynchronous ZK execution via message passing and a kind of schematic diagram of how this can work is I'm going to make an initial transaction that kind of sets off the action that I want to do. It's going to consume my private inputs, but because I don't know what I'm going to be sort of filling in the gaps with yet, I can't finish the computation immediately. So instead I'm going to send a message to whatever public contract I'm interacting with, maybe that's on some other shard of the state.
05:26:17.588 - 05:27:42.928, Speaker A: And my output for my initial transaction is actually going to be a commitment to some future in the programming language sense of an asynchronous computation that's waiting for some fields to get filled in and resumed. If you've done async await programming, you can imagine sort of each await point where you're waiting for some message to come in as turning into a point where you need to pause the computation. You need to commit to all of the intermediate execution state at that moment. And the reason that you do that is so that later, once you get the message from the contract coming back, maybe that's like, hey, your swap was executed with this price, now we know what the price is. Now you can mint your outputs. The user who had created that commitment to their intermediate execution state can now resume execution by spending their commitment and slotting in the public inputs from the contract into the appropriate places and minting their private outputs. So when we do this on Penumbra for swaps, the public input that's coming in from the contract is what was the batch price for that block? And the private outputs are then that user's pro rata share of the batch.
05:27:42.928 - 05:28:29.124, Speaker A: So what I think is really interesting about this perspective is that it means that you can kind of generalize, right? You start with this idea of oh, we have a multi asset shielded pool. We can record any asset. But since those assets can be anything, they can also be assets that represent arbitrary intermediate execution states. And so now in the shielded pool, the invariant that I can't double spend funds, that I can't just print tokens is saying that I'm not allowed to just kind of restart execution. I can't clone my program. I need to sort of only act advancing my computation in the allowable way. But the shielded pool can be recording anything that I want.
05:28:29.124 - 05:29:23.616, Speaker A: And so this I think is what kind of gives me this sense that right now there's kind of this split in the ecosystem where there's a bunch of efforts that are working on ZK for scaling. There's another collection of efforts, Penumbra included, that are working on ZK for privacy. But I think we're actually going to see a kind of convergent evolution of these things into some sort of glorious future. This is the shill portion of the talk and the perspective there is that what privacy enables is a kind of edge compute for blockchains. So in the current world, right, all of the execution is happening sequentially, one after the other. Everybody is doing a transaction, taking a lock on the entire state of the world. Everybody else has to stop everything while I run my transaction.
05:29:23.616 - 05:30:33.736, Speaker A: And in this private world where we do computation asynchronously we can push the computation off of the base layer and out to the edges of the network. But that's not necessarily saying like, oh, let's just make a second copy of the exact same state model, but we can go much, much further and have this sort of fractal pushing all the way out until the end user device, possibly over multiple hops. And then those users are going to send back their transactions that have only their proofs and data, and those things can get summarized as they move toward the core of the network. So in this picture, the reason that it's sort of like grayed out, right, is that there's no actual computation happening there. It's just certification of the data. And these are all users with their own wallets doing their own computation locally. With this perspective, there's another kind of neat thing that this enables, which is that there's new possibilities that come out for state management.
05:30:33.736 - 05:31:08.544, Speaker A: I think this is one of the hardest problems with blockchains and with scaling blockchains. Having every single person in the world just use one world computer doesn't quite scale. And if we go back to this picture of like, okay, what is actually in this shielded transaction? We can look at it and say, what's really the expensive part here? Right? So we've got this proof. Let's say it costs like, two milliseconds to verify. That's not a big deal. We have the commitments to the output states. Those are 32 byte values.
05:31:08.544 - 05:31:47.456, Speaker A: Clients might have to feed those into a tree that they're using locally for proving, but they can filter. Like, hey, these ones I don't really care about because they don't relate to me. The nullifiers are also 32 bytes. Really, only full nodes have to be processing those. And although there's discussion of like, hey, there's this nullifier bloat problem where the nullifier set grows forever, I mean, realistically, you can have a lot of 32 byte values before you really run into problems. And so I don't think that's a pretty significant part either. The real problem is this encrypted payload, right? It's not 32 bytes.
05:31:47.456 - 05:32:49.560, Speaker A: It's going to be bigger than that because you need to have the full transaction data. And unless you can build some other kind of system for allowing users to identify which states are related to them, a priori would have to be scanned by every client. And most critically, if you lose this payload, it's actually equivalent to losing your funds, right? Because the chain doesn't have it has perfect privacy. There's no way to know what all of these state commitments are. And so if you don't know, like, hey, what was the note that recorded my funds that I control? There's no way that you could possibly spend it. And if we look back over the last, like, ten years of doing blockchains, managing state managing keys is very, very difficult. Even if you have fully deterministic derivation of all the key seed phrases are basically a technology that was invented to solve the key loss problem.
05:32:49.560 - 05:33:33.204, Speaker A: Like, let's just figure out how we can derive everything from one secret that we can somehow get a handle on these payload data. It's equivalent to a key, but it's created dynamically. And so putting it on chain is not just, oh, this is a convenient way to schlep the data around, but it's also a kind of a security feature. Right. If you want to move that off chain, you have to have a really, really robust story of how does a user ensure that they have backups of all their data. So this has led us to thinking about a subproject that we called Narsil. And the idea of Narsil is to try to create a personal roll up.
05:33:33.204 - 05:34:46.924, Speaker A: So using the same diagram that I had before of, okay, well, we have our base chain. It's just mostly going to be focused on doing certification and we're moving the compute out to the edges. Well, we can have this fractal perspective where those edges, maybe some of them are a web extension that's syncing your client state. Maybe there's an app on your phone or maybe one of those edges is actually its own chain. Right? So we started actually thinking about this because we wanted to have a story about how to do Threshold custody where multiple keyshard holders can collaborate to produce a signature on a particular transaction. This way you can do like multisig transaction authorization flows. But when you start thinking about that, you get into questions like, okay, well, if we're not using the chain to coordinate, how do the signers who are participating know what all the signing requests are? How would we make those signers have a consistent view of what transactions have been requested? And we're already using Tendermint Comet BFT, so sounds great, we know how to do that.
05:34:46.924 - 05:36:06.708, Speaker A: Why don't we just have the shards communicate with their own cometbft network? They can run it in proof of authority mode. And what that means is that you actually not only get strong consistency of like, hey, what are these people signing between the custodians, but it also means that every custodian has a fault tolerantly replicated audit log of everything that has ever been signed by this Threshold key. And that's exactly what you need in order to be able to have assurance that I can safely move my user state off chain. I can skip posting those encrypted payloads and keep that only for myself. And I know that I am going to be relatively secure and resilient because I'm already replicating that across my own internal network. Right, so you could imagine, say, like a market maker who is going to be updating quotes on chain reasonably frequently, right? Why should every user have to be scanning that market maker's state updates? If we could have that market maker just run their own personal roll up, replicate their state internally and get less. Gas fees.
05:36:06.708 - 05:36:39.920, Speaker A: So that hopefully sounds interesting. The plug is if you want to play with any of the stuff that we're building, here are a bunch of links. Everything we do is totally in the open. We have testnets, you can play with them. We have a discord. If anybody has any questions or wants to talk about it with us at any point, just know, show up and we're always happy to chat. Thank you very much, Henry.
05:36:39.920 - 05:37:22.896, Speaker A: If you have any questions to Henry, please find him somewhere in a lobby. And now I would like to introduce to the stage, our panelists and our moderator, Anna Rose. Welcome to the stage once again. And then we're going to have Nico, Benedict, Zach, and Chris. Keep it consistent. Cool. Well, what a panel I have here.
05:37:22.896 - 05:37:57.320, Speaker A: This is so awesome. This panel was inspired by an episode Benedict and I did recently in that we talked about the different parts of a ZK system. So this is going to be a pretty technical panel. I think we're going to go pretty deep in, but I think let's start with introductions. So I introduced myself earlier, but I'm the host of a show called Zero Knowledge. Yeah. And I also am curating this afternoon of ZK through the ZK validator.
05:37:57.320 - 05:38:20.960, Speaker A: Hi, I'm Nico and thinkeria I'm a researcher in cryptography, mostly applied cryptography geometry. Hey. My name is Benedict. I'm the co founder and chief scientist of Espresso systems work on decentralized sequencing. There's a whole nother track for that. The other one, actually. But I also dabble in zero knowledge proofs.
05:38:20.960 - 05:38:45.704, Speaker A: Hey there, I'm Zach. I'm the CEO of Aztec. We're a privacy infrastructure provider for Web Three. And yeah, I also dabbled in zero knowledge proofs. I'm Christopher, I'm one of the co founders of the Inoma project. I am not a classically trained cryptographer, so I'm probably horribly out of my depth, but perhaps I can provide a perspective on how these zero knowledge proof systems look like from the outside. We'll see.
05:38:45.704 - 05:39:09.312, Speaker A: Very cool. Yeah. So I wanted in this intro. So before we dive in, I do wonder if you could choose sort of one work paper project that you think most defines you that people here may be familiar with. Sure. I think I'll go after Zach, actually. It makes more sense in that the work I'll talk about probably builds upon the one he's going to mention now.
05:39:09.312 - 05:39:32.568, Speaker A: Okay, why don't we actually start from that side then? Right. I have not written any zero knowledge proof systems, sadly. I hope to someday. But a few years ago, I was starting to go into the space, and there just seemed to be a lot of brilliant people writing zero knowledge proof systems, and I figured we need people doing something else. So maybe in ten years when I'm, like, off in a cabin, I'll do that. But probably the most popular paper was IBC. Unfortunately, I think the paper is terrible oh, no.
05:39:32.568 - 05:39:58.272, Speaker A: Yeah, I want to rewrite it to be clearer, but maybe I'm just biased after the fact. Cool. Yeah, I guess. Yeah. Plunk, I guess that's the thing. That's what I'm known for. And does everyone know what Plonk means at this point? Who here knows what Plonk means? Actually? Who here knows the Plonk proving system? Yeah.
05:39:58.272 - 05:40:35.192, Speaker A: Okay, so people know the proving system, but nobody knows where that word comes from. Amazing stands for permutations over the LaGrange base for Ecumenical Double. The non interactive arguments of knowledge. And the A is Silent some creative liberty. And the word Plonk also has a double entendre. There is a definition. Yeah, it's British slang for cheaper quality wine because like a bottle of bad booze getting to the bottom of Plonk is going to give you one hell of a headache.
05:40:35.192 - 05:41:00.756, Speaker A: Is it like a bottle or are we talking like bag? Oh, it's box wine. Is box, okay, box wine, okay. Yeah, I guess. Probably still. I'm most known for bulletproofs, which is a zero knowledge proof that is used in Monero and many other different proof systems. I think Zcash and IO actually as well are part of it. Yeah.
05:41:00.756 - 05:41:32.450, Speaker A: Not as known as these two cryptographers. But I did put out, let's say, an observation building up on Plonk that some techniques that we saw elsewhere in the ZK stack were applicable to Plonk. So that thing is called Sangria fittingly. Very nice. Okay, so I wanted to start this with a little bit of a very kind of high level history of Snark systems. I think these names may be familiar for anyone who's been following the space. And what I'd like to do is kind of go through them.
05:41:32.450 - 05:42:20.300, Speaker A: And any of you can actually weigh in here, basically let us know what was the big change or what part of the Stack was identified and optimized as we went through each one of these things. I'm going to start and Benedict, you can tell me if I'm wrong here, but I'd start with like gross 16. Sure. This is like starting in 2016. We can also start in like 2012 or I was going to say like 1980 something, but yes, let's start with practical proof systems. Okay, so we're starting with Graph 16 still. I mean, it's still a system that's used a lot, right? These libraries were really well developed, very much, and it's still the system with the shortest proofs that we know of.
05:42:20.300 - 05:42:57.770, Speaker A: So that's sort of why it still has a place in people's heart, because if you're trying to post a proof on chain, that will be the cheapest way to do it. Yeah. And just a note here, there's a ton of proving systems that came out in 2019. What I'm trying to identify are the proving systems that we just got like a lot of mind share around. They kind of won that round in a way and then you kind of see those be used for a few years and then a bunch of new ones come out. One of those kind of becomes the standard in a way. This is like subjective, but the next one I have is Plonk actually.
05:42:57.770 - 05:43:44.948, Speaker A: So Zach, maybe you can help. What was the big innovation or what was the part of the ZK stack that it yeah, the key innovation of Plonk was what it enabled was practical universal ZK. Starks so one of the big downsides of gossip is you need to do a trustless setup ceremony for every single circuit you make. And so we wanted to preserve the nice succinct polyglothmic properties of elliptical based snarks, but one where you didn't need per second trust a setup. So it was integration of Sonic, which is suppose iteration on bulletproofs and a lot of other things. But the innovation was really it was a way of efficiently validating copy constraints. So that was kind of the big bottleneck with universal Snarks.
05:43:44.948 - 05:44:51.630, Speaker A: It was, how do you verify that all your gates are wired up correctly? So we had an efficient way of doing that. And the effect of that also meant that a nice way of Arithmetizing SNOX circuits kind of fell out of that, which was then turned into kind of the Plonkish Arithmetization. And this is where people were optimizing on what you had done. But the thing they were focused on was this Arithmetization part, like they were changing. Basically the idea is with Plonk. One of the nice things about Plonk is that the algebraic expressions that you're checking on every single gate nicely map to the overall proverb and verify algorithms that are being run, which means that you can then construct relatively complex custom algebraic statements specifically for a system that you're building your SNOC for. And so, yeah, that kind of took off in a big way.
05:44:51.630 - 05:45:56.176, Speaker A: There's many different ways to look at the innovations of systems. But I think one of the realizations sort of that came out in that time and where Plonk sonig and Marlon and Dark and others were sort of a big part of is this modularity right, like the separation. So the separation in this particular part into kind of I have a computation and then there's something called a polynomial IOP, which is basically reducing it to some polynomial checks. So I just checked that some polynomial is at some point is evaluated to, I don't know, zero. And this basically allows you to sort of pull apart these monolithic proof systems into two components. One of them is this Arithmetization component, something like Plong, something like Marlin or there's others stark like Air, I guess, would be the other one. And then the polynomial commitment constraint.
05:45:56.176 - 05:46:43.492, Speaker A: And now you can plug in different pieces for these different things. So you can plug in for Plunk, you could plug in KZG, but you could also plug in some hash based thing like Fry. And the nice thing is. And sort of there were also form of theorems about this that basically if you take a secure thing from one side and I take a secure thing on one side, I get like basically the plugging together works and this gives you a new proof system. So, for example, now we have Halo Two, which is Plonk plus like a bulletproof style polynomial commitment. Or we have I think Plonky Two, which is Plonk plus Fry. And I'm sure we have like Air plus KZG and there's basically a bunch of these things.
05:46:43.492 - 05:47:33.820, Speaker A: And I think this separation is what I would view as the sort of realization of that time that then enabled innovations like plank. Yeah. So you mentioned sort of the next stage that I would say had a lot of mind share, although I do feel like it's just part of the ecosystem. But it was like the halo work and then the Halo Two work especially got a lot of mind share, a lot of people building on it. You had just said that it was using something like Plonk had its two defined parts and then you could take out one, put in another. Did Halo Two also introduce any other places to all of a sudden break off into another module? Okay, to be honest, I don't really know what Halo Two is anymore because it's like Halo Two to me is one of the no offenses. It's very confusing because it's a library.
05:47:33.820 - 05:47:55.252, Speaker A: There's a paper halo. Then there's, like, Halo Two. There's also what people think it does and what it really does. It's an Arithmetization language, so it's many, many different things. So it's like also names just like terribly confusing. We're really good at that. I think what I actually view halo and Halo Two quite separately.
05:47:55.252 - 05:48:45.640, Speaker A: And what Halo Two does, it's really a variation on the Planck Arithmetization, like a really nice way to sort of encode the Plonk Arithmetization in this column format with coming with a library, a very good, seemingly good and widely used library, like cryptographic library with it. Do you agree? Yeah, broadly. I think maybe I would add one other thing, which is that I think one of the things that Halo Two really pioneered was basically the use of cycles of non parenting friendly curves to enable recursive precomposition. That's just not implemented in Halo Two right now. This is like the really confusing thing. Everybody thinks this is implemented in the library. It is not right now.
05:48:45.640 - 05:49:54.000, Speaker A: So I think the confusion comes from the paper that came out from the interesting Zcash people. So I think it's Dara Shanbo and I forget there's a third author on the paper and that paper sort of describes this yeah, thanks. That paper describes this idea of how can we get cheap recursion using cycles of curves? So maybe that is what people refer to as halo. Yeah, there's the halo paper, which is about recursion, which is another interesting area, but it's not implemented in Halo Two. Maybe right now, maybe in the future it will be. But this is why I'm like this is why this grinds mega is because it's just like so confusing these things, because cycles of curves, which is what people think Halo is, which is what Halo is, is not implemented in Halo Two. One of thing which complicates matters, which I'm certain that I've culped in this as well, is that all these proving systems are also kind of attached to brands.
05:49:54.000 - 05:50:33.304, Speaker A: Well, they become brands for the companies that the people inside these companies are political handed inventing them. And so I think that also has a big effect on the confusion aspect as well. Yeah, I was just going to ask because I was curious in sort of sounds like you've looked at the Halo Two library and there are 15 different forks. Every time I go to a crypto conference, I learn about two new Halo Two forks and at least one of them implements something I want and I didn't know about it. It's a big counterparty discovery problem. But I'm curious. In the modular stack, we see clearly separate roles, like data availability, solving, execution.
05:50:33.304 - 05:51:11.192, Speaker A: It sounds like they're kind of two now, like the first Arithmetization part and the polynomial equipment part. Do you think they'll be two forever? Like, is this the final decomposition? I'd wager already at three, where you have your Arithmetization, then docking Pots is something that your planomer IOP can deal with, and then you can use whatever planomer commitment scheme you want. So it's already, I think, like a three layered thing. And then you add folding for a fourth layer and lookup tables for optimization. So those would fall. Is that more of a technique? I have all of this. This is actually my next question.
05:51:11.192 - 05:52:08.490, Speaker A: Maybe let's finish our history. But I'm getting into well, I guess it is the next step of the history after sort of plumbing you're right. Because obviously the next one that a lot of people have been talking about for the last year is the Nova work, which also leads to the hypernova and protostar work, which introduces this technique of folding or accumulation schemes, which I've always understood as being deeply built in. But it sounds like does it change one of those three that you just described? Is it in polynomial IOPS. Is it in the arithmetization? Is it in the polynomial commitments? Is it a sub? So similarly to how graph 16 was like this monolithic thing and we started to take it apart. Yeah, I think this technique started from a very specific application. So in the Halo paper, they were like, we are using this bullet proof style binomial commitment scheme and we can actually defer some of this to later.
05:52:08.490 - 05:52:38.260, Speaker A: And then slowly but surely, we're picking threads out of this and got this to be very, very generic with proto star super. Recently, I will also say, I know Benedict. You had done work that sort of did this before Nova, but described slightly differently, am I correct? Yeah, we just didn't give it a fancy name. So it didn't get any know. But you did do Bulletproof, so you knew, you knew that's less. No, you got to give it fancy names. No being facetious.
05:52:38.260 - 05:53:36.712, Speaker A: So the most important separation for me is sometimes it's called front end and back end, where front end is what the developer interfaces. This is how you code up your computation, how you express your computation, right? This could be and we have this separation in normal computing as well. We don't need to think about sort of snarks, we can think about your programming language, right? That is the front end, right? This is what you write and C, C, whatever. And for us, this is something like the Halo two front end. Sometimes it's r one CS. Sometimes it's like a higher level thing, right? Like there could be also multiple levels in the front end, like Circum or whatever, that then gets compiled down to something below. And on the back end there is the proving system.
05:53:36.712 - 05:54:13.356, Speaker A: And the proving system can also have multiple layers, right? It can have like, as you were saying, the compilation down to a polynomial commitment. And a polynomial commitment. I would say that these folding schemes give you a proving system with very particular properties. They especially work for these or they're designed for these Iterative computations. So a computation where you have like a step that you do over and over again. For example, a blockchain. It's just like one block and then another block and another block and another block.
05:54:13.356 - 05:54:32.324, Speaker A: That's a computation that you do over and over again. It's a so called Iterative computation. And for those we can use kind of these folding schemes or these IBC things. But this is all in the back end. This is the sort of the technical infrastructure. The assembly? No, it's really not even the assembly. It's the CPU, right, that does the execution.
05:54:32.324 - 05:55:25.556, Speaker A: And again, like CPU, we can have multiple layers in there, we can have the instruction set, we can have all of these things and then some things fit together, some things don't fit together and some things you can build a compiler to make them fit together. This is why this picture looks sort of complicated. But really the thing that the image that I think helps sort of people think about it is just like front end. How do you express your computation? Hopefully in the future this will get easier and easier and you can literally just write it in, I don't know, rust or python and then back end. Hopefully those things get more efficient and powerful as we go along, which sort of executes the computation. And both of these within each other, within itself can have modularity, which is the cool thing. And the more modularity.
05:55:25.556 - 05:56:10.810, Speaker A: I think historically what we've seen is exactly this trend that you just said. Usually sort of the innovation comes like someone looks at something monolithically, comes up with some genius new idea, something like halo, which is, by the way, a beautiful genius idea. And halo is more in the Nova folding kind of line of work. That's where I see it. And then people start picking it apart and making it more modular. And this enables new innovation and this enables new sort of techniques. So we should sort of always cherish, I think, the name here of the conference and the panel is very good because the modularity seems to be extremely helpful for both new innovation and for understanding these things.
05:56:10.810 - 05:56:58.052, Speaker A: Yeah, I was just going to ask me one thing. Modularity helps me understand in the distributed systems context is what the hard axes of trade offs are, right? Like there's a trade off between privacy inefficiency in a sort of counterparty discovery matching sense. There's a trade off between trust and efficiency in a kind of how much do you need to replicate your verification sense. And these trade offs don't change. Like you can make the primitives faster, but the trade offs will always be there. They're just properties of how the components fit together and what kinds of properties you want out of them. Holistically and I'm curious, does the modular decomposition of zero knowledge proof systems, is it yet at a point to provide clarity on these kinds of axes? And what are they? This is like benchmarking a lot of this stuff, I guess, I think not necessarily.
05:56:58.052 - 05:58:19.788, Speaker A: I'll say sometimes in our case modularity comes at some cost where because we're not looking at things monolithically, we can't open these black boxes anymore and there are some small tricks and optimizations or things that we could have done otherwise that we don't do anymore. Not to say that we don't do them at all. We do have systems that go in and break the black boxes, but I don't think the modularity itself okay, I guess, yeah, it can draw some lines of the trade offs, but it also draws them slightly, somewhat artificially. But I guess maybe your question is in distributed systems we have these pretty strong lower bounds, right? Like we have some sort of impossibility results that are pretty strong. What is interesting is that in zero knowledge proofs there's some lower bounds but not really that many and that meaningful ones. It's not even clear that proving is necessarily more expensive than computing something, right? Like if I want to compute a square root, then I need to compute the square root. But if I want to show to you that the square root is correct, I can do this with just a squaring, right? I can go in the inverse direction, which may be cheaper.
05:58:19.788 - 05:59:05.744, Speaker A: So it's not even clear. Or maybe executing a computation is sequential, but the proving can be done in parallel. As far as I'm aware of, there's some lower bounds, but these lower bounds are usually in stylized models, and if I go out of these models, I can oftentimes even circumvent these lower bounds. So one of the beauties is we don't have strong lower bounds, but this also means that modularity does it doesn't show oh, here's clear trade offs. Yeah. Just to further that. Yeah, it's tricky because it will be nice to be able to clearly define a trade off space.
05:59:05.744 - 05:59:51.260, Speaker A: But I think if you take a snapshot of the ZK landscape at any one time, then, yeah, you can probably define some kind of trade off space between all the various proving systems and all made components. But because the low bounds have nowhere near been reached yet. Yeah, you flash forward six months and everything you've done your analysis on is now obsolete, and it's been replaced by new stuff. That's better. Justine, I still don't think we've properly placed folding schemes and lookup tables into the where are they in this stack? I've been sort of referring to them as techniques because I'm just like something you use on top of, but that it's not its own system in its own right. I don't know if I'm I can take a step. I think where I would place lookup tables is they're part of the circuit arithmeticization.
05:59:51.260 - 06:01:10.148, Speaker A: They're a subset. Then it's a way of converting a lookup table into algebraic statements just like the rest of the addition, like arithmetic gates of your circuit. So I think you can place it at that layer, more or less. When it comes to folding schemes, I would say it's at a higher level than the underlying proving system, because there's been some work that the proto Star and proto Galaxy are basically the actual proving system is left as a black box, more or less. The only requirement is that you have an additive homomorphism between the protestar doesn't even define the use of polynomials, but assuming you're using polynomials, then you have some kind of additively homomorphic commitment scheme for your polynomials. So I would place it at a layer above the underlying proving system, and it's a layer at which if you have some higher level architecture that composes multiple proofs together, then you apply a polling scheme on top of your proving system to get that capability. So could it have its own category? Would you? Yeah.
06:01:10.148 - 06:01:51.572, Speaker A: Okay. And I guess the reason, like those two that I highlighted, the reason I wanted so we've mentioned a few of them. Each one of these sort of has a line of work and there's researchers who are focused just on those things, something we didn't actually say. And now I'm realizing I'm not sure I know this at what stage, like lookups, which I've just brought up, there was pluckup there had been work before, as I learned on a panel yesterday, there had been some work done before. Mary Mallor was part of some work that was doing that. But then there was the Pluckup was created. But was that like what system does it go with? Plonk? Does Plonk then come with lookups or like a second generation yes.
06:01:51.572 - 06:02:20.876, Speaker A: Is that the introduction? That's kind of what ultra Plunk was. It certainly wasn't the first lookup table scheme. I think it was the first. Well, at least at the time I thought. I don't think it was. I think it was the first that had the access cost of your lookup tables was constant. Most of the ones before required you like the number of constraints in your system that you needed to do a lookup was logarithmic in the size of your table.
06:02:20.876 - 06:03:34.564, Speaker A: And Flukup is this one gate. I think there were systems before that did that, but Flukeup was the kind of the if I make the claim that it was the first constant time practical lookup scheme, is that controversial? I'm not sure. But yeah, I think also that this is another great case where sort of modularity was sort of observed later on, where a lot of these lookup schemes were invented, and they were invented sort of in the Plonkish land. I'll say that. But most recently, and I think, for example, this was observed in a paper about CCS and other things, is that you can pull this out. You can pull it out and reuse this component in other systems, and it is not tightly coupled with the Plunk proving system. That's sort of a very nice observation where, again, we've observed some modularity and this helped sort of improve the space.
06:03:34.564 - 06:04:08.290, Speaker A: I want to just kind of go over what we've already said, just again, just in case people aren't fully following. So, so far we have polynomial IOP, polynomial commitment scheme, arithmetization subset lookup tables, and somewhere else in somewhere is folding schemes beforehand. Does recursion and folding schemes get to live together? Yeah. Okay. Same thing. Ready? Okay. So that sort of maps for roughly what I had written down here.
06:04:08.290 - 06:04:58.832, Speaker A: But it's nice to see it mapped, though. But is there anything else? What else is there? So going back to your point, as we define these components, researchers can focus in and optimize them and find new combinations, find interesting properties of other parts that we just described that interact really nicely with. Yeah, I don't know if I described that so well, but we're looking for new things. We're on a search now. I think you can add other layers of abstraction, but I think that's where you would, like, you'd stop writing sound as proofs and papers. There are plenty of high level constructs, but maybe you could well, yeah, you're saying we're at the end of all modularization. This is no, we're done maybe at the end today.
06:04:58.832 - 06:05:47.616, Speaker A: But I think Benedict has some words to say on that. I don't know. I mean, you can go even lower level right? So one of the things that this is like another slight rent it's not a rent when people start learning about sometimes have people approach me, oh, I want to learn about cryptography. And it's like, oh, but I've been looking at these elliptic curves and they look so complicated, and I got stuck. And I probably after I wrote, I don't really understand elliptic curves. And for at least the first three years of my PhD and I'd written bulletproofs by that I really did not understand elliptic curves. And that's totally okay, because, again, we can use abstraction, right? These elliptic curves are just one tool.
06:05:47.616 - 06:06:36.240, Speaker A: Like, they're mathematically, they're an algebraic group with a group operation where the discrete logarithm is hard or something like this, right? We can beautifully use abstraction. And you do not need to understand how these other components work. And there's sort of this whole cryptographic layer where, for example, a lot of these proof systems use a hash function, and we have like, there's a new hash function coming out every week. And especially there's been a lot of focus on these Snark friendly hash functions. So that's another component. Or there's different elliptic curves with different security and efficiency properties that's sort of another component at the cryptography layer. That's a really good point.
06:06:36.240 - 06:07:30.300, Speaker A: Maybe just to add to that, maybe to try and systemize that a bit more. All of these Snark systems, all of their security proofs boil down to a relatively common set of computational hardness assumptions. And by that I mean, basically you're saying you basically prove that the only way for a versatry to break a proving system is they can solve a particular problem that we assume is basically impossible to solve. And so, for example, one of the common ones is the elliptic curve discrete logarithm problem, basically saying you can't find the discrete logarithm elliptic curve. There's a whole very, very low level abstraction here of the cryptographic primitives themselves, the actual constructions that you pull those hardness assumptions out of. And yeah, there's a whole other level field of work. And this, again, beauty of modularization.
06:07:30.300 - 06:08:03.284, Speaker A: You don't need to understand it. It's totally fine. Just sort of deal like, be happy with abstraction when you're trying to look at things. Really be happy and sort of embrace the abstraction, embrace the modularity. I will say maybe that's one of the points of friction that I want to see sort of solved now. It's this friction between the back end and the front end, because all this ZK stack we've been talking about, so that's mostly in the back end, these things change very quickly, and we get better and better very quickly. And they also like you're essentially changing the interface, right, that you have with the front end.
06:08:03.284 - 06:08:52.696, Speaker A: So there are new things that are available for the front end, and it's really hard to know. Like, okay, can I start thinking about a good front end for. This yet. Can I start thinking about a good language or a good representation of computation to throw to these backends or is the ground going to move under my feet? Yeah, this is actually to you, Chris? A little, yeah. I mean, looking at things a little bit more from that perspective as we do, it seems like the really hard problem is dealing with differently sized finite fields. That's, like the essence of the problem that we see from the higher level language perspective is that the choice of the field to me is like an implementation detail that should live in the back end and the front. End program should be portable across this choice so that as there are different interactions between different systems and the underlying systems change and certain things become cheaper with small fields or whatever, or you need certain cycles for recursion.
06:08:52.696 - 06:09:24.612, Speaker A: These are like details in the back end to me that we want to abstract. But it's difficult because at the same time, in order to get efficient execution, you kind of need to know about this detail when you're writing your programs. So there's this bleed through of something that is really at the very bottom. Like this is the thing that you're abstracting over right there's. This bleed through all the way to the front end language. And I have not seen a convincing general approach to translating between just modular arithmetic over different finite fields. All the mathematicians I meet tell me it's really hard.
06:09:24.612 - 06:10:05.180, Speaker A: It's like discrete algebraic geometry, like hard stuff, but I don't know. I'm curious. Yeah, I think you hit the nail on the head when you said that there's an efficiency problem there where every abstraction layer has an implicit cost associated with it. And right now, over the last ten years, I'd say we've been building up a lot of abstraction layers from what used to be just a core monolithic proving system like the Gloss 16 or Bcgtv 14. I can't remember their names, but yeah, the one that's really missing is creating something which is field agnostic. And I don't think it's probably not going to turn up for a while because it's a hard problem. The one way to solve it is you basically you create a virtual machine.
06:10:05.180 - 06:11:09.636, Speaker A: So instead of writing turning your program into constraints for a specific priming system, you turn them into operations over some imagined virtual machine that you then prove in your underlying priming system. And then that assumingly one would then not work in finite fields, one would work in just regular base two fields over the integers. Those are finite fields. Yeah, but we are not well depend. It's far too use case specific because the slowdowns in that are gargantuan. I feel like often I think that the history of ZK Snarks and ZK Proofs kind of tracks similar to the history of computing in that if you think about where we were in 1990 is similar to where we were in 1936 when Alan Turing was writing papers saying in theory, we could do this wonderful stuff, but God knows how. And then the very early ZK snarks, they were basically the very first digital computers using vacuum tubes, where forget about custom programs, your programs.
06:11:09.636 - 06:11:53.380, Speaker A: You had to rewire your computer in a plugboard to get different programs because it was that low level. And then as the performance improves, you start to layer on more abstraction levels, so you start to get primitive programming languages in the computer space. And that could be translated to creating some of these very basic abstraction layers that we have today. But we're still early. If you can draw analogues between the path of ZK proof and the path of cryptography, we're in, like, the 1960s at best. Yeah, but in the 1960s, we went to the Moon, so maybe we can do that again when Moon if you draw that analogy out, I think the conclusions are pessimistic, not optimistic. We haven't been back to the Moon.
06:11:53.380 - 06:12:20.544, Speaker A: Well, it was damn hard and expensive. Right? True. Are we saying that we're doing something damn hard and expensive that won't be repeated in the future, but we did it on the computational power of, I don't know, probably my watch that I'm not even wearing. But yeah. All right. Well, I think we've covered most of the questions. I mean, I had one last one, but this kind of in the weeds, you'd sort of mentioned Hash functions, and I was thinking, like, Hash functions, pairing based.
06:12:20.544 - 06:12:58.264, Speaker A: Where do we place this? Is this under the polynomial commitment scheme? So, like, you go into the polynomial commitment fry based will be using Hash function, KZG is using pairing based. Yeah, it's a technique. Can't believe that. Is that no, exactly. I think it's a technique that is used. It's very tightly coupled, usually, to the polynomial commitment scheme. Like, each polynomial commitment scheme requires is built on a different cryptographic primitives, which then defines a lot of the efficiency that's the problem that you were talking about, that it bleeds up.
06:12:58.264 - 06:13:40.650, Speaker A: Right. Because it does define a lot of the efficiency properties. But it even defines to some way, unfortunately, it defines to some way, like, how you can express your computation, but it also defines your security assumptions and your trusted setup and all of these things. And then the other thing that I was saying, the modularization, there is just that there's many different elliptic curves, and KZG can be implemented on many different curves. And which one you choose again, different trade offs all the way up. Yeah. Then depends, like, oh, what does ethereum have pre compiled for? Right.
06:13:40.650 - 06:14:17.936, Speaker A: And also, the landscape of cryptographic primitives is much more slower moving than the high level construction. It's because these computational hardness assumptions, you can only really get consensus around them over time because it's not like you can formally prove that a particular computation cannot be done in polynomial time. That's kind of the P equals MP problem. So, for example, pairings were around since the 1980s. Elliptica pairings. They weren't used in commercial cryptography software until the mid two thousand s, I think. I believe just because people just didn't trust them.
06:14:17.936 - 06:14:41.768, Speaker A: Didn't trust them. It takes about 20 years, but I'm sure we'll see new premises. We're going to start to see, I suspect, polynomial equipment schemes based around Lattices turn it cropping up in ZK, say, pairings. 1980s, I think so. Pairing? What? You said they were invented in 1980. Oh, no, they're like from, like, 19. The concept of using a Valencia pairing in a cryptographic protocol was 1980s.
06:14:41.768 - 06:15:13.028, Speaker A: Wait, you were about to say something else. What did you just say? What was I going to say? That we're going to see some new primitives eventually. Although we get the schemes based around Lattices are cool. I'm kind of curious about higher level elliptic curves is like one of the most primitive algebraic structures that you can use. There's a whole field of things like taurus based cryptography, or you can add dimensions on, and you can get possibly some interesting properties. Anyway, we'll see what happens. Yeah, maybe that's sort of the last.
06:15:13.028 - 06:15:57.488, Speaker A: So the area of potential improvements or direction, you're thinking like, Lattice, I guess that would open up all sorts of cool new techniques. There any other specifics trilingar maps? What? Trilinear maps. Trilinear maps? Yeah. Well, I lived a curved pairings or a bilinear map, which basically allows you to do quasim multiplications, but if you had a trilinear map, then that would open up. Okay, this has been a long time since I've looked into this, and I'm a bit of a diletente in this field anyways, so take this as just as, like, some guy on the Internet. But I think it would open up the ability to create Fhe type constructions with elliptic. Well, with whatever you have the trilinium map with, the same with the elliptic curves are additively homomorphic.
06:15:57.488 - 06:17:04.840, Speaker A: I think you can think you can get something which is multiplicatively homomorphic with the trilinium map, maybe completely or just partially. So I was under the impression that bilinear maps allow you, like, one multiplication, and if you go trilingar, you get one extra one. So you get two multiplications that you would need probably more if you want, like, fully homomorphic. So the thing that trilinear maps, like, one of the key this is outside the ZK space, but really exciting is the thing that trilinear maps, and more generally, these multilinear maps give you, is Obfuscation program obfuscation, which is sort of the uber crypto primitive. Because what it means is that basically I can have a smart contract which can have an embedded secret key, and nobody can read the secret key. So, for example, the smart contract could literally store some balance in it, and nobody could see sort of the balance, and it can update it locally or it could have I think we had some talks on private state and public state. I guess you talked about this.
06:17:04.840 - 06:17:45.700, Speaker A: And with Obfuscation, these things would become significantly more powerful. And yeah, there's really cool things that could happen there. That's sort of the part of the crypto future out there. Yeah, exactly. In the private transaction world, somebody has to own encrypted state and control it, and they're there the entity that needs to be able to construct the proofs of computation over that private state, which is kind of a pain. If you need to, for example, liquidate that person, they're not going to make you a liquidation proof, but yeah, anyway, I could waffle on for ages. I think we're getting the sign to leave the stage, which is a bummer because I didn't leave any time for questions.
06:17:45.700 - 06:18:07.550, Speaker A: I think I had too many. But you've met these wonderful panelists, and I guess if you have questions, please come join us after. Thank you so much. Thank you to all of you for thank you very much, panel. Thank you very much. And now we're going to have 20 minutes breaks break. So basically we're going to resume at 430.
06:18:07.550 - 06:40:32.580, Speaker A: Hello. Hello. Welcome back. After the break, I hope you had fun. Now I would like to introduce to the stage Omaroy, who's going to talk about Aggregation is all you need. Hello, I'm uma, one of the co founders of Sisync, and I'm going to be talking about Aggregation is All You Need. How do I do next? Okay, so I'm going to start with some background on Succinct.
06:40:32.580 - 06:41:32.670, Speaker A: So in particular, if you look at the ZK landscape today, there's been a lot of focus on two types of applications. One is ZKE EVMs, and there's a lot of teams we know that are working on Zkevms month. Okay. Before press. Okay, sorry about that. Okay, back on the correct slide. So there's Zkevms and then there's also privacy preserving protocols like tornado cache.
06:41:32.670 - 06:42:48.490, Speaker A: And at Succinct, we're pretty interested in exploring the rich application design space beyond just these two types of protocols that are really well explored today. So we thought a lot about how else can ZK help scale and make blockchains better? And we started by working on ZK like clients, which is basically verifying consensus protocols inside a ZK circuit to allow for efficient verification of consensus protocols in EVM. So in particular, to get into a little more technical detail on how this works, you'd have a source chain that has some consensus mechanism. You verify the consensus in a Zksnarc, and then you verify that proof in an execution layer very cheaply. And then you can run a Succinct on chain like client so that one chain, the target chain, can talk to the source chain natively. And this solves a lot of existing problems with interoperability protocols today where generally, if you want to transmit information or data between one chain and the other, you have to rely on a trusted multisig or some trusted group of entities. And with Succinct on chain ZK lite clients, you can basically do interoperability without these trust assumptions, and you can get much more secure interoperability.
06:42:48.490 - 06:43:35.430, Speaker A: And the first ZK lite client we built was a ZK lite client for ethereum. So our first protocol, Telepathy, which has been live on Mainnet since March, uses our ethereum ZK lite client. And with that, you can send arbitrary messages from ethereum to any other chain. And you can also read ethereum state on all these destination chains because you have the ethereum state route on these chains. And then it's also useful for Bridging information from ethereum's consensus layer to its execution layer. And so we have a few people using this. One great example is Gnosischain is using our telepathy protocol to secure their native bridge, and then Eigen Layer is using us to operate their restaking protocol by getting ethereum consensus information in the execution layer.
06:43:35.430 - 06:44:24.214, Speaker A: So we've built this ethereum ZK like client, but we want to expand ZK interoperability by supporting more consensus protocols. So there's only really a few consensus protocols that matter. Tendermint is one that's commonly used across a bunch of ecosystems because it's the native consensus protocol of the Cosmos SDK. Another consensus protocol that's used by the substrate SDK, which is used by the Polkadot ecosystem, is Grandpa and Babe Consensus. And so there's a few consensus protocols that we at Succinct care about proving in a ZK circuit to kind of expand the domains that can talk to each other through these ZK like clients. And these ZK like clients are in general useful for two different things. One is l one to l one.
06:44:24.214 - 06:45:04.610, Speaker A: Bridging. So, for example, having an ethereum chain talk to a Cosmos chain that's using Tendermint, and then also another subcategory of this that will be the main focus of my talk is data availability layer. Bridging. So if you have a DA layer, you can bridge the state of the DA layer to ethereum. And so I'm really excited to announce that we're working with Celestia to build a ZK bridge to bring Celestia state to ethereum. And this is kind of like the big scope of my talk. And one question you might ask is, okay, why are we interested in bringing Celestia state to ethereum? And so here's a really helpful diagram to show why that's useful.
06:45:04.610 - 06:45:53.270, Speaker A: Basically, in the modular stack, a chain can decide to have their data available on Celestia, but then they might decide to settle on ethereum. So this diagram shows how an L two operator might send proofs, whether they're ZK proofs or they might send like, you don't send an optimistic fraud proof, but you have that settled on ethereum. So you send it to your l two ethereum contract, and then you send your transaction data or your DA to Celestia. And Celestia has this super scalable really high throughput DA. And so for the roll up, it's much cheaper for them to operate their roll up in this way. And then you would use our ZK Lite client to basically attest on Ethereum that the data is actually available on Celestia. And so roll ups kind of get the best of all worlds in this way and it's much cheaper for them to operate.
06:45:53.270 - 06:46:42.530, Speaker A: So there is an existing protocol to bring Celestia state to Ethereum. It's called the Qgb, which stands for the quantum gravity bridge. And we are basically turning that into the Zkqgb. And the Zkqgb has a lot of benefits. So right now the Quantum Gravity Bridge is this kind of sidecar on the Celestia protocol. And one of their core values is really having this minimal protocol that's minimally, simple and only does one thing, which is data availability. And so by having the Zkgb, we can take out the existing Qgb and really simplify the core Celestia protocol and then just take the existing Celestia validator signatures, verify them in a ZK snark, and then move that out of this core protocol.
06:46:42.530 - 06:47:28.686, Speaker A: And this is really nice because also Celestia can scale their validator count without having to worry about an on chain like client gas costs. And we in general can reduce their gas costs a lot for the existing Qgb by bundling all these Dkqgb verifications together. So now I'm going to dive into some of the technical challenges that we faced while making the Zkgb for Celestia and in particular ZK Tendermint. And one interesting thing to note is, as I mentioned, a lot of chains use tendermint. So this is actually reusable work throughout the cosmos ecosystem. So in general, when you verify a consensus protocol in ZK, you have to do a few different things. You have to verify signatures, you have to verify hash functions, and you have to do some decoding.
06:47:28.686 - 06:48:19.230, Speaker A: And in general, the pseudocode looks generally always very similar. You verify signatures, you make sure at least two thirds of the validators have signed, then you have to prove that the existing set of validators is like the correct validators. And then you have to merkel prove any important information against the header, such as a message was sent or some amount of money was deposited in a contract or burned to be minted on the other side. And then in particular, tendermint has some particular technical challenges associated with their consensus algorithm. So they use this signature scheme Ed 25519, and you have to verify these N ed 2519 validator signatures. Unfortunately, the signature scheme has no aggregation in it, like the Ethereum BLS signature scheme. So that is a technical difficulty.
06:48:19.230 - 06:49:10.398, Speaker A: And then tendermint also has no epoch time. So in the worst case, you might have to verify many headers in a row. And then finally, when Tenermint was designed, they didn't really design it to be maximally Snark friendly. So they have these Snark unfriendly serializations used throughout, such as protobuf that have various challenges with being implemented in a circuit. So I'm going to now cover some of the more specifics of what we implemented at Succinct to basically have our ZK circuits be able to handle tendermint and verify tendermint consensus in a Snark. So here's an outline of the techniques we used and then combined together. So one interesting thing to note is that verifying validator signatures and hashes is actually embarrassingly repetitive and a very parallel task.
06:49:10.398 - 06:49:58.370, Speaker A: If I verify one signature, it has nothing to do with the validity of another signature. So you can really trivially parallelize the verification. And if you just do this naively in a normal circuit and you just verify 100 signatures serially, you're not taking advantage of the innate structure of the problem. And so we think we can do a lot better. And so we call this idea ZK SIMD. SIMD stands for Single Instruction Multiple Data, which is like a concept that's prevalent in a lot of other contexts, like GPUs and ABX instructions. But basically it provides this form of data level parallelism which lets you compute the same function F on a bunch of different inputs x one through XN in parallel.
06:49:58.370 - 06:50:38.666, Speaker A: And so we realized that Starks are actually really convenient for these sorts of parallelizable computations. So typically people use Starks for VMs. Like a lot of the Zkvms or even Zkevms are written in Stark based languages. And so Starks have this single state transition function that repeats across all the rows of the circuit. And as a result, they're much more lightweight to prove. And often they have much faster proving times than other Arithmetizations like Plonc. And so we figured out a way to arithmetize constraints within a Stark to implement an abstraction very similar to Simdi.
06:50:38.666 - 06:51:47.506, Speaker A: So as I mentioned before, we have this kind of abstraction that lets us, in a very general way, specify an F, a function F that we want to compute independently over a set of inputs and compute a bunch of outputs. And in the particular case of signature verification, f is just the function of verifying a signature and X is just the actual signature that we want to verify is valid. And this is like a very simplified diagram of what's going on. But basically in our Stark that's verifying in parallel a lot of these signatures. We have two to the 16 rows and then we have 256 signatures that are getting verified throughout the course of the circuit. So they're not getting verified serially. What happens is that we have a is there something so anyways, there's something that happens where basically we are able to verify all the signatures in parallel, and then we have an accumulator column that accumulates and verifies the results of all the computations together.
06:51:47.506 - 06:52:33.018, Speaker A: And at the end is a random linear combination check to make sure all the verifications actually went through. Okay? So at a very high level, we talked a little bit about how the subtraction works. And we built this nice framework to let us do these computations in parallel, and then we compared it to some existing implementations. And so, in short, these benchmarks were taken on an M Two Mac. But end to end, our Stark framework proof generation for verifying 256 Ed, 25519 signatures took 80 seconds. And so the proving time per signature is around 320 milliseconds. In contrast, if you were just to verify one signature in the Planky Two proving framework using Planckish Arithmetization, that would take around 17 seconds.
06:52:33.018 - 06:53:39.246, Speaker A: And then if you verified it in Ganark, which is a Grot 16 based proving framework, it would take around 14 seconds. And so you can see our abstraction of basically paralyzing verifying all these signatures as a batch results in a much faster per signature verification time, which is what we would need for verifying something like tendermint, which you have to verify a lot of validator signatures because they're not aggregatable. So I've talked a little bit about how to do this parallel computation of the signature verification. Another interesting thing is that you can further use recursion to further reduce the untend latency of these parallelizable computations. So in particular, at the root of this tree, each leaf sorry, the leaves of the tree at each leaf, we verify a batch of signatures using our ZK SIMD abstraction, and then we actually verify each of these batches in parallel if we want to verify something like 1000 signatures. So each leaf is a Stark. And then we recursively combine the verification of the Starks together, and then we're able to do this in a tree like structure.
06:53:39.246 - 06:54:40.330, Speaker A: So the end to end latency of our whole computation is simply the depth of the tree, which is log two of the number of signatures that we actually want to verify. And this means that even though we're able to throw more compute at something, the end to end latency of verifying a lot of signatures is greatly reduced. So one problem is that when we're doing all this stuff with ZK SIMD, with the Stark based framework or the Recursion, we use this proving system called Plonky Two. And in general, recursion friendly proof systems are typically not compatible with the EVM. In the EVM, if you want to verify a proof really cheaply, it's best to do it in graph 16 or something pairing based, because Ethereum has pairing pre compiles. So our solution is that we have to wrap a recursion friendly proof system with an EVM compatible Snark. And so in particular, what we do is we take Ganark, which is Grot 16 or Ponkish KZG based, and then we take that and we wrap the Ponky Two proof, and then we verify it in EVM.
06:54:40.330 - 06:55:32.906, Speaker A: So our proof system composition, where we're combining three different proof systems, a Stark based one, a Ponkish Fry based one, and then a Ponkish KZG based one, unlocks the best of all worlds. You have really fast proving for batches of signatures, then you have really fast recursion for reducing end to end latency. And then finally you wrap it all in the final layer that gives you really cheap EVM verification. And you kind of need all three of these components to prove a consensus algorithm like Tendermint that has all these extra challenges associated with it. And in practice, I kind of touched upon this. We have this particular proof pipeline where we're doing this proof composition and aggregation. So the first step of our proof pipeline is we have an application specific circuit that can recursively verify batches of things.
06:55:32.906 - 06:56:29.382, Speaker A: And so in this application specific circuit, we kind of have the business logic of the consensus protocol where we're verifying validator signatures, maybe we're verifying headers and hashes and other things we need to do. Then we have a recursive circuit that verifies the proofs from step one and it normalizes everything and makes the proof size and the custom gates a constant. And then finally we have this grot 16 recursive circuit that verifies the proofs from step two. And basically what that does is the cheap EVM verification. So we have this like three step proof pipeline that composes to get us all the properties that we want, which is it's really fast to generate a proof and then also really cheap to verify and just to throw out some benchmarks about the proof recursion. So the recursive circuit for ponky two, which is step two in the three step process I described, is actually really fast. They heavily optimized their framework for recursion.
06:56:29.382 - 06:57:11.526, Speaker A: And so in net it takes less than 2 seconds to do the witness generation and the proof time. And then for the final wrapper circuit for cheap EVM, it takes around like 16 seconds to generate the proof. So it's actually still very feasible and manageable to generate this proof. And then as you can see, the onchain verification cost is around 400K gas and probably could be optimized further. So it's very feasible to run something like this on the EVM today. And then finally proof aggregation we think is super important. So so far we've done proof composition of a bunch of different proving systems, but eventually you can imagine that we would have a bunch of different consensus protocols that we're verifying in ZK circuits.
06:57:11.526 - 06:58:11.470, Speaker A: So we'd have this grandpa proof of consensus, we'd have tenermint proof of consensus, we'd have our Ethereum like client, and maybe we have a bunch of other proofs as well. And one nice thing you can do is you can take all these different consensus proofs that are coming into Ethereum, which is a very constrained computational environment, and you can aggregate all of them, which can save even further on the cost of verifying all these proofs. And so when you aggregate all these proofs, you dramatically reduce the cost of verifying them on chain. And you can also verify proofs that aren't like clients as well. And in the end, we think this will be like a huge unlock for making gas costs much cheaper. And then also you can have the state of all these different chains on Ethereum more frequently, because the gas cost of verifying an individual one will be much less. So, yeah, this is a meme about kind of all the ways we stacked these different techniques to finally get to something where it's actually fast enough and feasible to verify in EVM.
06:58:11.470 - 06:59:04.800, Speaker A: So our ZK SIMD, which is a starkey based framework, will be open sourced soon. It's written pretty generally so that basically, if you have a function that you want to prove over a set of inputs in parallel, you can use that and write a circuit. And we'll open source it soon, and we want people to contribute and collaborate on it and use it for whatever parallel functions they want to prove. And then our Ganark based Ponky Two Verifier is actually already released under an MIT license. It's open source, it's available at that link. And we would love for other people who are using Plonky Two, we know it's like a proof system that has a bunch of users, because it's very fast to use it to verify their proofs on EVM and also collaborate and contribute. So if you're interested in using any of these things, you can check out the GitHub repo and then also talk with me after and ask if you have any questions about using it.
06:59:04.800 - 07:00:25.620, Speaker A: Cool, thank you very much. Uma, if you have any questions to, uma, find her in the lobby. And I would like to introduce to the stage Yisan. So the title of my talk today is scaling data reach applications on ethereum with Axiom. And the starting point of this is the realization that if you're writing smart contracts today on Ethereum, or really on any blockchain VM, you're really operating in a very data starved environment. If you look at the listing for this cute penguin on OpenSea and you try to identify of all the pieces of on chain data on the page, what can actually be used on chain? You'll find that the answer is only the owner, namely Zac Efron. All the other rich information on the page, like the transaction history, the historical prices, and all that good stuff that OpenSea users get to see is simply not available to your smart contract.
07:00:25.620 - 07:01:26.360, Speaker A: And this is not just an implementation flaw of Ethereum. Any blockchain that wants to be decentralized can't impose the requirement that validating nodes can access history. Otherwise that would require all full nodes to become archive nodes. Now, of course, developers are very creative and they work around this in many ways. Today they have this trade off between putting more data in state and paying for that, or by reducing their security somewhat and relying on trusted Oracles, which in many cases is a fancy way. Of saying that the team itself puts the relevant data on chain in a fully trusted way and so developers who want to scale data access on chain today really have to trade off between increasing their cost or reducing the security of their application. So at Axiom we're thinking about whether we can scale data rich on chain applications.
07:01:26.360 - 07:03:00.240, Speaker A: Now on blockchains we have a special tool namely in any blockchain the current block always commits to the full history of the chain and that means we can use cryptography instead of consensus to access on chain history. So how does this work on Ethereum? The current block is linked to all pass blocks by a Ketchek chain of block headers and of course every pass block commits to all the information in that block, namely the state of Ethereum at that block as well as all transactions and receipts. The problem though is that if you try to decommit all the way back to a million blocks ago in Ethereum that's going to be prohibitively expensive. You can never do that in the EVM. So what we realized at Axiom is that we can shove all of these verifications into ZK we can check a merkel patricia triproof as well as a chain of blockheader hashes and make that feasible to verify on chain. This has a couple side advantages of providing scale in accessing the historic data and composition. So what we've packaged that into is something we're calling a ZK coprocessor for Ethereum, smart contracts can query Axiom on chain to do a combination of historic data reads and verified compute.
07:03:00.240 - 07:04:13.650, Speaker A: Over that data we generate the results off chain and also provide a zero knowledge proof that everything we computed was valid. Once we verify that proof on chain you can use that result in your smart contract however you like and because of the zero knowledge proof, every result that we return from Axiom has security that's cryptographically equivalent to something you're accessing on chain in Ethereum itself. So let me talk through what the two components of Axiom give you. The first component, reading arbitrary historic on chain data means that you can scale your application while interoperating with existing applications that are already on the chain. Unlike something like a rollup, you don't have to move your state or really do anything to access more data. On the compute side we envision supporting computations that really cannot fit in any blockchain VM either today or in the future. You might imagine running some sort of local neural network inference that's never going to happen on a timeshared global computer.
07:04:13.650 - 07:05:19.380, Speaker A: And so I've told you what our ZK coprocessor is and I now want to talk about what it can enable. So I've drawn a rough graph on the x axis is the amount of data you're accessing and on the y axis is the amount of compute you're using to process that data. Obviously they're correlated if you have a lot of data, you're going to use more compute. So in the beginning we think that ZK Coprocessing will make the devex for certain operations that are already possible, but at great cost in the EVM. Much simpler this will be things like computing a trustless volatility Oracle verifying a user's account age or simply computing a consensus level randomness. But where I think it really gets exciting is when the data and the compute both get ratcheted up. You might imagine accessing fully trustlessly the historic balance of any ERC 20 token at any historic block.
07:05:19.380 - 07:06:40.000, Speaker A: Or if you're running and designing an onchain protocol, you could define objective slashing conditions over the entire history of your protocol participants that cause them to be punished or rewarded. And once Ckml gets sufficiently fine grained, you can imagine adjusting your parameters of a DFI protocol based on machine learning algorithms applied to the historic data on chain. In this way, we think you can bridge the gap between traditional web, two applications that take in vast streams of data and process it to the current trustless onchain applications that are data starved. So let me walk through what the state of ZK Coprocessing is today. So we just went live on mainnet with trustless access to any historic blockheader account or contract storage variable two weeks ago. And this week at ECC, we've launched transactions and receipts to testnet. So in this way we allow smart contracts to access any piece of execution layer data on chain today, to decompute over that data, we offer the ability to write custom ZK circuits to come to a result for a developer.
07:06:40.000 - 07:07:20.664, Speaker A: All of that can be verified on chain fully trustlessly. Now, what does that mean for your actual application? I'm going to walk through a few examples in a very concrete way. Suppose you want to access a user's account age. What you can do is trustlessly read the historic nonce of their account at two different blocks. Then you compute the first block that has a nonzero nonce and deposit the age of their account on chain. We have this running live in a demo on our website today. Suppose you want to enhance your governance.
07:07:20.664 - 07:08:12.190, Speaker A: In traditional corporate governance, there's a very complex governance structure. It's not just one stock, one vote. Most governance today is simply one token, one vote. And I think the reason for that is it's very hard for governance to actually know any other information about the participants. To have a more complex voting weight scheme with Axiom, all you need to do is trustlessly read the history of your users voting by looking at the on chain events. Then you can compute derived quantities like the number of times someone has voted, their voter participation, or even things involving when they vote and how reliably they vote. You can then compute the custom voting weight using that and really tailor your governance to incentivize whatever you'd like.
07:08:12.190 - 07:08:53.820, Speaker A: In DeFi. You might imagine adjusting fees for historic participation in standard exchanges like Binance and Nasdaq. Obviously, if you're a higher volume trader, you get a fee rebate in DeFi. Today everyone gets the same fee. And we think that kind of violates the fundamental law of economics. The only reason it hasn't happened yet on Chain is that AMMS actually can't know how much their users have traded. To implement that with Axiom, all you need to do is trustlessly read the trade events of your users on Chain, add them up, and then apply the appropriate discount to fees.
07:08:53.820 - 07:09:56.320, Speaker A: So all of the applications I just talked through are possible today on Mainnet with Axiom, but I want to talk through where we're going. So we started by giving smart contracts access to the execution layer data, and we think ultimately they want access to all data. Once Cancun lands in September, we'll be able to access consensus relatable data on Ethereum. And perhaps through bridges like Succinct, we can access data from other blockchains and roll ups. After that, we think developers want to process the data they're getting in the most native format possible for Ethereum. That's going to be simulating the result of view functions through Zkevm proofs. Once we're able to do these first two steps, we'll essentially have a ZK version of an archive node or indexer which caches these values and returns them to smart contracts with lower latencies.
07:09:56.320 - 07:10:56.300, Speaker A: Finally, we think developers will want to use different forms of computation that exceed the balance of blockchain VMs, and we think the best way to provide that will be through a ZK native VM. So we've started on the first piece of this roadmap and shipped it to Mainnet two weeks ago, and we're excited to continue over the next months and years. If you want to try out Axiom today, you can check out our developer docs at docs axiom XYZ. In my remaining time, I want to talk a bit more meta level about the usage of ZK in blockchain applications. I think a lot of developers today are very excited about ZK as a concept and technology. To be frank, they don't really know too much about it. And this week I've been talking to developers about using ZK and they view it either as a black box, which either they like or are very afraid about.
07:10:56.300 - 07:11:39.090, Speaker A: And so at Axiom we started something we're calling the Open Source Program to educate developers about how they can develop ZK circuits and what ZK can do for their applications. So in the first round of our program, we had a number of community written open source circuits for things like fixed point arithmetic, ed 25519 signature verification and BLS signature verification. So we're opening up the second round of applications next week. You can go to the URL above to apply, and we'd love to see what you can build using ZK. Thanks so much, guys. Thank you very much. He.
07:11:39.090 - 07:12:17.320, Speaker A: And actually, this is the last part of our truck. So I would like to introduce to this stage Tracy, who's going to be our moderator. E, uma, and Ismail. Hello everyone. Hello. Hello. Okay, so we have a pretty unique set of teams here today.
07:12:17.320 - 07:13:15.080, Speaker A: For the most part over the last few years, as people have thought about ZK Snarks, they've thought about how they relate to roll ups and validity roll ups. But I would say in the last twelve to 18 months, there's a handful of teams that have started exploring what it looks like to use Snarks for creative applications beyond roll ups. And I think this panel represents a lot of those teams. Maybe we could talk a bit about some of the unique opportunities that introduces, why we think it's interesting and compelling, but also some of the challenges that come with it. So one of the first things I think we can start with is just like a motivating question around ZK and the trust assumptions. Sort of inherent in your interest in ZK is the belief that trust is very important in blockchains. For years we've had solutions like off chain data processing that sort of either happen optimistically or just with an assumption of trust.
07:13:15.080 - 07:14:23.454, Speaker A: What about trust assumptions is important to you? And why do you think it's so important that we do it cryptographically instead of relying on some of these weaker assumptions? What I like about ZK is it gives you an opportunity to establish a point of trust and then inherit computation from that point. And I think the the notion of trustlessness is oftentimes misleading. So, you know, when we think of a ZK roll up, what you're functionally doing is you're inheriting trust in a state transition from the consensus level collateral. And when we think about coprocessing or we think about any type of access to on chain data from a contract that can't be accessed to the VM's native execution, what you're trying to do is get as close as possible to your computation running on the same trust assumptions as the underlying base layer. So it's not trustless per se as much as it is the inheritance of trust from a very specific designated point. Yeah, it's a different boundary box a little bit than where we were before, but I think if you look at the base layer of no trust, this moves it up a level. It is.
07:14:23.454 - 07:15:08.720, Speaker A: Any other thoughts on that? Why is it important for you guys? Yeah, we've been talking to a lot of application teams and really trying to educate them on what ZK can do for them. And as I'm sure everyone in the audience knows, although we talk about these very secure ZK based systems, in reality, a lot of things on chain today rely on development teams being honest. Even a lot of the systems in production will eventually become fully secure, optimistic rollups. But today have permission sequencers. And I think where in the next year or so. I see ZK really being valuable is in cases where social consensus will not accept a trusted oracle. And those are typically cases where composition is very important.
07:15:08.720 - 07:16:07.310, Speaker A: So if you're a protocol team, maybe your users will accept your trusted oracle. But where it really gets difficult is when another protocol wants to compose on top of that protocol. Will that protocol's users trust this random other protocol's team? I think the chain of trust gets very tenuous. I think ZK really helps in establishing more clear trust abstractions and boundaries. Yeah. To add to that, I think there are some existing situations where we've already seen trust be super problematic, abused, and then eventually leads to things like bridge hacks and results in very tangible amounts of money getting taken from users and protocols. I think we're all aware of all the bridge hacks that have happened very recently and I think in those situations it makes it really clear why the existing trust assumptions of multisigs or whatever other mechanisms they currently use aren't acceptable because it has already led to material loss.
07:16:07.310 - 07:17:14.058, Speaker A: I think that's a great one. Composability is often overlooked, but if you have like a key trust assumption in the middle of a chain of trust, then you can't really build another highly valuable protocol on top of something that has weak trust assumptions. So it's a good one. Yeah. Sort of implicit in that conversation was an attempt to do it a bit differently than we've done in the past to avoid some of the bridge hacks that have happened, help improve composability and protocols. How are your teams thinking about doing it a bit differently? Maybe starting decentralized or enabling that very early in your product's lifecycle? Yeah, so what we focus on at LaGrange is supporting data parallel computation on top of large chunks of on chain data to be provable efficiently. So, broadly speaking, when we think about the space of on chain data, we're functionally constrained by the ability to access from the execution layer the majority of the state that has been created in the canonical history of a chain.
07:17:14.058 - 07:18:07.558, Speaker A: And this is even more prevalent when we think about the modular context, when you have a series of different execution spaces with a series of different state structures, transaction trees, receipt trees and block histories. And so what we focus on is to allow you to treat this, what is more or less a morass of unstructured data, as if it was your data lake in Web two to run SQL MapReduce RDD, massively parallel processing computational models on top of this, to be able to derive and extract properties that are relevant to your application's function. Yeah. Maybe pulling it in a bit to focus on decentralization. Yeah. And so, I mean, I think functionally decentralization in the context of this question has to do with a few relevant vectors. So firstly, you have to think about the decentralization of the prover which confers a liveness assumption on the overall protocols that inherit computation from it.
07:18:07.558 - 07:18:54.034, Speaker A: And moreover, you have to also think about the assumption of where you're deriving data from in the single chain context. It's very straightforward. When you get MultiChain, some of the work that the sync does, it gets a little bit more opaque. You have to derive data ideally from the underlying consensus of a source chain or from a close approximately like a light client, as you can get to that. Yeah, I would say one really important piece for this is for users to be able to audit what every team is doing and what's actually been deployed. I think that the four of us on stage here can talk all we want about how our teams have a security mindset and all the things we're doing, but ultimately users actually have to be able to verify and put our systems to the test. So I think there are a lot of things that go into that.
07:18:54.034 - 07:20:01.126, Speaker A: One is just standing the test of time in a production environment. The second is having a lot of transparency around code base being open source, having a reproducible build for any verify or you deploy on chain. And third is adopting cutting edge security techniques like formal verification or fuzzing to give a higher degree of guarantee to users. So I think all of these systems are going to be difficult to trust at a super high level until they've been running Live for years. And whatever we can do to allow users to audit that more quickly is going to move the space as a whole forward. Yeah, to talk more about security, I think vitalik advocates even in the context of Zkevms for this two factor approach where maybe you have one computation that's done in ZK and then maybe you have a trusted Tee SGX based two factor or maybe you have many different implementations of the same function. I think one really interesting thing about ZK is that at the end of the day all the functions we are computing are just F of x equals y.
07:20:01.126 - 07:20:51.410, Speaker A: And so it's very clearly specked. And so you can actually have multiple redundant implementations of the same computation very easily because it's just inputs and outputs and the circuit has to be the same. And I think that will be really powerful in the security story of actually getting ZK adopted and for people to feel comfortable using it in their DAP in a very critical context. Yeah, those are good points. How are you thinking about, I mean, multi prover kind of implies a separate software stack. Do any of us think maybe we would push in the direction of a full separate software stack for another prover? I mean, the T's may be a shorter path where you don't necessarily need a whole new stack. I think you can already see that there's a few primitives that are implemented across a few different proving systems.
07:20:51.410 - 07:22:02.378, Speaker A: At the end of the day, I think all of us on stage are doing pretty similar stuff with hash functions, signature schemes and other cryptographic primitives that are fundamental and it's really not too hard to take the same primitive and reimplement it in a new stack. And we already see multiple implementations today, so I think it's quite feasible to actually have multiple implementations and something we can push towards. So I'm not advocating for a different prover for the same proving system, but more multiple redundant implementations and even different proving systems. I think it also requires being thoughtful over the scope of what you are proving. I mean, I think if you're proving something like a light client where you have a very fixed set of parameters over what a correct execution of that is, it's more straightforward than if you're building a general purpose VM. When you're incurring a significant amount of technical debt, when you're anchoring to a specific proving system, potentially, if there's a change in the state of the art and you can't have your back end be agnostic your front end be agnostic to the back end. And so there's complexities there, I think, that are inherently incurred as you develop applications that have more and more zero knowledge intrinsic to the core purpose.
07:22:02.378 - 07:22:42.790, Speaker A: And I think in those situations, having multiple back ends becomes an imperative if you are trying to ensure that you can stay up to date and your performance can stay relevant. Yeah, that makes sense. What are you doing in the meantime? We're not quite there to a multiproover world. I think most all of you are working on getting into production pretty soon. Are you putting in gates or checks in your contracts or every proof needs a signature along with it for now. How are you thinking about that in the short term? Make sure you don't have a catastrophic bug in prod. Yeah, we deployed the main net two weeks ago and we put in gating on the prover.
07:22:42.790 - 07:23:34.794, Speaker A: So if there is a circuit bug in soundness, we certainly will not trigger that bug. We've also put in time lock upgrades on our Verifiers so that we can actually fix any issues that come up. We do feel like this should be a temporary phase until we're able to introduce stronger security techniques like formal verification. Yeah, I think we can follow a lot of the in production ZK rollups today. They all have similar setups where they have approved provers, you have time locked upgrade and governance over the Verifiers. I think all of those things are very reasonable because again, if they are in the critical path and there is a hack that can potentially be catastrophic, I think we think about following their lead and their design choices and we think that's like a very reasonable short term trade off before we get multiprover and trusted execution environments. We agree.
07:23:34.794 - 07:24:42.560, Speaker A: I mean, I think there's good precedence over teams who have pushed large production code bases with complicated underlying circuits and have done so in a way that has to date been more or less secure. I actually had a conversation with an auditing team that will remain unnamed that had done some ZKE EVM audits, and they were nervous. They said to themselves, like, I don't know if we understand this well enough to really put it in production. We've done our best here. Are there things beyond audits that you're trying to do internally at your companies, maybe to have a culture of security or help ensure at the code development time that when you go to production you don't have problems? Yeah, I think having a culture of security is very important, and I think you need to be very clear on your code reviews and your best practices as you're implementing and developing your underlying infrastructure. I'd extend that broadly and say I think just from a business operations standpoint right now, you should be resourcing and hiring people who have an understanding of the primitives that they're working with, especially when you're building these highly complicated systems. It's important that the work that's being done is done by people who have an awareness and a context for how the things they're building works.
07:24:42.560 - 07:25:49.362, Speaker A: Yeah, we think that, of course, standard security practices are very valuable, but actually, I think there's some very obvious things which are very helpful. One would be having less code, just having a more minimal and well designed system so you don't have as large a security surface area. The second would be looking at the interfaces between ZK and blockchain systems, where the two systems are really of quite different natures. And we think these boundaries are places where issues are more likely to arise. So of course, circuits might have bugs, but I think it's much more likely you just completely misparse part of your circuit and do something quite obviously wrong. Yeah, you can even see this in the bridge context where a lot of bridge hacks have been due to trust assumptions getting violated, but then other bridge hacks are simply due to smart contract bugs, I think goes to support Yee's point, maybe zooming out a little bit and thinking about modularity, given where we are. Uma, I guess you just announced that you're working on a Celestia bridge.
07:25:49.362 - 07:26:59.850, Speaker A: Are any of the other teams thinking about sort of modular DA layers in their environments, or are you mostly focused on specific chains? We focus very heavily on modular with how we're developing our infrastructure. I think being able to permissionlessly, support data access in a modular context is very important, I think especially as we see a proliferation of new execution spaces, whether those be roll up as a service providers or L three S on top of existing scalability solutions. I think it's very important to ensure that in terms of state access that you're not constrained by the ecosystems that you're interrupting with or interacting with principally. Yeah, we're currently focused on EVM, but I actually want to point out another aspect of the word modular that I think ZK is very useful for. If you are able to use ZK introspection onto the state of chain as part of your application, you can sometimes dramatically simplify the onchain architecture of your smart contracts. Basically, you don't need to be recording a lot of extraneous information at a state that you could later read using ZK. And so we think that this can contribute to a trend in smart contracts actually getting more modular.
07:26:59.850 - 07:28:12.610, Speaker A: Yeah, I agree with that. I think state access on chain has led to development practices that if you had principally better data access or in principally better compute on top of that data could be alleviated. And I think if we look at, like, optimistic roll ups and we look at the Bisection game there, there's things there that can be simplified drastically, I would argue, reducing some of the implicit security assumptions to it. Cool. Let me think if there's anything on that topic, I guess yeah, one of the challenges of having proof or a light client built off of a given chain, I mean, in l one, we can kind of assume there's a lot of economic stake behind this route of trust. When we go into a modular chain ecosystem where you have a route of trust that maybe has lower economic state or longer times to finalize, I think that introduces a challenge in the level of trust you can put in that. How are you alleviating that problem? I think LaGrange, I know you've published some thoughts on state commitments, and I think this challenge will also impact axiom as you guys try to look at other chain ecosystems.
07:28:12.610 - 07:29:09.666, Speaker A: Yeah, I think the core challenge is essentially, although let's say an optimistic roll up might have a longer finality period, let's say seven days. Users really demand some sort of weaker guarantee that can hold much faster. And so we think it's actually more appropriate to leave that sort of guarantee to the application. If you're trying to withdraw $100 million from optimism, maybe you should wait seven days before someone else accepts it. If you're trying to play a game on optimism, who cares? Just accept it. And so we think it's important that for the end user, the guarantee that you're precisely offering is extremely clear. I think part of the complexity with having a clear guarantee for the end user application is that it opens the design space up for less transparent infrastructure providers to have opacity over the underlying design decisions of their protocols.
07:29:09.666 - 07:30:15.738, Speaker A: And this is, I think, the concern with a lot of cross chain protocols today that originate messages from optimistic execution environments. And so one of the things that our team works on is using existing ethereum valid asset collateral with eigen layer to be able to assert an early degree of economic attestation behind the validity of a purported state transition for an optimistic roll up. And the reason that we think this is very valuable is you can have bridges, permissionlessly, consuming state from a shared layer with a clear amount of economic trust and economic security behind the state that they're using. And it means that if you want to understand how much security is behind a given attestation estate, you can very quickly look at the size of the committee and the stake within that committee and you can derive that assertion from there. And you don't have to worry about whether or not a K of N assumption for an arbitrary bridge that could be used on some intermediary protocol has a sufficiently decentralized underlying validator set. We think of this as like very much a public good. Great.
07:30:15.738 - 07:31:11.834, Speaker A: Maybe we could spend the last few minutes here zooming out and thinking about sort of ZK beyond the blockchain. I think Snarks in general just receive no attention or minimal attention outside of the crypto domain. And I think crypto served as a good incubator for this technology to kind of grow up and get mature. But over time, there's probably an intersection with Zika, Snarks and the broader Internet at large. Is there anything kind of exciting and interesting that starts to happen as we see more verifiable computation used throughout the Internet? Or are we all just crypto? Maxis so I've done some academic work on Zkml, putting some of the largest known machine learning models into ZK. And as consequence, I've had to explain to some pretty well known machine learning professors, like, what is ZK? Their reaction is always the same. Number one, is that's impossible? Like, you got something wrong.
07:31:11.834 - 07:32:08.830, Speaker A: Number two, and this takes varying periods of time for different faculty members is okay, maybe it's possible, but it's useless for us. And number three, some of them are like, oh, maybe in this edge, edge, edge case it might be useful. And so I think it's a lot of education and finding the places where having verifiable computation actually makes sense in a non hostile environment. Typically their response, hey, like, why would I use a Snark? I could just run it. Or I trust Amazon more than your weird crypto system. And so I think it's very sobering in showing us that as a space we need to be delivering real world value that's exogenous to this relatively insular crypto world. That said, I think one trend that pushes a lot of people I know who've been bearish on crypto, bearish on ZK forever to be very interested is the rise of AI.
07:32:08.830 - 07:32:46.166, Speaker A: People are very worried about spoofing, about deepfakes and they really want a notion of provenance to exist. Just the other day I called my bank to verify my identity for a wire. I'm pretty sure that's just not going to be a thing in a couple of years. And I think people are. Very hungry for a solution. And I do think ZK can play a role in that. I think use cases like Zkml and WorldCoin are an interesting case where they are in crypto, but they're also kind of bridging the real world and it's a very real world application of Zkml.
07:32:46.166 - 07:34:08.680, Speaker A: But of course, also WorldCoin is going to be settled on an op stack roll up and so they are actually using a lot of crypto properties as well in their system. I think use cases like that that straddle the real world and the crypto system are really important and what really excite me, and so looking forward to more of those, I'm sure there's more Zkml use cases for similar or new sorts of products like that. And one thing I'd also add is I think when we think of Verifiable computing crypto, we have a tendency to think principally about the succinctness of the computation. And I think most of us here are not talking about the ability to compute Verifiably on a private set of inputs. And in the Web Two context, there's a lot of examples of where that is and will be highly viable. And I would say if we think about Enterprise transfer of data and the inability for a lot of web Two companies to have effective orchestration of computation across shared data assets to mitigate fraud, to have better user experience and customer experience. The fragmentation of data within major companies today makes it very difficult in financial services in the healthcare sector for there to be applications that can act in the best interest of the underlying user base while still being able to preserve privacy of each of the enterprises that has that data.
07:34:08.680 - 07:34:36.080, Speaker A: Cool. Are we doing questions? I don't know. Okay. I think we're pretty close to time, but I also think we're wrapping up the end of the conference, so maybe we could do a few questions from the audience. There's still one talk after this, but we okay, sorry. So we can do the questions? Of course. Anybody? All right.
07:34:36.080 - 07:36:00.998, Speaker A: Anna, maybe you want to say something. We don't bite. What's storage proof? Yeah, just a general question about, like, how much more efficiency gains do you think there's left for ZK proving systems from here? Is it a ten x? 100 x is like a two x? Because people are still kind of skeptical about how practical ZK is from a computational perspective. Yeah, I mean, I think that's a good question, but I think if we were to assess the trends in computation over the last 2030 years, there are a number of buoying factors that will result in ZK becoming increasingly performant irrespective of the underlying proving systems that we're talking about. I'd say that you can discuss improvements in the proving system as well as improvements in underlying computing infrastructure that both likely will have positive effects over time. Yeah, I think to add to that, currently, I think all of us perhaps run our circuits on fairly commodity hardware. And there's a lot of ZK hardware companies that are out there trying to make hardware level improvements to these proving systems and make it faster.
07:36:00.998 - 07:36:32.418, Speaker A: So that's one frontier to push. And then, of course, there's also new proving systems all the time. I think Nova came out this year, and perhaps there will be more in that line of work that make an algorithmic improvement. So I'm very optimistic that the algorithmic improvement plus the hardware improvement is going to result in huge gains to ZK. Yeah. And if we want to think about the theoretical limit, justin Taylor put out a blog post, I believe, last year discussing this. Essentially, there are two sources of overhead.
07:36:32.418 - 07:37:08.378, Speaker A: One is in converting a normal computer program into a ZK circuit. There he thinks that the limit is maybe a factor of 100. And the second is in the actual proof system, where maybe you lose another factor of ten in the limit. So that would add up to a 1000 x overhead over a normal computer. Right now, we are nowhere close to that. So I think we can easily get 100 to 1000 x improvement without any hardware. Yeah, and one last thing I'll add on that is that doesn't account for the hardware side of things, which typically is between ten and sometimes up to 100 times improvement depending on the specialization of the hardware.
07:37:08.378 - 07:38:20.840, Speaker A: So fast proofs are the future. All right, do we have time for one more? Yes, we do. Okay, thank you. So, as someone who's been on a team, who's seen developers probably not elegantly put data on Chain and they're just pushing a lot there, I'm very excited for this idea of what you guys calling coprocessors. Any predictions on how long it'll take to make the switch from these very inelegant on chain protocols to these, in my mind, much more elegant protocols? A year? Two years, three years? How long do you guys think that switch will take? I think the principal constraint there is twofold. I think there's developer adoption and there is whether or not developers feel comfortable altering the paradigm and the function of their application to now include new primitives, irrespective of how secure those primitives may be. I would say secondly, there's the question over whether or not there are clear instances on chain now where there are applications that could leverage better data access and will leverage better data access in changes that they will make to their underlying infrastructure in a shorter period of time.
07:38:20.840 - 07:38:51.990, Speaker A: I think another factor that comes into play is really social consensus. I think right now it's frankly still pretty difficult to have trustless versions of a lot of the things that developers just putting up the number on chain for. I think once it becomes easy enough, users will start demanding it. And it's something that we'll see slowly and then suddenly. Okay, thank you, everyone. For your time and enjoy the rest of the conference. Thank you.
07:38:51.990 - 07:39:54.498, Speaker A: All right, so, yeah, just before you all leave, I want to say a big thank you for coming to our ZK track as part of modular summit. We are a little bit ahead of schedule, but I've heard that everyone's pretty set to go ahead. Are we going to be doing anything about the chairs? I think because we're going to have just two speakers, so maybe we just set that up a little bit. Yes. But yeah. How have you enjoyed the day so far? Do you want to make some noise? Are you excited? We're rounding it out. Day one, modular summit.
07:39:54.498 - 07:40:13.430, Speaker A: Very cool. All right, so I think maybe we'll just pull this. That's going to be really loud. Okay, sorry. Also have a crappy knee right now, so I'm the worst person to do that. Very nice. All right.
07:40:13.430 - 07:40:37.870, Speaker A: That looks more like a fireside. Right? Good. Maybe we can even grab some of those mics. All right, I want to introduce to the stage Mike and Mustafa, who are going to be doing a fireside to wrap up day one of the modular summit. Let's give them a round of applause. Welcome. All right, everyone having a good modular summit.
07:40:37.870 - 07:41:07.866, Speaker A: Let's give a hand to Celestia for organizing this maven Eleven. This is amazing. And this is the last panel of the day, so everyone gets a trophy and a cookie for staying and watching us. Thank you. Mustafa, I've got a lot of questions for you about Celestia, both the history and how we've gone from lazy ledger to here and sort of what you expect from the future. But I was told actually that you at one point got arrested for hacking the CIA, and I've just got to get you to tell that story on stage. So give us the inside scoop.
07:41:07.866 - 07:41:54.426, Speaker A: What's the history here? Yeah, so we're starting straight with that. Okay. Yeah, I mean, I kind of got into programming at an early age, and the first programming language I learned was PHP. And I started thinking about ways that programmers could make mistakes in their code. And then I started learning hacking naturally through that way. My first experience of a hack was when I was doing my math homework and I didn't have a calculator with me. How old were you when you were doing this? Well, the calculator thing, I was like eleven, and I don't have a calculator, but I need to do my math homework.
07:41:54.426 - 07:42:41.260, Speaker A: So I searched online for an online calculator and I found this shitty pearl calculator on this maths professor's website. I think it was like University of Maryland. And it was like a text box. It was like a text box where you can type in sums and it will give you the result. So I was, hmm, I wonder if he implemented this in the most basic way that a programmer would probably implement this which is he was using Perl. So the way he would do it is you would take the user input and you would input it into a function called evaluate and that basically evaluates Perl computer code. So not only you can type in like sums into this calculator, you could actually type in computer code into this calculator and the server would execute it.
07:42:41.260 - 07:43:35.046, Speaker A: So you could actually hack the entire server through this exploitable calculator. And so I managed to get access to this university server by typing computer commands in this calculator. And then I emailed the professor and he tried to fix it and then I found a way around it and that was kind of like a very interesting experience for me. But then I kind of got more involved in hacking when I started becoming more involved in internet activism. I was involved with the groups like Anonymous and Laltec. Anonymous was doing denial of service attacks against various entities that they were protesting against. So for and when PayPal and Visa and Mastercard blocked donations to WikiLeaks, they did a denial of service attack against PayPal and Visa and Mastercard to take them offline.
07:43:35.046 - 07:44:48.094, Speaker A: And there was like thousands of people in the chat rooms kind of coordinating this. But for me I thought that was kind of interesting, but it didn't really do much except for gain some media attention. So I decided, okay, why don't actually try to hack something and get information that could shed light on wrongdoing? So then I found some technical people. I noticed a few people that were more technical than others and I brought them into a private channel and kind of started a hacking group through there, eventually spun out into Lulsec. So just to sum this up, by the way, the difference Mustaf between you and me when we're eleven, if I didn't have a calculator, I just didn't do my homework and try to hack a new one out of the web and just to bring us up to the current time. And I think you're about 16 years old at this point, right? You're sort of casting around and looking for a righteous group that you can hack and you settled on the CIA, which is that's a bold choice. Tell me, how does the CIA take to hackers? Is that particularly welcomed? Yeah, I mean, so the funny thing is, CIA was one of the many things that we kind of attacked.
07:44:48.094 - 07:45:30.260, Speaker A: But the funny thing is the CIA wasn't even technically a hack. It was a denial of service attack. So it's like denial of service attack, you're not actually getting confidential information. We took the CIA website offline and that's like a very basic thing. We did more advanced things than that. But that was the thing that got the most attention, or one of the things that got the most press attention because it was very embarrassing for the CIA to have their website taken offline. Did they thank you for this and say thanks for showing a vulnerability in their system or were they a little less forgiving? They thanked me in some very strange ways.
07:45:30.260 - 07:46:28.526, Speaker A: All right, we'll dig at that a little later. But I think there's a great story to tell because there are a lot of very unique people in the crypto space. And I think the people who have made an enormous impact actually come at this not from a short term money making perspective, but from an ideological, how can we change the world perspective. So I think that's just important background for folks in the audience to understand. Now I want to actually move us forward a little bit in time and talk about lazy ledger, which was originally the name of Celestia and sort of the white paper and idea that launched this entire wave of modularity that's frankly blossomed into something super amazing that we're witnessing this week in Paris. So tell us a little bit about how lazy ledger came to be and what did the early iterations of that modular idea look like? Yeah, sure. So I've been interested in peer to peer systems for even before bitcoin existed.
07:46:28.526 - 07:47:23.922, Speaker A: Like I was interested in BitTorrent and peer to peer file sharing because I used to download stuff from pirate bay and it was very interesting to me that people could just download stuff kind of like permissionlessly. And the reason why I was so interested in pay to pay systems is because so many people try to shut down their private bay, but to this day it's still online. So it's like extremely censorship resistant. And that's thanks to technology like BitTorrent and DHDS. So then I learned about bitcoin in 2010 2011, and I was following that and I was kind of like following the research conversations going on. There was like this IRC channel called bitcoin wizards where people were discussing theoretical improvements to bitcoin. And I noticed that there was like a 1 MB block size limit in bitcoin.
07:47:23.922 - 07:48:19.650, Speaker A: And I was like asking people, what are you going to do when it gets reached? And people weren't worried about it at a time it would never get reached. But then it got hit pretty soon in around 2013 2012, and transaction fees became very expensive. And then the bitcoin community started debating about how to fix that problem. And it kind of like split into two camps. One camp wanted to increase the block size and that camp span out into bitcoin cash and another camp wanted to use layer two technologies and payment channels and lightning network. And that was the bitcoin camp that kind of prevailed, the bitcoin main network. But the reason why they didn't want to increase the block size was because the fundamental principle of blockchains and cryptocurrency is that end users should be able to fully verify and validate the chain.
07:48:19.650 - 07:48:54.498, Speaker A: And if you increase the block size, it will make it more expensive for users to run full nodes. So then I started thinking about like, well, about this problem more. And then I started doing a PhD at UCL in 2016, focusing on layer one scaling. And at the time people were talking about sharding. That was like the most interesting getting solution at the time. And I co authored a paper called Chainspace, which was like the first sharding protocol for smart contracts to be proposed. This was around the time when Ethereum 2.0
07:48:54.498 - 07:49:35.374, Speaker A: was researching sharding. But the problem with all of these proposals were that they weren't dealing with the case where a shard goes bad. It was pretty much like a block size increase. Like, the security model for those proposals were just like increasing the block size. But there was no way to actually validate what the shards were doing. To fix that, you need to have fraud proofs and ZK proofs, and that's what roll ups are doing. But at the time, the reason people there was a missing problem to making foolproof sensitive decay proof works, which was the data availability problem, which was an unsolved problem at the time.
07:49:35.374 - 07:50:38.760, Speaker A: And so I started doing more research into the data availability problem and I co authored this paper with vitalik on how to scale data availability using data availability sampling. And then I realized that this is actually basically the core primitive that makes a blockchain work. A blockchain fundamentally at its core is basically a data availability layer and a consensus layer. So then I proposed Lazy Ledger, which was a kind of a paper that proposed a blockchain or a layer one chain that takes layer one back to its core components. That's why it's called lazy Ledger because it's a lazy blockchain that does not do any computation, only does consensus and data availability. And this was an idea I proposed about three months before optimistic roll ups were proposed. So when optimistic roll ups were proposed, everything kind of clicked together because in my paper I didn't really have a fully fleshed out execution model and optimistic roll ups provided that.
07:50:38.760 - 07:51:33.730, Speaker A: So then it kind of made a lot of sense to actually build this because roll up central roadmaps need a scalable data layer. Yeah, I actually want to get into the weeds of data availability. I think that's a word that many people understand kind of on a surface level. But it's such an important sort of roadblock in terms of realizing the grand vision that many great talks have actually laid out today. So I kind of want to actually go through and maybe just define sort of the basic components of a stack, which to me is the execution, the data availability, the settlement, and then the consensus. And can we focus on that data availability question? Let's say there were a bunch of five year olds in this audience. How would you explain the importance of that? And then why is solving data availability such a critical roadblock not only for scaling base layer infrastructure, but for the cost of apps.
07:51:33.730 - 07:52:46.320, Speaker A: Yeah. So here's how I would explain it. When Bitcoin was created, bitcoin was created to solve what's called the double spend problem. And the double spend problem is, where is this fundamental problem in creating digital cash? Where if Alice has a certain amount of funds, how do you prevent Alice from spending their funds, the same funds twice? And the way that you prevent that is by having a blockchain that orders transactions, because if Alice tries to spend their fund twice, then only the first transaction will go through and the second transaction will be rejected. But in Bitcoin, this rule where only the first transaction can go through, or the rule where you can only spend funds that you actually have, is enshrined into every Bitcoin node. So that if you run a full node and you receive a block that has an invalid transaction, your full node will execute every transaction and reject a block that contains any invalid transactions. So miners can't misbehave in that way.
07:52:46.320 - 07:54:22.906, Speaker A: But the question that kind of led to lazy ledger was, well, and why data availability is so important is well, is a thought experiment, which is, what is the simplest version of Bitcoin you can create? What if you had a version of Bitcoin with no rules about what transactions can go into the chain? Which means imagine a version of Bitcoin where conflicting transactions or transactions that double spend coins are actually allowed to be on the chain. Well, how can that still be secure? How can that still prevent double spending problem? Well, it's pretty easy. All you have to do is make sure that the clients simply ignore the second transaction, right? So technically, you don't need to enshrine computation or transaction validity rules to the chain itself. You can push that away to another layer, or you can push that away to a client side node, where the layer that you're pushing it to simply ignores those invalid transactions. And if you do that, then you're basically using the blockchain not for computation, but only for A, ordering, and b data availability. And the reason why ordering is important is obvious because you need to know which transaction came first to know which is the real transaction that actually got to spend those coins. And the reason why you need data availability is because you need to know the complete set.
07:54:22.906 - 07:55:14.730, Speaker A: You need to know all the transactions that happened to know which one even came first in the first place. Like, if not all of the transactions were published, only some of them, then you don't know if there's a missing transaction in the set that wasn't published that might have come before. That makes sense. So I want to understand from the perspective of I mean, in kind of simple words, it's a lot of applications, they need data. They want to do it in a way that's cheap and I want to start painting a picture for those in the audience. Like if we solve this problem, what market structure changes are going to happen here? So from a cost structure, my understanding for especially roll ups and things like that is that data availability is a massive cost for them, right? And critically, it's a variable cost that it doesn't get cheaper. It's not some fixed cost you can amortize across a whole bunch of users.
07:55:14.730 - 07:56:26.094, Speaker A: It's actually something that is going to scale relative to the amount of transactions that happen. So I want to get a sense of, let's say these apps start using Celestia for data of availability, they massively lower their cost. What is the impact of this from a market structure standpoint? Do we see lots of new apps launching? Are there business models that are enabled with a lower cost of transactions that end up happening, that have been precluded from happening before? What are the first order implications of this? Yeah, I mean the kind of most immediate obvious implication is you have cheaper data availability leads to cheaper transaction fees. And I do think that a lot of usage of Web Three applications have been bottlenecked by the fact that transaction fees are too high. Like if we had cheap transaction fees, I would honestly, genuinely think that we would see a lot more applications being deployed on Web Three. And not just DeFi applications, but also, for example, to give an example, this will be discussed in the gaming track tomorrow. But there's various on chain games that are only practical with a high data three port or even for financial applications.
07:56:26.094 - 07:57:48.058, Speaker A: Imagine you wanted to just use the original purpose of Bitcoin was to use it as a peer to peer cash system. But we don't have a single kind of widely used blockchain that is usable as a peer to peer cash system because of the transaction fees. So if you have Scalable DA, you have cheap transaction fees, then it can actually be used to what the original purpose of Bitcoin was, which is peer to peer cash. Not just a store of value or an investment or something that people hold because they think it will go up or trade on DEXes and so on and so forth. I think the other kind of effect is we will see a lot more applications become practical as a result of cheaper transaction fees. But also by having scalable DA and having on a modular blockchain stack, we'll be able to see a lot more experimentation with different execution environments where certain things that people wanted to do before are now possible. So some examples of that is that various projects have banta, for example, have modified the EVM to add certain ZK or privacy friendly opcodes that aren't possible on a standard EVM.
07:57:48.058 - 07:58:41.920, Speaker A: Or for example, Curio have modified the EVM to create a 0.5 tick game engine to run a real time strategy game on, which wouldn't be possible on a standard EVM. And historically, it wasn't possible to do that without deploying a new layer. One, if you wanted to deploy a new execution environment, which is a lot of overhead to use, like a web. Two, analog, imagine if you had to have a physical server somewhere just to experiment with a new programming language or a new database. Today that's not the case because you can just put up a virtual server in the cloud on AWS or DigitalOcean. So you can think of roll ups on a scalable Dlao as like virtual blockchains that enable people to experiment with different execution environments, that unlock things and enable things that fundamentally weren't possible before.
07:58:41.920 - 07:59:57.858, Speaker A: I have a question for you, Mustafa, which is I sort of found myself wondering, as you were just discussing there, there's kind of this question that's a little fuzzy to me of like just as a hypothetical. With the proliferation of many different execution environments now, it's not just the EVM, many data availability layers, right? It used to be just Ethereum, DA, but now we've got Eigenda and Celestia avail other providers and different choices for settlement and consensus as well. It's very possible in a very near time in the future that you could be using an application that's built on the Solana virtual machine, using Celestia as DA, but settling to Ethereum. And the question is, what chain are you on at that point? And where is the lock in for these different environments? Yeah, that's a good question. The whole point of my generality is to kind of shift away from a world where you have this tribalistic crypto environment where it's like this chain versus that chain. It's like Ethereum versus Solana versus Avalanche, which is a very zero sum mindset. And long term, it's important for chains to have a social mode, but long term for crypto to go to mainstream.
07:59:57.858 - 08:00:39.380, Speaker A: Ultimately, users care about usable products, not necessarily which chain that they're on, as long as the chain has basic unnecessary decentralization and security properties. Yeah, and I think people right now kind of frame things like this app on this chain. You need to swap on Polygon, you need to swap on Avalanche C chain. In the future, people won't be thinking in those kind of terms. People will be thinking, okay, this is the app, and it uses this underneath. When you interact with a website today on the web, you don't necessarily care too much about what Stack is using underneath. In many cases, you don't even know.
08:00:39.380 - 08:01:41.130, Speaker A: Like when you go on Google, you don't know is it using Linux, is it using FreeBSD? You don't necessarily know as long as it provides properties that you need. Yeah, I want to actually do a little thought exercise here and imagine that you and I are sitting down. It's five years from now when we're having this fireside. And I've got some questions for you to dust off the old crystal ball here and I want to ask you about how things have played out during that time. What does the market structure look like and what are some of the big changes that we might not be super obvious today? So we've heard from a lot of great projects today, these past couple of days basically new approaches to scaling infrastructure, new types of specific blockchains. What does the market structure sort of look like for the modular stack? I mean, are we going to live in a world of thousands of blockchains? Like how many different general purpose roll ups do we really need? Are these going to be across two or three trust environments? Is the MultiChain future really going to play out? Sorry, I never just ask one question at a time here. Yeah.
08:01:41.130 - 08:03:05.518, Speaker A: From an engineering perspective when I kind of started this. Lesture what I envisioned happening in a few years and it's basically already kind of materializing today was a world where you can go on the docs and you can click a roll up as a service provider and you can deploy roll up chain in 2 seconds where effectively you have a world where deploying a roll up chain for your application is easier and more convenient than deploying a smart contract. And so there is a potential world where there's millions of interconnected chains that share security similar to how today there's millions of web applications running. We've seen a very similar evolution in Web 210. 15 years ago, if you wanted to create a new website or web application, you would use like an existing hosted service provider. You might use Squarespace or Blogspot or WordPress, but that's very limiting. So now today if you're application developer and you wanted to deploy a new web application, it's easier and more convenient to simply deploy a virtual machine on the cloud, on AWS or DigitalOcean than using a shared hosting provider.
08:03:05.518 - 08:04:13.530, Speaker A: And I think you'll see a very similar evolution in web Three where shared web hosting providers are like analogous to shared smart contract platforms. And in the future it might not seem obvious now, but in the future, the obvious ways to develop new applications will not be to deploy a smart contract on a shared smart contract environment that everyone shares, but to deploy a new all up chain similar to how on Web Two, you deploy a new virtual machine for applications. So at the risk of asking potentially a spicy question here in this future sort of envisioned state of yours where we might have millions of roll ups, I can't help but notice many of the roll ups today have these multibillion dollar valuations, a couple of million roll ups at a couple of billion dollars each. Starting to talk about some real numbers. So how do you kind of see that shaking out? Is there consolidation in the future here? How many of these general purpose ones. Do we really need? Yeah, we don't need like these millions of roll ups won't have multibillion dollar valuations. They won't all be like massive roll ups.
08:04:13.530 - 08:04:42.850, Speaker A: They might all be small applications. Just like how there's different websites that offer small services or small applications. You might imagine like Dows having their own roll up chains. Like, similar to how organizations have their own discord servers. They don't share the same discord server as everyone else. They have their own namespace. You can imagine that Dows might have or projects might have their own roll up chains that interoperate with other roll up chains.
08:04:42.850 - 08:05:32.910, Speaker A: Understood. What about I mean, one sort of theme and maybe I'm reading a little bit too much into this and other folks think differently, is that I think five years into bridges, we can all admit that it hasn't been as smooth as we thought it might have been at one point. And some of especially like intense based architecture sort of points this idea. Maybe we're not going to find out bridges as easily as we thought. And there'll kind of be a couple of different sort of trust zones or economic sort of zones. I mean, how do you kind of see the it's very easy for me to imagine a world where there's apps that pick out, hey, I'd like this particular EVM or execution environment in Celestia for DA or whatever. But how much do you actually see assets and data being interoperable between these different base chains? Yeah, I mean, I think interoperability is absolutely critical.
08:05:32.910 - 08:06:56.794, Speaker A: And that's kind of like the reason why we're building Celestia as a shared security layer that can be used by roll ups to interoperate without fragmenting their security. And that's why in the cosmos ecosystem, we want to replace these committee based IBC bridges which have a heterogeneous security model that's fragment security with a more homogeneous security model where roll up share security and use fraud and ZK proofs, not committees. But it is the case that bridges today are very janky and very don't have a good user experience. But I honestly think that the fundamental problems there's practical solutions to all these fundamental problems. Fundamentally, a lot of it is just like an engineering slug and a lot of missing pieces of infrastructure that just need to get built. For example, if you take the fact that optimistic roll ups have a seven day challenge period to withdraw from the roll up to the L one or to another chain, that's solvable with atomic swaps, if you have an atomic swap to swap tokens that's instantaneous, and that's what projects like Connects do, for example. And there's problems like you have to have multiple fee tokens to bridge across chains.
08:06:56.794 - 08:07:35.398, Speaker A: That's also very solvable and that's like Skip is solving those problems. I think fundamentally it's just that we're very early it's comparable to using the Internet in the 90s. Like it was a very janky experience. You know, you couldn't stream video, for example. You have to, you know, connect. You have to manually click connect and then dial up, and then do a dial up connection that takes like a minute to run. I think it's just a matter of being early, and I think the challenges will be solved.
08:07:35.398 - 08:08:13.346, Speaker A: Yeah, I tend to agree with that. And that's kind of a nice segue into the next line of questioning here. And do we have infrastructure to take questions for Mustafa from the audience? If I kind of want to maybe leave, like five minutes at the end or so for that. Yeah, raise your hand if you have any question. Yeah, we can make it a true fireside here. But one thing that I'd love to get your opinion on Mustafa is how we eventually end up bringing more app builders into the space. Because I think the thing that we all want to see is, especially in this next cycle, a couple apps that really take product, market fit and bring millions of people on chain.
08:08:13.346 - 08:09:20.506, Speaker A: I think that's the goal here. And one theme that's come up many times is sort of this chicken and egg problem in between apps and infrastructure, where we need good infrastructure to build good apps, but then the infrastructure also has to serve the apps that exist, and it's kind of like, which comes first. So how do you kind of think about that problem? And then maybe we can talk about how to bring more builders into the space. Yeah, I mean, it's definitely a complaint that people bring up that it's a valid observation that there's a lot more infrastructure projects. It seems like there's a lot more infrastructure projects right now than actual applications. And that might seem wrong, but I actually think that's kind of fine in theorem, because fundamentally, I think the reason why there's not a lot of Web three developers, it's just that a lot of the things that seem easy to do with web three is actually not possible to do because of various either scalability or execution environment challenges. For example, take the fact that people want to build Uber in Web Three.
08:09:20.506 - 08:09:54.200, Speaker A: That's like a very stereotypical classic thing. People say, oh, yeah. Why isn't Uber on web three? Theoretically, it's possible. It's just, like, all the tools are very janky. It's impossible to do on Ethereum l one, no one's going to pay $20 for transaction fee. You need tooling to share location data in a decentralized way. That tooling is being built there's, like Lip PDP to kind of exchange messages in a decentralized way.
08:09:54.200 - 08:11:21.474, Speaker A: Yeah, I just think it's just a matter. So I think it is actually a good idea that there's a lot of infrastructure projects out there to make the developer experience less janky and more practical to build the things that seem obvious in hindsight, like Uber for Web Three but haven't been built. Yeah, completely agree with that sentiment. I think maybe one thing I'd also love to frankly just ask you as the founder and developer of the Celestia ecosystem, is what is the right way to do BD from your perspective? And there's a couple of different approaches you could take kind of like the bottoms up ecosystem approach that several blockchain ecosystems have employed successfully and then there's a little bit more top down and actually asking people to incentivizing builders and apps to come onto the platform. How do you kind of think about building an ecosystem and doing BD within the context of web three? Yeah, I think it really depends on what you're building. If you're building something that's fundamentally new and you're creating a new category and you're the only product that provides that, then you probably don't need to do as much direct BD and a community will naturally form around that. But if you're doing something that's more competitive to things in an existing category, then your main differentiator will probably be having a better BD team.
08:11:21.474 - 08:12:24.642, Speaker A: So I really think it depends on the product and I see like roll up as a service providers doing a lot of BD to kind of attract roll up developers onto it and so on and so forth. But really Celestia, we do have a large community of people naturally building on Celestia, but we do have also BD team members trying to kind of help people and explain the technology. But the way I see it is that we're just trying to create a distributed community and we're trying to bootstrap a modular stack. We're very happy to have competing DA layers come and talk and kind of come to the summit even though we're co organizing it. Because ultimately a modular stack is only credible if there's actually developers have choice in the stack. People are only going to build if there's a lot of choice so that they know that they're not locked into a specific. Understood.
08:12:24.642 - 08:13:27.420, Speaker A: I think folks are it's been really inspiring to watch what Celestia has done and what you guys have achieved over a relatively short amount of time. And I think one of the things that there's no shortage of love for in crypto is a little bit of drops of alpha. So what can folks expect from Celestia? Not revealing anything you can't obviously, but what should people be looking out for over the coming months and year or so? Yeah, I mean next milestone from here is mainnet currently planned in the fall of this year and that's kind of like what we're kind of like heads down working on right now. We're trying to ship main net as soon as possible because there's a lot of people in the ecosystem that really need a DA layer, a scalable DA layer and nothing exists right now. Soon we'll have EIP polygon available and so on and so forth. But there's literally no DA layer right now that's actually usable as a kind of DA layer that provides more than like 10 throughput. So it's like that's a thing that we really need to unblock people on.
08:13:27.420 - 08:14:20.300, Speaker A: We also have a lot of kind of like people joining the ecosystem and making announcements, integrating different parts of the stack. Recently we had an integration with Opstack where we provided a data availability interface with Opstack so people can deploy Opstack roll ups on Celestia, using Celestia as a DA and ethereum as a solemn layer. And I think we'll be seeing a lot more of that and a lot more integrations like that with other stacks. Nice. I've got my last question here and then we can open it up to questions from the audience. But I'm always interested to hear from sort of leaders in this space, like the two things that you find yourself thinking about the most or maybe it's like a worry when you're falling asleep at night. Like man, I really just want to make sure that we get this done.
08:14:20.300 - 08:16:18.594, Speaker A: What are those maybe two things for you at the current moment? Well, I guess from a very low level perspective, I'm very active on the engineering side of things, trying to make performance improvements when necessary and I try to follow the developments like people using Celestia to see what the pain points and bottlenecks are with our testnets that we've done. But from a more high level perspective, one thing that I kind of think about is to what extent in the long term as crypto goes mainstream, will users kind of care about how decentralized a blockchain is or to what extent a blockchain kind of conforms with the core values of crypto, which is decentralization, sensitive, resistant, verifiability? Because fundamentally, from a user perspective, if someone just created a centralized blockchain with two validators, from user experience perspective it's very similar experience. So I think about one fear I have is the market will naturally just evolve to centralized solutions if users don't care. But so far we haven't seen that to too much of an extent. We haven't seen an overly centralized L, one with ten validate, a proof of authority that would be a very low hanging fruit to build that could have like a billion TPS because it's not decentralized. So I think about how communities or blockchains have social moat and users care about using applications that they know or they believe are actually decentralized and censorship resistance. But I think about to what extent that will hold true in the future because we're still very early.
08:16:18.594 - 08:17:30.902, Speaker A: Crypto hasn't really a mainstream adoption, but I wonder, once we do reach mainstream adoption, if the ideals of crypto will ever become significantly diluted. I've actually asked myself a similar question. One thing that the way I've sort of phrased it to myself internally is do you think there needs to be some sort of overton window shift in order for people to adapt? So, for instance, privacy. I feel like this comes up often in privacy discussions. I mean in web Two, right? The one thing that's been proven after 20, some OD years is that users don't really care that much about their privacy and most the vast, vast majority will not take even very basic steps to protect it. And how are you going to build privacy related infrastructure for people that simply do not demand or seem to value it with their actions almost at all? So do we need some kind of overton window shift from a societal standpoint for some of these market structures to take place the way that we want, or what do you think about that? Yeah, I mean, we've seen web two evolve in a very similar way. The early days of the web, it was much more decentralized.
08:17:30.902 - 08:17:58.814, Speaker A: People had their own blogs. They weren't sharing data with big tech. Now we have people just using Facebook, most of all using Facebook, sharing the data with everyone. But in web two, the interesting, unfortunately, humanity is very reactive and not proactive. Before 2011, most websites were just using Http. You would log into Facebook. This is actually something I saw when I was in my hacking days.
08:17:58.814 - 08:18:40.026, Speaker A: The Tunisian government was capturing people's logins to Facebook because Facebook did not have Http enabled or enforced on login page. But the fundamental thing that changed made people have a big push to encryption and encrypted messaging apps and HTPs and more privacy was when Snowden leaked their NSA files in 2011 or 2010. And that was kind of like a big moment where it was like a massive difference. Before and after. Before nothing was encrypted. Basically no one cared about Https. Like all messaging apps were not encrypted.
08:18:40.026 - 08:19:37.570, Speaker A: After that, everything started. Everything was basically encrypted by default. WhatsApp is now encrypted, everything uses HTPs and there's been various scandals that actually has made web two privacy the forefront of many people's mind, like the Cambridge Analytica scandal. Facebook has a huge problem, has a huge perception problem with privacy. And ultimately, unfortunately, I think it could end up similarly to web Three. Right now, people don't care about financial privacy, but there probably will be decentralization to some extent, but there probably will be a moment in the future where people learn that actually it's very important because there'll be kind of like a Pearl Harbor of financial transaction privacy in the web three. Maybe someone is doing something very bad with all these on chain transactions.
08:19:37.570 - 08:20:27.082, Speaker A: Like Archam analytics is one of them. For example, deononomizing people based on their on chain activity, for example. Yeah, I tend to agree with that. Guys, we're in the final minutes here and I want to open it up to the audience to see if they have anything they want to ask. Mustafa so there's a lot of DA solutions that are going to be live in the next year or two, which is really exciting. What do you think are the known unknowns around having these things live around scaling them. What do you think might potentially break? And the kind of open questions around the design space here.
08:20:27.082 - 08:21:21.670, Speaker A: Yeah, I think there's a few open questions. I think one of the biggest one is how do we get people to run light nodes? Because the only way to securely scale data availability is to having data availability sampling light nodes. And the more light nodes you have, the bigger the block size you can have. But historically, over the past decade, we've had a model of web Three where people just interact with decentralized RPC endpoints, which kind of defeats the whole point of web Three anyway because that's like a very Web Two model. You might as well just use web Two because you're just interacting with a centralized database and trusting a trusted third party. So I think we need to think about ways to get more people to run light nodes, maybe by having I know Mina is doing some great work on this. They have a browser version of their light node so you can actually run the amina node in your browser.
08:21:21.670 - 08:22:23.760, Speaker A: So I think that's a good first step. And then we need to figure out how to integrate these light nodes into wallets by default. So for example, instead of MetaMask connecting to infuria, MetaMask could run an in browser light node in the background and connect directly to the ethereum peer to peer network. And I think that's a very important known unknown that people should think about, like how do we incentivize and get more people running wallets with light nodes? Thanks for the talk. It was really enlightening to hear. So I think one difference between the last bear market and this one is the fragmentation of the ecosystem that now we have so many different layer tools going on that historians might call this the l two wars that are going on now. So I was wondering, if someone creating a DAP, how do you choose the right l two to build on? And what happens if the l two you've built on fails? You might be able to retrieve account balances, but can you retrieve transaction history, reputation, all of those other aspects? Yeah, that's a good question.
08:22:23.760 - 08:23:25.486, Speaker A: I would actually say that there's a lot of components in the modular stack. There's a lot of l two S. But I would actually say it's actually less fragmented than the previous bull markets because in the previous bull markets, it was way more fragmented because people were just building alternative layer one networks that did not collaborate with each other. At least with layer twos, they can all coordinate with each other in the same stack. And one of the advantages or what we're trying to achieve with modular stack is that you can replace components in the stack. So if you deploy something using a specific type of if you deploy something on arbitrary roll up and you could swap that out with the optimism roll up, for example, or you're not locked in to a specific vendor. That's like a very important component of the modular stack.
08:23:25.486 - 08:24:20.390, Speaker A: I think that's more practical when you take into an example of roll up app chains. If you want to develop a roll up app chain, let's say using the Opstack, and you choose Celestia as a DA, if Celestia fails, you can replace that Celestia with a different DA layer very easily because there's a common DA interface. So I wouldn't necessarily say it's fragmentation. I would say that there's more freedom of choice. And sometimes that's not necessarily always a good thing. Sometimes there's too much choice, and that's very difficult for developers to compare the trade offs. But that's also an open problem, which is how do we get developers to understand the trade offs between these different components and execution environments in the stack? Thank you.
08:24:20.390 - 08:25:44.300, Speaker A: Great talk, mustafa and Mike, thank you so much for all the details. I love that you touch the business development area, and I think it's great that you guys have a dedicated team inside the protocol. I would say that probably many of the projects, at least at the start, may not have this. So I was wondering, what are some learnings that you see in terms of what works, what doesn't work in terms of business development, or maybe some advices that you would give to your builders and maybe, what's your strategy? So you were saying that would depend on the product itself, but in particular case for Celestia, as you are closer and closer to the main net, what would be some strategies for growing the ecosystem that you would want to explore? Yeah, I think our kind of overall long term goal is to bootstrap a kind of like, self serving community. Ideally, we're trying to create a new category. So I think for any protocol, the success of any protocol should not depend on any kind of centralized BD team. Otherwise it's not really decentralized protocol.
08:25:44.300 - 08:26:55.456, Speaker A: And so that's why we've kind of tried to create a community, a modular community that can kind of have a mass or a network effect. And so if you think about other pieces of infrastructure with network effects, let's take AWS for example. AWS has a massive community of developers, and you don't need a centralized BD team of WS because it has a network effect where lots of APIs integrate. Like, it's very easy to use AWS because it has wider community support. And so for Celestia and other daily, there's a very similar process there where we want to make sure that Celestia as a DA is supported by as many DA interfaces and roll up frameworks as possible. We started with the Opstack integration. We want to have the community develop more integrations so that Celestial will be like a default DA option for these roll up stacks.
08:26:55.456 - 08:27:16.052, Speaker A: And eventually you have a community that kind of bootstraps itself. And you don't necessarily need a centralized BD team to kind of push it forward. Just like Ethereum doesn't have a centralized BD team, for example. Guys, I think unfortunately, that is all the time that we have, guys. Everyone give it up for Mustafa. First of all, an excellent event. Thank you for the chat.
08:27:16.052 - 08:27:16.790, Speaker A: This was really great.
