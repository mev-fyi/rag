00:00:01.210 - 00:01:07.940, Speaker A: I'm Jordan, I'm the CTO, co founder of Astria. We're building a shared sequencer network, which just like, well, not just like, but similar to what Espresso has been showing off lately and going to walk us through here what we have and what that means. And really what we're trying to show is we talk about this thing, that deploying a roll up should be as easy as deploying a smart contract. And so a big part of what we're demoing here today is how you can use a shared sequencer and how this can be possible, that there will be a part here that in theory you could walk along on your computer. But as I get to in just a bit, you'll see it probably not practical given Wi Fi restraints. So when we talk about a shared sequencer, this was pulled directly from Josh's talk earlier about what a shared sequencer is. And so the first thing we want to do is we want to get this shared sequencer and we want to get a singular roll up running on top of it, because without anything, you're not sequencing anything.
00:01:07.940 - 00:02:05.262, Speaker A: And so the question becomes like, what does the architecture look like? This is a nice, pretty diagram, but we're going to go a little bit more technical here. It's like, what are we actually doing underneath the hood in order to make this possible? And so this is more of an internal architecture diagram of what we're going to be running. We have shared sequencer where the sequencer here is a cometbft tendermint, depending on your preferred terminology, base chain. And then we have a relayer which runs and it participates in a gossip network, and it also takes the transactions and writes them to the data availability. So our sequencer here is running on approximately 1 second block times. Our slash is running at 15 2nd block times. At each 1 second block time, there's a gossip that's created of that block, which our conductor piece over here can read as a soft commitment to execute inside of the chain.
00:02:05.262 - 00:03:14.140, Speaker A: Additionally, at the end of every celestial block, we submit the data for all blocks that have occurred since the last write to the DA, which what we call the conductor as well can read off of that DA what the job of our conductor here is, is it's the drive of a roll up. So similar to what the beacon chain is, or the engine API for Ethereum, or what op node functions in the op stack, this is going to be executing via a API we have here that's pretty simple to drive the deterministic execution of blocks it does this in such a way that is blockchain agnostic. It doesn't actually know what Ethereum blocks are. It just knows that it receives a hashback in return. And we have a modified geth node which consumes that API and is able to send those through to our shared sequencer. And in the end you as a user get a completely processed transaction. There's a little bit of tooling here that I'm going to use to spin this up locally and show you how all it works.
00:03:14.140 - 00:04:00.982, Speaker A: We have a whole bunch of DevOps tools, so if you were to do this on your own machine, you would need Docker and Kubernetes tooling. And we use helm and then some just for some commands here to make our scripting easier. And then if you were also following along, these are the two GitHub repos I'm going to be interacting with. We have others, but these are the two for kind of demonstration purposes. Our dev cluster repo has all of the configuration and whatnot. We also have like a mono repo for most of our rust based code, and then those are going to spin up a whole bunch of docker images. I put this list here because this is why if you were to grab your computer and go to run things, it would need to pull all of these images.
00:04:00.982 - 00:04:56.780, Speaker A: And the WiFi is always great at conferences. And I think that even just having twelve people in this room pulling a list of eyeballing it 15 ish docker images might be a bit painful. I've already pulled all of these locally, so I'll be able to walk through kind of how we spin up this whole system as well as how we interact with it, and then we can go through and kind of demonstrate what's happening underneath behind the scenes. All right, that said, let's actually deploy this thing. So I'm going to go over here into, let's clear this. So within the dev cluster repo I referenced earlier, we have a whole set of Kubernetes configuration pieces to deploy things. Thankfully, you don't actually have to know how to use kubernetes in order to get deployed because that would be a terrible experience for someone trying to test this.
00:04:56.780 - 00:05:39.290, Speaker A: The first step to create this would be to create a cluster. I'm not going to actually run this command right now because I have already spun up my Kubernetes instance because this starts all of the image pulling and then once we get this done, this is all in the readme of our repo as well in terms of what it takes to basically get things spun up. The first thing we're going to do is we're going to deploy Astria local. Cool. This is going to spin up a couple of docker containers, which I can. And containers in kubernetes, which we can see here. Yeah, there's going to be some.
00:05:39.290 - 00:06:12.434, Speaker A: The magic of Kubernetes is that these things will reboot until they come up. So within our sequencer. Yeah. Let me see what I can do to. Yeah. Is that a little bit better here, let me make it even bigger here. There we go.
00:06:12.434 - 00:06:39.978, Speaker A: Okay, so now what we spun up. When we do this, we have two things. We have a celestia local network and then our sequencer just came up online. So if we go back and reference our slides here, I've spun up this data availability, which we have a celestia local network as well as the sequencer and the relayer. These all exist in one. Let's just leave that up. Slide over.
00:06:39.978 - 00:07:08.870, Speaker A: So in our sequencer here, we can see that we have a comet BFT based chain. We have the sequencer and then we have the relayer that I alluded to. If we look into comet BFT here, we will see that we are producing blocks rather quickly. There's nothing going on in these, though. We don't have any chains added to it. Right. And if we look in our sequencer, we'll see there's nothing happening because there's no transactions going through.
00:07:08.870 - 00:07:47.450, Speaker A: This is the app side of our chain we are built on top of. Instead of a cosmos based chain, on top of tendermint, we have a rust app that is built on top of the ABCI interface. And then the relayer here you can see submits blocks to the data availability layer, handles the sequencer blocks, and every time, every so many blocks, it will write to DA when there's another da block. There was another one. Great. Let me zoom in on this one, too. Cool.
00:07:47.450 - 00:08:20.284, Speaker A: All right, so we have that deployed and now we want to deploy a roll up. Well, let's do this the simple way. Just deploy roll up and we're going to deploy a whole bunch of stuff. Now we can go back over here and we can see we have spinning up. We have three different things. We have this geth container which contains what I called the conductor. Again, the drive of our roll up as well as an instance of geth.
00:08:20.284 - 00:08:51.660, Speaker A: And then we also have a block explorer as well as a faucet. So if we go over to our browser here, by default, it deploys a chain at this address. We can see in metamask that I'm connected to the wrong network. Let's connect to localnet. Yeah, we have 300 tokens in this by default. This account is funded, see, 300 and nothing. We can go here.
00:08:51.660 - 00:09:52.928, Speaker A: We can go to our faucet and let's send some funds here. This will come from our other account that I have linked here. We're just going to send through request and then it should show up here. But metamask is sometimes buggy. If we go over here, we can do a cast balance and we can see that we have transferred money pretty quickly there. Additionally, like we just did a very simple account transfer, right? If we look at, let's see if I do a cast balance other account, you can see that it used to have something, paid some gas and paid for some tokens. Additionally, like I mentioned before, we have a block explorer so we can just go to use, we have block scout configured to load on this.
00:09:52.928 - 00:10:27.200, Speaker A: We can instantly load see our transaction here in blockscout. And that's all fine and dandy. Additionally, we have some test scripts here. We have this like EVM test data script that we created. So we can just say it's trying to remember what my command is for test data.
00:10:28.610 - 00:10:30.120, Speaker B: I it.
00:10:32.570 - 00:11:26.470, Speaker A: Generate transactions. Yeah, so we can generate some transactions here. This will start running through foundry script will run through, we're going to do like 30 transactions, runs it usually about one every 6 seconds or so here it's going to go through. We can go look at our block scout instance here and we can again see live these transactions showing up, creating some token minting, creating contracts, just populating us with some test data here. This is all fine and dandy. We have a geth based roll up that functions generally like Geth, but we promised you a shared sequencer. We go here, it's like there's one.
00:11:26.470 - 00:12:09.394, Speaker A: But what we actually wanted was this, right? We wanted to see that we have two sets of rpcs and two roll ups here that are running. So why don't we go back here and while we have all these transactions executing, we can go here and we can say, oh, I want to create a new roll up. So let's deploy a roll up and we need to pass it. Two things. One is a name. Anyone want to give me a random name just to show that I didn't pre program this GM? All right, and then anyone got a favorite number? I have to seven. I'm going to do.
00:12:09.394 - 00:12:13.246, Speaker A: How about seven sevens? We'll do some sevens.
00:12:13.278 - 00:12:13.762, Speaker B: There.
00:12:13.896 - 00:12:39.440, Speaker A: Cool. So this will be our roll up name and our network id. These will be distinct. We'll be able to send transactions to both of them and see that they go through without impacting the other. So if we go back here to our Kubernetes setup, we can see we have two distinct things that are deployed here. We have two different roll ups that the faucet will come up as. There we go.
00:12:39.440 - 00:13:29.646, Speaker A: So we have a whole nother instance of block scout, a faucet and this GM. And we'll be able to access that. Now if we go here, we had that cast check the balance this is going to be at. So our, one of our accounts has zero there. And our other account has our same initial balance as we had before. We have transactions running over here. Let's go ahead and do the same thing here as well.
00:13:29.646 - 00:14:00.370, Speaker A: We're going to just generate transactions on GM. I think it was five, seven. Cool. So now we have transactions going through on both of these chains. We just about finished here. If we look at this initial one, we'll see we freeze at 32. We can go here.
00:14:00.370 - 00:14:39.390, Speaker A: Let's see our instance here. Populating. So we've truly said as easy as deploying a smart contract, it was one call. We deployed a roll up. We have transactions going through. If we go and we look into our logs of our geth node, we will see transactions coming through here. We can see in our conductor that we're getting blocks both.
00:14:39.390 - 00:15:27.448, Speaker A: Let's stop this from going through. We can see that we have two things going on. One is that there are blocks being received from the P to P network as well as we have these finalized block calls which are being made as soon as we see the data in DA. So things are being executed as soon as they come through, but they are also being finalized as soon as they're visible in the DA layer. And if we look in our single sequencer here, let's see if we can see what's going on here. Transactions going through. We can see we have quite a few blocks.
00:15:27.448 - 00:15:36.290, Speaker A: We're limited in our transaction throughput at this point by the speed at which foundry is actually sending them, not by the speed of the blocks that are actually coming in.
00:15:39.060 - 00:15:41.730, Speaker B: There's two different testing transactions coming in.
00:15:42.260 - 00:16:04.360, Speaker A: Yeah, I can. Yeah, let's see if we can get that. Yeah, I have two screens here. So this will go and execute again on our default roll up. So we can see them coming through on this roll up. They're still going through here. We can easily monitor.
00:16:04.360 - 00:16:49.770, Speaker A: There was a log. I don't know if it's showing up right now. Let's see if we set this up to auto scroll, if we can see as one in one namespace. So we have the list of how many namespaces are contained. I think one of these commands may have finished, so we're only processing on. Yeah, so that was kind of the demo, I realized I went through it pretty quickly here. But does anyone have any questions about kind of what's going on in the Stack? Anything I can answer?
00:16:51.740 - 00:16:54.360, Speaker B: What can we expect in terms of latency?
00:16:56.540 - 00:17:22.480, Speaker A: Yeah, I think the reality of like this is a demo is that it's kind of early to say in terms of latency here. It's going to be network effects, right. It's going to come down to how well connected and how close you are to everything. If you are running a sequencing node and you're running a roll up node, like you can theoretically have network latency decreased.
00:17:26.500 - 00:18:27.888, Speaker B: Yeah. What is the lowest tendermin and what size? That is like 1.6 seconds is what you get validators optimization on tendermint. And they've got down to like half a second or something with like 30 validators. So again, it's like sliding scale, the most optimized algorithm like implementation. You can make it go faster, you can optimize, you can use the validator set, because the way tender works is timeout. And then you kind of like fail if you hit a timeout window and you don't have two thirds of the state coding.
00:18:27.888 - 00:18:54.960, Speaker B: But in this demo, for example, we could run the block time. We're going to do like 100 millisecond block time. That's our timeout. As long as we have one node has all the states, then it doesn't have to broadcast. Anyone say, hey, I vote for this thing within 100 milliseconds. And I have two, somewhere in the range. Worth thinking somewhere in the 2 seconds.
00:18:54.960 - 00:20:38.370, Speaker B: But again, it's kind of like up in the air. Is it worth the effort of highly optimizing the tendermint like internals to try to shade latency and get more performance on layer or whatever? But like, tendermint is a chunky code base, a lot of parties using it, right. 2 seconds is well within capacity, easy to do, and like runnable validators and users generally want responsiveness. We like to be closer to 1 second because if you can get like a reset in latency on, I did a bridging, you know, I did like the lock on one thing. I did like the mint on the other chain and you can do that kind of between both of them that might be really good in that range fundamentally, right? If you look at non VFP or something, if you want global distribution, you're not getting. That's the best part. Make sure site picked up by roll up.
00:20:39.700 - 00:21:04.536, Speaker A: So there's an arrow missing on here as well, which is that currently in our current architecture, this relayer has a gossip net that it starts up via Lib P to P that the conductor is also connected to to drive. So it gets soft commitments from the relayer as well as reading firm commitments. No, generally how roll ups work, right.
00:21:04.558 - 00:22:24.236, Speaker B: Like we're most architecture, the finality definition is usually defined on roll up node itself will receive input from various sources. In this case, right, like the DA layer of one source and the shared signature of another source. And it will choose to define what it considers. Usually you have ahead, in our case centralized. The moment that would be head safe would be when, and then final would be when you pass the chain. The moment the shared sequence are getting back, bounded by how fast the user is transaction understand to the roll up here, the roll up, but the roll up sending already the roll up gets finality because again the missing error is the user sends a transaction to get here, right? Yeah. So the user sends a transaction to roll up RPC, which is the guest node that sends it to the shared sequencer.
00:22:24.236 - 00:22:36.580, Speaker B: And then the top thing that's documented, the shared sequencer actually sending the block it created back to. In this case it's document, but it has to be sending that block which then executes.
00:22:39.640 - 00:23:06.450, Speaker A: Yeah. So what's happening in our actual guest node here is we alluded to the fork choice rule. And this is to an extent this is up to a roll up developer. The way we've done it in this is that every time a soft commitment comes in, you get a new head and safe block because the sequencer gives you a strong commitment to ordering, but it doesn't give you a firm commitment that everyone has been able to see it. Once it's seen in the DA, we update it as final.
00:23:10.500 - 00:24:22.644, Speaker B: You see a buddhist sequencer. How does it table so, because roughly the shared sequencer bottleneck is like simplistic view, right? Whatever the ratio of block time of the shared sequencer is to block time of the DA layer, the DA layer is going to have some bandwidth limitation, right? The limitation of throughput of my roll up today, how much call data? So if we say 15 2nd block time, your bandwidth of transactions is 1115 of the throughput of the VA layer. But the nice thing is that there's no chance that execution is a bounding function in this, because the sequencer is not doing. Ordering is like a computational thing. At this scale, we're not going into like a PBS thing, which is long term, how we think this will be ordered. Right. And to some degree, that's like computationally, maybe more expensive.
00:24:22.644 - 00:25:45.792, Speaker B: Like doing an abstract problem of calculating an optimal block is like a computationally expensive thing, and there will be congestion. If you say, okay, we have 15 megabytes blocks on the a layer, you can have 1 mb every 1 second on shared sequencing layer, that's going to be your throughput limitation, but that's on raw data, because execution is happening. It's literally just like, how fast do people generate 1 mb block in an ordered fashion, which is like, that should never be. And it's just wondering, how many roll up transactions can you fit 1 mb block? And I usually refer to Vitalik has a post on, it's called an incomplete guide to roll ups. And that goes in quite a bit of detail on various compression mechanisms, like roll up, transaction format capacity. And off the top of my head, I think if you get like an EVM compressed transaction to like, well, bytes, I think per transaction, that sounds to me there's like more numbers. Again, your throughput is going to be 1 data.
00:25:45.792 - 00:26:08.964, Speaker B: How many roll up transactions can be good? That's going to be defined by what state machine you use and how much, because you're essentially putting serialized data into the shared sequencer. And then your roll up just has to have a mechanism of deserializing that data. What is your serialization format? You do like a. Broadly. Jesus. Whatever. Compression, right? Like lossless compression, you can easily get two x ratio roughly.
00:26:08.964 - 00:26:19.870, Speaker B: Right. And then how do you do, like, mark compression in the literal data package format of defined in your state machine and whatever mechanism there? Relatively large.
00:26:20.320 - 00:26:22.270, Speaker A: Yeah, pretty high.
00:26:25.920 - 00:26:28.430, Speaker B: Every second and a half. Right? That's a lot.
00:26:30.820 - 00:27:04.010, Speaker A: One of the things I was going to show here, in terms of showing that we're not actually executing, is that the chain id, which is what we call each person, similar to a namespace on celestia, you just pick some set of bytes, and you post your data to that. It's just an array of, it's a vector of U eights. And then the same thing with the data. This whole set is just a whole bunch of bytes that we never process or do anything with. It's just bytes. Yeah.
00:27:08.140 - 00:27:08.890, Speaker B: Like.
00:27:11.420 - 00:27:14.210, Speaker A: Let'S go to that one.
00:27:18.900 - 00:27:31.892, Speaker B: The users transactions are still coming into rpcs, right? Right.
00:27:31.946 - 00:28:24.900, Speaker A: So we're actually working on a new component to throw in here that's just not quite ready, which would basically as long as you are running, because this process of throwing the transaction in here is fundamentally, I'd argue, kind of flawed, which is that now we have to have a direct submission. What we're working on is this piece that will kind of capture and encapsulate what we call the MEV piece, which is the way a MEV searcher today works, is that it reads over some RPC mem pools and then does some stuff and then can work with builders to build the blocks. So what we'll be doing is if you have something with an EtherPC, you can connect this to it. It'll be able to read your transactions directly out of it instead of being submitted, and then process those and submit them itself down to the shared sequencer. So you don't actually have to have your roll up, have any sort of special submission logic.
00:28:27.420 - 00:29:15.590, Speaker B: Going back to the question, I just like twelve bytes, but like there's some subtle detail there. But if you have an uncompressed transaction, and I'm assuming it's like a simple transfer, it's about 112 bytes, we assume we have 1 mb lock, 99,360 transactions per block or whatever. You saw it like equal up. And there's potentially a ten x improvement in the efficiency in various kind of compression mechanism and whatnot. Somewhere in the like 10,000 tps. And that would presume the data layer had 15 megabytes. But that's just a ballpark we're looking for.
00:29:15.590 - 00:30:00.484, Speaker B: That's like the worst case. But that would be like the total, right? That's like you have to buy all those. If you buy all the data, you might be able to get a ten x improvement in compression. So we're like somewhere like the 5100 thousand epS. If you just use all of the bandwidth of the DA layer for GM transactions. Again, assuming 15 megabyte blocks, I don't think Ethereum or celestial planning to have like 15 megabytes blocks at like launch more. The soft commitment again, right.
00:30:00.484 - 00:31:26.210, Speaker B: Is like pretty flexible here, right? And there's a question of like does the shared sequencer, is it at a consensus level responsible for posting data? That is like opinionated question of whether it's responsible and then also when. So the naive assumption is the shared sequencer always posts all the DA layer immediately. And thus it must post like 15 blocks immediately in like a DA layer block. Theoretically you could have lag on that or whatever. You could have some kind of block space futures market, right? You could see that pressure mechanism. There could be some longer timing, right? The, the role kick guys have a lot of good writing on what they call profitable, which is if the shared sequencer is a requiring 95% of bandwidth, then it's a strong opportunity for layer to charge the shared sequencer and say, I'll send for you unless you pay me a dramatically higher gas fee because I know that you're very close to the limit of the total bandwidth. And if I block you for one block, then you can't catch up.
00:31:26.210 - 00:32:49.956, Speaker B: There's a big design space around what are the margins of safety from sequencer. Does any roll ups need to post data on base layer architecture? And so the design space, very broad from a soft commitment perspective. But ninety K, is that assuming 1000 rolls? Yeah, for like a given DA, because you're overall so fundamentally going to be limited by what is the base layer throughput. And the DA layer is assumed to be the layer most optimized, the total data bandwidth. And the reason why, because a DA layer selecting folks will have better info on this. But DA layers generally in the erasure coding scheme, they use scale better to have larger and slower blocks. So potentially the DA layer could get to 128 megapixel blocks, but that may require them to say go to 32nd block again, it's like more to have higher prior to that, but that increases more and half a lock speed.
00:32:49.956 - 00:34:06.432, Speaker B: Right now we have a five x. Fundamentally your role is always going to be by the throughput of the baseball. Unless you just give up on having like a strictly put shared sequencer gives a guarantee that it supports that it will post at least to one DA layer. Then you have kind of trying yourself to that. And if someone else wants to support that, they will have to pay the implicit cost the shared sequencer recognizes of needing to pay the DA. When you look at optimism, optimism has in the last 30 days when I checked or whatever, they had somewhere in the $80,000 worth of l two fees, but they had to pay out somewhere in the ballpark of like $50,000. Shares are a similar thing on roll up today.
00:34:06.432 - 00:34:46.556, Speaker B: The bulk of the cost users is from the DA layer. And that's like a fundamental question if you're required. This is how we actually see IBC working today, where you try to send it for a strain of action between two chains. There are relayers to do this. No one pays, not yet. There's many times, but they all have many indications.
00:34:46.668 - 00:34:47.888, Speaker A: Do you have another question?
00:34:48.054 - 00:35:14.680, Speaker B: If the A layer is very, very cheap, right, then maybe it's fine if someone just says, hey, adoption, I'm just going to allocate like $50,000 and that is just fund that post out of the a layer broad design space. Like how would you choose for this demo though it is posting the data to relayer component.
00:35:16.860 - 00:35:22.920, Speaker C: In this design, you're basically limiting your own data by that of the DA layer.
00:35:23.000 - 00:35:51.650, Speaker B: Yeah, and from a strictly, I guess like semantic definition of a roll up, when we use the term roll up, it assumes data is posted to an EA layer where a VA layer is a self network. Whereas term like a validia, right, which is generally someone running off chain data, which is pretty.
00:35:54.580 - 00:35:59.160, Speaker C: Mean. The roll ups are like getting data disposal between shared semen.
00:36:00.220 - 00:36:45.812, Speaker B: Yeah. Always the security that a roll up wants to give. Theoretically we assume, right, that roll ups want to be able to clients that can then receive fraud proofs or like ek proof, right. But they have the guarantee that the data of that proof is easily bindable. And again, the benefit of select you're right through data availability sampling is that you can have a very large amount of data, 128 megabyte blocks, like a 32nd window. Right. But you can prove that a given proof was for data that was present in that without having to store all of the data.
00:36:45.812 - 00:36:55.530, Speaker B: Because you now have a relatively high bandwidth system, we're looking at, call it like 256 megabytes a minute. That's not that much bandwidth, but it's not nothing.
00:36:57.340 - 00:37:11.704, Speaker C: The sensitive thing you mentioned is there also another scenario. If the DNA is already at capacity and your sequencer can't afford, for whatever reason, to get it fast enough, how do you catch up in that scenario?
00:37:11.832 - 00:38:16.384, Speaker B: Yeah, again, it's a fundamental question of like what is the ratio of bandwidth sequencer needs relative to the DA layer, right? If it's a tight integration, right, where the shares, to some degree, the share has to be throttled to add to the DA layer. And again, you have these censorship attacks, right? Same thing exist on like roll ups on Ethereum today. Someone could just say, we know who the signer for the sequencer relay or whatever, and we're just going to send the messages to. They usually have the ratio of kind of throughput is such that they batch relatively frequently, so they have a relatively low ratio of needing to buy a relevant fraction every 30 seconds or whatever. But again, it's all that ratio. If we run into something where the throughput of all of them shared sequencer is starting to surpass the capacity layer. You get in this kind of awkward question.
00:38:16.384 - 00:38:31.080, Speaker B: What if you fall behind? How can you catch up? There's just like a structural question. How you handle that is like generally you would assume that you would take a relatively conservative position. Throughput the shared sequence guarantee relative to the overall.
00:38:33.980 - 00:38:42.100, Speaker C: It just makes more sense to let royals themselves figure it out. If they value faster finality through the.
00:38:42.110 - 00:38:44.092, Speaker B: A layer, they'll pay it more rather.
00:38:44.146 - 00:38:46.108, Speaker C: Than trying to force everyone.
00:38:46.194 - 00:39:24.104, Speaker B: A little bit of my assumption, right? But a lot of it is also like the large promise, right. We believe she's like as easy as deploying smart contract. So there's a lot of this question. I want to create a roll up. How much of my problems would I like you to solve? Some people preference. If I'm going to solve all your problems, and I have to do so in an opinionated manner, there's going to be some fundamental trade off to any opinion you take, right. You may mean that, okay, your base cost are going to be higher because I'm going to assume I'm having to cover all of it.
00:39:24.104 - 00:40:13.832, Speaker B: We get into various things around how does the sequencer insulate itself from DA layer fees, right? To some degree just get into the block space as a commodity, right? And so you say, well, sequencers or relayers for the share sequencer to DA layer would presumably hold a large quantity of the DA layer based asset to protect themselves. Inflation the price of that asset. We obviously have seen various of gas tokens, but there's obvious demand for that. I know that persistently. In the future I would like to acquire 1 data every single block. In the future I would pay a premium to myself. Some volatility in that.
00:40:13.832 - 00:40:35.630, Speaker B: Is there some means of purchasing this part of the app to white paper? Sam Hart, who is maybe a tier page but I'll move on, wrote that promotes some ideas over how can you acquire guarantees around purchasing the market?
