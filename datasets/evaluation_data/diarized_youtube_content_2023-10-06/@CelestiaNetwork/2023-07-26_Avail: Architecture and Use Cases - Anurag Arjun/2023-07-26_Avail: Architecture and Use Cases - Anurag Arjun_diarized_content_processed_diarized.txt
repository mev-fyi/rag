00:00:01.530 - 00:00:27.320, Speaker A: Hello. Yeah, good morning. Very pleased to be here. Very well organized summit from the chair, folks. My name is Anurag and I'm going to talk about avail architecture today. Can I have the next slide, please? It. How do I.
00:00:27.320 - 00:01:32.826, Speaker A: Okay, yeah, so just to provide some context on what avail is. Right, like, I mean, I. So Avail was started within Polygon in November 2020, and we recently spun avail out in March 2023 to become a completely separate independent entity. What avail is, is a data availability layer which uses combination of virus accoding, KZG polynomial commitments and data ability sampling. Essentially. And I'll provide some context on where we are in that. Some of my background is I previously founded Polygon in 2017 and I started the project with my co founder Prabhul in 2020 and we just spun it out just very recently.
00:01:32.826 - 00:02:31.066, Speaker A: So the entire team of Polygon came over to join us at Avail. So that's some of the background and you can see some of the history here. So today a lot of people ask me on Twitter regarding the architecture. And so that's why I wanted to focus on what avail is, what are the use cases? And so I'll try to get a bit into the technical details as much as possible within the short time window. So before I get into the meat of the presentation, right, like some context, I know this is a modeler summit audience, and so in general, no major introduction required. But essentially what I want to talk is that roll ups are now acknowledged to be the main way to do off chain execution. And if you can see the rise of ethereum roll ups, right, like all the big activities happening on layers like Polygon, ZKVM or arbitrum and optimism.
00:02:31.066 - 00:03:21.722, Speaker A: And so the roll up is now considered as the best way to do off chain execution. But if we consider that the roll up is the way to go, and blockchain constructions are becoming more and more modular with the rise of roll ups. Now it is important to see what these roll ups really want and what are they hungry for. And the answer is that they really want lots of DA or data availability. And that's kind of the primary reason why we are working on avail. And I want to make a bold statement here in the sense that every base layer blockchain in the future is going to be a DA layer. Even Ethereum has already pivoted to a roll up centric roadmap.
00:03:21.722 - 00:04:00.800, Speaker A: And it is like prioritizing pivoting to a DA layer. If you've heard of Protodank sharding, dank sharding all of this point to the fact that the base layer is going to be a DA layer and all the execution is going to move to the roll ups on top. And that is the context in which you should view avail. That avail is a base layer that provides scalable data availability for roll ups. Now, what is mean? This is a celestia. This is the model summit, and Celestia is one of the organizers. And so you'll ask what are the differences between avail and celestia? And I'll get into that.
00:04:00.800 - 00:04:52.110, Speaker A: But essentially, avail is a modular layer that focuses on datability. It does not do any execution. It accepts transactions from roll ups and makes them available via a combination of erasa coding and KZG polynomial commitments. And in a sense, what it does is it kind of orders the transactions that come to it and provides it to the light client network. In general, the mental model for looking at avail is very similar to what an Ethereum like layer provides to the roll ups on top. So in that case, the roll ups do the execution on layer two, and then there's the base layer that does the data availability. And so you can have a variety of roll up execution environments.
00:04:52.110 - 00:05:42.250, Speaker A: So this includes something like the EVM, but also more complex environments like SVM, but also app specific chains. And how we do it is a combination of erasure coding, KZG polynomial commitments, and the USP of data quality sampling, which allows downloading of block data within a few random samples. And we'll get into how that gets done. But in general, a variety of roll ups can leverage this capability. This is the base layer architecture. And in general, if you look at it right, like, I'll just go through it in a little bit of detail. So what is happening is the primary consumers of avail are roll ups.
00:05:42.250 - 00:06:32.826, Speaker A: Roll ups accept transactions and basically submit transactions directly to avail. We have this concept of application id, where each roll up corresponds to a particular application id, and then they can submit that onto the same base layer so that we can have multiple roll ups submitting data to avail, demarcated by application Id. What we then do is kind of extend the data or eraser code the data. So if you see this diagram. So the original data is then extended in general via eraser coding. And then what we do is primarily create commitments of the data. So, I mean, if you see this slide.
00:06:32.826 - 00:07:46.512, Speaker A: So this is the rough structure of the blocks, right? So if you see the original data, the data from the roll ups is packaged into the block, and we create polynomial KZG, polynomial commitments for the data and it's erasure coded in such a way. And the homomorphic property of KZG allows us to mirror the erasure coding of the encoded data on the commitments as well. So if you see in the right hand side, the c one to cn are the commitments of the original data. And because of the homophomorphic property of the KZG commitments, we are able to kind of extend that to the erasure coded data as well. And so once that happens, sorry, how do I go back this one? Yeah. Okay. In general we are able to adjust the matrix size.
00:07:46.512 - 00:08:33.410, Speaker A: So what we really create, as I told you, is like an m cross n matrix. It is erasure coded to create. In general, we double the data in general, and what we then do is take the commitment and put into the header. So the header has all the commitments to the data. It also has the app index and certain other meta information. Now this block data is then propagated to all the other validators, and basically each validator at the moment in the current implementation regenerates those commitments and comes to consensus on the block. So that's how the base layer works.
00:08:33.410 - 00:09:27.628, Speaker A: And we've already gone through the block production stack. In general, we've used substrate to build the validator node, and the consensus that we use on the network is grandpa and Babe. So babe is the block production mechanism and we have grandpa as the finality gadget. So it's a hybrid ledger in that sense, and protects against large number of roads crashing, et cetera. The incentive mechanism is nominated proof of stake. Why we chose that is basically because it allows for wide stake distribution. And so what happens in nominated proof of stake is that you do not delegate to a single validator, you delegate or nominate to a pool, which then is fairly distributed to a large number of validators.
00:09:27.628 - 00:10:09.528, Speaker A: So you can do a ranked choice of validators. And so essentially why is this important is it allows us to have a pretty decentralized set of validators and we can have up to 1000 validators in the validator set. That's what we get from using substrate. We use substrate, but of course it's a data availability layer so there is no execution. So we've disabled all the runtimes and such. And so it's a very light runtime on the base layer itself. Now the beauty of this whole construction is we are able to do a pretty neat lightline network.
00:10:09.528 - 00:11:12.476, Speaker A: And in general, once the blocks are finalized and the headers are propagated to the light clients, even if a validator withholds the data. The lightline networks, basically, because they can sample the block pretty efficiently, they can come to know if there's some withholding of data possible. And in general, we want to target a large number of light lines. But in a sense, a few hundred or 1000 nodes are pretty much enough to kind of sample the block data pretty quickly to get into the lightlined architecture a little bit. Right? So we started with a different implementation initially, but then we had to kind of build the lightline node from scratch. We use a cademlia DHT implementation, distributed hash table implementation. And so the lcs basically form an overlay p two p network on top of the base layer.
00:11:12.476 - 00:11:56.220, Speaker A: Initially they use the full node for the bootstrapping. But over time, when you have a number of light lines on network, a new light line that enters the network essentially can start sampling from the light line. So within the light line network, there's a p two p network as well as a DHT. And so you can think of this from a mental model. The lightline network almost is like a torrent like network. It's not the same, but you can think of it like that because it stores some of the sample data locally as well, for a period of time. How we do the proof verification is that we generate cell level proofs.
00:11:56.220 - 00:12:41.740, Speaker A: So as I mentioned, we have this M Cross N matrix. And so depending upon the site, we'll have these proofs generated on the cell level. And so that's why there'll be a random sampling from these light lines and they are able to verify these cell level proofs. And as I said, within a few samples, or if we have like a few hundred or even thousand light lines, the entire block gets verified. And of course, there's this property that the larger the number of light lines in the network, we can increase the size of the blocks as well. In that sense. And I will say from an effort engineering effort perspective, bulk of the work that has gone into building avail.
00:12:41.740 - 00:13:34.752, Speaker A: And so we've been building this for more than two and a half years now, first within polygon, but now as a separate entity. And most of the work is on, the bulk of the effort is on the lightline p two p, because there are a number of issues that performance, et cetera, that have to be looked into there. This is sort of a visual representation of the data metrics that I talked about. We can play around with the number of rows, the number of columns, and you can see a reference benchmark on our performance in the sense that if you see the table on the right. For a two mb block size. Right. The rows and columns are such.
00:13:34.752 - 00:14:31.810, Speaker A: And the times to generate the commitments, the polynomial commitments, are pretty neat. And even if we increase the block size to a 32 mb or a 128 mb, you can see that these are well within our target block time, which we have at the moment kept as 20 seconds for now. And this allows for propagation across the network as well as well as verification of the commitments as well. Okay, so I will have time for Q a after the session. So in general, if you have any questions on the architecture, happy to answer. Post the talk. But having said that, once we've arrived at this whole construction, what is the ecosystem that we are envisioning that will be built on avail? So, as I said, this is a roll up centric blockchain, right.
00:14:31.810 - 00:15:28.724, Speaker A: Our primary customers are roll up developers, infrastructure developers. And so these are the different kinds of solutions that we are kind of looking at, right? So sovereign roll ups, validiums, optimistic chains and app specific chains. Right? Like, I think cosmos style app chains, but more in the validity proof or optimistic construction manner. And of course, general roll ups as well. And recently, the way we are thinking about this go to market, and I will get into that a little bit, is that there's been a lot of activity on the l three area. So a lot of the l two s, all the major ethereum l two s, are now looking at their own l three initiatives. So if you look at something like an arbitrum orbit or a Zksync hyperchain or a polygon 2.0
00:15:28.724 - 00:16:25.852, Speaker A: or starquest fractal scaling strategy or optimism superchain strategy. And so they are basically optimizing for a lot of l three s in general, because what they want is to optimize the l two as a liquidity hub for all the l three s on top. So that's why you'll see in the coming days a lot of one click l three deployment stacks. Now, why am I talking about. This is basically because when we talk about l three s on ethereum, right, the first thing that they need is a DLA to do for the datability needs, and they cannot use ethereum for that perspective. And so this is sort of the graphical representation of how avail will be used in conjunction with these l two s. And we are starting to work with a number of these to come up with these constructions.
00:16:25.852 - 00:17:09.904, Speaker A: We also released the avail attestation bridge recently. It's a pretty interesting construction in the sense that there is a data attestation bridge between the avail base layer to Ethereum. Right now, of course, it's on Testnet. For now, we are doing an optimistic style construction of the bridge, but we've been also been working on a ZK snark based data attention bridge with our partners at Susint. In fact, we just shot a whiteboard session on the construction. It's pretty neat. And so we are going to be working with suscent in general to create this bi directional bridge between avail and Ethereum.
00:17:09.904 - 00:18:23.628, Speaker A: Because suscent, if you know, already has a telepathy bridge which proves Ethereum's proof of consensus. And now we have with them like ZKsnarc based construction, which proves avails proof of consensus, which is Grandpa Babe. So that's one focus area that we are going to work with. And of course we are going to work with sovereign labs, sovereign roll ups in that sense, in a sense that current roll ups are primarily implemented to be verified on a smart contract on the Ethereum based layer. But with our data already sampling lightlines, what is also possible, and especially with ZK constructions and recursive proof mechanisms, we are also able to kind of propagate these proofs to the lightline layer. And in fact, we are also talking with a bunch of wallet teams in general to kind of embed the lightline into the wallet itself. Right now, lightlines are run via desktop apps or CLI or something like that.
00:18:23.628 - 00:19:25.156, Speaker A: But what we envision is eventually they will make their way to wallets and so very similar to, let's say, bitcoin Lightlines or such, right, where the user doesn't even know that the Lightline is working in the background. And why we are able to make this possible is basically because of the lightweight construction in which these light lines can actually even work on mobile devices and hopefully in the browser and such at some point in time. And so we envision that there will be a lot of light clients. And essentially, I mean, this is an underrated development. But today, for a user to verify the state of the blockchain, it's not that straightforward on today's blockchains. And what we will enable with this combination of Da lite clients plus recursive ZK proofs is that any user will be able to verify the state of the blockchain pretty easily. And as I said, modular base layers are perfect for sovereign rollups.
00:19:25.156 - 00:20:11.384, Speaker A: And along with us, of course, Celeste is also taking up the mantle. And so we are very happy to grow the ecosystem together. I'll quickly end with the development stage and timelines. As I said, we have been in development since two and a half years. Currently on our second long running testnet, which is we call it the carte testnet. It's named after Anikate Kate, who was one of the researchers behind the KZG polynomial commitment. We already have a robust set of external validators already on the testnet and we are targeting 200 next month, I guess.
00:20:11.384 - 00:21:11.010, Speaker A: And we want to do an incentivized testnet which we want to scale to a 5000 light client number pretty quickly. We'll have an incentivized testnet also this quarter. And the main net target is end of Q four or early Q one. That's the main net target for now. We are pretty comfortable in terms of where we are in terms of development, quickly getting to the optimizations. So currently in our base layer, what happens is when the block producer creates the proposes, the block creates the commitments, we propagate this to other validators and they regenerate the commitments at their end. What we want to move to a construction in the future is a construction where other validators can just verify the commitments and not regenerate them, which will make it much faster to arrive at finality and such.
00:21:11.010 - 00:22:20.410, Speaker A: We are also working on a very neat construction called KZG multi proof. So if you remember the matrix that we create, we create these proofs at the cell level. And so what we want to do is do submatrix level openings. And so we kind of reduce the complexity of verification pretty significantly and this will create huge improvements in terms of the opening generation, the DST population and overall keeping the network streams manageable. And of course while ensuring backward compatibility. And as I said a little bit earlier, we are working on this Zksnar based data attestation bridge and this is a pretty neat construction where we are able to prove avail proof of consensus, which is Grandpa BAPE consensus within the snark circuit. And I think this will be pretty useful for deploying or connecting our chain to a variety of other ecosystems because of the nature of the bridge itself.
00:22:20.410 - 00:22:52.272, Speaker A: Yeah, I think I wanted to cover whatever I wanted. These are some of the links, important links. Please feel free to scan the QR code. We'll have this presentation available online as well. And generally, yes, we are hiring across the board. When we started three months ago, we were at 18, now we are at 27. And so we are looking for quality folks to join the team.
00:22:52.272 - 00:23:16.650, Speaker A: And so if you are looking to join a growing team, please feel free to ping us. And this is also our Twitter handle. So looking forward to talk to some of. And of course, I'm available, and a lot of our team is also available in the event. And happy to talk to you as well. Yeah, thank you.
