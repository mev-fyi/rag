00:00:03.210 - 00:01:36.700, Speaker A: Hello. Slight change of research topic, but I'm going to talk a little bit about liquidity management in DeFi, which I think in a modular world becomes even harder as we talked about earlier on the panel. So I want to kind of talk through this recent paper I wrote with three of my collaborators on kind of how some of these mechanisms that I think maybe they have misleading advertising, there actually is some real thing going on from a math standpoint and we'll kind of talk about that. So just as a basic background, what's sort of the goal of DeFi in this scenario? It's really sort of in my mind, this concept of unbundling investment bank decentralized protocols basically let you compose things. They also let you automate things like incentive management of how much you're paying users for liquidity in a way that was just not really true before, but hopefully many of you probably knows. I think one important thing here I wanted to call out is like you could basically compare side by side the DeFi protocol version of the world to sort of their traditional finance version. And the main difference is that incentives are sort of algorithmically optimized in the DeFi side, amongst other things.
00:01:36.700 - 00:02:43.620, Speaker A: And if we zoom out a little bit, there's sort of three ish ish being a catch. All three ish principal agents in DeFi. There's sort of liquidity providers who are providing liquidity to be used for risky activities. They're traders and asset managers who are actually trying to build positions, also kind of synchronize their portfolios and then they're keepers. And the keepers are sort of the most important part who somehow need to be incentivized and compensated by the protocol for ensuring that some sort of high level KPI is met, such as assets are always greater than liabilities. So what is DeFi? Liquidity management protocols basically attract liquidity by offering incentives. Usually these incentives are some combination of fees and governance tokens or rights to some future governance rights, sort of like the Ve model.
00:02:43.620 - 00:03:45.330, Speaker A: But one problem is that protocols generally have many pools that they have to distribute incentives to think about, say SushiSwap thousands of pools. But most of them probably don't really need sushi incentives. And the process of allocating incentives to different protocols and deciding how to do that is DeFi liquidity management. So for the most part this is sort of important because if you overpay for incentives, oftentimes what happens is you have much more concentrated token ownership things becomes less decentralized. If you underpay, you effectively can't attract liquidity from your competitors and kind of can be cannibalized. This has led to activist investing from the form of like urine, convex, et cetera. And there's tons of examples of protocols that have died without incentives because basically they got cannibalized.
00:03:45.330 - 00:04:58.298, Speaker A: So one question is why is it hard? And so this is sort of the problem that ohm, maybe accidentally solved, but to get some sort of intuition for why a protocol may have hundreds or thousands of pools that it needs to distribute incentives to. And it can be extremely expensive, gas wise, to manage that state, especially on chain. Secondly, incentives need to be dynamic. If there's suddenly a ton of volume in some pool and it's draining all its liquidity, you need to actually increase your incentives. And if you don't rebalance fast enough, you may have something where you're overpaying for one pool that basically has no usage and underpaying for another. And the last and most sort of subtle thing is these incentives interact with each other and can cause feedback loops. So, for instance, trading volume can become correlated with liquidity because suppose a bunch of trades are driven to a certain pool, then other LPs notice the APY has gone up, then they add more liquidity, the slippage in that pool goes down.
00:04:58.298 - 00:05:49.450, Speaker A: And then now routing algorithms like Matcha, one inch, et cetera, will tend to route to those pools more. And this rich getting richer feedback loop, especially driven by aggregators like aggregators who route orders to different places, is also quite tied to how incentives are distributed. So at a really 10,000 foot level, we could view protocol's incentive optimization strategy as doing sort of three things. One is A, they're sort of renting liquidity. They're instantaneously paying for liquidity that could leave at any time. Two is they're sort of buying liquidity. The Dow might have a treasury, it may have a ton of assets of its own, and it provides liquidity to different pools.
00:05:49.450 - 00:07:05.910, Speaker A: And three is this concept of leasing liquidity. And this is sort of, I would say, the 2021 innovation in DFI of effectively coming up with novel versions of bond like instruments where people commit to provide liquidity for a certain amount of time or for a certain amount of or to guarantee a certain minimum amount of liquidity. And then they get rewarded sort of in a time locked, coupon like fashion. So one question is, can you automate liquidity management? And I realize I don't have a slide that explains what Ohm does, but this is sort of abstract, more mathematical definition of what Ohm does, but at a very high level. Olympusdao has a reserve asset, or what they want to be a reserve asset. They wanted to have low volatility. And so in order to kind of ensure low volatility, the goal is to have basically the protocol decide how to incentivize people to lock up assets, depend if the supply is too liquid, and distributes more rewards if the supply is there's not enough supply and high demand.
00:07:05.910 - 00:08:02.950, Speaker A: So abstractly, however, we could actually represent this as sort of a control theory problem. So how that works is you say we have sort of a notion of inflation that can be adjusted at every time step. That's sort of the emissions we have n capital pools. And each capital pool you could think of as has some high level summary statistics. So the reason this is in RK here is K represents sort of like the summary statistics of the pool. So for instance, AMM pool with two assets, you would provide the reserves of the two assets for each pool. Or similarly for maybe like a multi collateral asset pool, you may have to provide all of the assets plus some extra state about the assets, like how much is liquid, how much is locked.
00:08:02.950 - 00:09:21.450, Speaker A: And then the other thing you do is you kind of have an objective function. You could think this is like a KPI for the Dow. So maybe it's hey, we want very low volatility given the pools and inflation and distribution of inflation that we have. Maybe we actually want high trading volume, maybe we want the fees generated for liquidity price to go up that's represented by this function lambda capital lambda. So the idea is that for each pool pi, we add an incentive which is some fraction alpha times the inflation at each time. And we can think of the liquidity in the pool as sort of a random variable whose value is adjusted by how much incentives are paid to it. And the goal is to construct an allocation rule which is how much of inflation should I distribute to each of these pools? So that's sort of this middle line which says hey, construct this probability distribution pi such that basically we optimize the expected value of that KPI over the next time period.
00:09:21.450 - 00:10:38.470, Speaker A: So maybe we optimize it in the way that we want the volatility to be lowered over one time period or maybe we want sort of the net fees generated to be optimized over one time period. So again, as I mentioned earlier, you can really think of this in terms of like feedback loops, right? And so as a very simple description, this sort of looks like a sort of classical feedback controller where our controller is the thing that's adjusting the inflation, the amount of inflation distributed to each pool. And so what we're really doing is we observe some pool values, we choose the inflation and how we want to allocate to each of these pools and we make a prediction about the future pool value. Then we distribute, we have some algorithm, let's say, which we'll talk about in the next slide. But just for now, think of as an oracle that says hey, here is the inflation, how many tokens you should mint at this time. Step here's how much you should distribute each pool. Then you observe after one time period the real pool values and then you look at the difference between your predicted and your observed pool values.
00:10:38.470 - 00:11:50.320, Speaker A: If the difference is small, then you can reduce your inflation. You actually have been able to predict pretty well and you can see if hey, actually maybe if we spend a little less we can get the same result. But if the difference is large, then you may have to actually increase the inflation and rebalance the allocation. And so one question that's natural is do we know if this is stable? Does it actually optimize our metric? So what we did in this paper is we formulated this problem sort of generically in terms of optimal control. So optimal control is sort of different than sort of what you may have learned in sort of like an initial controls class, like a PID controller or something like reinforcement learning. Optimal control tries to have much stronger guarantees on what sort of bounds are on performance of a system. And these kind of bounds are actually quite important for something like incentive optimization, where you really care about not spending too much money.
00:11:50.320 - 00:12:51.246, Speaker A: Something like reinforcement learning has this problem of A, you don't know if it converges and then B you also don't know if the variance of the estimator sort of grows unboundedly. And so optimal control does have some guarantees. And the reason this is a nice formulation is it lets you really control your budget for these protocols. So the main thing that kind of classical sort of control what you do is you sort of construct a cost function, which is a cost of what your expected payout and KPI is given your incentive portfolio. So the allocation of each different type of inflation. And then you say, hey, we're going to compute a value function which represents sort of if we start at a certain point in space. So that means a certain set of pool reserves, a certain set of inflation.
00:12:51.246 - 00:14:01.370, Speaker A: We compute sort of this minimum which tells us what's the sort of best action we can take given our current state. And this sort of gives you an Iterative update that says, hey, given the value function, we can pick the next inflation schedule and the next distribution of rewards. And then it turns out that it's a very hard equation to solve. But there is sort of this equation that lets you solve for the value function called the Hamilton Jig Hobby Bellman equation. Now this is all abstract and in some ways the reason it's hard is that differential equation, the infimum in, it basically guarantees you no smooth solutions for almost every type of game you construct. But there is one type of HAB equation that actually is very well known and people kind of understand concretely. And it's also the one that's used in, say, like autopilot in a plane, which is something called a linear quadratic regulator.
00:14:01.370 - 00:15:23.890, Speaker A: So just one thing that it took us a little time to kind of read the Ohm contracts, which I don't think they were designed in this way, thinking that they were going to build this linear quadratic regulator. I think they kind of wrote this code and hoped it worked and then got a couple of billion dollars in it, and then they were like, okay, I guess it works. But if you actually read what the code is doing and convert it, it turns out that if you do a coordinate transformation of how the Ohm mechanism pays out incentives, it is exactly this linear quadratic regulator. And so what is a linear quadratic regulator? Well, what it is, is basically maybe I'll just manually point so we have this stochastic differential equation and this value U is the control. So that is how much are we inflating and how are we distributing the incentives in the protocol? And effectively, what you're doing is you're saying, hey, you have a normal geometric brownian motion. So I have the si terms there, and I have this control which is trying to sort of either reduce the variance or reduce the drift. And the quadratic part comes from the fact that the lambda that you use here, you use a quadratic functional.
00:15:23.890 - 00:16:48.990, Speaker A: And so the idea is you adjust your control. So this is your inflation and your distribution in order to try to basically minimize quadratic loss. And in practice, the crazy thing, right, most people in crypto would be like, oh, who cares? This percent staked distribution is not that interesting but you show this to a control theory person, they're like, how the fuck did this work? There's absolutely no reason that the stake percentage should be this stable given that there's a ton of noise in the price process and so that's kind of what led us to try to look at this. So one thing that we showed is, Ohm has this mechanism where you give them an LP share, you give them liquidity, and then they give you sort of ohm, over a time interval. So, like, maybe you lock up your liquidity for a year, you'll get paid ohm, for a year. Kind of like a bond, like a linear interest. And one interesting thing is that for the linear quadratic regulator that Ohm has where you don't really even know what the state of the system is, we basically showed that hey, if you start with sort of random drift and random volatilities as long as you have an increasing number of bonds.
00:16:48.990 - 00:18:05.320, Speaker A: So number of durations of bonds, the Controller converges faster over time and so if you look at the Ohm data, the moment they start introducing bonds, you actually see that basically the price volatility of the asset, obviously its drift has been very negative, but the volatility has been shrinking. And part of the reason is that these bonds act as sort of like a term that sort of slows things down from an inflation perspective. So last few things r1 question is like, hey, this result almost seems like too good to be true, that it's like that simple. Well, one thing is we didn't add in a bunch of constraints and when you add in the constraints, of course you can't really solve this analytically anymore and so kind of seeing how well that works and also note a bunch of the assumptions can be removed over time. Cool questions.
00:18:11.510 - 00:18:14.820, Speaker B: Have you thought about other objective functions that might be?
00:18:16.870 - 00:18:40.300, Speaker A: Yeah, I mean that's a good question. I think in general yes, but only numerically. You're not getting a nice provable result. Right. These grand wall type estimates don't really work I think for generic functions. So out of luck with that.
00:18:47.160 - 00:18:48.580, Speaker B: Similar objective.
00:18:52.430 - 00:19:50.560, Speaker A: Yeah, that's a great question actually. So you could actually view this as the mean field limit and because we're sort of assuming sort of some generic behavior like the distribution of agent actions is effectively the same. Which is why we kind of can assume this kind of stochastic model of hey, I have this SDE Se has some function that's controlling the drift and invariance that assumption there is actually like a mean field assumption. So to Zaki's point the way you'd want to analyze it for other functions is to move away from the mean field thing and perturb by those kind of corrections. But yeah, you should definitely think of this as a mean field know, we're using tons of concentration, equalities assuming like large end limits and stuff like that. Think of this as the mean field result and you want to expand outwards to the different types of agent scenario. Yeah.
00:19:54.850 - 00:20:09.114, Speaker B: Mentioned that whole team kind of backed into this design sort of accidentally various optimization as part of their design. Is there any shared sort of intellectual pedigree variance optimization?
00:20:09.182 - 00:20:35.854, Speaker A: Some of the stuff, yeah. So I think their goal was well maybe not even mean variance optimization, just variance optimization. They just wanted low volatility. What it turned out is like what the bond mechanism basically does some type of mean variance optimization. I don't think they purposely did that. Right. They started with this pure farm staking thing.
00:20:35.854 - 00:21:19.850, Speaker A: It had no bonds, there was like nothing other than whatever 10,000% APY which there's no free lunch in the world. But they didn't actually build kind of the mean variance stuff until they basically started having these huge drawdowns. But what I meant by they backed into it was they were just like oh look, our token price is crashing, let's try to get people to lock up their tokens. And then they made this mechanism. Now did they have some divine insight? I don't think so. I think they literally were just trying to throw things at the wall and see what reduced volatility and that too. It's also not obvious they were doing mean variance optimization.
00:21:19.850 - 00:21:34.240, Speaker A: There's sort of a nontrivial coordinate change you have to make in the way they've designed their controller which is in this paper but nevertheless they kind of ran into it on their own.
00:21:37.250 - 00:21:44.430, Speaker B: Liquidity and leasing way to go instead of like renting.
00:21:45.650 - 00:22:24.480, Speaker A: I mean, I think like the long term thing is you have all three. Right? And I think the interesting thing that Ohm's bond design showed was like they kind of had all three because you could view the staking, like the Ohm locked in staking as a zero duration bond. And that's like effectively renting. Now, of course, it wasn't really providing liquidity to anyone in the protocol. It was just getting locked up and doing nothing. But I think in the long run, it's going to be all three. I don't think it's going to be purely renter, purely buyer, purely leased, and that market conditions will dictate that.
00:22:27.810 - 00:22:28.830, Speaker B: Generic.
00:22:31.840 - 00:23:43.604, Speaker A: There might be. That's why formulating it as this control problem, I think basically lets you say I could design something that takes the bond aspects from ohm, takes sort of the full protocol owned liquidity aspects. Not just like leasing from, say, like a fay or something like that, or orca I guess, now and you can merge them together under the same controller. That's just allocating to how much do we spend on buy, rent and lease? And that's sort of like the natural next step. If you took this paper and wanted to extend it or make a protocol that's new from it, you would basically try to build something that is a controller that's allocating to those three, instead of it just being like, hey, we're allocating to these different pools and we cover this in a section in the appendix. You could have something where you have duration weighted liquidity. So your objective function is not just quadratic, but you say, I value lease liquidity more than rented, and you account for that with like a time dependent factor, like how long it's locked up.
00:23:43.604 - 00:24:19.030, Speaker A: The longer it's locked up, the more it contributes to the objective function. And so you can definitely do that. You unfortunately lose the analytic solvability. So if you've ever done any reinforcement learning or game simulations, you know, that basically it's. Your analytics solutions are basically impossible for these types of things the moment you add any constraints. Anyone else? No? Cool. All.
