00:00:00.330 - 00:00:47.370, Speaker A: The reason that we wanted to do this space is that we've been exploring a lot of different core concepts to modular blockchains and just blockchain infrastructure and research and the cutting edge of what people are building. And so, more recently, we had a Twitter space about sovereign rollups. Then we did one about intents and a natural topic to follow. Follow up from those conversations is about ZK, or zero knowledge proofs. I think everyone has definitely heard of zero knowledge proofs by now. If you haven't, you're probably living under a rock. Zero knowledge proofs are extremely exciting, but they're also not very well understood.
00:00:47.370 - 00:01:20.710, Speaker A: And so we've invited three speakers today to help enlighten us explain what zero knowledge proofs are, how they work, how they're going to be used in blockchain stack and beyond. And so I'm really excited to introduce our three guests today. So we have Brian Redford from RIsC Zero. We have Preston Evans from Sovereign Labs, and Yi sun from Axiom. So, to kick things off, why don't you guys go ahead and introduce yourselves and tell us a bit about your guys'projects?
00:01:24.670 - 00:02:12.042, Speaker B: Sure. So, Brian Redford, CEO and co founder at Risc Zero. And our focus at Ris Zero is on enabling anyone to use zero knowledge technology by leveraging existing code base. So our sort of core open source technology is the Risero ZK VM, which actually implements the RISC five instruction set, which is just a complex way of saying that it lets you run normal code in a ZK verifiable context without needing to program custom circuits. Yeah, our focus right now is on maturing that technology and also bringing our sort of massively parallel high speed proving network to market. That's called Bonsai. Yeah.
00:02:12.042 - 00:02:13.820, Speaker B: Thanks for having me here.
00:02:16.190 - 00:02:19.290, Speaker A: Thanks, Brian. Preston or yee?
00:02:21.890 - 00:02:23.390, Speaker C: Sorry, go ahead, Yi.
00:02:25.730 - 00:02:56.550, Speaker D: Hey, everyone, my name is Yi sun. I'm one of the founders of Axiom. We're building a way to use ZK to provide application specific scaling to smart contract applications. And we're packaging that in something we're calling a ZK coprocessor that allows smart contracts to trustlessly access more data and do compute over that data. And as part of that, we're building an open source library of ZK proof frameworks that we're allowing anyone to use to build their own ZK applications.
00:02:59.150 - 00:03:23.662, Speaker C: Awesome. And then. Hey, everybody, I'm Preston Evans. I'm the CTO at Sovereign Labs. Sovereign is a company that builds tools to help you build ZK roll ups. So, basically, we saw the great work that Brian and many others were doing at the bottom layer of the stack, making ZK proofs more accessible. But when we looked around, the only teams we saw actually using those things in production were extremely well funded and extremely technical companies.
00:03:23.662 - 00:03:49.100, Speaker C: So you've got the 100 million dollar seed round kind of startups and we wanted to solve that problem as well. So what we do is we let you write your business logic in rust, and then we integrate with a number of zkvms, including risk Zero, to convert that into a fully fledged ZK roll up. So basically you act much more like a smart contract developer, but you get deployed as a sovereign roll up running on top of any DA layer that you want.
00:03:51.550 - 00:04:31.480, Speaker A: Thanks for that, guys. I think you guys are all way too humble. I'd love to hear more about your backgrounds. You guys are all extremely smart and knowledgeable in the ZK space and have accomplished a lot. But I think the first thing that I want to do is establish just a baseline background about ZK that we can build off of as we go deeper into the conversation. So I'd love for someone to take a stab at explain it like I'm five on zero knowledge proofs. How do they work at a very high level and what are they useful for?
00:04:36.850 - 00:05:38.482, Speaker B: I did write something down here. I'm actually going for the explain it to somebody like they're five. So I don't know if people have this outside of the states, but if you're sick and you miss school, you're kind of required to produce this note from your parents proving that you're sick, and this gets you out of being in trouble. However, I think that the ZK proof is kind of like this. It basically is portable note that proves something about you in a limited sense, that you can then present to somebody else who can, in theory, quickly verify it to produce some other sort of desired action on the basis of the truth of that receipt. So in this case, this note proves that you were sick to your teacher without revealing any of the details of exactly how you were sick. This is like a way for computers to do this.
00:05:38.482 - 00:05:54.300, Speaker B: Zero knowledge proofs about any kind of data except for you don't need to trust a parent or a teacher. It actually removes any kind of need for a trust relationship there. The sort of validity of the statement is encoded in the statement itself.
00:05:58.110 - 00:06:17.760, Speaker A: I like that. Yeah, we've all, I think, experienced that being sick and needing a way out of it. Do Preston or Yee have any sort of further analogies you guys want to use or maybe expand a little bit about what zero knowledge proofs are useful for.
00:06:19.810 - 00:07:09.310, Speaker D: Yeah, I can talk a bit about why zero knowledge proofs are especially interesting for blockchains. So the core principle here is that when you validate a blockchain, you have to re execute all the computation that's being done by everyone globally. So for a blockchain like Ethereum or roll up on Celestia, that's extremely expensive and has a lot of redundancy. What serenol proofs allow you to do is to have one person or machine execute some computation and prove that they did it correctly. And then everyone else only has to verify that proof, which is much cheaper. So the difference between everyone rerunning every transaction and only having one node generate a proof of validity of that transaction is fundamental gain that zero knowledge proofs give for onchain applications.
00:07:13.970 - 00:07:14.346, Speaker A: Cool.
00:07:14.388 - 00:07:37.282, Speaker C: That was very well put. Yeah. The last thing that I would add is that there are two very cool things about ZK proofs. So one of them we've touched on is what people call sisyghness. That's this property that Yee was mentioning, where it's much faster to check the proof than it would have been to do the computation for yourself. The second thing which we haven't touched on yet is zero knowledge or privacy. You can prove something about a computation without revealing other things that you want to hide.
00:07:37.282 - 00:07:57.680, Speaker C: So kind of the canonical example here is like, maybe you could prove to somebody that you are over 21 and you can buy an alcohol drink without revealing your actual age. Now that's not a very interesting example, because it still relies on a trusted third party. Maybe a more interesting thing is that you want to reveal to somebody that you have the right to spend some money without revealing what your bank account number is.
00:08:00.310 - 00:08:57.886, Speaker A: I like that. So the clear two key features of zero knowledge proofs are the succinctness property, which means that you don't have to actually recompute something on your own, so you don't have to duplicate all this work. You can have one node do the work and generate a proof so that other people can just verify it and be done. And then also there's this notion of privacy that you don't have to reveal the exact details, but you can prove to someone that a certain statement is true. And I think both are extremely relevant in blockchains, because blockchains have this problem of forcing everyone in the network to verify things. And if everyone has to do that redundantly, that's way too much work. And then also blockchains have this problem of sometimes they're public and transparent, and they should be, but perhaps they're too transparent.
00:08:57.886 - 00:09:49.060, Speaker A: And we need to find ways to only share the information that's necessary and be able to keep other things private, because we don't want our whole lives, if we transition things on chain, we don't want our whole lives visible there to continue. Expand on this baseline background. I'm curious if there's any key first principles or building blocks that people should know about zero knowledge proofs and how they work that is helpful mental models so they can kind of reason about them. So are there things that have to do with the cryptography involved? Or maybe there's certain concepts like proving systems or circuits, EK circuits, that people should have some kind of mental model of, so they can understand what's actually happening.
00:09:55.920 - 00:10:52.140, Speaker D: I think it might be helpful to have just an API level view of how you actually can concretely create a zero knowledge proof from a computer program that you might want to run, let's say, on your normal computer. And so there's basically a two step process. The first is you have to translate that computer program into what's called a ZK circuit. So the word circuit, there is an analogy for a physical circuit, for a physical computer chip. And in Zkland, it means that you have to translate your program into a system of polynomial equations. And those equations have to be somehow equivalent to the execution of your program. The second step, once you've done that, is you can apply a cryptographic algorithm called a proof system, to prove that you've actually run the program successfully, and thereby produced a solution to the system of polynomial equations.
00:10:52.140 - 00:11:12.760, Speaker D: So whenever you hear people talk about ZK circuits, these are polynomials over these certain large prime fields that they're actually referencing. So whenever ZK companies build circuits, what is actually happening is that there are specialized ways of encoding certain programs in these polynomial systems.
00:11:15.660 - 00:12:33.600, Speaker B: Yeah, that's an excellent explanation. I'll say from a ZKVM perspective, it's slightly different, because the circuit is actually a machine that interprets other programs similar to an actual processor. So in this case, in the case of, like Risero and many other kind of ZKevM systems, the circuit that's described is a circuit that can take in another program and then produce a proof of the sort of correct execution of that program through a VM. But to the original kind of question, I think understanding sort of applied cryptography and some of the key features of ZK, what does it really mean to commit to something what aspects of a particular circuit are actually private? In what circumstances might you be actually leaking information that you expected to be private? Or where might you not be committing to things that you need to commit to in order to actually be proving what you want to prove within pretty type bounds? So I think this concept of committing cryptographic commitments is really critical.
00:12:40.000 - 00:12:42.030, Speaker A: Preston, do you have anything to add?
00:12:43.760 - 00:13:16.730, Speaker C: I think these two guys covered it really well. The only thing that we might want to touch on really quickly is just some of the terminology that we throw around. So yee mentioned the notion of a circuit. You'll hear people talk about circuits all the time. If you hang around ZK spaces, the other things you'll hear are words like snarks or starks or planck. All of those are that tool that you mentioned, the proof system, which converts a circuit into an actual proof. So snarks and starks and proofs are all just like slightly different underlying mathematical implementations of the same broad idea.
00:13:19.340 - 00:13:57.748, Speaker A: Awesome. That was great background. I think it makes a lot of sense to imagine that you can take your program, express it as a polynomial, and then if you have a solution, I guess to that, I'm assuming it's like an equation. It's very easy to verify that it satisfies those constraints or what have you, and I don't know. I heard this is like my own understanding, but I remember listening to some podcast where the way that the succinctness works is sort of like check. You don't have to check the entire computation. You kind of check it.
00:13:57.748 - 00:14:17.260, Speaker A: It's like, probabilistically true. Like, your proof is not actually 100% true, but because you check it at certain intervals or something throughout the execution, you have a probabilistic guarantee that's extremely high, that the proof is valid. Can someone explain that concept?
00:14:19.840 - 00:14:42.900, Speaker B: I'm going to leave that one to yee. I think I could explain it kind of poorly, but it's roughly correct, though. Yes. It involves error correcting codes and Fiat Shamir as well. To really kind of have to understand both of these concepts, I think, to understand really what's going on there, at least from a stark perspective.
00:14:43.800 - 00:15:27.910, Speaker A: Yeah, well, rather than getting too deep into that, we will save that for a little bit later. That's my own intellectual interest. But I want to also, as just like the last part of background, talk about what are the main applications of zero knowledge technology in terms of the overlap with blockchain. So people talk a lot about things like privacy. Obviously, there are things around, and that kind of also encompasses identity. There's this new field of coprocessing and ZKML. Obviously there's roll ups and bridging and things more on the infrastructure side.
00:15:27.910 - 00:15:38.600, Speaker A: So I'm curious if you guys can give kind of an overview of the different applications that you can see for ZK in crypto and blockchain.
00:15:41.980 - 00:16:16.412, Speaker C: Sure, I'm happy to jump in here. I think you gave a pretty good roundup, actually, of a lot of the things that people are very excited about. Just to sort of briefly mention. This all goes back to those two properties we talked about before, of privacy on the one hand and scalability on the other. ZK is great for both of those things, and it can be great for the combination of those two things. So in general, if you're looking at a problem and you say oh, this is great, but I wish we could have some way to do it privately, then you should probably reach for ZK. Or if you're saying, oh, this is great, but it's too much work because everybody has to check it, then you should probably reach for ZK.
00:16:16.412 - 00:16:43.470, Speaker C: So roll ups are a fantastic example of this. People build roll ups because they want much more scalable blockchains, and people build roll ups because they want privacy. So that's of course the thing I'm biased towards because that's what we do at sovereign. But there's a lot of other really exciting things that I think are developing and will gradually come out in the space. So I know yee has done some interesting work around ZKML. Yee, if you want to chat about that a little bit.
00:16:45.600 - 00:17:42.536, Speaker D: Yeah, definitely. Our general orientation at axiom towards using ZK for on chain applications is to enable developers to do new things that they really couldn't before. And so ZKML is an example of that. So ZKML is a very fancy acronym, and a lot of people are hyped about it, but it just means that you take a neural network or any other machine learning algorithm, and you treat it just as a computer program. And as we're mentioning, you can verify the correct execution of any computer program using ZK. And so in particular, you can verify the correct execution of a neural network, and you could also verify that execution while hiding certain inputs, even hiding the network itself or other fancy iterations. So our vision at axiom is we want to allow on chain applications to be able to use the outputs of these very complex computations that wouldn't really fit into any blockchain VM.
00:17:42.536 - 00:18:13.850, Speaker D: Basically, we don't think it's ever going to make sense for you to train a neural network or run a neural network on this timeshared VM that anyone in the world has access to. Instead, we think that you should be doing that on your own, generating a proof to a blockchain or roll up that you did it correctly, and then sharing the outputs with the whole world. And so within DKML, we've done some work to actually enable that for some of the larger scale neural networks that are in production around you today.
00:18:18.520 - 00:18:24.170, Speaker B: I don't think I have much to add there, actually. I think that's excellent coverage of that.
00:18:26.300 - 00:19:43.410, Speaker A: Cool. So let's transition a little bit more into some of the deeper questions around ZK. Obviously, the title of this conversation is exploring ZK proofs within the modular stack. So there's going to be definitely an infrastructure and modularity bend to some of these questions. But I also want to leave room for lots of just like pure ZK or even things around ZKML, which sounds like a very interesting and powerful use case. But my first question is, what is holding back ZK from more widespread adoption? Is it the cost of generating ZK proofs? Is it the speed? Like the fact that it takes so long and is slow? Is it that the developer experience is kind of difficult, it's hard for people to actually program ZK circuits? Or is it something to do with the security? People don't feel comfortable entrusting lots of value to these systems because they are kind of relatively new cryptographic primitives. So I'm curious if you guys have any thoughts on.
00:19:44.580 - 00:20:42.070, Speaker B: I mean, I think it's really just overall maturity, and I think all of these are kind of factors in what sort of mature technical system looks like. So it's great to see people like Sov building user accessible kind of frameworks that take advantage of, you know, I think, as Preston said, start. It's pretty complicated right now, and so I think there's just some learning that we as an industry have to do in terms of what design patterns are going to be most useful at what sort of different layers of abstraction. And then similarly, you can see a lot of debates going on around the exact security level of various protocols and really debating what's necessary for securing certain amounts of assets. And I think there's just a lot of sophistication of the way the industry talks about these things as well as measures these things, and we're starting to see a lot of that work get done.
00:20:45.880 - 00:21:28.096, Speaker C: Yeah, I think Brian is spot on there. Two things that I would add are first on the security front, I think there is a bit of confusion in sort of the broader community around the security of snarks. There's two dimensions to security, right? There's the security of the underlying protocol that you're using, and then there's the security of the implementation of that protocol. So implementations, it's true, are often very new, and lots of teams are doing great work around making implementations really ready for primetime use. So implementations are new and maybe a bit untested. So there are some very justifiable security concerns on those fronts, especially if you're using a brand new tool. On the other hand, the protocols we're using are not new and risky in the way that a lot of people seem to think they are.
00:21:28.096 - 00:21:57.900, Speaker C: So things like starks are pretty based on hash functions, which are extremely, extremely well studied. Now, sometimes people make riskier cryptographic assumptions in the quest for performance, of course. So your mileage may vary from protocol to protocol, but in general, I'd say the risks are really at the implementation side. And it's more a matter of good software engineering at this point than it is of new research and development, although there is some very exciting new research coming out as well.
00:21:58.050 - 00:22:17.970, Speaker B: And then I guess maybe there's some communication challenges for us collectively to sort of explain some of this to people better. Maybe. I don't know. It does seem like a constant kind of fear that's in the background. Yeah, seems like as an industry, we probably need to develop some educational materials around this.
00:22:18.740 - 00:23:12.950, Speaker A: From my perspective, I always heard people, critics of Zka say, oh, well, security is unproven, and they kind of point to things like hash functions, which are, you never know until they're secure, until they've been around for enough time. And I think maybe hash functions are harder to prove that they are secure than some kind of cryptographic tool like ZK proofs. I think there is a conflation of the implementation security and then the actual underlying theory, security that Preston touched on, I definitely was not aware of that. But obviously, implementation is always risky and error prone. So if that's the case, then I think people which, yeah, there's definitely a communication gap there. Yee, do you have anything to add?
00:23:14.280 - 00:24:06.324, Speaker D: I definitely would echo the concerns around security that Preston and Brian highlighted just communicating that security. I think a different factor is also performance. So I don't know about everyone in the audience, but I remember trying to run a zcash transaction in 2018, and I think it took at least ten minutes, maybe 15, and we've come a long way since then. I think maybe this year or last year was the first time that it felt that CK proofs could concretely reach levels of performance to be useful for the first applications in a more scalable way. And so that's progressing very quickly with the new waves of proof systems that are coming out. And I think there's this trade off between performance and developer experience that the space as a whole is working through. So once we're able to get the best of both worlds, I think it'll be much easier for an ordinary developer to use.
00:24:06.362 - 00:24:07.060, Speaker B: Ek.
00:24:08.120 - 00:24:22.756, Speaker A: Yeah, I totally agree with that. But in regards to performance, it's not just the speed, right? But also the cost. I keep hearing about the cost is huge to, for example, prove one roll.
00:24:22.788 - 00:24:23.768, Speaker B: Up block or something.
00:24:23.854 - 00:24:25.124, Speaker A: But go ahead, Brian.
00:24:25.252 - 00:25:03.610, Speaker B: Yeah, you're starting to see that really drop in a lot of places though, and kind of computations get past this. If it costs $15 to prove one ETH transaction, that's obviously way too much. But you only need a couple like find 100 x improvement before it starts to become reasonable. But we're seeing those kinds of improvements in proving systems all the time, so we continue to make huge leaps and bounds in the performance of our system by parallelizing things. And you're seeing all of this amazing research and the sort of stark based ecosystems as well.
00:25:07.420 - 00:25:45.750, Speaker C: Yeah, certainly cost is a factor for a lot of applications, but in the context of blockchains, what we expect is for the cost of the data you're posting onto the l one to far and away be the dominant factor in your total price. So I think Polygon ZKE EVM is claiming something on the order of like a 10th of a cent per transaction or 100th of a cent per transaction. And we've been doing some back of the envelope numbers, and we certainly think that subscent transactions in terms of proving costs are extremely achievable, even without needing crazy performance optimizations. So cost obviously matters more for web scale applications, but for blockchains, I don't think that will be a factor going forward.
00:25:48.750 - 00:26:33.690, Speaker A: Got it. We covered a couple of these. The security, you guys don't feel as just kind of misunderstood, but this Performance cost and developer experience is a real problem that the industry is trying to tackle and work through. Is there anything where do you think the most gains are to be made? Is it at the proof system level? Is it in deep research? Is it at the implementation level like software? Or is it in hardware optimizations? Where and how do we actually solve the problems that are blocking ZK from being more widely adopted.
00:26:39.010 - 00:27:11.302, Speaker C: I'm sure Brian has an interesting perspective on this, but I think it's a little bit of all of the above. We've seen massive improvements in proof systems over the last couple of years, and I expect that to continue going forward. Specifically, there's some really interesting research around folding schemes that we're keeping an eye on. And then there's also some really interesting research about using even smaller finite fields than we use today. So I think RISC Zero was actually one of the teams that led the charge on this. They use one of the smallest fields that's live today, and there's more work going on on that front. Yeah, feel free to jump in, Brian.
00:27:11.366 - 00:28:24.106, Speaker B: Yeah, I mean, I think it's all of the above. There's pretty obvious potential for some of these advances in the sort of core research to very meaningfully change the sort of scaling properties of some of these systems. On the other hand, just pure engineering seems to yield a lot of really positive results. You see this kind of work lambda is doing with Cairo, and then also just we've been focusing on performance a bit more and very easy to, to find sort of large performance opportunities by just engineering your system more intelligently. And then, of course, there's this sort of core ability to parallelize things, and that's something we've been focused on a lot. And I think also with these folding schemes, you'll start to see the ability to really use a much broader set of much cheaper machines to do very large computations. Although, as Preston points out, if you look at the polygons numbers for blockchain context, we're kind of already cheap enough for most applications.
00:28:24.106 - 00:28:24.880, Speaker B: I think.
00:28:29.570 - 00:29:05.170, Speaker D: One thing maybe to point out is that there's kind of a coupled relationship between the developer experience and performance. Essentially, if you can get performance good enough and cost low enough, then it's fine actually to be a little bit wasteful in your circuit design and precise implementation details so that developers can do things in a simpler way. And so as the proof systems get better and the hardware gets better, we can actually design systems to optimize for developer point of view rather than just eking out every last inch of performance.
00:29:05.710 - 00:30:05.820, Speaker A: You guys are throwing around some things that I think most people aren't familiar with, although I keep hearing about this term folding schemes. Can someone just unpack that a little bit more, and what is its role and what does that mean and why is it important? And I also think it would be interesting to talk a little bit about recursion too, because I know that's a really big part of making ZK proofs more practical. And I feel like there's. I remember in 2018 going to a presentation by the founders of Coda, or formerly Coda, now Mina, and at that time, it seemed absolutely crazy that you might be able to actually recursively prove ZK proofs, but now that's very much a reality. So can you guys explain some of those concepts for people and why they're important?
00:30:09.390 - 00:31:20.770, Speaker D: Yeah, I can take a crack at folding. So folding is a sort of new tool in the ZK toolbox that was introduced in, I believe, early 2022, or maybe late 2021. And it's the idea that you can take two ZK proofs of different instances of the same statement, let's say, applying the same function twice on different inputs, and you can generate a random combination of those two statements and only generate a zero knowledge proof of that random combination. And this can scale up. So I gave an example of two statements, but you can imagine taking a random combination of 1000 statements, randomly combining them together, and only generating one proof. And so the whole idea behind folding, which was introduced in the Nova paper, is to find a way to make that cryptographically secure so that your final proof will actually imply a zero knowledge proof for your original 1000 statements. Now, obviously, there's a huge savings there, because instead of generating 1000 independent proofs, you're at the end only generating one proof for what's called the folded instance.
00:31:20.770 - 00:31:51.866, Speaker D: And so this idea has recently been kind of weaponized in several different instantiations. At first, it was in a very limited context, and now it's been discovered that folding can be applied as a generic tool to many different types of proof systems. And so folding allows for very cheap instances of what's called accumulation, where you take multiple statements and group them into a single thing to be proved. I think maybe Brian or Preston can talk more about other approaches to that that involve recursion.
00:31:51.978 - 00:33:01.534, Speaker B: Yeah, I mean, recursion, as you pointed know, Mino is sort of the pioneer, one of the pioneers of actually building functional recursion systems. And actually, it's still probably one of the easiest ones for programmers to experiment with. But recursion is simply the idea that you can, inside a ZK program, verify multiple other ZK programs. So you can take similar to this idea of folding, if you have ten proofs, you can then pass those ten proofs into a ZK program, which is something that verifies those proofs, and then it will produce, effectively one proof that verifies all ten proofs. And you can kind of nest that procedure as many times as you want to, to get down to a single proof. So we do this a lot in our bonsai service, and we sort of utilize different properties of starks to be able to chunk programs into very small bits that can be parallelized, run on a bunch of different machines. And then we utilize recursion rather than the sort of folding property to produce one succinct proof.
00:33:01.534 - 00:33:11.990, Speaker B: So this is really just kind of nesting the succinctness capabilities of zero knowledge proofs to produce a very succinct single proof.
00:33:19.090 - 00:33:22.562, Speaker A: Preston, anything you want to add?
00:33:22.696 - 00:34:02.234, Speaker C: I think these two guys covered it pretty well. Maybe taking a step back and looking at the very high level from a very high level, folding and recursion are pretty much the same. Rough idea. The idea is that you have a bunch of things that you want to prove to somebody, but you want to do it as cheaply as possible for yourself and make it as easy as possible for them to verify. So instead of making ten separate snarks and then sending them all to that person over to the network, you combine your ten snarks together and then just send them the combined instance. Now, technically speaking, folding lets you sort of skip a certain portion of the proverbial work, but there's a trade off that's involved. You have to use basically like a larger finite field and actually use some group arithmetic.
00:34:02.234 - 00:34:16.930, Speaker C: So there's mathematical trade offs involved, but both of them are the same sort of high level technique. Let's combine a bunch of proofs together and then send the combined proof to somebody, and it'll be just as fast to verify, or actually much faster to verify than verifying all ten of them separately.
00:34:18.390 - 00:35:09.300, Speaker A: Got it. And as I understand, recursion is very important in terms of actually proving the entire blocks of a blockchain, because as I understand, let's say you express your vm as a circuit. There's actually a limit to the size of the input that you can pass to the circuit to prove, but the size of an actual block might be much bigger than that. So then you prove smaller chunks in parallel, and then you kind of recurse them up, kind of like make a tree in which you prove those proofs through recursion into one master proof. Is that the right way to think about it?
00:35:09.910 - 00:35:37.040, Speaker B: Yeah, definitely. Although, as yee points out, folding is a different mechanism of proof aggregation. And as Preston points out, the goal is really just to get down to sufficiently succinct proof for your use case, which obviously, if you're talking about proving state or data availability or things like this between different blockchain systems, making these proofs as small as possible seems to be pretty important.
00:35:41.680 - 00:36:34.830, Speaker A: So, one last question just on the fundamental stuff, and then I want to transition a little bit more into the applications and the blockchain stack and infrastructure is, why is it that certain types of computations that seem pretty easy and straightforward are so expensive to do within the ZK circuit? So, things like hashing. Why is hashing, like the Shaw 256 hash, so expensive? Or even, as I understand Preston, certain kinds of serialization are really expensive to do within a ZK circuit. So what's going on there? And why do we need. People are developing ZK friendly hash functions like Poseidon. Why do we need these things? I just have never really understood what's happening under the hood that makes this so difficult.
00:36:37.760 - 00:37:55.320, Speaker D: The core reason for this goes back, actually, to the definition of a ZK circuit as a polynomial over some prime or over some prime field. So, the problem is that every variable you use in ZK is not sort of a binary number like you're used to on your computer, and actually is at the core of these hash functions, like shot 256, but rather a number modulo some prime that could be small, like is used in risk zero, like a 32 bit prime, or in these elliptic curve based snarks, a 254 bit or even 381 bit prime. And so the problem is just that you have to express a binary operation for hash functions in an operation over this prime field. And so the conversion between the two has extremely high overhead. And maybe to give one cartoon of why this is the case, in some cases, you might be using one prime field element, which perhaps is 254 bits or maybe 381 bits, to represent something that, you know, is a bit so zero or one. So, obviously, that wastes the other 253 bits. And that's the source of most of the overhead for hash functions.
00:37:56.620 - 00:38:12.212, Speaker A: Okay, that makes a lot of sense, because hash functions are all these bitwise operations, just like a ton of them. And that doesn't really play nicely with the sort of fundamental mathematics of ZK proofs. Brian, to go ahead.
00:38:12.286 - 00:38:35.970, Speaker B: Right. I was just going to say. And that's exactly kind of what Poseidon and other kind of finite field friendly hashes kind of move away from these kind of very heavy bitwise operations and more into algebraic operations that can actually be executed in the sort of finite field that everything else is happening in. Not an expert at exactly how all that works, though.
00:38:39.880 - 00:39:42.490, Speaker A: Okay, cool. That was something I've wondered for a really long time. All right, so my next question is also, I think, more directed towards Preston and Brian. But I feel like people have been talking about Zkavms and building ZK rollouts for years now, and yet we only have just finally started to have the first rollouts of actual implementations of the ZKE EVM. Why has it been so hard for people to build, like, ZK virtual machines? Is it because the EVM itself is just not a particularly easy vm to sort of put into this kind of structure? Or is it just, we didn't understand ZK stuff well enough to prove generalized computation? Like, what? What has been the thing that's like, bottlenecked this and is that going to change?
00:39:47.010 - 00:40:15.174, Speaker C: Yeah, it's a really good question. I think we really have hit an inflection point in the availability of these systems. So before, maybe three or four years ago, yee, maybe jump in here. I can't remember when the halo two paper came out, but before then, proving nontrivial programs was basically just impossible. It wasn't like you just needed clever engineering, or you needed to spend more money on compute. It was essentially impossible. That was only a couple of years ago.
00:40:15.174 - 00:41:03.570, Speaker C: And then since then there's been this cambrian explosion of proof systems getting more and more efficient. You had Halo two, and you had plank, and now you've had all the Nova related work. You've had starks, we've had tons and tons and tons of proof systems, which are just way more amenable to proving on the sort of hardware that actually exists in the world. So that's one reason why it's been so hard historically, is just because these proof systems just appeared a few years ago, and none of it was very well understood. They were all academic papers, which are extremely complex, and so really only phds were working on these sorts of things. So it just takes a long time to actually build implementations and all that sort of stuff. So there was that whole factor, and then there was also the whole factor of the EVM being a pretty snark, unfriendly vm.
00:41:03.570 - 00:41:40.014, Speaker C: I think it's gotten much less snark unfriendly in the last couple of years, because snark systems have gotten so much more capable. But before, a couple of years ago, the EVM was kind of like, it was kind of unimaginable that you might try to prove the EVM. So basically the technology just came so far in the last couple of years that suddenly these things are just way more feasible than they've ever been. The other answer is that people got excited. And it turns out when you put a lot of smart people on a particular problem, it's often easier than you think. And so not too long ago, there was no one working on ZKe evms. And then a couple of years ago, suddenly everyone was working on zkevms.
00:41:40.014 - 00:41:48.966, Speaker C: And a couple of weeks ago, suddenly we had three or four first ZKe evms, all within a week of each other. So I think that's a lot of what happened.
00:41:49.148 - 00:42:30.882, Speaker B: Yeah. In a certain sense, the EVM is as hostile, if not more hostile to ZK proving than these kind of old school hashes. Right. It was never designed to be turned into a computer or a ZK proving system. So I think it was just always a huge challenge for ZK. But I think part of the reason ZK has advanced so much over the past couple of years is exactly because people really wanted to prove the EVM with it, and it just required many advancements along lots of different axes and just massive improvements in code quality and optimization across the board. So now we kind of have that.
00:42:30.882 - 00:42:38.920, Speaker B: So it's just a huge set of new things is going to be possible with ZK over the next couple of years.
00:42:43.790 - 00:42:45.580, Speaker A: Specifically, though, go ahead.
00:42:45.950 - 00:43:24.234, Speaker D: I just wanted to add one last part here about that. This illustrates a tradeoff between the developer experience and the difficulty of snarking a vm or putting a vm in ZK. So if you look back at Cairo from Starkware, I think that was probably the first ZK virtual machine that got any level of usage. But Cairo, if any of you have looked at it, is very adapted to ZK fundamentally. And so that's a lot easier to implement. But that means that the developers using Cairo have to know a little bit about ZK. Whereas if you look at ZKE evms, developers really in principle are not supposed to know that there's even ZK going on.
00:43:24.234 - 00:43:32.860, Speaker D: You just write EVM code. And so I think those are almost on polar opposite ends of the spectrum of difficulty and developer experience.
00:43:35.550 - 00:44:48.174, Speaker A: That's a great point, and I want to dig a little bit more into that, because from what I've seen, there's a couple of different approaches to making ZK vms practical and also have friendly developer experience. So on the one hand, as yee, as you mentioned, there's approaches that the Starcore team have taken, which is, hey, we're actually going to build our own domain specific language and our own vm that's tailored to ZK. And that way it'll actually play nice with the proof system and all that stuff. And maybe we can also design it in a way that's just more optimized, essentially. But then we've also seen other approaches, and I think Risero is really a good example of this, where you can use an existing vm or instruction set that a lot of different languages can compile down to, and just build a proof system or proverb for that. And that way developers don't have to learn a whole new programming language. They just kind of use what they already know and it can be made compatible.
00:44:48.174 - 00:45:08.890, Speaker A: Maybe like RISC five is not the most optimized vm for that, but perhaps that's a fine trade off for the fact that the developer experience is better. How do you guys see these two approaches? Is this the right way to think about it? Are there more approaches and what are the trade offs?
00:45:12.750 - 00:46:09.870, Speaker B: Yeah, I mean, there are definitely lots of approaches to doing to building developer friendly pathways to ZK, certainly. While RISC five, I think it's pretty well suited for ZK. But obviously the really cool work by the MIdN team and the Triton team and all these other people building very ZK specific and Cairo itself, obviously all very valid approaches, I think risk five strikes a pretty reasonable balance. But you do see still the need to provide access to the underlying proving system for certain operations, like hashing functions or elliptic curve cryptography. So we have one perspective, which is focus on the general purpose and provide access to acceleration primitives for other people to build these small accelerators and kind of attach them to this general purpose system. But there are lots of other approaches.
00:46:15.910 - 00:47:01.850, Speaker C: Yeah, I think we'll come to look back on the current era of snarps, or maybe now it's already the previous era of snarps, but we'll kind of look back on this the same way we look back on people like handwriting things in assembly code. Right. So in the early days of computing, performance was so bad that you really did need to handwrite your assembly. And also the tools like compilers just weren't advanced enough to generate optimal code for you. Now, we're getting to the point with ZK where, yes, you could get better performance by handwriting things in many cases, but for almost everything you want to do, the vast majority of your logic, it's still going to be cheap enough that you don't really care. And then for a few specialized things, like Brian was saying, you probably do want to implement sort of the cryptographic equivalent of an ASIC for this particular operation. So we're really excited about those approaches.
00:47:01.850 - 00:47:26.840, Speaker C: At the end of the day, we think developers kind of are the ultimate key to getting these things in people's hands. So we're really excited about developer friendly tooling. Now, the other great work that's being done, though, and people like Starkware and Lambda class are doing great work as well on making the more ZK friendly environments more developer accessible as well. So the gap between those two systems is slowly narrowing, and we're excited to see what comes from both ends of it.
00:47:31.390 - 00:47:33.980, Speaker A: Yee, was there anything you wanted to add on that point?
00:47:34.750 - 00:48:01.460, Speaker D: Maybe just wanted to echo Brian's point that getting the interface right between these more generic part of the system, where maybe performance is not as big a concern, and these specific computations like hashes or elliptic curve operations, or even things like ZKML, where performance is quite critical and does at least right now, need to be hand optimized, is one of the bigger challenges right now in the space.
00:48:02.870 - 00:48:14.310, Speaker A: I see. So there's kind of merit to both approaches, and maybe there's like a hybrid that allows kind of the optimal trade off. That's very interesting.
00:48:14.380 - 00:49:10.330, Speaker B: Yeah, you're starting to see the validia effort by Daniel and other, and I'm not sure who else is working on that know Delendom's working on. You're starting to see people kind of evolve this notion of what a ZK system is and starting to know more and more, kind of unsurprisingly to me, like, kind of like patterns we already see in actual computers. So people are starting to think of actually having kind of the equivalent of buses to connect these different components. And to Yee's point, I think ZKML, if this becomes kind of an important part of how all of these systems operate, I think it will. You're going to definitely have the moral equivalent of an Nvidia chip that's sort of attached to your ZK proving system, specifically focused on density linear algebra.
00:49:13.310 - 00:50:09.100, Speaker A: Whoa. So there's going to be different components that can communicate with each other, and they each kind of have different functions, sort of similar to how you would have that in a computer architecture. That's super interesting. And you guys keep on talking about hardware. And one of the things that I understood at least about Visira's approach is that, for example, or just generally like this, taking an approach where you use a more common sort of instruction set, is that it plays nicer with hardware optimizations. But in general, I kind of want to open up the topic of what role does hardware acceleration play in making ZK proofs like the performance with the speed and also the cost better? Because that does seem like something that people talk about a lot, but I really have no idea exactly what's going on there.
00:50:12.860 - 00:51:25.440, Speaker B: I can talk a little bit about. Yeah. So certainly we picked risk five for a bunch of reasons, none of which actually had to do with potential suitability for hardware acceleration, although we're starting to see witness generation become a bottleneck in certain points of the system, in which case it's probably kind of nice to have a more actually physically friendly kind of microarchitecture as your primary kind of circuit. That said, I think that the advances in folding and sort of recursion and continuations and these kind of things are really going to be the thing that makes hardware. The ability for hardware confirms to deliver chips much more of a reality. When you require like 500gb or to a terabyte of ram, it really massively complicates the process of building an effective accelerator. But if you can push those memory requirements down, then you can start to do like HBM, interpose directly on top of the logic, and you can probably start to achieve really kind of astonishing performance improvements.
00:51:25.440 - 00:51:48.840, Speaker B: I think prior to these kind of techniques existing, I was kind of a believer that gpus would be the primary way in which proving systems are accelerated in the three to five to ten year range. But I'm really starting to actually become pretty optimistic that there will be some specific hardware that's going to provide some really amazing capabilities.
00:51:52.000 - 00:51:56.620, Speaker A: So you're saying that they're going to be asics, rather than just gpus?
00:51:57.520 - 00:52:08.640, Speaker B: I think there will be some Asics in the next one to three years that actually provide meaningful performance bumps for several different sort of proving systems.
00:52:14.140 - 00:52:16.744, Speaker A: Preston, are you any thoughts on hardware stuff?
00:52:16.942 - 00:53:14.350, Speaker D: Yeah, I think with hardware, the core challenge is that the proof backends basically the proof system, part of generating user knowledge. Proof actually involves some pretty heterogeneous operations, and so those are also changing all the time as proof systems get more evolved. Maybe we're using different types of prime fields, different types of elliptic curves, or different types of hash functions. And so I think the challenge for hardware companies is to be able to develop a system that's generic enough to not get outdated in, let's say, one year, but still custom enough to actually deliver performance. And so a big part of why gpus have been really popular is, of course, you're not taping out a GPU so you can actually evolve as the proof system evolves. We're also seeing companies work on fpgas, which are sort of intermediate between a GPU type solution. You can program your FPGA, it's much more difficult.
00:53:14.350 - 00:53:27.520, Speaker D: And the endpoint, of course, would be an ASIC. But I think the challenge there is how can you make your ASIC generic enough to accommodate some change in the proof system? Although maybe you can't hope to get something fully generic.
00:53:29.620 - 00:53:57.560, Speaker A: So it sounds like because the proof systems and the underlying technology is not yet hardened, or sort of like at a level of maturity where we can kind of use it as a foundation, it's a little bit hard to sort of make these lower level optimizations and hardware. And I bet that's probably representative across the other parts of the stack, too.
00:53:57.630 - 00:54:18.930, Speaker B: Yeah, absolutely. Exactly. As he says, the sort of trade off between making a big bet now and the sort of uncertainty and the evolution of the field is pretty difficult spot to be in as a hardware company. That said, have seen some impressive work coming out of various companies.
00:54:20.420 - 00:54:47.370, Speaker A: How far are we away from that point, do you guys think? From where we've hardened everything, there's maybe a limited set of proof systems that everyone's like, okay, we've reviewed on this kind of maybe. I don't know if this happened in the same way with hash functions or something, but now everyone just uses shot 256, pretty much. Are we going to reach that point? If so, when will we get there?
00:54:52.940 - 00:55:31.604, Speaker C: I would say as somebody who uses these systems, I hope we're still far away, because the reason we keep moving is because we keep finding things that are so much faster. So it's not about security. We're not migrating because things get broken the way we did for hash functions. Like we kept moving because Sha one got broken, so we had to move to sha two. In these systems, we're still finding very meaningful performance gains so big that they balance out even the gains you can get from hardware acceleration. But that being said, the field has gotten incredibly mature over the last couple of years compared to where it was. So we might be only a year or two away from having something that we've hit where we think the limits are and the performance.
00:55:31.604 - 00:55:36.490, Speaker C: You can get to the point where hardware starts to matter more than new systems do.
00:55:39.740 - 00:57:03.148, Speaker A: Super interesting. I want to talk a bit about the intersection of ZK proofs and specifically blockchain infrastructure and modular blockchains at Celestia. We're close to launching Mainnet, but we already have some ideas of how we want to improve the core protocol for our next iteration after we've launched. And a lot of them actually involve zero knowledge proofs. One of them is that we want to add a ZK sort of verification opcode to our core protocol, and that will allow for trust minimized bridging from Celestia up to the roll ups. And we also have ideas for how we can use ZK proofs to prove the correctness of the erasure coding of our extended block data, as that's really important for the security of the data availability sampling scheme that we use and make latency better and also prevent, basically the need for our battery erasure coding fraud proofs. And then we also have another one, which is unfortunately or fortunately, we use Shaw 256 as the way that we hash our data route.
00:57:03.148 - 00:58:16.490, Speaker A: And that means that if you want to prove inclusion of data in a celestia block inside of a ZK circuit, it can be really expensive. And so we have this idea that we could use some kind of service to generate an equivalent data route, but in a ZK friendly hash function like Poseidon or Peterson, and then we can have a proof that proves that those two different data route hashes are equivalent to each other. So those are a few of the improvements that we have. But in general, it just seems like ZK, there's almost like no bound on the different applications that the ways that it can be applied to improve blockchain infrastructure. So I would love to hear where you guys think are the most, like the highest leverage places. I know, for example, sovereign. You guys have a vision for doing proof aggregation across a bunch of the roll ups in the sovereign SDK ecosystem, and then they can have this succinct, sort of like many to many bridging, but through just one proof, for example.
00:58:16.490 - 00:58:31.772, Speaker A: Anyway, I'd love for you guys to talk about where you see the most exciting applications of ZK in the infrastructure side of crypto. Sure.
00:58:31.826 - 00:59:02.836, Speaker C: Yeah. Happy to jump in here a little bit. And Brian, feel free to speak as well if you have things you're excited about. But yeah, at Sovereign, we're very excited about, as you mentioned, the application of ZK to interoperability. So I think there's kind of two different challenges you have with interoperability today. One of them is trust, and one of them is cost. So when you're bridging between two chains, say like two cosmos chains, for example, you have a fundamental problem, which is that one of the chains can't validate the other chain's logic.
00:59:02.836 - 00:59:37.270, Speaker C: Right. If you did that, you would just have one bigger blockchain with more expensive blocks. So you fundamentally can't validate the other chain. All you can do is you can check that their validator set has signed off on some signature. And so in most cases, it turns out that's good enough. Right? The trouble is, if the validator set ever lies, then they can steal money from the bridge. And so if you have a world with hundreds or thousands of blockchains where some of them have very small market caps, suddenly you have to be very careful about who are you bridging to because if you bridge to the wrong person, they could steal your money.
00:59:37.270 - 01:00:15.728, Speaker C: So ZK just completely solves that problem, right? You can check once that they have rules that you're comfortable opting into. And once you've done that check, you can check forever that they are following their own rules. And so you have no opportunity for loss of funds. The other property, like you mentioned, is this succinctness that we keep coming back to. If you're bridging between two different blockchains, say in cosmos, for every other chain that you want to bridge to, you have to verify that their validator set is correctly producing blocks. And that means checking a bunch of digital signatures inside of your blockchain's vm. So for one or two chains, that works pretty well.
01:00:15.728 - 01:00:52.750, Speaker C: But if you start to think about having 100 or 1000 bridges, suddenly you'd be spending all of your time just checking signatures to operate these bridges and you wouldn't have any time left to do computation. And the number where you hit that point is very small. It's like maybe low double digit number of bridges before your chain is completely unusable. With ZK, like we talked about, you can aggregate all the proofs together and suddenly this is a complete non issue. You just check the proof once and it takes, in native terms, maybe a few milliseconds to validate that, say, 100 or 1000 other roll ups are following the rules that they prescribe for themselves. So those are two things we're very excited about.
01:00:55.440 - 01:02:05.780, Speaker B: Yeah, I think the thing, as you pointed out, CK touches I think almost everything in this space and can make almost anything better. I think the thing I'm really excited about right now is to see the kind of combination of optimistic approaches with CK approaches. So you can get the benefits of an optimistic system without a lot of the downsides of needing to build this kind of separate fraud proof mechanism. You can kind of let ZK handle a lot of the sort of heavy lift and then you can deal with the fact that ZK is maybe a bit slower than you want your system to be. I think SOB is actually doing some work on that front. I don't know, Preston, talk about your one shot. The optimistic stuff you're doing would be really interesting, but I think seeing that be applied know potentially stuff Celestia is doing and sort of what optimism's looking into with their latest RFP is really interesting and going to provide a lot of capability without a lot of capital intensive research and development.
01:02:08.600 - 01:02:09.268, Speaker A: Yeah, absolutely.
01:02:09.354 - 01:02:59.076, Speaker C: That's a fantastic point. So maybe to give a little bit of context for listeners who may not be deep in the weeds like we are, when you're building an optimistic roll up, you have this sort of fundamental problem that it's expensive for some other blockchain to verify a proof that fraud has been committed. So let's take the case of optimism, for example. They're running a full Ethereum block as a roll up on top of Ethereum. So anytime there's fraud in the naive implementation, the Ethereum blockchain would have to re execute basically a whole other Ethereum blockchain in order to check whether or not fraud had been committed. And that's just way too expensive to ever work in practice. So what optimism and arbitrum did was they came up with this clever interactive on chain game where they play back and forth between a challenger and the guy who claimed that a certain transition was made.
01:02:59.076 - 01:03:59.860, Speaker C: And after maybe 50 rounds of on chain interaction, they can find the exact location so the exact assembly instruction of the cpu code, of the code, they can find the exact assembly instruction where the two players disagree with each other. And then the EVM just executes that single assembly instruction. So that's a really cool trick, but it has this super big drawback, which is that it takes a long time because you have to send maybe 50 different messages onto the blockchain, and any one of those messages might get censored. And so you need to wait a long time to make sure that everybody who wanted to challenge this could challenge a state transition. So one of the things we're working on at Sovereign is just remove this interactive game entirely. So basically, if you've written your code using the sovereign framework, you already have all the capabilities to generate ZK proofs. So now the idea is, okay, sure, take your blockchain, but instead of making the proofs all the time, just make proofs when there's a dispute about what happened in a particular block.
01:03:59.860 - 01:04:26.960, Speaker C: And that way you never have to pay any proving costs, at least not on the happy case and you can design the economics in a cool way to make sure that the only way you ever have to pay the proving costs is if someone else paid them for you, because you can slash whoever lied about the state transition. So you can avoid ever having to pay any proving costs, but still have the full security or the security that you expect, and significantly reduce the latency that you'd have to wait for a block to finalize.
01:04:30.450 - 01:05:32.480, Speaker A: ZK fraud proofs. Yeah, I remember first hearing about that idea, from at least the first time I heard of it was Mustafa brought it up in a conversation in October last year, and I was like totally mind blown. I hadn't ever thought of the fact that you could actually ZK prove the validity of a fraud proof. And then you get the best of both worlds, where basically you don't have to prove every single block, but you also don't have to worry about these very big fraud proof sizes that are difficult to verify. So I'm a huge fan of this idea. Yee, is there anything that you wanted to chime in there? I know coprocessing is something that you're working on, and it seems like very relevant to the fact that it expands the capability of what blockchain infrastructure can do. So feel free to chime in here on where you see ZK plugging in.
01:05:33.090 - 01:06:37.670, Speaker D: Yeah, what we're working on at axiom is offering a way to, I guess, modularly expand the set of capabilities that are possible in a blockchain VM by allowing contracts to make asynchronous calls to off chain actors without adding additional trust assumptions. So we see that coming in two forms. First, we allow contracts to access more data trustlessly from the chain that they're actually on. And second, we let them actually compute on that data to perform computations that might not be possible within a blockchain VM, either for cost reasons now, or just for fundamental scalability reasons, if the computation is large enough. And so we think this breaks the current model, where all blockchain computations are fundamentally synchronous. That's very expensive from the blockchain validation point of view. And so by relaxing that assumption, we're hoping to enable smart contracts to do many more types of operations without adding additional trust assumptions.
01:06:42.500 - 01:07:42.740, Speaker A: That's absolutely huge. I want to talk just a last point on the infrastructure thing is, what is the relationship between zero knowledge proofs and data availability? Obviously, there's a very clear connection with roll ups because you need the data behind the chain, you need the minimum amount of data to be able to reconstruct the state of your roll up and data availability is the way to ensure that that's the case. But are there other interactions between data availability and ZK proofs in some of these other applications, like maybe in coprocessing or ZKML? I don't know. I'm curious if there's more ways in which those two things interact, obviously, because at Celestia we are building a data availability layer.
01:07:46.440 - 01:08:48.760, Speaker D: I think this ties into some of the specifics about what the guarantee of data availability actually is. So my understanding is that we say that data is available if it was possible at some point for anyone to download that data. But what that doesn't guarantee, and we're probably going to see this more and more as more data is available, is that in one year or in ten years, you'll be able to access that data. And so there's a proposal across blockchains of having stateless blockchains, which means that when you send a transaction, you actually have to prove that all the data you access was somehow validly in the blockchain. And so this could apply to historic data or even data in the current state. And so what ZK can allow you to do is to compress the proofs that all of the data you're accessing actually is committed to in the state, and that allows you to access much more data. And I view it as almost complementary to data availability.
01:08:52.140 - 01:10:06.272, Speaker A: Absolutely. That's such a good point. And sort of like an extension expansion of kind of what I was talking about earlier, as one of the things that we want to help make proving inclusion of data in celestia cheaper by having a more friendly ZK hashing data route. But I think there's a lot more, like you said, and I think, Preston, you've written about this when we were talking about adding this trust minimized, bridging ZK verification opcode, how you could be able to prove the inclusion of data, like arbitrary data, arbitrarily back, far back in the past, essentially, which sounds related to what YI is talking about. But anyway, I think that's kind of a bit of a rabbit hole, unless, Preston, if you think that's something interesting to talk about. Otherwise, I think the last kind of connection between the modular stack and ZK in my mind is this idea of proving networks. And this is something that I'm kind of new to.
01:10:06.272 - 01:11:23.770, Speaker A: But we've seen one of the thesis behind modular blockchains and the modular stack is that we're going to start to build these decentralized services that perform certain functions necessary to build these trust minimized systems that we call blockchains. And so we started out with just kind of two basic types of services, or layers. One is the data availability and consensus layer, like Celestia. Then we have execution layers like roll ups, like optimism, and all the starquare and all the ones that we're familiar with are sovereign. But now we're even expanding that even more with, for example, things like shared sequencer networks, where all of a sudden you can outsource sequencing to this decentralized sort of service or layer. And to me at least, that's my frame of mind when I hear about these proving networks as I understand them. It's sort of like a network of people that have the hardware necessary to prove certain computations, generate proofs for you, which can be expensive, and you need resources to do that.
01:11:23.770 - 01:11:38.590, Speaker A: So it kind of becomes this decentralized marketplace for proving. And so do you guys see that as something that's part of the modular stack? Also, maybe you can explain what these proving networks are in the first place and how they would work.
01:11:38.960 - 01:13:20.480, Speaker B: I mean, there are a couple of them already out there like equals nil. They have theirs live. And I think Delphinus Labs for their ZK wasm also has like a sort of nation proving market. I think as we see demand for ZK proofs hopefully massively expand over the course of the next couple of years, as ZK coprocessing becomes, I think, a major theme in how work gets done in these systems, you will start to see collectives of people that can actually bring serious infrastructure power to this market in a way that is probably possibly more cost efficient than relying on big tech, but certainly more scalable and more able to respond to various widely different compute kind of needs in the ecosystem. Because we've seen, at least historically, huge spikes have really negative impacts on pricing and user experience. And I think by pushing more computation off chain to enter CK coprocessors, and by enabling people to kind of participate in these proving networks and really kind of instantly decide to start performing proofs for money, you're going to see a much more resilient compute fabric evolve over time. But beyond that, I think there are still huge opens around the economics of how the system's going to work and the sort of breadth and scope of which proving systems will support and all of this kind of stuff.
01:13:20.480 - 01:13:25.010, Speaker B: But I do expect it to be like an important part of the future, for sure.
01:13:28.260 - 01:13:37.590, Speaker A: Preston or e, do you have any thoughts on this? I'm curious, why does it need to be a network, and why can't it just be a centralized service or something?
01:13:37.960 - 01:14:14.640, Speaker B: I think it can be. And Bonsai right now is very centralized, and there are potentially benefits for that. And some of those could be related to privacy and letting people produce proofs on data that they control, on infrastructure that they control. But there's something kind of magical about if the hardware requirements can get low enough allowing anybody to kind of participate in this network, and it's really just its ability to soak up demand anywhere in the world and also help power these kind of censorship resistant, hyperstructure type systems.
01:14:16.740 - 01:14:24.400, Speaker A: Got it. So I'm not wrong in thinking of it as possibly another layer or service within the modular stack?
01:14:24.760 - 01:14:38.010, Speaker B: No, I don't think so. Yeah, it seems very much like a type of execution layer that's different than, I think, what people would have called an execution layer in the past.
01:14:41.740 - 01:14:44.970, Speaker A: Preston Yee, do you guys have any other thoughts there?
01:14:45.820 - 01:15:38.350, Speaker D: I think, to add on to what Brian said, I definitely think one thing that will affect the shape of how these markets evolve is the actual aggregation properties of the proof systems. And one reason why probably the market structure hasn't really stabilized is that the proof systems are evolving so quickly. And the basic problem there is, as Brian mentioned, the resource requirements vary with what proof system you're using, but also the way that is most optimal to chop up a large computation into smaller computations and do recursion or aggregation is actually somewhat different between, let's say, folding or stark pace recursion. And as these things evolved, they actually can imply dramatically different forms of market structure. So it'll be really interesting to see how the different proof marketplaces handle that.
01:15:44.330 - 01:17:04.030, Speaker A: Very interesting. Well, we have about ten minutes left, so I want to start wrapping up. So I have two more questions. My first one is a little bit more of a philosophical question, and basically what it is is why do we need to prove things and verify things in the first place? And the reason I bring this up is just that at Celestia we're big believers in the importance of verifying things, rather than just trusting third parties or committees. But sometimes it feels like a lot of people just aren't really aware of the fact that's really the core of the blockchain security and decentralization that we are all here for and trying to build. So that's why we think data availability sampling is really important, rather than using a data availability committee or an l, one that doesn't actually support data availability sampling. We think roll ups are so great because you can, using very light hardware, actually verify the execution, rather than just verifying that there's enough signatures on a block header.
01:17:04.030 - 01:17:20.210, Speaker A: So I would love to hear from you guys. Why is verifiability, why are proofs such an important part of what we're doing and what we're building in blockchain and crypto?
01:17:25.820 - 01:18:09.940, Speaker C: Sure. Happy to jump in on this one. It's kind of funny that we even have to ask this question, because if you look back at history, the history of humanity is the history of people being able to work with larger and larger groups of people. So if you think way back, like the good old sapien story, in the early days of humanity, it was maybe your family. And before humanity at all, maybe it was just individual organisms sort of looking out for themselves. And then you get the family structure, which is much, much more stable than individuals, right? Families could easily outcompete a single individual trying to fend for themselves. And then you get tribes, which are all genetically related to each other, and they can outcompete small families.
01:18:09.940 - 01:18:55.768, Speaker C: But that's where it stops, because humans can only maintain, at most, a few hundred interpersonal connections. And so in order to cooperate with someone, you have to be able to trust them, right? In all of your actual meaningful relationships, there's asymmetric downside. If somebody really wanted to hurt you, they could very easily do something which would be catastrophic to you. So you fundamentally, you really have to trust these people. Well, one of the next big stories in human history is cooperation through memes. So that could be shared government, it could be shared religion, it could be shared ideals, but cooperation together through shared ideas. And suddenly, even though, Nick, you and I are not genetically related, and honestly, up until a year ago, we didn't even know any of the same people.
01:18:55.768 - 01:19:38.710, Speaker C: But because you and I share a lot of ideas and we work in the same space, we're able to work together in a great way. But still, we're relying on sort of on social proof, on things that are not ultimately scalable. And so if you look at the world know, cooperation kind of stops along the boundaries of nations. So, like, I can't just Venmo somebody in Argentina, right? And so what proofs do is they let you scale trust, because suddenly there's no social component left. It's all just purely mathematical. And everybody in the world has the same laws of physics and the same mathematics, and so suddenly you can cooperate with people that you have nothing in common with. And you can know that there's no way for them to break this trust relationship that you have.
01:19:38.710 - 01:19:46.650, Speaker C: So fundamentally, that's why we're so excited about proving. And then, of course, there's lots of concrete applications that we get to talk about all day long, which is fun, too.
01:19:48.700 - 01:19:57.320, Speaker A: Damn, that was a mic drop right there. Seriously, Brian. Go ahead, Brian.
01:19:57.660 - 01:21:15.360, Speaker B: I was just appreciating Preston's explanation. I think that was amazing. Yeah. The only thing I would add is that in addition to everything Preston said, these systems that rely on kind of macroeconomic security kind of games like staking and slashing and all this stuff, it's amazing that these work as ways to prove sort of what happened, but it's extraordinarily complex, and there are a whole bunch of potential failure modes and odd interactions and stuff that I think fundamentally limit the complexity of the kinds of applications you can build with these systems. So I think just in order for blockchains and decentralized systems to sort of compete with their centralized ancestors, I guess they kind of need to reduce the complexity of analyzing a lot of the more interesting interactions. And ZK gives us a path to do that and build more complex systems in ways that the brains of the people building them can grok the sort of emergent behaviors of the system in a reasonable manner.
01:21:23.420 - 01:22:18.520, Speaker D: I think what Brian and Preston said are both really important properties of decay. I would only add one thing, which is when you compare something guaranteed by your knowledge, proof on a blockchain, to a guarantee from, let's say, a government regulator, the government is fundamentally running an optimistic system. Namely, if you perform some action and it's fraudulent, they offer the guarantee that maybe later on via audit, they'll catch you and you'll be punished. That's sort of an incentive to not do commit fraud. Whereas if you have to give a zero knowledge proof, or just any type of proof that your action is actually legitimate, then I think that offers an even higher level of guarantee that the system is behaving as planned. And so I think that's a big step towards achieving the goal of blockchain staffer, a more objective point of view on trustless systems.
01:22:20.540 - 01:23:16.510, Speaker A: That's a great point. And even it could be turned around the other way. Rather than governments enforcing rules on people, people also enforcing sort of the rules on governments, like government officials being forced to prove that they actually carried out things in the way that they claim. I love this vision stuff and the social stuff. That was a great answer from all of you guys. So to wrap up, my last question is just where will zk, where will blockchains and where will your project be in five years if we're successful in some of the things that we're attempting to do? And in general, do you have any final takeaways for the audience as we wrap up?
01:23:24.880 - 01:24:13.772, Speaker C: I think, like everything in the world, it's going to be a story of gradually and then suddenly, right now we're in the phase of gradually people are adopting ZK proofs. Gradually people are deploying roll ups, and a year or two from now, it's suddenly going to be everywhere. So five years from now, we envision a future where many, many things, probably almost every app on your phone, behind the scenes is doing something that touches a blockchain, something that involves zero knowledge. Proof, right. But the magic is that in five years, it'll be just like the Internet. Every app on your phone uses the Internet, but you don't think about the fact that you're using the Internet, you're just thinking about the apps you're using. We hope, and we're working very hard to make sure that that future is five years or less away and we should have production systems, hopefully soon.
01:24:13.772 - 01:24:14.540, Speaker C: TM.
01:24:17.840 - 01:24:22.540, Speaker A: And what about sovereign and any last takeaways for the audience?
01:24:26.160 - 01:24:39.910, Speaker C: I mean, if you're interested in this sort of stuff, we would love to chat. We are always looking for people who are interested in building applications. If you've wanted to use ek, but it's been too hard. Come talk to us and yeah, come let's build together.
01:24:44.680 - 01:25:54.270, Speaker B: Yeah. I think five year vision is always hard, but I hope that, and really believe that five years from now we're going to live in a world where big tech hegemony over kind of like every aspect of our digital lives is over. And it will be over because we've built this trustless set of systems that can interact with each other and that are based on these very resilient networks such that we can actually trust our digital identity to this fabric, sort of resilient fabric of systems. By having that kind of core rooting of our digital identities in something that we actually kind of own ourselves and get to control how it's used and the policy around it, I think that's going to enable a huge set of new applications that are going to be like pro social instead of antisocial. And I hope that Rishero is a huge part of powering the computations behind these systems. Yeah.
01:25:58.400 - 01:26:01.740, Speaker A: Love it. Yee. Any last thoughts?
01:26:02.560 - 01:27:22.600, Speaker D: Yeah, I think for ZK, more broadly, the metric I think is most interesting is sort of what's the ratio between the cost of giving a ZK proof for a computation and the cost of executing that computation on your computer is in a normal way. And I think that ratio right now is pretty high, maybe between 10,000 and a million, depending on how you compute. But we're going to see that fall more and more. And I think within five years, I'm hopeful we'll be within sort of the theoretical limit, maybe 100 or even less. And so once that happens, once the cost of generating ZK verifying anything drops, well, it'll become a question of why aren't you ZK verifying your computation, instead of why are you? And I think we're going to see trust minimization extend to many different types of applications. And we're hopeful that there can be hybrid on chain and off chain applications that offer the sort of liveness and consensus guarantees of a blockchain while actually having the same performance and rich experiences of normal applications today, sort of empowered by ZK. And so our goal at Axiom is to be part of that transition between purely off chain and partially on chain off chain.
01:27:22.600 - 01:27:29.560, Speaker D: We're launching pretty soon with the first steps for purely on chain applications today, so you can check us out at Axiom XYZ.
01:27:31.840 - 01:28:08.470, Speaker A: Amazing. Well, I am so grateful that you guys came on today to share all this knowledge with us and the listeners. There's, I'm sure, a lot more to look forward to in the future from all of your projects and from ZK and the modular space as a whole. And we're going to keep doing more of these spaces to uncover and talk about the more technical aspects of blockchain and crypto. So please join us next time. And until then, Brian Preston Yee, it's been a pleasure and an honor. Thank you so much.
