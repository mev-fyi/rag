00:00:01.850 - 00:00:35.042, Speaker A: Welcome everyone. My name is Rohan. I'm one of the senior research engineers at Sovereign. I work with Chen and yeah, welcome to the workshop. So a little bit about the agenda. I'll spend a little bit of time talking about sovereign and some general stuff that a lot of people might be familiar with, but it's still worth creating. We'll go through the bulk of the demo would actually be building like a very simple module using the sovereign SDK.
00:00:35.042 - 00:01:31.914, Speaker A: And after that we'll also cover other aspects of the SDK that I'm not able to cover in the demo because it's relatively time bound. And then we'll spend a little bit of time talking about some of the current and future work that we're looking at. Sovereign's mission if I had to summarize it in the most succinct way possible, it's to make it as simple as possible for developers to build powerful blockchain based applications. So what does it mean? Right, like when we say simple, it's both in terms of effort and time. We want to be able to make it easier for developers to build. And by powerful, sure, you can deploy a smart contract anywhere. You can deploy applications in several places, but we want to give developers as much flexibility and their applications to be as powerful as possible.
00:01:31.914 - 00:02:09.142, Speaker A: So let's look at what the problem statement is. Right? Yeah, building blockchain applications is hard, like most of us here know it. And what are some of the common approaches? So you can deploy your application as a smart contract on a layer one. But what are the problems with that? There's no dedicated throughput. Your state transition function is rigid. That means the abstract state transition function is already coded into the layer one. And you can't really change or customize consensus or anything if you want to.
00:02:09.142 - 00:02:44.122, Speaker A: And there's also a problem with value accrual because most of the value is accrued to the infra layer or the layer one and not your application. So that's the issue with smart contract roll ups. You have application chains. They do solve some of the problems above, but there are new problems now. So you need to spend developer resources on non application logic. Because now if you have to build your own app chain, you need to look at blockchain logic. There are some sdks, but you still have to consider networking, consensus, storage.
00:02:44.122 - 00:03:17.766, Speaker A: These are all choices you need to make. And you also need to recruit a validator set. That's like a very prominent problem, because now you still have to be decentralized. You need to satisfy byzantine fault tolerance and your coefficient needs to be high enough to prevent any sort of reorg attacks. So standard problems. So what about roll ups? Yeah, they solve some of the above problems. They eliminate the need to recruit a validator set, because now you're using the underlying DA layer.
00:03:17.766 - 00:03:58.406, Speaker A: You have dedicated throughput, something that your smart contract chains don't have, because now all the resources are. You have resources for your application. The value accrues to the application, which is pretty great because you are capturing all the value and not some other layer one. And the STF is customizable, so that is beneficial. Similar to application chains, you have full control over the logic that you encode around the state transition function. But this is the core of the problem, right? Even building roll ups is actually hard. It solves a lot of the problems.
00:03:58.406 - 00:04:46.598, Speaker A: But the effort involved in building a roll up is not trivial. Like you have to make a choice about the proof system. Like, okay, am I going to be an optimistic roll up? Or how is my finality determined? How are nodes going to consider something final? Like is it optimistic? Or are they going to wait for proofs? And the second problem is censorship resistance. So now this is a commonly brought up problem that your application roll up can censor or they can prevent users from interacting with the chain. So that's one other problem. And then obviously, even in a roll up, there is slightly less protocol engineering. But you do need to write code that interacts with the DA layer or whatever layer, one that you're using.
00:04:46.598 - 00:05:32.258, Speaker A: And that includes things like rpcs. Your goal as an application developer is to build the application, but now you're doing all of these things like you're writing code to submit blocks to the DA layer, consume them, and then build like an RPC server to let other people interact with it. And by tooling, I mean even like wallets and everything, there's a lot of additional stuff you're doing as an application developer, and there's protocol development. I made it separate because even if you use an SDK, you still need to debug networking issues. And for testing, you need to be able to launch multiple machines. So there is a lot of work that you're doing. So the direct consequence of this is that there are very few feature complete roll ups.
00:05:32.258 - 00:06:11.902, Speaker A: I think if you went by the actual definition of a roll up, inheriting security from the underlying layer, there's just one that actually does that. Everyone else probably makes some compromises, usually because they're like, okay, we'll decentralize later, or we'll add in fraud proofs later or only this specific address can submit fraud proofs. So they're trade offs like that. Right. So this is basically where sovereign SDK comes in. The goal is to address each of these problems. So what is sovereign SDK? It's a free and open source toolkit that we can use to build roll ups.
00:06:11.902 - 00:06:59.470, Speaker A: So sovereign lets you seamlessly plug in either an optimistic or a ZK proof system. So by seamless we make use of like sovereign is built in rust and lot of it is actually, we provide abstractions around having the right traits that a developer needs to implement. So you have a lot of scaffolding and boilerplate already built in and auto generated. So the end result is you can seamlessly change the proof system. You don't particularly need to give too much thought to it, not even just ZK, but even multiple types of ZK proving systems. And then censorship resistance. Yeah, we have a single sequencer, but we also support base sequencing.
00:06:59.470 - 00:07:37.854, Speaker A: So those other kinds of sequencing algorithms are also pluggable. And the code to interface with the DA layer. There are already some built in adapters that we already provide Celestia and avail, and there are more DA adapters that are in development. And we also auto generate a lot of the tooling and other infra that you need. And even the protocol development part, which is like I mentioned, the storage is abstracted, the networking is abstracted. Typically when you have a chain, what happens when a new node joins? Right. It has to start syncing and catch up.
00:07:37.854 - 00:08:15.370, Speaker A: There's like config management. So all of these are handled by sovereign SDK and how to use it. Yeah, you focus purely on the application logic and the rest of it is mostly like plug and play. Like you can plug in existing DA adapters, you can also customize them if you really want to. It's the same for prover systems and for the SDK. Once you implement the necessary traits, the SDK auto generates the RPC for you. We're also working on generating an open API specification, so you can have any client.
00:08:15.370 - 00:09:22.270, Speaker A: Lots of languages have tools to auto generate a client. If you have an open API spec, so the CLI tooling and then the prover, and you also get the node implementation, which is the full node and the light node you don't really need to particularly care about. Yeah, the ultimate goal is that you focus purely on the application logic. And if you implement the right set of traits or you plug in adapters that are already implemented, that does the job for you. And one of the reasons we use rust is performant and the type system is actually pretty expressive. So we try to encode as much as possible into the type system so that when you're running something, it fails at compile time rather than, hey, you plug in this protobuff for something, you start running and then the only way you know something is wrong is when you actually send a message and it breaks in production. Right? So yeah, that being said, we can probably step into the demo, which is like a simple coin module.
00:09:22.270 - 00:10:00.300, Speaker A: I'll try to live code it, but I do have the changes on a branch in case we start seeing like weird compilation errors or something. I hope the font is readable for everyone. Okay, yeah, so a small overview of the repository itself. There's a folder called examples, and this is actually one of the best places to start if you really want to look at sovereign. Like get into sovereign. There's like a demo roll up and a demo prover. And we'll mostly be in the demo roll up area.
00:10:00.300 - 00:10:47.800, Speaker A: But yeah, if you want to create a new module or a new, like in our case it's a coin module, right? So it's typically the same as starting like a new crate or something. So if we go into. Yeah, simple, I'm calling it simple token. And at this point it's just a standard rust crate. There is nothing in it, just the test code. So I will import some dependencies. I'm mostly copy pasting a lot of this stuff because typing it will take much longer.
00:10:47.800 - 00:11:45.534, Speaker A: But if we look at what we imported. Yeah, it's just some standard serialization libraries and everything, but we're also importing the module API and a few other things that are relevant to when you create a new module. What do you primarily need when you create a new module? So I have three files here, or let's make it a little bigger. Yeah, so when you create a new module, you need to be able to submit transactions to it, you need to be able to read from it, which is what is the value of a certain account. Or you need to be able to query state, you need to be able to modify state. And there's genesis, which means when your module is first deployed, what is the initial state? Right. So for each of these we can create like three files.
00:11:45.534 - 00:12:21.838, Speaker A: We'll call them like call RS, Genesis Rs, no, and query rs. So those are the three files. Before we even look into that, let's look at what the structure of our module is. Right. So what we really want is like this is the core structure. When you create a new module. It's like creating a new contract, essentially.
00:12:21.838 - 00:12:41.898, Speaker A: So let's look at what I did here. I'm auto deriving module, which is like part of the documentation. You can see that. But ultimately I'm creating a struct and I'm annotating a few things. I'm saying that, okay, there is an address for the module itself, which is mandatory. If you don't do that, you get a compilation error. And there are two pieces of state.
00:12:41.898 - 00:13:20.614, Speaker A: One of them is the supply, which is U 64 value. The other one is balances, which if you're familiar with Ethereum, it's literally just a hash map that stores an address to the balance. So this is a very simplified version of the module because for the purpose of demo, just to understand the core aspects of it. Right. And then you have the standard imports that I just want to add here. And there is something called like the config, which I can explain in a little bit. It's basically this is what determines what the initial supply is.
00:13:20.614 - 00:14:17.188, Speaker A: So yeah, you created your struct, which represents your state, but after this, now you actually need to implement the module trait. As I mentioned, a lot of sovereign is actually implementing traits that are defined in the SDK because the rest of the engine or the runner makes use of these traits to run. So as long as you satisfy the constraints of a certain trait, you're fine. Like you can write anything that satisfies those trait bounds. If I do this, just looking at my autocomplete isn't showing up once again. Yeah, so basically if I try to do a cargo check or something at this point, it'll say that some of these tray. Oh yeah.
00:14:17.188 - 00:15:00.290, Speaker A: So one of the things you need to do is all of it is based on a workspace. So I do need to add it into the main. So we created a new module. So I'll just add it into the workspace here. So this is the simple token module. So now it should be able to tell me if something is broken. Yeah, so it basically highlights the error saying that, hey, you are saying that you want to implement the module trait, but you're missing a whole bunch of things, right? So let's just say implement members and all the mandatory members.
00:15:00.290 - 00:15:38.908, Speaker A: For now we can just leave this as unimplemented, which is pretty cool with rust. You can add that in later as long as it type checks. So for context, we already have a value c. I'll just use that. The config represents the initial state and we just created a struct here that says, what do I need? In order to initialize my genesis state? I just need a supply of the token, and I need an address of the token. Naturally, you can make this more complicated and say that I also want to have a name for the token or whatever those fields are. As long as they satisfy certain traits, it's fine.
00:15:38.908 - 00:16:08.710, Speaker A: Like, they need to be serializable because you're storing it in your rocKsDB or whatever storage. Right. So at this point, I'll just say, okay, my config type is actually this, and it's generic. And my call message. So the call message is. What's the call message that you're using for the transaction? At this point, we didn't define the call message yet. So once we define this, we can just see if it type checks or not.
00:16:08.710 - 00:16:30.300, Speaker A: It should. Yeah. And it did, because that's actually one of the cool things about rust. Even though you didn't implement it, you're not using it. So you can leave unimplemented for now. And as long as the types are correct, it's still okay. Now that we have our basic struct, let's look at what the genesis looks like.
00:16:30.300 - 00:16:58.740, Speaker A: Right? So in Genesis, what I'm doing is. Let's see. Okay, so all we're doing here is we're implementing a trait for our. Yeah, this is actually pretty simple as far as logic goes. I have my struct, and I'm implementing a function for it. This is just a private function. This is not really enforced by the trait or anything.
00:16:58.740 - 00:17:39.910, Speaker A: But all I'm doing here is, I'm saying, okay, I'm taking the config object, which is of the type that we defined. And I'm just setting the supply to whatever supply is part of the config, and I'm setting the balance of the creator to the supply. So, just to explain this part, what's happening here is when I create my new token, the genesis state will specify what the supply of simple token is. So I need to provide, like, a supply and a creator, the address of the creator. Those are the only two things that are needed for Genesis. And now, in Genesis, we can actually write the specific code for calling that, which is. Let's see.
00:17:39.910 - 00:18:19.920, Speaker A: Yeah. All we're doing here in this place, the implementation for Genesis, is basically just call init module, the function that we just defined here with whatever config is passed into Genesis. So naturally, these variables are used now. So it'll complain. Yeah. For these things to be usable, you do need to add them as like, this is a standard rust thing. So mod genesis, mod call, and mod query is not really needed, but I'll just add it anyway at this point.
00:18:19.920 - 00:19:11.280, Speaker A: Let's see. Yeah, so this is the init module function that's being called when the module is initialized. And if there are any issues, it'll say genesis failure, which is fine, because when you're starting up your new chain, you want it to break immediately if you misconfigure it. Right, so let's get to the more interesting part now, which is how are we going to define what a transaction is for our call module, for our simple coin module, right? So the most basic thing you want to do, a more complicated module for tokens, would have things like mint burn, freeze. There are a lot of additional features, but for purpose of simplicity, we're already fixing the supplier genesis. So the only real call that we would need is actually just transfer. So this is kind of what it looks like.
00:19:11.280 - 00:19:58.970, Speaker A: Let me just have this here. So what we have here is a call message. So this is an enum type, which is, if you're familiar with rust, it's like the sum type. So what are the types of messages that my system can accept? So here it's just a transfer. And what does a transfer need to specify? Who am I transferring the tokens to and how many tokens do I want to transfer? Right. So that's basically the only message that is supported. But if you want to look at a more feature, complete implementation of this in module implementations, sovereign does provide like the bank module, which is if you look at the call there, it'll have like a create token transfer and then a burn.
00:19:58.970 - 00:20:27.006, Speaker A: There is like a mint functionality and a freeze functionality, as I mentioned. And this entire thing is an enum. So as long as you send the serialized enum to the application, it'll accept it. Like all of that is auto generated. We'll see that in a little bit. But yeah, so now I need to actually encode what happens when I get a transfer message. So for that you just need this function like you can write.
00:20:27.006 - 00:21:06.050, Speaker A: This is the core of the business logic, right? What am I doing when I need to, how do I transfer things so we can look at the function. It's basically you're getting the sender. So there's a context object that has information like who signed the transaction and everything, which is available to all modules. So you can get who the sender is and the signer is. So I have some very simple logic here. Without checked math, or anything so that we can focus on the core part of it. So all I'm doing is I'm getting the sender, I'm getting the balance of the sender, I'm checking if what the sender wants to transfer.
00:21:06.050 - 00:21:52.322, Speaker A: If it's less than the amount, I'm just going to bail with, okay, not enough funds, and then the rest of it is just some basic checkmath. Like, okay, what's the balance of the person you're transferring to? I'll just increment that. That's it, actually. So in a production implementation, you would have like, checked math, if you're familiar with solidity, which is like, check dad, check, subtract, prevent overflows and things like that. But the core logic is still the same. If you're writing any module, this is where you put your, like, you define what the type of message is, which is an enum. Different types of different variants of the enum would be the actual messages that you want to pass to the system to update the state, and then you encode the logic on how to handle it.
00:21:52.322 - 00:22:23.146, Speaker A: But these two are not connected yet. Like, we have a call message and we just have a function. Right? So how do I connect this to the actual module? So that's where this call comes in. So in Genesis, when you implement a module, you need to tell sovereign SDK requires you to specify Genesis and specify how to handle a call. So in Genesis, we wrote the logic. We're like, okay, there is a config that is always part of Genesis. Once it's passed in, you initialize the module with that.
00:22:23.146 - 00:23:06.686, Speaker A: And for the call, this is actually quite basically, this is like the routing mechanism. When you get a message, you need to decide which function to call. So let me just see. Yeah, so this is all it really is. Like what you're doing here is you're basically matching the message and you're deciding like, okay, what is my type? So here, the call message. So this is one of the nice things about rust as well, because you have type checking at this level. If you're trying to handle the wrong kind of like an enum that doesn't exist or something, it'll just fail to compile right at the beginning.
00:23:06.686 - 00:23:46.800, Speaker A: So I'm just saying that if the message that you got is a transfer message, then just call the transfer function by extracting these elements from the enum. So that's basically all that's actually happening here. So at this point we have, yeah, let me just quickly cargo check it and make sure it works or doesn't break. Yeah, so this happened because when we auto filled the crate, it set call message to blank. Right. But now we actually have an implementation for call message. So I'll just set it to that.
00:23:46.800 - 00:24:24.230, Speaker A: So all of these are things that you need to specify. Yeah, it's a generic. So now we specify what the context is, what the config is for the genesis, and we're specifying what the call message type is. So at this point it should type check without any issues. But yeah, so now we have, now what did we do? We filled out the genesis. We filled out what kind of messages the system handles. So now we can also do the query part of it, which is the RPC.
00:24:24.230 - 00:25:04.614, Speaker A: Like how do you read? So we looked at initialization write. Now we're looking at read, which is also fairly, let me see. Yeah, so some standard imports that you need to use. By the way, I wanted to cover this later, but I can actually do it now. The thing is, I'm creating the module from scratch just to illustrate how it's done. But in the modules there is like a module template that you can also use which has a very simple example with a lot of documentation around how you can fill out all of these. Right.
00:25:04.614 - 00:26:03.562, Speaker A: And in going a little bit, I think we're already currently working on a cargo template, so you can just generate the scaffolding directly. So let's get to the query. There are some imports and yeah, this is the part where we're using macros to auto generate a lot of this code. So what we're actually doing here is I just defined response type. And for the response, all that the RPC requires is that it's serializable, right? So we have, okay, we're deriving some traits that rust provides, which is serialized and deserialized. And then what we're doing is we're defining a simple function that says, okay, if somebody passes you an account, then query the balances. Like if you look at our original struct, it had these address supply and balances, right? So we're just querying balances and getting the account.
00:26:03.562 - 00:26:40.550, Speaker A: So how does the actual RPC infrastructure come in? That's where these macros come in. So like sovereign has a pretty rich set of macros where we auto generate a lot of code. So if you say that you want to RPC generate, you just provide the namespace and what the name of the method is. And this is very similar to JSON RPC. Like there is a rust crate called JSON RPC, which is how you define RPC servers. And we use the exact same annotation scheme. So all we're doing here is these are the two annotations that we added.
00:26:40.550 - 00:27:08.362, Speaker A: This RPC is in the simple token namespace and this function's name is get balance. That's it. And all we're doing here is I'm querying the balance from RocKsDB and I'm returning this serializable struct. That's all the RPC macro cares about. And if you do something wrong here, it'll fail with a compilation error. Yeah, okay, this is fine. So at this point our module is actually completed.
00:27:08.362 - 00:28:03.898, Speaker A: Like we have the query, we have the Genesis, and we have the library. Right? So now how do you plug in this module into an existing roll up? Like I said, you can customize a lot of logic, including the state transition function and everything. But we do have a simple demo roll up implementation. When you build your own roll up completely from scratch, there are a lot of things you might want to consider. Like what is my address type? Is it 32 bytes, 20 bytes, or what is my signature scheme? Like, am I going to use sec p ed 255 19 or BLS? So all of those there are pre built adapters that you can just use. The demo roll up uses ED 250 519. It has like standard 32 byte addresses, it uses rocksDB for storage, and it uses Celestia for the DA layer.
00:28:03.898 - 00:29:06.230, Speaker A: So all we're doing now is we're basically just saying, okay, I have a new module, I want to plug this module into my roll up, right? So what you actually need to do is the logic of your state transition function is defined in the runtime. So this runtime is basically just a struct that says what are the modules that are loaded? So demo roll up comes preloaded with the bank module, the election module, and a few others. We just want to import our module here. So to import it, first we have to just add it into this thing here. Let me just see if I have. Yeah, so into the demo STF, I'm just saying, hey, I created a new module called simple token and I just want to import it. So once I do that, I can import the other things that I need here.
00:29:06.230 - 00:30:43.210, Speaker A: So I'll just say we called it like 1 second, do this. Yeah, so these are auto generated actually. So there is an RPC implementation and an RPC server. So if you notice, we didn't have to write anything related to RPC directly. So we're just saying like simple token and I'll just call it like 1 second. It can actually see why it's complaining about it. It.
00:30:43.210 - 00:31:17.952, Speaker A: Okay, yeah, this had to be like a public module. And I just made it like a private module, which is fine. It's a very simple fix. Yeah, just made them public. So now it shouldn't complain anymore. Okay, so now we have the RPC implementation, and we need to plug in the module here. Right.
00:31:17.952 - 00:31:54.412, Speaker A: So let's put it right after accounts. We'll call it simple token. And that will be simple token. And it's not accounts, it's basically the simple token struct. You might notice there are a few places where we have like CFG feature experimental. So what we do is when we write new modules that we're actually experimenting on, we feature gate it with experimental so that people who are using it know that it's actually experimental. It's part of our release process.
00:31:54.412 - 00:32:35.892, Speaker A: And you might also see like feature native. So the reason you have feature native is because there are certain things that you only need in your native code and certain things you don't need in the ZK prover system. So that's basically what distinguishes your non ZK provable code from your native code. And it's fine. Like, most of this is abstracted, so you wouldn't need to directly delve into it unless you're modifying it. But yeah, so there is like the simple token here, and I will add it here as well. The thing is, this actually derives Genesis.
00:32:35.892 - 00:33:12.744, Speaker A: So if I try to compile it now, it should complain that I'm not actually. Basically, it's saying that, hey, you gave me a new module, but you didn't really tell me how to initialize it or how to do the genesis. Right. So it tells you exactly where you need to add it, which is pretty nice. So there is a genesis config, and this is where, when the roll up is starting, you're like, configuring all. So all the modules that we defined in the code, all the other modules have genesis. So we'll just end up adding the genesis here as well.
00:33:12.744 - 00:33:44.014, Speaker A: So for Genesis, it's basically. Let me see. Yeah, and we just need to import that as well. So once it's imported. Yeah, it says that, hey, I need this genesis. So the simple token config should be part of this. Okay.
00:33:44.014 - 00:34:20.876, Speaker A: I have a clone here, but I don't have a clone here. Yeah. So we'll just see if this checks. Okay. Yeah, so at this point, we've basically completed our new coin module. So we can actually let me just check if I missed something because I implemented it, called it in. Okay, I think we're good because it's checking at this point.
00:34:20.876 - 00:34:54.380, Speaker A: So one of the things we provide as part of sovereign SDK is we containerize Celestia so that you don't need to connect to the network live if you're doing something. So we have a make file that is pretty comprehensive in terms of starting the Docker container and everything. So let me see if Docker is running. It drains battery, so I usually kill it. Okay. So I'm just doing make clean, which cleans up the docker container. So at this point, if I do make start, it starts Celestia locally for me.
00:34:54.380 - 00:35:18.676, Speaker A: And yeah, it's just waiting for the container to start up. How are we on time? Okay, good. On time. Yeah, perfect. So now the container is started and you can actually see Celestia actually generating these blocks. Right. So now we start our roll up.
00:35:18.676 - 00:35:51.620, Speaker A: So just cargo run in demo roll up. And it should actually just start picking up blocks. Yeah. So whatever blocks are being generated by Celestia are being picked up here. The good thing is we just did a genesis so we can check if our new token actually exists. Right? Because in Genesis we basically, let's look at what we said in Genesis. I said, okay, create a token with a supply of 1000 using the sequencer address.
00:35:51.620 - 00:36:17.836, Speaker A: I just used that because I didn't want to generate a new key. And if I do like this is just a curl command, it hits the RPC. And if you notice the namespace and the function are basically what we specified in query. So in the query part we said this is the namespace and this is the method. Right. So that's basically what we're using here. Simple token, underscore, get balance.
00:36:17.836 - 00:36:50.116, Speaker A: And for the parameters I'm passing in the address, let me just see if this. Yeah, so there's a token of balance 1000 against this address. So now we can also try to do a simple transfer. So in order to do the transfer, what you need is, what is our call? We define call. Right, let's look at call. So we need an enum of the type transfer with a two and an amount. So we automatically provide JSoN surdy from your enum.
00:36:50.116 - 00:37:22.884, Speaker A: So as long as you have a json that can be deserialized into this enum, you're good. Which is basically what I ended up doing here. Let me quickly see where it is. Yeah, so in the request there is a simple transfer JSon. And if you see it's pretty identical in structure to the enum that we have the transfer enum, I'm transferring it to a specific address, and I'll transfer like 100 tokens from. So now I need to create the transaction. Right.
00:37:22.884 - 00:37:53.116, Speaker A: How do I do it? Yeah, so we have like the sovereign Cli, which handles a lot of these things. So I can quickly show you what it looks like. So if I do cargo run h, it shows you the functions that are. By the way, let me know if any you can see font is visible, right? Okay. Yeah, so at this point I can just. So this is evolving. We're trying to make it significantly easier as well.
00:37:53.116 - 00:38:17.844, Speaker A: But once you have your json message, you want to serialize it to pass it to the server. Right. So that's actually what we're doing here. So when I serialize, I tell it what the private key that it needs to use is. So the private key in this case is the sequencer private key. So let me see, keys and. Yeah, so it's this key and then the module name.
00:38:17.844 - 00:38:40.344, Speaker A: So that's also auto generated, so we don't need to. The module name is basically simple token. That's what we generated just now. The call data is the json that I just created with the 100 tokens. So test data and request. And you have the simple transfer Json. And then you need to provide the non.
00:38:40.344 - 00:39:14.564, Speaker A: We didn't do any transactions from here, so the nons is zero. So once I do this, it's essentially serialized. This transaction. Celestia accepts blobs, right? So we need to bundle the transaction into a blob. By the way, I'm just doing all of this manually, just to illustrate how it works. To give people an idea, it's much simpler. We already have a pr where the CLI gets the complete command line tooling for the module directly.
00:39:14.564 - 00:39:54.464, Speaker A: So there's something like if you do sob Cli submit h, now, the module will turn up as an so. So we serialized the call. Now we'll just bundle it into like. Yeah, we have something called make blob, which bundles the transaction into a celestia blob. You can bundle multiple transactions, but we only have one transaction here. So there's like a simple transfer DAC, so it prints out hex, which is the API that Celestia accepts. I'll just put it into like a blob and submit it to the DA layer.
00:39:54.464 - 00:40:32.696, Speaker A: Right. We also have the RPC. There's also a sequencer module. So there's something called a published batch and a submit call. So the sequencer does all of this, but just to illustrate it, I'm trying to run it manually. So once we have a blob, in order to submit it, we just let me see it make, submit, and then it. Okay, yeah, so it's make, submit, transaction.
00:40:32.696 - 00:41:11.950, Speaker A: And all of this is in the documentation test data. Sorry. Yeah, so this is the blob and we can see our roll up is running here. Right. It's still fetching blobs. I'll just submit this and see if it's okay. It's submitted it and we just have to wait for it to go through the celestia and for the roll up to pick it up.
00:41:11.950 - 00:41:49.770, Speaker A: Okay. Yeah, so the sequencer outcome is like rewarded and it applied the state transition. So if we check the balance, I mean, we can check the balance of the new address, but we can just check the old one because that was 1000 tokens. So now it should be 900 tokens because 100 tokens have been transferred to the new address. 12 minutes more. But yeah, that's basically what an end to end flow looks like. You define your module and then you plug in your module into the roll up.
00:41:49.770 - 00:42:44.090, Speaker A: That just requires like two lines of code. As you saw, there's an existing demo roll up and whatever rust, create native code that you created, as long as it satisfies the necessary traits. You just plug it in and you write some logic for Genesis and then you're good to go. And if you make any mistakes along this process, you get a compilation error, which is pretty nice. That's the demo part of it, and I think we have some time to go over some internals of the sovereign SDK as well, which is we have the demo prover. It works in a very similar way to demo roll up, except that when it's running, it's picking blobs and also proving them. So we don't have enough time to demo the prover itself, but it's completely compatible with the roll up.
00:42:44.090 - 00:43:37.240, Speaker A: If you put something into the roll up, you get the prover for free. So if a machine is running the prover, whatever is part of the roll up, as long as you're able to generate valid risk five code, it ends up generating proofs for it. And the prover is also capable so your light clients can validate the proofs before accepting the blocks. So that's the prover part. And let me see. So regarding work in progress, we have an EVM module that's already checked into main with the experimental feature. As I mentioned, the goal of the EVM module is right now, the example that I showed you is native rust code for business logic.
00:43:37.240 - 00:44:14.010, Speaker A: But this one encodes the entire EVM as a sovereign module. So you can actually deploy contracts and submit contracts and everything and interact with it as you would with a regular EVM roll up. Sorry, regular EVM chain. Our goal is to make it fully compatible and RPC compatible with metamask as well. It's a work in progress, and I think it's pretty close, to be honest. And there's the open API spec generation as well. So these are two of the most immediate things we're working on, but we're also working on a lot of other stuff.
00:44:14.010 - 00:45:00.500, Speaker A: Again, as I said, it's like fully open source. So anybody can we absolutely welcome anyone to visit, take a look. And we also pride ourselves a good amount on the documentation. So if there's anything wrong with the documents or anything, or if anything is not clear, you can always ask in discord and we'll be more than happy to answer. So everything that I explained is a small part of the tutorial. And all of that is here, actually. So when you get started, how do you run like a local DA instance and what does the make file do? It's pretty comprehensive and we want it to be anybody.
00:45:00.500 - 00:45:30.290, Speaker A: Like we said, the goal for sovereign is to let you focus on application logic. Like you shouldn't have to worry about anything to do with the prover, networking, storage consensus. Like just plug and play, start focusing on your business logic. Yeah, we have 10 minutes for questions or. Yeah, anything. Yeah, go ahead. Oh, thank you.
00:45:30.290 - 00:45:35.250, Speaker A: What.
00:45:37.220 - 00:45:39.010, Speaker B: Determines tender means?
00:45:41.460 - 00:46:22.316, Speaker A: So, because we're like, because it's a sovereign roll up, the consensus is actually left to the base layer. What you care about is sequencing because the ordering is done by the base layer. So we support Celestia and avail right now, and any DA layer that fits our adapter model can support it. So the consensus, that's actually one of the beautiful things about rollups or sovereign rollups. You don't really need full BFT consensus in the application layer. The sequencing and the ordering and the Riog protection is actually from the base layer. So whatever Celestia uses, that's tenderament.
00:46:22.316 - 00:46:58.312, Speaker A: Avail might use something slightly different, but I think. Do you know if the diagram is in the repo? I'm not sure if we put it there. Okay, yeah, that's fine. Yeah. So on the roll up, what we care about in terms of protocol is decentralizing the sequencing. Yes. The blob, it's similar, actually so like you bundle your transactions into blobs.
00:46:58.312 - 00:47:15.810, Speaker A: The blobs go into a celestia namespace and your roll up full nodes, all of them. Let me actually see if I have the diagram. That would be cool if I did. Don't have the concept of.
00:47:16.420 - 00:47:22.704, Speaker B: Yes, the concept of block is also virtual. Only the transaction data is sent to the basement.
00:47:22.832 - 00:47:23.750, Speaker A: And once.
00:47:31.100 - 00:47:49.420, Speaker B: You handle a patch from the DA, it's like creating a build show through our concept. And the transaction from the DA is handled after it's proved on it, or it needs to be both. Verified.
00:47:52.160 - 00:48:44.172, Speaker A: Not depends on which node, because full nodes execute the transactions on their own, so they don't need to verify proofs. They can if they want to, but okay, you have your blobs going to the DA layer, right? You also have the proofs going to the DA layer. The full nodes are going to take the blobs from the namespace and directly execute the transactions on their own. The light nodes will wait for groups because, so that way, I mean, full nodes don't really need. So yeah, in terms of, for the user, if you're a light node, naturally you need to wait until it's proven. If you're a full node, if you're a full node, it'll be bound completely by the DLA. So as soon as the DLA sequences the block, you're ready.
00:48:44.172 - 00:49:17.336, Speaker A: You can apply state transition. I mean there are optimistic models that you can use where you can actually take the blobs from the sequencer directly as well. And if you have some assurance that they will be sequenced in a specific order, like degrees of finality. Right. You can have like a soft finality where it's faster. So in the case of a centralized sequencer, you can directly take the blobs from the centralized sequencer because you know the order. In case of base sequencing, it's a little harder because the DLA could reorganize it.
00:49:17.336 - 00:49:34.850, Speaker A: If you have a more complex sequencing algorithm where they have a leader election process, that becomes slightly better. But there can be a reorg if something happens and the algorithm isn't. Yeah, okay.
00:49:38.820 - 00:49:46.748, Speaker B: And there's more transaction, wait until the blob is finalized for the first transaction.
00:49:46.844 - 00:50:19.864, Speaker A: No, because you just keep posting blobs at network speed. The sequence keeps posting. So the sequencer technically doesn't need to execute, it just needs to check. There are some validity checks that we actually do. So there's a sequencer logic here which handles the reward system. But to answer your question, the sequencer's job is not to execute, it's mainly to sequence blobs and post them to the DLA. So you're only bound by the speed at the scalability of the DLA and how many blobs it can accept.
00:50:19.864 - 00:50:41.990, Speaker A: So you can keep submitting transactions, they keep going to the DA layer, and as long as you have a full node that is accept like reading blocks, as soon as they're sequenced, they're executed in order of what they're being sequenced. As the full nodes execute the raw logic, they don't need to care about proofs. But yeah.
00:50:45.640 - 00:50:46.660, Speaker B: The solution.
00:50:49.580 - 00:51:23.344, Speaker A: It is. Yeah, so that's actually another you can completely customize logic module. But yeah, it's basically currently what we have is in the CLI. I have an example actually, so let me quickly run. So there's actually two calls here. One that says submit call and publish batch. This is more like when you keep submitting, the sequencer keeps holding them.
00:51:23.344 - 00:52:11.330, Speaker A: When you trigger publish batch, it sends the blob. So this is in case you manually want to play with it, but you can tune it however you like. Like I said, the base sequencing model is more like as soon as you get the blob, you throw it into the DA layer. But yeah, there is some batching like by the default logic we have it based on both time and number of transactions. We're actually looking at IBC compatibility right now. Bridging for sovereign roll ups is more based on light clients. Because one of the reasons why for us actually provers are like first class citizens, we didn't want to go live saying we'll add this later.
00:52:11.330 - 00:52:33.210, Speaker A: All the work we're doing with the prover system is completely concurrent to the full node and light node. So any other roll up that can run a light client can actually validate it on another roll up. So we're looking at IBC compatibility. Chem can probably talk a bit more about that.
00:52:37.020 - 00:53:03.790, Speaker B: Basically. And this all we already have integrated as module that for the other direction of going from. We're also going to build basically an IBC connection. But now we.
00:53:31.370 - 00:54:23.222, Speaker A: Oh yeah, here it is. Full screen it. Yeah, basically this is what the architecture would look like. Do I need to zoom in? Maybe a little bit, yeah, so basically you have the DA layer, it's stuck for some reason, but yeah, you have the DA layer, you have the sequencer, the user submits the transaction to the sequencer. The blobs are posted to the DA layer and they get sequenced in a specific namespace. And in terms of what is interacting with the DA layer, you have both the full node, the prover and the light node. The full node's job is, let me just pick the blocks, let me extract the blobs that are relevant to my namespace, I will validate them, I will execute them and I will modify the state.
00:54:23.222 - 00:54:53.300, Speaker A: The prover on the other hand is like a superset of the full node. In addition to executing it, it also creates completeness and correctness proofs for each of the blobs, generates the proofs and those proofs get posted back to the SQL like the DA layer. So what you have here is the light nodes. On the other hand, they sync only the headers of the DA chain and they sync the proofs from a different namespace. So this is basically what the very raw minimal architecture looks like.
00:54:58.390 - 00:54:59.730, Speaker B: How do you scan?
00:55:00.250 - 00:55:40.180, Speaker A: Oh, that is completely, that's basically what the roll up was doing here when I ran it, which is, yeah, typically what you would actually do is every DLA also has a light client, right? The lite client already syncs the headers and fetches the data. So you would actually use the logic of the light client to do that. You can write custom logic and interact with the API as well. But the good thing is most DA layers come with light clients and nodes that also do some validation for you, right, because you can fetch a block, but now you need to validate whether the block is correct or not. It's better to leave that to the dlas. So.
00:55:43.990 - 00:55:55.460, Speaker B: Yeah, architecture design right now such that all historical statements are recorded in some database or some.
00:56:03.950 - 00:56:33.570, Speaker A: Actually use the default implementation uses jmps like the jellyfish Merkel tree, because it's easier to prove. I'm not sure I understood your question about indexing, but yeah, currently there is no state pruning if that's what you're asking. But we have discussed on state expiration or something to prevent state bloat. But yeah, the full node implementation is basically that as you keep getting blocks, you keep building your JMT and it keeps growing.
00:56:34.390 - 00:56:40.130, Speaker B: Well, I guess all historical state and state changes would be under the air layer.
00:56:42.790 - 00:57:23.940, Speaker A: So when you say state, there are two things, right? There is the actual state tree, there is the ledger. So the ledger can be pruned very easily because you only might need to maintain the last 1000 blocks or last two or three epochs or something. But the state tree itself is maintained in full. But also one of the reasons why we went with risk zero initially is there's recursive proofs. So we're looking at a model where in your ledger, as you add new blocks, you recursively prove so that at any point in time, you can sync the entire state tree and know what the Merkel route of that is. And there is a proof of that as well.
00:57:25.270 - 00:57:29.700, Speaker B: So in order to get the state of your rollout one year ago.
00:57:31.530 - 00:58:18.420, Speaker A: You have to reread it from the. Like I said, it's recursive proofs, but the intermediate proofs are also submitted on chain. So, two options, right? One is let's look at it in terms of blocks. If you want this, like, let's say currently we're at block 10,000 and you want the state at block 4001. Option is you play from Genesis. The other option is if you have snapshots, because we have recursive proofs. If somebody says the state at block 800 is x, and this is the proof for that, if you can just download the state snapshot, you already have proof that that's the correct state because the proof is also published on the chain, and you only need to play the remaining 200 blocks to get from block 800 to block 1000.
00:58:18.420 - 00:58:32.966, Speaker A: Yeah. So that's actually one reason why we're very interested in recursive proofs. Sorry. Yeah. Okay. Yeah, we're already 5 minutes above, but we'll be around for questions, and we'll.
00:58:32.998 - 00:58:34.840, Speaker B: Definitely have some high rule support as.
