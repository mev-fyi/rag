00:00:02.680 - 00:00:12.574, Speaker A: Welcome, everyone, to the first whiteboard session. I'm Nash Q. I'll be moderating today's whiteboard session with Henry.
00:00:12.742 - 00:00:13.350, Speaker B: Hey.
00:00:13.470 - 00:00:23.886, Speaker A: Henry is the co founder of Penumbra, and today Henry will explain me how Penumbra works and go into the details of the protocol design.
00:00:24.078 - 00:00:49.504, Speaker B: Sounds good. So, you know, how do you want to start off? Well, I think that we should start by writing Penumbra. Yes. At the top of the board. This one's better. Great. So, penumbra is a shielded dex that is IBC enabled.
00:00:49.504 - 00:01:41.220, Speaker B: So you can take any IBC asset, transfer it into penumbra. It's recorded privately inside of the multi asset shielded pool, and then you can do trading. So there's on chain trading, but you can have private strategies. So in order to kind of see how this fits together, I think it would be useful to start by making kind of a overview map of some of the different pieces of this protocol so we can see how it fits together. And then once we've had this kind of like 30,000 foot view of what's going on with Penimbra, we could look at a few different flows. Like, suppose I make a send transaction, or I interact with the Dex. What does that translate to? And what data flows between all the different parts of the system.
00:01:41.220 - 00:02:48.150, Speaker B: All right. Yeah. So one thing that is somewhat unique about penumbra is that when we designed it, we really had a concept of doing end to end protocol design. So, especially for a privacy protocol, the way that you're going to get privacy is by moving the data off the chain onto the end user device. And that means that as a protocol designer, you can't just think about, like, oh, this is what happens on the full node, and then I'll assume the existence of infurae that just does everything and makes it actually practical to use. You actually have to design this whole thing together. So the first distinction that I need to make is that there's one part, which is the client side, and then there's also going to be some stuff inside of a full node, and then we'll also off to the side.
00:02:48.150 - 00:03:38.190, Speaker B: We probably won't focus on this, we'll also have the interchange. Another thing that is notable about penumbra is we never designed penumbra to be its own perfect, isolated world. It's always thought to be, this is going to be IBC enabled, it's going to be connected to other IBC chains, and that's let us have a kind of a differentiated or opinionated protocol design. Right? Like our programmability layer is like other IBC chains. So let's start with the full node. There's going to be two pieces of the full node. So the first part is there's the actual BFT replication consensus system, which is comet BFT.
00:03:38.190 - 00:04:54.880, Speaker B: And we basically treat comet BFT as kind of a black box. So we integrate with the API hooks that comet provides for replicating the state of the application. But we didn't go off and build our own consensus engine, we just took comet off the shelf. So comment VFT communicates with the penumbra application, and that application is structured as a bunch of components. And I'm not going to draw all the components on the board at the moment, but I'll just draw a few of them that are particularly relevant for some of the flows that we might want to discuss today. So one component is there's a shielded pool, we also have a staking component and there's a dex component. And then so these, I'll draw kind of those like boxes that are next to each other.
00:04:54.880 - 00:06:27.982, Speaker B: And then there's two other components that I'll draw, one on the top and one on the bottom and we'll see why in a second. Well, maybe in a few minutes. One is called the state commitment tree or SCT, and the other is the compact block component. Cool. So what are these two things and why are they sort of drawn above and below in this way? The state commitment tree is a tree of all of the commitments to private state. So at the very beginning I said as this sort of high level idea of how do we have privacy on chain, you take the user data and you move it off chain onto the end user device, and then each end user device is going to make a transaction and send proofs back and we'll see that flow in a few minutes. But the state commitment tree is a component of the penumbra application that manages all of the different opaque commitments to state fragments that are created by end users.
00:06:27.982 - 00:07:40.802, Speaker B: The second piece that I've drawn here, so that's underlying in the sense that all of these different application level components, anytime they need to interact with any user per user state, they're not actually going to see that state directly. But they might want to be either adding new, new commitments to new states to this SCT, or they are going to be verifying proofs that this state was previously validly included in the tree. The second piece that I drawn that was overarching is the compact block component. This is a really cool and underappreciated part of the penumbra design. One of the things that we did very early on in the design process for penumbra was to conceptualize it as we need to have a client protocol that's built in from day one. You can't just assume every user is going to be running a full node. Obviously, it should be as easy to run a full node as possible, but you want all of the clients to be able to synchronize their private state very efficiently.
00:07:40.802 - 00:08:18.884, Speaker B: And so you have to think about that from the beginning. And what we did was we built a second data structure called a compact block, which is basically a minified version of the block data that has just the smallest possible subset of information that a client would need to be able to synchronize its own private state. So it doesn't include all the proofs or all the IPC packets or whatever, but it just has like here's the encrypted payloads that were added, and here are the pieces that were spent.
00:08:19.052 - 00:08:24.260, Speaker A: So Penumbra knows which client needs what.
00:08:24.300 - 00:09:25.880, Speaker B: Packages is actually the opposite. It's that every client is going to be downloading this stream of, if you imagine, okay, imagine I have a full node, right? And I stand up my full node, and now it's going to be synchronizing with the network. It's going to download all the blocks. And as soon as you use the blocks to, it's going to apply each of the blocks in turn, and that'll build up a materialized copy of like, okay, here's what the chain state is. What we've built with the compact block system and the Penumbra client protocol is basically an ultralight version of that process that can be restricted to just that one wallet's account data. So when you start up your client, and I'll draw some boxes here in a sec, it's going to synchronize by going to an RPC. And instead of downloading all of the full blocks, which are potentially big bandwidth heavy etcetera, it's going to stream these minified compact blocks.
00:09:25.880 - 00:09:51.958, Speaker B: As it is scanning through those, it'll look to see is any data that is in these compact blocks relevant to me? And if so, then it saves it. Otherwise it discards it. And that whole software stack is optimized around the idea that you're going to want to discard as much as possible, as fast as possible, because your goal is to only synchronize this minimal subset of your own data.
00:09:52.054 - 00:09:58.190, Speaker A: Oh, so the RPC doesn't know which packages are client. Exactly. Okay.
00:09:58.350 - 00:10:01.930, Speaker C: Is it like a header with the side charts for commonly?
00:10:03.190 - 00:10:56.256, Speaker B: No, because, so there's actually an interesting point, or there's a kind of interesting conceptual distinction here. So thanks. A lot of the time when people talk about, like clients, what they're trying to do is get a trust story. I want to be able to do this header only verification so I can get a root. In our case, though, we're less concerned about the trust and the integrity and more concerned about the privacy and the data access. So it's not that the, that we're having the compact block data, when it's processed by the client, it's not actually verified. So you're still going to trust the RPC for integrity, but you're not going to be revealing.
00:10:56.256 - 00:12:10.140, Speaker B: Like, here's exactly which data is relevant to me, so you preserve privacy. So maybe it's actually we should look into how does this work on the client side? I'll draw an arrow. Right, because these are going to be streamed. That's not an so inside of the client part of the software stack, there's two, um, relevant sort of pieces, like logical pieces of functionality. One of them is called a view service, and the other is called the custody service. And you can think of these as having kind of two like levels of capability, namely read access to a user's data and write access via transaction authorization. So I'll draw two boxes, view service, and a little later down, there's like a custody service.
00:12:10.140 - 00:12:22.712, Speaker B: We go in there. So just to go back to the.
00:12:22.736 - 00:13:03.694, Speaker C: Previous point for the streaming and minimizing the amount of data, I'm assuming that what you'll do is you'll ping to get the compound blocks. You'll have like blocks one to ten you're going through, and you're potentially decrypting or using views to say, okay, this is my data. And let's say you have blocks one, five and ten, which are yours. You don't want any of the other blocks. So I'm also wondering, do you store anything except the latest compound block? Because I would assume maybe you try and do something to store like a summation of everything that's happened. So you store the most minimum amount of data, or do you store every single block that you have for that particular user? So then you have a little bit more data than just the summary.
00:13:03.862 - 00:14:38.760, Speaker B: So on the full node, it's going to store an archive of all of the compact blocks that have been produced, that they can be streamed across and then on the client, none of the compact blocks are stored so they're going to be processed and used to build up a, a view of what that user's status. So it actually might be. Before we get into like, I'll say, I'll give kind of an overview and then we'll jump to sort of a conceptual view of like, okay, how does a shielded transaction work? So what this view service is primarily responsible for tracking and maintaining is, let's say three things. The first is what is the set of notes that are controlled by this wallet? So as I'll explain in a minute, a note is basically like the sort of base unit of value. You can think of it as being like a shielded utxo that like maybe without all of the bitcoin baggage that comes with that word. And it also has to synchronize a filtered copy or a filtered instance of the state commitment tree. And that's also a kind of interesting point that I'll get to in a sec.
00:14:38.760 - 00:15:50.134, Speaker B: And then the third thing that it has is other transaction metadata. So by default the vservice, if it detects that there's a transaction that's relevant to you, it'll go download it, save it locally and then you just have it forever. And that way when you want to query your own transactions, they're all just locally there on your own device and you don't have to leak information to some network service. This Custis service just has designs. So before filling in the rest of these boxes, it might be useful to have a bit of a kind of brief intermission about what is the kind of basic state model for Zcash derived shielded chain. And in particular, what is a node. So I'll go over here and then do like a little sidebar for um, what is a node? And uh, the answer is in penumbra.
00:15:50.134 - 00:16:39.300, Speaker B: This is like the base. Um, I'll switch sides a little bit. This is the like base unit of state or of user state. And it consists of three things. There's a, there's an amount which is a 128 bit integer. And there's an asset id, which is a, it's a random field element. I'll put Fq element.
00:16:39.300 - 00:18:05.650, Speaker B: This is going to be like the hash of some asset metadata or something. And finally it has a control address. So you can think of this as being a typed value plus a capability that gets to control when it is spent. And this data of a note has two associated, let's say commitments. So if I have a note, there are two identifiers for a note. The first is there's a note commitment, and this is just a hash of the note. So we're going to use this when some client makes a new piece of value, like a transaction output, and they want to record that this was validly created, they're going to take the note, hash it to form the note commitment, and then that will get included in this state commitment tree we have.
00:18:05.650 - 00:19:51.694, Speaker B: It's called the state commitment tree rather than a note commitment tree, because technically you can have other kinds of state fragments, but this is the one that is like used for most data. The second piece of, the second kind of identifier for the node is a piece of data called the nullifier. And the way to think about this is that it is a keyed half of the note commitment, and it's keyed by a sub key bound to, bound to the control address. So only the owner of the node can compute this nullifier. And so you can think of this as being basically like a serial number for a bank node that you can only derive if it's actually yours to control. And this is how we manage the double spend problem. So when you want to spend a note, the way that you prevent yourself from spending the note in the future is you're going to publish the nullifier for that note, and you're going to prove that that nullifier was correctly derived for that specific note contents.
00:19:51.694 - 00:20:19.130, Speaker B: And what that allows is the spending of the note can be unlinkable from the creation of the note. So in a transaction that creates a note, we're going to have the note commitment be public and there will be a proof that this is a validly constructed note, and so on. And later, when someone wants to spend it, they're going to privately derive this serial number for the note and then reveal that as part of their transaction.
00:20:20.750 - 00:20:31.046, Speaker A: Does that mean that the client has to basically custody their data, otherwise it would be gone forever?
00:20:31.238 - 00:21:52.068, Speaker B: Yes, so it wouldn't be gone forever, because every time you make a transaction, you're going to include an encryption of the plaintext value of the transaction outputs in the actual body of the transaction itself. So what that lets, if you think about it as like being its own little kind of micro roll up, it's like you're including the sort of new state root of your private state fragments and you're making a proof that all of this was done correctly. And then you'll also encrypt the full sort of rolled up data of the plaintext of these notes both to yourself and to the recipient of the transaction. Or maybe if you're sending it to yourself, that's the same. But each transaction is going to have an encrypted copy of all of the data. What is this client trying to do? If we go back to these boxes? When the client synchronizes, its job is to download all of the compact blocks and then trial decrypt all of the encrypted payloads to see which ones it can see. And then it has two pieces.
00:21:52.068 - 00:22:58.260, Speaker B: It has two responsibilities. One is if it detects something that is relevant to its own wallet, it needs to save that so that it can later use that note for spending or forming a new transaction, et cetera. Right? Like the set of notes that are not yet spent by that wallet is the kind of combined balance. But the second thing that it's going to need to do is it needs to ingest all of these state commitments into its own local instance of this state commitment tree. And this is kind of an interesting, sort of like merkle tree. Merkle tree role reversal, where if you imagine how a merkle tree is normally used in a blockchain, right? There's like the full node and it's got the big Merkle tree. Cool.
00:22:58.260 - 00:24:34.960, Speaker B: And then there's some small client and the client is going to get a copy of this Merkle root. And now the client can make RPC queries or something, or it can ask for like, hey, what's this data inside of this big data structure? And I'm going to use my small authenticator, namely the Merkle root, to be able to verify the answers to queries on this big dataset that I'm not storing locally. But now think about what's going to happen when we're making a shielded chain and we're using a Merkle tree to store all of the private state that is held by individual clients. In this case, we're going to have a, oh, in that case, right. So I'll just put a little bracket here, which is like a like normal or transparent chain. So this is like big and that is small. On a shielded chain, we have a different situation because each client is trying to prove to the chain, hey, I have this piece of data that is validly included in the chain state, but I don't want to reveal which specific transaction output I'm trying to spend.
00:24:34.960 - 00:25:51.950, Speaker B: And now that means that it's actually the client that needs to be making the proofs about this merkle tree and sending those to the server. So that means that for a shielded chain, it's actually the, this big merkle tree is going to be on the client device, and it's the full node that has the roots of this merkle tree. And the client is going to be basically boxing up a proof of. Here is this specific piece of data I want to prove about this one leaf of the tree, which is a note that I control. I'll make some kind of off path, and I'll bundle that up into a proof, and I'll send that off to you, and then you can verify it. But now this is a messed up system, because we want the full node to be the thing that has all of the computational resources and maintaining all the data. And we don't want each client to have to maintain this giant merkle tree of everybody's data.
00:25:51.950 - 00:27:08.190, Speaker B: And so the thing that's really interesting and unique about the state commitment tree that we use for penumbra is that it's a merkle tree that is designed to be filterable and efficiently synchronizable with that filter. So if you imagine, you know, I've kind of drawn this like, very rough schematic picture of like vaguely something that looks like a merkle tree. Most of the pieces of data in the state commitment tree are not going to be yours. Ideally, one user is only a small portion of the activity of the entire chain. And so most of these leaves are data that the client doesn't care about. And so the implementation of this tree has an API where explicitly, whenever you insert a new leaf node, you have to mark, is this leaf node one that I care about? Do I want to retain this, or do I want to try to forget it as soon as possible? As this client is synchronizing it can see this whole little subtree is data that I don't care about. I'll just erase that out of my memory.
00:27:08.190 - 00:27:38.730, Speaker B: And I can also get rid of this one. And I might need like, I might need like this leaf, you know, as a side bar to be able to maintain this merkle path to the piece of data that I do care about. And what that lets us do is actually build these client side filtered instances of this one big merkle tree that are restricted to only the subset of data that the user cares about.
00:27:41.590 - 00:27:42.886, Speaker A: I have a question here.
00:27:43.038 - 00:27:43.678, Speaker B: Yeah.
00:27:43.814 - 00:28:20.232, Speaker C: From earlier, because I'm still wondering. You've mentioned that there's client side computation, but then you're also going through these blocks, compound blocks, and you're decrypting something. I'm just wondering why. What are you exactly decrypting there for? If you potentially already have that client side, because if you need to ingest the blocks for the commitment tree, is that part of the data? So you need to somehow verify that. Or if it's not now, why would you be, if you already have that information in the first place that the client then submit it to the node, then you're going to go over it and then you decrypt something which you.
00:28:20.256 - 00:30:10.150, Speaker B: Potentially already know about, like what? So this is a great question. And what it gets to is what is the overall kind of end to end data pipeline of if someone wanted to make a shielded transaction? So let's say, thinking about this description of what's a note, let's say I wanted to make a super, like what's the simplest transaction that someone could make, which would be like a transfer. I want to send a note to someone else and let's say that I, so I have like you know, one penumbra and let's say I already have a note with that exact amount so we don't have to deal with like change outputs or whatever, because that isn't very relevant. What I'm, I'm going to do as a, we can have even here's the box that like usually doesn't appear in the crypto like architecture diagrams, namely like the user. But let's see what they're going to have to do in order to make a basic transfer transaction. So the first thing that they're going to do is they're going to query the view service for their latest stake. If you think about a model for a transparent chain, normally the wallet doesn't have to maintain any state locally like your front end, just like queries the RPC, what we built, the reason that it's called the view service is that this actually provides basically a standardized local RPC so that whatever the front end code is, can just make RPC calls that are processed by this local component.
00:30:10.150 - 00:31:51.210, Speaker B: So that'll learn, like okay, now you say I have, here is a note n with one. And now the user wants to send that to some address a. So what the user is going to do is to send to one, um, two. A they're going to make a transaction that has two actions in it. So the first is they're going to do a spend, a spend action that consumes the input node and then they're going to create a new node using an output action, produces note controlled by a. And it's also going to have an encryption of the note plaintext. And actually because it'll help answer the question of like why are we downloading this data instead of just already having it locally? Let's actually not assume.
00:31:51.210 - 00:33:36.952, Speaker B: Let's suppose that the note that we already have has like three, so we'll do like one arm here and then there will be output for our change and that'll have two controlled by the user. So when they make that, when they want to make this transaction, they're going to have to query data from the view service, and the data that they're going to have to query to build this spend action is they need to get what's called an auth path. So in order to make this proof that I, as this user, control this three node, I need to get a set of values for this merkle tree that I can hash together and end up with the root of the tree. And that needs to be a recent root because I want to be indistinguishable from all of the other users of the chain. So I want to say basically here is a note. It was created at some point in the past, but I'm not going to compromise my privacy by revealing which one I can do that if I have access to this view service. Because the view service has synchronized an updated version of the state commitment tree that has all of the notes included in it, and I've retained here is the data needed to do these inclusion proofs.
00:33:36.952 - 00:34:02.396, Speaker B: Is the filter pre updated every time the new computer? Exactly. So there's one compact block per block. The block time is like 5 seconds. So every 5 seconds there's a new batch of transaction activity on chain and that gets merged into the client's state commitment tree comma blog contains everything that.
00:34:02.428 - 00:34:06.040, Speaker C: Needs to be to update.
00:34:06.660 - 00:35:23.296, Speaker B: Yeah, it has the minimal data that a client needs to update. And the intuition that you should have here is if you think of Merkle Tree for a ZK proof as being like an accumulator structure, I want to be able to prove that something is part of this set. Like the updating and synchronizing the SDT instance is how that user's anonymity set expands to include other users transactions. Like when there's a block, the compact block is sent out to the client. And I have in these new commitments now when I make a proof, I'm going to be making a proof that is with an anonymity set that includes those new other people's transaction outputs. So they'll get all this data required to make the transaction, and all of this will be assembled into a bundle of data called a transaction plan. And this is basically a deterministic bundle of all of the data that will be used when we're going to be building the transaction.
00:35:23.296 - 00:36:26.974, Speaker B: And this transaction plan gets sent off to the custody service for authorization. And then hopefully we'll get back some signatures. Because the transaction plan has this fully deterministic bundle of all of the data that's going to be used to build the transaction. This custody service can present a comprehended, rendered view of this is exactly what the transaction is going to do. We don't have to do just this blind signing. If you think about how would you sign a shielded transaction, you really need to know what you're signing, hopefully. But if the transaction is shielded and opaque, how would you know what you're doing? And our solution for that is you don't actually sign the transaction once it's built.
00:36:26.974 - 00:37:26.190, Speaker B: Rather, you can reduce the signatures using this plaintext transaction plan, and that way the custodian, this is a kind of pluggable service, so it could be your browser extensions pop up, it could be a hardware wallet, it could be a third party custodian, it could be a multisig committee, whatever. They have this full plan that they can inspect and decide whether or not to sign. Now this transaction is going to be built. So we'll do the ZK proofs that say all of these. This new note that is produced by this action but is not the plaintext, isn't there. We're going to have a proof that says that was actually validly included. And this transaction will get, so we'll build the transaction and then this is going to get sent out to comet BFT.
00:37:26.190 - 00:38:36.432, Speaker B: And maybe I'll just expand this like diagram here and make like a cool like pentagram of all of the nodes. And so we'll think of like this as being one instance of a node on the penumbra network. So this transaction that gets built, it'll get sent to Comet. Comet will first send this to the application for check TX, which is how the application decides like is this transaction validore? We'll assume that it was verified. Comet will then broadcast this through the network, and we only have one of these. But let's assume that there's different nodes or whatever. Eventually this transaction will be sent for execution as part of a block.
00:38:36.432 - 00:39:32.040, Speaker B: And this is done in the comment API through a method called delivertX. So now let's say that this, okay, so we're now processing this transaction as part of a block, as part of the funnel. Each of these actions is going to be processed by a different component in this case, like there's no staking or dex actions, there's just shielded pool stuff. What is this? This will go to the shielded pool. And now what happens? Right, so remember that we have one spend and two output actions. So for the spend action, actually I'll cut in line. So let's see what happens when we process the spend action.
00:39:32.040 - 00:40:51.818, Speaker B: The first thing that we're going to do is we're going to check the proofs. This is purely stateless, right? So we can do this in a separate parallel cpu bound task, it doesn't block any other stuff, and we can parallelize all the proof verification of all the proofs in the transaction. The second thing that we need to do is we need to check the signatures. So the way that signing works on a spend action is every time you want to spend a note, you have to prove that you have spend authority over it. And the way that you do that is you prove this address that is part of the node has an associated spend key which is not publicly known, but you can make a ZK proof of like this is the correct derivation or linking of the spend key for this control address. And then you're also going to prove here is a unique new fresh randomization of that spend key. And then you'll reveal the randomization and then there's an ordinary signature made by that randomized key.
00:40:51.818 - 00:41:47.480, Speaker B: And what this accomplishes is the signature that is actually certifying the spend authority is just a normal signature. So you can compute that on a super constrained device like a hardware wallet without having to do a full ZK proof. And the signature checking can be done upfront. It's very fast, lightweight and easy, but because it's signed with this randomized key, there's no way to link here's oh, this signing key was used for this spend and this spend. So actually those were controlled by the same address. The third thing that happens for a spend action is we record the nullifier. So one of the statements that's part of this spend proof is given this note that I've proved was validly included in the tree.
00:41:47.480 - 00:42:57.420, Speaker B: I am also proving that this random nullifier that I am telling you about is the correctly derived nullifier for this piece of state. And that nullifier gets appended to an append only set that's stored in the chain state. And that way this spend can't ever happen twice because there's only one nullifier for that node. And we proved that it was correctly derived and we checked that it was not already part of this append only set that we're adding. Once we've done these three things in the shielded pool component, we have successfully consumed this existing state fragment without revealing any information about, you know, who had it or when it was created, or so on. Do full loads have to keep those nullifiers forever? Yes. However, each nullifier is 32 bytes large, so if you think about, you know, how many nullifiers can fit in an SSD, you have a lot of room to grow.
00:42:57.420 - 00:44:00.120, Speaker B: And you can also do things where like you can. If in the optimistic world where penumbra succeeds and nullifier state growth is our biggest problem, I would love that to be our world. But if that's the case, there's techniques like you could segment the nullifiers, or you could eventually move to a proof of non inclusion. So I make a proof that I have that this nullifier was not included in some archival set of nullifiers. Then that way I can segment like you can throw away these big batches of old nullifiers and replace them with forcing a client to just make non inclusion proofs. So at this point we've spent an existing node. What are we going to do to create new nodes for an output action? We again are going to check the proofs.
00:44:00.120 - 00:45:27.060, Speaker B: So what is the proof statement, roughly, that we're trying to prove or that we're trying to verify here? It's that the note that is being committed to was actually validly created. Right? We don't want someone to basically like infinite mint a new banknote secretly with like 1 billion value while claiming that it was only one. So there's checks that the values were all correctly formed, and then what we do is we record in the state commitment tree. So this will, we'll add new commitments and no, I forgot to update. Well, I'll get to that in a second. So we'll record these new commitments into the state commitment tree, and then we'll also queue the encrypted payload to this compact block component. So as part of this output action over here, we produced this encryption of the noteplane text in Panama.
00:45:27.060 - 00:46:47.266, Speaker B: We refer to this as a state payload, and that payload is going to be copied or sent over to the compact block component for inclusion in the compact block. A piece of this spend action that I forgot to mention is that we'll also be sending over the nullifier so that that can be included. So when we add a new node, there's verification that it was done correctly, it's added to the state commitment tree, and the payload ends up in the contact block. Now, let's say that this transaction was included in the block. We're finalizing it and we're going to send it back to a client. This data will come back here, and our client is going to scan through the set of payloads in the compact block. It'll detect that the newly created note payload is visible to the user, and it'll also see the nullifier that it spent and mark the previous note as being consumed.
00:46:47.266 - 00:47:54.100, Speaker B: And so this is kind of a long winded answer. But the answer to the original question of why is there scanning when I, in principle over here, could already know about this, is that we intentionally made it so the right path for the view service data runs through the chain. And the consensus mechanism what that ensures is that you have super, super strong data guarantees for the client data that I'm never going to only learn about stuff that just happened on my device. Like suppose this is my one laptop, and then over here I have my other laptop. How is my other laptop going to learn about the transaction that I made here? Or if I'm trying to recover from my seed phrase, I want to have a super clear deterministic picture of this data. And it also provides strong guarantees against hardware failure. Imagine what happens if I make this transaction, but I didn't properly locally record or my output note.
00:47:54.100 - 00:48:52.810, Speaker B: Then I would learn that my previous note was spent, but I would never get my chain and it would effectively be burned just because my computer crashed along the way. When you're trying to do something like recover from sysphrase, do you have to replay other compact blocks or is there some kind of fastpass from the rampantry? Yeah. So you do have to replay all the compact blocks for now. But there is a fast path on the merkle tree. And the fast path is that the structure of our snark friendly tree, I think we're about to be wrapped up, so I'll just do it quickly. But the structure of the Merkle tree that we have is that it's basically this tree of quad trees. And so we have one that is per block, and then we have one that's per epoch, and then the global one we call eternity.
00:48:52.810 - 00:49:47.420, Speaker B: But the cool advantage of this is, let's say that I'm streaming these compact blocks. In this case, there was something that we did, but the average case is that most of the activity is not yours and there's no relevant data in the block. What we do is we include this intermediate node in the data in the compact block. If your client does this trial decryption, which is symmetric crypto. It's like normal crypto. So it's very fast and it can be done in parallel. But if you detect nothing, then instead of having to hash everybody else's data and ingest it, you can do this fast forwarding filter where instead of inserting and then deleting, you just insert that block level route and then fast forward over everybody else's stuff.
00:49:50.720 - 00:50:03.620, Speaker A: Okay, great. I think we're going to wrap up. Thank you so much for being here. For the viewers, where can they find out more about penumbra and how to interact with it?
00:50:04.240 - 00:50:34.582, Speaker B: Well, I guess so. This is kind of the technical overview. If you want like the full rundown on the, the entire protocol. If you go to protocol penumbra zone, that has the 200 page protocol spec and that has all of these details and more, there's also guide penumbra zone, which has info about more oriented around. How do I actually use the software?
00:50:34.766 - 00:50:37.070, Speaker A: Okay, great. Thank you so much. Cool.
