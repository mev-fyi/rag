00:00:04.630 - 00:00:05.180, Speaker A: Cool.
00:00:05.550 - 00:00:10.314, Speaker B: So I've known two of these people for many years, but tomer, it's really nice to meet you.
00:00:10.352 - 00:00:11.386, Speaker C: Nice to meet you, too.
00:00:11.488 - 00:00:32.050, Speaker B: Yeah. Welcome to the back to the conference circuit. How's it going? All right. Why don't we start out with introductions who are you? And a little bit about ZK. Sync and scroll. And I think Uri gave a good introduction to StarkNet.
00:00:33.190 - 00:00:33.554, Speaker A: Sure.
00:00:33.592 - 00:01:02.890, Speaker D: I'm happy to start. My name is Alex Kuhovsky. I'm CEO and co founder of Matterlabs, which is the company behind Zksync. We have a specialized ZK roll up on mainnet now for payments and swaps. But now we're working on the EVM compatible ZK roll up, which is the version two of ZK Sync, which we have now live on testnet. So it's the first Zkavm live on testnet.
00:01:04.190 - 00:01:28.340, Speaker C: I'm talrul Maharamov. I work for scroll. I'm a senior researcher there. So what we do is we're building an EVM equivalent ZK rollup, which uses Zkevm, which is implemented by US and Ethereum Foundation, as a collaborative effort. And we're planning to launch the testnets somewhere around the end of the year and hopefully the main net by next year.
00:01:29.510 - 00:01:54.542, Speaker A: So I'm Louis. I'm the ecosystem at Starkware. I'm going to repeat still what my overlord Uri said before. So stockware is a zero company. We do a scanning through the usage of proof system, specifically Starks. And we have been settling over $1.5 billion wait, much more.
00:01:54.542 - 00:02:14.340, Speaker A: $350,000,000,000. Sorry, one or two order of magnitude through our customers dYdX immutable, Soher and diversify. And we have been launching the first production ready VK roll up in production on Ethereum today. So if you want to try it out, have fun.
00:02:15.910 - 00:02:16.562, Speaker B: Awesome.
00:02:16.696 - 00:02:17.380, Speaker A: Yeah.
00:02:18.310 - 00:02:45.150, Speaker B: Okay, so let's kind of get into it. One of the questions that we were discussing in our chat is this question of decentralization. And I think Uri started this question with StarkNet has a single sequencer. How do each of you think about the sort of trajectory towards not just having scale, but decentralization in sort of general purpose zero knowledge execution?
00:02:45.570 - 00:02:50.910, Speaker D: So maybe we can begin with the question, what is decentralization? Why it's important?
00:02:51.060 - 00:02:51.374, Speaker A: Sure.
00:02:51.412 - 00:02:52.000, Speaker B: Absolutely.
00:02:52.610 - 00:04:03.960, Speaker D: At Zksync, we are following the philosophy of Bitcoin Ethereum, the decentralized public blockchains, where decentralization is important to ensure that no one has complete control over the network. There is no single point of failure. There is no single point of leverage. So we don't want the blockchain world to devolve into something that happened to Internet, which began as a decentralized network of networks, but now it's essentially controlled to a very high degree by, like, five corporations because of natural process of centralization, of power, of amassing of resources. So we need to prevent that. And the way to prevent that is to put the control of the system in the hands of the users and making sure that the community always have the leverage over the developers of the protocol, over the operators of the protocol. So if something happens to Ethereum and let's say miners or validators in proof of stake after the merge become malicious and have a malicious intention to control the network, extract value from the users, the community can always coordinate and fork away.
00:04:03.960 - 00:04:41.678, Speaker D: And for me, this is the very fundamental thing which we want to preserve in Zksync by having permissionless software license, by having the network governed by the community but also supporting even though we hope it never will never. Happen ability for mass exits, mass transfers to a fork of Ziki sync or to any other roll up which will preserve, which will keep this philosophy. But it's also important to keep decentralization in L2 itself, which is why we're working on a decentralized consensus inside L2 to make sure that the transactions also remain.
00:04:41.854 - 00:05:05.610, Speaker B: I guess maybe it would be a bit better to start this question why is it particularly hard to decentralize sort of execution with the zero knowledge technology? We've had decentralized blockchains now for ten years. What is the unique challenge that comes out of zero knowledge that makes decentralization more of a challenge? Do you want to?
00:05:05.760 - 00:05:32.100, Speaker A: Yeah, I guess I can take it. It's more that there is no real challenge. It's an engineering challenge. It's only engineering, as people say, except that it takes time. So we're going to make it right, we're all going to make it. We're going to make it decentralized making open source, everyone will be able to run it. What's complicated though is that you have less flexibility when you build an L two than when you build an L1.
00:05:32.100 - 00:06:36.470, Speaker A: Because as opposed to an L1, at the end of the day, L1 is layer relay on an L zero and you can always roll back. You can do things on an L two. The problem that you have is that you have this breach and when something is a breach, valid proof and the money is out, the money is out, you don't control anything anymore. And so the breach is the most scary piece of software when it comes to these L two S. And for the funny story, the reference paper and that no one think about it, but the reference paper on L two S, it was written by Patrick McCauley and it's the name of it. And I would think about it scaling blockchains through validating bridges as a scaling solution because at the end of the day, everything is about the bridge. And so now there is a bunch of challenges also with the proof system which is you need to have all of a sudden you don't have only executors and your sequencers, you have this proving and this printing has this latency and you don't want to want to make the sequencing as distrust as possible and the Proving also.
00:06:36.470 - 00:06:45.820, Speaker A: And you have a bunch of challenging that exists elsewhere. And so there is a bunch of engineering challenges that still need to be solved and we just want to get it right. So it takes time.
00:06:46.190 - 00:07:42.780, Speaker C: Can I add? So I think the problem with L two specifically is that because we are already outsourcing the consensus to an L1, the goal of an L two is to minimize the overhead which is consumed by the consensus. So adding a consensus on an L two doesn't really make a lot of sense because it just will add the same overhead that you already are trying to outsource to an L1. So the point is how do you decentralize in a way which also minimizes the overhead? And it's quite difficult to achieve, especially with ZK roll ups because the proverb efficiency is not great. So it consumes quite a bit of computational energy to compute the validity proofs and we need a way to make sure that the system remains efficient, but at the same time people can just join in and participate without an issue.
00:07:44.990 - 00:08:27.206, Speaker D: I just want to add we're here at the modular summit so we should think about the decentralization also in modular way. There are multiple layers that can be decentralized with regard to L2. The first is obviously the bridge itself and the general ability to control it, to upgrade, to exit and so on. Then there comes execution, which is actually easier to decentralize than layer ones. We have less challenges because we can always tap into ethereum as a court of a final appeal to resolve disputes. So our consensus mechanism is actually simpler than the one required for layer one. And we can also have some significant advantages.
00:08:27.206 - 00:09:18.230, Speaker D: We can reduce the number of validators because we don't depend with zero knowledge proofs on them. For security, we don't need such a large number, we can have delegated proof of stake, so we can reduce latency a lot and we can have very fast confirmations for transactions in L2. But then there comes also zero knowledge proofs generation, which is a layer on top of that, which can be decoupled from the execution layer, which can happen asynchronously in some time, which is probably going to be much lower than what we currently have. It's going down like under minutes, like some seconds, but it still needs to happen and it needs to happen in a decentralized way. We don't want to depend on a single provider, on a single cloud or on a single powerful player for generating the proofs, not to have it in the supply chain as a point of failure.
00:09:18.390 - 00:09:28.880, Speaker B: Well, so do you imagine that there's going to be economic incentives eventually in these systems for generating proofs? Like how are we going to convince somebody to spend a huge amount of money?
00:09:29.490 - 00:10:04.614, Speaker A: I'm going to troll my colleagues here. So we have this darknet research group internally where we discuss those things and this question about how you decentralize, this has been an open joke. We have ideas and direction, but an open question for nine months to a point I dropped the meetings just for a joke. But no, there will be obviously incentive. Today we only pay the sequencer either through leader election, through proof of work or proof of stake. But there will be the same kind of mechanism. Now, you're perfectly right.
00:10:04.614 - 00:10:49.750, Speaker A: The challenge there is for instance, let's give you a random idea, which is, oh, let's make a competition. And then if you make a competition that's great. It seems like just the fastest gets to the network, we are all happy, whatever. So the main issue you get with that is that it basically brings down to centralization because the fastest guy, the most efficient guy would always win and there is no incentive for the others to make it. And so all of a sudden now you're relying on one or two single point of failure and your system is not redundant. And while e cannot of course steal money because it's a zero knowledge proof system, whatever, because of the bridge, it still can stall the system and you don't want that property. And so you need to come up with new design and find the right trade off on this latency decentralization, which still is an open question to some extent.
00:10:50.890 - 00:10:51.446, Speaker B: Cool.
00:10:51.548 - 00:11:05.134, Speaker D: But to come back to the original question, if there is a work that needs to be done, if there's something people need and they were willing to pay for, there will always be found some market mechanism to provide this value and to get compensated for this.
00:11:05.172 - 00:11:46.140, Speaker B: Absolutely. I think what do you so, you know, I think the progress in, in like the fact that, like, we even have testnets and main nets of general purpose zero knowledge computations is just like insane. From the first time I saw the zero cash paper in 2014, it's just been like, I can't believe that we've made so much progress in eight years. But what have been the challenges in building execution environments? You each have sort of very different models of sort of how the zero knowledge is actually executed. How do you differentiate them or think about them as being different?
00:11:48.110 - 00:12:49.406, Speaker C: So for us, our main goal is to be EVM equivalent, which means that you can copy the bytecode smartphone bytecode from ethereum and just plop it on scroll and it'll just work without any need for transpolation or any other fancy trickery to make it work. And the challenge with that is that EVM is not really circuit friendly and zero knowledge proof friendly. So a lot of the Opcodes and a lot of the functionality inside EVM needs to be made in a way which minimizes the overhead inside the circuit and that's quite challenging to do while also remaining efficient. So that's one of the main challenges for us that we're tackling right now. And the way we're trying to solve it is by using hardware acceleration. So we already have a GPU acceleration implementation, accelerated implementation, and we're working on FPGAs as well and we're just going to see which.
00:12:49.508 - 00:12:52.030, Speaker B: Doesn't that make decentralization more challenging?
00:12:52.690 - 00:13:00.740, Speaker C: To some degree it does, but it depends on the incentives. If you provide enough incentives, there are going to be always enough people who are going to participate in it.
00:13:02.230 - 00:13:35.866, Speaker A: About the decentralization challenge, I just want to bounce on two things here. The first one is we all have the same challenges. Like basically you have the same prime. We want to separate the sequencer, we want to separate approver. Ideally if we're just going to merge, if there is no other way, but basically we have the same technical challenge. But now the trade off that we all have is different target. The Starquare just took a different bet to say, you know what, EVM is great, we all love it, but seriously, it's a piece of crap and let's do something that is optimizing for proof.
00:13:35.866 - 00:14:13.938, Speaker A: Right, I'm going to rephrase that. It's a great software and it could be improved. I'm making jokes, but yeah, it's darkware. Just decided to say, you know what, ZKPs are very different computing paradigm. Just deal with it, just accept it and embrace it and be as fast as you can using that paradigm and then you can retrofit stuff if you want afterwards. But optimize for the performance and scroll obviously is going for pure EVM. EVM compatibility or the ability to actually prove blocks on Ethereum.
00:14:13.938 - 00:15:00.178, Speaker A: Dksync is optimizing for simplicity of developer experience. Based on previous experience, I would let you blunt on that. But the question now about decentralization is the decentralization, as we say, also is less important for ZK roll up than it is for Ethereum or for an L1. And because decentralization matter on a rail one, because you need it for consensus, you need to know that the state you're holding is a true one. You need that. Otherwise everyone could lie to you when in a VK roll up you're like, I just need for censorship resistance. I'm not going to be lied to because they cannot lie to me because of the proof.
00:15:00.178 - 00:15:10.860, Speaker A: So I just need like a bunch of incentivized people, probably. I don't even give numbers because we want as many as possible, but it could be smaller and it's not necessarily a big deal.
00:15:11.470 - 00:15:17.118, Speaker B: So I think the ZK sync CPU target is kind of very unique. You want to sort of explain?
00:15:17.284 - 00:15:17.662, Speaker A: Sure.
00:15:17.716 - 00:16:22.130, Speaker D: So, as Louis correctly said, we've chosen the middle ground between the two extremes of having EVM equivalents and having completely separate from scratch execution environment. What we're trying to maintain is a balance between the balance, not a balance. We're not making compromises. We want to get the best of both worlds of the super efficiency, which zero knowledge proofs can provide if you optimize for them specifically and having a fully EVM compatible chain where you can take essentially any application written for EVM chains in Solidity and Viper and other languages. Just port it on Ziki sync and it will work out of box with full web3 API access, with your deployment scripts, with all the tooling that you depend on. It must just work. But we're not willing to take the compromises in the penalty in security orders of magnitude more sorry, not security performance that you would do if you went for full EVM efficiency.
00:16:22.130 - 00:17:22.722, Speaker D: So the challenge for us was there were a lot of challenges like you just go them one by one and you tackle them once you set your priorities. But one specific example would be we need to follow and fully preserve EVM security model. And what we're building is a hybrid solution. It's a volution between ZK rollup and ZK Porter which is a data availability off chain solution which is connected to Zikke rollup. So users can choose whether their account is fully secured by Ethereum or has this external data availability layer which makes some assumptions about security. But the challenge was like how do you maintain the security model of Ethereum because the roll up contracts cannot depend on the Porter contracts. Like if their data becomes unavailable there, how do we make sure that the users can still always withdraw their funds from the roll up and this can be enforced by Ethereum.
00:17:22.722 - 00:18:05.860, Speaker D: So there were multiple layers of thinking there. One is that we need to ensure that every transaction is executable in zero knowledge proofs, even if it fails. We don't only accept transactions that are valid, we can accept invalid transactions as well, whether they're coming through priority queue on Ethereum and we will always execute them no matter what. And just if they fail, we will prove that they fail. But there was also a challenge of how do you design this interaction between roll up and Porter in such a way that to protect 100% secure roll up users no matter what happens on the Porter, we found a solution and we're going to be happy to present it once Porter is out.
00:18:06.790 - 00:18:16.280, Speaker B: So Louis, the challenge with having your own sort of CPU target is you've had to build a developer community around zero knowledge. So how is that.
00:18:18.810 - 00:19:06.774, Speaker A: To be honest, let's say that nine months ago or since I've joined Starquare and we've been talking about Cairo, everyone say EVM compatibility, even compatibility like it's like the fucking gold. And the truth is, at first we were very worried. Of course we have to start community from scratch. And the counterintuitive thing is that it actually helped. The counterintuitive thing is that we are not gathering. I mean, the whole thing we're going after today is not to cater to the, what, let's say 20,003 D developer worldwide, 3000 different 3D developers. The older new guys, they are willing to accept something new, they are trying something new.
00:19:06.774 - 00:20:00.120, Speaker A: And even worse, they feel that they're bounded by fire because Cairo is hell. And so carries are a CPU and a language. The truth is we have seen a surprising interest from the dev community, from people who came from existing solid dev, from other places in the world, just popping up and learning it and improving the tooling and having extremely dedicated community and without sort of buying their way. We just explain them, talk to them, give them ideas, fostering the community. Basically helped and worked. And as of today, I would say that wordwide right now on a daily basis, there is around 300 to 500 devs nine months after inception. So that's pretty successful and the growth is not stopping at all.
00:20:00.120 - 00:20:07.338, Speaker A: For now we are pretty considered, pretty safe with our bets to say people are willing to learn new things.
00:20:07.424 - 00:20:09.514, Speaker B: I certainly see you tweeting some new.
00:20:09.552 - 00:20:12.838, Speaker A: Exciting thing every single time I wake.
00:20:12.854 - 00:20:13.450, Speaker B: Up in the morning.
00:20:13.520 - 00:20:58.858, Speaker A: Yeah, I know. At this point I'm like cheerleader like an echo chamber. I have to say if you're a dev and I always tell to the dev wants to actually break into the space that the most important thing they need to do is I have two things that I tell them. The first one is it's better to be early than to be great. Which means the following thing. You're better off being early in ecosystem than coming from fucking DeepMind and going make solidity dev right now. Why? Because you are all of a sudden facing people who have been doing that stuff for five years and you're like you don't know the foot guns, you don't know all that stuff and you're going to be like competing with people that are just better than you because they've been around.
00:20:58.858 - 00:21:29.438, Speaker A: On the other hand, if you come early in an environment that is a bit hellish, but at least the fundamental is very interesting, then the competition is a lot easier. Everyone is new. There is no great dev, there is no sort of magic trick that you know. And so that's what I'm saying. I'm seeing people realizing oh, the tech is super cool and web free, looks amazing, but Solidity is already kind of crowded place. It's not fully but it's a kind of crowded place. Here I am, there is 500 people worldwide.
00:21:29.438 - 00:22:12.142, Speaker A: If I'm bad, I'm better than anyone else. And so that's the frequent one. And second one is I always tell them whenever you do something in this space, you're going to break in by being visible. And the only way you do to do that and that's magic of crypto is that everyone is on Twitter and Twitter is a very easy marketing tool. So every time I'm telling, oh, you do this paper from Paradigm, just tweet it and retweet it and the community is being what it is, it's basically retweeting and a lot of excitement. And I got at least five or six people getting hired just because they posted three tweets hired OpenSea hire various places because they just become visible. So yes, I've become nico chamber and I don't do much in my life except Tweeting.
00:22:12.142 - 00:22:18.654, Speaker A: So people should just follow you actually don't know. Don't think you should follow all three of them. Yeah, you should, but it's really boring.
00:22:18.702 - 00:22:19.506, Speaker C: Not him though.
00:22:19.608 - 00:22:26.598, Speaker A: No, not me. It's very boring. It's like it's advertisement, advertisement, advertisement. Just don't do that. But if you want, if we go board in your life, feel free.
00:22:26.684 - 00:22:37.100, Speaker B: Okay? I think we're supposed to get off the stage around now, right? You have time. Okay. I thought we were going to transition, so now I have to come up.
00:22:38.270 - 00:22:42.506, Speaker E: We can take questions. Yeah, any questions?
00:22:42.688 - 00:22:44.450, Speaker C: I have one question for Louis.
00:22:44.550 - 00:22:47.342, Speaker E: Oh God, I saw you first.
00:22:47.396 - 00:22:48.880, Speaker A: I'll give it to you.
00:22:51.570 - 00:22:52.560, Speaker E: Sorry guys.
00:22:53.170 - 00:22:53.534, Speaker C: Hey.
00:22:53.572 - 00:23:24.790, Speaker F: So we discussed at some point ZK over consensus, right? So one of the properties that we have for off chain computing is that we're not time constraint. So we can perform any proof there and just use this in the chain. So for the consensus, this is more complicated since we're time constrained and maybe we cannot even benefit from some state of the art techniques such as recursions. So how do you guys see that? Should we expect some improvement on the current state of the research or the implementation to make this feasible?
00:23:26.490 - 00:23:33.594, Speaker A: To summarize the question, I mean, if I may, you're asking about Ezekiel ones and recursion as a tool for improving what?
00:23:33.712 - 00:23:34.214, Speaker F: Consensus.
00:23:34.262 - 00:23:37.918, Speaker A: Consensus. Okay. Anyone want to take it or you want to?
00:23:38.084 - 00:24:21.050, Speaker D: I think those are actually orthogonal problems. Like, you don't improve consensus, you improve other things. You can significantly improve decentralization if you implement L One fully. Like let's call it fully succinct l One, what Mina protocol is doing. For example, it's a lot easier for everyone to verify for full nodes. You don't have to re execute all the transactions, you just verify 10 knowledge proof which will take you 30 milliseconds or something, and the rest is just data availability. It's a great approach if you can this challenge would be to compete with big established ones, but at least that's something where you can differentiate from them.
00:24:21.050 - 00:24:26.238, Speaker D: But I don't see it affecting the consensus mechanism as such in any way.
00:24:26.324 - 00:24:43.090, Speaker C: Also, bear in mind that a lot of consensus mechanisms are synchronous or partially synchronous. And when you use validity proofs, validity proofs don't have a sense of time. So there is no way you can use validity proofs to optimize consensus. In most cases.
00:24:44.790 - 00:24:57.240, Speaker F: Yeah, but I mean, like the added time to prove statements for the consensus. So the proof is far more expensive than the verification. So to verify it's trivial to prove it's not. So that's the main problem.
00:24:58.410 - 00:25:45.222, Speaker A: As Alex said, the proof generation can improve some bits in some proof of work stake, like weeks. There might be something about the weak subjective subjective weak checkpoint or weak subjectivity checkpoint. Or something like this. Thank you. There can maybe be some improvement there, but the main thing that you break when you have a VK at the L1 level is that you are improving the topology of the network. Meaning all of a sudden you have validators that can node, non mining node, non sequencing node that can be on your phone and the miners or sequencer can be on a fucking data center for they care. And so all of a sudden you can be increasing your throughput dramatically solana style, but you still have the decentralization that you're hoping for for net one.
00:25:45.222 - 00:25:52.698, Speaker A: So that is not the consensus level though. It's at the topology of network topology. Cool.
00:25:52.864 - 00:25:53.610, Speaker F: Okay, perfect.
00:25:53.680 - 00:25:54.460, Speaker D: Thank you.
00:25:56.670 - 00:25:58.250, Speaker E: There was one more question, right?
00:25:58.320 - 00:25:58.746, Speaker C: Yeah.
00:25:58.848 - 00:25:59.820, Speaker E: Oh perfect.
00:26:06.750 - 00:26:42.482, Speaker A: All right, thank you very much guys. This question is for Louis because you were Duncan on EVM. What would you change about EVM? Hula you want all the we're going to be here for a long time. Let's give it a top three. Top three, okay. You win 256 as default kitschack, as default hash function. Which is like why from the get go, this is not technically EVM, I guess, but exadree for storing state top three.
00:26:42.636 - 00:26:44.634, Speaker B: Those are all on Vitalik's list too.
00:26:44.752 - 00:26:45.322, Speaker A: What?
00:26:45.456 - 00:26:48.570, Speaker B: Those are also on Vitalik's list, right? I mean what do you regret?
00:26:49.710 - 00:26:54.574, Speaker A: Rest in peace. Yeah, just copy paste. Exactly. They're the top three.
00:26:54.612 - 00:26:55.200, Speaker B: Right?
00:26:55.730 - 00:27:03.834, Speaker A: What else? The choice of 80 bytes addresses if a mistake.
00:27:03.962 - 00:27:06.190, Speaker C: RLP RLP.
00:27:06.610 - 00:27:38.762, Speaker A: That's something I don't even know. Right, there is a bunch you can continue there. But yeah, I mean I'm not even the expert in VVM, just the top three that come because when you work on VK you're like why did they use Ksheck? Why did they do that? Could I just like sha oblig two S or something? Something simpler or standard, preferably poseidon. Yeah, but whatever. Actually we don't use poseidon for the story. We would have yeah, we don't otherwise for the story. For the story by that actually the hash function.
00:27:38.762 - 00:28:19.478, Speaker A: So star query is very conservative in choices and so you have to understand that the hardest thing to prove or the most consuming in the proof is usually the hash function. And so most of the industry have been using poseidon and we are using Peterson hash which is provably secure but ten times worse to prove. And so we kind of like to shoot ourselves in the foot with being conservative sometimes. So here's one example. Conservative doesn't mean better or different, it's just like it's different. Such a choice. And I kind of hope one day that we convince my engineer that posed is good enough, we can adopt it.
00:28:19.478 - 00:28:21.302, Speaker A: But for now we are still struggling with that.
00:28:21.436 - 00:28:25.686, Speaker B: We got to wait for someone else to settle like a trillion dollars on poseidon hashes.
00:28:25.798 - 00:28:32.650, Speaker A: Right? Exactly. Thank you very much. My pleasure.
00:28:34.450 - 00:28:39.838, Speaker E: Any other questions? You had a question yeah, right there.
00:28:39.924 - 00:28:43.200, Speaker A: Okay, interesting. So.
00:28:45.170 - 00:29:05.110, Speaker G: The question for all three of you, or four if zach, you want to throw in what proof systems are you guys kind of like most, I don't know, interested by or kind of like where it could go? Take it. Bulletproofs, Marlin, halo, the list goes on. What of those ZK proof systems are you kind of most stoked.
00:29:10.410 - 00:29:30.542, Speaker B: Saw it. I think we know your answer. It's on your shirt. Starks. I saw Ariel yesterday and I was like, you won. Like everyone is using plonk except for the Starks people. And the arithmeticization inside of Plonk and Starks is very similar.
00:29:30.542 - 00:29:36.990, Speaker B: So it's just like really the last step of their fire. But yeah. Congrats to Ariel for winning stark. Timber.
00:29:41.570 - 00:30:28.474, Speaker A: I just want to react to this. It's a very interesting question. My answer is Stark, but obviously there's not the answer for everyone. It's obviously a trade off. What people don't know though is that every time they talk about Star versus Starks, they always care about the theoretical differences like the trusted setup or the non trusted setup, quantum resistance or not. The reality of actually why I care more about Starks, to be honest, is because the proving time proving like by default, which is like why scroll? I mean you said so they are looking into acceleration is because it's a lot harder to make a snark proof because it's n log n but it's n log nvpk operation when a Stark proof is n log n but hash function. So the constant here is pretty significant and does have an impact on the proof generation.
00:30:28.474 - 00:30:38.462, Speaker A: So this is also why I believe that me, I mean polygon basically went only the star quotes because they believe that it's more scalable, at least in the short term.
00:30:38.606 - 00:30:45.460, Speaker C: But you should also bear in mind that long term we're probably going to be limited by data availability rather by proving times.
00:30:46.390 - 00:31:07.516, Speaker A: I mean we'll say I like to quote Kent in those cases, in the long term all dead. So from there, I don't know, I'm just a normal guy. Of course I'm dunking here at all. I'm just saying that this is why scare it more for Starks. Cool.
00:31:07.618 - 00:31:09.788, Speaker E: All right, you guys are done. Thank you.
00:31:09.954 - 00:31:10.410, Speaker D: Thank you.
