00:00:50.170 - 00:04:59.590, Speaker A: Sam. Sam. Sam. Sam. Sam ram. Sam SA. Sam ram.
00:04:59.590 - 00:08:45.186, Speaker A: Sam. Sam. It Sam. Sam. Sam. Om. Sam.
00:08:45.186 - 00:09:53.798, Speaker A: Sam om. All right, looks like it works. We're going to get started to introduce myself. I'mcram from Celestia. We'll be your MC today. Looking at the list here, it's looking pretty good. First off, we're going to talk about Roll Kit.
00:09:53.798 - 00:10:43.858, Speaker A: Who's heard of Roll Kit? Yes. Rollkit has made some buz and our first guest is Gabriel from the Roll Kit team who will speak about unleashing the power of open interfaces. Gabe. Can I click? I need a clicker. Good morning, guys. I hope you had a great week in Paris so far. Just here, I'll tell you about the story of Rollkit.
00:10:43.858 - 00:12:05.358, Speaker A: I'll go into what Rollkit is. I'll tell you about the motivation of this talk about open interfaces. I'll tell you what the interfaces are that Rollkit integrates with and like the D interface and the sequencing API and then wrap it up, presenting you the rollkit node and how it interacts with those components. So where do we begin? Mustafa wrote in his lazy ledger paper about lazy ledger, and one of the things that was still an open question was execution light clients. One of the current limitations of lazy ledger is that it's not obvious how to build light clients for applications so that clients do not have to download all of the applications messages to let the applications know the application state. So how do we solve that? John and Mustafa went to Defcon Japan and thought about this problem and came back with the idea that, okay, it has been roll ups, it always has. The trio of Ismail thought about and said, okay, we're going to implement an enshrined settlement layer which will be a roll up itself and other rollups will be able to settle on this enshrined settlement layer.
00:12:05.358 - 00:13:02.950, Speaker A: And the thing that the stack that they chose we was very close to them was tenement with Cosmos SDK. So here we already have the first interface, the ABCI interface that they had to integrate with and they had to make a decision. Either we're going to use tendermint and rip out the core, the consensus engine, or we're going to rewrite it from the bottom up and use a new P to P layer. And they chose the happy path because to have a cleaner repository and make it more general and usable. So optimint was born and in the first years, Ismail and Thomas were working on it. But in the last year, we expanded and rebranded. The team grew via rollkit.
00:13:02.950 - 00:14:13.280, Speaker A: We rebranded because we will not only support optimistic roles, but in the future, Ziki rollups as well. Okay, so where are we at? We have Rollkit in the middle. It creates the blockchain and it talks to the state machines through the ABC interface application blockchain interface and posts the data and the headers to the data availability chain. And this is us delegating data availability and consensus to data availability layer. So we are a roll up and we already see just with this one interface how much innovation it can create. So the first idea is hey, we have an existing tenermint chain and we will switch out tendermint with Rocket and we use the same state machine, the same application, switch to lines in the gomod and we will have a tenement chain. Now as a roll up, Crescent did that and have a Dex as a roll up, for example.
00:14:13.280 - 00:15:46.918, Speaker A: The other thing which is okay, I'm not going to use the Cosmos SDK, but the ABCI directly and a community member did that and used Fuel to make it ABCI compatible and basically created a Fuel Sovereign roll up on top of Celestia just by complying with this ABCI interface. And now what you can also do is choose a VM on top of the Cosmos SDK. That's what we're very excited about, what Argus is doing. Argus is building the world engine on top of rollkit and they use Polaris. Polaris is a great VM EVM that is built on top of the Cosmos SDK. And they had some talks and I think there will be another talk today, so please check them out. Okay? Now, what is the motivation here? You already saw that how ABCI creates innovation through just being an interface you can be compatible with and it creates an effect of collaboration because now we have a community, which is the only goal to build state machines through the ABCI interface.
00:15:46.918 - 00:17:16.630, Speaker A: And while complying with an interface you can innovate through different means. So let's say we have the datability interface and just because there exists this interface, you can now say okay, I want to build another data availability layer to comply with that and build it out. The next motivation is we will have interoperability and I think IBC is a perfect example for that, where different chains can use IBC together to communicate with each other, or I can use different data availabilities layers to switch them out. We also have transparency. Transparency is important because by just understanding that there is an interface and to see how the components talk to each other, we can derive functionalities and we will understand what kind of trust assumptions people make, how they interact with each other. And it will decrease the learning curve of people getting into this space because they will be able to focus on only one component and we'll be able to understand how these components work together. Okay, open interfaces make it possible to be future proof.
00:17:16.630 - 00:18:48.738, Speaker A: If I built my application and the underlying component would change, I would need to upgrade the whole stack, possibly. But if I can modulize it, I can only upgrade one and specialize it and this will give developers the ease of that. If you develop with such a framework, you'll be able to pick and choose and it will stay compatible for the future. The fifth point that I think is, I guess, a little bit nuanced and interesting is that when an application, let's say in web two, looks for funding, let's say it's Twitter, the people who fund the application don't really care about the underlying stack infrastructure stack. So it is either AWS or Google Cloud, but they will use some cloud provider. And the same also applies for web3. In web3, when we want to fund an application, we are also taking a bet on the underlying ecosystem and on the underlying infrastructure.
00:18:48.738 - 00:20:12.190, Speaker A: And if you can give the guarantee that, hey, you're not taking a risk on this ecosystem because you can switch out components in the future, you can give much more funding and capital to application developers because they will be able to make it future proof and the risk is basically eliminated. And the last point is lower cost and lower switching cost. I will reference the talk by Chris Gold about it's a type of mev. And his presentation was a TLDR, and I'll just make a TLDR of his TLDR. It's basically if a bridge can give you two options to access to the L1, the bridge can basically charge you as much as you would be charged as going through the L1. So they have the power, the monopoly to be able to take that. So what you have to do is keep switching costs low from one to another and basically have like a threat of fork.
00:20:12.190 - 00:21:12.210, Speaker A: This is the slow game how we introduced it to. So those sequencers or the availability layers or bridges, whatever, cannot extract that value because if they do, the other components would be able to switch and this would, of course, result in lower costs for the end user. Okay, so we were in the same developers get some water. Sorry, guys. All right. Thanks, Kalam. We were the same developers who did the DA interface for the Op stack.
00:21:12.210 - 00:22:32.190, Speaker A: Imagine what this now gives as an option for the Op stack to give to choose and pick the different data availability layer. And we saw the same thing happen with what we did in rollkit. So a similar data availability interface, and we chose to deploy the first sovereign roll up on top of Bitcoin. And now there was a huge craze and so many L2 S and whatnot are popping up on Bitcoin. And the only thing that we did is make bitcoin comply with this interface. And now we spawned a craze of innovation to see like, okay, how do we improve Bitcoin? Do we want to introduce Opcodes into Bitcoin to verify ZK roll ups easier? And this is a prime example of how you can spawn innovation through just an interface. So the goal would be to enhance the interface, make it even more compatible with other data availability layers.
00:22:32.190 - 00:24:26.258, Speaker A: One major goal is for Orca to stay credibly neutral to Celestia, which is important for to keep developing the modular ecosystem and to allow developers to create roll ups regardless of what data Availability layer they want to choose. And this will of course go forth with a more canonical Bitcoin implementation, avail, integration and then following more Data Availability layers when they come out. And the other part is that the moment we create those Data Availability clients for layers, other frameworks can also just comply from the other side, which means that the moment we have a new roll up framework, there will already be like those Data Availability clients that they can comply with. So new roll up frameworks can emerge and use those clients for the benefit. Okay, I'll give you a nice thought experiment. Let's imagine we have two Data Availability layers and we can now, because it's switch and choose, we can optimize as a community what we want to prioritize available blob space, cost per byte, latency economic security, liveness failure finality, not liveness failure. Well, I'll give you an example, right? Let's say we have two Data Availability layers and you don't have to pick and choose.
00:24:26.258 - 00:25:45.900, Speaker A: Let's say we have one that prioritizes finality and the other one that prioritizes liveness. So you can always have the finality favoring Data Availability layer. But if that fails, we can have the liveness favoring Data Availability layer pick up and us as a community, we can decide how we can interpret and read those bytes. The next interface would be the Sequencing API or the sequencing interface, right? When the modular stack evolves, we know we see specialized software and the same we see also with sequencing shared sequencing, emerging with Astria, Espresso, Fairblock and others. And we work together to come up with a common sequencing API with the goal to have a unified interface between all shared sequencers and roll up frameworks so we can plug and play this component as well. And of course this is a collaborative effort, so we have to decide as a community how to come to consensus. And it's great that we can work on this together.
00:25:45.900 - 00:26:45.518, Speaker A: And of course there's also based, right? I mean we can use the Data Availability layer as a shared sequencer. Now if we think further, okay, we have the Bitcoin implementation, now we have based. The logical conclusion is to make a social experiment on the 20 April there will be a based Bitcoin roll up. And what does it mean? Based means that I can delegate inclusion and ordering to the Data Availability layer. That means if I control ordering, I can also control reordering, which means I can control mev, I'm leaking mev to Bitcoin. And now I feel this will be a very interesting sight to see how Bitcoin as a whole will deal with mev. I'll leave it at that.
00:26:45.518 - 00:27:45.794, Speaker A: I think that will be a fun social experiment. To wrap up. We have the rocket node in the middle, we can talk to the state machine through the ABCI interface. We can talk to the sequencer through the sequencing API, to the DA layer, to the DA interface, and through the bridge to something like IBC. And in the middle we have the core, the folk choice rule, which we define how we want to follow the blockchain. How do you want to interpret that and how much power do you want to give to those components? And the exciting part here, why I love working on this, is because every single part is influencing mev, extracting mev, or giving back mev to the community. So this is a highly complicated and nuanced topic that you have to deal with.
00:27:45.794 - 00:28:57.154, Speaker A: So I encourage everyone to dive more into this research and to see how you can contribute going forward. And this is just the tip of the iceberg. I mean, the moment we have plug and play components, we can have crazy roll up variants and designs to optimize especially for your use case and like redundancy and picking multiple DA layers or having, for example, also multiple proving systems. I don't know. You can have the same data availability bytes and have a optimistic and a ZK roll up on top, a hybrid version. So this is another top of research that I encourage everyone to look into because the design that we created by decoupling components is exploding. Okay, to conclude, modular blockchains need open interfaces.
00:28:57.154 - 00:29:51.498, Speaker A: We have high dependencies between each other, so to communicate, we have to agree on standards. We need to keep switching costs low because we don't want to go into the slow game where one thing can extract the mev and keep it, but we have to have a credible thread of a fork to go to something else. Bitcoin will get mev. And the last part is rollkit is the openrollup framework, which will give you the opportunity to integrate with those things, those interfaces. Okay, thank you so much. Yeah, I want to go back. Okay.
00:29:51.498 - 00:30:15.570, Speaker A: I'll still say it. Please join the workshop. We'll have in 2 hours build a roll up on rollcit Dev, contribute to our GitHub, and most importantly, ask questions. I'll be upstairs with a whiteboard god my. Yeah. Thank you, guys. Let's roll.
00:30:15.570 - 00:30:54.000, Speaker A: Thank you. Thank you. Gabriel roll kit is a force. Okay, next we're going to have a panel, and I'm reading that the title is Build Whatever, which we'll figure out what that means. And the panelists include some big, heavy hitter names in devrel in this industry. So our guests are going to be Cammy Yaz Henry with David Phelps moderating. Please welcome our guests for a devrel panel on Build Whatever.
00:30:54.000 - 00:31:10.690, Speaker A: Can we not do the thing where I ask a question and we just pass it down instead? Just like talk openly. Let's have a conversation.
00:31:13.670 - 00:31:14.900, Speaker B: Curse him out.
00:31:15.990 - 00:31:33.178, Speaker A: Build whatever you want. Talk about whatever you want. We also have a special guest here today, dangle from scroll. Yeah, a little cameo appeared. I was going to just try to act like I was Tina, but whatever works. Yeah, I think you could do a fair impression. Hi, everyone.
00:31:33.178 - 00:32:29.146, Speaker A: I'm David Phelps. I am not a devroll. I am a founder of a project called Jokerace, where we build on chain contests that are really used for customer acquisition and retention, which means my job is talking to devrels all day because they're really, I think, the people in the ecosystem who do the acquiring and retention of our users who are devs. So I'd love to talk about that a bit. And I think to start, I guess, an open kind of question I have is what is the responsibility of getting people into this space versus what is the responsibility of getting people who are already in the space into your specific project? And how do you think about that? I can start. So when it comes to getting people into the space, you have to think about it from their developer persona. Like, are they coming from the gaming sector? Are they coming from the web two sector? And then for each one of them, you have a different strategy for onboarding them into your specific platform.
00:32:29.146 - 00:32:55.870, Speaker A: But for people already in the space who understand those technical complexities that come with webtree, you definitely have a different strategy for the developer journey for web3 people. So it actually depends on the developer persona that you're targeting. So for Celestia. I mean, we're modular. We go for everyone. Right? But there are priorities, obviously. Yeah.
00:32:55.870 - 00:33:27.840, Speaker A: It depends on what is the priority. So right now, we really like EVM developers because EVM is the biggest developer market for web3, and we're also looking at Cosmos. Like we focus on cosmos. And within that, people who want to build in DFI or Gaming or NFDS, each one of them would have a specific kind of developer journey so that we onboard them with their favorite smart contract language to whatever demo application they want to build, so that they know how to add complexity over time and build their dream products.
00:33:29.090 - 00:34:05.706, Speaker B: I think generally all the devrels here and all the devrels in general see themselves as educators. So I think all of us see our roles as bigger than our companies because people appear I don't really know exactly, but we've all worked at different protocols. No one works for one project for like, 20 years, and that's the only thing you care about onboarding to, I think, at least for me speaking, I like meeting people where they are. And for me, getting them onboarded into my specific project matters less just because, first and foremost, I believe in education. I have certain principles that I stand by. So for me, it's more important to get people thinking about blockchain and meeting people where they are. If it's like they know nothing about it, then we go there.
00:34:05.706 - 00:34:10.140, Speaker B: If they're like, I really want to spin up my own roll up, then we go there.
00:34:10.670 - 00:34:30.706, Speaker A: Yeah. As for me, I don't do what you describe. I don't have an overarching goal. I like educating people, but it's a mean to an end, and my objective is to get as many people building in my ecosystem. Right. So I guess one of the challenges where do you draw the line as to what people need to know? And that's what you were describing. Right.
00:34:30.706 - 00:35:01.942, Speaker A: You have to define your user persona. My perception is that especially when you're a small company and we're in a small ecosystem, you got to focus. So there are some people I just don't talk to. I have a bunch of resources to point them, and if they want to pick these up, I'm happy to help them. But the people I go after and I make sure they follow up, I tend to draw the line to a pretty small number of people. So I tend to focus on specific developer and a specific developer category. How do you find them and target them? Usually they find us.
00:35:01.942 - 00:35:17.070, Speaker A: Usually we don't do a lot of outreach. We spot people on social media who ask questions and then we follow up with them. It's really targeted towards certain individuals. Yeah, exactly. Interesting first candidate. Your approach is broader.
00:35:17.570 - 00:35:28.040, Speaker B: Yeah, but I think, like you said, maybe we just have different kind of profiles because before I was in crypto, I was an educator. I taught K to twelve computer science. So I just have a different view of what devrel looks like, I think.
00:35:28.410 - 00:36:03.040, Speaker A: Yeah, and I think for any of these right. Oftentimes devrels are a developer's first contact in the space with any project, whether that's through YouTube videos or being at a conference. We're the accessible piece of our projects. And so I'm happy to have lots of conversations with people that want to build on StarkNet or want to build anywhere else. Right. I think we have kind of a responsibility to give developers the best experience in the space, to keep them around in the space. That is the thing we're still trying to do.
00:36:03.040 - 00:36:27.960, Speaker A: And if that doesn't mean they land on scroll no real skin off my back, honestly. Right. I want them to have build the best products they can. And so if that means I can make connections to other folks, that's great too. All right, so let's make this a little bit spicier. I think everyone's given the nice answer here, which is we all want to grow the pie for the ecosystem and get more people here. But there are some l two wars going on right now, really.
00:36:27.960 - 00:37:35.280, Speaker A: And so I'm especially curious we have two great L two, if we can call if we even use that term anymore. Devrel is here. And there's been certain battles between, let's say scroll versus Polygon, I think has been like the WWE for modular people to follow on twitter. So I am also curious, how do you think about differentiation with competitors? You probably have an easy answer for this one with Cairo, which we'll come back to, but to what degree are you really competing with others who have very similar tech but maybe different Ideologies or different communities and different values? And to what degree does that play a role in your work? And Daniel, I'd love to start with you on this one. Yeah, I mean, I think you named out a lot of the things that are differentiators, right? Like maybe at the tech level, if I'm not talking to someone that's interested in protocol level research stuff, they might not be too interested in the difference between Zkevm backends and zero knowledge proofs. Right. And so there's no reason to really get into some of those details if we think that.
00:37:35.280 - 00:38:18.506, Speaker A: Again, I view devrel as a social practice, not just a technological education thing, which education is social too. But at that level, I think you can let how you interact with devs, how you engage them, how you see and try to meet their needs, be a differentiator. Right. And other teams do great devrel, and it's great to look at them and be like they're supporting their developers better than we are. What are we doing wrong here? What can we steal from them to better support devs? But I don't know. Maybe that's the too nice of an answer. But yeah, it's a differentiator for you.
00:38:18.506 - 00:39:25.342, Speaker A: I mean, those are the categories of differentiation, but what's the specific differentiator? Yeah, I guess it is, but there's another aspect too, where it's know the background context being like, obviously the polygon folks are brilliant, right? And the worst engineer at Zkevm is like a far better dev than Am. Right. So I think I can talk about what drew me to scroll, which is this kind of really true intention of being community first and trying to engage with individuals and less of a transactional relationship with the people that are maybe building on us. Right. And maybe that just kind of comes back to my interests personally. But I think we're all trying to solve a really hard question, which is if we're sticking to certain crypto ethos at least that I believe in, is that we're trying to solve multimillion dollar engineering, things like that's, the human capital that it takes to have this happen, which brings in VCs in the world. If there's VZ in the audience, I love you.
00:39:25.342 - 00:40:28.494, Speaker A: But we also don't want corporate capture of these protocols and the technology that we're building. Right. And so doing our best to make it clear and to make sure that there's pathways that say as we're building scaling solutions for Ethereum, we have to scale the technology but we also have to scale the community and the ethos and the things that actually make it special or else we'll lose Ethereum before we end up getting these billion users right. The way I approach differentiation, when people be like, okay, so why should I build on Celestia over ethereum because of 48, 44, or polygon avail or Eigen layer? And one of the biggest differentiators that actually drew me in was that we don't have an execution environment. And that's so liberating. We don't really care what people's execution environments are. So if you're coming from the EVM world, you can build a roll up on top of Celestia.
00:40:28.494 - 00:42:13.462, Speaker A: If you're coming from the Solana world, you can also build a roll up on top of Celestia compared to any what you would call competitors, which I don't really call competitors, but they're all attached to one execution environment, which is the EVM, and they're very limited in the kind of market share they can capture in developer space. Whereas because we're actually more like we don't have an execution environment, that for me was the biggest, differentiator any kind of l one that comes out right now with a unique execution environment, the way I see them is, oh, that's a cool new roll up on top of Celestia. So that's how I approach it. Can I ask you a question about that? I feel like there's something interesting there where as a product, your developer audience would seem to stray a lot from, I think these people just building smart contracts, right, like doing protocol design that needs your product is like a much deeper tier of knowledge and experience and other things. And so do you feel like you're put in a position to suggest a DA layer as a solution to things that it's not a solution for? Or do you just try to mostly hone in on these? I guess maybe the underlying question is how many roll ups do there need to be and how do you see that for talking to devs about what they're trying to build? Yeah, so the challenge there has been initially coming into Celestia, my assumption was like, the whole thing about it was deploy a roll up as easy as deploying a smart contract. But to capture most devs in the smart contract space, they come with their smart contract code. They're so excited, oh, modularity and stuff.
00:42:13.462 - 00:43:18.694, Speaker A: And then they go to the docs and the first thing they got to do is run a node and then deploy a roll up. And they're like, wait a second, I just want to deploy my smart contracts and play around with things, right? So that was initially an interesting challenge to try to address because you want to capture two different types of persona that emerged as a property of modular ecosystem. One is those smart contract developers, and then there's a new category that I'm calling roll up developers. And the roll up devs are what I would consider a merge between the application smart contract devs on the upper layer and core devs on the bottom layer because it allows you to mix strengths from deploying a roll up and then deploying your smart contract on it. But it was a gap in the beginning, and how I addressed it was we deploy testnet. So I have a big testnet driven development strategy for onboarding developers. So the first thing, if they're interested in an EVM, we provide them a test first so they can deploy their smart contract.
00:43:18.694 - 00:43:27.040, Speaker A: They're like, okay, I'm playing around with it, and then take them into the roll up. If they're interested to deploy a roll up, which should be like a few lines of code for them.
00:43:28.050 - 00:44:10.122, Speaker B: That's actually a good story because I remember like six months ago or something, I was helping the ETH. Denver team organize like a pre ETH. Denver workshop series and they were like, we specifically want to get people who were maybe thinking about hacking for the first time pulled all the way in. So let's do some really easy beginner workshops for something that they can come to East Denver and do and feel proud of. Like, yes, I did this one thing for the first time, and I remember that was kind of one of my worries when Josh, my friend at Celestia, was like, can we do a Celestia one? And I was like, well, these are people who maybe have never coded before, have never done anything in crypto coding before, so I don't know if they're going to be able to deploy their own roll up. I don't know if that makes sense. And Josh was no like, it's going to be easy.
00:44:10.122 - 00:44:34.546, Speaker B: And I was like, I don't know. There are people who don't even know how to set the solidity version in the compiler. And we ended up doing it and they ended up finding a way to address it. But I do remember at one point in Celestia that was something that I was worried about, is like, how do you capture the vast majority of developers when the vast majority of developers are not protocol developers, but rather app developers? So it's good to see how you guys have solved for that in the last couple of months.
00:44:34.648 - 00:45:17.722, Speaker A: Like, the answer testnets for every new architecture that you want to build. Let's say it's an emerging market kind of EVM testnet. L One that wants to become an L2 on top of Ethereum stuff. You can build an architecture for that. We have a thing called the Quantum Gravity Bridge, where it allows you to roll up to settle on Ethereum or any other settlement layer, but take data from Celestia. So for the application dev to be like, okay, that allows me to have an optimism roll up that is scalable for my application. But I don't know if I'm confident enough in running that roll up in a test environment.
00:45:17.722 - 00:45:53.966, Speaker A: So you provide them a testnet for that architecture for them to see the benefits of scalability. Others might want to do a sovereign roll. Up, which is what we call, that allows them not to pay a fee to the settlement layer and stuff to become sovereign in that sense. And people might be interested in that and see what that architecture is like. We provide them a testnet for that. So for any kind of specific architecture, we just give them an RPC endpoint and a Faucet. They don't have to really understand how it got deployed, but it allows them to deploy the application and see how easy it is.
00:45:53.966 - 00:46:21.894, Speaker A: And then it takes them to that next step after which would deploy their own node. Frankly, I mean, I'm very biased about nodes and stuff. I love running a node. I think everyone should be running a node, even a light node. It doesn't matter if it's Celestia or ethereum or any kind of protocol. Running a node should be really straightforward and I think everyone who's a developer should do that. But also you don't have to maintain one because that's like infrastructure level stuff.
00:46:21.894 - 00:46:37.146, Speaker A: So as a dev, it's nice to have something like infura or something else that actually improved your developer experience. But even though that's available just for to practice, you should be running a node just to practice, just practice for.
00:46:37.168 - 00:46:39.846, Speaker B: The end of the world. We all have to run our own node.
00:46:40.038 - 00:47:26.154, Speaker A: I think there's a recurring theme here, which is the tech is getting more complex every year, which means the learning curve for new people entering the ecosystem is getting harder. And so this answer create testnets, like create sandboxes for devs to play in. The person I'd really love to hear from here is Henri, because I think in some ways, probably Starquare has the most formidable tech, maybe perhaps we could say in the space. And so I'd love to hear how you're facing those challenges of getting people onboarded to Cairo, which even for deep devs is not often their background. Yeah, I mean, I agree with you. We do have interesting tech, but there are really interesting stuff everywhere and there's a lot of stuff I don't know about. So just we don't have the greatest tech.
00:47:26.154 - 00:48:03.446, Speaker A: We have great tech. Everyone has a lot of formidable, formidable sorry. So, yeah, so how do you onboard people on Cairo and how hard it is? Well, I don't know. Often the question comes like, why should I learn Cairo? Is it difficult and everything? And I have a few one liner, so usually I ask people like, hey, is solidity the last language you'll ever learn? Most people say no, right, because it doesn't make sense. The point is convincing them that it makes sense. Right. So it's showing them why it's worth learning a new language versus using something you already know.
00:48:03.446 - 00:48:41.170, Speaker A: And I think that at the end of the day, nobody stays in an ecosystem because there has slightly better tooling or the developer experience is slightly better. People stay for either a deep conviction they have in the fact that the tech is good and powerful or for the community, like David said earlier. And I fully agree with that. So I tend to focus on those stuff. Right. The developer experience is not great, of course, but this is something that most people can overcome. And if you explain and you take time to explain, then most people will overcome it.
00:48:41.170 - 00:49:34.980, Speaker A: Will overcome it. So we just focus on what's in the distance and the people around you. That's what we tend to do. Does that make sense? Yeah. I feel like Starkware is in a pretty cool position, right? Because on the one hand, you do have this barrier to entry, but it lets you kind of have that style, right, where you are finding people that are already interested and then you can pull them in a little bit. I think the quote that I think of is someone in your team has compared it to the eating glass, right? And Web Three is full of masochists that for some reason want to run up against the bleeding edge of technology and just hit their head against a wall a lot. But once you've done that, you've kind of created a community of people that have gone through this collectively, together, fully and they understand deeply what everyone else is kind of like working with and doing.
00:49:34.980 - 00:50:12.414, Speaker A: And it kind of creates this moat for your project and your product where a L2, right? You can take that smart contract. Our goal is to kind of be a boring product in a sense, right. We want to be as EVM compatible as possible. So you can take your code from Mainnet and put it on scroll, but that also means you can take your code off of scroll and throw it anywhere else. Right. And so we really have to focus in on figuring out what does it mean to build that community when we don't have a shared glass eating ritual. I think the glass eating meme is awful, but it is fun.
00:50:12.414 - 00:50:40.940, Speaker A: I'll say this, and we're at an interesting point now because the dev experience and the user experience is getting much better. So we're going to have to switch a little bit the way it works. And I'm really curious to see how we're going to get the community who are like super tight, eight class together, how they get familiar and nice with people who are not and who are not in the club yet and who won't have to smash their head against the wall.
00:50:43.870 - 00:51:28.034, Speaker B: I think I maybe had a similar experience from my time at Fuel because we also had a programming language that we created and we're trying to onboard people to and what I found was kind of the same thing as you, is that people are masochists and they're like there's barely any documentation. We have a new release every day, so my code breaks like every other day. The tooling changes every day. Like, my commands from yesterday don't work. But they were into it and they were in the form, helping other people answer these questions. They kind of became advocates for the community, for other developers. And I always tell people, they're like, oh, is it going to be hard to learn a new language? Am I going to be able to pick this up? And I'm like, I think all the developers, I always tell them, what's the first language you ever learned? They're like Python or JavaScript or whatever, and then I compare it to Solidity.
00:51:28.034 - 00:51:52.382, Speaker B: I'm like, how different is it really? It's not that different, like, just different syntax, but everything is pretty much the same. And that kind of helps them get to a place where it's like the language itself really isn't the problem. And I think Cairo is probably the same thing. The language itself really isn't that hard to learn. It's really not that different. All of the mental models for how to program are the same. You just have to look up, is it a colon? Is it a semicolon, is it a comma? And that isn't that hard, you just look it up.
00:51:52.382 - 00:52:04.850, Speaker B: But the real thing is, why are people coming in at Fuel? I think it was like we were building a different VM and that was something really interesting to developers who were trying to look beyond the EVM. So less about the language.
00:52:05.270 - 00:52:42.094, Speaker A: I see a lot of mistakes people make when it comes to languages. They look at, oh, we're designing this really good language that's going to replace Solidity or something like that, and that's valid. But the thing is, the best language never wins in terms of adoption. It's always what people are most interested in. Solidity is notorious for not being that great, but it still has the largest adoption, like Sway versus Cosmosm, for example. Both are rust based. Cosmosm is inferior to Sway, but yet Cosmosm dominates the Cosmos community.
00:52:42.094 - 00:53:27.286, Speaker A: Nobody's know all the things about the features added in Sway are much better than in Cosmosm, and yet Cosmosm dominates. And even though it's hazard things so it's always like the way I see it, I don't see, oh, Python is better than JavaScript, solidity is like, Sway is better than Solidity. I see where's the market share and I'll just go for that because I'm not going to be there to improve the programming language. I'm just going to go for the developers that are interested in the ones that have the largest programming language adoption. Yeah, this is interesting. I mean, something I think a lot about with community building is that there's almost a trade off between acquisition and retention. If users are very easy to acquire, they're often very hard to retain.
00:53:27.286 - 00:53:42.782, Speaker A: Because if they were easy for you to acquire, they're going to be easy for someone else to acquire as well. I think this is what you're getting at with the broken glass. Right. It's like it's very hard to acquire devs to your ecosystem. Once they are in, they are going to stay. Right. So hard acquisition often leads to really good retention.
00:53:42.782 - 00:54:05.930, Speaker A: And I think that there's almost a trade off here, right. Because it's like there are things we want network effects for, like languages. For languages, you want as many people as possible using that because that's what's going to dominate. Right. If Cosmosm Solidity take over, it doesn't matter what else comes along if they're dominant in that position. On the other hand, within your own project, like StarkNet, you might not want to get as many people involved as possible. You might just want to get the really good ones.
00:54:05.930 - 00:54:25.374, Speaker A: Right. And so there's different goals at play between trying to scale language versus trying to scale a company. And yeah, I'm just curious how you guys think about retention here and of users, too. Yes. So when we talk about acquisition versus retention, we're looking at a sales funnel, right? Correct. Right. But in Devrel, you can't look at it as a sales funnel.
00:54:25.374 - 00:55:28.630, Speaker A: You have to look at it as a cycle. It's more like a cycle that the developer journey has to be a continuous cycle. So on the acquisition part, this is what we call developer marketing. How do you get them to notice you, whether it's a hackathon that they see, oh, you're a protocol building a hackathon. There's a conference, like a Twitter post, a blog post and stuff that gets their interest so that they go to your documentation and then there's like five stages of that journey and stuff. But basically by the time that there's a retention at the retention model, you basically have, you know them so well, you know what they're looking for. They helped improve your product with feedback and stuff, and you're just thinking about developer success, how do we give them a grant and stuff? Because they're like they went through all of that journey and stuff and when they go get a grant or they maybe get an investment to start their startup with that new application they built on your protocol that actually continues the cycle because you can use that success story to pull a new dev back to their acquisition model.
00:55:28.630 - 00:55:59.374, Speaker A: So it's actually more of a cycle because keep in mind, we're not selling anything that requires you to buy something and then you sell it. It's actually open source code, right. So it's much harder to quantify it from a sales point of view rather than from contributions and project being built on top of that protocol. I mean, it almost sounds like we're building religions in a way, right, where it's like you want to draw somebody in so that they will then draw more people in as well. That's your point.
00:55:59.412 - 00:56:40.650, Speaker B: Here is Crypto and MLM. There's this thing that I use called the Orbit model that's used to describe how we grow communities. And I learned about it sometime last year, actually, while I was working in Crypto. And it kind of describes what yasa is, where it's not about how long do they stay in and then when do they leave and how much money did it take to acquire them or how much time, but rather a way of looking at it as like orbits, where there's an outer orbit, where there are people who are aware of you but don't really care, are not involved. There's people who are observers. Sorry, these are the Observers, and there are people who participate somehow. They like your tweets or they want to show up to your thing, they want to show up to your side event.
00:56:40.650 - 00:57:16.262, Speaker B: And then there's people that are building on you. And then finally there's like the core team. There's people who are diehards, people who are the advocates or whatever program you have who fall into that category. So instead of looking at it as acquiring and retention, it's like, where are they in that map? And for a lot of people, it's like a Ven diagram. Of all the different ecosystems, we're all observers to most ecosystems. We tend to be participants in a few, and then we're like the core of one or two. It's not like winner takes all, some zero sum, but rather, how can you get the most people as close to your inner circle as possible?
00:57:16.396 - 00:57:57.118, Speaker A: Yeah. And your strategy with that orbit model is basically for every orbit level, you're trying to think of a strategy to pull them to the next orbit level until they get to become fanatics. That's like the closest orbit to the core devs. These people are like your diehard fan. They see your core devs as superstars and rock stars. You see a lot in the past, especially with Ethereum, if you go to any ethereum hackathon, a lot of these hackers, when they see a core dev from the EF or from the EIP process, they look up to them, they read about them, they see them on Twitter, they see them on the calls and stuff. So these are your superstars.
00:57:57.118 - 00:58:52.198, Speaker A: And having those fanatics at that closed level. There's a lot of strategies you can do for keeping them in that orbit. Like inviting them to dinner or special limited edition swag or like hanging out with your core devs and stuff compared to people at the Farthest orbit where might have signed up to a newsletter and you're just sending them updates until something piques their interest that they go into your community and find out more. I kind of like thinking about it in terms of acquisition and retention. The orbit model, I hadn't heard about it, but it sounds interesting. But in terms of acquisition and retention, I feel like it's not exclusively a sales thing, right? I mean, it's about how you get people interested and do they stick around? They can go somewhere else. But I don't know, I feel like it makes sense and on my end, should devrel focus on acquisition versus retention? I think you should be aware of both because doing acquisition without retention makes no sense.
00:58:52.198 - 00:59:28.914, Speaker A: And you can't do retention if you don't have acquisition. Right. And from my perspective, what really impacts this is mostly like the maturity of your product. If the product is really hard to use, you have to focus on retention a lot. And you mentioned this earlier, right? And retention, we do a lot of retention through community stuff like you mentioned, because that's what make people stick around in my experience. And as far as acquisition go, we do a little bit of that, but it's not where we spend most of our focus now. We're again in a fun place where this is going to change in the next few weeks.
00:59:28.914 - 00:59:52.806, Speaker A: I'm super curious to see how this is going to go, but how do you quantify it? I don't try to quantify it. I don't try to quantify it and put numbers on because you mentioned community for the retention part. But in fact, community should be for both. Like with community you get acquisition, and with community you get retention. I mean, you can use community for everything. And we try to scale through the community. Everything we do.
00:59:52.806 - 01:00:27.734, Speaker A: At the end of the day, we don't want to be a central hub where everything goes through us. We want to empower the community to grow itself route all right. And to a certain extent this goes for tech infrastructure, but also for this kind of stuff. So, I mean, we use community for retention but also for acquisition. I look forward to the KPI session we have after this on our metrics here. But I think there's also something fun, I think that maybe in this panel unique to scroll, but I'm not sure which is that you mentioned dev personas a little bit. Right.
01:00:27.734 - 01:01:45.050, Speaker A: But more and more at Scroll I've been trying to evangelize internally that it's hard to call our devs a singular community. Right. We really have to start thinking about the devs that are building on our platform and consuming scroll as a product versus the devs that we want building scroll, right? Like we want people contributing to our core protocol. We want people contributing to our ZK circuits and building interesting things on the edges of the ecosystem that blur the line between core code and stuff that ends up living on an EVM, right. And for each of these, you kind of have to have different tactics and different ways to approach it and really think about what motivates someone that's writing halo two circuits in an open source context versus your DAP dev. It must be a much smaller community, right? I mean, the number of people who are able to write this kind of circuit and contribute to scroll, you don't treat a community with a few tens of people or hundreds of people the same way you treat a community with like thousands of people. No, I think that's kind of the point though, right? We have 500,000 people in our discord.
01:01:45.050 - 01:02:43.600, Speaker A: That's not a community, right? It's something very different. Right. And so I think most of the time when people in Web Three use the word community, they're talking about an audience. And I don't think that's something that we should be diluting the word community. And that's why I think identifying actually what are the sub communities, the set of communities that make up the scroll community lets us really meet the needs of those devs, right? Whether it's five hardcore people that really want to learn these circuits really deeply, or it's people. We have a decently large devrel team at scroll and we have folks that I think do general education super well and they'd be very poorly suited to be devrels for those circuits. And by letting people kind of focus on those things and know what they're good at and know where they need to push things off to other folks, really helps us kind of focus in on that.
01:02:43.600 - 01:03:05.766, Speaker A: You mentioned that one sector is smaller than the other. I don't think it matters because if you find me one really good core dev who's external to the team, that's worth a lot. My point is not in term of value, it's in terms of tool. Right. You don't communicate with ten people the same way you communicate with 1000, right? Of course. Telegram group. Yeah.
01:03:05.766 - 01:04:04.358, Speaker A: Because you want to follow the one to end relationship with developers. Right. But when it comes to the core protocol, there are a lot of programs and strategies you can do to actually align core devs, external and internal around your core protocol. Like the EIP process, which is actually based on IETF standards and Python enhancement proposals as a process, is something that's actually really well done because in the end, when you have so many external core devs, like in Ethereum, they call it cat herding because it's really hard just to get everyone together and stuff. But at the end of the day, you have a really dedicated community, even though it's small, of core devs who actually understand the protocol in and out, even if they're not paid or they're know an external researcher and stuff, they actually care and they're actually building that community. Like, look at Tim Baco. Tim Baco is not necessarily a devrel in the traditional sense, but he does have that devrel persona.
01:04:04.358 - 01:04:42.130, Speaker A: When he would go on every EIP call for the all core devs, he would live tweet about what people are saying and then he became the coordinator of the EIP process after that because a lot of people follow what he's saying. There was a lot of debate in Ethereum. There was like, hey, what's updates on the merge? And stuff like that. So yeah, even it's a smaller community, it's still like that's the community that changes everything related to the protocol. If you look at Geth, there's probably like 30, 40 people in the world who understand Geth in and out. Right. So even though it's smaller, these people are super valuable because geth is so important for ethereum.
01:04:42.130 - 01:05:42.666, Speaker A: I think you're going to add something that I'm really interested in, which is Attention economy. To what degree is important to play into what you referred to before as rock stars in the ecosystem. Right? If you can get one of those rock stars in the ecosystem, that's going to do a lot of damage for you. Cammy, this is really for you. I think you've been pretty unique in this because you've developed, I think, a persona as a creator in the space, which is you're gen Z devrel, right? So people follow you and they know you as a personality, not just as a dev. They know you for where you're going, what you're doing, and it's like people love you for yeah, and this is what Yaz was saying before, too. To what degree is important for everyone here to be a rock star? Or to what degree is important to think about Attention Economy and how you're commanding this new world we live in of the creator economy, where you are a creator, or at least maybe some of your devs are also creators that command wide audiences and people care about on a personal level as well.
01:05:42.666 - 01:05:45.180, Speaker A: This is really a question about brand and brand building.
01:05:47.470 - 01:05:53.370, Speaker B: I'll start by saying I don't think it was a purposeful thing. I think it just happened over a long period of time.
01:05:53.440 - 01:05:54.670, Speaker A: Personality was just too strong.
01:05:54.740 - 01:06:14.318, Speaker B: Yeah. Vibes were too good, and then people were just, like, feeling it. No. But I do think that in devrel, a lot of your job is to be on the forefront of technology. So it's like this new thing exists. You're one of the first people in the space that knows how to do it. So you have to make the video, you have to write the docs, you have to make it possible for everyone else to know how to do it.
01:06:14.318 - 01:07:05.394, Speaker B: And I think because of that, it kind of creates this image of rockstar ism where it's just by nature of you're one of the first people in the space who's able to do something, because you're the closest to the devs who made it. You're the closest to the people, the John Adlers of the world who you can sit down and be like, hey, how do you do this? And then you make it palatable for everyone else. But how important is it? Honestly? People ask me this all the time. Teams come up to me and they're like, how important is it to have a Twitter personality be my devrel versus just someone who's a good engineer? And I always tell them it's not the most important thing that's not the thing you should be optimizing for. But it is important, I think it is important, and not just because for your brand, but because brands are built around people. They're not built necessarily around tech. Because like Danny was saying, there might be a point where your tech is so similar to ten other companies, especially as the L2 wars rage on.
01:07:05.394 - 01:08:00.630, Speaker B: I think in a year, there'll be five to ten more EVM L2 S. So it's like, if they're all the same, the differentiator isn't the tech. It's not like the I don't know, the low level stuff that matters, but it's how does the people on your team make everyone feel? How do you lead a community? It's just like, how do you make people feel? How do people feel about you? And how do people trust you? I think when you're a devrel, it's more about trust. Someone asked me, I posted a picture of you playing basketball here in Paris, and they're like, do you ever work at these conferences? And then I replied, I was like, Deverel is a long game as a joke, but it's true because it's not about me trying to sell you whatever project I'm working on right this instant, and you have to use it today. But it's a relationship that gets built over many months and sometimes many years. And it's something that in a year and two years, when I'm working on something, people are going to be like, oh, I trust what she's working on. Because over the last two years, I've seen her be like, XYZ adjectives, and now I have trust in this person.
01:08:00.630 - 01:08:02.534, Speaker B: And that's how brands are built, I think.
01:08:02.652 - 01:09:02.458, Speaker A: Yeah, I mean, this feels like a huge shift that we're seeing across all of culture, which is that brands are now a reflection of the people who work for them, rather than people being a reflection of the brands they work for. And yeah, I'd be curious. Everyone else is thinking about this question of how important is it to be a Twitter personality? My strategy has always been to make the core dev the rock star. So I tried as much as possible to create that culture in Ethereum that allowed for core devs to actually become more visible and stuff. Now, it's kind of tricky because core devs, they don't necessarily want to be very public facing and stuff, and they don't have to be right? But people should know who the core devs are. So I focus more on actually making the core devs on my team more visible. And there's a lot of ways that we're planning to do that and stuff, because I want an element of trust in who is building this protocol and who they are and stuff, and to follow what kind of features we want to implement, the trade offs and stuff.
01:09:02.458 - 01:09:34.580, Speaker A: When it comes to me personally, I've been in the space for six, seven years I don't really care about building a Twitter persona. I think it's important for a lot of people. But for me, I'm not tweeting that much. I tweet maybe like spicy takes every now and then, right. But it's not my way of trying to pull in people to the space. I'd rather utilize more the protocol Twitter account for that kind of stuff. For me, I think that's more for the personal brand, and personal brand is super important.
01:09:34.580 - 01:10:22.370, Speaker A: But I think getting people into that space doesn't only have to come from your Twitter account, it can come from different Twitter accounts, and that can be utilized more. Yeah, I wish I'd lived in a world where personal brand was a phrase that didn't exist, but it does. I think core tenets of devrel values that you just have to have are accessibility and empathy. Right. I think the Twitter thing can be a double edged sword, where one, if people feel like they kind of know you and they can kind of know your personality, it makes them feel very free to come up to you and start a conversation and tell you about what they're building and ask you questions and kind of breaks down that barrier. So for accessibility, it's incredible. Right.
01:10:22.370 - 01:11:27.094, Speaker A: But I think also if your persona and brand pushes too far from letting you actually be real and empathetic to needs and people come and start seeing falsity there, that can be an issue as well. Because I think what it can do is as a devrel, it can push you to have transactional relationships as opposed to ones that are more long lived and more, I don't know, actually worrying about people as opposed to the transaction. So does personality matter? I'm not very good at it, so I said no. But joke aside, I'm kind of like you guys. I don't like that much making things about me, but I see the power of this and people follow people. So you need people leading the pack. And then again, I think my approach would be slightly like the same as you shine light on people who can lead other people, I would tend to shine light on people in the community, people who have interesting life.
01:11:27.094 - 01:12:04.560, Speaker A: Stories, people who are doing inspiring stuff, people who are accessible and trying to instead of being the light in the dark that leads people, being the catalyzer that makes various lights shine. And if possible, ideally outside the core team and outside the lead developer because it helps with decentralization and it helps multiply viewpoints. All right, well, this is all the time we have. I just want to say everyone's being modest on this stage, but these actually are four rock stars of the ecosystem. So big round of applause for everyone here. Thank you, guys. It was fun.
01:12:04.560 - 01:13:23.640, Speaker A: Okay, so that concludes our developer theme. We're going to switch gears here and go into gaming as it's a big hot topic. In the modular stack. So to kick things off we're going to invite David Zhu from Argus Morris and David Brillenbraugh. Please welcome to Kick Off Gaming in a Fireside Chat building and funding on chain games. Check, check. How's everyone doing today? Hopefully everyone's doing well.
01:13:23.640 - 01:14:27.370, Speaker A: Would love to kind of start this panel with some great people that I've come to know and research for a little bit and so the first kind of question I'd love for y'all to do is do a quick intro, talk about yourselves, what you do and how you got into gaming. Hello, nice to meet you all and thanks for coming. My name is Morris, I'm the co founder of Promodium which is currently a fully on chain game that you can play it's live. Before Promotium I built a crypto product called Critters which was basically a play to earn version of minecraft. Unfortunately Microsoft decided to pull the plug on it so we moved to building fully on chain games afterwards. Also figured out a bunch of theses along the way of building that before building stuff in crypto I was your typical B, two B SaaS startup founder. We went through YC in summer 2019 and have been building our company since then and yeah that's about my career background.
01:14:27.370 - 01:15:01.080, Speaker A: My name is David, I run a venture fund called Dune Ventures. We invest at the intersection of gaming and technology. We do everything from Web three to VR to AI and all the above. We've been investing for the last three years and yeah awesome. Thanks for the introductions y'all. So I think today's panel will be a really quick chat about things from about onchain gaming both from the builders and the investors perspective and hopefully we'll have like a casual conversation with some hot takes sprinkled throughout. So yeah, I'd love to first start off with kind of the general kind of questions we were thinking about.
01:15:01.080 - 01:16:07.626, Speaker A: I think the first question is just are on chain games even made for normal gamers for the time being? I don't think so and I don't think it'll be for a very well for quite a while at the very least. The reason why is just because if you want to get people to play well, the main reason, in my view, to build an on chain game is because when you have decentralized scarcity, well, when you have decentralized consensus for every in game action and every in game asset, all of these in game assets and so on have real value. And this is something that people in crypto specifically are happy to ascribe real value to. So kind of like Bitcoin and NFTs and so on, such that normal players, while they can understand that it will take a lot longer to grock. And so you can launch a game, put it on Reddit or put it on Steam or something like that. A lot of people will try it, but pretty much nobody will bother sticking. Whereas if you give that to a crypto audience, a lot more people will stick.
01:16:07.626 - 01:16:47.690, Speaker A: And so it's like, well, at this point, if you're trying to launch to a normal audience and like 99.9% of them churn, then it's a bit of a fool's errand at the moment to focus fully on that. Yeah, I think the short answer is no. But I think the more interesting question is what is a gamer? I think if we look back 15 years ago, I think if you had shown mobile games or Candy Crush to a World of Warcraft dev, he would have told you that's not a game. And so the question is, how do we define what a gamer is? And does crypto gaming and crypto gaming audience just change or expand the audience definition? I think that's the more interesting question. So I think the short answer is no. I think crypto games will be an entirely new audience.
01:16:47.690 - 01:17:38.998, Speaker A: Sounds good. So kind of extrapolating from that kind of crypto gamers being a new audience, I think you've talked a little bit about normal gamers as well. That's kind of how the initial question. We'd love to kind of see what you all think in the next advent of when on chain gaming will become a trend or will become some kind of category within gaming, how will a normal gamer ever find an onchain game? Will it be through? Do you think traditional platforms will embrace this kind of onchain game narrative or will be like entirely new avenues? Well, right now the primary methods of user acquisition for on chain games and the like is Twitter. Right. And Twitter has normal people on it. That's not involved in critter sorry, that's not involved in crypto.
01:17:38.998 - 01:18:36.090, Speaker A: And so at that point, yeah, I think that the traditional channels would be that. But then again, the focus is that do you really need to focus on traditional, as in non crypto native people to be playing your game? And I would argue the answer is no, because if you look at DeFi, for example, there's clearly enough of a market size of people familiar with crypto participating in it, rather than needing to really onboard whoever else that aren't already familiar. Yeah, I don't have much to add. I think the interesting thing about crypto is you have entirely new viral distribution mechanisms you can use, right. Which is like Gabe Lynn uses this term, like free to own, I think it is, and basically using tokens as a way to onboard users. And so I think there's a lot of really novel ways we don't understand today that we'll use to acquire players. Awesome.
01:18:36.090 - 01:19:14.040, Speaker A: Yeah, that makes a lot of sense. So, kind of moving on, I think here's kind of the first kind of question that could lead to some discussion. Think a little bit about UGC, like user generated content, stuff like that. How do you solve the dilemma that on chain games aim to thrive through Composability, but players mostly create UGC or use gender content if the game is already thriving, such as Minecraft or Roblox. Yeah, so that's actually one of the main reasons why I don't focus on Composability for the time being. It's something there, it's something people can do. We're built on top of Mud, so people can just add more systems and components if they'd like.
01:19:14.040 - 01:20:37.522, Speaker A: But that's not the main focus of why people should play the game. So I think there are a couple of games out there that they primarily focus on the part that you can play this aspect of the game a little bit differently just because now you can write specific rule sets between players and so on. But as you said, that's not something that players are going to bother doing until the game is much more advanced, if you will. And so that's why, as I was going back to it, you have to really figure out, well, aside from Composability, then what kind of features does an onchain game provide in terms of entertainment for players? What kind of features does it provide to players such that they can't get anywhere else so they would come play your onchain game? In my view it's basically an in game economy that is real. So real as in the sense that they are denominated in real value like Ethereum or USDC or whatever else, and that the game is completely decentralized such that the economy isn't really owned by anybody and something that's very similar to Eve, except everything is of real value. This aspect of building an in game economy is a lot harder to build if the game wasn't on chain because of regulatory problems, for one. We can dig more into that.
01:20:37.522 - 01:21:16.030, Speaker A: And also the other side is Oracles are pretty insecure and so when you have an entire game, entire logic on chain, then you can have a much more secure game economy that isn't really as vulnerable. Nothing add. Okay, sounds good. Yeah. I think we could dig into the regulatory side a little bit from both the builder and the investor perspective. You kind of mentioned a little bit about that. What are some of the kind of regulatory kind of insights that you have both as a game builder also maybe as an investor? So I'm not a lawyer.
01:21:16.030 - 01:22:43.450, Speaker A: This is not legal advice, but this is, to the best of my understanding as as somebody building this space. So the main reason why most games today don't allow you to cash out, like for example, if you have a sword or something and you get like 40 coins out of it, they don't let you convert that 40 coins into like $40 is because of a thing called money transmission rules. So basically if the company is the one that's like if your currency in game has real value and the company is the one that's helping facilitate the transactions of people getting stuff to one another. This is only pertaining to the US as well. I know we're in France, but as a yeah, so you have this money transmission problem that if your currency is of real value and you're transferring money from one player to another, you'd be considered a money transmitter. The thing about on chain games is that even if people are transferring assets of real value to players, you are not actually the entity that's doing that for them. The ethereum network is So what you are as a game company is more of a peer to peer service provider that you link up two people that want to send stuff to one another as opposed to you as the company that's facilitating that.
01:22:43.450 - 01:23:30.940, Speaker A: Now that's not to say that you do that and everything's all fine. Now obviously if there are money laundering that's happening on your platform, it doesn't matter if you're not a money transmitter or you don't have to register as one. But yeah, that just means that compared to a normal game in terms of regulatory stuff, there's a different architecture that you can do in terms of building the game such that it doesn't touch on these roles as much. There's a very good article written by Amy Madison who is talking about web3 gaming regulatory stuff. I would recommend you guys look it up and read through that. Yeah, I don't have much to add. I think the short is we just don't know.
01:23:30.940 - 01:23:59.234, Speaker A: Money in games is very complicated legal topic. Things like skill based gaming, et cetera, have different regulatory regimes. Depending on which US state you're in, you need different licenses and opinions. Some traditional gaming companies we work with have to get opinions from 50 plus states and that's just the US. And it doesn't even include massive countries like China and India. So yeah, the TLDR is that it's going to be very complicated. We don't know what it's going to look like, so we're very early.
01:23:59.234 - 01:24:37.614, Speaker A: Also, I'm talking primarily in terms of money transmission because that's what pertains most to the type of game that we want to build in terms of having an economy and having people trade with one another. But there are other considerations like tokens, whether or not they're securities and so on and so forth, that fortunately for us, we don't have to think about that too much in terms of the way we're designing our product. But that is also a big topic that isn't covered here yet. Absolutely. Yeah. Thanks for sharing that kind of perspective. I think we can probably move on to a little bit more of a builder perspective and obviously if there's any kind of investor takes on it as well.
01:24:37.614 - 01:26:06.310, Speaker A: I think the first question is what is the hook for onchain gaming as a challenge? Can you phrase in a way that does not exist inside the traditional gaming market? Yeah, so that's going back to what I was kind of hammering on again, which was just that it's the fact that the analogy that I like to give is and Matt Levine and Satoshi has kind of written about this in better terms than I can put. But basically if you have bitcoin running on one person's computer saying that I've got one bitcoin, you've got three bitcoin, and so on just one person's computer, nobody's going to bother ascribing real value to that bitcoin there. But as soon as you put it on a network, and as soon as there is decentralized consensus now, there is provable scarcity of that one bitcoin and three bitcoin and so on, such that people are happy to ascribe real value to it. Now, whether that real value should be a dollar or $30,000 is up to debate. But the point is, because of this product architecture, now all of a sudden people are happy to ascribe real value to it. And that in my view, is the main product market fit of crypto as a product, if you will. And so when you kind of push that out to a bit of a logical extreme and you build a game entirely on chain, in this case, basically you get to create a world, a story and so on around these numbers that are running on chain.
01:26:06.310 - 01:26:49.706, Speaker A: So you can build a story around ethereum, you can build a story around bitcoin. Obviously those already have its own myths and so on that have been built. But when you build an on chain game, you get to do that entirely from scratch, with your own resources, with your own tokens and so on, such that people are happy to ascribe real value to everything that goes on in there. And that is, in my view, a very different way of looking at it from your traditional game. Such as if I made a carrot farming app today and it was spawning ten carrots a day and so on, nobody would bother ascribing real value to that ten carrots. Just because it's not on chain, it's not crypto, it's not real. And that is the way that people feel it.
01:26:49.706 - 01:27:13.986, Speaker A: And so if you kind of look at on chain games, that is the different feeling that people get from playing something that is on chain versus playing something that isn't. We actually see that today. That when we have a testnet version of the game. Sure, it's live, and it's a game that you can play. It's just the same as any other web flash games that you can play from anywhere else. It's just the same. But it's not live.
01:27:13.986 - 01:27:46.282, Speaker A: It's not real because it's not on main net and the assets aren't real. And so that's my view of what the differences are from off chain games and on chain games, if you will. Yeah, I think the only thing I have yeah, I think that's right. I think it's the addition of value is going to be at least the initial hook to why people play games. Right. I think value has existed in games for a long time, but tends to happen in sort of gray markets or dark markets. When you think about like yield farming on World of Warcraft or Diablo or whatever it is, those things happen.
01:27:46.282 - 01:28:03.250, Speaker A: Value is transacted, but it's very, very difficult. Publishers don't like it for things like regulatory and legal reasons. They don't want to get money transmitter licenses. And so the idea, hopefully is that blockchains make it much easier to transmit value across games. The value exists. It's a question of unlocking it. Awesome.
01:28:03.250 - 01:28:33.194, Speaker A: Yeah, thanks. I think a quick kind of follow up there. I think there's taking the devil's advocate or kind of the opposite perspective of do we really need to put everything on chain? For example, there's some people who say like, computationally intensive things don't need to put on chain or only certain assets, especially within play to earn or GameFi. It's only the tokens or the assets, not the actual gameplay. Yeah. In my view it's a security. Not securities, but security in terms of vulnerabilities and so on.
01:28:33.194 - 01:29:06.946, Speaker A: And so in our case, when we were running our previous project, basically we had this in game economy. It was a Minecraft server that had tokens that are in there that people can buy and sell different items. It's your typical Minecraft gaming economy. Sorry, Minecraft server economy, where Minecraft has a lot of items people buy and sell it to one another. So there is a way that you can do it off chain, which yes, it's computation, and also it's Minecraft, which is an off chain game. Right. So everything was computed off chain.
01:29:06.946 - 01:29:47.746, Speaker A: And then there's an Oracle that gives tokens back to them based on kind of how they've transacted and so on and how much time they played. It was a playdoh and implementation. So how much you played Minecraft and so on, you would not believe. Well, maybe you could, but people were spending a lot of effort and a lot of time every single day just trying to break the Oracle. And then if they do, they'd start by salami, slicing a little bit, and then afterwards they come through and then now they're exploiting like 50 ethereum at a time. That was happening a lot. Well, we were catching most of it, but it's like even one time it's a lot.
01:29:47.746 - 01:30:35.378, Speaker A: And so when you have an Oracle for an off chain compute, I think we could talk about ZK roll ups, which is probably a little bit different, but I'm not enough of an expert on that as of yet. But either way, when we were doing an Oracle there, it was extremely insecure and people were trying to break it all the time. And it's just like, well, if you can try to duplicate a Minecraft diamond, like twice, you could just try to sell it. If a diamond is like ten cents. Now you'll just try to exploit that to get like 500,000 diamonds. That would be the main reason why I would think most stuff have to be on chain. But obviously there's some breakthroughs here that are making it so that security will be less of an issue there.
01:30:35.378 - 01:31:01.674, Speaker A: Yeah, I just think of it as like a design space. I think some people will do like Web 2.5 where it's only NFTs on chain and some people will do everything on chain and they'll have different benefits and trade offs. Some will be more massive peel the less on chain, probably the more massive peel, at least in the short term, but maybe more value you have and more vibrant community you have if more things are on chain. So I think it's just a question of the person who's building the game and the design trade offs. Absolutely. So yeah.
01:31:01.674 - 01:32:05.390, Speaker A: Now digging kind of like the future of onchain games or games in general, do you all feel like the web can become a solid platform for games? Again, I know Primodium is built on Web for now and I think kind of for context for the audience, like parallel trading card game. They have a downloadable on Windows Obaby which is a upcoming games has like a desktop client launcher and Alluvium also has a downloadable what are y'all's thoughts? So this may be maybe a bit of a naivete from my end, but I think most people have metamasks only in their browser and don't want to bother figuring out like if they download a thing, just how do they connect web3 wallet to it. Then maybe you'd have like, I don't know, email, sign up, so on and so forth. For one, I personally actually like web games a lot, but I know I'm definitely in the minority. For two, these are just clients to put the game on the web. These are just clients. You could build it in Unity and then it'd be cross platform instantly.
01:32:05.390 - 01:33:05.506, Speaker A: Maybe not instantly, but either way, the point is I think the web is like web as a client is good just because that's where most people do their crypto stuff and so if they have their metamass wallet there, that's just the easiest way of doing it in my view. And yeah, if there is a need to have it be cross platform, then there are pretty easy ways to port a game from the browser to an app and so on. Yeah, I think there's been a lot of development on the browser side from a technical perspective that now enables you to have sort of near native performance in terms of games or 3D graphics on a web browser like Chrome. And so it used to be a technical question, I think in terms of can you build performant games on browser? And the answer was usually no, at least on the higher end. I think that's changing quite quickly and so maybe we'll see a renaissance of browser. I'm a little bit skeptical, but I think it's definitely a possibility. Yeah.
01:33:05.506 - 01:33:33.286, Speaker A: There's also phaser that's quite good for browser based games. It improved our game performance by a lot. Awesome. Well, I think that kind of wraps up the builder side of things. I think we're going to dive into the more investor focused. We'll start off with a heater within the onchain game space. It says we wrote like most people and teams working on on chain games right now have never had much or any experience in making games.
01:33:33.286 - 01:34:10.930, Speaker A: Are investors supposed to assume they can just learn about game making through trial and error? I'll take it I can comment a thing or two, but I'm not an investor. I'll take it then. Yeah, I differ from most investors in the sense that I'm actually quite uninterested in backing insiders. I think if you have game development experience, it might be even a negative. You have too many biases, just like you wouldn't want an Activision Blizzard developer building a mobile game, they just wouldn't get it. You want some dude in Finland to build the game who had never built games before. You need an entirely new kind of open design space to think about on chain games.
01:34:10.930 - 01:34:44.930, Speaker A: I think it's been true for most of the history of technology that you want outsiders for almost anything big. I think it's definitely been true in media and games. It's always net new players, not existing players. And so I think it'll tend to be true for launching games where the majority of the really big companies will come from outsiders. It'll come from probably College Dropouts or something like that. Tim Sweeney built a lot of the architecture for 3D graphics and now obviously Fortnite, and it's true for mobile games and it's been true for almost every big shift in games. So I'm sort of uninterested in investing in insiders.
01:34:44.930 - 01:35:35.954, Speaker A: My view is the main thing about younger founders, based on all the folks that I know around me, is that by definition, if you have two college kids working in the dorm room, by definition it's not a defensible product. But it is, especially when they have a new insight that people who didn't live with the technology or so on just don't understand it nearly as much. And so when you have a new design space, like what David said here, I think that's where people that don't otherwise have existing exposure, existing biases, would probably shine better than people that have been just been in the space, been trying it out much more. Yeah. And you see it in the decisions they make. Right. I think if you look at the Web 2.5
01:35:35.954 - 01:36:09.150, Speaker A: space where it's heavily traditional games with NFTs added to it with little innovation, it's mostly existing developers that have built games at previous companies. If you look at people building on chain games like primordial Morgue, it's net new founders that have never built games before and are learning, I'd say it's really hard to build games. Obviously they're the most complex form of media, but it's happened many times before. Most of the big games that you play were built by modders or by outsiders. And so yeah, I'm sort of not concerned. Awesome. Yeah, thanks for those answers.
01:36:09.150 - 01:36:39.398, Speaker A: Here's. Kind of a point on investing, I think. What's the point in investing in infrastructure that powers games or the games themselves? 90% of the value of the market accrues to content always, or IP, however you want to describe it. But we're at an interesting phase in the market where there's no essentially core infrastructure scalability. And so the question is, do you focus on content? Do you focus on infrastructure? I think that's actually a psyop. I think the right answer is actually both. You want to build both core infrastructure while you build content.
01:36:39.398 - 01:37:30.230, Speaker A: It's the hardest fattest startup you can build, but it's actually probably the right startup. The most important companies in media or games tend to build both. If you look at sort of Epic games, they build three architecture, they build Unreal Engine and then they also built unreal Tournament and they also built obviously, Fortnite. And so you look at that same in kind of Niantech building AR games, they build core AR infrastructure and they obviously built Pokemon Go. And so I think it's going to continue to rhyme true that the best companies are full stack, they build core infrastructure, world engines or whatever, and then they also build content on top of it and the content dog foods, the infrastructure. And so it's actually a very important two way parallel street where you have a deep understanding of what's enabled by the technology and then you're also building and iterating it through content. So you want to own the most valuable part of the stack, which is content, but you also want to have a deep understanding of our underlying infrastructure, which can be a massive moat or loss leader.
01:37:30.230 - 01:38:42.874, Speaker A: Yeah, I don't have too much comments here because again, I'm not investing. But is on chain gaming a different platform such as Mobile or VR gaming, or is it a different means of development, composability, et cetera? Yeah, I think that the interesting question is is on chain gaming mostly a monetization innovation, or is it a platform innovation or distribution innovation? Right, and I think we look at I think a good way to think about it is like Mobile was a platform innovation, new ways of distributing to a billion plus users, but it was also a monetization innovation in the sense that the apps are created free to play gaming at scale, which wasn't really a thing before. And so I think crypto will end up being both where you obviously have a monetization innovation using on chain payments, but you also maybe have a platform innovation and so it'll be interesting to see. I think the monetization innovation is a very obvious pathway that a lot of people are taking both in Web 2.5 and on chain games. I think it's more unclear, like the more pure on chain games, like what the value add is there. We're also seeing building an onchain game as also a back end innovation, just architecture wise.
01:38:42.874 - 01:39:42.722, Speaker A: And so yeah, that's the way that we're looking at it. Awesome. So I think we can probably start wrapping up. I think the final question that I have is really kind of for both of you all, I was thinking, how can we better align as an on chain game community to innovate and advance more effectively both from the investors and builders perspective. What do you mean by alignment in this just moving like net benefit, moving forward, open sourcing, things that we can do, ways that we can contribute? So far I think the community has been pretty nice to everybody and so I'm not quite sure if there's an issue with alignment for the time being. Like all the investors are very happily trying out everything and also giving feedback. The open source contributors are obviously looking for everybody to develop on their thing and also trying to get more people on board.
01:39:42.722 - 01:40:07.170, Speaker A: And so there's hackathons, there's, community meetups, there's so on. And I think all of that is coming together quite nicely for the time being. Yeah, I'm actually leaving Paris. I'm quite optimistic about the on chain games community. I think if you contrast it with the Web 2.5 space, rugs are probably the right word to use. It's very NFT drop heavy, feel like I'm getting scammed out of anything and no value being added.
01:40:07.170 - 01:40:36.354, Speaker A: And then you look at the onchain game space. I was at Hacker house on Wednesday and I'd say four or five of the top builders in onchain gaming were all together in the same room collaborating. Even though they're all making different decisions around architecture and design and all this stuff, they're all collaborating, all open source, all trying to push forward what we can do technically on the on chain game space. And so it makes me very optimistic about where we're headed. Awesome, well, thank you all so much. This is a great conversation and yeah, we should be good. Thanks for coming.
01:40:36.354 - 01:44:27.260, Speaker A: Thank you, David. And David. It wasn't able to make it. Give it a minute or two. A little more. Sam. Sam, don't.
01:44:27.260 - 01:46:11.760, Speaker A: Sam. Sam. Okay, so we're going to get started and for those of you who are live streaming, tune in. To deepen our gaming theme, we're going to introduce the founder and CEO of Argus Labs, scott Sonarto. To talk the inner game thesis endgame for on chain games. Please welcome Scott. Awesome.
01:46:11.760 - 01:46:51.390, Speaker A: Thank you. Cool. Hello everyone. So today I want to talk about a little bit that I have been very passionate about and it is about thinking about the. Practical thesis for onchain games. For a long time we've talked about onchain games from perspective of really kind of hobbyists, right? Thinking about what could the future be where the world is on chain. However, we haven't really thought about how could businesses succeed in an environment of fully on chain games and this is what the InterGame thesis is going to be.
01:46:51.390 - 01:47:41.028, Speaker A: Oops to just quickly introduce myself. My name is Scott. I'm the founder of argus. Previously I was co creator of Dark Force, the first full on chain Mmrts game built using ZK Snarks. Before we kind of get on to what the InterGame thesis actually is, I think it's useful for us to have a quick of background of how onchain games have evolved in the past few years, since the Phase Zero, which is around 2020, when we started thinking of publishing Dark Forest and creating the first fully on chain games. So Dark Forest, as some of you might have already known, is like a fully on chain space exploration Mmrts game on Ethereum powered by Cksnarks. Back then, not a lot of games really exist, not even the current thing that people think about as crypto games.
01:47:41.028 - 01:48:40.808, Speaker A: And Dark Force became this beacon of hope for a consumer application. On Ethereum, we use the blockchain not only as a way to host NFTs or host in game assets, but to also as a way to have user generated content that directly interoperates with a game state that again is fully on chain. As a result, we had thousands of players who are playing the game within the first few weeks of launch and flooded a blockchain which back then was called Xdai, now known as Gnosis chain, with tens of thousands of transactions burning trillions of units of gas for executing a game move. So this became the big bang for fully on chain games. We created a realization that building an interoperable open and tropical system is powerful even outside of decentralized finance. Consumer applications benefit from this open and droppable system by creating an open and user generated content layer on top of their game. By default.
01:48:40.808 - 01:49:43.692, Speaker A: With Dark Forest, people are able to build their own clients and players can also build platforms on top of the game, expanding the design space for things like guild systems, mercenary systems and so on and so forth. For example, in Dark Forest, people created a bounty marketplace where you can basically pay people to kill another player in the game and they will get rewarded if that player is killed automatically on chain without the need of an off chain platform. Smart contract are basically able to read directly from the game state to firefly whether or not a certain player or certain planet have been killed. And this became a really powerful realization for a lot of game developers in terms of designing their game with that open and composability in mind. And the next phase became the autonomous world. That right. Now we are living in like after the Dark Forest had its success in 2020, a lot of people continues to be perplexed by fully on chain games.
01:49:43.692 - 01:51:00.616, Speaker A: And this lead to the birth of the concept of autonomous world with companies like Lattice who have been pushing autonomous world libraries like Mud and so on and so forth to make it easier for people to build their fully on chain games. Content companies like primordium is also building a fully on chain game that uses this autonomous world concepts to create constraints of physics for digital worlds. And again, there's also increasing number of people building libraries and frameworks for people that are interested in building fully on chain game autonomous world like things like Mud dojo and some others built by companies like Hero and so on and so forth. So, the key unlock with this phase of on chain games is digital physics. It's the realization that open doesn't necessarily mean it's lost. It answers the question of how do we handle open, interoperability and composability with a game when there is some constraint that is imposed on. To give you an example here is that you have to contrast an autonomous world or kind of like digital physics with client side modding, right? If you play, let's say, I don't know GTA five with a mod, you can make anyone look however you want, you can add cheat code, that makes you have unlimited amount of money.
01:51:00.616 - 01:51:58.408, Speaker A: And this is the key thing that we need to kind of understand when we're building flea on chain games is that just because you have an open system and you have an untrupable system that people can contribute to, it doesn't mean that the player can do whatever it wants. The creator, kind of the architect of the world are able to set constraints and create some level of digital physics that define what a player can and cannot do, when they are kind of building on top of these games. And this is like how it manifests itself, like true things like an Op craft and Mud where player agree upon a shared digital physical constraint. And again, open doesn't mean that users can do whatever. For example, in Opcraft you can't just spawn a block from thin air. You got to get the block first, you need to go to that location to place the block and so on and so forth, doesn't mean that just because a smart contract or other players can contraperate with the game or build a client that interoperates with the game that they can do whatever they want. So that's digital physics.
01:51:58.408 - 01:52:47.916, Speaker A: The next phase is the InterGame, which us at Argus are personally excited about. So the InterGame is basically just a short for the intro of games. So far, when we are thinking about interoperability and composability in fully on chain games, we're talking about it within the context of one game. Talking with, let's say a smart contract that is talking directly with a smart contract of the game. These are kind of the most rudimentary forms of interoperability and composability that we saw exist in onchain games. However, an open system in games doesn't always need to center around the games itself. As you can see right now, the game industry surprisingly is actually much more siloed than our financial system.
01:52:47.916 - 01:53:43.580, Speaker A: For example, if you are using again, banks, you can still use things like Plaid to integrate your banking data with let's say a fintech platform. So right now, banks are more open in terms of API and interop than how a game would. For example, if you're playing a game using Steam, you can't even use your Epic account to play the game. You can't transfer your item out outside of the game. If a game that you deploy, a multi game that you deploy on Steam wants to migrate off to a different platform, you can't carry over those data. You would have to just basically create a new slate of game items. And this creates a lot of problem when you want to kind of basically build like a large scale fully on chain games with kind of more complicated stakeholders.
01:53:43.580 - 01:54:36.912, Speaker A: In reality, the game industry is not built only by this independent studios. There are kind of other people like indie game publishers. There's also distribution platforms, there's like Marketplaces, there's like first party Marketplaces, there's third party Marketplaces, there are console companies. And so this leads to the question of how these different layers of the gaming stakeholders are able to support and communicate with each other. And again, we have to understand that even though we want this intraopen and travelable system, that doesn't mean that we want to allow everyone to be able to do whatever. In the same way, let's say with one game, you can't just want to allow another game to spawn millions and hundreds of dollars in your other game, another person's game. So the concept of physics and digital physics goes beyond than just a single game.
01:54:36.912 - 01:55:40.756, Speaker A: It is a constraint that needs to be imposed on this different interoperability layer within the game, the game publisher and game publisher to let's say another game publisher and so on and so forth. And so this is what the internet of game is. It's a realization that improbably can go way beyond games than just between games, between user generated content, beyond just working with smart contracts, with another smart contracts. And last but not least of course, the communication of games between games. There's a lot of nuances when people want to build this. Since the dawn of NFTs in 2018, when CryptoKitties have first came out, people have always asked the question, right, like what does it look like when a game like half interoperabble game assets across different games? A lot of people in the NFD crowd would have come up with this idea that hey, you can bring your sword from World of Warcraft to Counterstrike, which, if you think about It, doesn't really make sense. There needs to be some level of Constraints and how do you coordinate these transferable Assets or Even transferable Data or Transferable State or transferable Identity? And this is where a protocol is needed to kind of resolve that.
01:55:40.756 - 01:56:40.132, Speaker A: There needs to be some level of Coordination and there Needs to be Some level of physics constraints across these different games for these kind of cross stakeholder interoperability and composability to work. And this is like what the world engine is. The World Engine is basically a shared Substrate, a shared agreement between different Publishers, between different Games, between different smart contracts to basically have a seamless interoperability across them. It's the same way to interpret identity, the same way for us to understand assets, the same way for us to add limitations over what kind of interoperability is allowed across these different systems. And Again, at the end of the Day, we Want to allow each Developers, each game publishers and each game Studios to have full power over their own game. We Want to allow them to be able to set this physics constraints the Same Way that they are able to deal with Autonomous Worlds. And at the end of the Day, the end game for the World Engine is to basically be the TCP IP for games.
01:56:40.132 - 01:57:15.996, Speaker A: I know that's a term that is abused a lot in Blockchain. Everyone wants to be the TCP IP of Everything, but that's basically kind of like how I would describe it. So now Again, the World Engine is what we're trying to do with it is to become a Trojan Horse for the interim game era. We want to make it as easy as possible for people to build games that are default Open. Right now, if you want to build like an open game or you want to build an autonomous World, you have to chew Wallets. It's very hard to coordinate it. You have to figure out how to deploy your own roll Ups, how to configure your roll ups, to be able to bridge out to other roll ups and communicate with other games.
01:57:15.996 - 01:58:12.148, Speaker A: And the World Engine is designed to really abstract away all those problems from the lowest level, which is like, how do you do shared sequencing to the highest level, such as building your own game engine runtime and so on and so forth. The Role Engine is designed to be contributor friendly. So we are very open with kind of the idea of people building their own game Engine runtimes, for example, and it's again maximally extensible so that you can fit it to however you want to fit your game. And it's also open source right now at ARGUSLots. So this is a Star game template if you guys are interested in building a game Shard, which is like our kind of game runtime that is designed to be open by default, you can use this. And by launching a game shard on top of the World engine sequencer, you're allowing other game shards to be able to communicate with you through the shared sequencer system. And yeah, that's all for my talk.
01:58:12.148 - 02:07:00.790, Speaker A: Thank you for here's don't. Sam. Eight. Sam. Sam. Our. Sam.
02:07:00.790 - 02:14:18.600, Speaker A: Sam. Sam. Sam. Sam. Sam. SA wager. Sam nam it.
02:14:18.600 - 02:18:44.830, Speaker A: Sam it's. I don't like. I okay, to continue our gaming theme, I'd like to introduce Norswap shared Sequencing for Gamers and Small Brains. All right, hey everyone. This is going to be fun because I literally have two talks and way too many slides. So it's gonna be a little bit rush, but I'm putting ideas out there. If you don't understand something, don't feel dumb and just come find me, ask about it.
02:18:44.830 - 02:19:26.112, Speaker A: So it's like for Small Brains, but also brains are on crack, basically. So I hope you follow me in this journey. So basically, the two things I'm going to speak about is why onchain games should be roll up. Should use roll up, what's the point and what are the benefits? And then something I'm really excited about roll ups is the possibilities afforded by sequencing to do good things for games. And so I'm going to speak about the design space for decentralized and shared sequencing, which is a little bit technical, but a little bit fun to talk about if you're a little bit nerdy like me. So I hope you find exciting. So I had this talk in ETCC.
02:19:26.112 - 02:20:04.020, Speaker A: Which I'm basically going to give you the smooth speed up version, like the executive summary of that. Also, I've done this talk like three times this week, so I have that part phrased down so I can do it fast. So I'm working on a card game called Xerics Fable. It's fully on chain and it's like Magic the Gathering or Earthstone. It's under development and I want to do this game right, so I want to make it decentralized. I want to make it such that every can extend it. And for this, it needs to be on chain and it needs to be, I think, its own roll up for a bunch of reasons.
02:20:04.020 - 02:20:27.488, Speaker A: This is a gaming track, so I probably guess you know why games should be on chain, but if you don't, I have a talk at Denver that explains my reasons for that. So here are some challenges that game face. Like game may have a lot of action, so they need throughput. You probably want to onboard a lot of users. So this user probably don't want to pay $0.10 for each of their transactions. So transaction should be free.
02:20:27.488 - 02:20:55.316, Speaker A: You don't want NFT Drop to just disrupt your gas, so that's gas isolation. You can already see having your own chain ensures gas isolation. You want actually decentralization. You don't want anybody to be able to shut down the game if they press a button. That's the goal. You want cost to be low because you probably still want to make money from your game. So you're running a business, you might have cost and you have future free transactions, so you're not making money on the roll up.
02:20:55.316 - 02:21:35.892, Speaker A: So costs should be really low. As you know, if you ask a user to install MetaMask and this guy is not in crypto, he's like, okay, I want to play this game so I have to install this thing. I have to remember these twelve words, write them down, store them under my underwear and inside my Swiss bank and then I need to log on to my bank and transfer to Coinbase. And then from Coinbase to optimism, it's not going to work. So you need a way to at least let people try the game for free and then without doing all these shenanigans. Then there are a few more challenges. These are not super specific to roll up, but like my game versus my card game, you draw a hand of cards and these cards the opponent cannot see them.
02:21:35.892 - 02:22:04.860, Speaker A: So it's private information. You can't do that in a normal blockchain. You need something like zero knowledge proofs. I think games should be extensible so that there's a whole software architecture for that. And if your game has some financial aspect like NFTs, maybe you need trading liquidity so that you can connect the buyers and the sellers. And probably you want to have some kind of people that speculate so that they can supply liquidity on your stuff. So my point is like, roll ups help with most of these for various reasons.
02:22:04.860 - 02:22:41.370, Speaker A: And really what they do is that they have low cost and they have good integration and security. So if you don't know why roll ups are secure, I don't have time to explain it to you, but just ask me. It's a way to make a chain that inherit the security of another chain, often ethereum. What's not obvious that they are cheaper to run than their own chain? There's like Cosmos, which is an ecosystem of sovereign chains. Everybody can run their own chain and sort of all talk to each other. It might not be obvious why running a roll up is cheaper than to run a custom chain. So I want to explain that.
02:22:41.370 - 02:23:05.884, Speaker A: So there's two reasons why the roll up is cheaper. One is the security is taken care by the layer one chain. So for instance, ethereum. And so what you need when you run a L2 is something called a sequencer. A sequencer is the entity that takes a transaction and makes a block out of it. If the sequencer is evil, two bad things can happen. First, if the sequencer just stops, your game is stopped.
02:23:05.884 - 02:23:32.404, Speaker A: You roll up, stop, nothing happens. Second thing is the sequencer does not like you. We can just say, ha ha, I'm not going to put your transaction in the block. That's censorship. But at least the sequencer cannot steal your shit. So that's what you call liveness and super distance and stealing your stuff, that's security. And so because it's a little bit less crucial, you need less stakers, need less people to run the sequencers versus the validators.
02:23:32.404 - 02:24:03.316, Speaker A: And L two S can do something like restaking and restake. So basically, you want to pay the validators if you're a chain, if you are a roll up, you want to pay the sequencers. The problem if you are a chain is that you either have your own token, which means you're printing money. This is great. The problem is like, people need to stake your token so they're exposed to a very volatile crypto something. So they will want to be paid like 50% return every year because of the risk. And you could also print your own token and just spend it.
02:24:03.316 - 02:24:32.220, Speaker A: So that's still money. The other thing is you could use an existing token, so you could use ethereum. But if you do that, people can use ethereum to get returns somewhere else, right? They could do staking and earn 6% per year. So you need to beat that rate. If you use something like restaking, it means like, people are already earning returns by validating ethereum. But on top of that, they can also validate your node. And if they cheat there, they'll be slashed also.
02:24:32.220 - 02:25:21.980, Speaker A: So basically they stack the yield and they stack the responsibilities. And because it's additive, you need to pay less of a yield for that. And so that's why roll ups are cheaper. The other thing that's annoying, if you're a game developer like myself and you want to launch a roll up, is you need to recruit all these sequencers, right? If you're a big roll up, if you're optimism arbitram it's okay. You just go to Chainlink, you go to Alchemy, you go to Infura and you say, can you run my node? Can you sign on the website? And they're like, yes, of course you will know this is a good thing for us. If it's me, they're like, who the fuck are you? Fuck off. And so what I want to do is build a marketplace where everybody that's interested in sequencing can come and say, hey, I'm posting my bond here that is open to be slashed and I will let you contract my services to sequence your roll up.
02:25:21.980 - 02:26:06.920, Speaker A: And then if I'm a roll up, a game like say, that wants to be a roll up, then I come and say, hey, here's my Note software. Here's how much I will pay you. And then you sort of match these two things and you cut a lot of the red tape. And so it's easy. Finally, the final opportunity, and this is the bridge. The second part of the talk is that decentralized sequencing is when you just don't have a single sequencer, right? So you avoid the censorship and the liveness issue because if a guy goes offline, or if a guy is evil and wants to censor, there's always another guy that can do it. After shared sequencing is when you have decentralized sequencing, but also you say, well, all these sequencers, they're not just going to validate one roll up, they're going to validate many roll ups.
02:26:06.920 - 02:27:05.660, Speaker A: And then if they do that, what they can actually do is they can do cross chain transaction that are safe, at least they're as safe as the set of sequencers and they're fast, they're immediate in fact. And so this is not very figured out how to do that. There's a big design space where multiple trade offs and I want to talk a little bit about these trade offs today, but it would be amazing, right? Like you have your own chain, you have all the benefits of your own chain, it doesn't have a lot of cost, it's easy to set up. And also you can get instantaneous bridging to other chain, you can get liquidity, you can even split your own game into multiple roll ups. I think I have a slide, speaking about that later. And so one of the design you will explore is that all your sequencers that you have, a few hundred of them, one of them is picked to propose and all the others sign on it. And if you do that well, you have 200 signatures on your block.
02:27:05.660 - 02:27:46.184, Speaker A: And this is less secure than security of Ethereum, but it's still more secure than just one random dude saying like, hey, this is the block. And so if you're comfortable with a lower level of security, you could use that to do fast bridges even outside of shared sequencing. And you could also do that for cheap data availability. So data availability is where you put your blocks so that other people can reexecute them. And right now you need to put it on Ethereum, very expensive, on something cheaper like Celestia Eigenda. But the cheapest thing you can do is not put it anywhere, just on the nodes. And if 200 sequencers, say I have seen this block and I can give it to you, that can be a good enough insurance.
02:27:46.184 - 02:28:18.996, Speaker A: Like if you're doing a game especially, and if it's not the economic part, it might be a little bit less secure. Like you say oh, I'm going to roll back the last 2 hours of World of Warcraft, it's not going to crash the financial system. So you might accept the trade off there. Yeah. So why centralized sequencing is good for game, I just pretty much explained this. Another thing is that the only way right now to deploy your own roll up is to something called the Op stack. The Op stack is as of now, not super secure because there's no fault proof, which is a system that ensures the security.
02:28:18.996 - 02:29:10.230, Speaker A: And so in the meantime, like having this signature from the sequencers, that could improve the security of the op stack. So if we can implement that fast, that would be great. So shared sequencing, this thing where a bunch of sequencer lets you sequence a bunch of stuff, a bunch of roll ups, what can you do with that? You can do game style sharding. So if your game is so big that it doesn't even fit on a single roll up, you have so many transactions that it doesn't fit well, why not split it in multiple parts? Like, if you have RPG and you have a continent, put multiple continents in your game in game world, just put a continent on every roll up or maybe split by system. Like, the financial system is going to be on a roll up that's going to be a little bit more secure. And the game system, the battle system, is going to be on our roll up. And then you just do instant transactions between them.
02:29:10.230 - 02:30:04.324, Speaker A: Okay, this first part of the cock, this is going to be tough. Shared sequencing, I'm going to rush through six model of shared sequencing of decentralized and shared sequencing and sort of highlight some trade offs that you can have there because it's not like so far I painted a rosy picture like oh, we can do instantaneous things that are secure, right? It's not that easy and I explain why. So the basic, basic thing you can do is you have a bunch of sequencers, 300 of them. You just pick one every turn to propose and that's it. You can do one at a time. So like you're number one, you're number two, you're number three, or you can just pick randomly and after a while the probabilities will even out. This is actually safe, right? The thing is, it's safe in the sense of roll ups.
02:30:04.324 - 02:30:48.928, Speaker A: So you can't really a roll up is really safe to bridge from L1 to L two. So from ethereum to optimism, really safe, really fast. From optimism to ethereum, you have to wait seven days to see if nobody says, hey, there's a problem there. This is an easy design, it's easy to extend, to be shared. The problem is like if some guy says it's your turn to do a block and it doesn't answer, well, you just missed the block, too bad. And then there's this thing called Mev where people just do Arbitrage and stuff and it would be good if we could capture some of this profit for the chain and this model doesn't do that and some of the model we'll see later can do that. Okay? This really simply is just this idea that you have 300 sequencers.
02:30:48.928 - 02:31:30.770, Speaker A: You want them to sign on the block that's being proposed, at least two thirds and you can do a bunch of things that I already mentioned data availability for cheap, low security bridge. The thing is though, because you need everybody to sign on it, it limits the set of sequencers. So you cannot do 10,000 sequencers, because then the time to just get everybody to know about everybody else's votes would be too long. So there's a trade off there, but because we're not using this for security, it's kind of fine. Okay, so this is the model. We just take the previous one. Just a guy proposes and we add all the other guys sign on it, right? This is basically it.
02:31:30.770 - 02:32:09.470, Speaker A: It's still safe and fast. It's still easy extensible. Now we get all the extra goodies, we get data availability. Bridging you can also sort of once you see the signature, you're like, okay, my thing is in I can have some confidence that this thing is going to be on the chain. The problem is if nobody signs, if there's a network split and one third of the network is there, one third is there, and then one other third is like, I don't know, just nuked by China. Your consensus is broken. And the question is, how do you deal with that? Do you let it go? Do you go into recovery mode where the signatures are no longer needed? So that's something you need to think about.
02:32:09.470 - 02:32:55.048, Speaker A: You're still not capturing any mev. I'm going to skip that. There's some way to mitigate the problems. That's why I just said, like, just go into recovery mode if you don't get signatures. Okay, so no shared sequencing, simplest model. You take the previous model and then what you add on top is like, oh, instead of validating one roll up, now you're validating a bunch of roll ups and you're proposing blocks for all of them. And so what you can do now is like, oh, this transaction wants to do something on chain A and it wants to do something on chain B, and you're going to put these two things and if you lie, the other sequencer will be like, hey, you did the thing on chain B, but you didn't do the thing on chain A and you'll get slashed and the block will be invalid.
02:32:55.048 - 02:33:31.370, Speaker A: So this is super secure, it's super fast. What's the problem? Well, the problem is essentially the red thing there, which is you're essentially one huge roll up that just happens to be made of smaller roll ups. And so this doesn't scale at all, but it can still be useful, right? Because each roll up can be configured completely differently. It can have blocks that take different time, it can have different tokens to pay the fees. It can even be implemented completely differently. And it's easy to paralyze because most transactions will not be cross roll ups. You just have one server for roll up A, one server for roll up B, and then you have one server to handle sort of the coordination of them.
02:33:31.370 - 02:34:16.872, Speaker A: But still it's not rescalable. Some idea to make this better is like, okay, we have a big set and we're really strict about which roll up can be in this set. But maybe I'm making my game roll up and I really want to share, I really want to do cross chain transaction with optimism. So I'm like, hey, I'm looking for guys to sequence my roll up, but only the guys that are sequencing optimism can be included. And so let's make it a little bit more permissionless and let's use sometimes the cross chain transaction. Another model is, and this is decentralized, so we're going back to decentralized instead of shared, but then we'll do that. But shared is the Meva model.
02:34:16.872 - 02:34:54.120, Speaker A: And so Mev is this thing where you extract value from the chain from Arbitrage and Oracle backgrounds. And A is for auctions. And so in this model, there's no sequencer that's being selected to propose the block. Instead, it's like people are bidding, they say, I want to propose the block and then we'll pay the network two east, ten east, whatever, three cents to have this right. And yeah, this is pretty simple. It lets you capture the image, which is good, and otherwise it has all the same characteristic as before. The extension to share sequencing is tornie.
02:34:54.120 - 02:35:39.648, Speaker A: That is the next slide. And there's multiple auction models. So either you need to bid in advance, but then you don't really know about the meg that you will have. Like you don't really know how much money you can make. So you need to bid very little or multiple people are bidding, but they're including all their transaction in public. And that's sort of annoying because people have like secret sauce to make money and they don't like when that's revealed and then you can take that and make it shared. So now the model is the person that's bidding is not only bidding to put a block on a single roll up, they're bidding to put a block on Rob A-B-C-D for instance, but could also be on Rob D and E.
02:35:39.648 - 02:36:13.436, Speaker A: Like, they can select the set of blocks that they want to bid for. So this is a great model. I like it because sort of the market decides which roll ups are going to be connected with cross chain transactions. Now, there's a hard problem with this, which is like these bids can be overlapping and how do you select which bin is winning? So I gave an example. You got the first of auctioneer on the right. The first bidder is like, okay, I'm going to pay Rob A, $2, rob B one dollars, roy C $2. And then the other guy says, I'm going to pay A, $2 as well.
02:36:13.436 - 02:37:00.028, Speaker A: I'm going to pay B $3, so that's more. I'm going to pay C one dollars, so that's less. And the question is, which one do you pick? Because you can accept both at the same time. And if you try to think about all the simple solution, they don't really work. Like in this case, I'm going to pick the one that pays the most in total, but then that's unfair to that would be the one on the right, but that's unfair to B, which is paid less and maybe B is systematically being paid less and that's a problem. And you can't also there are a few solutions and we don't really have time to go into it, but it's also important that it's not manipulatable. So people sort of make fake bids to manipulate the thing.
02:37:00.028 - 02:37:36.228, Speaker A: So I have a strong solution there. This is like just a shower idea, basically. Is that something that's interesting to you? Just talk to me. Oh, we already had the last one, so I think we'll make it in time. Amazing. So far the model we've had was the model of atomic execution where I have a transaction that has multiple parts on multiple roll ups and I want everything to succeed. The sort of the classical example is I want to swap like a token on a chain, bridge the swap token and buy NFT with that.
02:37:36.228 - 02:38:05.228, Speaker A: But if I can't buy the NFT, then I don't want to do the swap in the first place. I don't want to pay fees, I don't want to be exposed to volatility. Another model of cross chain execution is atomic inclusion where I say I want to do this action on chain A and I want to do this action on chain B. But it's sort of not a causal relationship between the two. And atomic inclusion does not guarantee execution, does not guarantee success. So you say both of these transactions will be included on their chain. That's what I guarantee.
02:38:05.228 - 02:38:30.984, Speaker A: But maybe they will fail there. I should guarantee they'll be included at the same time. So that's a lot less powerful. So you can't do Bridging for instance, because Bridging is typically okay, you lock some tokens here and then you mint them on the other side. And if you just take that and you include them, both of them and the locking transaction fails. Now you've minted token and they've not been locked on the other side. So you've just printed money.
02:38:30.984 - 02:39:21.704, Speaker A: So that's obviously bad. But if you can do that and there's some ways to use that with Escrow models or for some other examples that are not Bridging, it can be useful. If that's good enough for you, then that's really easy to operationalize because basically you just make a new roll up, you let it sequence all the cross chain transactions. So you say, okay, this set of transaction needs to be included, this set needs to be included, et cetera. And then all the other roll ups have to follow this single roll up and just obey the rules and say okay, this bundle was a transaction for me, so I should include it. And so that's pretty simple, but it's powerful. We can skip this, I'll skip this.
02:39:21.704 - 02:39:48.592, Speaker A: So I'm interested in working on this. I know there's a lot of companies like working on something similar right now and I'm trying to talk to them. My interest is building something that's fully open source. Everybody can deploy permissionlessly, make it really easy on the small guy, essentially, because some of the sequencing work is really for the big roll ups. I want it to be like, hey, I'm just two guy in a bedroom. We work in a game, we want to do our own roll up. It shouldn't require even tens of thousands of dollars.
02:39:48.592 - 02:40:16.990, Speaker A: Just a few hundred should be enough. It should be easy to do. Just clone a GitHub repo and just do some comments. And there's a lot of people interested in pursuing that vision. And I'm looking for people to build with me, to use this stuff, to comment on it, to fund it, that kind of stuff. So yeah, if you have any kind of interest or any kind of question, if you did not understand anything but you're interested, just talk to me and yeah, that's about it. Thank you.
02:40:16.990 - 02:41:23.250, Speaker A: As a way to secure the decentralized sequencers and then do they apply to all of these models? How do you see them overlapping with the different ways of doing it? Different kinds of designs that you showed? Yeah. So in general, there's two roles to staking. One is sibyl resistance. So if you're in a select in a set, like round robin or randomly, you need staking to make sure that I don't come in and behave I'm 10,000 sequencers. The other role is to prevent misbehavior or discourage. I think in a lot of case, there's not a lot of safety thing that you really need slashing for, but it's good to desensitivize, like annoying behavior like, oh, I'm not answering. And so encourage liveness discourage, censorship, resistance and things like that.
02:41:23.250 - 02:41:59.404, Speaker A: So does it apply to all of the models? I think it mostly apply to all the models. You probably don't need it in the auction model as long as you ensure that the auctioneer always pay their bid. Because it might be that they say like, they win the bid, but then they don't post the block or they do something invalid. So you want to make sure that in any case, they paid what they say they would pay. Thank you. Thank you. Thank you so much.
02:41:59.404 - 02:43:04.064, Speaker A: I wanted to ask one of the important priority of shared sequencers is that not to be like a bottleneck, they skip the execution part, it just be the mechanism for ordering. But when we start talking about mev, especially like cross chain mev, does it mean that shared sequencers need to be protocol aware and actually execute transactions to understand where is the mev? Yeah, I don't think any design is really going to ever be you don't execute, I think you always execute. And the reason for that, even if you design something or you don't theoretically execute, there's going to be the proposer builder separation, where the proposer is the person that's in the protocol and has a right to propose and the builder is the person that's actually going to supply the transaction. Just going to have an off chain deal if you want with the proposer to do that so that you can extract indeed the IMEV. I guess it could be possible if you don't have any which for games, it's very possible that you don't have any mev. Right, so that would be possible. I see.
02:43:04.064 - 02:43:54.256, Speaker A: Okay, thanks. I was just like wondering if isn't like it's like another bottleneck that we trying to avoid. Going from layer one to L2 is like another yeah, that's a big problem. That's why this thing don't scale. And the shared one for atomic execution, you really want execution? Unless you're doing the last model of atomic inclusion there, you can literally just do sequencing like you said, without execution. All right, thank you very much. Do you have anything on just this.
02:43:54.256 - 02:44:19.310, Speaker A: Okay, yes. Nice. So I just hold this and then talk into here. I just hold this and talk into the mic. Okay, great. Cool. Are we good to start? Okay, perfect.
02:44:19.310 - 02:45:01.350, Speaker A: So, yeah, let's talk about Infra for fully on chain games. I'm sure you heard of a couple of talks on this, but I want to take a slightly different approach. By the way, how do you progress the next slide? Green button. Oh, there we go. And back is okay, great. Yeah. So let's talk about Keystone, which is what we recently developed as the new flagship fully on chain game, infra.
02:45:01.350 - 02:45:56.118, Speaker A: But I'll take a slightly different approach where we start from the experience that onchain games need to or are uniquely poised to create and then the design paradigms that can create those experiences and finally, what Infra can support those type of design paradigms. And for those of you who aren't familiar, curio is a crypto native game and infrastructure company. We're basically all in on fully on chain games and we want to make it happen as soon as possible. And we're both building games to push forward the game design barriers as well as the infrastructure. So yeah, that will be the structure of the talk. First, a quick philosophical detour. What do you think makes the real world feel real? Is it just the rendering quality or is there something else? And there's definitely an important piece to this, which is the social relations we have with other people.
02:45:56.118 - 02:46:34.686, Speaker A: If you think about it, humans are social animals from the beginning and the relations we have with other people are very complex. You can't really define someone as a pure friend or a pure enemy. It's very rare that that happens. And we have all kinds of interactions with people, varying degrees of promises and agreements and contracts that happen now. Virtual worlds today and games today have very limited social interactions if you think about it, compared to the real world. And why is that? Let's take a look at a couple of game models today. The first genre I would call Social 1.0,
02:46:34.686 - 02:47:18.162, Speaker A: which is purely PvP based, and the only social relationship that happens in these games is competition. So players fight each other. This is StarCraft, by the way, and CSGO and Overwatch, and these games all belong to this category. There's not really any cooperation, and the only thing that happens is competition, which is very drastically different from the real world. And a lot of player complaints in this category is like one, the onboarding cost, the barrier to entry is pretty high, usually because there's a lot of skills involved and there's not that much social interaction. And then the next step kind of in the multiplayer genre with social relations is Social 2.0, where most of the MMOs are.
02:47:18.162 - 02:48:23.142, Speaker A: And in this category, there are two types of primary interactions. One is cooperation within certain groups, such as the alliances defined by the game, or competition between the groups. So players are no longer just competing on the individual level, they're also cooperating within a group and competing between groups. But this is still very different from what we see in the real world, where people can act anywhere between friends and enemies, and that's what makes it interesting. So what exactly is missing? Right, let's take a specific example from the previous picture, which is Rise of Kingdoms, which is a MMO style civilization. In this game, let's say the two players highlighted in two different guilds or alliances want to trade with each other, let's say, want to trade resources, two types of resources. They can't and they also don't need to because the game is defined as competition between guilds, right? There's no need for play to player interactions, which takes away a lot of the thought.
02:48:23.142 - 02:48:51.290, Speaker A: And this does not limit to players. That happens between guilds as well. Let's say the two guilds on the top want to sign a peace treaty for some time. They really can't do that. They can promise to each other. But if one side betrays, there's no punishment that will come, right? And that equals no deal. So what does the real world do better than games? At these? In my mind, there's two very important components.
02:48:51.290 - 02:50:04.600, Speaker A: The existence of highly valuable assets such as your house or anything that makes you really care, and effective law enforcement that protects people's interactions around these assets, right? And that's why people, when you walk to work and fight your colleague over a small disagreement, you don't have to worry about them taking your house or killing you or something like that, because there's a mechanism that protects you from getting those harm. But there's not such thing in virtual world or games today. So Blockchain has actually already figured out half of the solution, which is the existence of digital assets. We have digital assets that can get very highly valuable and decentralized. So it's protected some people might even say it's more secure than in the real world. But there's not really the equivalence of law enforcement, right? When people make promises to each other, there's not an entity that can say, okay, I will enforce these for you and today's game. The problem is that there's not a language in which the players can tell the game engine what they're agreeing to so that the game engine can take care of the promises for them.
02:50:04.600 - 02:51:18.074, Speaker A: So this is like a pretty trivial example where, let's say in a grand strategy game I don't know if you can see the text, but let's say you have a player named Vitalik right next to you, and you are equal strength, and you're fighting over a couple gold mines. And suddenly there's this player called Justin Sun that approaches both of you with a lot of high level horsemen, right? And reasonably you want to stop attacking each other for a bit so that you can both defend against this much stronger player. But let's say you agreed to do this for three turns, but in two turns you decided you just really want the gold mines and you're going to attack Vitalik anyways. What happens? Well, nothing really happens. And in fact, if everyone does that to Vitalik, he's going to quit the game and blame the game for not being designed very well. So what can make this up, right? What can allow justice to come in virtual worlds and let players not worry about worry about breaking promises and actually interact fairly with other people? Well, we designed a system called Treaty, which is actually very straightforward. It's using smart contracts to enforce social contracts in games.
02:51:18.074 - 02:52:35.606, Speaker A: So basically in MMOs and grand strategy games, players are able to create or reuse smart contracts that define their relations with other people in a way that gets enforced automatically by the blockchain over some trigger condition. So what do I mean by this? Some examples of this include a simple thing like non aggression pact, which we just showed with an example with Vitalik to more complex things like North Atlantic Treaty Organization. You can have guild currencies that on chain Saudi Arabia can launch to back with their oil. You can even have different kinds of governance models that govern how policies get passed. And treaties are, we think, important social building blocks for blockchain, for online games to allow players to do exactly the social interactions that they couldn't do for a long time. And this is uniquely suited for blockchain because smart contracts already exist, but they currently don't have a function in much more context rich worlds like games compared to things like DFI NFTs. So yeah, we built a game out of Treaty and we launched this January 20 23rd so this year, and people really enjoyed it.
02:52:35.606 - 02:53:34.138, Speaker A: People made some wife jokes about this for playing the game too long and not sleeping while not washing dishes. But anyways, the game lasted for a week and actually the end result of the game was affected by some player creating a treaty called Loan agreement. Basically the ultimate victor of the game were able to loan troops from another player and to raid the alliance who already occupied the center of the game, which is the flag tile that is the victory condition. So that was pretty cool. But we ran into some other problem. I think players really enjoyed it and we're so confident about the design space of high Stake PvP adding on the social interactions that we're building a real time war strategy game with high stakes, fully on chain and hopefully launch that in a couple of months. But I think we ran into problems as well, which is with regards to performance, right.
02:53:34.138 - 02:54:16.386, Speaker A: Each transaction of our game, which wasn't that complicated when there were 20 players playing, you check a couple of treaties, you check permissions and you move people, you battle. They took anywhere from 20 million gas to a couple of hundred million gas. And that's not even because the smart contract is fully written. It's just because the game is so context rich. You need to perform a lot of logic in one transaction. And that's why players were joking that the blue troops, which indicate movement in our game, don't actually move, and the red ones don't actually attack, because the moment of intent was just too far from the moment of execution in the games. And this is not unique to our games.
02:54:16.386 - 02:55:23.226, Speaker A: This is a sheer problem with all fully on chain game frameworks today, right, because all the logic is implemented as smart contracts that live on top of the blockchain, that use usually EVM or StarkNet execution environments to run the game, which these systems are not meant to do. And games like Dark Forest kind of avoid this problem by designing their game in a way so that all the actions take really long time and that's what meant to be. So Dark Forest had energy rays taking anywhere between 15 minutes to several hours to send, which work, I would say potentially works for some players. But if you want to get to mass adoption and have a lot of players who don't really care about the blockchain elements and just want new experience come this is very limiting, right? And so specific problems. Games are very context rich and they have much bigger transaction size. And because of that, you have some fall on problems like stack two dip deep. So you have to do more gets, which even just cripples the system.
02:55:23.226 - 02:56:23.550, Speaker A: And there's a lack of game tick. So what that means is let's say if you want to move from point A to point B, the intent is moving to point B but the actions are you might want to move many times before you get to point B. That usually traditional engines take care of using this thing called game tick which is kind of like the sun rising every day or seasons coming every year automatically and they don't come because someone send a transaction that the sun would rise today. And in the same way you move in the game, you take every step that the game engine basically take for you. You don't have to send transactions every time and check the permissions every time and check intent which are wasted computation. And finally, Solidity is not okay for smaller scale games, but to create a game as complex as some of the even medium stage sized games in web Two, it's very, very complex. And there are much better suited languages and stacks that allow players to develop or allow game developers to develop much faster and more stably.
02:56:23.550 - 02:57:48.762, Speaker A: So because of all these challenges, we thought, okay, we wanted to preserve the social composability, we wanted to create to preserve the social composability, aka the ability to create contracts, social contracts and the trustlessness of the logic and the assets. But also we want to add high performance and great graphics and basically all the UX stuff. So great game developers can create these fully on chain games that will take us to the next stage, right? And that's why we created Keystone, which is a sovereign roll up based onchain game engine framework meant to specifically 100 x the performance while preserving some of the best parts of fully on chain games. I should need to switch screen to show the demo, but you can find this on our Twitter. Basically we spent two weeks putting Warcraft Three, the game, on chain and all the movements and battles you see with the dozens of troops moving and attacking at the same time, subsecond level happen in real time on chain, right? And all the logic are on chain. So that's like a huge step from what we see today with today's frameworks, including our past framework with treaty. So key features of Keystone include its high throughput ECS based game engine.
02:57:48.762 - 02:58:41.222, Speaker A: ECS just states the way game data is stored and the fast GameTech supports classic RTS style games like Age of Empires. You can theoretically for a fairly simple game get to 60 ticks per second, which is pretty much all you need for a close to very real type of type of experience. It supports historical state querying, so you can write treaties such as if something didn't happen for three turns, then whatever reward that is, or punishment, this is not about the current state, it's about historical state. And finally, it supports EVM based treaties. So treaties still use Solidity but use pre compiles to get and set certain states. And finally it's written go. So basically I'll talk about the architecture.
02:58:41.222 - 03:00:14.426, Speaker A: You have the game logic that lives on the same layer as the EVM. So it's not really fully on chain but fully in chain because if you think about it, the digital physics or the game rules don't really change that often just like the EVM layer right? And what happens is the EVM supports all of the social contracts players make with each other in real time, whereas the game tick and a lot of the features like Indexer and a lot of other features that blockchains currently don't have with games are added in parallel with EVM and interoperate with custom staple pre compiles and this whole system settle data to Celestia DA for anyone to basically observe and be able to challenge the validity and also to Ethereum for payouts and the asset related features, right? And basically there will be a multi SIG of guardians that are basically sequencers of this whole thing. And the multi SIG is able to kick out, to vote out anyone who acts maliciously based on players input. We basically open source this by the end of May and we're in the process of still improving the system. Coming up with Keystone V Two probably in the next couple of weeks. Our games obviously built on top. We're getting more people who are interested in building onchain games that are not just hobby projects to build on top and to really impress people.
03:00:14.426 - 03:01:13.310, Speaker A: So yeah, definitely welcome all suggestions or questions here but yeah, we think this is the first step toward games that have massively complex social relations like Ready Player One that also have highly valuable assets. This is what we don't have today in today's virtual games. So, yeah, we're building Keystone B Two. We're working on the flagship game, which is the High Stake strategy game, and ultimately we are, as I said earlier, the game space that fully ongoing games open is massive. And we alone cannot possibly explore even probably 5% of everything but in order for games voting on Keystone to succeed we can just host hackathons we don't think that's enough for a game like you have to worry about so many things like art, distribution and capital. We will provide all of that for you. As long as if you talk to us, we'll figure out what's the best way to be on chain for you.
03:01:13.310 - 03:02:27.394, Speaker A: What's the tick rate design? How do you settle data? How do you get your initial group of players? We'll even match you with the art teams we have for basically much better looking games than today's. Onchain games, we'll support you with all that so you can focus on the mechanism. So yeah, that's everything. I think we're slightly over but I think we can take some questions. Okay, you're next. Hey great presentation thank you. So I'm just curious in terms of the users for these games, right, who do you think this appeals to primarily? Is it your hardcore on chain gamers like the ones who played Dark Forest or is it more crypto gamers who are like your axes et cetera who are probably there more for speculation or do you think there is a pathway for this to get entirely non crypto gamers from outside who are both mobile or console based.
03:02:27.394 - 03:03:19.446, Speaker A: Right. Just curious, where is this going from a user standpoint? Yeah, that's a great question. So we're solving a very fundamental user need and it's more on the psychological rather than technical side where we're attracting players who are generally very willing to who are interested in highly PvP types of games with high stakes and at the same time perhaps are looking for more complex social interactions in online games. So imagine if you're a really good Delta player but you're not competitive, so you're not enough to participate in Esports. Right. But you still really want to play RTS games and play with stakes with other people and perhaps win or lose based on your skills and you want to improve your skills so you can perhaps earn more. Right.
03:03:19.446 - 03:04:05.142, Speaker A: That's the type of people that we think we'll attract, we will attract that's. Obviously in hypothesis, we still need to test it, but it's drastically different from today's blockchain or like Web 2.5 gamers who are I would say, looking for asset appreciation, which is unsustainable. It might have a big overlap with today's fully on chain players, but not perfectly because I do think a lot of fully on chain players today are only interested in the technicals but are not really looking for a good experience. I do think the ultimate goal is to get people who, for example like StarCraft or like Poker and basically fit the criteria I described earlier to come right. And we need to basically solve a lot of problems before that comes true. Okay, thank you.
03:04:05.142 - 03:04:43.266, Speaker A: Yeah. Okay, I got one technical and one social question. Let's do it. My technical question is in my mental model of time, it takes change to execute thing like the bottleneck is really the miracleization committing to the state any form of commitment. How do you do that? Because you manage to bypass this bottleneck by doing something clever or don't you or something. Yeah, that is the slow part. You're right.
03:04:43.266 - 03:05:33.602, Speaker A: And there's kind of different ways to think about it. If you need real time settlement. I don't know, on a subsecond level I think that might just have trade offs with how complex the game can be. Or alternatively you can have kind of a challenge period after, let's say each game lasts for three days or something at the beginning. I don't think we'll have very persistent games at the beginning. Do you have strategic depth? In that case, you can have a challenge period after the game so that people can run the game over after the data is settled, mercalized and settled and make sure that everything's dispare and then basically the payout then happens. So I think it really depends on the specific game setup that you have.
03:05:33.602 - 03:06:16.126, Speaker A: Yeah, and also this is very early, we haven't thought too much about consensus of the mercalized data. That's a hard problem that. I think we're thinking about the idea, if I'm correct, is that there are some outcomes that you care about. Like this player won and maybe these resources shifted after the game and so the game nodes will commit to that and then you can sort of challenge that these are not the correct outcome. But you'll need to mercuryize the whole state with all the health at any point, right? Exactly. Or we could mercalize the whole state if it's doable. Yeah, definitely.
03:06:16.126 - 03:06:58.314, Speaker A: Interesting question there. The other one is like okay, the premise of your game is well, not the premise. One of the big features is you want the smart contract agreement between player. But don't you think that sometimes the sort of drama of it is what gets people hooked? Like the betrayal and the game theory of it like, oh, I have this agreement with this guy, but I need to watch my back because he might betray me. But if he does, his reputation is going to be ruined and it's like sort of an iterated game thing. Don't you think that's worth preserving some of that? Or how do you think about that? No, this is a key question. So let me just ask you, right.
03:06:58.314 - 03:07:26.886, Speaker A: Let's say in today's corporate world, right, everyone signs contracts like NDAs with each other and freaking, I don't know, whatever, other types of contracts. But there's still tons of betrayal that happen, right. And they are basically kind of like loopholes of the contract that gets exploited and that makes the whole betrayal more interesting. So let's say I sign a non aggression pact with you. It doesn't say I can't have a vassal state that can attack you, right? I can pass the troops to them to attack you. Right. That's something.
03:07:26.886 - 03:07:59.520, Speaker A: So we're creating the structure for very blatant violations to be impossible. But you can think of it basically requires players to think on a higher level and betray on a more interesting level. If that makes sense. Yeah, that's interesting. That's cool. The goal is definitely not to avoid any social relationship change. In fact it's to encourage that, but by freeing a lot of the minute for the small things to worry about.
03:07:59.520 - 03:08:51.070, Speaker A: If I want to do a trade for three turns with you, I don't want that to be broken if I'm doing that with like 500 other players. But then I can think strategically, like oh, maybe what vassal stage should I set up over the next 500 turns to have? What kind of XYZ goal? Right? So you are actually expanding design space of the interaction that are possible because there are things you would never ever do if you couldn't trust people. Yes. And there are also things such as we probably won't allow permanent non aggression packs just for the reason you mentioned. Right. We can have all treaties time bounded to like 500 seconds or something like that. That's also a pretty straightforward thing to do to limit the negative consequences.
03:08:51.070 - 03:25:54.620, Speaker A: Cool. Yeah. Awesome. Sam sam sam sam sam sam it's. Sam it jam SA jam sam sam don't. Dam. Sam sam sam sam sam sam ram.
03:25:54.620 - 03:46:53.710, Speaker A: Sam sam sam sam sam SA. Channel ram. Sam sam sam sam sam don't. Ram. Sam dam. Sam SA sam sam sam it. Sam don't.
03:46:53.710 - 04:01:04.740, Speaker A: Don't. Sam it. Sam sam. Jam. Sam sam sam sam sam sam it. Sam sam SA. Sam it.
04:01:04.740 - 04:06:54.102, Speaker A: Sam sam. Sam it. Sam it. Up next we have Michael from Blockworks and Alex from ZK Sync for a Fireside chat about ZK Sync and ZK technology. Welcome to stage, guys. All right, thanks everyone for coming. Alex, it's great to be here with you, enjoying the week.
04:06:54.102 - 04:07:37.254, Speaker A: It's a pleasure. All right, so we've only got about 20 minutes, I believe, and I want you to basically I want to get deep in the weeds with you about ZK Sync and what you're building at ZK Stack. Maybe we could actually start at a high level. I've heard you describe Zkstack actually as a set of architectures or frameworks for people who want to build app chains. So maybe we could get into some of the high level and almost like take us back to the room when you were originally ideating some of those decisions for ZK Era and then we can get into some of the actual components about what makes up the ZK stack. Sure. So let's start with what is ZK Stack? We have been focused on ZKsync ZK roll up technology for several years now.
04:07:37.254 - 04:08:35.026, Speaker A: We released era. Era is the first Zkvm Live on Manet and it's now the most popular ZK roll up, also on Ethereum network. We are the most used L two by number of transactions over the last month and third by TVL. And we have seen a lot of interest in reusing, a lot of interest from builders who want to create their own blockchains, their own roll ups, their own application specific chains. And we realized that we should just open the code was open source from the beginning, but we want to adopt it in a form that makes it easy for anyone to build their own custom chains on ZK technology. So ZK Stack is a framework, it's a modular customizable framework that allows you to take our code and create your own custom ZK chains. You have a choice of all the different components.
04:08:35.026 - 04:09:12.046, Speaker A: For example, you can choose the different data availability modes. It can be an on chain data availability mode which make your chain a ZK roll up. It can be a validium, it can be a mix of both called Volition or we call it Zika Porter. You can customize the sequencer, you can go for centralized sequencer or decentralized sequencer, which we're working on. Or you might use something entirely different and enrich the stack and share it with others. You can customize the way you deal with maybe you want to use your own token for some things. You have full control over all aspects of the Stack.
04:09:12.046 - 04:10:11.922, Speaker A: But really interesting property of ZK Stack is it's designed from the beginning to be interoperable with other ZK stacks like what we call hyperchains, using a new primitive we call hyper bridges. This is something only possible if you start building in a shared ecosystem. There are certain technical properties that you need to enforce. All of these chains have to use a shared base, contract on layer one or the shared, like people call it bridge. I don't like the word bridge for this thing, but it must be a shared thing. And then you need to use a common standard for passing messages and for transferring assets. And this is really powerful because then the hyperchains can be connected seamlessly and trustlessly in a huge, unified liquidity ecosystem which can grow indefinitely, just like the internet.
04:10:11.922 - 04:11:02.386, Speaker A: So that's what ZK Stack and hyperchains are in a nutshell. That's a really helpful explanation there and I want to get into the weeds on some of those individual components. I'd also actually even like to start a little bit more high level and just get some of the what were some of the original design principles and problems that you were trying to solves for. So in talking about the HyperChain infrastructure, it's clear interoperability was a big sort of design decision or something that you prioritize in the beginning. Walk us through some of the other original problems that you were trying to solve with the ZK Stack. So we started ZKsync with a mission to make blockchains universally accessible for everyone. So to scale blockchains while preserving the core values, the core properties that make the blockchains valuable, specifically public permissionless blockchains.
04:11:02.386 - 04:12:12.506, Speaker A: And I think by now it's very clear that Ethereum has crystallized as the well, I mean, to me it's my personal take. Ethereum is likely going to be the settlement layer for most of the Internet of value. We'll have some alternatives, but for now they will have to compete with ethereum. So we're focusing on ethereum, but we're making Ethereum modular and extensible and spreadable. We have been thinking about what's the end state of this Internet of value have to look like, what must be the eventual. If we take the certain properties, we have to articulate them well, what do we want to preserve? And then we have to imagine how does this look like at scale? So we had an intuition which has been guiding us on those properties. Recently we published something called the ZK Credo, which is a manifesto, a mission statement, mission philosophy statement for the ZK Singh ZK community in general, where we articulate very specifically like we want trustlessness, we want resilience, we want censorship, resistance.
04:12:12.506 - 04:13:15.102, Speaker A: The system must be accessible, affordable by anyone. Then the list goes on. There are, I think, eight properties there and then all of that has to work at scale, meaning the systems have to be able to grow just like the Internet. So one of the properties is called hyperscalability. If you think of the Internet, it's impossible to conceive that all of the world's Internet transactions, web servers, application servers are running on a single server or even on a single data center. It just physically won't scale, right. So you end up with a multi server multicomputational paradigm which supports parallelism to arbitrary degree where you can add more servers, more links, more data clusters to it and it will just grow and absorb arbitrary number of users and arbitrary number of transactions and apply to the world of blockchains.
04:13:15.102 - 04:13:49.258, Speaker A: This means you need to plan for a multi chain world. And if we will have multiple chains running in parallel, they have to be connected the same way Internet services are connected. Like a simple example is an email. You have your address, I have my address. Like Alex at MetaLabs, you have your address, michael at I don't know, Gmail or blockworks or something. You can send email from any address on any domain to any other address or on any other domain in one click. It doesn't cost you more in terms of effort time.
04:13:49.258 - 04:14:48.394, Speaker A: Like it will just arrive in a few seconds or cost the same with web pages and hyperlinks you can go from any page on any domain to any other page in just exactly one click. You don't have to spend more. This is the property of hyperpigs that we aim to preserve there. Or this has been the core part of our design. How do we create a multi chain system following the vision of the Internet of chains first articulated by Cosmos and then polkadot, that actually works, that actually gives you trustlessness, where you don't have to rely on validators of those bridges on some custodians where you have native assets that follow the Bridging paradigm. When we say bridge, we think of pieces of land connected by something over water. So if you have a bridge between two islands, you have assets on one island, you have like a pile of gold there or a car of gold.
04:14:48.394 - 04:15:33.050, Speaker A: You can just move it over the bridge and so it will disappear on the first island and it will appear on the other island. Yeah, but that's not how the bridges work today. What we have like the interchange bridges between layer ones or sovereign chains work more. Like you have to park the car on one side, then you get an IOU and then you go to the other side. Like we swim over and then you carry over this IOU where the assets are actually in custody by someone else, which is not native, not extensible. Like you have to trust those guys and so on. So we wanted to preserve this and the only technology which we have today which is capable of doing it at arbitrary scale is decay.
04:15:33.050 - 04:16:56.598, Speaker A: Your knowledge proofs, validity proofs this is what hyperbaric and hyperchains are based on. So I really like that example of thinking about how the Internet scaled and how it can't all be on one server, obviously. But I think it's actually if you maybe go one layer deeper and look at the interoperability of the current Internet, there is part of the web which is open, and then you start to see like closed off clusters or parts that are slightly less interoperable with other parts of the web. An example might be the Great Firewall of China, right, and how the Internet works over there, or know government sort of restricted networks or even, frankly, like Google or Facebook, right, which are kind of open and interoperable, but really are really closed gardens that want to keep you on their platform. So I would love to get a little bit more specific about how you see interoperability across different L2s, because my sort of in hearing you describe what the ZK stack will enable is a bunch of hyperchains that are highly interoperable with one another. But is the plan for that to be interoperable with other L2s, like either other ZK roll ups or like the optimisms or arbitrums of the world? How do you see big sort of L2s interacting with one another going forward? That would be an ideal goal for us in Ethereum to come up with a standard that allows us to be completely interoperable. That would unfortunately require some very fundamental changes at layer one protocol.
04:16:56.598 - 04:17:38.950, Speaker A: So we have to build some kind of common bridge that is designed for ZK architectures that is shared across all of us and it is basically enshrined into the base architecture. Maybe one day we'll arrive at that. But first we need to experiment. We will see this experimentation happening between different ecosystems. A few of them are experimenting with this kind of designs, with a local ecosystem of application specific or generic chains more tightly connected than just arbitrary roll ups. On Ethereum, we'll see experimentation and maybe some standards will crystallize over time. Got it.
04:17:38.950 - 04:18:22.690, Speaker A: I want to actually get into some of the different modular components that you were alluding to before. So there's data availability, provers sequencers, et cetera. Let's try to close our eyes and put ourselves in the perspective of someone who wants to build their own HyperChain on ZKsync or using the ZK stack. How are they kind of thinking about what are the most important components kind of to tackle first and what are some of the options that they're looking at? When I'm thinking from a builder perspective, my builder hat on, I'm thinking about properties of a system. I want to get I'm not thinking in terms of components. I'm thinking like, what do I want from this system? For me, the priorities would be, number one, security. This is something non negotiable.
04:18:22.690 - 04:19:07.550, Speaker A: If you're building a system, there is security. It does not matter. You probably don't need blockchain. You're on blockchain for decentralization and for the ultimate security, where every user can verify all the transactions. So that is something highly important. And here ZK Stack shines because this is one of the most bottle tested frameworks that we have today, with over half a billion dollar TVL on Ethereum and being the most used L2, at least for the last month, with 25 million transactions almost. We invested over $4 million in security audits, like various audits for all components of the system, with top tier auditors in security contests.
04:19:07.550 - 04:19:48.858, Speaker A: We are having mechanisms for security defense in depth with delayed withdrawals and various other techniques. And we're working on extending this thing because we have to keep in mind that all the roll up technologies are still relatively new and there can be bugs. And you want multiple layers of defense. Even if one layer fails, you still want to rely on others for security. So that's the security perspective. The second property I would want from a system or from a framework to build my chain on is reliability. Like highly reliable, liveness.
04:19:48.858 - 04:20:54.670, Speaker A: This is critical for DeFi applications. This is critical for your users, for the fantastic UX, because if your system goes down frequently for a prolonged period of times, you're not going to make it. People will lose trust and the system is not going to work. So you really want to take a bottle as architecture, and this includes also being able to accommodate spikes in activity, like surges in the demand for transactions. Let's say we're doing like ten TPS on average right now, organic demand. But if the capacity was only like 20 TPS, any event like a popular NFT Mint drop or some token issue would bring the network to its knees because a lot of people would try to reach it, and we've seen it on other networks, they are not capable of accommodating the actual lot. So this is why we knew that that's going to be an important factor.
04:20:54.670 - 04:21:35.986, Speaker A: We did not take gas as the base for the chain. We started building our own custom sequencer, implemented Rust from the beginning, and we're capable of doing over 100 TPS comfortably now. And we're still working on making things like we are far from optimal. We can do thousands of TPS, but the real lot shows over 100. Nice. And I think ZK Sync, you guys were some of the early ones that really experimented with account abstraction as well, which just allows you so much more flexibility. So am I correct in assuming that creators of these hyperchains, they'll be able to specify whatever gas token they want? Even the users today on ZKsync Era can specify what they want to pay their gas in.
04:21:35.986 - 04:22:19.886, Speaker A: Wow. Or you can have sponsored transactions for certain users. They don't have to need any gas at all. Pay from smart contracts or you can sponsor transactions from some accounts. You have full flexibility and we have account observation, indeed, natively implemented at the protocol level. So it's not limited to certain types of accounts that you have to specifically deploy any MetaMask wallet, any other wallet on ethereum, can profit from it and can do gas less transactions or pay fees in any token. Yeah, I want to ask you a couple of questions about sort of decentralizing some of the more centralizing parts of L2 design today.
04:22:19.886 - 04:23:14.398, Speaker A: Maybe we can talk a little bit about the sequencer and then also the state of the proverb, how proving is done on ZK sync. Sure. So sequencer is for some applications, they will be happy with centralized sequencer, especially for high frequency transactions with low latency. But most blockchain use cases require full decentralization of the entire stack. We are very aware of this. This is one of the priorities we're working with high priority on decentralizing the sequencer. We will have some results in probably a few months from now, but it's kind of even a lesser priority than decentralizing the proverb paradoxically, because you need to be able to at least distribute the prover across many different types of distributed compute systems, cloud services or user systems.
04:23:14.398 - 04:23:50.782, Speaker A: Because otherwise, if your prover has very high computing requirements on the hardware, you're going to be locked into the most efficient cloud providers and they will have a lot of power over you. Right. It will be able to just shut it down. This was very clear to us from the beginning and we invested heavily in GPU optimizations. And recently, just this week we announced boojam a new implementation of the proof system thank you. Orders of making it more performant than what we had before. It's one of the fastest proof systems in the world.
04:23:50.782 - 04:24:54.794, Speaker A: We made joint benchmarks, or like seller network made benchmarks across multiple different implementations which we published. And interesting thing is, our implementation of the GPU for this new proof system only requires six to 16GB of Ram. We can bring it down to six by default, it's configured for 16. So you can run it on any GPU, like on consumer GPUs, on gaming computers, on anywhere in the cloud, on all the cloud services. Previous systems required 500GB of Ram, which would be prohibitively high barrier of entry. Right. So the idea maybe to state it for any five year old brain like mine is right now, if proofs are being done by one very large expensive computer in an AWS sort of service right now, the idea would be to distribute that computation actually to hardware devices with very low or very cheap.
04:24:54.794 - 04:25:44.722, Speaker A: Well, right now, for existing Ezk rollups, they are not done on one huge computer, they are done on many huge computers parallel which you have to spin up and shut down just in time, depending on how your demand fluctuates. So it's actually like if the systems are not efficient enough or have very tight specialization requirements for hardware you might just run out of the hardware on those cloud providers. So you really want something very generic that can reuse any type of GPUs designed for machine learning for all the generic computations. Nice. Sounds like a really easy problem to solve. Well, zero knowledge proofs made huge progress over the last couple of years. So we have very mature systems now.
04:25:44.722 - 04:26:31.060, Speaker A: Indeed, it took work of many brilliant minds to get to this state. All the incremental improvements are not as hard as they used to be, like five years ago. Yeah, I know we've got to wrap it up here now, but yeah, as you mentioned before, to just give ZK Sync a shout out. Yeah, the TVL has been going up into the right, transactions have been going up into the right. What do you attribute that success to? And then give us a little hint of what we should expect in the next twelve months. Anything you can share with the audience? Sure. So we treat it more as a responsibility than a success at this stage because we know that those systems are experimental, people are trying things out.
04:26:31.060 - 04:27:14.000, Speaker A: The next step for us would be lowering transaction fees very significantly from where we are today. We have two paths for this. One is the transaction or like data compression, which will soon find its way to Mainet. And right now we're slightly above optimistic roll ups. Data compression techniques that optimistic roll ups already use would bring us below them. And on top of that, we'll have some really interesting properties resulting from the fact that we use state diffs for data availability, not transaction inputs like optimistic roll ups. And like basically all the other Ezk AVM projects that I've seen.
04:27:14.000 - 04:27:48.362, Speaker A: Which means for certain transaction types, we will be hundreds, if not thousand times cheaper. Or in other words, you will be able to do many more of those transactions. Like, think of Oracle updates, can do 100 Oracle updates for like 100 ticks of the same Oracle update in the same batch. And you will only have to pay for one data availability slot because you're constantly updating the same slot. And at the end we only have to publish this one delta, which will open really interesting possibilities for whole class of applications. That's really exciting. Alex, congratulations on the news.
04:27:48.362 - 04:28:15.134, Speaker A: And yeah, thanks guys for listening. Good job, guys. Well done. Sure, thanks. The next one is just be Kenny. Kenny, cool. David.
04:28:15.134 - 04:29:13.560, Speaker A: Nice to meet you. Kenny. You were with Manta. Ready? So thank you guys for the state of and glimpse into the future of ZK through ZK Sync. Next up we have Kenny from Manta to talk about Celestiums and EVM specific ZK applications. All right, thank you. Hey there.
04:29:13.560 - 04:30:11.858, Speaker A: I guess I'm just going to take a seat. Is there a clicker? Can I get a clicker? Do I have presentations here? Oh, perfect. All right, no worries, no worries. Okay, so I'm going to try to go through these really quickly because I know we're running a little bit behind, but very simple explanation here is just this is a very fancy car, right? It's a Lamborghini and this is a Toyota Prius, right? And so, despite being so different in some ways, they actually share a lot of similarities. And one of the similarities they share here is this company called Autolive. And autolive it manufactures seatbelts. And so whether you're sitting in a Lamborghini or whether you're sitting in a Toyota Prius, you are using an Auto live seatbelt.
04:30:11.858 - 04:30:55.282, Speaker A: And the reason is because seatbelt manufacturing requires compliance, adherence, regulations, making sure people are safe when accidents happen. And car companies, right, Lamborghini, they want to build these super luxurious cars. Prius toyota Prius. Toyota wants to build these super economical cars that are efficient, right? And so everyone has what they want to build in mind, their vision for a perfect car. But that perfect car doesn't require them to build the seatbelts from scratch by themselves. And so instead, they purchase seatbelts from Autolive. And it's the same in many industries, right? So in technology, you've got software as a service system integrators that bring together pieces from all over the place and then even airline seats.
04:30:55.282 - 04:31:59.050, Speaker A: So Airbus, for example, they don't build their own airline seats. They just buy manufactured airline seats and stick them into planes. And so that's exactly what we're doing, using zero knowledge. And so the problem with zero knowledge is that it's traditionally extremely complicated for a solidity engineer, right? So if you're building a decentralized application, you're focused on building an NFT marketplace, you're focused on building some sort of DeFi lending protocol. You don't know cryptography, you may not know rust, you may not understand math behind elliptic curve cryptography and all that other stuff. But that's a problem because ZK right now, and I think in the future of web3, is a critical piece for actually scaling out a huge user base, right? If you've got 5000 daily active users, sure, maybe ZK isn't necessarily important. But when you try to scale that out to 10 million daily active users, like what Duolingo has, then all of a sudden now you've got 10 million people making data points on the blockchain every single day.
04:31:59.050 - 04:32:40.386, Speaker A: And every single data point is fully public. And so that becomes a huge issue, right? Because now it becomes a huge surveillance tool. And so ZK inevitably is a critical component for decentralized applications. But the problem is that there's not really any way for solidity engineers to easily build these types of applications. And so that's where we come in. So we offer in an environment where we actually abstract away all that complexity on the ZK side and offer it as SDKs, offer it as interface contracts, so that solidity developers can actually integrate these features into their applications without having to build it from scratch themselves. So back to the seatbelt analogy.
04:32:40.386 - 04:33:45.830, Speaker A: You don't have to worry about the compliance, you don't have to worry about the regulation, you don't have to worry about the safety, you don't have to worry about the materials, right? Like it's all there through our circuits that are abstracted away through the smart contracts. And so why is Manta Pacific that perfect environment to build it's? Because we have lowered and optimized transaction fees, transaction costs for ZK related transactions on chain. And so if you've ever used like a ZK tool on Ethereum, it's super expensive, especially compared relatively to other applications on ETH. And so we've taken away some of that cost through using Celestia for the data availability layer. We have modified the Op stack through Caldera and we are introducing our universal circuits through the ZK side. And so we call it ZK as a service. It essentially allows people to build ZK enabled features directly into their applications, only using Solidity.
04:33:45.830 - 04:34:18.002, Speaker A: And I'll give an example of this, just a second. So ZK Shuffle is actually one of those example circuits ZK Shuffle allows on chain private shuffling. So what that means is, like, if you're playing a card game, right, you probably want to shuffle the deck. But if you shuffle the deck without any privacy, then everyone can see the state of the deck. So everyone can see where every card is. And that ruins the whole card game. And that's why card games on chain don't exist, right? You might have card games off chain that bet with crypto, but you don't have fully on chain card games.
04:34:18.002 - 04:35:00.114, Speaker A: But with ZK Shuffle using zero knowledge proofs and homomorphic encryption, you're actually able to achieve that. And yeah, if you don't want to interface with the actual contract itself, we have SDKs as well. And so our SDK allows developers to actually build even faster than just interacting with our contracts. ZK Holdum is actually live on our testnet right now. And in the past two days of being live, they've run about over 5000 games now around the world from people that are actively playing online on chain poker, fully on chain poker with each other. You can try it out yourself. And yeah, so just to kind of give you an example to quantify this.
04:35:00.114 - 04:35:51.806, Speaker A: So if we're building a very simple game, a very simple game where we shuffle the deck and then we choose a card from the top and then before we reveal the cards, we place a bet, and then we place a bet, we reveal the card, the higher card wins, basically, right? So it's a pretty simple game. But if you were to build this game from scratch on any sort of EVM machine, it would probably take 1000 lines of code. By comparison, if you use the Manta interface contracts through the universal contracts, it's going to take about 150 lines of code. And then if you use the SDK itself, it's only going to take you. 30 lines of code to build that same exact game. Right. So the amount of time and effort that it takes is significantly reduced, not just on the coding side, but also on the mental readiness side because you don't have to think about the cryptography, learn Rust, learn Circom, et cetera, et cetera.
04:35:51.806 - 04:36:26.444, Speaker A: So it takes a lot of time, or it takes a lot less time and it takes a lot less coding friction. Yeah. So we're live and yeah, that's that cool. I think I have two minutes left. Any questions? All right, yes. So the use of Op stack in conjunction with the universal circuit, how does that architecture work? Yeah, good question. So there's two phases to this.
04:36:26.444 - 04:37:06.360, Speaker A: The first phase is actually we're deploying the universal circuits as contracts themselves. Right? And so that's going to enable us to just allow developers to hit the ground running from day one. But what we are doing is we're essentially building them into the pre compiles on the execution layer. And so in the future, the verification happens directly within the execution layer, and so that can help with covering costs even further. So right now it's just smart contracts and in the future it'll be on the execution layer. Cool. For the VM you're using, are you using the PSC Zkevm or using your own Zkevm or how do you sort things? Oh, it's not a ZKE EVM.
04:37:06.360 - 04:37:36.532, Speaker A: It's op. Right. So we're not using any sort of ZK on the infrastructure level or the scalability side. Yes. So some of these circuits will be using like, some ofhore from PSE and then some of these we're actually going to be building in house. Okay, cool. Yeah, that's that's it.
04:37:36.532 - 04:38:09.452, Speaker A: Thanks. Yeah, let's do it. Cool. Thank you so much, Kenny. Really appreciate that. And seeing Zkake technology turn from theoretical to application, which is extremely exciting. Next up we have ishmael to talk about zero knowledge proofs sorry, zero Knowledge Proofs for Modular State ishmael thank you.
04:38:09.452 - 04:39:05.804, Speaker A: Thank you very much. And I'm guessing this is the clicker. Awesome. Perfect. So the title of this talk is Zero knowledge Proofs for Modular State Composability. So broadly speaking, when we discuss building expressive zero knowledge computation that uses on chain data, we typically discuss it from the landscape of how to build VMs that support different execution languages that run on top of onchain state. One of the things that we often don't discuss is the state itself and what is the data that an application has access to at the point that it is executing a state transition.
04:39:05.804 - 04:40:27.768, Speaker A: And so to start with, broadly, I like to make the statement that state machines and modular state machines in particular are constrained by access to state. And at the point of execution, the existing on chain data structures that we interact with typically have limitations in the computation that we can run on this data. So by on chain state, what we mean typically is all of the blockchain state that's contained for all contracts, all account balances. And then you can think more broadly as all receipt routes or transaction routes that exist with respect to a single point in time, the totality of a block and by extension all previous blocks that are linked to it. So, when we think about state access on chain in the context of a monolithic infrastructure, typically execution, storage and access are rather simple. So contracts have access to all variables contained in them intuitively as well as to the broader storage tree through running inclusion proofs and they have access to state of other contracts through calling view functions. When we think about this in the modular context, things get a little bit more out of hand.
04:40:27.768 - 04:41:29.420, Speaker A: Typically execution environments are separate, state trees are separate and access is asynchronous and can only occur through bridging and messaging. And what this means is that we are fundamentally in the modular context, existing and building applications. In an environment where computation on state is fundamentally fragmented. We have 0.4 second, two second block times, or even faster in some cases spitting out new version data structures that we are for the most part unable to execute computation over a morass of historical data, a morass of cross chain data, functionally, a large data lake that we really are unable to verifiably access. If you are on a given modular roll up, you likely don't have access to your own historical state at the point that your application is executing state transitions. And even more probably you don't have access to the state of other applications and other modular roll ups.
04:41:29.420 - 04:42:31.916, Speaker A: Very likely not their current state and even more likely not even their historical. So you might say what about storage proofs? What about storage proofs? There's a lot that people are talking about storage proofs now, but in reality they are really only a slight improvement. What they're good at is saying you have a block header and with respect to some historical index of some historical block number, there exists some state in the world tree. So the value of memory slot n at address x at block height, let's say p minus k equals something that you now want to compute on. What this can't let you do is things that are rather expressive. So assume I have a contract on a GameFi application with a canonical bridge to Ethereum. My DAP needs to access, as part of its logic every pudgy penguins NFT owned by a given address in SQL.
04:42:31.916 - 04:43:42.970, Speaker A: If this was off chain, it'd be very easy to compute this select all from Pudgy penguins where owner equals XYZ. But as a thought experiment, how do we do this on chain and verifiably? Well, for those of you familiar with Solidity, you would know you have to iterate through every storage slot for that NFT and check who the owner is. This is a mapping type and it could be, of an arbitrary size. And this is the same principle that applies for computing on most on chain data. There is really, unless you are to build a custom side index to store information, not really an ability to verifiably compute on top of large sets of onchain data, let alone to extract meaningful properties through the execution of query specific languages or data parallel computation broadly. And so what this leads us to is a very specific set of parameters under which we can access state data on chain. If you want to ask a question like variables contained in a given contract, you could likely message something back and forth between chains or if it's on the same chain as you or roll up access that to a function call.
04:43:42.970 - 04:45:09.812, Speaker A: You can also access with storage proofs in the simplest context a single slot at a single point in time of an arbitrary contract that you have the block header of. But what you can't do is very expressive questions searching and computation on data the same way you could do in a web two database. You can't say what are all of the NFTs owned by address x with property y? What you can't say is what is the price of ETH to USD pair in univ three over the past ten blocks when doing some computation to factor outliers and broadly you can't do volatility either. There's really a data constrained and data scarce environment when interacting on chain increasing the expressivity of the VM and now supporting Rust or now supporting Go or now supporting another front end language for the execution of contracts on an execution layer doesn't solve these fundamental issues. And this is where we get to what our team at LaGrange does. So LaGrange builds what we call ZK MapReduce, which is a proprietary proof construction optimized for generating large scale batch proofs on top of on chain state concurrently with arbitrary dynamic data parallel computation. It's a lot of words.
04:45:09.812 - 04:46:26.700, Speaker A: What that means is you can run MapReduce SQL, spark RDD Verifiably on top of large amounts of on chain data and consume the results from your contract. Now you might say well, where does the data come from? What makes this interesting is that this is transport layer agnostic. So if you have a canonical bridge, if there's a cross chain protocol that you've opted into using for your liquidity transfer, or if you have some inherent interoperability in the modular protocol you use for other instances and other modular roll ups, you can consume all of that state as you would traditionally. But you can now add this type of additional computation on top of it to natively inherit the same security assumptions. So how to generate a proof is quite straightforward. What you do is you take a block header from a source chain that you trust from some crosschain protocol and it could even be from the chain you're on where the block header call and then you specify a range of block numbers and storage slots and then a data parallel computation to run on top of it. Next, all you need is a verification contract on the destination chain or even on the chain itself that you're computing on to derive and to verify the result of that execution.
04:46:26.700 - 04:47:33.228, Speaker A: And why is this valuable? It's because it allows you to start treating on chain state from the perspective of an application like the underlying database that it is. You can now compute over historical blocks at a scale that was otherwise unprecedented, you can compute over multi chain blocks and cross chain data at a scale that was otherwise unprecedented. And you can do so without incurring any additional security assumptions or risk to your protocol. So let's go through a couple of use cases. We talked about the NFT one before, where you want to select some feature set or some property, or you want to select NFTs based on some property. So in this example, you can compute with our construction directly on top of the onchain data and derive a result that is verifiable with respect to where you want that query to be verified. You can think of the second use case, right? Let's say a dow wants to issue a reward for users who have completed some prerequisite or some preconditions that are relevant to that dow's operations.
04:47:33.228 - 04:49:13.280, Speaker A: In this case, what you can do is you can take the on chain data relevant to that activity, you can split it, you can map computation to it, then you can reduce and you can get a final proof that you can use and verify to understand this complex query and to be able to run that verifiably on top of onchain state. And another interesting use case, for example, if you would like to do a black scholes analysis and what you would need for that is volatility, you can derive that trustlessly on chain the same way you would from a database. Your computation can run on top of historical blocks verifiably at a scale of, let's say, one month of ethereum blocks, roughly 220,000 blocks, and the latency on the computation is de minimis at scale, since these constructions have some very interesting properties that are otherwise unavailable in zero knowledge implementations. And so what our proofs have is something that we call full updatability. And what this means is that portions of our computation, in particular data parallel computation that run on top of overlapping or non overlapping data sets can be composed and combined back together to form new proofs. And what this means is that when you think of the constraints on zero knowledge computation historically it is the proving time. It is what can I compute in a relevant period of time such that my computation can be verified without incurring or resulting in a time delay that makes the computation not useful.
04:49:13.280 - 04:50:20.568, Speaker A: And so with the updatability and the composability of proofs, you can, as you compute more, leverage the same underlying portions of that proof to compose them together to form new ones. And so what this does is, at scale, it minimizes proofing time for generating large and expressive computations on data. And simultaneously, it allows you to scale not through expensive recomputation, but through composition and recombination of proofs. And because this can run on overlapping and non overlapping data sets, and portions of proofs can be very quickly and very easily cataloged and recomposed, the result is proof generation times that are in the orders of magnitude of I think in our paper, it's about 1600 times faster than the naive implementations. And so for more information on this, we'll be doing a talk at SPC later this summer. We'll be unveiling the details of the construction and the performance of it, as well as releasing a test net around this point as well. That concludes the talk.
04:50:20.568 - 04:51:17.076, Speaker A: Any questions? Yeah, so the question do you require yeah, so what we require typically is a prover that is running. We require a prover. Right. And you also require the prover to have have access to correctly indexed data from the chain in question so that compute the computation out of circuit and then verify the result, then verify it in circuit. Initially, we're running the provers ourself because that makes the overhead much less and it makes it much easier to integrate with. But eventually we'll decentralize the proving infrastructure. Yeah, our construction is proving system agnostic, so we can support anything that uses either recursion or folding.
04:51:17.076 - 04:52:20.780, Speaker A: So, like most modern proof systems, the first implementation implements we have use Plonky Two and Nova. Thank you. Anything else? All right, well, thank you, everyone. Thank you so much. Thank you, Ishmael, for the beautiful talk on the important work LaGrange is doing for composability and ZK technology. Next up, we have a panel for ZK applications moderated by Ashita with Daniel, Harry, Naraj and Lakshman. Again, to talk about novel applications of ZK.
04:52:20.780 - 04:53:24.970, Speaker A: Welcome to the stage. Thanks, guys. Okay, so yes, Harry harry's a charismatic guy. Too beef.
04:53:30.390 - 04:54:02.780, Speaker B: Okay. All right. We are going to be talking about some exciting ZK applications. As you may know, ZKPs have only really become feasible in the last few years, and there's been tons of experimentation going around trying to use them to extend and improve on existing technology. So specifically, we're going to talk about machine learning and a little bit on identity, given the expertise of our panelists. So we'll start off with introductions. I'm Ishida and I do research in the space.
04:54:03.310 - 04:54:21.410, Speaker A: Hey, everybody, I'm Daniel, and we start a company called Modulus. We work on zero knowledge machine learning. And I'm Lakshman. I work on Personi, where we think about what identity looks like on Ethereum in 100 years, which turns out to use zero knowledge.
04:54:24.230 - 04:54:33.590, Speaker B: Can each of you walk us through the state of ZK tech right now and specifically, what's been built within and outside of crypto and what's still a work in progress.
04:54:35.450 - 04:54:59.454, Speaker A: Sure. Yeah. I think actually Lakshman and I agree on maybe a lot of this, so I'll make sure to leave some protein to be shared. But kind of from our perspective at Modulus, the big scary thing about all this investment into ZK and its application to blockchain networks is that it's all focused on proving VM operations or EVMs. Right. Which kind of makes sense. Right.
04:54:59.454 - 04:55:29.894, Speaker A: We want to prove the entire execution of a blockchain network, compress that effectively and then bring that onto the network that's doing the verification. Awesome. But it leaves a lot of space for other kinds of kind of specialization. Right. So in the case of Modulus, we're really excited about machine learning. And if you know anything about AI operations, you know that it's super repetitive, it's highly structured and it has these architectural features that represent a downselection of all possible VM operations. Right.
04:55:29.894 - 04:56:15.480, Speaker A: And so I guess our insight was that potentially if we build a ZK system that's purpose oriented for machine learning, then we can get much better proving overhead. And actually this is a little bit of what Harry works on as. So that's kind of our orientation. We think that might be mean, we were just talking about this backstage. But a very similar observation that what, a zero knowledge proof isn't the ideal. There's many different parameters you can improve on based on what specifically you're trying to do. And so a lot obviously most attention is on scaling VMs, which I think is a good thing.
04:56:15.480 - 04:57:14.150, Speaker A: We think mostly about the zero knowledge side and specifically getting things to work in a very resource constrained environment. It's super important for a true zero knowledge application, for the users of the application to not be delegating the proving anywhere else or sending the private information anywhere else. So we spent a lot of time thinking about this very specific problem about making these very specific ZK circuits I e. Like proving a signature for a given crypto system work on a mobile device. And that's led us down like a pretty unique path in terms of proofing system and stuff like that. But at the same time, it's super inspiring seeing we take a lot of inspiration from all of the stuff happening in the more general systems because the thing I'm learning, I'm honestly not a cryptographer by training, I've been learning a lot as we go. But the moving pieces of a lot of these systems are kind of very common.
04:57:14.150 - 04:57:23.146, Speaker A: And so we're learning lot, just learning like seeing what's being tried and understanding the different trade offs between using the different components of these systems, which is cool.
04:57:23.328 - 04:57:34.826, Speaker B: Hey Harry, you want to do a little intro? And we're just talking about the state of ZK tech, so talking about what's been built within and outside of crypto and what's still a work in progress.
04:57:35.018 - 04:58:29.890, Speaker A: Awesome. So yeah, I'm harry I'm one of the co founders of Jensen. We're a machine learning compute protocol which uses proof systems in order to lower the costs of verifying that trading has happened. Concretely, we use polynomial interactive oracle proofs in our verification system, which were quite recently popularized by Justin Fowler, I think, broadly speaking, in the kind of wider applied geo knowledge space, there's like three trends which come to mind. The first would be around the overall kind of trend away from using elliptic curve cryptography into using more of kind of hash function security. So you see that with examples like StarkNet. You also look at things like the kind of emergence of zero knowledge ethereum virtual machines which assist with the wider kind of confirmation of the blocks on the chain itself.
04:58:29.890 - 04:59:06.558, Speaker A: So you see that with scroll. But then, most relevantly for us, it's about very kind of operation specific forms of zero knowledge. So for example, you see with Z conduits a ZCO library for doing zero knowledge image classification. Like Zkcnn, they typically use kind of off the shelf proof systems for VAT. But we think that that can become much faster, like maybe free orders of magnitude faster if you specialize the proof systems. So that's a bit about me and then the kind of free areas that I'm excited about.
04:59:06.724 - 04:59:23.140, Speaker B: Thank you. So first, let's touch on identity. I'm excited to see what you say about this. So in the advent of AI, do you think it's important to distinguish bots from humans? And how far can biometric verification take us?
04:59:23.830 - 05:00:43.686, Speaker A: Yeah, it's funny, I've been having this conversation with a lot of people recently, and I'm in a strongly opinions weekly held place of mind where I want people to tell me I'm wrong. But also something feels quite true about what I think right now, which is I think there's a conflation between AI and bots, which is humans controlling AI. In the sense that if we're distinguishing between a human having made the signature on some piece of data as whether or not it's like an AI. A human could stamp a thousand pieces of data and send them to a bunch of bots and distribute to a bunch of people. And I think that's the thing I think a lot of people are scared about when they look at Twitter bots and sort of connect that problem with sort of the value of distinguishing AI and humans. And I think that that's really distinguishing humans from humans with bots, which is slightly different. As for literally whether or not it's good to distinguish humans and AI, yeah, I think in the stuff we're building, we think a lot about the importance of exit in our systems.
05:00:43.686 - 05:01:28.060, Speaker A: We want humans or AI to sort of be able to shed their old identity and form a new identity very easily. This feels like fundamental to some of my personal values and some of the old crypto libertarian, crypto anarchist values and feels like a good property of a system to have. And it's hard to have such a system if every human is only one identity or is identifiable as a human. So I think that's quite challenging. I also worry that if the probability of AI doom is high, then any system that is good at identifying humans is really like a human directory for bots, for AI. It becomes a tool for control by not necessarily something that humans are using anymore. So it's a very nuanced question.
05:01:28.060 - 05:01:33.230, Speaker A: I don't know what the answers are here, but I also think that the narratives are probably a little simplified.
05:01:34.610 - 05:01:36.286, Speaker B: Do you guys have anything to add?
05:01:36.388 - 05:02:18.854, Speaker A: Harry I've got a lot to add. I think it's important for two reasons. The first one is there will be, I reckon in the next nine months, a major public outcry about the use of, or I guess, the acquisition of other people's identities. And it's used to defraud people. With the state of the art, kind of generative voice and vision models. It will become essentially impossible to delineate real from fake. And a really kind of clean way of solving that problem is by somehow proving your humanity and then using that proof to, say, chunk and sign any calls or kind of video that you create on chain.
05:02:18.854 - 05:03:14.030, Speaker A: So you can imagine that if you were in a Zoom call, this simultaneous to the Zoom call, you could be kind of independently confirming that, yeah, you're on that call. It's you, it's your private key, et cetera. That's the kind of short run benefit of it. I think it's just more of a kind of security and fraud piece. But the longer term advantage, I think, is we see at Jensen within the next five years, the majority of compute being requisitioned will be by machines, not people. We think that the kind of substrate for artificial general intelligence will be on Chain. And when that happens, you get these kind of very philosophical, almost like Blade Runner style questions about who's real and who has rights and what are the rights of the kind of autonomous beings out there? I think having a way to confirm yourself is essentially the same as having a passport or an identity in general, and you need that to function in a world where we're kind of coexisting with another artificial species.
05:03:15.570 - 05:03:23.680, Speaker B: So how do you think humans and bots, how do you think we can be collaborative in the future?
05:03:27.730 - 05:03:56.262, Speaker A: Yeah, I think it starts with the ability to, I think, kind of what's already been mentioned, but just stamp one as one and the other as the other. Right. So in some sense, what we work on at Modulus is the ability for AI processes or AI models or AGI one day potentially to stamp their work. Right. We do the Is model kind of process. Right. I mean, the way we do it is using ZK circuits and generating a proof that's verified somewhere.
05:03:56.262 - 05:04:53.686, Speaker A: But you can imagine the analog being like an artist signing their masterpiece and then giving it to somebody, and then that signature being what testifies to the authenticity of that work, right? In fact, we're in the process of taking a generative model. In this case, it outputs pixel art and putting it on chain via this ZK process and embedding it in kind of the NFT container, right? So the actual machine artist is itself on chain. And that's really interesting, right? Because suddenly this AI process is a distinguishable entity. And obviously you can imagine lots of these different agents interacting with each other as a kind of collaborative process on chain. Because there is that agreed upon immutable basis for reality there. So that's like one vision, there's lots of other ones. But I try to get very specific because AI is so expressive and it's so easy to get into the AGI conversation.
05:04:53.686 - 05:05:56.394, Speaker A: I'm still trying to get my toast to come out right every time. So there's always more nuance and more kind of process to get there. I'm actually kind of curious to ask both of you and just to push back because I think I represent the other perspective. Why fundamentally is it good for AI and humans to sort of be distinguished online? I think this is more of a feeling than a rational thought, but it seems conceivable that such a delineation creates less empathy between the two agents in the very long term. I'm sort of maybe naively envisioning a world where there are these more powerful beings out there, but we're all sort of doing the thing and it doesn't matter who's who. Yeah, I'm curious to dig into that a bit more. Yeah, so for me it fundamentally comes down to liability.
05:05:56.394 - 05:06:39.630, Speaker A: A good example of this would be autonomous driving. If you have a car which is self driving and the car crashes into someone, do you think you're as responsible as if you were driving it? If the answer is no, then at fault, which is for me. Then it follows that when you have a brain machine interface attached to your scal in approximately ten years and that interface is using on your own network to expand the scope and memory that you have in your mind and also making some decisions for you. Like maybe it's filtering your emails or something. Or maybe it's doing even more things. Are the actions of that model your actions? I think the answer is also no. So that requires there to be an individual identity for the organic part of your brain and the artificial part of your brain.
05:06:39.630 - 05:07:12.730, Speaker A: Yeah. I think the liability point is very concrete, but maybe the other orientation to tackle this precise kind of premise around empathy. And to be clear, I don't think it will be binary. Right. There will be some operations and some ambiguity as a default and then there will be certain kind of properties or processes that are stamped with signatures in the future. That's kind of my impression of things. But I actually think the better path to having empathy for one another or making the collaboration more productive is the capacity to delineate bot from human.
05:07:12.730 - 05:07:36.100, Speaker A: Right. Just look at kind of stability, AI and stable diffusion and all this uproar against like I'm an artist. I feel like my artwork is being attacked because it's being used to train. Models are much more performant than I can be at a really high scale. We're looking at the Hollywood kind of protests right now between the writers and the actors saying, I just want you to put in the contract that you're not going to replace me with an LLM. That's very performant. Right.
05:07:36.100 - 05:08:19.360, Speaker A: I don't think it's at all practical to put the brakes on these life changing technologies that can be very productive for society, like LLMs or generative models. But I also think that if we want to build some, this is what, I guess, blockchain systems are really good at, right? Which is like ground truth. This is the reality that in the digital world anyways, that we all have to agree to. So if we can take advantage of that property and put the things that we want to differentiate or delineate on that ledger, then that, for me anyways, is a more egalitarian way of approaching empathy. Right. Between humans and agents. Yeah, that makes sense.
05:08:19.360 - 05:08:37.394, Speaker A: I suppose there are also humans whose factors on chain make them see more AI than some AIS. Right? Yeah, that's a fair point. Don't want to navel gaze on this too much and take the panel away from actual applications of ZK, but I'm excited to jam more on this later.
05:08:37.432 - 05:08:49.000, Speaker B: Yeah, that's interesting. So we can switch gears and talk about Zkml a little bit. So, Daniel and Harry, what's sort of the state of machine learning at the moment and the role that trust.
05:08:52.830 - 05:09:49.914, Speaker A: Yeah, I guess from my angle modulus, we spent a lot of time thinking about cost. I'm sure Jensen is actually quite similar in this regard, in part because to me, as a you know, we all kind of were doing AI research before this. Right? The story of AI is a little bit the story of cost. Right? Like, what happened in 2009 ish that suddenly deep learning as a method for training models and in generating these emergent amazing magical outputs, why is it that 2009 was this pivotal year where decades of theory suddenly got put into practice? For me, it's GPUs and the fact that this new computing form factor became widely accessible. And look, you can build a GPU server farm out of CPUs, it would just be significantly more expensive. Right? So we brought the cost of compute down significantly, made it much more accessible. And in selling these methods of training models, deep learning, oftentimes known, gave us the models that we are.
05:09:49.914 - 05:10:46.362, Speaker A: All really excited about today, right? And obviously we can draw parallels to the ZK world and circuits. I think that's later in the conversation, but maybe I'll just end by saying it's like super non obvious, right? It's not like if I get more parameters, my performance on my model is linearly better. It's this weird step function looking thing where emergent properties just show up as models get bigger and more sophisticated. That doesn't seem to be changing anytime soon. So we're just all marching towards more parameters and more compute and more electricity and so forth. I'm sure that blows up Jensen's tam into something amazing, which is awesome. I'm very excited for Harry and Ben, but I think from my angle we're just at the beginning, right? Because although that is the dominant trend, there's also the trend of more sophisticated architectures, better learning models or better learning methodologies, right? Quantizing our models to save on electricity and make them more ZK friendly.
05:10:46.362 - 05:11:36.294, Speaker A: There's all these subfields of research which are emerging as well underneath the current of more compute. More compute, more performance, more attention, more money. So, yeah, AI is most likely or almost certainly going to be the most expressive transformative technology of our time besides maybe crypto. But I'm biased. Yeah, I would just vibe on all of that. For anyone who doesn't know, the kind of force of deep learning came up in the kind of late 1940s. The neural network architectures then during the kind of 50s had its first applied use case predicting Weber for the US Navy big kind of winter in between late 2000s, stacking different layers of a neural network and combining it with something called stochastic gradient descent.
05:11:36.294 - 05:12:27.198, Speaker A: So basically the idea that you are comfortable with the model learning in a slightly randomized but more efficient way combined with the compute power that Daniel mentioned, causing explosion in image recognition models. So the classic was like predicting is it a cat or is it like a muffin or is it a dog or is it a muffin? And all this kind of like these kind of funny toy examples. And then throughout the kind of 2010s we got to the point where around 2018, 2019 transformer models came out. Transformer models were basically used a concept called attention where you have different heads on the model. You could think of it like a kind of hydro of lots of different heads and they're each paying attention to different things in a sentence. And then more recently kind of diffusion models came out. I think core to our thinking around the kind of reasoning for using zero knowledge machine learning in this space is really what Daniel said, it's around cost.
05:12:27.198 - 05:13:02.770, Speaker A: So just kind of two important things here. Generally speaking, if you throw more GPUs at something, it gets better. The kind of scaling laws do hold. We haven't broken them yet. Number two, there's enormous kind of margins charged by the cloud oligopolists like AWS and Azure around 70% to 80%. If you can find a way in a decentralized setting, peer to peer with no middleman to train these models you essentially increase every dollar spent on training by four to five X. The best way to do that, in our opinion is to use very lightweight proof systems.
05:13:02.770 - 05:13:44.790, Speaker A: And then immediately you find yourself in this kind of conversation around zero knowledge machine learning because you don't want to have lots of people training. The model gets super expensive. A good example of, like an attempt at that would have been TrueBit by a Toych and others in kind of 2016 17 which was a really good kind of first step in the direction but it didn't go far enough to getting the overhead down. I think we calculated it to be like six X which basically adds the margin back on. However, with the recent advancements in proofs you get that cost way down. We anticipate that the cost right now is about 25% in Jensen to verifying the model has been trained which makes it the unit economics substantially better versus the cloud giants.
05:13:45.930 - 05:13:58.086, Speaker B: So, Daniel, could you discuss some of the trade offs between using CKPS amongst other, like, FHES or something else with marrying to ML?
05:13:58.198 - 05:14:41.894, Speaker A: Sure. Yeah. I guess ZK has become kind of a dominant narrative within the crypto zeitgeist because it has this kind of bizarre property that we call Succinctness that makes it a great fit for blockchains. But it's always important to remind ourselves, especially here in the ZK world that there are, in fact, other cryptographic techniques and they do different things and they're all super exciting. Right. Fhe kind of, I guess, homophobic encryption is most, I guess by default associated with privacy the ability to operate over data without understanding or having the capacity to understand that data the actual content of that data. So there's a lot of privacy applications on that.
05:14:41.894 - 05:15:15.586, Speaker A: And of course, there's MPC where you want to split up some key or any number of other techniques as well within the canon of cryptography a lot of which is often used in the crypto industry. I guess what I'll say on that is and not to hammer a point constantly but is nonetheless cost. Right? There is always a cost to doing these sophisticated cryptographic operations than to not do them. Right. In fact, when we talk to our early customers the first thing we always ask them is can you get away with not using us? Just like, don't do any crypto. Just do it the normal way. Right.
05:15:15.586 - 05:15:50.814, Speaker A: Do it centrally. Are things broken? Okay, then we can talk the cryptography. Right? Because it's always a premium. And so that's what I try to focus on just to make things really specific and practical again. But certainly when it comes to the day to day work of what it looks like to marry machine learning and ZK we actually still have a lot to learn from these other avenues, including Fhe, which have been dealing with the intersection for a lot longer just as an academic discipline. So a non trivial portion of my time is spent talking to academics in those kind of disciplines as well.
05:15:51.012 - 05:16:08.070, Speaker B: So what has been achieved so far as representing ML models as a circuit? Are we at the point where we can express really complex models with a lot of parameters? Or has it just been pretty much R D early stage?
05:16:10.330 - 05:17:17.340, Speaker A: Yeah, training maybe. Yeah, with training it isn't at a level yet. So if we think about the circuit generation or even, as Daniel said, looking at from a fully homomorphic perspective, it's just too slow for our use case, which is generic training without the kind of privacy component, it's just too slow. What we do use it for vo is inference within training to monitor model loss. And that works well typically because you don't have to a put all the data through, you can just put like a batch through, and also because you can typically checkpoint loss in kind of intervals as opposed to having to do it all the time. And the benefit, of course, of being able to monitor the model's training, like learning, I guess over time, is that if things start to look unusual, you can then pinpoint kind of audits around the area which looks unusual, which makes the whole system much more secure. I think, broadly speaking, there's always going to be a lag, particularly with the fully homomorphic stuff behind the kind of state of the art just using a GPU kind of locally in kind of clear text.
05:17:17.340 - 05:18:26.734, Speaker A: But there are definitely use cases for it, as Daniel mentioned, I guess Lakshm. What does that look like in the world of signatures? Right. Which to me in the machine learning world, and probably Harry as well, it's like oh, signatures should be relatively straightforward, but maybe does your focus on client end devices change? So one of the really challenging things that I think is true of a lot of ZK applications is we need to sort of adapt sort of the numerical primitives of sort of whatever we're trying to do to some field, some finite field. I think a lot of our challenges have come from doing trying to do arithmetic in a field that's smaller than the field of the cryptography. We're actually trying to do wrong field arithmetic. And this is really untenable on a mobile device because the witness of the ZK circuit becomes really large. So we've spent a lot of time so literally the memory requirements become very big.
05:18:26.734 - 05:19:18.446, Speaker A: That's the thing that probably heat up your device if you're using some other zero knowledge proof stuff. So we spent a lot of time kind of trying to see, I think broadly have the premise that there are certain crypto systems and signature schemes that will probably be 80% of signatures that matter in the world but then kind of want to be open to making statements about many different systems. So we're choosing proving systems based on their ability to be agnostic to the underlying field so that we can kind of change that up as we go. But yeah, it's a very different, much tighter and scoped problem but also one with the constraint of memory on a mobile device. Yeah. Okay very interesting and part of why I ask is because we also struggle with memory. Now the scale might be a little different right? It's like vGPUs and AWS and azure.
05:19:18.446 - 05:20:09.970, Speaker A: Sorry, Harry, for now. And it gets very expensive very quickly because oftentimes it's like the peak memory consumption in the proving process. Right? And so we need to rent a massive machine even if we don't use all the capabilities of that machine most of the time. Right and that's just a product of what proving systems often are. But that doesn't make any sense when you think about the structure of machine learning compute which is generally pretty predictable, very consistent. And so I guess our insight when we bumped into that initially in our early days of just looking at this almost as a scientific curiosity is this proving schema is not appropriately acknowledging the fact that we're downselecting for a more specific kind of problem. Right? And maybe the two Harry's earlier point about building custom provers which are actually tailor made for that specific function.
05:20:09.970 - 05:21:06.402, Speaker A: I think one of the narratives which will pick up some more gas again in the crypto zeitgeist, if I'm allowed to hazard a guess here is this idea around specialized proving schemas or proving systems and I'm very excited for modulus to put in our kind of spin on that as well as Personi or Jensen. But in some sense, proof is in the pudding. Performance one of those. And I think when y'all see these numbers, it's like, man, for these really intensive applications, whether it's training inference for machine learning, or for proving anything on a mobile phone on an edge device, it makes sense to go down the specialized route. And once you do, the water is warmer, the sky's clear, your spouse is more attractive, your kids are more disciplined, life's just better. So this panel I think actually represents like one side of this debate. Unfortunately we all agree with each other maybe that's not very exciting to work.
05:21:06.402 - 05:21:45.954, Speaker A: Well, there are some other I don't know if this is true, if lookups are something you guys think much about. But I think in my perception, there's this one day magical technology improving systems like a lookup table, basically. And at some point it'll be economical to just do whatever we want with a lookup table and we're sort of like following the space. And we expect at some point that curve will cross the way. We're doing things in a very specialized way, but also maybe not. I'm also actually on this topic just because I have two experts here to ask about it. What's it like doing like, floating point arithmetic in a field? I mean, that's a brilliant question.
05:21:45.954 - 05:22:19.546, Speaker A: We spent a lot of time thinking about this, where basically for our system more broadly, we need reproducibility in the kind of model training process. There's four places it gets ruined. The first is machine learning frameworks typically just aren't reproducible. They have randomness in them, obviously, like random seeds, et cetera. A bit lower down. You then have if you're training it in a decentralized system, you have all the various devices because we don't use a standardized piece of hardware. It's lots of different kind of GPUs from different manufacturers, mainly navidea, and then they're all different.
05:22:19.546 - 05:22:55.350, Speaker A: And then thirdly, the way the GPUs kind of compile the code, the way they execute them in the kernels are all different. So we're actually having to rewrite lots of the kernels ourselves, the hiring GPU engineers to do that, so that our runtime does it. And then you get even more abstract that you can get like a BitFlip or something from like a cosmic ray. You add all that in and it's a really tough challenge. We looked a lot at kind of like model quantization. Of course the issue around the quantization is if you quantize it too much, then you lose the kind of the signal, so to speak. Then if you're plotting kind of loss, but it's quantized to like three or four bits or something like that.
05:22:55.350 - 05:23:20.174, Speaker A: It's just like a straight line. So yeah, it's a really good question. The trade off would be if you create custom hardware, which works really well with it, then you kind of fall into like a centralized trap, which is something that we think about. There's a good example of Internet computer protocol. They went down this route of like, we're going to make this one type of hardware, it's going to work really well. They call them like canisters or whatever. To us, that's just way too much.
05:23:20.174 - 05:24:31.074, Speaker A: Centralization also makes you into a hardware company, which makes a hard problem even harder. I mean, I would just echo everything that Harry just said. I think the problem is it's acute, but it's not as acute on the inference side. We get away with a little more, thankfully, but maybe just to attack the same question from a different angle. When we talk to our partners and early customers never do. They go, what is your quantization schema? Is it eight bit? What happens when you lose a couple of digits, when you blow up your floating point into a finite field? They just go, hey, does it still work? Question mark, right? Like, how much performance do I lose when I need to prove to the world that I didn't manipulate my algorithm? Right? So if you want to take it from the customer centric approach, the pain point lies in that more precision less loss usually means more expensive on the proving side, right? So we're helped by the fact that, again, machine learning models are generally a bit squishier, so they're a little more tolerant to these kinds of accuracy losses. But it is very much an ongoing kind of research question.
05:24:31.074 - 05:25:12.062, Speaker A: The last thing I'll say is we're not the only ones thinking about it, right? Because obviously when you quantize a model, first of all, it's a lot more memory efficient. And so all these folks working on LLMs that are eating into massive hardware, there's only three of these machines in the world and so they're super expensive. They're really incentivized to find ways to bring that memory footprint down. And so we're helped by that research interest. We're super aligned on that. The other thing is, when you quantize a model, they're a lot more energy efficient, as in literally electricity sipped from the wall, right? And so if you want to run some complex model on embedded hardware on your smart glasses, or on your phone, or on your smartwatch, you want to quantize it and do all that. So there are a lot of complements.
05:25:12.062 - 05:25:44.938, Speaker A: And in the same way that earlier I mentioned ZK ML, anyways, we have a lot to learn from the Fhe world because they've already been working with machine learning operations for a while. It applies to quantization as well. Yeah, I'd add one final point to that. Again, agree with all of it, just common Fred on this panel. But we very much think of the kind of, I guess, extent of quantization as being very connected to kind of computational liberty. A good example, this was around the Llama model, which someone kind of quite quickly quantized when it was leaked. And then he got kind of running on a Raspberry Pi.
05:25:44.938 - 05:26:04.100, Speaker A: And then there's a decentralization kind of metapoint there about if you can actually make these things work on consumer hardware devices in a relatively kind of constrained way that does a lot for people's ability to, number one, compute, but also then to just build a different kind of models as well.
05:26:05.110 - 05:26:34.586, Speaker B: So you touched a bit on how expensive it is to build out a decentralized GPU network specifically for machine learning. Could you talk about why it was important to stay narrow and just focus on training machine learning models? And why would a developer or team go for a decentralized network that's potentially expensive? Potentially, potentially expensive, potentially take a long time to put together?
05:26:34.768 - 05:26:59.538, Speaker A: Yeah, sure. I think just kind of three points here. So my co founder and I, we kind of came from the machine learning world. We didn't come from the kind of cryptography or wider crypto world. And we faced these problems ourselves. So he was doing a PhD, my co founder, Ben, he had like four GPUs under his desk, but he was competing know, papers from Google which were using a thousand GPUs. And it was just completely kind know, impossible.
05:26:59.538 - 05:27:37.934, Speaker A: And then I was in industry where we were training fire detection models at large scale and that burned a lot of cash. So our thinking was, number one, it's important to get these costs down and that's only going to get worse with time, which is proven true with the recent advances in the LLM training and the numbers we're seeing. The second point is that if people are going to use this, number one, they're not going to be crypto people, they're going to be machine learning people. So immediately they're kind of at odds with holding cryptocurrencies and everything that comes with it. You have to drop into your workflow. Usually people will have kind of free clusters. They'll have their kind of data cluster where they store the training data.
05:27:37.934 - 05:28:27.978, Speaker A: They'll have their parameter cluster where they store the updated parameters, and then they'll have their compute cluster, which is where the kind of the cache gets burned and we basically drop in where that compute cluster is. I should say that for us, it's very expensive to build what we're building, but it's expensive to build it because it makes it super cheap to use it. And then the benefit will be we anticipate a roughly 75% cost reduction to 80% cost reduction versus using AWS. And a really interesting way of thinking about this is if you look at some of the rounds which are happening just now in the market, you see inflection AI and stuff like that, they raise like over a billion dollars. You could easily kind of bet the building that roughly 800 to 900 million of that is going on compute training costs. And if you come to someone and say, hey, we can turn that kind of 900 million into $4.5 billion for you, that's ultra compelling.
05:28:27.978 - 05:28:59.706, Speaker A: So when we saw that a couple of years ago, we were like, this is the most important thing. And finally, to your point about staying narrow, we think that the problems in training are quite distinct from the problems in inference. And we're very big believers in the kind of fin versus fat protocol dichotomy. So lots of fin protocols makes more sense than one kind of big generic fat protocol. For the same way that if you're actually building proof systems, it makes sense to have super kind of custom provers, et cetera. It's just more efficient and effective. So for us, we thought we're just going to do training.
05:28:59.706 - 05:29:32.870, Speaker A: It's just going to be the kind of neural network training. It's not going to be like statistical machine learning models and we're going to ignore all calls to sort of do inference. And lots of people tried to sway us over the years during the Art boom, like the stability AI generative art boom. People were like, oh, should do art for NFTs and stuff like this. And we were like, no thank you. It's interesting because we've also gone the other direction we focus on inference and we're like, we're going to stay narrow. I see Jensen out over there and I'm terrified of them, so I never want to compete with them.
05:29:32.870 - 05:30:08.850, Speaker A: And throughout our past year or so that we've existed, there's been a lot of like, why don't you tackle the training problem, right? Why don't you look at the opportunity space there and yes, plus one to everything Harry mentioned, and then one last thing, which is in operation, these two kind of steps look quite different. I mean, that's the truth of it, right? So even if kind of from the outside, it's ah, it's a uniform surface area, right? Just machine learning. What does that mean? When you actually dive into the pond, so to speak, they're radically different animals, right? And it makes sense to have specialized approaches to tackle each problem statement.
05:30:09.350 - 05:30:25.186, Speaker B: So how big of a problem is it that models like OpenAI is behind an API and these developers are just trusting the output that comes out of them, right. Would OpenAI even want to, in the future, build in a verification system? They're just right now trying to keep this model alive.
05:30:25.298 - 05:31:38.320, Speaker A: Yeah. I mean, anyone who's a student of history knows that when you trust huge centralized entities run by a small number of people with something as important, as kind of as what Daniel said, like what's truth? What's ground truth, what's real, what's not real, it ends up getting pretty dark pretty quickly. So there is that kind of philosophically it's not a good idea, I'd say, number one. But number two, there's just also a kind of I think a lot about the idea that AI is really just like the kind of artificial extension of your own brain. And it comes to this idea that if you have to trust something to read your thoughts, it gets to a point where you just have to know that it's what it says it is. You can't let OpenAI read your thoughts on a high bandwidth BMI and just be like cool with, yeah, sure, it's probably fine. It's like Google was the last point where you could do that, I think, because most people, what they think versus what they put into Google is probably like 99.9%
05:31:38.320 - 05:32:05.510, Speaker A: true. But there's all of that holdout, there's all of that stuff that you might not want to type into Google for various reasons, particularly if you're not living in the Western world, say you're living in a more oppressive country and you don't want to kind of expose yourself to have different political views, et cetera. If you've got something drilled into your skull and you think the wrong fault and then that can result in you being detained or something bad happening, that's a horror story. So it's a very good kind of antidote to the kind of tyranny which will be ushered in if it's kept centralized.
05:32:07.450 - 05:32:08.946, Speaker B: Anything to add, Luxwan?
05:32:08.978 - 05:33:08.300, Speaker A: Daniel, honestly, I'm just learning from you guys right now, which is really sweet. I'm curious. Yeah. I think it's very clear to me why, from the perspective of a company doing training, why training verification is very valuable. And it's obvious why, from a human interacting with OpenAI why inference verification is valuable, do you perceive that either of these sets of proofs are valuable to have on chain for some future notion of a chain? Or is it sufficient for them to just be kind of like just being distributed on the Internet? I've got lots of thoughts on that. I think, like a kind of slightly less like glamorous one is actually just for tax purposes. If you sell your likeness, which everybody here will do.
05:33:08.300 - 05:34:04.646, Speaker A: I don't know if anyone's watched the movie her with Yakwin Phoenix. There's a scene in it when he's talking to the machine, which is played by Scarlett Johansson, and she's having kind of like 5000 simultaneous conversations. The fact that we are constrained as people to one Zoom meeting, I think is pretty bad. Number one, because I dislike Zoom meetings intensely. But number two, because you could do a week's worth of Zoom meetings if you just licensed out your likeness and it did all these things, however, and then you yourself could be in the real you could be inserting in Zoom meetings. But when you think about if you're, say, an actor or something and you make your likeness available to a studio, how can you prove that you were in the model? How can you prove that it's in your training data? How can you prove that actually they produced it with you? And having the receipts for that on chain, even if they're shielded in some ways very useful? That's like a boring example tax. But it has way more kind of like there's way more philosophical reasons.
05:34:04.646 - 05:34:42.226, Speaker A: I think it's a very general, cool generalization of that, which is that chains are really auditable. So if you want it seen and understood by everyone, then it's a good place to put it. I spend a lot of time thinking about why are we in crypto? Earnestly, right? There's a lot about crypto that is not great. A lot of grifters, a lot of fraudsters, a lot of ups and downs. Emotionally, it's just like taxing. My parents are not proud of me because they don't know what I'm doing. I think a lot of us in this room can just walk away and go into AI.
05:34:42.226 - 05:35:37.258, Speaker A: There's, like, more money there right now, right? More excitement, more intention. And for me, it comes down to something very simple, which is I like it. Like I like these crypto values around decentralization building, really robust secure systems and networks, distributing ownership in a way that my backyard, Silicon Valley, has been so bad at, despite all the prosperity that has been built over the past however many years. I like those values and it's built what I think is this very exciting, dynamic industry that we get to play in. And then when it comes to AI, here is a technology which is diametrically opposite in personality, while cryptography, which underpins the crypto industry, is generally speaking, very humble, very discreet, very specific. This is a statement that I am making and no further, right? AI is so expressive, right? It's infinitely expressive and it's so centralizing. It's taking all these resources and gathering it.
05:35:37.258 - 05:36:35.610, Speaker A: And so when it comes to marrying the two in a way that I think acknowledges what both are good at and also for me has this mission driven dimension to it where it's like, maybe these values can travel beyond crypto. I hope crypto is huge in the future, but maybe we can bring these values around, don't trust verify out to the rest of the world, right? It's like a really exciting intersection, right, to marry these really appropriately, but then also leverage as a vehicle to bring these values elsewhere, right? We often talk about you get a credit score, you get an AI process, whatever, and you see like a Twitter Verified Checkmark next to that score, at least back when Twitter Verified Checkmarks meant anything. And you can click on that and interface with the on chain Verifier contract, see the model card, understand what that actually means for you as the end consumer, and then potentially bringing that to the rest of the world, right? I mean, maybe the rest of the world doesn't care, that's fine. But I'm very excited to try nonetheless.
05:36:35.950 - 05:36:38.940, Speaker B: Thank you. This is really insightful. We are at time.
05:36:40.750 - 05:37:26.332, Speaker A: Thanks everyone. Thank it's changed. Thank you. Thank you, sir. Thank you. Okay, now I know how to operate this, so thank you. Tough crowd.
05:37:26.332 - 05:38:00.140, Speaker A: Tough crowd. I know you. So my name is ishai now the co founder of Dimension. And this talk is going to be about EIBC and IBC in a new dimension. So EIBC is a paper that I wrote with the team about optimistic roll ups in an IBC setting. But let's talk a little bit about context. What is Dimension and how does it connect to IBC? So Dimension is a network of modular blockchains called rollapse.
05:38:00.140 - 05:38:50.136, Speaker A: We're in the modular summit. So it's worthy to say that these modular blockchains, they post data to a DA layer like Celestia or like Avail or any other DA modules or DA capabilities that we're going to have in the future. And these roll apps are modular by execution as well, so they can choose our execution environment. And they settle to the Dimension Hub, which acts kind of as a router to this Internet of roll ups, to this network of modular blockchains that is called Dimension. So the Dimension Hub is a settlement layer with an enshrined support for our roller framework, which is called the RDK. This is a unique architecture. I think seeing a lot of these lectures here and a lot of talks in the modular summit.
05:38:50.136 - 05:39:32.004, Speaker A: I noticed that Dimension has a very unique architecture. As one of or I would say probably the only settlement layer that focuses on modular settlement and validating bridge for rollups. So that's interesting. And it also enshrines the roll up framework that we've built. I think the difference between Dimension to Ethereum, for that matter, in terms of settlement, is that Dimension doesn't use smart contracts for roll ups. It actually uses the chain itself to support these roll ups and to gain more decentralization for sequencers from day one. So this is kind of why we built Dimension.
05:39:32.004 - 05:40:15.364, Speaker A: But let's talk about EIBC and what it is. So EIBC is an IBC based protocol for supporting optimistic rollups. What does this mean and why do we need this protocol? Why can't we use just IBC? This is a question that comes to mind. Why can't we use vanilla IBC for rollapse? Just reminding you all that mentioned, rollaps are optimistic rollaps. So this question comes to mind, and the answer is we can actually use IBC. We can use IBC and we use IBC. So from the Dimension hub to the rollups themselves, we use regular IBC.
05:40:15.364 - 05:40:55.270, Speaker A: It is not an issue. But from the roll apps themselves back to the Dimension hub, we can use IBC. But we have a setback. We have a setback that is common to every optimistic rollup because of a characteristic that optimistic rollups have, which is the fraud proof dispute period. So optimistic roll ups are the most popular scaling solution. I guess most of you know that on Ethereum, in terms of TVL, in terms of popularity, you see arbitram, you see optimism, you see all these roll ups. They're very, very popular, but they have a few issues.
05:40:55.270 - 05:41:58.250, Speaker A: One of them, as I mentioned, is the dispute period. So the dispute period basically prevents us from trusting the roll up immediately. We need to allow other participants in the network to submit a fraud proof. So it's a one event assumption that somebody's listening or somebody's capable of submitting a fraud proof to the settlement layer. But there's another setback that's not that known or not that common of people to talk about or to think about, is the Verifier's dilemma. The Verifier's dilemma is the situation where a sequencer is honest and is being honest for quite a while. Let's say that and other full node operators don't have any incentive to run a full node for roll app contrary to an L1 or a Cosmos chain where they can validate and earn rewards, earn fees, roll up full node does not have any incentive to continue running this full node when he knows that the sequencer is honest.
05:41:58.250 - 05:43:09.200, Speaker A: He's assuming that this might be a possibility for him to catch fraud and maybe make money. But the more we go in time, the more we understand, okay, this sequencer is honest, and most of the Verifiers would just stop operating these nodes. These nodes cost money so they just eventually stop and then the full node or the sequencer, by that time of point of time, they can be tempted to start cheating or doing any malicious state transitions. So this is the Verifier's dilemma. We have to make sure that in an Internet of roll ups, like we envision in dimension, where we say cosmos is just the tip of the iceberg, we can build an Internet of roll ups which are connected to the Cosmos ecosystem via IBC and via the dimension Hub. We must figure out a way to understand who will verify these roll ups and to understand how we can trust these roll ups or these sequencers if they don't have a lot of full nodes out there. So these are two big problems if you want to address an Internet of rollups that is based on the optimistic approach.
05:43:09.200 - 05:43:57.740, Speaker A: So we have a solution, and this is what I want to talk about today. The solution is called EIBC. E is for escrow. So EIBC is an IBC wrapper on the settlement layer that basically transforms the settlement layer into a clearinghouse of IBC withdrawals that are tradable or that you can use to clear them before the dispute period. So we want to address the dispute period. We want people to get an instant transfer experience and not damage the UX for everybody because of the dispute period we have for optimistic roll ups. So this is a short kind of brief about EIBC.
05:43:57.740 - 05:44:40.012, Speaker A: Let's understand how it works in detail. So for understanding that, we need to address the assumptions. So the first assumption is that users are willing to pay an additional fee for instant withdrawals. This is obvious. If you want to get out of a roll up, if you want to get out of an exchange, you would pay a little bit more to get out of that roll up or that exchange or whatever it is, you would pay a fee. The second assumption is that verifying a roll up eliminates the rollback risk for the Verifier. So assuming I am suspecting something or whatever it is, I'm running a full note of the roll up.
05:44:40.012 - 05:45:12.596, Speaker A: I have now verified the state. I eliminated the risk of rollback. The third assumption is that Verifiers who encounter fraud will submit a fraud proof. I think these are pretty feasible assumptions that we can take, and it will help us move forward with the flow of EIBC and how it works. So let's talk about the flow. So, Alice is a user on roll up X. She sends an EIBC withdrawal requesting ten dime out of the roll up.
05:45:12.596 - 05:45:51.412, Speaker A: So Alice says, I want to withdraw ten dime, and I'm willing to pay one dime fee. Okay, this is an example. It's not the implementation, but just an example. So Alice wants to withdraw ten dime. She sends it as a regular transaction to the roll app with the fee with the fee requirements. Later on the EIBC message is relayed to the settlement layer Escrow module, which queues it, and this is an important detail here. The EIBC message is relayed to the settlement layer, to the Escrow module and not to Alice itself.
05:45:51.412 - 05:46:30.464, Speaker A: So Alice is not getting the EIBC withdrawal. Remember, we have to wait for finalization. There is a fraud proof dispute period. So we need to make sure that we only accept the real IBC message after the fraud dispute period. Super important. So we need to figure out how we can counterpass this hurdle. Okay, now, Bob, which is a liquidity provider, and remember, Bob, because he's also a verifier remember him, bob is liquidity provider who decides to fulfill the EIBC message, the IBC transfer.
05:46:30.464 - 05:47:12.540, Speaker A: He's willing to purchase the delayed IBC message that Alice has already sent. Remember, the settlement layer escrows these IBC messages, and he can actually auction them. So Bob is capable of purchasing a future withdrawal and paying nine dime. Right. Now, remember, Alice wanted to pay a fee for getting out of the roll up immediately. So she's willing to get nine dime for a ten dime in the future, which Bob is willing to wait for. So Bob actually sends the nine dime to the settlement layer Escrow module.
05:47:12.540 - 05:47:40.324, Speaker A: The EIBC module on the settlement layer delivers Bob's tokens. Right. Bob sent nine tokens and now delivers the nine tokens to Alice immediately. Right. So Alice wasn't capable or she didn't want to wait. She got her nine dime immediately. And the original EIBC message of Alice, the ten dime, is now waiting for finalization, but it's rerouted for Bob's address.
05:47:40.324 - 05:48:44.516, Speaker A: So what we did here, we made the settlement layer a clearinghouse for roll up withdrawals. So we took one need of Alice is I want to get out of the roll up immediately. I'm willing to pay the settlement layer escrows the message. It knows it can't accept it because of the optimistic assumption, and Bob is willing to take that additional risk and additional waiting time for that fee. Okay, but what happens if there's no one who wants to fulfill the EIBC transfer, the IBC liquidity transfer? In this scenario, the message would just wait transfer. The EIBC message would just wait as usual and would finalize and would be transferred to Alice according to the optimistic assumption that we'll wait for the dispute period. So this EIBC protocol is fully compatible with IBC.
05:48:44.516 - 05:49:39.096, Speaker A: It just uses a wrapper on the settlement layer to create a market for the future withdrawals. It's a market, and this is what it is. And this is, I think, what makes it super exciting, because it elegantly provides economic and crypto economic assumptions or game theory to create a better system. So let's look at the result. The result is Alice received her tokens immediately without waiting the dispute period. Alice is now capable of transferring VIBC, of course, her funds or tokens, whatever she wants, to any other roll up in the ecosystem. That's pretty amazing, because once you withdraw out of roll up X or whatever roll up you are, you can easily deliver the tokens or transfer the tokens to any other roll up that's connected to the ecosystem.
05:49:39.096 - 05:50:40.128, Speaker A: So by solving what we did in the first leg in the first flow, how do we get out of the roll up? We actually get a roll up to roll up system that works fluently. So Alice also didn't need to trust any centralized actor for Bridging. This is super important because right now, everybody's using Arbitral optimism, whatever roll up on ethereum or whatever roll up that is optimistic, they're using it with the liquidity provider that they trust. So we leverage the IBC mechanism, and we leverage the trust in the validator set of the dimension hub to create a trust minimized environment for people to use and basically to leverage it. The other result from Bob's side is Bob made money. I think it's super important to understand that we want to allow, or we want to enable as much ecosystem participants that will make money. This creates a community.
05:50:40.128 - 05:51:47.760, Speaker A: It creates an economy, it rewards the players that are in the ecosystem, and it's good. But Bob, apart from making money, has a crucial incentive right now to verify the roll up. And I think this is what makes the ABC super special and super important and elegant, is that we kill two birds with 1 st, and we allow the ecosystem to verify itself. So according to usage, according to if a roll up is popular, you would know that it's being verified because somebody wants to get out and somebody's willing to give liquidity. So you would know that this roll up is safe because it's being verified. So we're using crypto economics. We're using game theory instead of computation to create this incentive mechanism to verify thousands of or tens of thousands of roll ups with crypto economics, and by that, solving the Verifier's dilemma.
05:51:47.760 - 05:52:27.696, Speaker A: So the Verifier dilemma is solved. The fraud proof dispute period is mostly solved if there's interest in the roll up. Right? So if somebody is in the roll up, if somebody wants to take the risk of running it, it's also solved. So it's also kind of an on demand situation, and we get a better ecosystem and a better experience for our users creating this Internet of roll ups with immediate withdrawals and usage of the original IBC mechanism. I guess we're getting to the end of this presentation. Thank you. Maybe time for Q A.
05:52:27.696 - 05:52:51.720, Speaker A: Don't be too rough. I know you're going to have hard questions, especially you in the back there. Yeah. Any q a Anybody wants to ask something? I assume not. Okay, thank you. Hello.
05:52:51.870 - 05:53:08.400, Speaker B: All right. Thank you so much. I'm Chango. I'm your MC. For this next segment or for the rest of the Cosmos track, we're going to welcome our next speaker, Jack Zamplin, to talk about modular IBC patterns. Welcome on stage.
05:53:18.440 - 05:53:29.584, Speaker A: Hello. Hi. Hard to compete with vitalik. Okay, so this is forward. That's backwards. Okay. Modular IBC patterns.
05:53:29.584 - 05:53:40.276, Speaker A: Since we're here at the modular summit, everything's modular. Just a quick introduction. Hi. My name is Jack Samplin. I'm the co founder and CEO of Strangelove. You might know me from the cosmos. Hub the cosmos.
05:53:40.276 - 05:54:16.772, Speaker A: SDK IBC akash sommelier and or Noble. I've been working deeply within the Cosmos ecosystem for many years. Strangelove is dedicated to building IBC and supporting its growth. And that's what I've spent most of the last few years doing, and that's what my company does. So just a quick refresher on IBC for folks. I think this particular audience probably doesn't need it, but I think the broader modular audience, there's a lot of people unfamiliar with IBC, especially the core underlying primitives. So I do think it's important to kind of dig through these.
05:54:16.772 - 05:54:49.452, Speaker A: So IBC is a general message passing framework for blockchains. It's modular by design, and there's some key primitives that are really important to think about. The first one is clients. So clients contain all the authentication logic. Which chain am I talking to? What are its validators? How do I know that the state transitions it's providing are valid connections? And channels make up the middle layer, which is the transport layer. And this is the sort of like, how do you connect one smart contract to another? And it's the on chain pieces that help define that. And then there's packets.
05:54:49.452 - 05:55:42.236, Speaker A: So from a developer's perspective, that middle layer, which we call the Tau transport, ordering and authentication, you don't really need to care about. It's kind of the key part of the protocol, and it helps you build everything on top. As a developer, if you're a chain developer, you might want to care about clients, because maybe you have some unique consensus or you're an optimistic roll up, or you're a ZK roll up, and you've got some different cryptographic properties that your chain needs to authenticate it. And you would encode that into a client, and that runs on all of the chains that you want to connect to. And then on top of all of this infrastructure, we build the packets. And this is the application data formats that you send data back and forth with. So roll ups, we've got ZK roll ups, we've got optimistic roll ups and some other stuff based roll ups, modular pilled roll ups.
05:55:42.236 - 05:56:09.620, Speaker A: I don't know. People are coming up with all kinds of stuff, but I think for practical purposes, there's really two. We've got ZK roll ups, which are kind of the future, and optimistic Roll ups, which are the today technology. And I think that bringing IBC to each of those. There's some unique challenges to each of these different architectures. So I'm going to dig into optimistic rollups a little bit. The Dimension team was just up here, and they showed you guys the right way to do optimistic IBC.
05:56:09.620 - 05:56:53.352, Speaker A: And I think that the escrow thing that you guys presented was really cool. And being able to optimistically accept that is one way to get around the dispute period. This is the hard way, this is not the easy way, and the dispute period does lead to some bad UX. But you guys had a really elegant way of getting around that. This is similar to what Uma protocol is doing on Ethereum, and it is definitely one way forward that's kind of tough UX for users in a lot of ways and I think it can end up being hard to reason about. Then there's the easier way. And basically this comes down to a committee, whether it's a multisig, a validator set, or some sort of combination of those.
05:56:53.352 - 05:57:41.824, Speaker A: There's a committee of keys that provides that authentication logic and helps validate the data coming into your chain. This tends to lead to much better UX and is a lot easier for users to reason about, even though it introduces additional trust assumptions. So the right and the hard way, I've talked to a lot of folks about it. It's been hard to get people behind building that out and building on top of that for a variety of reasons. So I've been thinking a lot about committees and I came up with kind of a fun little committee design that seeing if anyone's interested in. So let me talk about this stamper idea. You'd have a tendermint chain where essentially validators attest to the optimistic roll up header.
05:57:41.824 - 05:58:48.860, Speaker A: So what does this mean? Each of the validators would be running a full node for the optimistic roll up and then submitting the app hashes to the chain via vote extensions or some other mechanism. Once you reach consensus on those headers, you can write them to state and then clients would be able to detrust those. And the onchain lite client of this is basically a tendermint like client, which is a huge benefit. But instead of checking the stamper chain's state for packet inclusion, you go check the state of the roll up for packet inclusion. So one of the benefits of this design is the minimal change to the set of underlying primitives and libraries that are maintained by the core teams that would enable us to offer the developer experience that we want, which is that the roll up connects directly to any other chain in IBC and is able to bridge seamlessly that way. And from a developer perspective, you just send a packet directly from your roll up to any chain in IBC that connects to that network. And this is a cool design.
05:58:48.860 - 05:59:29.284, Speaker A: But I think the biggest problem with this is it's a committee like any other. It's kind of fundamentally the same trust assumption as Hyperlane or Axilar or Wormhole or any of these other committee based interoperability solutions. So I probably not going to build this. Maybe somebody does. If you're interested, please hit me up. But what I'm more excited about, I think, is ZK rollups, where the hard way and the easy way is the same. You use a Celestia light node to check for data availability, and then you add the ZK proof to the sovereign rollup, and suddenly you're able to connect directly from that ZK roll up to anywhere else in the IBC network.
05:59:29.284 - 06:00:26.708, Speaker A: This is what users want bring in. It's all chains and bridges at the end of the day. And this kind of eliminates this idea of the settlement layer in a lot of ways, which is I think is why it's tough from a mental model for a lot of folks. So on that side of things, we're going to be working with duality labs, sovereign labs, and informal systems to help bring the first IBC ZK lite client to market and try to drive some value directly to the hub. So from an architecture perspective, what does this look like? We've got Celestia, where the ZK rollup is writing all of its state to for the data availability layer. And then duality, which is an ICS chain, would be running this light client for the ZK sovereign roll up and helping enable communication with the rest of the IBC ecosystem. That light client could run on any other chain, but I think initially we'll probably just run it on duality.
06:00:26.708 - 06:01:02.824, Speaker A: And I think that this project is going to be a good blueprint for future ZK roll up integrations because there's a lot of different ZK technologies out there. We're not going to have to do this once. We're going to have to do this like ten to 15 times as these cryptographic standards change, as the proofs get faster and faster and help improve in performance. So yeah, I guess that's kind of what I had prepared. It's just kind of a brief overview, but happy to answer any questions that folks have. Yes, sir. Can you elaborate on the temporary? Yeah, for sure.
06:01:02.824 - 06:01:57.240, Speaker A: So which part should I break down? A little bit more chain? Yes. Where does the tendermint chain come into play in this sense? Yeah, for sure. So the roll up would incentivize the tendermint chain in some way. I think the easiest way is to have it the roll up operator would deposit IBC into this would deposit USDC into this tendermint chain and say, hey, validators, I want you to finalize my roll up headers. Basically, the validators would then spin up full nodes for each of the roll ups. And because they're listening directly to the state transitions on the network, on that p to p network, they're going to see the latest block headers as they come in. Each validator will have an independent view of that state.
06:01:57.240 - 06:02:35.568, Speaker A: And then every time they see a new header, they write it to the tendermint chain. And then you're checking every block. Once two thirds of validators have seen the same header, we commit that header into state. You can then prove that header using the light client on a counterparty chain so that you can prove that header is included in the stamper state and then you can check it for packet inclusion on the roll up side. Does that make sense? Yeah so basically it's really cutting short the optimistic time. Yeah, exactly. So there is Ek which is directly this and this is like sort of invalidating you, so no need to worry.
06:02:35.568 - 06:03:21.012, Speaker A: Yes and I think the key idea here is one thing that I don't like about committees and I think a lot of other folks don't like about committees is you end up having these multi hop transactions and it kind of forces this really suboptimal developer user experience and adds a lot of latency in many cases. And with this committee the latency issue is still potentially there. But I think that there's a lot of ways to shave that down. And also, from an app developer's perspective, it offers that point to point IBC experience that most developers are familiar with. And I think practically roll ups are going to want to connect to five to ten key chains to help bring in liquidity and provide other markets and potentially provide services via IBC. And having to do that over multi hop. It's a complicated developer user experience.
06:03:21.012 - 06:03:52.760, Speaker A: So that was kind of the motivation behind the design. Cool. I think just one thing maybe I would say my only concern with this is that there is some bootstrapping cost in terms of making validators. Actually do it for your roll up. Because initially way back then like one and a half years ago, we also had this notion in mind, but we really wanted to make it super easy. And to Bootstrap basically a roll up. Yeah, I think this is the complex part and the devils and the detail of all these designs.
06:03:52.760 - 06:04:46.830, Speaker A: I think for that there's a lot of restaking designs that you could potentially do where you're locking up tokens with the validator and they're getting paid in yield, most of those are in kind, it's not really compelling for validators. I think in order to make this system work you'd have to literally just pay the validators money in order to cover it. One of the cool and fun aspects of that design is that the stamper chain could easily degrade service for the roll ups. In the event that user doesn't pay, they just start maybe finalizing only one every ten headers and then if payment doesn't continue, maybe one every hundred. And then finally dropping service after a predefined period of time. And you can easily offer different service tiers for different roll ups and different payment methods. And I think that there's a lot of room and flexibility to innovate on that side of things.
06:04:46.830 - 06:05:03.590, Speaker A: But yeah, it's like again the market needs to be there a little bit in order for this solution to really be viable. Cool thank you. Yeah, absolutely. Anyone else have any questions? Susanna, I saw you had one. Did I answer it already?
06:05:06.600 - 06:05:19.396, Speaker B: Thank you. With the stamper idea, this would be, like, linked to the conditional clients idea that you were wanting so that you link the two clients.
06:05:19.508 - 06:05:53.990, Speaker A: I think conditional clients is like a nice to have. It's not necessarily a must have. We've got that WASM client in Go now that accepts sort of like arbitrary WASM code. And I think this is going to go a long way to helping users only have to write their client once in Rust and then kind of deploy everywhere. So that's one key piece of this. But one of the things we did when building that WASM client is we took the tendermint lite client from Ibcrs and wrapped it in a smart contract. And a lot of the testing we did was against that tendermint client to ensure that it has the same properties as the tendermint client in Go.
06:05:53.990 - 06:06:29.360, Speaker A: And we could easily fork that contract. And then in the functions where we're checking for packet inclusion within IBC, you just substitute the roll up header instead of the stamper header. So it's a really light fork of that client. It would be an extremely minimal change set. Conditional clients would make this nice and easy and be a lot more ergonomic and space saving. But in order to ship it sooner, you could easily do it the other way. But I think that this is one of many use cases for conditional like clients, but it's not a dependency.
06:06:29.360 - 06:06:41.010, Speaker A: Any other questions for folks? Awesome. That sounds great. Thank you guys very much for your time. Really appreciate it today. You guys have a great one.
06:06:44.740 - 06:06:51.240, Speaker B: All right. We ended up being early, so we're going to take a little break. Until the next one. Next one's at 420.
06:06:51.430 - 06:12:43.030, Speaker A: That's Sam. Sam. Sam. Sam. Sam. It's Sam. Sam.
06:12:43.030 - 06:16:59.090, Speaker A: Sam. Sam. Sam SA. Sam. Sam. Sam SA. Sam WAM.
06:17:09.070 - 06:17:25.470, Speaker B: Okay. And we are back. We have David C from Babylon here. Next, he's going to talk about Bitcoin staking unlocking 21 million bitcoins to secure the decentralized economy. Welcome, David.
06:17:26.530 - 06:17:27.114, Speaker A: Thank you.
06:17:27.172 - 06:17:29.534, Speaker B: Going to give a Stanford lecture.
06:17:29.662 - 06:17:56.422, Speaker A: Thank you very much. Always good to be introduced by an astronaut. Gives me some special status. You thank you. Great. All right, so today I'm going to talk about this new protocol that we just designed in the past few months and currently building right now. Bitcoin sticking.
06:17:56.422 - 06:18:40.042, Speaker A: All right, so let's first understand what's the problem we're trying to address here. Okay? The problem we're trying to address is the proof of stake security. By definition, proof of stake means that the security is staked protected by capital, and therefore capital intensive. So right now, many of these chains are paying pretty high reward for staking ranging from 5% on ethereum to 20 40% on a Cosmos chain. So this is very expensive. And right now, this cost is paid by basically inflation of the coin. So inflation is not good.
06:18:40.042 - 06:19:21.366, Speaker A: You don't want to inflate the coin until it's necessary. There's also a pretty big competition between the utility of the coin and the security. And so we would like to reduce this tension and reduce the cost of capital in general. Okay? So that's the problem trying to solve. And the way we try to solve this problem is to recognize that outside this proof of stake world, there is another world in the crypto land, which is basically bitcoin. So proof of stake has slightly less than 50% of the market cap. Bitcoin has slightly more than 50% of the market cap.
06:19:21.366 - 06:20:09.106, Speaker A: So the observation here is that bitcoin can be a source of capital for sticking. So let's think about why it is a good source of capital for staking. Okay? Everyone has some bitcoins, all right? But most of them has next to zero yield. So most of bitcoin is actually idle. So idle bitcoin means that these bitcoin, you are not competing, like, for example, ethereum for yield, for DeFi, for example. So providing a staking option for them could be attractive. All right? Number two is that the bitcoin holders in general are quite decentralized, at least more decentralized compared to many proof of stake chains.
06:20:09.106 - 06:20:53.490, Speaker A: So by providing bitcoin as a staking asset, that would also help to decentralize your network. Three, bitcoin relatively is the least volatile asset within the crypto market. And we've done some study on that. The volatility of bitcoin is less than half of many assets. So if you combine the three together, you can think of bitcoin as actually a pretty excellent staking asset. Staking asset. So our idea was to use this bitcoin asset, 600 billion worth of staking asset, potentially to secure all the proof of stick chains in the world.
06:20:53.490 - 06:21:44.898, Speaker A: Okay? All right, let's think about this a little bit more, right? Let's step back and think about this. So bitcoin from bitcoin, you provide security to proof of stake chain. In return, bitcoin holders can get yield from these chains. Now, these chains are already providing yield, but they're providing very high yield to attract native token, to have a large supply of bitcoin and a relatively stable market value is a good source of capital for these chains. So that is the assumption. That is the problem we're trying to solve. Okay? All right, so why is this not a trivial problem? Okay, so so far this is the setup of the problem.
06:21:44.898 - 06:22:25.246, Speaker A: Our goal is to use bitcoin to secure proof of stake chain. So what do we call this concept? So we are in some sense inventing a new concept here and this new concept we called bitcoin staking. Okay? So if you look at this concept, it's kind of a weird concept, weird name, right? Because bitcoin everyone associate with proof of work. Staking, of course, is a natural key concept in proof of stake. How can. We put these two words together, it doesn't make sense, right? But actually it does make sense because bitcoin, although is a proof of work chain, it is still an asset. It is still an asset.
06:22:25.246 - 06:22:53.050, Speaker A: It's still a crypto asset. And we're saying we're using this very secure crypto asset secured by proof of work to secure proof of sticking or to add security to proof of sticking. So bitcoin sticking. Okay, so anyone have any question about this concept? All right. Okay. Man, I do sound like a professor or something like that. It's like habit.
06:22:53.050 - 06:23:04.874, Speaker A: It's hard to break habit. Oh, there's a question. All right. Thank you very much. Thank you very much. Okay. Using a bitcoin to secure a network compared to using any USDT.
06:23:04.874 - 06:23:24.002, Speaker A: USDT, if it's not related to native staking at all, it's just a money bond for you to not make. Yeah, yeah. But of course it's a collateral. Right? Yeah. I think what I'm trying to say is yes, maybe someone else can come up with that idea. What I'm trying to say but our idea is focused on bitcoin. Yeah, that's fine.
06:23:24.002 - 06:23:47.450, Speaker A: I'm just saying it's a POW coin, right? So if you use that to provide security to a POS chain, then it's the same as any other asset to secure those POS chain. It's a crypto asset, though. Yeah, but it's also an asset in which a lot of people wants to hold. That's true. Right. So what we're saying is this. One way of thinking about this is this.
06:23:47.450 - 06:24:17.366, Speaker A: Suppose you are a person, and your choice is the following, okay? You want to buy some altcoin and earn altcoin yield. Or this is now giving you an option. You can buy bitcoin and get out coin yield. Okay, so that's a pretty, I think, interesting concept, right? Because then you can reduce your volatility of your principal, but still get some upside on the yield. Okay, so that's a setup. Good. Thank you very much.
06:24:17.366 - 06:24:31.980, Speaker A: Yeah. Any more questions? Okay, good. Actually become the UCC's permission. So I don't think it may work well. But you're right. Good. Thank you.
06:24:31.980 - 06:24:57.890, Speaker A: See, the trick of a good professor is to get the student answer each other questions. That's the challenge. Yeah. Okay. Thank you. All right, so now we shift a little bit more to the technology, right? So this is the problem we're trying to solve. So why is this problem challenging? Well, it is basically a two sided market, right? We have bitcoin holders on the left hand side and proof of stake chains on the right hand side.
06:24:57.890 - 06:25:34.410, Speaker A: So on the left hand side, we want to make sure that this is a very secure experience for them, because bitcoin holders are very conservative. They don't want to lose the bitcoin. So this protocol should protect the BTC stakers holders very strongly. On the other hand, for the proof of stake chain, they want to get security from the bitcoin. In particular, they want it to be a true security so that in case there's anything bad happens, then they can slash the bitcoin stickers. Okay? So the slashing therefore has to be very sharp. It is only under bad behavior of the validator will that slashing occur and not under any other circumstances.
06:25:34.410 - 06:26:34.778, Speaker A: Okay? All right, so this is the problem that we solved. So we came up with a protocol, a bitcoin sticking protocol, which essentially solves this dual objective of protecting both sides of the market, all right? Now, the biggest challenge actually to achieve this is the fact that bitcoin doesn't have a smart contract, okay? So I'll come back to this point a little bit later, but the constraint is that there are no smart contract bitcoin, okay? Now, what we can achieve is full security. So full slashing. So in other words, validators do bad things. We can slash the bitcoin. And number two is we can do trustless staking, which means that there is no bridging, no bridging of the bitcoin from the bitcoin chain to the proof of stake chain through custodial or otherwise. Okay? Through a centralized custodial or otherwise.
06:26:34.778 - 06:27:08.282, Speaker A: Okay? So let's think about these two objectives at these two properties that we achieve. The first one is to provide full security to bitcoin chain, right? That's the right side of the market. The second one is to provide full protection to the bitcoin holders. That's the second one. Now, if you relax one of them for example, if you bridge the bitcoin from the bitcoin chain to the proof of stake chain through some wrap asset like wrap BTC, then it's quite easy to achieve full security because now your token is as though it's sitting on the proof of stake chain. Your smart contract proof of stake chain. You can do all the slashing you want.
06:27:08.282 - 06:27:47.506, Speaker A: Okay? Now, however, we don't want to bridge, so the bitcoin stays on the bitcoin chain, but we don't have a smart contract on bitcoin chain to do the slashing. So thereby that's the challenge of the problem. And in fact, that is kind of coming to the heart of why so many bitcoins are idle. So many bitcoins are idle precisely because there is no smart contract, no intelligence on the smart chain. So it's very hard to securely bridge the money away and get it back. Okay? All right, so what we're saying is that for this particular use case of providing security, very important use case, I think there's no bridging needed. Okay, so how do we do this? So let's run through experience from a staker.
06:27:47.506 - 06:28:20.722, Speaker A: So Alice has one bitcoin. What Alice do is assigns a contract, a staking contract, a staking contract on the bitcoin chain. Bitcoin doesn't have smart contract, but it does have a scripting language which allows you to write some very simple contract. So in this simple contract, there are two possible ways of spending this bitcoin. Either Alice can withdraw this bitcoin and claim back the bitcoin in, say, three days. Okay? This is standard, unbonding with some delay. Okay? Second, Alice will burn the BTC into a burn address.
06:28:20.722 - 06:28:36.646, Speaker A: Okay? So these are the only two ways of spending this bitcoin, both of which are controlled by Alice. Both of which are controlled by Alice. So this is self custodial contract. No third party. Yeah. Why would Alice use the second? Exactly. Why would Alice burn his own money? Right? Her own money.
06:28:36.646 - 06:29:13.470, Speaker A: That doesn't make sense at all. This option is needed, though, because we needed to have an option of slashing Alice. This is where the second option is. Why would Alice call the slashing? Yeah, so wait 1 minute. All right, good question, though. All right, so Alice, after signing this contract, can now validate a proof of stake chain. Okay? So Alice, to tie the two system together, alice will use the same key that control the address that controls the BTC address to sign blocks, to sign blocks on the proof of stake chain.
06:29:13.470 - 06:29:48.954, Speaker A: Okay, I'll tell you a little bit more about how the signing is done, but right now, we're using the same key. In the happy pass, alice will be behaving honestly honestly and will continue validating blocks on the proof of stake chain. And the money will stay on the bitcoin chain until Alice wants to resign or wants to withdraw the money. And after a few days, alice will claim back the stake and obtain the reward. Good. This is a happy path. Now, in the unhappy path, in the unhappy path, alice will turn evil and now tries to do something bad on the proof of stake chain.
06:29:48.954 - 06:30:38.160, Speaker A: The worst thing you can do proof of stake chain is to create a fork on the proof of stake chain. Okay? Now, this is a slashable offense, creating a fork. In fact, it's one of the very few slashable offense to protect the safety of the chain. So what we can do using cryptography is that using a special modification of Alice signature, what we can do is to force Alice key, alice private key, to be leak, to be leaked. This enables anybody to send a transaction back to the proof of stake chain to Alice, okay? To slash Alice. Now, how do I go back? Do I go back here? I don't know how to go back. Maybe here? I don't know.
06:30:38.160 - 06:31:07.382, Speaker A: Okay, go back. All right. Okay, so now anybody can activate can pretend to be Alice and activate the second path. So this answers the question of why Alice will ever burn the BC, because it has done something bad. And that something bad will force the review of the private key, which will automatically enable anybody to slash this money. Okay, so this allows slashing to happen. Very good.
06:31:07.382 - 06:31:36.850, Speaker A: So that's basically the key design. We have some extra additional complexities protocol. For example, we would like to allow a fast unbonding of the stake BDC, to ensure that the bitcoin holders can maintain high liquidity. This is achieved by providing some kind of synchronization between the bitcoin chain and a proof of stake chain. Like cosmos, like osmosis. In this example. So we need some cross chain synchronization.
06:31:36.850 - 06:32:28.770, Speaker A: And we already have a timestamping protocol. If you have heard my talk at Cosmoverse, for example, last year, we built a timestamping protocol which allows precisely the synchronization. So that enables us to do fast, unbonding, and more importantly, in the context of this summit, we're in a modular summit. Okay? So someone asked me what the hell your talk has to do with modularity? And then very quickly, I have to think about something. It turns out that actually our protocol is very modular. So our vision right in the first sight, is to be able to provide bitcoin security through staking, bitcoin staking to as many proof of stake chain in the world. Okay? So if our protocol has to adapt to change everybody's proof of stake protocol, forget it.
06:32:28.770 - 06:33:22.610, Speaker A: Nobody wants to change the protocol. So how we design it is we design using a modular tool called fanatic gadget. Okay? So fanatic gadget is a concept invented by Vitalik many years ago, six years ago, around which essentially is a way to add a modular addition to the proof of stake chain to any proof of stake chain to give you additional finality functioning. And in this case, the functionality we add is we can think of as BTC validated, fast BTC validated, finality. Okay? So because the revealing of the private key requires some special signature, that's why we have to add this finality gadget, because the original protocol would not be using the signature. In fact, it could be using another signature like ed 141519, something like that. 25519, yes.
06:33:22.610 - 06:34:07.074, Speaker A: Okay, all right, so this allows us to broaden the application to many different chains. Okay? So modularity, we use this concept fanatic gadget already mentioned, started in this paper, casper the friendly ghost friendly fanatic gadget, casper FFG, invented by Vitalik. And we had a paper two years ago which have a secure design of this fanatic gadget. So fanatic gadget is a concept invented by Vitalik, but there's some security issues with the design. We fixed that in this paper. Okay, so we apply similar concept here. All right, so here's the architecture of the whole system you can think of.
06:34:07.074 - 06:34:44.586, Speaker A: There is a data plane and there's a control plane. So in the data plane, the validators will be using these special signatures to provide BTC security to all these chains, whatever chains that want to accept it as a BTC stake. On the control plane, babylon will be running timestamping protocol, some kind of interface that allows the BTC to be synchronized with all these chains. Synchronized all these chains. Babylon itself will also be BTC secured. Okay? So that's the architecture that we have. All right? So we have built this architecture to some extent.
06:34:44.586 - 06:34:59.858, Speaker A: We already have a timestamping testnet. So now there are 31 chains. 31 Cosmos chains. Cosmos SDK chains sitting on our testnet synchronized with the bitcoin chain. Synchronized with the bitcoin chain. You can visit the website. Okay.
06:34:59.858 - 06:35:24.780, Speaker A: All right, so finally, I think this is a pretty interesting use case for bitcoin. Bitcoin can't really leave the bitcoin chain securely, but it can be used to secure proof of stake chain securely. So security means two things here. One, security to the proof of stake chain. Two, security to the staker as well. And through this, we're hoping to provide some integration to the two economies. Thank you.
06:35:24.780 - 06:36:36.362, Speaker A: Next steps. Right now, what are the steps to mainnet and what progress with these 31 chains? Yeah, this 31 chains is on the timestamping test net. The next step is we're going to evolve this testnet into a BTC staking test net and invite all these 31 chains and more chains to join to do the BTC staking. And then we want to involve the main net. Our target is to launch the main net before the next halfing. Okay, so can you give an example? What kind of decision would the governance of any of these chains we'll see, let's say, in a governance proposal to vote? If I'm a holder of any of the governance token on these chains, what would be example of? We propose to enable X and Y. Like, what will it say? So the proposal will be following the proposal would be okay, we allow a certain percentage of our sticking market cap to be BTC stake.
06:36:36.362 - 06:36:47.362, Speaker A: Okay. Think about it. It's like an investment portfolio. Right now. It's all stocks, high volatility. I said, hey, I don't want all stocks. I don't want high volatility.
06:36:47.362 - 06:37:17.054, Speaker A: I want some bonds. I want 30% bonds in my portfolio, 40% bonds. That is decided by the governance proposal. Thank you. Thank you. Well, then, another question. What are the main challenges that are yet to be resolved for this to go live on main net and actually be usable by some of these consumer chains? Yeah, the challenges will be a two sided market.
06:37:17.054 - 06:38:15.650, Speaker A: So the challenge will be to bring two sides onto the table. We have to get some BDC holders to stick and some chains to accept the sticking through the governor's proposal. And how would the other side, the consumer side, interact with the protocol? Let's say now we're talking about implementation. For example, if it's a cosmo chain, it will be tenement consensus. We would add this finality gadget on top of tenement, and this will be a software upgrade. So this will also be a software governance proposal change. And this finality gadget, what would it do? It would sign an extra so tenement blocks is finalized through tenement, and then after it's finalized, then we will sign an extra round of signatures to do a BTC finalization of the block.
06:38:15.650 - 06:38:49.498, Speaker A: And these signatures will be the special signatures for which, if you double sign, the private key will be leaked. And how will this then get to that point of a private key of Alice being leaked? Like, what will actually trigger the transaction? Where does the Alice's private key stored? Private key is not stored anywhere, but just from the signatures on the two blocks that you're double signing, we can extract anyone who observes these two signatures will be able to extract the privacy. I see.
06:38:49.664 - 06:38:54.270, Speaker B: Let's have that be the final question. You guys could do further q A.
06:38:54.420 - 06:38:54.894, Speaker A: Thank you.
06:38:54.932 - 06:38:58.238, Speaker B: Thank you back there. Thank you, David, for your excellent talk.
06:38:58.324 - 06:38:59.102, Speaker A: Thank you. Thank you.
06:38:59.156 - 06:39:02.800, Speaker B: Talk at Osmocon as well. Trying to break open.
06:39:04.630 - 06:39:09.646, Speaker A: Thank you. I didn't manage to introduce myself the previous time. My name is Kamen. We're investors.
06:39:09.758 - 06:39:34.218, Speaker B: Okay, panel Cyborg is our moderator. We're Cyborg, and we're going to invite our panelists. We'll just let people settle in first. By the way, Cyborg fought an incredible fight at the Osmocon boxing ring last night. That was kick ass. Thank you. Oh, man.
06:39:34.218 - 06:39:56.110, Speaker B: I know. You know, we should do that. All of the, like, too much drama to be resolved in Cosmos. Okay, so our moderator is Mr. Cyborg. And going from this way to that, Jack is missing. Okay, well, we've got Marco Barr, SDK core developer.
06:39:56.110 - 06:40:10.340, Speaker B: Marco Barr? Yes. Marco Barr. Sean. Sean Brethwit. Yes. Magnus Marinik from Skip Protocol and Jack Zamplin from Somalier and Strange Love.
06:40:11.290 - 06:40:28.330, Speaker A: Cool. So we have a small, intimate audience, so everyone will get a chance to ask questions. Looks like cosmoverse, right? Looks like a cosmoverse panel. We had model, but it looks like cosmoverse. Yeah. So cosmos infrastructure. I think I'm going to start with Marco here.
06:40:28.330 - 06:41:08.706, Speaker A: Maybe you can explain for the audience that is not familiar with how things work in Cosmos. What's the lay of the land? What are the different part of the stack for Cosmos and who are the builders behind each of these parts? Yeah, so at the core layer, there's three distinct protocols. So there's Comet BFT, currently maintained by informal systems. Above that comes the I like to call it the middleware and the state machine. That is a Cosmos SDK that is maintained by myself and binary builders. Then we have IBC, Go, and this is like, the third component of the core layer. I like to call it the kernel layer that is maintained by interchange gambah.
06:41:08.706 - 06:42:09.190, Speaker A: And then above that, we have a few application layers. So we have polaris ethermint, Cosmosm, some moviem stuff like that. But I'd say, like, the three core layers that are fundamental to the interchange stack would be Comet BFT, Ibcgo, and the Cosmos SDK. And Jack, I think you have a lot of thoughts about, like, you guys, especially Marco and Jack, you worked on something called the Cosmos Builder Foundation, right? And you have thoughts about the way that funding for all the different part of the stack should work in Cosmos. Maybe you can share a little bit of your thoughts on that. Yeah, I think right now the Interchain Foundation is doing most of the funding, and that's not a long term sustainable path. This is kind of a classic open source funding problem where there's a lot of economically viable businesses, essentially, that build on top of this core software, and we need to find a way to get those folks to help contribute to the maintenance of the core stack.
06:42:09.190 - 06:42:36.546, Speaker A: And I've been talking to a lot of folks for a long time, but that's kind of the fundamental friction point there is. There's folks out there who want to do it. There's folks out there who have the money to do it, but there's nowhere to put that right now that's sort of credibly neutral and sort of made up of a council of the core builders. And yeah. Still looking for folks to work with on that. Been chatting with Skip quite a bit about that. Yeah.
06:42:36.546 - 06:42:56.860, Speaker A: So Skip actually is kind of a new project. Right. Marco and Jack have been Cosmos for a while. Was there for you anything like surprising or obvious that Cosmos should do to kind of improve the current model that we have? The current model of funding. Of funding, yeah. And the way work is being done in general to build the stack. Yeah.
06:42:56.860 - 06:43:32.230, Speaker A: I don't want to be too critical, but you should be very critical. It's a very small audience. You can say whatever you want. I mean, we started out by basically learning Tendermint, and Tendermint was a very complicated code base for us to learn. And we quickly realized one of the things we realized quite quickly was how the stack is meant to be modular, meaning ABCI is sort of this abstraction that is outside of Tendermint. We realized that Tendermint and the Cosmos SDK are actually quite coupled in many different ways. And I think this was found out by other teams afterwards, like Polaris, et cetera.
06:43:32.230 - 06:44:26.758, Speaker A: And so I think one thing that could be done is making those different pieces of the stack a little bit more accessible. I think there's been really good work that's sort of happening behind the scenes here, but Tendermint basically becoming more comprehensible and easier to understand, more modular, more easy to add things to and take things out of. We started by having to modify Tendermint and build this thing called Mev Tenerment, which now we're moving sort of away from, but basically we try to build in basically an ordering mechanism into the Mempool, which didn't really exist. It was sort of this FIFO based system and it was very difficult. It took a long time, and I think it was tough to modify the core stack in that way. And we learned a lot through that and I think hopefully shared a couple of those things with core builders. Sean, I think you were part of the initial tournament team.
06:44:26.758 - 06:45:14.840, Speaker A: You took part in some of these decisions right, at informal. So while I was at informal, we spent a lot of work trying to position the software of Tendermint as a core consensus engine in a position where it could be like testable. I think it was very hard to make significant changes. I think that Tendermint, when it was conceived, was like heads and shoulders above any sort of alternative. And since then, I think the industry has had a lot of time to catch up while Tendermint innovation has really slowed. And I think we could attribute that to some of the complexity and the architecture there. Coming back to your original question about funding and what role does the capital flows have to do with this, which I think is sort of the more interesting question.
06:45:14.840 - 06:46:04.498, Speaker A: We have to look at where the money came from. And I think most of the money that funds most of the core infrastructure comes from a single entity, right, which did not intend to raise enough money for that when it raised it. So, like, Tenderman raised like 16 million that ballooned to like half a billion dollar treasury and then spent years reconfiguring itself to be capital efficient with that much money, which I think is a very hard thing to do. Still ongoing. Still ongoing. And I think fundamentally, it's like this is at the core of the broken model because there isn't really any sort of genuine economic surplus that you could sort of redirect to places that are more productive or take away from places that are unproductive. So, broadly speaking, most of the capital that is allocated is unproductive.
06:46:04.498 - 06:46:57.794, Speaker A: So that's like the base level, right, that you don't know. There's very few profitable protocols modulo the Cosmos hub. Most other protocols are very unprofitable. So it's very hard to say, like, oh, as an ecosystem, we're going to let those ecosystems die or let those chains die or redirect it where it needs to be. So I think what's really missing is a new way of doing surplus recycling and not just funding a new entity, a new centralized entity or a set of entities with VC money or whatever. But still, the ICF has like $300 million right at the moment on the books. If you were the president of the ACF or the managing director of the ACF, what would you change on day one? I would send it all to a burn address and then see if the ecosystem was meant to survive.
06:46:57.794 - 06:47:41.060, Speaker A: Or I would take the Cosmos hub and I would inflate it maybe like $1 billion into a fund that was governed by a multi SIG of people who could get easily unelected. It could be jack or whatever. I don't care the initial version. But what's important is everything else should go away. Any capital that is held underneath a body which is not elected in a transparent way should probably go away. It is kind of like in Cosmos and all these ecosystems, it's always been like, everyone we license the software, apache, we stand by Apache. Everyone can take the software, do whatever.
06:47:41.060 - 06:48:39.026, Speaker A: It's a pro and a con, because if you license it some other way, they're not going to come, users aren't going to come. And so it's like a double edged sword there. But the other part is, from the get go, no one ever really thought of, oh, what will be the funding story in five to ten years? And so no one really built that narrative of like, oh, we should build a narrative of when there's a launch of a chain, the team coordinates, assists, the team helps them launch, and then they get an allocation, or they get some sort of continuous allocation to help align roadmaps that was never thought. And now it's kind of like I mean, we're having this conversation not only in the cosmos ecosystem, in ethereum polka dot, literally everywhere that oh, raising into a single foundation off a single ICO is a good idea for the first five years. But if you want to last, like, a hundred years, you're not going to be here if that's the only plan. Yeah, because if you hear Composable Finance talking about parity and the way things work there, it doesn't seem to work much better than we work in Cosmos with the ACA. I think there's like an age old startup montage.
06:48:39.026 - 06:49:22.354, Speaker A: Raising too much money will kill your company just as surely as not having enough money. And I think that there's a lot of lessons that we've forgotten from the Web two world and kind of the classic startup world and blockchain, and I think that that's a really key one. So you, Jack, if you were president of the ICF, what do you do on day one? I like Sean's idea. That's real base, burn it all. I also think we need to stop hating on developers who want to monetize their software. The ICS nine nine nine stuff is kind of like cringe to me because Larry put a lot of work into that and it seems very good. But people sort of are saying to him, well, no one's going to use it because it's not open source.
06:49:22.354 - 06:49:52.350, Speaker A: And you get a grant for it. But then like, okay, the grants expired, then money's out, and then it's open source and you can't make any more money off of it's. Like, why would you do this? I think for a lot of reasons. Application. So I agree with you, we need to get developers paid more. But I think the ICS Nine Nine One and IBC application protocols are kind of a special case because really, in order to do that, you have to bootstrap a bunch of chains that do it in order for it to have any value. And then there needs to be apps built on top of it.
06:49:52.350 - 06:50:45.214, Speaker A: But how do you bootstrap that without giving it away? And then at what point are you going to start charging for it? And there's no real built in mechanism to be able to do that. At the application protocol layer, the light client layer, I do think is a much better layer to add those types of BSL licenses and to allow applications to have certain exclusivity periods or put in riders where there's a certain amount of public good funding that needs to be reached before those things become fully open source. Because it kind of, like, forces the settlement layer narrative, and it allows that chain a chance to help build some IBC routes through it, build those trade routes and bootstrap them. Whereas at the application protocol layer, there is that bootstrapping problem that exists. So, yeah, the Larry thing is tough. I love Larry. I really like ICS nine nine nine.
06:50:45.214 - 06:51:25.130, Speaker A: I'd love to use it more, but it's yeah, Larry should be the president of the ICF. We should just change the burn address. But if you go down that path and it's like I like to think of early parody. They license everything. Like GPL and Substrate was licensed. GPL and enterprises would come and just be like, hey, can you please re license it to Apache so we can work out a way to use it? And I think the Cosmos SDK has a ton more users that we actually don't know of that I find out daily that it's oh, like, I was talking with Wormhole and Jump and they have a private Cosmos SDK chain for their signature process and stuff like this. And it's just like, people use it because they don't have to worry about this license issue.
06:51:25.130 - 06:52:02.754, Speaker A: But then it's like the downside is, like, everyone's been saying everyone uses it, but no one ever thinks back. Like, oh, we have to give back because it's sustainable. But it might be that if the ICF were to say, every year now, the budget's going down and down for these products, and it's like, if the ICF says, oh, we're cutting it off at this number, whatever else is more you have to get from anywhere else, I think teams might start showing up and being like, hey, we need this feature. We want this feature. We want this feature. And if the teams are like, sorry, we're getting paid only to do this, then teams will be like, okay, we're happy to contribute. And then that kind of bootstraps the conversation.
06:52:02.754 - 06:52:24.686, Speaker A: An interesting thing about the BSL license is you can set a cap beyond which you go full open source. So, for instance, you could have some software developed under BSL, and then each chain, we need to contribute some amount, and when that cap is reached, it becomes fully open source for everyone to use. I like that model a lot. That's what we're working on with Duality. I think that's a great idea. Yeah, I think more people should be using that. So talking.
06:52:24.686 - 06:52:49.558, Speaker A: About duality, and I think you're talking about the roll up stuff with duality. So let's talk about that and how we make IBC roll up friendly. We are at the Celestia summit here. Jack, for you. You are the IBC guy. What's needed to make sure that roll ups use IBC between themselves and also between app? You know, we need to build out the infrastructure. This requires funding and it requires know.
06:52:49.558 - 06:53:42.714, Speaker A: I'm sitting here talking to the Dimension guys and they have a working fraud proof IBC implementation, which is really cool, seeing that more broadly adopted. But I think the biggest problem with the roll up model right now is there's no overall consensus on what the final architecture for these roll ups is going to be. Each of the architectures that's been developed by these different companies, each has their own sort of like unique cryptographic assumptions that you would have to bake into a light client. So there's, I think, a two pronged approach to this problem. One is we've done a lot of work in the core code bases, ibcgo, Ibcrs Relayers, both Hermes and Relay to help abstract away the sort of like, Cosmos specific assumptions. A lot of this work has happened over the last year, helping bring the composable finance work to market. And I think we're going to do three or four non standard IBC integrations this year.
06:53:42.714 - 06:54:32.730, Speaker A: As a community. Next year, we need to do 30 to twelve to 30 probably. We need to mini X, this capacity, this ability to bring IBC light clients to new places. So I think that's one part of it. But I think also building in better relationships with teams like Wormhole, Hyperlane and Axelar, where there's these committee based solutions that might have different transport layers, but we can build Shims or other compatibility mechanisms to allow developers to use those IBC interfaces anywhere and sub in different transport layers because that's fundamentally what they are. And those committee based solutions can go places that a light client based architecture can't a little bit quicker. And we've always thought that they're going to be hubs, and those are potentially Hubs.
06:54:32.730 - 06:55:28.294, Speaker A: I want to come back later to the Hyperlain stuff because that was an interesting topic last week or the week before that. But first, the Skip team has been working. I think you guys got a research grant from Celestia on the mev stuff. How do you know about that? Yeah, we're working a little bit. Yeah. Okay, well, what is it about? Do you know about that or do you not know about that, about IBC or about the sales tech grant? Is there something there or is it just something I made up? We are working with them. On the research side, I think the most apt thing there is less about mev, but more about sort of IBC and this idea of these APIs that we put out, where the idea is like, no matter what bridging protocol you use, if it's hyperlane, if it's layer zero or whatever else it is, or if it's IBC.
06:55:28.294 - 06:56:15.210, Speaker A: Of course, you can do these things in one click, right? You can transfer, you can swap, and it's not a complicated pathing problem. And so a lot of that work has been helped out by PFM and IBC hooks, one by Strangelove and the other by Osmosis. But they allow you to just mesh together all of these different bridges and just have these workflows be extremely simple. And I think in the roll up land, this is talk that I gave before, but in roll up land, that's going to become a lot worse, I think, right? Like, you're going to have so many different roll ups. They're all going to have their own gas tokens, they're all going to have their own routing protocols, and you just want to create an abstraction above that. And I think I'm going to say the word, but I also think intents might be helpful here. CC wraps.
06:56:15.210 - 06:56:56.098, Speaker A: But there needs to be a solution there to help fix that. Right? Now, from the Cosmos SDK side, there's already stuff like rollkit, for instance, where they're taking the Cosmos SDK, they are essentially removing Comet from the Cosmos SDK. Can you talk a little bit about that? How you see rollup framework evolve and how they compare to the SDK? So the direction we're going with SDK is we want to abstract ABCI away from the user. We want to abstract Consensus Engine away from the user. It's already kind of there and kind of like when you're developing an SDK application. EVM or no EVM? Cosmosm. No Cosmosm.
06:56:56.098 - 06:57:22.218, Speaker A: That it's like either a roll up or a chain. And so we've been working with the roll kit team. They have a lot of ideas in terms of different block construction, different header construction to enable different use cases. What we're really learning the past months is that the ABCI is really limiting in what you can build with it just because it provides these limitations. You can't access the P to P layer. It has this mempool. People want to do different things with the mempool.
06:57:22.218 - 06:58:40.070, Speaker A: But you have this app side mempool now, and now it's kind of like being conflated on what is app, what is the Consensus Engine, what the app should take care of, and what the Consensus Engine should take care of. And so, going forward, we're working with a few teams, roll kit dimension, and just talking on what they need to enable a better roll up future just so we are as competitive as everyone else and also just rebuilding the software in a way that, okay, roll ups are today. What is tomorrow? If tomorrow is something else, are we ready? Can we take advantage of that? And so we're just taking the software, like, basically almost rewriting the software to enable mean, Marco, the way that you've described it to me is the SDK is this layer that helps coordinate between different consensus mechanisms and different security mechanisms on the bottom and then different VM layers on the top. Is that kind of how you describe it? Yeah. The SDK is kind of like this middleware kernel and then the modules, like in EVM world we can kind of call them like pre compiles and then those are like their kernel level APIs. And then the application layer will be your EVM cosmosm move whatever you really want. I just think that that is clearly the final architecture for application or VM framework.
06:58:40.070 - 06:59:34.410, Speaker A: And the work that you guys are doing right now, I just think it's awesome. Can I ask a question? How Marco? No, Sean. But how do you design a general framework for roll ups when there aren't any roll ups and they're still like, I don't know how roll ups are going to work. I've spent a lot of time looking at shared sequencers, a lot of time studying like across fuel, across roll kit, all the roll ups that will be right. What are the core primitives that they need and how do we embed that in the SDK? Well, the sequencer, the roll up aspect of it is like a replacement of the consensus engine. And above we're kind of meant to only really worry about here's a consensus interface. If you do the consensus interface, we can just mimic ABCI, prepare process, vote extension and finalize.
06:59:34.410 - 07:00:41.598, Speaker A: And then it's just like the roll up has the option to implement it as no ops or not. And so it's less so, like what is a roll up and what is not a roll up to the application? To the application it's just a distributed system state machine and underneath it can be Comet, it can be whatever. And then in the future the idea is like, okay, so we kick started this runtime working group and now we're doing some research on different consensus interfaces. So you kind of have this immediate finality that is tendermint. Then you have roll kit, you have longest chain, you have roll ups, you have longest chain, and then you have this hybrid consensus world and it's like, what is a consensus interface that kind of fits these worlds to enable users to do really whatever they want? I think, Sean, just to your direct question, while we don't have a lot of idea what rollups are, are they real? We don't know. What we do have a lot of idea about and a lot of prior art on is what application developers one need. And I think that fundamentally the whole purpose of rollup is to decouple the application development experience from the other concerns there.
07:00:41.598 - 07:01:13.180, Speaker A: And I think that we do know what application developers need. They want to write in the programming languages and paradigms that they're used to. And the closer we can come to providing those in an ergonomic way to as many users as possible. That's how we get the adoption. On the developer side. Also, just curious, when Polaris was sort of being developed, did that inform any or I guess bring out any sore spots in the SDK that, oh, 100%. I'm curious about those.
07:01:13.180 - 07:01:47.474, Speaker A: No, it's perfect. We can all go home today. The SDK is perfect. You don't need to build any more SDKs. No, I mean, they're really pushing so the SDK and Comet in the initial design. And if you talk to Jay, if you talk to like, it's meant to be this modular software that raised too much money and then all of a sudden slowed down, and then they needed to launch and they needed to glue things together real quick. And so we're kind of like going in that original, but it's like Polaris is pushing the modularity of the software to the brink, and we're learning a lot.
07:01:47.474 - 07:02:21.194, Speaker A: And the feedback loop is very close with them just because they're a user that we're considering, a power user that will push the envelope of the SDK as a middleware. They just had their workshop devs giving a talk, and they have asynchronous pre compiles for their EVM and so on. And so all this stuff is really pushing it. And then it's like, Dave, we hop on a call every two weeks, and he's like, okay, these are the issues. He gives me a list of like, 50, and then we try and fix at least 20 of them before the next call. And then the list just keeps growing. One more question for you, Marco.
07:02:21.194 - 07:03:18.082, Speaker A: Is that Dave from Osmosis or Dave from Barachain? And does it even matter? Do you just add them together and just call it all Dave? We just aggregate them. Yes. Sometimes, depending on who you talk to, it could be a ZK proof, except Barrachain isn't real. So do you see the Cosmos SDK to move? So I like the way you describe the kernel and the VMs on top, but do you see the Cosmos SDK becoming itself like a roll up framework? At the end of the day, we want it to be more of a library, and today it's roll ups. People are raising millions of dollars on the phrase in their pitch decks of roll up. I am a roll up. And so it's like, yeah, sure, it's a roll up SDK, and then maybe tomorrow it's a blockchain, and everyone's back on the blockchain sovereignty, like owning your own DA and everything like that.
07:03:18.082 - 07:03:51.322, Speaker A: And then all of a sudden, okay, we want it to be where the application developer can decide between a roll up or a blockchain up until launch. So basically, they're building their state machine. Everything stays the same. They're just deciding between finality or longest chain. And then all of a sudden, it's like, okay, we want to be a roll up, or we want to be chain. Because maybe it's like they raise money to be a roll up. And then all of a sudden they're getting a lot of hype, they're getting a lot of users on their testnets and stuff like this, and they're like, okay, maybe we can own our own sovereignty and our own DA and let's see how that goes.
07:03:51.322 - 07:04:11.970, Speaker A: Because at the end of the day, if it doesn't go well, they can also roll back to be a roll up. I mean, we're seeing this with Crescent is also kind of like playing around with the idea of being a roll up instead of a chain. If it turns out roll ups aren't real, they can come back to being a chain. Exactly. App chain. Good deal for them. Sean, you're going to be my tournament Comet expert.
07:04:11.970 - 07:04:57.540, Speaker A: We because you have experience in that. How do we make Comet more roll up friendly? The same way we do with IBC and rewrite it in Rust. Rewrite it in rust. That's step one. I think at this point, I've thought about this a lot and there's one idea of making incremental progress on sort of the core components like block sync, state sync, the P to P layer, these sorts of things, rebranding it. I think what would be interesting and a good sign of commitment to the future is if we actually put together a little bit of money. It doesn't have to be that much all at once, maybe 5 million to start and we actually just pot commit to building a second implementation of the core client that's supporting $100 billion of value.
07:04:57.540 - 07:05:35.130, Speaker A: I think probably 5 million is enough to start and I think it'll probably cost 20 to finish something like this. And I think you could build in different incentivizations. Maybe it's mev, maybe it's relayer fees. I don't know. There's probably a bunch of ways to sort of create a sustainable model for when this infrastructure has product market fit. You should probably not monetize infrastructure before it does, but when it does, it should be monetized. And yeah, I think it'd be interesting to start from scratch because it's like tendermint is, what, ten years old in crypto time, which is yeah, it's ten years old, which is 100 years old in crypto.
07:05:35.130 - 07:06:34.466, Speaker A: Yeah. I think whether it's in Rust or another programming language, what we fundamentally need is better modularity out of tendermint. We accumulated users before we able to make the breaking changes that we needed to really have the flexibility in the core software. And if you go look at the one function that is the state machine with the giant switch statement, you see all of the pieces of storage and P to P and Mimpool just plugged directly into that function in a way that makes it literally impossible to pull out the different pieces and work on them individually. If we had a more modular Tendermint, then we could allow users that want to continue to use the core protocol to do it and then allow better experimentation at each of those layers in an independent way where it doesn't block on a core team. And yeah, I've got a lot of thoughts on this just to build on what Marco does too. And there's something we can't I got, like, talking to the Roll Kit team.
07:06:34.466 - 07:07:31.234, Speaker A: If you look at Roll Kit and what it is, if Comet was modular and they had interface separation between everything, roll Kit would most likely be using the glue, like the node package to glue their specific packages together, their own P to P, whatever they want. But at the fundamental base, at the fundamental foundation, it would still be Comet and they could still be using the mempool in a much more easier way than they are now. Potentially, you could say that if Comet tendermint was fully modular and it was actually nicely written, there wouldn't be a need for Roll Kit. Roll Kit would just be another couple of modules for Comet. That's interesting. I don't know if they talk to us about that, but that would be something to follow up on. I also think actively chatting with Audi about it quite a bit.
07:07:31.234 - 07:08:25.780, Speaker A: There's like this opportunity. Like, I know, for example, scroll is looking for a decentralized sequencer and they're considering Tendermint, right? And it's like tendermint can play multiple roles because it is many things at once. It is a consensus layer, but it is also a sequencer and a decentralized sequencer. And I think when you ask the question, how do you make Tendermint more friendly to roll ups? First of all, it begs the question of what is a roll up and sort of like, what do you need to abstract out? In the case of Celestia, it's posting the data to the data availability layer. So it's like maybe creating some kind of better tooling around that. But I also think it's like, I think you want to separate out the pieces of tendermint so that the people who need the decentralized sequencer, which I think is going to be a huge thing in roll ups, can just use that. And the people who need the consensus, which they may not need, can be just like separated out.
07:08:25.780 - 07:09:37.370, Speaker A: On that note, and this sounds horribly heretical, but here it is, I think that the POA design space is something that's incredibly underexplored. Why would you pay a DA and a sequencer and all of these different pieces if you just need to spin up like a five to ten node validator set? That maybe there's some ability to rotate those validators, some different controls over who the validators are, different ways to punish misbehavior. And then you've got full Tendermint, you've got the ability to IBC to anywhere, and maybe it allows you to still opt into some of those services like data availability that you might need. But I think that that's probably one of the biggest needs is a better POA module for the SDK. I've got one that I've been working on that's sort of like a copy of the Open Libra one where validators vouch for each other and if you've got enough vouchers, you can join the set and if your vouchers get pulled away, you get kicked out. A very simple mechanism, but you can easily imagine advancing that more how those vouchers are given out, the number of vouches that are needed. There's a lot of easy parameters to play around with there that I think would offer a lot of flexibility to app builders.
07:09:37.370 - 07:10:02.914, Speaker A: Right. As a short term solution or as a long term solution. This POA thing, I think that there's this idea of progressive decentralization. Everything starts out a little bit more centralized. Like an idea starts in one person's head and then it sort of propagates outwards and we see this deeply in the ethereum community with the optimistic roll ups. Op is the super common stack, but I don't think they have the fraud proofs working on mainnet. Right.
07:10:02.914 - 07:10:57.598, Speaker A: So like, does it give the trust assumptions that they're billing it? No, it's like one sequencer running in AWS. So is a five person validator set better than one sequencer running in AWS? Can you pull that POA module and then plug in a different staking module or a different implementation for security later? Absolutely. And I think that because of the flexibility out of the core stack, the ability to sort of progressively decentralize and change those mechanisms over time is a huge benefit. So, Marco, you may take note for the POA module from Jack. Yeah. So let's talk a little bit. Since we have Skip and Mechatech here, you guys have been evolving, like both projects kind of move, I believe maybe that's what I feel at least away from Mev and more into interns and UX and more like emitted topics.
07:10:57.598 - 07:11:31.280, Speaker A: So maybe Sean, I think you retired a module recently, like you had built an Mev module for chains. Maybe. Can you talk a little bit about that decision and where Mechatech is heading? Well, I think Mechatech never wanted to introduce new intermediaries into the supply chain. I think we wanted to integrate the parties that were there. Mev was founded to sorry, mechatech was founded to rewrite tendermint and rust is the truth. And we explored Mev because we thought it was going to be a viable way for sustainable funding for infrastructure. That's why you had the budget and everything.
07:11:31.280 - 07:12:06.618, Speaker A: That was like the whole premise. When we look at Mev and I think Skip also diagnosed, I think we had similar diagnosis at the same time, is like one, there's not that much and it's really an application level thing that is inevitably going to be internalized by protocols. Right. I don't think there's going to be a singular block space think or unified execution environment or whatever you call it, not swap. I think Swab is going to be great, but I think uniswap. X is also going to be great for those things. I don't think Cowswap is going to go away the day that Swab is live.
07:12:06.618 - 07:12:33.794, Speaker A: There's going to be many markets and so we want to build the infrastructure that supports those many markets. Sorry. Oh, all good? Yeah. I mean, mev is very small in Cosmos. If you want to be a searcher, I would not recommend it. I'd say where we've looked more is we got very deep into this transaction pipeline. Right.
07:12:33.794 - 07:13:41.826, Speaker A: So looking at where transactions are coming from, IBC based chains really have two sources of order flow. One is from the front ends, so like your wallet or a front end. And then the second one is relayers. Right? So relayers are inputting transactions into a chain. And we just started to think a little bit more about, well, how can we sort of move a little bit higher up the stack to actually leverage some of the stuff that we had done to make things easier to use. And so that's really when we got into this IPC routing issue, which we realized was a problem for a lot of people. That's generally where we'd like to be in the future is really thinking about how can we better use the transaction lifecycle to help users and make Cosmos usable and interoperable and sort of power it by mev, use the pieces of mev like simulation and bundling and ordering and block space construction to try to make that process financially sustainable, potentially, who knows? But then definitely better usage.
07:13:41.826 - 07:14:34.078, Speaker A: Yeah. And just to tack onto the end of that real quick over at the other stage, there's a lot of folks talking about proposer builder separation, complex mev strategies, and a number of other things. And I think a lot of the reason for that is in Ethereum, there's one big pipe where all of the exploitable information comes through and that's where the mev comes from. And because of the architecture of Cosmos, there's never going to be that one big pipe. And because of the ability of app devs in the app chain world to control their underlying consensus, it gives them a lot more tools to combat this and there's a lot more different ways. But I think that we're very lucky as a community to be able to watch that one big pipe, see how the battle over it plays out, and pick the best in class solutions for that over time. I was wondering for one more question for Skip and Makeate.
07:14:34.078 - 07:15:09.762, Speaker A: You guys are both evolving in the same space. Are you looking at each other to see what the other is doing and trying to fill another need or what's the strategy between your two companies? We talk every month. We talk a month? Yeah, talk every month. Nice chat. What do you talk about? Whether NDA boxing rematches. No, that'll be fun. We talk about what we see the ecosystem needing and then how we can complement each other in growing that pie and seeing if there's room to collaboration or friendly competition.
07:15:09.762 - 07:16:03.974, Speaker A: We try and stay open to both. Yeah, I'd say both have a lot of respect for each other's team. And, yeah, I think in general, we want to make sure that I think both of us want to make sure that we're actually moving in this direction of UX, as far as I understand it, and making sure that we're not just like we've recently been tweeting this thing, like grow the pie, right. That's really where we want our mindset to be, because we've just realized ourselves, like, cosmos is not big enough to support us and to support the amount of teams that I think want to grow. And so we're just really laser focused on trying to bring do anything we can to try to grow the ecosystem, no matter what it takes, because otherwise it's like, what are we doing? That's really where we've shifted. Cool. Jack, I wanted to talk about hyperlane with you.
07:16:03.974 - 07:16:53.382, Speaker A: So that was a big deal last week, right? Like, hyperlane is going to be, at least the way I understand it, quite competitive to IBC within the cosmos ecosystem. So what's your take on hyperlane? And why do you decide that translucent should work with them? I mean, mag's just down there talking about growing the pie, and I really think that that is sort of key to my thinking as well. We need better routes in and out of this ecosystem, and for better or for worse, this is just the reality of the situation. We haven't been able to deliver them as quickly as we needed to with IBC, and especially with the advent of roll ups and all of these things coming, we need a way to be able to participate in those economies as cosmos. And if we don't have them, then this problem of ecosystem growth continues to be compounded. That's why we chose to take the work with hyperlane. They've got a lot of traction within their existing community.
07:16:53.382 - 07:17:49.842, Speaker A: They're incredibly ideologically aligned with IBC. And as part of the contract with working with them, they've agreed to figure out a way to align hyperlain with IBC. Whether it's figure out a way to fit the hyperlain verification into an IBC client, which looks practically impossible in a lot of ways, but building a shim layer that allows developers to use either semantics and the different transport layers moving to and from these roll ups. I think even like we were talking earlier about the number of different light clients that need to be developed for these different roll up architectures, that's a huge lift. It's going to be hard, and people are going to be inventing these things faster than we can connect them, and there's always going to be a need for these kind of committee based solutions. And I think it was like, in january, I had a long dialogue with Zachie about this. I went back and forth with him for like, a month, and I was personally, really, honestly, like, I've dedicated the last bunch of years of my life to IBC, and I was like, this seems competitive.
07:17:49.842 - 07:18:23.220, Speaker A: Do I need to do you know, we do need to do it. And we're not going to win by sitting off on an island and telling everyone we're right. We're going to win by being pragmatic. We're going to win by working with more and more people. And IBC is this open standard with no token attached, with this sort of like immaculate conception. I do fundamentally believe it will win, but we need help, and we need to work with some of these other things and figure out how to build those developer experiences and keep linking this ecosystem to more ecosystems. And that's why I took the hyperlane work.
07:18:23.220 - 07:18:49.142, Speaker A: It's kind of like early on where Cosmos came about because it was like, oh, there's a bunch of chains. They're all siloed. They can't communicate to each other. Let's create a communication protocol. But then there's now so many communication protocols, but they're all like, ecosystem bound. So, like, within Polkadot, you have XCM that's within their ecosystem. But then you have a multi SIG bridge going out in Cosmos, you have IBC, and then a multi SIG bridge going out in Ethereum.
07:18:49.142 - 07:19:05.134, Speaker A: It's like, more or less all multi SIG bridges. And Solana, it's all multi SIG bridges. And so now it's like, we kind of have siloed ecosystems of trustless bridges. So now it's okay. Like, we can connect chains in ecosystems. Now we have to pass the barrier between ecosystems. But I also have a question.
07:19:05.134 - 07:19:22.360, Speaker A: What is it? I think they just asked me to stop the I wish we could continue, but I don't know if we have, like this lady asked me to stop the bell. I don't know. I think someone's up next. Tebow, thank you very much. Thank you, Tibo, for moderating this. Great questions, great discussion. Thank you.
07:19:27.770 - 07:20:15.184, Speaker B: I think you just give that back to them. All right, thank you to our panelists. We're going to welcome Yong in Lee next to talk about securing connected liquidity for modular blockchains. Okay. My name is Yang Yin Lee. I'm in charge of the head of partnership at Crescent. And I'm here to share what we've done and what we are going to do about the modular blockchain.
07:20:15.184 - 07:21:45.456, Speaker B: And before I start about what we are going to do and what we've done on the modular blockchain, I just want to point it out some strengths and advantage of the modular blockchain. Can I get some help? How can I always see? So I want to start with the why modular blockchain first. So why modular blockchain first? So instead of one blockchain doing everything modular blockchain specialize and optimize to perform a given function, to perform a given functions. And this specialization provide provide a breakthrough in. Scalability low entry barrier and flexibility also interoperability. It's enabling developer to build smart blockchain application for mass adoption. Okay, let's deep dive into the each strength one by one.
07:21:45.456 - 07:23:16.610, Speaker B: The scalability scaling difficulties mostly occur when one blockchain tries to handle all functions at once under a single layer. But when we think about the core concept of the modular blockchain is that they separate all functions, they separate the function across the multiple chains. So this kind of the concept bring a lot of the scalability. So modular layer one like a Celestia they can specialize in providing data availability and security. So modular layer one can focus its all resource on providing data availability and security for the L2 like a roll up. Okay, next one is the next one is actually the next page is the low entry barrier for the developers. If developer could focus on the minimum such as execution and they can just plug in another modular blockchain component to handle the specific task like execution, then it can highly reduce the burden of running a new blockchain so burden of learning a new blockchain.
07:23:16.610 - 07:24:39.654, Speaker B: So next one is the back yeah, I want to explain about yeah, I'm sorry about that. So the last one is the flexibility. So purpose built blockchain offers greater flexibility with respect to trade off and implement design, implementation. So layer, layer, layer so they design implement. So when you think about the security and data availability layer, they can get an extra benefit from the scalability because all kind of the transaction is processed separately. And when you think about the security execution layer which is optimized to the scalability, that can get a huge benefit from the security. So we just talk about what kind of the benefit and all kind of the advantage of the modular blockchain has then I just want to share about how we can flourish the modular blockchain.
07:24:39.654 - 07:26:13.830, Speaker B: So it is really important for the modular blockchain to expand and build their market that can utilize and intensify the modularity. So there are so many segments you can see the DeFi game and NFT metaphors which can intensify and tap into the modularity. So I just want to talk about the DeFi more deeply. So when it comes to the DeFi so to bring out the best in the modularity is that securing the connected liquidity. So Crescent has a vision to support the securing the connected liquidity then securing the connected liquidity is why is that so important? That's because securing connected liquidity play an important role not only for the DeFi but also the entire ecosystem. There is a great example of the Arbitrum actually Arbitrum set the market strategy that can bring a high TVL and high connected liquidity entire Arbitrum ecosystem. So they set the high TVL market strategy so they start to incubate a various part of the segment is DM and they expand and they support a lot of the various segments DeFi and various segments is DAP protocol.
07:26:13.830 - 07:27:50.358, Speaker B: So here's interesting data that we can see how Arbitrum actually make a great market strategy and go to market. So first of all the TVL, they set the high TVL market strategy so they incubate and they start to expand their market which can bring a high TVL. So as you can see at that point you can see the high TVL and it can make a lot of DAP as well because their go to market strategy was the incubating various DF and various segments. So the number of the DF is increased and next one is the daily active user because of their go to market strategy, their daily active user grow naturally because the number of DF increase organically, a lot of user broad a lot of transaction as well. So consequently, as a result you can see a lot of the transaction can make a revenue of the entire platform. So here is the virtual cycle that Arbystrom actually going through the TBL growth and they set the market strategy that can bring a high TVL. So they incubate a lot of the D app and they incubate the utility and it makes a lot of user and it brings a lot of transaction.
07:27:50.358 - 07:28:44.650, Speaker B: So transaction brings a lot of revenue. So this revenue can contribute a lot of great utility and a lot of tones of the user experience and user interface. So this kind of the virtual cycle going through over and over and over and again on the entire Arbitrum ecosystem. So it's the modular ecosystem. So to thrive in the modularity ecosystem they need to focus on the securing high TVL. So Crescent has a vision to migrate specific feature on the Celestia to contribute the Celestia ecosystem. We have been built Crescent on the Celestia for securing the connected liquidity which means a TVL.
07:28:44.650 - 07:29:35.318, Speaker B: So I'm here to share about our experience, about our experience about making a modularity and migration on the Celestia. All the experience consists of the first step. The first step is the Roll Kit version. So actually the Celestia roll kit is made by the Asica and the most appchain or the most made by the SDK also. So you guys need to check each version of the SDK to make it compatible. You need to understand what kind of version SDK each thing has been made. The next step is for the customized SDK you need to choose the two direction.
07:29:35.318 - 07:30:30.186, Speaker B: The first direction is the rebasing and remove and second direction is the overriding and debugging. The reason why we need to go through this process is that most app chain heavily customize their SDK for their app chain and this kind of SDK like customized SDK can cause a conflict because Roll Kit also made by the SK and appchain also made by the SK. So you need to figure out what is the conflict and you need to fix the conflict. The first way is just rebasing and removing all the conflict. And second one is just overriding and debugging all the conflict. So actually we tried both of one. The first direction, we tried it under the block raised testnet on the Celestia and it only took one day.
07:30:30.186 - 07:31:00.562, Speaker B: And second direction, we tried it for today's demo and it took three days. The first direction is easy and fast and second direction takes much more time. But there's a big pros and cons. The first one is easy and fast. That is a big strength. But you cannot completely migrate your whole functions on the Celestia. But second because it's just removing it, the second direction is overriding and debugging all the conflict.
07:31:00.562 - 07:31:45.890, Speaker B: So you can use complete version of your old protocol on the Celestia. So on the right side, we write down all the module that make a conflict between locate SDK and Crescent SDK on the block race, we just remove all the conflict. But for today's demo, we debug all the function. So I can show you the complete version of Crescent on the Celestia. So, here's the last step of our experience. So, we finished to make a Volob on the Celestia. So I just want to share about our complete version of Crescent on Celestia.
07:31:45.890 - 07:33:59.322, Speaker B: So, this is the way we choose the second direction. So because we debug all the conflict, we can use complete version of the Crescent on the Celestia. So not only for the swap but also the order book, transaction and range pool, we can use all our protocol function on the Celestia very completely. So, this is the demo that we walk through all our protocol function on Celestia and it works very well. So this demo video is the two minutes lasting because we are testing all our function and all our protocol on the Celestia to check it working well. So all transaction and all swap, all order vuk, all the thing is working well. Okay, can we move to the last slide? So, I just want to share about our review.
07:33:59.322 - 07:34:24.580, Speaker B: So without the critical modification. So it's really easy to integrate it on the Celestia. So that is the big strength. And actually Celestia is made by the Asica. So it's really easy and convenient to the developer who has experience on the Cosmos ecosystem. And the last one is rollopchain has a role dependency to the Celestia main chain. So it's easy to convenient to operating the.
07:34:24.580 - 07:34:50.570, Speaker B: So this is the thing we want to share about the experience we've got. And thank you for listening this presentation. Thank you. All right. Thanks, Jungyan. We're going to bring up the next Cosmos panel, the applications panel. So we have our moderator, Zion Thomas.
07:34:50.570 - 07:34:54.510, Speaker B: Okay, what's your name?
07:34:54.580 - 07:34:55.050, Speaker A: Walt.
07:34:55.130 - 07:35:04.720, Speaker B: Walt. Walt what? Smith. All right, walt Smith is our moderator of the applications panel. And our panelists are albert Chan.
07:35:05.190 - 07:35:05.746, Speaker A: Hi.
07:35:05.848 - 07:35:44.670, Speaker B: Hi. All right. That's Albert Yongin Lee, who you just saw, right? Yelena Yurik of Jurich maybe I shouldn't have been so optimistic about this. Noble and Dan Lynch of Cosmology and Dev Bear. Dev bear is coming. And dev bear is coming. But do we want to wait for him or go on and have him join in a bit? Okay, we are waiting uno momento.
07:35:44.670 - 07:36:22.922, Speaker B: No problemo. We are waiting for Dev Bear. So how does Dev bear is anonymous? Is he oh, he has a mask. Yeah. Okay. All right. Is it a bear mask? Oh, wait, is that him? We got Dev.
07:36:22.922 - 07:36:56.050, Speaker B: We got smokey. Wait. Oh, I totally see his face. I don't want to anticipate anything. I don't want to give it away, but it's possible that the bears are the most bearish, so let's see what that's all about. But we'll see what he has to say for himself. Dev Bear is putting what project are you with? Are you with the team? You're with galaxy.
07:36:56.050 - 07:37:13.930, Speaker B: Okay. All right. I see a bear in the back. He looks like a college mascot. Yes. All right, and we're on. We've got Dev Bear.
07:37:14.430 - 07:37:16.220, Speaker A: Am I sitting on the left?
07:37:17.070 - 07:37:21.420, Speaker B: Yes. How's it going?
07:37:22.850 - 07:37:24.640, Speaker A: Okay, perfect. I'm good.
07:37:25.090 - 07:37:27.790, Speaker B: All right. Cosmos applications.
07:37:28.450 - 07:38:10.214, Speaker A: Thanks for joining us, guys. I'm Walt. I'm an investor at Galaxy. We've got a panel talking about applications in Cosmos. The over under on this guy taking that off is 20 minutes, so you can place your bets in the back with Skip. But to kick things off, I'm going to let everyone kind of introduce themselves, what they're focused on within the Cosmos ecosystem or adjacent to it, and kind of talk about applications that they see today that their ecosystems are focused on but also have already built and then kind of who they are as well. So if you want to go ahead and kick it off hey, I'm Dan Lynch, founder of Cosmology.
07:38:10.214 - 07:38:44.710, Speaker A: We're building the Adobe for Web Three, meaning we have a suite of products for the interchange in which our products are actually becoming the standard. We're probably powering try to give exact numbers, but probably about 80% of all the front end applications in the Cosmos, everything from low level encoding to wallet interfaces, all the top projects like Osmosis, Doidx, Juno, and Stargaze are using our tooling for everything. And essentially our focus is really on just minimizing the gap between intention and execution, helping developers execute their ideas.
07:38:45.850 - 07:39:04.220, Speaker B: Big fan of what you're doing. So I'm yelena. I'm a co founder of Noble. Noble is a native asset issuance chain in the Cosmos ecosystem. We're bringing native USDC to Cosmos and also other stable coins and working with asset issuers one on one to really enable that on ramp into the ecosystem. So happy to be here.
07:39:05.070 - 07:39:34.470, Speaker A: Hi, I'm Albert. I'm the co founder and CTO of Injective Labs, which is the development company behind injective the layer one blockchain. So Injective is a blockchain built around finance, focusing on DeFi. At our core, we offer a high performance on chain, fully on chain, decentralized order book based exchange for spot perps futures and binary options, and have a whole host of other DeFi applications and now are looking into integrating with roll ups to bring SVM and EVM compatibility as well. Cosmos.
07:39:38.570 - 07:40:07.758, Speaker B: Hello. My name is Yin Lee. I'm in charge of the head of partnership at the Crescent. Crescent is one of the spinoff project by the B Harvest. Crescent is the order book hybrid deck. So we are supporting old swap and transaction and DeFi protocol transaction. And we want to build tour service for the yeah.
07:40:07.758 - 07:40:09.200, Speaker B: Nice to meet you.
07:40:09.970 - 07:40:26.278, Speaker A: And last but not least, I'm Dev Bear CTO, co founder of Bear Chain. Bear chain is EVM compatible, all one built using Cosmos. Oh, do I not have my mic on? Is it not working? Yeah, I have a mic. I'm good. Is it good? It's ridiculous, but it's good. So, yeah, to kind of start again. Yeah.
07:40:26.278 - 07:41:07.922, Speaker A: Bear chain EVM compatible all on cosmos. What we did is we replaced proof of stake with what we call proof of liquidity. It's the extension of delegated proof of stake that aligns validators with providing liquidity on the chain and makes it so that the act of providing liquidity is what's basically giving you network ownership. In addition to that, we've also built out Polaris, which is an EVM framework that allows for true native EVM equivalents that's interoperable with Cosmos, IBC and all of that fun stuff. And super excited to be here to talk about Cosmos applications. Yeah, and just super quick because I think we missed it, if you could just say where your protocol or project is at. Is it live? Is it on testnet? So just starting back with you going quickly through yeah, we're live.
07:41:07.922 - 07:41:24.090, Speaker A: We're currently powering stuff on Osmosis, Juno, Stargaze, dYdX. I think they're using our tools, but they're probably not launched yet. The new Cosmos version, as well as a ton of other projects. So, yeah, we have about eight different products, all live. We're shippers.
07:41:24.430 - 07:41:35.120, Speaker B: Yeah. So we have a main net with a validator set that is a hub validator set. So it's a subset of the hub. And we will be transitioning to interchange security in a few months.
07:41:35.810 - 07:41:40.560, Speaker A: Yeah, injective has been on main net since 2021, so about two years now.
07:41:41.650 - 07:41:57.220, Speaker B: So Crescent has been launched since last year and it's been a year and a half. And for the P Harvest, we are running a market maker and R and D Center and Tas as well. So all of our protocol are on main net.
07:41:57.590 - 07:42:30.574, Speaker A: Beautiful. And Bearish will be entering testnet throughout Q three, and we have multiple teams starting to integrate polaris into their devnets as well. Great. So kind of, I think, for the audience, we're all at a crypto conference, so we're all pretty familiar, but just kind of interested in hearing different perspectives. We have a general purpose chain. We have a couple versions of interchange security or applied some sort of restaking or shared security. And then someone who touches almost every chain in Cosmos in kind of like your words.
07:42:30.574 - 07:43:31.134, Speaker A: And we'll start with Dev Bear, who's working on something a little bit more general purpose. Where do you kind of see application design and Cosmos being different? And why is it different? Or if it's not different, why do you think there's such an emphasis on application sovereignty and kind of like a tight coupling with consensus and the other pieces in the stack? Yeah, I think having an app chain, it's really interesting because it does give you that sovereignty, as you mentioned, right? It allows you to define your own rules. It allows you to define how you want to build your application, not only at an application layer, but at an infrastructure layer as well. And a big part of what allows for that is the dead UX of Cosmos. The ability to take something that's complicated, like a blockchain, bring it down into things that are simple to understand, like modules, and really break things apart, I think has done a really good job of that. Additionally, I think what's a really cool, interesting thing is the concept of app chains that are built on top of VMs as well. So things like Op stack, where they're like, okay, you can launch your app chain, et cetera, et cetera, and write all your things in Solidity, which is like a framework that people know how to use really well.
07:43:31.134 - 07:43:46.760, Speaker A: And that's part of why we built Polaris as well as we see that we want people to build app chains. We think app chains definitely have a future, but we want to make them more accessible to devs who maybe don't know go or don't know Rust and feel that sometimes having a virtual machine is the way to go and we can just bring it back this way.
07:43:48.410 - 07:44:33.138, Speaker B: I think appchain concept is pretty interesting because I'm very new in Cosmos ecosystem. I know Cosmos has a pretty long history, but I work at chainlink for almost three years and I moved to here last year, so it's pretty interesting. So all the application on top of the app chain can fully enjoy the customization. So it can pretty friendly for the protocol level. So, for example, on the Ethereum basis, it's really hard to use a fast transaction. Like, the autobong need a fast transaction, but it has a challenge on the Ethereum. But on the appoint, we can customize it and we can use all the different module.
07:44:33.138 - 07:44:37.880, Speaker B: So it's pretty interesting for the protocol and DApps level.
07:44:40.970 - 07:45:54.974, Speaker A: I think one new area that Injective has been thinking as an app chain, as a sovereign app chain has been with regards to roll ups and mainly roll ups you might see on more established layer ones like Ethereum, where you cannot make protocol level changes. So it's restrictive to what you can do but something that we've been thinking through is what if you could have for example, a more of an enshrined roll up and the reason for doing so is to have more atomic composability where suppose each of the validators of the injective blockchain were also part of the sequencer network. Then they could just submit the header and state root data information to injective. And for example that could allow for bypassing a fraud proof window and allow for more atomic composability. But these sorts of protocol level changes you would never be able to make on something like Ethereum because it's just Ossified and it would take years for anything as radical as that to occur. But you have this flexibility, right? Alternatively you could also explore things like interchange security. So if you want to, instead of a roll up have something like a fork of Avmos or Barretchain to have EVM capability, you can do so.
07:45:54.974 - 07:46:21.666, Speaker A: So there's a lot bigger design space that you can experiment with and I would say at all layers of the stack, right? And with people like Skip, you can do things like prioritize mempools injective, for example, you can use Ethereum accounts, you can support MetaMask, so at all layers of the stack you have flexibility and that really allows for chains to differentiate themselves and find a unique value proposition to offer to end users eventually.
07:46:21.858 - 07:46:46.980, Speaker B: Yeah, I mean, I think for Noble, we leverage the Cosmos SDK quite a lot. And, I mean, there's all of these modules that make a lot of sense for Noble's use case. And so we really leverage a lot of the open source sort of modules that teams like Osmosis have built when it comes to things like token factory or even things on the protocol side, like the packet forward middleware. So just really leveraging the open source kind of primitives that exist is why we think it's really easy to build in Cosmos. To be honest.
07:46:49.430 - 07:47:41.470, Speaker A: I think that the Cosmos represents the flagship representation of the third generation of blockchain. If you look at the first generation of blockchain which is digital money, like store value, couldn't do much with it. So we fixed that by creating the second generation of blockchain which is like Ethereum and programmability. And so the third generation represents interoperability kind of like horizontal sharding where you can create your own app chain and customize things. What's also interesting is it also opens up a whole new design space. A lot of the tools in Ethereum are very monochromatic because everything is just built on one chain and there's sort of one library for sort of interacting with everything. Cosmos is very unique because now we have to actually look at all these different chains with all their customizations and still try to come up with tooling to create these cohesive ways of working with their functionalities.
07:47:41.470 - 07:48:10.618, Speaker A: So that's actually really interesting challenge for us that we've also solved with compilers and transpilers it's actually really fun working with all the chains and helping them expose all these unique, different, unique features, but then creating, like, a unified way to access them. So really love the Cosmos and what everyone's building. Yeah. So I think it'd be interesting to get everyone's kind of perspective. Ethereum is 2015. Cosmos is 2017. Really 2019.
07:48:10.618 - 07:48:58.026, Speaker A: And right around 2019, ethereum kind of transitions away from Plasma and starts thinking about optimistic scaling with roll ups and then even kind of looking into the future with validity roll ups. Injective, you've mentioned it. You've also mentioned kind of this transition from money to programmability to kind of the Cosmos third vision of, like, an Internet scale type system. So how do you guys kind of see roll ups within this framework? Do you think it's all compatible or very differentiated so you can start? I've definitely heard some people talking about creating IBC connections to L2s. I will admit I'm not the expert on roll up, so I'll pass it. But I will say that I have heard some interoperable support.
07:48:58.208 - 07:49:45.610, Speaker B: Well, we're really excited to kind know, connect with the rollup ecosystem, especially, obviously, with the Celeste ecosystem from what is currently kind of the status quo. It is not a solved kind of question of how you bring IBC to roll ups, but Bridging protocols like hyperlane and kind of other examples are trying to solve for that. So I don't know if that answers your question, but I think that at the end of the day IBC as this kind of standardized, interoperable kind of protocol for permissionless message passing, at least for Cosmos kind of bulls or whatever. That's what we see as the future of this message passing across different layer ones and L2s. Right now, it's not the case yet, other than, I guess, poke it out to Cosmos. But I think we'll get there eventually.
07:49:46.590 - 07:50:27.560, Speaker A: Yeah, I would say there's a lot of engineering work involved. Know, for IBC, you have to currently wait for the fraud proof window to pass, which in the case of Ethereum is seven days. So obviously these windows can be shorter. But that really hampers composability, especially from developers or users in Cosmos expecting instant finality because that's how it currently is. You don't want to have to make that trade off. Right? So I think that's why, as I mentioned before, we're exploring ways to have more atomic composability. And the design space is quite large as an app chain because you're not restrained by existing consensus rules or protocols that you have to follow.
07:50:27.560 - 07:50:35.420, Speaker A: But, yeah, I will say that Injective is still quite early here, but we are partnering with Espresso and Systems and many others to figure this out.
07:50:38.430 - 07:51:31.302, Speaker B: Actually, in 2019 and only of 2020, IBC and Cosmos is quite vague, and it's pretty early for me. So I don't know what they're doing and what they will going to build. But after they launch the IBC and ICA stuff, I pretty understand what it is. And it's pretty simple and easy to not only for the retail user, but also the developers. So thanks to the IBC, a lot of the developers, a lot of the retail user doesn't need to worry about the bridge risk and bridge issue recently, except IBC. A lot of the bridge, they still suffering from the bridge risk. So if we can utilize IBC concept or ICA all Cosmos basis technology, then we can solve a lot of problem.
07:51:31.302 - 07:51:43.280, Speaker B: I think it's pretty good to start and with the modularity and intensifying the modularity technology, we can break through all those challenges, I guess.
07:51:44.210 - 07:52:26.938, Speaker A: Yeah, I think at the end of the day, speaking about it specifically from a roll up context is an interesting lens because at the end of the day, I think it's important that people have the scalability and have the tools to build applications that they want. And we can talk about what's the best scaling mechanism is it roll up is it things like ICS, but at the end of the day, people who are building applications just want to build applications. People that are building with React or people that are building with Django, they don't really care what they're using to get the job done. It's just the fact that it works for their use case. And I think we need to think about these scalability and these Bridging solutions kind of in the same way where if I'm an application, I need access to these other applications or to liquidity or something of the sort. And what's the easiest and lowest cost way to get that for them?
07:52:27.104 - 07:53:13.258, Speaker B: Yeah, I mean, I think it comes down to the path of least resistance and what is the most optimal path from getting from point A to point B. And I think in Cosmos we see that really simply with IBC and doing a token transfer. It used to be kind of hard, like when IBC first launched in 2020, I remember being like, what is going on? How do I do a manual token transfer like in Kepler with the dev mode, the developer mode, sorry, but that has greatly improved in terms of the user experience in the past, I guess, three and a half years. Things like automatic routing, things like packet forward, middleware, things like what Skip is doing, to just make it super straightforward to get from point A to point B without having to do a million swaps in between, which used to be the case. Not quite a million, but you get my point.
07:53:13.424 - 07:53:54.940, Speaker A: Yeah. So kind of coming from the sidelines, I think obviously developers love kind of like being in Cosmos land. It's very friendly, it's great documentation. There's a lot of things that you can kind of plug and play and build how you want. But crypto broadly, but also specifically, cosmos has struggled with adoption. Everything's pretty prone to the financial cycle. But what are kind of the applications and also the changes to the stack, whether it's UX custody something, what are the kind of the next steps in the next six months to close out the year and then going into the future that makes you really excited both on the application side and then on kind of the user experience friction side.
07:53:54.940 - 07:54:59.738, Speaker A: Yeah, well, I mean the way I see this is that you rewind about a year ago and to build an application in the cosmos, first it would have taken you between three to seven days to connect a wallet and you'd have to manually write all your encoders. So this is something where you're trying to build an app. You have this great idea, but you get lost in the weeds with all these complexities around things that as a developer, let's say you're trying to build the user interface and you're trying to create that user experience that will onboard these nontechnical people or these laypeople from the real world into crypto. But in reality, the type of technology skill set required was like a senior engineer that understood blockchain. So after Cosmology came onto the scene, we basically abstracted all that so we can minimize that distance to where now you have your idea, you can actually build it. And this is actually quite new. We've only recently kind of gotten through the full stack to where if you have a Cosmos chain or a Cosmos and smart contract, all of the front end tooling can now be completely automated.
07:54:59.738 - 07:55:37.050, Speaker A: So they were kind of fighting this impedance for the past year or a couple of years now to where the user experience has greatly suffered because the developer experience has suffered. So now that we've fixed that developer experience and provided a streamlined workflow for developers to the point where if you're a junior web two react UI developer, you don't have to know anything about blockchain now to start building DApps inside of the cosmos. So I think then we can bring in open the floodgates to all the web two developers who now have all these great ideas and they can build something. And I think that's how we can create beautiful user experiences to have mass adoption.
07:55:37.630 - 07:56:11.154, Speaker B: Yeah, I mean, I'll talk about it from the perspective just to look at your average user and then I'll talk about it from noble's perspective, but from the perspective of an average user, I think it's a lot of what Dan just said. I think me is my profile or kind of my persona or my user persona is pretty good. I know pretty basic, like HTML, some markdown I can read a little bit of go, I know logic, but I can't fully code it adapt. But there's all of these tools, all of these really seamless ways of interacting with the cosmos stack. This is kind of like back in the day. This isn't necessarily a thing that is around. Anymore.
07:56:11.154 - 07:56:46.854, Speaker B: Maybe I'm wrong, but things like Starport, with a couple lines of code, you can create your own Cosmos blockchain. This is a tool that the Ignite team built a couple of years ago. So just making it extremely user friendly, making Kepler kind of more versatile, making all these different integration points just accessible, obviously good documentation. I mean, these are all things that need to drive our ecosystem forward. And I think we're getting a lot better at it, and I'm quite optimistic. So that's more on the user side. Just on your Saturday night, what am I going to do? I'll kind of play with Starport.
07:56:46.854 - 07:57:49.278, Speaker B: Why not? But then more mainstream, kind of like, what's going to bring thousands of people to the next thousands of people to Injective or to Osmosis or to all of these app chains? I talk a lot about this. It is liquidity. It is like the institutions. It is actually partnering with them and being able to speak their language and actually being able to advocate for Cosmos standards and the Cosmos stack, and going through the motions of explaining what is IBC, what is middleware in this IBC world, how do you mint, how do you burn? What are the standards for things like asset issuance, which is obviously what Noble is focused on. So in terms of the application itself, I'm very excited and proud of what we're doing with Noble and bringing native USDC to the ecosystem and just really making sure that Noble can serve all of the other app chains that need this liquidity, need these more institutional focused actors to kind of come in and feel comfortable with the ecosystem.
07:57:49.454 - 07:57:56.614, Speaker A: Not to put you on the spot, but do you see other asset types for Noble down the line? And what does that kind of look like?
07:57:56.732 - 07:58:37.620, Speaker B: Yeah, I mean, we're very focused on stablecoins, and we see these fiat collateralized stablecoins as having huge real world use cases. And one of the things that motivates a lot of our team is real world adoption. And I think we've seen USDC really do well in the space. So really stablecoins are a big part of it, and really just being an ambassador for that asset issuer within the space. So not competing on DeFi, not competing as a Dex, just saying, we are here to propagate your asset as far and wide as possible and making sure we have the right integration points and we can have people actually use it properly and not have these concerns. So stablecoins wrapped assets. RWAs we'll see where the world takes us.
07:58:39.370 - 07:59:14.174, Speaker A: Yeah. In terms of the next wave of adoption, I think there's two things here to look at. One is that there have to be sufficiently compelling opportunities and products for people to use on chain that are better or that they cannot find in TradFi or CFI. And two, that we have to have infrastructure and tooling that can enable a lot more experimentations. A lot more shots on goal to do that. I think the work that Dan does, for example, at Cosmology is great, but it's at all layers. Right.
07:59:14.174 - 07:59:52.854, Speaker A: It's from the RPC stack. It's from even in Cosmos, the kinds of applications like making the bar lower, it's quite challenging for people to learn not only Rust, but Cosmosm and all these Cosmos concepts in order to just build a simple application. Right. So expanding even at the virtual machine level, allowing for EVM and SVM and having good composability with that is really important. Yeah. And I think also that bringing real world assets is very valuable here. USDC as an example, but also on chain Treasuries and the initial promise of DeFi.
07:59:52.854 - 08:00:10.546, Speaker A: Right. I just talked to a protocol, I forgot their name. You can, as a user right now buy Treasuries in your shop account. Sure, that's great. But this protocol was trying to do Treasuries. Plus you sell a straddle so you can juice your yield further. And it's that sort of thing.
08:00:10.546 - 08:00:31.050, Speaker A: So instead of earning 5%, you can earn 8%, 10%. Right. That's one example of how DeFi can offer more compelling opportunities. Yes. Maybe it's not as crazy as it was in DFI summer, but it's these sorts of experimentations that we have to enable at a much larger scale if you want to have a chance, I think, at greater mass adoption.
08:00:34.270 - 08:01:19.318, Speaker B: I think when it comes to the mass adoption, the most important thing is we need to make it easy from the protocol level. Like, for example, when you think about the Sushisops, like beginning, they just add one code and one function from the uniswap. So it's really easy to make and easy to deploy another D app. But on the Cosmos ecosystem, they need to operate and they need to concern about how they can support the chain security. So it is the very tricky part of the Cosmos app chain perspective. So not only for the running protocol and DF, but also they need to concern about how they can secure the chain security and how they can make their own validator set. So it is a real tricky part.
08:01:19.318 - 08:01:53.300, Speaker B: But thanks to the Celestia, so we can easily use their module and their layer so they don't need to worry about bootstrapping their own validator set on security or data availability. So we can utilize that kind of product very well. As well as the Saga saga, they support the app chain a lot so they can reduce the burden of running a new app chain. So thanks to this kind of the new protocol, I think mass adoption will be coming very soon.
08:01:53.750 - 08:02:31.482, Speaker A: Yeah. To get a tie on to, I think, a little bit of what everyone said. Obviously, bias here is definitely coming from the ethereum ecosystem. But seeing how easy it is for people to deploy solidity and Viper smart contracts in that ecosystem in retrospect or in comparison to Cosmosm you need to be like a senior engineer to deploy a Cosmosm DAP. But my twelve year old cousin can build solidity DApps and send tokens to his friends and it's really easy. So I think getting more simple at the end of the day, right, people don't want to have to worry about maintaining a validator set. People don't want to worry about learning Rust or having to learn new tools.
08:02:31.482 - 08:03:21.758, Speaker A: They just want to be able to reuse things that may be legacy and maybe old. As you mentioned, Ethereum 2015, it's approaching almost a decade old at this point. But in that sense it's kind of reached somewhat of the JavaScript legacy where when people who are maybe doing a coding boot camp or people who are self taught get into computer science, they quite often more than not get into JavaScript because it's easy. They can start making fun things, they can see their progress if they're building a little web app or a little node JS app or React app or whatever. And that's kind of how I see Solidity, is that you can clone soulmate, deploy a little token and it just kind of works, right? So it's really friendly and makes it really easy to onboard not only experienced devs who are building crazy protocols with off chain components and stuff, but the next generation of builders as well who may be still in high school or elementary school. I just want to make a comment. I completely agree.
08:03:21.758 - 08:03:58.982, Speaker A: In general, I think it's not a technology problem, it's a people problem. And he's right. Like, Rust is a very difficult language and Cosmosm is actually based on Rust. And if you look at things languages like TypeScript, there are millions and millions of developers. So if there's a way to open those floodgates, to bring those folks into writing smart contracts, that's like a huge lower barrier of entry. Actually. I don't know if you guys know this, but Cosmology, while we've mostly been focused on front end technology, recently acquired the Terran One team and we actually have a compiler to Cosmologism that actually has a very sort of hybrid Rust to end TypeScript like languages.
08:03:58.982 - 08:04:49.210, Speaker A: It's very simple, bakes the semantics of the smart contracts into the syntax. So it's actually very simple. And our goal with our Cosmosm compiler is to actually onboard a lot of these Python and TypeScript developers that I see those languages as languages of the people. And so that's a big part of our strategy to onboard more developers from web two into web3. So kind of to wrap things up here. Do you guys want to? Each outside of the project you're associated with what's one application you think represents kind of the cosmos coupling of consensus and application and kind of creating something unique really well, and what's something that could be within your project's. Ecosystem or could be outside of it, but still in cosmos that you're really excited for and think could be a potential catalyst or kind of bring in more adoption.
08:04:52.350 - 08:04:55.390, Speaker B: I don't remember preparing for this. You put me on the spot.
08:04:56.290 - 08:05:56.174, Speaker A: In terms of projects, I think the Bad Kids NFT collection is pretty awesome because it's super unique. And while at first I was kind of into the punks and all these other things because I was looking at Ethereum for inspiration, it took me a while to realize it. But the bad kids are so unique. I know it's not actually a project that's powered by developers, but the Stargaze team did build the NFT marketplace. And I'm really excited about Bad Kids and just like, kind of like what Joe was talking about with the art world yesterday to Osmocon. I just found it really inspiring to think about, and I'm really excited about kind of catalyzing more of a creative, artistic creations by just giving more tools so people can actually execute their ideas. I think it's been so technically difficult to build things that as we lower the bar, just giving creators what they need to do and what they want to build.
08:05:56.174 - 08:05:57.586, Speaker A: So that's about it.
08:05:57.688 - 08:06:38.670, Speaker B: Yeah. I mean, I'm having a hard time coming up with one example. I don't want to just say one name and have that kind of live forever. So what I'm excited about is actually the components coming together, so these different projects coming together and actually powering something very meaningful. So to Dan's point about NFTs, we'll have a little bit more to share about this in a couple of weeks, but being able to buy a bad kid on Stargaze one click with USDC, that's not possible right now. There was a bit of like a Twitter kind of viral thing that happened a few weeks ago where a bunch of Ethereum people wanted to buy a bad kid. They didn't know how.
08:06:38.670 - 08:07:01.560, Speaker B: They were like, what are stars? What is this nonsense? Well, very soon it'll be abstracted away, and it'll actually be very easy within your wallet to one click buy an NFT with something like USDC or maybe even another currency. So things like this are just really exciting and I think will make people realize how powerful the Cosmos ecosystem really is.
08:07:02.970 - 08:07:24.640, Speaker A: Yeah. I'm excited quite about the L2 work that we're doing with Eclipse and also Espresso actually having a decentralized sequencer network and having it in production with IBC as well. So there's a lot to figure out there for Interoperability and Optimicity, but I think these are solvable problems and it's just a matter of actually doing it.
08:07:27.890 - 08:08:24.858, Speaker B: I think the Liquid Staking protocol improve a lot of user experience. I guess from the capital efficiency perspective, most of the crypto people, they don't care about the capital efficiency and the risk management. They're not good at this kind of stuff. But thanks to Laker Staking protocol, user can experience and user can understand what is the capital efficiency and what is the better choice to invest and what is the risk management. So I think it was a great use case. It was a great application for the user experience about the DeFi protocol for the upcoming protocol, not the new protocol. But I heard that I don't want to speak the specific protocol's name, but I heard that some protocol and some app chain, they're preparing the interchange query Oracle right now.
08:08:24.858 - 08:08:50.310, Speaker B: So actually, Cosmos ecosystem, we don't have any specific Oracle protocol, so we need it. Definitely. Also, using an interchange query is very important to the entire Cosmos ecosystem because using IBC can make a delay. So interchange query would be ideal. So I'm looking for the and I'm very excited about the new Oracle protocol.
08:08:51.130 - 08:09:50.326, Speaker A: Yeah, a lot of what we're focusing on is figuring out ways to take existing ethereum things and bring them into Cosmos and onboard those users in a way that's really easy. For instance, one of the things we're working on with the guys at Skip is a vote extension based Oracle that will tie into Barachain to provide pricing. And those pricing will be exposed to the EVM via an interface that's identical to chainlink. So we're trying to find ways to onboard users and bring users from other chains, from the EVM ecosystem into the Cosmos ecosystem through things that they're familiar with. When you have to reinvent the wheel, when you add friction, when you force people to learn what Beck 32 is, when you force people to learn what Cosmosm is, it can add that friction. But I think what kind of in the angle that we're really approaching is, you know, Barachain is an EVM chain. How do we bring EVM users that's kind know it's very familiar to them, et cetera, and over time, they'll start to, you know, what's this IBC pre compile you guys have? Oh, what's this? What's this? And you onboard them into Cosmos in a way where they don't even realize they're being onboarded because they just see it as an EVM chain.
08:09:50.326 - 08:10:03.760, Speaker A: That's something they're super familiar with. Great. Thank you. We have about three minutes for any audience questions. Could be for a specific project or more generally. Otherwise we can wrap it up. So any questions?
08:10:07.410 - 08:10:11.278, Speaker B: I think everyone's just building Cosmos applications, so they don't have any questions.
08:10:11.444 - 08:10:16.766, Speaker A: Well, thanks, guys. We'll stick around after if you want to grab contacts for anyone. But thank you very much for joining us.
08:10:16.868 - 08:10:17.638, Speaker B: Thank you.
08:10:17.764 - 08:10:19.670, Speaker A: Eight IBC.
08:10:31.690 - 08:10:35.560, Speaker B: Yeah, I'm going to put like, another seven days. Do you keep this?
08:11:30.150 - 08:15:20.780, Speaker A: Sam? Ram it Sam. Sam. Thank you. Sam Dam.
08:15:53.040 - 08:16:01.680, Speaker B: Now we are on the very last talk of the day. We have dev bear from Bear chain to talk about polaris.
08:16:02.900 - 08:16:06.050, Speaker A: Thank you very much. Audio good.
08:16:07.380 - 08:16:12.576, Speaker B: Is your clicker? No, your clicker. You have a deck.
08:16:12.608 - 08:16:13.780, Speaker A: I don't have a clicker.
08:16:14.200 - 08:16:16.340, Speaker B: This is the clicker. Can you see?
08:16:16.490 - 08:16:20.612, Speaker A: I can't see. This is the clicker. I can't see anything.
08:16:20.666 - 08:16:21.910, Speaker B: Putting it in front of me.
08:16:22.520 - 08:16:23.856, Speaker A: I got, like, limited vision.
08:16:23.888 - 08:16:25.784, Speaker B: Your eyes are like, okay, yeah.
08:16:25.902 - 08:16:46.664, Speaker A: It's not ideal, but the audio is good, though. Perfect green button to go forward. Perfect. So today I'm going to be talking about player CVM framework and kind of give a high level overview of Bear Chain as well. So a little context of myself. Dev Bear CTO of Bear chain. And Bear chain is an EVM compatible L1 built on the Cosmos SDK.
08:16:46.664 - 08:17:38.904, Speaker A: And what we did is we replaced the traditional Proof of stake system with what we call proof of liquidity, which allows for reutilizing staked assets to provide liquidity and making validators stake weight associated with how much liquidity they're providing. So before we get started, just want to give a little intro of our team. Our team comes from a wide variety of team comes from a wide variety of backgrounds, including Web Two, web Three all over the place and wouldn't be able to build anything without them. Yeah. So why EVM? This being a Cosmos adjacent conference, a lot of people ask us, why EVM? And the simple answer is the developer tooling. And just the access to utilizing the technology is so much simpler. There's millions and millions of JavaScript developers, and we see the EVM as kind of the JavaScript of blockchain.
08:17:38.904 - 08:18:36.400, Speaker A: And we think that by utilizing these type of technologies and making it really easy for people to build applications, that we can onboard more users into blockchain in general. And the next question is, why polaris? And when we sought out to build Barachain, we had a lot of complicated logic that we had to add to the base layer. And we saw that just like simply forking guest or taking another EVM framework made it extremely, extremely difficult to do this. We looked into kind of some other app chain frameworks, and we realized that the commonality between them was that a lot of the time that they would get to something that was EVM ish. And the problem with EVM ish, is that it's different than what developers assume, it's different than what people assume. And you end up with compatibility issues like we've seen with, like, ZKsync and some of the other L1s, where it's just not close enough and it can lead to problems and just poor developer experience. And the idea here is that we want to introduce an easy way for app chains to create an EVM equivalent EVM that allows for them to just plug it in, drop it in, and create easy and a nice environment for extensions, et cetera.
08:18:36.400 - 08:19:37.860, Speaker A: And the idea of this is that we wanted it to be modular, so we wanted a way to completely break apart all of the different aspects of the stack. Very similarly to the concept taken from the Cosmos SDK being built on Cosmos. We have the networking layer, the consensus layer and then the Cosmos layer as our traditional application layer. And we propose adding a new runtime layer on top of it which allows for the VM to be completely separated from the Cosmos runtime which allows for it to be running a fully native standard EVM like you would see on Ethereum Mainnet. And how we accomplish this right now, given the limitations of ComEd and the SDK is we actually are able to build EVM blocks from the data that's provided from Comet. So we take things like just the block header in general block height, block timestamp, et cetera, aggregate all the transactions out of it and are able to construct an EVM block that is equivalent to how you would see on mainnet. Then what we do is we apply all the EVM state transitions and then we essentially post that block back up to the Comet chain or the Cosmos chain and store its state route in the IVL tree.
08:19:37.860 - 08:20:31.060, Speaker A: So how do we do this? When we went to set up to build an EVM that's interactable with Cosmos and ABCI in general, we wanted to come up with what is the simplest, easiest way to build a blockchain. And at the end of the day, we see it as pretty much three main processes. So the first is that you need a way to build the block and assemble the block. So you're taking effectively a list of transactions ordering them in some way you need to then once you have that list of transactions you need a way to process them, apply the VM state transition, store the data, et cetera. And then lastly, you need a way to actually commit the block and make that part of the canonical chain. So in ABCI this is prepare proposal, process proposal and finalize block. And in Geth this is your Geth miner your guess state processor and then your store is your statedb or your blockchain.
08:20:31.060 - 08:21:31.416, Speaker A: So we realized that a lot of common blockchain frameworks other than just Geth and Cosmos kind of follow the same general pattern. So we realized that in Polaris what we could do is we can go and actually generalize this into basically three main interfaces or three main kind of constructs a block builder, a block processor and then a storage provider. So what we do in Polaris is we essentially bifurcate the actual block building and processing and storage from the execution itself. So in order to maintain that perfect kind of JSON RPC compatibility with EVM, mempool, blockbuilding, et cetera, is we actually run all of the kind of block building and storage on the cosmo side and effectively just use message passing to allow the EVM execution client to do all the things that it needs to do. So for instance, here we have an example here if I would just submit a transaction for executing an EVM I would actually do that directly to the Ethereum side JSON. RPC. And that would get inserted in the mempool from there that gets passed over, basically a message system which allows for ABCI to pick it up.
08:21:31.416 - 08:22:07.732, Speaker A: So then the builder is able to pick it up and then that's how the proposal is made. So all of that is done on the Cosmos side, but the actual user was interacting with the EVM side. Secondly, from there we process that proposal that basically, again, is taking that list of transactions that was built, processing it through the system that's talking to storage, that's talking to the actual EVM interpreter, et cetera. And it's all done through this message passing channel. And then lastly, we need a way to store what we did. So simply in finalize block, we're able to go and just write it to disk and say this now block is in the chain. This also kind of allows us for Queries as well.
08:22:07.732 - 08:22:54.340, Speaker A: So not only do we able to support native EVM transactions, but also full JSON RPC compatibility for Queries, et cetera. And we do it through a similar model. The message dispatcher is able to get the request from the RPC, talk to the Cosmos storage and handle it that way. This also works for transaction simulation, for if you're running a searcher that's trying to build a block, you could do it this way. So the ETH call goes through JSON RPC, goes to the dispatcher and is able to touch not only the storage, but can touch the processor, which touches the EVM interpreter, et cetera, et cetera. What this also allows us to do is it allows us to make all these components really, really pluggable. So in the traditional kind of if you were to compile it all together, you'd have to kind of choose an EVM and stuck with what you kind of choose with.
08:22:54.340 - 08:23:38.512, Speaker A: But what we're working on is making it so that you can plug in kind of different components from different clients and make it so you can really increase the client diversity. So in this example here, you could take the transaction pool from Rest or the mempool from Rest. You could use Aragon's RPC, which is notably very well done. And then if you just wanted to use like the standard Geth EVM, you could totally do that. At the same time, you could just choose to use all of them, which know, if you really like Rust, for instance, you could do it this way. Lastly, we also engineered something that we call the Pre Compiled Development Kit, which allows people to write these extensions to the EVM or these precompiles in a way that's really, really friendly. Traditionally, if you look at this precompile example from Geth, you have to write these really convoluted, simple kind of structs that are really kind of hard to use.
08:23:38.512 - 08:24:13.224, Speaker A: You take input bytes. The return type is bytes. And overall it's just a very kind of raw experience which makes it really difficult for developers to add functionality to their app chain or to their layer one or two. But what we did is we made it. So that you are trying to get towards the concept of writing. A pre compile should be as familiar as possible to someone who is solidity or viper native. So if you look on the left here, this is for the Cosmos Staking module we have like a Staking sol which is effectively getting your validators delegating, et cetera, et cetera and we're actually able to map those types directly to ghost trucks using reflection, et cetera.
08:24:13.224 - 08:25:18.460, Speaker A: So if you look here, comma address is the address which adheres to kind of what we see in solidity return types being big int is like a UN 256, et cetera, et cetera. And then what we do is we utilize that same concept of the message passing. So when we kind of see a precompile and we call it, we're able to use the message dispatcher to pass that over to the pre compile manager which can call the corresponding Cosmos module, which is really nice from that developer user experience perspective. So at the end of the day we can have all this cool tech but what do the advancements actually enable? So the first thing that I think is really cool is it allows for in the first time in Cosmos to have genuine client diversity. On Barachain, for instance, we route all of our transactions to the EVM and by proxy all of them will be utilizing different clients depending on what execution client that validator or node is running. This is really good for exploits, it's just generally good practice to have and overall it is the first time in Cosmos that we've seen this which we think is a really really exciting addition. Lastly, it allows for some really interesting things to happen at the base layer.
08:25:18.460 - 08:26:17.152, Speaker A: You've created a nice way to not only write application logic but also introduce that application logic into the base layer and create really cool things. It allowed us to write some of our DeFi logic that we use in our proof of liquidity system on bare chain and tie that into ABCI in a really native way. And it allowed us to kind of start building some of these interesting products like Flashbarra Slinky, which is the Oracle that I mentioned during the panel, and then things like integrating rollkit into polaris, IBC into polaris, et cetera. So earlier today I did a workshop with Diego and Josh from Celestia that was basically showing some of the modularity of polaris and really showcasing how truly EVM equivalent it is. So we had the Celestia DA DevNet running that's being used for data availability and consensus. Then we had a Polaris SDK chain that was integrated with Rollget that was rolling up to the DA. And then lastly, to really show off that EVM compatibility, we were able to deploy an Op Stack chain that's utilizing that Polaris chain for its settlement.
08:26:17.152 - 08:26:38.750, Speaker A: Which really goes to show how we truly have like an EVM equivalent chain not only from an execution perspective but RPCs and everything all the way down as the Op Stack deployment stuff is notoriously a little finicky and is a really good test to see where we are at. And that's everything I have. Thank you guys for coming. It.
08:26:46.400 - 08:26:52.604, Speaker B: Question enabling client diversity for each, right?
08:26:52.802 - 08:27:06.464, Speaker A: So it would be for any polaris chain? So it would be enabling like a chain that is running in polaris that has 100 validators to create client diversity within their chain themselves. So it would allow for the EVM? Yeah, for the EVM.
08:27:06.512 - 08:27:17.120, Speaker B: Right. Okay. Is polaris like a consensus middle layer?
08:27:17.280 - 08:27:29.656, Speaker A: So the way you think about it is it allows for any cosmos chain to integrate to EVM really easily. So because it's completely separated, all you have to do is kind of import this module and it allows it to.
08:27:29.678 - 08:27:31.032, Speaker B: Talk to the module.
08:27:31.176 - 08:27:33.740, Speaker A: Yeah. Allows it to talk to an execution client.
08:27:34.480 - 08:27:41.692, Speaker B: So if an existing app chain that does not have it yet, they could plug it in and upgrade?
08:27:41.756 - 08:27:43.008, Speaker A: Yes, 100%.
08:27:43.094 - 08:27:49.776, Speaker B: Oh, cool. That's useful. Thanks.
08:27:49.878 - 08:28:39.280, Speaker A: Thank you. Hey, is the idea behind something like a Flashberra to essentially have like proposer builder separation for polaris chains or is it more like the current version of like MEB boost kind of thing? Yeah. So right now the current implementation of Flashberra is built on top of Skip's Pob. So it's basically taking Skip's Pob all of their very cosmo specific logic and adapting it over to integrate into Flashbots. So it'll be not Flashbots as in what we see on Mainnet, but the Flashbots interfaces. And what that allows us to do is it allows for searchers that are running their bots on mainnet or allows them to integrate their existing code into a polaris chain. Again with the concept of reducing friction for bringing ethereum ecosystem devs into cosmos because they don't have to go rewrite all their tooling, et cetera.
08:28:39.280 - 08:30:28.070, Speaker A: Nice. Thank you. Sweet. Awesome. Thank you. Sam. Sam.
08:30:28.070 - 08:37:08.010, Speaker A: Jam don't. Sam. It's Sam. Sam. It's Sam. It's Sam.
