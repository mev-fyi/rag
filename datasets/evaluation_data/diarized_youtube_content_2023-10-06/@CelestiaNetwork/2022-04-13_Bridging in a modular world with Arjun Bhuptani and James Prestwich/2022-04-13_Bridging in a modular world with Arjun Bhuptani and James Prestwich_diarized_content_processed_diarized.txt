00:00:08.010 - 00:00:23.200, Speaker A: Because I wanted to begin with introductions and Arjun, you are our guest. And so why don't you quickly introduce yourself, tell us a little bit more about connect, and then we'll move to the others.
00:00:24.610 - 00:00:42.166, Speaker B: Hi. Yeah, absolutely. I'm Arjun. I'm one of the founders and project lead of Connect. Let's see. I've been in this space since 2016. I became interested in the space because I wanted to find ways to build public goods that are both non sovereign and non corporate.
00:00:42.166 - 00:01:31.222, Speaker B: I think it's really important for the world if you can have some types of things that exist, sort of like the Internet itself, that exist, that are accessible by anybody and that are really untamperable by any specific entity. I think that's really how we get to a world that makes sense in light of things like globalization. I was always really interested in infrastructure. So I was working on infrastructure startups in the space in 2016 and 2017. And then in May of 2017, I started connects with my co founders, Landon Rahul. And the goal was always this technology. Ethereum specifically at the time, is extremely important, and let's figure out ways to get it into people's hands faster.
00:01:31.222 - 00:02:21.398, Speaker B: And so, pretty quickly that led us down the path of scalability, because that was really the biggest bottleneck for everybody. And we became scalability researchers. In early 2018, we built one of the first ever l two s on Ethereum. This was a state channel network in partnership with Spank chain, because spank chain was the only place that actually had any traction in the early days of the Ethereum community. And then we ended up continuing to do research alongside the kind of core scalability community. That's how I met John, for instance. And then as that community started to focus more and more on rollups, we became interested in interoperability, basically communicating between these different roll ups and between potentially other sovereign chains, finding ways to make that experience seamless.
00:02:21.398 - 00:02:29.420, Speaker B: If we're moving execution into many parallelized environments, which is actually a really good segue into this talk.
00:02:31.410 - 00:02:37.280, Speaker A: Awesome. And then from our side, we have John and Mustafa. Mustafa, please introduce yourself. And then John.
00:02:37.970 - 00:03:27.920, Speaker C: Hello. I'm the co founder and CEO of Celestial Apps, which is the first modular blockchain network. And the basic idea is that we're building a modular consensus and data network to enable anyone to very easily deploy their own blockchain with very minimal overhead, without them having to worry about deploying their own consensus network. And they would do so. They can do that by deploying a roll up, or more specifically, a sovereign roll up on top of celestial so I've been thinking quite a lot about bridging in a multi chain world. And I've written some things about this topic and this idea of trust minimized bridging and also committee based bridging. So I'm looking forward to have discussed these topics more in this space.
00:03:30.940 - 00:03:31.492, Speaker D: Sweet.
00:03:31.556 - 00:03:32.170, Speaker B: John.
00:03:32.940 - 00:03:52.770, Speaker E: Hello everyone. My name is John, also co founder of Celestial Labs and the chief research officer. So I generally do protocol research and specification. Previously I was at consensus doing general layer two scalability research where I met a lot of the fine folks that are on this call today.
00:03:56.100 - 00:03:56.656, Speaker B: Great.
00:03:56.758 - 00:04:31.976, Speaker A: And we'll introduce James when he joins. In the meantime, I will set the stage for the conversation. And let's get right into it. So if you are listening to this twitter space, you know something about where the world is going. You can likely see that we are in the midst of a complete tectonic shift in blockchain infrastructure. Much of this innovation is driven by deep frustrations with monolithic chains in a single word, that change can be summed up in the word modular. And many are saying that modular is now a movement where we're seeing early signs of a new culture, values and even customs.
00:04:31.976 - 00:05:05.220, Speaker A: Lately, what has caught my interest is the intersection of interoperability and modular blockchains. I found myself asking the question, what will the future of secure bridging and cross communication actually look and feel like in the modular world? Gents, I'm intensely curious about the changes that are going on right now at this exact moment in time in regards to that intersection that I just described. Specifically, I want to go deep on the trends and forces at play that are obvious and not so obvious. And I want to cover a wide range of topics.
00:05:05.380 - 00:05:06.984, Speaker C: For example, I want to go into.
00:05:07.022 - 00:05:48.340, Speaker A: Clusters, optimistic bridging, token fragmentation, bridge failure. We'll talk a little bit about modular interoperability, systemic risks, the whole nine yards. And what better way to just explore all this stuff through a true meeting of the minds with Arjun from connect, James from Nomad, and then Mustafa and John. Arjun, on the 24 January, you and Nomad announced a strategic partnership introducing the modular interoperability stack. Why don't we start off with you? Can you introduce the concept of modular interoperability and how it fits into the modular blockchain thesis?
00:05:49.000 - 00:06:33.940, Speaker B: Yeah, absolutely. So before I do that, let me just give a really high level background on what connect is right now and then what Nomad is right now and then how those two things really fit together. So Kinext at the moment is a liquidity network, cross chain liquidity network. Basically what this means is that we have a network of lps called routers, that hold pools of funds on different chains. And using a technique that is very similar to atomic swaps, allow users to swap kind of like one for one assets across chain. From a user experience perspective, this doesn't look like a swap, it just looks like a transfer of funds. But the goal is that the user ends up with the right asset on the right destination chain for fairly minimal amount of fees and almost no trust.
00:06:33.940 - 00:07:30.324, Speaker B: And this is a model that has been explored for a very long time. We have optimized the shit out of it, but it is something that we've understood the trust considerations around for quite a while. Nomad, on the other hand, is a completely novel mechanism for interoperability, and it's specifically targeted around more generalized message passing. So what we do with connect, which is sort of more of like an atomic swap style thing, what we call like a locally verified mechanism for bridging that mechanism is trust minimized. And it's easy to deploy places, but we can't really use it for arbitrary data passing, only some kinds of data passing. Whereas Nomad, on the other hand, is the other end of the spectrum, where it is specifically designed to allow for passing any kind of message between chains. So allowing for one contract to read the state of another contract, for instance, which is a really important and difficult problem.
00:07:30.324 - 00:08:48.568, Speaker B: Now, the way that, historically, the way that this has been done is by using something called one of two ways, either through light client header verification, like IBC, where you have one chain actually running a light client of another chain. And of course, this is difficult to do because it's custom to each chain. And each time you have a new consensus model or anything like that, you have to come up with an entirely new strategy for how to do it, or through what is pretty much the most dominant paradigm right now in interop, which is using an external set of verifies, so using like a POS network or an MPC network, or in layer zero's case, like two POS networks. But the idea is you have some external set of parties that is responsible for relaying the data across chains and verifying it. Now, what Nomad does kind of differently is that it says, instead of just trusting some set of third parties to relay this data across chains, it uses an optimistic pattern, similar to an optimistic roll up, where you relay the data across chains optimistically, and then you have a period of 30 minutes within which people can dispute. And if they dispute and find that fraud has actually occurred when relaying that data. Then the entity that signed off on relaying the data, the updater in Nomad system, ends up getting slashed.
00:08:48.568 - 00:09:43.516, Speaker B: So the security model is more similar to that of something like a roll up than to something like a side chain, which is true for most of the other kind of externally verified mechanisms. Now, the trade off for Nomad, there are always trade offs. The trade off for Nomad is latency, right? You have this 30 minutes window within which you can't really do anything with that data because you can't really trust that it's accurate. On the other hand, you have connects where these transactions are near instant, but they have limited functionality. You have a limited ability to pass around data between chains. So our thesis around module interoperability is that similar to the modular blockchain stack where you can take different parts of the system, different kind of pieces of what make blockchains important and what make them work and separate them out into their own independent networks. So separate out data, separate out execution.
00:09:43.516 - 00:10:32.640, Speaker B: And by having all of these things be built in a modular way, you can kind of offset the trade offs of trying to build this single monolithic blockchain that has this really complicated trade off space around the scalability trilemma that Vitalik has talked about. And similar to that, we think we can do the exact same thing with interoperability, where we use Nomad as the core message passing layer to pass data between chains. And then you use connect as the liquidity layer. That gives you the ability to get the right kinds of assets on the receiving chain. While Nomad messages would have historically taken 30 minutes, you're able to short circuit them in any user facing case and make sure that they happen almost instantly through connect. But you still end up getting the core security guarantees of Nomad and really, by extension of the underlying chains.
00:10:34.660 - 00:10:52.170, Speaker A: Thanks. I want to kick it over to this last year. Guys, guys react to like, tell me about how you feel about this modular interoperability thing that Arjun is talking about. Any trends and forces at play that we'll have to take into consideration or things that we cannot ignore, but just give me your general points of.
00:10:54.540 - 00:12:10.530, Speaker C: So I think, I think one thing that might be helpful to the listeners here is to kind of explain this difference between what connects and nomad they're trying to achieve, in the sense that there's this idea of a lock and mint protocol and a swap protocol. And correct me if I'm wrong, Arjun, but so, like with Kinex, you have instant transfers, right? Because you can swap tokens. So with connects, you can basically swap tokens that already exist on two chains. If you have two chains like chain a and B, and some tokens exist on there, you can basically swap those tokens instantly. Yes. But then the question is, like, what if you want to mint new tokens into that chain? So let's say you have a bunch of tokens on chain A and you want to move them to chain b, but there isn't any existing tokens of your type on chain b, so you can't just swap something on chain B. To get to move those tokens to chain B, you have to actually move them to chain B, like mint new of those tokens on chain B.
00:12:10.530 - 00:12:41.960, Speaker C: And that's referred to as lock and mint protocol because you have to lock those tokens on chain a and then mint them on chain b, and you're basically moving them to chain b rather than swapping them for existing tokens. And as far as I can understand, that's where Nomad fits in, and that's where you have this 30 minutes latency because you'd only use Nomad if there's not enough liquidity on both chains so that you can just swap those tokens instantly.
00:12:43.260 - 00:12:46.270, Speaker B: Exactly. Sorry, go ahead.
00:12:46.720 - 00:12:48.030, Speaker C: Sorry, go ahead.
00:12:49.520 - 00:12:51.630, Speaker B: No, definitely finish your thought.
00:12:52.080 - 00:14:13.780, Speaker C: Yeah, so I was saying that actually fits in really well with the kind of like a roll up centric or modular blockchain stack. Because with roll ups, the way I see that roll up bridging working in the future, at the moment, there's quite a lot of people critical of optimistic roll ups, for example, because they have this one week withdrawal period. For example, if you withdraw the tokens to Ethereum, if you want to withdraw tokens to Ethereum from a roll up, you have to wait a one week fraud proof period. But you don't actually have to do that if you just swap the tokens. A naive way of doing that would just be like deposit the tokens to an exchange, swap them for the tokens, and then withdraw them to the chain that you want. Using the exchanges withdrawal feature and connects is kind of doing that, but in a decentralized and more trust minimized way by having a cryptographic swapping protocol, which basically does the same thing as if, let's say you want to move Ethereum to polygon instead of using official polygon bridge, you can deposit Ethereum into Coinbase, or maybe not coinbase. I don't know if Coinbase supports polygon, but let's suppose it did you deposit the ETH to Coinbase and then you withdraw it from Coinbase to the polygon chain.
00:14:13.780 - 00:15:07.320, Speaker C: So that's basically what connects does, but in a decentralized way. And so I see with roll ups and with multi chain bridging in the future, the way that I see it working is that you have one official or enshrined lock and mint bridge, where you move the tokens across those assets that is completely trust minimized, that has a high latency period, might be expensive to use, but only liquidity providers would use that bridge. Like the actual users themselves don't actually swap tokens to their official bridge, but just use atomic swap between chain a and chain b, rather than going through the official bridge, which only should be used by liquidity providers and whales, effectively.
00:15:08.300 - 00:15:51.952, Speaker B: Exactly. Yeah. And I'm really glad that you brought up this example of tokens and token transfers and token minting, because that is actually the simplest, most quintessential example of the kind of domain space that connects and nomad occupy where with connect. The fundamental assumption, again, in the token transfer case specifically, the fundamental assumption is that some liquidity exists on the receiving chain. You have to have some minted representation. So if you're transferring, if you're on polygon and you want to get to XYZ chain, and XYZ chain doesn't have a USDC representation, there's really no way to use Kinex to swap USDC into that chain. We need somebody to mint those assets first.
00:15:51.952 - 00:16:51.896, Speaker B: And that is really where Nomad comes in, is that Nomad has the capacity to, with minimal trust, mint representative assets on a chain where they don't exist. Know, from our perspective, for instance, with Moonbeam, this was a core part of how we've collaborated, is that exactly, as you said, liquidity providers, market makers, other institutional people who are trying to mint ten, $20 million of liquidity at a time will do that through Nomad. They'll migrate that liquidity from Ethereum or from some other chain to moonbeam. And then now that liquidity is available, some of it can end up in connect and that can be swapped over for users. And so the users still get this less than two minute turnaround time, very cheap swap that can go from any other chain, but you still have this safe mechanism to mint tokens. Now, it definitely gets a bit more complicated than that. I think we'll certainly touch on inter and intracluster communication in the future at some point in this space.
00:16:51.896 - 00:17:31.590, Speaker B: And I think there's a lot of really interesting things that we can do there around Nomad may be quite possibly the best inter cluster communication option that there is. But at the same time, there are certainly instances where you may want to use something more native for the intracluster piece. So use a roll up Amb in order to achieve the best possible security guarantees. And then the other dimension along which it gets more complicated is if you're doing something that's more complex than simply transferring tokens. And that is entirely possible with Nomad. And in certain cases, it's also possible with. So, you know, those pieces we can kind of dig into in the future, as.
00:17:32.300 - 00:17:35.140, Speaker A: James, welcome. Do we have your audio?
00:17:35.220 - 00:17:36.330, Speaker D: I made it.
00:17:36.860 - 00:17:43.960, Speaker A: You made it. Okay, cool. Real quickly, introduce yourself, James, and then give us one or two lines on Nomad.
00:17:44.300 - 00:18:22.280, Speaker D: Sure. I'm James Prestwich, one of the co founders of Nomad. I've been working on cross chain communication and bridges for probably four and a half years now. I started off in cross chain atomic transactions with bitcoin back in 2017. So Nomad is a new cross chain protocol that leverages some of the techniques from optimistic systems in cross domain communication. So you get cheaper, you get better throughput at the expense of some latency.
00:18:24.940 - 00:18:49.010, Speaker A: Awesome. We had opened up with the concept of modular interoperability. I mentioned the announcement on January 24 where connect and Nomad formed this D partnership, and I asked, how does the concept of modular interoperability fit into the broader modular blockchain thesis? And so that's where we're dialoguing on now. John, we'll kick it over to you. Your point of view.
00:18:49.940 - 00:20:24.270, Speaker E: I think it's a particularly important subject because as we've seen from the rise of various new blockchains in the past couple years, things like Solana, for example, there is a need and a demand for systems beyond just a single chain with a single virtual machine, with a single set of constraints and limitations. There's demand for exploring the trade off space, not just in terms of security and decentralization, like maximalists would have you believe, but the trade off space in terms of things like complexity, in terms of novelty and so on. There's a large trade off space that doesn't necessarily involve harming the user's security or harming this notion of decentralization or sovereignty, while still being different than a single monolithic chain. And we are moving towards a world that is increasingly multi chain that involve multiple blockchains. Ideally, we want these blockchains to share security, for example, if they use a common consensus and data availability layer, such as celestia or some other mechanism, and these chains need some mechanism of communicating with each other. And this is where cross chain communication protocols such as what connects the nomadder building are going to be very important in this future.
00:20:26.420 - 00:21:04.010, Speaker D: James, one of the things I love about this is that what people don't realize is that multi chain systems have always been and will always be the only scaling roadmap. Ethereum called them shards. We have Cosmos zones, we have polkadot parachains. But it's all different flavors of multi chain systems. When we talk about cross chain and cross domain communication, this isn't something new. This has been an inevitable consequence of the Ethereum roadmap from the.
00:21:10.700 - 00:21:53.160, Speaker C: Like in the Solana ecosystem. Like Anatoly's view is basically like we should have a single synchronous world computer blockchain that processes transactions synchronously. But I think, like anyone who's kind of built a large scale system before will know that's very difficult to do. If you look at the Internet itself, the web itself is not a synchronous system. It's a bunch of web servers communicating with each other asynchronously. And I think we will end up with a similar kind of architecture for web three as well, a set of blockchains communicating with each other asynchronously.
00:21:53.760 - 00:21:55.020, Speaker B: Does anyone disagree.
00:21:57.600 - 00:22:02.380, Speaker A: I was going to ask, does anyone disagree with what Mustafa said, or does anyone see it differently?
00:22:05.040 - 00:22:40.730, Speaker B: We agree to the extent that, and I think James and I have talked about this, that effectively what connects plus Nomad is really offering is asynchronous. Not really as a service, but really just asynchrony to this world. Like async comms to this world that fundamentally doesn't exist yet. It's going to be completely new paradigm. People are going to have to completely change the way that they think about things in the same way that we have to completely change development paradigms to be able to build web applications. We will have to do the same thing in blockchain development, which is certainly going to be a giant pain in the ass.
00:22:42.060 - 00:22:43.850, Speaker A: James, I think you were saying something.
00:22:44.220 - 00:23:21.750, Speaker D: Yeah, well, I was mostly agreeing and adding colors. The web is not a synchronous system and it's certainly not a homogeneous system. The Internet is composed of thousands and thousands of independent networks with slightly different software and slightly different semantics and network address translators and all of these messy systems just to resolve the fact that the web is not synchronous, it's not homogeneous. You need cross domain communication in the Internet, and that happens ten times every time you make a web request without you thinking about it.
00:23:24.520 - 00:23:43.900, Speaker A: Question for the group. Celestia and roll ups are examples of a modular blockchain protocols, while Nomad and Kinext are examples of modular cross chain communication protocols. Why do you think that modularity as a design principle has emerged in both of these contexts? And do you see this emerging in other areas of the blockchain stack?
00:23:47.760 - 00:23:51.356, Speaker B: Can any of us go ahead?
00:23:51.378 - 00:25:16.060, Speaker C: Vasaka well, I think there's several elements there. I think Arjun also kind of elaborated on that. Well, by talking about the trade offs from a cross chain perspective. Like every, like every cross chain mechanism has obviously, like in the case, like the atomic swap protocol doesn't work if there's no liquidity, and that's why you need Nomad if there's liquidity. But the trade off there is like Nomad has higher latency. So in that kind of world, it's a way to kind of beat the trade offs by having kind of the best of both worlds in a way, in the sense that because the protocol is modular, in the sense that it supports multiple cross chain mechanisms depending on the use case or what's needed at the, like, the user kind of effectively has more flexibility over the use cases that it needs. And I think from a Celestia perspective, that's also similar in the sense that Celestia decouples consensus from execution, and it allows developers to define their own execution environments.
00:25:16.060 - 00:25:44.580, Speaker C: And different execution environments have different trade offs. So the beauty about it is that because Celestia doesn't enshrine or enforce a specific execution environment for you, developer has the freedom to choose whatever execution environment they want based on their use case and what trade offs they need, like whether they want to use EVM, for example, or use the cosmos SDK or some other, or fuel or some other execution environment.
00:25:48.200 - 00:26:18.370, Speaker D: I think to that point, modularity is kind of an inevitable response to these unavoidable trade offs we see in consensus systems, in cross chain communication systems, the response is rather than choosing one point in the trade off space and planting a flag there and telling everyone they should come join it, we try to design these systems so that users can move throughout the trade off space based on their use case.
00:26:21.220 - 00:27:42.324, Speaker B: I think also, just to echo James's point from earlier, this is something, modularity is a term that we have started to use in the last year and a half around describing these different systems. But fundamentally, what this is is just like layered architecture for distributed systems. That's something that we've sort of understood for a very long time that when building networks, this is true on the Internet as well. By the way, when building networks, it makes sense to have each protocol or each level of the stack be hyper specialized into doing one specific kind of activity, rather than trying to build a network that tries to do everything. And the reason for this is that it's extremely difficult to constrain risk if you have a lot of like for instance, if TCP IP was also handling network address translation, if it was also handling some of the higher level functionalities of things like HTP requests, you would end up with just a much larger surface area that you have to think about in order to do things with the protocol. And in fact, you would also end up with a lot of this extra stuff that you're not doing anything with most of the time. And I think that's sort of what exists with the whole modular blockchain thesis and the whole modular interoperability thesis, is that fundamentally what we're doing is we're separating everything out to layers.
00:27:42.324 - 00:28:31.412, Speaker B: We're saying the settlement layer is responsible for economic security. We're saying the data availability layer is responsible for storing data. We're saying the execution layer is responsible for executing in a virtual environment, and then translating that into proofs that can be stored in a data availability layer. And by separating out the responsibilities of each of these different pieces of the puzzle, we are not only constraining the risk that anything goes wrong, but we are also limiting the need for each of these pieces to always be used. And this is really where the scalability piece comes in. I know this is something that John has talked about a bunch, which is that you don't actually need to have a general purpose VM, the full stack general purpose VM and chain running every single type of thing you should. And in the future, we will hyper optimize vms to be specific to certain kinds of activities.
00:28:31.412 - 00:28:53.330, Speaker B: And as long as the data layer itself and the settlement layer itself are the same, you can do that freely with minimal risk and still have the same level of security. And that is how you eke out the best performance, right, is allowing people at higher levels to optimize without affecting any of the core properties of the system, and without having to rebuild an entire system.
00:28:58.370 - 00:29:00.800, Speaker A: John, I want to give you an opportunity to weigh in.
00:29:03.990 - 00:29:08.610, Speaker E: Could you repeat the question to refresh the listeners memories?
00:29:09.110 - 00:29:27.690, Speaker A: Yeah. Celestia and rollups are examples of modular blockchain protocols, while Nomad and Kinext are examples of modular cross chain communication protocols. Why do you think that modularity as a design principle has emerged in both of these contexts? Do you see this emerging in other areas of the blockchain stack?
00:29:29.070 - 00:29:29.434, Speaker C: Yeah.
00:29:29.472 - 00:31:06.506, Speaker E: So I would say that before the separation of the actual blockchain process kind of became popular into what we currently see today as being in the modular blockchain stack, you have your consensus layer, data availability layer, your execution layer, and your settlement layer, potentially wrapping some or all of them together into different layers. There were a number of projects that did modularity at the implementation level. So an example of this was the cosmos SDK and just the entire tendermint and Cosmos development ecosystem, where essentially they black boxed the consensus process and they made specifying the execution semantics modular. They allowed you to define your own execution semantics, bring your own VM, essentially, and consensus would be provided. There's other projects that are also doing something similar. Avalanche and subnets, for instance, is essentially copying the Cosmos model, where they have consensus stack, their avalanche consensus stack, and then they're providing some, for now, relatively primitive, but I guess they're in development mechanisms for actually developing your own virtual machine. And they have a few currently application specific vms that are being played around with.
00:31:06.506 - 00:32:10.186, Speaker E: I think they're launching some new Ethereum EVM based vms with some tweaks and so on. But that's an example where the implementation, rather than the fundamental design, was built around a kind of modular design principles. And with the rise of the modular blockchain stack, we've kind of extended this notion not just to the implantation, but fundamentally to the design. And this kind of takes the benefits off the modular implantation and makes them even better. And it takes them even more to an extreme. And what are those benefits? Why would the Cosmos SDK be designed like this? It's because when you make something modular, it means that whoever uses that system or whoever builds a system doesn't have to know and care about everything. They can only care about a subset of the features.
00:32:10.186 - 00:32:51.980, Speaker E: And therefore it allows specialization. And as we know, specialization is one way to really eke out maximum performance or maximum anything, really. It's just an amazing way to min Max, because if someone has to keep in their head how an entire blockchain stack works, that's just fundamentally not scalable, right? I don't think there's anyone in the world today that knows how the entirety of a go Ethereum node works. It's just not humanly possible. It's just too complex. But it is possible to know, for instance, how the EVM works. And not care about the peer to peer networking, not care about the consensus protocol, any of those edge cases or anything like that.
00:32:51.980 - 00:33:13.250, Speaker E: Many people know entirely how the EVM works. So this allowing specialization allows us to get maximum performance. And this is why I think we've seen the rise of modularity, not just at the design layer, but also even at the implementation layer.
00:33:15.670 - 00:33:34.280, Speaker A: I want to move towards clusters. What are the biggest security differences between inner cluster bridging versus intracluster bridging? And how do we see that changing in the next twelve months, 24 months? Mustafa, why don't we go with you?
00:33:35.710 - 00:34:21.362, Speaker C: Yeah. So this idea of clusters was set out in a blog post I released in October, where I was trying to kind of make sense and categorize the communications in the multi chain ecosystem landscape. And the basic idea is there isn't really such thing as layer one or layer two or layer three. All there, it really is, is blockchains that bridge to each other. Conceptually, layer one and layer two aren't really useful terms nowadays. It's more useful to think about what are the trust assumptions between different blockchains. Because layer two is basically a blockchain, layer one is basically a blockchain.
00:34:21.362 - 00:35:25.530, Speaker C: But why it's called layer two and what differentiates it is how they bridge with each other and the trust assumptions used by that bridge. So I came up with this idea of intercluster and intracluster communication. I defined communication between two chains into two categories. The first category is this idea of committee based security. And that's kind of like the more traditional bridging mechanisms you would use. For example, like the bridge between Ethereum and Solana that uses wormhole, for example, that requires a committee and requires you to trust a committee to not steal your funds. Because effectively it's the wormhole bridge committee that signs messages with that bridge to and from Solana and Ethereum.
00:35:25.530 - 00:36:15.398, Speaker C: And that committee could lie about to Ethereum, for example, about how many funds there are in Solana, or what your balances are on Solana and vice versa. It could theoretically steal all the ETH in that bridge. Same with Polygon, right? The Ethereum to Polygon Bridge, that's also a committee based bridge, if I remember correctly. There's like a multi sig of a few people that if they're compromised, all the ETH from polygon could potentially be compromised. And that's why polygon is not usually referred to as layer two to Ethereum. It's kind of referred to like, polygon is just another layer one. Even though Polygon, for example, the value proposition of polygon is that it helps to scale Ethereum.
00:36:15.398 - 00:37:09.550, Speaker C: All polygon really is, it's just another blockchain. It's a fork of Ethereum with tendermint on top of it, consensus with a kind of like committee based bridge. So that's committee based security. And then you have this idea of trust minimized bridging, and that's where roll ups come into play. So with roll up chains, if you deploy a roll up chain on Ethereum, for example, and you have a bridge between that roll up chain and Ethereum, then that's what I would describe as the trust minimized bridge. Because even if the operator and the nodes that operate that roll up misbehave and go completely rogue, then in theory they can't steal your funds. And the reason for that is because the Ethereum chain runs light client as a smart contract for the roll up.
00:37:09.550 - 00:38:07.850, Speaker C: And if your roll up chain has a malicious block producer that produces invalid blocks, then that can be proven using either a fraud proof or a ZK proof to prove that the block was valid. This is what I call chains that communicate with each other using trust minimized assumption. I call that as an intracluster communication mechanism. And I call chains that communicate with each other using a committee based assumption as an intercluster communication. And this is kind of named after the Internet and intranet. So with an intranet, it's basically communication across your local network. So if you have an office and all the computers in the office are connected to the same local network, to the same router, they can communicate with each other using the intranet.
00:38:07.850 - 00:38:25.830, Speaker C: But to communicate to other computers that are not in the same office, for example, or that are not in the same local network, you would use the Internet. And so I kind of applied that concept to the multi chain ecosystem, and you have intracluster and intercluster communication.
00:38:29.610 - 00:38:38.620, Speaker A: Good overview. Would anyone else like to add on this topic of clusters, interverse, intra, and the changes that you expect to see?
00:38:39.150 - 00:40:09.810, Speaker D: Yeah, definitely. Back in 2017, 2018, when people were first investigating and talking about these kinds of things, we came up with a terminology hierarchy, which was essentially you have what Mustafa would call committee based, which is usually a multi sig, and you have the intracluster, the shared security, which we referred to as usually dependent or merged consensus, where the secondary chain is dependent on the primary chain. So an example of this is roll ups, where the roll ups consensus security is dependent on the l one, and the l one is running a fully verifying roll up client under some assumptions. And in the middle is where we would put things like IBC, which is based on a relayed light verification. IBC relays don't do full state verification of the remote chain, they only do header chain verification. So I think in Mustafa's terminology, the relay and the dependent consensus would both fall as intracluster, while the multi Sig or committee based would be inter cluster.
00:40:12.730 - 00:40:50.530, Speaker C: So I would still define with committee based, I would still define light clients that only verify the headers, like light clients that verify the signatures in the headers. To do, to check that the block headers have consensus. I think that's still theoretically committee based, because the only difference is that the committee is simply the chain itself. But the reason why I make that distinction is because if you create a new Cosmos zone with one validator, it's potentially even less secure than a third party committee.
00:40:52.870 - 00:40:53.960, Speaker D: That's true.
00:40:55.210 - 00:40:56.120, Speaker A: Go ahead.
00:40:57.530 - 00:41:59.530, Speaker B: By the way, the impetus to this is just to kind of give a little bit more context for everybody who's listening to this. This is the same thing that was kind of talked about in Vitalik's cross chain multi chain post, where he was saying, fundamentally, there are limits to the security that you can achieve when you have two chains that have independent validator sets that are communicating with each other. And even if you built the most theoretically secure communication mechanism between each other, like IBC, for instance, the bridge security might be foolproof. But the fact that you have two independent chains means that you now are still adding another risk layer. And that risk layer is that one of the chains could get 51% attacked. You could force a fake update on that chain that then gets translated to fake funds being sent over the bridge onto the other chain. And so this is a way to kind of spoof malicious data and send it over a bridge and into a higher security environment like Ethereum.
00:41:59.530 - 00:42:54.186, Speaker B: And fundamentally, that is true. There is this fundamental limitation that exists when you have multiple different chains that have their own independent validators. This exists within Cosmos and IBC, when you create new cosmos zones. This wouldn't exist in the case of something like Ethereum and roll ups, or I guess, in any sort of situation where you have shared security, because in those cases, the risk that you're taking on is really the risk of the single validator set being attacked. Now, I think that there is a fundamental limitation there, but I don't necessarily agree with the Vitalik kind of conclusion, which is, okay, we should just not have multiple validator sets. We should have just one. We should just not have intercluster communication, because the reality is people are just going to do it, they're going to find ways to do it.
00:42:54.186 - 00:43:16.100, Speaker B: And even if those ways are completely trusted, because people are idiots, and also because the market wants it, I think you had a tweet about this where you were just like, it's man in the mountain energy. Clearly people are going to do it. USDC exists, WBTC exists. We're not going to avoid these things. Let's find ways to actually lean into it.
00:43:17.350 - 00:43:48.960, Speaker D: Fundamentally, you have to look at the market and say, people are using USDC, they're using WBTC. These are functionally committee based cross domain systems. And there's such incredible market demand for these committee based cross domain systems that you have to admit that people will always use them and there will always be huge amounts of value locked up in them. You can't have this mountain man single validator set for everything. The market has rejected that.
00:43:49.810 - 00:44:15.810, Speaker B: I have a spicy analogy for this, which is that sex is risky, it can give you stds, it can lead to pregnancy. But the solution is not to tell people don't have sex and be abstinent, because they're never going to listen to you, they're still going to have sex. The solution is to make contraceptives extremely easy to get. So we just need bridge condoms.
00:44:20.310 - 00:45:01.582, Speaker C: But I think the idea why. I think the logic of Vitalik saying we should not have a multi chain ecosystem, just a cross chain ecosystem. I think the kind of Ethereum roadmap expectation is that Ethereum will scale kind of like infinitely, to some extent, using this roll up centric roadmap. It will have very high data availability capacity, so you can support lots of roll ups onto there. But I think that's still basically an important part. Even if, theoretically speaking. Even if historically speaking, you had a chain like Ethereum that could support an infinite number of roll ups, an infinite number of data, so you don't need to spin up a new chain.
00:45:01.582 - 00:45:46.670, Speaker C: I still think that's kind of missing an important part of why we need the multi chain ecosystem. And there's several reasons for that. The most important, underrated reason is these different clusters of chains have different values, both from a technical perspective, but from also a social perspective. Maybe you don't want to be limited to the EVM as a Solomon layer. Maybe you want to have a paralyzable virtual machine, like Solana, for example. But also you might have different social values, like look at Ethereum and Ethereum, classic different values there. So I think either way we will have a multi chain.
00:45:49.650 - 00:46:37.570, Speaker B: So, mustaf, I definitely agree with you. By the way, I want to couch this in saying I agree with you, but I want to also play devil's advocate because this is definitely an important question and I think it gets asked a lot. And the question is, if the thesis of having the economic security associated with a settlement layer being as high as possible to resist sovereign corruption, to resist corporate corruption, if the thesis is you want to make that as secure as you can, why not just have a single settlement layer? Why not have, you could potentially have diversity at the execution layers and the data availability layer. But from an objective world outcome point of view, isn't it much better for security to say, let's just stack all of the security in one network of validators?
00:46:39.350 - 00:48:12.554, Speaker C: So that's theoretically possible as long as you are happy with the idea that the resource requirements to produce a block, or to participate in the production of blocks in that settlement layer could potentially be very high. So as a thought experiment, let's say you want to take Solana to extreme, because Solana's value, for example, is like to produce blocks in Solana, you just need very high resource requirements. And we're just going to make a web scale blockchain where you have unlimited resource requirements to produce blocks. How far would you take that? Let's say in theory you could take that to something that eventually looks like a MySQL database, like Web, two MySQL database, and you have something like Google scale, and you somehow figure out how to mercurialize that database into a single Merkel route. So it's basically a blockchain. The question there is, would the community be happy with the fact that the kind of requirements to produce blocks would be so high that you basically need to be like a corporation to produce those blocks. The answer might be yes, to some extent, because if you've looked at Vitalik's recent endgame post, his entire thesis, there is that you will have centralized block production, but decentralized verification using things like data availability, sampling and fraud proofs and SDK proofs.
00:48:12.554 - 00:48:31.350, Speaker C: But then the question is still how far do you want to take that to an extreme? Do you really want to take that to an extreme where only Google level companies can produce blocks? And if the answer is no, then you do need multiple settlement layers and a multi chain ecosystem.
00:48:34.570 - 00:49:56.580, Speaker D: I think at this point, we've basically all agreed that there will be a multichain ecosystem with multiple settlement layers. And I want to bring it back to talking about modularity in bridges, because that's fundamentally a very difficult thing to reconcile in cross chain communication. How do you deal with so many different chains, so many different execution environments, and so many different choices for cross domain communication? If we want to use this intracluster communication as much as possible, but we're forced to occasionally use intercluster communication, how do we resolve that for any application? The way we've been approaching this at Nomad is to completely abstract the communication mechanism from the application and to have any application be able to enroll any number of communication mechanisms. So the same logical token bridge application can use IBC or Nomad channels or whatever other interdomain communication mechanism is available. We're still in really early stages of rolling this out, and it's very complex in practice. I was wondering if anyone else had strong opinions on what standards in that should look.
00:50:02.870 - 00:50:20.300, Speaker C: Like. I know there's IBC, for example, and then there's also like Polkadot standard, but would you consider IBC as a standard, for example, as these packet format protocol, for example?
00:50:21.070 - 00:51:08.540, Speaker D: I like packet format protocols. I like as a developer, just getting some message in off the wire and deserializing it locally. I guess my question is like, we have to separate the packet protocol layer from the application layer. How does your token bridge handle these packets? And how many different channels can it get packets. So we need standards for enrolling channel, connecting the channel to the application, and passing packets from the channel to the application and handling the packet. And I'm still not sure what those look like in the long run. We're talking about modularity a lot.
00:51:08.540 - 00:51:14.890, Speaker D: This is what do you do at the layer boundary and what is the correct format for that interface?
00:51:16.990 - 00:51:48.642, Speaker C: So is the problem there, if I understand it correctly, like one where if you have different execution environments on your chains, you have to have different libraries to process those packets effectively and to, for example, deposit and withdraw assets compatible with those execution environments. Like with the EVM. Like for example, with the EVM you can have a solidity contract, or with Solana, you have a Solana contract. Am I understanding that correctly?
00:51:48.786 - 00:53:05.950, Speaker D: The question so suppose that you have a consider like EVM, Ethereum, Evmos and osmosis, for example. This is something that we think about today. You can connect Ethereum to Evmos with solidity on both ends. You can connect evmos to osmosis with IBC, hypothetically, you can connect osmosis directly to Ethereum with some other channel. How do you reconcile the unified application that you want moving tokens between those three chains and the different communication layers underneath them. Each of those three connections might have different semantics or different layout, and we need to connect them to essentially the same application on top, which might be expressed as a cosmos module or a solidity contract. So there's kind of this just interfacing between the application layer and the communication layer is very difficult and kind of not yet standardized.
00:53:08.290 - 00:54:04.980, Speaker C: Yeah, I guess the main difficulty there is that all these chains have different execution layers. For example, osmosis does not support generalizable smart contracts. Hypothetically speaking, if Ethereum, evmos, and osmosis all supported general EVM contracts, then you can probably have the same kind of communication between all of those chains if you just deploy the same smart contracts in solidity. So I think it sounds to me like the bottleneck there is, the fact that in osmosis, you've got a hard coded. You have to use IBC, for example, if you want to use official bridge, but there's no generalizable environment where you can deploy code into it.
00:54:06.150 - 00:55:20.010, Speaker D: Yeah, cosmosm soon, I hope. But for each communication channel, you have to build it in the local language, in the cosmosm, or in the EVM, and then for each application, you have to connect it to those channels in a predictable, understandable way, and you have to build the application so that it is abstract over many different potential channel types. This is a huge challenge, and we are kind of slow rolling our approach to it today. We have this very abstract handler pattern where some channel calls into the handler with a bunch of raw bytes that have already been authenticated, but we don't know whether that'll continue to work for IBC or for Polkadot's XCMP or for other communication patterns in the future. So we're still in very early stages of building out the modular bridge stack and getting the interfaces between all the different modules correct and usable.
00:55:23.150 - 00:56:23.982, Speaker B: Yeah, there's also a lot of other pieces, other open questions that exist around this, too. So we have the liquidity and messaging layers that are live and in production and kind of now being figured out how those will interface with each other. But there's also a lot of open pieces around, like, okay, I think you need some sort of gateway protocol that allows you to connect between nomad to other localized communication mechanisms like IBC. I think you probably need some sort of NAT protocol that allows you to translate addresses between these different systems. Nomad has a concept of chain agnostic domains, but we will need to translate those into localized addresses everywhere in order to actually execute anything to any destination contract. So these are the kinds of interesting challenges that remain where there is a similar to the Internet itself, there's a whole host of protocols like BGP and things like that, that really will kind of need corollaries in this space. We will need to have that kind of functionality exist, and we'll need to make sure that that functionality is simple, it's very well specified, and it's available for everybody.
00:56:23.982 - 00:56:44.420, Speaker B: These are open protocols that people can then start building against. So that we do end up in a world where there is a single path for users to get from contract on chain a to contract on chain b, even if those chains are in totally different clusters, and if you're having to go through a few different hops of messaging protocols in between to get there.
00:56:46.730 - 00:57:12.940, Speaker A: I'd like to transition a bit here. You guys are already somewhat discussing this. The question is, part of the value proposition for modular blockchains is that we can have many different execution environments running side by side. However, having a multitude of execution environments poses difficulties in bridging between those chains. How do we resolve this conflict? Is this where modular bridging protocols come into play?
00:57:16.830 - 00:58:44.380, Speaker D: Yeah, we're running right now, and I touched on this a little bit. For any good cross chain communication system, you have to implement it in the language both ends of the communication channel. So if you want to connect EVM to Solana, you have to write it in solidity and in Solana's rust, and you have to write the same application on both ends so that they can communicate together. Right now, we have planned ahead for that, but we haven't yet done a second implementation of the Nomad protocol. What we've planned ahead for is allowing extra space for addresses, because not all chains use 20 byte addresses like Ethereum, most of them use 32, putting as many things as possible in very agnostic, very general patterns. So, for example, we don't use ABI encoding because that's a solidity pattern, not a Solana or a near or a cosmos pattern, and making the interfaces as simple as possible. We kind of expect that as we spread out to more chains, we're going to run into more and more things that we neglected to consider in the initial design.
00:58:44.380 - 00:58:51.760, Speaker D: So, really excited about all of the fun bug fixing and resolution we have to do for the protocol in the future.
00:58:54.890 - 01:00:23.090, Speaker C: So I think one thing that might or might not help with this in the so optimism recently released this interactive fool proofing system called canon. And what they've done there is they've implemented a MIPS machine in the EVM, and then they've compiled like go code into MIPS. And so this MIPS machine kind of acts as like a virtual machine on the ethereum virtual machine. And you can compile arbitrary like go code and other code, potentially cosm WASM code or like WASM code into MIPS. In theory, you could potentially run any kind of execution environment on top of the EVM as a virtual machine, in the same way you could run a docker container with a different operating system on your own computer to run other applications that might not be supported on your machine, instead of having to necessarily rewrite the application in every single language or execution environment that needs to be supported. So that could potentially help in the future with preventing the need to write multiple code for bridging in multiple different execution environments.
01:00:24.150 - 01:00:56.060, Speaker D: Yeah, optimism's MIPS machine, kind of based on arbitrum's earlier WASM machine doing the same thing. These really open up a lot of possibilities for more direct, optimistic, full verification of remote chains in the EVM. I'm excited to see these productionized over the next, probably like two to five years. It's going to take a while, but they're going to do really cool things in the long run.
01:00:59.230 - 01:01:20.420, Speaker A: I want to begin taking q and a from the audience pretty soon here. So if you have a question, raise your hand. Before we take that first question, Arjun, I wanted to ask you something specific. Can you explain for the audience what is optimistic bridging and what makes it different for most other bridges that operate today? And why is kinext so bullish on?
01:01:24.650 - 01:02:13.814, Speaker B: I mean, I'm sure James probably going to give a better explanation than I can, but I'll try to give the best explanation that I can. And then James, let me know if there's anything that I missed or that is important that I've missed. So last year we wrote about something called the interoperability trilemma, which is that, at least with where the market was at the time, every single interoperability project that existed kind of fell into one of three categories. And that was either. It was basically came down to who is verifying the system, who is actually responsible for deciding what data gets relayed between chains, and how that data gets relayed. And there were three real options there. The first was native verification, which is the chain's own validator set, or the receiving chain's own validator set is responsible for validating the data that actually gets posted to it.
01:02:13.814 - 01:03:06.414, Speaker B: And there's two options here. There's things like XCMP, where the core validator set of Polkadot is responsible for validating the data that goes across chains. Or you have things like Cosmos and IBC, where the receiving chain validator set is like running a light client of the sending chain and then validating the block header of something that gets passed through it. Then you have externally verified systems which as I talked about before, are like multisig MPC systems. Layer Zero is another example of this, where really you just have like a third party set of validators doing this that has their completely own set of security trade offs that is different from either of the two chains. This one adds a lot of trust assumptions. In some cases it could be okay.
01:03:06.414 - 01:03:47.198, Speaker B: So one case where it could potentially be okay is if the security of that third party validator set is much better than the security of either of the chains. In that case, it's totally fine. But of course it can be hard to really figure that out for at least any of the larger clusters like Ethereum. You're realistically not going to end up in a situation where your interoperability protocol has better economic security than Ethereum does. It's just very highly unlikely at this stage. And then the last one is kind of what connects does currently, which is local verification. So instead of having this big end party problem where you have some set of people that are responsible for passing arbitrary data between chains, you do something like an atomic swap.
01:03:47.198 - 01:04:57.850, Speaker B: You isolate to two people, the two parties, and then you have them just communicate with each other. And taking a two party communication mechanism and making that trustless is something that we do actually understand quite well. And it is quite easy to make that as at least very reasonably, trust minimized more than most other options out there. Now, the assumption at the time when we wrote that post was that there was this trade off space that exists between these things, and you could really only have two out of the three core properties of trust minimization, generalizability, which means being able to pass around arbitrary data and what we called extensibility, which means the ability to take this core product and copy pasta across many different chains, which unfortunately with like client header relays, things like that. It's not very easy to do because you have to build custom implementations. The reason that we're really, really bullish about optimistic verification is it's the first approach to interoperability that is genuinely novel beyond the three prongs of the trilemma that we have explored in the past. We think it's interesting because fundamentally it changes the trade off space.
01:04:57.850 - 01:05:35.062, Speaker B: So that rather than having to choose between being trust minimized, being generalizable, or being extensible, you are able to actually get all three of those things fairly easily. And instead you choose this other trade off, which is latency. Now, latency is bad from a user perspective, but from an institutional perspective, or from a security perspective, or even from just an implementation design perspective, it's really not a huge problem. And latency is something that we can fix. Latency. The march of technology is very, very good at making things faster. We're very good at optimizing on speed and on cost.
01:05:35.062 - 01:06:18.254, Speaker B: We're very bad at optimizing on some of these other core properties like security, just through engineering work. And the reason that we're extremely bullish on the optimistic verification model is that in combination with the way that connects works currently, we can get pretty darn close to what looks like a perfect model. Of course, there are still instances where you'll have the 30 minutes nomad latency. But aside from that, aside from the rare instances where that happens, and most of those are like Dow facing use cases, institutional facing use cases. Aside from those cases, you have a system that actually gets the best of all worlds. I hope I explained that well. I can definitely answer more questions about it in the question section.
01:06:18.254 - 01:06:20.660, Speaker B: And also, James, if I missed anything, let me know.
01:06:22.410 - 01:06:42.858, Speaker A: I want to take Q-A-I see some cool names here. Justin Drake joined Angela Liu, Defi Frog. Shayram Kanan would love to field some questions, but we'll go with Seg as the first question.
01:06:42.944 - 01:06:44.620, Speaker D: Let's see if he can join here.
01:06:52.260 - 01:06:55.572, Speaker A: Seg, if you can hear me. Yeah, ask your question.
01:06:55.626 - 01:08:04.908, Speaker F: Yeah, it just takes a couple of seconds to connect. So for future attendees as well, I just had a general question for whoever wants to answer it about a more modular blockchain world. One was, I heard Arjun speak about the liquidity layer. Fragmented liquidity is probably going to be a pretty big issue in the future. And one solution that I can see potentially is to have an application specific chain or roll up, for example, on Celestia, that specifically deals with liquidity, that talks to all of the other rollups that are trying to do general purpose stuff. Do you see this as the solution to liquidity fragmentation? And also, do you see a world where you have more application specific blockchains than general purpose blockchains? Because I guess you can design a virtual machine that's more efficient for some applications than others? And do you see them talking to each other more than you just have a single dumb general purpose computer trying to process everything, kind of like cpu versus GPU type thing. Anyone who wants to answer can answer.
01:08:04.908 - 01:08:05.790, Speaker F: Thank you.
01:08:12.960 - 01:08:56.830, Speaker D: I don't see fragmentation going away anytime soon. There really aren't many good solutions to this. Adding new application specific AMM chains or stable swaps between them really just embeds the fragmentation deeper in the ecosystem. And adding new domains to try to resolve it just provides more routes for fragmentation to creep in. Fragmentation is a function of having many domains and many bridges between them, and I don't think that we can prevent that from happening or unspill the milk on this one.
01:09:00.000 - 01:09:39.396, Speaker B: Go ahead. I was going to say there is an interesting incentive though, which is that basically just to take a quick step back. So fragmentation exists because you have potentially five or six different bridges that are all connecting the same chain, producing their own flavored asset, and that nobody has really quite decided which asset is canonical. Nobody's been able to take the time to figure out which asset is canonical. The chains themselves are of course trying to be agnostic if they can, or in some cases they don't. And they say like, no, this is the right bridge. But even then these are open systems.
01:09:39.396 - 01:10:49.010, Speaker B: So you do end up having people coming in, deploying stuff anyway and giving you the wrong assets. And that's a giant pain for users. I agree that it's a problem, and it's going to be a problem. I think there is something that's kind of interesting though, which is that having a stable swap, the one piece of incentivization here, which is interesting, is that having a stable swap necessarily means, or even having some sort of swap chain necessarily means that you are going to have one bridge, which is the one that is canonical, ends up becoming, over time becoming the embedded version. Because if you have a stable swap, every single version of an asset except for the one that you're swapping into, is going to have slippage as a fundamental part of transferring. So the assumption is that if you're, again, obviously markets are messy, and it's unclear if and when this would happen. But the assumption is that if we are heading towards a world where everybody starts trying to optimize on price, everybody starts trying to optimize on slippage and user experience.
01:10:49.010 - 01:11:13.770, Speaker B: There is this really interesting thing, which is that you can go to asset issuers and say, hey, why don't you just explicitly tell everybody this is the officially supported asset, and if they use this version of the officially supported asset, they can transfer it between chains without any slippage at all. I think that's an interesting economic level lever. We're not sure yet whether it's going to work, but it is interesting.
01:11:15.260 - 01:12:30.320, Speaker D: One of the fun side effects of stable swaps is that when you have a stable swap between like three or four different fragmented versions of the same asset, typically that stable swap is going to incentivize creation of more fragments, first of all, to go into metapools, and it's going to incentivize the minting of each of those fragments at about the same rate. So your ideal stable swap pool with three fragments is going to be 33% of each one. So by introducing this additional mechanism to try to fix fragmentation, you're going to end up incentivizing higher tvl for all of the fragments, and you're going to end up with a huge stable swap pool that exists just to resolve the fragmentation issue that you kind of created with the stable swap pool in the first place. So as long as stable swaps are still incentivizing liquidity at the rate they are, I think this is going to remain true. And as long we can make money by launching a stable swap, I think we're going to see fragmentation get worse.
01:12:30.900 - 01:12:38.050, Speaker A: I'd like to quickly interject here for that question. Justin Drake, please go ahead and ask.
01:12:39.640 - 01:13:52.010, Speaker G: Oh, hi there. I was listening to the bankless episode today on Celestial with Nick, and I love the name Celestium. And I guess I have a couple of questions about celestiums. One is, would it be fair to say that the security of a celestium is kind of, at best, a degraded version of Celestia? And the reason being that when you have a settlement layer, for example Ethereum, that settlement layer can't do data availability sampling itself. And so basically you lose data sampling as a security layer for your celestium. I guess my second question is that Nick mentioned that the celestial validators can make attestations, then these attestations can go on the settlement layer, and there would be some form of slashing if there's some incorrect assessations. And I guess my question is, how does this slashing mechanism work?
01:13:53.660 - 01:15:44.220, Speaker C: Yeah, first of all, it's definitely the case that celestiums do not have the same data availability guarantees as roll ups for the reason that you stated, which is that the Ethereum chain cannot do data availability sampling of third party chains, including celestia. And so if you want Ethereum smart contract to check data availability on a third party chain, you have to use a committee based assumption where some committee signs the block to attest to its availability, kind of like a data availability committee. But the difference, you can think of Celestia as a data availability committee with crypto economic assumptions, because in celestia, the data availability committee is the kind of validator set of celestia. And this is better than a standard normal closed data development committee because you have this crypto economic assumption where because Celestia itself is kind of like a normal blockchain used by other applications. And the purpose of Celestia isn't solely to provide data availability for some specific ethereum roll up, it's just a standard blockchain in its own right, and with a proof of stake validator set and so on and so forth. And because celestial has data availability sampling, and we have data availability sampling, light clients, so that if a block producer produces an invalid or attests to a block that's not available, the light clients can detect that. The light clients in celestial can detect that, thanks to data availability sampling, and they can halt the chain and then slash the stake of the validators that sign the invalid block.
01:15:44.220 - 01:15:59.030, Speaker C: So you can kind of think of it as a more secure, like a slightly more secure version of a plasma or a validium in the sense that you have this crypto economic security.
01:16:00.840 - 01:16:04.070, Speaker E: I would actually like to. Go ahead.
01:16:04.520 - 01:16:17.720, Speaker G: I was just kind of summarizing in my own words, which is basically that you're using the database sampling as a mechanism for the like clients to enforce the slashing of the committee of validators.
01:16:18.300 - 01:16:21.130, Speaker C: Exactly. That's the plan.
01:16:24.800 - 01:17:42.224, Speaker E: Yeah. I'd also like to add some nuance here, which is that most committee based bridges, not just most, but like all the committee based bridges that have been deployed so far, essentially don't have slashing, which is why analyzing this new thing, where there's actually a way of penalizing the committee, in this case the celestial validator set through slashing, if they do in fact misbehave, essentially either making an invalid block or equivocating or something like that. There's kind of this subtle thing in the analysis, which is that if you model it where the cost to corrupt the bridge is simply the cost of two thirds of the stake, because, say, 100% of the misbehaving validator stake will get burned. So at least two thirds have to be malicious. So the cost is two thirds of the stake, which could be in the billions of dollars or whatever, who knows? So this would already be an enormous cost. But if you say that's the cost of corrupting the bridge, that's true. If there's no slashing.
01:17:42.224 - 01:18:33.940, Speaker E: If there is slashing, things get a bit subtle because you can say, what is the cost of corrupting, let's say ethereum today? What's the cost of corrupting Ethereum? From the perspective of an Ethereum application, you could say that it's 51% of the hash power. Proof of work is a bit weird. We consider proof of stake lens, right? Two thirds of the e two stake to corrupt the ethereum chain. But that's not exactly true, because if two thirds of the Ethereum validator set were to try to make an invalid block, everyone would simply reject it. So the cost of actually making something invalid is infinite. It doesn't actually matter how much is actually a stake in terms of dollar value. The actual cost of corrupting the chain is infinite because it cannot be corrupted.
01:18:33.940 - 01:19:29.140, Speaker E: So in that context, you can't analyze it in the same way that you can analyze classical committees, because there's this notion that you can slash the committee out of protocol. You can slash them in a way that you can't do it with regular committee based bridges. So this isn't really like presenting a conclusion or anything, but it's more that there is some nuance here that is not a simple analysis of here's x amount of money, you can corrupt the bridge, here's x amount of money, you can corrupt the l one. There's some subtle differences here because you should be modeling the cost of corrupting an l one is essentially infinite. Okay, that's all I had to say.
01:19:29.290 - 01:19:33.930, Speaker A: Justin, I'll give you another opportunity if you want to say something before we move on to the others.
01:19:35.900 - 01:19:38.250, Speaker G: No, that's all good. Thank you so much for the answer.
01:19:38.940 - 01:19:41.290, Speaker A: Thanks, Sriram. We'll go with you next.
01:19:43.020 - 01:20:57.020, Speaker H: Hi, thank you. Actually, I want to pick up on a thread started by Justin here, trying to understand the crypto economics of slashing when data is unavailable. Seems there are fundamental differences between what happens when you have something invalid, when a committee signs an invalid message, versus when the data is unavailable. When the data is unavailable, and a two third majority committee is actually signing a block in favor of this unavailable block. It seems it's quite difficult to actually enforce slashing even if light nodes are observing this, because that committee could temporarily withhold data and then later reveal it. And the point that John Adler just made on the cost being infinite is the cost is infinite to fool one of those light clients. But really, as the Twitter space is really secure, bridging in a modular world, you could still fool the other side of the bridge, because the other side of the bridge is ethereum or something else, which simply cannot parse anything more than whether there is a two third committee signature.
01:20:57.020 - 01:21:16.810, Speaker H: So the broader question is, how do we enforce slashing for data availability? Because that's, while it's perceivable, there's also ways in which you can hide data, withhold data for some time, and then later reveal it, as well as how do you protect it across a pitch. Thank you.
01:21:17.740 - 01:22:20.172, Speaker C: Yeah. So with this attack that you mentioned, where you can withhold data temporarily and then release it later. So in celestial, there's this weak subjectivity assumption, which is kind of embedded into the tendermint protocol anyway, because we use proof of stake. So there's this three week unbounding period in the cosmos, proof of stake protocol. And so we rely on this week's subjectivity assumption, which basically says that if you kind of go offline for three weeks, then in order to kind of think the chain from scratch, you have to ask a friend to let you know what's the trusted or true hash of the network. The block height of the network is with a block hash. That's the first thing.
01:22:20.172 - 01:22:51.770, Speaker C: Which means, firstly, if a block producer withholds a block, and then that client sees not available, but then the chain somehow moves forward and then actually releases the historic block in four weeks, that block will still be ignored, even though it's available in the future because of this weak subjectivity assumption. But secondly, the second thing. Were you going to say something?
01:22:52.700 - 01:23:01.080, Speaker H: No, go on. I think I was going to say that the weak subjectivity period therefore bounds the period of withholding.
01:23:02.060 - 01:24:06.300, Speaker C: Yes, that's the first thing. But the second thing is that because tendermint is a fork free protocol, which means that if the validator set misbehaves and forks the chain and creates two conflicting blockheaders, then that's considered to be kind of like a safety failure. And what happens is that all the nodes and clients in the network will simply halt the chain, and so they will stop processing new blocks and abort the node. And then you have to kind of go back to social consensus in order to kind of recover the chain, which means if the block producer has withheld latest block, you can't really kind of proceed to the next block until that block is released, because there's no concept of forking. There isn't really any concept of like, a historic block on a different fork of the chain that's longer, has now become available, and now that's the real fork of the chain.
01:24:10.220 - 01:24:12.092, Speaker A: Sriram any follow up to that?
01:24:12.226 - 01:24:33.590, Speaker H: Yeah. It seems, therefore, that all I need to do to attack the chain is I get a two third committee sign a block, withhold the block, and then I have a three week period in which I can release that block. I don't produce any conflicting blocks. I just withhold that block. So the cost of actually executing this attack, rather than being infinite, is actually zero.
01:24:35.880 - 01:24:38.820, Speaker C: But that would violate the weak subjectivity assumption.
01:24:41.160 - 01:24:45.050, Speaker H: Only if I don't release the block within three weeks.
01:24:47.580 - 01:25:10.504, Speaker C: Okay, if you release the block within three weeks, why would that violate the assumption? Why does that violate the protocol? As long as that's, like, the latest block, all that means is that it took three weeks to finalize that block. But the block is optimistic.
01:25:10.552 - 01:25:17.920, Speaker H: Roll up on ethereum. Nobody could contest that block. If I keep the contesting period less than the weak subjectivity period.
01:25:21.860 - 01:25:29.860, Speaker C: Yeah. So in which case, the weak subjectivity period, well, the challenge period will have to be longer than the weak subjectivity period.
01:25:30.920 - 01:25:40.650, Speaker H: That's right. So it seems like fundamentally, the data availability finality is over this week's subjectivity period, and all crypto economics is relative to that.
01:25:42.220 - 01:25:45.290, Speaker C: Yeah. I think maybe John has something to add as well.
01:25:49.120 - 01:26:00.610, Speaker E: I would have to think about your question a bit. It's a bit unclear exactly what your model is. So if you could ask us offline, that'll be great.
01:26:02.500 - 01:26:03.250, Speaker H: Absolutely.
01:26:03.620 - 01:26:11.010, Speaker A: Sure. Thank you so much for your question. We will now move to angelfish. Please ask your question.
01:26:12.840 - 01:27:22.730, Speaker I: Hello, everybody. I've been thinking a little bit about how a modular world would give rise to things such as cross train MeV, and thinking about how the most popular way that it could manifest would be in terms of multi domain arbitrage. But this could serve as the centralizing force because people that could capitalize on this arbitrage opportunity would either have to have money on two chains or three chains where the arbitrage opportunity is, and swap them at the same time, or the sequencers or would be incentivized to collude to get these opportunities. So for nomad and connect, I'm curious whether you guys modularizing bridging could be an avenue to democratize cross chain arbitrage opportunities specifically. And for Celestia, I'm curious how, in a more general sense, how you guys deal with ordering. And could data availability layers be a good hub for somebody like flashbots to build on top of to enable multi chain block building?
01:27:23.820 - 01:27:50.476, Speaker B: It's a really good. So, wait, can you hear me? Yes, we can. Okay, cool. So on the interop side, there's a couple of really important points here. There is arbitrage in general as a space cross chain arbitrage particularly. And there's some really interesting stuff there. But there's two things that I think are kind of like potentially problematic.
01:27:50.476 - 01:28:54.736, Speaker B: The first is you talk about crosschain MeV, which is like allowing frontrunners to basically, frontrunners extracting value from a transaction that goes across chains. For example, say you do something like in a single transaction, swap on uniswap on ethereum, bridge those funds, and then swap on some other decks on ethmos. And because this is happening in an asynchronous paradigm, it is possible for some front runner to see that the transaction happened on the sending chain and then front run the transaction. See the event that happened on sending chain, front run the transaction on the receiving chain. Unfortunately, there's not a lot of ways to get around that. One thing that we've explored is potentially the data that goes across chains to call the receiving chain contract, encrypt that and then decrypt it. You'd have some sort of like, yeah, it's messy.
01:28:54.736 - 01:29:40.128, Speaker B: You'd have to like encrypt the data that is being emitted in the event and then decrypt it on the receiving chain, and then have push that into the contract. Then you would still be subject to on chain mev, but you would not be subject to this cross chain front running attack. But that one is super messy. And I think, unfortunately, it doesn't seem like there's a lot of ways to get around it. The other one that's even more messy is actually the paradigm that is currently being followed by things like synapse. And this is actually where I think the bigger problem rises, which is a situation where you actually have an amm itself that straddles multiple chains. And so you have a liquidity pool on one chain and then a liquidity pool on another chain, and then you are determining pricing between those two chains based on the size of those liquidity pools.
01:29:40.128 - 01:30:31.860, Speaker B: When a transfer occurs, the fundamental problem there is that you're introducing asynchronous to this model that is not designed to be asynchronous. There's kind of two approaches to building distributed systems. There's the approach of having asynchronous in your distributed system, and then there's what are called CRDTs, where you just have reconciliation happen after the fact in a conflict free way. And I'm kind of bearish on the whole concept of let's build an amm that has pools on different chains for exactly this reason, because as is currently happening with synapse, it is extremely possible for people to just manipulate the price of the AMM itself directly as liquidity goes across chains by simply sending transactions in the opposite direction and front running users.
01:30:34.600 - 01:31:44.476, Speaker D: One more future avenue that we're really interested in right now. MeV largely relies on these miners running specialized nodes that the flashbots nodes, because cross chain communication is asynchronous, building that kind of thing for it is much more difficult and more complex. But it's still possible. A lot of these tendermint zones, for example, share big chunks of the validator set. And hypothetically, those validators can use their leeway in the tendermint protocol to bring those chains into something resembling synchrony that only the validator can access. So because tendermint is fork free, and because it has one block finality, they can, in effect, if they are validating both of those chains under certain circumstances, bring those chains into synchrony, make transactions on both chains in a synchronous manner, and have whatever effect they want to. So we're really interested in this in the future.
01:31:44.476 - 01:31:55.840, Speaker D: It is a much more long term research avenue. And again, it relies on running custom node software on dozens of validators across dozens of chains.
01:31:59.830 - 01:32:04.440, Speaker A: Mustafa, John, anything to add on angel Fish's question?
01:32:10.270 - 01:32:12.540, Speaker E: Nope. James gave a pretty good answer.
01:32:14.830 - 01:32:38.020, Speaker A: Okay, cool. Well, I think we can wrap up here. James, Arjun, you guys are our guests. We will leave you with closing thoughts. Arjun, why don't you go first? As it relates to the modular blockchain thesis and most of the stuff we talked about, what are your closing thoughts on this Twitter space, and what would you like to leave the audience with?
01:32:39.830 - 01:33:28.558, Speaker B: Oh, man. Yeah, there's so many closing thoughts. So I think modularity is a term that is probably going to be used more and more as we explore this space. It does seem to be the direction that a lot of decentralized application, decentralized network development is going, because I think people are beginning to understand that it is better to have hyper specialization at each layer of the stack. I would love to see more people kind of getting involved in that research and getting involved in this conversation. It's fantastic that we have Celestia here. It's fantastic that the modular thesis is playing out really well in a bunch of different ecosystems.
01:33:28.558 - 01:33:40.466, Speaker B: So I think just getting more researchers involved is certainly going to make it much more, is going to help us get to solutions faster. Yeah, also, thanks for having us, James.
01:33:40.578 - 01:34:31.640, Speaker D: Thank you yeah, definitely. So the great part of the modular blockchain thesis and modular bridges is we get to specialize things at their own domains and we get to work at the boundaries. The whole project of bridging and the whole project of cross chain communication is to work at the boundaries between these different ecosystems, to reconcile their differences and build something coherent on top of them that interacts with them directly. That's what I really enjoy doing and that's what keeps me coming back to it for so many years. This ability to work with so many systems, learn so many things about each execution environment in each chain, and then bring them into practice and write code for them. So if that sounds fun to you, definitely reach out.
01:34:34.410 - 01:34:55.022, Speaker A: All right, everyone, we'll conclude at this. Thank you so much. One last piece. Celestia is hosting the modular summit at DevConnect in Amsterdam next month. And I believe Anatoli from Solana will be in attendance with a one on one with Mustafa. It will be exciting and there will be more on Twitter for that. Thank you so much for your time.
01:34:55.022 - 01:34:58.800, Speaker A: James, Arjun. We'll see you guys when we see you guys.
01:34:59.410 - 01:35:00.798, Speaker D: Thanks for having us.
01:35:00.964 - 01:35:02.800, Speaker A: Mustafa, John, thank you.
01:35:03.490 - 01:35:04.158, Speaker C: Thank you.
01:35:04.244 - 01:35:04.926, Speaker E: Thanks.
01:35:05.108 - 01:35:05.740, Speaker B: Thanks so much.
