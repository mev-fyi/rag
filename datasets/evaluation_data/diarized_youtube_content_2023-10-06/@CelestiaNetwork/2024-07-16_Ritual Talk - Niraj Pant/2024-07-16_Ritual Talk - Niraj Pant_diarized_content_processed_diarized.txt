00:00:01.880 - 00:00:28.302, Speaker A: All right. Hi guys, I'm Sunil. I am a founding member over at ritual, and today I am here to, I don't know, give free form thoughts, explain what ritual is, and give freeform thoughts on AI and crypto because potentially one of the most grifty spaces right now. And I'm here to help you guys instill what's actually real. So I'm not going to go spend too much time on this. I don't think I need to explain that AI is probably the fastest growing technology of all time. AI is eating the world, blah, blah, blah, office here.
00:00:28.302 - 00:01:06.004, Speaker A: Know that. But I do want to talk about the structural challenges of AI. I think there's been a lot of development in AI and crypto that's revolved around like, okay, we can make this part better. In theory, some part of this is centralized, but what actually matters, and why does this actually matter to users? Not from an ideological perspective, obviously. Decentralization and censorship resistance are important, but unless people actually care about it because it affects of life or the products they can build, they don't really care. And the reality is that currently the state of AI is really highly centralized. The best models are owned by a few select companies, and that has real actual implications.
00:01:06.004 - 00:01:59.520, Speaker A: Two, first and foremost, one is censorship resistant. A lot of these models that come out of OpenAI and Microsoft or Google or whatnot, Claude, will never be able to encapsulate an entire value set or do certain things because it's in their interest to curtail behavior of the model that does not fit within the current regime. So if you go to OpenAI, that model is extremely western biased. It will not tell you how to do certain tasks, sometimes for the better, but sometimes it's very, very heavily curtailed. And this is really because a lot of the alignment and a lot of the red teaming that goes into this model is a bunch of Silicon Valley engineers that are sitting in a room saying, okay, this is what the model should be. The reality is it doesn't express the value set of most of the world or even touch on a lot of the utility that the model could do, largely because it exists within the us regime. So that's one thing.
00:01:59.520 - 00:02:37.332, Speaker A: And then two is structurally accessing these things is actually hard. If you want to go and use OpenAI or cloud, you need to be able to pay in us dollars. And if you can't, then you're kind of screwed because that's the only thing their API takes it in. Now, on the other end of that, there's plenty of open source models that you could probably go and use that equally will get the job done by either downloading them or accessing them is hard. Or we'll just start to see maybe even though we've managed to keep parity in open source land, we'll start to see maybe a degradation of quality relative to closed source. Because there aren't the right incentives for open source models. Because largely when you centralize, you can build your own payment rails and you can build your own methods of distribution, but most importantly, you keep the secret sauce or the weights in house.
00:02:37.332 - 00:03:30.832, Speaker A: There's actually no good way to one do providence for open source models and to monetize them, right. However, and I just talked with impact, censorship, optimization, blah, blah blah, however, this is where crypto comes into play. What has crypto definitively been good at? I think if I look at at least my experience with the past few cycles, both being an investor and operator, crypto does a really good job of creating new monetization schemes for open source software. It does a really good job of providing incredibly neutral layer for Providence. And it does a really good job of creating a general purpose layer for application development, which is actually highly relevant, I guess, to the theme of modularity. But the idea is that when you have any sort of execution layer, you basically have one big open backend. And that means that the composability and the speed at which applications will be built are massive.
00:03:30.832 - 00:04:05.950, Speaker A: So right now, today, if you wanted to go use an open source model, you have to go to hugging face, download it, deploy to gpu, build the application around it. In comparison, if you could write a smart contract over the model and access it natively, and that model is run somewhere in the network, then a bunch of applications go tap into that. All they need is solidity or rest or whatever the language of your vm is. When you take those three things, there's a lot of things structurally that crypto can help solve in AI. It's not just boohoo. We should decentralize AI because we want to give too much power. The reality is that you actually increase developer velocity in the design space.
00:04:05.950 - 00:04:56.454, Speaker A: When you can make AI accessible in a crypto native way, you can open up payments, you can open up accessibility, you can open up incentives for open source model development. All of that can and will be solved. And oops. And this is primarily what ritual is really focused on. Right. When we started ritual, we really came down to the question of how do we make AI accessible to all? Not just all in the sense of like okay, we want to juice this nice narrative and have some crypto AI bullshit applications, but truly, all right, if you were to ask me today what my envision for the virtual network, I want every model that comes up a new model to be accessible on the network. And we realized that the only way to do this was to build dedicated sort of infrastructure around being able for users to be able to run models, make them available for end application operations.
00:04:56.454 - 00:05:45.746, Speaker A: So things like inference, quantization, embedding, distillation, and also provide a general developer platform for people to go and access models in a way where model creators are rewarded for it. So we wanted to build tooling that makes it basically best in class tooling for allowing developers to coordinate, sort of like the best of crypto, the best of AI, and also fold in any of the financial and cryptographic primitives that exist in crypto into the AI supply chain. So our phase one which is complete and we have people building on our phase one is infranet, which is our decentralized Oracle network. This was really like our first phase to see if people want to go and use models on chain and actually do this. So the way it works is you have a bunch of nodes, these nodes are running models. We have an on chain set of contracts to let you access these models via the nodes. We have payments, we have things like inference, batching, et cetera.
00:05:45.746 - 00:06:31.550, Speaker A: And we've actually started to see applications adopt them. They use them for everything from as simple as on chain payments for inference requests to tying more elaborate smart contract schemes over like their protocols, to using protocols to use models to like parameterize various parts of their protocols. And a result both. And it's flexible with both on chain and off chain. You can do any sort of combination of on chain, off chain inputs and outputs, and you can pass in proofs if you care about verifiability. And a result of all that flexibility and just being able to expose, you know, both models to smart contract developers, give them a clear monetization path, and fold in cryptographic primitives around verifiability. We've done a few million inference requests to date over the past few months, which is quite interesting because a lot of this activity is going to be tracked or is tracked on chain.
00:06:31.550 - 00:07:04.020, Speaker A: In building infranet we realized, ok, this is really great, we're going to continue to support this. This is a really good way to have people call out to models and use them, but there's a way to build this better. So we went to ritual chain, and I know you guys are real tired of infrastructure. I am too. But I promise there's a real good reason we actually built ritual chain, which is that when you have a lot of flexibility over the customizability of your vm, you get a lot more done. And more concretely, what that means here is that we actually took a base in Ethereum implementation. We're building a custom vm on top of it.
00:07:04.020 - 00:08:17.214, Speaker A: And a lot of that revolves around adding pre compiles and stateful precompiles. Think of them sort of like sidecars around AI operations. So when I go to the chain and I'm building an application anywhere, anywhere on the chain, as long as that model is provided for by one of the node operators set and they can access that model anywhere on the chain, I will be able to write a single line smart contract call that does inference. And so suddenly you as any smart contract developer, anyone on the chain, you don't need to do an oracle, you don't need to specify anything, will be able to access models for inference natively and write smart contracts over them. And it's all made possible through essentially the development of what we're calling the virtual vm. And a lot of the work that goes into how do you make models atomic? How do you make inference atomic? How do you make that play? Well with consensus, which I won't bore you with, but what's actually possible with this? Suddenly you can have dynamically updated, and I hate to use self intelligent applications because it sounds really buzzword, but there really is no better way to describe it. If you're building a DeFi protocol and let's say it's a lending protocol and you need to set the loan to value ratio, the LTV ratio for liquidity pool, before you would have to go to gauntlet, spend like months, get a governance vote, get that to go through.
00:08:17.214 - 00:08:48.688, Speaker A: Now you can just pick the best model available, have that, set the LTV based on some sort of underlying features, and just have that recurrently do that. That protocol is self governing, self autonomous, and just functions on its own. And that frees up governance to focus on more critical strategy things. And I think that's a very, very crypto native example. So I'll give you guys another example. We work with a number of consumer protocols that are building things around chatbots. Own your own agent, there's some LLM, there's some Persona that people create out of an LLM and a vector DB, and you can go own part of that.
00:08:48.688 - 00:09:53.506, Speaker A: Now this is really interesting because this creates dynamics that are very, very important off chain, because if I have like, someone goes and creates a Persona that I really like, and I want to go and increase adoption of Persona so that the revenues going into that Persona increase and my staking that goes increase. There's now an incentive for people to not just interact with these Personas that LLMs create, but own a bit and get them adopted beyond just the primary place of content. But then the question becomes, how do you reconcile all of the activity around actually using the model inferences with ownership? Eventually that has to flow back. If you can link on chain payments in this trustless way and do the splits between all the people who own part of this application or agent to the actual metered inference request, then that's great. But it turns out that we don't really have great financial infrastructure for doing that. It's actually pretty annoying for someone who's going and building this to accept USD and then do the splits automatically. And that doesn't even get into a bunch of the implications around being able to do collective ownership in web two world.
00:09:53.506 - 00:10:28.972, Speaker A: But it turns out people want this. We have applications that we're working with that have millions of daus have done millions in ARR per year that are now moving to this ownership model, and their users are very receptive to it. And most of those users may not even know that they're using an underlying chain because of a lot of other work around account extraction stuff that the application does on their end. I think in the future there's even more interesting things that we can do. There's been a lot of work around fhe, around ZK, etcetera. This will become important for models. This will become important in terms of giving certain guarantees around integrity or privacy for use cases that care.
00:10:28.972 - 00:10:55.820, Speaker A: But the reality is that if any sort of proof is generated, you still need some sort of arbiter or arbitrator to go and determine if that proof is valid. If you give it to a trusted third party, they can easily be bribed and do the verification. Say this proof is not valid. Even if it is. However, a chain is a perfect substrata for that. Specifically within the context of AI, though, proofs can span a multitude of things. Skip that, that.
00:10:55.820 - 00:11:52.504, Speaker A: Skip that too. Where is it? Where is it? Where is it? All right, maybe not in this presentation, but proofs can do a multitude of things, and there's a lot of things to prove around the stack. Was the model run correctly? Was the model run privately? Can we do Mpz with DK? One thing that we folded into the chain is actually a proof marketplace. So one thing that we'll be working on is basically like if I'm a user, right? And I do care about a proof, right? How do I express my preferences about wanting that proof around associated with my model? Usually that comes in terms of an economic preference, and that means that people who are running the models and are able to generate these proofs around inference, et cetera, will get additionally rewarded. For the users to care about that in a traditional system, that wouldn't necessarily be possible. There would need to be a lot of work done. Pricing, you would have to do some weird things around trying to build a model for how do you price the value of a ZK proof based on someone's preference set.
00:11:52.504 - 00:12:21.982, Speaker A: But here, thanks to a lot of innovations around intent solvers, et cetera, you can actually build some of these systems. That's a peak of what we do. But essentially I think the way to conceptualize virtual chain is that you'll be able to write smart contracts models. You will be able to fold in all of your favorite cryptographic primitives as it relates to running models. So both building general purpose applications and building AI enabled applications will be possible. And I talk about that here. This is sort of like an example of that, where we fold in a lot of the primitives around privacy.
00:12:21.982 - 00:12:52.858, Speaker A: We have a proof marketplace. Everything from remote attestations is ek. We work with storage layers to do decentralized model storage, and we have on chain semantics you can literally specify and write agents as smart contracts. You can take payments cross border. You don't need to worry about that. If you're running an open source model on the virtual network and applications are using this, you will have payment flow come to you. We're also working on some very interesting things around provenance with a few of our partners like story to create IP and actually create a benchmark like ok, this was the first person to upload this model to the network.
00:12:52.858 - 00:13:58.620, Speaker A: If people go and use that model, there should be a value flow back. And the very interesting thing is, even though we do like to see open source, we do hope open source will grow. Our network is also flexible enough that if yes, people do want OpenAI and cloud and therapy and whatnot, they can use that, right? The end goal for this is to just make a layer that anyone can go and just have general programmability over models which does not exist. Which is why we consider ourselves the execution layer for Aih, that's our pillars. I mentioned a lot of these, but we do care about decentralization, privacy, integrity and incentives, but in a practical manner to the end that it actually better is the value proposition of the AI supply chain and what could you build? So these are very, very crypto native models, crypto native ideas, I would say, because I think there's a bird actionable bridge where crypto is enabled by AI, which a lot of this covers. We can take the existing DeFi landscape, reimagine it with models in the loop. There's a lot of really, really fun consumer experiments that are being done on chain, which I'll cover one that was built on ritual early around, like generating mass content by AI, minting them, etcetera.
00:13:58.620 - 00:14:43.556, Speaker A: You can take, there's a lot of rote tasks associated with on chain governance that you can have agents take over. And then there's a lot of really interesting plays in gaming, right? We see applications like AI arena for example, where you train your own model or parallel colony that learns from your actions in playing the game and learns to play the game for you. And then there's a reward system. These are, I think, fun things that will drive users, are appealing to users, appealing to traders potentially. But I think what I'm personally very, very excited about is how do we enable AI with crypto? I talked a little bit about how you can fold cryptographic primitives. The reality is that we have a lot of really good work being done around verifiability, new proving schemes, et cetera. But the actual monetization path for logging are pretty slow.
00:14:43.556 - 00:15:36.938, Speaker A: The actual number of general purpose applications that are using this or need to use this right now are low on AI. On the other hand, things like integrity and trusting a third party is actually becoming increasingly important. And so if I'm trusting someone to run my model, usually if I'm going to OpenAI, et cetera, and I want to know they didn't fudge with it, or I want a tee. I need to get a very, very expensive, very, very hard to get baa with them in Microsoft Azure, which may not even be possible now given that they've terminated their agreement to make sure that I can get some sort of remote attestation as an enterprise that they're running my model correctly in comparison. If we have a good substrauter for folding some of these cryptographic primitives into the running of a model, you can actually circumvent a lot of this and you can make it composable with any open source model. One thing we're also very keenly interested in is data provenance. Which is how do I measure how much my data contributes to the fine tuning or training of a model, right.
00:15:36.938 - 00:16:22.990, Speaker A: You know, I'm sure all of us have used the model at some point. Maybe some of us have used open source models and guarantee none of us have gotten paid for any of our data. That's like being used into, that's being used to train these models and improve them further. That's a really important feedback loop, right? Because I think as we start to see more and more models that are good at more and more things, what actually becomes more and more relevant is proprietary data and like personal data and user data, right? That's what actually becomes, like, very, very valuable, because models are good at learning how to do things, but they're not good at remembering things, right? And they can learn how to do things if they have more specific examples. So as you continue to see models evolve, being able to show them new environments, and thus that comes from having proprietary data become important, but we also want to have good reward mechanisms. I talked about incentivization for open source and contributing to that. And the last part is governance.
00:16:22.990 - 00:17:43.662, Speaker A: I touched briefly on how the processes that go into aligning these models are very, very opaque, right? It's not actually easy to, or it's not actually easy to say over what value set OpenAI is trading its model on and doing its climate. And for that reason, it's banned in most parts of the world, not because people don't want it or they think it's telling them to do bad stuff, but actually because it's viewed as a lens for western propaganda, or it's too representative, maybe a Silicon Valley mindset or value set. The reality is that we can actually use a lot of really interesting on chain primitives to do things like incentivize people to do RLHF, right? Like vote on prompts out of like a, you know, unbiased model and capture a wider value preference set in order to essentially create better governance and alignment around these models. So these are all things that I'm personally very excited about, because this is where we can go. And there's actually utility in bringing cryptographic primitives and bringing primitives from crypto as an industry to AI, right? Like off on that last point alone, we're already seeing that there are companies getting hundreds of millions of funding to build localized AI for countries like India and Japan. And the UA built their own project, right? There's a way to do this that scales beyond just countries or nationals, but around communities, et cetera. It'll be something that's quite valuable and quite interesting.
00:17:43.662 - 00:18:01.964, Speaker A: And this is a case study that we mentioned. It was a very, very simple case study, back when frantic was a thing. This was the first application built on ritual. And this was to show, really, that agents were good at coordinating on chain payments. And I handle real money. People who built this gave this thing like ten eth. You had to go convince it to buy and sell your friend tech keys.
00:18:01.964 - 00:18:49.512, Speaker A: And what it was was accounts of LLMs that would read your prompt, decide if it was convincing, and then have like, a classifier that would be the segment of that, and then take an on chain action. And to my knowledge, this was like the first time that there was an on chain agent that was like handling this much money, right? And it worked. I mean, no one could convince it to, like, buy or sell keys, but we, you know, in testing, we saw that people that was able to move money around take actions on its own. And so we were very excited because a lot of people bought the keys for Fenrir, started trying to interact with it, and we were like, okay, cool. There's actually net new features, net new behavior that people are excited to use when it comes to both AI agents and actually how they interact with on chain protocols. This is a very simple one, so we'll skip that one. And the last part, I'll leave it on, because I know I'm over time, is that we have quite a few open research directions that we're very interested in.
00:18:49.512 - 00:19:36.176, Speaker A: People on open model governance and ownership is definitely one of them, and I've talked a little bit of that. But like, you know, how can we, you know, collectively collect, you know, data or opinions or, or value sets to change the way these models are fine tuned? How can we reward people for doing that? We're very interested in efficient proofs of compute and efficient models as well, specifically for larger models. How can we get stronger guarantees? Right now, your best shot with like a very large LLM is to like, like, run it in a te that feeds into further privacy improvements. We're very interested in open source, decentralized, transparent FM's. There's been a lot of good work on decentralized training or just open source labs, like news research that take existing models and make them available to the public. We're always interested in supporting that. We're very interested in routing.
00:19:36.176 - 00:20:02.160, Speaker A: I think there's been a huge focus on how do you evaluate AI models, but there's also things that you should be concerned about, which is like, how do I know where my model is run. How do I know it's the cheapest? And how do you, like, reconcile all of those? Like I said, we're interested in data providence, and we're always interested in extending agentic work. And how can you help? You can run an infranet node. That's our phase one. You don't need a gpu. It takes, like, five minutes, even if you don't know how to code. We made it dumb easy.
00:20:02.160 - 00:20:13.150, Speaker A: You can come build on us. Come talk to me. We've been working on a number of products that are building natively on the virtual chain. Ah. You can use the applications built on ritual. We have quite a few. I'm happy to provide a list.
00:20:13.150 - 00:20:23.870, Speaker A: We will be publishing this soon. You can contribute to our open source repositories. You can join our discord. I don't even think people talk about AI and crypto there. They just want to play poker, so it's actually a pretty fun place to hang out. Thank you.
