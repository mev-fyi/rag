00:00:02.120 - 00:00:46.184, Speaker A: All right, we good to go? Yeah. Okay. Yeah, it's exactly my screen. Perfect. So today we'll be demoing doing spinning up a local roll up arbitrum roll up on a local ethereum l one, and then connecting that via IBC to our running testnet. So first step, of course, is to run the roll up in the l one. So this is a script from arbitrum that spins up the full local configuration with the l one, and the l two does all the contract, deploying all the fancy stuff we have forked it to.
00:00:46.184 - 00:01:40.388, Speaker A: Instead of using prism as the consensus client, we now use Loadstar, which is what we require to track the consensus of the l one. The l one consensus on ethereum is the beacon chain, and they provide in the APIs certain endpoints that allow us to do light client updates on a counterparty of that chain. So Lodestar is actually the only chain, the only implementation that fully supports this. So we had to stop using prism in this, unfortunately, and fork it to use Loadstar, which was fine. You can see here it's spinning up all the different nodes. You can see the sequencers, the consensus chain, all that, and then spinning up the l two here. And then it has to deploy a whole bunch of contracts.
00:01:40.388 - 00:02:16.810, Speaker A: So this will take a while. Yeah. Alongside this, we'll also be running our relayer voyager. You can see here the logs look very scary, but it's actually not that bad. This is connected, again to our running testnet. So 111-5511 this is the chain id for sepolia. The local configuration I have does not have fully configured in it, so it will spit out all these logs every time it sees an event that it doesn't know the counterparty chain for.
00:02:16.810 - 00:03:04.140, Speaker A: So just pretend those logs aren't there. No way to turn them off right yet. But if you go back here, we can see still deploying all the contracts. This is the entire arbitrum l two stack, and it will spin up everything for you, and then we'll be able to connect to it. This is the, we randomized the l one and l two chain ids. The idea is that eventually people can use this as a local demo to connect either to a local or a remote running testnet for union to demonstrate how easy union makes it to connect to the like to connect IBC between us and the counterparty. So wait till this is up.
00:03:04.140 - 00:03:08.060, Speaker A: This is the part that takes the longest. So.
00:03:12.120 - 00:03:15.528, Speaker B: The contracts that you're deploying those, that's the arbitrum.
00:03:15.624 - 00:03:42.786, Speaker A: This is the arbitrum stuff. Yeah, so they have the sequencer inbox and the inbox. That's for, like, the l one. L two messaging. A bunch of this stuff is for the fraud proofs, the challenges in a lot of this, I admit, I don't actually know what it does. I just know it's fancy. Just a few more here, and then once this spins up, we'll be able.
00:03:42.786 - 00:04:06.400, Speaker A: We'll be running block scout as well for the l two, so we can connect. We'll have a front end, the only front end in this entire demo we'll have shortly here. Almost done. It gets more interesting than this. So you are deploying an l three. No, we're not doing an l three stack. That is also possible with this configuration.
00:04:06.400 - 00:04:27.649, Speaker A: What we're doing is more. It's if you wanted to use the arbitrum l two stack as your own l two. Like, use it in the same way you'd use the op framework and then use that to then settle on an l one. And that l one you settle on could be ethereum. You could settle on barachain if you wanted to. Anything. Anything evm compatible.
00:04:27.649 - 00:05:03.454, Speaker A: Of course, you could then also settle with the arbitram stack on arbitration directly via their l three configuration. But this is a lot simpler to do in a local demo because I don't have to spin up three layers of chains, but it would be possible to do so to connect to an arbitram l three. All right, so the contracts have been deployed. Step one here is. I have to take this address and paste it. Where is the. This is the voyager config file.
00:05:03.454 - 00:05:47.276, Speaker A: There will be a few values in here that I have to adjust as the demonstration goes on. The first one here is the l one contract address. This is because the arbitrum lite client is a conditional client in that we have to first create a client on union, tracking the l one that tracks the finality of the l one, and then the arbitrum client will track the finality of that lite client, and then it will update its state when we provide a light client update to arbitrum. So it's like a two step process. So something we'll be doing here shortly is creating a client on the l one of the l one on union. So coming up shortly. Yeah.
00:05:47.276 - 00:05:48.360, Speaker A: Here's block scout.
00:05:52.100 - 00:05:54.160, Speaker C: So you need the nail one line client.
00:05:54.660 - 00:05:55.172, Speaker A: Pardon?
00:05:55.236 - 00:06:02.572, Speaker C: That's what you said. So you need the nail one line client as well, not the armor file. So why do you need the l one night client as well?
00:06:02.676 - 00:06:42.828, Speaker A: Because the arbitrum chain itself. The finality of arbitrum is defined by the finality of the l one. So we couldn't just directly do arbitrum as the track, the finality of arbitrum, because as everybody's probably aware, they have the one week finality window. They periodically settle on the l one, as do all rollups. So we need to track both the finality of the arbitrum train itself and then the finality of the chain it's settling on. So if we were to do an l three stack, we would track the finality of the l three, and then the l three on the l two, and then finally the l two on the l one. Multiple levels of connection.
00:06:42.964 - 00:07:03.670, Speaker B: Can I just ask a dumb nonetheless question? Okay, maybe this is obvious to other people, but not for me. So is it novel, like, to be settling on another EVM chain, or is that just like something that was already impossible? Like, is that like a game without the. Or is it just like a difference in execution environments?
00:07:04.410 - 00:07:22.250, Speaker A: That's how all the EVM rollups work. Whether there's ek roll up or optimistic rollup, they have their functionality to connect or to settle their chain state somehow on the l one because the l two s exists to make interacting with the chain cheaper.
00:07:22.370 - 00:07:24.510, Speaker B: You mentioned back and settle on.
00:07:25.210 - 00:07:44.108, Speaker A: You could also settle on Bara because bear chain is an ethereum chain. Like, you can interact with it the same way. The bear chain versus ethereum is only different in the consensus mechanism. So bear chain uses beaconkit, their custom engine API implementation, whereas ethereum l one, like what everybody's familiar with, uses the beacon chain.
00:07:44.204 - 00:07:45.636, Speaker B: Yeah. Okay. You just.
00:07:45.828 - 00:08:19.330, Speaker A: Yeah, no, no worries. So this is the block scout tracking the arbitrum l two. You can see here, it only produces blocks when there's transactions on the chain. So we're not getting spam with blocks like you would see on arbitrary Mainnet, which is very nice for my computer. You can see here, this would have been one of the deployments we just saw would have been in some of these transactions. Then after all that, they do a batch posting report. That's how they settle on the.
00:08:19.330 - 00:08:57.310, Speaker A: They do a transaction on the l one, and then there's a response back that says, yep, we settled on the l one. Here's the information on that. So there's like a two way transfer of state between the two chains. Now that this is up, we need to deploy the. Where was it? We need to deploy the IBC contracts. So we have a. A script here, a dockerfile that you can run and it will deploy our entire IBC stack on whatever l one l two l three evm based chain you want to connect to.
00:08:57.310 - 00:09:38.000, Speaker A: And this runs very fast on arbitrum, which is very nice. Then here we have, these are all the addresses. So one thing we do, which is actually arbitrum does the same thing, is we use a deployer or a proxy, however you want to call it, such that the addresses are deterministic on the chain you deploy it to. So these are the same addresses. If I were to use the same private key with the same nonce with the exact same addresses on any EVM chain, assuming they do the same address calculation. So IBC handler, all of these should be deterministic. Yep, all good.
00:09:38.000 - 00:10:14.594, Speaker A: So now with that, we need to create the l one client. Let's see here, which is this message. So we have pre deployed a two union testnet, the wasm lite client that will then track Ethereum. You can see it says minimal here somewhere. The ethereum defines two main chain specs, which define the different values in the chain. Minimal is good for local configurations. That's what we're using.
00:10:14.594 - 00:10:25.130, Speaker A: But if you're tracking something like sepolia, it will be mainnet. So I need to run, you say.
00:10:25.170 - 00:10:27.830, Speaker C: Replace the chain id.
00:10:29.050 - 00:10:30.400, Speaker A: Oh, yeah. Thank you.
00:10:34.340 - 00:10:37.640, Speaker C: So is the chain id different for every chain?
00:10:37.940 - 00:11:11.570, Speaker A: Yes. So the chain id. One assumption we have in the relayer is that the chain id uniquely associates chain state. So to make sure that there's no issues with chain ids mapping to different chain states, as you bring up and down local ephemeral test nets, we randomize the chain id on startup such that they're always different. Well, it's 4 billion different ids. So first thing I have to do is flush the database. All right.
00:11:11.570 - 00:11:41.220, Speaker A: And then bring up the relayer. There we go. Yeah, I have it set to only emit anything worn or higher, and then just logs the event logs so that we don't get massive spammed logs. There's quite a lot. So the next thing we'll do, actually, before we create the client, is fetch blocks on. Where is that? Fetch blocks on union. Yeah, right here.
00:11:41.220 - 00:12:21.260, Speaker A: It's a bit cumbersome right now, but it's nice to see that this is Voyagerworks based off of message based queue. So this JSON here defines exactly the exact action that Voyager will take. And this here says to fetch blocks from this height, and then this message will produce another message that fetches blocks at this height of. And then a message that says, fetch blocks at that height plus one. So it sort of unfolds its own state. Now. We should have here soon.
00:12:21.260 - 00:12:42.208, Speaker A: We'll see some events come up. Yeah. So these are events from the actual running testnet right now. People doing transactions send packets. Then you can see, because we don't know, we don't have the chain configured. It just doesn't do anything. It thumps an error and doesn't do anything, which is fine.
00:12:42.208 - 00:13:38.154, Speaker A: And then we got. I need to update the chain id one more spot. Let's see if we can see them here. So, this is our datadog tracking the actual remote running Voyager on Testnet. So we should hopefully see the create client in here shortly, assuming I did everything correctly. Yep. There we go.
00:13:38.154 - 00:14:11.216, Speaker A: So, the new client id tracking on union testnet, tracking my local running ethereum l one is zero eight, wasn't 58. So now we need to. Because the l two tracks the l one via the conditional client, it needs to know which client to track the state from. So here, I tested this earlier. You can see it's 56. Now it's 58. And then we'll need to restart Voyager such that it has that config in it again.
00:14:11.216 - 00:14:22.820, Speaker A: It's a bit cumbersome. This will become more user friendly down the line, but creating clients is not something that happens quite frequently. So it's fine.
00:14:24.190 - 00:14:27.210, Speaker C: So whenever I deploy a new protocol, do I have to.
00:14:28.110 - 00:14:55.000, Speaker A: If you have an existing client, you can build multiple chains on top of that. Multiple connections on top of that client. But if you have multiple chains, you will need multiple clients. Yes, exactly. Let's see. Yes, that's going to. And now, because we know how to track finality in arbitrum, we can safely fetch blocks on arbitrum.
00:14:55.000 - 00:15:24.720, Speaker A: Let's go back arbitrum. Oh, yes. One other thing we need to do. L two s are complicated people, so something we need to do as well. Oh, no, not. This message is update the l one client. So this message here, I need to update the client id to 58 as well.
00:15:24.720 - 00:15:53.100, Speaker A: This message here defines the update for the l one client that we just created. So this will. Every 10 seconds. So it's very fast for local testing. It will update the. On union testnet running remotely, update the state it tracks of our locally running l one. So that's what this should do.
00:15:53.100 - 00:16:48.736, Speaker A: Then we'll go back to the logs here and then update this. It should come through in a minute or two, but while we're waiting for that, we can take a quick look at this stuff just to do an explainer of this so this inner message here is what actually does the client update? This will fetch all the required state, aggregate it all on how it's needed, and then create a transaction that will then be sent out to the chain. We wrap this in a retry. Sometimes things break. It's testnet. We're still developing things. So a lot of the errors that we have, we found.
00:16:48.736 - 00:17:24.678, Speaker A: If you just try again once or twice, it goes through. So for the sake of the demo, to be safe, wrap it in a retry. Therefore nothing will explode on us. For like, a production mainnet environment, you'd, of course want to have a lot more logging around this. Wrapping things in retry is still sane, I think, but you want to just like if an update fails on a production mainnet client, you probably want to be notified of that. So you can hook into this as well. And then this repeat message will, the outer message here, it will just repeat all of this every time.
00:17:24.678 - 00:17:58.512, Speaker A: So we can do an update every 10 seconds. And let's go see if the client's been created yet. Unless an error happened. We've been updated. Wait, am I on the right? Update client. That's WASM 39. All right.
00:17:58.512 - 00:18:55.730, Speaker A: WASM 39 is our client tracking sequolia, and that's the one from earlier. So it has not come through yet. Indeed, which is very strange. Let's see. So this is the postgres database that Voyager uses internally to track all of its state such that if Voyager does crash, it doesn't lose anything. Every message is handled in a postgres transaction, making everything fully transactional. No states ever lost.
00:18:55.730 - 00:19:44.142, Speaker A: And the nice thing is we can check if anything's failed. That makes sense because there was other stuff in there. I'm just checking to see if the. If the update failed. Because if it did, it will be in here with a message order. No, it looks unrelated, like, it's not been in here. Yeah, so don't.
00:19:44.142 - 00:20:26.312, Speaker A: I don't think it failed, which is good, but I'm very confused. That's. I tried this five minutes ago, and it worked perfectly. It's okay. We can figure it out. Tracking properly. 2058.
00:20:26.312 - 00:20:27.260, Speaker A: I suppose.
00:20:30.050 - 00:20:32.442, Speaker C: So you're creating the l one client?
00:20:32.546 - 00:21:50.300, Speaker A: The l one client is created. I'm trying to post updates to that l one client on unit testnet. So I'm wondering, let's search the database for WASm 58. Seems it may have been dumped. It was 58, right? Yeah. Well, that's interesting. So now we get to go into debug mode.
00:21:50.300 - 00:22:29.990, Speaker A: This here shows we have one item in the queue right now. If we go select from optimize, this is going to be the block fetching message, I believe. Yes, it is invalid duty missing. I may have forgotten to update a value. Let's go back and look. So it's this value. It's missing.
00:22:29.990 - 00:23:25.190, Speaker A: Let's go back. So the reason we have to update this value is the l one client is designed to track the IVC state, but currently we're not uploading the IVC contracts to the l one because we're not doing any IVC on the l one. So what we need to do here is provide a contract that does exist on the l one. I think this contract does not exist on the l one, hence we cannot prove against it. We just need to do an account proof that that account does exist in the state. What happens then? If it is the IBC handler address is it will be used in the state proof verification because it's trusted in the local client state in the light client, and then you verify a slot in there for the IBC commitment proof. But since we're not doing that, we don't need to do all that IBC stuff for the l one.
00:23:25.190 - 00:23:54.110, Speaker A: This field will become optional down the line once we do it that. So let's see here. Let's go up. We just need a address on the l one. Actually, we can use this one because this address exists on the l one. So we'll have to create the client again, which is fine. Very quick.
00:23:54.110 - 00:24:19.160, Speaker A: Yes. Oh, I need to also restart normally. These errors would be much more prominent. You typically don't have every third log being an error log. Alright. And that's up. So let's see.
00:24:19.160 - 00:25:33.774, Speaker A: Create client plant should go through shortly. This is. Yeah. Oh, it wasn't 59. Nice. I think 59, I believe will be the l two client that we maybe wait, unless the thing. Oh, the bar at the right there lagged.
00:25:33.774 - 00:26:43.140, Speaker A: That's what I was looking at. So zero eight wasn't. 59 is our new l one client should hopefully be tracking all the correct 58, 59. And then this also needs to be updated to 59. This needs to be. So watch the logs, make sure there's no IBC errors. Back to the logs.
00:26:43.140 - 00:27:26.240, Speaker A: Yeah, there we go. Perfect. So this is a successful update of our local running l one on union testnet. This will happen every ten to 30 seconds. Now, thankfully I have lots of gas token, so we should be good. Now what we need to do is restart Voyager again now that it has a new config, and then fetch blocks on arbitrum I can just copy this one. There we go.
00:27:26.240 - 00:28:03.930, Speaker A: So what this command does is it gives you the JSON at the latest height, so you fetch from the latest current state. And then as the state progresses, the chain progresses, it will again unfold. Those blocks. There we go. Should be coming through now. Now what we need to do is create the connection, the client's connection and channel between union Testnet and arbitrum Devnet. So that's what this big message does here.
00:28:03.930 - 00:28:31.792, Speaker A: Oh, nope. I just paste this one and there is an error here. Actually, I caught that one. If we look. Just want to sanity check myself here. Yes. Glad I did that.
00:28:31.792 - 00:29:16.230, Speaker A: So the ucs zero one id is this. And I need to lower case that. And then this client checksum. Here is the pre deployed arbitrum lite client on union testnet. So this is what will be tracking the actual state of our arbitrum Devnet through the L1 client we provided in the config, which is WASm 59. And then this will give us a very large JSON message. You can see all the internal, internal queue.
00:29:16.230 - 00:29:40.620, Speaker A: There we go. Just to sanity check. The l two chain is running on this. You can see this chain ID is the same. So. Very nice. Now, we should see some logs on this one.
00:29:40.620 - 00:30:14.970, Speaker A: Yeah. Create client waSm 59. We should see some client creation going through. We should see an zero eight wasm 60 and a Bls zero. Comment bls zero will be the one on arbitrum. Actually, we won't see comment bls in here because that's running locally and the Voyager running on Testnet will not be aware of that client at all. Yeah, that's our client.
00:30:14.970 - 00:31:19.138, Speaker A: And this message here, this big, huge JSON message, will create the clients. Once they're created, it will start posting updates for them. I believe at a ten second interval, it will then once that's done, create initiate the connection handshake opening, I believe on union. And then once that connection is opened, it will do the channel handshake initiation. So this is mostly intended for Devnet, stuff like this, where, you know, you're the only person doing these kind of interactions. Otherwise you get issues with sequences and mismatch, and it's a pain if you're doing this on a production with lots of activity on it, you would want to do all of these steps individually such that you have full control. So zero eight WASM 60 is the arbitrum lite client that exists on the unit testnet, tracking our local arbitrum l two.
00:31:19.138 - 00:32:17.440, Speaker A: So 59 tracks the l 160 tracks the l two and 60 uses 59 to get finalized to get the finalization of its own chain. And again, if anything doesn't make any sense, feel free to ask any questions. I deal with this stuff day in, day out in my sleep. This is union testnet. Yes. Yeah, we can go to this. We can see the client creation transaction when it loads the Wi Fi.
00:32:17.440 - 00:33:13.470, Speaker A: Also my porn node just getting wrecked by multiple relayers. So you can see here it was a message create client. And then. Yeah, just your typical oof. Create client event here that shows the consensus height it tracks, which zero, 272 is the consensus height was updated to. And then because it's tracking the local arbitrum devnet, but that the consent, the finalized height of the l two is defined as the finalized height of the l one. So you have to go up a level to the l one and up one more level to the consensus chain, which is the beacon chain in this case.
00:33:13.470 - 00:33:58.180, Speaker A: Lots of indirection, but it's how you track the finality that's done. So we should see, hopefully something in here while we wait for this to go through. Might take a minute. The reason the, the l two state is defined as the settlement of the l two on the l one, once that's finalized, is IBC requires that all updates are fully finalized. You can't do an update to height ten that reorgs do an update back to height seven and then go forward again. If that happens, the client freezes. So IBC requires again that it is fully finalized.
00:33:58.180 - 00:35:02.624, Speaker A: So we have to be very careful to track state that is never going to be reorganized, which in the case of optimistic rollup like arbitrum, becomes an issue with off ramping. So if you want to leave arbitrum, you need to wait a full week for that full finality window. This obviously is not great. If you want to send your assets out, do a transfer out very quick to on ramp onto arbitrum or optimism or base. We do have solutions in the works for this. Potentially an intents based API where a middleman market maker could say, I see user wants to transfer assets from arbitrum to union or off to whatever the protocol would then have an additional entry point where a third party could say, I'm going to do this transfer front, run this packet for you, define it, say, I want to send 100 USDC. The market maker has that asset over that bridge like that same wrapped asset already on the other end.
00:35:02.624 - 00:35:40.010, Speaker A: They do the transfer for you, submit the packet when the packet lands. Finally, after a week, the market maker would receive a small fee, assuming they were honest and they actually submitted the correct packet, and then the user doesn't make any difference with the user. All they see is that their transfer happened a lot faster than a week. The worst case scenario here is no market maker picks it up and it takes a full week. If you're not putting enough fee, same idea is you don't have enough gas on your transaction, it's never going to get out of the mempool, no one's going to want to run your transaction. Similar idea here. This looks new.
00:35:40.010 - 00:36:30.664, Speaker A: All right, nice. The opening it. The difference is that for the end user they still have the full security guarantees of the underlying connection. So if that underlying connection tracks full arbitrum finality, the full week plus the full finalized epoch on Ethereum, that there's no way they're going to lose any assets. The worst case is the, I guess best case for the user here is a market maker picks up the packet, submits it. They do the front run, but they submit the incorrect packet, but they still do the transfer. The user just ends up with double their funds because they get both when the packet actually lands and the mistake by the market maker, which is obviously something that they would have to take that risk upon themselves.
00:36:30.664 - 00:37:35.628, Speaker A: And that's what the fee is for trying to submit a transaction. So that's our intended API down the line, which will make dealing with optimistic roll ups and chains with very long finality much, much nicer. So you can apply the same thing to Ethereum mainnet where normal fully finalized block takes twelve plus minutes. Obviously if you want to do a fast swap or a trade or something, you don't want to deal with like a twelve minute and then twelve minute back a half hour to do a swap is not great. So this could be integrated again with the same API concept that would allow again anybody to trustlessly, permissionlessly interact with this intent based system. So here you can see the connection open init on union. We will soon see an open try locally, but it won't show up in the logs because the open try happens on the counterparty chain.
00:37:35.628 - 00:38:27.062, Speaker A: So I'll might be able to find it in the logs themselves. You can see here zero eight wasm 59, kind of strange, no wasm? Oh, because I didn't put it here, of course. There you go. So you can see that the l two client here updates after the l one client. So in updates you can see to the same height. So the l one client updates to whatever heights it does and then the l two client can only update to those like steps of heights. So if you wanted to update your l two client for some reason, to a very specific height that the l one client has not updated to yet, you would have to provide both an update to the l one client to that specific height.
00:38:27.062 - 00:39:24.390, Speaker A: Because then with that you can get the consensus state of the l one from that client through to your l two. So these are going very nice. Let's see if we can find the open try message. I feel like that one's way too old. Yeah, I think probably should have cleared my terminal history before seed, but there should be. Yes, this is the open init here. Oh, it was m 60.
00:39:24.390 - 00:39:56.098, Speaker A: Nice. So this will take a while to go through the handshakes. It's a four way handshake. For anybody not familiar with IBC. It's a four way of from a to b, you do init, try ack confirm. This handshake does the for the connection handshake, specifically it does the negotiation of the different client states. So it ensures that a does indeed track the client state of b and b does indeed track the client state of a.
00:39:56.098 - 00:41:05.856, Speaker A: And they negotiate a few parameters to the connection, which are not important for this demo, but it enables, you could think of it as the security layer underlying the actual app layer, which is the channel. So you define the security guarantees you want in the semantics of the connection with the connection handshake, which is separate from the apps, if that makes sense. It's hard to explain, I think, but. So we should see. And then due to the local union evM l one running on my computer right now still has somewhat long finality for the epoch. I think it's about three to four minutes for that. So every time we do a message we need to wait for the l one to finalize.
00:41:05.856 - 00:41:39.280, Speaker A: Update the l one, update the l one client, update the l two client tracking the l one, because then we can get the events from the l two that are the handshake events here. So it does take a little bit long to get through, but the updates are going through nicely, which is good. And it appears I am attempting to send a transaction that's an update. Let's see here. I think it should be through now.
00:41:40.540 - 00:41:44.360, Speaker C: So do we also need to update the client on behalf?
00:41:45.380 - 00:41:48.600, Speaker A: Yes, so let's see if we can find that in the logs here.
00:41:51.300 - 00:41:58.330, Speaker B: Does one trigger the other? Are they all? Does one like update trigger the other?
00:42:00.350 - 00:42:36.910, Speaker A: The updates from a to B and b to a are completely independent at this stage. The chains don't know about each other. They don't know that they know about each other. That's what the connection defines is that the chain a and b can talk to each other. They can submit messages like they know how to interact, but they don't. Like you can just have chain a updating on chain b without having the other way, which is actually what we're doing for the l one client. So again, typically, as I said before, with that IBC handler address, you would have a two way connection between union and ethereum or and vice versa.
00:42:36.910 - 00:43:11.700, Speaker A: But in this case we don't need that. So you can indeed just have a one way update of a like client on a chain tracking the state. Another chain. So yes, column BlS zero is being updated as well. Or that's the create. Is the update going through? Oh yeah, update. Interesting that they came through in reverse order it.
00:43:11.700 - 00:44:24.440, Speaker A: That's very strange. This is admittedly somewhat difficult to debug without proper logs. We typically when running on testnet we see every log here. Like if we go to those errors are fine. If you go to here, if I can just clear this, you can see we have tons and tons of logs that we can filter on, but half of the logs are not ingested by the relayer running that this is tracking. So we are flying a little bit blind.
00:44:26.180 - 00:44:29.236, Speaker C: Do you have a way to check if the update went through?
00:44:29.428 - 00:45:41.960, Speaker A: That's what I'm looking for right now running at that point. So there was an update that went through. I think the finality, maybe my computer is a little overloaded running all of this locally. So I think the finality is perhaps taking a long time I guess to do one final thing here is take a look at the regular logs. As you can see, we do successfully handle thousands of packet messages an hour. This is running on Testnet. You will have arbitrum live soon on Testnet.
00:45:41.960 - 00:45:59.400, Speaker A: So maybe there was a bug you got to find. We'll get that all figured out and then you'll be able to test this yourself on the german testing, which thankfully is only 1 hour window for testing for fraud proof. So it's a lot faster than. All right, I think that's it.
