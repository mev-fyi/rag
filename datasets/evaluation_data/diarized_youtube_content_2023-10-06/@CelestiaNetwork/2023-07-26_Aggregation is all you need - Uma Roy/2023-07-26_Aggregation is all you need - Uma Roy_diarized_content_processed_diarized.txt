00:00:00.890 - 00:01:02.346, Speaker A: Hello, I'm Uma, one of the co founders of Succinct, and I'm going to be talking about aggregation is all you need. Okay, so I'm going to start with some background on succinct. So in particular, if you look at the ZK landscape today, there's been a lot of focus on two types of applications. So there's zkevms, and then there's also privacy preserving protocols like tornado cache. And at succinct, we're pretty interested in exploring the rich application design space beyond just these two types of protocols that are really well explored today. So we thought a lot about how else can ZK help scale and make blockchains better? And we started by working on ZK like clients, which is basically verifying consensus protocols inside a ZK circuit to allow for efficient verification of consensus protocols in EVM. So in particular, to get into a little more technical detail on how this works, you would have a source chain that has some consensus mechanism.
00:01:02.346 - 00:02:01.086, Speaker A: You verify the consensus in a ZKsnarc, and then you verify that proof in an execution layer very cheaply. And then you can run a succinct on chain like client so that one chain, the target chain, can talk to the source chain natively. And this solves a lot of existing problems with interoperability protocols today, where generally if you want to transmit information or data between one chain and the other, you have to rely on a trusted multisig or some trusted group of entities. And with succinct on chain ZK lite clients, you can basically do interoperability without these trust assumptions, and you can get much more secure interoperability. And the first ZK Lite client we built was a ZK lite client for Ethereum. So our first protocol, telepathy, which has been live on Mainnet since March, uses our Ethereum ZK Lite client. And with that you can send arbitrary messages from Ethereum to any other chain.
00:02:01.086 - 00:02:51.406, Speaker A: And you can also read Ethereum state on all these destination chains, because you have the Ethereum state route on these chains. And then it's also useful for bridging information from Ethereum's consensus layer to its execution layer. And so we have a few people using this. One great example is gnosischain is using our telepathy protocol to secure their native bridge, and then Eigen layer is using us to operate their restaking protocol by getting Ethereum consensus information in the execution layer. So we've built this Ethereum Zk like client, but we want to expand Zk interoperability by supporting more consensus protocols. So there's only really a few consensus protocols that matter. Tendermint is one that's commonly used across a bunch of ecosystems because it's the native consensus protocol of the Cosmos SDK.
00:02:51.406 - 00:03:46.350, Speaker A: Another consensus protocol that's used by the substrate SDK, which is used by the Polkadot ecosystem, is grandpa and Babe consensus. And so there's a few consensus protocols that we at succinct care about proving in a Zk circuit to kind of expand the domains that can talk to each other through these Zk lite clients. And these Zk lite clients are in general useful for two different things. One is l, one to l, one bridging. So, for example, having an Ethereum chain talk to a cosmos chain that's using tendermint, and then also another subcategory of this that will be the main focus of my talk is data availability layer bridging. So if you have a DA layer, you can bridge the state of the DA layer to Ethereum. And so I'm really excited to announce that we're working with Celestia to build a Zk bridge to bring celestia state to Ethereum.
00:03:46.350 - 00:04:33.634, Speaker A: And this is kind of like the big scope of my talk. And one question you might ask is, okay, why are we interested in bringing Celestia state to Ethereum? And so here's a really helpful diagram to show why that's useful. Basically, in the modular stack, a chain can decide to have their data available on Celestia, but then they might decide to settle on Ethereum. So this diagram shows how an L two operator might send proofs, whether they're ZK proofs, or they might send, like, you don't send an optimistic fraud proof, but you have that settled on Ethereum. So you send it to your l two Ethereum contract, and then you send your transaction data or your DA to celestia. And Celestia has this super scalable, really high throughput DA. And so for the roll up, it's much cheaper for them to operate their rollup in this way.
00:04:33.634 - 00:05:13.482, Speaker A: And then you would use our zk lite client to basically attest on Ethereum that the data is actually available on Celestia. And so roll ups kind of get the best of all worlds in this way, and it's much cheaper for them to operate. So there is an existing protocol to bring Celestia state to Ethereum. It's called the QGB, which stands for the quantum gravity bridge. And we are basically turning that into the ZKQGB. And the ZKQGB has a lot of benefits. So right now, the quantum gravity bridge is this kind of sidecar on the Celestia protocol, and one of their core values.
00:05:13.482 - 00:06:12.050, Speaker A: Know, really having this minimal protocol that's minimally simple and only does one thing, which is data availability. And so by having the ZKGB, we can take out the existing QGB and really simplify the core Celestia protocol and then just take the existing celestia validator signatures, verify them in a ZK snark, and then move that out of this core protocol. And this is really nice because also Celestia can scale their validator count without having to worry about an on chain like client gas costs. And we in general can reduce their gas costs a lot for the existing QGB by bundling all these ZKGB verifications together. So now I'm going to dive into some of the technical challenges that we faced while making the ZKGB for Celestia and in particular ZK tendermint. And one interesting thing to note is, as I mentioned, a lot of chains use tendermint. So this is actually reusable work throughout the cosmos ecosystem.
00:06:12.050 - 00:06:54.942, Speaker A: So in general, when you verify a consensus protocol in ZK, you have to do a few different things. You have to verify signatures, you have to verify hash functions, and you have to do some decoding. And in general, the pseudocode looks generally always very similar. You verify signatures, you make sure at least two thirds of the validators have signed. Then you have to prove that the existing set of validators is like the correct validators. And then you have to Merkel, prove any important information against the header, such as a message was sent or some amount of money was deposited in a contract or burned to be minted on the other side. And then in particular, tendermint has some particular technical challenges associated with their consensus algorithm.
00:06:54.942 - 00:07:33.310, Speaker A: So they use this signature scheme, ED 25519, and you have to verify these n ed 2519 validator signatures. Unfortunately, the signature scheme has no aggregation in it like the Ethereum BLS signature scheme. So that is a technical difficulty. And then tendermint also has no epoch time. So in the worst case you might have to verify many headers in a row. And then finally, when tendermint was designed, they didn't really design it to be maximally snark friendly. So they have these snark unfriendly serializations used throughout, such as Protobuf, that have various challenges with being implemented in a circuit.
00:07:33.310 - 00:08:24.560, Speaker A: So I'm going to now cover some of the more specifics of what we implemented at succinct to basically have our Zk circuits be able to handle tendermint and verify tendermint consensus in a snark. So here's an outline of the techniques we used and then combined together. So one interesting thing to note is that verifying validator signatures and hashes is actually embarrassingly repetitive and a very parallel task. If I verify one signature, it has nothing to do with the validity of another signature. So you can really trivially parallelize the verification. And if you just do this naively in a normal circuit and you just verify 100 signatures serially, you're not taking advantage of the innate structure of the problem. And so we think we can do a lot better.
00:08:24.560 - 00:09:24.902, Speaker A: And so we call this idea zk Simd. SIMD stands for single instruction multiple data, which is like a concept that's prevalent in a lot of other contexts like gpus and ABX instructions. But basically it provides this form of data level parallelism, which lets you compute the same function, f on a bunch of different inputs, x one through xn, in parallel. And so we realized that starks are actually really convenient for these sorts of parallelizable computations. So typically people use starks for VMs, like a lot of the ZkvMs or even Zkevms are written in stark based languages. And so starks have this single state transition function that repeats across all the rows of the circuit. And as a result, they're much more lightweight to prove, and often they have much faster proving times than other arithmetizations like Planck.
00:09:24.902 - 00:10:26.110, Speaker A: And so we figured out a way to arithmetize constraints within a stark to implement an abstraction very similar to SiMD. So as I mentioned before, we have this kind of abstraction that lets us, in a very general way, specify an f, a function f that we want to compute independently over a set of inputs and compute a bunch of outputs. And in the particular case of signature verification, f is just the function of verifying a signature, and x is just the actual signature that we want to verify is valid. And this is like a very simplified diagram of what's going on. But basically in our stark, that's verifying in parallel a lot of these signatures, we have two to the 16 rows, and then we have 256 signatures that are getting verified throughout the course of the circuit. So they're not getting verified serially. What happens is that we have a, is there something.
00:10:26.110 - 00:11:26.716, Speaker A: So anyways, there's something that happens where basically we are able to verify all the signatures in parallel. And then we have an accumulator column that accumulates and verifies the results of all the computations together. And at the end is a random linear combination check to make sure all the verifications actually went through okay. So at a very high level, we talked a little bit about how the subtraction works, and we built this nice framework to let us do these computations in parallel, and then we compared it to some existing implementations. And so in short, these benchmarks were taken on an M two Mac. But end to end, our stark framework proof generation for verifying 256 ED 25519 signatures took 80 seconds, and so the proving time per signature is around 320 milliseconds. In contrast, if you were just to verify one signature in the planky two proving framework using plankish arithmetization, that would take around 17 seconds.
00:11:26.716 - 00:12:32.944, Speaker A: And then if you verified it in Ganark, which is a grot 16 based proving framework, it would take around 14 seconds. And so you can see our abstraction of basically paralyzing. Verifying all these signatures as a batch results in a much faster per signature verification time, which is what we would need for verifying something like tendermint, which you have to verify a lot of validator signatures because they're not aggregatable. So I've talked a little bit about how to do this, like parallel computation of the signature verification. Another interesting thing is that you can further use recursion to further reduce the untend latency of these parallelizable computations. So in particular, at the root of this tree, each leaf, sorry, the leaves of the tree, at each leaf, we verify a batch of signatures using our zk SiMD abstraction, and then we actually verify each of these batches in parallel if we want to verify something like 1000 signatures. So each leaf is a stark, and then we recursively combine the verification of the starks together, and then we're able to do this in a tree like structure.
00:12:32.944 - 00:13:34.040, Speaker A: So the end to end latency of our whole computation is simply the depth of the tree, which is log two of the number of signatures that we actually want to verify. And this means that even though we're able to throw more compute at something, the n ten latency of verifying a lot of signatures is greatly reduced. So one problem is that when we're doing all this stuff with zk Simd, with the stark based framework or the recursion, we use this proofing system called plonky two. And in general, recursion friendly proof systems are typically not compatible with the EVM. In the EVM, if you want to verify a proof really cheaply, it's best to do it in Grot 16 or something pairing based, because Ethereum has pairing pre compiles. So our solution is that we have to wrap a recursion friendly proof system with an EVM compatible snark. And so in particular, what we do is we take ganark, which is Grot 16, or ponkish KZG based, and then we take that and we wrap the ponky two proof and then we verify it in EVM.
00:13:34.040 - 00:14:18.260, Speaker A: So our proof system composition, where we're combining three different proof systems, a stark based one, a plonkish fry based one, and then a plonkish KZG based one, unlocks the best of all worlds. You have really fast proving for batches of signatures, then you have really fast recursion for reducing end to end latency. And then finally you wrap it all in the final layer. That gives you really cheap EVM verification. And you kind of need all three of these components to prove a consensus algorithm like tendermint that has all these extra challenges associated with it. And in practice, I kind of touched upon this. We have this particular proof pipeline where we're doing this proof composition and aggregation.
00:14:18.260 - 00:15:20.076, Speaker A: So the first step of our proof pipeline is we have an application specific circuit that can recursively verify batches of things. And so in this application specific circuit, we kind of have the business logic of the consensus protocol where we're verifying validator signatures, maybe we're verifying headers and hashes and other things we need to do. Then we have a recursive circuit that verifies the proofs from step one and it normalizes everything and makes the proof size and the custom gates a constant. And then finally we have this Grot 16 recursive circuit that verifies the proofs from step two. And basically what that does is the cheap EVM verification. So we have this like three step proof pipeline that composes to get us all the properties that we want, which is, it's really fast to generate a proof, and then also really cheap to verify and just to throw out some benchmarks about the proof recursion. So the recursive circuit for ponky two, which is step two in the three step process I described, is actually really fast.
00:15:20.076 - 00:16:05.204, Speaker A: They heavily optimized their framework for recursion. And so in net it takes less than 2 seconds to do the witness generation and the proof time. And then for the final wrapper circuit for cheap EVM, it takes around like 16 seconds to generate the proof, so it's actually still very feasible and manageable to generate this proof. And then as you can see, the on chain verification cost is around 400k gas and probably could be optimized further. So it's very feasible to run something like this on the EVM today. And then finally, proof aggregation, we think is super important. So so far we've done proof composition of a bunch of different proving systems, but eventually you can imagine that we would have a bunch of different consensus protocols that we're verifying in ZK circuits.
00:16:05.204 - 00:17:12.370, Speaker A: So we'd have this grandpa proof of consensus, we'd have tenermint proof of consensus, we'd have our ethereum like client, and maybe we have a bunch of other proofs as well. And one nice thing you can do is you can take all these different consensus proofs that are coming into Ethereum, which is a very constrained computational environment, and you can aggregate all of them, which can save even further on the cost of verifying all these proofs. And so when you aggregate all these proofs, you dramatically reduce the cost of verifying them on chain, and you can also verify proofs that aren't like clients as well. And in the end, we think this will be like a huge unlock for making gas costs much cheaper. And then also you can have the state of all these different chains on Ethereum more frequently, because the gas cost of verifying an individual one will be much less. So yeah, this is a meme about kind of all the ways we stacked these different techniques to finally get to something where it's actually fast enough and feasible to verify in EVM. So our ZK SiMD, which is a starkey based framework, will be open source soon.
00:17:12.370 - 00:17:58.530, Speaker A: It's written pretty generally so that basically, if you have a function that you want to prove over a set of inputs in parallel, you can use that and write a circuit, and we'll open source it soon. And we want people to contribute and collaborate on it, and use it for whatever parallel functions they want to prove. And then our Ganark based plonky two verifier is actually already released under an MIT license. It's open source, it's available at that link. And we would love for other people who are using plonky two, we know it's like a proof system that has a bunch of users because it's very fast to use it to verify their proofs on EVM and also collaborate and contribute. So if you're interested in using any of these things. You can check out the GitHub repo and then also talk with me after and ask if you have any questions about using it.
00:17:58.530 - 00:18:03.820, Speaker A: Cool. Thank you very much Omar.
