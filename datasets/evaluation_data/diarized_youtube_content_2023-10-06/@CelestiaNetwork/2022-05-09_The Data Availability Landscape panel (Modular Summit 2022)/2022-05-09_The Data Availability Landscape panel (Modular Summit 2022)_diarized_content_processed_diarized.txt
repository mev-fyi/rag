00:00:06.620 - 00:00:35.512, Speaker A: Good evening everybody. Welcome to this panel discussion. It's a great pleasure to be here. Thanks to the celestial organizers and the modeler summit organizers for bringing us here. It's a pleasure to share this stage with these illustrious gentlemen here. Okay, maybe we get started with a brief introduction of the data availability stack that each of you are building. I'm just here to moderate this discussion, please.
00:00:35.512 - 00:00:36.500, Speaker A: Anurag.
00:00:36.660 - 00:01:29.144, Speaker B: Yeah. So I'm anurag I'm representing polywen avail. Polywen Avail is a data availability focused blockchain. So we began thinking about this problem in around 2020, Celestia or Lazy Ledger, I think Mustafa had written a paper regarding that and we were interested in exploring that. But they went with the fraud proof approach. And when KZG polynomial commitments really became better known, I would say we thought that we could structure our datability focus solution on using KZG proofs. So the architecture is of the datability focus in the sense that we do focus on transaction ordering and not execution.
00:01:29.144 - 00:02:13.240, Speaker B: And specifically we use validity proofs, KZG proofs for that. And the first use case which we are focusing on because we have three different projects under the polygon umbrella, namely Polygon Hermes, which is focusing on zkevm, for example, polygon zero and polygonmiden. So we have three zkevm solutions within the polygon suit. And so the first focus was to kind of working on validium solutions for these three solutions. So that's the focus. And then of course, we will move to sovereign app chains going forward. So that's the rough.
00:02:15.820 - 00:03:16.172, Speaker C: On. Okay, so hi everyone. My name is John. I am the co founder and chief research officer of Celestia Labs. And we're building Celestia, the first modular blockchain network. This was formerly known as Lazyledger and it's a data availability only chain that is overhead minimized, which means in order to use the chain for data availability, in order to verify that the chain is valid, you have to do a very small amount of work, ideally as minimal as possible. The particular data availability scheme that we use is actually essentially completely identical to the one that was first proposed by my co founder Mustafa in his earlier paper on fraud and data availability proofs to secure trust minimized like clients, effectively completely unchanged from that paper, which was three, four years ago at this point.
00:03:16.172 - 00:03:24.850, Speaker C: So there aren't many cases in the blockchain space where you don't have to fundamentally pivot your technology over that many years. But in this case, it did happen.
00:03:26.580 - 00:03:27.424, Speaker A: Thanks.
00:03:27.622 - 00:04:45.572, Speaker D: Matt hi, my name is Matt. I am a researcher working on Ethereum, and you've probably been hearing a lot about the Ethereum scaling stack called Dank Sharding. And Dank Sharding is a new take on the Ethereum sharding roadmap that exploits some of the observations that people have been making over the last couple years related to the centralization of block builders in the ecosystem. And Danksharding relies on the fact that in the future we expect that the people who are building blocks are going to be highly sophisticated, more resourced actors, and the people who are proposing blocks are going to be lower resourced actors. That's coming from a more decentralized set. And so with Dank sharding we are able to avoid some of the clunkiness that existed in previous sharding proposals where there were 64 shards, 64 shard proposers there was latency between creating a block on a shard and that block being checkpointed onto the beacon chain. And so now we're taking advantage of this more centralized building actors and they're able to aggregate 64 shards worth of data into a single 2D KCG scheme and provide that to the network.
00:04:45.572 - 00:05:38.410, Speaker D: And it's also inspired by this lazy ledger paper that was written four or five years ago where people are sampling the data elements from this super 2D encoded block. I would say also for the encoding scheme for the blocks is definitely more efficient because you're able to only do 75 samples of the block rather than 30 samples for each shard block. So it's a lot better than the previous proposals for how much data needs to be downloaded from each validator that exists in the validator set.
00:05:39.600 - 00:06:20.360, Speaker A: Thanks. So the main agenda of this discussion is to understand the different data availability architectures. So what we will do is try to set up the axes of comparisons. One axis I think is obviously security. What is the security model that each of these different projects are trying to optimize for? One way to phrase this question is what would it cost for me to get an unavailable data and pass it as being available? That's something maybe we can discuss about the different schemes.
00:06:21.980 - 00:07:25.310, Speaker B: In the KZG commitment scheme, the scheme is set up as such that I think safety of data is not a big problem. The problem I think would be maybe insufficient number of light lands, for example, sampling the data because there's no incentive as such. So the idea is that if you are building like an app specific chain, you would have the incentive to have light lands sampling the data. So I would imagine having insufficient number of light lands sampling the data or part of the data is available not from the block itself, but if you consider the entire app history. So that could be a case. But in general, due to the construction of the scheme, it's pretty nicely laid out that either data is available or not available.
00:07:27.440 - 00:08:56.120, Speaker A: So one point I want to add here is we call this data availability sampling, and when we call something data availability sampling, what we mean is any light node, like Arjun just mentioned, anurag just mentioned is any light node should be able to sample an arbitrary subset of the chunks of a given block and then verify that they are available. A very important underlying assumption here is private random sampling that a light node should be able to do check a random sample of different chunks. This I think goes back to the security question and maybe we can address this, but the question is how do we think about private random sampling? How do we make sure that light nodes actually when they're sampling, it is private and it is random because there are all kinds of de anonymization attacks. For example, if I know that the same IP address is requesting these 32 chunks, maybe I can just send those 32 chunks to that guy and withhold the rest of the data. So private random sampling is a very important model and I think this is not something that is part of any other blockchain architecture prior to this. So we need to take special attention as data availability builders to this. So I think that goes back to the light client point that you mentioned, whether they're able to sample correctly.
00:08:56.120 - 00:09:00.510, Speaker A: John yeah, or you want to add anything to that please.
00:09:01.840 - 00:09:03.996, Speaker C: Is the specific question still the security?
00:09:04.098 - 00:09:08.124, Speaker A: And then a part of the security is private random sampling. So that's how I'm guessing, sure.
00:09:08.162 - 00:10:22.576, Speaker C: So I guess this is the first data availability sampling scheme proposed and the assumptions are pretty concretely laid out in the paper, right? You need a certain minimum number of nodes, not necessarily light nodes, but nodes in general, to perform sample requests. You need a synchronous communication in order to reconstruct a block. If there's no single node that starts with the entire block and you have to reconstruct it from byte nodes. And you also need, as Sriram has said, under certain adversarial assumptions, you need private sampling so that the attacker can't link together who is trying to sample, because if they can, then they can just trick that one particular node into believing that the block data is available when it's not. And that's essentially, I mean, barring bugs and the codes and stuff, assuming that that's all correct and that's essentially the failure modes for the scheme. So a certain number of light nodes is a pretty weak assumption because there's thousands of bitcoin nodes. There's all ten, what, 10,000 ethereum nodes.
00:10:22.576 - 00:11:16.916, Speaker C: There's even something like 6000 salana nodes, right? People will run nodes even if they're fairly expensive, just purely because they want to have access to the chain. So an assumption that there are 100 light nodes sampling or 200 light nodes is like a really easy assumption to satisfy. We can essentially guarantee that the synchrony assumption for being able to rebuild the block is something that you unfortunately can't avoid. And this is one of the reasons why validity proof schemes aren't inherently better. Because yes, you avoid a synchrony assumption for fraud proof, but you still need synchronous communication to rebuild a block anyways. So if you're not fixing the worst case, you still need that synchrony assumption. So you don't really get too much out of validity proofs, except potentially some lower latency happy path cases, which is debatable and yeah, that's about it.
00:11:16.916 - 00:12:35.440, Speaker C: Those are essentially the failure modes. So as you can see, these are fairly weak assumptions, which is why we tend to classify the process of data availability sampling as being trust minimized because these are fairly weak assumptions. Now, in terms of private sampling, this is something that I don't think any chain that's implementing data availability sampling has done yet, mostly because this would require a fairly well resourced attacker. But if you don't want to go through fairly advanced schemes like mixnets or whatever, or Tor, there are other alternatives. For instance, you can have users run full nodes in multiple cloud service providers and unless every single one of those cloud service providers colludes to reveal that with the same user running those nodes, a third party observer can't actually connect that those nodes are actually run by the same person. So even if you don't directly solve it through some advanced networking, you can solve it in practice fairly easily against an attacker that is just one person observing the network, rather than literally every single cloud provider in the world all colluding together because that's a much stronger adversary which is much less practical.
00:12:36.900 - 00:12:42.636, Speaker A: Thank you. John Matt, do you want to address how Ethereum is thinking about private random sampling?
00:12:42.748 - 00:14:01.630, Speaker D: Right, so in the first iteration of Dink Sharding, their plan is to not do the private random sampling. And the reason being is that being able to quickly access random chunks of this data is actually a really difficult networking engineering problem to solve. And so the assumption that we'll make is that with an honest majority, we're going to be able to assign validators certain rows and columns that they'll go ahead and download and be able to, with this mechanism, propagate this. Information over the lib p to p gossip network in a lot more efficient manner than having to push things into a DHT and then retrieve them. But this does have the assumption if you are a client who is just watching the network, you don't have a strong grasp of if that block is available or not. You're just trusting the honest majority of the network to tell you that block was available because the validator said it was available. And this is where the private random sampling goes in, where even if you have a malicious majority of the network saying that a block is available, you're able to do the samples yourself and with high confidence determine whether or not the block was actually available.
00:14:02.160 - 00:14:27.860, Speaker A: Would you say in Dank Sharding, if somebody indeed wanted to do random sampling they could just join those particular peer to peer networks where those chunks are propagated and thus check whether that is available on those peer to peer networks and then verify for themselves that the data is available? Is that something that you guys are planning for?
00:14:28.010 - 00:14:50.350, Speaker D: So the way that the rows and columns will be encoded is not going to be as amenable to the proper sampling structure that people will want to do, because you'll want to have more granularity when you do these random samples. And the rows and columns won't provide that level of granularity, but you will be able to connect to as many of the gossip channels that you want to download. As many rows as you want.
00:14:50.720 - 00:15:09.264, Speaker A: Okay so this is basically the level of granularity that you can achieve in the random sampling. The peer to peer network and what goes on a certain peer to peer network dictates that granularity but it's still not everything. As a light node you don't have to necessarily download everything. Is that correct?
00:15:09.382 - 00:15:10.050, Speaker D: Correct.
00:15:12.900 - 00:15:57.520, Speaker A: Excellent. So that's the first access that we talked about is security and as a subset of security was the private random sampling which is a radically different networking model than what was needed before. The next axis that we can think about is how do we think about the scaling ability of these systems? How many nodes should to make it concrete, let's say there are N nodes, all of them have a certain bandwidth. What is the net commitment rate or data availability rate that a system like yours can achieve? Is there some heuristic idea on that?
00:15:57.590 - 00:16:13.030, Speaker B: Yeah, so we are still playing with those around. But in DevNet for example, the configuration that we have set is like two MB block sizes, like four MB, two MB coded in.
00:16:15.240 - 00:16:16.200, Speaker A: Block time.
00:16:16.350 - 00:16:51.836, Speaker B: Yeah, that's around the DevNet config is set at around 20 seconds at the moment. But we have not optimized it too much, we have just benchmarking. We will do a lot of benchmarking in general in the future. We will also do things like commitment generation optimization. So those kind of things are there but the current configuration is like two MB block sizes at 20 seconds. That is what we think. We'll experiment more in the upcoming testnets.
00:16:51.948 - 00:17:05.596, Speaker A: Awesome. So when you look at your avail network, do you think of this as like a peer to peer network and you're doing both consensus and data availability or is it a pure data availability network?
00:17:05.728 - 00:17:45.440, Speaker B: So of mean the validators need to compute the certainly so we are using substrate stack for that which we have like grandpa babe for the actual commitment generation for example. And at the moment the scheme is such that all validators compute the commitment individually as well. So after the proposal proposes then all to get on its majority consensus. So everyone has to commit it. But we are looking at certain alternatives in the future which might optimize that as then you know, we can experiment with block.
00:17:48.100 - 00:17:48.850, Speaker A: Yeah.
00:17:49.400 - 00:17:57.124, Speaker C: John, so to clarify, was your question around the bigger complexity throughput the data.
00:17:57.162 - 00:18:00.992, Speaker A: Availability throughput that you can achieve using this kind of like an architecture?
00:18:01.056 - 00:18:03.956, Speaker C: Is there a question around throughput and not the size of the commitment?
00:18:04.068 - 00:18:05.236, Speaker A: No, not on the size of the commitment.
00:18:05.268 - 00:18:06.068, Speaker C: Okay, I must have misheard.
00:18:06.084 - 00:18:06.424, Speaker A: My bad.
00:18:06.462 - 00:18:45.664, Speaker C: So the throughput, I think I have to refresh numbers. It might have been something like 1.4 megabytes per second, I don't exactly remember. But we're going to have a respectably large block size in the order of several megabytes and a respectably long block time. We aren't really aiming for a short block time. The reason for this is that the longer your block time, the larger your block, even if the throughput is the same. But the larger the block, the more you can take advantage of the fact that the size of the commitment isn't linear in the size of the block, but rather is the square root of the size of the block.
00:18:45.664 - 00:19:39.290, Speaker C: And therefore you get more advantage, you have more scalability benefit if you have larger blocks and a longer block time than if you had really, really tiny blocks and a short block time. Longer block times for the base network. Well, a lot of people say you need really fast finality for the base network, right? You need 400 milliseconds or you need 1 second, right? But it turns out that as people have used roll ups, users are perfectly happy with the fast instant soft confirmations that roll up operators provide. And as long as you're happy with that, which is perfectly fine assumption, right? As long as the roll up operators are honest, they will include the transactions they promise to include in that order. Then the base layer doesn't actually need blazing fast block times. So you can have slightly longer block times and take better advantage of this square root data commitment. Scalability property.
00:19:41.260 - 00:19:43.016, Speaker A: Matt, do you want to comment on that?
00:19:43.118 - 00:19:43.432, Speaker C: Right?
00:19:43.486 - 00:20:02.680, Speaker D: Yeah. So for the Ethereum Dink Sharding proposal, the throughput will also be around 1.4 megabytes. I didn't want to say 1.3 because you had just said 1.4. Yeah. So the throughput is supposed to be 32 megabytes for every twelve second slot.
00:20:02.680 - 00:20:13.708, Speaker D: And of the 32 megabytes, 16 megabytes will be the erasure encoded data. So the actual data that will be useful will be the 16 megabytes. And so that works out to 1.3 megabytes.
00:20:13.804 - 00:20:35.256, Speaker A: Something interesting here is in Dank Sharding, every node doesn't have to download all the megabytes per second. Each node can download much fewer. At least the quorum that you're certifying is not every node downloading everything, whereas in Celestia, I understand, at least as of now, all nodes are downloading everything. But both of you have the same bandwidth. Is there?
00:20:35.438 - 00:20:39.816, Speaker C: I'll let Matt answer that first. I have actually a lot to say about that, but I'll let Matt answer that first.
00:20:39.918 - 00:20:41.780, Speaker D: I didn't catch what you're saying.
00:20:41.870 - 00:20:59.408, Speaker A: I'm just asking. Just clarifying first that in Dank Sharding, every node, even though your total throughput is 1.3 megabytes per second, each node, each validator, does not download data at like 1.3 megabyte per second. Is that correct?
00:20:59.494 - 00:21:28.360, Speaker D: Right. Exactly. So in the first phase, where we are not doing the random sampling, the current parameter is to do four samples from the rows and columns. So you would download two rows and two columns and I forget exactly how much that is per validator, but that would be the current parameter. And after that, yeah, I would need to look to see what the numbers is for the exact row and column throughput.
00:21:28.800 - 00:21:29.548, Speaker A: Awesome.
00:21:29.714 - 00:22:25.992, Speaker C: Yeah. So for Celestia, we have kind of a few different ways to answer that question essentially, which is that the first is we don't just have full nodes and light nodes and nothing in between, where you either download all blocks like all the block data or you just do data availability sampling. But we have a node in the middle called a partial node. And these partial nodes, rather than downloading full blocks, they can either download a subset of all blocks or they can download sorry, they can download like entire blocks, but a subset of all the blocks, or they can download a subset of each block. This is configurable up to the end user. But these partial nodes still contribute to the security of the network because they can still produce fraud proofs. And more importantly, even if you eliminated fraud proofs and just had validity proofs, you still need that worst case of being able to reconstruct a block.
00:22:25.992 - 00:23:11.368, Speaker C: And partial nodes contribute to that because they will do things like fully download either a full block or a full row or a full column or however you want to configure it. There's an implementation detail. So there's this node type in the middle. Now, with respect to our validator nodes, if you're a validator, the initial implementation will require validators to fully download the block. And the reason for this is not that there's a fundamental thing in our technology because we use fraud proofs or anything, because again, you still need a synchrony assumption for reconstructing the block. And this is kind of the key, is that by requiring validators specifically to download full blocks, it means we avoid having to deal with the complexities of dealing with the synchrony assumption. If you had validators sample blocks to determine their availability, you totally could do this.
00:23:11.368 - 00:23:44.500, Speaker C: But then the networking and the assumptions around this and how tight your timing constraints are become much more complex to reason about because you have to worry about the case where the block is available if the whole network can reconstruct a block by communicating together. And that is like a much harder process to reason about. So I wish the Ethereum researchers well on implementing this. But subsequent implementations of Celestia, if these problems are ironed out, will not require validators to fully download blocks. They'll allow validators to simply sample blocks for their availability.
00:23:45.320 - 00:24:35.830, Speaker B: Just to point there in the sense. So our header structure is also constructed in such a way that when we want to increase the block, the number of commitments kind of increase in the block, which is not a very it's not a linear increase in that sense. Right? So we have not experimented with those at the moment. But once we get to more better testing conditions, we will experiment with those. Second thing is, like John said, not from engineering, we have not started that, but we have experimented with the idea of columnar full nodes because that is also another entity taking play part in the network and that will also, I think, be more prevalent in the future.
00:24:37.160 - 00:24:38.980, Speaker A: That could be a scaling path.
00:24:40.540 - 00:24:50.670, Speaker B: Basically you have validators and today you have validators and you have full nodes and you have the light clients and this is like another participant into the mix storing some of the data.
00:24:51.920 - 00:25:29.610, Speaker A: Great. I think we're coming to the close of the panel. So I want to switch into a broader question on all of you have taken very deep bets on this modular world and maybe the Celestia team coined maybe the modular paradigm and polygon has a variety of projects building a whole stack and Ethereum is shifted to a rollup centric world. I want to just get your outlook on where this paradigm is headed and what you think the interesting opportunities and challenges are.
00:25:29.980 - 00:26:23.290, Speaker B: So I think what we have learned a lot from our polygon journey is we have got the opportunity to talk to so many devs so many users. And what we have finally, the broader polygon thesis is in general not focusing on one particular. So we have a sheet of scaling solutions, right? So we've invested in ZK EVMs m three. In fact. We believe that in general, developers want, like different developers, different users want a variety of different use cases. So for example, when you talk to enterprise clients, right, so they want some version of the blockchain with different properties. Maybe they don't want to have the data on a shared chain, for example.
00:26:23.290 - 00:26:51.760, Speaker B: Some people want private data as well. Some people. So there's a variety of use cases. Enterprises have different DApps are different. There's a wide variety in terms of what requirements is. Right? And so what we thought is we should focus on getting customers or users or developers their choice. So if they want to run a roll up, for example, on Ethereum, that is their choice.
00:26:51.760 - 00:26:58.470, Speaker B: If they want to run validiums, that is their choice. If they want to run sovereign appchains, that is their choice. Right?
00:27:00.440 - 00:27:04.084, Speaker A: It's not a one size fits all. John yeah.
00:27:04.122 - 00:28:13.980, Speaker C: So throughout the years since we started building this, it's definitely been kind of a journey of learning. When we first started, a lot of people who first heard about the project didn't really understand what it was about and didn't really understand the use cases, how it be used and so on. So it took quite a while to get to the point that we are now that there's a lot of excitement around this notion of modular blockchains and the modular blockchain stack. Yeah, I guess I should talk about kind of from the early days when it was not exactly clear what applications we had to now when there's much more concrete vision of applications. So we have things like there are ways and mechanisms to use Celestia for data availability and provide this service to roll ups on Ethereum, and we call these Celestiums. I gave a talk at ETH Denver earlier this year. I think if anyone is interested in learning more and I think a colleague of mine, Evan, gave one yesterday or the day before also here at Ethamp DevConnect.
00:28:13.980 - 00:29:22.892, Speaker C: And there's also the notion of savmos, which I think Mustafa talked about earlier, where you have a settlement layer that runs as a solvent roll up on top of Celestia and then roll ups that as they exist on Ethereum, can use this as a kind of drop in replacement to Ethereum. And then finally we've really been pushing this notion that I really believe in is the end game for roll ups. This notion of sovereign roll ups where it's roll ups that run directly on top of a data layer and therefore have the ability to hard and soft fork, to change their consensus rules, to change their block validity rules with off chain governance. Which is not something that applications can do if they're built directly into an execution layer like DApps on ethereum or Role of Smart contracts on ethereum. Those to upgrade require something like a multi SIG or a Dao, some sort of on chain governance. And the entire Ethereum and Bitcoin ethos is that on chain governance is no bueno because on chain governance can be captured by whales. Right? So it's kind of weird that you have this entire notion that the core blockchain layer can't be captured by whales.
00:29:22.892 - 00:29:37.530, Speaker C: It does offchain governance, but then literally every single application on it does have on chain governance because it needs to upgrade. These sovereign roll ups are really, I think, the end game of roll ups where you can have the best of both worlds. You have off chain governance and you have shared security.
00:29:39.180 - 00:29:41.876, Speaker A: Thanks so much, Matt.
00:29:42.068 - 00:31:03.904, Speaker D: Yeah, so for Ethereum, I think that there's just different choices that Ethereum has to make, being a chain that already exists and a chain that's focused on decentralization at the very core. But what I am really excited about is that even though Dank Sharding has many things on the critical path to eventually reaching that end goal, we're working on an EIP 4844 called Protodank Sharding, which hopefully will provide a lot of the facilities for roll ups to start using what will exist at some point in the future, which is basically there will exist some oracle to provide a route that says this data was available. And as long as the rollups understand that this route was made available by the protocol, then they can trust and they can build their chain based on that or if there is ZK roll up, use that for their ZK roll up to do synchronous communication within that. And so I think that with 44 four, which is actually very similar to what John proposed in 2019 with 22 42, I think that as things continue to change over the next couple of years, at least we can start providing that facility that primitive for roll ups to build on this data availability layer that will start to exist in the next couple of years.
00:31:04.102 - 00:31:18.390, Speaker A: Awesome. I think that brings us to the conclusion of this panel. I think it's the right question to finish to lead into the next one. Thank you so much to the panelists for taking the time to be here are anurag, John and matt. Thank you.
