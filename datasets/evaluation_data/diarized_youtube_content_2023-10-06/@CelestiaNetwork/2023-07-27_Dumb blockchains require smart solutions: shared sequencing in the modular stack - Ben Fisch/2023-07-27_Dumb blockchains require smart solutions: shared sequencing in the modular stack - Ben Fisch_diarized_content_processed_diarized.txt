00:00:02.330 - 00:00:53.600, Speaker A: Right? So perfect, right? Dumb blockchains need clever titles. This talk will primarily be about shared sequencing in the modular stack and is somewhat related to the title, or maybe not. But as it will become clear, shared sequencers are designed as dumb or lazy ledger systems that don't execute transactions in just sequence. And that poses some challenges that require some solutions. First, just quickly, about our organization, espresso systems. We are a team of engineers, researchers and designers. We are building infrastructure that is going to assist roll ups to achieve better scale, better interoperability, and better security.
00:00:53.600 - 00:01:45.902, Speaker A: We are a very distributed team. I am Ben Fish and I am the CEO. And my colleague Jill was just introducing the event. She is our chief strategy officer, and there's a number of other executives as well. So let me start talking about this problem of sequencing in the rollup ecosystem and the modular stack more generally. But first, a starting point is to talk about what exactly are roll ups doing? Okay, so roll ups are horizontally scaling the application layer of blockchains, primarily. Today, roll ups are being built on ethereum, but of course, or on top of celestia too.
00:01:45.902 - 00:02:36.874, Speaker A: So if you view Celestia as an l one as well. But in general, the idea is that the layer one, all it does is just verify fraud or ZK proofs. It may also provide availability of data and some other services, but it doesn't execute transactions. And so as you add new applications, the reason why this horizontally scales is as you add new applications, you don't burden the layer one with having to execute for those applications. You can introduce a new set of servers that executes for that application and just proves the state result to the l one. Roll ups are applications that host other applications. So things like optimism or Zksync are general virtual machines which can host other applications.
00:02:36.874 - 00:03:24.586, Speaker A: But now we also have app chains that just execute their own application specific states directly. And all of these are horizontally scaling the execution layer of blockchains. Another key point, besides sharding of computation across applications, is that roll ups leverage heterogeneity in the network they're leveraging. The fact that realistically, in networks, not all nodes are the same. If we want to scale layer ones to be extremely decentralized, so that there's 12,000 nodes participating, and one of them is a raspberry PI. Or as I heard yesterday, apparently a Turing PI is a thing too. But then the weakest node obviously will not be able to compute transactions as fast as the strongest nodes.
00:03:24.586 - 00:04:18.330, Speaker A: So roll ups allow the more powerful nodes to do even more computation when we're talking about ZK roll ups. It's 10,000 times or more expensive to produce a proof than to even just execute itself. So we increase even more the computation of some powerful nodes, and this helps weaker nodes catch up. So the thing is, today roll ups control a lot. They control a lot more than just doing execution and horizontally scaling ethereum. In fact, they centralize the entire process of deciding which transactions to include and in what order. And this brings us back to a world in where there are central actors that have the power to discriminate, that have the power to impose monopolistic pricing or maximally extract mev from users.
00:04:18.330 - 00:05:18.580, Speaker A: And these are all the things that blockchains were promising to get us away from. If you think about it, the roll up ecosystem today is essentially web two applications that are being audited by an l one, right? But the actual processing and everything is really just being done by the web two application. It's just being followed and verified by an l one. So what is the solution? One solution is to separate ordering from execution, so that application layer roll up servers will execute transactions and not handle any of the ordering process, not handle any of the process of even making data available. The l one would not build or execute. Its roles would only include finality on transaction ordering, availability of transaction data, and the verification of state proofs. And users would submit to transactions directly to this decentralized l one.
00:05:18.580 - 00:06:59.998, Speaker A: So this has also been called a based roll up architecture, where you have the consensus network that users submit transactions to. Roll up servers, read a transaction stream from this consensus network, and then post a state route and approve that, then gets verified. Challenges arise from this, however. If the l one is not doing any of the execution, then how does it set prices for including transactions that doesn't execute? How does it verify that the fees that it's receiving are paid correctly? Does it need to have some kind of minimal state to do that? How is revenue that it's generating shared back with roll ups or applications? How do we prevent spam? Could the l one overload the proving network without compensation for work? All of these are challenges, not all of which I will address in this short talk today, but these are all things that need to be considered when working with such based roll up architecture, or more generally what I would call decentralized sequencer architectures. So we can also look at hybrid centralized base architectures where, and this has also been called escape hatches or l one inboxes. I would say that is a form of based roll up or related to maybe not exactly the same, where there is a designated roll up server that can order transactions, but users can also submit directly to the l one. So the l one orders some of the transactions, the designated roll up server orders others, and we have some way of deciding, well, maybe the transactions from the roll up server go at the head, maybe they go at the tail.
00:06:59.998 - 00:07:54.686, Speaker A: There's different approaches to doing that. And this hybrid architecture can be applied to any decentralized sequencing design as well. We can mix and match. So there's also perhaps a reason to separate the order finalization layer, at least logically, from the layer one. And why might we do that? Why would we use anything except for Ethereum's gaffer protocol today for ordering and making available transactions? Well, specialization, right? Ethereum makes some decisions on how it works to optimize for certain sets of properties. For example, Ethereum is extremely available. Ethereum is extremely decentralized, right? If you wanted to design a protocol that is still decentralized but prioritizes fast finality and can give faster pre confirmations, then that could be a reason to design another layer.
00:07:54.686 - 00:08:46.286, Speaker A: If perhaps it's something that's designed for high throughput data availability, that's another reason to design what I would call some layer one and a half that sits between the layer one and the layer two. So protocol modularity, opportunity to make different design trade offs, higher throughput, lower latency, faster pre confirmations when it comes to talking about faster pre confirmations, a common misconception is that Ethereum's finality is 12 seconds. This is not actually true. This is the average block time of Ethereum transactions must be several blocks deep to be confirmed. And it takes, in fact, 15 minutes for transactions to finally be finalized by Casper FFG, which is the finality gadget of Ethereum. Often users may consider a transaction final after five minutes. Some may consider it final after 12 seconds.
00:08:46.286 - 00:09:19.886, Speaker A: But that's very, very risky. You can read more about this by looking at Ethereum's discussion. Vitalik has a post on single slot finality where he explains that Ethereum's finality is actually 15 minutes, not 12 seconds. So let's say you wanted to design a decentralized protocol that could provide faster finality than Ethereum. Why doesn't Ethereum have fast finality? Well, it runs something called a dynamically available protocol, which means that even if 10% of the network is online, not 100%, it still can process transactions. It still can make progress. Bitcoin has this property, too.
00:09:19.886 - 00:10:21.822, Speaker A: This is in fact one of the innovations of consensus protocols in the last ten years that started with the Nakamoto consensus protocol. But we also have protocols that are on a different side of what's called the cap theorem, meaning they cannot be dynamically available, but they can achieve what's called optimistic responsiveness, which colloquially may be called fast finality. An optimistic responsiveness is the ability to respond as fast as the network will allow. When network conditions are good, the consensus protocol can give you instant finality. It also has an asynchronous fallback path, so that if the network conditions aren't good, it will eventually make progress. But it will stall if enough nodes go offline, right? Ethereum doesn't have that property. Ethereum cannot achieve fast finality because it will remain live even if a small participation set is online, whereas protocols like hotshot, developed by espresso systems, will go stall if too many nodes go offline.
00:10:21.822 - 00:11:14.660, Speaker A: But if they're all online and the network conditions are good, they can give you very, very fast finality, almost like a centralized sequencer. There are protocols that achieve neither property, like tendermint. So responsiveness, it's the idea. When the sun is shining, you get this high rate instant confirmation, but it's still robust when it's raining. So we call this web two performance. With web three security, we can also blur the lines between the physical lines, at least between the layers. When it comes to things like eigen layer, things like restaking enable you to incentivize, or at least subsidize, the participation of the l one nodes in this layer, one and a half, so that the same physical set of nodes are running it, and you're making ultimately the same trust assumption, but just the protocol properties are different.
00:11:14.660 - 00:12:01.570, Speaker A: So moving on to sort of a second challenge, fragmented liquidity and interoperability. It's beautiful that this application layer is being sharded by roll ups, but this fragments the interoperability. So now two applications can no longer call each other, they can no longer make function calls to each other. Your ave liquidity pool is no longer shared across all applications if they sit on different roll ups. Bridging across roll ups is complex. Atomicity is limited. So how can we recover this? And specifically, to what degree does sharing an ordering layer only and not an execution layer, which would make everything the same roll up? Right.
00:12:01.570 - 00:12:44.174, Speaker A: Does sharing an ordering layer only help? Okay, so this is the next question I want to explore. So there are three advantages. I will only focus on one. One advantage is simplifying cross roller bridging and atomicity, because you are sharing at least the same ordering protocol so you don't have to verify each other's consensus. The second is mitigating what I would call systemic risks of bridging overall and the opportunities for profit that they provide to adversaries. You can mitigate that and reduce it by sharing a consensus protocol. I won't address these first two, we have blogs on it, but I will focus today on explaining why this supports what I would say cross roll up building with economic bonding.
00:12:44.174 - 00:13:20.250, Speaker A: This is not the same as sharing an execution layer, but it gives you some very interesting guarantees. So first quickly on proposer builder separation. This is the way things work in protocols like ethereum today. It also works like this. In hotshot, there's a proposer that's elected by the consensus protocol in every slot to propose transactions. But this proposer, while it needs to sign and broadcast information, doesn't need to come up with everything that needs to go into that block that could be outsourced. In fact, it could run an auction among competing builders who are able to build an optimized block.
00:13:20.250 - 00:14:23.214, Speaker A: There are different things that this might be optimizing for. Generally, if we let the proposer run this auction, then naturally the builders who win the block auction are going to be maximizing the revenue of the proposer. And that's what motivates us to do some other designs which are designing like an ideal functionality that implements an order flow auction, which is optimal for users and stable for users. Could we design something where users are getting the best execution prices available to them, or guaranteed that their transactions don't fail? And this is something that teams like flashbots and Swab have been working on, and other teams too, and is sort of the future, I think, of sort of the order flow design of blockchains. But in order for this to work, the consensus protocol needs to be extremely decentralized so that the proposer doesn't just say, okay, you know what? Screw this ideal functionality. It's not good for me, it's not maximizing my revenue. I will just run my own auction the way I want to.
00:14:23.214 - 00:14:49.206, Speaker A: No, if it's extremely decentralized, the proposer gets elected once in a blue moon. It can either take transactions from the ideal functionality or have no transactions at all. It can't make a credible threat to users to abort the ideal functionality. And that's why decentralization is important. This is in fact why EIP 1559 works. It only works because ethereum is extremely decentralized. But with proposer builder separation over an extremely decentralized shared sequencing layer.
00:14:49.206 - 00:15:31.190, Speaker A: Decentralization is key. Then you can also introduce these builder functionalities on top, and these builders, or ideal functionalities can also start to provide some interoperability guarantees. Example, a builder could say to a user, okay, the user wants to trade on optimism and zksync. It sees some arbitrage opportunity. It wants the builder to include both of these transactions in the block and ensure that they both succeed. Because the builder is basically able to win this auction of a super block wholesale that sends blocks to both zksync and arbitram. It controls, basically has a lock on the state of both zksync and arbitram, and thus can make this guarantee to the user.
00:15:31.190 - 00:16:47.870, Speaker A: It can promise that it will include both trades in the same block, and it can also write this promise in a cryptographically authenticated way, so that if it violates this promise, then the user can go to some smart contract and slash it. Okay, so if I don't do this, then you can use this as evidence to slash me and you can put up a big collateral in order to give the user confidence that this promise will be satisfied. This is a simple example, but we could go through an example of even cross roll up flash loans. It will take more time, so I don't have time for it in the short talk today. But just to summarize, so espresso sequencer is one of these layer 1.5 sequencers. It sits between the l one ethereum and roll ups, although we're trying to blur the physical lines by using eigen layer, and it provides this consensus on ordering, but it also supports proposer builder separation, and we envision most blocks being built in that builder layer, which can actually provide the interoperability guarantees to users, which wouldn't be possible if you had different consensus protocols because builders would not.
00:16:47.870 - 00:18:05.270, Speaker A: Well, builders would have to simultaneously win many auctions, and that carries a lot of risk. A common concern is how is revenue from cross roll up bundles allocated back to roll ups? Another common concern is whether this leads to builder centralization. To address the first concern, the concern that builders will need to execute for all interoperable roll ups, and that may create a higher barrier to entry, leading to fewer competing builders. So first, I think it's important to recognize that builders and validators are not the same thing, and we don't require the same level of decentralization at each layer of the stack. We need to work from first principles and look at first of all, what does decentralization mean? Does it mean a lot of economic stake, does it mean a lot of distinct physical nodes, does it mean geographical? And what is required from each layer, given the, given the service that it's providing? Executing for all roll ups is not actually that hard. Maybe it's hard for 12,000 nodes, including a raspberry PI, maybe it's just hard for a raspberry PI. But executing is not a huge barrier to entry, and we don't necessarily need 12,000 competing builders, we just need enough competing builders to create competition in the market.
00:18:05.270 - 00:19:02.426, Speaker A: That drives down monopoly pricing, not necessarily 12,000 of them. So it's unlike the decentralization requirements of proposers. And in fact, if you have a highly decentralized base layer or consensus protocol, then proposed, that's what, that's what enables proposer builder separation, that's what makes these proposers more passive and able to be compatible with ideal functionalities that are designed in an ideal way for users, that are thus stable for users. Furthermore, the barrier to entry is much higher without shared consensus, because it requires more capital to win simultaneous auctions. The builder absorbs more risk, and it can't promise user atomicity without this risk. This phenomenon of builders executing for all roll ups is an inevitable consequence of the user desire for interoperability. It will happen no matter what.
00:19:02.426 - 00:19:55.174, Speaker A: And so I would argue that designing a shared sequencing layer is going to lower the barrier rather than create more asymmetry among builders and increase the barrier, which is the alternative of many independent roll ups with their own sequencers. This is a small meme about that. Where there's a will, there's a way builders that execute for all roll ups anyways will find a way to do it. Shared sequencing enables more builders. I don't want to go into the revenue sharing problem because I'm out of time, but I just want to say it's connected to order flow auctions. And there's a cool idea of splitting up slots, so that at even slots you don't have cross roll up bundles, and it's very easy to divide the revenue. And in OD slots it may be trickier, but you'll get a good approximation from the even slots of what to do in the OD slots announcements we have Adopio Testnet.
00:19:55.174 - 00:20:27.080, Speaker A: We also have double shots of espresso available at our booth. But the second testnet of espresso sequencer called Dopio is now released, and it features an end to end integration with Polygon Zke EVM, a fork of Polygon ZKE EVM next, we're working on opistack integrations. We're working with Caldera, Spire catalyst, injective alt layer. We recently won an optimism sequencer, decentralization RFP, so we'll be working on that. And here's a little picture of our little growing ecosystem. If you want to join this, please let us know. We're happy to talk to everyone, so thank you so much.
