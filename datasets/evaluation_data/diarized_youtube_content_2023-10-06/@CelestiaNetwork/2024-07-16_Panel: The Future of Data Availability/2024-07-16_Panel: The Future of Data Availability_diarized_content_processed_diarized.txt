00:00:02.040 - 00:00:44.154, Speaker A: Hello, everybody. I'm Bartik from l two beat. And every year I have a pleasure of hosting this amazing panel on Da on this summit. So we met last year, we're gonna discuss Da this year. Hopefully next year it's gonna be equally fun. So I kind of, like, did a bit of a reshuffling here, maybe wasn't smart, but, you know, we're gonna see. So I put togru right next to me because, you know, he's supposed to represent Ethereum.
00:00:44.154 - 00:01:29.864, Speaker A: I have no idea why, but, you know, there's like a tradition on this conference. Last year he did the same. And that's surprising because he's been sometimes, I don't know. I mean, let's see what he's going to say about Ethereum da, right? But, you know, I'm, like, thinking that he might represent Ethereum here. And then on the other side, we've got two projects. We've got Mustafa from Celestia, and we've got Prabhal from Aveil. And again, from the Ethereum point of view, I kind of see these guys as being like an external DA provider trying to kill Ethereum's da robot or whatever.
00:01:29.864 - 00:01:56.260, Speaker A: So maybe they, like, on the opposite side with Toggl. Again, we'll see. And in the middle we've got Matt representing Eigen Da. And I have no idea where Eigen Da is, so I just put him in the middle. So we're gonna find out. Right? So this is a setup. And I put myself right next to Toggl because my background is Ethereum.
00:01:56.260 - 00:02:50.260, Speaker A: So I'm really glad that the module people keep inviting me, even though I tend to take Ethereum's point of view. But we're going to see. So my first question, I guess, to the panel is that last year, when we discussed Da, like, every single project was either a white paper or a PowerPoint or maybe a testnet. And my main question from last year was like, where do you think the space is going to be in a year time? And we got some interesting answers. I do remember we're going to see thousands of roll ups being in the production or so. And I got really scared at that time because I had to beat. We are trying to evaluate all of them, but so far, thankfully, we only have less than 100.
00:02:50.260 - 00:03:06.690, Speaker A: But anyway, I want to ask you guys, do you like, how do you see the space one year after? Is it where you hoped it to be? So maybe I'll start with the left side of the panel.
00:03:07.590 - 00:03:59.800, Speaker B: Yeah, I think I still hope that your job gets tougher, tougher, because not only we see only ethereum clones of EVM roll ups, but also application specific ones. So when I say thousands of roll ups, some of the types of roll ups are not, you know, defined in the roll up const as a roll up construct. In the familiar world, we have seen, you know, various numbers flowing around and some of them, you might also argue that they are not even a roll up. Right? And so how you define a roll up also plays a role. But on the other hand, that's where we would like to be. We would like to be in a place where there is lots of application specific roll ups and then that's where the scalability should appear.
00:04:00.580 - 00:04:06.720, Speaker A: And in terms of product, the avail. I mean, last year you were at the testnet, where are you today?
00:04:07.180 - 00:04:11.080, Speaker B: We are still at the Testnet and we are going to launch mainnet very soon.
00:04:12.500 - 00:04:58.040, Speaker C: Mustafa, we will see 1000 roll ups, we're going to see 10,000 roll ups, even maybe 10,000 roll ups. I don't think I ever said it's going to be happening in a year, but it will happen for the next talk. The moderator state of the union I actually did a comparison to how many roll ups we had last year to this year. And we have an insane amount more of application specific roll ups this year than last year. Last year was something like ten, this year we have something like 40. I did actually archive wayback machine to look at the LTB website a year ago and now, and there are a lot more roll ups. And the main difference is what's happened is that this time, this year there were zero blockchains that supported data availability.
00:04:58.040 - 00:05:27.110, Speaker C: Blobs, zero. This year there's three. EIP 4844, Celestia, Eigen Da. And because now we have more blob space, we're seeing a huge influx of developers deploying roll ups using roll up as a service providers like conduit, Caldera, gelato and the likes. And there's literally hundreds of roll ups in the pipeline with those roll up as a service providers because they make it so easy to deploy a roll up. So whether you like it or not, it will happen.
00:05:28.530 - 00:05:45.872, Speaker A: Matt, like I said, I mean, I put you in the middle because I don't know, I mean, are you guys building an external da solutions? Are you aligned with Ethereum? Are you like part of ethereum route now? Who are you with your solution? And are you live as a product?
00:05:46.056 - 00:05:47.056, Speaker D: We are live.
00:05:47.208 - 00:05:49.472, Speaker A: What are the roll ups that are using Eigen da?
00:05:49.536 - 00:05:51.032, Speaker D: And we are currently doing ten megabytes.
00:05:51.056 - 00:05:53.020, Speaker A: Per second on Mainnet for something.
00:05:53.480 - 00:06:26.348, Speaker D: So appreciate it. You can hear me now? Yes. To confirm we are live, we launched live Q two mainnet and we're doing ten megabytes per second. And eigenda is a data availability layer, it's not a blockchain. The focus is for data availability. And yeah, current roll ups that are using, whether they are at Testnet, moving to mainnet soon might be like layer n a team like Celo. We've got a variety of roll ups, but also non roll up use cases that are of interest.
00:06:26.348 - 00:06:50.358, Speaker D: Essentially, data availability is useful for things that you want to make publicly verifiable. And we see that there's a massive space obviously in the, the roll up space and the abstractions that have led from things like roll ups as a service. And we expect the demand for rollups to increase and hopefully continue to accelerate on that front. So look forward to it.
00:06:50.454 - 00:07:11.910, Speaker A: And in terms of alignment, do you consider yourself to be like a competition to avail Celestia or are you trying to find the middle ground between, I don't know, who is your primary user? Are you competing with Ethereum or you're aligned with Ethereum? I mean, how do you like position yourself?
00:07:13.330 - 00:07:50.300, Speaker D: I wouldn't say that. I think generally you have different options, right. And data availability is like an operational key, operational aspect of roll ups and other services. It's a data availability layer that is generalized and can be used for anything. And as far as the alignment side of things, I mean, I guess we're secured inspired by dank sharding. So what that means is horizontally scaling. And so for us, decentralization means that we can get increased throughput and increased scaling.
00:07:50.300 - 00:08:15.280, Speaker D: And on that front we're secured by ethereum. So over 3 million ethereum today, as well as Eigen token. So additional quorums. So I would view them as. It's an option that applications that within the ethereum space, it can make a lot of sense working within, but also non ethereum like roll ups use cases. If you have anything else.
00:08:16.260 - 00:08:23.280, Speaker A: Yeah, Tagril, I see you're itching for some hot take here.
00:08:23.580 - 00:09:25.980, Speaker E: So firstly, I would say ethereum is actually by ethereum unlike everybody else here. And secondly, I think, I hope that we don't get 10,000 roll ups for your sake as well as for everybody's else's sake. I think the meta of dying l two s becoming l one's becoming l two s is dying a bit. So they're going to become dying l two soon there are still going to be quite a few general purpose roll ups, but I think the future is shifting more towards app specific roll ups. Again, I'm not sure how many you actually need, how many use cases need their own block space, but there are certain reasons why you would do that, and there are even l two s popping up on Solana now. So I think the future is more correlated to app specific roll ups rather than people just launching 27 other op stack chains that have basically nothing different to every other op stack chain.
00:09:27.640 - 00:10:13.894, Speaker A: So it seems to me that we are all betting on the future need, in a way, for abandoned block space. At least some of us are. And today I've checked that the price of posting data to Ethereum is actually surprisingly cheaper than posting to Celestia. I'm not really sure. It fluctuates a lot. Sometimes it can be quite high and sometimes it's just next to zero. So I can definitely understand some people saying that we need to focus more on apps and not the infrastructure.
00:10:13.894 - 00:11:10.378, Speaker A: And you probably must have heard these times types of comments. We're building a lot of potential blob space. Are we hoping that that's going to enable new use cases or it's just too much at the moment and we should as a community say, hey, with that much blob space, maybe we should focus more on applications. I know that Mustafa, you had some take on that recently on Twitter. So what do you think? Do we actually need that much block space? Considering, especially from the Ethereum point of view, that ethereum very soon is going to introduce peer to us, that's going to increase quite substantially the amount of blob space available in Ethereum. And ethereum is moving really, really fast in this direction. At the same time, I've just seen that celestial has promised amazing amount of blob space.
00:11:10.378 - 00:11:23.210, Speaker A: Same with eigenvalue. It's a lot and seems like we are building for some future world. But is that future worlds going to come soon?
00:11:24.150 - 00:12:16.708, Speaker C: Yeah. So first of all, it's not true that Ethereum blob space is cheaper than sedation. Right now, if you look at the data, it's all the way around. But in terms of block space, either way I look at it, is that I look at it in terms of induced demand. Now, as I said on Twitter, I think what you're referring to, I asked the question is why is it that 16 years later, since the creation of bitcoin white paper, the original use case of crypto, which is payments, we still haven't achieved wide scale adoption of that, like venmo is newer than bitcoin, but venmo has succeeded more than bitcoin, at least in the us market, at payments. And I think a lot of that does have to do with the supply of block space. Because if you look at what happened in the early 2010s in the bitcoin ecosystem, for example, there was this whole movement where, like, bitcoin was never.
00:12:16.708 - 00:13:12.050, Speaker C: Bitcoin never had this narrative about being a store of value. The original narrative was that everyone's going to be using bitcoin to pay for coffees and for payments. But what happened is that the bitcoin community convinced a whole bunch of people to start accepting bitcoin for payments. Even Microsoft. Microsoft accepted bitcoin payments for things like Xbox games. And then what happened is around 2013, 2014, the transaction fee skyrocketed and less retailers actually stopped supporting bitcoin for payments because there simply wasn't enough supply. So I think a lot of it does have to do with the fact that if we keep building infrastructure that simply does not have, that simply falls over after two years when there's much of demand, that does fundamentally limit how much adoption you're going to get.
00:13:12.050 - 00:13:26.110, Speaker C: Like, even Solana, Solana claimed to be 50 ktps. You can actually do only two ktps. And since it's been had a lot of demand, the user experience, even for simple things like payments, have been terrible.
00:13:27.690 - 00:13:50.910, Speaker A: So how does it that the fee market on celestial compares with Ethereum? I said that right now Ethereum is cheaper because there's a data that I got in the morning. But, you know, again, it fluctuates, right? So it may be temporary and like few weeks ago it was super expensive because something happened, you know, and, you know, what was that?
00:13:50.990 - 00:13:52.690, Speaker E: The casing air drop happens.
00:13:54.030 - 00:14:13.580, Speaker A: So we all know that, you know, the price can like fluctuate. How is price set up on celestial compared to Ethereum? Same question for Matt. I'm not familiar with how prices are actually set on eigen da. And who decides on the prices on eigen da?
00:14:14.000 - 00:14:48.970, Speaker C: Yeah, so in Celestia, it's just a standard fee market kind of similar to Ethereum. So right now, both on Ethereum and Celestia, blob space is basically practically free. The only difference right now is that in Ethereum, there's a higher, much higher overhead cost to kind of like having a block carrying transaction. So that's kind of what makes you much higher, much more expensive right now than Sylvester. But other than that, the actual pair byte cost of the block space is free, which is just that the overhead costs of submitting a transaction is higher on Ethereum.
00:14:51.790 - 00:15:13.034, Speaker B: Yeah, and on avail. Also, the transaction fee mechanism is such that there is, of course, a standard base fee, but on top of that, depending on the amount of data that you want to publish, as well as the congestion that is there over a period of time, the prices are going to fluctuate. So I think it's standard free market practices. Yeah.
00:15:13.082 - 00:16:09.752, Speaker D: I guess in a brief, like mention on the Ethereum block space, we do expect that the costs are going to go down as this roadmap is continuing to be implemented. We've seen that. We also have seen spikes of volatility, in fact, where blobs became more expensive than call data. So being able to have some predictable prices or expected costs is potentially useful on that front. So how Inda is structured is based off of the amount of data throughput that you need. So there are various use cases or various requirements, maybe for something that is, posting proofs might require a higher data throughput than a smaller, just like transaction sort of commitment hash. So it's based on the amount of data you want to post, but also, let's say, market conditions and some minimal exposure to that.
00:16:09.752 - 00:16:27.840, Speaker D: But we also offer the ability for reservation of bandwidth. So what I mean by that is being able to guarantee certain bandwidth for a set price for a certain duration of time, over a certain period of time. So you can have more expected costs, essentially on data availability.
00:16:31.220 - 00:16:43.720, Speaker A: Talking about just that, you represent Ethereum, but you're also working for Scroll. Scroll is a roll up, right. Pays a lot of, I guess, for data on Ethereum. Like why.
00:16:46.620 - 00:17:47.146, Speaker E: We were we, when we initially set up to build it, we decided that we want to minimize dependencies as much as possible, third party dependencies. So I, and especially, I think for a general purpose roll up, it's very important that you don't introduce any unnecessary dependencies, even if it comes at a trade off of having a higher cost. Because if it's an app specific roll up, let's say, and it's doing gaming or something like that, the failure of that DA provider is not as much of a big deal as a roll up that holds potentially billions of dollars. We currently have around $1 billion bridged into scroll. So imagine if for some reason, the DA provider, a third party DA provider, stops working at some point. That will be catastrophic. And so we made a conscious choice to trade off a bit of transaction costs, like make them higher, but make the protocol itself more secure.
00:17:47.146 - 00:17:55.030, Speaker E: From the perspective of being as trust minimized as possible, both in terms of correctness assumptions and liveness assumptions.
00:17:57.570 - 00:19:23.080, Speaker A: So I wanted to touch a little bit more on that point. There was a great question from the audience in the previous talk when we discussed enshrined versus non enshrined da, right? Like an external Da versus enshrined Da from the point of view of Ethereum. And just to set the context, my personal understanding of what would be the enshrined DA is if you are running a full node of ethereum, then with enshrined dA, your full node with essentially be able to check if the data was made available, right? And if there was a data withholding attack, the data wasn't available. Your full node would simply see that block as being invalid. So even if the majority of the Ethereum validators would like confirm such a block, your full node would reject such a block. And that would likely result in some kind of a user activated fog, and that is considered to be super secure. On the other hand, as Sriram from Eigen da once said, the external DA looks like data available to Ethereum in the sense that the Ethereum cannot do data availability sampling for external DA solutions, right? So you need some kind of an attestation, like for example the Blobstream bridge from celestia.
00:19:23.080 - 00:20:41.332, Speaker A: Essentially celestial validators are attesting that the data was available so you are not protected from the majority attack. You will be protected if you are a sovereign roll up. So my question now is if there is enough blob space on Ethereum in the future and you are in Ethereum roll up, then arguably using Ethereum DA is more secure. If you are a sovereign roll up, then arguably using celestial or avail or Eigenva I'm not sure is better because you can yourself detect the data wasn't available and that the network is under attack. Would that be a fair statement moving towards the future? Would it be fair in other way to say that if there is enough blob space for roll ups on Ethereum, then it's maybe from the security point of view better for them to actually use Ethereum da? Or is there something compelling other than price to use external DA given that the security is slightly. It's different, right? There's a different security assumption. How do you guys think about this problem?
00:20:41.516 - 00:21:59.310, Speaker B: So essentially, I think I would love to have the world that black and white as you painted. But to be honest, every one of the roll up implementations have nuances when it comes to using external DA or even Ethereum dA. For example, what can happen if the sequencer doesn't publish on an external DA? Is that a trusted sequencer, which means that you always trust that the sequencer has published it? Or are there validators inside that roll up system which attest to that fact, irrespective of what the external Da committee or the validator said they attest or not? That is not what is the source of truth in the settlement contract that has to be determined first. And this is a conversation that we have been having with various roll up teams right now, and each one of them has made different trade offs. Some have made trade offs that you optimistically accept that the transaction has been published and you just move forward. You move forward because you just rely that it will do it job. And Ethereum, it doesn't matter for Ethereum you just trust the external committee and we have seen production grade roll ups with huge amounts of TVL doing that.
00:21:59.310 - 00:23:18.920, Speaker B: On the other hand, there has been designs in which we have proposed and we have been rejected where we wanted to have, you know, not only verification by the attestation bridge, but also verification by the validator set. Because the roll up doesn't cannot handle invalid batches. It always assumes there are some roll up constructs which assumes that all published batches are correct, irrespective whether the execution attestation has been correct or not. And hence like depending on what kind of roll up you're talking about, what is the implementation there? What are the security assumptions? What are the participants that are there? Who are the watchtowers? Are there a need for watchtowers? Is it a ZK verified? Is it not? Are we talking about liveness? Are we talking about safety? Is it centralized sequencer? Is it multiple provers? It all depends on those kind of design choices. On the other hand, it's not just about the cost. The cost is of course one of the major factors, but it's also about what kind of orientation do you want for the application? For example, you want lite client verifiability for that application. The application users want to verify the execution and the data availability on Ethereum today at least, you cannot do it because there is no database sampling.
00:23:18.920 - 00:23:29.452, Speaker B: With Dank sharding it will come. So there are also product requirements which guide this and not just cost. So again, it's very nuanced, but happy to, you know, get more.
00:23:29.596 - 00:24:08.012, Speaker E: But just to add here, a lite client that only does data availability sampling does can still follow an invalid head. Because if you don't verify the execution, then the execution can be invalid and all the full nodes will drop it that block. But your lite client will just assume that that block is correct. So even if Ethereum doesn't have das at the moment, no other chain that is live right now has a lite client that verifies both das and execution. Which means that you're to a certain degree still trusting the multisig of the.
00:24:08.036 - 00:24:17.748, Speaker B: Consensus, but you are now changing the premise. Like we are talking about data availability part only and we are talking about blockchains, which doesn't support too much execution.
00:24:17.804 - 00:24:28.398, Speaker C: Right, but the thing is, hold on, your statement is also true when Ethereum supports dassing until Ethereum supports ZK proofs of the execution environment.
00:24:28.494 - 00:24:45.462, Speaker E: Oh, but I'm not disagreeing with that. I'm not saying that Ethereum is going to have it. I'm just saying that no other chain has it either. Because even if you verify das right now on celestia, for example, it can still be an invalid block. So verifying just data availability is not sufficient.
00:24:45.566 - 00:24:56.540, Speaker C: Yeah, but the point is that celestia has minimal state. So you can run a partial node that does execute all the state, but does the sampling, which is lighter than a standard full node for ethereum or bitcoin, for example.
00:24:59.360 - 00:25:19.976, Speaker A: But Mustafa, in general, would you kind of agree with the statement that external das are more, I don't know, useful, I guess, for sovereign rollops rather than the Ethereum rollouts going forward in the future, assuming that there is an abundance of a cheap blob space on theory?
00:25:20.088 - 00:25:27.060, Speaker C: Well, that's a loaded question. First of all, what do you mean by useful? And secondly, what do you mean by sovereign? Those are very controversial words in themselves, right?
00:25:27.880 - 00:25:43.738, Speaker A: So like sovereign meaning that, you know, they define their own ordering. You know, like they don't follow Ethereum, you know, ethereum reorgs, they may like go their own way and whatnot, right? They don't follow the Ethereum consensus.
00:25:43.834 - 00:26:04.738, Speaker C: Sure. I mean, look, the way I would put it is the following. I would say two points. First of all, the idea is that celestia scales any ecosystem. So yes, it can scale ethereum l two s or validiums or optimum chains. It can also scale bitcoin l two s. It can also scale mina l two s.
00:26:04.738 - 00:26:54.486, Speaker C: And someone is even building a celestial da bridge to Solana so that you can verify the a proofs on Solana. So and then of course you have sovereign roll ups or native, let's ignore the word sovereign now, because that's a very loaded word. No one even. I don't think anyone on this panel even agrees what it means, or let's call them native Sydney roll ups. Yes, you can use them for all of those things. And that's the first point, I would say, and it does make sense for all those use cases. And the second point, I think, look, fundamentally, the fundamental fact is that, yes, everyone on this panel agrees that if you're Ethereum roll up or ethereum l two, using Ethereum, inheriting Ethereum security and posting it on Ethereum is the most secure solution if you need that level of security.
00:26:54.486 - 00:27:30.514, Speaker C: But the fundamental reality is that Ethereum, for good reason, needs to be very conservative with this technological roadmap. It can't jump straight away to 16 megabyte blocks. Right now it's got 375 kilobyte blob space, fair block. Even after peer das, it's not going to jump to 16 megabytes. The current discussion is that it will jump to 1. That's still only a few hundred tps. So the alternative DLA is they simply, they provide a way to have like ten x 100 x 1000 x as their three port while still giving you the ability to sell to Ethereum.
00:27:30.514 - 00:27:34.710, Speaker C: And yes, there is a security trade off, and everyone acknowledges that.
00:27:38.170 - 00:27:47.390, Speaker A: Anything to add, Matt, from your camp or you kind of agree mostly with this characterization that we've just put forward?
00:27:48.530 - 00:28:27.388, Speaker D: Yeah, sure. I can maybe give it a little bit more color. And I agree, the use of data availability depends on your use case. Depending on what you need to prioritize, then it makes maybe a little bit more sense. If you need like, depending on your needs, essentially that is like, there are costs prohibitive, you know, with DA maybe on the l one itself, but we again expect that to decrease. So there's not just costs that maybe is boiled into decisions for roll ups or other use cases. Like we expect that AVSs will be consumer of DA in the future.
00:28:27.388 - 00:29:23.786, Speaker D: You need to be able to publicly verify any of the data there. So the bottleneck isn't necessarily on even like the execution of like these state transition functions anymore, but it begins to also place a lot of pressure on the amount of data that you can post, the throughput that you have. And we've seen that as far as teams that are pushing extremely aggressive transactions per second, like a mega eth, right, where that is their focus of maximizing the amount of transactions, and then DA becomes their bottleneck. And high throughput is then a key requirement for them, not just cost, but also high throughput as well. As underlying security. And that for Eigen, DA is inherited through Ethereum. So I would say that there are different designs, require different trade offs, and depending on your use case, I think that it's a complete evaluation.
00:29:23.786 - 00:29:25.750, Speaker D: It's not like a one size fits all.
00:29:27.570 - 00:29:31.562, Speaker A: So, yeah, you wanted to know.
00:29:31.626 - 00:30:29.742, Speaker E: I was just going to say that paradoxically, from our perspective, the biggest bottleneck on Ethereum right now is execution costs rather than DA or anything else at the moment. Yes, yes. So for example, we are currently the, the fastest l two in terms of finality. But the trade off there is, we pay a lot for settling quite frequently. And basically in our next upgrade, we'll add batching of aggregation of proofs for a lot of batches and basically delay the finality further than what it is now, purely because the execution of Ethereum is prohibitive. And it's going to become even more prohibitive once rollups becomes decentralized, because there are certain things and certain overheads that you need to do on chain. And then to add to that, once you add multi provers and multi verifiers, computation actually becomes super expensive on Ethereum.
00:30:29.742 - 00:30:53.990, Speaker E: And so unless you have like thousands of tps of demand, there's no way to amortize it properly. And so I think what I'm trying to put pressure on Ethereum and like a lot of other builders of l two s are trying to put pressure on Ethereum to actually increase the gas capacity because at the moment, decentralizing and also improving the security of l two s is actually quite cost prohibitive.
00:30:55.570 - 00:31:45.460, Speaker A: Okay, now I kind of wanted to switch gears a little bit and maybe get more technical so that certain concepts are more clear for the audience. For myself as well, quite recently, a newish, I think, term has been thrown around in the discussions around Da. The old term was data sampling. Das, like people say das, you should probably know what is data availability sampling and what is its role. Newer term is vid. I had to check it, you know, a few weeks ago. I think it stands for verifiable information dispersal.
00:31:45.460 - 00:32:35.900, Speaker A: And I believe that this is a technique used by Eigen Da to disperse data amongst, you know, different validators. Today I've heard that Celestia wants to do sharding to achieve, you know, a bigger blob space. On the other hand, Dankshad Inc. Also in the future will not require full nodes to download the full blob, just a portion of it, which is, by the way, the case, I believe, in Celestia right now. And indeed the current ethereum for 844. I'm not really sure about avail, frankly. Can you like guys sort of set the stage for these concepts? Are we talking about completely different things or, you know, or these are like similar techniques in a way and.
00:32:35.900 - 00:32:48.728, Speaker A: Yeah, what is this? You know, like vid, the verifiable information disposal. Maybe I'll start with Matt and then let me hear from Mustafa if this sharding is the same thing or not.
00:32:48.904 - 00:33:35.952, Speaker D: Sure that is, I mean, verifiable like information dispersal. Personally, I think that's less of maybe like that is an aspect of the architecture, but less of maybe directly related to data availability sampling. That is more. So to give a little bit of context as far as how it works. And when I mentioned it's inspired by dank charting, essentially you have a blob, you have amount of data that's sent, it's chunked into blobs and dispersed amongst a network of operators. These operators are AV's operators in the case of Eigen DA, and they secure proportional amounts that they have staked. Essentially you can only secure the amount of data that you could possibly safely and securely do so.
00:33:35.952 - 00:34:08.450, Speaker D: So the dispersal is this sort of portion that goes from dispersing to those operators and ensuring that it is dispersed. But the data availability sampling is a separate aspect and ensuring that once it's a different problem to have it dispersed than to make sure that it is available. So there's two different sides is being able to ensure that that data is then made available after the fact and not necessarily just dispersed and dispersed forever. So if that distinguishes.
00:34:10.910 - 00:35:24.914, Speaker C: That sounds about right. I think, yes, this is. I think you also talk about something called proof of retrievability, which is a very old concept from like the early two thousands that is very similar to data availability sampling. It's just the main difference is that proof of retrievability and verifiable information vessel proves to you that the information has been dispersed or to some service, or can be retrieved from some service, but it doesn't actually prove that it's actually being published to the Internet such that anyone can actually download it. But to answer your question about their sharding, yeah, I mean, you brought up the point that with, for example, down sharding word map, the idea is that consensus nodes or blockbuilders or validators, they don't have to download the full block to verify it. So it is the case that in Celestia right now, you do have to download the full block if you're a consensus participating node to build on top of it. And the reason for that is because Celestia relies on bad encoding fraud proofs, whereas dank sharding uses kzgs to prove the correctness of the Asia code.
00:35:24.914 - 00:36:12.508, Speaker C: And because we use bad encoding fraud proofs, there's this five minute or ten minute fraud challenge period. So obviously validators cannot wait that long to build on the next block. They only have 10 seconds, right? So I will say two things to that. First of all, I'm skeptical of the scalability advantages of making it that valid is that I have to download the full block to build on top of it. And the reason for that is because that only makes sense if you assume a perfectly distributed staking setup where they have 1000 different validators and they all own 0.1% of the stake each, such that they're perfectly uniform. But what happens in practice, and if you look at Ethereum today, there's three validators or a small number of validators that control two thirds of the stake or most of the stake.
00:36:12.508 - 00:36:55.780, Speaker C: And so what happens in practice is that most validators that actually control most of the stake have to download all of the block data anyway. Where this is advantage, where this does have provided advantage is for like home stakers that have very little stake. The homestakers now have to download the whole block. But I'm very skeptical in the first place of optimizing for thousands of home stakers. Instead of optimizing for a Nakamoto coefficient that makes sense. For example, like in Celestia, we have 100 validators, and then having fast immediate finality, we have ten second finality. For example, instead of optimizing for thousands of validators, when you have, when Nakamoto coefficient is very small anyway, and having five minutes, much lower finality.
00:36:55.780 - 00:37:18.380, Speaker C: But the second point I would mention is that we do actually also, as you saw in Ismail's roadmap talk, we do actually also want to and have plans to zk prove the correctness of the erasure code. But our current thinking is that we want to skip straight to seeing if we can stock proof these erasure code rather than using KCG to do so.
00:37:19.320 - 00:37:23.740, Speaker A: But would you be equally skeptical of the current Eigen DA architecture?
00:37:24.890 - 00:37:40.830, Speaker C: Yeah, I'm also skeptical of that for the same reason. I know that Eigenva claims that you can scale the number of nodes, but again, that's only the case if you assume a perfectly uniform number of distribution of stake across 1000 nodes, which we've never seen in practice.
00:37:43.570 - 00:38:11.080, Speaker D: Sure, Eigen DA doesn't have consensus on top so there is no concept of consensus within Eigen DA. It is a data availability layer. So in the fact of this data is dispersed to operators that is made available so that full nodes, if required, can retrieve the data to ensure that the state transitions are made. So I guess that's like one point just of clarification that there's not additional consensus for.
00:38:11.420 - 00:38:18.674, Speaker C: We're talking about the data availability nodes. Talking about like you're dispersing the data to 1000 nodes and the more nodes you have, the more data you have.
00:38:18.852 - 00:38:53.482, Speaker D: Yeah. And the operators of a data availability layer aren't also ensuring the correct state transition functions. Those are on the services that would require use. Right. So correct me if I'm wrong, but the roll ups that are using this data availability layer would retrieve that data and if required, would also download that to a full block, full node rather, and ensure those state transitions, let's say, were made correct. And there's different, it's no, but the.
00:38:53.506 - 00:39:45.090, Speaker C: Argument that I'm making, or that Bhartik is asking is that the scaling story for Eigen DA is that you can scale with number of nodes, right? So the more nodes you add to the network, the more operators, the more like validators in this AV's, in this I can AV's, the more data throughput you have. But what I'm saying is that that only is true if you assume, like if you're saying you have ten megabytes per second with 100 validators, then you have a gigabyte per second with a thousand. I'm saying that's only true if you're, if you make the assumption that. Yeah, exactly. It's only true if you're making the assumption that the stake is equally distributed to the thousand values, when in practice what will happen is ten value is, I think is the case right now. Six, like five ballet is in the Eigen dA, Av's control, two thirds of the zig.
00:39:45.750 - 00:40:47.140, Speaker B: Plus all of this, plus there are many more nuances which we are missing. Like the disburser right now is centralized and it is able to send ten megabytes per second as they claim. And with one GPS connections and such, it's yet to be seen, like with thousands of operators. What is the requirements for this disbursement? First, then there are other things also like we are talking about two very different kinds of systems here. One is consensus driven and the other is committee driven. And if you take a look at the protocol level, guarantees that both of them provide they are different, and not only from the protocol level, the protocol participants, but from the user level, the requirements are very different from data availability sampling. It's about the user level guarantees and not the protocol participant guarantees in that sense, because the database sampling says is that I do not want to rely on whoever that is.
00:40:47.140 - 00:41:50.616, Speaker B: But the verifiable information dispersal, that is about how does the information get disbursed within the protocol participants. And they are kind of different things in terms of like, I think a lot of different concepts were covered, but in terms of stake distribution, there is problems with that because the incentives are not aligned correctly in my worldview. I think the delegated proof of stake and the concentration of stake are kind of related because if the incentives are not there to delegate in the least staker, then it's there to be seen what is happening. And that's why at a whale we are trying to do nominated proof of stake. We are trying to do fragment in a bit to make things much more decentralized, the Nakamoto coefficient to be much higher. And yeah, I mean, that's why there are many different topics. Each one of them should be discussed separately and only then we would be able to talk about the differences in the deltas.
00:41:50.728 - 00:42:58.516, Speaker A: So before we run out of time, there's just one more concept that I wanted to clarify, which I think is enormously misunderstood. And there is a proof of custody. It was introduced a while ago by I don't know who, but Dankrat defined it as a technique to prevent what he called lazy validators, validators that wouldn't download the full block because it costs time and money, but they would just attest. Right? So that would be like a crypto economic or a cryptographic technique to make sure that we don't deal with lazy validators. But this is certainly not any type of guarantee that the data is being stored or held by the validator or whatever, right? So it was just, you know, a technique that the data was actually downloaded by the validated before a testing. And then Eigen da kind of used this term, I think, for something else, if I'm not mistaken, at least a year ago. Now it seems like this concept maybe is forgotten.
00:42:58.516 - 00:43:17.190, Speaker A: I'm not really sure. I just wanted to sort of clarify what is the current thinking in Eigen Da team about this concept of custody, which was heavily marketed a year ago. And now I'm not even sure if that's a the thing. So have you guys like, changed your mind about this? Or like, what's the story here?
00:43:18.130 - 00:43:25.050, Speaker D: Are you speaking to the fact of operators being able to attest to the fact that they have data or what is.
00:43:25.210 - 00:43:50.340, Speaker A: Yeah, I mean that was the concept that was pushing and slashing condition was also defined on that in one of the testnets. But I think it's not on the mainnet. So I'm kind of a little bit confused and I just wanted to get from the source like what's the story about the proof of custody here? And also the question to other guys, you know, if this is, this is something actually like important in, in your opinion?
00:43:51.280 - 00:45:05.368, Speaker D: Sure. Yeah, I'd say, I mean it might like still like be a piece of like the, along from like dispersal to like data availability sampling. There's like still a requirement to be able to have attestations from those operators to ensure that it has the data. But I wouldn't say that the architectural design as far as implementation on slashing conditions for data availability, you have to be very cognizant of the fact that there are different things at play that a proper actor can still, if you just solely say, hey, do you have the data? And you don't receive a response back, that does not necessarily mean they're acting maliciously. In fact, that's like possible that they could be getting like dost essentially that they have too many requests to respond to. Essentially they don't have the ability. So more so speaking to the fact that there are other things that you, you have to be very careful about, like these slashing conditions and custody is like an aspect of it and like how you are able to ensure that you can retrieve data that has been dispersed.
00:45:05.504 - 00:45:08.500, Speaker A: Mustafa, a quick take and we have to wrap up.
00:45:09.360 - 00:45:25.740, Speaker C: Yeah, I mean, I think it's pretty clear, like proof of custody does not provide you with data availability guarantees. It's more like a nice to have or like if you can't do the availability then custody gets you kind of like maybe something, but it's better than nothing.
00:45:26.660 - 00:46:02.476, Speaker B: I mean, again, we are talking about different kinds of guarantees within the protocol. If the guarantee that we want is on the dispersal then that's one. It's about retrievability later and you want to prove that within the protocol, then there are proofs of retrievability that are, that are there. You can have to play challenge response mechanism there. If the end user wants guarantees, it has to do sampling because that's the efficient way to do it. And after 18 days, after 20 days, if you still would want to have guarantees, you have to play a fresh round of challenge response mechanism and not rely on original attestations talk to last.
00:46:02.508 - 00:46:05.060, Speaker A: Chance for a controversial take, and we're wrapping it up.
00:46:05.220 - 00:46:06.600, Speaker E: Just use Ethereum.
00:46:07.060 - 00:46:20.500, Speaker A: All right, thank you, guys. Thank you, the panelists. I hope that it was as entertaining as it was to me. And thank you so much, guys. I hope you see you next year.
