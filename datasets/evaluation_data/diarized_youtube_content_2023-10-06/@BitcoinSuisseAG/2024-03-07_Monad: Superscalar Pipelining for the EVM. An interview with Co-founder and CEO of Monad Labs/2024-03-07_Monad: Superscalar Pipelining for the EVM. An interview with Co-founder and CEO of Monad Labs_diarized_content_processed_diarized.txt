00:00:10.840 - 00:00:58.866, Speaker A: Today I have the absolute pleasure, and I seriously mean it, to be joined by Keone Hon, who is a co founder and CEO of Monad Labs. Keone basically isn't your average blockchain character. I did my homework. So his reputation builds on his unique background in high frequency trading and also his passion for operating infrastructure at the limits of what is physically possible. He set out to revolutionize the industry with his brainchild. That is Monad, a high performance parallelized EVM layer one blockchain. So Monad, we were following Monad since a couple of months already, Monad has been cooking.
00:00:58.866 - 00:01:14.254, Speaker A: And today we tap into Keone's expertise to understand how Monad paves the way for a faster and more efficient future for decentralized applications. Keone, I'm stoked to have you and thank you for joining us.
00:01:14.634 - 00:01:19.374, Speaker B: Yeah, thank you so much for that kind introduction, Dominic, pleasure to be here today.
00:01:19.684 - 00:01:46.228, Speaker A: You're welcome. So to kick things off, I want to let you know that I really like the memes around Monad's community. I really dig the purple papers. And as an avid NFT collector, I also hope that there will be an NFT collection somewhere in the future. But let's see. So we have a lot of ground to cover today. I would suggest we start with a brief introduction of yourself.
00:01:46.228 - 00:01:56.804, Speaker A: Maybe you share with us how you ended up in the blockchain or crypto industry and what keeps you motivated and passionate about it.
00:01:56.884 - 00:03:26.914, Speaker B: Yeah, my name is Keone. I have been working on Monad for about two years and before that was in the high frequency trading space working as a quant for about ten years. So I started in HFT right after college, studied computer science, and then ended up joining a high frequency trading firm called Getgo. Was there for two years and then joined jump trading in 2013. High frequency trading is just about building systems from scratch and there's a huge emphasis on performance, which I think is a great background for then trying to push the performance of crypto forward. Spent a number of years basically building up a trading team that ended up being a significant participant in a number of futures markets such as the Chicago Mercantile Exchange or CME, which is the biggest futures market in the world, and did that for a number of years. Built systems from scratch, got tick to trade times down to sub microsecond level, meaning the that over a million cycles per second of tick to trade possible packet comes in, decide whether to how to update fair values for a particular instrument that we were trading and then send an order out in less than a microsecond, just a good experience.
00:03:26.914 - 00:04:57.604, Speaker B: Definitely really fun and fun to work in. A small team that was working on low level systems engineering, then ended up joining the crypto team at JMP in mid 2021 and mostly focused on Solana Defi for about six months. I think Solana made a lot of sense for our team at JMP because it was really performant transactions were really cheap, and there was new, different focus on user experience and making sure that people who are not crypto native coming into the space for the first time could utilize high performance applications and pay very little in transaction fees in order to do that. I think seeing the rise of Solana during the Solana summer, when the DJNape academy NFT happened, and then there was kind of a cambrian explosion of NFT projects launching on Solana, people could see that for the first time. You could mint nfts with really cheap fees and trade them with really cheap fees. So there was a lot of potential for mass user adoption out of that. So basically just kind of observing that and observing from the sidelines, it became really clear to me, as well as to one of my co founders at Monad, James, who I started working with since 2014.
00:04:57.604 - 00:05:31.410, Speaker B: We joined the crypto team at Jump. Together we're working on Solana Defi. We could just see that there's a huge need for a more performant EVM, and it seemed like a lot of exciting applied cryptography research going on in the Ethereum space. There's a ton of interesting work going on at roll ups and data availability in certain other verticals, but it seemed like no one was really focusing on actually improving the execution system or very few people focused on that. So we decided to leave jump at the very beginning of 2022 and start Monad.
00:05:31.482 - 00:06:38.452, Speaker A: Okay, super exciting background, especially the high frequency part. Just a couple of weeks ago, a colleague of mine suggested that I should read about the history of high frequency trading back then when they started to move closer to the actual exchange, like physically, because they had such a big advantage by doing that. So, yeah, great background, and I would suggest we start very high level, scratching the surface somewhat. You kind of outlined it a little bit, like, what is the story behind Monad? Also, what is the story behind the name? I couldn't find how you came up, or does it have a certain meaning? And then maybe you could dig a little deeper on what exactly inspired you to create Monad. You said scaling issues and high transaction fees that kind of limit utility, basically. And what specific problems are you trying to tackle with monad? Currently, yeah.
00:06:38.508 - 00:07:04.756, Speaker B: Maybe I'll touch on the name first, just because that's the kind of the simplest thing. We just thought it was a good name. It's an uncommon common noun in functional programming. A monad is. It's. Well, I mean, without getting into too many details, it's a component in functional programming. So maybe one in every 20 developers that we talk to is extremely excited about the project just because of the name.
00:07:04.756 - 00:07:38.592, Speaker B: It's a cool concept. But then beyond that, a monad is also a fundamental unit. It's, I think in Greek, a single celled organism. And just like a fundamental component, there's also some religious aspects where it refers to the one God. So in general, you could see the mono part in it. It's the idea of the one ring to rule them all, or one effort that could really change things.
00:07:38.728 - 00:08:37.674, Speaker A: Cool. There's a lot of untangling the name. I like it. So you really put some effort into the branding and the identity. Very nice. So interestingly, for this high performance blockchain, you went out and picked the EVM, right? Maintaining kind of bytecode, bytecode level equivalents, and also Ethereum RPC tooling compatibility. So now I would be keen to know what were the reasons for you to stick with the EVM and why did you not build your own EVM, like other entities like Solana are doing, for example, and more like, what are the trade offs now of using the EBM? Like, what kind of boundaries you have to operate in now because you chose the EVM?
00:08:39.883 - 00:09:21.190, Speaker B: Yeah, I think. Well, the reason we chose to build for the EVM is pretty simple. It's that most developers are building for the EVM. So most applications, most developers, there's a ton of tooling that's been built out to support either the higher level languages that compile down to EVM, or for the EVM itself. I think at the end of the day, the EVM, and again, the languages that ultimately compile down to it, like solidity or Viper, really have a strong network effect. Over 97% of all TVL and crypto is in EVM applications. And there are other things as well.
00:09:21.190 - 00:09:58.846, Speaker B: People don't really think about the research aspect, but almost all of the applied cryptography research is being done in the context of the EVM as well. So it's really kind of the dominant standard, and there's a huge amount of demand for a more performant vm. That way developers don't have to change anything, applications don't have to be rewritten. If we can just make improvements under the hood to the execution system and basically upgrade the engine of the car without changing how the steering wheel works so that people don't have to learn how to drive a different kind of thing. You can actually unlock a lot of productivity for the space.
00:09:58.990 - 00:10:40.204, Speaker A: Great answer. Maybe we dive a little bit into the substance on what makes Monad such an outstanding solution. And I know there are four major areas that we can touch base on. Let's go step by step. I would start with parallelization. It's also a mad tongue twister for me. It's hard to say, but the word itself, like the term, gets thrown around like in a hyper inflationary fashion already, right? Like projects are quick to jump the bandwagon currently.
00:10:40.204 - 00:11:32.608, Speaker A: And for me, it's important to really highlight the projects that actually come up with disruptive and substantial improvements. And I consider Monad to be among these projects because you are kind of like almost disruptive on plenty of verticals. Right. So let's use some time here to debunk some misconceptions and outline what makes Monad so special in an environment where basically everybody tries to join the parallel kind of bandwagon. Yeah, so let's start with the parallelization. As I understood it brings improvement, but it's not the entire magic that is happening around Monad. But let's start with parallelization.
00:11:32.608 - 00:11:43.204, Speaker A: Maybe you could explain what that actually means to our audience and why is it actually enabling us to achieve a higher performing blockchain.
00:11:44.944 - 00:13:10.724, Speaker B: Yeah, parallelization, and I guess maybe more specifically in monad, optimistic parallelization of execution is basically the practice of running many transactions at the same time on separate threads, and generating a bunch of pending results which each track the inputs and outputs for that transaction. And when I say inputs, I mean basically the dependencies on state. So like for example, if a very simple transaction would be a USDC transfer. So for example, if the first transaction in the block was like me starting from 100 USDC in my account, sending ten USDC to you, Dominic, then the inputs would be the balance of my USDC account, which is a particular address slot tuple, which currently has a value 100, then one of the outputs of that transaction would be updating that same slot from 100 to 90. Then of course there's also a second slot involved here, which is your USDC balance, which maybe is going from zero to ten. But let's not focus on that for a second. In any event, parallelization means running a number of transactions in parallel and generating a bunch of pending results which keep track of the inputs and outputs.
00:13:10.724 - 00:14:02.610, Speaker B: Given that initial run. Then we step through those transactions, those pending results, one by one in the original order of the transactions, and commit them if they're valid. Or reschedule the work if they've since been invalidated due to one of the changes to one of the inputs. So maybe to make this a little bit more concrete, in example terms, lets imagine that theres a monad block which has a bunch of transactions in it. The transactions are still in the linear order like similar to an ethereum, and the job is to get to the end state after running each of those transactions one by one. But with parallelization, we run a bunch of those transactions in parallel. So for example, imagine that the first transaction is, like I said, me starting from 100 USDC and sending ten USDC to you.
00:14:02.610 - 00:14:43.618, Speaker B: That's transaction one. Transaction two is some other unrelated thing, like someone minting an NFT. And then maybe imagine that transaction three is me sending five USDC to another person. Let's call them Alice. So in optimistic parallel execution, what happens is the first transaction gets executed, so the input is 100 and the output is 90 for that particular USDC slot. The second transaction is just like I said, minting an NFT. And then the third transaction, when it's run optimistically, still has an input of 100 USDC and an output of 95 USDC.
00:14:43.618 - 00:15:16.214, Speaker B: And then, so these are just pending results that are generated temporarily. And then we step through those pending results. So we get to transaction one. Pending result one, we commit that, that's fine. We get to pending result two, we commit that, that's fine. But then when we get to pending result three, at that point we realize that that pending result wrote down an input of 100 and an output of 95. But now that 100 has actually since changed to 90, so this input has become invalidated.
00:15:16.214 - 00:15:17.846, Speaker B: We need to go reschedule that work.
00:15:17.990 - 00:15:23.686, Speaker A: Okay, is that also some kind of trade off compared to a deterministic approach then?
00:15:23.790 - 00:16:12.920, Speaker B: Right, so the benefit of this actually, maybe one other thing to mention about that first. So when running, when we're trying to commit pending result three, and we notice that the input for that pending result was 100. But now, since then, the value for that slot should actually be 90. So we have to go re execute that work. That re execution is usually very very cheap because that slot is already in memory, it's cached, so we don't have to go all the way back to the SSD to go retrieve that value. And in general, the re execution of the computational steps, like in this case, doing a subtraction is not very expensive. So, yeah, I mean, in terms of the, you were asking about the trade offs.
00:16:12.920 - 00:17:16.862, Speaker B: The alternative approach that I think you're referring to is one where all of the dependencies for every transaction are pre specified as part of the transaction. For example, at Solana, every transaction has to prespecify all of the accounts that it depends upon. There's also a proposal in Ethereum called the introduction of access lists, which is in place in Ethereum but is not mandatory. Um, so, so like in either, about that thing that you're referring to, like pre specification of dependencies, what are the trade offs between those two? I think the, there are a couple of reasons why we think that the optimistic parallel execution approach that Monad takes is the better one. Number one is because the transactions are smaller, because you don't have to have like all these extra, this extra information that has those access lists. And we think ultimately that network bandwidth is going to be one of the biggest constraints. So it's better to have smaller transactions that can travel over the network more efficiently.
00:17:16.862 - 00:17:51.298, Speaker B: So that'd be number one. Number two is that when you pre specify dependencies, sometimes those dependencies are wrong because maybe the state has changed from the time that you sent that transaction until now. If the dependencies are wrong and you require that the transaction specifies exactly what it needs, and since then the dependencies change, that transaction is just going to fail. So it can result in unexpected failures and a bad user experience for users. There are a couple of other reasons, but I'll pause there in case, you know.
00:17:51.386 - 00:18:27.248, Speaker A: Yeah, I seriously think it just clicked for me because if I think about just a processor, kind of, so as I understand, you utilize now kind of a multicore approach, right? Like in, when I think about a processor, you run a bunch of evms in parallel instead of sequential and then kind of stag before you commit to a block. And because you do that in an optimistic fashion, you don't actually have to identify independent transactions upfront, is that correct?
00:18:27.446 - 00:18:28.348, Speaker B: That's correct.
00:18:28.476 - 00:18:57.998, Speaker A: That's really cool. Okay, what percentages of transactions are independent currently? Do we have a number there? And do you think that will change in the future as the whole ecosystem becomes kind of more interconnected? We see that already with, for example, restaking. Right now you have LST and restaking, and now you put it in pendle, and then there's like a huge daisy chain already of state that is kind of interconnected.
00:18:58.086 - 00:19:53.294, Speaker B: Yeah, that's a great question. I think maybe another, the percentage of interconnectedness depends. And I think it's also a little bit hard to define, because if transaction one and two are related, but then two and three are not related, but then two and four are related, what does that mean? How would you actually measure the interconnectedness? I think ultimately it's really just performance. Like the way to comment on, like, to what extent is this a problem is just to comment on performance. But I think, you know, in the context talking about performance for a second. Another way to explain the parallel execution is to think of it as like kind of two phases. So the first phase is actually, before I even say that, like, let me just emphasize that the single biggest bottleneck in execution is not the computational steps, it's retrieving state from disk.
00:19:53.294 - 00:20:37.474, Speaker B: So cpu time is pretty cheap. CPU's can do a lot of computation. It's really easy to add numbers together, multiply or generate hashes, et cetera. All of that is happening in nanoseconds per instruction, or even less than that sometimes. But state retrieval is what's expensive, because if you have to go to the SSD to retrieve a value that's 40 to 100 microseconds, aka 40,000 to 100,000 nanoseconds. So just way, way more than the cost of compute. So at the end of the day, the single biggest bottleneck for executing any transaction is going to disk and retrieving it.
00:20:37.474 - 00:21:17.386, Speaker B: And this will play into maybe the next topic that we talk about later, which is MonadDB. But before I go there, I'll just say, on the topic of parallel execution, you can think of it as two separate phases. The first one is running a bunch of transactions in parallel. Running each of those transactions basically surfaces all of the dependencies for that transaction. So it means that then when that thread is running that vm, it's like realizing, oh, we need this piece of state from SSD, so let's go retrieve it. And when that data gets retrieved, then it gets put in the cache. Cache is way faster than SSD, just to be clear.
00:21:17.386 - 00:21:55.494, Speaker B: So the first stage is like running a bunch of transactions in parallel, generating pending results for each of them, and also resurfacing all the dependencies and pulling those into cache. And then the second stage, which is stepping through the pending results one by one, is either for each pending result, like immediately able to commit it. If there's not been any changes, or if there have been some changes to some inputs, then it's re executing. But when re execution happens, those dependencies are generally in cache, so they're much, much faster to re execute than they would have been the first time because they don't have to go back to disk.
00:21:55.574 - 00:22:41.960, Speaker A: Very cool. It reminds me a little bit of entropy that we know from physics, kind of these interdependencies. I imagine they only get more. But yeah, let's touch base on where Monad really starts to shine. And yeah, it's what you mentioned already, it's the Monad database. So if we explore the verticals where we can optimize, aside from parallelization, one of the pillars actually is state access via optimized databases. I know other players are also trying to come up with innovative database solutions like psy or avalanche to gain these, to gain increased performance.
00:22:41.960 - 00:22:54.464, Speaker A: And as I understood, the Monad database is actually the enabling puzzle piece for optimistic execution. Is that correct? What makes Monad's database so exciting?
00:22:54.584 - 00:24:22.184, Speaker B: Monaddb is a completely from scratch database written in c that's specifically designed for the problem of storing Ethereum Merkle tree data on SSD. So Ethereum state is all stored inside of a Merkle tree, which means that at the end of every block there's a Merkle root, which is like a hash at the very top of the tree that is computed based on all of the values in that tree through kind of iterative, like compute a hash and all the children nodes in the tree to generate the hash of the parent. And then, you know, combine that with other siblings to then generate the grandparents hash all the way up to the top of the tree. The merkle tree and the fact that Ethereum has a commitment to all the state in the form of the Merkel root is one of the sort of fundamental pillars of Ethereum because it allows other blockchains or other systems to interface with Ethereum while being assured that all of the state is that different nodes that basically have the same merkle root have all of the same state. So it's basically like a commitment to all of the state in a very abbreviated, very efficient format. Merkel roots can't be unpacked. That is to say, if I just give you a merkle root, you can't unpack that into the entire tree.
00:24:22.184 - 00:25:18.532, Speaker B: Of course, just logic would suggest that a merkle root is very short. I think it's 32 bytes, whereas the merkle tree, all of that state is massive. But, but it, but it, you know, you can verify that a value in the merkle tree is what it is if you know the root due to something called merkel proof, which we won't get into, but anyway, the point is that there's all this state that lives in Ethereum which represents everything. Like all of the states of every smart contract, everyone's account balances everything and transactions, they require small parts of that overall global state in order to proceed. So like I was saying before the processing, a USDC transfer requires knowing what my USDC balance was before that transfer and then requires mutating that balance into a new value. And that's a very simple example. There are a lot of other ones.
00:25:18.532 - 00:26:42.304, Speaker B: And at the end of the day, because this state is pretty big, like in Ethereum, it's over 100gb. So it kind of, you know, has to live on an SSD. It's not feasible for all of it to live in memory. And if you know all of this data, all the state is on SSD, but the it, you know, it's quite, if it's expensive to go retrieve that data from SSD, then it's just going to be hard to execute very quickly because there's a significant cost to going and retrieving data in order to be able to proceed with the execution. MonadDB is basically a way of making data that lives on SSD, this overall global state, to be much more efficiently accessed. I think the really dumb description of MonaDB is that it's enabling parallel access to the state. So when many virtual machines are running in parallel in Monad's optimistic parallel execution, those vms can all make calls to the state and retrieve values from there in parallel, as opposed to with existing blockchain systems which use commodity databases where there's a bottleneck in terms of the access to the database due to lack of full support for asynchronous I o in these databases.
00:26:42.424 - 00:27:29.608, Speaker A: Yeah, that makes perfect sense. Very exciting. So what you're saying is now we have these two parallel execution, but also parallel access enabled by the Monad DB. And kind of the missing puzzle piece that we didn't touch base on is now the Monad consensus. Right. So it's kind of another piece that you are optimizing or that you crafted in a very elegant fashion in order to allow this, the monad blockchain in its entirety, to be highly performant. Maybe you could explain what is new with the monad BFT compared to other cool consensus mechanisms like Snowman or tendermint or the like.
00:27:29.608 - 00:27:41.280, Speaker A: And I also know, I read somewhere that there's still some kind of research needed when it comes to prevention of spam attacks. Is that correct or.
00:27:41.392 - 00:28:34.914, Speaker B: Yeah. Well, let me start by describing monad BFT really quickly. So, Monad, BFT is a derivative of the hot stuff consensus mechanism. Hot stuff is a performant consensus mechanism that generally operates with linear communication between nodes, meaning that if there are n nodes in the network, then there's a multiplier of n messages that need to be sent. So in particular in hot stuff, it's a leader based algorithm where the leader comes up with a block proposal, sends it to all the other nodes. All the other nodes evaluate that proposal, and then vote and send their vote directly to the next leader. So it's kind of a fan out, fan in approach, where the leader fans out to everyone else and everyone fans into the next leader in the happy path.
00:28:34.914 - 00:29:23.358, Speaker B: As long as things are running smoothly, there's no need for all the nodes to message all the other nodes. In other algorithms, where it is necessary for all nodes to message all other nodes, it basically means that the communication complexity becomes o of n squared, aka quadratic complexity, which then severely limits the size of the network in terms of the number of nodes it could participate. So like in tendermint, tendermint is a quadratic communication complexity algorithm. And you see that most tendermint chains are about 100 nodes participating in consensus or less. Because of that communication overhead with Monad, we felt that it was really important not only to have really performant consensus, but also to be able to scale the network to hundreds of nodes.
00:29:23.446 - 00:29:35.150, Speaker A: Okay, very cool. I think one piece is missing that I wrote about. What is super scalar pipelining. I think that makes the picture complete, right?
00:29:35.262 - 00:30:58.558, Speaker B: Yeah. So that actually kind of is a good segue to also the other question you were asking about, which is spam attacks and where that comes in, what the concern might be and how Monad has addressed that. So pipelining is the practice of creating multiple stages of work and then intelligently pushing work through those multiple stages so that the work can proceed in parallel. So a really simple example of this is like say that you have four loads of laundry to do, then the naive thing to do would be to, you know, do everything for the first load before starting on the second one. Like, you know, putting the first load in the washer, then moving it to the dryer, then folding it, then moving it back to your closet before starting on the second one. But, you know, everyone intuitively understands that it's much smarter to, as soon as you move the first load to the dryer, then put the second load in the washer, um, and kind of progress all of them in parallel, that's really all the pipelining is it's just creating multiple phases of work so that we can utilize all the resources of the computer as efficiently as possible. And in the context of blockchain, you know, one obvious thing that comes up is blockchain has at least two major components, which are consensus and execution.
00:30:58.558 - 00:31:48.310, Speaker B: And in most blockchain systems, those two are interleaved, meaning that the nodes have to execute before they come to consensus. And this has a kind of bad property that actually very much limits the budget, the time budget for execution, because in practice, consensys ends up taking up most of the block time. So, for example, in ethereum, the block time is 12 seconds, but the rough time budget for execution is only 100 milliseconds. And the reason for this is because consensus and process of all the nodes in Ethereum coming to agreement about that block, that's very expensive. So it means that there's actually only a very small portion of time that can be set aside for execution. But 100 milliseconds in 12 seconds is tiny. It's 1%.
00:31:48.310 - 00:32:44.516, Speaker B: So literally, there's a situation going on where there's a time dilation factor of 100 x, where even though the block time is 12 seconds, we only have a very small portion of that time to go execute. So if we could somehow address this, then we could actually massively raise the budget for execution and utilize the full block time. And the way that we've done that in Monad is by introducing a concept we call deferred executions. The idea is basically that first the nodes come to consensus on a block, and then once that happens, then two things happen in parallel. One is that we start doing consensus on the next block. And the other thing that happens is we start executing the list of transactions that we've just consensused on. So, in effect, the pipelining here is that there's multiple swim lanes, there's consensus, and there's execution.
00:32:44.516 - 00:32:54.434, Speaker B: And as soon as we complete consensus on one payload, like one block, then we can start executing it, but in parallel, we can do consensus on the next one.
00:32:55.094 - 00:34:14.490, Speaker A: Pretty much reminds me of assembly lines that were back then invented by Henry Ford, just on a super cool computer science level. Very nice. So, as I understand, if we stack all of these innovations being monadb, parallelized execution, the super scalar pipelining, and then your new highly innovative consensus mechanism, it results in this very performant integrated blockchain. The space likes to compare metrics, maybe. Can you throw some numbers at us? Like, what TBS can we expect? Where will the fees range? I know it depends on block space demand, then maybe also how many validating nodes will be possible? Like is there a limit with monad or not? And then I think you tackled it before, but I'd also be keen to know since I understood with Monaddb you still have to store all of the data. Like how do you tackle state plot at some point in the future? Will it become a problem? Or is this disk space just no problem? It grows faster than state load maybe?
00:34:14.602 - 00:35:07.426, Speaker B: Yeah, all the important questions Monad at Mainnet will offer over 10,000 transactions per second of throughput. Or actually the number that I like to think of is more like transactions per day. So Monad will offer over a billion transactions per day, and those are full transactions, not just token transfers. I think one way to like kind of inflate the numbers of a blockchain is to just test only using token transfers, which are way simpler. But with Monad, the backtest is the historical history of Ethereum. So over 10,000 tps of throughput, 1 second block times, and single slot finality. I think what this results in is much, much, much more block space than with some existing networks like you mentioned, which ultimately results in much lower costs for end users.
00:35:07.426 - 00:35:46.462, Speaker B: So we currently think that a transaction with that costs 100,000 or that has the complexity of 100,000 gas, which is about the complexity of a uniswap v two swap. So that kind of transaction generally costs five to $50 on Ethereum main net right now, and on the order of on existing Ethereum l two s that are using ethereum for data availability on Monad, we expect that transaction to be a fraction of a cent. So orders of magnitude cheaper coming from orders of magnitude of higher throughput.
00:35:46.558 - 00:37:00.420, Speaker A: Very nice. Recently I studied a little bit. You released hardware specs for nodes, and frankly I was pretty impressed on how reasonable they appeared, like they were actually quite doable for the average, let's say, retail node operator. I feel it's quite a significant USP for Monad already if you manage to optimize at that magnitude on multiple venues while keeping hardware requirements comparably low. I also imagine it being a very thin line in balancing these hard engineering problems with the with the resulting hardware specs that we see right now. However, there are always critics or controversies, and it's fair that some argue that prioritizing performance only might at some point or to some degree compromise decentralization. I know the trilemma is not that big thing as it was anymore, but I how do you what's your take on that like is it an issue or not at all?
00:37:00.612 - 00:37:51.110, Speaker B: Yeah, well I think maybe a related question which you asked before, which I didn't, didn't get to, just because I paused, was about state growth. So yeah, just to maybe touch on that for a second, because it's very related to what you were asking as well. Right now, like I said, ethereum state in aggregate, after ten years of Ethereum running, is over 100 gigs of, of data in the active state. Of course the transaction history is larger, but what we really care about is the active state of Ethereum. And like I said, that state lives on SSD. And the critical problem to solve with respect to execution is making that state more available, if you will. It's almost like the state availability is the problem that we're solving here in Monad.
00:37:51.110 - 00:39:06.874, Speaker B: Monad DB makes state much more available, much more efficient to be accessed for the purposes of execution. And that's because this custom DB that we've built from scratch is specifically optimized for storing merkle tree data and supports parallel access, so that then when multiple vms are running in parallel, they can all read different parts of the state tree in parallel. I think the thing to know about SSD's is that they're actually really cool pieces of technology that are really performant. It's slower than reading data from memory, like I said, 40 to 100 microseconds, but they're pretty wide in terms of bandwidth. It's a high latency but high throughput storage offering, but you need to have software that appropriately allows all of that width of data transfer to happen efficiently with existing databases. Like Ethereum uses leveldB, Solana uses rocksdb. These requests basically get marshaled through a single pipe, which means that when trying to make reads to many different pieces of the state tree, those reads end up blocking each other and the access is quite slow.
00:39:06.874 - 00:39:50.850, Speaker B: But with MonadDB, it just makes it so that a lot more data can live on, on the SSD and can live in this state tree that's growing and growing without concerns about the cost of those accesses degrading as the state tree gets bigger. To bring it back to your original question, I know a lot of people in Ethereum are concerned about state growth. I think state growth, two things. One is in some sense is fundamentally a reflection of user adoption. Like if you have aave with 100,000 depositors in it, the state is going to be a certain size. If there's 10 million depositors, it's literally just going to be bigger. That is actually a reflection of adoption.
00:39:50.850 - 00:40:27.242, Speaker B: But it's understandable that people are worried about state growth because right now existing clients are using an existing database system with an existing database schema built on top of that. That's really inefficient. And that actually gets more inefficient as the state tree gets bigger. So that motivates people to be very worried about state growth and want to limit the throughput of Ethereum in order to limit the state growth. But if you change that calculus fundamentally with a new database, like a zero to one innovation of a new database, then a lot of that calculus changes.
00:40:27.338 - 00:40:32.904, Speaker A: Okay. That's an exciting outlook, to be honest. Right? Very cool.
00:40:33.524 - 00:41:02.000, Speaker B: Yeah, let's sorry for the long answer on that one also. But I think it's just important to keep in mind that like a lot of our assumptions are somewhat predicated on the existing set of tools that we have. So if you have a new tool, the new tool can actually make us much less worried about state growth, for example, or at least like enable state to grow to the size of the SSD, which is like two terabytes on Monads hardware requirements, which is much, much bigger than the current state right now.
00:41:02.192 - 00:41:13.160, Speaker A: Yeah. And two terabytes is not crazy, to be honest. It's not a huge hardware requirement in my opinion.
00:41:13.352 - 00:41:17.280, Speaker B: Yeah, it costs about $200 to buy a two terabyte SSD.
00:41:17.472 - 00:41:46.114, Speaker A: Yeah. As I said, it's very reasonable. Even if that's possible. I know Testnet is coming somewhere in the not too distant future. I will also look into run if that is possible. Let's look a little bit into kind of other approaches. There are strong voices out there and I think we should discuss it and get your take on it.
00:41:46.114 - 00:43:07.120, Speaker A: Because not only monolithic or integrated approaches benefit from optimizations at the VM level, or at the client level, or from faster hardware, but also validity proven execution layers. Correct? I'm going to cite Polinia here, who recently, it's a couple months back, he quote unquote, every single monolithic blockchain seeking scale will upgrade to tech like validity proofs and data availability, sampling or risk obsoletion. So I'd be keen to know what is your perspective on the future of validity proven execution layers? Because they come with a lot of benefits too, as they allow for very low latency bandwidth requirements, the possibility to verify the integrity of all the transactions from mobile devices, which is like super nice, right? And they also come with potential cross chain composability via proof aggregation layers. So will monad at some point look into this kind of technology as well, or does it not make sense? Yeah. What's your take on that? Right, so are you not a fan of validity proofs? I'm just keen to know what you think about it.
00:43:07.272 - 00:44:10.602, Speaker B: Yeah. Yeah. So I think that the way that I think about it is there's ethereum l one, which currently has a bunch of l one specific state. For example, Aave is deployed on ethereum l one, and there's a bunch of accounts, and then all of the nodes in Ethereum know about all of the state that's directly on l one. But roll ups kind of give us a different style of scaling, which is fractal scaling, whereby there could be like a separate roll up, basically a separate blockchain, who has a whole bunch of unpacked state on it. Like there's aave deployed on an l two and all the balances there, et cetera. But then there's just a mechanism that kind of summarizes all of that state, which is very verbose of one of those like l two blockchains, and condenses it into just a single merkel root on the bridge contract on the l one, which is like a commitment to all of that state.
00:44:10.602 - 00:45:22.378, Speaker B: So you could, I imagine it being like an entire world that is being carried around in Inspector Gadget's briefcase. And so it's very compact, like it doesn't take up a lot of space in our world, but the other world is in some sense is like in his briefcase, just walking around on Main street. But the way that the packing works is that we can basically verify that particular state on the l two is true. We can verify that on the l one. But we can't actually unpack the briefcase itself within while Inspector Gadget is on our world, because obviously unpacking it would then unpack to something else that's equally the size of our world, and that wouldn't work. The fractal scaling is just all about state commitments. But at the end of the day, if you want to actually know the values of any particular state, if you want to know your deposit on Aave, you actually have to go interact with an l two node, which is maintaining all that state processing, all of that stuff.
00:45:22.378 - 00:46:05.304, Speaker B: So yeah, I think it's a good mechanism. Basically, the whole roll up mechanism is the facility that creates this briefcase for Inspector Gadget, that shrinks stuff down and creates those commitments. But at the end of the day, the problem of actually having a world that operates efficiently, that has a full ability to process all of the work that's being done on that world, that's a separate consideration. The consideration of execution. That's actually really what Monad focuses on, is making execution really perform it so that any world, whether it's an l one or an l two, could do much more work and maintain much more state sustainably.
00:46:05.424 - 00:47:14.908, Speaker A: Okay, fair take. I mean, do you think, like, in your opinion, is it possible, say, I mean, the end game for our industry is basically to onboard almost everything, right? Via Internet, like on, like onboard be the rails for the financial system, be the base layer for value transfer, like all of the crazy nice stuff that blockchains enable. So the end game is at some point, we want to onboard the entire planet. Can we do that on one single monolithic chain? And if not, I know Monad already announced a lot of very exciting partnerships with Axela or layer zero that you implement, I think, basically from scratch. So if not, like, do you think we at some point also solve the trade offs or the risks that we have with cross chain bridging? As of now, what do you think it's possible to onboard everything that everybody has one shared state? Like, what is Monad trying? Is that possible to onboard, like, 8 billion people?
00:47:15.036 - 00:48:02.282, Speaker B: Right. Well, like I said, Monad currently has the ability to support a billion transactions per day of the typical complexity of current Ethereum history, transactions in the future. I think that future improvements that we're working on right now may push that to several billion transactions per day. But as you're hinting at, that's probably still not enough to onboard all 8 billion people. Support all 8 billion people. If people are using blockchain apps for everything, like, for personal finance or achievements, or, like, ordering Ubers or, like, I don't know, like, paying for Internet, what, whatever. Like, all these different things, that's, you know, a couple billion transactions per day is not going to be enough for all 8 billion people.
00:48:02.282 - 00:48:47.438, Speaker B: Of course, I think at that point, there will be multiple monads as well. So Monad is really just, you know, a very computationally dense network. Um, it's a single fundamental unit of blockchain, but there could be many monads that are more like application specific or sector specific or regional specific. The exact division doesn't matter and will just evolve over time. But I think it is important for each one of those units to be much more computationally dense than existing units are. Right now, Ethereum is processing a million transactions per day, so there's a huge improvement going from a million to a billion. You're going to get much closer to be able to serving all 8 billion people with a reasonable number of networks.
00:48:47.438 - 00:48:56.954, Speaker B: If each network can support a billion or a couple billion transactions per day, as opposed to if every network can only support a million or a couple of million transactions per day.
00:48:57.254 - 00:49:47.942, Speaker A: Yeah, yeah. We will say, like, it's still a little bit unclear, right, how the space evolves. Like, of course it's very optimist obvious to me, like how big the benefits are of having one computationally dense monolithic blockchain. Right. You have so many, you have so much less issues. Of course you have to operate at the physical limits, your hardware, and you might need a more performant node at home, but then you solve all of these composability problems and the fragmentation problems. So it's definitely, if you can make it happen, I think it's the most elegant solution we can, we can have because on the l two side, then we face all of these fragmentation problems even at the community level.
00:49:47.942 - 00:50:19.334, Speaker A: Right. Then you have all of these little fragmented communities that almost behave like religions or something like that. But yeah, very excited to see where Monad is taking, how Monad will change the game. And as we inch closer to the end of our interview, maybe you could outline like, what's the current stage of development for Monad and when can we expect to see the first apps running on the platform?
00:50:21.754 - 00:50:40.814, Speaker B: Monad is in active development right now, but I think you can expect the first public testnet in Q two of this year. And then we expect that we'll be running testnet for about six months. So you can expect Monad by the end of Monad main net by the end of this year.
00:50:40.894 - 00:51:24.874, Speaker A: Okay, exciting. So I have to get my hardware already, I guess. Yeah. So to our audience, if you're interested in learning more about Monad and Kyo and what Keone and the team behind is working on, be sure to visit their website, follow them on their social streams. I will post links in the video description. Christian Chion, honestly, it was an absolute pleasure to host you today. I think we, despite diving a little bit deep in more complex territory, I think we navigated the discussion to be both engaging for the worst crypto natives, but also being accessible for the less advanced viewers.
00:51:24.874 - 00:51:42.078, Speaker A: I am convinced that the audience gained valuable insights today. And on behalf of bitcoin Swiss, we are very thankful that you shared time with us. Best of success for the upcoming internal testnet. And as I mentioned before, I'm very excited for Monad.
00:51:42.166 - 00:51:46.854, Speaker B: Yeah. Thank you so much, Dominic. It's a pleasure to be here. Yeah. Thanks for. Thanks for having me.
