00:00:00.280 - 00:00:25.014, Speaker A: Back for segment two of blockchain data 101. Had a fantastic week. Just kind of quick review. There are some awesome questions and answering great conversation in the discord for our course chat channels. If you're not yet in that, definitely head over there. If you've had any questions as you're going through the content after the fact, that would be a great place to go see what happened. Maybe someone else had your question, or maybe you haven't had it answered yet.
00:00:25.014 - 00:00:47.280, Speaker A: We had two office hours, one unplanned. Again, sorry for the delay for session two, but thanks for your patience. We are here now and hopefully we'll have no technical difficulties today, but it was great to be able to have that extra little session. We will be having another office hours tomorrow. I know it's kind of quick hit, but this will be after segment two. So that'll be at the regularly scheduled 07:00 p.m. eastern on Wednesday.
00:00:47.280 - 00:01:00.110, Speaker A: And we have an exciting announcement about what you're going to get in addition to a poap as we go throughout this course. So I'll hand that one over to Rochelle to talk a little bit about Xmetrich and how that ties in with metric style and the course.
00:01:01.410 - 00:01:26.610, Speaker B: Yeah. Hey, everyone, I'm excited to tell you about Xmetric. So we launched this non transferable token about three weeks ago. It's like monopoly money. It's here to help us test our protocol. It's like a pre metric token, not worth anything, but you can use it to get access to different parts of our protocol. So real quick, metric style, we've got three components.
00:01:26.610 - 00:01:52.144, Speaker B: We've got the community brainstorming. I don't know if anyone here has participated in that, but you get to suggest questions and tool ideas that we can then turn into analytics and you can also upvote other people's questions. So that's community brainstorming. You don't need any X metric to participate in that, but you'll earn xmetric for it. Then we've got analytics. We put out those questions. The most upvoted questions are turned into actual bounties.
00:01:52.144 - 00:02:43.930, Speaker B: Same thing, no xmetric needed, but you will earn some for completing those analytics. And third segment is peer review, where you do need 2000 xmetric to become a peer reviewer. So it's good to add up those x metrics so you can access that and more to come in terms of if you have a certain amount of xmetric, you get these perks. So we'd love to reward you guys for participating in this course with xmetrichead. If you receive a pOap, which I believe was if you pass the quiz with a seven or more or get the grading exactly. I think seven, you got a PO app and you will also get x ten xmetric on top of it. So we'll share some documentation more on xmetric, more on where to view your balance on polygon and the perks of having xmetric and taking part in our protocol.
00:02:47.960 - 00:02:52.456, Speaker A: Awesome. Thanks to that, I think we are.
00:02:52.488 - 00:02:53.528, Speaker B: I'll leave it to you.
00:02:53.704 - 00:03:37.748, Speaker A: Ready to go? Okay, so if you are curious about finding out more about Xmetric, I believe a document was just posted in the chat there, but there's also information on our website, metricsdao XYZ. You can learn a little bit more about the three step process just by scrolling down here. And at the bottom there is a link to go learn more about xmetric. So fantastic there. Another important place is let's see go here on our docs here. We do have notes from the office hours. If you missed that on Wednesday, you can catch up a little bit about what was discussed in that and what questions everyone else had.
00:03:37.748 - 00:04:41.184, Speaker A: So in that and in the conversations afterward, there was a great discussion in the chat about one of the questions from the quiz about dates, specifically date ranges using between and do you cast timestamp to date? Or what happens with is it inclusive? So out of that, just wanted to dive into that quickly, a query over here. So a lot of the questions asked about, you know, how many transactions occurred in x month on Ethereum or how many NFT sales happened between here and here. So I wanted to kind of dive in and run this query here. So what we're doing is we're just grabbing a couple, couple columns from Ethereum core NFT sales, and the first one block timestamp as is. And this is a timestamp date data type. And in the second one we're casting it to a date just to see what's happening here. So for all these, we have August 1 at, you know, zero five.
00:04:41.184 - 00:05:15.850, Speaker A: But when it's cast to a date, it truncates it down just to just the August 1 portion of that. So while between is inclusive, what's happening with this second date up here is it's basically presuming that it is August 2 at midnight. So in theory there should be no real difference between block timestamp between August 1 at midnight and actually writing in August 2 at midnight. So if we rerun this again. Hey Jack, sorry to interrupt, but could you zoom in on your window a little bit of. Yes. Let's go forward.
00:05:15.850 - 00:05:35.466, Speaker A: Just make the browser itself. Yeah, there you go. And let's hide the sidebar better. Yep. All good. Okay, so in this first one that we just ran, it was block timestamp between the two dates we were interested in, which happened to be August 1 and August 2. And we're getting 37 results.
00:05:35.466 - 00:06:15.198, Speaker A: And if we scroll through this, we notice it ends with the very last result being at 1136 utc. But I thought we were trying to get it inclusively to include the first and the second. So what's going on there? Basically, it's inferring this eight two to be at midnight. So when we run it, by actually hard coding in midnight, unexpected zero, run it again. It must have had a selection. We get those same exact 37. So if you're the problem with that is that you're casting from.
00:06:15.198 - 00:06:53.692, Speaker A: You're comparing a timestamp to two different dates. So two ways to get around that are either to straight up cast the timestamp here, block timestamp, date to a date, and then you're comparing between, like, data types. So what happens here is we get to 37, which was previously the last record, but then we continue on to August 2 because we can see that this is being truncated down to August 2. And we are actually going through. So what's happening here is block timestamp. This is coming through as eight two. And this is equal to what we're looking for here, August 2.
00:06:53.692 - 00:07:37.744, Speaker A: So therefore it is returned because it's in the set that we're actually looking for. So that's effectively the same as. We can check that wherever. Now, if we don't actually cast this to a date, but we do write this out as a timestamp, we should get the same 58 results. And where we go through here on the 38th row, we are still continuing onward onto August 2 because we're actually telling it to go through the end of August 2. So while it is inclusive, just rehashing one more time between is inclusive. That is only the case when we're actually looking at it, when block timestamp is a date and not a timestamp.
00:07:37.744 - 00:08:08.106, Speaker A: So that was a great conversation that came about in the chat. Just wanted to quickly hit on that as we continue. So then there was a one brief thing that we did not finish up on with the first segment. And that's just with a couple extra functions that we could use. Let this open up. So we went over some. I believe we also went over average, but there are other statistical functions you could plug in over a numeric column like median min max.
00:08:08.106 - 00:08:44.144, Speaker A: So over this month of June is what we are looking for. Let's go ahead and actually cast that to a date. So it's a little bit more inclusive of that last day in June. Platform equals openSea. And as a reminder, we are looking at only where currency equals Ethereum because there was a bunch of other non Ethereum tokens in there that were causing the absolute number, which is the platform fee, to be a little bit skewed. And that was giving us kind of an odd metric. So this is taking a second to run, so I will wait.
00:08:44.144 - 00:10:02.660, Speaker A: Hopefully it won't take too long. So presumably when this returns, we'll have columns with some average median min and max for those fees collected by Opensea in the month of June. And that's great. We can calculate some stats and metrics across our results set, but how do we go further? We often want to see what's happening within the set over time and across groups. So when we were going at figuring out the fees that occurred in Opensea, we kind of hard coded ETH in here because it was easy and it's convenient and there's a bunch of other tokens in there that were obfuscating that token, the token denominated fees as opposed to just the USD fees. So how do we, I don't know what are better ways to account for that? Anyone have any suggestions? Instead of just saying, all right, we're only going to look at one currency because it's easier that way, instead of just hard coding that in. Do we have any suggestions in the chat about, you know, a different way that we could go about slicing our result set? You know, we still want to look at maybe the sum of fees in the month of June, but we don't want to just look at ETH, we want to see it for all of them.
00:10:02.660 - 00:11:38.812, Speaker A: Yes, exactly. So we can use group by, and what group by will let us do is basically take those different currencies and apply that sum kind of across the board. So let's fork this. So we don't, let's cut these out for now and let's still look at, how about the sum fee in USD from Ethereum crypt sales and currency symbol? So this is where we were before. We were just summing these up, really hoping this one runs through and before we're getting an answer, somewhere around 12 million for total platform fee in USD and a massive number for total platform fees. But what we can do instead is we want something that looks more like this, where we have a column with the currency. So Ethereum, and then we have total platform fees in just native ethnicity, kind of like what we had before when we use just ETH in the where clause to filter down to that.
00:11:38.812 - 00:12:18.170, Speaker A: And then the USD fees from that category, and then whatever the next one was, maybe it's USDC is the second most one. And in that we have whatever the USD fees are in tokens, which should more or less equal what it is in USD as it's a stable coin. So in order to do that, currency symbol is a group. It is a category with however many number of symbols in there. I think we came up with 26 when we actually went in and looked at, you know, how many distinct currency symbols were in this. Yeah. So this is what we want to split, end up splitting out in, you know, appropriating these numbers to each individual group.
00:12:18.170 - 00:12:55.960, Speaker A: So all we really have to do here is we're still going to look at the month of June, we're still going to look at just open sea. That's going to be our result set. But we're going to add a, a group by one. And what that does is it tells SQL that basically slice our data set across all the different available groups within currency symbol and apply sum only to that group. So everywhere currency symbol is eth, it will sum that up. If it equals something else, like wrapped etH, it will not add it to that group, but it will make a group separately for ETH. So I like to think of it as squishing, kind of squishing all the data into each other.
00:12:55.960 - 00:14:09.700, Speaker A: And then let's order by. How about f USD descending? So we can start with the largest one first, which is taking a little bit of time. So let's just pretend that ran, and it did, but I just had done it in the past, so we don't have to wait. So now what we have is the same query that I just wrote out where we group them by one, which is currency symbol. And the three columns that we have are currency, total platform, fee and fees in us dollars, split out by whatever currency was used for all of the staked ETH that was used to buy an NFT in that month of June on Opensea, we are summing up that to be 0.2, which comes out to $300 in us at the time, 1687 in wrapped ETH, $0.83 in tether, 31,000 in USDC.
00:14:09.700 - 00:14:49.530, Speaker A: So this is how we can actually go in and split that big group of numbers out into individual groups. That's why it's called group by. So let's take a look at this a little bit of closer because we can also use the tools at our disposal to investigate the data going through this and just kind of reading off each one over four pages. I can get the information, but it may not be the most efficient way of doing that. So let's start. Let's slice this currency symbol. We want to look at fees and dollars so we can see there's taking all that data that was just in our results, which is the month of June.
00:14:49.530 - 00:15:09.116, Speaker A: 86% of fees are paid in eth. 13.3 in wrapped ETH, which leaves. What's that? 93.39.3. So a very small amount in anything else. So let's maybe hide Eth. Still massive amount of the remaining is wrapped eth.
00:15:09.116 - 00:15:34.158, Speaker A: So let's kind of hone in on everything else that's going on in here. The 51% of the remaining 3% of all sales were in this thing called gala, 26% in USDC. That's. I would have expected that to be different. That's kind of interesting. 1012 cent in sand ape. So this is interesting.
00:15:34.158 - 00:16:46.416, Speaker A: I really thought USDC would be much higher than gala, so I would like to maybe figure this out a little bit. Let's just kind of explore something. So let's select all from. Was it currency symbol? We don't need many, so let's just do a limit ten. Look, I did that one before, too, so it's just something that I ran before with the very same select all where security symbol equals ten. I don't know what gala is, frankly. So I'm going to go ahead and kind of take a look at one of these transactions just to see a little bit more about, you know, what it's about.
00:16:46.416 - 00:17:22.200, Speaker A: So a tool that you can use that's probably the most popular one to actually go see what's going on on Ethereum is Etherscan. So it's a block explorer. You can paste in, you know, a user address, a transaction, hash a block id, and go see a bunch of information about it. So what I did was I went and found a transaction where the currency was gala. I just grabbed that hash and we're putting it into Etherscan here just to see what it looks like and what's going on. Kind of see transactions in a different light. So one NFT sold for 20,000 gala on OpenSea transfer of town star.
00:17:22.200 - 00:18:46.814, Speaker A: From here to here, we can find a lot more information about what happened and what steps it took by going into the logs and even if we wanted to, let's see, take this same transaction and see some of the more raw details about it, raw being what's in the actual transaction itself as opposed to these easy NFT sales table. The goal really is to move beyond those and just really start to figure out what's in Ethereum transactions. How can we pull more insight out of it? The easy table is a very good and clean for using and learning. So let's take this here. So here, is this the right one? No, this is just looking at a transaction on Etherscan and in the raw fact transactions tables, how we get it, a lot of the same information, block number, timestamp, hash, transaction hash, are stuff that are, you're going to see as boilerplate in a lot of different records, no matter what they're in. Then we have some other stuff that you're going to be able to see here. If we click to see more like nonce and position, can you zoom in on the signature? Which one? The Etherscan? Yes, we'll see from address zero x 32 is here.
00:18:46.814 - 00:19:54.270, Speaker A: Interacting with the two is the Opensea Wyvern contract. So you might be thinking where does all that information that ends up in the easy NFT table come from? So that's going to come from the logs here, which are going to be, let's go way past the input here in this item called Txjson. So this is not to necessarily scare or deter people, but just to highlight what's actually in the transactions, what's behind this curated and cleaned up data. So I'm just going to open up a text editor and paste the JSON in here because it's a little bit easier to see. And let's scroll over and maybe zoom in on that too. So this is a lot of that same information that we have here, pulled out some of this pulled out into columns of its own, like block hash and block number, the from address gas information, that stuff that we expect people to commonly use. So that's going to be priority information for the table.
00:19:54.270 - 00:20:35.144, Speaker A: But where the kind of the meat of the transaction is, is in logs, which you can see represented here on Etherscan and it's matching here on the actual transaction object within the raw table for this first log here. So logs is going to be, you can kind of tell by this, an array. So this tells us there are six logs. So each array is going to be a dictionary, if you can see tiny little squiggles underneath this curly brace that I just clicked. That's Adam telling me that this is the end of that curly brace. So this is log one. So we have the address here that should match up ox 15 data that's encoded.
00:20:35.144 - 00:21:30.750, Speaker A: So if we go down here and go to hexadecimal that matches up, that looks like the number that we end up with on the overview where it's 2001 of these is probably going to be the contract for the gala right here, decoded contract name, gala transfer. So these are all the steps that are happening. And I'm not going to go too much deeper into this because like I said, I don't want to kind of deter people away or scare people with big ugly JSON that you have to parse through. But just showing that this is where the information comes from, we get this from the block and then it gets cleaned up nicely. It ends up in an easy table like that. So on the overview page here, where the transaction action is, the highlighted events of the transaction, it's a sale of an NFT. That's effectively what the easy tables are trying to reproduce is here's the pertinent information about what happened in this transaction very easily and directly shown to you.
00:21:30.750 - 00:22:23.388, Speaker A: Everything else is still available there. You just go to the transaction log itself and you can pull out any other information in here that might be of interest. So not going to spend too much more time here. In fact, I'm just going to close out of the JSON altogether and frankly, of Etherscan and come back to this. So then if we kind of take one last look at this transaction unselected, sometimes it doesn't recognize that I selected it and see that in the core NFT sales, we get just that information that we want. Where's Ethereum core? That's mins sales, where we're just going to get, you know, the general boilerplate block number, timestamp. But then that address is going to be labeled properly as the buyer address.
00:22:23.388 - 00:23:15.138, Speaker A: You know, different creator fees, currencies that we used, that's kind of spread out across those logs. So really that's, we're going to get to that. We'll kind of dive more into parsing through a JSON in segment three, but wanted to kind of give that background and overview about what happens, you know, between the chain. There is a little organization transformation that happens on the chain walker level, but then all the different layers that are happening to get it to this easy NFT sales table so that we have this cleaned up data. So all that to stem from just taking a look at the data where we are grouping by different types of currencies and using visualizations to prompt more investigation. So I didn't know what Gala was. So I was like, let's go find out a little bit more about it.
00:23:15.138 - 00:24:08.238, Speaker A: Let's take a look at a transaction that's using gala. And if I wanted to, I could go back with that same hash and go to the NFT buyer, look at them at Opensea, actually. What does that NFT actually physically look like that they just purchased there? So as we're here in group by. Any quick questions, will group by make queries run faster similar to count? To a certain extent, yes. So it is a function that is being called on a result set of data so that you can run a sum over a year's worth of transaction data, which is going to be a lot. And that will be much faster than going and getting all of those individual records. Because what's happening is the database.
00:24:08.238 - 00:25:06.890, Speaker A: It's applying that aggregation specifically to one or more columns within that data set. And then it's kind of holding that as it runs through it. And you will get back the aggregation faster than just getting back the raw data, because presumably what you're getting is an abstraction of the data that you're looking at, which is going to be much smaller storage wise, like size wise, than the actual data itself. So if you're looking at to compute metrics over something, you can much more easily say, all right, I want to see average rolling price data or number of sales or fees computed over a year. That should run fairly quickly, much more so than getting a year's worth of transaction data. So let's, I didn't save that visual, but that's okay. So let's keep this, let's hit fork so we have a new one so I don't ruin that prior.
00:25:06.890 - 00:25:24.690, Speaker A: And in here we group by one. Let's run this because it's a new query and get our results back. There was a lot in here that was very small dollar amounts. I think if we. Oops, let's open and go back. All right, great. That went pretty quickly.
00:25:24.690 - 00:26:17.322, Speaker A: No, it didn't. That's a different one. So there's a lot of currencies in here that had either no USD amount because we didn't have the price for that, or because there is no price for that, or a very small nominal amount of tokens sold. So maybe we want to filter on our aggregated results. So we can do that. We can filter on aggregated results, but we can't use where so where is going to live with the from? So this is if you think about like the select from where all go together for our result set, telling us what data we want, where we're going to get it, and then how it's going to be transformed. So if we want to say, all right, I want the sum of total platform fees by currency, but maybe let's cut out some of the lower outliers.
00:26:17.322 - 00:27:23.978, Speaker A: I know we just talked earlier about not cutting things out because, you know, it's a convenient way to do so, but, you know, deciding not to have an outlier in there is an analytical decision, and it's one that should be documented if it is a presentation worthy thing. But what we could do here is use the keyword having. That is not how you spell it. There we go, having. And then it works very similarly to where in which I could say total, total platform fees, USD, let's say anything greater than $1,000 is all I want to have returned. I make a little executive decision, of course, as soon as I do that. All right, so here, these are the same results as before, except we're cutting some out.
00:27:23.978 - 00:28:04.710, Speaker A: So I guess in theory, it's not the same results as we have before. So on any given currency, we are only going to see the ones that have $1,000 or more in platform fees. So RAPD, USDC, sand, mana, gala, eth, and Apeze, everything else has just been cut out. So that's, you know, you may have thought like, okay, maybe that's something that should be in the where down there. But if you want to group, if you want to aggregate and then you want to filter your aggregation, then you have to use having. So from and where are partnered together, whereas having and group byte are partnered together. And then we can group by multiple things.
00:28:04.710 - 00:29:12.340, Speaker A: One very important one that we tend to group by is dates. So we've got this, you know, big couple million dollars and you in wrapped ETH. What's that? $14 million in ETh over the course of June. But how does it, was that a bunch of, bunch of sales at the beginning of June, at the end of June? Was that even over time? So a very common way that we will actually start to apply this is by saying, all right, let's take a look at each day or week or month, if that was the time frame we were looking at. And we can group by that, but what we'd have to do. So we can't if we were to just put in Buck timestamp and let's say we would want to group by that probably wouldn't go too well. Does anyone have any theories as to why we may not want to do that? Yes.
00:29:12.340 - 00:29:42.114, Speaker A: So if you recall from before, I don't think I still have that one up. Yeah. Each block I do each block timestamp, you know, in this case was August 1 at 632 and 653. Each one of these individual records is going to be what it's grouped into. So if it does not match down to the second in this case, it looks like we're chunking it millisecond, then it will be in a separate group. However, Harken back to what we talked about before. If we cast it to a date, then all these are the same.
00:29:42.114 - 00:30:14.828, Speaker A: And categorically, August 1 is the same as August 1. But here on row 38, any record on this, you know, 38 beyond is going to be in a second group. Yeah. So Rumusi says too many increments of time. So we can either cast by date as we did before, or we can use a different function called date trunk. And what that does is it basically truncates the date exactly as it sounds into whatever part you put in here. So if I really did want to get down to the minute, I could.
00:30:14.828 - 00:30:47.140, Speaker A: And you could take. And it will basically, you know, cut the seconds away and just be like, you know, 231 instead of 231 and 30 seconds, we could go down to the hour. So it's 230 right now, this would be something that happened in the 02:00 hour. So everything for you could look at, you know, the granularity within a 24 hours period. But we've got a month up, and we probably don't want to get so granular as to be looking at an hour. Let's go for day. There are 30 days in the month of June, so this should be good, right? You know, we're going to group by date, trunk, day.
00:30:47.140 - 00:31:20.460, Speaker A: Let's alias it, because I like to use clean aliases. Currency symbol, total platform fee, total platform fees, USD. So it should give us the fees and fee denominated in dollars for each day. But what we have to do is group by one, two. So just like order by this is, it'll follow a hierarchy here. So one refers to the first column, two refers to the second column. So basically it'll take everything from the day first, and then it'll split it down into the currency symbol within that day.
00:31:20.460 - 00:31:57.874, Speaker A: If we flipped it the other way, where it was currency symbol, group by first and then date second, it would group, you know, all of the Ethereum sales and then the theorem sales within that date, and then you'd end up with a much longer. All the ethereum sales, each bidet, and then all like the rapt eat sales for each day need to be a little bit messier. So please go quickly. So we're going to do this twice. Let's do ascending just visually so we can see it in order kind of for each day. Great. So we now have staked ETH 0.0,
00:31:57.874 - 00:32:36.328, Speaker A: $120 of staked Eth on the first sand, $0.45 in sand on June 1. So there's still a lot of dust in here. But we're not excluding, we can actually start to see how that's splitting it up. So similarly, using visualizations for exploration is a very powerful way to actually see what's happening. So we'll put date on the x, let's go with fees, and then we want to slice it by the currency symbol. So right now we're using platform fees in native token, which isn't very helpful because I don't know what that rise, I don't even know what this.
00:32:36.328 - 00:33:23.468, Speaker A: There's so many tokens that I can't even see kind of what each individual one is. And, you know, three and a half times ten to the 16th is a massive number. It doesn't really tell me too much. So when we're comparing like this, let's use like amounts, right? We can see here that green is most likely ETh here. There's something that bumps up here, USDC, but there's still a lot of noise going on. There's so many days where a lot of these have zero sales that it's, it's kind of tough to figure out what's what. Like, can I know that ETH is the biggest one? And I can kind of see, if I point my finger here, that on this date they had about 700,000 infusions from ETH and a little bit more, but it's very hard to see.
00:33:23.468 - 00:34:26.740, Speaker A: So how do we get rid of that noise without totally excluding it? What's a way that we should account for that? So what we want to do is we want to take currency symbol and we want to clean this up. So we know that there's a couple in here that have a lot of the volume. And from that pie chart from before, we know it's EtH and wrapped ETH, but maybe let's add a second. So group order by date, and then how about four descending so we can see. All right, so on some days there's null, which is returning. First. We have a null currency total platform fee.
00:34:26.740 - 00:34:56.756, Speaker A: So I saw a limit currency symbol pop across the screen. So we did that before, but we don't want to exclude data now, we still want to include it. So I see Robert Simmons popping up case statement. Select a few you care about and everything else is other. That's exactly what I would like to do. So it looks like ETH, perhaps ETH, at least on June 1, are some pretty strong contenders. USDC is surprisingly not very impactful, at least on one day.
00:34:56.756 - 00:35:44.102, Speaker A: So let's leave it in there for now. So what we can do with a case statement is basically add some conditional logic within our select. So we're going to be operating on currency symbol here, and we're basically going to be replacing currency symbol kind of with some conditional logic. So the syntax is you start with you tell it a case, and then you would say case ends and you alias it as. So let's just keep it as currency symbol so that anything in between here is where you actually start to fill that out. So what we want to do is basically keep ETH because that's the biggest one that's going on in there. So when I eth, and we'll keep eth.
00:35:44.102 - 00:36:24.684, Speaker A: Simple enough, forgot to be wrap eth, and we'll keep wrapped eth. And I don't have to hard code this in here, I'm just saying whenever equals wrap, wrapped eth. And this is a logical statement that you may have put into a where clause. But now we're basically having this check within the case statement, currency symbol equals eth and true. Then it's going to come out. Then it's going to evaluate to this. When it's wrapped eth, this first statement will be false, so it'll skip it.
00:36:24.684 - 00:37:03.560, Speaker A: So then this is kind of like the else in an if else statement. And then the second condition we're hard coding in is wrapped eth equals wrapdeeth. I could also put in just currency symbol there. So like when currency symbol equals wrap eth, then just take the value for whatever is happening within that column. There's no reason not to do that over another. Maybe sometimes you want to take a column that's success or fail and turn it into a boolean one or zero, true or false. That's where you would potentially put in kind of whatever you want here.
00:37:03.560 - 00:37:41.742, Speaker A: I'll leave it as is. It doesn't much matter at this point. And then we're going to keep USDC, then USDC so then that's it, right? Not quite so. Exactly. So it's original message. Everything else is other. So we do need to have everything else.
00:37:41.742 - 00:39:20.120, Speaker A: So anything that gets through all these, you know, evaluates false, false, false, you know, cascading down, we basically have a catch all where we just say other and it's current symbol and nothing else is changing. We're still going to group by that. So basically what this is doing here is taking the 26 currency symbols that were available in the columns here and parsing it down to four ETH, wrapped ETH, USDC, and other, where anything that is not these top three is just going to be grouped into other. So then what we end up with is very much the same thing, slightly different, because I did this earlier and I don't feel like waiting for that to spin. So what I did was I expanded the time horizon to be January 1, and I'm using date chunk by week. Otherwise, it is the same statement that we just wrote with the case being eth, wrapped ETH, USDC and other. So now we can actually go in and start to look at it a little bit more cleanly, where we just have, you know, four items on our axis here we can go in and see.
00:39:20.120 - 00:39:43.478, Speaker A: So numbers are gonna be different because it's weekly, not daily. And it's from January all the way through June. And we can see, you know, April 25, bump back up 25 mil in Ethan fees that week, 2.4 and wrapped ETH 358,000. Other a mere $38 in us. No, 38,000 in USDC. Almost didn't see that k there.
00:39:43.478 - 00:40:14.784, Speaker A: So similarly, we can use visualizations to prompt more analytical thought processes. Be like, all right, I can clearly see what's happening with Ethereum. Let's, you know, pop that down. We see a similar increase in wrapped Ethan fee generation in around this April may timeline. Maybe let's pop that away too. But now, this is very intriguing to me. Anyway, this other category that we excluded, that we considered excluding before, actually might have something that's worth investigating.
00:40:14.784 - 00:40:46.754, Speaker A: This week of April 25, it went from, you know, the respectable 40 50,000, you know, pop up to 160, but not much in the grand scheme of the ETH sales. But, you know, ten xing in from one week to another to 350,000 in fees. By other tokens, that's somewhere that I would say, all right, let's go dive into that. Let's open up a new query. If I wanted to figure out what was going on, I'm not going to do that. At this point. But that's just the analytical process of this seems interesting.
00:40:46.754 - 00:41:37.414, Speaker A: Let's go find out what went on. And that's the goal here is equipped you all with those tools to actually go be able to see these patterns and to go figure out what happened. And even USDC popped that day from, you know, less than 3000 to 8000 all the way up to 40k in that same week at the end of April. So clearly something was happening. If we keep going back out, you know, that's a noticeable increase for Ethereum for sure. You know, 10 million over two weeks, 4 million week to week, but not nearly as starkly different as going from, you know, ten x from $34 to $358,000 in fees there. So, you know, using visualizations for analysis are also, you know, it's a great process.
00:41:37.414 - 00:42:08.050, Speaker A: That's a very good tool that you can use as you're going through this. It doesn't have to always be a production quality viz. You could just slap everything together if it's only you that's ever going to see it. But it's a great way to figure out what you're actually looking at. Much more so than, you know, clicking through all eleven pages of this and hoping that you pick up on what it is that you're looking to pick up on. So, you know, we're clicking through, we're seeing, you know, we might pick up. That doesn't look that much different.
00:42:08.050 - 00:42:58.076, Speaker A: You know, it's 22, 25. Oh wait, there's, you know, that's wrapped e, there's another decimal there. If you're going through quickly it can be very easy to miss the fact that this other went from 34 then it's at the top of the page, not even on the same page as the previous week. Point is, visualizations are powerful and when you have something as dominant as ETH is for the opensea fees, taking it away can help reveal some of that because there is a key point here about relativity and scale. You wouldn't necessarily see this spike in these other transactions if you're just only looking at it from a super high level view. So that's all on that. But though, you know, there are many ways to get about this, sometimes we need data in multiple formats.
00:42:58.076 - 00:44:17.788, Speaker A: We need to use it in multiple times or in different ways or we need to get it from somewhere, you know, but it's based on a table elsewhere. So we've been writing this whole time, you know, single line queries. Single line, you know, just being kind of one select statement where everything's together here. But there are other ways that we could structure a query. So this whole thing could be rewritten in a two step process where let's fork this, pull this down, where let's, let's split out what happens when we actually grab the data and then when we apply this transform on it. So what's going to happen here is I'm going to write something called a CTE, a common table expression, and I'm just going to copy this to up here because we want the same result set that we're going to operate on here. So what this first clause is going to do is it's just going to run this, you know, select all from Ethereum NFT sales and it's going to return what we're looking for the first six months of NFT sales that happened on Opensea and then separately.
00:44:17.788 - 00:45:02.430, Speaker A: What we're going to do is we're still going to keep the date trunk of block time stamp as date. We're still going to apply the case statement to currency symbol so that we can limit how many tokens that we're working with and all that noise. We're not going to take this from Ethereum NFT sales, though. So this where transform filtering, whatever you want to call it, is already happening up here, basically in this fairly basic CTE. What we're doing is we're factoring out the bringing in of the data that we're looking for in the transform that's happening on it. So the columns that we're looking at are still the same lock, timestamp, currency symbol and the two fee column. But now we're referencing OpenSea sales.
00:45:02.430 - 00:46:46.600, Speaker A: So I won't run this because it's going to bring back a lot of records for a six month span for NFT sales. But what is going to happen is it's going to return basically a filtered result set that looks exactly like the easy NFT sales data. If we just open up the preview instead, where the dates are just going to be range bound between the first six months, platform name is only going to equal OpeNC and then that should be all that we need to change there. We're still going to group by and we're still going to order by and these will be the same exact results. So even though we, the query looks different and we are doing something differently here, it's just taking two parts instead of administering it in one part. So while this is running, I guess I'll just ask questions as we wait for the spinning wheel of death. How can we use ctes in our work and what ways can we incorporate factoring out parts of a query into different parts in our analytical process? Well, one thing that we could do is use it to bring in data from other sources.
00:46:46.600 - 00:47:38.910, Speaker A: So let's say we didn't just do that, reset this back to where it was. Let's say we have opensea sales and fees generated over time. But we were curious about how that interacted with, you know, the price of Ethereum, which does not obviously exist in the easy NFT sales table. I'm sure it's somewhere, you know, let's say we're in Ethereum, we're looking through hourly token prices. That's probably where we're going to find token prices. It's the granularity is each hour. But that's okay because all we're looking for here is on a daily basis over, or actually even a weekly over a six month period.
00:47:38.910 - 00:48:23.630, Speaker A: Maybe we can change that granularity. So we have prices in this table and we have easy NFT sales in another table. Can we, how do we get, how do we use a CTE to get data from those two disparate sources and bring it together? You know, it's not, they're not in the same, I can't just say, you know, select from Ethereum core easy NFT sales and in fact hourly token prices. We know that won't fly. We've got a question from the audience coming in here. Yeah, absolutely. And we can get there right now.
00:48:23.630 - 00:49:05.080, Speaker A: So let's continue with this and then we can use multiple ctes as the example unfolds. So let's say we want to get prices and we're going to take a look at the prices table. Ethereum core fact hourly token prices. Okay, so hour and price are basically the two that we're looking for as. Yes, exactly. So we'll be able to take the prices table here and join it to our result here. So that's what we're about to do.
00:49:05.080 - 00:49:36.280, Speaker A: Let's take daily hour because that is what the date column is called in this table. I can see on the side there as date. And then, you know, presumably we have 24 records in there for the hour. We don't want to just take one of them. We obviously don't want to take the sum of them. So we use what we learned earlier and we'll just take the average price. So the daily average price.
00:49:36.280 - 00:50:32.390, Speaker A: And let's just remind each other that it's us dollars from core tokenization. Let's not just grab everything because that might be a little too slow. So let's range bound that. What do we say? Zero? 630. Now, if we are to preview this, we see that this is for just about every ERC 20 token in here. I don't even want to guess at how many of these we have. So obviously we don't want the average of all prices for each day.
00:50:32.390 - 00:51:38.690, Speaker A: So we do also have to designate where symbol equals. Well, let's see what's in here. Using what we discovered last time were anythings like ETH, because we don't know for sure that it's going to be in there as ETH could be in there as Ethereum. It could be wrapped ETH, it could be staked ETH that we have in here, 89 rows in here. So that's going to take a little while to look through. So maybe we order it by one and take a look at it in an alphabetical order. Looks like ETH itself is not in here.
00:51:38.690 - 00:52:38.800, Speaker A: So what do we do about that? Can we even find the price of ETH? Well, we've got, if I can get there alphabetically, well, we've got wrap thief, and that's an ERC 20. So presumably, if I were to look at the docs, it would tell me that this is token prices. So we won't have native EtH in here, but we will have wrapped ethan, which is okay, we can use that as a proxy for the price of ETH as it trades one to one. So this should give us, Jen, just the price of ETH. Something that I love to forget to do is actually write the final group by. So now we're just getting the daily price of ETh from January 1 to the end of June. So then let's say we want to write, do something else.
00:52:38.800 - 00:53:43.262, Speaker A: We want to write another query and we're just going to take the information that we had before. So let's hide the results for a second and then let's hide the sidebar so we can just focus in on what we're doing and we can write another CTE. So basically, each one of these is just going to come through and return a result set that's stored almost as a table, right. It's sitting there in active memory and we can continue to, you know, use it for as long as the query is running. The second the query is done, this prices object, this CTE, this, you know, temporary table will no longer exist. So now let's take what we wrote before with Openc fees and make another Cte. So these are comma delineated, bound in parentheses and labeled almost inversely as aliases where you have what's happening as date, you have the name open fee as, and then your entire query is going to go in the parentheses here.
00:53:43.262 - 00:54:12.060, Speaker A: So we are basically going to take the same exact thing from before. We don't need the order by in there. We're just getting daily fee for these currencies and other over this time, over this time range. So now this, we know what this returns. We just looked at that and we just took a look at it in the table and in a chart. And now we have daily token prices. One thing to remember is we got to use like units we were looking at weekly before.
00:54:12.060 - 00:54:48.910, Speaker A: So I'm going to change this today. You can use day or just the letter D. They are both date and time parts, which I can post a link to in the notes for afterwards. But yes, now, as Vermuski mentioned on the side in the YouTube chat, we can do both and run both and do a join. So joins are very powerful and it's how you're able to use a lot of different data sources within a single query here. So we're segmenting out different processes by using ctes. We're getting prices in one, open fee in another.
00:54:48.910 - 00:55:34.568, Speaker A: So we want to use them together. Right. So we can select from, you know, kind of following the same structure. And what we're going to want is date. We want the average price, USD. And let's just look at the fees in USD. So, but what are we actually selecting from? How does that look? Which one of these goes first, which one's joined into the other? And what does a join even really look like? Well, typically the way I do it is I take the primary information first.
00:55:34.568 - 00:56:21.640, Speaker A: I know that there's going to be, I want fees to happen every day and we're going to use this thing called a left join, which I'll get into in a moment. And we know the prices happen every day, so there shouldn't be too much of a disparity there. So I'm going to say Openfee happens first and then I'm going to do this where I alias it with the letter o. When it's table, you don't necessarily have to use as, even with columns, you don't necessarily have to use as I believe. But this is just the syntax that I personally follow. And then immediately following the from, we're going to left join prices. Are we done here? Can we just, are they just now joined? And while that question is hanging out there for the chat.
00:56:21.640 - 00:57:01.800, Speaker A: Remuski asks, so starting the query with width allows you to essentially define and create new tables effectively? Yes, that's one way of looking at it. But notice that I'm starting only the first one with the word. With each subsequent one just follows the comma. And basically it knows that. All right, we are stringing together some ctes here and then there's no CtE here. So these are, think of these are as creating those local tables in memory that are happening up here. And then my main query down here is selecting from those that were created before in the openfee and prices ctes, common table expressions.
00:57:01.800 - 00:57:51.660, Speaker A: I could, if I wanted to wrap this one in another CTE syntax here. There still needs to be a query at the end of this, so I would need to do something like this to ultimately tell Snowflake that this is what's being selected. Here's all the transform that's happening in a three step process where we're getting prices, we're getting fees, then we're joining them together, and then just go grab all that. All right, so Harken back to the question from before, correct? No, you have to choose a common column between the two. Yeah. So, hoping this actually returns an error. See SQL compilation error.
00:57:51.660 - 00:58:36.674, Speaker A: Whole bunch of unexpected. Unfortunately, it's a little bit cryptic, but we know that the problem that's going on here is the left joint, open prices. You basically have to tell it, you know, what, what are you joining them on? So joining is basically an overlap. Let me pull something up here where you are taking data from one. So we're using a left join. So what's, you know, your resulting set is going to be basically everything in the red circle here. So the left table being our open fee, we want to keep all of that, and we only want to bring in data from the right side that overlaps with that.
00:58:36.674 - 00:59:17.090, Speaker A: So you need to kind of tell like, what is that overlap? In this example, we have an id name and a user id and what they like. So in a left join, we are using their id and their user id. So id one exists here and in this table here. So in a left join, we'll have Patrick climbing Patrick code, because we're joining on that one there. Albert, id two does not exist in this table. So you know, that result is going to be null for that. So effectively, just wanted to kind of demonstrate that we need to tell what's common here.
00:59:17.090 - 01:00:01.166, Speaker A: So the date, we're looking at this as a time series where we have the average daily price and the platform fees on that day. So there's a great syntax that I like to use with SQL that's just a snowflake. Rather, that's just a little bit less typing is using. This could similarly be written as on O date equals p date, but I alias them the same for a reason, because I like to type less, if that's possible. And then one final thing that we are going to need to do is be explicit. SQL doesn't know what you mean even when you're joining on something. And there's two date columns in here.
01:00:01.166 - 01:00:36.270, Speaker A: Let's say we also had a price table in here. It's always good practice, and in most cases essential practice to just explicitly declare what you're taking it from. So this O dot date is basically saying take the date from Openfee. We, we alias Openfee as the letter o here. If I didn't do that, I have to type out Openfee dot date. And we just, you know, revealed that I do not like to type if I do not have to. Price came from the price and then the fee came from open platform fees.
01:00:36.270 - 01:01:21.080, Speaker A: Let's see if I made SQL unexpected select position one unexpected comma usually means that I made a typo somewhere final as that was the only typo. Nope, lots of typos. Line three, unexpected comma. Oh, I forgot as twice mistakes happen. And I forgot an. And this is what happens when you do things live. Look at that, only four typos.
01:01:21.080 - 01:01:57.468, Speaker A: So now it's not ordered, but that's okay. But we have on January 7, average price of ETH was a lot higher than it is now. Total platform fees in us dollars were 7262. Let's order this because it's a little messy. So let's go order by one. And we have four dates, perennial. Four records per date.
01:01:57.468 - 01:02:48.150, Speaker A: That's kind of weird. Anyone know why? Well, we were looking at currencies before, which we don't need to do at this juncture. I'm just going to comment that out, not totally delete it, because maybe we'll want to come back and look at it later. And we're only going to group by one up in the open fee because that was returning exactly what we were looking at before. So we want to change it just a little bit. And this will go back to just for each day, the total fees without splitting it out by each individual currency. So now we should have 1st, 2nd, 3rd, 4th, 5th.
01:02:48.150 - 01:03:26.010, Speaker A: Great. Each day. And on here, Opensea platform fees changing the data around. So now we can kind of take a look at this x axis. We'll put the date, we will go total platform fees and dollars go into settings and the price, because, you know, dollar fees are in millions, prices in thousands. So we need to put that on another access, and then let's make it a line. So then what we can kind of see here is an overlay.
01:03:26.010 - 01:04:20.744, Speaker A: You know, we took prices data from one table, we took opensea sales data from another table, filtered it heavily to get down to just the first six months of the year. We could go back and split out the currency data if we wanted to and stack those bars on top just to see, you know, is this 11 million on May 1? Like, you know, all eth, is it something else? And start just infer, you know, some, some stories out of this. We all know what happened in May. Price of ETH plummeted, continued its downward trend. But at the same time, we can see that Opensea sales just looking at, you know, the overall area of their weekly. No, this is daily over. Their daily fee intake went from, you know, multi million spike all the way up to 11 million, their best day of the year on May 1, down to barely touching a million.
01:04:20.744 - 01:04:57.312, Speaker A: Shortly after. ETH plummeted at $2,000. And then the further eth drops, the further down our daily fees get for Opensea, most likely correlated to some extent. They collect their fees in Ethereum. So the dollar value of what they're collecting is down. But then maybe we can go in and start to think, all right, how can we account for that? Should we look at volume of sales in a count metric instead of looking at it as the fees that they're bringing in? Let's, you know, this is one way of looking at it. But we could do count of transactions.
01:04:57.312 - 01:05:53.960, Speaker A: We could do distinct users that are going through by using the from column, you know, using these pretty straightforward techniques that we've gotten to so far and kind of built up and iterated on, we can get there pretty in depth view of what's happening within Opensea, and we can start to notice things and find out where we want to ask questions. So before, when we were looking at it weekly, we had that big spike in fees around the week of April 25. May 7 is in that week. So a lot of that was concentrated around one day on the 1 May. So maybe we hone in on that and see, was there a big drop that day that was spraying a lot of sales going on? What's behind this? And visualizations really help you find those insights as you're going throughout your work. It's a really important and powerful tool to use while you're in the middle of it. So, yeah, I see that question.
01:05:53.960 - 01:06:37.200, Speaker A: I love this site. Joins can be confusing at times if you are struggling to comprehend, like what is a left join versus right join? Do you know outer or inner? Just kind of going through this visual process. I think it was dropped into the chat there of just kind of clicking through and seeing what's going to happen. Outer joins, just bringing it all in. If you don't care about nulls, you just want all your data together. Yeah, that will definitely be part of the notes as well. So in terms of the SQL, that is about it for today.
01:06:37.200 - 01:07:47.626, Speaker A: But I want to kind of hit on that idea of, you know, joins and ctes are kind of a contained representation of what a relational, relational database is. So one way that we can kind of view this right here is, you know, if we were to look at it as, you know, where, what are the sources of the data? How is it interacting? So what this is is a relational diagram. A lot of times you'll have it just with full on databases that show the full flow of information from one step to another. But if it helps to kind of see, think about what's happening when we're grabbing data from hourly token prices and then we are building this query, or building this CTE with a query where we're just getting the date and the average price of it separately. We are taking data from, you know, the core easy deck swaps table and we're grabbing information that we want out of that with, you know, the where clauses. And then we're aggregating it with our group by. And we're building this once again, you know, temporary local table that only exists kind of within the query as it's running.
01:07:47.626 - 01:08:53.852, Speaker A: And these are both then filtering down into our final query where we're taking the date from each one and those are joining on top of each other. Total platform fees is coming from over here from the Dex swap side of the house, and average price USD is coming over from the hourly tokens side of the house. And that's effectively very similar to how the not docs dot Google, the easy tables are built out. So like I mentioned at the start, all that information that is in Dex swaps is contained in those transactions and in the logs and in traces, and it's out there, but you have to bring it in from a lot of sources to get it to be as clean as it is, if you also think back to last Monday with the Dex table or. Yeah, the Dex table that we're looking at here, or like the, the information that's in there where we have the pool name, we have each token name in there in the message that's not necessarily available. A pool contract is a contract address. A token is a contract address that's just zero x.
01:08:53.852 - 01:09:28.060, Speaker A: So we've built out these dimensional tables that have liquidity pool labels. We have just a labels. One, we have contracts labeled on the backend. What's happening with these easy tables is very much, a lot of that I know we were talking about. Well, let's stick with NFT stales. So like, what's building this out is very much behind the scenes, a very complicated query if you want to think about it, kind of in a similar regard. So end result is that we get this core easy NFT cells.
01:09:28.060 - 01:10:36.560, Speaker A: So along the way, what's built out, and this is just available on the documentation for each of the models, is that we've got one model computing looks rare sales, another for NFTx and Opensea and Rarible X two XY seaport, which is the updated version, or I guess it's v two of Opensea. But these don't just come out of nowhere. Token transfers feeds into those, the regular transaction data transfers. Dex v two swapshe even flows into NfTx sales. So they're not, again, not to overwhelm anyone thinking, oh, if I wanted to recreate that easy Dex swaps table, I would need to put something this comprehensive together. But just to show that from the simple ethereum transactions table with that big ugly JSON, a lot of information can be pulled out. And so far we've been largely focused on the easy tables, which are great for answering questions and they are very great for learning.
01:10:36.560 - 01:11:10.750, Speaker A: But my goal is really to start getting you used to using JSON objects, looking at those transactions. So that's a lot of what next week's going to be focused on. But that being said, there is a lot that can be done with what we've learned so far. Metric style does run a bounty program. There are a couple open at this point. So if you want to find it, metric style XYZ, but most importantly, go down to analytics open requests. Most importantly, there have been ones that have already run.
01:11:10.750 - 01:11:35.516, Speaker A: So for me, I always found a great way to learn was to actually look at work that people had done in the beginner level. I was looking at beginner ones. So like, all right, if someone's fees and Abe V two versus v three. That's maybe. That's interesting to me. And I can go try that with some of the tables, but if I get stuck, it's a completed bounty. That's work.
01:11:35.516 - 01:12:22.136, Speaker A: I can go find and be like, all right, maybe I can use someone for inspiration. Or we go back to Uniswap. Right? I know that there's that easy Dex table uniswap on polygon. Which bridge to where over the past 30 days? Which bridge users? Okay, so I start to think I have to determine what a bridge user is of Uniswap are which bridge, which bridges are users of Uniswap using the most. So I got to think, who is a uniswap user? And then can I determine if they're using a bridge, which ones are passing the most value? That's just counting up how much is going from one place to another user participation in governance. I don't know if there's a gov table. Dex usage patterns.
01:12:22.136 - 01:13:02.418, Speaker A: Sounds like we could use that easy Dex table surge activity on any one platform. So cross comparing platforms, looking at the maybe some metrics, maybe averages over time while using a group by on the different types of platforms that are available in there. So you start to take a look at. All right, here's what we've learned. How can I actually go apply that? These aren't live questions. These are all completed, but there's a fantastic repository within the bounty program of practice type questions, and you can also start to get used to what might be asked in the future. You know, what types of questions is uniswap interested in? If we get another unigrant from them, that type of thing.
01:13:02.418 - 01:13:39.500, Speaker A: So if you do want extra practice beyond just going through and being like, all right, let me do a bunch of group bys again, without direction, this would be a great place to start. I believe there's convex harmony, Uniswap, Olympus, the hacks, scandals, and scams. Aave and true freeze are all available ones. And don't know if it's up at the moment, but after the fact, you could always then go look at some of the best work there. There's one in here that's marked as, you know, advanced. You're like, I don't even know where I'd be begin with that. But it's interesting.
01:13:39.500 - 01:14:48.140, Speaker A: The showcase will have other people's work in here where you can actually go sort through work that was submitted to metric style and showcased by metric style. So awesome resources. And that's really what we're striving to move towards is getting you all ready to be able to answer these questions and build these types of things. So I see a couple questions on the side as we wind down, just do a little q and a. Any reason to use right join instead of just switching the order of tables? No, use left join. You can't, you absolutely can use right join, but just don't put the other one in front. What's the most common column used to join ctes together? Well, it depends on what you're actually trying to get at, right? So here we were looking at data over time where we have the fees and we have dollars, and it makes sense because we're using a time series here.
01:14:48.140 - 01:15:35.260, Speaker A: But maybe you want to look at different activities by same address. So you're joining on maybe a from address or a buyer. If you're looking at NFT data and you've calculated some metrics there, and then you've also looked at deck swaps. So you have some metrics on volume for a couple different addresses. So then you can actually join on multiple things there. You don't have to join on just one column, so you can join on a date and an address if you have, you know, those columns in your, in your various ctes, and they'll kind of, you know, link in like that and then the rest of the data will fill out with, you know, throughout the rest of the table there. As long as those match up, because that's how you've set up your transforms, then you can do that.
01:15:35.260 - 01:16:40.498, Speaker A: So there aren't really super common columns to join ctes together because it really does depend on what you're actually putting together. But yeah, the ones that come to mind are user address, contract address, maybe a Dex pool, a token. If you're looking at different Dex swaps for ens in sushi versus uniswap versus balancer versus a couple other dexs there, maybe that's what it makes sense to join on. You calculate the metrics in different form and different ctes and you bring all those together. Is there a link to that map? Yes. So if I'm here in the flip side editor, flipside documentation, go to the tables. We are in Ethereum core, so all the general documentation is going to be in here where it'll direct you to that.
01:16:40.498 - 01:17:29.614, Speaker A: So we use a tool called DBT to do a lot of our data modeling at flip side, and then that will generate updated documentation with every change to the model. So quick links exist in here which make it a lot easier to get to where we want to go. So we were in NFT sales. You're now here in the DBT docs here, where it'll have the description for the table, all the columns, you know, the general documentation that you might be looking for might be used to just down here. It's called the lineage graph. You can maximize it up here, and then if you really want to stress yourself out, you can delete this filter and you can see everything that goes into the Ethereum core models. So this is all, all the interactor and interplay and interactions going on behind the scenes.
01:17:29.614 - 01:18:07.590, Speaker A: So anything with core under underscore is that's what's being kind of exposed to in the velocity app. And then if you click on one, it's kind of fun. You can just kind of backtrack to see. All right, token trace transfers, where is that coming from? Okay, we have silver transfers, which come from logs, which comes from the root transactions. And I believe we have that up for everything. So metrics, now we do some data creation of our own. And for near, we have the same thing up auto generated with any change.
01:18:07.590 - 01:18:52.860, Speaker A: A little bit less intertwined than the Ethereum one because we have fewer tables for it and fewer going on with near. But yeah, that's great. The sidebar is a little bit tough to navigate, I do agree. But if you find any, if you go basically to core, that's where you will be able to find the tables that are actually on flip side. Everything else in the middle might be of interest as well. But I would go straight to core to get the immediate information joined with two columns at a time. Can you please demonstrate that? Sure.
01:18:52.860 - 01:19:33.670, Speaker A: Let's just start a new query about. Well, mints has anything that we could. Oh, what if we did mints? We have the NFT two address. The NFT address is going to be the contract. NFT sales. I think that's also going to have an NFT address. So maybe we can do.
01:19:33.670 - 01:20:52.810, Speaker A: We are in sales right now, so I just want to make sure I grab the right column name, NFT address. And let's sum the price as ETH because we're using price, not price, USDA. And then we're grouped by one, two here, because what we're looking to do is sum up the esales by NFT address for each date. For an example. I don't know if I want to just let it do every single NfT contract out there. That might be a lot, but we'll see. Maybe I'll apply a filter in a second.
01:20:52.810 - 01:22:22.410, Speaker A: And then for mints, let's see what mints looks like. Do they use NFT address in this one or no? Yes, mint price eth is what we want. I'm realizing this might not be the best example because most mints are going to happen well before the sales start. But you got me on the fly, so that's what I thought of. Let's limit this though. I don't want it to run forever. How about past 30 days so I could hard code in what is today, August 9? I could say just July 9.
01:22:22.410 - 01:23:28.806, Speaker A: I don't remember if I mentioned this last time or there is this thing keyword in Snowflake. Probably other flavors of SQL as well. Just current date -30 and then I'm going to apply this up here too. Just so we're only working with a date range of 30 instead of this running over years and years of data, because that would be a pain. So let's do mint state IP address sales. Hopefully I got the syntax right for one like this. But I think you can just comma delineate with this parentheses syntax.
01:23:28.806 - 01:24:27.380, Speaker A: If not, I'll just use on eight. But then let's say I want highest mint first. And after that, sales. Unexpected parentheses. I know you. I have double joined using the using parentheses syntax before. I just don't exactly remember.
01:24:27.380 - 01:25:31.650, Speaker A: Oh my God. Like I said, I'm on the fly left join sales. I forgot to actually tell it what we're joining it to. When I did that, I deleted that. Like I said, lots of opportunity for little typos when you're doing stuff live. Anything else in here? NFT address. NFT address.
01:25:31.650 - 01:26:22.638, Speaker A: So now as this is running, it's going through easy NFT sales and it's taking the total eth sales for each NFT address on each specific date. And then it's going through and it's taking, you know, the total eth minted for each NFT contract address on the date. And then it's joining those. So the date is easy enough to join on. It finds July 10, then it labels that, you know, slaps that onto July 10, and then it looks for records where the NFT address from the mits table also exists in the sales table. And it slaps those on top of each other. So in this case, I chose mints to be the left table.
01:26:22.638 - 01:27:10.142, Speaker A: So if we go back to visual join to what a left join is, basically we're trying to keep everything that was minted in the past 30 days and see what sales happened out there. And the reason I did that is because I imagine a lot more contracts have been sold in the past 30 days than minted. So this is basically saying, all right, let's find NFT contracts in the left table that have been minted and we have some data for them within the past 30 days and then only join the contracts that are found in the right table to them. And that's what's going to be in this interior overlap there. Anything else that was sold within the past 30 days that is not found in that left table is going to be excluded from the final query here. Because that's how I set it up. If I wanted it to be the other way around, where I wanted everything that was sold.
01:27:10.142 - 01:27:48.776, Speaker A: But also let's bring in stuff that was minted recently, then I could have used either flipped which one was brought in first or use a right join. I think the only time I'd use a right join is in a case like this where I'm like, all right, let's quickly see what it looks like. But otherwise I would move mints down here in sales there. So let's hide you and let's see what the results look like. July 10 NFT address. Find out what that is. I'm sure I could check that is uniswap positions.
01:27:48.776 - 01:28:25.840, Speaker A: NFT total mint in east with 13,000 minted that day and 200 ETH sold. That's a lot of volume. So this is a month and we have 65,000 records. So we know we have at least 30 days in there. So what's that to come out to like 2000 records a day? And that's going to be like 2000 nfts on average a day. But then we kind of quickly start to fall off. No sales, 25 mints starts to go down, down, down.
01:28:25.840 - 01:29:30.390, Speaker A: So maybe we just want the top of the top. We're still on the 10th and now we're down to 0.08 minted that day. So how about they had to have at least, I don't know, ten Ethan sales or 28th minted that day, and then you can start to, you know, build that out. I don't know how many of these, especially if they are minted so recently, are going to actually be in the Ethereum core doom contracts. Will that have an NFT? It might. So then you can now just only stick around if you really want to at this point.
01:29:30.390 - 01:30:22.910, Speaker A: Now I was going to go in through some analytical process where I'm kind of curious. So then you can use a CTE as an intermediary tool. So I'm just going to call this temp and we're going to wrap our results that we just figured out here in a, in a new CTE. I'm curious about something that might happen here. So basically I want to know what of these NFT addresses are labeled. And the way we do that is we say, all right, let's take these NFT addresses and let's check are they in dimm contracts or are they in dim contracts extended? Because I believe those or even labels, I'm frankly not sure without checking the documentation, which we actually have open here. Dim contracts table, contract deployed, contracts extended.
01:30:22.910 - 01:30:51.062, Speaker A: That's frankly not very helpful. No offense to myself. Okay, so don't really know based on that. So I might have to just check them all. So now here what I'm going to do is select all from or dim contracts. Let's just start with that one. But then we can use a subquery, to use a subquery as a filter.
01:30:51.062 - 01:31:37.294, Speaker A: So we've got everything going on up in the ctes developing out this result set, which is NFTs minted in the past 30 days and their sale volume on each individual day. And we only want to look at the NFT addresses for that. So I can use a where clause and a parentheses to say where they call it address being, you know, address being from DiMM contract. So this is a where clause. Actually put that outside of parentheses in. And then we can just simply say select NFT from temp. So screens got a little messy.
01:31:37.294 - 01:32:20.710, Speaker A: So let me hide that and hide that. So those that are left. Does this part right here kind of make sense? Ethereum. So basically we have this from before. I just want to basically check Ethereum core contracts where the address from diem contracts is in this final CTE here. Line 33, line 25. That's up here as that's like the fifth time I've forgotten as.
01:32:20.710 - 01:32:50.610, Speaker A: So now we have 983 rows. I wonder if that will be cut down. It shouldn't be because I can't imagine there's duplicates in here. No. So that's awesome. Some little pudgies. We got the heart project.
01:32:50.610 - 01:33:27.472, Speaker A: So now I'm going to actually keep this temp table. I could rename it, but I won't select all. Well, I don't want to select all. Let's go eight. Now let's just call it t dot. Total eth minted, total eth sale from temp. Because that's what we called it, t left join.
01:33:27.472 - 01:34:01.408, Speaker A: Because now we really only want the data that's in temp. So left join is very important here. Ethereum core dimm contracts spell outdimcontract. So let's alias that with a c equals c dot address. But then let's actually tell it to grab the information we want. So maybe name. We just want to know what it is.
01:34:01.408 - 01:34:50.980, Speaker A: I don't care about this metadata at the moment. Whoops, that's not the run button. Sorry. Ignore me. All right, so now we've got some that are ordered. Not everything had a label in there, but now we at least have a lot more information. Instead of just this bland NfT address where I have to go to etherscan to look it up, we can start to look at the names of it and have a little bit of human readability to it.
01:34:50.980 - 01:35:48.308, Speaker A: A lot of these have no mints on certain days with a little bit of sales behind them. So we can start, you know, if we wanted to keep going further into like an NFT exploration, start filtering some of that out, be like, you know what, we only care about what, you know, the top ten volume. So let's limit it to that using aware or having, depending on, you know, in which part of the query you're doing it. But that's where. That's where we're going to call it a day. So I think this great question led to an awesome example here where we not only joined on two different columns here using ctes, took it a step further to actually pull in information from one extra source, that being the labels table with the dim contracts. So we could add a little human readability to it that helps our analytical journey there and figuring out what's been going on, on opensea in the past 30 days, sourcing out what projects are up and coming.
01:35:48.308 - 01:36:16.378, Speaker A: How did mints go? You know, what was the hot mint? We could start to really dive into a lot of that info. So I told myself at the beginning, let's hold this one closer to an hour. We are now an hour and 37 minutes. Thank you all for sticking around and indulging me in this. We. Let's do a quick mention of the transpose. Do you the transpose that just happened? Believe so, yeah.
01:36:16.378 - 01:36:42.014, Speaker A: We just posted the recording to the channel and also the Twitter space. Well, do you want to go ahead and continue that? To be honest, I don't know much about that. So that's all on you. Okay, fair enough. Well, we had a workshop this morning with transpose that was also hosted on Twitter spaces yesterday. So I think we had a space first then a workshop today. If you missed either of those, they were recorded.
01:36:42.014 - 01:37:09.890, Speaker A: And let's just blindly pull up discord. That's always a good idea. I think they have been, haven't been announced that they are available yet, but apparently they are. There we go. We have a YouTube link to it. Can we show that comment? So within our own channel? The workshop from this morning was recorded. However, I will go back to discord for a second because I do believe we have another workshop coming up.
01:37:09.890 - 01:37:44.458, Speaker A: Wednesday, August 17 we have a workshop with footprint on how to visualize cross chain dov decentralized option vault data using footprint analytics. That's an exciting one. And then tomorrow, don't forget we have an analytics course, open office hours. Any questions you might have, if you do get a chance to dive into some of those past bounties and to try them yourself and you get stuck, or if you get all the way through and you want to show one off and yeah, that's perfectly allowed as well, pop in. Andrew will be leading that one. Hope to see you all there and at the footprint workshop. Many thanks.
01:37:44.458 - 01:37:45.370, Speaker A: Hope to see you all next week.
