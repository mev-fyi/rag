00:00:00.240 - 00:01:13.026, Speaker A: Welcome to the workshop with Playgrounds. Today we have a very interesting topic, which is the curve words where we are going to investigate who really want we'll investigate the curve liquidity data before and after convex was introduced. And today with us we have Mochan, who is a founding engineer at CurB. The workshop is brought to you by Metricsdao. Metricsdao is a web free analytics community. We have thousands of analysts that are actively completing a wide range of analytics challenges we have available for our community, as well as going through the different educational programs to learn how to become web free data analysts. So let's dive right in, and if you have any questions during the workshop today, do drop those questions in the chat and we will get back to them over the course of this workshop.
00:01:13.026 - 00:01:15.390, Speaker A: So Mochin, over to you.
00:01:15.970 - 00:02:06.550, Speaker B: Thank you so much for the introduction. Hello everyone. I am Mochin, a founding engineer here at Playgrounds, and I'm excited to show you a bit more about subgrounds. Hopefully you have seen some of the materials from last time with Stouffer as we worked on curve governance, and today we'll be doing a bit of recap, but then transitioning into looking at some of the curve liquidity pools. I hope today is helpful in your data analytics journey. As a fun fact, I actually got involved with playgrounds analytics from the metrics community, which is really interesting. So I was able to grow as a data analyst here and start working on subgrounds, the library we'll be going to today.
00:02:06.550 - 00:02:54.534, Speaker B: So at any point in today, if you want to throw any questions in chat, I think a couple of my team members will be perusing them and I'll try to get to them as some of the queries are running. But for now, let's jump right in. So today we'll start with a quick recap of what Stouffer was able to showcase last time with the curve governance. But I hope to kind of go in a bit more in depth with curve finance, specifically the liquidity pools. Curve is one of the most notorious verticals in this space, as we kind of talked about last time. And the listed curve wars was one of the more interesting events that came out of the first two DeFi summers. If you're not too familiar with how curve finance works, that's all right.
00:02:54.534 - 00:03:57.060, Speaker B: We'll try to go over some of the details in my slides, and you can always ask questions in chat if you like to. More. More importantly, I really hope to go over some of my data mythology for how I approach some of the problems with understanding and perusing the data that I'm gathering. I think oftentimes we're really excited to see all this data out there, but we might know, we might not know exactly how to start as we're looking through the data and trying to figure out what questions we can ask. Part of my strategy for understanding a data set is to try to tie it down into a series of steps so that I have kind of a methodology that I can go through step by step, kind of like a process similar to like the scientific method or something like that. As a recap for who we are. We're playgrounds, a data solutions company, and we're building as many tools to make blockchain analytics using subgraphs as easy and reliable as possible.
00:03:57.060 - 00:05:25.660, Speaker B: So hopefully stick around for the entire hour and let's jump in. So first, as a recap for last time, we were looking at curve governance. Some of the questions we were asking were what were the participation rates with governments? Did we have a lot of individual analysts or individual participants participating in the governance with the proposals and the gauges that they're passing? Or was it just the founders and various protocols that have a large portion of v curve in terms of participation? Which specific proposals were they? Did a lot of proposals pass through with, you know, little to no friction or were there a lot of, you know, controversials? And can we actually measure the controversy, controversy of a proposal? So there's a lot of interesting questions with governance. And I, you know, we went through this last time and actually our workshop from last time is available on our GitHub. I'll try to showcase that towards the end as a, the link to that again. But yeah, we actually got to learn a lot of really cool things from the. I'm unable to switch my screens here.
00:05:25.660 - 00:06:27.110, Speaker B: Let's see. Well, I'll try to, try to resolve that so I can show it later. It seems like I'm unable to switch my screens. I might need a bit of help with that, but we'll go ahead and move forward. So last time we were, as we were looking at some of the curve governance, we actually noticed some interesting things that actually relate to maybe what we've seen on Twitter and some of the discourse with curve wars. For example, early on in the curve history, Vcurve was actually locked to only individual participants until a whitelist feature was added that allowed protocols to get involved. What we found as we looked through the governance data was that the most controversial and the most interacted with proposals were the one directly related to either the whitelisting of protocols or specifically related to a protocols token.
00:06:27.110 - 00:07:37.240, Speaker B: This included an example was the USD token, as this was a very prolific moment in curve history, and the proposal relating to UST and the gauge of one of the USC pools was the most controversial, with it barely failing by like 0.5%. So it's very interesting that as we look through the the data of curve, it seems like we can actually relate it to a lot of the events that we see on Twitter. This week we'll be looking at the liquidity of curve pools and trying to compare and contrast it with the launch of other protocols getting involved, such as convex. As a recap, we can kind of dive into the curve wars. So if you're unfamiliar with how curve works, curve is a AMM Dex, which allows users and other protocols to trade assets against each other. The unique thing about curve is it is ultra defined for pegged assets. So think stablecoin pools, where if both assets are around the same value, the slippage you get from making such trades would be very efficient.
00:07:37.240 - 00:09:22.140, Speaker B: This was very favorable for a lot of people defi as slippage and liquidity operation was a bit tricky process to be able to nail down. And curve had an interesting V curve locking system that allowed liquidity to be incentivized and boosted based on how much of their native token was locked. The catch was, if you were to lock this token for four years, you would get the maximum amount of power to be able to incentivize those pools, but you also wouldn't be able to sell the curve that you were walking or maybe getting incentivized with or boosted with. This created a sort of viable effect and ended up causing the phenomenon called the curve wars. Since originally, well, originally Vcurve was only available to normal wallets, as we heard from before, an early on proposal was passed that allowed complete protocols to be able to interact with the V curve voting system. What we'll discover is that convex was a later protocol that was introduced that actually was able to leverage this V curve system to the point of being able to airdrop their native token to anyone who whitelisted their protocol, and we'll see that a bit. In terms of data mythology, the way that we approach a dataset actually has a lot of roots with methods such as the scientific method, and in terms of how we interface with this data compared to what we see on Twitter or colloquially, as we talk with other data analysts, we might try to make connections with the data that we're perusing through with also our experience, maybe with using the protocol or maybe watching some of the drama unfold on Twitter.
00:09:22.140 - 00:11:10.290, Speaker B: So how exactly can we go from some intuitions for how a protocol such as curve operates and, you know, going directly to hypothesis and testing this hypothesis? So, in my personal experience, when, as I was watching some of the curve wars unfold on Twitter, I saw a lot of claims being thrown, a lot thrown around, especially when we had the USC event around last year, may 12 ish, there was a lot of talk on stable coins and how the liquidity of stable coins is one of the more important aspects in crypto. But as we were talking about curve wars, a lot of the focus was on airdrops and providing and where, you know, if we were a curve holder or a convex holder, we would essentially try to farm as many of these airdrops as possible. Possible. But how do these wars actually affect where the liquidity end up at the core? If we have all these protocols fighting to acquire this curve token, including fracs, butterfly, and convex, what happened with the underlying liquidity, which is what we really care about? As I was coming up with these ideas and data points, I was wondering how exactly I could. How exactly could look through the data and understand the system enough to be able to answer these questions. What fields do we look at? What top level entities are we looking through? What. What exactly is the shape of the data? And where exactly should we be looking? Should we be looking in curve or convex? These questions end up leading me to various tendencies for how I interact and adjust the data.
00:11:10.290 - 00:12:16.850, Speaker B: I find myself creating graphs and charts so that I can quickly understand from a high level what the data generally looks like. My goal is to construct a hypothesis, which I can test with more depth. Hypothesis such as the introduction to convex cause liquidity to get centralized in the top five pools, making it a much more centralized position. This hypothesis may or may not be correct, you know, I am not sure, but what it allows me to do is ask more targeted and directed questions for me to uncover. Then, once I either accept or reject this hypothesis, or if you're rooted in science, you might say null hypothesis, the opposite of this hypothesis, then I can unlock further questions to kind of further my reasoning. These sort of steps for my data mythology helped me be a bit structured. While there is a lot of data in crypto, it is very easy to get lost and end up not with much analysis at all, or with analysis that is loosely strung together.
00:12:16.850 - 00:12:56.830, Speaker B: While this may not be the only way to approach data problems in this space, this is one way that's kind of rooted in the core of data science that can really help you unlock the next level of becoming a data analyst in this space. So further ado, let's hop in. Before I continue, are there any other questions with some of the points I went over in the slides, especially with the data mythology? Hopefully everyone can see.
00:13:00.290 - 00:13:01.470, Speaker A: Yeah, it's okay.
00:13:02.690 - 00:13:49.260, Speaker B: Yep, it's good. Thank you. So as a quick showcase, since I missed this, this was some of the data from last week. You can see we were looking at some of the high level points, but we got to do some really fun things like look at controversy and even figure out, you know, which proposals based on high controversy and their vote ratio were to pass. And we can see our friend USD here, which no longer exists since their stablecoin was not successful. Also, for this demo, I will be using our playgrounds gateway in the background. If you're interested in using our gateway, you should join our discord and we'll have will reach out to help you get on our beta.
00:13:49.260 - 00:14:41.520, Speaker B: This is helping me access the decentralized network with ease. So the two subgraphs will be going through are the curve and convex subgraphs. So I'll go and paste our links in here. So since we are using the decentralized network, I do have specific subgraph ids here and our proxy just allows us to use these APIs directly. Since I have the key loaded as an environment, grab a SG first and flip that in. So to start in my data mythology, let's do some data exploration to start with. So we can jump into the the playground from the graph ecosystem.
00:14:41.520 - 00:15:43.430, Speaker B: There we go. And this is the curve finance Ethereum subgraph from Masarity. And here we can mess around with some of the entities here. So from a high level we have a couple of interesting entities, such as the snapshots of the liquidity pool. We also have high level liquidity pool here and the finances of the curve subgraph as a whole. One thing that's really nice about the Missouri subgraphs is that they're standardized across all dexs. So if I'm looking at a different Dex, such as Uniswap, or maybe on a different chain, maybe looking at Trader Joe's, I'm able to compare and contrast these the same way, even if internally they work differently.
00:15:43.430 - 00:16:39.070, Speaker B: So looking at the liquidity pools entity, we can see, we can actually look at different statistics such as vertical side revenue, supply side revenue, total revenue, cumulative revenue, and so forth. So the two that I'm most interested in with are looking at the total value locked and we can also take a look at cumulative revenue or volume USD. So the playground kind of gives us an opportunity to look at some of the highest earning pools or the highest volume pools in this space. So to do that, we can order by cumulative volume and go for decreasing. And we can see here, we might see some recognizable pools here. So for example, curve Phi, and we have die USDC USDT. This is the infamous three pool, or three curve pool that many assets bond to.
00:16:39.070 - 00:17:44.010, Speaker B: And we can see, while these numbers may not look as usable in this interface, when we move over to subgrants, we can see how we can make this a bit more workable. But even from this kind of raw form, you can see that this pool is very popular, has a lot of value locked in, and has a lot of cumulative volume. We also see some other friends, such as USC BTC and ETH. If you scroll out a bit more, we see the newer and more popular Eth staked eth pool. Sorry, trying to change my screen here, but I'm going to share my whole window so I can just hop around. That might be a bit easier. Sorry about that.
00:17:44.010 - 00:18:34.230, Speaker B: Right, so should be good. So you can see here we have total value locked and cumulative volume. You'll see. So we can go ahead and start pulling that into here. So maybe we can try getting the first 20 polls you can order by our cumulative volume, and we'll go by order direction. Oops. And let's just go ahead and query this some stats for entity.
00:18:34.230 - 00:19:29.090, Speaker B: We'll want our entity dot name entity dot id, and then the other things we'll want to volume and total value blocked. Oops, did I spell everything right? Aha. Liquidity pools. We have the same data in here, except the numbers might be a bit nicer. Looks like they're still pretty large. Oh, and we'll. Yes, so it's interesting here is, since we went by volume three, the three curve pool di Usc Usct is higher than the eth staked eth pool.
00:19:29.090 - 00:20:43.110, Speaker B: However, if we were to switch to total value locked, we can see that we actually have a flip here. We see some new friends, we see our fracs USDC pool. But the eTH staked eTH pool is actually higher than the three curve pool, which is the most prolific pool in the space. So now this is already getting me thinking a little bit of why is a pool having so much liquidity when it doesn't actually reach that much volume? With staked it specifically, only just recently were we able to actually unstake stake to ETH into ETH and even then, currently there is a queue for unstaking. So you might think that because you can't go from staked EtH natively due to the queuing and the limitations placed on the Ethereum protocol or I Ethereum blockchain itself, then wouldn't this pool be receiving more action? Or perhaps people won't mostly want to hold staked eth. The other flip side of this is we have a three curve pool which gets the most volume in the space, but doesn't have the most total locked in value. So these are some interesting ideas I'm kind of thinking about as I'm going through these pools.
00:20:43.110 - 00:21:38.000, Speaker B: The next entity I want to look at are the liquidity. Where is it? The daily snapshots. Now this one is very interesting because I will start getting data, historical data, from the beginning of when curve started to, you know, today. This allows me to kind of analyze the change, excuse me, the change in the equity pools and which pools are incentivized over time. So start with getting liquidity pulled snapshots. We can first figure out what snapshots we would like to get. I'm actually going to look for the first thousand so we can try to get 1000 days worth of data.
00:21:38.000 - 00:22:59.162, Speaker B: Next we want to order by timestamp and we want to order direction decreasing. Another thing we want to do with the daily snapshot is we actually would rather use date time instead of timestamp since date time is a bit more ergonomic to use in python. So as we kind of went through last time, you can grab our synthetic synthetic field. I think I spelled that correctly. And we'll grab our timestamp. And to start, let's just quickly plot this or just print out some data just to see what we're working with here. So we'll save our query entity as a variables so we can quickly reference it, and we'll grab our date time over the pool name.
00:22:59.162 - 00:24:13.760, Speaker B: Oops. And let's grab the actual values we look at, which is total value locked USD and one is daily volume USD. So we can see here we actually pulled in the top thousand days. However, because there's multiple pools in liquidity snapshot, we actually got data back to 2022. So this might be something you run into when you're using subgrounds or generate the graph as well. And you might be wondering, you know, what is the best way to kind of pull in some of the pools? You know, there are a lot of pools on curve, especially since factory pools can be deployed permission permissionlessly. So there might be a lot of noise in your snapshots, but as we're kind of doing our analysis, we really only care about, you know, the top 20 pulls per se.
00:24:13.760 - 00:25:02.200, Speaker B: So a really cool way to structure code in python. And one of the real big benefits of using something like subgrounds to go through this data is that we can actually link our queries together. So earlier we were looking at the top 20 pools by total value locked, and we just query DF'd it. But what we can also do is directly use our query function, which will return the data directly. So in our query, we're looking at the top 20 pools by total value locked. We'll go and start with ten, so it's a bit smaller. And if we query this here, we get a direct list of ids.
00:25:02.200 - 00:27:01.220, Speaker B: The really nice thing about that is we can actually use that in our where clause here, you do this method. So if I grab our top entity, we'll do pull, pull in, and we can actually multiply this by the number of pools we're looking at so that we get all the data while we do wait for queries to run. If there are any questions, feel free to throw them in chat. So we see here now we actually get all our data from the beginning of at least when the subgraph considers the beginning curve, which is 20 2010 October 30. But we do have all the data, and with these pools, they seem like the more popular pools in terms of curve finance. So some pools and curves, since anybody can deploy them there, might be just filled with dust or just small amount of material, there might even be some scams on there. But if we look at only the top pools by total value locked, since likely people are only putting their money where the pools are legit, then we can actually grab like 12,000 rows of data without worrying about what valid pools there are.
00:27:01.220 - 00:28:20.450, Speaker B: Let's go ahead and save this as pools snapshots. So as we're kind of doing this again, we can start going through some of this data. So my first instinct is maybe to do a scatterplot and we'll go ahead and use plotly for this and we'll just quickly go through snapshots once that saves and look at time y is equal to pull the value locked. And we might want some color in there that might be helpful with a poll name. I think about bot Li is we can just turn everything off since there is a lot of noise and noise in here, but we can definitely clear, pardon me. We can clearly see when Defi summer was in here which is kind of nice to see. And even this cliff right here, if we zoom in around this time, you can see there's a big cliff here.
00:28:20.450 - 00:28:58.030, Speaker B: I actually forgot about this, but this is May 12. This was the day that terra Luna blew up. So it's nice that we actually see that correlation in the data. It seemed like curve as a protocol actually was hit hard by this since a lot of money was flowing through it. But that also meant a lot of people were drawing since they were a bit scared. Or in the case of some of the USD T pools, they wanted to make sure they weren't exposed to it. If I were to just go over the two pools, which I think are the most interesting, we have the three curve pool and the EtH pool.
00:28:58.030 - 00:29:46.948, Speaker B: Right now I'm plotting total value locked and we can see they do have similar curves. But it is something to note that since TVLs and USD, our data actually is a bit biased here. It's a bit too correlated with the price of assets. Specifically, the staked ETH pool is going to be pretty heavily correlated with the price of ETH at that time. While the three curve pool is kind of just does correlate since a lot of tokens are paired with recurve. But the price of the total value locked will always stay static, theoretically, as long as it stays at $1. So one way we can kind of fix this is by pulling in more data, but maybe we choose a different entity.
00:29:46.948 - 00:30:24.210, Speaker B: So the entity I like to chose with this was financial data snapshots. I see that question. I'll try to hit it once I start querying some data. Financials, daily snapshots kind of gives me the greater finances of the protocol, and I can use that to actually match with the liquidity pool data. So we'll want to daily volume and TVL, but we can go ahead and do that directly in our subgrounds code. It's actually very similar to this. So all I to do is find financials.
00:30:24.210 - 00:31:25.988, Speaker B: I can spell financial. And since we're just looking at the protocol as a whole, we don't need to care about which pools are going on, but we do want our date time conversion. This seems to be pretty handy in terms of data. We mostly only care about total value locked, daily volume, USD, and we'll name this something else. Initials financial snapshots. So good. And pull that data in, Amir says.
00:31:25.988 - 00:32:14.338, Speaker B: Can we add our data source to it for migration? For example, add fuzzy crazy data or games data to curve data, and you migrate together for auto detect all the decision market trader with bloatfish trade. Yes. So another benefit of being in a python environment such as subgrounds is that with pandas you can just read in any CSV you want. You can do read CSV and you'll be able to look at any file source here. However your CSV is organized, if your data is available on a different API, you can always put that in as well. That's one of my favorite things about being in Python for this stuff, especially as a Python nerd and a data science nerd, I feel like I have a lot of control in the data I work with. So here we got some of our finance snapshots here.
00:32:14.338 - 00:33:13.900, Speaker B: It's interesting, it's called liquidity pull data snapshots here. Oh, because I pulled in snapshots makes sense. So another feature in subgrounds that I find is pretty useful specifically for the query DF is we can actually rename some of the columns here so it's a bit easier to follow. So I'll actually call this total and we'll do total volume USD. Go and do the same earlier here for consistency. So we'll do date time, pool name, pool volume USD and pull DVL USD and looks like I switched these so it won't sound so good. All that's running, we can go ahead and continue.
00:33:13.900 - 00:34:11.860, Speaker B: So we see here we have total value locked USD, daily volume USD, and we can divide this by each pool as we merge the data frames together. So once you have your data in a panda data frame, it's really handy to be able to use normal panda operations. So the merge operations will help us merge on the date time column. So here we have 943 rows. Here we have 11,000 rows. Basically there's ten rows per, ten rows per day in this one, and this is one row per day. So if we merge on the, on old snapshots and then we merge on intra snapshots and we're okay, having all this data will be explicit.
00:34:11.860 - 00:34:54.917, Speaker B: This is not pull snapshots, full snapshots. Okay, we want to merge on date time and how is left. This just tells me where to put the nulls. Oh, that came out pretty fast. Total Tbl USDA. Ah yes, this was another thing I remembered. So one thing that's interesting about this is that the snapshots that we get from the pools, you see, we have exact date times.
00:34:54.917 - 00:35:14.622, Speaker B: They actually aren't the same date. They aren't taken at the same time every time. So even between the current, the three curve pool and the frac usc pool. We have like three minutes difference. This is one of the more annoying things that we might deal with. Just messy data. But we actually do have a solution.
00:35:14.622 - 00:36:16.430, Speaker B: I'm just going to grab this from my templates. We can normalize these values here. So one thing that's useful with pandas is they have a normalized function which helps remove the time aspect of the date. After we normalize the data and then merge them, then we actually get all the data merged in together. Lastly, the whole point of us merging, let's just do some operations. So if we take a pool total, we'll call this pool DVL percent, and we'll do TVL USD divided by thank you, autocomplete Total TvL USD. And we'll want to multiply this by 100.
00:36:16.430 - 00:37:08.740, Speaker B: And let's do total TVL or, sorry, pool volume USDA. And now we have some percentage points for the TVL and USD, or, yeah, I guess we don't even care about USD right now. This helps us normalize a lot of USD. So we were removing the USD aspect from studying the pool and the volume. Sorry, the TVL and the volume. And now we can now graph this. So we'll be using merged, and now we can just use normal date time.
00:37:08.740 - 00:37:46.260, Speaker B: And let's look at TVL percent. Easier names to use. TVL. I did not write that correctly. Now, this graph is still a bit messy, but it is normalized since we only have percentages listed. So for sake of ease or. Yeah, we can not just pull this a three curve and this one is eth.
00:37:46.260 - 00:38:49.580, Speaker B: So you can see here is as we graph TvL percentage over time, three curve actually started with the higher TVL. Especially, you know, it came out before, you know, Lido came out with staked ETh. But at some point, the eth pull over past the percentage of TVL on the entire platform. So since we're comparing this pool as a percentage of the entire platform, we have an equal playing field here, which is actually also normalized for the entire market. So no matter what the market conditions are, bear market, bull market, lido is actually 40% of the TVL, while three curve, which is one of the most popular pools, is only 10%. If we were to plot volume percent. And let's get rid of some of the noise here.
00:38:49.580 - 00:39:42.100, Speaker B: This is still quite noisy, but it should give you a bit of a take of what the volume looks like. We can see here that, you know, around May 12, we had the lowest times other than like the actual May 12 when everyone was using the pools. But in general, three curve gets nearly 90% at the peak of all volume, while Ethan steeth, it seems like, is used less than 20% of the time. What's going on here? Why would there be so much liquidity in a pool if no one's using that pool? Well, it has to do with rewards. It seems like there is a lot of incentives to store ETH and staked ETH. Whether you are just getting curve rewards, you might be getting lido rewards. You might even be borrowing and leveraging against it.
00:39:42.100 - 00:40:30.600, Speaker B: This is a very interesting thing. Since this is an opposite expectation. It is very rare to or not rare, but it's very interesting. The most interesting part about looking at data is doing some exploratory analysis and actually seeing a situation that is the opposite of your expectation, because something that is interesting has these opposites, has something that you don't expect, and that allows you to understand why it's happening or what is going on. That is the best part and is what you should be looking at as a data analyst. So I'll say, are there any other questions? Unfortunately, I'm going to leak some of my notes here, but that's okay. And I'll give everyone a breather real quick.
00:40:30.600 - 00:41:30.090, Speaker B: Seems like I have no question, so I'll go ahead and move forward. So it seems like there is something to investigate here. And so far, throughout our experimentation here, we started with an inkling of an idea that, you know, curve is one of the most widely used protocols in the space. There is a big war being based on who's accumulating curve, who's acquiring it, and who's using it to incentivize these pools. And we can see there is a non standard approach to these pools. The volume of a pool is not correlated with the total value locked, which is perhaps the inverse. And if we were to compare this to a protocol like Uniswap, it might even be the opposite of Uniswap.
00:41:30.090 - 00:42:47.766, Speaker B: So what's going on here? Why is three curve getting so much more volume? Or, to be honest, why is staked eth not getting as much volume as three curve? Feel free to throw it in the chat. What your thought process is. How do you think most people are using steak deep beneath? Are there any questions that you can ask that can help you incentivize further exploration? Do you have any hypothesis that you want to test? This is where you really want to drill in deep and answer these data questions. This is where I'm going to bring in the convex protocol. So convex was a protocol that was started April 2021, I think, or 2022, sorry. And in the whole ecosystem of how curve was kind of profligated in the 2021 DeFi, the first DeFi summer, convex came out and essentially bribed everyone through a nice airdrop to whitelist them into this curve ecosystem and then incentivize so much curve into convexitive. Then our convex owns more than 51% of all curve, which is kind of crazy to think about that.
00:42:47.766 - 00:43:56.240, Speaker B: The way that we interact with the curve voting system is not even through convex, but through voting, which is built on top of convex. So we have our convex subgraph and we can go through convex, so we can take a look on convex. So I think the first way to start with how convex is as an ecosystem is by studying their financials. So you see, I have a query loaded up here, and if we take a look at their daily snapshots, you can see daily revenue, personal side revenue, supply side revenue. The one I'm most interested are the total value locked? And volume, do they have volume here? Forgot. We'll do cumulative total revenue. Since this is a vault, we can grab query dot in short daily snapshots.
00:43:56.240 - 00:44:52.900, Speaker B: And we can do the same trick we did earlier with financial state snapshot. This synthetic field helper is one of the most useful things. I use this nearly every day and let's just grab some data. Maybe we can even use some of our poly wrappers to kind of get a greater insight. We'll do name or probably don't name. We need total value locked daily. Total revenue.
00:44:52.900 - 00:45:39.550, Speaker B: Start with that. Oh, and we need. So it seems like we do have some good numbers in here. We actually might want more data. Let's try to get 1000 days worth of data and what's sort this by and decreasing. Now we have it sorted. And since I cannot just read data and understand what's going on, let's actually use some of our plotly wrappers, which Stouffer was showing last time.
00:45:39.550 - 00:46:41.370, Speaker B: What I like about these is they're extremely quick for me being able to explore data very quickly, since I can just use the yield paths directly. Let's start with total value locked this time. It's all right. Yeah, I think that's it. Oh, and let me make sure I show that. Yeah, we can add some access stuff, but that's okay for now. We have on the left total value locked.
00:46:41.370 - 00:47:28.986, Speaker B: We have overtime and you know, we can see some of our favorite events. We have the May 12 USD death event. You can see Defi summary here lately. It seems like it's been tailing off, but perhaps that's because something else is going on. And if we take a look on, you know, these other graphs, it does follow a similar pattern. But again, because we have TVL USD, it actually is in the unit of USD, which might not be helpful for us since we're kind of based on the entire crypto market. It's unfortunate that we don't have direct, just total value in the tokens, just to understand how the tokens increase.
00:47:28.986 - 00:49:20.990, Speaker B: But that's all right. It might be helpful for us to overlay this graph on this graph for us to understand the relationship of the growth of convex with the growth or the TVL of some of our favorite pools. So we can also do that in our figure and we'll see here, we'll do what they bring in merged. We'll want our is this full snapshots? And I think we'll want color for this. Oh right, because we need to rename this simply snapshots. Right. This was also questions.
00:49:20.990 - 00:50:48.240, Speaker B: One thing I recommend as you're working with subgrounds or a lot of data work, is to save your data in a CSV as you're working with it, especially as you're figuring out what exact fields you want to work with or you just want to understand the shape of the data. It is very helpful to have a CSV, even if it's just ten days worth of data, just so you can have that shape to work with. It seems like I hit an error here, maybe I should go. But I see that we're nearing the end. It seems like we have ten minutes left, so I don't want to go over while this is running. I want to show some of the other resources we have at playgrounds as you're exploring your data. So the graph explorer in the playground is one of the more helpful things we have when we're exploring some of these field paths and entities.
00:50:48.240 - 00:51:43.470, Speaker B: I also want to take a look at our docs as you're going through and running subgrounds. We have a lot of information on how to work with some examples and techniques that we're showing off in both the last workshop and this workshop. So in both these workshop we are using our potlay wrappers here, which help us kind of get access to data a lot quicker. If you're wanting to get started and you're seeing subgrounds for the first time today, welcome. We have a really nice getting started guide for jumping right in all the way to even being able to run code in our docs itself so you can get started with subgrounds immediately. We also are trying to fill in more advanced topics. One part about subgrounds that is really nice is we have a pagination mechanism which allows you to grab a lot of historical data at the same time.
00:51:43.470 - 00:52:40.064, Speaker B: Like a bit earlier here we grabbed 11,000 rows of data with no sweat, and behind the scenes subgrounds probably made at least 1015 queries to get this data in one go. So if you're curious on how that works, you can go to pagination. We also have a lot of examples here which will hopefully get your mind starting on what sort of data questions you can answer. An example for non sub graph source. Yes, that is, I think it. Yes, you can load in a vanilla graphql API using the API URL directly. Yeah, I will say with loading raw vanilla APIs you will not benefit for some of our syntax juice fancy features such as pagination, but you will be able to merge your data like normal.
00:52:40.064 - 00:54:09.768, Speaker B: And if you have an external data source like a CSV or something you want to merge in, perhaps you have some off chain data you want to merge in with some of the subgraph data that works as well. It seems like I'm getting the color wrong here. My last example is a bit broken here, but that's okay. See, I'll go and skip to one of the last things I wanted to show just so we can kind of wrap up here. One thing I'll also mention is bottle has a lot of different types of plotting mechanisms. So even further, if I were to highlight some of the intelligence that come here, I really recommend you head to the poly website and see all the different charts you can make. Some charts are more helpful than others, and as you're working with data, you might find different insights by using a different chart type.
00:54:09.768 - 00:55:01.232, Speaker B: So for example, histogram here. It's quite interesting as well, especially if we start zooming in towards the end and turn on some of the pools we can see. Some of it might be a bit easier to see in this way, since this buckets weeks at a time. Last thing is, there is a very interesting feature in which allows us to do animated plots. So if you've ever seen those cool bar charts, bar charts on like this beautiful subreddit, or just on Twitter in general, we can actually do that with plotly itself. So here I'm bringing it back to timestamps since it's a bit easier for the animated scatterplot to go through it. But I'm going through the daily volume, USDA and the TVL.
00:55:01.232 - 00:55:36.042, Speaker B: USD percent, the value. Oh, right. Because these column names are different. Let's see. So we have this one and this one. So we'll do TVL here. And what we're actually marking here is, you know, across time, how does volume and t the percent of volume? Oh, we should do volume.
00:55:36.042 - 00:56:35.780, Speaker B: Percent, actually. Now I think about it, how does volume and tv all change over time? So we can kind of see each pool as a dot and we can actually animate. So this is actually going backwards. So it's starting with the newest state. If I clean things up here, just look at ETH and steeth, we can see, we see kind of ethereum and staked Ethereum hovering around, you know, 20% to 40% on TVL, but only 20% of the volume. Well, this is nearly the inverse. But as we go back in time, there is a lot of movement that's happening here.
00:56:35.780 - 00:57:13.050, Speaker B: If we pull in a lot of them, we can even see this is the tri crypto, I think the tricrypto two pull. That seems like it gets a lot of volume. And then we have a lot of friends down here. Oops, let me reset. Awesome. This is the fracs pool. If we go before May 12, we might even see a USD pool in here.
00:57:13.050 - 00:57:57.514, Speaker B: Here's the magic money. One we won't see in here because we actually, in the very beginning, we filtered it out by going by TVL. If you go by cumulative volume, we actually would see pools like UST and USC. Three pool. See? Are there any questions here? Should be good. So it seems like I have three minutes, two minutes left, but I'll go ahead and wrap up here so we can sign off. I appreciate everyone for tuning in for our workshop too.
00:57:57.514 - 00:58:52.450, Speaker B: And hopefully someone can throw the discord and chat along with other resources we have here at playgrounds. Thank you for joining us. And yeah, if you are interested to learn more, please, please, please join our discord and start chatting. I got involved with doing data science and data analytics and crypto by saying hi and being really nerdy with Python, just trying to soup it up. But after a year, I have kind of transformed into a data analyst and I'm, you know, really enjoying working with Python every day and data science. So I implore you, if you're interested in getting started, if you're new to Python, if you're new to subgrounds, don't be shy. We try to, you know, invite everyone to come into our discord and say hi at the least, and maybe even try doing some problem solving.
00:58:52.450 - 00:59:19.100, Speaker B: All of our workshops are available in our GitHub. I think I have it here. We have our governance from last week is here, and I'll be uploading today's workshop in here as well. If there's any last questions, feel free to throw them in the chat. I'll try to stick around for a bit. Also, you can head to our discord and just say hi to me there.
00:59:25.050 - 00:59:52.058, Speaker A: Okay? Amazing. Thank you so much moten, for your excellent workshop. And thank you everyone for attending. The links to the discord server and the website are in the YouTube description. So thank you everyone for attending attending and we'll see you in future workshops.
00:59:52.154 - 00:59:55.930, Speaker B: Bye bye. Thanks everyone. Bye.
