00:00:00.570 - 00:00:48.380, Speaker A: Okay, so, hello, I am Luke Zarko from Google, and I'm here to talk to you about indexing large mixed language code bases. So I do this at Google under the Kithe project, and this project aims to establish open data formats and protocols for interoperable developer tools. What does that actually mean to people? Well, if you've heard about an internal project we have called Grok, this is basically open sourcing a version of Grok. So with the most important of the proper nouns out of the way, I can give you my outline. First, I'll begin with an introduction to the kinds of problems we're aiming to solve with this project. Then I'll give you an example of a system that we use to solve those problems, talk about how we get support for c plus plus using the client front end, and then discuss some future work. So everyone's probably been in this situation.
00:00:48.380 - 00:01:54.798, Speaker A: You have a bunch of code in some language with some feature x, whether x is the language is c, or maybe the language is like c plus plus, and you'd like to do something that goes beyond just compiling code written in that language. Maybe you want to do some kind of analysis on it, so you can write an analysis tool that works with one of these mostly compatible to c languages. Maybe you want to generate cross references, so you want to be able to open a web browser and click on a keyword or some identifier and find where that was defined, if such a thing even makes sense. Maybe you can do this for any language supported by clang you use USRs, for example. Maybe you can generate documentation from any language that has curly braces. You have a sufficiently lenient parser, right? Or maybe you're just writing a code review tool or some full text code search tool, and you can support any program that's in plain text. The problem is that you're really kind of doing this in a very ad hoc way, right? If you run into a language like squeak small talk, which is image based, then you suddenly have more work to do.
00:01:54.798 - 00:02:26.214, Speaker A: You hadn't predicted that. You thought, right, that all of your programs would be in plain text. You'd be able to get to the source code in an easy way. But for some languages, this is impossible. For some languages, you have to go through some weird special process, and you're constantly having to make new special cases. Furthermore, the programming language that you use to develop software isn't the only tool that you use to develop software, right? You have source code generators. You have things like protocol buffers or thrift or cap and proto.
00:02:26.214 - 00:03:48.310, Speaker A: Maybe you use parser generators like yak or Antler. Or maybe you use JNI to connect Java code to C code. All right, do your tools understand all of those things? What do you have to do to your tools to make them understand a new source code generator? What about your build system? Right? Do you use cmake, gmake, or omake? Are you in Java land or are you using Maven? Do you connect your project together with a bunch of shell scripts? Are you using ant? Okay, and do your tools understand these choices? Where is your source code located? Can your tools get to the source code? Do they need to? Are you storing things in git, in subversion, in CVS? On your company's filer? On your local disk? Does your build system allow you to go to some web server every time you compile your project and pull down some code from there? Do your tools understand this? Usually the answer is no. Usually the answer is you have to go back and make some changes or develop new tools, or make some new special cases. And in our experience at Google, we've had to do this, unless we step back and think about unifying principles. What we advocate is the development of a common interchange format, which we'll call the Kive schema or the kive data format. With this format, you implement support once for each tool in your ecosystem.
00:03:48.310 - 00:04:29.378, Speaker A: These all emit some approximation of the full semantic meaning of the source program or what the tool does. So it's not a complete replacement for the front end. We're not trying to replace the compiler, we're trying to get just enough information out that we're able to do interesting things without knowing operi, which languages or which tools are in your system. So all of these front ends, we say, produce this common interchange format, and all of your tools consume the common interchange format. So we've changed the problem from I use languages with feature x and want to do y to. I use tools that support this kaive data format. My build systems support it.
00:04:29.378 - 00:05:06.666, Speaker A: My language front ends do my other tools, do my protocol buffer generators, do my documentation generators understand the data, so do my editor tools, so do my cross reference servers. This is the essence of the Kaith project. We give you a specification for a base idea of what the interchange format should look like. It's extensible. You can define your own extensions to this and give them to others. We'll give you some tools that understand how to generate and consume this data, and altogether everything works well. Okay, so that's the problem we're trying to solve.
00:05:06.666 - 00:05:49.546, Speaker A: We're trying to do interesting things that allow people to get a better understanding of their code. How do we accomplish this, and how do we scale it? Let's imagine the following scenario. We have a cmake based project, like maybe LLVM, and we want to browse it using a web browser. How do we connect these two boxes? I'll go over the tools that we use in our system. First is the extractor. The extractor is built on the existing support that was developed for LLVM for generating a compilation database from the cmake tool. This is built as a clang tool, as a preprocessor frontend action that produces this hermetic build data.
00:05:49.546 - 00:06:43.914, Speaker A: What is that and how is it different from the compilation database? The unit of Hermetic build data is a compilation unit. This contains every dependency that a front end might need for doing semantic analysis. It contains the full text of the header files involved, of the source files involved, and the environment that the compiler used to build the software. It also gives files identifiers that can be used to locate them back in the original repositories from once they came. This is really important because it means that after you get your compilation units, your tools no longer need to understand how to get to your source repositories. Furthermore, because all of the necessary resources are packaged together, these are really easy to pass around. In fact, we found that it's useful to use these for doing other kinds of static analyses that go beyond our simple cross reference generation.
00:06:43.914 - 00:07:05.410, Speaker A: So our tool isn't the only tool that consumes these. At Google. Other teams have produced other tools that use the same infrastructure. All right, so we finished with cmake. We've produced a whole bunch of these compilation units. Now we want to start analyzing them. Now we want to start building structures we can use to do our cross referencing.
00:07:05.410 - 00:07:44.474, Speaker A: For this, we have a tool called an indexer, and an indexer uses the information in the compilation units to build a persistent graph. How does this work? First, just as a clang tool, right. As a manner of administration, we load this hermetic build data into memory using map virtual file. What this means is that no longer are the paths that clang uses to look at source code. More important, no longer do they have any structure beyond simple identifiers that reference into the hermetic data. Right. All they are are just identifiers for particular files.
00:07:44.474 - 00:08:51.350, Speaker A: And we know that clang won't try to reach outside that hermetic blob because we've already run the preprocessor on it in the first pass of the indexer. Recall that what we're trying to do is we're trying to summarize this data in some format, is we need to be able to assign good names to everything interesting in the program. Naming is a real common theme in what we're trying to do. Remember, one of the big deals with the extractor was that we were able to name each file in such a way that we could figure out where that file came from. In clang, we have to be able to give names to nameless declarations like anonymous structs and also shadowed variables. We want these names to be consistent such that if you rerun the Cfron or the indexer tool on the same source file, you should get the same names out so you can do later analysis on them. Unfortunately for us, clang emits edges in the ASD that are really useful for us to have, such as parent relationships, so that you know that a variable was in some particular block.
00:08:51.350 - 00:09:39.406, Speaker A: In clang you can say that a variable is in some particular decalscope, but that decalscope doesn't have a one to one correspondence with a block statement, for example. So we have to recover this information. Again, we want to be able to give these stable names to any declaration we see referenced at any point in the program. So it's not sufficient for us to just keep a context around as we run our recursive ast visitor through the program. We have to be able to say if we're here in the ast and we see a reference, maybe in a type to a node or a deckle that was made over here, we still need to be able to give a stable name to this guy over here. And we want to be able to distinguish with these shadow names again stably. So our solution is to build a map from AST nodes to tuples of parents and visitation indexes.
00:09:39.406 - 00:10:13.310, Speaker A: The visitation index is defined by the recursive ast visitors path through the tree. And there could be more than one of these things. And this is one of the reasons that Klein doesn't keep track of these parent relationships, because for things like template instantiations, they can appear many times in the resulting ast after elaboration is complete. So let's give a concrete example about what this means. Let's say we have this program and we want to be able to name each one of these instances of the variable x. They're not instances, I'm sorry, they're different variables x. And we have to maintain that distinction.
00:10:13.310 - 00:10:37.906, Speaker A: So after we build this map, we have the ability to give them proper names. We can name this one according to the path from foo. We name this one according to the path from foo through that first block. And it's just ordinal based on the visitation order. And then we name this one in the same way. Every time we run the indexer, we get the same names. And it doesn't matter which header files we've included.
00:10:37.906 - 00:11:27.030, Speaker A: For example, because these aren't counts of all of the nodes we've seen up to this point, the names terminate at the first name deckle. That's reachable from, let's say, outside the current context. All right, so we've kind of done some grunt work we've prepared for naming. Now what we'll do is we'll go through and we'll notify a new interface we've developed called graph observer about abstracted program relationships. Well, what does that mean? In kite? We abstract all programs to a graph made of nodes and edges. A node has a unique name that globally refers uniquely to that node. Uniquely very important nodes have facts associated with them.
00:11:27.030 - 00:11:47.840, Speaker A: This fact is called kith nodekind, and it's saying it's a record. So this from c might have come from a class definition. The format is extensible. This is a path name. You can define your own facts. And if you tell other people what your facts mean, they can use them in their own analyses. You can set them to whatever strings you'd like.
00:11:47.840 - 00:12:44.766, Speaker A: One interesting thing about rgraph is that we store information about semantic information, like the platonic idea of some class c in the same graph. As we store syntactic information about the utterance class c in a particular file. We store this as a special anchor node, similar to anchor in maybe a web page that has its own kind and that has its own unique name. You can draw relationships between any two nodes, even if they're between syntactic and semantic nodes. So here we'll say that when you utter class c, that actually is a definition of the class c, you get the reverse edge for free, right? A definition in some sense, you get the reverse edge for free. So you can issue queries against your graph. Like I have this class, where was it defined? Or I have this definition.
00:12:44.766 - 00:13:39.246, Speaker A: What semantic object is created there? I said that you can set these to whatever you want. And in some sense that's true. But if you want to participate in society, there are some rules. The rules are given in the Kaiv schema document. We provide a base set of nodes and edges, such as anchor nodes, so that people have a common vocabulary to work with. We provide some naming rules, and again, naming is very important for dubbing certain kinds of nodes. This is really helpful if, say, you have a protocol buffer, or if you have a JNI interface and you want to be able to search for the Java qualified name of a class that corresponds to some C plus plus qualified name of a class that are the same, you want that name to be uttered the same way so that when you search for it, you get both instances.
00:13:39.246 - 00:14:18.750, Speaker A: So these rules allow you to do that. The schema, again is extensible. You can use whichever node and edge kinds you want. Our overarching principle here is to be conservative in what you generate, but liberal in what you accept. And we've found that even internal to the company, this principle has been great for us to work with. It turns out that no matter how careful you are, some data might be missing. We're talking about C plus plus today, but this is really a system that's meant to consume and produce data from many languages and front ends, and some of these may just not be able to do the necessary analyses to produce static type information, like, say, a dynamic language.
00:14:18.750 - 00:14:47.702, Speaker A: There may be more data in the graph than you can understand. This is easy to demonstrate, because if you come up with your own name for a fact that someone else's tool doesn't know, that other tools should be able to tolerate that situation. And finally, someone might make a mistake, and this always happens, and you should be able to deal with it. We'll give you a document that describes the schema. We'll say, here are a bunch of relationships. Here are a bunch of nodes. Here's where they come up.
00:14:47.702 - 00:15:30.582, Speaker A: So we have this child of edge. And for the child of edge we'll say that a is child of b. If a is somehow dominated by b. We'll also give you checked examples in the languages that we support. So we'll give you this example in c plus plus, where we have an enum class named enum that defines an enumeration, and we have some enumerator etor that is defined in some sense by this token. And the relationship we want to look for is that the enumerator is a child of the enumeration. This describes a subgraph that we're interested in verifying that when we run the tool, we get so it looks like this.
00:15:30.582 - 00:16:12.900, Speaker A: We have two nodes in the graph. And remember, in the kithe graph we have representations for both syntax and semantics. We have one for the enumeration and one for the enumerator. We have these semantic nodes that represent the enumeration and the enumerator, and we have the edges that the test looks for. So at once we have an example that you can read and see what this relationship actually means in source code. And we also have a mechanism by which this is verified so that every time we rerun our test suite, we regenerate our documentation so it's always fresh. This is also how we test the rest of the system, but some of those cases are a little less nice to put up on the screen.
00:16:12.900 - 00:17:01.170, Speaker A: So I mentioned this graph observer thing. The graph observer interface sees just this abstract view of the program. It's notified of things like there is a class here with some particular name. So the graph observer is given the same names as the kythindexer. There's not to note a one to one mapping between ast nodes and program graph nodes. It's not like we just took Klang's ast and dumped it to a graph. We take Klang's ast and we think, what does this mean when we generalize it to something that all programming languages use? For instance, in Klein there is something called a class template, partial specialization deckle that you get when you have a partial specialization of a class template.
00:17:01.170 - 00:17:37.280, Speaker A: In our system. We separate this into two concerns. We separate it into an abstraction which is the class template that can bind new type variables and a record that that abstraction dominates. So in a partial specialization of a class template, you can bind new type variables that you can use later in a record that that thing covers. And you can mention those type variables. As long as you're underneath the abstraction, you can imagine what a template or a class template looks like in this system. It actually looks exactly the same.
00:17:37.280 - 00:18:25.998, Speaker A: All right, so we've run our clang tool to produce a subgraph of the giant graph that describes your entire project. We store all of these in the same graph database persistently. So you do this once for every time that you change your code base at large. And you can use this graph from your front end services to ask for, say, definition sites, or you can use it to generate your static documentation files. You can use it to power your code review tools or your other analyses. One thing to note is that we know that this design scales really well. A version of this runs or is the back end for the public chromium code search service that you can get to on the public web.
00:18:25.998 - 00:19:15.258, Speaker A: And this we consider to be a small data set. It's only about 22 600 c plus plus compilations, which is about 31gb of serving data internally. Code search is larger, 100 million lines of code, and this is in multiple languages, and they all work together. As I mentioned before, other internal tools make use of this build data, these compilation units for doing analysis. So it's not like we're only making all of these stored compilation units, using them once to build our graph and then dropping them on the floor. All right, so I've told you that we're trying to do things with programs that go beyond just compiling them. We do this by generating an approximation of the program, so we're not trying to compile them specifically.
00:19:15.258 - 00:19:59.206, Speaker A: We're trying to get information that is valid across different programming languages so we can make inferences across front ends. We do this using several clang tools, and we do this with a common interchange format defined by the clang schema. How has Klang helped us? Clang has really made C tooling possible, just to wax poetic for a little bit. So it's shown that having a tooling friendly compiler really leads to this ecosystem of software tools. So before Clang, there was kind of GCC. There were some efforts like GCC XML, and they've made great progress. But if you look at what we have with Clang now, we have things like Asan Tsan Msan, we have clang format and Clang tidy.
00:19:59.206 - 00:20:22.290, Speaker A: We have doxygen, even has lib Clang integration. Now, this is all because Clang's code base is really easy to hack on, right? It's separated into all of these libraries. You can pick only what you want. It's easy to get into. It's very well documented. The interface is very clean, even things like the preprocessor. Maybe the hairiest part of compiling CRC is easy to instrument.
00:20:22.290 - 00:21:15.250, Speaker A: Clang's semantic analysis is done extremely well. It has excellent template support. If you've ever had a template error from Clang, you really start to like it a lot compared to what's happened before. So Clang does this by storing more information than what's actually strictly necessary to compile C, right? It has first class representations of a class template, right? This turns into a class template echo. It has first class representations of partial specializations, of specializations, and of implicit specializations. Really the only representation clang actually needs to compile C are the implicit specializations that you get from here. But it keeps all of this information around later, and we use that to build our graph.
00:21:15.250 - 00:22:49.386, Speaker A: Let's look at the different kinds of queries that you can do within clang on a certain type. So here we have an instance of this partial specialization on class C. We can ask this instance, well, what is the template that you specialize? So what's kind of the primary template? And we'll get that back here. We could also ask it what is if you happen to specialize a partial specialization, what deckle is that? This is really important to us when we're trying to build our index, because one of the things that programmers frequently want to know is what's this code actually doing? Right? You point at a function call, and this could be a call to a function template, some overload, some partial specialization, and it's really difficult sometimes to figure out what's going on. But because Klein keeps all of this information around and because it makes it easy to query, we're able to put that in our database and have it just as accessible as any other aspect of the graph. One of the other cool things we can do is ask, well, if we're just talking about the primary template, what are the arguments that we used? Or how were the type variables derived once we did semantic analysis? So we'll see that we initialized this type variable t to x or pointer to x. When I was writing the code to support this, I was worried about having to figure out for partial specializations.
00:22:49.386 - 00:23:34.346, Speaker A: Well, how am I going to figure out what the assignment of s is? Because I really want to present that to the user. And as it turns out, there's already support for that. And this has happened a couple of times. So there's another call you can issue that's just get template instantiation args and it will tell you how it inferred the template arguments of partial specializations. And again, in the end, when you're just interested in compiling c plus plus, you don't have to keep this information around. It's available though for people writing tools, for people trying to generate good diagnostics, and we make use of it in our project. So speaking of weird things that happen with sort of textual substitution, let's talk about actual textual substitution.
00:23:34.346 - 00:24:18.798, Speaker A: So Klein really makes macros manageable. Look at this program. We have a macro m, one that just adds its two macro arguments, and we have two variables, x and y, and we're returning what turns out to be their sum. Let's concentrate on that return expression. Imagine you're trying to build your cross reference tool here. What you really want to do is you really want to be able to mouse over x or y here and point to their definitions but if you think about it, what the compiler has actually analyzed, what's gone through the typechatter, what you get at the end of SemA is the result of this expansion. This is not at all related to this.
00:24:18.798 - 00:25:08.590, Speaker A: It makes sense to you looking at it first off, but if you start thinking about it, where is this expression located? This is just some thing that happens. It parses out to an ast, you get deckel ref experts, but where are these actually located? Right. Klein goes through the trouble of associating these deckel ref experts with their original locations in the source file. So as it turns out, when you're building a tool like this, you don't have to make any special effort to connect this back to here. Klang does it for you. So Klang's made the macro transparent. We're interested in compiling actual code, right? And actual code uses extensions.
00:25:08.590 - 00:25:46.060, Speaker A: Some of the extensions that clang has supported. And remember, these are not part of the standard in any sense, but they come up in places like the Linux kernel. Our indirect go to address of labels. These are really great if you're implementing your threaded compiler or your threaded interpreter statement expressions. I guess these are no longer cool now that we have lambdas conditional expressions without the middle operand, and I think these are still kind of neat. This means call f, and if it's not null, use the result without calling f again. Otherwise call G.
00:25:46.060 - 00:26:32.410, Speaker A: Case labels with ranges, ranges and array initializers and so on. Clangs can build software that uses all of these things. It can build a Linux kernel, modulo a couple of patches. There's a LLVM project you can get@linuxfoundation.org that has all of these patches there. And as it turns out, the patches aren't that bad, right? The harious feature that is not supported by clang is this variable length arrays and structs thing, which is easy enough to get around, and the patch does that. There's also a couple little things about getting the current stack pointer that their patches are able to build in support for that and still compile in both compilers.
00:26:32.410 - 00:27:27.260, Speaker A: And there's been recent work on support for visual C and its ABI that's documented by its implementation. And there's been some success building chromium on Windows. The really cool thing here is that when Klang doesn't understand a program, it can fall back to the visual C compiler and still link against the resulting module. What do we propose to add to Klein's tooling support? So we have this story for persistence of abstract program data. We talk about records, not cxx record decals. We have this story for hermetic storage of compilation units. Remember, this allowed us to package all of the dependencies up in one ball and easily distribute it through a data center or a build form.
00:27:27.260 - 00:28:04.360, Speaker A: The individual nodes in the data center don't have to have access back to the original repository either, so you don't have to worry about that. We give you this technique for unambiguous naming for more program entities than are supported by usRs. We can name things, for example, that are underneath implicit template specializations, and we name them in a stable way. So every time you rerun your indexer, you'll get the same name at those points. So you can later use this for analysis. We give you this interface and implementation for performing abstract ast traversal. So we give you this graph observer thing that you can use.
00:28:04.360 - 00:29:07.580, Speaker A: We want C to be a first class citizen among the languages that Kythe supports. I mentioned that we didn't try to take Klein's ast and just dump it to a graph. What we did though is we said, well, what's going to be the most difficult languages of the interesting languages to support in our schema? And it was C plus plus, and we figured that if we can represent C plus plus, then we could probably represent the other languages as well. And that largely turned out to be the case. We want to support all of C Plus Plus 14 templates, generic lambdas, auto because of the way that we represent C Plus Plus's version of parametric polymorphism templates, should they be ratified. We expect that support for concepts like will not be difficult, or whichever kinds of constrained parametric polymorphism are eventually accepted. That won't be hard, because we have that refactored representation of type abstraction that I showed you.
00:29:07.580 - 00:30:07.520, Speaker A: So our idea for the path to upstream this and what we'll propose to upstream is nothing kite specific. We don't want LLVM to grow dependencies on protocol buffers. For example, we would like to give a library that will live in clang tools extra that will call appropriate members on one of these graph observers. The kaithindexer is just a particular implementation of this thing. And remember, this thing is the class that gets notified whenever the indexer frontend action observes a abstract record, right? It just says there is a record here. Okay, so with all of that there, what's left to be done? Well, there's quite a bit. We haven't done very much work on UI or IDE integration, and I know there's a lot of great work being done in the community on this, and we're interested to see what people can do with the data that we'll provide.
00:30:07.520 - 00:30:58.170, Speaker A: We're interested in seeing support for other programming languages, including maybe one or two that are supported by clang already. We'll give you c and C. Other analyses that work over or contribute to the graph are also very interesting. Because of this sable naming, you can use the kive graph as a starting point for driving harder analyses. So let's say you want to perform some kind of dataflow analysis on all functions with a particular type signature, or take certain security types in. You can search for them over a large code base using the kithe graph. Then later, once you have the names of those nodes, write your own graph observer that looks in the compilation units that define those nodes.
00:30:58.170 - 00:31:56.094, Speaker A: Then when you find them again, you have access to the full resolved semantic ast to do your in depth analysis. We're interested in adding more build information to our graph. For example, we'd like to instrument the linker so that we have ideas about which translation units are linked to which other translation units. Because our naming scheme provides us with resistance to ODR violations, we're able to deal with situations where, say, you have a bunch of programs in your project that all define something called main. But if you're searching for main, you'd really like to filter on a particular executable, right? Or generally, if you're searching for anything, you'd like to be able to provide the engine with some idea about what context you're in. Having this extra information would allow us to provide that. We'd also like to be able to do quicker updates of the graph.
00:31:56.094 - 00:33:03.400, Speaker A: Right now, it's kind of a whole program analysis to build this thing. All right, so in summary, the openkive data format enables interoperable tooling. The pipeline is designed to scale to 100 million line repositories. Yes, C Plus plus support is possible thanks to the work done on clang tooling. And as kind of as a side note, just because the language is simpler like go or Java, doesn't necessarily mean it's easier to tool. It's really the quality of the front end that makes it easy to provide language tools for. And we hope that with the baseline of providing us with this graph data, other language communities will be encouraged to build toolable front ends in the sense that clang is the code that will propose to upstream, doesn't depend on Kythe, and there are plenty of opportunities for community development so with that being said, we have a mailing list you can sign up for if you're interested, and I'd be happy to take any questions.
00:33:03.400 - 00:33:38.180, Speaker A: Start the front and work my way back. Cool. So I actually have lots, but I think I'll come to you after. One of the more interesting ones was you mentioned macros a little bit. Do you actually support keeping macros in the graph? So macro definitions and where their expansions are and so on and so forth. Right. So our plans are, and what we've observed is that keeping like a full record of macro expansions would cause the graph to just become way too big.
00:33:38.180 - 00:33:50.600, Speaker A: Our goal is in the graph to be able to look through maybe one level of macro expansion, but we also keep information around, of course, about macro definitions. So we do instrument the preprocessor as well.
00:33:55.530 - 00:33:59.058, Speaker B: Could you say a little bit more about how you resolve ODR violations?
00:33:59.234 - 00:34:29.746, Speaker A: Right. So in our actual graph, we separate the notion. And I can show you a little bit, I don't know if you'll be able to see this. So here's the actual graph that we generate from that one example. And if you notice here, this is the representation of the enumeration, and the enumeration had a particular name, it was just called enum. Right. We separate the notion of assigning a name to an object from that object itself.
00:34:29.746 - 00:35:06.814, Speaker A: So it's possible that other subgraphs from other parts of your program may also have something named n that are also associated with this particular reified name. That doesn't affect the semantic thing that is being named. So if we think about the example of main, right, we might have a bunch of things that are assigned the name main, but they can all coexist because that relationship is separate from the way that we store information about each individual instance of main.
00:35:06.932 - 00:35:38.670, Speaker B: I'm thinking like, if you're trying to traverse the call graph, so let's say you have a function foo that's defined in some CpP file, and you have a bunch of other Cpp files that foolishly do not include a common header that declares foo, but just have an external declaration in each file. Are you able to map calls to foo in those other files back to the original foo?
00:35:38.770 - 00:36:39.066, Speaker A: So there are two ways that we can handle this. The first way, without the extra linker information, is that we have a notion of a declaration completing definitions. And in some contexts it's possible to say in the same translation unit, if you have void f and then later void f with a definition, that second void f kind of uniquely completes that first declaration. So that if you say, where is that called? We can point directly to that thing in the context you're talking about. There are still completion relationships, but they're not unique. So we can tell you that if you make a call to foo and we don't have the linker information available, it may have gone to any one of the full definitions of the function. If we do have the linker information available, then we can use that to rank those search results and say, well, we know that this translation unit was linked to this other translation unit, and the definitions in this translation unit include a definition of f.
00:36:39.066 - 00:37:29.340, Speaker A: So that call to f from this declaration that we can see must have gone to this one. Having built a source code indexer before, I'm surprised no one has asked this question yet. But how do you handle the ifdef situation where you have on some platforms code is enabled, on some platforms it's not enabled. So right now we consider the state of the preprocessor to be part of the state of one of those translation units. But an element of future work is to look at how to slice up a translation unit to figure out which parts are dependent on maybe a user specified list of semantically important preprocessor declarations. Yeah, that's a hard problem.
00:37:31.470 - 00:37:49.554, Speaker C: So how powerful does the system need to be to run this whole thing? I mean, can you expect a user who is doing their personal project at some point if this were available, can they run the whole thing and in an evening, within a few minutes or something? And can they have their private instance of this up and running?
00:37:49.672 - 00:38:01.874, Speaker A: Oh, yeah, you can run this on one machine and it scales, right? You can run this on your laptop. We run a whole bunch of little tests like this. The answer is yes, you can run it wherever.
00:38:01.922 - 00:38:03.734, Speaker C: So I don't need a cluster to run the whole thing?
00:38:03.772 - 00:39:01.560, Speaker A: No, you do not need a cluster to run the whole thing. Okay, what's the advantage of pushing the graph resolver back into clang at all? Because it's already hard coded to the Kaith schema of what things resolve to a record and what things are a name and et cetera. So the abstract representation isn't directly related to the kite schema. There's still certain decisions that come between the observer and spitting out the records that represent the graph. The advantage is it allows other people to build these other analyses that combine information from Klang's semantic representation with information that you get by searching one of these graph repositories. Not necessarily convinced, but thanks and we're out of time, so let's give a big round of applause. Thank you.
00:39:01.560 - 00:39:07.750, Speaker A: And if you have additional questions, you should grab them in the hall.
