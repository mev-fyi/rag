00:00:00.250 - 00:00:39.510, Speaker A: Hello, I'm Michael Spencer and I am with Sony Computer Entertainment America, and I'm working on LD, and today I'm going to talk about solving the linker performance problem. But first I'd actually like to talk a bit about LLD itself. So Lod is a system linker for many platforms. We currently intend to target windows, Linux, Darwin, FreeBSD. We're also open to supporting other platforms people want. It's easy to add new platforms, it's just these are the platforms that the people that are currently contributing really care about. So Lod is a cross linker and it's always a cross linker.
00:00:39.510 - 00:01:42.874, Speaker A: So given a single binary and the appropriate environment to target appropriate chrue, you can link for any target you want, as long as the linker supports it. We also have a feature where single binary can emulate different modes. So we have LD 64 which has its own flags, Ganu LD which has its own separate flags, and then link Exe on Windows which has yet another set of flags, and then also internal tools that have again different flags. And so we can target all these with the same binary just by sim linking to it and giving it a name. And it auto detects what it should act as a part of the llvm project. LoD is free software under the University of Illinois license. This also means that we take full advantage of the LVM infrastructure, including source control, the website, the mailing list, build bots, and most importantly the reviewers.
00:01:42.874 - 00:02:49.330, Speaker A: So we have lots of people that don't necessarily work at LD that still look at the code. So LoD currently self hosts, but only on x 86 64 Linux. It works with both lib SCd C plus plus and lib C plus plus. Now led itself is a moderately complicated C plus plus program, but it really doesn't use that many linking features other than what C plus plus uses, which are basically just merging of inline functions and templates, and running of global constructors and destructors. We also support the C standard library, which has thread local storage and lots of weird things required to get even hello world to run. We also actually have to support a subset of linker scripts for this to work, as on Ubuntu and probably many other systems. Libc so is actually a linker script which contains instructions to load a bunch of other different libraries.
00:02:49.330 - 00:03:26.590, Speaker A: So outside of this, not much currently works. I've tested on lots of smaller programs like the test suite for LVM. All that stuff works, but not large programs yet. As for other platforms, Nick Kledzik is currently working on machco support and friends at Qualcomm are working on hexagon support with it's. So it's not production ready, but we're getting there. As for performance, we are currently on par with gold on Windows. I haven't done extensive testing on Linux yet.
00:03:26.590 - 00:04:13.822, Speaker A: My main concern is running on Windows targeting Unix like system. That's where we are right now with performance. So, on to content. So why is linking actually slow? I looked at this a lot and I've determined that it's due to the complex semantics of our current linking situation. And this is basically symbol resolution in the face of both archives and the command line arguments. So as an example of the complication of archives, here we have an object file which has two undefined symbols, a and b. We also have two archives on our command line.
00:04:13.822 - 00:04:34.830, Speaker A: One which defines b, or which contains an object file which defines b. Another which contains an object file which defines both a and b. So the specific order matters. If you have a, then B, the first resolution, which. I'm sorry, which you cannot see. You first look up a. You look at the different archives.
00:04:34.830 - 00:05:04.966, Speaker A: This archive file, its symbol table doesn't have a, so you ignore it. You then go to this archive file. It has a in this object file. You load this object file and add a and B to the global symbol table. Then when you go to look up B again, it's already in the global symbol table and it's resolved. You never look at this object. However, if you take the exact same object file and switch the order to B and a you now find b up here and add b to the global symbol table.
00:05:04.966 - 00:05:16.670, Speaker A: Next, when you look up A, it's not in the global symbol table yet. It's not in this archive. It's in this archive. You add a. Then you try and add B. It's already there. Now you have a multiple symbol definition error.
00:05:16.670 - 00:06:00.640, Speaker A: This absolutely kills parallel linking. You can't just load symbols independently and then pick whichever one comes first in terms of weak and then just say that if there's multiple, there's an. So this is what I call the Abba problem. This isn't actually the fundamental problem. The fundamental problem is that the resolution of a single symbol depends on the resolution of all symbols which precede it. This is also a related issue that complicates this are groups and rescanning. So a group is when you get to this point in the command line.
00:06:00.640 - 00:06:34.982, Speaker A: You keep scanning these archive files until no new object files are loaded. Rescan is once you're at the very end of the command line. You go and rescan. Rescan now is when you hit that argument. You have to go and rescan from the beginning. These again are related to the same issue of what symbol you pick depends on the full history of the link. The next problem is that our current input formats are not optimized for linking.
00:06:34.982 - 00:07:29.100, Speaker A: ELF itself is really optimized for runtime linking and dynamic loading. Now, the linker really doesn't want Elf. It wants a format to analyze the relations between all these different pieces of data, and so it spends a significant amount of time messing around internally to get into this internal data structure. In doing so, it actually ends up touching a large number of pages of the ELF file and bringing them all into memory. It doesn't actually need all the state that's loading, it's just, it's spread out throughout the ELF file. And as for archives, they're really bad. They're in ascii format with fixed width fields, so sizes are all text.
00:07:29.100 - 00:08:20.166, Speaker A: You have to convert it into, parse the number, and then to find each location. Also, you can only have an alignment of two bytes. ELF requires either an eight or four byte alignment. We work around this on X 86, because X 86 doesn't care about alignment. However, on other processors it does, and so we have special support for emitting the correct loads if the object file you happen to load is unaligned. So how does Lod make this fast? Without changing anything else? One of the major things is to speculatively resolve. So we ignore that the semantics are wrong, and we speculatively read all the inputs in parallel.
00:08:20.166 - 00:09:52.010, Speaker A: So this is first done by reading each object file in library shared library on the command line, all in parallel, and adding these all to the global symbol table, and all the symbols they defined they define are stored in a lock free hash set. We then across many threads, we take every undefined symbol that came from this initial read and insert and do an atomic assert into the lock free hash set. If the item was trying to insert already exists, then that means that the symbol has either already been loaded or another thread is currently loading. That if it doesn't exist, if it did not already exist when it inserted, that thread then proceeds to go and find the first archive file which defines this symbol and loads the object file out of that. It then recursively proceeds to add all the symbols that that object file defines into the lock free hash set and continues looking up the undefined symbols. Now, when it loads these files, it caches that fully parsed into what format the linker wants. But this operation is not fully semantically correct because what symbols you actually pick are not what symbols that this operation picks.
00:09:52.010 - 00:10:47.306, Speaker A: So we have to do yet another step after this. But this operation is actually very fast. So next we do a sequential resolve, and this implements the correct semantics following Gneld and archives. So for this we process each object file in command line order. So in the order it came in, then every undefined symbol is looked up in the global symbol table. If it's not there, then we start searching for in the correct order for which one to get. When we find that archive that defines that symbol, most likely we already read that object file in the archive in the speculative read step, and so in that case we just directly grabbed that cached version and no time at all.
00:10:47.306 - 00:12:08.440, Speaker A: In the case that we guessed or that we speculated wrong, we have to then go and do a normal load. So far this has shown I get about a 20% performance improvement out of this instead of straight, instead of reading all instead of not doing the sequential or, sorry, the speculative read and the rest of the performance really seems to be I o bound, grabbing all those pages from the else into memory. So as for how we actually look up the files, we build an input file graph. So you start with Crto, have your other things, then you end up with a group for libgcC, libc you might have a rescan and then your end. So what this does is you essentially just directly walk this graph in the sequential resolves process and you have your special nodes come out. This can also represent all the linker script groups we also have as needed. We just tag which libraries are as needed.
00:12:08.440 - 00:13:01.110, Speaker A: So can we make it faster? Yes, but we have to change how we link. We do that by simplifying semantics. The combination of this command line order in archive extraction forces us into the sequential resolve. We don't actually have to do this if we change our semantics. So the ideal is to fully resolve in parallel and never have this sequential step. And this can be achieved by throwing out archives and linking everything on the command line. Now some people actually do this already, they don't use archives and it's f data sections, f function sections, and throw all the object files in the command line and link and a dead strip.
00:13:01.110 - 00:14:05.340, Speaker A: So this helps, but it's still having to keep track of these semantics even though they're not really needed. So for the semantics that we actually want is where the symbol you pick is not dependent on any other symbol that's picked. And so with this you actually have a very fast process of, in parallel, you just have essentially a hash map of symbols and each, the pointed two thing is a list of defined symbols. And you read each object file in. For every defined symbol, you just add it to that list. Then at the end of this operation, once you've read everything in, you go and do a parallel sort over. Well, you sort all these buckets in parallel, according to which order they came on the command line, and like if they're a weak symbol or not.
00:14:05.340 - 00:15:00.534, Speaker A: After you do this, if you have two defined symbols or two more defined symbols in any single bucket, then you have a multiple symbol definition error. Otherwise, the final resolve for each undefined symbol, you do a constant time hash look table lookup, and there you have your resolve symbol. So this makes everything completely independent and parallel. The next thing that we can do to solve reading the cache performance issue is defining a new object file format. So we really want to reduce the number of pages, memory pages we need to touch to do a link. So what we have is a format that the data is highly compacted. So we actually do have a format that we have, and it currently supports ELF, Muko and Kof.
00:15:00.534 - 00:16:05.374, Speaker A: And we actually have tested these with, we have partial support for these other formats and have tested it round tripping through this. And this is essentially a serialization of the atom model in LEd. So I'm going to quickly explain the atom model, in case you haven't heard it. So the atom model in LD is that instead of having a section in symbols like what is actually there in ELF, what we do is we break every single symbol and its associated data in the section up into individual atoms. And they're atoms because you can't divide them any further. Each atom then has references or links to other atoms that are, and these represent both relocations and ordering constraints from when you have to extract from when you have already relaxed code in a section. So we have this full graph, and doing operations on this graph is really easy and is easy to parallelize.
00:16:05.374 - 00:17:12.360, Speaker A: And so that's our internal model in LLD. We then also defined this binary representation which fully encodes all the elf semantics and maco semantics and cough semantics, but in a much more compact form. So this compact form is basically, there's just a list of chunks, which are the member data for each atom, which consists of essentially like what type of atom is it? Like code or data or debug info. The alignment requirements for the atom, the scope like global, local protected. Actually a lot of other semantics that you get, but so all this is stored and it's unique. So generally you have a bunch of atoms that all share the same attributes. So all those just, it's a pointer to a single item, then each atom, then you just have a list of atom which is directly memory mapped in and used in the program.
00:17:12.360 - 00:17:59.960, Speaker A: There's no extra processing. Loading this file and be ready to use it is incredibly fast. It's just hitting a few pages of memory. And so with this, I haven't been able to extensively test it because I don't have a compiler that outputs this. We do have a way to take an ELf file and generate this, but it really hasn't been extensively tested. But in the small scale tests I'm seeing pretty big performance impact increase. And I think that we can cut reading time at least in half with this.
00:17:59.960 - 00:18:14.060, Speaker A: So it's not the full linking time that's in half, it's the reading, which is reading is about a third of the linking process. And so that's it. Are there any questions?
00:18:37.190 - 00:18:52.700, Speaker B: Sorry. The do you expect to need to build tools to convert from the special files that you're going to be needing so that standard object manipulation tools can deal with them?
00:18:53.310 - 00:19:41.538, Speaker A: All right, so as for binutils, tools like size and NM and such, yes, we have to have custom tools for that. The plan is to have clang learn how to directly emit these, and in some cases not even have to write them out to file, just directly generate the atom model in memory and pass it to the linker. Because the linker is a library, there's not a big performance win as there is with integrated assembler here, because you don't have a single process that generates all the object files. So it's a small win. And if you can use it for jitting, it's not really designed for that. But there are some wins there. And then the output format is elf out of the linker.
00:19:41.538 - 00:19:48.030, Speaker A: But yes, so the middle parts need to have the dumping tools written.
00:20:01.030 - 00:20:05.294, Speaker C: Example, can you show it with the A's and B's?
00:20:05.342 - 00:20:06.100, Speaker A: Oh yes.
00:20:07.910 - 00:20:13.830, Speaker C: I don't understand why it is allowed to have multiple B's in these object files.
00:20:14.330 - 00:20:33.440, Speaker A: So these are two separate archive files. You end up with situations like this where you're allowed to do this because it scans in order, and so if it finds it above or someplace else, it never looks for that again.
00:20:35.410 - 00:20:44.626, Speaker C: I see that there are problems, but why is it allowed at all to have multiple names defined, the same name defined multiple times?
00:20:44.808 - 00:21:14.140, Speaker A: Because whoever wrote ld decided this, it's been this way for a long time, because all the original linkers were all single threaded, and this parallelism didn't matter. You couldn't get any win out of it. And so this is an easy way to do it. And so this was just a fallout of that model. We have one down here.
00:21:16.910 - 00:21:21.920, Speaker D: I think C explicitly allows this. There's no ODR in C.
00:21:25.170 - 00:21:27.360, Speaker A: No linker is going to accept this, though.
00:21:28.770 - 00:21:37.380, Speaker D: Yes, there are things which you can define twice, and the linker C allows you to do this.
00:21:39.110 - 00:21:40.258, Speaker A: I have to look into this.
00:21:40.344 - 00:21:49.890, Speaker D: My question was, first off, is there a design document somewhere for the linker format?
00:21:50.230 - 00:21:53.990, Speaker A: Yes, on the LEd website. It is not an extensive design document.
00:21:54.490 - 00:21:56.610, Speaker D: Has it been sent to the mailing list for discussion?
00:21:56.690 - 00:21:57.222, Speaker A: No.
00:21:57.356 - 00:22:12.646, Speaker D: It'd be awesome for that to happen. Okay. And second was when I would really like to have an obs copy, like an LVM object copy tool that will convert. And I would really like to have clang start emitting these so that we can test them at scale.
00:22:12.838 - 00:22:46.600, Speaker A: So LoD can already, like if you just run your elf file through LoD with some special options, it'll already dump it out to this native format. As for the whole rest of the step, to actually make this fast, which is to not do that copy, is to teach clang. And I don't currently have plans. I do have plans to implement that eventually, but there's still a lot of work that needs to be done to get the link to work before that's reasonable to do. I definitely think that within the next year we'll have it, but I can't really judge more than that.
00:23:01.880 - 00:23:17.770, Speaker B: So how do you handle debug data in your atom model? Since in practice much of the performance problem is actually dealing with enormous amounts of dwarf rather than enormous amounts of object code.
00:23:18.540 - 00:23:54.660, Speaker A: So the real fix to this is debug fission. But yes, we do handle the debug info. It's its own atom. It gets relocated. We don't have it implemented yet, but we're going to support the compacting of debug info and linking that. But yeah, that's also a performance problem. But I don't care so much about speeding that up specifically because I think fission is a much better solution than trying to somehow make that magically faster.
00:23:57.080 - 00:24:00.176, Speaker B: What are you referring to here? Debug vision?
00:24:00.288 - 00:24:01.140, Speaker A: Fission?
00:24:01.660 - 00:24:03.192, Speaker E: Wait about 15 minutes.
00:24:03.326 - 00:24:13.470, Speaker A: Yes, he's going to talk about it. It'll be a surprise. No?
00:24:18.000 - 00:24:19.310, Speaker C: Any other question?
00:24:21.600 - 00:24:41.010, Speaker E: The one comment on that, though the slowness of linking with debug information is just no different than a large text section. It's just the size and the number of relocations. So debug itself doesn't make it any worse or better in the general linking case, depending on what you're doing, of course.
00:24:41.940 - 00:24:58.310, Speaker A: Yeah. And so we do do relocations in parallel and writing out in parallel. The actual writing out seems to not take very long, but I'm not sure if that's because the operating system is lying to me when it says that it's written everything but the actual mem copy is to do that. It's not very much time.
00:25:02.880 - 00:25:04.320, Speaker C: Stephen. Thank you, Michael.
