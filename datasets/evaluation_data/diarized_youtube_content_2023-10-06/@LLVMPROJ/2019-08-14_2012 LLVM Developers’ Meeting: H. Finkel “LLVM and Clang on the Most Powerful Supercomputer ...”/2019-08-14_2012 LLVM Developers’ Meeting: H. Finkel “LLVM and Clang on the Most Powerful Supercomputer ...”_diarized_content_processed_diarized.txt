00:00:00.410 - 00:00:53.340, Speaker A: Okay, well, thank you very much. I was surprised and honored to be given the first speaking slot at the meeting. And so hopefully you all enjoyed this as well as the title implies, be telling you about my work on putting LVM and clang on what is now the most powerful supercomputer in the entire world, at least that we know about. So first I will be telling you about sort of where I work and what we do, and then I will give you some details about the architecture of the computer, and talk a little bit about what was involved in porting to the computer. Then some brief performance results, and then I will conclude. I first, I'd start off with some basic background, since many people are not familiar with this, but I work at Argon National Laboratory. There's an aerial photo there, so now you know everything that's going on.
00:00:53.340 - 00:01:22.150, Speaker A: It's actually a very large place. There are over 1200 scientists and engineers, over 3000 total employees. And so it always amuses me when people say, so what do you do there? Well, a lot of stuff. There are 13 research divisions and six user facilities. And the difference is that the user facilities serve national interests. They're not just there to serve the laboratory. And I work for one of these user facilities, which is the Argon leadership computing facility.
00:01:22.150 - 00:02:11.298, Speaker A: The computing facility is one of two major facilities that's run by the Department of Energy, and they serve the scientific community of the nation and the world at large. We give computing time to research groups from all over the world, both in academia and on the commercial side. We currently have two computers. We have the one that we've run for many years, the blue Gene P, called intrepid, which is a half a petaflop machine, and the new one, which we're now getting, which is a ten petaflop blue gene Q machine from IBM. This machine is currently undergoing acceptance, and I'll be talking about my efforts with regards back to that. So first I start out by showing you what it is. So this is a supercomputer.
00:02:11.298 - 00:02:29.450, Speaker A: So that doesn't look like much. Actually. It's one of the surprising things that always surprised me anyway, which is the supercomputers are actually pretty small. I mean, I always sort of think of supercomputers as like something that fills up, like whole buildings or something. That was before I actually saw one. They're actually not all that big. So this one here is 48 racks.
00:02:29.450 - 00:03:21.610, Speaker A: Its larger cousin at Lawrence Livermore is 96 racks, so it's twice as big, but nevertheless, they're not really big. I mean, if you look at the area that this computer takes up relative to the machine room, which is not necessarily obvious in this picture, it's really, really small. I mean, the machine room is much, much larger than the machine, and that doesn't necessarily mean we could afford to power many more of them, but it's still small. And so what do I mean by the most powerful supercomputer in the world? Well, what I mean by that is that. So there's a list called the top 500 list, which people run a very large, dense linear algebra problem on the entire machine. Very few people do this for actual applications anymore, but it is still a standard, simple benchmark that people use it. And so you can see here that the number one and number three are now blue gene cues.
00:03:21.610 - 00:04:16.560, Speaker A: The number two is the K machine. In Japan, that machine was much, much more expensive. So I think we're still ahead. The nice thing about this machine is not only is it one of the most powerful machines in the world, and not only is it fairly easy to program, because you don't have to worry about accelerators and other things, but it is also essentially the most energy efficient supercomputer in the world, almost by a factor of two. And if you look at the green 500 list, which is essentially the top 500 list ordered by energy efficiency, blue gene Q's occupy slots one through 20. Well, okay, so that doesn't necessarily mean anything because it's all the same machine. But the next slot on the list is occupied by an Intel Xeon Xeon mic cluster, Xeon fire cluster, which is almost a factor of two worth.
00:04:16.560 - 00:04:37.918, Speaker A: And that cluster is at intel. So it's probably still experimental. So I mean, obviously a lot of other companies are catching up to this fairly quickly. But at the moment we're ahead. So there we go. I'm safe, at least until the end of the talk. Okay, so we're all here to talk about compilers.
00:04:37.918 - 00:05:24.340, Speaker A: So I tell you about the chips that run these things. So this is a picture of the blue gene core chip. It's a system on a chip design. It has 18 processing units around the outside, the inside is, there's a crossbar in the middle, and then there's a lot of l two cache, 32 megs of it, the 18 cores around the outside. So one of them is a spare for yield, then one of them runs the operating system and the others are accessible to the user. It has some interesting features. Probably the most interesting feature, or at least most unique feature, is that the l two cache is multiversioned so that means that not only does each cache line have an address associated with it, but it also has a processor core thread and transaction id.
00:05:24.340 - 00:06:14.660, Speaker A: And the machine supports true hardware, transactional memory and speculative execution. We are working on finding good use cases for this, but it's a really cool feature. So something which is perhaps slightly more immediately practical is that the cache, since it serves as it's a common cache between all the processing units, it serves as a common synchronization point. And there are built in atomic operations that operate directly in the circuitry of the cache. So instead of having to use the normal ISA atomic operations, we have very efficient atomic operations that are built in. The node is attached to 16 gigs of ram, and all these things are connected in what they call a 5D Taurus. All right.
00:06:14.660 - 00:06:59.662, Speaker A: But the nice thing about it is that it has two gigs a second in and out over the network with DMA, and remote gets inputs, which is of course important for a supercomputer where you have to do a lot of communication. So here's the core. Now, the core actually itself is very simple. They tell me this is how they get the nice power efficiency, and I suppose I tend to believe them. So the interesting thing about the design is, one, it's completely in order. It supports four hardware threads, and the four hardware threads dispatch into two pipelines. There is a pipeline for floating point and a pipeline for everything else, and it will dispatch two instructions into each pipeline every cycle.
00:06:59.662 - 00:07:52.654, Speaker A: But those two instructions must come from different threads. So there is essentially no way of fully using the capability of these cores without threading. Now, to be fair, you can also run multiple processes, but there are other limitations to doing that. So for the most part, you really need threads to make full use of this machine, which is an interesting design decision, not necessarily a bad one, but is certainly one that pushes for parallelism at the thread level in almost all of our codes. And the nice thing about the layout of the pipelines is that most of the common instructions, in fact almost all the common instructions, don't stall. So loads and stores and loads hit at l one. And just about all the other common operations have single cycle throughput.
00:07:52.654 - 00:08:43.270, Speaker A: So I mean, they have a latency. They're floating points, they have five cycles, but you can dispatch one per cycle, and I should say it is a 64 bit core also. So for the most part, this uses the normal power PC ISA, but it's been extended with a special floating point unit. This floating point unit, they call it quad vector floating point. It's four doubles, four double precision floating point numbers, and the lane zero of the vector registers aliases the normal scalar registers. So in that sense, it makes it very easy to load things into at least part of the vector register. And you get most of the sort of normal vector instructions you might expect.
00:08:43.270 - 00:09:07.322, Speaker A: We have loads and stores both double precision and single precision. We have permutations. We have loads and stores of integer values, and we have float to integer conversions. But those are the only integer operations that you can do in these vector registers. There is nothing else you can convert between integers and floating point. And that's it. For floating point things, we have normal scalar operations like absolute value integration.
00:09:07.322 - 00:09:35.954, Speaker A: We have the normal things you might expect to do, like adds and subtracts. No division, though. But we do have reciprocal estimates and reciprocal square root estimates. We have the fused operations you'd likely expect. Fused multiply adds, multiply subtracts, they're negated versions. And some special cross things for doing complex arithmetic that's common in, say, Fourier transforms. We have some comparisons, so we have less than greater than test for nans.
00:09:35.954 - 00:10:47.082, Speaker A: We have general Boolean operations on the results of those, and we have a vector select, and that's it. And I want to say something quickly about the way that booleans are represented, because this is one of these unique features of the chip, is that the booleans are floating point numbers, because, remember, there are no integer operations, so what else could they be? And so the convention that they used was that values that are greater than or equal to zero are true and negative values and nans are false. And the Boolean operations, like the comparisons, always produce plus or minus one. And one nice thing that they did was they just gave us one general Boolean operation that takes a truth table. And so from that you can form all the sort of normal combinations, but you can also just combine arbitrarily and given any kind of truth table, which actually turns out to be nice from an optimization perspective. All right, so now that I told you all this, I was like, why are you doing this? So one thing about supercomputers is that they tend to be sort of point in time purchases. And so you buy the supercomputer, and then you want to use it probably for the better part of a decade.
00:10:47.082 - 00:11:40.858, Speaker A: And so the question is, well, what do you do for software maintenance? Because normally, I mean, aside from some bug fixes and things, your software stacks don't get updated. And this actually turns out to be a real problem because six years from now, when people come in with codes, they're going to assume that they can use, say, language features that have now become common over the past six years. And it's very difficult to, you know, sorry, you should use your version of the code from six years ago. It's not going to work. So one thing that was very appealing about the LVM projects with clang and its various other front ends is that we could provide a high performance, up to date compiler and toolchain that we would be able to maintain over the lifetime of the machine. We would not only be able to provide a modern, up to date c and c front end, but we would also be able to take advantage of other front ends. There are of course, various scripting languages now that use LVM as the backend.
00:11:40.858 - 00:12:40.798, Speaker A: There are some compiled front ends, like Intel's ISPC project, which should be very interesting for us. And of course, being a research facility, we also wanted to provide a platform for computer science research. And one thing that you want to do when you provide a platform for research is you want to be able to really evaluate the results compared to existing techniques. But of course this becomes hard when existing techniques use highly optimizing compilers, and the research results use toy compilers that don't produce good code. So one thing that we really wanted was to have a research platform that produced good code. And with lvm and clang we can do this. So I'm going to put in a small plug and an example of one of the things that we can do with control of the front an there's an extension, these tag type diagnostics that were developed by Dimitri and these were inspired by HPC, but have now been generalized and are now inclined.
00:12:40.798 - 00:13:29.646, Speaker A: And this is an example of one of these things that we can do to help our users, and we can only do this because of the strength of these projects. And so this is a very simple example. So this is an MPI call. MPI is the message passing interface, the API that's used for the communication on these machines. And like many other sort of old C style APIs, it has this nice feature that of course it takes a void pointer, which is great for type checking, and then it takes some tag that tells the function what type of data went in there. And of course, people who write modern C plus plus programs say, well yeah, you should just template this thing and not worry about it. But of course we can't change the interface.
00:13:29.646 - 00:14:35.410, Speaker A: And we have a lot of people who still write C code, but these kinds of errors are really common where especially what happens is people copy and paste and then they change the type of the pointer and forget to change the tag or vice versa. And this leads to problems which are sometimes very difficult to find. But now the LVM and clang that's installed and hooked up with our various wrapper scripts and everything else will now warn users if they do this. And this is an incredibly useful and very unique feature that now only we have, and it has already caught numerous bugs in real production code, so very happy about that. And this same feature can be applied to POSiX APIs for various things like the FCTL call for file descriptor manipulation, and we can catch similar errors there. So about the porting. So I would say I divide the porting into two parts.
00:14:35.410 - 00:15:18.838, Speaker A: There was the easy part and then there were the not so easy parts. To the great credit of the LVM community, there are a lot of things that were in the easy part. In fact, almost all the initial work was in the easy part. So there was a pre existing PowerPC backend in LVM. And so to get it to compile code that included the vector registers, there were some things that had to be done. You had to put in new vector, new register definitions, change the calling conventions, put in patterns in tablegen to match the various vector things. But all of that was actually very easy.
00:15:18.838 - 00:16:21.146, Speaker A: And in fact, just by looking at code in the other backends, it was pretty easy to sort of pattern match and put in what needed to happen. And the amazing thing was that between that and putting the intrinsics into clang, which was similarly easy, it just worked. And which I thought was really amazing. In fact, that whole operation took me about a week, and that was the first time I had ever touched LVM. So the next week I spent developing the itinerary for the a two core, which again was a lot of pattern matching from some things in the arm, back end, et cetera. And by the time I was done with that and put in the necessary things, which was about another week, I was, and I'll show you some of this later, I was beating the performance of IBM's GCC port by a factor of again. And that was easy.
00:16:21.146 - 00:16:53.586, Speaker A: I mean, not that there weren't hard parts, but that was easy. And that, I think was absolutely amazing and is completely to the credit of all of you. There were some other things that happened. So I took the hexagon hardware loop paths and modified it to do power pc things. That also turned out to be pretty easy. So now of course there were hard parts, so I'll mention some hard parts too. Do you know why you were reading GCC? Yes.
00:16:53.586 - 00:17:23.200, Speaker A: Kind mean. Okay, I mean, the answer is sort of complicated, right? Because it depends on many things. IBM didn't, let's just say that they could have put more effort into the GCC scheduling processor scheduling definitions and other things. And there are various factors. It's also an older version of GCC that they gave us. It's GCC 44 or something. I mean it's not too old, but it's certainly not the latest and greatest thing.
00:17:23.200 - 00:18:06.794, Speaker A: I'm not going to rag on them too much, but nevertheless I think that the result was still fairly impressive. Okay, so there were obviously some harder parts. So one thing of course was that it was really nice to generate vector instructions with intrinsics, but that's not really what most people want to do. They want to generate vector instructions from scalar code. And so I figured that the quote unquote easiest way to enable people to start doing that was to write a basic log vectorizer. And only because while the combinatorics are difficult, the underlying problem is conceptually pretty easy. And so I've done that.
00:18:06.794 - 00:19:05.886, Speaker A: If you'd like to know more about that, you can look at my slides and other things from the European LVM Developers conference earlier this year. And we're having a boss later, which you should all come to if you're interested. Another thing that ended up being more difficult was that the current LVM scheduler has this critical chain concept which strictly orders, loads and stores relative to the original source order. And because we're dealing with an in order chip, being able to arrange those, rearrange them, I should say, is extremely important in various cases, especially say, once you've unrolled and vectorized a loop. And so making modifications to that in order to get better performance actually turned out to be nontrivial. Adding support for the vector boolean operations also ended up being slightly nontrivial. It required adding new built in type.
00:19:05.886 - 00:19:58.462, Speaker A: And of course when you do that, you expose other bugs and other assumptions in other places that all have to be fixed. Of course, this is true in general for any kind of new target code. You add new target code and you find bugs in other places in the infrastructure. Tracking some of those down ended up being pretty difficult, as with just cleaning up the power bind, which hadn't really been well maintained for quite some. So, okay, now I'm going to show you some benchmarks, and I'm going to start out with a bad one, maybe because it makes the good one look better. But there's a good reason for this. So the boost library, the c plus plus library, has the library called microbloth, which is a linear algebra package that is templated and you can template it over a number of different things.
00:19:58.462 - 00:20:51.594, Speaker A: So you can use it for dense linear algebra, for sparse linear algebra, for all sorts of things. And you can do it over built in types, over custom types. And it makes a really good compiler test in my opinion. So this graph is comparing the performance that I get over various test cases in this benchmark from Klang and LVM versus IBM's compiler. And so in this case you can see that Klang and LVM produce worse code than IBM's compiler. And this currently is generally the case for dense floating point kernels. It's not really a vectorization issue so much as it's just a general code quality issue.
00:20:51.594 - 00:21:48.260, Speaker A: In the PowerPC backend and other things, I saw that and I wasn't too excited, although I was actually more excited about it than you might think. But I'll show you why in a couple of slides. But here's what happens if you template over the sparse matrix and vector types. One thing to notice is that most of the time LVM and clang produce much better code for this than IBM's compiler does. And actually the cases that are in the middle are, some of these are reference cases, so they're sort of the cases from the previous slide. Now what does that mean? That's 100%. It's x minus y over x.
00:21:48.260 - 00:22:54.680, Speaker A: The nice thing about this is that yes, we have a lot of code that's dense numeric kernels. And for those currently, anyway, IVM's compiler does a better job than we do, but we also have a lot of code that does other stuff, that uses a lot of c libraries that are templated, and other things that require a lot of inlining and propagation of constraints and all this kind of stuff. This is really a mid level optimizer issue. And the code quality issues that presented themselves in the previous slide haven't disappeared. They're still there, but we're doing better than the vendor's compiler, despite those issues. And again, this is to the credit of all of you, because I didn't do that, I didn't build that. So, okay, so again, here's another dense neural, again, we don't do so well.
00:22:54.680 - 00:23:46.724, Speaker A: And this, this house is sort of my favorite one. So this is also dense linear algebra, but this one is templated over the boost numeric interval type. So it's not a simple data type, it's a complex data type, and we do significantly better. And the performance on these kinds of codes is actually very important to us because most code that our users want to run is not formed completely from highly optimized, very tight numeric kernels. It's messy stuff, and we're doing better on the messy stuff. So again, this is dense numeric kernels. We do better in a few cases.
00:23:46.724 - 00:24:31.416, Speaker A: This TSPC benchmark is an auto vectorization benchmark, which is now in the test suite, which is nice. And as you can see, so this is with vectorization. IBM's compiler does a better job of vectorization, but also most of these things are just code quality issues, because if you turn off vectorization and bolts, you get essentially the same result. But here's in some sense, what got me excited about these results is, was not comparing to IBM's compiler, but comparing to the GCC port. And again, there are some cases where GCC does better than us. That's true, but that's actually not most of the time. Most of the time we do much better than the provided GCC port.
00:24:31.416 - 00:25:07.490, Speaker A: And this is what got people at ALCF really excited about this, because normally GCC was their only other option. And now I can provide an option that is better than that also includes vectorization support. Because IBM didn't put any vectorization support into their GCC port, it doesn't even understand the vector. Know we can beat them on that. And of course, if I made this plot, turning on vectorization and comparing with GCC, with vectorization off, we would be doing much, much better. But that's not really a fair comparison. Maybe that hasn't stopped me before.
00:25:07.490 - 00:26:05.800, Speaker A: And so we're getting to the end, I suppose. But I will say something about future work. So one thing of course is for vectorization, we're going to improve the generation, the sort of coverage for automated generation of the various vector instructions, because we have such a simple, in order core, combining things into the vector operations and combining different vector operations is really important because all of those things are a savings. And so this is going to require enhancements to the vectorizers to generate the more complicated instructions. It's going to require enhancements to the backend egg combiner. Another thing that it's going to require is putting in support for the vendor supplied vectorized math libraries. So IBM, for instance, provides a vectorized cosine function and we should use it.
00:26:05.800 - 00:26:54.440, Speaker A: There will be enhancements that will be necessary to the PowerPC backend, just general enhancements. So we're going to have to work on how the register spilling works. PowerPC has this really nice feature that the condition registers, which are the things that hold the results of comparisons, can't be directly spilled to memory, and so they have to go through general purpose registers. And we always have this issue with how to allocate spill slots and make sure we're not overalllocating stack space and all this kind of stuff. So that has to happen. We are going to integrate higher level loop transformations from poly. This will improve a number of those benchmarking cases where the only thing that's preventing us from doing vectorization is things like we need a loop interchange and poly will do that for us.
00:26:54.440 - 00:27:46.754, Speaker A: We want to put in optimization specifically for MPI, which is this main communication library. And for so you can think of complicated things like doing communication coalescing and all this kind of stuff. But in some sense, some of the simple things also matter, like teaching the aliasing analysis, which parts of the buffers the various communication or teams touch. And of course, lastly, but of course I didn't put it last in the slide, is parallelization support. This is something that's going to be very, very important to us. Of course, in some sense this is driven by our current hardware, because you really can't take advantage of our hardware without threads. And although this was not true a few years ago, almost all new scientific codes that make use of threaded parallelism use openmp.
00:27:46.754 - 00:28:42.310, Speaker A: It has really become the primary way that people make use of threads, although there certainly are some codes that use P threads and other APIs. And we're also now of course, we're coming up on when OpenMP four will come out. I don't know exactly when, but I think it's sort of in its more or less final stages. And so we'd like to see support for OpenMP, the latest openMP, with all the sort of various useful pieces being put to good use to generate good threaded. So that's something that's going to be very important for us. And I think putting all these things together, what we can really do is make lvm and make clang and the other associated front ends really a powerful force in high performance computing. We can make it the sort of go to compiler infrastructure.
00:28:42.310 - 00:29:40.080, Speaker A: And I think that that will make a very big impact on not only what we can do, but the overall efficiency with which the whole community operates. So to conclude, I suppose I will say that there are a number of people that I have to thank. This was sort of a long road, and there's still a long road ahead of us. And there are a lot of people who have contributed. So I want to specifically recognize Roman from the FreebSD Foundation. I would like to recognize Tobias who had a internship at freescale and contributed a number of very useful PowerPC optimizations recently. I would like to recognize our new contributors from the IBM Linux Technology center, who have been working very hard over the last few months at cleaning up various ABI issues that existed in the PowerPC backend.
00:29:40.080 - 00:30:23.584, Speaker A: And of course these are extremely important in turning our testers green. This is obviously an important issue for us. The goal of this is to produce a production stable compiler for our users, not a toy. And they have put in much more time than I could have as a single person. And we've really come a long way in just a short period. I would like to thank Toby and the dove for helping with the vectorizer and the cost implementation, and a whole bunch of other things. I would like to thank Andy and Jacob for answering numerous questions of mine about scheduling and register allocation and everything else.
00:30:23.584 - 00:31:03.340, Speaker A: Thank you. And of course I would like to thank all of you, because without all of you, none of this would have been possible. Okay, that's so sort of obvious, but nevertheless it should be said. And of course I would like to thank the people who pay me. So that's always important. But I would also like, in all fairness, I would like to thank, especially the ALCF, for being very supportive of this and for really being very committed to pushing this forward and providing the necessary resources to really make it happen, which includes things like testing resources which do cost money. So with that being said, thank you all for your attention.
00:31:03.340 - 00:31:05.550, Speaker A: You.
