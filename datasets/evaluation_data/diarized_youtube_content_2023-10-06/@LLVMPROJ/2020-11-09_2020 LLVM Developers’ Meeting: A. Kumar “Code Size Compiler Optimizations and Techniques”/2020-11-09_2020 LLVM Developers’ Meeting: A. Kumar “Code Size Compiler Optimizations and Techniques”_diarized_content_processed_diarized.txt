00:00:14.410 - 00:01:16.230, Speaker A: Hi, I'm Aditya Kumar. I have contributed a few compiler optimizations in LLVM Middleland like GVN hoist and hot cold splitting. Currently I'm working on some interesting compiler optimizations like merge functions, hot cold splitting, and few others. Please check out my fabricator page if you're interested in learning more or if you want to collaborate. I've also contributed some of some really impactful performance optimizations in lip C and lip standard C like string find and some I O stream optimizations. Currently I work as a software engineer at Facebook, where I work on performance optimizations and code size optimizations of system libraries. I also work on enforcing best software engineering practices and large scale source code refactoring.
00:01:16.230 - 00:02:30.980, Speaker A: In this presentation I'll be talking about code size, compiler optimizations and techniques for embedded systems. In this presentation I'll be talking about several methods of code size optimization. So I've divided the presentation into five different parts. The first few are very general and mostly geared towards educating people who may not have a lot of compiler experience. As we go down the list, the techniques become more interesting, and the last one is the most interesting, where we'll be talking about things which are still not in LLVM. So the first one is getting familiar with some of the common compiler optimizations like OS and Oz. These compiler optimization flags tell the compiler to optimize for code size.
00:02:30.980 - 00:03:33.880, Speaker A: Oz asks the compiler to do compiler optimizations regardless of whether it'll affect the performance, but at OS the compiler still makes a reasonable trade off to not compromise performance. The third flag is it's not directly a flag to the compiler. It passes the flag to linker where we ask the linker to strip unused symbols or debug sections. This can be avoided also if you are careful enough to not pass g means add the debug symbols. The fourth one is f no function sections. In principle, it is possible for compiler to generate each function in a separate section. This helps with several things like debugging and other things as well.
00:03:33.880 - 00:04:42.390, Speaker A: But for code size optimization we want to disable this. So passing this flag at the top level of build rule will make sure that no other build targets are even if they are passing this flag f function sections, they will be overridden if we have this f no function section at the top level. Similarly for f no unroll loops which disables loop unrolling because loop unrolling increases the code size quite a bit. F no exceptions disables the code generation of exception handling code in the binary. Similarly for fnorti if you're not using like dynamic cast or something, then we can pass this flag and compiler will not generate those extra symbols. There are different strategies to generate the code generate the switch statement. One of them is jump tables, which is the most common one, and by avoiding jump tables the compiler will generate the switch statement as a sequence of if, else or some other strategy.
00:04:42.390 - 00:05:40.782, Speaker A: So that may save code size. There are some compiler optimizations which may require some additional work, like enabling link time optimization in full LTO. What happens is the compile time increases dramatically because of the code size. The memory footprint of the compiler increases with thin LTO, the code generation is still parallel and some of the steps are sequential where compiler collects information from different modules and gives most of the advantages of full LTO, but still the compilation time does not suffer as much as full LTO, but they both save code size quite a bit. The third option is like a variant of identical code folding. In LLVM we have fmerge functions which is not enabled by default, so you might want to try them. It'll save code size.
00:05:40.782 - 00:06:44.210, Speaker A: What it does is it deduplicates identical functions and hence save the code size. Other optimizations like GvM hoist and GvM sync, they save some of the code size not quite a bit, but it is useful to try them as well. Machine outliner is a recent compiler optimization in LLVM. It saves quite a bit of code size. I believe it is only supported for Arm 64, so you might want to try them as well. The next option is enabling hot cold splitting. Hot cold splitting by itself does not save code size, but if we use hot cold splitting in conjunction with merge functions like scheduling hot cold splitting before and then merge functions, what happens is hot cold splitting outlines some pieces of code from cold parts of the code from a function, and in many cases these cold parts are very similar or identical.
00:06:44.210 - 00:07:49.590, Speaker A: And a very common example is assertion handling code. So the false part of assertion handler has calls to a function which fails the program like it'll exit the program, and that is very similar in most of the assertion handling part, they can be deduplicated. The last option is playing with the inliner threshold. Inlining is one of the most useful compiler optimizations for performance, but it can increase the code size quite a bit. So when the inline threshold is high, inlining is more aggressive, so reducing the inline threshold can help as well. Many of the C plus plus libraries, even the standard C plus plus libraries, are header, mostly header, not only header, but mostly like vector string. When we compile programs which include the standard header only libraries, the code is generated into each translation unit.
00:07:49.590 - 00:08:52.502, Speaker A: So what we can do is we can explicitly instantiate some of the commonly used functions or methods from these header only libraries. That will save code size and that will reduce compile time dramatically, because when the template is externally instantiated, then the definitions are not generated in each translation and saves a lot of compile time. We can also add attribute no inline to some of the commonly used functions like let's say std sort or find. These may incur some software engineering overhead, of course, but they'll still be able to reduce code size. The next section is source code level optimizations. This is for the application code that you might be working on. I have divided this into four subsections.
00:08:52.502 - 00:09:41.782, Speaker A: The first one is code refactoring, second one is source code annotations, and the third is using cheaper data structure, and the fourth one is using cheaper algorithms. So let's look at the first section of code refactoring. It is a general practice where many people write function definitions in the header file, assuming that these functions look small enough. But we often forget the abstraction penalty. With C plus plus, the abstraction penalty becomes more and more complicated. Like newer versions of the standard like C plus plus 17, we have few more constructors. I think in C plus plus eleven itself we have like copy constructor, move constructor, and then assignment operator destructor.
00:09:41.782 - 00:10:49.214, Speaker A: All these look benign and maybe not even visible. Like if you have not written them explicitly in the code then they will not be there, but it still is generated. And these small tiny looking functions, they get inlined everywhere and then have overhead. So explicitly generating these definitions in C plus plus cpp file, or if you have template class or template functions, then explicitly generating those in the C file cpp file. In principle it is not possible to know all the types that will be used for a generic library, but it is quite common to have only a couple of use cases which are more widely used than others, like vector of int for example. So in your case it might be something like that, and explicitly generating those definitions in the Cpp file will be more than enough to save quite a bit of code size. The other section of source code level optimization is having source code annotations like attribute cold and attribute inline.
00:10:49.214 - 00:11:51.190, Speaker A: Other techniques are switching off or on the compiler optimization for an entire translation unit by using pragma clang optimize on or off. Be careful not to put this in the header file, then it will be like a debugging nightmare that why things are happening that way. The other way is to use clang attribute and applying to all the functions in a translation unit so you don't have to manually put attributes at each function in a file, but adding this construct like apply to function, then it'll apply this attribute to all the functions. The third section is having source code level optimization by using cheaper data structures. So std list is quite cheaper compared to std vector in terms of code size. Similarly, unordered map and unordered set are more expensive with respect to code size. I'll share the numbers in the next slide.
00:11:51.190 - 00:12:36.050, Speaker A: So here we have example where on the left side we have a CD map and a CD list. On the right side we have unordered map and vector. In the first case we see it's a small program where we just declare the map and assign a value and return one of the values of a map so that the compiler do not optimize this map altogether. Compilers are very smart these days, so we want to be just sure that the map itself is not optimized. So on the left side we have code size footprint of around 14. On the right side, when we use unordered maps, the code size footprint is around 15 kB. So in a small program we have overhead of 1 kB.
00:12:36.050 - 00:13:38.114, Speaker A: Similarly for list, the code size footprint is around 13 kilobyte, but when we use vector it is 14.3 kB. So again 1 kb for a very small function. It is totally possible that in your case vector gives a less code size, which is weird, but it is possible. But the thought I want to share here is while using data structure, we should have this in mind. Like why are we using this data structure? So it's a small program where list, for the first time ever, list wins over vector when we are talking about code size, but we have to measure basically, in your case it might be different, but it is one way of thinking about it in terms of code size. The last section subsection here follows from the previous one by using cheaper algorithms.
00:13:38.114 - 00:14:19.700, Speaker A: So this may also be again slightly controversial. For many people, we commonly use quicksort for sorting. Quicksort is not a few lines of code. If you are very interested, I would encourage you to look into the C plus plus standard library implementation. It calls insertion sort and all kinds of tricks it does to make it faster for some use case that was benchmarked at some point of time, but it may not be very useful in our case. What if we don't care about performance? What if the container size is small enough using bubble sort. Bubble sort can be very small, like really few lines of code.
00:14:19.700 - 00:15:25.350, Speaker A: Similarly, we use binary search and we can use linear search. Linear search is, I don't know, like two, three lines of code. Binary search may be a few more lines of code. The other thought I want to provide here is why should we even sort? The purpose of sorting is to have faster searching at later point of time. What if the search was not required to be super fast in the first place, so we can do like the slowest linear search? In many cases, linear search is much faster even compared to binary search. Also, what if the container size itself is like eight elements or 18 teachers, for example? Another one is we're using standard library algorithms instead of rolling out our own pseudo mem copy, we can directly use mem copy of C or C plus plus library. On the left side we see it is ten instructions because compiler generates this code.
00:15:25.350 - 00:16:10.034, Speaker A: But when we call mem copy, there is just one branch to Mem copy, only two instructions. Okay, the next approach is to get insights into the source code. These are not very popular techniques, but when code size becomes bigger problems, then we can resolve to these techniques as well. LlvM provides a couple of lags like f instrument functions and fpchable function entry, where we can use these to collect information from application at runtime. And once we collect this information from every function using that knowledge we can explore what are the code size optimization that can be done. I have implemented function entry instrumentation. It has a very low overhead.
00:16:10.034 - 00:16:50.200, Speaker A: It is lock free and I have put this for review. It is much lower low overhead compared to other facilities. We have like patchable function entry or x ray instruments that we have. Once we get data from the previous approaches, we can explore more like less frequently used features or functions or libraries into a shared library. It doesn't reduce the code size, but it reduces the startup time of application and code size and startup time they go together. In many cases. You can also compress the less frequently used code.
00:16:50.200 - 00:17:54.134, Speaker A: I found a library online which has very low memory footprint and faster decoding. So what we do is once a library has been called, we can uncompress them in memory and then they can be used as function calls. Self compress is a tool which compresses debug sections, so similar techniques can be used to other sections as well. And LLVM has llvm strip which removes so many symbols which are unused and unnecessary for running the program so they can reduce the code size quite a bit. Now this is the last section which is of course one of the most interesting sections. These are the compiler optimizations which have not been implemented in Llvm yet. So these present like opportunity for us compiler of engineers to improve Llvm even more for code size optimizations.
00:17:54.134 - 00:18:49.100, Speaker A: The first one is outlining prologue and epilogue of functions. When the compiler generates the binary of a function, we have frame setup and frame destroy code for taking care of the collie saved registers. So these sequence of code are almost identical in many all the functions basically, except some cases like if the function is a leaf function, et cetera. But we don't have to generate all these identical sequence in each function. What we can do is we can just have a function call which generates the same, which has the same sequence of instructions and we can put the function call in the prologue and epilogue. GcC has this optimization for quite some time. Llvm can have this as well.
00:18:49.100 - 00:19:31.378, Speaker A: The second one is having short int. It is useful for some embedded applications. They have issues with standard compatibility and maintenance issues. But what if the integer can be shortened to, I don't know, like 16 bits or eight bits depending on the use case. Like some cases where we don't really need like 32 bit for integer. So these can be done as well. They will save quite a bit of code size support for popular attributes and pragmas like attribute opt size and attribute pragma GCC optimize OS.
00:19:31.378 - 00:20:13.400, Speaker A: These can help selectively optimize for code size. LlVM has attribute cold. When we put that attribute, it is optimized for code size, but semantically these are not same. So there might be differences once we have this opt size attribute implementation. The fourth one is basic block reordering to minimize code size when the compiler generates code generates the function. The call graph sorry, the control flow graph. There is a specific order that is selected based on the cost model.
00:20:13.400 - 00:21:26.560, Speaker A: Now depending on the cost model, the order will be different, but each order can have different code size footprint because of the branches that will be required to be added. So currently I believe the basic block ordering is using a cost model for performance optimization. We can have a similar cost model for code size so that number of branches or some other optimizations are reduced. The next one is splitting the function before inlining. So we have hot coal splitting optimization already. What if we can work put the inliner and hot coal splitting together? There has been some discussion in some of the patches in upstream. This will be interesting because when the LLVM inlines a function, inlining is mostly done to improve the performance.
00:21:26.560 - 00:22:12.928, Speaker A: But it is totally possible that all the parts of Kali are actually not hot. Some parts may be cold. So if we split the function cold part and then do the inlining, the next optimization we can implement is deduplicating the code from sibling branches. So if we have imagine like Daniel's branch and then both have similar code. So we have some optimization like machine sync and GVN sync, which syncs identical instructions. But these are not done across the entire control for graph. Gvn hoist does this, some of these and hoist them to common dominator.
00:22:12.928 - 00:23:13.524, Speaker A: But these are not very aggressive and not really geared towards code size. So if we can start deduplicating identical instructions or identical evaluations and put them in some basic block, we can think of this as kind of opposite to jump threading. The other optimization. It's like kind of a trick where we can leverage linker and the one definition rule to save quite a bit of code size. Let's say we have two functions which do the same thing. And what if we rename these functions? Let's say we are in thinity or whole program optimization mode, we can rename the function and the name of the function represents the body of the code. It's a hash, but it is, I don't know, like a bijective hash.
00:23:13.524 - 00:24:19.100, Speaker A: So we can reconstruct the function just by looking at the name. Think of that way so that there is no hash collision. We could totally do hashing. I'm not sure if there will be collision or something, but if the name is deterministic, then it will be definitely possible to deduplicate them. So in this small example where we rename this function to f returns ten and then linker will deduplicate. But I think it is only possible when we are in a whole program optimization mode or in some cases it can be done in non LTO, but it'll require passing a file and a flag to do this across entire program, like providing a map of function and then what it will be the rename, and that way it can be done in non lto as well. So I know we have a merge function which can do similar things, but in merge functions we cannot avoid generating the original function body.
00:24:19.100 - 00:25:22.960, Speaker A: So in merge function, the function is name is there and it branches to a stub which has the identical pieces of code. But the first part is still there, there's still some overhead, but with this trick, that thing is completely gone. The last one is loop IDM recognition LLVM has loop IDM recognition, but it still has several opportunities which are missing. I recently found an example where we are not doing mem copy or memset here and I think we should be able to do that. So there is a link if somebody is interested. And when we can find these small examples, I'm sure there are many more which can be recognized and we can put this logic in the compiler too. Basically, when we recognize a piece of code to be a standard call, then we save code size, but we also improve performance quite a bit in these cases.
00:25:22.960 - 00:26:13.890, Speaker A: Some of the references that may be useful are some of the patches that I mentioned earlier and then Mangcc. There are many optimizations which are described in enough detail for most people to understand what is going on. Then Clang has help hidden which explains a lot more optimizations which are not widely used, like for example Gvn, Horace Gv and sync. You can see the explanation there. Then, as I explained elf compress and LlvM strip, this may have more options. I would encourage you to try them as well. Thank you.
00:26:24.120 - 00:26:48.830, Speaker B: Welcome to the q A for the code size compiler optimization talk. Please keep posting questions and let's get started with our first question. So some optimizations can increase size locally, but can lead to size savings due to optimizations kicking in later on, for example, unrolling. Can we better detect such issues and fix those issues?
00:26:51.200 - 00:26:54.030, Speaker C: Yeah, so it's an interesting question.
00:26:58.820 - 00:26:59.184, Speaker A: As.
00:26:59.222 - 00:28:00.532, Speaker C: The example that you suggested, I think they can be implemented, but I would like to .1 thing that in NLBM there are still much bigger code size opportunities, and if we focus on them, we'll get more bang for the buck. But this is possible. But it'll require analysis which are slightly nontrivial, especially like loop unrolling. If we unroll too much, then maybe the code size saving may not happen. But if we only unroll like a couple of times, depending on it's very vague to generalize this, because if we unroll and then vectorization kicks in, then maybe the code size reduces dramatically. But if the loop trip count is not like a multiple of the code size, then the vector length, then there'll be prologue and epilogue of some kind of setup and some picks up will be there.
00:28:00.532 - 00:28:12.730, Speaker C: So it may increase the code size in that case. So it's a pretty tricky one, I would say, but it's a very good question. Gives me more thought to think about how we can achieve this.
00:28:13.100 - 00:28:23.500, Speaker B: Okay, thank you. The next question is, do you have any comments on the idea of a deep integration between the linker and the compiler for better code sales optimization.
00:28:24.880 - 00:29:04.744, Speaker C: Yeah, again, this is a tricky question. I can think of a couple of them like linker does, this identical code folding and garbage collection of sections. I don't know how we want to integrate compilers work on a translation unit, and linker works on the whole entire program. And to achieve this nice collaboration we have LTO or thin LTO and which already does these things. So I don't know. I'll basically pass this question for now.
00:29:04.782 - 00:29:05.736, Speaker A: Let's have to think about it.
00:29:05.758 - 00:29:07.096, Speaker C: It's a pretty easy question.
00:29:07.198 - 00:29:22.690, Speaker B: Fair enough. Let's take a look at the next question. You recommended to disable f function sections, but this would prevent GC sections from LLD. Is there a clear winner between the two of them?
00:29:25.300 - 00:29:34.240, Speaker C: You have to try for your specific workload. Some section can be garbage collected if there are no branches to that.
00:29:34.310 - 00:29:34.930, Speaker A: Basically.
00:29:35.620 - 00:30:02.056, Speaker C: Let me explain it a little bit for the wider audience. When we do f function sections, each function has a separate section, and when we want to garbage collect, what that means is if there is no call to a function, linker can easily detect that when each function is in a separate section, because linker in general operates on a section basis, so it will be able to remove so many functions.
00:30:02.088 - 00:30:03.068, Speaker A: That is true.
00:30:03.234 - 00:30:21.010, Speaker C: Now it may disable some optimization for compiler. I'm not an expert, but from what I read that there are some optimization that may not kick in. So depending on which is a clear winner, I don't know. It may depend on the workload also.
00:30:22.900 - 00:30:36.890, Speaker B: So the next question is there seem to be quite a few passes that help with reducing code size in LLVM, but lots of them are disabled by default. Which ones do you think we should focus on to enable by default? First?
00:30:37.660 - 00:31:29.524, Speaker C: Yeah, we should focus on the ones which give obviously more code size optimization. For example, match function is there, but there are some fundamental issues and there has been so many discussions and we are working on it. If you want to participate, I'll encourage people who are interested to collaborate more there. There are a few more like getting more insight into the inlining optimization. Inlining is super important in many cases, and for code size it is also tricky because in many cases it increases the code size. But there are many cases where inlining reduces the code size. So having some in depth understanding of how inliner can benefit code size optimization machine outliner is there.
00:31:29.524 - 00:31:54.690, Speaker C: It is enabled for ARm 64 and I learned it is for risk five as well. But maybe we can enable for other ones as well. So these are like pretty big ones if you want to go slightly smaller one, for example, GVN hoist gives around 1% codes as savings, so that is also reasonable. But merge function inlining sorry outliner, these games can give like five to 10% or maybe more.
00:31:56.260 - 00:32:04.340, Speaker B: Thanks. So the next question is what is the difference between hot and cold splitting together with inlining and the partial inlining?
00:32:07.320 - 00:32:47.760, Speaker C: Yeah, there is a difference, but hot code splitting can split from middle, like from anywhere in the function. But I think from what I remember from partial inlining is it inlines some part of the function from the beginning. That's why I remember. If I'm wrong, then maybe please feel free to correct me. But hot pulse splitting moves from anywhere in between the function because that part is cold, and then the rest of the function can be inlined. But currently hot pulse splitting and inlining are not working together currently in LLDM.
00:32:48.820 - 00:32:57.350, Speaker B: I think we have time for one quick question, or one more can you elaborate on use of Fno function sections to decrease size?
00:33:02.030 - 00:33:08.960, Speaker C: Can I elaborate? Just try it on your workload and then you see the data.
00:33:09.890 - 00:33:18.880, Speaker B: Okay, fair enough. Let's take another one. Any tips for debugging code size regressions caused by compiler changes?
00:33:19.490 - 00:33:20.990, Speaker C: Yes, git bicept.
00:33:23.030 - 00:34:04.190, Speaker B: Great. Yeah, so I think thank you for answering all those quite tough questions, and thanks everyone for joining the session, submitting the questions, and please continue submitting your questions and then maybe they can get answered on the app. And also, we encourage you to continue the conversation in the community board up and in roundtable sessions if you want to discuss this topic further.
00:34:05.970 - 00:34:06.430, Speaker A: Awesome.
00:34:06.500 - 00:34:08.410, Speaker C: Thank you everyone. Bye.
