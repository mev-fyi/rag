00:00:00.330 - 00:00:00.880, Speaker A: You.
00:00:01.330 - 00:00:29.250, Speaker B: So this work we did in early 21 and 22, kind of, and we started trying to understand how to write user, user written kernels, how to give a language to an end user, so that if you have a custom kernel to be written and that needs to be executed on an accelerator, right? So we had a custom accelerator, so we wanted to give an easy language and get the best performance.
00:00:29.330 - 00:00:29.526, Speaker A: Right?
00:00:29.548 - 00:00:55.120, Speaker B: So the management, what we had at the time was an intrinsic based language, it's seed driven, where kind of the manual was beginning to become pretty huge, the descriptions and all those things, and we had stopped printing them because it became very huge, like only soft copy available. And not many could write all the intrinsic to get the best performance. So that's the history of when we started.
00:00:55.490 - 00:00:56.286, Speaker A: Right?
00:00:56.468 - 00:01:22.600, Speaker B: So a typical example, I'm going to show it here. Like you can see the intrinsic, there is a load tensor, it says AI driven. Okay, I'm not doing anything. So it moves, slides, it does everything. So it's intrinsic driven. At the time, most of the custom kernel languages were entity driven. Really?
00:01:26.730 - 00:01:27.480, Speaker A: Okay.
00:01:28.330 - 00:02:16.306, Speaker B: So we started off in early 21, first quarter. Our job, the management asked, was to figure out an easy way of writing custom kernels and get the best performance, let's say 50%, 20% of the custom kernels, and it should work across the architecture, like different families of architecture, if necessary, different versions of the chip and so on and so forth. So we knew that, okay, MLiR can give me a very powerful optimization framework. And we had tried to convert CC plus plus photran around 2020 to MLIR. We had burned our hand in the general purpose languages, so we had understood the issues. So that's when we started looking at Python, Python Ast and what it takes to make things work, using Python as a front end language.
00:02:16.418 - 00:02:17.080, Speaker A: Right?
00:02:17.450 - 00:02:35.742, Speaker B: So our first task was to understand, can I intercept Python est in a particular manner, then take it to MLIR? And then aggressive optimizations, because our chip was kind of like vector engine. So there is not enough powerful scalar engine. So how do I operate this?
00:02:35.876 - 00:02:37.530, Speaker C: Green is forward and red is backwards.
00:02:37.610 - 00:02:46.080, Speaker B: Okay, do we need to point in a particular direction? It's okay. Red anywhere I press, it works.
00:02:46.610 - 00:02:47.920, Speaker C: Green, see if it works.
00:02:48.370 - 00:03:18.326, Speaker B: Lovely, thanks. So this is what everybody had in mind. You have a C based domain specific language, maybe simpler than the what that is already there. Then clang takes over, then LLVM and so on and so forth, right? So there was a typical expectation at that point. So we started defining something like. So these are some of the issues there, like limited representation. Again, it will lead to maybe a little bit smaller manual.
00:03:18.326 - 00:03:42.226, Speaker B: If you had really worked hard, we knew how to go to MLIR from C. At the time we had already worked on it. So if you had successfully taken C based code into MLIR, we knew that it will never go above SCF, it will never go to a fine, right? So what I wanted to do with the language was that I needed unroll, jam, unroll vectorizations to happen, right? And of course there is an alias and all those things will kick in.
00:03:42.248 - 00:03:42.386, Speaker A: Right.
00:03:42.408 - 00:04:17.130, Speaker B: Then I have to give some more constructs. So we said this will, on a longer run, not work out. In a sense, it's tough to maintain. So we tried to define a very simple language using Python. Okay, first step was to define something very simple, then intercept at the ASD level, take it to the mlir, then do the optimization, then lower it to LlVM and hope that the numbers look not too bad, right? It was early 21, like q one, q two. So not many precedent at that point. So we had to do something magical to survive.
00:04:17.290 - 00:04:18.000, Speaker A: Right?
00:04:19.810 - 00:04:56.586, Speaker B: So at the time you knew, okay, people are using. So all this pytorch was pretty much beginning to become very popular. So Python is the way to go. But question was how to do all these things and get it by the end of 21. Okay, so it's like timelines were stiff and we had to decide things at pace. Okay, so this was one of the third or fourth kernel we wrote make it work after our element wise add and reduction. This is softmax, and this is when we start trying to understand how bad it could get.
00:04:56.586 - 00:05:54.090, Speaker B: Okay, get the whole thing into MLIR, make sure it gets a reasonable vectorization, and so on and so forth, right? So this was our non trivial kernel, and we had it in our cubicles for a long time because we wanted to show everybody it works, right? So our first problem to solve at the time was softmax as an example. So if you look at it closely, there is no intrinsic asset, there's some parallel range, is there? Apart from that, it looks pretty much standard, and there is no data types mentioned at all. That's because our kernels need to work on around eight to ten data types. And if I try to specify for each kernel, it doesn't work. So on the instantiation, we'll give the data type, but the kernel is supposed to get generated on the fly, right? So that was the expectation. So we didn't specify any data types in the whole kernel, but the performance cannot be as low as python, which infers on the runtime. So we had to have an in between solution.
00:05:54.090 - 00:06:54.586, Speaker B: So some other examples that we are, at the end of the exercise, we had something like this, you can even give a register and get it to a lower dialect. You can take a scalar quantity, then you can take a pointer to LLVm directly. So finally, when we're done with the project, we can take it to any dialect directly. So what we did was if a guy who is writing a lower level dialect wants a test case, unit test case, we had a script that started generating directly any dialect, right? So all our dialects were developed independently, and we could get our test cases for everybody separately, right? We had a script that used to generate test case for every dialect. So currently it is like you can generate kind of like anything that you want, any dialect you want, right? So that's the current state. These are some examples I'm just showing you like how to get to assembly and some of the things that are just like, you can see the different stuff that are right there. So we use it for extensive testing how to generate unit test cases.
00:06:54.586 - 00:07:38.640, Speaker B: This tool is also for how to generate test cases on the fly and test them with our compiler, right? So our runner. So at the top you can see the add kernel, it's an element wise addition, and the bottom after that, the tensor one and tensor two are like kind of tensors. Then if you look at a DSL compile, you can see that it's getting instantiated. I can instantiate multiple ways, get multiple kernels for different data types. So that's when the actual thing kicks in, right. But the kernel remains same, number of instantiations are different for different data types. Back end engine, the vectorizers and all those things work seamlessly, you still get a good performant code.
00:07:38.640 - 00:08:23.012, Speaker B: So these are some more example function loading example, you can see. So again, these are all some additional stuff. As an sttris, we started supporting all these things, right? So these are some other examples. So the hints, if you want to give a hint, we have an unroll and jam hint, we can give some. If anybody wants to get a particular optimization, he wants to want to vectorize by eight, or unroll jam by four, or unroll by so on and so forth. Again, this was driven by a tool which used to explore the optimization spaces to get numbers. And of course the possibilities were like, the combinations became very huge.
00:08:23.012 - 00:08:42.632, Speaker B: So all these things was driven by a tool to say, what is the best combination for a particular kernel? And this was like to show the management that at any point which combination gets me the best numbers, right? So all of them are configured, and there is a nightly test suit that used to run to figure out how to get the best performance.
00:08:42.776 - 00:08:43.470, Speaker A: Right.
00:08:46.320 - 00:09:19.108, Speaker B: Now the question is, what are we doing behind the back? So the second half of the presentation is about what we did behind the back so that we started making it work, right? So we liked Python. We liked the Python est, which is one of the most beautiful and simple thing. Python. Of course, people hate it, I know, but Python Ast is one of the most beautiful things. We intercept the python est, then multiple dialects we use. Initially we convert to a non SSA thing, and then of course we lift it up. Then we move to MLIR.
00:09:19.108 - 00:09:51.492, Speaker B: Then from MLIR at a SCF dialect, or similar to SCF dialect, our optimization engine kicks in, because a lot of the constructs don't go to affined dialect, right? So we do a vectorization, we do a dependence analysis, we do vectorization, we do unroll, unroll, jam everything at a SCF dialect level. And that's because a lot of the constructs don't go to affine, and we had to get performance for every kernel. So there is no way to say 80% kernels do well and the 20% don't do well.
00:09:51.626 - 00:09:52.212, Speaker A: Right?
00:09:52.346 - 00:10:41.140, Speaker B: So this is something like, probably most of the MLR implementation rely on the fine. Or now people are looking at linal dialect. Let's say a lot of our kernels, we work like gathers catalogs, don't go to a fine, okay? The different steps are like we do a type inference because it's kernel per se, doesn't have a type. When you instantiate, we give a data type. So when I start interpreting that kernel, I have to figure out here is the input parameter, what's the data type? From there, the entire kernel, we start figuring out all the data types. And that is used for our vectorization and all other optimizations. So it's a non SSA.
00:10:41.140 - 00:11:11.144, Speaker B: So then we do intrinsic lowering, then you do type inference. Then there is a semantic check, then there's a code gen, right? That's where we get into Mlir. Till that point, it is like the different forms. So intrinsic lowering is the first step. And we use an TD file for doing most of the intrinsic lowering. We don't do it manually. It's like we have a TD file figuring out what are the types.
00:11:11.144 - 00:11:50.376, Speaker B: And if I mention DSL tensor, it's a tensor. And STd Bf 16 is a Bf 16 type. Then math scene at the end says that result is the same type as the input. Okay, so that's the last line. Intrinsic function, that's a sine function. So when we instantiate, if you specify that t one is a tensor of ten, sorry, not four tran floating point 32, then we decipher that Val is same thing. Then we interpret some function to figure out what's output of some function and say new val is a vector of four elements of f 32.
00:11:50.376 - 00:12:33.604, Speaker B: And then finally when we return back, we know new value is vector. And then math sin is one floating point 32 value because input val is a floating point 32. Then semi is like simple things we do, like undefined variables. Indentation is done by Python interpreter. We don't do that. We have a PI ast dialect which takes it from the Python ast and then moves it to mlir initially, right? And there is a final sema which does some of the final checks on the code. So assume that my language, I can add a Triton kind of thing.
00:12:33.604 - 00:12:52.604, Speaker B: How do I add. This is another example that I've just shown. How do I make Triton part of my language if I want to have a kernel written in Triton, immediately convert. Here is one way we have seen it work with us. Of course it's not optimal for us. Our architecture is different, but it's one way to do it.
00:12:52.722 - 00:12:53.390, Speaker A: Right?
00:12:54.560 - 00:13:02.012, Speaker B: Then the next level we tried was to start. Initially we had python, we tried it. So next level was to see, can I build a next level of abstraction?
00:13:02.156 - 00:13:02.704, Speaker A: Right.
00:13:02.822 - 00:13:25.400, Speaker B: So this was our next experiment where if you look at this right side example there, you can see for I in range ten, then omp, parallel, so on and so forth are there. So how do I parse things like that? A user wants his own language, let's say user wants to define his own language. And then how do I do it?
00:13:25.550 - 00:13:26.296, Speaker A: Right.
00:13:26.478 - 00:14:03.616, Speaker B: So user needs to specify different Td files for it. What is the asT? What's the semantic check? All those things in the example. In the preceding example, if you look at it, the moment I see far, the ast realizes that, or the set of tD files know that far has to go to Scf. It moves into a Scf TD file and then starts doing semi checks for Scf at the end of for loop happens. Then it goes back to the original check. Again, it's a separate TD file, one for the aSt, one for the scf dialect.
00:14:03.808 - 00:14:04.212, Speaker A: Right?
00:14:04.266 - 00:14:29.404, Speaker B: So you can define your own language using the Td files. And this should be handled elegantly. This is an extensible language that's what I did. So we open source in October 22. Of course we had working on a back end which was much more pressing thereafter. So we did not give a talk anywhere in between. And of course people have given a star and have started liking using it.
00:14:29.404 - 00:14:51.220, Speaker B: So the last name is. I think people have extended in a good way. I'm very happy that people are extending in a very interesting ways. There are nice use cases are seen. We have not envisioned such an elaborate extension. Some of them have done very happy about it. So what happens in the backend? I'll give very brief what happens in the backend.
00:14:51.220 - 00:15:37.520, Speaker B: Not many kernels. There are a lot of kernels which don't go to fine. Forget about linal, we can't lift it. So our first task was to have a dependence analysis in SCF dialect, right? So we used our llvm knowledge to get to something like some of those ideas from LLVM and wrote a dependence analysis. And then at SCF level, once a dependence is there, we have built an extensive vectorization if conversion unroll and unroll jam optimizations. So our optimization driver runs at SCF level where we do a registered pressure analysis there. Because one of the problems define is we are not able to target resource estimates.
00:15:37.520 - 00:16:18.728, Speaker B: We do a registered pressure analysis to make sure that there are not many spills because our chip doesn't have a spill area that's big enough. If I start doing aggressive optimizations, especially unroll and jam can add register pressure in a big way. So we have a rough register estimate for before every optimizations and the number of registers and all is roughly calculated. So scalar evolution is for dependence. Then we have a V plan kind of framework to drive all these optimizations. Thanks to the V plan folks, we have ported it and made it work. And then some amount of scheduling will be done here itself before loading it to LLVM.
00:16:18.728 - 00:16:47.220, Speaker B: So LLVM is like, except register allocation. We don't want anything to be done in LLVM. So we disable everything in LLVM and it goes to the low level and then gets optimized. So that's the number. So typically handwritten if on handwritten. If you look at the performance of this kernel versus the handwritten it used to be, most of the kernels were around 70% to 80% performance. But the elegance of the solution, like number of lines and lack of intrinsics.
00:16:47.300 - 00:16:47.592, Speaker A: Right.
00:16:47.646 - 00:16:55.260, Speaker B: So it was very simple to write and a new architecture comes along, you can use the same ideas to port it to the new architecture.
00:17:04.320 - 00:17:08.240, Speaker D: All right, questions for Prashantha, please use the mics.
00:17:14.540 - 00:17:30.060, Speaker E: So I wanted to ask about the rationale behind porting things from LLVM instead of letting LLVM do it after, like what's the reason? Do you get a performance benefit by doing it earlier in the pipeline?
00:17:31.760 - 00:17:32.124, Speaker A: Yeah.
00:17:32.162 - 00:17:37.736, Speaker B: So this is going on for ages, right. You need MLIR because you need a high level representation Fortran.
00:17:37.768 - 00:17:37.916, Speaker A: Right.
00:17:37.938 - 00:17:40.784, Speaker B: You can do all the loop transformations in LLVM, but we don't do.
00:17:40.822 - 00:17:41.024, Speaker A: Right.
00:17:41.062 - 00:17:47.810, Speaker B: Because at that level you have lost a lot of information. Same thing here. If I go to LLVM, I have lost information.
00:17:48.280 - 00:17:48.788, Speaker A: Right.
00:17:48.874 - 00:17:56.676, Speaker B: Then I can't do all that aggressive optimizations. And you hope that dependence analysis works because your IR is already very low level.
00:17:56.858 - 00:17:57.444, Speaker A: Right.
00:17:57.562 - 00:18:11.480, Speaker B: And what we are trying to do is like an aggressive vectorization. Aggressive if conversion loop, unroll, unroll, jam. Do we get all those things in Fortran? They should just use Llvm, right? It doesn't.
00:18:14.460 - 00:18:28.348, Speaker F: In one of your earlier slides you have things like unroll and I forgot like vectorize as things in your DSL, but they kind of disappeared after that. So in one of the first slides you had like dot unroll something and dot vectorize something.
00:18:28.434 - 00:18:36.624, Speaker B: Oh, that's because we want to give an expert programmer option. You try it out and tell us. And also we have scripts to set them and see the performance. Okay.
00:18:36.662 - 00:18:39.828, Speaker F: But it's still like in the final version of your DSL, it's still there?
00:18:39.914 - 00:18:55.370, Speaker B: Yeah, it's there. I'll show you. So basically there are two options. One, an expert end user wants to experiment. We give you the option or our scripts can pick it up and start figuring out what's the best performance. Right.
00:18:55.980 - 00:19:00.820, Speaker F: Have you looked at transform dialect in MLR? Have you looked at transform dialect in MLIR?
00:19:00.900 - 00:19:06.684, Speaker B: Yeah, we are looking at it now. Yeah, we are using it. Yeah, this was like 21, 22. Yeah.
00:19:06.722 - 00:19:11.020, Speaker F: I imagine a lot of these things can be currently done with upstream MLIR.
00:19:11.440 - 00:19:14.976, Speaker B: Yes, we are doing it. Our next thing is on.
00:19:15.158 - 00:19:15.712, Speaker F: Thank you.
00:19:15.766 - 00:19:21.884, Speaker B: Yeah, thanks. But there are cases where transform dialect might not work. I'll catch you offline.
00:19:21.932 - 00:19:22.530, Speaker A: Yeah.
00:19:35.650 - 00:20:06.060, Speaker G: So, I mean, hello. Thanks. This is really cool that you're re implementing. Like, I don't know many of the analyses that MLR doesn't have, but I guess I was wondering in particular, what's the information that's lost by the time we hit llvm that can't be recovered. So I'm asking because poly used to recover quite a lot of information from the LLVmir itself. And so I guess I'm kind of interested to know what is the precise information that would be lost where we to lower from MLR to LLVM and then to try to do the loop optimization there.
00:20:07.470 - 00:20:57.786, Speaker B: So we tried to use poly in 2017 when the spec came out. I know Toby's grass is here, so I had to be careful here, right? So we had a Fortran benchmarks at the time. And if it works, then why should I rewrite even? We started writing loop transforms in 20, 14, 15. We said Poly might not work and we start writing our own classical loop dependence analysis. So if look at spec, I'm giving a photran example, which is like straightforward for everybody, right? If you look at spec four, trans, poly did not optimize many, right? You cannot get accurate bounds and all those things. So there are issues, inaccuracies. Finally it gives a star in the dependency, right? So you lose information, then you can't do at that level right now too.
00:20:57.786 - 00:21:03.520, Speaker B: If you look at Photron 18, they will say, I'll go and do loop transformation at MLIR level. That's the reason.
00:21:06.370 - 00:21:13.438, Speaker H: Hi, are the plans to upstream any of the analysis transforms, et cetera, to the SEF direct?
00:21:13.524 - 00:21:26.390, Speaker B: They are tied to our architecture. So I'm very careful. Front end is already upstream, people can use it. ML is the front end part free. Anybody can pick it up and use it. The backend part is very much tied to the accelerator.
00:21:28.730 - 00:21:35.474, Speaker H: Sorry, let me try to rephrase. I'm talking specifically about memory, dependency analysis, transformations on SCF.
00:21:35.602 - 00:21:48.234, Speaker B: Yes, we are working on it. We'll try to find a way to upstream parts of the code, the dependence and what else? Everything else is easy, right? Once dependencies are there, everything else can be done, right?
00:21:48.272 - 00:21:52.538, Speaker H: But if you have done them in a certain way that you've connected all of these pieces, that's valuable.
00:21:52.634 - 00:22:06.740, Speaker B: Basically we have a driver that does orchestrates everything, right? Tiling and all those things. We are working on something similar to fusograph compiler, so we'll come back next year and talk about it, right?
00:22:07.110 - 00:22:07.860, Speaker A: Yeah.
00:22:11.830 - 00:22:19.510, Speaker C: So can you use this kind of like number, right, to actually call this function from python code and have it be accelerated?
00:22:19.930 - 00:22:20.710, Speaker B: Pardon me?
00:22:20.780 - 00:22:32.700, Speaker C: So does this generate a separate like Mii module that you then compiled or can you use, you can call this add kernel function in line in Python and execute it.
00:22:33.230 - 00:22:53.294, Speaker B: Initial days one path used to go to normal CPU execution. Another will be intercepted and goes to an accelerator simulator on the same machine. I get the output. I used to verify for many times I used to do that. Then it became a burden to keep CPU running and all those things. So finally we dropped it.
00:22:53.412 - 00:22:57.486, Speaker C: Okay. You can't just call add kernel in Python.
00:22:57.598 - 00:23:10.774, Speaker B: No, it used to run. It is not general Python. This Python, it runs on CPU. We made it work on CPU. Okay. For 80% of the project that was our verification like CPU result and then our result.
00:23:10.892 - 00:23:11.560, Speaker A: Right.
00:23:12.250 - 00:23:13.160, Speaker C: Thank you.
00:23:17.310 - 00:23:40.110, Speaker D: Thank you Prashanta. I just have a very last question. So your employment independence analysis MLIA has a lot of infrastructure for reasoning about constraints and stuff. Are you using any of the analysis support that in particular for affirm constraints in mlier?
00:23:40.930 - 00:23:45.120, Speaker B: No, the thing is, most of the MLIR is defined level, right? Does it do anything below.
00:23:47.430 - 00:23:50.338, Speaker D: A math library to compute some of them?
00:23:50.504 - 00:23:54.610, Speaker B: I assume that I built an affin expression at SEF level. Something like that.
00:23:54.680 - 00:23:59.460, Speaker D: Did you use some of these? How do you do dependent analysis on the actual mathematical level?
00:23:59.910 - 00:24:10.626, Speaker B: No, we built entire stuff ourselves. So we undo the use kev kind of thing at MLIR and we built the entire single variable stuff everything ourselves.
00:24:10.738 - 00:24:14.278, Speaker D: Okay, we should have a chat now.
00:24:14.364 - 00:24:33.600, Speaker B: We did it in LLVM before. We wrote a loop transformation framework in LLVM before CPU days. Of course we said poly will not work. We know Tobius is a great guy, but poly will not work. We built something at LLVM itself so that's our history. Of course we should talk.
00:24:35.250 - 00:24:38.940, Speaker D: All right, let's thank our speaker Prashanta. Thanks.
