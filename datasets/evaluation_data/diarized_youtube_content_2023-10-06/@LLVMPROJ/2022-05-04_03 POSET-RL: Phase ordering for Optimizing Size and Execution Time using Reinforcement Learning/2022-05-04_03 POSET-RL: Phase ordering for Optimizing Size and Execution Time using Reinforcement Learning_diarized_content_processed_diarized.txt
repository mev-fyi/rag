00:00:00.170 - 00:00:45.302, Speaker A: So thanks. Okay, here I'm going to present my talk poset RL, that is face ordering for optimizing size and execution time using reinforcement learning. So my co authors are Yasuz Andaluri and S. Venkata KT, and we did this work under the guidance of Dr. Ramakrishna Upadhastha. So, as we all know it. Okay, so as we all know that ever increased the increased memory requirements for several applications has led to the increased demand for the memory size which could not be met in the embedded systems.
00:00:45.302 - 00:01:33.610, Speaker A: So hence here, code size optimization plays an important role. But such an optimization should not have the negative impact for execution time. So here we propose an RL based framework to solve the problem of phase ordering with an objective of optimizing for port size and execution time. So as we all know that phase ordering is a problem of finding the optimization sequence and ordering those optimization sequences. So we can see with this example, suppose we have the n number of optimization passes. Here we can choose any optimization pass, and then again we have the same set of optimization passes. And then there is a problem to choose another pass, and this goes on.
00:01:33.610 - 00:02:30.998, Speaker A: And in the end we can find out an order of the optimization passes. That order may work well for one program or one set of programs, but may not work well for another set of programs. And also the different so the permutations of these optimization sequences in the same order, different permutations may have the different performance impact. So there are some optimization sequences which are already well designed with the compiler experts. Here we discuss some of them. Those optimization sequence are known for performing best with some objective, but it does not fit well across all the cases. So here let's see some of the optimization sequences.
00:02:30.998 - 00:03:19.146, Speaker A: Suppose zero, it disables all the optimizations we see. One, it enables few optimizations. O two also adds few more optimizations, but it changes the heuristic of those optimizations. Three is the optimization which is well known for improving the execution time, but it may decrease the port size as well. OS is the optimization which performs nearly equal to two in terms of execution time, but it reduces the port size. Oz optimization sequence is known for provide the more code size optimizations, but it may increase the execution time. So here we can see the three is the optimization sequence which performs very good in terms of execution time, but it can increase the code size.
00:03:19.146 - 00:04:18.190, Speaker A: Similarly, oz is the optimization sequence which performs good in terms of code size reductions, but on the penalty of execution time. So here is a trade off between both. So if we solve this problem with single objective like if we want to optimize only for code size, it may adversely and without considering the execution time, then it can ignore the optimization passes like unrolling and inlining. Similarly, if we go with a single objective for optimizing only for execution time, and we do not consider the code size, it can aggressively unroll or inline. So here for solving this problem, we go with the dual objective, we go optimize the code size and execution time. Let's see the comparison between runtime and code size for three and oz optimization sequence. So here we can see the oz optimization sequence takes more time to execute the programs than three.
00:04:18.190 - 00:04:59.900, Speaker A: Here, for all these set of benchmarks, we computed the execution time and we figured out that oz optimization sequence is taking approximately 10% more time than oz on an average. Also. So if we see the port sizes, then we can see the oz is able to reduce the port size for almost all the benchmarks. But the overall code size reduction is approximately 3.5% on an average for these benchmarks. So we get the 3.5% reduction in the code size on the cost of 10% increased execution time.
00:04:59.900 - 00:06:09.410, Speaker A: So here we propose a reinforcement learning model that solves the problem of phase ordering with the objective of optimizing for both port size and execution time. So for solving this problem, we need some mathematical representation of the program so that we can give the programs as an input to the model. So the one way is to give the feature vector to the model, but it requires the manual efforts. This problem has been solved by presenting some common representations. IR two VEc is one such representation. So here the IR two Vec is an LLVM IR based vector representations of the program, which represents the program as a higher dimensional vector, and also it encodes the program features, flow information and semantics of the code. Also, our goal is to predict the optimization sequences.
00:06:09.410 - 00:07:01.762, Speaker A: For that we put the number of subsequences, put them all together and try to find out the optimal order of those subsequences. We select the sequences from that set and try to find out the optimal order of that. So here there are two ways to do that. We choose the sequences manually from oz, and another is we generate the Oz dependence graph and we automatically generate the sequence from that oz dependence graph. Also, this approach can be extended for the other architectures. So we show the results on x 86 and AR architectures. Now this is a basic block diagram for reinforcement learning.
00:07:01.762 - 00:07:39.802, Speaker A: So in reinforcement learning environment represents the observations and RL isn't tries to see the state from the environment state is an observation based on the state. It chooses some action and send that action to environment. Environment performs that action and generate the new state. Also, based on the new state and the previous state, environment computes the reward. It conveys both the new state and reward to the RL agent. RL agent looks for the reward and it tries to learn based on the reward. Reward can be either positive or negative.
00:07:39.802 - 00:08:41.966, Speaker A: Based on that, it learns to predict the actions and this process goes on. So here, why is phase ordering an RL problem? So suppose we have the m number of optimization passes and the optimization sequence we are making is suppose we fix the length to n. And as we all know that optimization sequence can have the reputations of the passes. So there can be m to the power of n. Total combinations are possible for oz optimization sequence if we see there are in total 90 transformation passes out of which 54 are the unique transformation passes. So in total there are 54 to the power of 90 combinations are possible. So if we want to train the model with a supervised learning approach, then for each the data element from the data set, we need to generate these many combinations ten to the power of 150 combinations approximately for each data element.
00:08:41.966 - 00:09:53.980, Speaker A: And this is highly infeasible. So hence reinforcement learning is a good solution to go with that, here is the proposed workflow of our methodology. So in the environment, first we convert the source program to the LLVM IR representation. So this is converted by Clang or clang plus plus as we consider the C and CPP files, these IR will be converted to the IR two VEC embeddings through IR two VEC engine and this will act as an estate. So RL agent will learn the state from the environment and then it will choose some action based on the state. This action action space is a set of subsequences. So RL agent will choose some subsequence from this set, it will send this to environment, and then environment will try to in the environment, LLVM optimizer will optimize the existing program with that subsequence and then that optimized program.
00:09:53.980 - 00:10:46.298, Speaker A: For that optimized program, IR two VEC embeddings will be generated. So these new IR two VEC embeddings will be the new state, it will be represented as a new state and then environment will compute the reward based on the new state and the previous state. For the reward, we consider two metrics. One is the size counter and another one is the throughput which we compute through LLVM MCM. So this new state and the reward both will be conveyed to the RL agent. RL agent will look into the reward and based on the reward it will further learn for choosing predicting the next section and this process goes on until the model converges. So now as we already have seen, the agent interacts with the environment and produces new states.
00:10:46.298 - 00:11:30.546, Speaker A: Also the IR two vector embeddings acts as a state. So now let's see the two approaches for the action space one by one. One is the manual and second one is the subsequences generated by ODP. So the sequences which we create manually. So for that, first we do the study of the oz optimization sequence and then from that we created the chunk of optimization passes and created the 15 manual subsequences. These passes are grouped according to their functionality. So here we can see these 15 number of optimization sequences we created.
00:11:30.546 - 00:12:03.010, Speaker A: Like here we can see some module label passes are there. If we see the sequence having some loop related optimizations like loop simplify, loop rotate, loop invariant code motion, or loop and switch. In the end we do instruction combined. If we see here loop distribute and loop vectorize are there. So this is how we are creating the manual subsequences. Now second way is to go with the Oz dependence graph, automatic generation of the subsequences. So for that we construct the Oz dependence graph.
00:12:03.010 - 00:12:42.590, Speaker A: We take each individual optimization path from oz path sequence and then represent that as a node of the graph. So for each pair of consecutive nodes we create an a's in the graph. So suppose pass a precedes pass b. We create an a's a to b. So here we can see this is an odor dependence graph here. Suppose for an example in the OJ path sequence we have simplified CFG and then instamine. So we create an ace from simplified CFG to instamine.
00:12:42.590 - 00:13:16.566, Speaker A: Now the next step is to identify the critical nodes in the graph. So if a node is having a degree higher is greater than or equal to k, we choose that as a critical node. Here we choose a degree is equal to eight. So we get the three critical nodes. One is a simplify CFG, another one is inst combined, and the third one is loop simplify. The reason for choosing the critical node is these nodes are having higher degree means. They are having higher number of occurrences in the Oz pass sequence means these nodes are important nodes.
00:13:16.566 - 00:14:02.380, Speaker A: And also these are preserving the order of the optimization sequence. So next, after figuring out the critical nodes, we take a walk that starts and ends at a critical node. This will be represented as a subsequent. So let's see for example, like here we have the simplify CFG. So we take a walk from simplify CFG to loop simplify here by this way. So this will be represented as a subsequent. So for next example, if we see here this loop simplify, if we take a walk from here and again we reach to the loop simplify which is a critical node, then this will be an subsequence again.
00:14:02.380 - 00:15:14.426, Speaker A: So why we choose for odd subsequences? It is not easy to tune the subsequences manually because we require the knowledge of each individual path. Some domain knowledge is required and also it may not include all possible orders. So here, if we choose this graph like here, we can see there are many nodes which are having the multiple ages and it is uncovering the new optimization sequences which is following the order of the oz only here in total, with the three critical nodes we are able to get 34 number of subsequent. Now let's come to the reward computation. So as our objective is to optimize for code size and execution time, we consider the reward for binary size and the throughput of the binary throughput because we consider the static measure of the runtime which we compute through LLVM MCA. Also alpha and beta are the hyperparameters which we tune to ten and five respectively. So alpha is greater than beta.
00:15:14.426 - 00:16:51.710, Speaker A: Here represents we are giving more weightage to binary size, while if we said beta is greater than alpha, then means we are giving more weightage to execution. So here arbin size is computed is a ratio of difference in the binary size between each interim state of an episode to the binary size of the original file. Similarly, the r throughput is the ratio of the throughput between ratio of the throughput of the difference between the throughput for each interim state of an episode to the throughput of our base object file original object file. So here we do the training on Intel Xeon E 52690 and Intel Goal 5122 processors. Also we train the model on 130 files from single source benchmarks from LLVM test source and we use the double DQ one network algorithms. We run inference on Intel Xeon E 52697 processor for x 86 architecture. For ArC, we do the cross compiling of LLVM to target Cortex a 72 processor and also we show the results for spec CPU 2017, spec CPU 2006 and Mybench benchmark.
00:16:51.710 - 00:17:38.910, Speaker A: Next we show the results of our experiments. So here we show the percentage of min average and max size reduction with respect to manual and ODG subsequences where our baseline is oz. So we can see the min size reduction is negative, it seems negative. It means the maximum size increased. For the manual subsequences we can see maximum performance improvement is always higher. It is always better than the oZ. But average size reduction for some cases like Mybench, for both the architectures and even here for Spec 2006, it is not better than Oz.
00:17:38.910 - 00:18:57.058, Speaker A: So while if we see for ODC, it always performs better than both manual and Oz subsequences, even we can see here the average port size reduction for respect 2017 is approximately 6%. Here for respect 2017, for respect 2006 it is approximately 5%. Similarly, for all the benchmarks it is positive we can see in the graph. So here we discuss the improvement in the execution time with manual and odg subsequences here also baseline is ODG. So we can see here for respect 2017 and my bench benchmark, with both the manual and ODZ subsequences we are able to reduce the execution time. So here spec 2017 even it is able to reduce 12% with ODZ and for my bench we are able to reduce approximately 6%. But for spec 2006 it takes the more execution time than the OJ optimization sequence.
00:18:57.058 - 00:19:42.994, Speaker A: So the reason maybe there are some benchmarks like s 264 ref. Those are the regression cases, we need to handle those cases carefully. And also we see like for the code size, similarly for execution time, ODG sequences are always performing better than the manual sequences. Now let's see the detailed results for the binary size for spec benchmarks. So here we can see for almost all the benchmarks. So we are showing the results only for ODC because we have already seen ODC is performing better than manual one. So the baseline is Oz.
00:19:42.994 - 00:20:51.110, Speaker A: Here, like we do the comparison of Oz and ODC sequencing. So we see ODC is able to reduce the code size for almost all the benchmarks. Only the two cases we can see here h 264 ref and LVM, where it is giving approximately equal size. And other than that, all the benchmarks we are able to get the size reduction. So for execution time we can see here the benchmarks from spec 2017 we get the good code size reductions. While most of the benchmarks for spec 2006, it is taking more time, execution time than the oZ, we see some benchmarks like Omnet, Pp and Leela we are getting the good execution time reduction here. For Leela it is way more okay, so here is a summary.
00:20:51.110 - 00:21:31.050, Speaker A: So we propose a reinforcement learning based framework to solve the phase solving problem with the objective of both optimizing for both port size and execution time, we propose the two approaches for modeling our action space. One is we choose the subsequences manually. Here we find the logical routing of the passes. Another one is we generate the ODG OJ dependence graph and generate the subsequences from that. Also for the reward, we have the reward for code size and runtime. For code size we consider the binary size as a metric. For runtime we consider the throughput, which is a static nature of the runtime.
00:21:31.050 - 00:22:02.050, Speaker A: Also we show the results on both x 86 and arc architectures. This ODG oz dependence graph can be extended to other sequence like three. It will have few more optimizations which are not there in Oz. And also if we are making the graph for o three, then we need to tune the parameter like alpha beta accordingly. We need to set beta value to higher than alpha.
00:22:06.570 - 00:22:12.680, Speaker B: Thank you so much. Shalini. I think we have a question from Johannes, so I'm going to start with this.
00:22:13.850 - 00:22:30.278, Speaker C: Thanks. Yeah, great talk. I actually have two. The first is about the last few execution time slides you showed with the. Yes, those ones, the ones with the spec patch. Let's stay with this one. This one is great.
00:22:30.278 - 00:22:39.594, Speaker C: So you label it runtime in seconds. Did you actually execute it or is this the static MCA element?
00:22:39.642 - 00:22:42.990, Speaker A: No, we executed these programs.
00:22:43.410 - 00:23:08.950, Speaker C: So you actually got, what is this like a two x speed up on Leela and on. That is really interesting. That's nice. Okay, cool. That was the first. And the second question I have is what about compile time? We heard in the keynote that some people are interested in that. Do you have measurements on that? Or at least measurements on how many passes you run compared to Oz?
00:23:09.110 - 00:23:30.394, Speaker A: Yeah. So that is correct. The compile time is way high. That is the drawback of this. So for approximately 1 second of compile time, it is taking now like 1 minute of compile time. So compile time is like. I think that is something like we need to look for and it can be fixed.
00:23:30.394 - 00:23:34.546, Speaker A: I think by having the proper modeling, I get that.
00:23:34.568 - 00:24:10.620, Speaker C: And you can probably put that into your model and you should. But just for the record, if compile time doesn't matter, I could just run two twice or Oz twice and I probably get better code out of it. So the comparison then is comparing that Oz is a little bit, it's unclear how much that tells me. If I can run Oz three times, it's probably going to improve over time. It's just taking longer. But I'm happy to see a future model that has compile time as part of the learning process.
00:24:16.850 - 00:24:23.310, Speaker B: Excellent. Thank you, John. Do we have any other question from the audience? Brian?
00:24:25.170 - 00:24:38.840, Speaker D: Hi. I have a question about the compile time breakdown as well. You mentioned a 60 times increase in compile time, right? 1 second turns into 1 minute. Do you have a breakdown of that? Where is the extra time coming from?
00:24:39.530 - 00:25:01.020, Speaker A: So actually during our model, one thing, we are predicting the sequences, we are trying out the different combinations of the sequences, trying out all those. Also, the predicted sequences are bigger than the sequence, what we are having in Oz. So all those things are taking more compile time.
00:25:01.810 - 00:25:11.280, Speaker D: Let me understand it. You're not trying out subsequences at inference time. Correct. Like you infer the set of optimizations you want to run and you just run it once.
00:25:12.690 - 00:25:41.960, Speaker A: Yeah. Correct. Okay, so we predict the sequence one by one, one after the other, right? Okay. So what we do, however, inference is working. We choose one sequence, we apply that sequence, and then again for that file, that binary, we again try to predict. So this action process is going one after the other, like we are having here in the model. Yeah.
00:25:41.960 - 00:25:55.978, Speaker A: So RLsing predicts one sequence, then it is applied, then again it returns the state, and then again it predicts another sequence.
00:25:56.154 - 00:25:57.198, Speaker D: Oh, I get it. Yeah.
00:25:57.284 - 00:26:08.740, Speaker A: Okay. Maybe this is a thing we also discuss. We need to look, compile time is way higher. We agree.
00:26:15.180 - 00:26:16.616, Speaker D: I see. Thank you.
00:26:16.798 - 00:26:17.412, Speaker C: Excellent.
00:26:17.476 - 00:26:27.740, Speaker B: Thank you very much, Brian. Ok, so I think we may have time for one more question, if somebody has one. Otherwise we move on to the next presentation.
00:26:28.080 - 00:26:47.252, Speaker E: Yeah, Jose, I'm sorry, I'm not able to raise my hand some. Looks like the older version of zoom. Just thought of. Just asking. Hey Shalini, thanks for your presentation. This was really insightful. So I see based on, I have two questions with respect to the whole approach itself, like the algorithms being used.
00:26:47.252 - 00:27:29.008, Speaker E: So, can you put some light on what kind of policy learning algorithm is being used in this whole RL approach? That is one part and also another second question is based on how the integries of the pass is being used to identify the critical node and then generating the subsequences using those critical nodes. Looks like the approach is more like a mix of RL plus heuristics, where this is like one another heuristic that is being used. So how do we make sure that using this heuristic we are not skipping a sequence which could have provided the better performance with respect to both memory and the execution time? So those would be like my two questions.
00:27:29.174 - 00:28:08.750, Speaker A: Yeah. So I'll first answer the last question. We are trying to explore, I think all the possible path which we can generate from the OZ optimization sequence for generating those path. We follow this approach so that we cannot break the dependencies of some passes which are coming in between. So if there are multiple aces means from there we can start the difference in between. We did not break it. There may be a possibility we can break these somewhere, but we did not try that.
00:28:08.750 - 00:28:36.070, Speaker A: That was the concept. Like we are trying for the first question, the policy we are using just like double deep QN network here and we follow the. I couldn't get your question properly. Can you repeat the first question?
00:28:36.760 - 00:28:40.788, Speaker E: Yeah, it was exactly on what algorithm is being used for both the policy.
00:28:40.874 - 00:28:47.790, Speaker A: And the DDQ and double DQ and network algorithms used.
00:28:49.120 - 00:28:51.608, Speaker E: Okay, sure. Thanks. Thanks, Shani.
00:28:51.704 - 00:28:52.510, Speaker A: Thank you.
00:28:52.880 - 00:28:53.290, Speaker B: Thank you.
