00:00:00.250 - 00:00:00.414, Speaker A: Hello.
00:00:00.452 - 00:00:09.710, Speaker B: We're about to get started here and I'd like to introduce Aditya who will talk to us about merging solar functions in finaltia.
00:00:13.330 - 00:01:11.540, Speaker A: There are many applications which are sensitive to code size. They are larger applications and most of the pieces of the program, they don't have a hot loop or something like that. So most of the compiler optimizations which are centered around performance are not very meaningful in that area. So I was looking into the code size optimizations. There are not many in LLVM. Also there is if you see number of optimization passes, the amount of activity on the code size optimizations are not as much function merging is an optimization which is used to merge similar functions so that we can gain in the terms of code size. It is an interprocessural optimization, so it works on one translation unit at a time.
00:01:11.540 - 00:02:01.570, Speaker A: In order to have the full power of function merging, we need to have this facility across the entire program that will help reduce code size and will gain in those applications which are not very sensitive to performance. This is the slide which kind of explains a very brief overview of how thin LTO works. I just stole this slide from presentation a few years back. Thank you Teresa for making this slide. I think this slide has been copied over and over in many places. This explains how thin LTO works. Thin LTO is a lightweight version of linktime optimization.
00:02:01.570 - 00:03:16.970, Speaker A: It is mostly parallel. There is only a very thin layer of serial step if we think in terms of the traditional compilation steps. The first stage is where the compiler takes C or C plus plus source code and parses it a bitcode. There is only a little bit of addition here that when the compiler knows that this is a thin LTO stage, it also generates some kind of summary which can be used during the parallel link time optimization stage, the summary is added to the bitcode itself and during the thin link layer stage it is a serial step. So the compiler parses the summary from each module and populates to a giant module summary index and that module summary index is globally available during the parallel code gen stage. This helps compiler take a very good advantage of the linktime optimization, but at the same time it does not suffer from the compilation time overhead. Compared to traditional linktime optimization, function merging is a code size optimization.
00:03:16.970 - 00:04:26.210, Speaker A: In LLVM there is already a function merging path which can be used to merge identical functions, but I use the other version of function merging which is in the open source but it has not been merged to the llvm trunk yet. It is useful for merging similar functions as well and hence it can give much better code size gains, especially for c and C plus plus based code base where we have a lot of duplicate copies for C plus plus standard library. If you take for example, when we call those C plus plus standard library algorithms for each type, they get instantiated in each translation unit. They are structurally identical in many cases. In some cases they have very minor differences like typecasting or something. They can be deduplicated into one and then gain in terms of code size. For example, this is a very simple example where we have two functions which are identical.
00:04:26.210 - 00:05:29.942, Speaker A: They are in different translation units. Traditional compiler optimization will not, or function merging will not detect this. But when we perform linktime optimization or thin LTO, what will happen is there will be only one copy of function and the two interfaces will just redirect to the one copy. We don't want to get rid of the interfaces because that will result in linktime error to port function merging to thin alto. It comprises of essentially four steps. The first step is to add hash code to function summary so how the function merging works is it computes some kind of hash for each function, and that hash is a measure of some kind of structural and semantic properties of the function, and that can be used to compare one function to other. Instead of comparing the entire function, we just compare their hash code.
00:05:29.942 - 00:06:20.454, Speaker A: If the hash code of two functions are same, that means they have some similar properties and they can be merged, potentially merged later on. This helps discard a lot of functions which are obviously not similar, and then we have bucket of similar functions and then we can do expensive comparison like instruction by instruction comparison there. So we save in terms of compile time in this case. The second step is to make merge function decision. Once we find buckets of functions which are potentially mergeable, we have to decide that which of the functions can be merged actually. So we do an instruction by instruction comparison. It is an n square algorithm, but on a much smaller n.
00:06:20.454 - 00:07:09.750, Speaker A: And then we decide that which are obviously the candidates. And once we have those candidates, we keep them in the bucket and discard other ones which are not mergeable. The third step is to set up those similar functions to be imported. What this means is during the serial thin LTO stage, we have to mark those functions that they can be imported so that during the parallel code gen stage they'll be moved from one module to the other one by the IR mover. During the linktime optimization, I'll talk about them in detail. And the last tip is to run function merging as part of thin LTO pass pipeline. So the first one is to add hash code to the function summary.
00:07:09.750 - 00:08:28.540, Speaker A: What hashing means is to get some kind of measure of structural as well as some semantic properties like number of basic blocks or hashing, the formal arguments of the function and the return type. Some of the semantic properties like calling convention or like if function has where argues few things like that. The idea is to keep the hashing fragile. That way we want to have a lot of matches and then we can compare them instruction by instruction. During the in the second step, once we compute the hash function sorry, hash of a function, then we want to add that hash code to bitcode during the parallel the first stage of the linktime optimization where bitcode has the function summary. So we have to change bitcode writer as well as reader because later in the parallel code gen bitcode reader will read the function summary. These are some of the pointers like how to find the places where we want to add the function summary like these are bitcode indices there.
00:08:28.540 - 00:09:22.140, Speaker A: The second step is to make merge function decisions. There are two important data structures here. The first one is a map of functions which are identical and the hash code of that function. The second data structure is the list of functions which are the hosting functions. What does it mean by hosting function? When we want to merge a collection of functions into one, what we want to do is move the IR of all those similar functions into one module, such that during the parallel code gen stage the compiler doesn't have to look into other threads. It can make decision separately just by looking having all the information available in one module. There can be a cost model to decide which function can be a hosting function.
00:09:22.140 - 00:09:48.638, Speaker A: But for now I just selected the first one. It can affect the performance. Actually, as we will see later, the third step was to set up similar functions to be imported. The entire logic is in function import CPP. It is a sequential interprocessural pass. It runs for each module. It works on the module summary index.
00:09:48.638 - 00:10:26.640, Speaker A: So this time, at this point of time we have not parsed the entire bitcode. That's why it is very cheap. But we want to be able to make the decision before the parallel code gen stage. The function import works uses IR mover, which is a data structure, and it just works. You just have to mark which of the functions are needed to be imported and that's about it. It works by moving the IR as well as the relevant declarations as well. So it's pretty modular in that sense.
00:10:26.640 - 00:11:13.280, Speaker A: And the last step is to run the function merging optimization during the thin LTO stage. So all we need to do here is add a couple of lines in the llvm passmanager and schedule the function merging after the inliner. Because inliner can undo the effects of function merging. That's about it. To have some kind of experimental measure to report here. I was looking for some of the open source projects and then I started compiling using thin LTO and realized it's not so trivial. There are quirks there of build systems of plugins and they can come into your way.
00:11:13.280 - 00:11:58.948, Speaker A: The first thing is on the Linux based system we need to use gold plugin. So we have to install a gold plugin Llvm gold shared object, and you have to have bin utils somewhere. The other problem is the gold plugin doesn't take like OS or oz code size optimization flags. For some reason, function merging is a code size optimization, and using OS and oz makes sense, but it was not possible. So what I did was I just used three for the results. The other parts are like the archiver and the Ran lib. We need to use Llvm AR and Llvm ranlib, and then we have to deal with the quirks of the build system.
00:11:58.948 - 00:13:00.970, Speaker A: Like for example, cmake is pretty simple to understand, but it adds link flags to the archiver. An archiver complains that I don't recognize FLT equals thin flag and it will just error out. So I had to add a wrapper llvmar sh flag, which will just remove the FLT equals thin flag and pass it to the remaining flags to the llvm archival. Yeah, it might be helpful to change the build system as well, but depending on what the use case, you may resort to these kind of small tricks. This is the experimental result on opencv libraries. Opencv is a library used for computer vision. It has lot of shared libraries, but I use the top seven for this result, except for one we gain in code size for almost all of them, as high as 9%.
00:13:00.970 - 00:13:57.364, Speaker A: The one where we lose code size is around like it's pretty small, but still a loss in the code size. It may be because of the fact that I'm using three, because during the three compilation stage optimization flag there might be inlining which is aggressive and like loop unrolling. Many other optimizations which are aggressive and quite oblivious to code size increase are enabled at three. Okay, this is the part which requires a lot of effort, like tuning. We set up the optimization flag, everything works. Then you do experimental results and sometimes we gain in terms of code size, but the performance can be too bad to be tolerable. So we need to tune some of the flags.
00:13:57.364 - 00:14:36.980, Speaker A: Function merging has some flags like how large functions you want to merge or what is the dissimilarity index. I did not write the function merging pass thanks to people Tobias especially. I just ported this to thin LTO. The other issues with function merging across linktime optimization is it'll create cross module dependencies. Let's say there are two modules which have functions which are identical, and they don't have any cross module callbacks. Now what will happen is after function merging, there is an edge and there is a call from one module to the other. It can lead to page faults and that can be harmful.
00:14:36.980 - 00:15:31.124, Speaker A: So there's one alternative is to have some kind of profile guided information to know which module is called first and then make it as a hosting module. As we saw a few slides back that can be useful. The other part is lot of C plus plus standard library algorithms have forced inline attributes, so even at oz or os compilation flag they will get inlined. And those functions, if they were not inlined, they would appear as identical functions. But now they are in the middle of some giant function and it is difficult to deduplicate them without having a complex algorithm. So these are the tuning that can be kept in the mind, and that's about it. These are the patches for review I'm getting already so much reviews from Teresa.
00:15:31.124 - 00:15:37.850, Speaker A: Thanks for feedback. I'll have to do a lot of work there. Questions.
00:15:47.250 - 00:16:36.650, Speaker B: I have two questions for you. First one, why is it desirable to have another merge function than the one we already have, instead of fixing the one we have? Like a lot of the stuff in the one you have is stuff merge funks should do. So it's easier to incrementally improve the existing merge fungs than to just land a big patch that's a fork of a prior version of merge funks. Second question I have that's kind of connected is the main problem merge funks has is it tends to diverge from IR. When new things are added to IR, it doesn't know about them. It's easy if it's a new type of IR node, but if it's a property in an existing IR node, we just don't know about it. Which makes merge funks merge things that are actually not the same and makes it diverge, especially if you're trying to do fuzzy matching.
00:16:36.650 - 00:16:48.900, Speaker B: It gets kind of tricky to do that properly with kind of merge funks having its own view of what IR is and IR evolving its own merry way without merge funks knowing about it. So how do you plan on fixing that?
00:16:49.670 - 00:17:29.370, Speaker A: Okay, your second question dried my throat because I could not understand, but it was very complicated, I think. But I'll answer the first question first. The current mud function is strictly not as good as the mud similar function. There are two alternatives. What we can do is we can improve the current one. That will require a large number of porting and reviews. The other way is to just completely replace it and just remove the previous one if it is strictly better and if there are no reviews, that there are no regressions.
00:17:29.370 - 00:18:06.444, Speaker A: So these are the two ways we can go about it. I guess based on what most people agree, I'll probably do either one of these. Second question was merge function makes some intelligent guesses about the IR and it may be not possible to have good matching there. I agree. I don't have any good answer for that one for now. Yeah.
00:18:06.482 - 00:18:44.372, Speaker C: Thanks DJ for this work. It's really exciting to see people building on top of merge similar functions. What I was wondering about when you described all the flow there, what you do if merging actually fails. So it seems like you're making the decision ahead of time. Yeah, ahead of time. What you're going to merge or what you could potentially merge, right, because it has the same hash and then you import those and so on. But what if it actually turns out that you can't merge them, right, because they turn out to be not isomorphic or because there are some instructions that can't be merged.
00:18:44.436 - 00:18:53.464, Speaker A: Yeah. So the body of the IR will remain in the new module. Now that's about it. But from a correctness point of view, nothing bad will happen.
00:18:53.662 - 00:18:55.240, Speaker C: Oh, so you just move the function.
00:18:55.310 - 00:19:07.840, Speaker A: To all the IRs we wanted to move during the thin LTO stage. We make decision before. Ahead of time, right? Yeah, in the serial stage. So we mark those IRS to be moved and they'll get moved. Yeah.
00:19:07.990 - 00:19:13.250, Speaker C: Okay. So you might end up with functions being in a different place, but I think so. No other.
00:19:17.220 - 00:19:46.684, Speaker D: So related to that. So I noticed in your patch how you're doing the importing part. So normally today when we do importing in thin lTO for inlining, we import it as like an available externally copy and you don't change really the source module where you're coming from. But it sounds like here you're actually moving it, so you're actually importing it. So what do you do for the original location? Do you mark it as like turning it into an available externally linkage or how do you drop it?
00:19:46.722 - 00:20:08.032, Speaker A: Yeah, I have changed the linkage flag to available externally. If I remember correctly, I did that for there is only one place which asked for the linkage flag and I changed it to available externally if it is in the module summary index. Yeah. So I think it'll have one copy will get deleted. I guess. The linker will probably take care of it. I don't know.
00:20:08.166 - 00:20:13.600, Speaker D: Yeah, available externally will get dropped. Okay, so that's a change. So you'll actually drop it from the original?
00:20:13.760 - 00:20:14.470, Speaker A: Yes.
00:20:21.600 - 00:20:22.910, Speaker B: Are there any other hands?
00:20:26.020 - 00:20:26.770, Speaker A: Sorry.
00:20:32.980 - 00:20:40.150, Speaker B: What's the impact on debug information and debug ability for this function merging, if there is any?
00:20:41.640 - 00:20:57.450, Speaker A: Yeah, I think it'll get spoiled. Yeah, any compiler optimization has a great potential of spoiling the debug information. I'm sorry.
00:21:01.020 - 00:21:12.750, Speaker C: When you do inlining, it's done. You're also combining stuff for multiple functions similar to that. I think people at Cisco have done some work.
00:21:12.900 - 00:21:13.600, Speaker A: Okay.
00:21:15.570 - 00:21:20.370, Speaker C: Yeah, he's here, but he's actually made some improvements.
00:21:22.470 - 00:21:30.420, Speaker A: Debugging. Okay, awesome. Okay, thanks.
00:21:35.970 - 00:21:41.760, Speaker C: I was just wondering about the impact on compile time that this optimization has.
00:21:44.610 - 00:22:13.740, Speaker A: I didn't include the compile time because when I measured the compile time it was better, probably because I had a lot of similar functions in OpenCV, but it can go worse as well. It can go better in many cases where because you have less functions to process in the first place, so less number of things to do in the parallel code gen, it can go worse because function merging does something. So yeah, it'll depend on different kind of benchmark, I guess.
00:22:21.200 - 00:22:54.600, Speaker B: Just to answer the question on compile time, my former intern, maybe four or five years ago did work on merge funks for benchmarking on Chrome and Firefox and some of the big code bases and improved compile time. If you do it early enough, you end up not touching all the STL stuff and whatever else. You just dedup a bunch of it and then you do the fuzzy matching later. Then you spend more time trying to do fuzzy matching and reducing things, but you've spent less time optimizing things, as he was saying. So if you go back a few years ago and look at Jason's posting on merge funks, you can see numbers back then for chrome.
00:22:55.900 - 00:23:07.410, Speaker A: Awesome. I think that's it. Awesome. Facebook is hiding.
