00:00:01.290 - 00:01:05.922, Speaker A: Okay, welcome everybody to this session. So let's see what's in there under the hood and LlvM test suite. So intros first. Who am I? My name is Umar Javed and I am a tool chain engineer specializing in debug technologies and I work for Lenau, which is a collaboration space for open source software and we do software development and have contributions in Linux kernel, Nuan, LLVM toolchain, and many more other projects. And today I've chosen this topic of LLvM testuite because we were trying to port this resource on Windows specifically and our project is Windows on ARm for which we are putting LLvM test suite. So let's get going. So let's talk a bit about the agenda for next 20 minutes.
00:01:05.922 - 00:02:34.320, Speaker A: I'm going to start with a bit of intro of LlvM test suite, what it is and why it exists what is different in LLvM test suite from various tests that we have within LLvM main repository? What is different in LLVM project tests? So then I'm going to talk about the tools and technologies that it uses. We'll talk about the directory structure and what's in each of these directories that LLM test suite repository has. Then I'll talk briefly about how lit and CMEC works to run this test suite. And I'm thankful to Peter Hozak who did this presentation before me to get you guys started on LLVM testing and build. So if anyone didn't attend that talk, maybe I recommend that after my talk, just whenever you get the time, do have a look at that talk as well. That was valuable information about how LLPM is built and how test suite might be using some of the same concepts which are in there in that talk. And lastly, I'll share my project with you, what we are currently doing and what we aim on doing.
00:02:34.320 - 00:03:09.574, Speaker A: So let's get going. Okay, so LLvM test suite is a separate repository. Many of us don't even know about it. It exists at LlvM test suite in GitHub and you can just download it. It houses a variety of tests. Some of them are whole programs. Real world programs have single or multiple files, then they are written in c, c plus plus, photron or LLvM bitcode.
00:03:09.574 - 00:04:29.186, Speaker A: There are benchmarks, regression tests, unit tests. It's a huge resource of various testing things that you can use to test ROVM, and most importantly it also supports importing the external test sources which you might not be able to do within the main LLVM repository, the LLVM project one. So what it does, it tests LLVM correctness, which is similar to what other tests within LLVM project does, but how it is different. It also benchmarks LLVM's performance and it can export the performance data to tools like LNT. I've not covered LNT in this talk, but it's a visualization tool for the performance data that you can sort of use to see LLVM's performance results over time. So whenever results are produced using LVM test, those are uploaded to LNT. And there's a web UI in LNT which can show you very nice metrics of various test results over time.
00:04:29.186 - 00:05:42.410, Speaker A: So you can sort of build these tests within various build configurations and with various different optimization levels and then visualize your results in LMG. So why have a different repository? Why don't we host these test suite within the same LLVM project repository? So one thing is like the normal LLVM tests are tightly coupled with the LLVM source code. They conform to the same licensing and coding standards, and they are primarily sort of focused on testing LLVM code. Like for example LLVM algorithms that we have, or LLVM data structures, or various different classes. For those we have unit tests. And then we test LLVM's tools like Clang, like LLD or LLDB for various sets of inputs and then validate those outputs. So this is mostly the purpose of the test that we host within the LLVM project repository.
00:05:42.410 - 00:06:41.034, Speaker A: On the other hand, LLVM test suite, it contains real world programs. You can pull in more real world code bases into this repository as well. And then you generate performance metrics, and those performance metrics are visualized on a web UI over time and see if LLVM is improving or deteriorating as far as the performance is concerned. And it can host variety of coding standards. Like for example within LLVM project repository, you just have to have to comply with the LLVM coding standards and LLVM licenses. But in LLVM test suite you can host a GPL code, or Apache licensed code, or various other licenses. So I was reading somewhere that it hosts crazily licensed codes.
00:06:41.034 - 00:07:58.040, Speaker A: So that's another difference. Then what? I asked people around why we have this separate repository. I got different answers about the clutter which was being created within the LLVM project repository and its size was increasing, the build times were increasing, and then there was a shift and these tests were removed and also allowed the expansion of these tests more projects from real world applications would be able to get into this and we can test them over time. We can change the cmake build system specifically for this resource and we don't have to worry about the rest of the LLVM project. Like if we are going to do something different with regards to how we are configuring these projects or how we are running the tests. Like for example running the tests under perf or running the test under utility that can time those tests for execution or runtime. That becomes quite easy as well.
00:07:58.040 - 00:09:22.802, Speaker A: That's pretty much it on the differences now let's talk about what kind of performance matrices this resource actually generates. So compilation correctness, that is an important aspect. Like real world programs should be able to compile with LLVM code size. Their code size should remain at least improve over time or do not deteriorate, and then compilation speed should improve or at least stay the same. Then linking speed similar to compilation speed, we also try to generate the linking speed of various different applications that we have within LLVM test suite, then output correctness so the LLVM project tests do a good job at checking the output correctness, but in the real world program scenario we do that exclusively in LLVM test suites. So we test the reference output against the output generated after the tests are built with clan or any other LLVM tool. So then we have the running time calculation as well.
00:09:22.802 - 00:10:34.710, Speaker A: There's a utility that calculates the running time of generated executables. We can also sort of have benchmark scores exported to LNT server. All these information are like in the form of text as a JSON report, and that can be exported to the LNT server where you can visualize these as a nicely moving graphical interface. It also generates perf reports for detailed reporting of performance data like cache misses or also the hardware performance with so this is how the directory structure looks like. If you see it is totally different from what we have in LLVM project. And I will go over all of these folders one by one. Okay, first up is the ABI test suite.
00:10:34.710 - 00:11:35.020, Speaker A: So it is a set of tests to test C titanium ABI, and those were contributed by PlayStation. And it is important for sort of checking the ABI conformance of LLVM tools over a period of time. So these tests are mostly sort of, they are not tightly coupled with the rest of the test suite. Rather they have their own runner, but there is a readme file within the folder which can just help you get started on them. Then we have the bitcode folder. There's a language called halide which has generated these bitcode benchmarks, and they can be won with the rest of the test suite to generate the benchmark and performance results. Then there's a folder called CT mark.
00:11:35.020 - 00:12:34.080, Speaker A: It's actually not an actual folder, it's more like a pseudo folder that is created just to house the sim links to some of the applications which are long running so they can help with testing the compile time. Then there's clang analyzer tests. They are also somewhat disjoint from the rest of the test suite, but they can be run in tandem with the clang analyzer tests. Then Photron Photron folder is a work in progress. It's been recently added, and we have gfortron test suite recently included in that. And then NIST test suite is included, and snap application which has a bunch of photron code has been included in this. Then there's a folder called LNT based.
00:12:34.080 - 00:13:55.670, Speaker A: So LNT based folder has Python modules, the scripts which help you run tests with LNT infrastructure. So it is also a bit disjoint from rest of the test suite, but it also has the readme if you want to write LNT based tests which are directly called and run by LNT, rather than being run by the cmake and lit infrastructure of LLvM test suite. Then there is external folder which has sort of configuration data and cmake files to import external resources. Like for example the benchmarks, the spec benchmarks, those are all licensed code. And if you don't have a license or you cannot distribute them as part of a particular repository as such. So we have a mechanism within the external where we have all the infrastructure in place, and you just have to provide a folder containing the benchmarks with the licenses that you have purchased, and it can then run the benchmarks from there. So the external folder is for those type of tests.
00:13:55.670 - 00:14:44.066, Speaker A: Okay, then there is single source folder. So I think this folder has a lot of tests. Most part of the tests that we have within the LLVM test suite are in this folder. It's called single source because all these tests are from one single source file, and some of them are sort of obtained from external sources. Some have been included when the LLVM development initially started and the lit and cmake based infrastructure was probably not in place. I don't have the full knowledge about the history down there. Then we have multi source folder, which is similar to the single source folder.
00:14:44.066 - 00:15:51.290, Speaker A: It has lots of real world applications which are formed of multiple source files. Okay, so then there is a micro benchmark folder which is the benchmarks based on Google Benchmark library. There's a tools folder which provides the tools that help LLVM test suite run. Two important tools are timer utility which actually encapsulates the compilation process as well as linking and execution process and calculates the time and generates a time file which has the time information in there. And then there's FP compare which compares the reference output against the output generated after the run. Okay, so now I'm going to talk about a bit about how a lit and cmake based test suite actually works. So lit is a test suite runner plus discovery tool which is very powerful.
00:15:51.290 - 00:17:09.122, Speaker A: It can generate good test results as well. Whenever you have tests in a repository within RVM, you have include file lit side cfg. Whenever you run a build with cmake that gets transformed into a lit side configuration file or lit side CfG py file. And that file actually helps us with doing the out of pre testing of the LLvM test suite or LLVM project. And you can put the placeholders, for example, if I move on to the next slide, their cmake test on the left side is the include file. It has the placeholders variables which when we run cmake gets transformed into the file on the right, and they have been populated with different information like whichever tools which are currently configured for the particular test suite or the source folder or the modules. For example, in this case we are extending lit.
00:17:09.122 - 00:18:09.450, Speaker A: So we have custom built modules as well, which we have included. And we can also put all the information about test source directory if a source runner is a shell based runner, or a Google test based runner, or a custom runner that you can include on your own. So for example, if you want to include your own runner, you can populate the field called test format. So once you do that, then lit will pull in that runner instead of pulling in the runner from its own directories. So if we look at these two files, these are the normal test files from within LLvM test suite hosted in LLVM project. These are all LLvM project based tests. So these are test files which are recognizable by lit.
00:18:09.450 - 00:19:02.774, Speaker A: The file on the left has one run command, the file on the right has two run commands. They run the command that is in the run field, and then they test the output using the file check utility. So with LLvM test suite it is different because we have real world applications and we don't have the test files in there. So we put in a reference output, and we have put in a mechanism to generate those test files within LRvM. So we have a cmake directory in the main source tree. It has the cmake modules which are one whenever a build is created. And that build actually generates the test files for us, which will be won by lit.
00:19:02.774 - 00:20:12.160, Speaker A: So for example, if we see after a build is run using cmake, we have lit test reference, output, timing and size files, local CFG and litcfg py. So it's a complete test suite which was not there before running those cmake modules. So the cmake folder hosts all those cmake modules, and cache's folder has the most frequently used. So the test format I've already talked about, the test suite is the extension, the Python extension. So there is a folder called lit support. It has all the python sources that are extending the LLVM lit infrastructure. It does the whole of integration with lit and gives us the timing and other data.
00:20:12.160 - 00:21:07.940, Speaker A: So lastly our mission. So we are sort of porting all this for windows and specifically for windows on Arm, and we are trying to generate the performance metrics similar to Linux. So we have a bunch of build bots already running these tests for Linux, and we are sort of going to do the same. So our future goals are to export the text coverage and port all these test applications. We already have a few sort of patches that sort of port this on windows. So yeah, there was a lot to cover in 20 minutes. So thank you all very much.
00:21:07.940 - 00:21:12.340, Speaker A: And if you have any questions, please let me know. Thank you.
00:21:17.270 - 00:21:23.480, Speaker B: Thanks. Omar. Let's start our q and a session. Please come to the mics if you have any questions.
00:21:34.850 - 00:22:07.850, Speaker C: I guess I have to ask, because I haven't looked recently, but can you tell us anything about what kind of support there is in the test suite for ignoring tests? So if you're not running on a POSiX system or something that provides that kind of interface for ignoring tests that may not be supported by your particular target, particular in maybe the larger applications may, if you have a system that doesn't have file system support or something like that, is there a way to disable to mark the set of tests?
00:22:08.350 - 00:23:33.334, Speaker A: Yeah, so I think within the cmake configuration it's a very extensive bit of modules, cmake modules which are written specifically for this purpose, to configure, like for example, you can configure just to run a single folder. For example, I talked about CT mark, so it's ten applications selected from within test suite to run, just to test the compile time so similar to that, if there are bunch of tests which are not supported on a particular platform, those can be excluded and you can create your own set of rules there. You can write your own module to exclude certain tests which are not supported. So right now what I'm doing for Windows is to select the tests which are easily portable and I mark them enabled for Windows. And rest of those I mark them disabled. And when we run the test suite on windows, majority of tests at this stage do not build because they require some bit of tweaking for Windows. So similar, if I'm answering your question correctly, I think you can do that.
00:23:33.334 - 00:24:09.940, Speaker A: But the amount of configuration that you'll need, I'm not 100% sure if you'll need to set just one configuration or you need to disable lots of different tests within the test suite. That totally depends on your platform. Basically it's POSIX based, so it's being run on Linux for years. So probably for other file systems you'll have to have a layer that actually translates POSIX calls to your specific file system or platform.
00:24:11.670 - 00:24:30.310, Speaker C: Okay, I guess I'd be interested if I know that we were talking about LibcxX yesterday that has a list of carve outs basically for features that aren't supported. I don't know if other people might be interested in having the same kind of capability in the test suite for particular features that aren't supported in the target.
00:24:31.290 - 00:25:14.882, Speaker D: I just had a comment that might be related. I don't know, I'll just mention this now. Sorry for cutting in line. One of the things, I don't know, tell me if this relates. One of the things we're trying to do right now is to sort of mandate that our build and test code is also our product code. So kind of holding our build and test code to the same bar that we hold our product code, and determining things like cycle time threshold for execution time that all tests must execute within a certain period of time. Determinism thresholds for indeterminate behavior all tests must be less than five nines of indeterminate and also cost thresholds.
00:25:14.882 - 00:25:58.660, Speaker D: So the cost of executing the test in terms of compute must be within a certain threshold. And so what we're trying to do is figure out what's the best pattern for basically denoting this within our existing test and cmake targets, so that we can kind of clearly delineate between things that meet our threshold and things that don't. So we can begin to kind of refactor things. So I don't know if that's like a similar pattern, but we were kind of looking at the tagging or the labeling capabilities of not supported or unsupported kind of similar to that. So yeah. Is there like a pattern that you would recommend that you think is standard or that has worked well?
00:25:59.050 - 00:26:51.350, Speaker A: Yeah, so I think there is a timing constraint. I think the best thing is to at least run as a standard the tests which are within the LLVM project repository. But if you want to build the real world programs or benchmarks, then you have to based on your use case, they have a selection of executables. Like I keep referring to the ct mark, that is a collection of tests from within the test suite which have been used to test the compile time efficiency. Similar to that, I think anyone who wants to have a configuration which can build sort of a specific set of tests for their particular criteria, I think that can be included within LLVM test suite infrastructure.
00:26:52.250 - 00:26:58.050, Speaker D: And you would do that as a config file or would you do that with labels or some combination?
00:26:58.130 - 00:27:13.100, Speaker A: Yeah, I think you'll have to do a new cmake configuration and that create a folder within Llvm test suite with the sim links to those executables you want to run.
00:27:13.630 - 00:27:52.662, Speaker E: Just a quick note, because I also worked in this area a lot in the past. Traditionally we just add more cmake configurations and you can use ifs in those files. So if you have a good topic to put stuff under, you create a variable for that topic. It's mostly used for enabling on platform X or disabled on platform Y. And we have already this strange topic of is a good benchmark or is only a correctness test. And if you have better categories there, by all means create more variables and more ifs inside those cmax files. I think that can scale for a while before we need more complicated sorry.
00:27:52.716 - 00:28:00.086, Speaker B: For the interruption folks, but we need to end the Q a session to start the next speaker. Let's thank Omar again. Thank you so much.
00:28:00.188 - 00:28:00.580, Speaker A: Thank you.
