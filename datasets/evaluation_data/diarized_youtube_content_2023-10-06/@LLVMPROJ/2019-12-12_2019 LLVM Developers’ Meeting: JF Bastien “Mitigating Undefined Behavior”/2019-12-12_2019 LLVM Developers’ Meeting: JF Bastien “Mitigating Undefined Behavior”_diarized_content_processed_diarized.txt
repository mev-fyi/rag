00:00:00.250 - 00:00:27.160, Speaker A: All right, thank you, Michael. So, my name is Jeff. I work at Apple. So today I'm going to talk about security mitigation through automatic variable initialization. So what do I mean by that? What does it mean? So, so let me explain a bit what I mean by automatic variable initialization. When I say automatic, I mean as in the auto keyword and not the c plus plus eleven auto keyword, but the auto storage specifier from pre C plus plus eleven. So these are storage specifiers that exist in C plus.
00:00:27.160 - 00:00:46.566, Speaker A: So it defines where things are. Roughly, auto means somewhere in the stack. Then there's static, external thread, local, and there's also the heap where you store things. Today I'm only talking about auto stuff. So anything in the stack is what I'm automatically initializing. When I say initialization, I mean all of them. So c plus plus has a bunch of types of initializations.
00:00:46.566 - 00:00:48.010, Speaker A: I'm talking about all of those today.
00:00:48.080 - 00:00:48.266, Speaker B: Right.
00:00:48.288 - 00:01:23.894, Speaker A: So anything that's on the stack that's initialized or uninitialized is what I'm going to scope this talk to today, and I'm going to talk about the undefined behavior that's inherent in those things. So here's an example of what I'm talking about. So you have the example function here. It has a buffer with a few things in the array, and then I'm going to use the buffer, right? So if I look at this code in isolation, I don't know whether there's undefined behavior or not. If you were to use the buffer in the use function without initializing it, so doing a read from it, that would be undefined behavior. That's what I'm trying to mitigate. How am I trying to mitigate it? Well, here's the rough idea of what I'm doing.
00:01:23.894 - 00:01:53.594, Speaker A: Compiler magic, right? So this is the entire point of what I'm talking about, is adding compiler magic to automatically put stuff in the curly braces. Roughly. Why do I want to do this? There's a lot of pushback whenever I bring up this idea of like, well, if you're going to initialize all the stack variables, people are going to come to rely on it. So if you say, for example, anything that you don't initialize is initialized to zero for you magically, then people are going to rely on it. It's not what the standard says should happen. People come to rely on it. Your code's not portable anymore.
00:01:53.594 - 00:02:16.246, Speaker A: That's kind of a problem, right. So it's kind of a distasteful thing to suggest, and that's the main feedback I get when I suggest this. So I'm going to explain why I think it's a good idea to do that. The core of this is that using initialized stack variable is undefined behavior. Now the standard says you can define undefined behavior. An implementation is allowed to give it semantics. So that's what I'm trying to do.
00:02:16.246 - 00:02:40.606, Speaker A: Why am I trying to do this? Well, usually when you use an uninitialized stack variable, this is what happens. You just get a bug. There's a stale value in a register on the stack and you just kind of use it and you read whatever was there before. What was there before? It's whatever the compiler decided to put there based on either register allocation or stack slot coalescing or something like that. It's just an old value that was there before. It might have spilled a value. Then you read it again.
00:02:40.606 - 00:03:09.942, Speaker A: That's what happens, right? That's usually the outcome of doing reading stale values from the stack. I'm kind of simplifying. There's a lot more outcome there, but that's the main outcome that you notice, and that leads to two types of bugs. The first type of bug is just an unexpected result. You run your code, read an initialized value, and then something weird happens. You debug it, or maybe you don't notice, and then maybe you have a graphic application, the triangle kind of flickers or something like that. It's just a little weird outcome.
00:03:09.942 - 00:03:44.786, Speaker A: Whatever, I don't really care about that. The worst case is an exploit. So if you have security sensitive code and you use a stale variable, stuff sometimes happens to be exploitable when you do that. And that's what I'm trying to mitigate. That's what I care about. What does it mean to have an exploit where it really means one of two things? In this case, you either leak a secret and that in and of itself is not an exploit, right? So say you're the kernel and you leak a secret. Okay, you could leak a password or a hash value or some secret related to cryptography or something.
00:03:44.786 - 00:04:37.230, Speaker A: That's bad, right? But it's not an exploit in and of itself, right? Like you can't take over a machine by leaking a secret most of the time. What you can do though is also get attacker controlled values into the stack. So imagine I can trick you into putting something into the stack because I look at the assembly, I trick you to call a function, put a value I gave you as an attacker onto the stack, and I trick you into calling that part of the code that has undefined behavior and using the value I left on the stack, that means that I can control your process and do something bad with it. So I'm going to go into examples of how that happens. But basically this is what I'm trying to mitigate, trying to prevent you from leaking secrets and using attacker controlled values. Is this a theoretical line of attack? Not really. If you look at the LVM thread where we talked about this, Kosia posted numbers in chrome.
00:04:37.230 - 00:05:05.834, Speaker A: Right? Now, if you look at the bug tracker in Chrome, there's over 900 users of uninitialized values of this type, right? Like in their list of exploits they fix, there's 900 of those. Again, he posted other numbers where it's been 12% of exploits on Android over time. Right. So this is not a theoretical thing. It doesn't come from me or my company, it comes from somewhere else. So maybe you should trust it more. Like, I'm not trying to pedal something here, right? Other people say like, yeah, that's actually a problem for us as well, right? So this is a real thing that I'm trying to mitigate.
00:05:05.834 - 00:06:02.222, Speaker A: So why am I trying to do this? So let's talk about leaking secrets. I'm going to show you some code that comes from an actual CVE. So an actual bug that was fixed in production code from a kernel that I won't mention, but I've simplified the example a bit. But this is an exploit that exists or a Secret leaked that existed in actual production code that you probably have on one device that you run. If you look at this code here, there's the memory that's initialized here, right? So it's on the stack, it has a certain size, and you can leak that memory because if you do get hardware address and that doesn't fill the entire buffer, you still copy the entire value outside of the kernel. So this happens to be kernel code, and it would leak whatever was at the end of that array to user code. Right? Now you can see why the kernel telling users what's in its memory is a bad idea, right? That's something that was fixed a while ago in an actual piece of code.
00:06:02.222 - 00:06:35.958, Speaker A: What can that actually leak? Right, so I'm leaking things. What can leak? Well, so you can leak ASLR information. So address space layout randomization. So if an attacker can know where data is in the kernel, for example, if a pointer happens to be leaked, then if you have an arbitrary write to the kernel somewhere else as an exploit, knowing where things are through. SLR is a pretty good way to get a reproducible exploit. So that's a big problem. You can do things like rops or return oriented programming or things like that through that type of leak.
00:06:35.958 - 00:07:04.530, Speaker A: And when I give an example here of an array, it's not just arrays that can leak secrets. You can have an integer that's initialized that leaks like an integer's worth of stuff. But you could also leak stuff in padding, in a struct, right? So see, you have a struct with a car and an int, there's like two bytes of padding in the middle. That's enough to leak some amount of interesting information sometimes, right? So when I'm talking about leaking things, it's not just arrays, it's a bunch of different things on the stack. That's one example about leaking secrets. That's bad. I'm trying to prevent that.
00:07:04.530 - 00:07:37.006, Speaker A: The other thing that I care about the most is attacker controlled values. What does that look like? Well, again, here's an example from an actual exploit that was fixed a while ago in some production code, simplified it a lot. So it looks kind of obvious, but the code was basically like, I have a thing that's a backlog, and if the state is idle, I'm going to initialize it as something otherwise, and then afterwards I'm going to use it. If you look at this straight line code, obviously there's a bug, right? Obviously this is uninitialized. The value is usually true. So when I test it, it never fires. But then oops, I use the value.
00:07:37.006 - 00:08:16.390, Speaker A: So when I say like, you can trick an attacker to call a function you don't want, well, imagine I can put a pointer into the backlog variable, right? Because I can get you to call a function, I put a pointer there and then I get you to call this. Then it means I get you to call on any pointer the complete method here, and I get you to do whatever I want. So basically I trick you into executing a function with a bad pointer that leads to exploits directly. So that's what we're trying to prevent. So when I show these examples, people are like, oh, okay, so it's an actual problem. I don't like the semantic implications. Couldn't you just, that's usually the next part of the conversation.
00:08:16.390 - 00:09:18.826, Speaker A: Couldn't you just do something like use memory sanitizer or valgrint? Well, you could, but that requires recompiling everything in the process with memory sanitizer if you do it, or running valgrint you can't deploy that in production, right? Like memory sanitizer is not a thing you can ship on a laptop or a phone, right? So it only protects what you find on your testing environment. And finally, it has a three x slowdown. It's really memory hungry, right? So you should use those tools, but it doesn't actually prevent the problem. We know that there are exploits in the wild for this type of stuff. Some of these code bases use these tools, yet they still have these types of bugs, right? So sure, you should use those tools a bit more, but it doesn't actually fix the or it doesn't prevent the problem from happening knowing that that bug is already in the code. So then the next question is, couldn't you just test or fuzz or do code reviews or just write perfect code? Well, we are, except the perfect code part, but people are testing, they're fuzzing, and they're doing code reviews, but it's not sufficient. Again, we still see those bugs in the wild, so we still want to mitigate them somehow.
00:09:18.826 - 00:09:58.422, Speaker A: And the reason we want to mitigate it is not that we're not testing and fuzzing and code reviewing enough is that attackers tend to find bugs that programmers don't think of and that pretty often fuzzers don't find. Fuzzers are really good, code reviewers are usually pretty good, but attackers are kind of clever, and they have a different mindset on the type of things they find. Right. So that's kind of the problem that we're encountering here, that these techniques, except writing perfect code, aren't sufficient to preventing the bugs from happening at all. Right. Now the other thing is, couldn't you just go in your code, throw w uninitialized, and fix all of them? You could, you could do that, right? That would actually fix the issue. But I don't really like that approach.
00:09:58.422 - 00:10:39.670, Speaker A: Why? Well, first, it's a lot of code to change, and we could write a clang, tidy thing goes and fixes it everywhere. I'm not a fan of doing that, because once you initialize stuff, right? So you have int a equals zero, I, as a reader of your code, think that you've really thought about initializing a to zero, right? It looks like you had an intent when you wrote that. Whereas if I see an uninitialized value int a, I know that you haven't thought about what the initial value is. I assume you're going to initialize it later. But if you write int a equals zero, probably doesn't make sense here. Right, so you've lost the semantic of what you are trying to say. If you just go and initialize everything, so that's kind of a bad outcome.
00:10:39.670 - 00:11:04.690, Speaker A: If you look at w initialize, it'll catch everything, right. It has a lot of false positive and false negatives. It's not great. We could make it better by using something like a clang IR or something like that, but even then it still has some issues of that type. And same thing with static analysis. You should use static analysis to try to find places where you have initialized variables that are being used. But I don't think it'll fix all the cases that we're trying to prevent.
00:11:04.690 - 00:11:46.042, Speaker A: And there's another idea that people throw at me, which is using something like definitive initialization. It's, a few programming languages have that. Could we use that? Well, what does it do first of all? Well, it says if the compiler can't prove that you've initialized a value, then you have to initialize it. That's the basic idea as a programming language rule for what definitive initialization does. Could we do that? Well, it's not what C plus plus does today, right? So we could do that in c. It's kind of difficult because that's not the type of thing C usually specifies in that way, but it's an ongoing conversation that we have in the committee, so I think we might do that. But it's not there today, and I've got bugs today I'm trying to prevent.
00:11:46.042 - 00:12:17.986, Speaker A: Right. So I hope I've convinced you that maybe this is something we want to do. Right. So now that I say, okay, let's do this, automatic variable initialization, what are we actually going to do? How am I going to do this? Well, so I committed a patch to do this, I think, in December of last year. And here's the basic approach that it takes. So the real important thing here is I want to keep it simple and I want to keep it small. The reason is I'm affecting behavior that you as a programmer shouldn't notice.
00:12:17.986 - 00:12:42.618, Speaker A: Right. It's a guardrail against undefined behavior. And so it's kind of hard to test from a C program that the mitigation I've implemented actually does the right thing. Right. So if clang starts mitigating stuff and as the code evolves it stops mitigating it, I don't think anyone will notice. That's kind of a problem, right. So if we keep the code change kind of small and simple, it makes it easier to make sure that as clang evolves.
00:12:42.618 - 00:12:59.310, Speaker A: The mitigation doesn't just kind of go away, right? So I want to keep it as unintrusive as possible inside of clang. So here's what I did before committing the patch sending an RC or anything. I went and I changed cg deckle and its emit auto varnit function to handle more things in a more central manner.
00:12:59.390 - 00:12:59.586, Speaker B: Right.
00:12:59.608 - 00:13:35.262, Speaker A: So the idea there is the patch that I sent on later on was really, really small because it only changed a small amount of code. And the way I did that is I centralized everything and afterwards made the change to automatically initialize things. What it does in clang is it initializes every single user declaration. So anytime there's this declaration that clang sees, it initializes it. If you say int a equals zero, it'll go in, initialize it, and then set it to zero, right. It's really small, really simple, not intelligent at all. And that's a fundamental property of how I implemented this mitigation.
00:13:35.262 - 00:14:33.314, Speaker A: It's purposefully not smart, so that it's hard to get wrong. And how do you reduce the overheads? Well, you rely on a sufficiently smart optimizer, which I guess LVM is, but that's a fundamental trade off that I made in implementing this mitigation. Trying to make it really small and then having the optimizer be the smart one in the room. Here's the detail of what it actually does. So it looks at a variable that you initialize on the stack and says, is it a scalar? If it's a scalar, is it an integer? If it is an integer, I'm just going to repeat a bunch of a nibbles in there, right? So aaa if it's a pointer, I'll do the same thing. Why do I do that? Well, integers like the repeated a is not particularly useful, and in a backtrace it's probably something you're going to notice for pointers that's never mapped in any address space I know of on 64 platforms, so it's guaranteed to trap. If you use that pointer for fooling point, it repeats the f pattern, which happens to be a negative nan, so that's kind of nice.
00:14:33.314 - 00:14:47.266, Speaker A: Nans propagate and so it's kind of a neat value to use. And if it's an aggregate, instead of being a scalar, well, it's just recurse in there. So aggregates eventually boil down to scalars and so it just recurses in there. Don't forget to initialize the padding.
00:14:47.298 - 00:14:47.494, Speaker B: Right.
00:14:47.532 - 00:15:23.278, Speaker A: Because remember, I don't want to have leaks inside of padding. So I always initialize the padding if it's a union. Well, you don't necessarily want to initialize the initial member of a union, right? So if you have a union with a car and an int, you want to initialize the int because that's the biggest thing, right? So basically, initialize the padding in the union, try and initialize all of it, and then you have a bunch of weird things that you have to handle. If you have variable length array, you got to do that. If you have objective c blocks or the c block extension, you want to initialize those properly. And then if you want, we can also optionally zero initialize thing instead of having a's and s in there. So that's an option that is in the patch.
00:15:23.278 - 00:15:48.790, Speaker A: So the discussion then is, okay, you have pattern it and zero knit. What's the trade offs? Why would you want to have one versus the other? Well, both of them will cause a crash if it's a pointer, right? If you try to dereference zero on most platforms, they'll crash. If you try to dereference address all a's, they'll crash. Cool. Both of them are like on par here. However, if you use pattern, then you have a back trace. It's easier to see, like, there's all a's in the register.
00:15:48.790 - 00:16:19.374, Speaker A: That's probably a smoking gun, right? So that's kind of neat. Ff on floating point propagates. So, like, if you see a nan somewhere, you know something went wrong somewhere, and you just kind of try to backtrack. The nan pattern is unlikely to be relied on as language semantics. Most people are afraid of all zero values being used, right? So if you have int a, people are going to start relying on int a being equal to zero. We don't really want that pattern doesn't have that problem because all a is not particularly useful. We could change the value to be all b's or something like that, to just randomly be a pattern.
00:16:19.374 - 00:16:44.090, Speaker A: Instead, there's a bunch of stuff we could do with pattern in it to make people not rely on that value. And then zero, however, is a better value for a size. Imagine I'm trying to mitigate, like, an attacker controlled thing, and I have an array and a size, and it happens. I forgot to initialize the size. Well, if you're an attacker and you have a size of zero, that doesn't get you very far, right? Because you're trying to say access into the array. There's a bounce check. I forgot to initialize size.
00:16:44.090 - 00:17:09.634, Speaker A: If I have zero, I can't go outside of zero. That's fine. If I have all a's, that's like open season for accessing your entire address space. So maybe pattern is not that great for size, but has other upsides. The other thing that's really critical here is that zeronet generates better code. It happens that both for general purpose registers as well as for floating point registers or SIMD registers, it's way easier to generate zeros in all cases. Sometimes you have a zero register.
00:17:09.634 - 00:17:53.694, Speaker A: Sometimes just xoring a thing with itself gives you zero. It's easier to generate zeros than repeated byte patterns. But it's important to have byte patterns instead of just random things because that still generates better code, right? So the idea of having zero there is, it's kind of a baseline of like how much am I paying for pattern? I want to be able to compare the cost of that mitigation against itself if I use zero. And finally, in a lot of code, you want to be able to opt out of stuff, right? So you'll look at code and you'll convince yourself, well, the optimizer is not smart enough, so I want to opt out that part of the code from being initialized. You can spell it out now, so you keep the semantics. Either use the C style C plus plus style attribute, you can say this thing is uninitialized on purpose. And when you see that, well, first you can grab for it, and then you can do git blame and figure out who was convinced that the value was uninitialized.
00:17:53.694 - 00:18:15.434, Speaker A: When you eventually find an exploit in that code, it's pretty cool, right? And who did the code reviews and whatever. Remember writing perfect code, right? That's what's going to happen, right? Okay, so how does Clang and optimizer work to make that pretty efficient? So here's the workflow that the compiler goes through. First, the front end comes in and it goes in, it says, is that just a single store?
00:18:15.472 - 00:18:15.626, Speaker B: Right?
00:18:15.648 - 00:18:52.118, Speaker A: So you have the stack value size initialized it. Is it just a single store, like an int? Then I'll just do a single store. If it's not a single store, Clang says, can I just do b zero? So repeated like store of zero and then a handful of stores. It has some heuristic for what's a handful of stores. Can I do that? If that doesn't work, it'll say, well, is it a repeated byte pattern suitable for memset? If you can do that it'll do that. So you have a big struct, and it has like four integers or 100 integers. It'll just do a mem set of that struct, right? Does that make sense? And finally, is it an aggregate that's small enough for a handful of stores? So a struct with two integers in it, can I just do two stores? It'll try to do that.
00:18:52.118 - 00:19:28.654, Speaker A: And finally, clang, if it doesn't know what to do, it'll go in and just mem copy from a global. So it'll synthesize a global with right values in the constant section, and mem copy that to the stack. So the reason this is done this way is that's how Clang has worked for a long time. And the heuristics of optimizations and code gen are kind of tuned together, right? So I didn't want to be too disruptive in the way stuff is done. It's a bit weird to like, basically, these five steps are kind of an optimization inside of clang. It does mean that the code gen you get at zero is better, and it does mean that the optimizer has less work to do. Right, but I've basically taught clang how to optimize stuff a bit.
00:19:28.654 - 00:19:53.722, Speaker A: But it's been like that for a while, so I don't want to make it too smart. Right. Then in the middle lens, the optimizer comes in, and the main thing it does is dead store elimination, which it does kind of a bunch of times, try to eliminate all dead stores that can prove are dead. And finally, it tries to promote some globals as well. So if you did a mem copy from global, it tries to promote some of them. And then it also has a thing called mem copy optimizer, which tries to aggregate stuff or transform things or whatever. It's kind of clever, it does some good stuff.
00:19:53.722 - 00:20:24.014, Speaker A: And then the back end is where the real magic happens. So on next, 86 or arm, like, the instructions are completely different because it's for NYSA, and each back end knows how to generate its things really well. So how to synthesize zero, how to synthesize a pattern, how do you handle alignment other stuff? So, backends do a lot of the magic there. And if you want to make variable auto in it fast, backends is where most of the fun is. So let's look at some of the optimizations that we did. And when I say optimizations, a lot of this was my work, but most of the optimizations weren't done by me.
00:20:24.052 - 00:20:24.158, Speaker B: Right.
00:20:24.164 - 00:20:48.346, Speaker A: So what I did is I looked this stuff and I filed bugs. And then a bunch of people in my team and other people in the open source community fixed them. So I have a few examples of that. And the way I approached it is I looked at a code base. So I looked at a code base that's quite small. So the secure enclave on our phones, which is a small code base, and I looked at it with and without auto in it, and then I diffed the entire assembly, all of it. So it's not that much code.
00:20:48.346 - 00:21:09.730, Speaker A: I was able to diff all of it. And the optimizer in my head started running and said, there's green stuff here, more code being generated. Should the optimizer remove it? The optimizer in my head might say yes. So if it said yes, I filed a bug, right? That was my basic approach of how to do that. Well, then you have to fix the bugs too. That's the annoying part. Here's a few examples.
00:21:09.730 - 00:21:27.074, Speaker A: I have references for them if you want to go in. I won't give you too many details, but basically this is like an example of what the auto net code looks like to the optimizer, right? And it used to do a mem copy from a global for this, which isn't quite great. Instead you could just use memset.
00:21:27.122 - 00:21:27.286, Speaker B: Right?
00:21:27.308 - 00:21:49.262, Speaker A: So I just taught the optimizer to do memset. Cool. Here's another example. There's duplication between how clang figured out that something was a repeated memset and how LVm did it. And they both have functions with similar names that did similar but slightly different things. So I unified them into Lvm so that they both did the same thing. So, client does some amount of optimization and Lvm does the same thing.
00:21:49.262 - 00:22:13.862, Speaker A: Cool. So another one was reusing constants. So someone in our team did this. So the code here used to generate code like this. So it used to move a value to w eight and then a value to x nine. So that's a 32 bit register and a 64 bit register, both general purpose registers. If you look at the byte values of these, they're exactly the same, right? So the value in x nine is the same as w eight, but with zeros on top.
00:22:13.862 - 00:22:22.570, Speaker A: And it happens that you can just use the value in x nine as the same thing as w eight. So x nine and w nine contain the same value.
00:22:22.640 - 00:22:22.826, Speaker B: Right.
00:22:22.848 - 00:22:36.842, Speaker A: So you can just not generate the same immediate twice. And what's interesting, when you look at these types of optimizations, is usually, in optimizations, counting the number of instructions is not the way you get bigger gains.
00:22:36.906 - 00:22:37.134, Speaker B: Right.
00:22:37.172 - 00:23:02.438, Speaker A: Like, most of the time, that optimization doesn't pay off. I removed one instruction, like, big deal, but for variable auto knit, well, there's a lot of stack variables, so it ended up generating a lot of code. Right. And that code is kind of in a critical path. And so it happened to be that most of the cost for variable auto in, it was just like extra instructions. So it was kind of cool to do because you could isolate all of them and just kind of fix them one at a time. So here's another example.
00:23:02.438 - 00:23:35.826, Speaker A: It used to be that on arm, the code gen looked like this. And what's interesting here is there's a bunch of store pairs, which is good. Like, you're doing two stores of a thing to the stack, but it happens to be that there's better instructions to do the same thing. So you have vector store pairs that store twice as much stuff in one instruction. Now, they don't necessarily execute any faster, but it's way less code, so you end up hitting your icache way less, you end up doing fewer things, dispatching more things at a time, and this ends up paying off quite a bit and having a smaller code size impact. Okay, cool. Here's another example.
00:23:35.826 - 00:24:01.846, Speaker A: Used to be double sine extends in a store. So we removed the double sine extend, fairly simple thing. Loop unrolling wasn't very smart. So the unroller didn't know that this loop was a counted loop that did things. And so it was like, oh, I got to initialize the array, and now I'm going to go through this really complex loop. So, the loop unroller wasn't smart enough to realize that the loop was initializing the array, so all the stores were dead. So, that's a few example of optimizations.
00:24:01.846 - 00:24:31.010, Speaker A: There's a bunch more that went in. So, what's the size and performance impact? These are the two important metrics for this mitigation. Well, the previous research that I looked at when I started this work cited, like, 2.7 to 4.5% performance cost, which is kind of expensive. I don't want to pay that much. So, what I did a while ago, when we were trying to finally finalize the deployment, or whatever, is, I looked at XNU, which is the kernel that runs iOS and macOS, and it's a large C code base.
00:24:31.010 - 00:24:53.542, Speaker A: I looked at it on arm 64, and we do pattern it. I measured pattern it at Os, and the size regression was 1.3% after a bunch of the optimizations went in. So that's an acceptable number. And I happened to be able to remove, like, 2% of Xnu's size through some other optimization that had nothing to do with pattern in it. So they were happy anyways. And then we measured performance.
00:24:53.542 - 00:25:21.134, Speaker A: Now, what's interesting is we saw really no performance impact. When I say no performance impact, it's on the parts of the kernel that matter the most. So there's performance impact in token places, but we didn't measure any performance impact in the places where it mattered. So we were automatically protecting things without paying any real cause that mattered that much for it. But we're not the only ones who did that. So we deployed in a bunch of other places than just the kernel, but other people are looking at it. So Kosia posted numbers from Google where it's in bold.
00:25:21.134 - 00:25:34.360, Speaker A: He used a huge x 86 64 binary at two. So I'm just quoting him here. So it's huge. And they saw 1.6 size regression for pattern and 0.6% regression for zero knit. So it's a pretty big difference.
00:25:34.360 - 00:25:51.562, Speaker A: Kind of interesting. And then they saw a 1.2% performance impact on pattern it and a 0.9% progression on zero knit, which is amazing. Like, this mitigation is making your code 1% faster. I have no idea why. Right.
00:25:51.562 - 00:26:04.142, Speaker A: But I'll take it. It's cool. Okay. So there's a plenty of room for improvement there. I'll show you quickly some future work that we might do. There's some bugs that were filed. This one I never checked in because I haven't really finalized it.
00:26:04.142 - 00:26:15.262, Speaker A: Can look at the thing. Maybe someone wants to pick it up. There's stuff we could do there. There's a bunch of other things. Like here we have some duplicate stuff that we really shouldn't have. It's kind of silly. There's, like, interpretation optimization.
00:26:15.262 - 00:26:23.794, Speaker A: That one, I think, is not that hard to implement. Just someone needs to do it. Teach lvm how to handle stores through functions.
00:26:23.842 - 00:26:24.006, Speaker B: Right.
00:26:24.028 - 00:26:46.586, Speaker A: So saying seeing like, a function is going to store a thing, have an attribute that says that. Then there's like, memset s things like that. We don't eliminate stores to that. There's like, a bunch of dead stores like this code. Come on. How do we not optimize this? There's a thing that's uninitialized, and it's only using in a basic block, and we can't figure out that this is always initialized. That's kind of silly.
00:26:46.586 - 00:27:02.754, Speaker A: We should fix it. Then there's, like, double initialize a volatile variables that actually shows up in a lot of embedded low level code. Whatever. Okay, final thing. I want to talk about how to use it. It's actually quite straightforward. In your command line navigation, you do this, and then that's it.
00:27:02.754 - 00:27:19.500, Speaker A: Your code is kind of magically protected from uninitialized variables on the stack. So the end of my talk. Thank you. If there are any questions, if you could come up to the mic.
00:27:22.190 - 00:27:39.598, Speaker B: That'S interesting. You mentioned zero in it or pattern in it, and you said for pointers. Both calls a seg fault, but you didn't talk about zero. Looks like a null pointer. So it's going to suddenly your if null pointer checks suddenly do sensible things.
00:27:39.684 - 00:27:45.130, Speaker A: Yeah, definitely. That's one problem that I didn't mention that is worth considering if you choose zero in it.
00:27:45.140 - 00:27:45.266, Speaker B: Right.
00:27:45.288 - 00:27:52.674, Speaker A: So if you have a pointer and it's auto initiated to zero, then it looks like a valid thing. So, yeah, that's definitely a problem.
00:27:52.792 - 00:28:00.866, Speaker B: Okay, speculation on the progression. I wonder if it's prefetching cache lines, causing the Google progression.
00:28:00.978 - 00:28:24.340, Speaker A: I mean, it's not my code base. I don't know. You have to talk to Kosia about it. Anyone else? Thank you very much. You're welcome, Kosia. All right, thank you, everyone.
