00:00:00.250 - 00:00:48.246, Speaker A: Thank you. I'm going to very quickly introduce the parallel standard template library and say why? It's both interesting to us and you guys need to get involved. So what is it? Most of us probably know about the standard template library? It's a way of expressing, as someone was saying earlier, if you use all the modern features of C Plus plus, it's almost as good as py on, right. And the standard template library is part of that. Right. It gives you iterators over things, all of those, that kind of stuff. The parallel standard template library is now in c plus plus 17, and it's a way of exposing to the template library that the operations that you're performing can be parallelized.
00:00:48.246 - 00:01:19.270, Speaker A: So it adds an additional argument template argument on the front of the argument list. We'll have an example on the next slide. There are some complexity in terms of the different ways that you can parallelize things, which is what's captured on the right here. So the par says you can run this on multiple threads. The VEC policy says you could vectorize it. Par unsequenced says. And the unsequenced here says you can symdize it, which is kind of close to vectorization.
00:01:19.270 - 00:02:19.574, Speaker A: And there are issues here with whether you preserve forward dependencies within your loops, which introduces some restrictions on exactly what sets of parallelism you can have. So unseek is on track for the next c plus plus standard as of the last meeting. And the vector policies, they're going to the technical spec v two, which is going for an ISO vote. So here's an example of using it. If you take out what's turned out sort of muddy orange background color, then you have a fairly normal kind of way of doing a for each to apply some operation on this image, multiply it by this gamma or power it, whatever. And so all of this stuff here is you do a for each from the beginning to the end, you apply the lambda function and you transform from the beginning to the end of the row, applying the power function and assigning. So that's all fine.
00:02:19.574 - 00:03:01.414, Speaker A: We can, however, execute it in parallel. And the way we do that is we include the execution policies. We use the namespace, and we tell the far reach that it can go parallel and the transform that it can go unsequenced. And the effect of that, we've used two different parallelization techniques there. We've said use threads to handle the outer parallelism and use vectorization on the inner stuff. So we're achieving our thread outer vectorize inner, which is a fairly standard model of how you should parallelize on cpus. At least the versions of the things that are used is determined by use of the execution policy type, which is what you're passing in here.
00:03:01.414 - 00:03:50.918, Speaker A: And so you can see here that it gets passed into there. Of course you can lie, you can say vectorize this and it's not vectorizable, and if you do that, then it's not going to perform well. Why would you want to use it? Well, maybe you don't, but why would users want to use it? Which is what really matters to us, because it's standard, right? It's in C plus plus 17, so it's standard and portable, should be portable between different machines, different environments. It's in the standard. It's relatively simple. At least already you can express vectorization and the thread parallelism. There's compile time dispatch, so you're making the choice and generating the code appropriately at compile time.
00:03:50.918 - 00:04:21.934, Speaker A: And if you don't lie too much, then it's going to be correct. It's scalable and composable. Intel has an implementation, we're putting it out into the public domain, it's already there. The aim is that this is adopted by different areas. And of course LLVM is an area where we want it to be adopted. We use depending on the things. This is all quite complicated, but basically you can generate four different versions of things you can use.
00:04:21.934 - 00:04:47.000, Speaker A: Threading building blocks is the default backend. We come out via openmp SIMD to generate SIMD code. So this is also standard stuff. It's in GitHub, you can contribute, intel is already contributing. We want the LLVM community to be involved. GCC is ahead of us. GCC has started to adopt this and is working on putting this into Lib C.
00:04:47.000 - 00:05:10.060, Speaker A: And the idea is that intel thing is upstream of both of them, and we want to maintain a common code base and to be sharing. So we want your help in this intel. I can't do everything that's me, so contact us here and we want your help. Thank you.
