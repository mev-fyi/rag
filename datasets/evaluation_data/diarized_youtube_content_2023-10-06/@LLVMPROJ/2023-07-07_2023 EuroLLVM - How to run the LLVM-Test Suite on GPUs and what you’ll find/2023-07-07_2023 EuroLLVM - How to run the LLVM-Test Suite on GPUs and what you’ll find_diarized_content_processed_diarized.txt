00:00:01.290 - 00:00:29.346, Speaker A: Okay, last year. Last year L. A. Vmdev, I ranted a lot about, you know, GPU test coverage, CI and so on. And so, you know, I took some of my own medicine and actually went ahead and figured, okay, let's improve test coverage of our GPU backends and all of our GPU handling and so on, so forth. So what do we use for testing? We use the test suite. So let's run the Lovm test suite on GPUs, right? Easy peasy.
00:00:29.346 - 00:01:15.926, Speaker A: So all of this works with direct GPU compilation. Last year at Lovmdev Chile, who did all the work I'm just presenting, presented about this and people were not super into it, but I think it was just miscommunicated. So what happens is you take a legacy CPU code, whatever you have any code, right, with quotes, you take a legacy CCPU code and you compile it with our modified client. And what falls out is an executable that will execute your legacy ccpu code on the GPU entirely. It will call main on the GPU. That's what happens. How does it work? There's three components to it I will show here, and the third one you have to talk to me.
00:01:15.926 - 00:01:47.950, Speaker A: After I show the main wrapper, I show the user wrapper. Those are effectively source files that we include into each application. Into each translation unit we have a parse ellipse. All of that doesn't really matter. What matters is we have the RPC thread and offload library there. Effectively, whenever we encounter an external function call inside of your program, think of your program running in like it's compiled in LTO mode, monotholytic LTO. So we see everything.
00:01:47.950 - 00:02:40.030, Speaker A: If there is an external function call, let's say to a function called like f scanf that takes a file pointer, a format string, and a couple of other pointers where it writes into. We will take that call and we will replace it with effectively an RPC call to the host and then execute f scanf, a variatic call on the host, filling all those values in and copying the results back. So we translate all external calls into RPCs to the host, move all the associated objects to the host, run it there, move all the objects back. That's what happens. Okay, so this is the user header. So we kind of pre include this into every file just for good measure, ineffectively. What it does is it just renames your main into user main.
00:02:40.030 - 00:03:18.758, Speaker A: It's all it does, doesn't do anything else. And then we have this file that we compile and then link into your application. This is effectively the starter code that will start a kernel on the GPU and then execute user main. Pretty much all it does. It maps the arguments such that it can access the arguments on the device, but that's pretty much it. So with those two things together, and the automatic handling of external function calls, you can now execute arbitrary CPU code on your GPU. Why would we do that? We did that to multiple reasons.
00:03:18.758 - 00:03:58.130, Speaker A: Initially, we did this to actually do scaling studies of parallelism on a GPU. So we have parallelism on the host, and before we port everything to the GPU, do all the memory transfers and so on, so forth. We want to know if our algorithms are good, if they're going to scale properly on a GPU, or if we have to change the layout, like data layout, the algorithm, and so on and so forth. So here I show like two proxy applications for science. And what you see, the left bar is a manually offloaded version of it. And the right bar, like it's always like groups of two. The right bar is then the automatically offloaded host version.
00:03:58.130 - 00:04:32.034, Speaker A: And what we measure here is how well does it scale? Like, what is the performance improvement compared to the host? And what I'm trying to show is that it kind of matches. So the offloaded, like the automatic offloaded version, matches the hand offloaded version, which is good. That's kind of what we want to see. And what you also see is that in these apps, there are two different methods to do computation. One is called event based, one is called history based, don't ask me, it's some science stuff. And then the event based one was ported by hand to a GPU. The history one was not.
00:04:32.034 - 00:05:29.298, Speaker A: But we can still predict how well the history one is going to perform on a GPU by just taking the host code and running it on a GPU, and then seeing how that scales. And it turns out, you see, for Isbench, for example, that kind of performs the same for Xsbench two, we know that while this suggests history based method is reasonable for the GPU, we know it doesn't scale out if you increase the problem size. But that's a different story. Okay. And then we did another study here where we just looked at the parallel regions in the host code and compared their performance relative to scaling that parallel region onto the entire GPU. So how would that scale on a GPU? And what turns out is that we can match the manually offloaded version so we can accurately predict how well a code will execute on a GPU, like a host code. Okay, now let's go back to the LM test suite.
00:05:29.298 - 00:06:10.690, Speaker A: This is what I kind of promised, right? So this is like the interactive part. What do you think? How many of the 2007 tests that we run from the Lovm test suite, like the Lovmlovm test, how many of those successfully compiled, run in, verified when run on an Nvidia GPU? Give me a number. 90%. 90%. Do we have anyone that disagrees with that? 50%. Okay, I heard yesterday, I asked those questions a couple of times to people, and I heard like everything from 20 to 80 was yesterday the highest. Turns out 86.7%
00:06:10.690 - 00:06:35.114, Speaker A: of the test at the LM test suite we could just run and they verified correctly on the GPU. So that's a start. Now, what are our setup? So we run this on an Nvidia GPU, technically we can also do it on AMD and so on. There's nothing in the process that is restricted to one vendor or the other. And all we did, effectively is we set our own compiler as the compiler for the test suite. Cmate. That's it.
00:06:35.114 - 00:07:09.218, Speaker A: Let me just run it. This is a breakdown of the results. You see micro benchmarks. We didn't do that well. Single source was easier than multi source, which is not super surprising. And what I really want to get about for the last three minutes is what happened? Why couldn't we compile some of them, or why couldn't we verify some of them? So first things is some testers is broken. If you have any idea of openmp, a parallel four, like the four part, kind of indicates that there has to be a loop that follows.
00:07:09.218 - 00:07:39.094, Speaker A: Printf isn't a loop or f. Printf or whatever. So this is just a broken test never encountered in the test suite, because nobody executes this with OpenMP enabled. And if you don't enable OpenMP, this is just fine. So we had that, we had allocations that use char pointer Malloc, which clashes in our tool chain with other declarations. Apparently on the host we are more forgiving. But in the way we compiled it, things just didn't work out.
00:07:39.094 - 00:07:59.110, Speaker A: That's fine. So a couple of broken tests, not too many, we can deal with that. Then we had compiler bugs, so I only show one. We had multiple, or that we kind of found. This is VLA arrays and client complaints that the size is not set. Assertion crash, all not good. We can fix those as well.
00:07:59.110 - 00:08:38.258, Speaker A: This is kind of why we did this, to expose more code into our pipeline, see what happens and then lastly we got sort of limitations. And here gets a little trickier, because some of these are conceptual, some of these are doable but hard, and some of these are just, we should be able to handle that properly. C plus plus exceptions. I mean we potentially could, we could also just treat it as a know up and a trap and say, okay, as long as you don't actually use them. You just have them in your code, you're good, but don't use them. So that's an option. We don't know.
00:08:38.258 - 00:09:11.046, Speaker A: Inline assembly. Okay, we would need to rewrite that. That's kind of really hard. External globals we deal with external function calls, but not really with external globals that point into other things. Depends on your setup. We could make that work if we really wanted to, but so far we just didn't look into it. And then a couple of things like unsupported type built in bar arcs that are again, we support long double, for example, in your code, but you're not allowed to use it, you're allowed to declare it and have it as part of structs and so on and so forth.
00:09:11.046 - 00:09:39.046, Speaker A: As long as you don't do arithmetic with it, you're good. But as soon as you try to use it, okay, and then it depends on the hardware what we're going to do. That's pretty much it I have. So, conclusion we took the LVM test suite and actually run the C C plus plus programs on the GPU. And turns out most of them will compile and run and verify just fine. And the things that we found we'll try to address one by one. And maybe we should set up a CI to do these fun time things.
00:09:39.046 - 00:09:39.540, Speaker A: Thank you.
