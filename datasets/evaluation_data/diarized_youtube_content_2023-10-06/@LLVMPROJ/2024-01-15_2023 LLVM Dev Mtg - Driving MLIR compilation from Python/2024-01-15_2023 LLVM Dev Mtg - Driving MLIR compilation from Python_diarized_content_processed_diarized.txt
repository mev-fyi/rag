00:00:01.290 - 00:00:32.246, Speaker A: All right, hi, I'm Martin. I'm a PhD student from the University of Edinburgh, currently in Internet Google, and I'm talking about driving MLA compilation from Python. So let's first set the scene a little bit like whom is this actually for? So you're all compiler engineers in some way. So this is for like. So suppose you're an ML researcher and you have a new model with a custom op. It performs badly, of course, because it's a new op. But you might have an idea on how to tile it to your, your hardware, or maybe a performance engineer and you have an idea like you want to prototype an optimization recipe for your model.
00:00:32.246 - 00:01:21.910, Speaker A: Or maybe you are just a compiler engineer and are trying to figure out like I want to have this one transform fire, but I can't figure out which sequence of passes do I need to run, right? And a flow that all of these people might share is this, right? So you express your computation like ML or general purpose in Jax, lower through MLIR LVM, and then go to GPU. All right, so let's look at an example what this might look like. So we have a batch map model here, expressed in Jax. It's pretty straightforward, it's just Jax batchmatmo. All right, and then we go to mlir, we convert to stable ho, convert to Linux, and it's easy enough, we have just the Linux batchmap model, like a fill to the output. It's pretty straightforward, but the magic is actually right. How do we optimize that? So how it works conceptually is like you have this Linux representation, like five lines of code.
00:01:21.910 - 00:02:22.950, Speaker A: You optimize it somehow with some recipe, right? This might be C plus plus or transform dialect or whatever. And then you have still an mliR, like a lower level representation, and then convert to lvmir and execute. Sounds easy enough, as someone just kind of hands you this optimization recipe for your specific computation, for your specific hardware. But in practice it's not always as easy, right? Because this recipe. So in this particular example, we're using transform dialect. If you don't know what that is, there's an amazing tutorial from last year, but essentially you're using IR, like an MLR dialect, to express how you change transformations to optimize something, right? And yeah, this is very verbose, it's for a very simple thing, right? It's just a batch mark model, but it's still 200 lines of code and it's IR, right? It's not something you want to handwrite. You want to generate this somehow and even if you're an expert user and you can handwrite this, it's very tedious and error prone, right? Like if you mix up two instructions, the whole thing might just not compile anymore or yield to bad performance.
00:02:22.950 - 00:03:19.354, Speaker A: So for the use cases, like we said, it's like way too low of an abstraction to actually be useful for these people, right? So what did we do in this project? So you have still your Jax computation, right? Batch Mapmo on the right hand side you see the Linux thing, this is lower two. And next to your JaX computation you write now a schedule. And in the schedule it's like layered on top of the transform dialect you can say like you get handled to your module and you can say match my batch map mole, match my fill up and then tile my batch map mole to a four, fuse my fill into that, and then tile the map mole again. All right, regarding certain pile sizes. And then you jit the whole thing at the end and execute. All right, so how does this work? The schedule actually generates the transformer we saw before. Like it generates the optimization recipe, right? So it kind of gives the user an interface to define the schedule for this.
00:03:19.354 - 00:04:01.546, Speaker A: And what we do is we inject this schedule like this MLIR next to the payload, next to the computation, apply the transform script and get the optimized IR. Okay, that sounds easy enough, but the whole thing that we generate is still like this big. So the question is like, is all of this actually interesting? So if we zoom in a bit, there's actually interesting stuff here, right? It's like tiled for all. And we see that. That's very nice, but there's a whole lot of other stuff as well, right? So what is that actually? There's like canonicalization, folding, tiling, canonicalization, all of that stuff. So this is actually required for the schedule to work. It doesn't work without this, without these things.
00:04:01.546 - 00:04:38.200, Speaker A: So I think of these as kind of enabling transforms. They are required to enable the bigger, higher level conceptually transforms like tiling. But if we look at the whole schedule, almost 65% of this is enablers. And currently you kind of have to kind of guess where do you have to inject which enablers. But there is actually structure to this. So if you analyze this, there's only four different types of enablers here, right? So we thought let's try to capture this structure a little bit. So let's look at an example.
00:04:38.200 - 00:05:50.842, Speaker A: We just have a for loop here, and inside the inner for loop we load from a memRef, right? And you see like if we, if we would interchange these two loops, we might increase locality on, on some hardware, right? And a performance engineer looks at this and thinks like, all right, what do you have to do for this specific bit of IR to enable this interchange? Because here, this interchange is not possible because there is something in between the loops, right? It's not legal. So you think about it, how do I do this for this specific bit of IR? And you say, all right, like I have to do a loop environment code motion, hoist the thing out, and then it's interchangeable. All right, I'm happy. But the thing is, you have to do this for every example, right? For every optimization, for every piece of ir that you're interested in, you have to do this, right? And that's not what we want, right? So let's take a step back. And for the interchange thing, think about what do we actually expect in dir? Like what kind of structure, right? And for the interchange, it's actually, we expect our loops to be perfectly nested, right? So we can interchange them. There shouldn't be anything in between them. And then we can think about what kind of transforms, like what canonicalizations do we need to express that? So this is kind of the enabler category here.
00:05:50.842 - 00:06:32.194, Speaker A: So a bunch of random, it's not really important, but it's like a bunch of stuff that you do. And that should kind of put all dirs, all the payloads that could become a perfectly nested for loop should put them into that structure, right? All right. And I'm not actually calling them enabler categories. I'm borrowing like a word from the term rewriting community, which is normal forms. So what I define here is the perfect forenest form. So this gives me now a way to kind of say what kind of structure do I expect in the IR for my transform to fire? All right? Okay. So instead of doing just like the LSCM for this specific bit of IR, I can do the normalized.
00:06:32.194 - 00:07:06.786, Speaker A: This should work in more cases, and then do the interchange. And this should work, right? And we can push this even further. Right? So I've told you about the perfect four nest form, but we might define something like the four all plus four nest form. So if it has the four and four or four for all and wireless or whatever, right? And the weakest normal form is the any form. So any IR can be considered to be in this form, right. But you can also define more forms if you like, like the memory have no alias form or something. And so interesting fact is, MLR actually doesn't have a canonical form, right? They are canonicalizers, but there's no canonical form.
00:07:06.786 - 00:07:43.294, Speaker A: And I think that totally makes sense because MLRs have multiple abstraction levels, right? There shouldn't be a canonical form, there should be multiple normal forms or like multiple canonical forms or something, right? And not only one for each dialect, but here for the SAF dialect we have multiple already and makes sense. All right, so let's look at a small example. So we have our interchange transform here defined. And we defined like a precondition. We need the perfect forenest form. And if you think about it, the interchange just interchanges two loops, right? It doesn't destroy this form, this structure in the IR. So a post condition is also perfect forest form.
00:07:43.294 - 00:08:13.718, Speaker A: So if you look at an example, we have a module of two forenests. We start with a handle to our module, and then we match the outer for loop, the inner for loop, and the membrane inside. And we see like we know nothing about these handles, right? So they're all in the weakest normal form. And then we do the interchange. And we know for the interchange it requires a specific normal form. So what we do is we automatically inject the normalization to perfect fauna form. So these handles are in that form, right? Then we actually do the interchange.
00:08:13.718 - 00:09:06.694, Speaker A: And then we continue, we match the second for loop, the inner one, and the linear generic. And we want to tile this, right? And the tiling also requires the perfect forenest form. So again we normalize, right? So you see this is tiled for all up, right? So we introduce the for all up into this thing. So afterwards it's not in the perfect forenest form anymore, it's in the for all plus four nest form. And you realize the module also like this was propagated outwards to the module as well, because it also contains null fall, but not to the parallel loop nest. All right? And like an easy optimization you might do is maybe, you know, I have a module of perfect forenests where I want to do something. I can just normalize the whole module, right? So you do module normalize, then you don't need the normalization anymore because we track this through this handle system, we track the normal form they are in statically, right? Okay, so you don't need to do the normalization anymore.
00:09:06.694 - 00:10:20.178, Speaker A: So you even save passes over the IR. All right, so the gist of this is, not every user has to think about how do I enable this transform? But the designer of a transform thinks once about what is the structure I actually expect of the IR expresses it in the system and the user can just use these normal forms. All right? And kind of this abstraction that we have also very naturally enables auto tuning, right? So we have like the Matmo tile, Matmo tile here once with an outer tile size and an inner tile size. And all of this is parametric, right? And we even can even express constraints. So the outer tile is like in 1248 and the inner tile should divide the outer tile, right? So you could ship like a parametric, you can auto tune this thing very easily, but you can also ship this to a users and tune on the user device. Or maybe you have a company where your internal model is secret, but you still want to collaborate on the schedule somehow, right? You can just kind of take all the secret stuff out, leave it parametric and then still collaborate, right? But even better, right? Like the normal form. So think about it, the normal forms enables you to much more naturally express how you actually want to schedule your thing.
00:10:20.178 - 00:10:53.390, Speaker A: And it doesn't only enable you to do that. Also, an autotuner has now much, much like an auto scheduler has now a much easier time generating valid schedules, right? It's actually very, very difficult to generate schedules. Not only that kind of optimize well, but have the enablers at the right places. And lifting this burden from the auto tuner makes it much more easy. Yes. And that kind of enables auto tuning beyond only the built in stuff. You can just break your own transforms, your own normal forms and get auto scheduling.
00:10:53.390 - 00:11:50.800, Speaker A: Right, so the huge giant schedule is actually done in our system, just this. It's still quite complicated, but actually much, much easier. Right? So you have like tiling, vectorization, relower, and do some GPU specific transforms. At the end we lower to LVM, right, but you're saying, all right, this talk was like driving the compiler from Python, right? This is just, why isn't it called scheduling an MLIR or something? Well actually it is driving the whole compiler from Python because this lower to LVM is just module convert liner to loops, pass convert SCF to CF path, right? So we can do the transforms on different granularity levels, right? It can be a single transform, but it can also be a whole path, right? So we actually completely drive the compiler from here, from this schedule representation. All right, I'll skip over this because I'm out of time and with the contributions heads off to my collaborators. I had a very good time working on this. Yes, I'm looking forward to your questions.
00:11:58.290 - 00:12:02.020, Speaker B: As usual. Microphone from the center if you have any questions. Come on up.
00:12:06.490 - 00:12:59.902, Speaker A: How does this compare to the module keynote? Yeah, I mean, if you think about it, the concepts are quite similar. So modular is like a front end for mlir, right? And in a certain sense, this also tries. So modular is like an abstraction on top of MLIR, trying to give the user some control. Like you can define your library with like index dialect and whatever. And this is like, it feels to me like a similar approach to solving quite similar problem. So giving control into the hands of the user. How do you define the normal forms? Right? So the normal form is defined through the transforms to get to this normal form, right? So the perfect forest form aesthetically.
00:12:59.902 - 00:13:38.126, Speaker A: Right? So I don't know what the payload will be, but someone thinks about this. Like, which transformations do I need to apply to get to this normal form? Right? So it might just be that you have a program which is based on loops and you apply the perfect for nest form, but you don't have perfectly nested for loops because maybe you just have stuff that's not hoistable, right? But it's like, yeah, what happens if it's not possible to apply the transform to the normal form, right. Then the transformers fails, but it can fail gracefully. So you can do stuff like try to interchange these loops, and if it's not possible, do something else. Right. So an auto scheduler, for example, might benefit from that as well. Right.
00:13:38.126 - 00:13:45.678, Speaker A: You can try to. Try to parallelize or try to, I don't know, try to tile. Maybe that's not possible with your payload. So try something else.
00:13:45.844 - 00:13:57.580, Speaker B: Thank you. Got time for one or two more questions? Going once, going twice.
00:13:57.920 - 00:13:59.804, Speaker A: All right, thank you for your questions. Yes.
00:13:59.842 - 00:14:01.530, Speaker B: Let's give another hand. Thank.
