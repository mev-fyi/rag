00:00:00.810 - 00:00:52.734, Speaker A: Hi, today I'm going to talk about hot cold splitting. It's a function level optimization, basically useful for code layout. And to get started, I am just stealing some tweet from someone who was kind enough to tweet about this. Another one from a llvm reviewer when I submitted this proposal, so they thought that it was pretty interesting. Okay, back to introduction. Hot code splitting optimization is not applicable for all kinds of code base. It is very specific, so we need to be kind of careful about in which case we really want to use this.
00:00:52.734 - 00:01:44.990, Speaker A: It will not improve performance and it can degrade in many cases. So kind of reducing the expectation for many people. In the beginning, this optimization is based on finding pieces of code which is cold and trying to maximize the amount of cold code, trying to maximize the region which is cold, and getting them out into a separate function. When we talk about regions in the control program, there are two types of region which are very common. One is single entry single exit and the other one is single entry multiple exit. We have examples here. The idea about finding these regions is because they enable a lot of classical compiler optimizations.
00:01:44.990 - 00:02:36.298, Speaker A: For hot cold splitting, we look for single entry single exit and single entry multiple exit regions. There can be other strange regions. We are not interested in that part. There is a small transformation we can do to convert single entry multiple exit to single entry single exit, and this makes it much more easier. Single entry single exit regions are much easier to reason about because we can ideally extract them out into a separate function because they have only one entry point and one exit point, just like a function call. The first step in hot coal splitting is marking edges which are cold. How do we find cold edges? First, what is the definition of a cold edge? A basic block which executes less frequently during the runtime of a program is we consider them as cold.
00:02:36.298 - 00:03:25.310, Speaker A: The definition of cold can vary a little bit based on your workload. Some people may think 5% is less, some may think 20% is less. But idea is to find edges which are cold. When we get profile from field you mostly think about hot edges, but cold is just the opposite of hot. So I'm putting cold because that makes it easier to explain the idea. We can also use static analysis, some programmers who are careful enough to put built in expect or assertions which compiler can reason about, or in C plus plus you have higher level constructs like throw or functions which are non returning like abort exit kind of functions. These can be used to construct find out cold edges.
00:03:25.310 - 00:04:37.880, Speaker A: The second step is to propagate the coldness of basic blocks using concepts called these are concepts in the graph theory called dominance and post dominance. Basically, in this example, if we see if B two is cold, b four must be cold because there is no way to go to B four without reaching b two. So this is the concept of dominance in a very simple way, and it can be used to figure out that b two and b three, b four, both are cold basic blocks. And this is mostly about it at a higher level. And the later part is mostly transformation, which is extracting out those cold basic blocks into a separate function. So this is how it looks like ideally we want to expand the propagation of profile information to the entire CFG and find out the maximal region which can be outlined so that we can gain maximum amount of cache locality. There are other details like computing input output and adding function attributes to the outline function.
00:04:37.880 - 00:05:44.218, Speaker A: It's because once we outline a function, once we outline a function which is code, we don't want to do aggressive optimizations on them because we already know that this is code. So we can do code size optimizations like OS or oz, and we mark them as code so that later stages, inliner or other optimizations don't put them back again and undoing the effects of hot coal splitting so this is the high level detail. Now, one of the most important part of this work is the design decision. Classically, the hot coal splitting is not a new optimization. It has been there for decades, probably before I was born, and there are research papers there as well. I did not look at those papers before and I think that was a good thing because I do not get corrupted by their ideas. I decided to do at the middle end because in general I tried to follow the principle of doing optimization at the highest level possible.
00:05:44.218 - 00:06:23.846, Speaker A: And I'm not a C plus plus expert, so highest level is IR for me. Yeah, there are big advantages of doing at IR level. First thing is I can only focus on optimization and tuning. If you try to do any useful optimization at the RTL level, like post register allocator, it's a nightmare to focus on liveness and bit vector and all kinds of things. But at IR level things are much simpler. You can reason in terms of graph rather than registers and values. The second advantage is we can optimize the outline function for code size.
00:06:23.846 - 00:07:24.010, Speaker A: It is not possible to do this if we have post array or post linkage optimizations. It has all backend targets. I wrote it once and every other target on LLVM can benefit from it and low maintenance overhead of course there are drawbacks for sure. There is one very obvious one, which is architecture specific opportunities. We can have only so many target back end hooks to reason about what actual machine code will be generated, but there may be drawbacks for sure. Okay, what kind of applications will benefit from hot coal splitting? If we just apply hot coal everywhere, it may regress performance, quite badly actually. And I'll show in the experimental results mostly if we find out from the profile that we have lot of icache misses, or if there is an application, like a giant application on a small device.
00:07:24.010 - 00:08:24.750, Speaker A: So when we launch the application, it'll take a long time to launch because those devices are resource constraint. You'll have page faults, icache misses, like higher icache misses because the device is small and the binary is giant. Hot coil splitting will help put those unnecessary working set to just outside in a different section, or they might not be loaded in the program. Startup time. How do we do the experimental evaluation here? My approach was mostly for public consumption here, and results are not promising because it's very tricky to get these things right. And if you try, then you'll realize that using Linux, perf and things like that, they have a lot of uncertainties on all the devices. The numbers are fluctuating by more than 20% and it was not very impressive, but that's what we have.
00:08:24.750 - 00:08:54.306, Speaker A: So I think relying on production data is much more valuable. Okay, so I used llvm test suite because that was easiest to compile and build. And this is the performance numbers. And this is not impressive at all because llvm test suite is a performance benchmark. There are tiny applications, like a couple of megabytes each. And my x 86 machine, you can just load entire thing, no page fault, nothing. Right? So numbers are not very impressive.
00:08:54.306 - 00:09:44.134, Speaker A: But there's one benchmark, control flow. That's the only benchmark I got, 5%, more than 5% improvement. Others just regressed like sometimes more than 5%. When we talk about code size. Hot code splitting helps reduce code size in almost all the benchmarks, because the outlined functions can be optimized for code size, except for one. I don't know why it was the case, but I didn't look into it. When we try to find out how many functions were outlined in different benchmarks, it is very clear that more branchy code, you'll have lot more code, basic blocks or regions, and with good profile information we can outline lot more.
00:09:44.134 - 00:10:25.338, Speaker A: For example, sqlite is there, it's a database, so a lot of branches. Okay, I did one measurement with icache and instruction counts. These numbers are mostly to show that the graphs like Linux perf is mostly not very meaningful. Why? Because the numbers were fluctuating quite a bit. I just used one of the benchmarks where the hot cold was most effective, outlining the most functions. And we can see icache misses are lower, but on instruction number of instructions executed they vary quite a bit from iteration to iteration. The x axis is the number of iterations.
00:10:25.338 - 00:11:02.400, Speaker A: On the left side is number of instructions executed, which is the upper graph, the red and the blue and yellow and green is icache message. This is the most impactful slide because I heard that iOS, the entire iOS operating system was compiled with hot cold splitting enabled. Nobody told me. I just downloaded the iOS and run nm. You see dot cold symbols like everywhere. That's how you one of my good friends told me about this. So I wanted to share here.
00:11:02.400 - 00:12:07.126, Speaker A: So I wanted to divide the talk into two parts, focusing more heavily on the ideas for improvement and few more like hand waving. Basically because it's a big optimization and it need lot more tuning. It'll be great to take input from multiple people who have seen different kind of workload and they can help improve it quite a bit. The first concept I want to talk about is what is hot and cold like? Lot of hand waving, but I just want to convey what I'm getting here. Hot is basically interesting and cold is not interesting or vice versa. Why? I want to say this because when we talk about profile, when we are talking about performance, we are thinking hot as basic blocks which are executed most and cold which are executed less least. But we can have another cost function where hot is something else, like a basic block which is just for some reason it is not very interesting.
00:12:07.126 - 00:13:07.000, Speaker A: I can give concrete examples for let's say you have automatic reference counting like objective C or swift. There are sequence of code which occur everywhere. Like whenever you have a pointer which you want to do something with it like, or you pass to a function, you have a sequence of code which will basically increment the counter and then you'll have decrement of the counter, right or stack, save and restore quite a bit of spills. Some sequences of code occur much more often than others. We can reason that they are not interesting for applications which don't care about very high performance, we can mark them as not interesting and then outline them. It doesn't have to do with runtime performance numbers, but it can help reduce code size quite a bit. But this can happen only when we start thinking a little bit one level higher that why are we thinking in terms of hot and cold and not in terms of good and bad or interesting? And not interesting? Okay.
00:13:07.000 - 00:14:00.662, Speaker A: There are other ideas of improvement which are very concrete, like finding maximal regions. We do find regions, but we don't find maximal region because it is compute intensive. There is a way to find a reasonably maximal region by using better data structures, but currently it is not there. We have cases which are clearly not optimal. Improving static analysis is obviously, it may be an ongoing work because there'll always be some functions or some pattern of code which we can reason about as code, but it will be applicable on a code. It can vary from different code bases. Like some code bases may have logging functions, right? Like if we deploy some application on field, we want to collect data or we want to have assertions, but they are soft errors.
00:14:00.662 - 00:14:26.820, Speaker A: They don't crash the app, but we collect data. They are mostly cold code, but they are like domain specific things. The fourth one is improving code extractor. We found like several bugs. I just fixed two of them and there's one more under review. So there might be more, I don't know, we might want to fix them. And then like tuning cost model.
00:14:26.820 - 00:15:14.050, Speaker A: Okay, this is one example where this is a concrete example of hot cold splitting, where this is a test case. And if we run hot cold splitting, it will outline them into two separate functions, which is all good. But if I mark this blue colored basic block as cold now we have more information. Like the end basic block is cold based on the analysis already. But if I mark one more basic block as cold now it has more information, but I found that it will not outline them as you see in the right side. This is clearly a limitation of the way we are propagating our profile. So we need to improve them to get more, better cold basic blocks, basic block regions.
00:15:14.050 - 00:16:06.462, Speaker A: Okay, I'm kind of packaging two things in one, because when we do hard code splitting, it can increase code size in some cases. Now, how to reduce the code size? We can use much similar functions. I heard that there was another talk where they got some useful code size gains with reduce the code size with much similar functions. And I also saw improvements because hot code splitting can outline different patterns of code. And if we can use merge similar function, it is totally possible to find identical pieces of code outlined as different functions and we can just merge them. I ran merge similar functions without LTO, so I still got some of them. Some of the functions which are code and merged.
00:16:06.462 - 00:16:35.690, Speaker A: So it shows that we can find similar patterns when we talk about performance. This is the same Llvm test suite. Now we have three different bars. One is baseline and all of them are with profile information. So baseline with profile, then hot roll splitting, and then hot cold splitting with merge. Similar functions enabled. And then here we can see some of the benchmarks improved in performance.
00:16:35.690 - 00:17:31.822, Speaker A: Not all of them improved compared to base. And there is a reason for it that when we outline a function as code, we need to have some linker magic or some kind of order file to move those functions in a different section. And I did not do that. So we are still page faulting because the function is just next to the cold function is right next to the hot one. Gnoulinker does not understand the things I was trying to do. So probably we need like, I don't know, some linker magic or maybe teach the compiler at the end to put them somewhere else. But that will, I think help at least match the performance of base or get better performance in code size.
00:17:31.822 - 00:18:24.560, Speaker A: We gained all in almost all of them. Once you enable much similar functions because of obvious regions, that's about it and I want to thank several people. It was a joint effort by several people from community and they helped shape the ideas, write many useful pieces of code. The order of names are in the order of difficulty I had spelling them and small tidbit that why it is dot cold one, because if you run C plus plus field it understands that dot means it's the same function with a clone or something. So you will get the proper demangle symbols. Yeah, that's about it. We have possible questions, I can run through them, or if someone has more interesting questions I'll be happy to answer them.
00:18:31.330 - 00:18:39.220, Speaker B: Thank you very much. There's the mic in the middle and if you raise your hand I'll also try to run this mic to you if you have questions.
00:18:48.630 - 00:19:02.746, Speaker C: I thought the linker would lay out hot and cold. Like even if you're not doing hot cold splitting and you do function sections and you put things in, label them as hot or cold, I thought it laid those things out like the hot stuff together and the cold stuff together.
00:19:02.848 - 00:19:07.740, Speaker A: That was my expectation as well. But when I run object dump I found them just back to back.
00:19:08.910 - 00:19:10.346, Speaker C: Sorry, which linker was it?
00:19:10.368 - 00:19:11.206, Speaker A: Blue linker.
00:19:11.318 - 00:19:17.390, Speaker C: Okay, yeah, try gold. Or it may be the naming is slightly off of what it's expecting.
00:19:17.970 - 00:19:23.870, Speaker A: I also added unlikely to the function annotation. That also didn't help for me.
00:19:24.020 - 00:19:48.446, Speaker C: Maybe it's actually it's expecting dot Shree who gave the propeller talk is the person to talk to, because I did that in gold. No, they don't, but the compiler will mark hot functions without splitting them and cold functions without splitting them. And then also, I know GCC does the splitting and they mark it somehow.
00:19:48.478 - 00:19:51.140, Speaker A: That it gets laid out okay, thank you.
00:19:53.510 - 00:20:06.750, Speaker D: Are you doing hot code cold splitting after inlining or before? Because my thinking is that after you've done the splitting, some functions that were previously not considered inlineable because they're too large have become small enough to be inlined.
00:20:06.850 - 00:20:25.500, Speaker A: Yeah, I think it is before inlining, but I meant they have been rescheduled sometimes, and different ports of LLVM, like swift LlVM has different place and I think compared to the upstream LLVM. But yeah, possibly it is before inlining. I don't remember exactly right now.
00:20:29.910 - 00:20:39.320, Speaker B: Any more questions? Let's thank our speaker.
