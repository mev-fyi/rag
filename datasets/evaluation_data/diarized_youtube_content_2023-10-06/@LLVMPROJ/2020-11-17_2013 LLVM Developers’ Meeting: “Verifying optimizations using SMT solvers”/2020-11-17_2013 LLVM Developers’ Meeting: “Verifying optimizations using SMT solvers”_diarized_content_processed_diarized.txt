00:00:00.650 - 00:00:35.750, Speaker A: I'll speak a bit. So I'll give a short introduction to these SMT solvers and how we can leverage them today to verify some parts of the compilers, first of all. So why should we care about verifying these optimizations? And the reason is basically twofold. So first, we can catch the bugs even before they exist. So if you are just thinking in your new algorithm, if you are designing your new inst combined pattern, whatever, even before you write the code, you can verify that these are in fact correct.
00:00:35.820 - 00:00:36.486, Speaker B: Okay?
00:00:36.668 - 00:01:02.122, Speaker A: And I think we all know that debugging these wrong code emission bugs is really hard, right? And it takes a lot of time. So my point here is that it pays off in the long term. So although you will spend some time doing conditional verification in the long run, you'll save a lot of time in this debugging.
00:01:02.186 - 00:01:02.846, Speaker B: Okay?
00:01:03.028 - 00:01:09.922, Speaker A: And I'll only talk about technology that is available today, not any stuff that will be available in ten years.
00:01:09.976 - 00:01:10.580, Speaker B: Okay?
00:01:11.590 - 00:01:53.514, Speaker A: But let me show you a famous quote from Knud, very old quote. And I'm not saying that we should get rid of all testing whatever. So I'm saying that we should do verification in addition to testing. Okay, and I will elaborate a bit more later on this. Okay, so this is the outline of the talk. So I'll start with a short introduction to sat and Smt solvers, and then the rest of the talk will be with examples. So I'll show a bunch of examples on how to prove like inst combined, for example, some assembly code to check whether it matches to the IR code and some data structures.
00:01:53.514 - 00:02:25.242, Speaker A: And I'll use constant range as an example. Okay, satsilver. So who has used a satsolver before here? Okay, three people, four people. Okay, well, a satsovar is pretty simple tool, so it takes a boolean formula as input. So you can say, well, a or b or c and not b or C. And what the satsovar will try to find is a set of values for these variables that make the formula true. It's very simple.
00:02:25.242 - 00:02:51.938, Speaker A: So the satsovar can return either sat if you can find such an assignment, or it can return nsat if the formula is always false, regardless of what values you put in A, B, or C. And if the formula is satisfiable, we also get the model, and the model is an assignment to these variables. So in this example, if you replace a with true, b with false, and c with false, you get true.
00:02:52.024 - 00:02:52.660, Speaker B: Okay?
00:02:53.910 - 00:03:37.860, Speaker A: And there might be multiple models for some formula. So the solver is free to choose any of them. Okay, so this is what a SAt solver does. It's pretty simple. You can think that the naive SAt solver will just try all possible combinations and you'll get some exponential algorithm. But today's satsolver are way better than this exponential. Okay, so SMT solver is a generalization of a SAT solver, and the difference is that the variables are not only boolean, so they can take different ranges, different domains, including, for example, bit vectors, which is what we are going to use today.
00:03:37.860 - 00:04:01.418, Speaker A: Okay, and that's it. So there are many available SMT solvers out there. I'm not particularly endorsing any of these. These are the ones that I've used in the past, so they actually work. I like Ztree because they have this web service online, so you can do the proofs in your browser, so you don't need to install anything.
00:04:01.504 - 00:04:02.140, Speaker B: Okay.
00:04:04.110 - 00:04:23.834, Speaker A: With the disclaimer that I've worked in the Z three team in the past, of course I have to say that it's the best one. And also the warning is you should read the license of these tools before using them, because not all tools have a suitable license for your usage.
00:04:23.962 - 00:04:26.820, Speaker B: Probably. Anyway.
00:04:28.950 - 00:05:16.370, Speaker A: The bitvector theory of SMT solvers, which is what we care about. So it basically supports all operations that you see in the llVmir. So all the standard operations, and with addition to two operations, which is this concate, that takes two bit vectors and concatenates in one bit vector, and the extract takes a bit vector and extracts a set of contiguous bits of some bit vector. And the other important thing is that a variable has a fixed bitweet. So once you declare variable a with 32 bits, then it is 32 bits forever. And also you have to declare the variables with a constant bit width. So you cannot say, I have a variable a of x bit.
00:05:16.370 - 00:05:19.426, Speaker A: Okay, so it has to be a variable A of 32 bit.
00:05:19.528 - 00:05:20.178, Speaker B: Right?
00:05:20.344 - 00:06:15.720, Speaker A: We'll see later why this is important. Anyway, enough of talk, so let's look to some example. So let's prove that these two expressions are in fact equivalent. So who thinks that these expressions are actually equivalent? No one. So who thinks that they are not equivalent? Okay, so everyone is shy. Well, if you are not experienced with this instomine style of patterns, it might not be obvious when you look at it, right? But this is probably instombined 101 transformation. And does anyone know actually what this expression is trying to compute? Power of two.
00:06:15.720 - 00:07:06.514, Speaker A: Well, not almost. Okay, but now shut down your llvm part of the brain and let's take as an SMT solver and what do you need to prove? So, what we need to prove in order to be sure that these expressions are equivalent. Okay, so there are two ways of looking at this, so we can think, okay, so for all possible values of x, these expressions always return the same value. Okay, this is one way of looking at it. Or we can say, well, I cannot find a value for x such that these expressions are different. Well, the second one is a bit more complex, but we know from logic that these two statements are actually equivalent.
00:07:06.562 - 00:07:07.160, Speaker B: Okay?
00:07:09.050 - 00:07:42.000, Speaker A: And the second thing is that SMT solvers prefer the second version because they don't like these for all x, something, something. Okay, so they prefer these, there is no value or there exists some value. Okay, so we have to encode the problem in the second form. So let's do it. So in the bottom right, I include the URL to the z three web service. So you can see the proof there and you can play with it. So basically, the website has a play button you can play, and it proves for you.
00:07:42.000 - 00:08:13.590, Speaker A: So, first of all, we declare a variable x of 32 bits. So the syntax is list plike. So we'll see a lot of parentheses in this talk. Actually, we are declaring a function. So everything in SMT is a function. So we say we have a function from null to a bit tracker of 32 bits, and then we say, well, I want to find. So I have two expressions now that are not equal.
00:08:13.590 - 00:09:17.290, Speaker A: Okay, so let's encode the first expression, again with a lot of parentheses, and the functions are prefixed with the bv, which is short for bit vector. So you can say, well, I have this x ended with x minus one equal to zero, right? Then we encode the second expression. And finally you say, well, is it satisfiable or not? So what this example is saying again is, well, is there any value for x which makes these two expressions return a different value? So that's what we are asking from the SMT solver. And if you run this, after a few milliseconds, we get unsat. This means that there is no possible value for x such that these expressions are different, which means that they're all is equal. So we have just proved that these two expressions are equivalent. So from now on, you can be sure that you can use either of them because they are equivalent.
00:09:17.290 - 00:09:24.720, Speaker A: So we didn't need to think about it. We just give to the tool and play and press a button.
00:09:25.890 - 00:09:26.542, Speaker B: Okay?
00:09:26.676 - 00:09:46.626, Speaker A: But then Dimitri said, well, this is a power of two, but are we sure that it is testing for power, power of two. Well, I'm not. So let's ask the solver. So, I define X with four bits, because I don't want to enumerate all the power of two up to 32 bits.
00:09:46.658 - 00:09:47.142, Speaker B: Okay?
00:09:47.276 - 00:10:33.814, Speaker A: And then let's say, well, is this equivalent to being equal to one, two, four or eight, which are all power of two, of four bits. And then I ask, well, is it satisfiable or not? And it says it is. This means that the expression is not computing the power of two. So Dimitri was almost correct, but now the SMT solver can do a bit more, which is, it can give me a value for x such that these two expressions are different. So we ask for the model. This says, well, if x is equal to zero, then these two expressions are different. If you plug zero in the first expression, we get like zero and something equal to zero, which is true.
00:10:33.814 - 00:10:55.114, Speaker A: And in the second expression, we get false. Okay, so the first expression is almost the power of two, except for the case where x is equal to zero. Okay, so I give for your homework to fix the first expression to put and not x is equal to zero. And if you put that, then you can prove that the expressions are equivalent.
00:10:55.162 - 00:10:55.566, Speaker B: Okay?
00:10:55.668 - 00:11:39.380, Speaker A: And then you have just proved that the expression, at least for four bits, returns true if X is a power of two. Okay, so now that you are all experts in SMT, let's go for more complex examples. So let's look at instombine. Okay, so instomine doesn't really need any introduction here. The point is that it's a really cool target for SMT solvers, because instombine is just converting these little sequence of instructions into some other sequence of instructions. So there are no loops or nothing. It's just very simple straight line code.
00:11:39.380 - 00:12:29.890, Speaker A: Okay, so let's look at this example. So a few months ago, someone sent an email to the mailing list saying, well, I'd like to do this transformation, okay? And the first time I look at it, it was not really clear to me whether it was correct or not. Does anyone have an opinion? So I can give you a hint. So this was committed to LlVm, and then it was reverted. So some people thought it was correct. So it got committed, then it broke clang, self host, and then it was reverted. But let's actually prove that it is not correct, or if it is correct.
00:12:29.960 - 00:12:30.580, Speaker B: Okay?
00:12:31.830 - 00:12:53.258, Speaker A: Okay, again, so I show here the full code, because you are all experts now. So you know how to read this. And let's ask the solver. So check sat. And it says, well, it's satisfiable, so they are not equivalent. So this inst combined pattern apparently is not correct. But now we can ask for the model.
00:12:53.258 - 00:13:47.534, Speaker A: And he says, well, if a is zero and if B is that huge number 20,007, then the expressions are not equivalent, but B, if you see like here, it's the value where we do the left shift. But if you remember from C overflow, in left shift, it's undefined. So we don't care whether these expressions are equivalent. For this case, the question that we should ask is, are these expressions equivalent for the cases where we have defined behavior? Right? So let's do it again. We define the variables. Now, we say, well, b must be n sine lower equal to 31. And this condition guarantees that the left shift won't overflow, okay, because it's between zero and 31.
00:13:47.534 - 00:14:53.650, Speaker A: And now we copy paste the rest of the script and SmT solver says, and sat, okay, so in fact, this instomine transformation is correct if we assume that the left shift overflow is undefined behavior. Okay, I think this is really cool, because the first time you look at this transformation, at least I was not sure whether it was correct or not. And now we are sure that it is because we are leveraging certain undefined behavior. So if you look to the instombind code, well, it's, I don't know how many thousands of lines of code and you are not sure where we are leveraging undefined behavior where we are not. So if you want to change the semantic of the IR for your language, you are not sure where to start changing, right? But here we are now sure what undefined behavior we are leveraging. So the whole story was. So this patch was committed and it was reverted because apparently clang was executing some undefined behavior.
00:14:53.650 - 00:15:05.542, Speaker A: And I think the patch never came back again. So I don't know if the bug was fixed in clang in the meantime, but I think we are not doing this transformation at this point.
00:15:05.596 - 00:15:06.200, Speaker B: Okay.
00:15:09.610 - 00:16:24.906, Speaker A: So let's look at some assembly code to see that we can also do something different. Okay, another real story. So I submitted this bug report about the built in overflow intrinsics, okay? So I wanted to do x times y plus z and get zero if any of these operations overflows or otherwise return the result of the operation, okay? And Llvm was giving me 17 instructions of pure madness, which was like a horrible sequence of code that was crazy, basically. So this code compiles to this l of mir, okay, so these built ins written two values. One is the result of the operation, and the second value is a boolean indicating whether the operation overflowed or not. And the L of M was compiling these to this bunch of garbage, which does like push flags, pop flags, and you have their move Eax Vcx, move ECX back to Eax. So this is completely insane, okay, 17 instructions.
00:16:24.906 - 00:17:16.906, Speaker A: And then I said, well, maybe we can compile to just eight instructions, okay, doesn't matter what the code is. But then Michael analyzed the bug report and said, well, your code is not really correct, because instead of thinking of this add carry EDX EdX, we should have a at carry zero EDX because your operation may overflow. And then this is not really equivalent. And then I was really puzzled because I thought that my code was actually good. And he said, no, it's not. And then I went ahead and asked the power of SMT to prove which version was actually good, or if both were wrong, or if both were good, because this version. So the encoding of this Ed carry takes more space than the ad carry edx.
00:17:16.938 - 00:17:17.230, Speaker B: Okay?
00:17:17.300 - 00:18:13.220, Speaker A: So if the other was correct, it was better to use the other, right? So now we can compare this assembly code to the L of mir to see if they have the same semantic. So the first instruction is this mole. So this is multiplying x by I by y, and the semantics of x 86 multiplication is that v zero extends to 64 bits. And we do the multiplication in 64 bits. And then eax takes the less significant bits. So we do the extract, so the extract comes from right to left, so it's zero to 31, and EDX takes the most significant bits, okay? And that's the encoding for the multiplication. And then we do the addition, so we add the previous result with z.
00:18:13.220 - 00:18:47.346, Speaker A: Okay, it's simple as this. And we also update the carry flag, because this instruction also updates the carry flag. And we can do this by doing the addition in 33 bits and taking the sign bit. Right now we have the add carry. So what the add carry does, so it adds edX, EdX plus the carry flag. But we need to zero extend to 32 bits because the type checker requires all operands to have the same bitweet.
00:18:47.398 - 00:18:48.000, Speaker B: Okay?
00:18:48.770 - 00:19:23.640, Speaker A: But at carry also updates the zero flag. So the zero flag will be one if the result is zero, okay? And finally we are left with this set of jumps. So basically what it does is if zero flag is zero, we jump to the shore, to the Xor, and where we zero out ex. Otherwise we just return whatever is in ex. And so we can define something like this. So we have this new operator, which is if then else. So if zero flag, we return whatever is in ex, otherwise we return zero.
00:19:23.640 - 00:19:30.566, Speaker A: So now we have the result of what the assembly code is computing.
00:19:30.678 - 00:19:31.340, Speaker B: Right.
00:19:32.510 - 00:20:04.194, Speaker A: Now let's see what the IR is computing. So it says, well, if I have overflow, by the way, these let expressions are what you would expect from these functional languages. So this is like a local variable. So I have overflow. If I do the multiplication in 64 bits and the result doesn't fit in 32 bits, or if I do the addition in 33 bits, well, actually here I'm doing in 36, so that I can use the excess decimal notation.
00:20:04.242 - 00:20:04.840, Speaker B: Right.
00:20:06.970 - 00:20:26.794, Speaker A: I'm lazy. So if any of the operations overflow, then the whole thing overflows. And then also if overflows are returned zero. Otherwise I return the result of the operation. So now we have two expressions. One says whatever the assembly code is computing and what the IR is computing.
00:20:26.842 - 00:20:27.486, Speaker B: Right?
00:20:27.668 - 00:20:34.138, Speaker A: And now we can say, well, can you find me x, y and Z such that they are computing different values?
00:20:34.234 - 00:20:34.880, Speaker B: Okay.
00:20:35.570 - 00:21:03.794, Speaker A: And it says, well, yes, so they are not equivalent. Okay, so it means that Michael was right. So the proposed assembly, so that those eight instructions were actually wrong. And then it returns a model, which with these weird values. And I challenge you. If we didn't prove this way, how you could have found this in the real. Like, these values are really weird.
00:21:03.794 - 00:21:05.574, Speaker A: So how could it trigger this bug?
00:21:05.622 - 00:21:06.074, Speaker B: Right?
00:21:06.192 - 00:21:40.126, Speaker A: Oh, Michael is actually there. Yes, that's correct. So Michael found it. So we don't need SmT solver, so let's go home. I know, yeah, but. Yeah. And Michael, of course, has been working with this bit stuff for a long time, so it's easier for him to find these kind of bugs for me, which I only work in these things, like once in a while, so it's harder to catch these bugs.
00:21:40.158 - 00:21:40.740, Speaker B: Right.
00:21:41.430 - 00:21:48.310, Speaker A: So, actually, the model here, I'm not showing the whole thing, but it also shows the values for eax and DDX and the cariflex and whatever.
00:21:48.380 - 00:21:49.000, Speaker B: Right.
00:21:49.530 - 00:22:16.480, Speaker A: So I leave you for your homework, which is to prove that Michael's version is actually correct. Okay? So instead of doing the at carry EDX, EdX carry flag, it's just at carry zero, EdX carry flag. And his version is correct. Okay, so Michael was correct from the beginning, but I thought this gives a great example of what we can do.
00:22:18.130 - 00:22:18.842, Speaker B: Okay.
00:22:18.996 - 00:23:05.520, Speaker A: And. Yeah, that's it. So let's see, like a last example. So how we can use this to prove that certain data structure of LLVM are also good. Okay, this is a bit trickier example. Okay, so constant range is a really cute data structure of LVM, which represents ranges of integers, okay, but with overflow semantics. So we can have this interval from five to two, which means that we go from five to Max integer and then with union with zero, from two or from, okay, and it is used to perform optimizations using range analysis information.
00:23:05.520 - 00:23:55.854, Speaker A: And then this data structure supports many LMIR operations, so you can do like additions and multiplications between these ranges. And from this simple description you can maybe naively think, as I thought in the past, that it's not that hard to implement this data structure right, and the answer is wrong. So this is actually pretty hard to implement. And there have been a lot of bugs in the past in this file, both of correctness and optimality. So this makes a really nice target to prove the correctness of this thing. Okay, so let's take a look to the simplest operation that constant range has, which is this sine x ten. Okay, it's just eight lines of C plus plus code.
00:23:55.854 - 00:24:38.350, Speaker A: So what can go wrong in eight lines of code, right? I don't know. So I decided to go ahead and prove that these eight lines of code are doing what we expect, or maybe not. Let's see. Okay, so first we need to define some definitions. So this defined sort is like a type dev. So we are going to define, well, let's do operations on integers of 32 bits, and let's define an interval as a concatenation of two integers. Okay, so an interval will be a bit vector of 64 bits, and the result, so here I'll be proving a sine extent from 32 to 36 bit.
00:24:38.350 - 00:25:18.774, Speaker A: Okay, so the resulting interval will be 72 bits. And then let's define some functions. So this will extract the lower part of the interval and the upper limit of the interval. Okay, then we can define all the auxiliary functions, like is full set, this is just copy paste from the C plus plus code, okay, so just put the parentheses before the functions. And this is equal to whatever is in C plus plus, okay. And of course there is also we use empty set is sine, wrap set, whatever, but I'm not showing here. Okay, so let's convert sinextent to smt.
00:25:18.774 - 00:26:05.574, Speaker A: It's pretty simple. So we define this function sinextend that takes an interval of 64 bits and returns an interval of 72 bits. Then we do if then also if I get an empty set, I return an empty set so first line, then if I get a full set or signed wrapped set, I return that weird thing. So here I've just constant fold whatever API int is doing. So I didn't bother implementing API int, but those are constants. Otherwise we return the sine extents of the limits. And this is all what these eight lines of C plus plus are doing.
00:26:05.574 - 00:26:12.614, Speaker A: Okay, so I don't know if you agree, but it seems to me that it's a pretty standard transformation from the C plus plus scope to this SMT.
00:26:12.662 - 00:26:13.260, Speaker B: Okay.
00:26:15.550 - 00:27:08.890, Speaker A: And now we can ask well, is it correct or not? And now again we need to think what it means for this operation to be correct. And the correctness criteria is that if we pick one element of some interval and we do a sine extent, then this sine extended value should be contained in the sine extent version of the interval. And that's what we need to ask. So we define an integer and an interval. Then we say, well, I have some integer which is contained in some interval. And now let's try to find such an integer which the sine extent is not contained in the sine extent of the interval. This sounds a bit tricky at the first time you do this, but then it becomes pretty straightforward.
00:27:08.890 - 00:27:41.286, Speaker A: And now we can say, well, is this correct or not? And it says well and sad. So it means that sine extent is correct. So from now on you can rely on constant when sinextent, which is doing the right thing. Okay, okay, still pretty cool. It's correct. But if we implement sinextent as always returning a full set, it is always correct, right? Because it's an over approximation of the result. So it's always correct.
00:27:41.286 - 00:28:16.734, Speaker A: But then it's not very helpful for performing optimizations, right, because we want to know the tightest possible interval for the operation. So now let's do something different and try to prove that it's actually returning the best possible solution. Okay, so we are going to try to prove that it is optimal. Okay, this is a bit trickier. So let's see. So let's define two intervals. R is the result interval which is some other result interval than the one that we are computing.
00:28:16.782 - 00:28:17.282, Speaker B: Okay?
00:28:17.416 - 00:29:10.020, Speaker A: Then we say, well, find me some result interval which is smaller than what we are getting by applying sine x ten. Okay, but now we have to define that is also a correct solution, that R is also a correct solution. So now we need to go to the power of the forall quantifier and say, well, if for all possible numbers that are in this interval, n. The sine extended of this value must be in this result interval R. So this formula is a bit trickier than what we saw before. But to check for optimality, then we need to go for this for all quantifier. So I don't really expect you to understand the whole thing today, but you have the URL there and then you can play with this in your browser later.
00:29:10.020 - 00:29:23.094, Speaker A: In ztree, when you are using these quantified bit vectors, you should use this line.
00:29:23.132 - 00:29:23.720, Speaker B: Okay.
00:29:26.570 - 00:29:37.630, Speaker A: So this line means you should use a different algorithm. Okay, so this is just a ztree technicality. For other solvers, just use checksat as usual.
00:29:38.290 - 00:29:39.040, Speaker B: Okay?
00:29:39.410 - 00:30:25.580, Speaker A: And z three says, well, it's not optimal. Okay, so z three found a model where sinextent is not returning the best possible solution. But this solution is a bit tricky to read, right, because we define the interval as a concatenation of integers, whatever. So SMT has some functionalities to help us. So we have this eval which prints evaluation of expressions in the model. So we can ask, well, what are the limits of these intervals? And the limits of the result of the fan extend and it can give us some values. Okay, I'm not going to debug this video online, but the problem is that thing in bold, which is the int min.
00:30:25.580 - 00:31:09.814, Speaker A: So it is not optimal when the upper limit is int min. So last week, I don't know if you noticed on the mailing list, but I did this commit to synextend, which basically checks for this case. And now you have the URL and you can check that. In fact, now sinexcent is optimal. So not only we are sure that Sinexcent is correct, now we know that it's always returning the best possible value. Okay, so I have two URLs there because one is 32 bits and the other is only eight because 32 bits, it takes a long time to verify. So it's like, with eight bits is like a few milliseconds.
00:31:09.814 - 00:31:19.626, Speaker A: With twelve bits is six minutes. And with 32 bits, I didn't bother waiting. Okay, so I proved that it is optimal for twelve bits.
00:31:19.658 - 00:31:19.950, Speaker B: Okay?
00:31:20.020 - 00:31:23.226, Speaker A: And now we can pray that it's also optimal for 32 bits.
00:31:23.258 - 00:31:23.840, Speaker B: Okay?
00:31:26.530 - 00:32:25.810, Speaker A: Anyway, I think it's better than just not knowing what's happening there. Okay, so let me just share some of my thoughts for the future. So of course we'd love to have automatic translation from C Plus plus to SMT, right? And it's not actually very difficult because we saw that there is like a mechanical translation from the C to SMT. So we could have some front end that knows about llvm code and how it is structured and just transform automatically. The second thing, so I didn't talk about, but the functions in SMT are not recursive, so you cannot do recursion on the functions. But some solvers, including ztree, already have support for recursive functions in the form of these horn clauses. So I expect to be able to verify a lot more things in like a few months with these horn clauses.
00:32:25.810 - 00:33:29.510, Speaker A: The third point is floating point. So SMT solvers are also coming with a floating point these days. And I think this is a really cool opportunity for, for example, OpenCL, because OpenCL, the standard, defines the maximum error that the compiler can introduce when optimizing some expression. And with the SMT solver we can actually prove that we are respecting that bound for error, because if instombined patterns are difficult, like instomine patterns for floating point are very hard, right? So it's really hard to reason about floating point transformations, at least for me actually. Then I would really love to be able to verify a few more complex stuff. So I think scalar evolution, so constant range is already hard to implement, right? Scalar evolution is really hard.
00:33:29.580 - 00:33:30.054, Speaker B: Okay.
00:33:30.172 - 00:34:16.470, Speaker A: So it would be really nice if you could just verify that scalar evolution is okay because it has also been a source of bugs in the past because it's a really hard thing. And finally termination checking. So we have seen a lot of bugs like in combine and selection related stuff that do canonicalization of the IR, and it's really easy to implement a pattern there that it go back and forth and then we have some infinite loop. But today in verification community we already have some techniques to check for determination. So I think a good next step is to prove that insombine will never go into an infinite loop.
00:34:17.850 - 00:34:18.600, Speaker B: Yeah.
00:34:21.530 - 00:35:06.520, Speaker A: Okay. So just to summarize, I tried to convince you that there is already some potential verification technology that we can use today to avoid some bugs. And I think it's ideal for these kinds of what if I do this change questions, okay, because I want to optimize these little things. So what happens? So will the code be equivalent to the one that was before or not? I also leave here some links. So SMT also supports arrays, so I didn't go there, but you can also use it. And there's stack overflow. So there's a bunch of people there always answering questions about SMT, including myself.
00:35:06.520 - 00:35:23.640, Speaker A: But you can also email me directly if you want. And last but not least, I would like to thank codeplay for sponsoring my trip here, which otherwise wouldn't have been possible. And. Yeah, that's it. Thank you.
00:35:26.770 - 00:35:27.520, Speaker B: It.
00:35:34.210 - 00:35:36.510, Speaker A: Has a question. Did jump to the microphone.
00:35:41.090 - 00:35:44.034, Speaker C: It's asking over here.
00:35:44.072 - 00:35:48.900, Speaker A: I think my voice carries enough for the recording. Recording. Then.
00:35:53.590 - 00:35:54.500, Speaker B: You know.
00:35:58.410 - 00:36:20.654, Speaker C: My question is about the support for recursion in the solver in Z three. That's something that we wanted to use in a project ourselves. In fact, there'll be a talk today about our work, and we didn't do it because we couldn't find one that had reasonable support for recursion. You said they've been adding it. How well does it work and what can you use it for?
00:36:20.852 - 00:36:54.934, Speaker A: Yeah, so the recursion is coming along. So Ztree already has like three solvers for recursive Horton clauses. So Horton clauses are pretty expensive. So you can map like functions of C that have unlimited stack usage to horn clauses. And actually we have a front end that goes from C to horn closes. And the Ztri solver already works pretty well for certain theories. So it means that it can do integers, it can do booleans for bit vectors is not awesome.
00:36:54.934 - 00:37:22.958, Speaker A: But I can show you how to use it. And if you look to the software verification competition that is running already for three years, most tools are already using this. HORN CLoses solver there are already like Ztree, our own model checker also uses horn closes and so I think it's already very usable. And yeah, you can use it. I can show you offline.
00:37:23.134 - 00:37:24.660, Speaker C: I can talk to you more about that.
00:37:39.570 - 00:37:55.346, Speaker D: Is there any tool you could write your proof and extract C plus plus code from it? So you could write parts of instance combine in this file, which as part of compilation deprives it's correct. Or at least as part of.
00:37:55.528 - 00:37:57.154, Speaker A: Yes, make check.
00:37:57.192 - 00:37:59.074, Speaker D: It proves it's correct.
00:37:59.192 - 00:38:34.714, Speaker A: That's actually part of my evil plan. That's something that I've also been talking with Jacob for selection DAC. So it would be really cool if you could specify this thing in some DSL, right? And then we extract the proof and the C code all from the same place. It should work for instombine, for decligalizer, decompiner, whatever. I think that's like long term. I think that's the solution for llvm. Because right now instombine is just a huge chunk of code that it's insane.
00:38:34.714 - 00:38:40.590, Speaker A: It's very big. Okay, well, let's thank noon again.
00:38:40.740 - 00:38:41.100, Speaker B: Thank you.
