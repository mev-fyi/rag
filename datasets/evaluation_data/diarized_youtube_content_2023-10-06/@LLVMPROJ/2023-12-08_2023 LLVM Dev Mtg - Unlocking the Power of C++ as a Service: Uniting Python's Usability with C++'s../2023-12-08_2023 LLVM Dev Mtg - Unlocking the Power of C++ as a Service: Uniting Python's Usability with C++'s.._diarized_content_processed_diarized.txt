00:00:01.530 - 00:01:17.478, Speaker A: Good morning everybody. My name is Vasil and I'm a researcher with the University of Princeton in the field of energy physics at the particle physics Lab serum, located in Switzerland. I lead the compilerresearch.org group, where our primary focus is researching into foundational tools. And over the last decade we have developed an active C plus plus interpreter called Cling in the field of high energy physics in our data analysis workflows, and it's been in production since eight years or so, and it has been used to process more than one exabyte of data by around 10,000 scientists to publish around 1000 papers in one of the top tier conference in top tier conferences, and some of the recent results include the observational gravitational waves, for example. But in short, we like to build big machines and turn them on and see what happens. So C Plus plus has been a very important language in scientific computing because its design principles promote readability, reliability, efficiency, backward compatibility, which are vital aspects for any long lived code base.
00:01:17.478 - 00:02:32.638, Speaker A: And other ecosystems such as Python have prioritized better usability and safety, while making some trade offs on performance and backward compatibility. And that has led developers to actually think that there is a binary choice either to use something that does performance or something that's readable. So in our efforts, we tried to understand whether we can combine the power and the expressiveness of Python and the power of C plus plus without creating a new programming language. And our goal was to leverage the existing exploratory programming infrastructure that I have developed in the field and make it available to other scientific domains and industries via LovM and open source. So I'm going to talk a little bit about what we have landed in LovM mainline, and this is mainly the support for incremental compilation with Clunk repl. So it all started when we realized that we have accumulated enough technical debt and knowledge and things that are working for us that we decided to start looking into how to make it available to the broader public. We initially started with an RFC to the LVM community in 2020.
00:02:32.638 - 00:04:00.650, Speaker A: There was a positive outcome, and essentially we started shortly after landing some infrastructure. The first thing that happened, it was in LVM 13, the initial version of the incremental compilation infrastructure, and it has seen gradual improvements ever since. Since the first LVM 13 release, we had approximately 30 developers that have been contributing in that area. So in terms of like, maybe the most boring part of it was that we managed to offload some of the essential patches that we have developed in our internal forks, because clunk Repl actually allow us to describe the things that we want to do and to have an environment which allowed us to test the things that we wanted to be able to support. And most of that work has been released in LVM 17. So during the project, during our last three year project, we managed to upstream all of the essential patches for incremental compilation, and that led actually internally our field to upgrade the upgrade cycles to become much shorter. For example, just to give you an example, the upgrades went down from approximately one year when we upgraded from LVM five to LVM nine to several months, from LVM nine to 1113, and to several weeks from LVM 13 to 1116.
00:04:00.650 - 00:04:46.760, Speaker A: Now what our development clank repo drove over the years was that first we had now automatic completion for the prompt that improves the overall user experience and it will be released in LVM 18. Please go see threats. Talk about that. We implemented a shared memory manager enabling efficient out of process execution to improve system stability that's available in the LVM 15. We also have implemented some infrastructure in program reoptimization. You probably can learn more if you see the talk from soon who came from yesterday. That's going to be probably published in a couple of weeks from now.
00:04:46.760 - 00:05:48.810, Speaker A: We also did some developments on the jitlink infrastructure, which essentially we enabled the Windows support in cough. We did LVM 64 on Unix and helped somebody else do LvM 32 on Unix as well. So we had the first jigid program in RISC five, which is kind of nice. And we also helped external companies, IBM in that case to provide a PowerPC backend that's going to be available in LVM 18. Now the other thing that we started looking into is how we can interactively use CUDA. And in that case we essentially implemented a new way how we can pass PTX code down to the device. And the clank repo actually found a bug or two in the clank CUDA implementation.
00:05:48.810 - 00:06:56.050, Speaker A: Now that we have built all of the basic infrastructure that we have now, let's look into something that's more interesting and how to make C plus plus and python interoperable. So the whole goal was to be able to run programs in mixed mode. So essentially in that mockup here that we have initially imagined in our proposal three years ago was the fact that in cell one you can see that we use C plus plus to define some C plus plus entity. In cell two you can see that this entity is used within Python to instantiate an SD vector of that kind and assign it to a Python variable. And then from that point onwards you can use the Python variable the way that you would use it in normal Python. And we wanted to go a little bit further than that. And you can see in line four, in cell four we can derive from a C plus plus class within Python.
00:06:56.050 - 00:07:57.522, Speaker A: And then to make things even more interesting here is to be able to write in the same session CUDA code that's being able to do some high performance computation and then bring it back either to Python or C plus plus. So this was our original idea here. And of course the challenge in these systems is that how do you cross the language boundaries? And in many cases people do marshalling or like some implementations do, print variables on text files and then read them back. So here what we want to do is we want to do the incremental compiler infrastructure to cross the language barrier in an efficient way. And this is what we call compiler as a service. So essentially you get a bunch of classes and use LVM and clank as libraries so that you can kind of create a uniform execution environment. And what we have done afterwards is we enabled Clank Repl in data science.
00:07:57.522 - 00:09:08.118, Speaker A: So there is this Jupyter notebook interface where data scientists would. That's a de facto standard where if you implement a certain interface that you can enable your executor to run code. So the system would pass code to us, and then that's Zeus clank repl, which essentially enables incremental C plus plus with interoperability extensions in Jupyter by implementing that kernel protocol. So how we did that is we use a technology that was in house developed which is called cppyyy. I didn't invent that name, but you can feel the pain in it is a cpython and pypy extension using their C API. It offers automatic on demand mapping between Python and C concepts. So the way that it's done is that we use the way that the Python parser infrastructure works and then we hook up our interpreter, because now we have a dynamic C plus plus compiler behind, which is able to provide us with lookup resolutions and build the relevant C plus Python classes.
00:09:08.118 - 00:10:01.130, Speaker A: So essentially while we are parsing we can do mapping of C plus plus entities. And this approach is quite powerful, has been in servicing for 20 years and it has had a bunch of upgrades and so on. But the biggest advantage is that you can apply it to any library without expecting the author to provide Python bindings or any other support. Now what we did within our project is that we wanted to move that project closer to the LVM orbit, and that means that we wanted to build like a staging area for a CPP interop, which essentially targets interoperability primitives for C enable cross talk in an automatic way. You don't need to write swig, you don't need to write any wrapper classes, but it's not limited to Python. It can also work for D or Julia. And I think one of the packages in Julia follows a similar approach.
00:10:01.130 - 00:11:04.862, Speaker A: The library allows replacing the cppyyy backend with a specialized and more robust interrupt, and that allows us to move to newer C plus plus versions or to newer coder versions, and to implement features in much faster manner. What we have done also in terms of tutorials and community outreach, and if you are new to the compilers or service infrastructure, you can go and visit that tutorial. It has a nice set of examples where you can start off and see what kind of programming model you can use there and what it could enable for your particular use case. Over the years we have been very open. So we have hosted like open virtual team meetings and we have been having monthly meetings. We have had invited speakers from companies such as Apple, HDR, Quantstack, Max Plank, LBL, CERN, anti Electronic arts. We have been mentoring students, a lot of them around 20 or so, and we have had three technical documentation writers.
00:11:04.862 - 00:11:51.466, Speaker A: Now I'm rushing the slides a little bit because I really want to show you a demo, because a demo can replace 1000 slides. But what this concept enables is many people think it's a repo, so you just type stuff in it and then it gives you back and that's it. So the whole point is that now we have infrastructure that allows to think the way that we develop in a different way. Because normally when you develop, you have these very distinct phases between development, deployments, the starting phase, and the execution phase of the program. And normally when you develop, you have an abstract machine in mind, an abstract user base. The more users you get the better. But then in the end, when things get deployed, things become very concrete.
00:11:51.466 - 00:12:47.714, Speaker A: You have a specific target machine, specific set of users, and you can do much more than what the static compiler can do. And normally the static compiler would let you, if not force you do a lot of computation offline, and that's good because you would be folding up operations. However, there are opportunities that we can use, misuse or abuse depending on our purposes. But at these different phases we can have different set of optimizations that we can apply, for example, the head of type optimizations, that when you know your target, you can do certain things at deployment time, just in time. When you run your binary things, you upload it into memory. You can do certain amount of optimizations there as well. And then the continuous optimization that we saw from Sunho's work yesterday allows us to kind of record the execution information and then make use of it.
00:12:47.714 - 00:13:36.514, Speaker A: And here the study compiler and the compiler as a service can help each other. It's not like either or in these cases. For example, in certain operations can be deferred until runtime. And that's like, for example, when you want to instantiate templates in libraries, instead of like having this combinatorial explosion where we need to pre instantiate a lot of templates, we can defer that operation until runtime and we're going to gain performance, or we're going to reduce our binary sizes. And in reverse, we can record information about the runtime or the target architecture and then feed it back within the compilation process. And this is what PGO does in a certain way. So we have achieved quite a lot over three years and I'm pretty happy.
00:13:36.514 - 00:14:20.622, Speaker A: But there's a lot of things that we can do. One of the things is that we want to continue the open meetings policy, we want to continue bug fixing and stabilizing Clank Repl, we want to continue developing tutorials, and then we want to reach out to more people to actually help them understand what they can use that technology for. Now, so far I have sailed well, but I want to give you a live demo, how things actually are. Before that, I'll give you some context. So in the first demo we're going to see our mockup. So I don't want to go through it again here, but essentially our mockup. Now that we Photoshop, we can get it to work.
00:14:20.622 - 00:15:07.466, Speaker A: I'm going to show you a very quick demo on OpenMP. So how can run like openmp programs within Jupyter? And then in the end I'm going to show you a quick demo on how you could mix Python, C and Cuda within the same cell to do different operations. We have another one, which I don't think I'll have time to show, but we can scale to bigger workflows or to solve bigger problems. Like in this case, it's a Kalma filter. Now, if I move my browser, can you guys see it? Yes. So here's a Jupyter notebook system that. Here's how it looks like, and each cell is here written in a different programming language.
00:15:07.466 - 00:15:39.078, Speaker A: You can see here that we can execute the C plus plus thing and then we can go with this ampersand. Ampersand Python. And then we say, we are now in Python mode. So now you can see that here we say cppy SD vector of S. So basically s is like a metaclass that comes from the CPR puff land. And what's going to happen here is that we're going to instantiate that template and stuff it back like this. No, I need to just restart it.
00:15:39.078 - 00:16:09.794, Speaker A: The kernel I did something like. So this is one of the things that we also want to fix. Where I don't see it here. This is, I don't see it. I know which is this one, but that's going to run all the cells anyway. So the whole point is that here the python cell, we got a template instantiation here and then we can use it within Python. Right.
00:16:09.794 - 00:17:16.162, Speaker A: Now what we did here, and the next element is that we derived this class from python and then use it and then we can print it. Of course the runtime system tells us that it cannot help us with destroying cast because it doesn't have a virtual destructor. And here we can go and look at the cuda kernels so we can get that data, put it into Cuda, then do all of the allocations and so on here, and then execute, and then print the result and then stuff it back. So this is how it works in mixed mode. Now if you go to the next example here, we are in different mode and we have openMP, so we have like a compiler behind and our OpenMP program can start in normal openMP setup. Then what we can do here is that we use our library to load the OpenMP runtime, then run it and we get multiple threads. And this works in binder and all these kind of systems.
00:17:16.162 - 00:18:02.280, Speaker A: But I'm showing it on my personal notebook across ocean because I have a Cuda GPU and Binder doesn't support it. Now if we go, let's do something more useful. I guess we have an image that we want to make black and white essentially, and de emphasize on here the detector of CERN, and emphasize a little bit more on the Alps. Right. And this is like a highly parallel process, so it would make more sense to actually do it in a parallel system like Kuta. Right. But at the same time you can see here, that's the kernel that would brighten or underexpose pixel based on my threshold value.
00:18:02.280 - 00:19:45.900, Speaker A: Then I'm going to set some basic settings. I'm going to rerun it. Sorry about that, I don't know what's happening. It's the live demo effect. So I can run it here and then you can see, I can install certain python packages, I can install pillow, I can install numpy matpotlib and so on. Then I can go in the domain of Python, open my library, go to numpy, do some post processing and so on, then go back in C plus plus and then run my computations here, and then go back from the host to the device and we can see the image that we can visualize. How do I visualize this? Now hold on, this has been, but I need to write the markdown code, then go to Markdown, then into results.
00:19:45.900 - 00:20:22.170, Speaker A: It has a weird caching problem in Jupyter that I need to figure it out. Is that what I need to do? Yes. There you go. And now we have this overexposed. So a big note of gratitude to the community that has been supporting that work and invested a lot of intellectual work and thank you, thank you. Questions?
00:20:24.300 - 00:20:41.410, Speaker B: I have a question. So we all know that C plus plus functions generally don't just take things by value, right? You have constref, you could take unique pointers. How is this going to show up in the python world?
00:20:42.100 - 00:21:14.840, Speaker A: So the way that handled is that we can detect whether, because we are in the compiler and we can detect whether we have a smart pointer. So the detection is like something like, if you have specialized operator arrow, like an operator star, that this means that you're doing something funny with the memory. This means that Python doesn't need to do anything. So we leave that the C plus runtime to deal with it. And for the cases that you do like row pointer allocations, then without Python's garbage collector too.
00:21:14.910 - 00:21:28.192, Speaker B: So same goes for the STL containers versus the Python. Okay, and I have another question. Does the Python author need to be aware of C ownership rules? I guess the answer is yes in this case.
00:21:28.246 - 00:22:00.664, Speaker A: Right. We do most of the things automatic when we can, for example, these smart pointers. We know that we don't need the Python developer to do anything, but there are certain patterns that you use that you need to export it. What we do is we look at the LVmir and then we try to guess and see whether we need to do something with the ownership. But many of the things are handled well. Of course, if you have a smart person on both sides, it can do it better.
00:22:00.862 - 00:22:15.630, Speaker B: Yeah, say if you have struct s getting passed to a C plus plus function that moves out of it. In the python world, you should also treat it as move out, but unspecified but still a valid state or something like that.
00:22:16.100 - 00:22:18.732, Speaker A: That's an interesting discussion. I would like to follow it offline.
00:22:18.796 - 00:22:19.970, Speaker B: Yeah, thank you.
00:22:24.260 - 00:22:31.476, Speaker C: Does Python see only subset of C plus plus language? So for instance templates, Python sees what.
00:22:31.498 - 00:22:44.570, Speaker A: It wants to see. For example, when you do this std dot, so std is not found, so you trigger a lookup and then all this comes from the C plus plus infrastructure. So essentially yes.
00:22:46.620 - 00:23:01.400, Speaker C: C plus plus entities C plus plus templates that are instantiated and they are erased. Right. You cannot reference from Python.
00:23:01.480 - 00:23:14.368, Speaker A: We can because we have a compiler as a service. So your compiler run as long as your Python interpreter runs. So all of the templates instantiation stuff is in memory and it's available so you can curate whenever you like.
00:23:14.534 - 00:23:24.500, Speaker C: Interesting. And how hard it is to add another language which has backend into Llvm to such infrastructure.
00:23:25.320 - 00:23:55.370, Speaker A: I guess it depends on the language. I guess we tackled the most complicated case where we have like a static language like C and purely dynamic language like Python. So we have like a layer in between that you can query so that everything below that layer is works straight out of the box. So which language do you have in mind? Okay, I don't have knowledge about Ocaml, but thank you, Vasil, thank you.
