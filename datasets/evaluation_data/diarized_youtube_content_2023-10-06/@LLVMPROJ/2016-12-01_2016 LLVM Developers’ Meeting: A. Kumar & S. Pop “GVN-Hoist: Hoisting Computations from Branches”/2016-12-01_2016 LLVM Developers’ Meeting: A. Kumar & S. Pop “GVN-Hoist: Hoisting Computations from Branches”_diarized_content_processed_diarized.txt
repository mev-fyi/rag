00:00:00.090 - 00:01:05.234, Speaker A: I'm Sebastian Popp, and here is my colleague Aditya. We are working at SARC, that is a Samsung company that is located in Austin, and we are working mostly on compilers and benchmarking. We are working on actually designing microprocessor based on Arc 64, and we are doing tuning for that. It is called Exxonosm one. We have Evandro here that is the maintainer of that port, and we are doing various things to improve the performance of benchmarks on Exynosm one. So we are going to speak about some work that we did earlier this year that is related to hoisting expressions out of branches. So CFG simplify contains already a path that does this.
00:01:05.234 - 00:02:05.490, Speaker A: It hoists computations from basic blocks, moves them above at their merging point. So whenever there is a branch and computation that happens to be the same in the next base blocks, we are going to identify those expressions as being computationally equivalent. And cfg simplify would hoist them just at the beginning of just above the branch. And this is done mostly by using operand's equality. It has some limitations, so it is not perfect, but it works very, very nicely because it's very fast. It is so fast that by just removing this code from the compilation flow, it speeds up the compilation time. So by disabling this code, hoisting actually it slows down the compiler.
00:02:05.490 - 00:03:26.394, Speaker A: This path is run at least six times in the compilation flow on each function, and it represents probably less than 1% improvement in the compilation time. So it has several limitations, mostly due to the fact that it has to be very fast, and it has to also detect the equalities. Based on some ad hoc algorithm that looks at operand's equality. The algorithm stops at the very first difference. So there is no easy way to improve the algorithm to still maintain it in a linear time and to make it catch more cases. So one of those cases I'm going to speak about one of the cases that triggered our attention to this CFG simplify hoisting, and this justified our involvement in making this algorithm more powerful. This program contains expressions that cannot be hoisted by the current GVN hoist algorithm.
00:03:26.394 - 00:05:03.600, Speaker A: If you see, by going in sequential order from the beginning of the basic blocks, we have a times I on one side on the then close, and then b times I as a first operation in the else block. And the GVN hoist algorithm would just stop, it will not continue and it will not find the redundancy, the next expression, as being redundant and candidate to be hoisting. So here is what we wanted to achieve, it's to hoist those expressions above the if condition. So this is what the hoist algorithm would do. So the reason why we wanted these expressions ahead of the condition is because the condition uses I and I is computed as a division, and the division on some processors, most of the processors takes a long time. So by scheduling in between the computation of I and the branch condition, more computations, we are going to have the processor, we will have a better performance because we would have more things to be computed in between the computation of I and the condition on which we branch. So this is what we are going to try to explain.
00:05:03.600 - 00:06:19.922, Speaker A: The algorithm that we did is an extension of what exists already in CFG simplify, and we are going to try to remove all the limitations that CFG simplify has. So we are going to try to catch all the possible hoisting opportunities throughout a function. And this is going to be across several basic blocks. And we will catch also expressions that we will need to hoist across several basic blocks to a common dominator. We also removed the limitation of hoisting expressions past load store with side effects. And for this we used the memory SSA to look at the dependencies, to look at the dependency analysis part. The overall effect is that we are going to reduce the code size and we are going to also have some speed ups by exposing more ILP, because it is actually an hourly scheduling algorithm.
00:06:19.922 - 00:06:35.740, Speaker A: So we are going to see some numbers on this. I'll let Aditya to speak about the algorithm. Can we have the other mic on?
00:06:36.830 - 00:07:16.450, Speaker B: Okay, thank you. So this GVN hoisting algorithm, it is an optimistic algorithm for those compiler folks who have people who already know about very busy expressions. There are many classical optimizations which hoist very busy expression in the common denominator. Right. What we are doing here is following an optimistic approach. So we are not only considering very busy expressions, we are considering all the expressions which compute same value basically across the entire function. So we consider all the instructions amenable for hoisting, unless they are proven otherwise.
00:07:16.450 - 00:07:52.450, Speaker B: So that is the reason we started this. So this gives a lot of code size improvement. Right? So the first step is to value number all the instructions. For value numbering, we value number scalars, loads, stores and calls. Why? We have to illustrate all of them separately, because the GVN infrastructure currently doesn't have a good mechanism to value number stores and loads. We'll talk about that later. So we just rely on the current GVN infrastructure, but we also add our own functionality.
00:07:52.450 - 00:08:30.400, Speaker B: The second step is once we have the value number of all the instructions, we find out which are the identical ones, which is very easy because those are numbers. So there is an easy hash map lookup and we find out all the expressions which have the identical value number. Once we have the expressions which have the same value number, that means they compute the same thing. Now we have to find a common insertion point, basically a common dominator or a nearby dominator, where we can hoist those instructions. So this is a very basic overall view of the entire algorithm. So the first step is to value number the expressions. Right.
00:08:30.400 - 00:09:35.882, Speaker B: Now it's a very simple example. Except the loads, all of the instructions are scalars. So the value number infrastructure of LLVM will just value number them sequentially. If they are not computing identical values for the loads and stores, the current GVN infrastructure of LLVM will for each load and store it gives a new value number, which is not good because otherwise we are not going to find out identical loads. So we have to just improve this GVN infrastructure. So what we did was, since if the address of the load is the same, then that means it is loading the same value if there is no other side effect in between, right? So what we do is we value number the pointer operand in the load expression. So for stores we have to value number the pointer operand as well as the value that is being stored.
00:09:35.882 - 00:10:17.714, Speaker B: That is how we value numbers, loads and stores for calls. The current GVn of LLVM is already giving a value number, so we just rely on those value numbers. Okay, once we have the value numbers, we put them in a hash map, which each bucket in a hash map is a link list. So all the instructions which have the identical value numbers will end up in the same link list. For each set of expressions which compute the same value, they are the potential candidates to be hoisted. Since this is an optimistic algorithm, we try to hoist as much instructions as possible. So we try to find a common insertion point.
00:10:17.714 - 00:11:18.974, Speaker B: So the first thing to do is, optimistically, we'll find a common insertion point for all the expressions which compute identical values. Obviously it is not always possible to find a common insertion point, especially for loads or expressions with side effects. So what we try to do is we try to partition them, and we try to partition them in a way that the nearby expressions, I will explain the meaning of nearby the expressions which are close together or nearby each other. It is easier to find a common insertion point because the insertion point will not be too far away from the expression. The way to find nearby expressions is to sort them in the order of their DFs in numbers. So in Llvm it is very easy to compute DFs in number for each expression. Once you have the DFs in number, we sort all the instructions having the same value number in their DFs in number, and then we just apply a greedy algorithm.
00:11:18.974 - 00:12:09.154, Speaker B: So we take two instructions, we find okay, these are okay to hoist, then we go to the third one, and then we find a common dominator for all the three. Then if we don't find a common dominator for any instruction, that is our partition. So we just separate the list from the other one, and then we start continuing the partitioning for the remaining number of instructions. That is how we try to maximize the potentially hoistable candidates. Okay, so once we have found out the hoistable candidates and we found a potential hoisting point, now is the time to move the instructions. For scalars. It is very easy because they will not have any side effect, so we can just hoist them and delete all the other instructions.
00:12:09.154 - 00:13:15.574, Speaker B: We'll just move one of them to their common dominator and just remove all the others for loads and stores. What we do is since load and store rely on gap get element pointer in LLVM infrastructure, and gaps themselves are scalars, but we do not hoist get element pointers for reasons we'll explain later. What we do is we hoist the get element pointers only when there is a load instruction corresponding to a gap. So once we found a set of loads which have the same value number, we try to hoist them, and since the gaps were not hoisted, we rematerialize them early, before the hoisting point. And once we have gap available and then loads can be hoisted for stores also. We do the same thing, we rematerialize the gap first and then we hoist the stores. Of course we have to check the side effects and other invariants for loads there should not be any side effects, and for those checks we use the memory SSA infrastructure of LLVM.
00:13:15.574 - 00:14:12.590, Speaker B: It's a very recent the GVN hoist is the first user of memory SSA, and we help uncover few bugs in their memory SSA infrastructure for loads and stores. Once we have hoisted the loads and stores, we have to update the memory SSA. Memory SSA is a representation where the loads and stores are given an SSA like representation. That way it is very fast to switch between depths and use just like SSA representation of scalars. So that helps speed up the compile time a lot. So we have to update the memory SSA so that we can move on to the other loads and store and without recomputing the entire memory SSA mod tree, basically. So we have to update SSA as well as we have to update the memory SSA for loads and stores.
00:14:12.590 - 00:14:17.790, Speaker B: So I guess, Sebastian, we can continue, right?
00:14:17.940 - 00:15:07.758, Speaker A: So just a few comments about why we do not hoist get element pointers if we start hoisting. Get element pointers. Get element pointers are very easy to hoist because they are expressions, arithmetic expressions that do not have side effects. Now we are going to hoist them and then we are going to end up not being able to hoist the load or the store that they index in. And those loads and stores cannot be sometime moved on top because there are side effects on the path of hoisting. So we end up with having a long live range where the get element pointer is computed and where it is used. And then we are going to just pessimize a lot of code.
00:15:07.758 - 00:16:14.686, Speaker A: So this is one of the reasons why at minus two or three in optimization levels we are going to disable the hoisting of get element pointers. We are going to only do the hoisting of get element pointers systematically in minus os or minus oz. So it is mostly to address a problem in register allocation. So we could do rematerialize the gettermine pointer, but it's not done in the register allocator. So it's a deficiency probably of the register allocator. The other limitations that we had to implement to make the GBN hoist pass faster is to limit the number of blocks on the path in between where we hoist from and where we insert the hoisted expression. And that is mostly if you end up hoisting from the entire program, you may end up increasing the live ranges.
00:16:14.686 - 00:17:14.354, Speaker A: And also compilation time would increase a lot. Another limit on the number of instructions in between the initial position and from the position where we hoist the expression and the beginning of the basic block. And that one is again related to register pressure and compilation time. So if we hoist expressions from more than, say 100 instructions down in the base block, we will end up with most likely register spills because we increased the live range too much. So there are two limitations, and they are both for register pressure and for compilation time. We also have a limit on the number of dependent instructions to be hoisted. These are mostly deficiency of the current GVN implementation.
00:17:14.354 - 00:18:52.870, Speaker A: The current GVN implementation does not allow to find a value number that is identical for instructions that are dependent on each other. Like for example for a chain of dependent instructions we would have to rerun several times the Jivian hoist algorithm because we would hoist one of the expression and then the other expressions would have to be value numbered again to be able to recognize them as being redundant as computing the same thing. So for the moment I have the list of flags that we have to tune all these heuristics and for the moment, the second bullet from the bottom is the one that limits the number of dependent instructions. So we run the algorithm at most ten times. So this is again limitation of the current GVN framework and a limitation of our algorithm that is based on this GVN algorithm. So we do have some differences in between whether we are optimizing for size or optimizing for speed. So for Os and Oz we allow to hoist the get element pointers.
00:18:52.870 - 00:20:36.594, Speaker A: And I'm also thinking to remove other limitations that are due to register pressure for OS and Oz. So these may change in the future. So based on experimental data we may end up doing some analysis that is going to improve the cut size at OS or oz. All right, so these are the flags that we have to tune the heuristics we're going to speak about the elevation of the GVN hoist. So mostly we have seen that it is a very fast implementation, so it's less than 1% of the compilation time when compiling the full test suite and running it through Valgrind counting the number of instructions executed by the compiler compiling the test suite for X 86 Linux, we have 14 billion instructions more out of a trillion and a half. So it's a 1% compile time cost. We also hoist more expressions than the CFG simplify algorithm, so the 25,000 is on top of the 15,000 that CFG simplify already detects.
00:20:36.594 - 00:22:39.578, Speaker A: So it's mostly if we combine the two. So right now we left in place the CFG simplify because it is run six times in the past pipeline and we run the GVN hoist algorithm once before the inlining. So we end up hoisting a lot of scalars more loads than I was expecting to see a few stores mostly because it's hard to do that and a few calls and the calls are even harder than the stores to be hoisted. And for a total of instructions removed of 34,000 we have seen some code size reduction in this is again the test suite compiled at minus three for XT six Linux and we have also seen some of the benchmarks regressed in code size, and this is due to the fact that the GVN hoist is running very early in the pass pipeline, and there are a lot of side effects, notably the inlining pass that may end up inlining more because it reduced the number of the size of the functions. So we end up inlining more and the code size may increase. In that case, and I think we have quite some open questions that we could ask several other people from the community to answer. I certainly don't have an answer for these, except if I'm running a lot of experiments, and I have also heard quite some people complaining about some compilation time regressions.
00:22:39.578 - 00:23:56.578, Speaker A: We addressed several compilation time regressions in the past, and we are ready to also address new regressions as they come. We are wondering whether to run the GVN hoist pass several times. It is a very cheap pass to run, so we could probably run it a little bit later in the pass pipeline. Once we do inlining, we can probably schedule another hoisting pass that could catch some more opportunities. We were also thinking about removing the CFG simplify, but then the question is why removing something that is very fast and that makes the compiler even faster by running it? That's an open question. We have also somebody from arm working on the syncing based on GVN syncing, so it's improving the existing CFG simplify syncing pass by taking the same skeleton as our GVN hoist pass. And there are some interactions in between hoisting and syncing.
00:23:56.578 - 00:25:18.434, Speaker A: Which one are we going to do first? Are we going to hoist first or are we going to sync first? And if we are going to decide to merge the two passes together such that we are going to hoist and sink together based on register pressure, then I think it would be a better idea. These two hoisting and syncing passes are actually a scheduling technique, so they are providing the same thing as what the schedulers in the back end are providing. So it's moving code around, has take in consideration the target info, and for the moment the middle end does not care about looking at the target, querying the target, whether it's going to be beneficial, or. I had somebody from Qualcomm, Brendan Canhoon, who complained about the fact that by doing hoisting we regressed in performance one of the benchmarks that they care for. So if you see any of these benchmarks regressing in performance and due to the hoisting, please open a bug and assign it to me, I will fix.
00:25:18.472 - 00:25:19.060, Speaker B: It.
00:25:21.910 - 00:26:02.160, Speaker A: I also spoke about making the GvN hoist a little bit more aggressive for West Nozz, mostly by removing some of the limitations in the cost model. And we may need a better GVN implementation because it has some limitations. We also spoke about the memory SSA. We found the use of memory SSA very easy, and we found it also to speed up our paths. So it is easy to use and it is much faster than the existing alias analysis, memory dependence. So please use it. Thank you.
00:26:12.210 - 00:26:13.200, Speaker C: Any questions?
00:26:16.650 - 00:26:35.386, Speaker D: Hi, Pelspandlevsky. So I have a question. How does GVN hoist works with GvN? I mean, I didn't understand quite how is it another pass? Is it using GVN or what's the whole how it's built?
00:26:35.568 - 00:27:26.330, Speaker A: So we value number the operations. So instead of doing equivalents of operands, we're going to ask the GVN algorithm to number the operations and detect those operations that are similar. And based on that, we are going to hoist them, we are going to consider them equivalent, and they are candidates to be hoisted. So we are using GvN as an analysis pass, and the GVN implementation in LLVM contains pR e pass. This is why some people in the community think about GVN as being the PR e algorithm, but it is not, it is an analysis phase. So we are using GVN as an analysis, not as a partial residence elimination.
00:27:27.710 - 00:27:30.250, Speaker D: But you're reusing the mean.
00:27:30.320 - 00:27:33.342, Speaker A: We are using the GVN implementation, yes. Okay.
00:27:33.396 - 00:27:45.230, Speaker D: Yeah, I guess my concern was that the problem with GVN is that it can't be just analysis because there are just too much going on there and it has to do all those transformations.
00:27:45.390 - 00:27:48.734, Speaker A: Okay, thank you, Jeff.
00:27:48.862 - 00:28:05.062, Speaker C: So I have Jonathan Springer at a question. This is an optimistic optimization, right? So you may hoist something into a place where it would not be executed otherwise. In other words, you maybe host hoist it from an inside of you. Do not do that.
00:28:05.116 - 00:28:11.850, Speaker A: Okay. It is not hoisting. So if it appears on all the paths.
00:28:12.190 - 00:28:15.898, Speaker C: So if it's post dominated by all the uses, then you hoist it.
00:28:15.984 - 00:28:27.200, Speaker A: Yes. Okay. So we make sure that if we hoist, we have that expression appearing on all the paths from where we hoist it to the end of the function.
00:28:27.730 - 00:28:28.974, Speaker C: Okay, thank you.
00:28:29.012 - 00:28:37.958, Speaker A: So it is not specimizing the execution of the program, the running time of the program. Thank you, Jeff.
00:28:38.074 - 00:28:56.470, Speaker E: Yeah, sure. Just a real quick question. This seems to have a lot of similarities with Cliff click's work from the past. Have you guys evaluated that and seen try to compare what he did versus what you did, because his work in particular has a hoisting and a syncing phase to minimize register lifetimes, and it's essentially a scheduler.
00:28:56.810 - 00:29:02.940, Speaker A: Yes, this is one of the papers that Aditya brought up and we read that paper.
00:29:03.710 - 00:29:51.946, Speaker B: So his paper is like two paper in one, basically GVN and global code motion as well. So we only implemented the hoisting part most probably because it was more interesting from a benchmark point of view. But James Malloy, he started working on the syncing part and then he initially implemented in the CFG simplify, which is just operand comparison, not very efficient approach. But he got motivated by our GVN hoist and he said maybe I can just latch onto your GVN hoist and we can start working on that one. So I think it is still a work in progress. I'm not very updated about that part, but hopefully it will be together in one GVN scheduling kind of algorithm in LLBM. So we'll see.
00:29:52.048 - 00:29:53.180, Speaker E: Great, thank you.
00:29:56.390 - 00:30:42.258, Speaker F: So I have a question about the fact that you limit kind of GVN because of what you mentioned, register pressure, live ranches and those kind of thing. And apparently your pass runs quite early in the pipeline. And this is usually, I tend to view this part of the pipeline as canonicalizing the IR, simplifying it as much as possible. But you are putting some limitation that seems very target specific or backend specific. Doesn't this belong to syncing as you mentioned, that could run later code gen creeper or those kind of transformation that could undo what you're doing later if they are not profitable. You mentioned that you may want to fuse them which goes in the other direction. So I want you to have your take on that.
00:30:42.424 - 00:31:47.640, Speaker A: I think it is very hard to undo what syncing or hoisting are doing very early. And by undoing it, there are some simplifications that appear like for example after, in the example that I had, after we transform it in this form, we end up if converting the condition there and then undoing data is almost invisible. So bringing back the control flow is pretty hard to undo. So I'm not sure that this scheduling can be undone. So we had the exact opposite problem. The fact that the CFG simplify hoisting part of it refused to hoist these instructions and it was just syncing them. So we had to improve the hoisting algorithm to make sure that it gets hoisted because we were not able to undo the transformation later on.
00:31:49.530 - 00:31:58.650, Speaker F: That's kind of an issue because you could have Dir in the first place in this form. And what you're saying is we don't have a way to generate the optimal form that we want from this IR.
00:31:59.150 - 00:32:00.380, Speaker A: Okay, thanks.
00:32:03.550 - 00:32:23.380, Speaker G: Hi, there's another pass in LVM that also works on hoisting loads in stores. The merge load store motion pass. It has a variety of limitations. It only looks at diamonds and it limits itself to some fixed number of instructions. I was wondering if there's any chance that the new GVN hoist work could replace it.
00:32:25.190 - 00:32:45.100, Speaker B: We want to remove that path, actually, it is very small pass and it doesn't catch many cases. Right. I guess it was a hack that pass, load, store, merge, whatever pass. Right. We have looked into that pass and I think GVN is a much bigger superset of that pass, so it'll be good to just remove that one.
00:32:45.470 - 00:32:46.620, Speaker G: All right, thanks.
00:32:56.190 - 00:33:13.198, Speaker A: So I don't quite understand why you put limitations based on register pressure, but undo them for code size. Wouldn't you expect more spills and reloads generated because of register pressure?
00:33:13.374 - 00:33:18.930, Speaker B: I guess that is an open question, it has to be backed by some experimental result, right?
00:33:19.000 - 00:33:53.790, Speaker A: So the way we tuned this is by running benchmarks. And we looked at both x 86 64 Linux and AR 64 Linux. And we run the test suite, llvm test suite. We run CPU 2000 2006 and some other proprietary benchmarks. This is how we tuned it. And those numbers that appear as the default in the flags are actually just byproduct of just experimentation. So we have not looked at target info to tune the bus based on the characteristics of the machine.
00:34:05.170 - 00:34:26.630, Speaker H: It's great that you're preconscious about the live ranges register pressure, but then kind of bigger question that should the LVN just go, hey, implement the ssapie which was published later in the. Then once for all, eliminate both full redundancy and partial redundancies.
00:34:31.370 - 00:34:54.716, Speaker A: We do have a reimplementation of the GVN hoist. Daniel Berlin is the one who worked on that. I don't know if he's around any more questions. Then let's thank the speaker again.
00:34:54.818 - 00:34:55.130, Speaker B: Thank you.
