00:00:14.890 - 00:01:02.266, Speaker A: Hello, my name is Ivan Erke and today I want to present you the CUDA C Plus plus extension for the interactive just in time compiler, cling, which enable interactive gpu development on Nvidia graphic cards. At the beginning I want to give you an introduction in cling. How you can use cling cling is a terminal application like the Python interpreter. You can type in your code, execute it, and depending on the code you get some results. Sometimes cling is also named as interpreter, but it's not correct. It feels like to work like an interpreter, but it's a just in time compiler so it can optimize code. There are also some two other possibilities to use cling.
00:01:02.266 - 00:02:09.478, Speaker A: So the one is including as library in your application and are others like Python using cling in a Jupyter notebook I switch to my terminal where I already started the cling and I demonstrate how it works. At the beginning I define a simple variable and I do addition on it and I get a result. I can also define some functionality. This function simply returns a value, for example, and I execute a function and I get a result as expected. Let's switch to the Jupyter notebook for everyone who don't know Jupyter notebook in Jupyter notebook you have cells in a cell. You typed in your code and then you can execute a code and depending on the cell you get a result output under the cell, the state of the topla and also the memory space is shared between the cells. So let's start with the classic example, hello world.
00:02:09.478 - 00:02:38.894, Speaker A: I execute it and I get like expected hello world under the cell. I think you already mentioned it. The code looks different against classic C Plus plus. In this case, the main function is missing and the reason is reasonable for this. C Plus plus was not designed for interactive applications. Though the semantic and syntax needs small modification. The cling team decides to do not use a main function.
00:02:38.894 - 00:03:18.282, Speaker A: So cling handles every statement which you typed in. So you typed in those statements in the C plus plus global space and cling handled it. But don't worry, internally cling transform it. To valid C code it's necessary to reuse existing software which we see in a later example. But before I want to give you a feeling how it works if you type in some code in the cling. So the first what I do is defining a global variable. So I define the global variable g, one with value one.
00:03:18.282 - 00:04:04.540, Speaker A: Now I define a local variable l, one which lives in the animals namespace. After executing the cell, a local variable is not valid anymore because we left the anonymous namespace. So if I want to access the variable I get an error because of undeclared identifier. Also like in a classic c plus plus application, the hiding rule is working, so I can hide global variable g one with a local variable. In this case the local variable has the value three and I leave the enemy's namespace. So then the local variable is not valid anymore and I have the global one again. So the output should be one, three and one.
00:04:04.540 - 00:04:32.350, Speaker A: And it is like expected. I already mentioned it. Software we use it is supported by cling and we do it via including header files and linking shard libraries or just in time compile external files. In this case I used a self written library. So at first I define a header file. For this I use so called magic command. Magic commands are provided by Jupyter notebook.
00:04:32.350 - 00:05:12.686, Speaker A: In this case the magic component means write the following conduct to the file. In this case the header file. So I define the header file with the namespace foo and a function bar and I also write implementation in the CPP file. With the other magic command, I execute the Gcc to compile shard library. Now I can simply include the header file and with a special program link the shard library and you can see I can simply execute the function. And this you see a more complex example. It's a template class which has the template parameter a and b.
00:05:12.686 - 00:05:53.930, Speaker A: There's also a cuter global function which is executed on the GPU. The corners summarize a and b and we have some extra functionality which allocates memory and a function compute which runs the kernel and copy the result to the cpu. So I define a class and create an object of it. Now I can run the compute function and you see yes, the result is correct. Okay, a short summarize of the most important properties of the cling. Cling uses the readable print loop principle. So you typed in the code, the code is evolated and recite is printed.
00:05:53.930 - 00:07:04.398, Speaker A: Cling is not in top at all feels like, but it's adjust in time compiler. That means we can achieve the same performance like in a classic static in an optimized classic static application, or maybe better, it's fully compatible to existing libraries. So for my example I showed a self written library, but you can also use existing library like the lib SSL, and you don't need modification at the library to use it because Cling uses c, a valid static C interny later when I discuss sample implementation details, you will see why. And we have the modified syntax in semantic, for example, no main function. Or maybe you recognize that the missing semicolon at the end of a statement, which triggers a special function of cling and prints the return value of the last statement. But all this is just allowed in the terminal or in Jupyter notebook as input. Okay, but why we did develop the GPU extension.
00:07:04.398 - 00:07:51.454, Speaker A: To understand this, you have to understand how a GPU works. A cpu has a few strong cores and a GPU has many weak cores, but in some, often a GPU has more flops than a cpu. So if your algorithm is well paralyzed, you get a better performance on a GPU than on a cpu. And we have algorithm. So we wrote a laser wake feed acceleration simulation called Pigon GPU, which performs really well on gpus. And for Nvidia gpus, we use the QR library to execute it. So we have the two reasons gpus are really fast for our simulation and the simulation already exists.
00:07:51.454 - 00:08:40.290, Speaker A: And use the QDR library why we want to execute GPU and cling I will explain you on a later slide. Okay, but now something about a basic concept. How can cling handle every new statement which you typed in and executed? So the answer is it has non ending translation unit. So what it means every time if you type in a statement, you put a new part to the ast. A central concept of the cling is a transaction object. So a transaction object contains different information. For example, the input, the AST and AvMR code, the compiled code, and more information each time.
00:08:40.290 - 00:09:33.598, Speaker A: If you add a new statement or the initial state, you will add a new part to the AST, which you can see on the left side, so you can extend your application during runtime. The transaction system has also two other tasks. One is error recurring. In the case, if you typed in stem in which it's not valid, the transaction will fail and cling goes back to the last successful transaction and does not crash or becomes an invalid state. Other tasks of the transaction system is undoing some code so you can undo a transaction and get an earlier state. For example, you can use it to link, to load and unload a shard library. And also in the meantime, you can modify the shard library.
00:09:33.598 - 00:10:04.302, Speaker A: That allows the concept of raffle prototyping. Okay, let's talk about what this happens. If you create a new transaction. On the left side, you see the different stages of handing a single transaction. We have the input two different a transformer, a code generator, and in the end there's the executor. So I explain each single point with an example. Our input is a function call.
00:10:04.302 - 00:10:31.954, Speaker A: I defined a function before you can see it on the right side. So the first stage after the input is the meta parser. The metaphor has two tasks. The first task is transforming the C code. In this case we need two transformation. One is adding the semicolon and make a mark that cling has to print the return value of fool later. The second task is to add a function wrapper.
00:10:31.954 - 00:11:00.762, Speaker A: This is necessary because function calls are forbidden in the c plus plus global space. The other task of the meta parser is handling so called meta commands. We use matter commands for different tasks. For example linking shard libraries, set the optimization level for debugging and more. Okay, in the end of the meta parser we have valid C code which is necessary for the next stage. The parser. The parser is a non modified clang parser.
00:11:00.762 - 00:11:43.242, Speaker A: This is the reason why we need valid C code, otherwise the clang parser would fail. So the clang parser simply pass the code to ast in LvM error code and now we can use AST transformer to modify the ast. We have three different kinds of transformation. The first kind is enable the basic functionality of clean. For example, for the QDR mode we need a cutout device kernel in item. This modification is necessary that we can use a device function for different statements or in jubilee noble for different cells. The next kind is arrow protection.
00:11:43.242 - 00:12:33.630, Speaker A: We use different transformations to avoid arrows, for example an alpineter access and the last kind of transformations are cling specific features. For example, we have a shadow namespace features. The shadow namespace features allows a kind of redefinition for the user. It looked like you can redefine a function or a class, but internally cling hides existing definition with a namespace and a new definition of the function, and then it handles that. The correct namespace and function is accessed for the user. It looks like redefinition and it's pretty cool. Okay, we have the transformed AST and the AsT is simply compiled just in time to the machine code.
00:12:33.630 - 00:13:31.950, Speaker A: For example, on my notebook to x 86 code and depending on the code it is executed. In the case of our function, it is executed and we get a return value which you can see on the right side. What was the challenge which I have to solve for my CUDA extension? So the first challenge is, is interactive C possible? To develop a CUDA application you have two APIs which you can use. The first is a driver API and the driver API allows to add and remove device code which is executed on your gpu during runtime, so it's perfect for interactive usage. Unfortunately, the driver API was designed to integrate into this and so on. So nobody who wants to develop the application use the driver API. So everybody who is using who wants to develop the application use the runtime API.
00:13:31.950 - 00:14:23.006, Speaker A: So we have to support the runtime API. Unfortunately, the runtime API doesn't allow us to add or remove kernels at runtime, so there is no function. I tested a lot with modified Avmer code and some prototypes and I found out the runtime API can also handle it if you add a kernel at runtime or remove it. And it makes sense because the driver doesn't know at the beginning of application if you use the driver API or runtime API. The second question was how can cling understand CUDA C? CudA C is not fully well at C or C. For example, you have to function call with this special brackets which is not possible in c or c. So for the luckily there was a project.
00:14:23.006 - 00:15:18.242, Speaker A: Google's GPU CC project solved the problem in enabled to compiling CUDA C with the clang IVM. So we only needed to activate in clang, but for the middle parser it was not possible because the middle passer is self implemented. Later I will get you some details about this problem. The last challenge was how to integrate device pipeline to understand what is the device pipeline, you have to understand what is a CUTA application, how a CUDA application works a cuter application has two parts. One is the host part which is executed on the cpu, and one is the device part which is executed on the cpu. So if you run your application, you start your application on the cpu via library commands. The application controls the GPU, and the device code for the GPU is also integrated in the application.
00:15:18.242 - 00:16:24.242, Speaker A: So it means at a point if you want to execute a program, the CPU has to send the device code to the GPU, also the data, and then via library command execute the kernel. But the CPU and GPU has different architecture, so we need two different compiler pipelines to compile the host and the device code for each architecture. That's the reason why I have to integrate a second compiler pipeline in the cling, because the cling without CUDA extension can just compile for the host cpu. There was also some general problems. CUDA is so at first moment it's not so bad because there's a really good documentation, especially for application document development. There's also some background information, which is pretty nice, but at some points I need some information about details which was not documented, so I needed to do some black box testing. Documentation was also a subject for Cling, clang and LVM.
00:16:24.242 - 00:17:13.410, Speaker A: At the beginning of my work I was completely new with the clang and LVM library. So for the LVM part the documentation is really good and I had a lot of examples. The clang documentation has some parts of the clang was not so good documented, but in general it was okay. And I had also some good projects where I can get some information. The cling documentation is really rudimentary because there are just a few developers working on cling and they spend its time to develop new features. And also a really big problem was there is no similar project like Cling, so I had no other source for information. The last general problem was the killer runtime API is not designed for interactive usage.
00:17:13.410 - 00:17:44.378, Speaker A: So I get some really strange problems and there's no experience in the Internet how to solve because nobody has it before. So I had to develop some workarounds by myself. Okay, let's talk about implementation. In this image you can see the concept of the implementation. On the left side you have the input system, then you have both compiler pipelines. The red one is for the host cpu and the green one for the device is the device compiler pipeline. In the end you have the executors.
00:17:44.378 - 00:18:27.680, Speaker A: So at the beginning we have the input and the middle passer which transforms the C Plus plus code to valid C plus plus static C plus plus code. So both parsers get valid C plus plus. At first the device compiler passed the code. Do some AST transformation here some transformations are shared between both compiler pipelines and some transformation exclusive and then the AST is compiled to PDX. PDX is a kind of assembler which can be executed by the Nvidia GPU. The PDX code is wrapped in a fat bin wrapper. The Fatbin wrapper contains the PDX code and some meta information.
00:18:27.680 - 00:19:22.478, Speaker A: After this the host compiler pipeline is running it, pass the code, transform it, and then the code is going to the x 86 back end where it compiles. During the x 86 back end the fat binary code is embedded as code section in the object file and there will also generated some functions. For example a function which later sends register the PDX code on a gpu. If the code is completely compiled, it is executed in the executor and if we reach a function call for Cuda, we send the command to Cuda runtime and the code is executed. Here you can see some libraries. You see, it's a wide mixture between self implemented cling libraries, some clang libraries, and LVm libraries. So let's talk about some detailed problems.
00:19:22.478 - 00:19:56.726, Speaker A: The first was the metaphor. I already mentioned it. The metaphor is completely self written because the parser has to understand the interactive c plus plus semantic. But there is a big problem. The c plus plus semantic is really complex and the cling and the cure extension make it more complex. So there is a lot of work to cover all c plus plus valid cure c plus plus statements. At the moment there is no optimum solution, so we cover just most important cases.
00:19:56.726 - 00:20:42.870, Speaker A: There's also raw input mode which is a workaround. It simply says skip the metap parser and directly send the code to the parser. For the future. We think about to modify a clang parser to understand the interactive c plus plus code and so we can cover every valid QDC statement. Other general problem is catching errors because the user code and the just in time compiler shares the same process in memory space. Error in the user code can crush the whole application and we lost the ast. So our solution at the moment is using analysis, detect the error and modify the code before it is executed.
00:20:42.870 - 00:21:39.878, Speaker A: But this is not a general approach, so we have to develop it for each error case or kind of arrows. Maybe in the future we'll find a better solution for this. The last detailed problem is updating the Clang IVM base. So cling is heavily basing on clang IVM and with each new clang IVM base we get support for new CuriosDK version cpu. We get new c features and a lot of bug fixes, but we can simply change the base because the C AP is not stable anymore and we have also some cling specific patches on the clang. So we have to redo the patches each time if we update a base and it's a lot of work and often it takes also a lot of time. At the moment we work on there is a possible solution.
00:21:39.878 - 00:22:17.560, Speaker A: Vasievasi posted IRC on the clang mailing list to create a simple clang rebel in the LVM project. It was in August. So the idea is if we have the clang repl we can put a lot of clang specific patches for clang to the upstream and so we can reduce the work for the maintainers. So I talked about a lot what is working, but there are also some features missing. So some C Plus plus and QR statements does not work in a QR mode. The reason is really simple. Some CUDA statements need some extra work and the metaphor has also some problems.
00:22:17.560 - 00:22:50.274, Speaker A: We also restricted to the CuDA SDK version eight. The current version is eleven because of the old clang base. Not all cling features are working at a moment yet. So it means for example, redefinition of function classes and kernels does not work at the moment. But I want to work on it and hopefully it works in the future. The middle passer does not detect all killer statements. I already mentioned it, and error catching is a problem for all the time.
00:22:50.274 - 00:23:36.846, Speaker A: So what do you want to do with the cling? So at the beginning I already introduced it. We want to execute our plasma wagfeed simulation interactively. The motivation is that we have the simulation which really works well on HPC system, so we can generate petabytes of data per second. But if we want to analyze it, we have to store to disk and this in the best case we have 1 disk. So it's a big bottleneck. The first solution was to add the analyze to the simulation and so we can avoid the memory bottleneck. But then we have to know what we want to analyze before we run the simulation.
00:23:36.846 - 00:24:18.750, Speaker A: Otherwise we have to change the analysis and execute the simulation again, which can takes hours. So our wish is to have a loosely coupled analysis and we want to do it with Kling. So we want to start the simulation in Kling and add interactively and remove the analysis on runtime. We have also two other ideas what we can do with the cling. One is using it for teaching. We already did it in lecture for the students at a university and it was really well. We gave a drupal notebook with implementing Taskville kernel and for the students it was really good.
00:24:18.750 - 00:25:05.130, Speaker A: The other idea is to use cling for easy development and debugging. I already mentioned it. We can do rapid prototyping and with the reflection feature we get a lot of really important information. For example, we want to use it with our Parker frame and which allows which is a metaprogramming library for pyrization. Okay, let's summarize. So the cling is the first interactive c plus plus just in time compiler which can execute queda c plus plus code on the gpu. I added a dual compiler instance concept to cling which allows us to implement other GPU APIs, for example AMD or Intel.
00:25:05.130 - 00:25:25.470, Speaker A: Most of the features which I presented are in the Kling upstream already, and with Jupyter notebook we enable new areas of application, for example data analysis in notebooks, high performance data analysis in notebooks or teaching. So thank you very much for listening my presentation.
00:25:38.830 - 00:26:09.860, Speaker B: Hello everybody and welcome to the live q a session. You have watched the talk of Simeon Ehrlich. Simeon works at the Helmholt Centrum, Dresden, Rosendorf, Germany, and he is responsible for the Kudel supporting clink. Thank you Simeon. Now let's get to the questions. So have a question. The first question is could you share what kind of CUDA documentation was not available?
00:26:10.870 - 00:26:58.180, Speaker C: Yes, so in general there was some functionality for the range API for function which generates by the compiler, for example registering the judicialness. So I thought the compiler generates the function and then I found simply a header. And so for some arguments I was not sure what this doing. Another problem was how the fat binary format was defined. Thankfully the LVM project cxxjit implements open source version of it, because in our first approach we use the external tool from Nvidia fat bin tool, which takes a lot of time at compiling and now we have integrated it.
00:27:01.830 - 00:27:12.950, Speaker B: Okay, thank you. And the next question is, have you looked at the MDRTC which allows programmatic coder code compilation?
00:27:13.850 - 00:27:56.994, Speaker C: Yes, I saw the documentation and it's not suitable for solution. The reason is really simple. So it is developed to integrate in your application and it takes a string, but we use a compiler. So if we get our code, we pass it and we have the ast and so on, and then we have no string. So we have two options. Either we can try to cut out the original source code and pass it to in, or we generate the source code again from the ast. But in the past I used ast printer and it has a lot of bugs and it doesn't work.
00:27:56.994 - 00:28:14.410, Speaker C: And the other option also not works because we also modify the ast to enable some king specific function. For example in the future I have to add some namespaces to the corners for the shadow function to redefine colors.
00:28:16.990 - 00:28:27.226, Speaker B: Okay, thank you. And is it possible to support hip and gpus?
00:28:27.418 - 00:28:56.520, Speaker C: Yeah, I think it's possible because it's implemented in the clink base and a lot of implementation is shared with the killer implementation and also the workflows are similar. At the moment it's just a problem with the old clang base of clink. So at first we have to upgrade the clang base to get the hip features and then we can support it.
00:28:57.950 - 00:29:05.500, Speaker B: And do you have a feeling of how much time would it take? And is that a priority for you?
00:29:06.270 - 00:29:39.350, Speaker C: I think it would take some time and not a pride to you at the moment because we also use hip for static application in our research center. And at the moment we see it's really fast developing and they fix a lot of bugs and there are a lot of API changes. And so I think at first we have to develop stable static hip applications and then we can think about the interactive usage.
00:29:41.210 - 00:30:00.650, Speaker B: Thank you. I have two more questions. The first one is, given the state that you have described of the CUDA reporting link, are there possible ways to make the current implementation faster?
00:30:02.190 - 00:30:43.290, Speaker C: Yes. So at the moment the combination of the device code and the host code are strictly linear. So it means at first the device code has to be compiled, and then first the device code has to be compiled, then the host code, but for some statements. For example, if you define a function, it's not necessary that you define a kernel, it's not necessary that you have the compiling result immediately because you need adjust at first if you call it. So terrate can be done in parallel.
00:30:46.670 - 00:30:57.440, Speaker B: Thank you. And the last question, what are the next immediately planned features? What's next on your to do list?
00:30:58.530 - 00:31:28.578, Speaker C: On the one side is to support more CUDA features. So I already mentioned it in the video. So we have some trouble with constant memory and global device memory, and our main goal is to support our particle and cell simulation. And it has a set of functions which we have to support, which we need to support, and then also supporting.
00:31:28.594 - 00:31:29.798, Speaker A: The other clear features.
00:31:29.894 - 00:31:48.670, Speaker C: And on the other side, a really interesting link feature is the namespace shadering, which allows a redefinition of functions and so on. And at first I want to enable this in kilo code that we can redefine classes and function and then also kernels.
00:31:50.610 - 00:32:04.580, Speaker B: Okay, thank you. And we have a last minute question, actually, can you please comment on performance? How does performance with clink compare to the performance with the client compiler? And you have around two minutes.
00:32:05.750 - 00:33:00.870, Speaker C: Yeah, in clink we have to separate performance. On the one side we have the performance of compiling, and on the other side we have the performance of executing a code, because in summary it summarizes the execution time of a whole bunch of code. And the runtime of the code is fast as the clang compiler itself because it used the same optimization. So we can use three. And on the device, on compiling time it's okay, because it's limited how many code you can type in. And then the time for compiling is really fast and you can also just in time compile C plus plus source code. But for a real world application you would be compiled with a normal compiler as Shardli, and then the git.
00:33:07.770 - 00:33:09.238, Speaker B: Sorry. Go ahead.
00:33:09.404 - 00:33:13.480, Speaker C: Sorry. So, in general, the performance is really. Well, I would say.
00:33:15.050 - 00:33:38.080, Speaker B: Okay. Thank you. So there is no more questions, but if any questions shows up, you're welcome to discuss everything offline using the community board. Thanks you all for joining, and thank you all for asking the questions. Thank you, Simeon, for your talk.
00:33:38.450 - 00:33:40.170, Speaker C: Thank you, Vasil, for your moderation.
